{"page": 0, "image_path": "doc_images/2023.acl-long.386_0.jpg", "ocr_text": "Fact-Checking Complex Claims with Program-Guided Reasoning\n\nLiangming Pan!” Xiaobao Wu’? XinyuanLu* Anh Tuan Luu?\n\nWilliam Yang Wang! Min-Yen Kan*\n\nPreslav Nakov”\n\n! University of California, Santa Barbara ? MBZUAI\n3 Nanyang Technological University 4 National University of Singapore\n\nliangmingpan@ucsb. edu\n\nxiaobao002@e.ntu.edu.sg luxinyuan@u.nus.edu\n\nanhtuan.luu@ntu.edu.sg william@cs.ucsb. edu\nkanmy@comp.nus.edu.sg preslav.nakov@mbzuai.ac.ae\n\nAbstract\n\nFact-checking real-world claims often re-\nquires collecting multiple pieces of evidence\nand applying complex multi-step reasoning.\nIn this paper, we present Program-Guided\nFact-Checking (PROGRAMFC), a novel fact-\nchecking model that decomposes complex\nclaims into simpler sub-tasks that can be solved\nusing a shared library of specialized functions.\nWe first leverage the in-context learning ability\nof large language models to generate reason-\ning programs to guide the verification process.\nAfterward, we execute the program by delegat-\ning each sub-task to the corresponding sub-task\nhandler. This process makes our model both\nexplanatory and data-efficient, providing clear\nexplanations of its reasoning process and requir-\ning minimal training data. We evaluate PRO-\nGRAMEFC on two challenging fact-checking\ndatasets and show that it outperforms seven\nfact-checking baselines across different settings\nof evidence availability, with explicit output\nprograms that benefit human debugging.!\n\n1 Introduction\n\nThe proliferation of disinformation, e.g., in social\nmedia, has made automated fact-checking a crucial\napplication of natural language processing (NLP).\nGiven a claim, the goal is to find evidence and\nthen to make a verdict about the claim’s veracity\nbased on that evidence (Thorne and Vlachos, 2018;\nGlockner et al., 2022; Guo et al., 2022).\n\nEvaluating the veracity of real-world claims of-\nten involves collecting multiple pieces of evidence\nand applying complex reasoning (Jiang et al., 2020;\nNguyen et al., 2020; Aly and Vlachos, 2022; Chen\net al., 2022a). For instance, consider the claim\n“Both James Cameron and the director of the film\nInterstellar were born in Canada”. It may be chal-\nlenging to find direct evidence on the web that\nrefutes or supports this claim.\n\n'The program code and the data are publicly available at\nhttps: //github.com/mbzuai-nlp/ProgramFC\n\nInstead, a human fact-checker needs to decom-\npose the claim, gather multiple pieces of evidence,\nand perform step-by-step reasoning (Nakov et al.,\n2021a), as illustrated in Figure 1. This makes veri-\nfying complex claims much more challenging than\nthe typical setting explored in previous work, where\ninformation from a single article is sufficient to sup-\nport/refute the claim (Thorne et al., 2018; Saakyan\net al., 2021; Schuster et al., 2021; Pan et al., 2021;\nWadden et al., 2022a; Krishna et al., 2022).\n\nBesides multi-step reasoning, we still need to\nconsider two key aspects for developing a reliable\nfact-checking system: (i) Explanability: The model\nshould not only predict the veracity of the claim,\nbut it should also provide a clear explanation of its\nreasoning process to help users understand and trust\nthe results. (ii) Data efficiency: Human annotation\nis often time-consuming, costly, and potentially\nbiased, making it difficult to collect sufficient high-\nquality labeled data for model training, particularly\nfor complex claims. Therefore, it is desirable to\nbuild a model that can perform well with minimal\nor no training data. Despite a few models (Zhou\net al., 2019; Zhong et al., 2020; Aly and Vlachos,\n2022) being proposed to facilitate multi-step rea-\nsoning in fact-checking, they either lack explain-\nability in their reasoning process or require a large\nnumber of task-specific training examples.\n\nIn this paper, we present Program-Guided Fact-\nChecking (PROGRAMFC), a novel fact-checking\nframework that is both explanatory and data-\nefficient. Figure | illustrates our approach. To\nverify complex claims, PROGRAMFC decomposes\nthem into simpler sub-tasks that can be solved us-\ning a shared library of specialized sub-task func-\ntions. To be specific, PROGRAMFC begins by gen-\nerating a reasoning program for the input claim,\nwhich is a sequence of sub-tasks (e.g., S1-S4 in\nFigure 1) in the form of ACTION[ARGUMENT],\nwhere ACTION and ARGUMENT define the type\nand the content of the sub-task, respectively.\n\n6981\n\nProceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 6981-7004\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\n", "vlm_text": "Fact-Checking Complex Claims with Program-Guided Reasoning \nLiangming Pan 1 ,   Xiaobao   $\\mathbf{W}\\mathbf{u}^{3}$  Xinyuan  $\\mathbf{L}\\mathbf{u}^{4}$  Anh Tuan Luu 3 William Yang Wang 1 Min-Yen Kan 4 Preslav Nakov 2 \n1  University of California, Santa Barbara 2  MBZUAI 3  Nanyang Technological University 4  National University of Singapore liang ming pan@ucsb.edu xiaobao002@e.ntu.edu.sg luxinyuan@u.nus.edu anhtuan.luu@ntu.edu.sg william@cs.ucsb.edu kanmy@comp.nus.edu.sg preslav.nakov@mbzuai.ac.ae \nAbstract \nFact-checking real-world claims often re- quires collecting multiple pieces of evidence and applying complex multi-step reasoning. In this paper, we present  Program-Guided Fact-Checking  (P ROGRAM FC), a novel fact- checking model that decomposes complex claims into simpler sub-tasks that can be solved using a shared library of specialized functions. We first leverage the in-context learning ability of large language models to generate  reason- ing programs  to guide the verification process. Afterward, we  execute  the program by delegat- ing each sub-task to the corresponding sub-task handler. This process makes our model both explanatory and data-efficient, providing clear explanations of its reasoning process and requir- ing minimal training data. We evaluate P RO - GRAM FC on two challenging fact-checking datasets and show that it outperforms seven fact-checking baselines across different settings of evidence availability, with explicit output programs that benefit human debugging. \n1 Introduction \nThe proliferation of disinformation,  e.g. , in social media, has made  automated fact-checking  a crucial application of natural language processing (NLP). Given a  claim , the goal is to find  evidence  and then to make a  verdict  about the claim’s veracity based on that evidence ( Thorne and Vlachos ,  2018 ; Glockner et al. ,  2022 ;  Guo et al. ,  2022 ). \nEvaluating the veracity of real-world claims of- ten involves collecting multiple pieces of evidence and applying complex reasoning ( Jiang et al. ,  2020 ; Nguyen et al. ,  2020 ;  Aly and Vlachos ,  2022 ;  Chen et al. ,  2022a ). For instance, consider the claim “ Both James Cameron and the director of the film Interstellar were born in Canada ”. It may be chal- lenging to find direct evidence on the web that refutes or supports this claim. \nInstead, a human fact-checker needs to decom- pose the claim, gather multiple pieces of evidence, and perform step-by-step reasoning ( Nakov et al. , 2021a ), as illustrated in Figure  1 . This makes veri- fying complex claims much more challenging than the typical setting explored in previous work, where information from a single article is sufficient to sup- port/refute the claim ( Thorne et al. ,  2018 ;  Saakyan et al. ,  2021 ;  Schuster et al. ,  2021 ;  Pan et al. ,  2021 ; Wadden et al. ,  2022a ;  Krishna et al. ,  2022 ). \nBesides multi-step reasoning, we still need to consider two key aspects for developing a reliable fact-checking system: ( i )  Ex plan ability : The model should not only predict the veracity of the claim, but it should also provide a clear explanation of its reasoning process to help users understand and trust the results. ( ii )  Data efficiency : Human annotation is often time-consuming, costly, and potentially biased, making it difficult to collect sufficient high- quality labeled data for model training, particularly for complex claims. Therefore, it is desirable to build a model that can perform well with minimal or no training data. Despite a few models ( Zhou et al. ,  2019 ;  Zhong et al. ,  2020 ;  Aly and Vlachos , 2022 ) being proposed to facilitate multi-step rea- soning in fact-checking, they either lack explain- ability in their reasoning process or require a large number of task-specific training examples. \nIn this paper, we present  Program-Guided Fact- Checking  (P ROGRAM FC), a novel fact-checking framework that is both explanatory and data- efficient. Figure  1  illustrates our approach. To verify complex claims, P ROGRAM FC decomposes them into simpler sub-tasks that can be solved us- ing a shared library of specialized sub-task func- tions. To be specific, P ROGRAM FC begins by gen- erating a  reasoning program  for the input claim, which is a sequence of sub-tasks ( e.g. , S1-S4 in Figure  1 ) in the form of A CTION [A RGUMENT ], where A CTION  and A RGUMENT  define the type and the content of the sub-task, respectively. "}
{"page": 1, "image_path": "doc_images/2023.acl-long.386_1.jpg", "ocr_text": "Claim: Both James Cameron and the director of the film Interstellar were born in Canada.\n| Knowledge\nReasoning Program Functions Source\nLanguage\nModels @) Verify [James Cameron was born in Canada.] > Fact x\n(Codex, GPT3, ...) | =TRUE < Checker l=\n. . . . Gold Evidence\n@) Question [Who is the director of the film Interstellar?] > Qn .\nre’\n| = Christopher Nolan <}— Model i y\n{ Wixirep1A\n5 (63) Verify [ was born in Canada.] > Fact Open-book\nGlen. “ = FALSE <—}/ Checker\nP Claim: --- S\nClaim: --- 4\nProgram: «-- Predict [ AND ] ”| Logical Closed-book\n_ x) REFUTES < Reasoner\nExemplars ~\np yy Ne\n\nFigure 1: Overview of our PROGRAMFC model, which consists of two modules: (i) Program Generation generates\na reasoning program for the input claim using Codex with in-context learning, and then (ii) Program Execution\nsequentially interprets the program by delegating each step to the corresponding sub-task function.\n\nThe generated reasoning program serves as a\nstep-by-step guide for verifying the claim. We\nthen execute the program by sequentially delegat-\ning each sub-task to the corresponding sub-task\nhandler, as shown in the functions columns in Fig-\nure 1. These sub-tasks may include answering\nquestions, verifying simple claims, or conducting\nlogical reasoning.\n\nPROGRAMFC combines explainability with data\nefficiency. It uses reasoning programs to provide\nclear explanations of its reasoning process. For\ndata efficiency, Large Language Models (LLMs)\ncan solve various tasks given only a few exam-\nples as prompts, e.g., in-context learning (Brown\net al., 2020). We leverage this ability of LLMs to\ngenerate reasoning programs for a given claim by\nshowing the model just a few dozen of (claim, pro-\ngram) pairs as demonstrations. PROGRAMFC is\nalso flexible as it allows for easy swapping of sub-\ntask function implementations to work under dif-\nferent settings of fact-checking, without affecting\nthe rest of the system. We can allow the functions\nto retrieve information from external sources (in\nan open-book setting) or we can ask them to gen-\nerate answers based solely on the LLM’s internal\nparametric knowledge (in a closed-book setting).\n\nWe evaluate PROGRAMFC on two challeng-\ning datasets designed for fact-checking complex\nclaims: HOVER (Jiang et al., 2020) and FEVER-\nOUS (Aly et al., 2021), and we show that it outper-\nforms seven few-shot fact-checking baselines on\nboth datasets (§ 4.1).\n\nThe strategy of program-guided reasoning be-\ncomes increasingly effective as the required reason-\ning depth increases (§ 4.1). In the open-domain set-\nting, we find that reasoning programs can enhance\nthe retrieval of relevant evidence from knowledge\nsources (§ 4.2). Moreover, PROGRAMFC is robust\neven when we use weak models as sub-task solvers\n(§ 4.2). We also evaluate the interpretability of the\nreasoning programs through human evaluation and\nerror analysis (§ 4.3).\n\n2 Related Work\n\nFact-Checking. Automated fact-checking has\ngained significant attention in the NLP research\ncommunity in recent years as a means of combat-\ning misinformation and disinformation. Various\ndatasets have been proposed that enable the devel-\nopment and the evaluation of systems for automatic\nfact-checking, the most popular ones being based\non human-crafted claims from Wikipedia con-\ntent (Thorne et al., 2018; Sathe et al., 2020; Schus-\nter et al., 2021) and naturally occurring claims\nin the political or in the scientific domain (Wang,\n2017; Nakov et al., 2021b, 2022; Augenstein et al.,\n2019; Saakyan et al., 2021; Gupta and Srikumar,\n2021; Wadden et al., 2020, 2022a). Notably, most\nof these datasets are constructed in a way that\nthe evidence to support or to refute a claim can\nbe found in a single document. For example, in\nFEVER (Thorne et al., 2018), more than 87% of\nthe claims only require information from a single\nWikipedia article (Jiang et al., 2020).\n\n6982\n", "vlm_text": "The image presents a diagram of the PROGRAM FC model for reasoning about a claim. Here's a breakdown:\n\n- **Claim:** The assertion is that both James Cameron and the director of the film \"Interstellar\" were born in Canada.\n  \n- **Language Models:** Codex, GPT-3, and others are used for processing, enhanced by exemplars.\n\n- **Reasoning Program:**\n  \n  - **S1:** Verifies if \"James Cameron was born in Canada.\" Result: `FACT_1 = TRUE` using a Fact Checker.\n  \n  - **S2:** Asks, \"Who is the director of the film Interstellar?\" Result: `ANSWER_1 = Christopher Nolan` using a QA Model.\n  \n  - **S3:** Verifies if `{ANSWER_1} was born in Canada.\" Result: `FACT_2 = FALSE` using a Fact Checker.\n  \n  - **S4:** Predicts the outcome of `{FACT_1} AND {FACT_2}`. Result: `PREDICTED_LABEL = REFUTES` using a Logical Reasoner.\n\n- **Functions:** Fact Checkers and QA Model are utilized for different kinds of verification.\n  \n- **Knowledge Source:** Utilizes Gold Evidence, Wikipedia (open-book source), and closed-book sources for information.\n\nThis model showcases a method to process and verify claims using various AI techniques and sources.\nThe generated reasoning program serves as a step-by-step guide for verifying the claim. We then  execute  the program by sequentially delegat- ing each sub-task to the corresponding sub-task handler, as shown in the  functions  columns in Fig- ure  1 . These sub-tasks may include answering questions, verifying simple claims, or conducting logical reasoning. \nP ROGRAM FC combines explain ability with data efficiency. It uses reasoning programs to provide clear explanations of its reasoning process. For data efficiency, Large Language Models (LLMs) can solve various tasks given only a few exam- ples as prompts,  e.g. ,  in-context learning  ( Brown et al. ,  2020 ). We leverage this ability of LLMs to generate reasoning programs for a given claim by showing the model just a few dozen of (claim, pro- gram) pairs as demonstrations. P ROGRAM FC is also flexible as it allows for easy swapping of sub- task function implementations to work under dif- ferent settings of fact-checking, without affecting the rest of the system. We can allow the functions to retrieve information from external sources (in an open-book setting) or we can ask them to gen- erate answers based solely on the LLM’s internal parametric knowledge (in a closed-book setting). \nWe evaluate P ROGRAM FC on two challeng- ing datasets designed for fact-checking complex claims: HOVER ( Jiang et al. ,  2020 ) and FEVER- OUS ( Aly et al. ,  2021 ), and we show that it outper- forms seven few-shot fact-checking baselines on both datasets   $(\\S\\,4.1)$  . \nThe strategy of program-guided reasoning be- comes increasingly effective as the required reason- ing depth increases   $(\\S\\,4.1)$  . In the open-domain set- ting, we find that reasoning programs can enhance the retrieval of relevant evidence from knowledge sources   $(\\S\\ 4.2)$  . Moreover, P ROGRAM FC is robust even when we use weak models as sub-task solvers  $(\\S\\ 4.2)$  . We also evaluate the interpret ability of the reasoning programs through human evaluation and error analysis   $(\\S\\ 4.3)$  . \n2 Related Work \nFact-Checking. Automated fact-checking has gained significant attention in the NLP research community in recent years as a means of combat- ing misinformation and disinformation. Various datasets have been proposed that enable the devel- opment and the evaluation of systems for automatic fact-checking, the most popular ones being based on human-crafted claims from Wikipedia con- tent ( Thorne et al. ,  2018 ;  Sathe et al. ,  2020 ;  Schus- ter et al. ,  2021 ) and naturally occurring claims in the political or in the scientific domain ( Wang , 2017 ;  Nakov et al. ,  2021b ,  2022 ;  Augenstein et al. , 2019 ;  Saakyan et al. ,  2021 ;  Gupta and Srikumar , 2021 ;  Wadden et al. ,  2020 ,  2022a ). Notably, most of these datasets are constructed in a way that the evidence to support or to refute a claim can be found in a  single  document. For example, in FEVER ( Thorne et al. ,  2018 ), more than  $87\\%$   of the claims only require information from a single Wikipedia article ( Jiang et al. ,  2020 ). "}
{"page": 2, "image_path": "doc_images/2023.acl-long.386_2.jpg", "ocr_text": "To bridge this gap, datasets have been proposed\nto study fact-checking complex claims that require\nmulti-step reasoning (Jiang et al., 2020; Aly et al.,\n2021). Graph-based models (Zhou et al., 2019;\nLiu et al., 2020; Zhong et al., 2020; Nguyen et al.,\n2020; Barnabo et al., 2022, 2023) are used to fa-\ncilitate the reasoning over multiple pieces of evi-\ndence. Although such models achieve sizable per-\nformance gains, they lack explanability and thet\nrely on large amounts of training data. To address\nthe above problems, we propose an explainable,\nflexible, and data-efficient model that generates\nreasoning graphs as explanations and utilizes in-\ncontext learning to enable few-shot learning.\n\nExplanation Generation. Facing the complex-\nities of real-world claims, simply giving a final\nveracity to a claim often fails to be persuasive (Guo\net al., 2022). Previous research has proposed\nvarious approaches to provide post-hoc explana-\ntions for model predictions, such as using atten-\ntion weights to highlight relevant parts of the ev-\nidence (Popat et al., 2017; Cui et al., 2019; Yang\net al., 2019; Lu and Li, 2020), generating justifi-\ncations with logic-based systems based on knowl-\nedge graphs (Gad-Elrab et al., 2019; Ahmadi et al.,\n2019), and generating a summary of the retrieved\nrelevant evidence (Atanasova et al., 2020; Kotonya\nand Toni, 2020; Jolly et al., 2022). In contrast, we\npropose to use reasoning programs to provide ex-\nplanations that consist of sub-tasks described in a\nprogram-like natural language. This offers several\nadvantages: it allows for explanations that are not\nconfined to the evidence, like attention weights, it\nis more flexible than logic-based explanations, and\nit is more concise than free-form summarization.\n\nChain-of-Thought Reasoning. Moreover, un-\nlike previous work that generates post-hoc explana-\ntions, we also use reasoning programs as guidance\nfor predicting the veracity of the claim. This is mo-\ntivated by the recent success of chain-of-thought\nprompting (CoT) (Wei et al., 2022; Kojima et al.,\n2022; Wang et al., 2022), which generates step-by-\nstep natural language reasoning steps to guide the\nmodel in answering complex questions. We adopt\nthis idea to fact-checking complex claims. Unlike\nthe original CoT, which uses a single LLM for both\ndecomposition and question answering, we use the\nlanguage model only to generate reasoning pro-\ngrams as the blueprint for problem-solving, and we\ndelegate each sub-task to specialized functions.\n\nThis approach reduces the burden on the lan-\nguage model and allows for more flexibility\nin incorporating necessary components for fact-\nchecking such as an evidence retriever. The strat-\negy of program-guided reasoning is also in line\nwith the recent trend of tool-augmented language\nmodels (Mialon et al., 2023; Schick et al., 2023),\ni.e., augmenting language models with access to\nexternal tools and resources.\n\n3 PROGRAMFC\n\nWe first formulate the problem of fact-checking and\nthen we introduce our proposed model for Program-\nGuided Fact-Checking (PROGRAMFC).\n\n3.1 Problem Formulation\n\nGiven a claim C, a fact-checking model F aims to\npredict a label Y to evaluate the claim as TRUE or\nFALSE, based on a knowledge source K.. The model\nis also required to output an explanation E to jus-\ntify the predicted veracity label. We summarize\nthree different settings of fact-checking depending\non the type of knowledge source K.\n\ne Gold evidence: For each claim, K is the set\nof gold evidence documents that can support or\nrefute the claim. This setting is also called claim\nverification (Pan et al., 2021; Wright et al., 2022).\ne Open-book setting: XK is a large textual corpus\nsuch as Wikipedia. The model first retrieves rele-\nvant evidence from the corpus and then predicts the\nveracity label based on the evidence (Jiang et al.,\n2021; Wadden et al., 2022b).\n\ne Closed-book setting: The model does not have\naccess to any external knowledge source (K = 0)).\nIt needs to leverage the knowledge stored in its\nparameters (acquired during pre-training and fine-\ntuning) to verify the claim. This setting was ex-\nplored in work that applies large language models\nfor fact-checking (Lee et al., 2020, 2021).\n\n3.2. Program-Guided Reasoning\n\nOur goal is to fact-check a complex claim C that\nrequires multi-step reasoning. We focus on the few-\nshot setting, where only a small set of in-domain\nexamples are available to teach the model. To solve\nthis, PROGRAMFC follows a program generation-\nand-execution paradigm, as shown in Figure 1.\n\nProgram Generation. At this stage, given the\ninput claim C, a planner P generates a reasoning\nprogram P = [S\\,-++,S;,] for it, which consists\nof n sequentially ordered reasoning steps Sj.\n\n6983\n", "vlm_text": "To bridge this gap, datasets have been proposed to study fact-checking complex claims that require multi-step reasoning ( Jiang et al. ,  2020 ;  Aly et al. , 2021 ). Graph-based models ( Zhou et al. ,  2019 ; Liu et al. ,  2020 ;  Zhong et al. ,  2020 ;  Nguyen et al. , 2020 ;  Barnabò et al. ,  2022 ,  2023 ) are used to fa- cilitate the reasoning over multiple pieces of evi- dence. Although such models achieve sizable per- formance gains, they lack ex plan ability and thet rely on large amounts of training data. To address the above problems, we propose an explain able, flexible, and data-efficient model that generates reasoning graphs as explanations and utilizes in- context learning to enable few-shot learning. \nExplanation Generation. Facing the complex- ities of real-world claims, simply giving a final veracity to a claim often fails to be persuasive ( Guo et al. ,  2022 ). Previous research has proposed various approaches to provide post-hoc explana- tions for model predictions, such as using atten- tion weights to highlight relevant parts of the ev- idence ( Popat et al. ,  2017 ;  Cui et al. ,  2019 ;  Yang et al. ,  2019 ;  Lu and Li ,  2020 ), generating justifi- cations with logic-based systems based on knowl- edge graphs ( Gad-Elrab et al. ,  2019 ;  Ahmadi et al. , 2019 ), and generating a summary of the retrieved relevant evidence ( Atanasova et al. ,  2020 ;  Kotonya and Toni ,  2020 ;  Jolly et al. ,  2022 ). In contrast, we propose to use reasoning programs to provide ex- planations that consist of sub-tasks described in a program-like natural language. This offers several advantages: it allows for explanations that are not confined to the evidence, like attention weights, it is more flexible than logic-based explanations, and it is more concise than free-form sum mari z ation. \nChain-of-Thought Reasoning. Moreover, un- like previous work that generates post-hoc explana- tions, we also use reasoning programs as guidance for predicting the veracity of the claim. This is mo- tivated by the recent success of chain-of-thought prompting (CoT) ( Wei et al. ,  2022 ;  Kojima et al. , 2022 ;  Wang et al. ,  2022 ), which generates step-by- step natural language reasoning steps to guide the model in answering complex questions. We adopt this idea to fact-checking complex claims. Unlike the original CoT, which uses a single LLM for both decomposition and question answering, we use the language model only to generate reasoning pro- grams as the blueprint for problem-solving, and we delegate each sub-task to specialized functions. \nThis approach reduces the burden on the lan- guage model and allows for more flexibility in incorporating necessary components for fact- checking such as an evidence retriever. The strat- egy of program-guided reasoning is also in line with the recent trend of tool-augmented language models ( Mialon et al. ,  2023 ;  Schick et al. ,  2023 ), i.e. , augmenting language models with access to external tools and resources. \n3 P ROGRAM FC \nWe first formulate the problem of fact-checking and then we introduce our proposed model for  Program- Guided Fact-Checking  (P ROGRAM FC). \n3.1 Problem Formulation \nGiven a  claim    $C$   a  fact-checking model    $\\mathcal{F}$  aims to predict a  label  $Y$   to evaluate the claim as  TRUE  or FALSE , based on a  knowledge source  $\\mathcal{K}$  . T  model is also required to output an  explanation  E  to jus- tify the predicted veracity label. We summarize three different settings of fact-checking depending on the type of knowledge source  $\\mathcal{K}$  . \n•  Gold evidence : For each claim,    $\\mathcal{K}$   is the set of gold evidence documents that can support or refute the claim. This setting is also called  claim verification  ( Pan et al. ,  2021 ;  Wright et al. ,  2022 ). \n•  Open-book setting :    $\\mathcal{K}$   is a large textual corpus such as Wikipedia. The model first retrieves rele- vant  evidence  from the corpus and then predicts the veracity label based on the evidence ( Jiang et al. , 2021 ;  Wadden et al. ,  2022b ). \n•  Closed-book setting : The model does not have access to any external knowledge source   $(\\mathcal{K}=\\varnothing)$  ). It needs to leverage the knowledge stored in its parameters (acquired during pre-training and fine- tuning) to verify the claim. This setting was ex- plored in work that applies large language models for fact-checking ( Lee et al. ,  2020 ,  2021 ). \n3.2 Program-Guided Reasoning \nOur goal is to fact-check a complex claim    $C$   that requires multi-step reasoning. We focus on the  few- shot  setting, where only a small set of in-domain examples are available to teach the model. To solve this, P ROGRAM FC follows a  program generation- and-execution  paradigm, as shown in Figure  1 . \nProgram Generation. At this stage, given the input claim    $C$  , a  planner  $\\mathcal{P}$   generates a  reasoning program    $P=[S_{1},\\cdot\\cdot\\cdot,S_{n}]$   for it, which consists of    $n$   sequentially ordered  reasoning steps    $S_{i}$  . "}
{"page": 3, "image_path": "doc_images/2023.acl-long.386_3.jpg", "ocr_text": "Each reasoning step S; € P is an instruction\nin controlled natural language that directs S; to\na function in an auxiliary set of sub-task func-\ntions F available to the system. To be specific,\nwe define 5; = (fi, Ai, Vi), where fj; specifies\nthe sub-task function f; € F, A; is the argument\npassed to the function f;, and V; is the variable\nthat stores the returned result from the function call\nfi(A;). For a valid reasoning program, the return\nvalue of the last reasoning step must be a Boolean\nvalue indicating the veracity label of the claim C,\nie, V, € {TRUE, FALSE}.\n\nProgram Execution. In the execution stage, the\nreasoning program P is run by an interpreter to\nderive the veracity label of the claim C. The in-\nterpreter sequentially parses the reasoning steps in\nP. For each step 5; = (fi, Ai, Vi), it calls the cor-\nresponding off-the-shelf sub-task function f; and\npasses the argument A; to it. The argument A; is ei-\nther a logical expression or a natural language sen-\ntence, e.g., a question or a simple claim. The result\nof the function call is then stored in the variable Vj.\nAs it is common for a subsequent step to depend\non the results from previous steps, we allow the\nargument A; to refer to variables V;,---,Vj—1 in\nprevious steps. For example, in Figure 1, the argu-\nment in S3 is “{ANSWER_1} was born in Canada.”,\nwhich refers to the return variable f{ANSWER_1}\nfrom Sy. When executing 53, the variable is re-\nplaced by its actual value, and the argument be-\ncomes “Christopher Nolan was born in Canada’.\nAfter executing the last step, the return value is the\npredicted veracity of the claim C.\n\nAggregating Reasoning Paths. Note that there\nmight be multiple reasoning paths that can reach\nthe final veracity label. Therefore, we generate\na diverse set of N candidate reasoning programs\nP = {Pi,:-+, Pw} for the input claim. After exe-\ncuting all programs in P, we take the majority vote\nover all N predicted labels as the final label. This\napproach is similar to how humans rely on multiple\nmethods of validation to increase their confidence\nin fact-checking. It also makes the model less sus-\nceptible to errors in individual reasoning programs.\n\n3.3, Reasoning Program Generation\n\nWe base our program generator on Codex (Chen\net al., 2021), a code-pretrained LLM, which can\nparse natural language into symbolic representa-\ntions such as SQL (Cheng et al., 2022) or Python\nprograms (Gao et al., 2022; Chen et al., 2022b).\n\nHowever, the grammar of a reasoning program\nis different from the grammar of a programming\nlanguage. We take advantage of Codex’s few-shot\ngeneralization ability and we find that it can learn\neffectively from only a small number of in-context\nexamples D = {d1,---,d)pj)}. Each example d;\nconsists of a claim and a program. The program has\na Python-like grammar, where each reasoning step\nis written in the format V; = f;(A;). At inference\ntime, we prompt Codex with an instruction of the\ntask, AK in-context examples, and the input claim\nC. Codex then attempts to complete the follow-\ning texts, and thereby generates a program for C.\nThe prompt template is shown in Figure 2. We use\nke = 20 to maintain a tradeoff between the diver-\nsity of reasoning types and the model’s maximum\ninput capacity. We use sampling-based decoding\n(temperature of 0.7) to generate different reasoning\nprograms for multiple runs.\n\n3.4 Sub-Task Functions\n\nWe implement three sub-task functions for the\nmodel to call during the program execution.\n\n¢ QUESTION: This sub-task function is a question-\nanswering module that takes a question @ as the\ninput argument and returns the answer A to the\nquestion. We use FLAN-T5 (Chung et al., 2022), an\nimproved T5 model (Raffel et al., 2020) pretrained\non more than 1.8K tasks with instruction tuning,\nwhich has achieved state-of-the-art zero/few-shot\nperformance on many QA benchmarks. As shown\nin Figure 3, we prompt the model differently de-\npending on the settings defined in Section 3.1. For\nthe closed-book setting, the input prompt is\n\nQ: UES? The answer is:\n\nFor the other two settings, the input prompt is\n\nEVIDENCE (@Kag QUESTION @\n\nThe answer is:\n\ne VERIFY: This is a fact verification module that\ntakes a claim C as the input argument and returns\na label of either TRUE or FALSE. We also use\nFLAN-T5 for this module, by prompting the model\nwith the following question-answering format.\n\nEVIDENCE\nQ: Is it true that (@E\\)?\n\nTrue or False? The answer is:\n\ne PREDICT: This module takes as input a logical\nexpression that performs AND, OR, NOT operations\nover the variables in the previous steps. Its output\nis returned as the predicted veracity label.\n\n6984\n", "vlm_text": "Each  reasoning step    $S_{i}\\,\\in\\,P$   is an instru on in controlled natural language that directs  $S_{i}$   to a function in an auxiliary set of sub-task func- tions    $\\mathcal{F}$   available to the system. To be specific, we define    $S_{i}\\ =\\ (f_{i},A_{i},V_{i})$  , where    $f_{i}$   specifies the sub-task function  $f_{i}\\in\\mathcal{F}$  ,    $A_{i}$  is the  argument passed to the function  f  $f_{i}$  , and  $V_{i}$   is the  variable that stores the returned result from the function call  $f_{i}(A_{i})$  . For a valid reasoning program, the return value of the last reasoning step must be a Boolean value indicating the veracity label of the claim    $C$  , i.e. ,    $V_{n}\\in\\left\\{{\\mathrm{TRE}},{\\mathrm{FastSE}}\\right\\}$  . \nProgram Execution. In the execution stage, the reasoning program    $P$   is run by an  interpreter  to derive the veracity label of the claim  $C$  . The in- terpreter sequentially parses the reasoning steps in  $P$  . For each step    $S_{i}=(f_{i},A_{i},V_{i})$  , it calls the cor- responding off-the-shelf  sub-task function    $f_{i}$   and passes the argument  $A_{i}$   to it. The argument  $A_{i}$   is ei- ther a logical expression or a natural language sen- tence,  e.g. , a question or a simple claim. The result of the function call is then stored in the variable  $V_{i}$  . As it is common for a subsequent step to depend on the results from previous steps, we allow the argument  $A_{i}$   to  refer to  variables    $V_{1},\\cdot\\cdot\\cdot,V_{i-1}$   in previous steps. For example, in Figure  1 , the argu- ment in    $S_{3}$   is   $\\ddot{}\\langle A N S W E R\\_I\\rangle$   was born in Canada. ”, which refers to the return variable    $\\{A N S W E R\\_I\\}$  from    $S_{2}$  . When executing    $S_{3}$  , the variable is re- placed by its actual value, and the argument be- comes “ Christopher Nolan was born in Canada ”. After executing the last step, the return value is the predicted veracity of the claim    $C$  . \nAggregating Reasoning Paths. Note that there might be multiple reasoning paths that can reach the final veracity label. Therefore, we generate a diverse set of    $N$   candidate reasoning programs  $\\mathcal{P}=\\{P_{1},\\cdot\\cdot\\cdot,P_{N}\\}$   fo he input claim. After exe- cuting a rograms in  P , we take the majority vote over all  N  predicted labels as the final label. This approach is similar to how humans rely on multiple methods of validation to increase their confidence in fact-checking. It also makes the model less sus- ceptible to errors in individual reasoning programs. \n3.3 Reasoning Program Generation \nWe base our program generator on  Codex  ( Chen et al. ,  2021 ), a code-pretrained LLM, which can parse natural language into symbolic representa- tions such as SQL ( Cheng et al. ,  2022 ) or Python programs ( Gao et al. ,  2022 ;  Chen et al. ,  2022b ). \nHowever, the grammar of a reasoning program is different from the grammar of a programming language. We take advantage of Codex’s few-shot generalization ability and we find that it can learn effectively from only a small number of in-context examples    ${\\mathcal{D}}=\\{d_{1},\\cdot\\cdot\\cdot,d_{|D|}\\}$  . Each example    $d_{i}$  consists of a claim and a program. The program has a Python-like grammar, where each reasoning step is written in the format    $V_{i}=f_{i}(A_{i})$  . At inference time, we prompt Codex with an instruction of the task,  $K$   in-context examples, and the input claim  $C$  . Codex then attempts to complete the follow- ing texts, and thereby generates a program for    $C$  . The prompt template is shown in Figure  2 . We use  $K=20$   to maintain a tradeoff between the diver- sity of reasoning types and the model’s maximum input capacity. We use sampling-based decoding (temperature of 0.7) to generate different reasoning programs for multiple runs. \n3.4 Sub-Task Functions \nWe implement three sub-task functions for the model to call during the program execution. \n•  Q UESTION :  This sub-task function is a question- answering module that takes a question    $Q$   as the input argument and returns the answer    $A$   to the question. We use  FLAN-T5  ( Chung et al. ,  2022 ), an improved T5 model ( Raffel et al. ,  2020 ) pretrained on more than 1.8K tasks with instruction tuning, which has achieved state-of-the-art zero/few-shot performance on many QA benchmarks. As shown in Figure  3 , we prompt the model differently de- pending on the settings defined in Section  3.1 . For the closed-book setting, the input prompt is \nThe image contains the text \"Q: QUESTION ? The answer is:\". The word \"QUESTION\" is highlighted in a gray box.\nFor the other two settings, the input prompt is \nThe image appears to show a template or layout for a question-and-answer format. It includes a section labeled \"EVIDENCE,\" followed by \"Q: QUESTION ?\" and then \"The answer is:\" suggesting a structure where evidence is provided, a question is asked, and then an answer is given. This format might be used in educational or analytical contexts.\n•  V ERIFY :  T s is a fact verification module that takes a claim  C  as the input argument and returns a label of either T RUE  or F ALSE . We also use FLAN-T5  for this module, by prompting the model with the following question-answering format. \nQ: Is it true that  CLAIM  ? True or False? The answer is: \n•  P REDICT :  This module takes as input a logical expression that performs  AND ,  OR ,  NOT  operations over the variables in the previous steps. Its output is returned as the predicted veracity label. "}
{"page": 4, "image_path": "doc_images/2023.acl-long.386_4.jpg", "ocr_text": "'''Generate a python-like program that describes the reasoning steps\n\nrequired to verify the claim step-by-step.\nQuestion() to answer a question; 2.\nPredict() to predict the veracity label.'''\n\nin the program: 1.\nverify a simple claim; 3.\n\nYou can call three functions\nVerify() to\n\n# The claim is that Both James Cameron and the director of the film\n\nInterstellar were born in Canada.\ndef program():\n\nfact_1 = Verify(\"James Cameron was born in Canada.\")\n\nAnswer_1\n\n= Question(\"Who is the director of the film Interstellar?”)\n\nfact_2 = Verify(\"{Answer_1} was born in Canada.\"”)\n\nlabel = Predict(fact_1 and fact_2)\n\n(--- more in-context examples here ---)\n\n# The claim is that <input_claim>\ndef program():\n\nFigure 2: The Codex prompt template used to generate reasoning programs, consisting of a task instruction,\nin-context examples, and a prompt for the <input_claim>. The full templates are given in Appendix D.\n\n<Gold Evidence>\n\nQ: <Question>\nGold Evidence o: sO\nThe answer is:\nOpen-book\ni <Retrieved Evidence>\n\n<ouestion> —> IMB — 0: <auestion>\n\nRetriever The answer is:\nClosed-book Q: <Question>\n\nThe answer is:\n\nFLAN-TS [> Ans\n\nFigure 3: Implementation of the question-answering\nsub-task function for three different settings.\n\n4 Experiments\n\nDatasets. Most fact-checking datasets consist pri-\nmarily of simple claims that can be substantiated\nthrough a single piece of evidence. However, here\nwe focus on complex claims that need multi-step\nreasoning. Given this context, we opt to evalu-\nate our model on the only two datasets that, to\nthe best of our knowledge, fulfill these criteria:\nHOVER (Jiang et al., 2020) and FEVEROUS (Aly\net al., 2021). We use the validation sets for evalu-\nation since the test sets are not publicly released.\nHOVER contains claims that require integration\nand reasoning over multiple Wikipedia articles. We\ndivide its validation set into three subsets based on\nthe number of “hops” required to verify the claim:\n1,126 two-hop claims, 1,835 three-hop claims, and\n1,039 four-hop claims. FEVEROUS focuses on\nfact-checking complex claims over unstructured\nand structured data, where each claim is annotated\nwith evidence in the form of sentences and/or cells\nfrom tables in Wikipedia. Since we focus on textual\nfact-checking, we only selected claims that require\nexclusively sentence evidence, constituting 2,962\nclaims. We call this subset FEVEROUS-S.\n\nFor evaluation in the open-book setting, we use\nhe corresponding Wikipedia corpus constructed\n‘or these two datasets as the knowledge sources.\nHOVER uses the October 2017 Wikipedia dump\nprocessed by Yang et al. (2018), consisting of\nhe introductory sections of 5.2 million Wikipedia\npages. FEVEROUS uses the December 2020 dump,\nincluding 5.4 million full Wikipedia articles.\n\nBaselines. We compare PROGRAMFC to seven\nbaselines, categorized into three groups. (i) Pre-\ntrained models: BERT-FC (Soleimani et al., 2020)\nand LisT5 (Jiang et al., 2021) are two models\nhat leverage BERT and TS for fact verification,\nrespectively. (ii) FC/NLI fine-tuned models: we\nchoose three pretrained models that are fine-tuned\non other fact-checking datasets or natural language\ninference (NLI) datasets. ROBERTa-NLI (Nie et al.,\n2020) uses fine-tuned RoBERTa-large on four NLI\ndatasets; DeBERTaV3-NLI (He et al., 2021) fine-\ntunes the DeBERTaV3 model on 885,242 (claim,\nevidence, label) annotations from FEVER and four\nNLI datasets. MULTIVERS (Wadden et al., 2022b)\nis a LongFormer (Beltagy et al., 2020) model fine-\ntuned on FEVER. (iii) In-context learning models:\none baseline is that we directly use the FLAN-T5\nmodel in our VERIFY module for fact-checking.\nThe other baseline uses the in-context learning of\nCodex for few-shot fact-checking. The implemen-\ntation details are given in Appendix A.\n\nFew-Shot Learning. We study few-shot learning\nwhere only a few in-domain examples are available.\nTherefore, for a fair comparison, we restrict all\nmodels to have access to only 20 examples from\nHOVER or FEVEROUS-S.\n\n6985\n", "vlm_text": "'''Generate a python -like program that describes the reasoning steps required to verify the claim step -by-step. You can call three functions in the program: 1. Question () to answer a question; 2. Verify () to verify a simple claim; 3. Predict () to predict the veracity label.''' # The claim is that Both James Cameron and the director of the film Interstellar were born in Canada. def  program (): fact_1   $=$   Verify( \"James Cameron was born in Canada.\" ) Answer_1   $=$   Question( \"Who is the director of the film Interstellar?\" ) fact_2   $=$   Verify( \"{Answer_1} was born in Canada.\" ) label  $=$  Predict(fact_1 and fact_2)( · · ·  more in-context examples here  · · · ) # The claim is that  <input claim> def  program (): \nFigure 2: The Codex prompt template used to generate reasoning programs, consisting of a task instruction, in-context examples, and a prompt for the  <input claim> . The full templates are given in Appendix  D . \nThis image is a flowchart illustrating a process for answering questions using different methods. Here's a breakdown:\n\n1. **Gold Evidence**: \n   - Starts with a question <Question>.\n   - Uses gold standard evidence.\n   - Format: Q: <Question> The answer is:\n   - Flows into FLAN-T5 for generating an answer.\n\n2. **Open-book**:\n   - Begins with a question <Question>.\n   - Utilizes a retriever to obtain retrieved evidence.\n   - Format: Q: <Question> The answer is:\n   - Evidence and question flow into FLAN-T5 for processing.\n\n3. **Closed-book**:\n   - Directly uses <Question>.\n   - Format: Q: <Question> The answer is:\n   - Directly processes the question with FLAN-T5.\n\nAll paths converge at FLAN-T5, which produces the final answer.\nFigure 3: Implementation of the question-answering sub-task function for three different settings. \n4 Experiments \nDatasets. Most fact-checking datasets consist pri- marily of simple claims that can be substantiated through a single piece of evidence. However, here we focus on complex claims that need multi-step reasoning. Given this context, we opt to evalu- ate our model on the only two datasets that, to the best of our knowledge, fulfill these criteria: HOVER ( Jiang et al. ,  2020 ) and FEVEROUS ( Aly et al. ,  2021 ). We use the validation sets for evalu- ation since the test sets are not publicly released. HOVER contains claims that require integration and reasoning over multiple Wikipedia articles. We divide its validation set into three subsets based on the number of “hops” required to verify the claim: 1,126 two-hop claims, 1,835 three-hop claims, and 1,039 four-hop claims. FEVEROUS focuses on fact-checking complex claims over unstructured and structured data, where each claim is annotated with evidence in the form of sentences and/or cells from tables in Wikipedia. Since we focus on textual fact-checking, we only selected claims that require exclusively sentence evidence, constituting 2,962 claims. We call this subset FEVEROUS-S. \nFor evaluation in the open-book setting, we use the corresponding Wikipedia corpus constructed for these two datasets as the knowledge sources. HOVER uses the October 2017 Wikipedia dump processed by  Yang et al.  ( 2018 ), consisting of the introductory sections of 5.2 million Wikipedia pages. FEVEROUS uses the December 2020 dump, including 5.4 million full Wikipedia articles. \nBaselines. We compare P ROGRAM FC to seven baselines, categorized into three groups. ( i )  Pre- trained models :  BERT-FC  ( Soleimani et al. ,  2020 ) and  LisT5  ( Jiang et al. ,  2021 ) are two models that leverage BERT and T5 for fact verification, respectively. ( ii )  FC/NLI fine-tuned models : we choose three pretrained models that are fine-tuned 1 on other fact-checking datasets or natural language inference (NLI) datasets.  RoBERTa-NLI  ( Nie et al. , 2020 ) uses fine-tuned RoBERTa-large on four NLI datasets;  DeBERTaV3-NLI  ( He et al. ,  2021 ) fine- tunes the DeBERTaV3 model on 885,242 (claim, evidence, label) annotations from FEVER and four NLI datasets.  MULTIVERS  ( Wadden et al. ,  2022b ) is a LongFormer ( Beltagy et al. ,  2020 ) model fine- tuned on FEVER. ( iii )  In-context learning models : one baseline is that we directly use the  FLAN-T5 model in our VERIFY module for fact-checking. The other baseline uses the in-context learning of Codex  for few-shot fact-checking. The implemen- tation details are given in Appendix  A . \nFew-Shot Learning. We study few-shot learning where only a few in-domain examples are available. Therefore, for a fair comparison, we restrict all models to have access to only 20 examples from HOVER or FEVEROUS-S. "}
{"page": 5, "image_path": "doc_images/2023.acl-long.386_5.jpg", "ocr_text": "Few-shot learning models HOVER (2-hop) HOVER (3-hop) HOVER (4-hop) FEVEROUS-S\nGold Open Gold Open Gold Open Gold Open\nI BERT-FC (Soleimani et al., 2020) | 53.40 50.68 | 50.90 49.86 | 50.86 48.57 | 74.71 51.67\nLisT5 (Jiang et al., 2021) 56.15 52.56 | 53.76 51.89 | 51.67 50.46 | 77.88 54.15\nRoBERTa-NLI (Nie et al., 2020) 74.62 63.62 | 62.23 53.99 | 57.98 52.40 | 88.28 57.80\nIl DeBERTaV3-NLI (Heetal.,2021) | 77.22 68.72 | 65.98 60.76 | 60.49 56.00 | 91.98 58.81\nMULTIVERS (Wadden et al., 2022b) | 68.86 60.17 | 59.87 52.55 | 55.67 51.86 | 86.03 56.61\nnil Codex (Chen et al., 2021) 70.63 65.07 | 66.46 56.63 | 63.49 57.27 | 89.77 62.58\nFLAN-T5 (Chung et al., 2022) 73.69 69.02 | 65.66 60.23 | 58.08 55.42 | 90.81 63.73\nIV ProgramFC (N=1) 74.10 69.36 | 66.13 60.63 | 65.69 59.16 | 91.77 67.80\nProgramFC (N=5) 75.65 70.30 | 68.48 63.43 | 66.75 57.74 | 92.69 68.06\nTable 1: Macro-Fl scores of PROGRAMFC (IV) and baselines (I-III) on the evaluation set of HOVER and\nFEVEROUS-S for few-shot fact-checking. Gold and Open represent the gold evidence setting and the open book\n\nsetting, respectively. I: pretrained Transformers; II: FC/NLI fine-tuned models; III: in-context learning models.\n\nWe use these examples either for fine-tuning\npre-trained models (BERT-FC and LisT5), for con-\ntinuous fine-tuning the FC/NLI fine-tuned models,\nor as in-context examples for FLAN-T5 and Codex.\nFor PROGRAMEFC, we use them as in-context ex-\namples for reasoning program generation.\n\nWe evaluate both the gold evidence setting and\nthe open-book setting. The baseline models are the\nsame for both settings. However, during testing\nin the open-book setting, the models are given the\nretrieved evidence rather than the ground-truth ev-\nidence. We use BM25 (Robertson and Zaragoza,\n2009) implemented with the Pyserini toolkit (Lin\net al., 2021) as the retriever for both PROGRAMFC\nand the baselines. We use as evidence the top-10\nparagraphs retrieved from the knowledge corpus.\n\n4.1 Main Results\n\nWe report the overall results for PROGRAMFC and\nfor the baselines for few-shot fact-checking in Ta-\nble 1. PROGRAMFC achieves the best performance\non 7 out of 8 evaluations, demonstrating its effec-\ntiveness. We have three more specific observations.\n\nProgramFC is more effective on deeper claims.\nOn the HOVER dataset, ProgramFC (N=5) out-\nperforms the baselines on average by 10.38%,\n11.37%, and 14.77% on two-hop, three-hop, and\nfour-hop claims, respectively. This suggests that\nProgramFC becomes increasingly effective as the\nrequired reasoning depth increases. Among the\nbaselines, DeBERTaV3-NLI performs comparably\nto ProgramFC on two-hop claims, indicating that\nlarge-scale pre-training on simpler claims can help\nthe model generalize to more complex claims.\n\nHowever, this generalization becomes more chal-\nlenging as the complexity of the claims increases.\nOn HOVER, the F1 score of DeBERTaV3-NLI drops\nfrom 77.22 for 2-hop claims to 60.49 for 4-hop\nclaims, which is a decrease of 21.7%. In contrast,\nthe performance drop for ProgramFC, which uses\nthe strategy of program-guided reasoning, is much\nsmaller: just 11.7%.\n\nDecomposition is more effective than one-step\nprediction. The ProgramFC model, which uses\nthe same FLAN-TS model as the sub-task func-\ntions, outperforms the baseline of directly verify-\ning claims with FLAN-T5 on all four datasets. On\naverage, there is a 6.0% improvement in the gold\nevidence setting and a 4.5% improvement in the\nopen-book setting. This suggests that decomposing\na complex claim into simpler steps with a program\ncan facilitate more accurate reasoning. This is es-\npecially evident when the required reasoning is\ncomplex: there is a 14.9% improvement in the gold\nevidence setting and a 6.7% improvement in the\nopen-book setting for 4-hop claims.\n\nAggregating reasoning programs is helpful.\n\nWe find that aggregating the predictions of N = 5\nreasoning programs improves the performance over\nusing a single program by an average of 1.5%.\nThis aligns with the findings of Wang et al. (2022),\nwhere the idea was applied for question answering:\nif multiple different ways of thinking lead to the\nsame answer, we can have greater confidence that\nthe final answer is correct. This intuition also ap-\nplies to fact-checking, as each program represents\na unique reasoning chain to verify the claim.\n\n6986\n", "vlm_text": "The table presents a comparison of few-shot learning models across different datasets and settings. The models are grouped into four sections (I, II, III, IV) and evaluated on datasets: HOVER (2-hop, 3-hop, 4-hop) and FEVEROUS-S. The results are presented in terms of \"Gold\" and \"Open\" metrics.\n\n### Models and Results:\n\n#### I. \n- **BERT-FC**:\n  - HOVER (2-hop): 53.40 (Gold), 50.68 (Open)\n  - HOVER (3-hop): 50.90 (Gold), 49.86 (Open)\n  - HOVER (4-hop): 50.86 (Gold), 48.57 (Open)\n  - FEVEROUS-S: 74.71 (Gold), 51.67 (Open)\n\n- **ListT5**:\n  - HOVER (2-hop): 56.15 (Gold), 52.56 (Open)\n  - HOVER (3-hop): 53.76 (Gold), 51.89 (Open)\n  - HOVER (4-hop): 51.67 (Gold), 50.46 (Open)\n  - FEVEROUS-S: 77.88 (Gold), 54.15 (Open)\n\n#### II.\n- **RoBERTa-NLI**:\n  - HOVER (2-hop): 74.62 (Gold), 63.62 (Open)\n  - HOVER (3-hop): 62.23 (Gold), 53.99 (Open)\n  - HOVER (4-hop): 57.98 (Gold), 52.40 (Open)\n  - FEVEROUS-S: 88.28 (Gold), 57.80 (Open)\n\n- **DeBERTav3-NLI**:\n  - HOVER (2-hop): 77.22 (Gold), 68.72 (Open)\n  - HOVER (3-hop): 65.98 (Gold), 60.76 (Open)\n  - HOVER (4-hop): 60.49 (Gold), 56.00 (Open)\n  - FEVEROUS-S: 91.98 (Gold), 58.81 (Open)\n\n- **MULTIVERS**:\n  - HOVER (2-hop): 68.86 (Gold), 60.17 (Open)\n  - HOVER (3-hop): 59.87 (Gold), 52.55 (Open)\n  - HOVER (4-hop): 55.67 (Gold), 51.86 (Open)\n  - FEVEROUS-S: 86.03 (Gold), 56.61 (Open)\n\n#### III. \n- **Codex**:\n  - HOVER (2-hop): 70.63 (Gold), 65.07 (Open)\n  - HOVER (3\nWe use these examples either for fine-tuning pre-trained models ( BERT-FC  and  LisT5 ), for con- tinuous fine-tuning the FC/NLI fine-tuned models, or as in-context examples for  FLAN-T5  and  Codex . For P ROGRAM FC, we use them as in-context ex- amples for reasoning program generation. \nWe evaluate both the  gold evidence setting  and the  open-book setting . The baseline models are the same for both settings. However, during testing in the open-book setting, the models are given the retrieved evidence rather than the ground-truth ev- idence. We use BM25 ( Robertson and Zaragoza , 2009 ) implemented with the Pyserini toolkit ( Lin et al. ,  2021 ) as the retriever for both P ROGRAM FC and the baselines. We use as evidence the top-10 paragraphs retrieved from the knowledge corpus. \n4.1 Main Results \nWe report the overall results for P ROGRAM FC and for the baselines for few-shot fact-checking in Ta- ble  1 . P ROGRAM FC achieves the best performance on 7 out of 8 evaluations, demonstrating its effec- tiveness. We have three more specific observations. \nProgramFC  is more effective on deeper claims. \nOn the HOVER dataset,  ProgramFC   $(\\backslash e=5)$   out- performs the baselines on average by   $10.38\\%$  ,  $11.37\\%$  , and   $14.77\\%$   on two-hop, three-hop, and four-hop claims, respectively. This suggests that ProgramFC  becomes increasingly effective as the required reasoning depth increases. Among the baselines,  DeBERTaV3-NLI  performs comparably to  ProgramFC  on two-hop claims, indicating that large-scale pre-training on simpler claims can help the model generalize to more complex claims. \nHowever, this generalization becomes more chal- lenging as the complexity of the claims increases. On HOVER, the F1 score of  DeBERTaV3-NLI  drops from 77.22 for 2-hop claims to 60.49 for 4-hop claims, which is a decrease of   $21.7\\%$  . In contrast, the performance drop for  ProgramFC , which uses the strategy of program-guided reasoning, is much smaller: just   $11.7\\%$  . \nDecomposition is more effective than one-step prediction.  The  ProgramFC  model, which uses the same FLAN-T5 model as the sub-task func- tions, outperforms the baseline of directly verify- ing claims with  FLAN-T5  on all four datasets. On average, there is a   $6.0\\%$   improvement in the gold evidence setting and a  $4.5\\%$   improvement in the open-book setting. This suggests that decomposing a complex claim into simpler steps with a program can facilitate more accurate reasoning. This is es- pecially evident when the required reasoning is complex: there is a   $14.9\\%$   improvement in the gold evidence setting and a  $6.7\\%$   improvement in the open-book setting for 4-hop claims. \nAggregating reasoning programs is helpful. \nWe find that aggregating the predictions of    $N=5$  reasoning programs improves the performance over using a single program by an average of   $1.5\\%$  . This aligns with the findings of  Wang et al.  ( 2022 ), where the idea was applied for question answering: if multiple different ways of thinking lead to the same answer, we can have greater confidence that the final answer is correct. This intuition also ap- plies to fact-checking, as each program represents a unique reasoning chain to verify the claim. "}
{"page": 6, "image_path": "doc_images/2023.acl-long.386_6.jpg", "ocr_text": "-*FLAN-TS -eProgramFc HOVER (2-hop)\n\n80 76.11 ‘75.65 7782 80\n\n77.07\n70 67.88\n\n60\n\n50\n47.75 49.29\n40 40\n\n80M 250M 780M 3B 118 80M 250M\n\n-*FLAN-T5-*-ProgramFc HOVER (3-hop)\n\n780M 3B\n\n-*FLAN-T5-©-ProgramFc HOVER (4-hop)\n\n80\n\n69.56\n68.48 70 68.37 68.56 6e75 68.18\n\n62.46\n\n60 —— 63.39\ntose 38:08\n50\n\n48.59\n\n40\n118 80M 250M 780M 3B 118\n\nFigure 4: Fl score for fact-checking with gold evidence using FLAN-T5 (blue line) and PROGRAMFC (green line)\nfor language models of increasing sizes: FLAN-T5-smal1 (80M), FLAN-T5-base (250M), FLAN-large (780M),\nFLAN-T5-XL (3B), and FLAN-T5-XXL (11B) on HOVER 2-hop (left), 3-hop (middle), and 4-hop (right).\n\n90\n\nmOne-step Retrieval ml ProgramFC\n\n80\n70\n60\n50 51.33,\n40\n\n30\n\n36.43\n\nHOVER (4-hop)\n\n20\n\nHOVER (2-hop) HOVER (3-hop) FEVEROUS-S\n\nFigure 5: Retrieval recall @ 10 for the one-step retrieval\nand the iterative retrieval in PROGRAMFC.\n\n4.2 How Does the Reasoning Program Help?\n\nTo further understand how reasoning programs\nfacilitate fact-checking, we compare the perfor-\nmance of PROGRAMFC with FLAN-T5 using dif-\nferent language model sizes: smal1, base, large,\nXL, and XXL. The results are shown in Figure 4\nand indicate that program-guided reasoning is par-\nticularly effective when the model size is small.\nAs smaller models have less capacity for com-\nplex reasoning, the performance of the end-to-end\nFLAN-T5 model decreases significantly with de-\ncreasing model size. However, this trend is less\nnotable for PROGRAMFC. The high-level reason-\ning plan offered by reasoning programs substan-\ntially alleviates the demands on the subsequent sub-\ntask solvers. Our results show that the program-\nguided model using FLAN-T5-smal1 (80M param-\neters) as sub-task solvers can achieve comparable\nperformance to the 137x larger FLAN-T5-XXL (11B)\nmodel with end-to-end reasoning for 4-hop claims.\n\nIn the open-domain setting, we find that reason-\ning programs can enhance the retrieval of relevant\nevidence from the knowledge source. Figure 5\ncompares the retrieval performance of the one-step\nBM25 retriever used in the baselines to the iterative\nstep-by-step BM25 retriever in PROGRAMFC.\n\nWe measure the recall of the gold paragraphs\nfor the top-10 retrieved paragraphs (recall@ 10).\nFor PROGRAMEC, we combine the retrieved para-\ngraphs of all steps and we consider the top-10 re-\nsults. We can see in Figure 5 that PROGRAMFC\noutperforms one-step retrieval on all datasets, with\nthe largest improvement of 37.1% on HOVER 4-\nhop. This is because some information may not be\npresent in the original claim, but is only revealed\nduring the reasoning process (e.g., “Christopher\nNolan” in Figure 1). Thus, iterative retrieval guided\nby the reasoning program yields better results.\n\n4.3. Interpretability of Reasoning Programs\n\nAn advantage of PROGRAMEFC is that it improves\nthe interpretability of fact-checking compared to\nend-to-end models, as the explicit program can aid\nhuman understanding and debugging. Examples\nof generated reasoning programs can be found in\nFigure 7 of Appendix B. To assess the quality of\nthe generated reasoning programs, we sampled 300\nclaims where PROGRAMFC incorrectly predicted\nthe final veracity labels from the HOVER 2-hop,\n3-hop, and 4-hop datasets, with 100 examples per\ndataset. We asked human annotators to analyze the\nerror types and we classified the results into three\ncategories: (i) Syntactic errors, where the program\ndoes not conform to the defined grammar and can-\nnot be parsed, (ii) Semantic errors, which include\nincorrect or missing arguments/variables (Token),\nincorrect program structure (Structure), and incor-\nrect sub-task calls (Subtask), and (iii) Incorrect\nexecution, where the program is correct, but where\nthe incorrect prediction is a result of its execution.\nWe show the error analysis in Table 2. First,\nno syntax errors were found in our samples, indi-\ncating that Codex effectively generates executable\nprograms through few-shot in-context learning.\n\n6987\n", "vlm_text": "The image consists of three line graphs comparing the F1 scores of two fact-checking approaches, FLAN-T5 (blue line) and PROGRAM FC (green line), across different model sizes: FLAN-T5-small (80M), FLAN-T5-base (250M), FLAN-large (780M), FLAN-T5-XL (3B), and FLAN-T5-XXL (11B). The graphs assess performance on different HOVER fact-checking tasks, including 2-hop (left graph), 3-hop (middle graph), and 4-hop (right graph).\n\n- **In the 2-hop scenario**, both methods show increasing F1 scores with larger models. PROGRAM FC consistently outperforms FLAN-T5 across all model sizes, with the highest score at 11B size (77.62 for PROGRAM FC and 77.07 for FLAN-T5).\n\n- **In the 3-hop scenario**, similar trends are observed with increasing F1 scores as model size grows. Once again, PROGRAM FC shows consistently better performance than FLAN-T5, peaking at 69.56 for the 11B size, compared to 66.89 for FLAN-T5.\n\n- **In the 4-hop scenario**, the PROGRAM FC maintains a higher F1 score across all model sizes, with a gradual increase as model sizes get larger. PROGRAM FC achieves the highest score of 68.18 at 11B, compared to FLAN-T5's 63.39.\n\nOverall, PROGRAM FC consistently outperforms FLAN-T5 across all tested scenarios and model sizes.\nThe image is a bar chart comparing retrieval recall between two methods: one-step retrieval and ProgramFC. It displays data for different tasks: \n\n- **HOVER (2-hop):** One-step Retrieval (73.18), ProgramFC (77.13)\n- **HOVER (3-hop):** One-step Retrieval (51.33), ProgramFC (59.17)\n- **HOVER (4-hop):** One-step Retrieval (36.43), ProgramFC (49.93)\n- **FEVEROUS-S:** One-step Retrieval (76.25), ProgramFC (85.65)\n\nProgramFC shows higher retrieval recall in all categories compared to One-step Retrieval.\n4.2 How Does the Reasoning Program Help? \nTo further understand how reasoning programs facilitate fact-checking, we compare the perfor- mance of P ROGRAM FC with  FLAN-T5  using dif- ferent language model sizes:  small ,  base ,  large , XL , and  XXL . The results are shown in Figure  4 and indicate that program-guided reasoning is par- ticularly effective when the model size is small. As smaller models have less capacity for com- plex reasoning, the performance of the end-to-end FLAN-T5  model decreases significantly with de- creasing model size. However, this trend is less notable for P ROGRAM FC. The high-level reason- ing plan offered by reasoning programs substan- tially alleviates the demands on the subsequent sub- task solvers. Our results show that the program- guided model using  FLAN-T5-small  (80M param- eters) as sub-task solvers can achieve comparable performance to the   $137\\mathrm{x}$   larger  FLAN-T5-XXL  (11B) model with end-to-end reasoning for 4-hop claims. \nIn the open-domain setting, we find that reason- ing programs can enhance the retrieval of relevant evidence from the knowledge source. Figure  5 compares the retrieval performance of the one-step BM25 retriever used in the baselines to the iterative step-by-step BM25 retriever in P ROGRAM FC. \nWe measure the recall of the gold paragraphs for the top-10 retrieved paragraphs (recall  $@10_{,}$  . For P ROGRAM FC, we combine the retrieved para- graphs of all steps and we consider the top-10 re- sults. We can see in Figure  5  that P ROGRAM FC outperforms one-step retrieval on all datasets, with the largest improvement of   $37.1\\%$   on HOVER 4- hop. This is because some information may not be present in the original claim, but is only revealed during the reasoning process ( e.g. , “Christopher Nolan” in Figure  1 ). Thus, iterative retrieval guided by the reasoning program yields better results. \n4.3 Interpret ability of Reasoning Programs \nAn advantage of P ROGRAM FC is that it improves the interpret ability of fact-checking compared to end-to-end models, as the explicit program can aid human understanding and debugging. Examples of generated reasoning programs can be found in Figure  7  of Appendix  B . To assess the quality of the generated reasoning programs, we sampled 300 claims where P ROGRAM FC  incorrectly  predicted the final veracity labels from the HOVER 2-hop, 3-hop, and 4-hop datasets, with 100 examples per dataset. We asked human annotators to analyze the error types and we classified the results into three categories: ( i )  Syntactic errors , where the program does not conform to the defined grammar and can- not be parsed, ( ii )  Semantic errors , which include incorrect or missing arguments/variables ( Token ), incorrect program structure ( Structure ), and incor- rect sub-task calls ( Subtask ), and   $(i i i)$   Incorrect execution , where the program is correct, but where the incorrect prediction is a result of its execution. \nWe show the error analysis in Table  2 . First, no syntax errors were found in our samples, indi- cating that Codex effectively generates executable programs through few-shot in-context learning. "}
{"page": 7, "image_path": "doc_images/2023.acl-long.386_7.jpg", "ocr_text": "Claim:\n\nPredicted Program:\n\nfact_2 = Verify(\"Emery is a ghost town.\")\nanswer_3-= Question(\"Wwhi i i uw\n\nlabel = Predict(fact_1 and fact_2 and fact_3)\n\nEmery, located in the same state as Edison Local School District, is a ghost town. It is near the\ncity that lies close to the Ohio Turnpike, a 241.26 mi highway.\n\nanswer_1 = Question(\"Which state is Emery located in?\")\nanswer_2 = Question(\"Which state is Edison Local School District located in?\")\nfact_1 = Verify(\"{answer_1} and {answer_2} are the same state.\")\n\nanswer_4 = Question(\"Which city lies close to the Ohio Turnpike, a 241.26 mi highway?\")\nfact_3 = Verify(\"{answer_3} is near {answer_4}.\")—~»fact_3 = Verify(“Emery is near {answer_4}.”)\n\nFigure 6: An error case from the HOVER 4-hop dataset where the generated reasoning program has an incorrect\nprogram structure. The incorrect segment(s) are marked in red, and the correct revisions are marked in green.\n\nProportion (%)\nError Type 2-hop | 3-hop | 4-hop\nSyntax error 0% 0% 0%\nSemantic error 29% 38% 771%\nToken 8% 20% 18%\nStructure 19% 13% 57%\nSubtask 2% 5% 2%\nIncorrect execution 11% 62% 23%\nTable 2: Reasoning program evaluation for incorrectly-\npredicted examples from each hop length in HOVER.\n\nSecond, for 2-hop claims, we find that 71% of\nthe programs are correct. The majority of the er-\nrors are the result of incorrect program execution,\nwhere the question answering or the fact-checking\nmodules failed to return the correct answer.\n\nThird, as the complexity of the claims increases,\nthe proportion of semantic errors in the programs\nalso increases, with structural errors becoming par-\nticularly prevalent. This highlights the difficulty of\ngenerating the appropriate step-by-step reasoning\nstrategies for claims that require long-chain rea-\nsoning. An example structural error is shown in\nFigure 6, where the model fails to parse the second\nsentence of the claim into correct program instruc-\ntions. Additional error examples can be found in\nAppendix C.\n\n4.4 Closed-Book Fact-Checking\n\nFinally, we evaluate the closed-book setting, where\nthe model does not have access to any knowledge\nsource and needs to rely on its parametric knowl-\nedge only. The baseline models from groups I and\nII in Table | are trained with (evidence, claim)\npairs and thus are not applicable in this setting.\nWe compare our method to the baselines that use\nlarge language models for in-context learning, in-\ncluding Codex (code-davinci-@@2) and FLAN-T5\nfrom Table 1.\n\nModel HOVER FEVEROUS\n2-hop 3-hop 4-hop\n\nInstructGPT\n\n- Direct 56.51 51.75 49.68 60.13\n\n- ZS-CoT 50.30 52.30 51.58 54.78\n\n- CoT 57.20 53.66 51.83 61.05\n\n- Self-Ask | 51.54 51.47 52.45 56.82\nCodex 55.57 53.42 45.59 57.85\nFLAN-T5 48.27 52.11 51.13 55.16\nProgramFC 54.27 54.18 52.88 59.66\n\nTable 3: Closed-book setting: macro-F1 scores for PRO-\nGRAMFC and for the baselines.\n\nWe also include the 175B-parameter Instruct-\nGPT (text-davinci-002) (Ouyang et al., 2022)\nwith four different prompts: (i) direct prompt-\ning with the claim, (ii) CoT (Wei et al., 2022) or\nchain-of-thought prompting with demonstrations,\n(iii) ZS-CoT (Kojima et al., 2022) or zero-shot\nchain-of-thought with the prompt “let’s think step\nby step”, and (iv) Self-Ask (Press et al., 2022),\nwhich is a variant of CoT that guides the model rea-\nsoning by asking a series of questions. The detailed\nprompting templates are given in Appendix E.\n\nOur results, presented in Table 3, show that most\nmodels achieve a Macro-F1 score only slightly\nabove random guessing on the HOVER dataset,\nindicating the difficulty of solely relying on para-\nmetric knowledge of large language models for\nfact-checking complex claims. Similar to the obser-\nvations in Section 4.1, we see a trend of improved\nperformance as the number of the required rea-\nsoning hops increases. Chain-of-thought prompt-\ning scores an average 2.7 points higher than direct\nprompting, highlighting the importance of step-\nby-step reasoning for complex fact-checking. It\noutperforms our PROGRAMFC on HOVER 2-hop\nand FEVEROUS but performs worse on HOVER\n\n6988\n", "vlm_text": "The table presents an analysis of a claim and its corresponding predicted program, which seems to involve a series of questions, verification steps, and a final prediction. Here's a breakdown of the contents:\n\n- **Claim:** The claim states, \"Emery, located in the same state as Edison Local School District, is a ghost town. It is near the city that lies close to the Ohio Turnpike, a 241.26 mi highway.\"\n\n- **Predicted Program:** This section outlines a series of logical steps to evaluate the claim.\n\n  - `answer_1`: A question about which state Emery is located in.\n  \n  - `answer_2`: A question about which state the Edison Local School District is located in.\n  \n  - `fact_1`: A verification step to check if both answers (states) are the same.\n  \n  - `fact_2`: A verification step to confirm that Emery is a ghost town.\n  \n  - `answer_3`: (Initially present, but crossed out) A question regarding which city is near Emery. It appears to have been corrected or updated.\n  \n  - `answer_4`: A question about which city lies close to the Ohio Turnpike.\n  \n  - `fact_3`: Verification that \"Emery is near {answer_4}\", which replaces the initial \"fact_3\" (crossed out version) that was altered to reflect this formulation.\n  \n  - `label`: A final prediction that combines the results of `fact_1`, `fact_2`, and `fact_3`.\n\nThe table highlights an adjustment made to the verification step, indicating an improvement or correction in the logical flow of the analysis to support the final label prediction.\nThe table lists different error types and their proportions (percentages) for 2-hop, 3-hop, and 4-hop scenarios. Here's the breakdown:\n\n- **Syntax error**: 0% for all hops.\n- **Semantic error**:\n  - 2-hop: 29%\n  - 3-hop: 38%\n  - 4-hop: 77%\n- **Token**:\n  - 2-hop: 8%\n  - 3-hop: 20%\n  - 4-hop: 18%\n- **Structure**:\n  - 2-hop: 19%\n  - 3-hop: 13%\n  - 4-hop: 57%\n- **Subtask**:\n  - 2-hop: 2%\n  - 3-hop: 5%\n  - 4-hop: 2%\n- **Incorrect execution**:\n  - 2-hop: 71%\n  - 3-hop: 62%\n  - 4-hop: 23%\nSecond, for 2-hop claims, we find that   $71\\%$   of the programs are correct. The majority of the er- rors are the result of incorrect program execution, where the question answering or the fact-checking modules failed to return the correct answer. \nThird, as the complexity of the claims increases, the proportion of semantic errors in the programs also increases, with structural errors becoming par- ticularly prevalent. This highlights the difficulty of generating the appropriate step-by-step reasoning strategies for claims that require long-chain rea- soning. An example structural error is shown in Figure  6 , where the model fails to parse the second sentence of the claim into correct program instruc- tions. Additional error examples can be found in Appendix  C . \n4.4 Closed-Book Fact-Checking \nFinally, we evaluate the closed-book setting, where the model does not have access to any knowledge source and needs to rely on its parametric knowl- edge only. The baseline models from groups I and II in Table  1  are trained with (evidence, claim) pairs and thus are not applicable in this setting. We compare our method to the baselines that use large language models for in-context learning, in- cluding  Codex  ( code-davinci-002 ) and  FLAN-T5 from Table  1 . \nThe table presents experimental results for different models on two datasets: HOVER and FEVEROUS. The models compared are:\n\n1. InstructGPT (with variations):\n   - Direct\n   - ZS-CoT\n   - CoT\n   - Self-Ask\n\n2. Codex\n3. FLAN-T5\n4. ProgramFC\n\nFor HOVER, the models were evaluated on tasks with varying complexity: 2-hop, 3-hop, and 4-hop. The numbers represent performance metrics (likely accuracy or F1 scores), with higher numbers indicating better performance. The best scores for each task are highlighted in green.\n\nIn the FEVEROUS dataset, the models were evaluated on a single task, and the best score is highlighted in green. The results show that \"InstructGPT - CoT\" performed best on most tasks.\nWe also include the 175B-parameter Instruct- GPT ( text-davinci-002 ) ( Ouyang et al. ,  2022 ) with four different prompts: ( i )  direct  prompt- ing with the claim, ( ii )  CoT  ( Wei et al. ,  2022 ) or chain-of-thought prompting with demonstrations, ( iii )  ZS-CoT  ( Kojima et al. ,  2022 ) or zero-shot chain-of-thought with the prompt “let’s think step by step”, and ( iv )  Self-Ask  ( Press et al. ,  2022 ), which is a variant of CoT that guides the model rea- soning by asking a series of questions. The detailed prompting templates are given in Appendix  E . \nOur results, presented in Table  3 , show that most models achieve a Macro-F1 score only slightly above random guessing on the HOVER dataset, indicating the difficulty of solely relying on para- metric knowledge of large language models for fact-checking complex claims. Similar to the obser- vations in Section  4.1 , we see a trend of improved performance as the number of the required rea- soning hops increases. Chain-of-thought prompt- ing scores an average 2.7 points higher than direct prompting, highlighting the importance of step- by-step reasoning for complex fact-checking. It outperforms our P ROGRAM FC on HOVER 2-hop and FEVEROUS but performs worse on HOVER 3-hop and 4-hop. "}
{"page": 8, "image_path": "doc_images/2023.acl-long.386_8.jpg", "ocr_text": "3-hop and 4-hop.\n\nThis can be due to CoT generating free-form ex-\nplanations, which can lead to unpredictable errors\nin long reasoning chains. In contrast, our program\ngeneration-and-execution strategy is more stable\nfor longer reasoning chains.\n\n5 Conclusion and Future Work\n\nWe proposed PROGRAMFC, a few-shot neuro-\nsymbolic model for fact-checking that learns to\nmap input claims to a reasoning program consisting\nof a sequence of sub-task function calls for answer-\ning a question, for fact-checking a simple claim,\nand for computing a logical expression. Then fact-\nchecking is performed by executing that program.\nPROGRAMEFC combines the advantages of sym-\nbolic programs, such as explainability, with the\nflexibility of end-to-end neural models. Using\nCodex as the program generator, PROGRAMFC\ndemonstrates promising performance on HOVER\nand FEVEROUS with only a small number of in-\ncontext demonstrations and no additional training.\nWe also investigated the impact of model size and\nthe benefits of programs for retrieval, and we an-\nalyzed the errors. The results indicated that PRO-\nGRAMEC effectively balances model capability,\nlearning efficiency, and interpretability.\n\nIn future work, we want to adapt PROGRAMFC\nto more real-world fact-checking scenarios, such as\nfake news detection and multi-modal fact-checking,\nwith advanced reasoning program design and sub-\ntask functionalities.\n\nLimitations\n\nWe identify two main limitations of PROGRAMFC.\nFirst, despite being complex in their surface form,\nthe claims in the HOVER and FEVEROUS datasets\nmostly require only explicit multi-step reasoning,\nie., the decomposition can be derived from the\nclaim’s syntactic structure or how the claim is\nframed. This lowers the difficulty of generating rea-\nsoning programs. However, for many real-world\ncomplex claims, the reasoning is often implicit.\nFor example, for the claim “Aristotle couldn’t have\nused a laptop”, the reasoning program is:\nanswer_1 = Question(“When did Aristotle live?’’);\nanswer_2 = Question(‘““When was the laptop in-\nvented?”’);\n\nfact_1 = Verify(‘‘answer_1 is before answer_2.”);\nlabel = Predict(fact_1)\n\nGenerating reasoning programs for such implicit\ncomplex claims requires a deeper understanding\nof the claim and also access to world and com-\nmonsense knowledge. We conducted preliminary\nexperiments on these types of claims, but we found\nthat our Codex-based generator struggled to pro-\nduce a correct reasoning program. This highlights\nthe gap in applying our PROGRAMFC to fact-check\nreal-world claims. Addressing these challenges is\nan important direction for future work.\n\nSecond, PROGRAMFC incurs a higher computa-\ntional cost than baseline end-to-end fact-checking\nmodels. It requires calling large language models\nfor program generation and further calling multiple\nsub-task models. This results in the actual compu-\ntational time that is ~4—5 x higher than for an end-\nto-end FLAN-T5 model. Developing more efficient\nmethods for program generation and execution is\nan important direction for future work.\n\nEthics Statement\n\nBiases. We note that there might be some biases\nin the data used to train the LLMs, as well as in\nfactuality judgments. Both are beyond our control.\n\nIntended Use and Misuse Potential. Our mod-\nels can be of interest to the general public and\ncould also save a lot of time to human fact-checkers.\nHowever, they could also be misused by malicious\nactors. We ask researchers to exercise caution.\n\nEnvironmental Impact. The use of large lan-\nguage models requires a significant amount of\nenergy for computation for training, which con-\ntributes to global warming. Our work performs few-\nshot in-context learning instead of training models\nfrom scratch, so the energy footprint of our work is\nless. The large language model (Codex) whose API\nwe use for inference consumes significant energy.\n\nAcknowledgements\n\nThis work was supported in part by the National\nScience Foundation award #2048122 and by Sin-\ngapore’s Ministry of Education Tier 3 grant “Dig-\nital Information Resilience: Restoring Trust and\nNudging Behaviours in Digitalisation”. The views\nexpressed are those of the authors and do not reflect\nthe official policy or position of the US government.\nWe thank Alex Mei, Xinyi Wang, Danqing Wang,\nSharon Levy, Gyuwan Kim, and other members of\nthe UCSB NLP group for their valuable feedback.\n\n6989\n", "vlm_text": "\nThis can be due to CoT generating free-form ex- planations, which can lead to unpredictable errors in long reasoning chains. In contrast, our program generation-and-execution strategy is more stable for longer reasoning chains. \n5 Conclusion and Future Work \nWe proposed P ROGRAM FC, a few-shot neuro- symbolic model for fact-checking that learns to map input claims to a reasoning program consisting of a sequence of sub-task function calls for answer- ing a question, for fact-checking a simple claim, and for computing a logical expression. Then fact- checking is performed by executing that program. P ROGRAM FC combines the advantages of sym- bolic programs, such as explain ability, with the flexibility of end-to-end neural models. Using Codex as the program generator, P ROGRAM FC demonstrates promising performance on HOVER and FEVEROUS with only a small number of in- context demonstrations and no additional training. We also investigated the impact of model size and the benefits of programs for retrieval, and we an- alyzed the errors. The results indicated that P RO - GRAM FC effectively balances model capability, learning efficiency, and interpret ability. \nIn future work, we want to adapt P ROGRAM FC to more real-world fact-checking scenarios, such as fake news detection and multi-modal fact-checking, with advanced reasoning program design and sub- task functionalities. \nLimitations \nWe identify two main limitations of P ROGRAM FC. First, despite being complex in their surface form, the claims in the HOVER and FEVEROUS datasets mostly require only  explicit  multi-step reasoning, i.e. , the decomposition can be derived from the claim’s syntactic structure or how the claim is framed. This lowers the difficulty of generating rea- soning programs. However, for many real-world complex claims, the reasoning is often  implicit . For example, for the claim  “Aristotle couldn’t have used a laptop” , the reasoning program is: answer_  $1=$  Question(“When did Aristotle live?”); answer_  ${\\it2}={\\it\\Delta}$   Question(“When was the laptop in- \nfact_  $1=$  Verify(“answer_1 is before answer_2.”); label $=$ Predict(fact_1)\nGenerating reasoning programs for such implicit complex claims requires a deeper understanding of the claim and also access to world and com- monsense knowledge. We conducted preliminary experiments on these types of claims, but we found that our Codex-based generator struggled to pro- duce a correct reasoning program. This highlights the gap in applying our P ROGRAM FC to fact-check real-world claims. Addressing these challenges is an important direction for future work. \nSecond, P ROGRAM FC incurs a higher computa- tional cost than baseline end-to-end fact-checking models. It requires calling large language models for program generation and further calling multiple sub-task models. This results in the actual compu- tational time that is  ${\\sim}4{-}5\\times$   higher than for an end- to-end  FLAN-T5  model. Developing more efficient methods for program generation and execution is an important direction for future work. \nEthics Statement \nBiases. We note that there might be some biases in the data used to train the LLMs, as well as in factuality judgments. Both are beyond our control. \nIntended Use and Misuse Potential. Our mod- els can be of interest to the general public and could also save a lot of time to human fact-checkers. However, they could also be misused by malicious actors. We ask researchers to exercise caution. \nEnvironmental Impact. The use of large lan- guage models requires a significant amount of energy for computation for training, which con- tributes to global warming. Our work performs few- shot in-context learning instead of training models from scratch, so the energy footprint of our work is less. The large language model (Codex) whose API we use for inference consumes significant energy. \nAcknowledgements \nThis work was supported in part by the National Science Foundation award #2048122 and by Sin- gapore’s Ministry of Education Tier 3 grant “Dig- ital Information Resilience: Restoring Trust and Nudging Behaviours in Digitalis ation”. The views expressed are those of the authors and do not reflect the official policy or position of the US government. We thank Alex Mei, Xinyi Wang, Danqing Wang, Sharon Levy, Gyuwan Kim, and other members of the UCSB NLP group for their valuable feedback. "}
{"page": 9, "image_path": "doc_images/2023.acl-long.386_9.jpg", "ocr_text": "References\n\nNaser Ahmadi, Joohyung Lee, Paolo Papotti, and Mo-\nhammed Saeed. 2019. Explainable fact checking\nwith probabilistic answer set programming. In Pro-\nceedings of the Truth and Trust Online Conference\n(TTO), London, UK.\n\nRami Aly, Zhijiang Guo, Michael Sejr Schlichtkrull,\nJames Thorne, Andreas Vlachos, Christos\nChristodoulopoulos, Oana Cocarascu, and Arpit\nMittal. 2021. FEVEROUS: Fact Extraction and\nVERification Over Unstructured and Structured in-\nformation. In Proceedings of the Neural Information\nProcessing Systems (NeurIPS) Track on Datasets\nand Benchmarks, Online.\n\nRami Aly and Andreas Vlachos. 2022. Natural logic-\nguided autoregressive multi-hop document retrieval\nfor fact verification. In Proceedings of the 2022 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 6123-6135, Abu Dhabi,\nUnited Arab Emirates.\n\nPepa Atanasova, Jakob Grue Simonsen, Christina Li-\noma, and Isabelle Augenstein. 2020. Generating fact\nchecking explanations. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational\nLinguistics (ACL), pages 7352-7364, Online.\n\nIsabelle Augenstein, Christina Lioma, Dongsheng\nWang, Lucas Chaves Lima, Casper Hansen, Chris-\ntian Hansen, and Jakob Grue Simonsen. 2019. Mul-\ntiFC: A real-world multi-domain dataset for evidence-\nbased fact checking of claims. In Proceedings of\nthe 2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 4685-4697, Hong Kong,\nChina.\n\nGiorgio Barnabod, Federico Siciliano, Carlos Castillo,\nStefano Leonardi, Preslay Nakov, Giovanni\nDa San Martino, and Fabrizio Silvestri. 2022.\nFbMultiLingMisinfo: Challenging large-scale mul-\ntilingual benchmark for misinformation detection.\nIn Proceedings of the 2022 International Joint\nConference on Neural Networks (IJCNN), pages 1-8,\nPadova, Italy.\n\nGiorgio Barnabod, Federico Siciliano, Carlos Castillo,\nStefano Leonardi, Preslav Nakov, Giovanni Da San\nMartino, and Fabrizio Silvestri. 2023. Deep active\nlearning for misinformation detection using geomet-\nric deep learning. Online Social Networks and Media,\n33:100244.\n\nIz Beltagy, Matthew E. Peters, and Arman Cohan. 2020.\nLongformer: The long-document transformer. ArXiv\npreprint, abs/2004.05150.\n\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nIn Proceedings of the 2015 Conference on Empirical\n\nMethods in Natural Language Processing (EMNLP),\npages 632-642, Lisbon, Portugal.\n\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-Voss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In\nProceedings of the Annual Conference on Neural\nInformation Processing Systems (NeurIPS), Online.\n\nJifan Chen, Aniruddh Sriram, Eunsol Choi, and Greg\nDurrett. 2022a. Generating literal and implied sub-\nquestions to fact-check complex claims. In Proceed-\nings of the 2022 Conference on Empirical Methods\nin Natural Language Processing (EMNLP), pages\n3495-3516, Abu Dhabi, United Arab Emirates.\n\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan,\nHenrique Ponde de Oliveira Pinto, Jared Kaplan,\nHarrison Edwards, Yuri Burda, Nicholas Joseph,\nGreg Brockman, Alex Ray, Raul Puri, Gretchen\nKrueger, Michael Petrov, Heidy Khlaaf, Girish Sas-\ntry, Pamela Mishkin, Brooke Chan, Scott Gray,\nNick Ryder, Mikhail Pavlov, Alethea Power, Lukasz\nKaiser, Mohammad Bavarian, Clemens Winter,\nPhilippe Tillet, Felipe Petroski Such, Dave Cum-\nmings, Matthias Plappert, Fotios Chantzis, Eliza-\nbeth Barnes, Ariel Herbert-Voss, William Hebgen\nGuss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie\nTang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,\nWilliam Saunders, Christopher Hesse, Andrew N.\nCarr, Jan Leike, Joshua Achiam, Vedant Misra, Evan\nMorikawa, Alec Radford, Matthew Knight, Miles\nBrundage, Mira Murati, Katie Mayer, Peter Welinder,\nBob McGrew, Dario Amodei, Sam McCandlish, Ilya\nSutskever, and Wojciech Zaremba. 2021. Evaluat-\ning large language models trained on code. ArXiv\npreprint, abs/2107.03374.\n\nWenhu Chen, Xueguang Ma, Xinyi Wang, and\nWilliam W. Cohen. 2022b. Program of thoughts\nprompting: Disentangling computation from rea-\nsoning for numerical reasoning tasks. CoRR,\nabs/2211.12588.\n\nZhoujun Cheng, Tianbao Xie, Peng Shi, Chengzu\nLi, Rahul Nadkarni, Yushi Hu, Caiming Xiong,\nDragomir Radev, Mari Ostendorf, Luke Zettlemoyer,\nNoah A. Smith, and Tao Yu. 2022. Binding\nlanguage models in symbolic languages. CoRR,\nabs/2210.02875.\n\nHyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang,\nMostafa Dehghani, Siddhartha Brahma, Albert Web-\nson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suz-\ngun, Xinyun Chen, Aakanksha Chowdhery, Sharan\nNarang, Gaurav Mishra, Adams Yu, Vincent Y. Zhao,\n\n6990\n", "vlm_text": "References \nNaser Ahmadi, Joohyung Lee, Paolo Papotti, and Mo- hammed Saeed. 2019.  Explain able fact checking with probabilistic answer set programming . In  Pro- ceedings of the Truth and Trust Online Conference (TTO), London, UK.\nRami Aly, Zhijiang Guo, Michael Sejr Sch licht kru ll, James Thorne, Andreas Vlachos, Christos Christo dou lo poul os, Oana Cocarascu, and Arpit Mittal. 2021. FEVEROUS: Fact Extraction and VERification Over Unstructured and Structured in- formation . In  Proceedings of the Neural Information Processing Systems (NeurIPS) Track on Datasets and Benchmarks , Online. \nRami Aly and Andreas Vlachos. 2022.  Natural logic- guided auto regressive multi-hop document retrieval for fact verification . In  Proceedings of the 2022 Con- ference on Empirical Methods in Natural Language Processing (EMNLP) , pages 6123–6135, Abu Dhabi, United Arab Emirates. \nPepa Atanasova, Jakob Grue Simonsen, Christina Li- oma, and Isabelle Augenstein. 2020.  Generating fact checking explanations . In  Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL) , pages 7352–7364, Online. \nIsabelle Augenstein, Christina Lioma, Dongsheng Wang, Lucas Chaves Lima, Casper Hansen, Chris- tian Hansen, and Jakob Grue Simonsen. 2019.  Mul- tiFC: A real-world multi-domain dataset for evidence- based fact checking of claims . In  Proceedings of the 2019 Conference on Empirical Methods in Natu- ral Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) , pages 4685–4697, Hong Kong, China. \nGiorgio Barnabò, Federico Siciliano, Carlos Castillo, Stefano Leonardi, Preslav Nakov, Giovanni Da San Martino, and Fabrizio Silvestri. 2022. Fb Multi Ling Mis info: Challenging large-scale mul- tilingual benchmark for misinformation detection . In  Proceedings of the 2022 International Joint Conference on Neural Networks (IJCNN) , pages 1–8, Padova, Italy. \nGiorgio Barnabò, Federico Siciliano, Carlos Castillo, Stefano Leonardi, Preslav Nakov, Giovanni Da San Martino, and Fabrizio Silvestri. 2023.  Deep active learning for misinformation detection using geomet- ric deep learning .  Online Social Networks and Media , 33:100244. \nIz Beltagy, Matthew E. Peters, and Arman Cohan. 2020. Longformer: The long-document transformer .  ArXiv preprint , abs/2004.05150. \nSamuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015.  A large anno- tated corpus for learning natural language inference . In  Proceedings of the 2015 Conference on Empirical \nMethods in Natural Language Processing (EMNLP) , pages 632–642, Lisbon, Portugal. \nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neel a kant an, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020.  Language models are few-shot learners . In Proceedings of the Annual Conference on Neural Information Processing Systems (NeurIPS) , Online. \nJifan Chen, Aniruddh Sriram, Eunsol Choi, and Greg Durrett. 2022a.  Generating literal and implied sub- questions to fact-check complex claims . In  Proceed- ings of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pages 3495–3516, Abu Dhabi, United Arab Emirates. \nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sas- try, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cum- mings, Matthias Plappert, Fotios Chantzis, Eliza- beth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021.  Evaluat- ing large language models trained on code .  ArXiv preprint , abs/2107.03374. \nWenhu Chen, Xueguang Ma, Xinyi Wang, and William W. Cohen. 2022b. Program of thoughts prompting: Disentangling computation from rea- soning for numerical reasoning tasks . CoRR , abs/2211.12588. \nZhoujun Cheng, Tianbao Xie, Peng Shi, Chengzu Li, Rahul Nadkarni, Yushi Hu, Caiming Xiong, Dragomir Radev, Mari Ostendorf, Luke Z ett le moyer, Noah A. Smith, and Tao Yu. 2022. Binding language models in symbolic languages . CoRR , abs/2210.02875. \nHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Web- son, Shixiang Shane Gu, Zhuyun Dai, Mirac Suz- gun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Y. Zhao, "}
{"page": 10, "image_path": "doc_images/2023.acl-long.386_10.jpg", "ocr_text": "Yanping Huang, Andrew M. Dai, Hongkun Yu, Slav\nPetrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam\nRoberts, Denny Zhou, Quoc V. Le, and Jason Wei.\n2022. Scaling instruction-finetuned language models.\nCoRR, abs/2210.11416.\n\nLimeng Cui, Kai Shu, Suhang Wang, Dongwon Lee,\nand Huan Liu. 2019. dEFEND: A system for explain-\nable fake news detection. In Proceedings of the 28th\nACM International Conference on Information and\nKnowledge Management (CIKM), pages 2961-2964,\nBeijing, China.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies (NAACL-HLT), pages 4171-4186, Min-\nneapolis, Minnesota, USA.\n\nMohamed H. Gad-Elrab, Daria Stepanova, Jacopo Ur-\nbani, and Gerhard Weikum. 2019. Exfakt: A frame-\nwork for explaining facts over knowledge graphs and\ntext. In Proceedings of the Twelfth ACM Interna-\ntional Conference on Web Search and Data Mining\n(WSDM), pages 87-95, Melbourne, Australia.\n\nLuyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon,\nPengfei Liu, Yiming Yang, Jamie Callan, and Gra-\nham Neubig. 2022. PAL: program-aided language\nmodels. CoRR, abs/2211.10435.\n\nMax Glockner, Yufang Hou, and Iryna Gurevych. 2022.\nMissing counter-evidence renders NLP fact-checking\nunrealistic for misinformation. In Proceedings of the\n2022 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 5916-5936,\nAbu Dhabi, United Arab Emirates.\n\nZhijiang Guo, Michael Schlichtkrull, and Andreas Vla-\nchos. 2022. A survey on automated fact-checking.\nTransactions of the Association for Computational\nLinguistics, 10:178—206.\n\nAshim Gupta and Vivek Srikumar. 2021. X-Fact: A new\nbenchmark dataset for multilingual fact checking. In\nProceedings of the 59th Annual Meeting of the Asso-\nciation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing (ACL-IJCNLP), pages 675-682, Online.\n\nPengcheng He, Jianfeng Gao, and Weizhu Chen.\n2021. DeBERTaV3: Improving DeBERTa us-\ning ELECTRA-style pre-training with gradient-\ndisentangled embedding sharing. ArXiv preprint,\nabs/2111.09543.\n\nKelvin Jiang, Ronak Pradeep, and Jimmy Lin. 2021. Ex-\nploring listwise evidence reasoning with T5 for fact\nverification. In Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguis-\ntics and the 11th International Joint Conference on\nNatural Language Processing (ACL-IJCNLP), pages\n402-410, Online.\n\nYichen Jiang, Shikha Bordia, Zheng Zhong, Charles\nDognin, Maneesh Singh, and Mohit Bansal. 2020.\nHoVer: A dataset for many-hop fact extraction and\nclaim verification. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020, pages\n3441-3460, Online.\n\nShailza Jolly, Pepa Atanasova, and Isabelle Augen-\nstein. 2022. Generating fluent fact checking expla-\nnations with unsupervised post-editing. Information,\n13(10):500.\n\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2022. Large\nlanguage models are zero-shot reasoners. CoRR,\nabs/2205.11916.\n\nNeema Kotonya and Francesca Toni. 2020. Explainable\nautomated fact-checking for public health claims. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 7740-7754, Online.\n\nAmrith Krishna, Sebastian Riedel, and Andreas Vlachos.\n2022. ProoF Ver: Natural logic theorem proving for\nfact verification. Transactions of the Association for\nComputational Linguistics (TACL), 10:1013-1030.\n\nNayeon Lee, Yejin Bang, Andrea Madotto, and Pascale\nFung. 2021. Towards few-shot fact-checking via per-\nplexity. In Proceedings of the 2021 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies (NAACL-HLT), pages 1971-1981, Online.\n\nNayeon Lee, Belinda Z. Li, Sinong Wang, Wen-tau\nYih, Hao Ma, and Madian Khabsa. 2020. Language\nmodels as fact checkers? In Proceedings of the\nThird Workshop on Fact Extraction and VERification\n(FEVER), pages 36-41, Online.\n\nJimmy Lin, Xueguang Ma, Sheng-Chieh Lin, Jheng-\nHong Yang, Ronak Pradeep, and Rodrigo Nogueira.\n2021. Pyserini: A Python toolkit for reproducible\ninformation retrieval research with sparse and dense\nrepresentations. In Proceedings of the 44th Inter-\nnational ACM SIGIR Conference on Research and\nDevelopment in Information Retrieval (SIGIR), pages\n2356-2362, Online.\n\nAlisa Liu, Swabha Swayamdipta, Noah A. Smith, and\nYejin Choi. 2022. WANLI: Worker and AI collabora-\ntion for natural language inference dataset creation.\nIn Findings of the Association for Computational\nLinguistics: EMNLP 2022, pages 6826-6847, Abu\nDhabi, United Arab Emirates.\n\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Dangi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A robustly optimized BERT pretraining\napproach. ArXiv preprint, abs/1907.11692.\n\nZhenghao Liu, Chenyan Xiong, Maosong Sun, and\nZhiyuan Liu. 2020. Fine-grained fact verification\nwith kernel graph attention network. In Proceedings\n\n6991\n", "vlm_text": "Yanping Huang, Andrew M. Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. 2022.  Scaling instruction-finetuned language models . CoRR , abs/2210.11416. \nLimeng Cui, Kai Shu, Suhang Wang, Dongwon Lee, and Huan Liu. 2019.  dEFEND: A system for explain- able fake news detection . In  Proceedings of the 28th ACM International Conference on Information and Knowledge Management (CIKM) , pages 2961–2964, Beijing, China. \nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.  BERT: Pre-training of deep bidirectional transformers for language under- standing . In  Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT) , pages 4171–4186, Min- neapolis, Minnesota, USA. \nMohamed H. Gad-Elrab, Daria Stepanova, Jacopo Ur- bani, and Gerhard Weikum. 2019.  Exfakt: A frame- work for explaining facts over knowledge graphs and text . In  Proceedings of the Twelfth ACM Interna- tional Conference on Web Search and Data Mining (WSDM) , pages 87–95, Melbourne, Australia. \nLuyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Gra- ham Neubig. 2022. PAL: program-aided language models.  CoRR , abs/2211.10435. \nMax Glockner, Yufang Hou, and Iryna Gurevych. 2022. Missing counter-evidence renders NLP fact-checking unrealistic for misinformation . In  Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pages 5916–5936, Abu Dhabi, United Arab Emirates. \nZhijiang Guo, Michael Sch licht kru ll, and Andreas Vla- chos. 2022.  A survey on automated fact-checking . Transactions of the Association for Computational Linguistics , 10:178–206. \nAshim Gupta and Vivek Srikumar. 2021.  X-Fact: A new benchmark dataset for multilingual fact checking . In Proceedings of the 59th Annual Meeting of the Asso- ciation for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (ACL-IJCNLP) , pages 675–682, Online. \nPengcheng He, Jianfeng Gao, and Weizhu Chen. 2021. DeBERTaV3: Improving DeBERTa us- ing ELECTRA-style pre-training with gradient- disentangled embedding sharing . ArXiv preprint , abs/2111.09543. \nKelvin Jiang, Ronak Pradeep, and Jimmy Lin. 2021.  Ex- ploring listwise evidence reasoning with T5 for fact verification . In  Proceedings of the 59th Annual Meet- ing of the Association for Computational Linguis- tics and the 11th International Joint Conference on Natural Language Processing (ACL-IJCNLP) , pages 402–410, Online. \nYichen Jiang, Shikha Bordia, Zheng Zhong, Charles Dognin, Maneesh Singh, and Mohit Bansal. 2020. HoVer: A dataset for many-hop fact extraction and claim verification . In  Findings of the Association for Computational Linguistics: EMNLP 2020 , pages 3441–3460, Online. Shailza Jolly, Pepa Atanasova, and Isabelle Augen- stein. 2022.  Generating fluent fact checking expla- nations with unsupervised post-editing .  Information , 13(10):500. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu- taka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners . CoRR , abs/2205.11916. Neema Kotonya and Francesca Toni. 2020.  Explain able automated fact-checking for public health claims . In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pages 7740–7754, Online. Amrith Krishna, Sebastian Riedel, and Andreas Vlachos. 2022.  ProoFVer: Natural logic theorem proving for fact verification .  Transactions of the Association for Computational Linguistics (TACL) , 10:1013–1030. Nayeon Lee, Yejin Bang, Andrea Madotto, and Pascale Fung. 2021.  Towards few-shot fact-checking via per- plexity . In  Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech- nologies (NAACL-HLT) , pages 1971–1981, Online. Nayeon Lee, Belinda Z. Li, Sinong Wang, Wen-tau Yih, Hao Ma, and Madian Khabsa. 2020.  Language models as fact checkers? In  Proceedings of the Third Workshop on Fact Extraction and VERification (FEVER) , pages 36–41, Online. Jimmy Lin, Xueguang Ma, Sheng-Chieh Lin, Jheng- Hong Yang, Ronak Pradeep, and Rodrigo Nogueira. 2021.  Pyserini: A Python toolkit for reproducible information retrieval research with sparse and dense representations . In  Proceedings of the 44th Inter- national ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR) , pages 2356–2362, Online. Alisa Liu, Swabha S way am dip ta, Noah A. Smith, and Yejin Choi. 2022.  WANLI: Worker and AI collabora- tion for natural language inference dataset creation . In  Findings of the Association for Computational Linguistics: EMNLP 2022 , pages 6826–6847, Abu Dhabi, United Arab Emirates. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man- dar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Z ett le moyer, and Veselin Stoyanov. 2019. RoBERTa: A robustly optimized BERT pre training approach .  ArXiv preprint , abs/1907.11692. Zhenghao Liu, Chenyan Xiong, Maosong Sun, and Zhiyuan Liu. 2020.  Fine-grained fact verification with kernel graph attention network . In  Proceedings "}
{"page": 11, "image_path": "doc_images/2023.acl-long.386_11.jpg", "ocr_text": "of the 58th Annual Meeting of the Association for\nComputational Linguistics (ACL), pages 7342-7351,\nOnline.\n\nYi-Ju Lu and Cheng-Te Li. 2020. GCAN: Graph-aware\nco-attention networks for explainable fake news de-\ntection on social media. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational\nLinguistics (ACL), pages 505-514, Online.\n\nGrégoire Mialon, Roberto Dessi, Maria Lomeli, Christo-\nforos Nalmpantis, Ramakanth Pasunuru, Roberta\nRaileanu, Baptiste Roziére, Timo Schick, Jane\nDwivedi-Yu, Asli Celikyilmaz, Edouard Grave, Yann\nLeCun, and Thomas Scialom. 2023. Augmented\nlanguage models: a survey. CoRR, abs/2302.07842.\n\nPreslay Nakov, Alberto Barr6én-Cedefio, Giovanni\nDa San Martino, Firoj Alam, Julia Maria\nStru8, Thomas Mandl, Rubén Miguez, Tom-\nmaso Caselli, Mucahid Kutlu, Wajdi Zaghouani,\nChengkai Li, Shaden Shaar, Gautam Kishore Shahi,\nHamdy Mubarak, Alex Nikolov, Nikolay Babulkov,\nYavuz Selim Kartal, and Javier Beltran. 2022. The\nCLEF-2022 CheckThat! lab on fighting the COVID-\n19 infodemic and fake news detection. In Proceed-\nings of the 44th European Conference on IR Re-\nsearch: Advances in Information Retrieval (ECIR),\npages 416-428, Berlin, Heidelberg.\n\nPreslav Nakov, David Corney, Maram Hasanain, Firoj\nAlam, Tamer Elsayed, Alberto Barrén-Cedefio, Paolo\nPapotti, Shaden Shaar, and Giovanni Da San Mar-\ntino. 2021a. Automated fact-checking for assisting\nhuman fact-checkers. In Proceedings of the Joint\nConference on Artificial Intelligence (IJCAI), pages\n4551-4558, Online.\n\nPreslavy Nakov, Giovanni Da San Martino, Tamer\nElsayed, Alberto Barrén-Cedefio, Rubén Miguez,\nShaden Shaar, Firoj Alam, Fatima Haouari, Maram\nHasanain, Nikolay Babulkov, Alex Nikolov, Gau-\ntam Kishore Shahi, Julia Maria Stru8, and Thomas\nMandl. 2021b. The CLEF-2021 CheckThat! lab\non detecting check-worthy claims, previously fact-\nchecked claims, and fake news. In Proceedings of the\n43rd European Conference on Information Retrieval\n(ECIR), pages 639-649, Lucca, Italy.\n\nVan-Hoang Nguyen, Kazunari Sugiyama, Preslav\nNakov, and Min-Yen Kan. 2020. FANG: leveraging\nsocial context for fake news detection using graph\nrepresentation. In Proceedings of the 29th ACM Inter-\nnational Conference on Information and Knowledge\nManagement (CIKM), pages 1165-1174.\n\nYixin Nie, Haonan Chen, and Mohit Bansal. 2019.\nCombining fact extraction and verification with neu-\nral semantic matching networks. In Proceedings of\nthe 33rd AAAI Conference on Artificial Intelligence\n(AAAI), pages 6859-6866, Honolulu, Hawaii, USA.\n\nYixin Nie, Adina Williams, Emily Dinan, Mohit Bansal,\nJason Weston, and Douwe Kiela. 2020. Adversarial\n\nNLI: A new benchmark for natural language under-\nstanding. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics\n(ACL), pages 4885-4901, Online.\n\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L. Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder,\nPaul F. Christiano, Jan Leike, and Ryan Lowe. 2022.\nTraining language models to follow instructions with\nhuman feedback. CoRR, abs/2203.02155.\n\nLiangming Pan, Wenhu Chen, Wenhan Xiong, Min-\nYen Kan, and William Yang Wang. 2021. Zero-shot\nfact verification by claim generation. In Proceedings\nof the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(ACL-IJCNLP), pages 476-483, Online.\n\nAlicia Parrish, William Huang, Omar Agha, Soo-Hwan\nLee, Nikita Nangia, Alexia Warstadt, Karmanya Ag-\ngarwal, Emily Allaway, Tal Linzen, and Samuel R.\nBowman. 2021. Does putting a linguist in the loop\nimprove NLU data collection? In Findings of the\nAssociation for Computational Linguistics: EMNLP\n2021, pages 4886-4901, Punta Cana, Dominican Re-\npublic.\n\nKashyap Popat, Subhabrata Mukherjee, Jannik Strét-\ngen, and Gerhard Weikum. 2017. Where the truth\nlies: Explaining the credibility of emerging claims\non the web and social media. In Proceedngs of the\nInternational World Wide Web Conference (WWW),\npages 1003-1012.\n\nOfir Press, Muru Zhang, Sewon Min, Ludwig Schmidt,\nNoah A. Smith, and Mike Lewis. 2022. Measuring\nand narrowing the compositionality gap in language\nmodels. CoRR, abs/2210.03350.\n\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. J. Mach. Learn. Res., 21:140:1-140:67.\n\nStephen E. Robertson and Hugo Zaragoza. 2009. The\nprobabilistic relevance framework: BM25 and be-\nyond. Foundations and Trends in Information Re-\ntrieval, 3(4):333-389.\n\nArkadiy Saakyan, Tuhin Chakrabarty, and Smaranda\nMuresan. 2021. COVID-fact: Fact extraction and\nverification of real-world claims on COVID-19 pan-\ndemic. In Proceedings of the 59th Annual Meeting\nof the Association for Computational Linguistics and\nthe 11th International Joint Conference on Natural\nLanguage Processing (ACL-IJCNLP), pages 2116—\n2129, Online.\n\nAalok Sathe, Salar Ather, Tuan Manh Le, Nathan Perry,\nand Joonsuk Park. 2020. Automated fact-checking\n\n6992\n", "vlm_text": "of the 58th Annual Meeting of the Association for Computational Linguistics (ACL) , pages 7342–7351, Online. \nYi-Ju Lu and Cheng-Te Li. 2020.  GCAN: Graph-aware co-attention networks for explain able fake news de- tection on social media . In  Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL) , pages 505–514, Online. \nGrégoire Mialon, Roberto Dessì, Maria Lomeli, Christo- foros Nalmpantis, Ramakanth Pasunuru, Roberta Raileanu, Baptiste Rozière, Timo Schick, Jane Dwivedi-Yu, Asli Cel i kyi l maz, Edouard Grave, Yann LeCun, and Thomas Scialom. 2023.  Augmented language models: a survey .  CoRR , abs/2302.07842. \nPreslav Nakov, Alberto Barrón-Cedeño, Giovanni Da San Martino, Firoj Alam, Julia Maria Struß, Thomas Mandl, Rubén Míguez, Tom- maso Caselli, Mucahid Kutlu, Wajdi Zaghouani, Chengkai Li, Shaden Shaar, Gautam Kishore Shahi, Hamdy Mubarak, Alex Nikolov, Nikolay Babulkov, Yavuz Selim Kartal, and Javier Beltrán. 2022.  The CLEF-2022 CheckThat! lab on fighting the COVID- 19 infodemic and fake news detection . In  Proceed- ings of the 44th European Conference on IR Re- search: Advances in Information Retrieval (ECIR) , pages 416–428, Berlin, Heidelberg.\nPreslav Nakov, David Corney, Maram Hasanain, Firoj Alam, Tamer Elsayed, Alberto Barrón-Cedeño, Paolo Papotti, Shaden Shaar, and Giovanni Da San Mar- tino. 2021a.  Automated fact-checking for assisting human fact-checkers . In  Proceedings of the Joint Conference on Artificial Intelligence (IJCAI) , pages 4551–4558, Online. \nPreslav Nakov, Giovanni Da San Martino, Tamer Elsayed, Alberto Barrón-Cedeño, Rubén Míguez, Shaden Shaar, Firoj Alam, Fatima Haouari, Maram Hasanain, Nikolay Babulkov, Alex Nikolov, Gau- tam Kishore Shahi, Julia Maria Struß, and Thomas Mandl. 2021b.  The CLEF-2021 CheckThat! lab on detecting check-worthy claims, previously fact- checked claims, and fake news . In  Proceedings of the 43rd European Conference on Information Retrieval (ECIR) , pages 639–649, Lucca, Italy. \nVan-Hoang Nguyen, Kazunari Sugiyama, Preslav Nakov, and Min-Yen Kan. 2020.  FANG: leveraging social context for fake news detection using graph representation . In  Proceedings of the 29th ACM Inter- national Conference on Information and Knowledge Management (CIKM) , pages 1165–1174. \nYixin Nie, Haonan Chen, and Mohit Bansal. 2019. Combining fact extraction and verification with neu- ral semantic matching networks . In  Proceedings of the 33rd AAAI Conference on Artificial Intelligence (AAAI) , pages 6859–6866, Honolulu, Hawaii, USA. \nYixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela. 2020.  Adversarial \nNLI: A new benchmark for natural language under- standing . In  Proceedings of the 58th Annual Meet- ing of the Association for Computational Linguistics (ACL) , pages 4885–4901, Online. \nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car- roll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback .  CoRR , abs/2203.02155. \nLiangming Pan, Wenhu Chen, Wenhan Xiong, Min- Yen Kan, and William Yang Wang. 2021.  Zero-shot fact verification by claim generation . In  Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (ACL-IJCNLP) , pages 476–483, Online. \nAlicia Parrish, William Huang, Omar Agha, Soo-Hwan Lee, Nikita Nangia, Alexia Warstadt, Karmanya Ag- garwal, Emily Allaway, Tal Linzen, and Samuel R. Bowman. 2021.  Does putting a linguist in the loop improve NLU data collection?  In  Findings of the Association for Computational Linguistics: EMNLP 2021 , pages 4886–4901, Punta Cana, Dominican Re- public. \nKashyap Popat, Subhabrata Mukherjee, Jannik Ströt- gen, and Gerhard Weikum. 2017.  Where the truth lies: Explaining the credibility of emerging claims on the web and social media . In  Proceedngs of the International World Wide Web Conference (WWW) , pages 1003–1012. \nOfir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A. Smith, and Mike Lewis. 2022.  Measuring and narrowing the compositional it y gap in language models .  CoRR , abs/2210.03350. \nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020.  Exploring the limits of transfer learning with a unified text-to-text trans- former .  J. Mach. Learn. Res. , 21:140:1–140:67. \nStephen E. Robertson and Hugo Zaragoza. 2009.  The probabilistic relevance framework: BM25 and be- yond .  Foundations and Trends in Information Re- trieval , 3(4):333–389. \nArkadiy Saakyan, Tuhin Chakra barty, and Smaranda Muresan. 2021.  COVID-fact: Fact extraction and verification of real-world claims on COVID-19 pan- demic . In  Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (ACL-IJCNLP) , pages 2116– 2129, Online. \nAalok Sathe, Salar Ather, Tuan Manh Le, Nathan Perry, and Joonsuk Park. 2020.  Automated fact-checking "}
{"page": 12, "image_path": "doc_images/2023.acl-long.386_12.jpg", "ocr_text": "of claims from Wikipedia. In Proceedings of the\nTwelfth Language Resources and Evaluation Confer-\nence (LREC), pages 6874-6882, Marseille, France.\n\nTimo Schick, Jane Dwivedi-Yu, Roberto Dessi, Roberta\nRaileanu, Maria Lomeli, Luke Zettlemoyer, Nicola\nCancedda, and Thomas Scialom. 2023. Toolformer:\nLanguage models can teach themselves to use tools.\nCoRR, abs/2302.04761.\n\nTal Schuster, Adam Fisch, and Regina Barzilay. 2021.\nGet your vitamin C! robust fact verification with\ncontrastive evidence. In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies (NAACL-HLT), pages 624—\n643, Online.\n\nAmir Soleimani, Christof Monz, and Marcel Worring.\n2020. BERT for evidence retrieval and claim verifi-\ncation. In Advances in Information Retrieval (ECIR),\nvolume 12036, pages 359-366.\n\nJames Thorne and Andreas Vlachos. 2018. Automated\nfact checking: Task formulations, methods and future\ndirections. In Proceedings of the 27th International\nConference on Computational Linguistics (COLING),\npages 3346-3359, Santa Fe, New Mexico, USA.\n\nJames Thorne, Andreas  Vlachos, Christos\nChristodoulopoulos, and Arpit Mittal. 2018.\nFEVER: a large-scale dataset for fact extraction\nand VERification. In Proceedings of the 2018\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies (NAACL-HLT), pages\n809-819, New Orleans, Louisiana.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30: Annual Conference on Neural\nInformation Processing Systems (NeurIPS), pages\n5998-6008, Long Beach, California, USA.\n\nDavid Wadden, Shanchuan Lin, Kyle Lo, Lucy Lu\nWang, Madeleine van Zuylen, Arman Cohan, and\nHannaneh Hajishirzi. 2020. Fact or fiction: Verifying\nscientific claims. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 7534-7550, Online.\n\nDavid Wadden, Kyle Lo, Bailey Kuehl, Arman Cohan,\nIz Beltagy, Lucy Lu Wang, and Hannaneh Hajishirzi.\n2022a. SciFact-open: Towards open-domain scien-\ntific claim verification. In Findings of the Association\nfor Computational Linguistics: EMNLP 2022, pages\n4719-4734, Abu Dhabi, United Arab Emirates.\n\nDavid Wadden, Kyle Lo, Lucy Wang, Arman Cohan,\nIz Beltagy, and Hannaneh Hajishirzi. 2022b. Mul-\ntiVerS: Improving scientific claim verification with\nweak supervision and full-document context. In Find-\nings of the Association for Computational Linguis-\ntics: NAACL 2022, pages 61-76, Seattle, Washington,\nUSA.\n\nWilliam Yang Wang. 2017. “Liar, liar pants on fire”: A\nnew benchmark dataset for fake news detection. In\nProceedings of the 55th Annual Meeting of the Asso-\nciation for Computational Linguistics (ACL), pages\n422-426, Vancouver, Canada.\n\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V.\nLe, Ed H. Chi, and Denny Zhou. 2022.  Self-\nconsistency improves chain of thought reasoning in\nlanguage models. CoRR, abs/2203.11171.\n\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed H. Chi, Quoc Le, and Denny Zhou. 2022.\nChain of thought prompting elicits reasoning in large\nlanguage models. ArXiv preprint, abs/2201.11903.\n\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies (NAACL-\nHLT), pages 1112-1122, New Orleans, Louisiana,\nUSA.\n\nDustin Wright, David Wadden, Kyle Lo, Bailey Kuehl,\nArman Cohan, Isabelle Augenstein, and Lucy Wang.\n2022. Generating scientific claims for zero-shot sci-\nentific fact checking. In Proceedings of the 60th An-\nnual Meeting of the Association for Computational\nLinguistics (ACL), pages 2448-2460, Dublin, Ireland.\n\nFan Yang, Shiva K. Pentyala, Sina Mohseni, Meng-\nnan Du, Hao Yuan, Rhema Linder, Eric D. Ragan,\nShuiwang Ji, and Xia (Ben) Hu. 2019. XFake: Ex-\nplainable fake news detector with visualizations. In\nProceedings of the The World Wide Web Conference\n(WWW), pages 3600-3604, San Francisco, California,\nUSA.\n\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio,\nWilliam Cohen, Ruslan Salakhutdinov, and Christo-\npher D. Manning. 2018. HotpotQA: A dataset for\ndiverse, explainable multi-hop question answering.\nIn Proceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 2369-2380, Brussels, Belgium.\n\nWanjun Zhong, Jingjing Xu, Duyu Tang, Zenan Xu,\nNan Duan, Ming Zhou, Jiahai Wang, and Jian Yin.\n2020. Reasoning over semantic-level graph for fact\nchecking. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics\n(ACL), pages 6170-6180, Online.\n\nJie Zhou, Xu Han, Cheng Yang, Zhiyuan Liu, Lifeng\nWang, Changcheng Li, and Maosong Sun. 2019.\nGEAR: Graph-based evidence aggregating and rea-\nsoning for fact verification. In Proceedings of the\n57th Annual Meeting of the Association for Compu-\ntational Linguistics (ACL), pages 892-901, Florence,\nItaly.\n\n6993\n", "vlm_text": "of claims from Wikipedia . In  Proceedings of the Twelfth Language Resources and Evaluation Confer- ence (LREC) , pages 6874–6882, Marseille, France. \nTimo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Z ett le moyer, Nicola Cancedda, and Thomas Scialom. 2023.  Toolformer: Language models can teach themselves to use tools . CoRR , abs/2302.04761. \nTal Schuster, Adam Fisch, and Regina Barzilay. 2021. Get your vitamin C! robust fact verification with contrastive evidence . In  Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT) , pages 624– 643, Online. \nAmir Soleimani, Christof Monz, and Marcel Worring. 2020.  BERT for evidence retrieval and claim verifi- cation . In  Advances in Information Retrieval (ECIR) , volume 12036, pages 359–366. \nJames Thorne and Andreas Vlachos. 2018.  Automated fact checking: Task formulations, methods and future directions . In  Proceedings of the 27th International Conference on Computational Linguistics (COLING) , pages 3346–3359, Santa Fe, New Mexico, USA. \nJames Thorne, Andreas Vlachos, Christos Christo dou lo poul os, and Arpit Mittal. 2018. FEVER: a large-scale dataset for fact extraction and VERification . In  Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT) , pages 809–819, New Orleans, Louisiana. \nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017.  Attention is all you need . In  Advances in Neural Information Pro- cessing Systems 30: Annual Conference on Neural Information Processing Systems (NeurIPS) , pages 5998–6008, Long Beach, California, USA. \nDavid Wadden, Shanchuan Lin, Kyle Lo, Lucy Lu Wang, Madeleine van Zuylen, Arman Cohan, and Hannaneh Hajishirzi. 2020.  Fact or fiction: Verifying scientific claims . In  Proceedings of the 2020 Con- ference on Empirical Methods in Natural Language Processing (EMNLP) , pages 7534–7550, Online. \nDavid Wadden, Kyle Lo, Bailey Kuehl, Arman Cohan, Iz Beltagy, Lucy Lu Wang, and Hannaneh Hajishirzi. 2022a.  SciFact-open: Towards open-domain scien- tific claim verification . In  Findings of the Association for Computational Linguistics: EMNLP 2022 , pages 4719–4734, Abu Dhabi, United Arab Emirates. \nDavid Wadden, Kyle Lo, Lucy Wang, Arman Cohan, Iz Beltagy, and Hannaneh Hajishirzi. 2022b.  Mul- tiVerS: Improving scientific claim verification with weak supervision and full-document context . In  Find- ings of the Association for Computational Linguis- tics: NAACL 2022 , pages 61–76, Seattle, Washington, USA. \nWilliam Yang Wang. 2017.  “Liar, liar pants on fire”: A new benchmark dataset for fake news detection . In Proceedings of the 55th Annual Meeting of the Asso- ciation for Computational Linguistics (ACL) , pages 422–426, Vancouver, Canada. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V. Le, Ed H. Chi, and Denny Zhou. 2022. Self- consistency improves chain of thought reasoning in language models .  CoRR , abs/2203.11171. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed H. Chi, Quoc Le, and Denny Zhou. 2022. Chain of thought prompting elicits reasoning in large language models .  ArXiv preprint , abs/2201.11903. Adina Williams, Nikita Nangia, and Samuel Bowman. 2018.  A broad-coverage challenge corpus for sen- tence understanding through inference . In  Proceed- ings of the 2018 Conference of the North American Chapter of the Association for Computational Lin- guistics: Human Language Technologies (NAACL- HLT) , pages 1112–1122, New Orleans, Louisiana, USA. Dustin Wright, David Wadden, Kyle Lo, Bailey Kuehl, Arman Cohan, Isabelle Augenstein, and Lucy Wang. 2022.  Generating scientific claims for zero-shot sci- entific fact checking . In  Proceedings of the 60th An- nual Meeting of the Association for Computational Linguistics (ACL) , pages 2448–2460, Dublin, Ireland. Fan Yang, Shiva K. Pentyala, Sina Mohseni, Meng- nan Du, Hao Yuan, Rhema Linder, Eric D. Ragan, Shuiwang Ji, and Xia (Ben) Hu. 2019.  XFake: Ex- plainable fake news detector with visualization s . In Proceedings of the The World Wide Web Conference (WWW) , pages 3600–3604, San Francisco, California, USA. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salak hut dino v, and Christo- pher D. Manning. 2018.  HotpotQA: A dataset for diverse, explain able multi-hop question answering . In  Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pages 2369–2380, Brussels, Belgium. Wanjun Zhong, Jingjing Xu, Duyu Tang, Zenan Xu, Nan Duan, Ming Zhou, Jiahai Wang, and Jian Yin. 2020.  Reasoning over semantic-level graph for fact checking . In  Proceedings of the 58th Annual Meet- ing of the Association for Computational Linguistics (ACL) , pages 6170–6180, Online. Jie Zhou, Xu Han, Cheng Yang, Zhiyuan Liu, Lifeng Wang, Changcheng Li, and Maosong Sun. 2019. GEAR: Graph-based evidence aggregating and rea- soning for fact verification . In  Proceedings of the 57th Annual Meeting of the Association for Compu- tational Linguistics (ACL) , pages 892–901, Florence, Italy. "}
{"page": 13, "image_path": "doc_images/2023.acl-long.386_13.jpg", "ocr_text": "A_ Implementation Details about the\nBaselines\n\nIn this section, we give the implementation details\nfor the seven baselines we used in our work. Typ-\nical ways to perform few-shot fact-checking us-\ning large language models are fine-tuning and in-\ncontext learning. Thus, we categorize the baselines\ninto three categories.\n\nA.1_ Pre-trained Models\n\nPre-trained models use pretrained Transform-\ners (Vaswani et al., 2017) such as BERT (Devlin\net al., 2019) and T5 (Raffel et al., 2020) for fact-\nchecking. For few-shot learning, we fine-tune them\nusing 20 randomly sampled training examples from\nHOVER or FEVEROUS. We ran the training 10\ntimes with different random seeds and report the av-\nerage performance on the validation set. We chose\ntwo models:\n\n¢ BERT-FC (Soleimani et al., 2020): It uses\nBERT for claim verification. The claim\nand the evidence are concatenated ([CLS]\nclaim [SEP] evidence) and used as in-\nput for a binary classification task to pre-\ndict the veracity label of the claim. We use\nthe bert-large-uncased (345M parameters)\nmodel provided in HuggingFace.”\n\nLisT5 (Jiang et al., 2021): This is a fact-\nchecking framework built with a pretrained\nsequence-to-sequence transformer, namely\nT5 (Raffel et al., 2020), as its backbone. We\nadopt the “listwise concatenation” proposed in\nthe paper for label prediction, which concate-\nnates all candidate evidence sentences into a\nsingle input and we train the t5-large model\nto directly classify the claim as Supported or\nRefuted. We use the original implementation\nof this model.?\n\nA.2 FC/NLI Fine-Tuned Models\n\nThese models are pretrained Transformer models\nthat have been specifically fine-tuned on single-\nhop fact-checking datasets (e.g., FEVER) or nat-\nural language inference (NLD datasets. This ad-\nditional training allows these models to excel at\nfact-checking simple claims, and thus they can gen-\neralize better to complex claims that require multi-\nhop reasoning during further few-shot fine-tuning.\n“https: //huggingface.co/\n\n3https: //github.com/castorini/pygaggle/tree/\nmaster/experiments/list5\n\nIn this category, we selected the following three\nfine-tuned models:\n\n¢ RoBERTa-NLI (Nie et al., 2020) fine-tunes\nRoBERTa-large (Liu et al., 2019) on a com-\nbination of four well-known NLI datasets:\nSNLI (Bowman et al., 2015), MNLI (Williams\net al., 2018), FEVER-NLI (Nie et al., 2019),\nANLI(R1, R2, R3) (Nie et al., 2020). We used\nthe public model checkpoint available at Hug-\ngingFace* and we further fine-tuned it with\n20 random examples from HOVER/FEVER-\nOUS.\n\nDeBERTaV3-NLI (He et al., 2021) fine-\ntunes the DeBERTaV3-large model on\n885,242 NLI hypothesis—premise pairs from\nFEVER and on four NLI datasets: MNLI,\nANLI, LingNLI (Parrish et al., 2021), and\nWANLI (Liu et al., 2022). This is the best-\nperforming NLI model on HuggingFace as of\n06/06/2022.°\n\nMULTIVERS (Wadden et al., 2022b), formerly\nknown as LongChecker, uses the Long-\nFormer (Beltagy et al., 2020) for claim ver-\nification to address the long input evidence\nproblem. We use a model checkpoint fine-\ntuned on FEVER.®\n\nA.3 In-Context Learning Models\n\nThese models have recently shown strong few-shot\nlearning ability in various NLP tasks. By prompt-\ning a large language model with a few in-context\nexamples, the model can quickly learn a task from\ndemonstrations. To make a fair comparison to our\nmodel, we choose two in-context learning baselines\nas follows.\n\n* Codex (Chen et al., 2021) is used in\nour model to generate reasoning programs.\nOne straightforward baseline directly uses\nit for fact-checking. To this end, we\nprompt Codex (code-davinci-0Q2) as fol-\nlows: “<Evidence> Based on the above\ninformation, is it true that <Claim>?\nTrue or False? The answer is:”. We pre-\nfix the same 20 in-context examples for our\nmodel before the prompt as demonstrations.\n\n*https: //huggingface.co/ynie/\n\nroberta-large-snli_mnli_fever_anli_R1_R2_R3-nli\n\nShttps: //huggingface.co/MoritzLaurer/\n\nDeBERTa-v3-large-mnli-fever-anli-ling-wanli\nhttps: //github.com/dwadden/multivers\n\n6994\n", "vlm_text": "A Implementation Details about the Baselines \nIn this section, we give the implementation details for the seven baselines we used in our work. Typ- ical ways to perform few-shot fact-checking us- ing large language models are fine-tuning and in- context learning. Thus, we categorize the baselines into three categories. \nA.1 Pre-trained Models \nPre-trained models use pretrained Transform- ers ( Vaswani et al. ,  2017 ) such as BERT ( Devlin et al. ,  2019 ) and T5 ( Raffel et al. ,  2020 ) for fact- checking. For few-shot learning, we fine-tune them using 20 randomly sampled training examples from HOVER or FEVEROUS. We ran the training 10 times with different random seeds and report the av- erage performance on the validation set. We chose two models: \n•  BERT-FC  ( Soleimani et al. ,  2020 ): It uses BERT for claim verification. The claim and the evidence are concatenated ( [CLS] claim [SEP] evidence ) and used as in- put for a binary classification task to pre- dict the veracity label of the claim. We use the  bert-large-uncased  (345M parameters) model provided in Hugging Face. \n•  LisT5  ( Jiang et al. ,  2021 ): This is a fact- checking framework built with a pretrained sequence-to-sequence transformer, namely T5 ( Raffel et al. ,  2020 ), as its backbone. We adopt the “listwise concatenation” proposed in the paper for label prediction, which concate- nates all candidate evidence sentences into a single input and we train the  t5-large  model to directly classify the claim as  Supported  or Refuted . We use the original implementation of this model. \nA.2 FC/NLI Fine-Tuned Models \nThese models are pretrained Transformer models that have been specifically fine-tuned on single- hop fact-checking datasets ( e.g. , FEVER) or nat- ural language inference (NLI) datasets. This ad- ditional training allows these models to excel at fact-checking simple claims, and thus they can gen- eralize better to complex claims that require multi- hop reasoning during further few-shot fine-tuning. \nIn this category, we selected the following three fine-tuned models: \n•  RoBERTa-NLI  ( Nie et al. ,  2020 ) fine-tunes RoBERTa-large ( Liu et al. ,  2019 ) on a com- bination of four well-known NLI datasets: SNLI ( Bowman et al. ,  2015 ), MNLI ( Williams et al. ,  2018 ), FEVER-NLI ( Nie et al. ,  2019 ), ANLI (R1, R2, R3) ( Nie et al. ,  2020 ). We used the public model checkpoint available at Hug- gingFace 4   and we further fine-tuned it with 20 random examples from HOVER/FEVER- OUS. \n•  DeBERTaV3-NLI  ( He et al. ,  2021 ) fine- tunes the DeBERTaV3-large model on 885,242 NLI hypothesis–premise pairs from FEVER and on four NLI datasets: MNLI, ANLI, LingNLI ( Parrish et al. ,  2021 ), and WANLI ( Liu et al. ,  2022 ). This is the best- performing NLI model on Hugging Face as of 06/06/2022. \n•  MULTIVERS  ( Wadden et al. ,  2022b ), formerly known as  Long Checker , uses the Long- Former ( Beltagy et al. ,  2020 ) for claim ver- ification to address the long input evidence problem. We use a model checkpoint fine- tuned on FEVER. \nA.3 In-Context Learning Models \nThese models have recently shown strong few-shot learning ability in various NLP tasks. By prompt- ing a large language model with a few in-context examples, the model can quickly learn a task from demonstrations. To make a fair comparison to our model, we choose two in-context learning baselines as follows. \n•  Codex  ( Chen et al. ,  2021 ) is used in our model to generate reasoning programs. One straightforward baseline directly uses it for fact-checking. To this end, we prompt Codex ( code-davinci-002 ) as fol- lows: “ <Evidence> Based on the above information, is it true that <Claim>? True or False? The answer is: ”. We pre- fix the same 20 in-context examples for our model before the prompt as demonstrations. "}
{"page": 14, "image_path": "doc_images/2023.acl-long.386_14.jpg", "ocr_text": "¢ FLAN-T5 (Chung et al., 2022) is an improved\nversion of T5, which is fine-tuned on 1.8K\ntasks phrased as instructions, with and without\nexemplars, i.e., zero-shot and few-shot. The\nmodel has shown strong performance in var-\nious in-context few-shot learning NLP tasks,\nsuch as reasoning, and question-answering.\nWe prompt the model with the same format\nas we used in Section 3.4: “<Evidence> Q:\n<Claim> Is it true that <Claim>? True\nor False? The answer is:”, prefixing with\nthe same 20 in-context examples. We also use\nthe same model size (FLAN-T5-XXL 3B) with\nour model for fair comparison.\n\nB_ Examples of Generated Reasoning\nPrograms\n\nFigure 7 shows six examples of generated reason-\ning programs by PROGRAMFC that cover diverse\nreasoning chains.\n\nC_ Error Analysis for Reasoning\nPrograms\n\nFigure 8 shows five examples of erroneous cases\nwhere the generated reasoning programs are incor-\nrect. We provide explanations for each of the error\ncases below:\n\nExample 1 It generates a wrong logical reason-\ning operator for the final step. The correct logic\nshould be “not (fact_1 and fact_2)” instead\nof “fact_1 and fact_2”.\n\nExample 2 _ It fails to perform co-reference reso-\nlution for the arguments in the third and the fourth\nreasoning steps. “This album” should be replaced\nwith “The bluegrass” to make the sub-task context-\nindependent. “This musical” should be replaced\nwith the variable “answer_1” from the first step.\n\nExample 3 _ It fails to create a meaningful prob-\nlem decomposition for the claim. It generates a triv-\nial program that simply repeats the original claim.\n\nExample 4 __ It fails to generate a fine-grained rea-\nsoning structure for the input claim. It also gen-\nerates a trivial program that simply separates the\nclaim into sentences.\n\nExample 5 It generates a redundant reason-\ning step “Question(\"When was the musician\nborn?”)”, which does not add any new informa-\ntion to the reasoning chain.\n\nD_ Program Generation Prompts\n\nOur manually written prompts for the HOVER and\nthe FEVEROUS-S datasets are given in Listings 1\nand 2, respectively.\n\nE_ Prompts for Closed-Book\nFact-Checking\n\nBelow we show the templates for the four prompt-\ning methods used for InstructGPT for the closed-\nbook fact-checking setting in Section 4.4.\n\nDirect Prompting\n\n# Answer the following true/false questions:\nIs it true that The woman the story behind Girl Crazy|\nis credited to is older than Ted Kotcheff?\n\nThe answer is: False\n\n(-++ more in-context examples here ---)\n\nIs it true that <input_claim?\nThe answer is:\n\nZS-CoT Prompting\n\n# Answer the following true/false question:\n\nIs it true that <input_claim>? True or False?\nLet us think step-by-step. The answer is:\n\nCoT Prompting\n\n# Answer the following true/false questions:\n\nIs it true that The woman the story behind Girl Crazy|\nis credited to is older than Ted Kotcheff?\n\nLet's think step by step.\n\nGirl Crazy's story is credited to Hampton Del Ruth.\nHampton Del Ruth was born on September 7, 1879.\n\nTed Kotcheff was born on April 7, 1931.\n\nTherefore, the answer is: False.\n\n(-++ more in-context examples here ---)\n\nIs it true that <input_claim>?\nLet's think step by step.\n\nSelf-Ask Prompting\n\n# Answer the following true/false questions:\n\nIs it true that The woman the story behind Girl Crazy|\nis credited to is older than Ted Kotcheff?\n\nQ: The story behind Girl Crazy is credited to whom?\nA: Hampton Del Ruth\n\nQ: Is Hampton Del Ruth older than Ted Kotcheff?\n\nA: No\n\nSo the final answer is: False.\n\n(-++ more in-context examples here ---)\n\nIs it true that <input_claim?\n\n6995\n", "vlm_text": "•  FLAN-T5  ( Chung et al. ,  2022 ) is an improved version of T5, which is fine-tuned on 1.8K tasks phrased as instructions, with and without exemplars,  i.e. , zero-shot and few-shot. The model has shown strong performance in var- ious in-context few-shot learning NLP tasks, such as reasoning, and question-answering. We prompt the model with the same format as we used in Section  3.4 : “ <Evidence> Q: <Claim> Is it true that <Claim>? True or False? The answer is: ”, prefixing with the same 20 in-context examples. We also use the same model size ( FLAN-T5-XXL  3B) with our model for fair comparison. \nB Examples of Generated Reasoning Programs \nFigure  7  shows six examples of generated reason- ing programs by P ROGRAM FC that cover diverse reasoning chains. \nC Error Analysis for Reasoning Programs \nFigure  8  shows five examples of erroneous cases where the generated reasoning programs are incor- rect. We provide explanations for each of the error cases below: \nExample 1 It generates a wrong logical reason- ing operator for the final step. The correct logic should be “ not (fact_1 and fact_2) ” instead of “ fact_1 and fact_2 ”. \nExample 2 It fails to perform co-reference reso- lution for the arguments in the third and the fourth reasoning steps. “This album” should be replaced with “The bluegrass” to make the sub-task context- independent. “This musical” should be replaced with the variable “ answer_1 ” from the first step. \nExample 3 It fails to create a meaningful prob- lem decomposition for the claim. It generates a triv- ial program that simply repeats the original claim. \nExample 4 It fails to generate a fine-grained rea- soning structure for the input claim. It also gen- erates a trivial program that simply separates the claim into sentences. \nD Program Generation Prompts \nOur manually written prompts for the HOVER and the FEVEROUS-S datasets are given in Listings  1 and  2 , respectively. \nE Prompts for Closed-Book Fact-Checking \nBelow we show the templates for the four prompt- ing methods used for Instruct GP T for the closed- book fact-checking setting in Section  4.4 . \nDirect Prompting \nThe image appears to show a text about answering true/false questions. It includes an example question about whether a certain woman is older than Ted Kotcheff, with the answer being \"False.\" There is also a placeholder for another question labeled as \"<input_claim>,\" but no answer is provided for it.\nThe table contains a prompt for answering a true/false question:\n\n- It starts with an instruction in green text: \"Answer the following true/false question:\"\n- The question template is: \"Is it true that `<input_claim>`? True or False?\"\n- It then encourages a step-by-step thought process: \"Let us think step-by-step. The answer is:\"\nCoT Prompting \nThis image appears to be a screenshot of a text-based task, likely related to natural language processing. It provides an example of how to answer true/false questions by analyzing a claim step by step. The given example involves checking whether the person credited for the story of \"Girl Crazy\" is older than Ted Kotcheff by comparing their birth dates. The screenshot also includes placeholder text (<input_claim>) indicating where a new claim would be inserted for similar analysis.\nSelf-Ask Prompting \n# Answer the following true/false questions: Is it true that The woman the story behind Girl Crazy is credited to is older than Ted Kotcheff? Q: The story behind Girl Crazy is credited to whom? A: Hampton Del Ruth Q: Is Hampton Del Ruth older than Ted Kotcheff? A: No So the final answer is: False. ( · · ·  more in-context examples here  · · · ) Is it true that  <input claim> ? \nExample 5 It generates a redundant reason- ing step “ Question(\"When was the musician born?\") ”, which does not add any new informa- tion to the reasoning chain. "}
{"page": 15, "image_path": "doc_images/2023.acl-long.386_15.jpg", "ocr_text": "Claim:\nTritonia and Phyteuma are both names for a plant genus.\n\nPredicted Program:\n\nfact_1 = Verify(\"Tritonia is a name for a plant genus.\")\nfact_2 = Verify(\"Phyteuma is a name for a plant genus.\")\nlabel = Predict(fact_1 and fact_2)\n\nClaim:\nThe country that Fujairah College is located in had a 2013 population of 9.2 million until it was\nhit by the plague in 1483 when the population was halved.\n\nPredicted Program:\n\nanswer_1 = Question(\"Which country is Fujairah College located in?\")\nfact_1 = Verify(\"{answer_1} had a 2013 population of 9.2 million.\")\nfact_2 = Verify(\"{answer_1} was hit by the plague in 1483.\")\n\nfact_3 = Verify(\"The population of {answer_1} was halved in 1483.\")\nlabel = Predict(fact_1 and fact_2 and fact_3)\n\nClaim:\nThe first female board member for the Liberal Party, she was born in Vestfold county in Norway.\n\nPredicted Program:\n\nanswer_1 = Question(\"Who was the first female board member for the Liberal Party?\")\nfact_1 = Verify(\"{answer_1} was born in Vestfold county in Norway.\")\n\nlabel = Predict(fact_1)\n\nClaim:\nThe solicitor who won the show Back to Reality ahead of Maureen Rees and Craig Phillips is\nEnglish. The solicitor that was a chair of Global Witness is also English.\n\nPredicted Program:\n\nanswer_1 = Question(\"Which solicitor won the show Back to Reality ahead of Maureen Rees and Craig\nPhillips?\")\n\nanswer_2 = Question(\"Which solicitor was a chair of Global Witness?\")\n\nfact_1 = Verify(\"{answer_1} is English.\")\n\nfact_2 = Verify(\"{answer_2} is English.\")\n\nlabel = Predict(fact_1 and fact_2)\n\nClaim:\nAnthony Burgess addressed the novelist and essayist, the author of Grimus, in a lengthy love\nletter. The author is of the same nationality as Raj Koothrappali.\n\nPredicted Program:\n\nanswer_1 = Question(\"Which novelist and essayist did Anthony Burgess address in a lengthy love\nletter\nfact_1 = Verify(\"{answer_1} is the author of Grimus.\")\n\nanswer_2 = Question(\"What is the nationality of Raj Koothrappali?\")\nfact_2 = Verify(\"{answer_1} is of the same nationality as {answer_2}.\")\nlabel = Predict(fact_1 and fact_2)\n\nClaim:\nThe critically acclaimed film, that Buddy Baker scored in 1975, is a Walt Disney film. It was\nproduced first before the film that featured Bruce M. Fischer as Mr. Coogar.\n\nPredicted Program:\n\nanswer_1 = Question(\"Which critically acclaimed film did Buddy Baker score in 1975?\")\nfact_1 = Verify(\"{answer_1} is a Walt Disney film.\")\n\nanswer_2 = Question(\"Which film featured Bruce M. Fischer as Mr. Coogar?\")\n\nfact_2 = Verify(\"{answer_1} was produced first before {answer_2}.\")\n\nlabel = Predict(fact_1 and fact_2)\n\nFigure 7: Examples of generated reasoning programs by PROGRAMFC.\n\n6996\n\n", "vlm_text": "The table contains several entries, each including a \"Claim,\" followed by a \"Predicted Program,\" which is a set of logical steps designed to verify the claim. These steps are broken down into questions and verification statements. Finally, there's a \"label\" part that seems to integrate the results from the verification steps to predict the truth of the claim. Each entry is structured to assess the validity of various claims using factual verification and predictive logic."}
{"page": 16, "image_path": "doc_images/2023.acl-long.386_16.jpg", "ocr_text": "Semantic Error — Token: incorrect or missing arguments/variables\n\nExample 1:\nBitter Jester and The Future of Food are not both documentaries about food.\n\nPredicted Program:\n\nfact_1 = Verify(\"Bitter Jester is a documentary about food.\")\n\nfact_2 = Verify(\"The Future of Food is a documentary about food.\")\n\nlabel = Predict(fact_1 and fact_2)——~ label = Predict(not (fact_1 and fact_2))\n\nExample 2:\n\nThe record producer that produced the bluegrass album was born on 22 June, 1944. This album\ninspired a Tony award winning musical. This musical had a character that was originated by Carmen\nCusack.\n\nPredicted Program:\n\nanswer_1 = Question(\"Which record producer produced the bluegrass album?\")\n\nfact_1 = Verify(\"{answer_1} was born on 22 June, 1944.\")\n\nfact_2 = Verify(\"This album inspired a Tony award winning musical.\")\n\nfact_3 = Verify(\"This musical had a character that was originated by Carmen Cusack.\")\nlabel = Predict(fact_1 and fact_2 and fact_3)\n\n—>\n\nanswer_1 = Question(\"Which record producer produced the bluegrass album?\")\n\nfact_1 = Verify(\"{answer_1} was born on 22 June, 1944.\")\n\nfact_2 = Verify(\"The bluegrass album inspired a Tony award winning musical.\")\nfact_3 = Verify(\"{answer_1} had a character that was originated by Carmen Cusack.\")\nlabel = Predict(fact_1 and fact_2 and fact_3)\n\nSemantic Error — Structure: incorrect program structure\n\nExample 3:\nRichard Strauss, German composer of Die Nacht, composed another well-established opera called Der\nRosenkavalier.\n\nPredicted Program:\n\nfact_1 = Verify(\"Richard Strauss, German composer of Die Nacht, composed another well-established\nopera called Der Rosenkavalier.\")\n\nlabel = Predict(fact_1)\n\n—>\nfact_1 = Verify(\"Richard Strauss is a German composer of Die Nacht.”)\nfact_2 = Verify(\"Richard Strauss composed a well-established opera called Der Rosenkavalier.\")\n\nlabel = Predict(fact_1 and fact_2)\n\nExample 4:\n\nThe film Deanna Oliver produced in 1999 grossed $36.8 million domestically. The musical film\nbased on coach Herman Boone, did not.\n\nPredicted Program:\n\nfact_1 = Verify(\"The film Deanna Oliver produced in 1999 grossed $36.8 million domestically.\")\nfact_2 = Verify(\"The musical film based on coach Herman Boone, did not.\")\n\nlabel = Predict(fact_1 and fact_2)\n\n>\n\nanswer_1 = Question(\"What film did Deanna Oliver produce in 1999?\")\n\nfact_1 = Verify(\"{answer_1} grossed $36.8 million domestically.\")\n\nanswer_2 = Question(\"What musical film was based on coach Herman Boone?\")\nfact_2 = Verify(\"{answer_2} grossed less than $36.8 million domestically.\")\nlabel = Predict(fact_1 and fact_2)\n\nSemantic Error — Subtask: missing / redundant / incorrect sub-task calls\n\nExample 5:\nThe musician, who founded Morningwood with Max Green, is older than Max Green.\n\nPredicted Program:\nanswer_1 = Question(\"Who founded Morningwood with Max Green?\")\nanswer_2 Question(\"When was Max Green born?\")\n\nfact_: Verify(\"{answer_3} is older than {answer_2}.\")—> {answer_1} is older than {answer_2}.\nlabel = Verify(fact_1)\n\nFigure 8: Example error cases where the generated reasoning programs are incorrect. The incorrect segment(s) are\nmarked in red, and the correct revisions are marked in green.\n\n6997\n", "vlm_text": "The table illustrates examples of different types of semantic errors in predicted programs, which involve incorrect or missing arguments, incorrect program structures, and missing or redundant sub-task calls. The table categorizes these errors into three main types:\n\n1. **Semantic Error – Token: Incorrect or Missing Arguments/Variables**\n   - Example 1: \n     - Original statement: \"Bitter Jester and The Future of Food are not both documentaries about food.\"\n     - Initial incorrect program generates a `Predict` label using a logical 'and' operator.\n     - Correct program fixes this by using 'not' and 'and' operators appropriately.\n\n   - Example 2:\n     - Original statement involves a record producer, a bluegrass album, and a Tony award-winning musical.\n     - Initial incorrect program generates a `Predict` label using facts and questions where one variable is missing.\n     - Correct program includes the variable in all required sub-tasks.\n\n2. **Semantic Error – Structure: Incorrect Program Structure**\n   - Example 3:\n     - Original statement about Richard Strauss and his compositions.\n     - Initial program incorrectly treats the entire statement as a single fact.\n     - Correct program breaks it into separate verifiable facts.\n   \n   - Example 4:\n     - Statement about a film's gross and its relationship to coach Herman Boone.\n     - Initial program processes the statement as one fact.\n     - Correct program splits it into questions about the film and its financial performance.\n\n3. **Semantic Error – Subtask: Missing/Redundant/Incorrect Sub-task Calls**\n   - Example 5:\n     - Statement about the musician who founded Morningwood with Max Green, comparing ages.\n     - Incorrect program introduces an unnecessary question about when the musician was born.\n     - Correct program removes redundant questions, focusing instead on verifying age.\n\nEach example within these categories shows an original incorrect program and provides a correction, demonstrating how to better structure or modify tasks to reflect the intended meaning of the statement accurately.\nFigure 8: Example error cases where the generated reasoning programs are incorrect. The incorrect segment(s) are marked in  red , and the correct revisions are marked in  green . "}
{"page": 17, "image_path": "doc_images/2023.acl-long.386_17.jpg", "ocr_text": "'''Generate a python-like program that describes the reasoning steps required to\nverify the claim step-by-step. You can call three functions in the program: 1.\nQuestion () to answer a question; 2. Verify () to verify a simple claim; 3.\nPredict() to predict the veracity label.'''\n\n# The claim is that Howard University Hospital and Providence Hospital are both\nlocated in Washington, D.C.\n\ndef program():\nfact_1 = Verify(\"Howard University Hospital is located in Washington, D.C.\")\nfact_2 = Verify(\"Providence Hospital is located in Washington, D.C.\")\nlabel = Predict(fact_1 and fact_2)\n\n# The claim is that WWE Super Tuesday took place at an arena that currently goes by\nthe name TD Garden.\ndef program():\nanswer_1 = Question(\"Which arena the WWE Super Tuesday took place?”)\nfact_1 = Verify(f\"”{answer_1} currently goes by the name TD Garden.\"”)\nlabel = Predict(fact_1)\n\n# The claim is that Talking Heads, an American rock band that was \"one of the most\ncritically acclaimed bands of the 80's” is featured in KSPN's AAA format.\n\ndef program():\nfact_1 = Verify(\"Talking Heads is an American rock band that was ‘one of the\nmost critically acclaimed bands of the 8@'s'.\"”)\nfact_2 = Verify(\"Talking Heads is featured in KSPN's AAA format.\"”)\nlabel = Predict(fact_1 and fact_2)\n\n# The claim is that An IndyCar race driver drove a Formula 1 car designed by Peter\nMcCool during the 2007 Formula One season.\ndef program():\n\nanswer_1 = Question(\"Which Formula 1 car was designed by Peter McCool during the\n2007 Formula One season?\")\nfact_1 = Verify(f\"”An IndyCar race driver drove the car {answer_1}.\")\n\nlabel = Predict(fact_1)\n\n# The claim is that Gina Bramhill was born in a village. The 2011 population of the\narea that includes this village was 167,446\ndef program():\n\nanswer_1 = Question(\"Which village was Gina Bramhill born in?\")\nfact_1 = Verify(f\"The 2011 population of the area that includes {answer_1} was\n167,446.\")\n\nlabel = Predict(fact_1)\n\n# The claim is that Don Ashley Turlington graduated from Saint Joseph's College, a\nprivate Catholic liberal arts college in Standish.\n\ndef program():\nfact_1 = Verify(\"Saint Joseph's College is a private Catholic liberal arts\ncollege is located in Standish.\")\nfact_2 = Verify(f\"Don Ashley Turlington graduated from Saint Joseph's College.\")\nlabel = Predict(fact_1 and fact_2)\n\n# The claim is that Gael and Fitness are not published in the same country.\ndef program():\n\nanswer_1 = Question(\"Which country was Gael published in?\")\nanswer_2 = Question(\"Which country was Fitness published in?\")\nfact_1 = Verify(f\"{answer_1} and {answer_2} are not the same country.\"”)\n\nlabel = Predict(fact_1)\n\n# The claim is that Blackstar is the name of the album released by David Bowie that\nwas recorded in secret.\ndef program():\nfact_1 = Verify(\"David Bowie released an album called Blackstar.\")\nfact_2 = Verify(\"David Bowie recorded an album in secret.\")\nlabel = Predict(fact_1 and fact_2)\n\n# The claim is that In the 2004 Hockey film produced by a former major league\nbaseball pitcher Kurt Russell played the USA coach.\n\ndef program():\nanswer_1 = Question(\"Which 2004 Hockey film was produced a former major league\n\n6998\n\n", "vlm_text": "'''Generate a python -like program that describes the reasoning steps required to verify the claim step -by-step. You can call three functions in the program: 1. Question () to answer a question; 2. Verify () to verify a simple claim; 3. Predict () to predict the veracity label.'''\n\n \n# The claim is that Howard University Hospital and Providence Hospital are both located in Washington , D.C. \ndef  program (): fact_1  $=$   Verify( \"Howard University Hospital is located in Washington , D.C.\" ) fact_2   $=$   Verify( \"Providence Hospital is located in Washington , D.C.\" ) label  $=$  Predict(fact_1 and fact_2)\n# The claim is that WWE Super Tuesday took place at an arena that currently goes by the name TD Garden. \ndef  program (): answer_1   $=$   Question( \"Which arena the WWE Super Tuesday took place?\" ) fact_1   $=$   Verify(f \"{answer_1} currently goes by the name TD Garden.\" ) label  $=$  Predict(fact_1)\n# The claim is that Talking Heads , an American rock band that was \"one of the most critically acclaimed bands of the   $8\\ell^{\\prime}\\varsigma^{\\prime\\prime}$   is featured in KSPN's AAA format. \ndef  program (): fact_1  $=$   Verify( \"Talking Heads is an American rock band that was 'one of the most critically acclaimed bands of the 80's'.\" ) fact  $_-2\\;\\;=$   Verify( \"Talking Heads is featured in KSPN's AAA format.\" ) label  $=$  Predict(fact_1 and fact_2)\n# The claim is that An IndyCar race driver drove a Formula 1 car designed by Peter McCool during the 2007 Formula One season. \ndef  program (): answer_1   $=$   Question( \"Which Formula 1 car was designed by Peter McCool during the 2007 Formula One season?\" ) fact_1   $=$   Verify(f \"An IndyCar race driver drove the car {answer_1 }.\" ) label  $=$  Predict(fact_1)\n# The claim is that Gina Bramhill was born in a village. The 2011 population of the area that includes this village was 167 ,446. \ndef  program (): answer_1   $=$   Question( \"Which village was Gina Bramhill born in?\" ) fact_1   $=$   Verify(f \"The 2011 population of the area that includes {answer_1} was 167 ,446.\" ) label  $=$  Predict(fact_1)\n# The claim is that Don Ashley Turlington graduated from Saint Joseph 's College , a private Catholic liberal arts college in Standish. \ndef  program (): fact_1  $=$   Verify( \"Saint Joseph 's College is a private Catholic liberal arts college is located in Standish.\" ) fact  $_-2\\;\\;=$   Verify(f \"Don Ashley Turlington graduated from Saint Joseph 's College.\" ) label  $=$  Predict(fact_1 and fact_2)\n# The claim is that Gael and Fitness are not published in the same country. \ndef  program (): answer_1   $=$   Question( \"Which country was Gael published in?\" ) answer  $_-2\\;\\;=$   Question( \"Which country was Fitness published in?\" ) fact_1   $=$   Verify(f \"{answer_1} and {answer_2} are not the same country.\" ) label  $=$  Predict(fact_1)\n# The claim is that Blackstar is the name of the album released by David Bowie that was recorded in secret. \ndef  program (): fact_1  $=$   Verify( \"David Bowie released an album called Blackstar.\" fact  $_-2\\;\\;=$   Verify( \"David Bowie recorded an album in secret.\" ) label  $=$  Predict(fact_1 and fact_2)\n# The claim is that In the 2004 Hockey film produced by a former major league baseball pitcher Kurt Russell played the USA coach. def  program (): "}
{"page": 18, "image_path": "doc_images/2023.acl-long.386_18.jpg", "ocr_text": "baseball pitcher?\"”)\nfact_1 = Verify(\"Kurt Russell played the USA coach in the film {answer_1}.\")\nlabel = Predict(fact_1)\n\n# The claim is that Along with the New York Islanders and the New York Rangers, the\nNew Jersey Devils NFL franchise is popular in the New York metropolitan area.\ndef program():\nfact_1 = Verify(\"The New York Islanders and the New York Rangers are popular in\nthe New York metropolitan area.\")\nfact_2 = Verify(\"The New Jersey Devils NFL franchise is popular in the New York\nmetropolitan area.\")\nlabel = Predict(fact_1 and fact_2)\n\n# The claim is that Jack McFarland is the best known role of the host of the 64th\nAnnual Tony Awards.\n\ndef program():\nanswer_1 = Question(\"Who is the host of the 64th Annual Tony Awards?”)\nfact_1 = Verify(f\\\"Jack McFarland is the best known role of {answer_1}.\")\nlabel = Predict(fact_1)\n\n# The claim is that The song recorded by Fergie that was produced by Polow da Don\nand was followed by Life Goes On was M.I.L.F.$.\n\ndef program():\nfact_1 = Verify(\"M.I.L.F.$ was recorded by Fergie that was produced by Polow da\nDon.\")\nfact_2 = Verify(\"M.I.L.F.$ was was followed by Life Goes On.\")\nlabel = Predict(fact_1 and fact_2)\n\n# The claim is that Eatza Pizza and Your Pie were not founded in the same state.\ndef program():\n\nanswer_1 = Question(\"Which state was Eatza Pizza founded in?\")\nanswer_2 = Question(\"Which state was Your Pie founded in?\"”)\nfact_1 = Verify(f\"”{answer_1} and {answer_2} are not the same state.\")\n\nlabel = Predict(fact_1)\n\n# The claim is that Gregg Rolie and Rob Tyner, are not a keyboardist.\ndef program():\n\nfact_1 = Verify(\"Gregg Rolie is not a keyboardist.\")\n\nfact_2 = Verify(\"Rob Tyner is not a keyboardist.\"”)\n\nlabel = Predict(fact_1 and fact_2)\n\n# The claim is that Maria Esther Andion Bueno, not Jimmy Connors, is the player that\nis from Brazil.\ndef program():\nfact_1 = Verify(\"Maria Esther Andion Bueno is from Brazil.\"”)\nfact_2 = Verify(\"Jimmy Connors is not from Brazil.\")\nlabel = Predict(fact_1 and fact_2)\n\n# The claim is that Vladimir Igorevich Arnold died after Georg Cantor.\ndef program():\nanswer_1 = Question(\"When did Vladimir Igorevich Arnold die?\")\nanswer_2 = Question(\"When did Georg Cantor die?”)\nfact_1 = Verify(f\"{answer_1} is after {answer_2}.\")\nlabel = Predict(fact_1)\n\n# The claim is that Barton Mine was halted by a natural disaster not Camlaren Mine.\ndef program():\n\nfact_1 = Verify(\"Barton Mine was halted by a natural disaster.\")\n\nfact_2 = Verify(\"Camlaren Mine was not halted by a natural disaster.\")\n\nlabel = Predict(fact_1 and fact_2)\n\n# The claim is that John O'Hara and Rabindranath Tagore are not the same nationality\n\ndef program():\n\nanswer_1 = Question(\"What is the nationality of John O'Hara?\")\nanswer_2 = Question(\"What is the nationality of Rabindranath Tagore?”)\nfact_1 = Verify(f\"{answer_1} and {answer_2} are not the same nationality.”)\n\nlabel = Predict(fact_1)\n\n6999\n\n", "vlm_text": "baseball pitcher?\" ) \nfact_1   $=$   Verify( \"Kurt Russell played the USA coach in the film {answer_1 }.\" ) label  $=$  Predict(fact_1)\n# The claim is that Along with the New York Islanders and the New York Rangers , the New Jersey Devils NFL franchise is popular in the New York metropolitan area. \ndef  program (): fact_1  $=$   Verify( \"The New York Islanders and the New York Rangers are popular in the New York metropolitan area.\" ) fact  $_-2\\;\\;=$   Verify( \"The New Jersey Devils NFL franchise is popular in the New York metropolitan area.\" ) label  $=$  Predict(fact_1 and fact_2)\n# The claim is that Jack McFarland is the best known role of the host of the 64th Annual Tony Awards. \ndef  program (): answer_1   $=$   Question( \"Who is the host of the 64th Annual Tony Awards?\" ) fact_1   $=$   Verify(f\\ \"Jack McFarland is the best known role of {answer_1 }.\" ) label  $=$  Predict(fact_1)\n# The claim is that The song recorded by Fergie that was produced by Polow da Don and was followed by Life Goes On was M.I.L.F.\\$. \ndef  program (): fact_1  $=$   Verify( \"M.I.L.F.\\$ was recorded by Fergie that was produced by Polow da Don.\" ) fact  $_-2\\;\\;=$   Verify( \"M.I.L.F.\\$ was was followed by Life Goes On.\" ) label  $=$  Predict(fact_1 and fact_2)\n# The claim is that Eatza Pizza and Your Pie were not founded in the same state. def  program (): answer_1   $=$   Question( \"Which state was Eatza Pizza founded in?\" ) answer  $_-2\\;\\;=$   Question( \"Which state was Your Pie founded in?\" ) fact_1   $=$   Verify(f \"{answer_1} and {answer_2} are not the same state.\" ) label  $=$  Predict(fact_1)\n\n\n# The claim is that Gregg Rolie and Rob Tyner , are not a keyboardist. \nfact_1   $=$   Verify( \"Gregg Rolie is not a keyboardist.\" ) fact  $_-2\\;\\;=$   Verify( \"Rob Tyner is not a keyboardist.\" ) label  $=$  Predict(fact_1 and fact_2)\n\n\n# The claim is that Maria Esther Andion Bueno , not Jimmy Connors , is the player that is from Brazil. \ndef  program (): fact_1   $=$   Verify( \"Maria Esther Andion Bueno is from Brazil.\" ) fact_2   $=$   Verify( \"Jimmy Connors is not from Brazil.\" ) label  $=$  Predict(fact_1 and fact_2)\n\n\n# The claim is that Vladimir Igorevich Arnold died after Georg Cantor. \n program (): answer_1   $=$   Question( \"When did Vladimir Igorevich Arnold die?\" ) answer  $_-2\\;\\;=$   Question( \"When did Georg Cantor die?\" ) fact_1   $=$   Verify(f \"{answer_1} is after {answer_2 }.\" ) label  $=$  Predict(fact_1)\n\n\n# The claim is that Barton Mine was halted by a natural disaster not Camlaren Mine. def  program (): fact_1   $=$   Verify( \"Barton Mine was halted by a natural disaster.\" ) fact  $_-2\\;\\;=$   Verify( \"Camlaren Mine was not halted by a natural disaster.\" ) label  $=$  Predict(fact_1 and fact_2)\n# The claim is that John O'Hara and Rabindranath Tagore are not the same nationality \ndef  program (): \nanswer  $_-1\\;\\;=\\;\\;$   Question( \"What is the nationality of John O'Hara?\" ) answer  $_-2\\;\\;=$   Question( \"What is the nationality of Rabindranath Tagore?\" ) fact_1   $=$   Verify(f \"{answer_1} and {answer_2} are not the same nationality.\" ) label  $=$  Predict(fact_1)"}
{"page": 19, "image_path": "doc_images/2023.acl-long.386_19.jpg", "ocr_text": "# The claim is that Thomas Loren Friedman has won more Pulitzer Prizes than Colson\n\ndef\n\nWhitehead.\nprogram():\nanswer_1 = Question(\"How many Pulitzer Prizes has Thomas Loren Friedman won?\")\nanswer_2 = Question(”\"How many Pulitzer Prizes has Colson Whitehead won?”)\nfact_1 = Verify(f\"”{answer_1} is more than {answer_2}.\")\n\nlabel = Predict(fact_1)\n\n# The claim is that The model of car Trevor Bayne drives was introduced for model\n\ndef\n\nyear 2006. The Rookie of The Year in the 1997 CART season drives it in the\nNASCAR Sprint Cup.\n\nprogram():\n\nanswer_1 = Question(\"Which model of car is drived by Trevor Bayne?”)\n\nfact_1 = Verify(f\"”{answer_1} was introduced for model year 2006.\")\n\nanswer_2 = Question(\"Who is the Rookie of The Year in the 1997 CART season?\")\n\nfact_2 = Verify(f\"”{answer_2} drives the model of car Trevor Bayne drives in the\nNASCAR Sprint Cup.\")\n\nlabel = predict(fact_1 and fact_2)\n\n# The claim is that <input_claim>\n\ndef\n\nprogram():\n\nListing 1: The prompt used for Program Generation for HOVER.\n\n7000\n\n", "vlm_text": "# The claim is that Thomas Loren Friedman has won more Pulitzer Prizes than Colson Whitehead. def  program (): answer_1   $=$   Question( \"How many Pulitzer Prizes has Thomas Loren Friedman won?\" ) answer  $_{-}2\\;\\;=\\;\\;$   Question( \"How many Pulitzer Prizes has Colson Whitehead won?\" ) fact_1   $=$   Verify(f \"{answer_1} is more than {answer_2 }.\" ) label  $=$  Predict(fact_1)# The claim is that The model of car Trevor Bayne drives was introduced for model year 2006. The Rookie of The Year in the 1997 CART season drives it in the NASCAR Sprint Cup. def  program (): answer  $_-1\\;\\;=\\;\\;$   Question( \"Which model of car is drived by Trevor Bayne?\" ) fact_1  $=$   Verify(f \"{answer_1} was introduced for model year 2006.\" ) answer  $_-2\\;\\;=$   Question( \"Who is the Rookie of The Year in the 1997 CART season?\" ) fact  $_-2\\;\\;=$   Verify(f \"{answer_2} drives the model of car Trevor Bayne drives in the NASCAR Sprint Cup.\" ) label  $=$  predict(fact_1 and fact_2)# The claim is that  <input claim> def  program (): \nListing 1: The prompt used for Program Generation for HOVER. "}
{"page": 20, "image_path": "doc_images/2023.acl-long.386_20.jpg", "ocr_text": "'''Generate a python-like program that describes the reasoning steps required to\nverify the claim step-by-step. You can call three functions in the program: 1.\nQuestion () to answer a question; 2. Verify () to verify a simple claim; 3.\nPredict() to predict the veracity label.'''\n\n# The claim is that In 1959, former Chilean boxer Alfredo Cornejo Cuevas (born June\n6, 1933) won the gold medal in the welterweight division at the Pan American\nGames (held in Chicago, United States, from August 27 to September 7) in Chicago\n, United States, and the world amateur welterweight title in Mexico City.\n\ndef program():\nfact_1 = Verify( \"Alfredo Cornejo Cuevas was born in June 6, 1933.\")\nfact_2 = Verify(\"Alfredo Cornejo Cuevas won the gold medal in the welterweight\ndivision at the Pan American Games in 1959.\")\nfact_3 = Verify(\"The Pan American Games in 1959 was held in Chicago, United\nStates, from August 27 to September 7.\")\nfact_4 = Verify( \"Alfredo Cornejo Cuevas won the world amateur welterweight title\n\nin Mexico City.\")\nlabel = Predict(fact_1 and fact_2 and fact_3 and fact_4)\n\n# The claim is that The Footwork FA12, which was intended to start the season,\nfinally debuted at the San Marino Grand Prix, a Formula One motor race held at\nImola on 28 April 1991.\n\ndef program():\n\nfact_1 = Verify(\"The Footwork FA12, which was intended to start the season.”)\nfact_2 = Verify(\"The Footwork FA12 finally debuted at the San Marino Grand Prix.\n\"y\n\nfact_3 = Verify(\"The San Marino Grand Prix was a Formula One motor race held at\nImola on 28 April 1991.\")\nlabel = Predict(fact_1 and fact_2 and fact_3)\n\n# The claim is that SkyHigh Mount Dandenong (formerly Mount Dandenong Observatory)\nis a restaurant located on top of Mount Dandenong, Victoria, Australia.\n\ndef program():\nfact_1 = Verify(\"SkyHigh Mount Dandenong is a restaurant located on top of Mount\nDandenong, Victoria, Australia.\"”)\nfact_2 = Verify(\"SkyHigh Mount Dandenong is formerly known as Mount Dandenong\nObservatory.\")\nlabel = Predict(fact_1 and fact_2)\n\n# The claim is that Before the first Europeans arrived or copra companies leased it,\nMaupihaa was home to Inca's in ancient times.\n\ndef program():\nfact_1 = Verify(\"Maupihaa was home to Inca's in ancient times.\")\nfact_2 = Verify(\"Maupihaa was home to Inca's before the first Europeans arrived\nor copra companies leased it.\"”)\nlabel = Predict(fact_1 and fact_2)\n\n# The claim is that Shulin, a 33.1288 km (12.7911 sq mi) land located in New Taipei\nCity, China, a country in East Asia, has a total population of 183,946 in\nDecember 2018.\n\ndef program():\nfact_1 = Verify(\"Shulin is a 33.1288 km (12.7911 sq mi) land located in New\nTaipei City, China.\"”)\nfact_2 = Verify(\"Shulin has a total population of 183,946 in December 2018.\")\nlabel = Predict(fact_1 and fact_2)\n\n# The claim is that Sumo wrestler Toyozakura Toshiaki committed match-fixing, ending\nhis career in 2011 that started in 1989\n\ndef program():\nfact_1 = Verify(\"Toyozakura Toshiaki ended his career in 2011 that started in\n1989.\")\nfact_2 = Verify(\"Toyozakura Toshiaki is a Sumo wrestler.\")\nfact_3 = Verify(\"Toyozakura Toshiaki committed match-fixing.\")\nlabel = Predict(fact_1 and fact_2 and fact_3)\n\n# The claim is that In 1959, former Chilean boxer Alfredo Cornejo Cuevas (born June\n6, 1933) won the gold medal in the welterweight division at the Pan American\nGames (held in Chicago, United States, from August 27 to September 7) in Chicago\n\n7001\n\n", "vlm_text": "'''Generate a python -like program that describes the reasoning steps required to verify the claim step -by-step. You can call three functions in the program: 1. Question () to answer a question; 2. Verify () to verify a simple claim; 3. Predict () to predict the veracity label.'''\n\n \n# The claim is that In 1959, former Chilean boxer Alfredo Cornejo Cuevas (born June 6, 1933) won the gold medal in the welterweight division at the Pan American Games (held in Chicago , United States , from August 27 to September 7) in Chicago , United States , and the world amateur welterweight title in Mexico City. \ndef  program (): fact_1   $=$   Verify( \"Alfredo Cornejo Cuevas was born in June 6, 1933.\" ) fact_2   $=$   Verify( \"Alfredo Cornejo Cuevas won the gold medal in the welterweight division at the Pan American Games in 1959.\" ) fact  $\\begin{array}{r l}{\\_3}&{{}=}\\end{array}$   Verify( \"The Pan American Games in 1959 was held in Chicago , United States , from August 27 to September 7.\" ) fact_4   $=$   Verify( \"Alfredo Cornejo Cuevas won the world amateur welterweight title in Mexico City.\" ) label  $=$  Predict(fact_1 and fact_2 and fact_3 and fact_4)\n# The claim is that The Footwork FA12 , which was intended to start the season , finally debuted at the San Marino Grand Prix , a Formula One motor race held at Imola on 28 April 1991. \ndef  program (): fact_1   $=$   Verify( \"The Footwork FA12 , which was intended to start the season.\" ) fact_2   $=$   Verify( \"The Footwork FA12 finally debuted at the San Marino Grand Prix. \" ) fact  $\\begin{array}{r l}{\\_3}&{{}=}\\end{array}$   Verify( \"The San Marino Grand Prix was a Formula One motor race held at Imola on 28 April 1991.\" ) label   $=$   Predict(fact_1  and  fact_2  and  fact_3) \n# The claim is that SkyHigh Mount Dandenong (formerly Mount Dandenong Observatory) is a restaurant located on top of Mount Dandenong , Victoria , Australia. \ndef  program (): fact_1   $=$   Verify( \"SkyHigh Mount Dandenong is a restaurant located on top of Mount Dandenong , Victoria , Australia.\" ) fact  $_-2\\;\\;=$   Verify( \"SkyHigh Mount Dandenong is formerly known as Mount Dandenong Observatory.\" ) label  $=$  Predict(fact_1 and fact_2)\n# The claim is that Before the first Europeans arrived or copra companies leased it , Maupihaa was home to Inca's in ancient times. \ndef \nfact_1  $=$   Verify( \"Maupihaa was home to Inca's in ancient times.\" ) fact_2   $=$   Verify( \"Maupihaa was home to Inca's before the first Europeans arrived or copra companies leased it.\" ) label  $=$  Predict(fact_1 and fact_2)\n# The claim is that Shulin , a 33.1288 km (12.7911 sq mi) land located in New Taipei City , China , a country in East Asia , has a total population of 183 ,946 in December 2018. \ndef  program (): fact_1  $=$   Verify( \"Shulin is a 33.1288 km (12.7911 sq mi) land located in New Taipei City , China.\" ) fact  $_-2\\;\\;=$   Verify( \"Shulin has a total population of 183 ,946 in December 2018.\" ) label  $=$  Predict(fact_1 and fact_2)\n# The claim is that Sumo wrestler Toyozakura Toshiaki committed match -fixing , ending his career in 2011 that started in 1989. \ndef  program (): fact_1   $=$   Verify( \"Toyozakura Toshiaki ended his career in 2011 that started in 1989.\")fact  $_-2\\;\\;=$   Verify( \"Toyozakura Toshiaki is a Sumo wrestler.\" ) fact  $_-3\\;\\;=$   Verify( \"Toyozakura Toshiaki committed match -fixing.\" ) label   $=$   Predict(fact_1  and  fact_2  and  fact_3) \n# The claim is that In 1959, former Chilean boxer Alfredo Cornejo Cuevas (born June 6, 1933) won the gold medal in the welterweight division at the Pan American Games (held in Chicago , United States , from August 27 to September 7) in Chicago , United States , and the world amateur welterweight title in Mexico City. "}
{"page": 21, "image_path": "doc_images/2023.acl-long.386_21.jpg", "ocr_text": ", United States, and the world amateur welterweight\ndef program():\n\nfact_1 = Verify(\"Alfredo Cornejo Cuevas is a former\n\nfact_2 = Verify(\"Alfredo Cornejo won the gold medal\n\nat the Pan American Games.”)\n\nfact_3 = Verify(\"The Pan American Games was held in\n\nAugust 27 to September 7.\")\n\nfact_4 = Verify(\"Alfredo Cornejo won the world amateur welterweight title in\nMexico City.\"”)\n\nlabel = Predict(fact_1 and fact_2 and fact_3 and fact_4)\n\ntitle in Mexico City.\n\nChilean boxer.\"”)\nin the welterweight division\n\nChicago, United States, from\n\n# The claim is that Adductor hiatus is associated with nine structures,\nwhich enter and leave through hiatus.\n\ndef program():\nfact_1 = Verify(\"Adductor hiatus is associated with nine structures.”)\n\nfact_2 = Verify(\"Seven of the nine structures associated with Adductor hiatus\nenter and leave through hiatus.\")\n\nlabel = Predict(fact_1 and fact_2)\n\nseven of\n\n# The claim is that Ifor Bowen Lloyd was educated at Winchester (an independent\nboarding school for boys in the British public school tradition) and Exeter\nCollege, Oxford where he was a member of the Library Committee of the Oxford\n\nUnion Society, as well as, received a BA in Modern History in 1924\ndef program():\n\nfact_1 = Verify(\"Ifor Bowen Lloyd was educated at Winchester and Exeter College,\nOxford.\"”)\nfact_2 =\n\nVerify(\"Winchester is an independent boarding school for boys in the\nBritish public school tradition.”)\n\nfact_3 = Verify(\"While at Oxford, Ifor Bowen Lloyd was a member of the Library\nCommittee of the Oxford Union Society.\"”)\n\nfact_4 = Verify(\"Ifor Bowen Lloyd received a BA in Modern History in 1924 at\nOxford.\"”)\n\nlabel = Predict(fact_1 and fact_2 and fact_3 and fact_4)\n\n# The claim is that In the 2001 Stanley Cup playoffs Eastern Conference Semifinals\n\nDevils' Elias scored and Maple Leafs' left Devils player Scott Neidermayer hurt.\ndef program():\n\nfact_1 = Verify(\"In the 2001\nDevils' Elias scored.”)\n\nfact_2 = Verify(\"Maple Leafs' left Devils player Scott Neidermayer hurt.\")\nlabel = Predict(fact_1 and fact_2)\n\nStanley Cup playoffs Eastern Conference Semifinals\n\n# The claim is that Teldenia helena is a moth first described in 1967 by Wilkinson.\ndef program():\n\nfact_1 = Verify(\"Teldenia helena is a moth.\"”)\n\nfact_2 = Verify(\"Teldenia helena was first described by Wilkinson in 1967.\")\nlabel = Predict(fact_1 and fact_2)\n\n# The claim is that Born December 30, 1974, William Frick was a dark horse candidate\nin the Maryland House of Delegates appointment process.\n\ndef program():\n\nfact_1 = Verify(\"William Frick was born in December 30, 1974.\")\n\nfact_2 = Verify(\"William Frick was a dark horse candidate in the Maryland House\nof Delegates appointment process.\")\n\nlabel = Predict(fact_1 and fact_2)\n\n# The claim is that <input_claim>\ndef program():\n\nListing 2: The prompt used for Program Generation for FEVEROUS-S.\n\n7002\n", "vlm_text": "def  program (): fact_1  $=$   Verify( \"Alfredo Cornejo Cuevas is a former Chilean boxer.\" ) fact_2   $=$   Verify( \"Alfredo Cornejo won the gold medal in the welterweight division at the Pan American Games.\" ) fact  $\\begin{array}{r l}{\\_3}&{{}=}\\end{array}$   Verify( \"The Pan American Games was held in Chicago , United States , from August 27 to September 7.\" ) fact  $\\begin{array}{r l}{\\_4}&{{}=}\\end{array}$   Verify( \"Alfredo Cornejo won the world amateur welterweight title in Mexico City.\" ) label  $=$  Predict(fact_1 and fact_2 and fact_3 and fact_4)# The claim is that Adductor hiatus is associated with nine structures , seven of which enter and leave through hiatus. def  program (): fact_1   $=$   Verify( \"Adductor hiatus is associated with nine structures.\" ) fact  $_{-}2\\;\\;=\\;\\;$   Verify( \"Seven of the nine structures associated with Adductor hiatus enter and leave through hiatus.\" ) label  $=$  Predict(fact_1 and fact_2)# The claim is that Ifor Bowen Lloyd was educated at Winchester (an independent boarding school for boys in the British public school tradition) and Exeter College , Oxford where he was a member of the Library Committee of the Oxford Union Society , as well as, received a BA in Modern History in 1924. def  program (): fact_1   $=$   Verify( \"Ifor Bowen Lloyd was educated at Winchester and Exeter College , Oxford.\" ) fact  $_-2\\;\\;=$   Verify( \"Winchester is an independent boarding school for boys in the British public school tradition.\" ) fact  $\\begin{array}{r l}{\\_3}&{{}=}\\end{array}$   Verify( \"While at Oxford , Ifor Bowen Lloyd was a member of the Library Committee of the Oxford Union Society.\" ) fact_4   $=$   Verify( \"Ifor Bowen Lloyd received a BA in Modern History in 1924 at Oxford.\" ) label  $=$  Predict(fact_1 and fact_2 and fact_3 and fact_4)# The claim is that In the 2001 Stanley Cup playoffs Eastern Conference Semifinals Devils ' Elias scored and Maple Leafs ' left Devils player Scott N eider mayer hurt. def  program (): fact_1  $=$   Verify( \"In the 2001 Stanley Cup playoffs Eastern Conference Semifinals Devils ' Elias scored.\" ) fact  $_-2\\;\\;=$   Verify( \"Maple Leafs ' left Devils player Scott N eider mayer hurt.\" ) label  $=$  Predict(fact_1 and fact_2)# The claim is that Teldenia helena is a moth first described in 1967 by Wilkinson. def  program (): fact_1   $=$   Verify( \"Teldenia helena is a moth.\" ) fact_2   $=$   Verify( \"Teldenia helena was first described by Wilkinson in 1967.\" ) label  $=$  Predict(fact_1 and fact_2)# The claim is that Born December 30, 1974, William Frick was a dark horse candidate in the Maryland House of Delegates appointment process. def  program (): fact_1  $=$   Verify( \"William Frick was born in December 30, 1974.\" ) fact  $_-2\\;\\;=$   Verify( \"William Frick was a dark horse candidate in the Maryland House of Delegates appointment process.\" ) label  $=$  Predict(fact_1 and fact_2)# The claim is that  <input claim> def  program (): "}
{"page": 22, "image_path": "doc_images/2023.acl-long.386_22.jpg", "ocr_text": "ACL 2023 Responsible NLP Checklist\n\nA For every submission:\n\nAl. Did you describe the limitations of your work?\nLine 587 - 620\n\n A2. Did you discuss any potential risks of your work?\nLine 626 - 630\n\n“ A3. Do the abstract and introduction summarize the paper’s main claims?\nLine 67 - 86\n\n& A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\n\nB & Did you use or create scientific artifacts?\nLine 327 - 352\n\nW B1. Did you cite the creators of artifacts you used?\nLine 328 - 329\n\nB2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nNot applicable. The datasets used in this paper are publicly available datasets from existing works.\n\nM B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was specified? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\n\nLine 327 - 344\n\nB4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identifies individual people or offensive content, and the steps\ntaken to protect / anonymize it?\n\nNot applicable. Left blank.\n\nB5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nNot applicable. Left blank.\n\nM B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe significant, while on small test sets they may not be.\n\nLine 327 - 344\n\nCc Did you run computational experiments?\nSection 4\nMW Cl. Did you report the number of parameters in the models used, the total computational budget\n\n(e.g., GPU hours), and computing infrastructure used?\nFigure 4; Appendix A\n\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n\n7003\n", "vlm_text": "A For every submission: \n□  A1. Did you describe the limitations of your work? Line 587 - 620 \n□  A2. Did you discuss any potential risks of your work? Line 626 - 630 \n□  A3. Do the abstract and introduction summarize the paper’s main claims? Line 67 - 86 \n□  A4. Have you used AI writing assistants when working on this paper? Left blank. \nB □  Did you use or create scientiﬁc artifacts? Line 327 - 352 \n□  B1. Did you cite the creators of artifacts you used? Line 328 - 329 \n□ B2. Did you discuss the license or terms for use and / or distribution of any artifacts? Not applicable. The datasets used in this paper are publicly available datasets from existing works. \n□  B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)? Line 327 - 344 \n□ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any information that names or uniquely identiﬁes individual people or offensive content, and the steps taken to protect / anonymize it? Not applicable. Left blank. \n□ B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.? Not applicable. Left blank. \n□  B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits, etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the number of examples in train / validation / test splits, as these provide necessary context for a reader to understand experimental results. For example, small differences in accuracy on large test sets may be signiﬁcant, while on small test sets they may not be. Line 327 - 344 \nC □  Did you run computational experiments? \nSection 4 \n□  C1. Did you report the number of parameters in the models used, the total computational budget (e.g., GPU hours), and computing infrastructure used? Figure 4; Appendix A "}
{"page": 23, "image_path": "doc_images/2023.acl-long.386_23.jpg", "ocr_text": "D\n\nC2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\n\nAppendix A\n\nW C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\n\nAppendix A\n\nC4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\n\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nNot applicable. Left blank.\n\nDid you use human annotators (e.g., crowdworkers) or research with human participants?\n\nLeft blank.\n\nD1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNot applicable. Left blank.\n\nD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\n\nNot applicable. Left blank.\n\nD3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\n\nNot applicable. Left blank.\n\nD4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNot applicable. Left blank.\n\nDS. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNot applicable. Left blank.\n\n7004\n", "vlm_text": "□  C2. Did you discuss the experimental setup, including hyper parameter search and best-found hyper parameter values? Appendix A\n\n \n□  C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary statistics from sets of experiments), and is it transparent whether you are reporting the max, mean, etc. or just a single run? Appendix A\n\n □ C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE, etc.)? Not applicable. Left blank.\n\n \nD □  Did you use human annotators (e.g., crowd workers) or research with human participants? \n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots, disclaimers of any risks to participants or annotators, etc.? Not applicable. Left blank.\n\n □ D2. Did you report information about how you recruited (e.g., crowd sourcing platform, students) and paid participants, and discuss if such payment is adequate given the participants’ demographic (e.g., country of residence)? Not applicable. Left blank.\n\n □ D3. Did you discuss whether and how consent was obtained from people whose data you’re using/curating? For example, if you collected data via crowd sourcing, did your instructions to crowd workers explain how the data would be used? Not applicable. Left blank.\n\n □ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board? Not applicable. Left blank.\n\n □ D5. Did you report the basic demographic and geographic characteristics of the annotator population that is the source of the data? Not applicable. Left blank. "}
