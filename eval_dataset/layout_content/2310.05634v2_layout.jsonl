{"layout": 0, "type": "text", "text": "Towards Verifiable Generation: A Benchmark for Knowledge-aware Language Model Attribution ", "text_level": 1, "page_idx": 0, "bbox": [88, 72, 509, 106], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 1, "type": "text", "text": "Xinze  $\\mathbf{Li^{1}}$  , Yixin  $\\mathbf{C}\\mathbf{a}\\mathbf{o}^{2\\dagger}$  , Liangming  $\\mathbf{P}\\mathbf{a}\\mathbf{n}^{3}$  , Yubo  $\\mathbf{M}\\mathbf{a}^{1}$  , Aixin Sun 1 † ", "page_idx": 0, "bbox": [136.0659942626953, 115, 461.6988525390625, 131.30877685546875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 2, "type": "text", "text": "1  S-Lab, Nanyang Technological University 2  Singapore Management University  3  University of California, Santa Barbara {xinze002, yubo001}@e.ntu.edu.sg axsun@ntu.edu.sg yxcao@smu.edu.sg liang ming pan@ucsb.edu ", "page_idx": 0, "bbox": [108, 139.74246215820312, 486, 199.80445861816406], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 3, "type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0, "bbox": [157, 220, 204, 233], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 4, "type": "text", "text": "Although achieving great success, Large Lan- guage Models (LLMs) usually suffer from un- reliable hallucinations. Although language at- tribution can be a potential solution, there are no suitable benchmarks and evaluation metrics to attribute LLMs to structured knowledge. In this paper, we define a new task of Knowledge- aware Language Model Attribution (KaLMA) that improves upon three core concerns with conventional attributed LMs. First, we extend attribution source from unstructured texts to Knowledge Graph (KG), whose rich structures benefit both the attribution performance and working scenarios. Second, we propose a new “Conscious Incompetence\" setting considering the incomplete knowledge repository, where the model identifies the need for supporting knowledge beyond the provided KG. Third, we propose a comprehensive automatic evaluation metric encompassing text quality, citation qual- ity, and text citation alignment. To implement the above innovations, we build a dataset in biography domain BioKaLMA via evolution- ary question generation strategy, to control the question complexity and necessary knowledge to the answer. For evaluation, we develop a baseline solution and demonstrate the room for improvement in LLMs’ citation generation, em- phasizing the importance of incorporating the \"Conscious Incompetence\" setting, and the crit- ical role of retrieval accuracy. ", "page_idx": 0, "bbox": [86, 246.78558349609375, 273, 617.4454956054688], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 5, "type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0, "bbox": [71, 631, 154, 645], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 6, "type": "text", "text": "Recently, Large Language Models ( Brown et al. , 2020 ) (LLMs) have exhibited great capability in open-ended question answering ( Yang et al. ,  2019 ). However, the generated answers may include fac- tual errors and are not always reliable, and is com- monly known as the “hallucination” ( Shuster et al. , 2021 ;  Ji et al. ,  2023 ) problem. For instance, LLMs may give wrong diagnosis to patient’s symptoms. Hallucination has severe harms especially on indus- ", "page_idx": 0, "bbox": [70, 654.3610229492188, 291, 775.8994140625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 7, "type": "image", "page_idx": 0, "img_path": "layout_images/2310.05634v2_0.jpg", "bbox": [304, 219, 526, 447], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "Question: How did Orazio Gentileschis influence on Artemisia's life\nand career shape her development as a Baroque painter?\n\n—_ Retrieve\nww oO\nae\n\nKnowledge\nGraph\n\natozetat\nPainter o37e53\nof a\n\nos\n\nn\nae, child\nsy\n\nBe\nOrazio Artemisia\n\nGentileschi Gentileschi\n0367560 0212657\n\nLLM Attribute\n\nOutput: Artemisia Gentileschi was an Italian painter born on July 8, )\n1596 [NA] in Rome [Q212657, citizen: Italy, occupation: painter,\nplace of birth: Rome]. She was a member of ...\n\n", "vlm_text": "The image is a diagram illustrating the influence of Orazio Gentileschi on Artemisia Gentileschi's life and career in developing her as a Baroque painter. It uses a knowledge graph to show relationships between:\n\n- Orazio Gentileschi, a painter and the father of Artemisia.\n- Artemisia Gentileschi, a painter associated with the Baroque style.\n- Their citizenship in Italy.\n\nThe knowledge graph connects these figures with concepts like occupation, style, place of birth, and their citizen status. The output suggests information about Artemisia’s birth and her status as an Italian painter."}
{"layout": 8, "type": "text", "text": "tries that require precision and factual knowledge like finance, law, and medical treatment. ", "page_idx": 0, "bbox": [305, 531.468994140625, 526, 558.1634521484375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 9, "type": "text", "text": "To minimize the negative impacts, researchers have proposed the task of language attribu- tion ( Bohnet et al. ,  2023 ), which not only enables users to verify the generated text flexibly but also contributes to many important applications, such as situation reports ( Reddy et al. ,  2023 ), academic papers ( Salvagno et al. ,  2023 ), medical diagno- sis ( Zuccon and Koopman ,  2023 ).Existing works mainly attribute generated outputs to unstructured documents like web pages ( Nakano et al. ,  2021 ; Menick et al. ,  2022 ) or passages ( Gao et al. ,  2023 ). To verify the answer quality, they typically compare with a human annotated reference answer for auto- matic evaluation or conduct human evaluation. We argue that there are several concerns on such task definition.  Firstly , are documents the only source for attribution? Many real-world applications have their own knowledge bases or semi-structured re- ports.  Secondly , does the attribution source always include all the required knowledge? We consider the coverage issue since no perfect repository can contain all the information in this world.  Thirdly , how to systematically evaluate the attributed con- tent without references? For open-ended questions, there are unlimited number of answers and it is difficult to define a single ground truth. ", "page_idx": 0, "bbox": [305, 559.5159912109375, 526, 776.5540161132812], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 10, "type": "text", "text": "", "page_idx": 1, "bbox": [70, 71.74500274658203, 291, 206.83346557617188], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 11, "type": "text", "text": "To address the first challenge, we utilize knowl- edge graph (KG) as a reliable source for attribution, namely Knowledge-aware Language Model Attri- bution ( KaLMA ). We show a demonstration of task in Figure  1 . KGs efficiently organize world knowl- edge in a structured manner and has the poten- tial to unify various formats of data. For example, databases can be easily converted into KGs, or, pas- sages and web pages can be represented as a node in KG like Wikipedia. KaLMA differs from entity linking ( Sevgili et al. ,  2022 ) since the sentences or phrases are attributed to a knowledge triplet rather than a single entity. For the second challenge, we tackle the coverage problem by making the model aware of its limitations. We introduce a new setting “ Conscious Incompetence ” ( Curtiss and Warren , 1974 ), which is the psychological stage that one is aware of the knowledge gap. During generation, LLMs identify sentences that require supporting knowledge absent in the knowledge graph. Our setting enables an attributed LM to recognize the knowledge gaps and allows users to verify uncer- tain claims, which enhances trustworthiness. For the third challenge, we propose a comprehensive automatic evaluation metric including text quality, citation quality, and text citation alignment. The entire evaluation process does not require human annotated ground truth. ", "page_idx": 1, "bbox": [70, 209.75099182128906, 291, 588.7244873046875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 12, "type": "text", "text": "To implement the above innovations, we first design an automatic dataset construction pipeline. Using this pipeline, we construct a dataset 1   in the biographical domain, namely  BioKaLMA , for a benchmark with all-rounded automatic measure- ments. Biography forms a good test-set for attri- bution due to its practical application and conve- nient evaluation. The availability of high-quality knowledge graph like WikiData also benefits our dataset construction. Derived from the biograph- ical database 2   ( Plum et al. ,  2022 ) and WikiData, BioKaLMA contains 1,085 data entries. Each data entry includes question and knowledge required to answer the question. For evaluation, we separately evaluate the generated text, the generated citations, and the alignment between texts and citations. We use G-Eval ( Liu et al. ,  2023b ) to automatically eval- uate the text quality. We also design measurement for correctness, precision, and recall for citations. Lastly, we determine the alignment between texts and citations employing NLI ( Dagan et al. ,  2005 ) ", "page_idx": 1, "bbox": [70, 591.6420288085938, 291, 726.73046875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 13, "type": "text", "text": "", "page_idx": 1, "bbox": [305, 67.78601837158203, 526, 220.38247680664062], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 14, "type": "text", "text": "We summarize our contributions as follows: 1) We define the task of Knowledge-aware Language Model Attribution (KaLMA) that attributes lan- guage models to structured knowledge. 2) We de- sign a complete benchmarking pipeline, including dataset, baseline, and evaluation metrics. 3) We conduct extensive experiments and show room for improvement of the LLMs’ ability to generate ac- curate and thorough citations based on provided knowledge graphs. Our experiments on “Conscious Incompetence” investigate the capability of current LLMs to identify if there are required knowledge not in knowledge graph. We highlight the necessity of incorporating this setting in future language at- tribution works. Furthermore, our ablation studies demonstrate the crucial role of retrieval accuracy in achieving desirable generation results. ", "page_idx": 1, "bbox": [305, 221.38002014160156, 526, 451.3124694824219], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 15, "type": "text", "text": "2 Task and Dataset ", "text_level": 1, "page_idx": 1, "bbox": [305, 464, 413, 476], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 16, "type": "text", "text": "2.1 Task Formulation ", "text_level": 1, "page_idx": 1, "bbox": [305, 487, 415, 499], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 17, "type": "text", "text": "We hereby define the task Knowledge-aware Lan- guage Model Attribution  (KaLMA) : Given a ques- tion  $q$   and the knowledge graph  $G$  , the system gen- erates an output text    $t$   that answers the question. The output text consists of a list of    $m$   sentences    $s_{1}$  , ...,    $s_{m}$   grounded with a list of  $n$   grounded knowl- edge  $k_{1}$   ..    $k_{n}$   where    $\\{k_{1}..k_{n}\\}\\in G$  . Each  nowl- edge  k  is a sub-graph of  G . Each sentence  s  may be grounded by zero up to multiple knowledge. ", "page_idx": 1, "bbox": [305, 505.02398681640625, 526, 626.5634765625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 18, "type": "text", "text": "Setting of Conscious Incompetence We extend this task setting to include conscious incompetence. Given the same input, each sentence    $s$   in the output text  $t$   can map to a Not Applicable Citation (we use [NA] to represent it) if it includes some knowledge to be verified, but the knowledge is absent in the knowledge graph    $G$  . A sentence can map to both [NA] and a list of sub-graph knowledge if it can be partially verified by the knowledge graph    $G$  . [NA] is not a citation on conventional means, but a indicator of knowledge gap. ", "page_idx": 1, "bbox": [305, 635.9923095703125, 526, 744.3744506835938], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 19, "type": "text", "text": "", "page_idx": 2, "bbox": [70, 71.74500274658203, 291, 111.98947143554688], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 20, "type": "text", "text": "2.2 Dataset Construction ", "text_level": 1, "page_idx": 2, "bbox": [70, 126, 195, 137], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 21, "type": "text", "text": "Each entry of dataset bioKaLMA includes two questions and a minimum knowledge set. The two questions enquire about the same people on similar aspects of their life stories. The minimum knowl- edge set is the smallest set of knowledge that is required to answer each question. One question is a general version and the other is specific. The general questions are more concise and natural for human readers, and the specific version questions have a tighter bond to the minimum knowledge set, and is hence more accurate for evaluating LLMs. An example data piece is shown in Table  1 . ", "page_idx": 2, "bbox": [70, 145.63002014160156, 291, 307.8174743652344], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 22, "type": "text", "text": "We construct the dataset using an automatic pipeline consisting of three steps: Person Selection, Name Disambiguation, and Evolutionary Question Generation. In the first two steps, we use SPARQL queries to select related people from human written sentences and identify their identity in WikiData. In the third step, we iterative ly construct paragraph and question about the selected people. The first iteration starts with a human written sentence about the selected people. In each next iteration, we apply a data selection algorithm to select an appropriate knowledge from WikiData based on the existing paragraph, and extend the paragraph to include the additional knowledge using LLM. Then, LLM constructs the questions using the final paragraph as an answer. The general and specific questions are generated with different prompts and demon- strations. All the selected knowledge from each iteration form the “minimum knowledge set” for the question. While we use the human biography domain as an example, this method is applicable to all domains. We present the details of the data construction in Appendix  A . ", "page_idx": 2, "bbox": [70, 309.49200439453125, 291, 620.719482421875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 23, "type": "text", "text": "2.3 Dataset Analysis ", "text_level": 1, "page_idx": 2, "bbox": [70, 635, 173, 647], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 24, "type": "text", "text": "Statistics There are 1,085 data entries in BioKalMA. On average, there are 6.8 pieces of knowledge in each “minimum knowledge set”. BioKaLMA demonstrates a good demographic variation. It includes a wide range of geograph- ical distribution of people from 196 countries and 949 cities, taking 279 kinds of different occupa- tions. The eras of people span from 1950 B.C. to 2001 A.D. ", "page_idx": 2, "bbox": [70, 653.9683227539062, 291, 775.8994140625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 25, "type": "text", "text": "Table 1: An example for generated data entry in BioKaLMA. Q85907 and Q68753 are Richard Hertwig and Oscar Hertwig’s QIDs in WikiData ", "page_idx": 2, "bbox": [304, 268.0865478515625, 526, 304.00152587890625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 26, "type": "table", "page_idx": 2, "img_path": "layout_images/2310.05634v2_1.jpg", "table_footnote": "Table 2: Human Evaluation on BioKaLMA dataset. ", "bbox": [304, 316, 519, 398], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "Metric (full score) | General Specific\nAuthenticity (1) 1.00 1.00\nRelevance (1) 0.73 0.84\nNaturalness (5) 4.38 352\nSignificance (5) 3.94 3.68\n\n", "vlm_text": "The table presents data on four different metrics: Authenticity, Relevance, Naturalness, and Significance. Each metric has a specified full score indicated in parentheses next to the metric name. The scores are compared across two categories: General and Specific.\n\n- **Authenticity** (full score of 1):\n  - General: 1.00\n  - Specific: 1.00\n\n- **Relevance** (full score of 1):\n  - General: 0.73\n  - Specific: 0.84\n\n- **Naturalness** (full score of 5):\n  - General: 4.38\n  - Specific: 3.52\n\n- **Significance** (full score of 5):\n  - General: 3.94\n  - Specific: 3.68\n\nEach metric shows how both General and Specific categories are scored, with full scores given in parentheses next to each metric name for reference."}
{"layout": 27, "type": "text", "text": "Evaluation of Dataset Quality We evaluate the BioKaLMA dataset on the following four metrics to ensure the quality of the dataset: 1)  Authen- ticity : The generated questions should accurately reflect the objective facts. 2)  Relevance : Each min- imum knowledge set should provide support to the corresponding question. Each piece of knowledge from the minimum knowledge set is not redundant. 3)  Naturalness : The generated question should be concise and understandable by human readers. 4) Significance : The generated question should be meaningful and helpful to users. ", "page_idx": 2, "bbox": [304, 421.2402648925781, 526, 583.8194580078125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 28, "type": "text", "text": "To our best knowledge, there is no perfect au- tomatic evaluation for these metrics. Naturalness and significance are subjective. Hence, we apply human evaluation to ensure the dataset quality. ", "page_idx": 2, "bbox": [304, 585.02099609375, 526, 638.8134765625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 29, "type": "text", "text": "We randomly sample 50 data entries from BioKaLMA and ask human annotators to evalu- ate the data entries based on the four metrics. The general and specific questions are evaluated sepa- rately. More details are given in Appendix  C . ", "page_idx": 2, "bbox": [304, 640.0139770507812, 526, 707.3564453125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 30, "type": "text", "text": "The final result for each metric is taken average and reported in Table  2 . For both general and spe- cific settings, the questions from sample achieve a  $100\\%$   authenticity, which indicates that the overall authenticity of BioKaLMA dataset is high. The rel- evance on general and specific settings are  $73\\%$   and  $84\\%$   respectively. The specific question normally consists of more parts and include more details than its general version, and hence some knowl- edge are necessary to the specific version but not to the general version. However, the general ver- sion questions sacrifice relevance to achieve better naturalness and significance. ", "page_idx": 2, "bbox": [304, 708.5579833984375, 526, 775.8994140625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 31, "type": "text", "text": "", "page_idx": 3, "bbox": [70, 71.74500274658203, 291, 179.73550415039062], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 32, "type": "text", "text": "In practice, it is difficult to define a precise “min- imum knowledge set” for a question unless it is very specific. However, a very specific question tends to be artificial. The relevance and naturalness of a question have a trade-off relationship. It is yet challenging to generate questions that have both high relevance and high naturalness, but our gener- ation method allows for a control on the granularity of a question on whether it tends to be more natural or more relevant. ", "page_idx": 3, "bbox": [70, 180.3880157470703, 291, 315.4764709472656], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 33, "type": "text", "text": "3 Method ", "text_level": 1, "page_idx": 3, "bbox": [70, 327, 130, 339], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 34, "type": "text", "text": "We build a baseline to enable LLMs to generate knowledge-aware attributed answers. Following the approach of many retrieval augmented gener- ation works ( Lee et al. ,  2022 ;  Izacard and Grave , 2021 ), we utilize a pipeline consisting of three com- ponents: retrieval, re-ranking, and generation. ", "page_idx": 3, "bbox": [70, 348.49200439453125, 291, 429.38348388671875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 35, "type": "text", "text": "3.1 Retrieval ", "text_level": 1, "page_idx": 3, "bbox": [70, 440, 138, 451], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 36, "type": "text", "text": "Our baseline retrieval process consists of two parts: named entity recognition and graph retrieval. We utilize spaCy 3   to identify the named entities men- tioned in the question. Using these entities, we retrieve entity-centered sub-graphs using SPARQL. For each retrieved entity, we search for nodes in the graph that match the entity’s name. We use the named entity recognition (NER) entity type as a simple filter (e.g., the NER category “person” matches the “human” entity type in WikiData). Tak- ing each selected node as the center, we retrieve one-hop sub-graphs that contain properties associ- ated with the entity. ", "page_idx": 3, "bbox": [70, 457.4599914550781, 291, 633.1954345703125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 37, "type": "text", "text": "3.2 Re-ranking ", "text_level": 1, "page_idx": 3, "bbox": [70, 644, 149, 656], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 38, "type": "text", "text": "The re-ranking component plays a crucial role in disambiguating retrieved entities, as multiple en- tities may share the same name in the WikiData graph. Two common scenarios are different indi- viduals with the same name (e.g., Anne Hathaway the American actress and Anne Hathaway the wife of William Shakespeare) and different references to the same word (e.g., “Chinese” the language and “Chinese” the ethnic group). When multiple entities are retrieved from the graph for a given entity name, we rank the graphs based on the Exact Match (EM) between the neighboring nodes and the question. We select the entity with the highest number of matched neighboring nodes. ", "page_idx": 3, "bbox": [70, 661.27099609375, 291, 755.7124633789062], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 39, "type": "text", "text": "", "page_idx": 3, "bbox": [305, 71.74500274658203, 526, 166.18545532226562], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 40, "type": "text", "text": "3.3 Generation ", "text_level": 1, "page_idx": 3, "bbox": [306, 175, 384, 188], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 41, "type": "text", "text": "The generation component effectively prompt the LLMs with the retrieved knowledge graphs (KGs) to generate answers that attribute the KG. To adapt to the input format of the LLMs, we transform the structured KGs into flat texts. We preserve the information of the retrieved sub-graphs by map- ping each sub-graph to a set of triples. Each triple consists of two nodes and one edge, where one node is the centered entity, the other node is its neighbor, and the edge represents the relationship between them. For example, [Q212657 - place of birth - Q220] can be translated to [Artemisia Gen- tileschi - place of birth - Rome]. In this translation, we use the names of the entities for better com- prehension by both the models and humans, since WikiData utilizes QIDs (e.g., Q220) to represent unique entities. We construct a prompt (Table  13 in appendix  D ) which includes 1) instruction to the models to generate attributed answers. 2) re- trieved knowledge graph, and 3) the question. We employ one-shot in-context learning ( Brown et al. , 2020 ) by prepending one human written demon- stration. In the one-shot demonstration, we use the special token [NA] to represent the “Not Appli- cable Citations” for conscious incompetence. We deliberately omit some knowledge in the demon- stration example knowledge graph, and we insert [NA] tokens in the corresponding sentences that use these knowledge within the example answer. ", "page_idx": 3, "bbox": [305, 192.28700256347656, 526, 584.8104248046875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 42, "type": "text", "text": "4 Evaluation Metrics ", "text_level": 1, "page_idx": 3, "bbox": [306, 595, 422, 607], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 43, "type": "text", "text": "Our benchmark includes evaluation metrics for both the generated text and citations. We also evalu- ate the alignment between the text and correspond- ing citations. We provide more discussions on the design of evaluation metrics in subsection  4.5 . ", "page_idx": 3, "bbox": [305, 615.114013671875, 526, 682.4554443359375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 44, "type": "text", "text": "4.1 Text Evaluation ", "text_level": 1, "page_idx": 3, "bbox": [306, 691, 405, 703], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 45, "type": "text", "text": "Since our test-set has no human-written gold an- swers as references, we do not utilize comparison- based metrics such as BERTScore ( Zhang et al. , 2019a ) or MAUVE ( Pillutla et al. ,  2021 ). In- stead, we employ reference-free NLG evaluator G- ", "page_idx": 3, "bbox": [305, 708.5579833984375, 526, 775.8994140625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 46, "type": "image", "page_idx": 4, "img_path": "layout_images/2310.05634v2_2.jpg", "img_caption": "Figure 2: An illustration of how we evaluate the preci- sion and recall for generated citations. ", "bbox": [70, 69, 290, 253], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "\\\n\nModel Output: Mininum\nSentence1 [k1][k2]. Knowledge Set:\nSentence2 [k2][k6][NA]. | [k1] [k2]\nSentence3 [k6][k9]. [k3] [k4] [k5]\n\n( Citation Precision Citation Recall\nSentence’ [k1][k2]. Knowlege:\nSentence2 [k2][k6][NA]. <—> _[k1] [k2]\nSentence3 [k6][k9]. [k3] [k4] [k5]\n\ncorrect = 3, all=6 hit = 2, all =5\n\n| precision = 3/6 = 0.5 recall = 2/5 = 0.4\n\nMAN\n\n", "vlm_text": "The image shows a diagram explaining how to evaluate precision and recall for generated citations.\n\n- **Model Output**:\n  - Sentence1: [k1][k2]\n  - Sentence2: [k2][k6][NA]\n  - Sentence3: [k6][k9]\n\n- **Minimum Knowledge Set**:\n  - [k1] [k2]\n  - [k3] [k4] [k5]\n\n- **Citation Precision**:\n  - Sentence1: [k1][k2]\n  - Sentence2: [k2][k6][NA]\n  - Sentence3: [k6][k9]\n  - Correct = 3, All = 6\n  - Precision = 3/6 = 0.5\n\n- **Citation Recall**:\n  - Knowledge: [k1][k2][k3][k4][k5]\n  - Hit = 2, All = 5\n  - Recall = 2/5 = 0.4\n\nThe illustration highlights the calculation of precision and recall based on the model output and a set of minimum knowledge."}
{"layout": 47, "type": "text", "text": "Eval ( Liu et al. ,  2023b ), which defines the follow- ing four metrics: 1)  Coherence : whether the gen- erated text is well-structured and well-organized. 2)  Consistency : whether the generated text is con- sistent with the knowledge provided. 3)  Fluency : whether the generated text is well-written and gram- matical. 4)  Relevance : how well is the generated text relevant to the question. ", "page_idx": 4, "bbox": [70, 274.63397216796875, 291, 382.62347412109375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 48, "type": "text", "text": "We use the model text-davinci-003 for evalua- tion, which assigns an integer score of 1 to 5 for each metric. We follow the prompt provided in G- Eval ( Liu et al. ,  2023b ) and customize it based on our task. The full prompts are given in appendix  D . ", "page_idx": 4, "bbox": [70, 383.0270080566406, 291, 450.3694763183594], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 49, "type": "text", "text": "4.2 Citation Evaluation ", "text_level": 1, "page_idx": 4, "bbox": [70, 461, 188, 472], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 50, "type": "text", "text": "We evaluate the citation qualities from three as- pects: 1)  Correctness , which measures whether the generated knowledge matches the given knowledge from the knowledge graph, 2)  Precision , which determines how much of the generated citations are helpful to answer the question, and 3)  Recall , which measures how much of the minimum knowl- edge set are covered by the generated citations. We also calculate the F1-Score based on the Precision and Recall to reflect the overall quality of citations. ", "page_idx": 4, "bbox": [70, 477.0719909667969, 291, 612.1604614257812], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 51, "type": "text", "text": "Correctness We calculate the citation correct- ness for each citation (0 or 1) and average over all citations. Each citation comprises a triplet of 1) center entity QID, 2) relation 3) neighbour entity value. If the generated citation is complete with all three parts, and exactly matches a triplet from the question’s retrieved KG, correctness  $=1$  . ", "page_idx": 4, "bbox": [70, 619.5203247070312, 291, 714.3534545898438], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 52, "type": "text", "text": "Precision We calculate citation precision for each citation (0 or 1) and average over all citations to get micro precision. Precision  $=1$   for a citation if and only if 1) it is correct, and 2) it matches one ", "page_idx": 4, "bbox": [70, 721.7142944335938, 291, 775.8994140625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 53, "type": "image", "page_idx": 4, "img_path": "layout_images/2310.05634v2_3.jpg", "img_caption": "Figure 3: An illustration of how we evaluate the preci- sion and recall for conscious incompetence ([NA]) ", "bbox": [305, 69, 525, 253], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "Model Output:\nSentence1 [k1][NA].\nSentence2 [k3][k6][NA].\nSentence3 [k6][k9].\n\n(\nNLI\nP:Sentence1 H:[k2]} — P:Sentence2 H:[k2] ®)\nP:Sentence1 H:[k4]®) P:Sentence2 H:[k4] ®\nP:Sentence1 H:[k5]@ _—— P:Sentence2 H:[k5] ®)\n\nAbsent\nKnowledge Set:\n[k2] [k4] [k5]\n\n[NA] Precision [NA] Recall\ncorrect=1, all=2 hit=1, all=3\nwa precision = 1/2 = 0.5 recall = 1/3 = 0.33 -\n\n", "vlm_text": "The image depicts a method for evaluating precision and recall specifically for the concept of \"conscious incompetence\" as indicated by the tag [NA]. \n\n- **Model Output**: It shows two sentences with various knowledge elements marked by k1, k3, k6, etc., and tagged with [NA] for those marked as \"conscious incompetence.\"\n  - Sentence1 has k1 tagged with [NA].\n  - Sentence2 has k3 and k6 tagged with [NA].\n  - Sentence3, absent of [NA], has elements k6 and k9.\n\n- **Absent Knowledge Set**: It includes knowledge elements k2, k4, and k5 that are associated with the absence of knowledge.\n\n- **NLI (Natural Language Inference) Evaluation**:\n  - **[NA] Precision**: Evaluates the accuracy of [NA] tagging in sentences when compared to the absent knowledge set.\n    - Sentence1's correct identification of absent knowledge (k2) achieves 1 out of 2 attempted, resulting in a precision of 0.5.\n  - **[NA] Recall**: Measures how effectively the model identified all items in the absent knowledge set as [NA].\n    - Sentence2's hit of k2 against an all of 3 absence marks results in a recall of 0.33.\n\nThe image uses precise color-coding for clarity: green check marks for correct matches, red crosses for incorrect ones, and arrows to represent the calculation of precision and recall."}
{"layout": 54, "type": "text", "text": "knowledge triplet from minimum knowledge set of the question. (See Figure  2 .) ", "page_idx": 4, "bbox": [305, 275.6190185546875, 526, 302.3134460449219], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 55, "type": "text", "text": "Recall We calculate citation recall for each knowledge (0 or 1) in minimum knowledge set, and average over all knowledge to get micro recall. Recall  $=1$   if and only if the knowledge if hit by a correct citation. (See Figure  2 .) ", "page_idx": 4, "bbox": [305, 311.2942810058594, 526, 379.0284729003906], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 56, "type": "text", "text": "We average over all citations/knowledge in an an- swer, and average all answer-level precision/recall to get macro precision and recall. we calculate micro and macro F1-Score from corresponding pre- cision and recall. ", "page_idx": 4, "bbox": [305, 379.89599609375, 526, 447.23846435546875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 57, "type": "text", "text": "4.3 Text-Citation Alignment ", "text_level": 1, "page_idx": 4, "bbox": [305, 458, 445, 470], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 58, "type": "text", "text": "Other than the text quality and citation quality, we measure whether the generated citations provide support for the corresponding sentences. A piece of useful knowledge is not an ideal citation if it is irrelevant to the sentence it links to. Therefore, we propose the metric “Alignment” which deter- mines whether the generated citations are aligned to the sentences to which they belong. We use a state-of-the-art natural language inference (NLI) model TRUE ( Honovich et al. ,  2022 ), which is a fine-tuned T5-11B ( Raffel et al. ,  2020 ) model, to check whether the generated sentence entails the generated citation. Since one sentence could have multiple citations, we run NLI on all sentence- citation pairs and report the percentage of entail- ment. Additionally, we conduct human evaluation in  $\\S\\ 5.4$   to showcase if the automatic evaluation is correlated with human judgments. ", "page_idx": 4, "bbox": [305, 476.4809875488281, 526, 719.9624633789062], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 59, "type": "text", "text": "4.4 Conscious Incompetence Evaluation ", "text_level": 1, "page_idx": 4, "bbox": [306, 731, 500, 743], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 60, "type": "text", "text": "Theoretically, each [NA] mark should map to a piece of knowledge absent from the retrieved knowledge graph. However, it is difficult to identify if sentence requires any absent knowledge since there is no ground truth. Therefore, we conduct a three-round experiment to manually create ground truth for absent knowledge. In round 1, we select one knowledge from the minimum knowledge set, and remove it from the ground-truth knowledge graph. We let the LLMs attribute to this incomplete knowledge graph to generate answers, whereby the removed knowledge forms the “absent knowledge ground truth”. In subsequent rounds, we each re- move one additional knowledge from the minimum knowledge set, simulating a knowledge graph with more serious coverage problem. ", "page_idx": 4, "bbox": [305, 749.2050170898438, 526, 775.8994140625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 61, "type": "text", "text": "", "page_idx": 5, "bbox": [70, 71.74500274658203, 290, 261.0304870605469], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 62, "type": "text", "text": "We employ the NLI model TRUE ( Honovich et al. ,  2022 ) to measure the alignment between sentences and knowledge. A sentence with [NA] should be aligned to an absent knowledge. We calculate precision and recall for [NA]. ", "page_idx": 5, "bbox": [70, 261.843994140625, 290, 329.1864929199219], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 63, "type": "text", "text": "[NA] precision We calculate [NA] precision for each sentence with [NA] (0 or 1) and average over all sentences with [NA]. Precision  $=1$   for a sen- tence if and only if it entails one knowledge triplet from absent knowledge set of the question. (See Figure  3 .) ", "page_idx": 5, "bbox": [70, 337.9832763671875, 290, 419.2674865722656], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 64, "type": "text", "text": "[NA] Recall We calculate [NA] recall for each knowledge (0 or 1) in absent knowledge set and average over all absent knowledge. Recall  $=1$   if and only if the knowledge if entailed by a sentence with [NA]. (See Figure  3 .) ", "page_idx": 5, "bbox": [70, 428.0632629394531, 290, 495.7984619140625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 65, "type": "text", "text": "4.5 Discussion on Evaluation metrics ", "text_level": 1, "page_idx": 5, "bbox": [70, 507, 251, 519], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 66, "type": "text", "text": "In this section, we discuss on the evaluation metrics of benchmark BioBaLMA. We design the evalua- tion metrics from multiple dimensions to incorpo- rate different understandings on what makes a high quality citation. ", "page_idx": 5, "bbox": [70, 524.7510375976562, 290, 592.0934448242188], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 67, "type": "text", "text": "•  One understanding argues when the answer contains mistakes, even if the citation is cor- rectly answering the questions, it cannot rep- resent good LLM attribution performance. In this case, citation quality is considered as a measure of overall attribution performance, including the answer quality. ", "page_idx": 5, "bbox": [81, 603.10498046875, 290, 697.54541015625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 68, "type": "text", "text": "•  The other understanding argues for a complete decoupling of answer and citation quality. In this scenario, even if the answer is wrong, the citation is valuable as long as it provides reasonable support for the question. In such Both understandings are plausible, and hence we have considered both of them when we design met- rics. The alignment score is designed based on the first understanding, which measures whether the citations are closely linked to the answer. The precision and recall are designed for the second understanding, where the citations are completely decoupled from the answer, and are correct if they provide support for the question. ", "page_idx": 5, "bbox": [81, 708.5579833984375, 290, 775.8994140625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 69, "type": "text", "text": "", "page_idx": 5, "bbox": [305, 107.83698272705078, 526, 229.37649536132812], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 70, "type": "text", "text": "In addition, we also incorporate an edge case for design of the [NA] precision calculation. If an NA-marked sentence does not answer the question at all, it is considered correct in the [NA] precision calculation. In this case, the LLM correctly iden- tifies a sentence that requires further verification. ", "page_idx": 5, "bbox": [305, 229.78700256347656, 526, 310.6784973144531], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 71, "type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 5, "bbox": [305, 334, 390, 348], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 72, "type": "text", "text": "We run through the method pipeline described in  $\\S~3$   on different LLMs and present the results in this section. Since we aim to obtain a more accu- rate evaluation, we conduct our main experiments on the specific questions setting, since the mini- mum knowledge set has a higher relevance on the specific questions. However, we will also provide evaluation results for the general questions in   $\\S\\ 5.5$  as ablation studies. The implementation details are reported in appendix  B . We report five model baselines from both open and closed source model families: ", "page_idx": 5, "bbox": [305, 355.8009948730469, 526, 517.9884033203125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 73, "type": "text", "text": "OpenAI Models We use GPT4 (gpt-4-0314) and ChatGPT (gpt-3.5-turbo-0301) for our experiments. For ChatGPT, we experiment on temperature of 0.1, 0.5, and 0.9 to obtain different levels of randomness and creativity in generation. ", "page_idx": 5, "bbox": [305, 525.3862915039062, 526, 593.1214599609375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 74, "type": "text", "text": "LLaMA We conduct experiments with LLaMA- 7B ( Touvron et al. ,  2023 ) and LLaMA-13B since they are powerful open-source models that are widely accessible. We have also conducted hu- man instruction tuned LLaMA models, includ- ing Alpaca-7B ( Taori et al. ,  2023 ) and Vicuna- 13B ( Chiang et al. ,  2023 ). ", "page_idx": 5, "bbox": [305, 600.519287109375, 526, 695.3524169921875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 75, "type": "text", "text": "5.1 Main Results ", "text_level": 1, "page_idx": 5, "bbox": [306, 705, 393, 717], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 76, "type": "text", "text": "Citation Quality Evaluation We present the main results in Table  3 . For correctness, we report on a micro scale. For precision, recall, and F1- Score, we report on both micro and macro scales. ", "page_idx": 5, "bbox": [305, 721.7142944335938, 526, 775.8994140625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 77, "type": "table", "page_idx": 6, "img_path": "layout_images/2310.05634v2_4.jpg", "table_footnote": "Table 3: Citation Quality OpenAI models and LLaMA family models. The first five metrics are reported in Micro, and the last three metrics are reported in Macro. We also report text citation alignment. ", "bbox": [69, 69, 527, 233], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "Micro Macro\nModel Align. Corr. Prec. Rec. F1. Prec. Rec. F1.\nGPT-4(0.5) | 92.015) | 97.6(01) 36.0(06) 43.610) 39.4 | 40.701) 43.9; 423\nChatGPT (0.1) | 85.9(2.5) | 96.1(0.4) 29.0.9) 50.8(,3) 36.9 | 32.7(9.4) 51.2(.3) 39.9\nChatGPT (0.5) | 84.5¢.1) | 94-802) 29.9(0.2) 49.008) 37.2 | 34.1(95) 49.4(0.9) 40.4\nChatGPT (0.9) | 84.105) | 94.204) 28-7(0.2) 49.0.3) 36.2 | 32.5(0.2) 49.4(0.3) 39.2\nLLaMA-7B_— | 47.898) | 70.202) 7.7.4) 41-107) 13.0] 11.049) 4140.7) 174\nLLaMA-13B | 62.1(¢0.4) | 71.7.9) 10.5.3) 43.7110) 16.9 | 13.802) 43.5.0) 20.9\n\n", "vlm_text": "The table presents a comparison of different models, including GPT-4, ChatGPT, Alpaca-7B, LLaMA-7B, LLaMA-13B, and Vicuna-13B. It compares metrics such as Alignment, Correctness (Corr.), Precision (Prec.), Recall (Rec.), and F1-score (F1.) under both \"Micro\" and \"Macro\" settings. Each metric is accompanied by a smaller value in parentheses, indicating some form of sub-measurement or statistical variation. \n\nHere are the columns explained:\n\n1. **Align.** - Alignment score of the models.\n2. **Corr.** - Correctness score.\n3. **Micro:** \n   - **Prec.** - Precision under micro averaging.\n   - **Rec.** - Recall under micro averaging.\n   - **F1.** - F1-score under micro averaging.\n4. **Macro:** \n   - **Prec.** - Precision under macro averaging.\n   - **Rec.** - Recall under macro averaging.\n   - **F1.** - F1-score under macro averaging.\n\nThe numbers beside model names, such as GPT-4 (0.5), likely refer to some version or parameter setting used in the evaluation."}
{"layout": 78, "type": "text", "text": "The experimental results are the mean of three runs, and the standard deviation is reported in brackets. ", "page_idx": 6, "bbox": [70, 253.84901428222656, 291, 280.5434875488281], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 79, "type": "text", "text": "In general, there is a room of improvement for all models since no model can achieve a micro F1 Score of higher than 40. The OpenAI models out- perform the LLaMA family models in almost all metrics. The correctness is above 94 for OpenAI models, but around 70 for LLaMA based models. For ChatGPT, temperature does not play a signifi- cant role since it effect on F1 Score is at most 1.2. The GPT-4 model achieves the best performance across almost all metrics, except for recall, since GPT-4 models tend to generate shorter answers with fewer citations, resulting in higher precision. While LLaMA is better at Recall by generating long answers with many citations. The F1-Score of models from the same family are close to one an- other, showing that our automatic evaluation metric designed is reliable. ", "page_idx": 6, "bbox": [70, 281.7550048828125, 291, 511.6884765625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 80, "type": "text", "text": "Text-Citation Alignment From Table  3 , similar to citation quality, the OpenAI models also out- perform the LLaMA based models on text-citation alignment. In addition, models with 7B, 13B, 175B (ChatGPT), and trillion level (GPT4) parameters have an alignment score of  $40+,60+,80+$  , and 92 respectively. LLaMA-13B model has an improve- ment of 14.3 compared to LLaMA-7B model. This shows that parameter size may play an important role in generating sentences and citations with good alignment. ", "page_idx": 6, "bbox": [70, 521.8623046875, 291, 670.8924560546875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 81, "type": "text", "text": "Text Quality Evaluation We present the evalu- ation of generated text quality in Table  4 . From the results, we find that OpenAI models, in general, have better text quality in all metrics compared to LLaMA family models, which corresponds to the citation evaluation results. All models exhibit rather high consistency, indicating that the LLMs ", "page_idx": 6, "bbox": [70, 681.0662841796875, 291, 775.8994140625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 82, "type": "table", "page_idx": 6, "img_path": "layout_images/2310.05634v2_5.jpg", "table_footnote": "Table 4: Evaluation on generated text quality. ", "bbox": [305, 251, 513, 388], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "Model Coh. Con. Flu. Rel.\nGPT-4 (0.5) 448 489 4.64 4.72\nChatGPT (0.1) | 4.57 4.94 469 4.83\nChatGPT (0.5) | 4.57 4.94 4.71 4.81\nChatGPT (0.9) | 4.52 4.91 467 4.79\nAlpaca-7B 410 446 4.23 3.76\nLLaMa-7B 3.06 3.79 3.62 2.96\nLLaMa-13B 3.60 4.23 3.94 3.56\nVicuna-13B 3.67 4.50 3.96 3.64\n\n", "vlm_text": "The table presents a comparison of different language models across four evaluation metrics: Coherence (Coh.), Consistency (Con.), Fluency (Flu.), and Relevance (Rel.). The models compared include various versions of GPT-4 and ChatGPT, as well as Alpaca-7B, LLaMa-7B, LLaMa-13B, and Vicuna-13B. The numerical values in the table represent the scores for each metric, likely on a scale from 1 to 5, with higher scores indicating better performance. Here's a breakdown of the scores for each model:\n\n1. **GPT-4 (0.5)**\n   - Coh.: 4.48\n   - Con.: 4.89\n   - Flu.: 4.64\n   - Rel.: 4.72\n\n2. **ChatGPT (0.1)**\n   - Coh.: 4.57\n   - Con.: 4.94\n   - Flu.: 4.69\n   - Rel.: 4.83\n\n3. **ChatGPT (0.5)**\n   - Coh.: 4.57\n   - Con.: 4.94\n   - Flu.: 4.71\n   - Rel.: 4.81\n\n4. **ChatGPT (0.9)**\n   - Coh.: 4.52\n   - Con.: 4.91\n   - Flu.: 4.67\n   - Rel.: 4.79\n\n5. **Alpaca-7B**\n   - Coh.: 4.10\n   - Con.: 4.46\n   - Flu.: 4.23\n   - Rel.: 3.76\n\n6. **LLaMa-7B**\n   - Coh.: 3.06\n   - Con.: 3.79\n   - Flu.: 3.62\n   - Rel.: 2.96\n\n7. **LLaMa-13B**\n   - Coh.: 3.60\n   - Con.: 4.23\n   - Flu.: 3.94\n   - Rel.: 3.56\n\n8. **Vicuna-13B**\n   - Coh.: 3.67\n   - Con.: 4.50\n   - Flu.: 3.96\n   - Rel.: 3.64\n\nOverall, the ChatGPT variants, particularly ChatGPT (0.1) and ChatGPT (0.5), tend to have higher scores across all metrics compared to the other models."}
{"layout": 83, "type": "table", "page_idx": 6, "img_path": "layout_images/2310.05634v2_6.jpg", "table_footnote": "Table 5: Citation quality evaluation for generated texts using a KG with N pieces of knowledge removed. ", "bbox": [304, 399, 526, 494], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "Removed | Corr. Prec. Rec. FI.\n0 (gold) 95.55 30.1 57.1 39.4\n1 94.1 26.1 42.5 32.3\n2 940 21.0 314 25.2\n3 93.9 163 204 18.1\n\n", "vlm_text": "The table displays the following data across five columns: \"Removed\", \"Corr.\", \"Prec.\", \"Rec.\", and \"F1.\"\n\n- The row labeled \"0 (gold)\" has values 95.5 (Corr.), 30.1 (Prec.), 57.1 (Rec.), and 39.4 (F1.).\n- The row labeled \"1\" has values 94.1 (Corr.), 26.1 (Prec.), 42.5 (Rec.), and 32.3 (F1.).\n- The row labeled \"2\" has values 94.0 (Corr.), 21.0 (Prec.), 31.4 (Rec.), and 25.2 (F1.).\n- The row labeled \"3\" has values 93.9 (Corr.), 16.3 (Prec.), 20.4 (Rec.), and 18.1 (F1.).\n\nThese columns likely represent metrics used to evaluate the performance of a model or process."}
{"layout": 84, "type": "text", "text": "are capable of generating answers that are not contradictory to the provided knowledge or self- contradictory. However, the relevance is relatively low for smaller models, indicating the difficulty these models face in generating answers that are relevant to the questions. ", "page_idx": 6, "bbox": [305, 516.208984375, 526, 597.1004638671875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 85, "type": "text", "text": "5.2 Conscious Incompetence ", "text_level": 1, "page_idx": 6, "bbox": [305, 608, 447, 621], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 86, "type": "text", "text": "We first evaluate  citation quality  of the generated text with knowledge removed using method de- scribed in   $\\S~4.4$  . From Table  5 , the removal of required knowledge has a minimal impact on cor- rectness, but significantly affects citation precision and recall. With more knowledge absent from pro- vided knowledge graph, both precision and recall drops drastically, demonstrating that the coverage issue poses a considerable challenge to generating answers with high quality citations. ", "page_idx": 6, "bbox": [305, 626.3353271484375, 526, 761.8164672851562], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 87, "type": "text", "text": "Next, we evaluate  [NA] precision  and  recall . ", "page_idx": 6, "bbox": [317.0509948730469, 762.3612670898438, 526, 776.5540161132812], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 88, "type": "image", "page_idx": 7, "img_path": "layout_images/2310.05634v2_7.jpg", "img_caption": "Figure 4: Precision, Recall, and F1-Score for [NA]. ", "bbox": [76, 81, 283, 245], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "core:\n\n24\n\n22\n\n20\n\n18\n\n16\n\n14\n\nExperiment Result on Conscious Incompetence\n\n—® precision\n—— recall\n—# F1_Score\n\none two three\nnumber of knowledge removed\n\n", "vlm_text": "The image is a line graph titled \"Experiment Result on Conscious Incompetence.” It shows the relationship between the number of knowledge elements removed and three metrics: precision, recall, and F1-Score.\n\n- **Precision** (blue line with circle markers) increases significantly as more knowledge is removed, starting at around 14 and reaching about 26.\n- **Recall** (orange line with diamond markers) remains relatively stable, starting around 14 and ending slightly below 15.\n- **F1-Score** (green line with triangle markers) shows a moderate increase, starting around 14 and ending around 18.\n\nThe x-axis represents the number of knowledge elements removed (labeled as \"one,\" \"two,\" and \"three\"), and the y-axis represents the score values for each metric."}
{"layout": 89, "type": "text", "text": "From Figure  4 , The recall is stable at about 15 regardless of the number of absent knowledge. This indicates that the current LLMs have ability to identify absent knowledge to a limited extent. While precision and F1-Score exhibit a clear up- ward trend, which shows that with more absent knowledge in KG, [NA] enables generated out- puts to locate absent knowledge more accurately. Therefore, the “Conscious Incompetence” setting plays an increasingly crucial role when the cover- age problem of knowledge graph is more serious. ", "page_idx": 7, "bbox": [70, 269.32598876953125, 291, 417.9634704589844], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 90, "type": "text", "text": "5.3 Retrieval Analysis ", "text_level": 1, "page_idx": 7, "bbox": [70, 431, 181, 443], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 91, "type": "text", "text": "We conduct an ablation study to examine the impact of retrieval accuracy on the model’s output. The ex- periment simulates retrieval accuracy from 100 to 20 at intervals of 20. We start with the ground truth knowledge graphs that we used for question con- struction. In each subsequent rounds, we randomly replace additional   $20\\%$   knowledge graphs with ir- relevant knowledge graphs to simulate retrieving wrong graphs. The results for citation quality are in Figure  5 . Answers are generated using ChatGPT with a temperature of 0.5. ", "page_idx": 7, "bbox": [70, 450.1239929199219, 291, 598.761474609375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 92, "type": "text", "text": "The results show clear downward trends in all metrics as expected when retrieval accuracy dropped. Among precision and recall, the impact of poor retrieval quality on recall (green) is much more significant than on precision (yellow). This indicates that the model has the ability to filter out incorrect knowledge to a certain extent, resulting in less noticeable impact on precision compared to recall. The reduction in recall was nearly linear as retrieval accuracy decreased, which is under- standable since a knowledge cannot be cited if it is not provided. The greatest drop in recall occurred between the ground truth (57.1) and 80 accuracy ", "page_idx": 7, "bbox": [70, 600.1640014648438, 291, 775.8994140625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 93, "type": "image", "page_idx": 7, "img_path": "layout_images/2310.05634v2_8.jpg", "img_caption": "Figure 5: Citation evaluation (Micro) of generated texts using knowledge graphs with retrieval accuracy 100 (gold), 80, 60,40, and 20. ", "bbox": [304, 82, 526, 269], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "Score\n\n80\n\n60\n\n40\n\n20\n\nRetrieval Analysis\n\nprecision\nrecall\n\nF1 Score\ncorrectness\n\ngold\n\n80\n\n60\nRia itary Roars\n\n40\n\n20\n\n", "vlm_text": "The image is a line graph titled \"Retrieval Analysis,\" showing the evaluation of generated texts using knowledge graphs. The x-axis represents \"Retrieval Accuracy\" with values at gold (100), 80, 60, 40, and 20. The y-axis represents the \"Score.\"\n\nThere are four lines indicating different metrics:\n- **Precision** (blue circles), which decreases as retrieval accuracy decreases.\n- **Recall** (orange curve), also decreasing.\n- **F1 Score** (green inverted triangles), following a similar downward trend.\n- **Correctness** (red triangles), which starts high and slightly decreases.\n\nCorrectness has the highest score across all levels of retrieval accuracy, while precision shows the lowest."}
{"layout": 94, "type": "table", "page_idx": 7, "img_path": "layout_images/2310.05634v2_9.jpg", "table_footnote": "Table 6: Result of Human Evaluation on text-citation alignment ", "bbox": [305, 280, 525, 361], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "Alignment Human Avg.\n\nChatGPT(0.5) 84.5 82.0\nLLaMA-7B 47.8 45.5\nVicuna-13B 66.9 64.5\n\n", "vlm_text": "The table is comparing the \"Alignment\" and \"Human Avg.\" scores of three models: ChatGPT(0.5), LLaMA-7B, and Vicuna-13B. \n\n- ChatGPT(0.5) has an Alignment score of 84.5 and a Human Avg. score of 82.0.\n- LLaMA-7B has an Alignment score of 47.8 and a Human Avg. score of 45.5.\n- Vicuna-13B has an Alignment score of 66.9 and a Human Avg. score of 64.5. \n\nThese scores may represent evaluations of model performance on alignment and human-average ratings, but the specific context or criteria for these evaluations are not provided in the table."}
{"layout": 95, "type": "text", "text": "(42.5), demonstrating the potential of the model to generate high-quality citations under perfect re- trieval conditions. In practice, a retrieval accuracy of 80 is closest to the actual scenario of our exper- iment (our retrieval accuracy is 75.9). Therefore, when retrieval accuracy is reasonably high, the cor- rectness of citations is not the most significant con- cern compared to recall. ", "page_idx": 7, "bbox": [305, 383.1340026855469, 526, 491.1244812011719], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 96, "type": "text", "text": "5.4 Human Evaluation ", "text_level": 1, "page_idx": 7, "bbox": [306, 501, 420, 513], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 97, "type": "text", "text": "We conduct human evaluation to verify the correla- tion between automatic evaluation and human judg- ment. We randomly sample 100 sentence-citation pairs from each of the three baselines: ChatGPT (temperature 0.5), LLaMA-7B, and Vicuna-13B. We request two proficient English annotators for each baseline to determine if the citation aligns to the sentence and provides support for it. The reason we choose metric alignment here is in appendix  C , with instruction to annotators and IAA. ", "page_idx": 7, "bbox": [305, 518.708984375, 526, 653.7974243164062], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 98, "type": "text", "text": "The comparison between automatically calcu- lated Alignment and human evaluation results is shown in Table  6 . For all three baselines, the auto- matic and human scores are close with a gap within 2.5, despite the significant differences among the baselines. This indicates a strong correlation be- tween the automatically calculated alignment and human judgments. The experiment results demon- strate that the automatic evaluation serves as a reli- ", "page_idx": 7, "bbox": [305, 654.3610229492188, 526, 775.8994140625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 99, "type": "table", "page_idx": 8, "img_path": "layout_images/2310.05634v2_10.jpg", "table_footnote": "Table 7: Comparison of evaluation results on General and Specific question setting ", "bbox": [79, 69, 510, 166], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "Citation Eval. Text Eval.\nSetting | Model Align. | Corr. Prec. Rec. Fl. | Coh. Con. Flu. Rel.\nGeneral | GPT-4 (0.5) 90.9 | 976 30.8 42.1 35.6] 4.38 4.77 448 4.48\nChatGPT (0.5) | 82.7 | 94.5 25.2 474 32.9| 464 489 445 4.70\nSpecific | GPT-4 (0.5) 92.0 | 976 36.0 43.6 39.4) 448 489 464 4.72\nChatGPT (0.5) | 84.5 | 94.8 29.9 49.0 37.2 | 457 494 4.71 4.81\n\n", "vlm_text": "The table presents a comparison of performance metrics between two models, GPT-4 (0.5) and ChatGPT (0.5), under two different \"Settings\": General and Specific. The metrics are divided into two main categories: \"Citation Eval.\" and \"Text Eval.\"\n\n1. **Citation Eval.:** This section evaluates the models based on Alignment (Align.), Correctness (Corr.), Precision (Prec.), Recall (Rec.), and F1 Score (F1.). \n   - In the General setting:\n     - GPT-4 (0.5) scores 90.9 (Align.), 97.6 (Corr.), 30.8 (Prec.), 42.1 (Rec.), and 35.6 (F1.).\n     - ChatGPT (0.5) scores 82.7 (Align.), 94.5 (Corr.), 25.2 (Prec.), 47.4 (Rec.), and 32.9 (F1.).\n   - In the Specific setting:\n     - GPT-4 (0.5) scores 92.0 (Align.), 97.6 (Corr.), 36.0 (Prec.), 43.6 (Rec.), and 39.4 (F1.).\n     - ChatGPT (0.5) scores 84.5 (Align.), 94.8 (Corr.), 29.9 (Prec.), 49.0 (Rec.), and 37.2 (F1.).\n\n2. **Text Eval.:** This section assesses the models based on Coherence (Coh.), Conciseness (Con.), Fluency (Flu.), and Relevance (Rel.).\n   - In the General setting:\n     - GPT-4 (0.5) scores 4.38 (Coh.), 4.77 (Con.), 4.48 (Flu.), and 4.48 (Rel.).\n     - ChatGPT (0.5) scores 4.64 (Coh.), 4.89 (Con.), 4.45 (Flu.), and 4.70 (Rel.).\n   - In the Specific setting:\n     - GPT-4 (0.5) scores 4.48 (Coh.), 4.89 (Con.), 4.64 (Flu.), and 4.72 (Rel.).\n     - ChatGPT (0.5) scores 4.57 (Coh.), 4.94 (Con.), 4.71 (Flu.), and 4.81 (Rel.).\n\nOverall, the table indicates quantitative assessments of the two models across several performance metrics in different evaluation contexts (General vs. Specific). It seems that the \"Specific\" setting generally yields better performance on several evaluation metrics compared to the \"General\" setting, and ChatGPT tends to perform better in Text Evaluation, particularly in Conciseness, Fluency, and Relevance."}
{"layout": 100, "type": "text", "text": "able measurement of the alignment between gener- ated texts and citations. ", "page_idx": 8, "bbox": [70, 186.51502990722656, 290.9491882324219, 213.20950317382812], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 101, "type": "text", "text": "5.5 General and Specific Questions ", "text_level": 1, "page_idx": 8, "bbox": [69, 224, 242, 237], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 102, "type": "text", "text": "We compare experiments results of text, citation (micro), and alignment between the general and specific questions in Table  7 . The results show that the same model’s answers on specific questions outperform those on general questions in almost all metrics. The finding is not surprising because the specific questions provide clearer instructions to the models on which knowledge to use. In addition, the general questions in the dataset are inherently loosely bonded to the minimum knowledge set, and hence have impacts on the evaluation results. This experiment shows a trade-off between how explic- itly the question context mentions the knowledge, and how ir replace ably the knowledge is required by the question. The specific questions target the knowledge more explicitly in the question context, and hence cover the scope of the paragraph better. It stands for an upper bound for knowledge cover- age and a lower bound for question naturalness.The general questions implicitly target the knowledge in the question context, and there loosely cover the scope of the paragraph. It stands for an upper bound for question naturalness and a lower bound for knowledge coverage. ", "page_idx": 8, "bbox": [70, 241.8050079345703, 291, 566.5814208984375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 103, "type": "text", "text": "6 Related Work ", "text_level": 1, "page_idx": 8, "bbox": [69, 578, 161, 591], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 104, "type": "text", "text": "Retrieval-augmented LLMs KiC ( Pan et al. , 2022 ) empower models with external memory of multiple formats including knowledge graph but does not explore attribution. WebGPT ( Nakano et al. ,  2021 ) outsources document retrieval to Mi- crosoft Bing and fine-tunes GPT3 to answer ques- tions. GopherCite ( Menick et al. ,  2022 ) fine-tunes a Gopher ( Rae et al. ,  2021 ) model to generate text alongside quotes extracted from Google search. ALCE ( Gao et al. ,  2023 ) retrieves top-k passages from Wikipedia and asks LLMs to generate outputs with citations to corresponding supporting docu- ments. These works attribute LLMs to unstructured Evaluation ( Rashkin et al. ,  2021 ) define the “At- tributable to Identified Sources” (AIS) to mea- sure whether model-generated statements are sup- ported by underlying sources. ( Bohnet et al. ,  2022 ) study an automatic metric (AutoAIS) that formu- lates evaluation of automated question answer- ing as a NLI task. ( Yue et al. ,  2023 ) investigate the automatic evaluation of attribution by prompt- ing LLMs and fine-tuning smaller LMs. ( Liu et al. ,  2023a ) conduct human evaluation to audit generative search engines for their citation quali- ties. ALCE ( Gao et al. ,  2023 ) evaluates generated answers by comparing with gold answers using MAUVE, and calculates precision and recall for citations using NLI. To the best of our knowledge, our evaluation methods are the first framework that requires no human annotated data. ", "page_idx": 8, "bbox": [70, 599.7713012695312, 291, 775.8994140625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 105, "type": "text", "text": "", "page_idx": 8, "bbox": [304, 214.65025329589844, 526, 444.9754638671875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 106, "type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 8, "bbox": [305, 462, 383, 475], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 107, "type": "text", "text": "We propose KaLMA that comprises a new dataset BioKaLMA, a pipeline for generating attributed answers by retrieving from KGs, and a set of au- tomatic evaluation metrics to assess text quality, citation quality, and text-citation alignment. We introduce the “Conscious Incompetence” setting, enabling LLMs to identify the knowledge required to support the answers but is absent from the KG. Through this benchmark, we address three chal- lenges: incorporating diverse attribution sources, limited attribution source coverage, and the ab- sence of human annotated ground truth for auto- matic evaluation. ", "page_idx": 8, "bbox": [304, 489.5719909667969, 526, 665.3074340820312], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 108, "type": "text", "text": "Our extensive experimental results demonstrate that current LLMs still have room for improve- ment when utilizing KGs as attribution sources. We also highlight the increasing effectiveness of “Conscious Incompetence” setting as the coverage of attribution source becomes worse. Lastly, we prove the crucial role of retrieval accuracy in gen- erating high-quality attributed texts. ", "page_idx": 8, "bbox": [304, 667.9099731445312, 526, 775.8994140625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 109, "type": "text", "text": "Limitations ", "text_level": 1, "page_idx": 9, "bbox": [70, 72, 130, 83], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 110, "type": "text", "text": "One limitation is that our work only investigates a simple form of knowledge graph, where each node is an entity, and each sub-graph is a knowledge triple. There are more complicated forms of knowl- edge graph, where each node is a document. We will explore this setting in future works. ", "page_idx": 9, "bbox": [70, 92.89698028564453, 291, 173.78848266601562], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 111, "type": "text", "text": "Another limitation lies within the text quality evaluation. We uses ChatGPT as the model to eval- uate texts, which could potentially have a bias if the model prefers the text style generated by itself. Such bias can be observed from the abnormal phe- nomenon that the scores of ChatGPT generated answers are higher than that of the GPT4 gener- ated answers for all four dimensions. Due to cost considerations, we do not repeat the text quality evaluation with GPT-4. ", "page_idx": 9, "bbox": [70, 174.2880401611328, 291, 309.3764953613281], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 112, "type": "text", "text": "Ethical Considerations ", "text_level": 1, "page_idx": 9, "bbox": [70, 321, 188, 333], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 113, "type": "text", "text": "The potential risk is when users leverage the au- tomatic dataset construction pipeline to generate massive hazardous datasets. This can only happen when a structured knowledge of harmful content is available. Otherwise there is no risk as long as the benchmark is used correctly. All data are collected from WikiData which is publicly available. Hence there is no privacy issue. We also conduct human check to ensure there is no offensive content. ", "page_idx": 9, "bbox": [70, 341.48199462890625, 291, 463.021484375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 114, "type": "text", "text": "References ", "text_level": 1, "page_idx": 9, "bbox": [70, 486, 127, 498], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 115, "type": "text", "text": "Bernd Bohnet, Vinh Q. Tran, Pat Verga, Roee Aha- roni, Daniel Andor, Livio Baldini Soares, Massimil- iano Ciaramita, Jacob Eisenstein, Kuzman Ganchev, Jonathan Herzig, Kai Hui, Tom Kwiatkowski, Ji Ma, Jianmo Ni, Lierni Sestorain Saralegui, Tal Schus- ter, William W. Cohen, Michael Collins, Dipanjan Das, Donald Metzler, Slav Petrov, and Kellie Webster. 2023.  Attributed question answering: Evaluation and modeling for attributed large language models . ", "page_idx": 9, "bbox": [70, 505.43658447265625, 291, 605.1124877929688], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 116, "type": "text", "text": "Bernd Bohnet, Vinh Q Tran, Pat Verga, Roee Aharoni, Daniel Andor, Livio Baldini Soares, Jacob Eisenstein, Kuzman Ganchev, Jonathan Herzig, Kai Hui, et al. 2022.  Attributed question answering: Evaluation and modeling for attributed large language models .  arXiv preprint arXiv:2212.08037 . ", "page_idx": 9, "bbox": [70, 613.4185791015625, 291, 680.218505859375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 117, "type": "text", "text": "Samuel Bowman, Gabor Angeli, Christopher Potts, and Christopher D Manning. 2015. A large annotated corpus for learning natural language inference. ", "page_idx": 9, "bbox": [70, 688.5235595703125, 291, 722.446533203125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 118, "type": "text", "text": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neel a kant an, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot ", "page_idx": 9, "bbox": [70, 730.7525634765625, 291, 775.6334838867188], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 119, "type": "text", "text": "learners.  Advances in neural information processing systems , 33:1877–1901. ", "page_idx": 9, "bbox": [316, 72.61956787109375, 526, 95.58348846435547], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 120, "type": "text", "text": "Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. 2023.  Vicuna: An open- source chatbot impressing gpt-4 with   $90\\%^{*}$   chatgpt quality . ", "page_idx": 9, "bbox": [306, 104.5545654296875, 526, 171.35447692871094], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 121, "type": "text", "text": "Elizabeth Clark, Tal August, Sofia Serrano, Nikita Haduong, Suchin Gururangan, and Noah A. Smith. 2021.  All that’s ‘human’ is not gold: Evaluating human evaluation of generated text . In  Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers) , pages 7282–7296, Online. Association for Computational Linguistics. ", "page_idx": 9, "bbox": [306, 180.3255615234375, 526, 280.00152587890625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 122, "type": "text", "text": "Paul R Curtiss and Phillip W Warren. 1974. The dy- namics of life skills coaching. life skills series. ", "page_idx": 9, "bbox": [306, 288.97259521484375, 526, 311.9365234375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 123, "type": "text", "text": "Ido Dagan, Oren Glickman, and Bernardo Magnini. 2005. The pascal recognising textual entailment chal- lenge. In  Machine learning challenges workshop , pages 177–190. Springer. ", "page_idx": 9, "bbox": [306, 320.90655517578125, 526, 365.78851318359375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 124, "type": "text", "text": "Tianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen. 2023. Enabling large language models to generate text with citations. ", "page_idx": 9, "bbox": [306, 374.75958251953125, 526, 408.6825256347656], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 125, "type": "text", "text": "Or Honovich, Roee Aharoni, Jonathan Herzig, Ha- gai Taitelbaum, Doron Kukliansy, Vered Cohen, Thomas Scialom, Idan Szpektor, Avinatan Has- sidim, and Yossi Matias. 2022. True: Re-evaluating factual consistency evaluation. arXiv preprint arXiv:2204.04991 . ", "page_idx": 9, "bbox": [306, 417.653564453125, 526, 484.4525146484375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 126, "type": "text", "text": "Gautier Izacard and Edouard Grave. 2021.  Leveraging passage retrieval with generative models for open do- main question answering . In  Proceedings of the 16th Conference of the European Chapter of the Associ- ation for Computational Linguistics: Main Volume , pages 874–880, Online. Association for Computa- tional Linguistics. ", "page_idx": 9, "bbox": [306, 493.423583984375, 526, 571.1825561523438], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 127, "type": "text", "text": "Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. 2023. Survey of halluci- nation in natural language generation.  ACM Comput- ing Surveys , 55(12):1–38. ", "page_idx": 9, "bbox": [306, 580.153564453125, 526, 635.9935302734375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 128, "type": "text", "text": "Tushar Khot, Ashish Sabharwal, and Peter Clark. 2018. Scitail: A textual entailment dataset from science question answering . volume 32. ", "page_idx": 9, "bbox": [306, 644.964599609375, 526, 678.8875122070312], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 129, "type": "text", "text": "Haejun Lee, Akhil Kedia, Jongwon Lee, Ashwin Paran- jape, Christopher D. Manning, and Kyoung-Gu Woo. 2022.  You only need one model for open-domain question answering . ", "page_idx": 9, "bbox": [306, 687.8585815429688, 526, 732.7405395507812], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 130, "type": "text", "text": "Nelson F Liu, Tianyi Zhang, and Percy Liang. 2023a. Evaluating verifiability in generative search engines. arXiv preprint arXiv:2304.09848 . ", "page_idx": 9, "bbox": [306, 741.7105712890625, 526, 775.6334838867188], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 131, "type": "text", "text": "Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. 2023b.  G-eval: Nlg evaluation using gpt-4 with better human align- ment . ", "page_idx": 10, "bbox": [70, 72.61956787109375, 290, 117.50151824951172], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 132, "type": "text", "text": "Jacob Menick, Maja Trebacz, Vladimir Mikulik, John Aslanides, Francis Song, Martin Chadwick, Mia Glaese, Susannah Young, Lucy Campbell- Gillingham, Geoffrey Irving, et al. 2022. Teaching language models to support answers with verified quotes.  arXiv preprint arXiv:2203.11147 . ", "page_idx": 10, "bbox": [70, 125.2825927734375, 290, 192.08152770996094], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 133, "type": "text", "text": "Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. 2021. Webgpt: Browser-assisted question- answering with human feedback. arXiv preprint arXiv:2112.09332 . ", "page_idx": 10, "bbox": [70, 199.862548828125, 290, 266.66253662109375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 134, "type": "text", "text": "Xiaoman Pan, Wenlin Yao, Hongming Zhang, Dian Yu, Dong Yu, and Jianshu Chen. 2022. Knowledge-in- context: Towards knowledgeable semi-parametric language models.  arXiv preprint arXiv:2210.16433 . ", "page_idx": 10, "bbox": [70, 274.44354248046875, 290, 319.32452392578125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 135, "type": "text", "text": "Jorge Pérez, Marcelo Arenas, and Claudio Gutierrez. 2009. Semantics and complexity of sparql.  ACM Transactions on Database Systems (TODS) , 34(3):1– 45. ", "page_idx": 10, "bbox": [70, 327.1055603027344, 290, 371.9875183105469], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 136, "type": "text", "text": "Krishna Pillutla, Swabha S way am dip ta, Rowan Zellers, John Thickstun, Sean Welleck, Yejin Choi, and Zaid Harchaoui. 2021. Mauve: Measuring the gap be- tween neural text and human text using divergence frontiers.  Advances in Neural Information Process- ing Systems , 34:4816–4828. ", "page_idx": 10, "bbox": [70, 379.7685852050781, 290, 446.5675048828125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 137, "type": "text", "text": "Alistair Plum, Tharindu Ranasinghe, Spencer Jones, Constantin Orasan, and Ruslan Mitkov. 2022. Bio- graphical semi-supervised relation extraction dataset. In  Proceedings of the 45th International ACM SI- GIR Conference on Research and Development in Information Retrieval , pages 3121–3130. ", "page_idx": 10, "bbox": [70, 454.34857177734375, 290, 521.1484985351562], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 138, "type": "text", "text": "Hongjin Qian, Zhicheng Dou, Jiejun Tan, Haonan Chen, Haoqi Gu, Ruofei Lai, Xinyu Zhang, Zhao Cao, and Ji-Rong Wen. 2023.  Optimizing factual accuracy in text generation through dynamic knowledge selec- tion . ", "page_idx": 10, "bbox": [70, 528.9285888671875, 290, 584.7694702148438], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 139, "type": "text", "text": "Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susan- nah Young, et al. 2021. Scaling language models: Methods, analysis & insights from training gopher. arXiv preprint arXiv:2112.11446 . ", "page_idx": 10, "bbox": [70, 592.550537109375, 290, 659.3495483398438], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 140, "type": "text", "text": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text trans- former.  The Journal of Machine Learning Research , 21(1):5485–5551. ", "page_idx": 10, "bbox": [70, 667.1305541992188, 290, 733.9304809570312], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 141, "type": "text", "text": "Hannah Rashkin, Vitaly Nikolaev, Matthew Lamm, Lora Aroyo, Michael Collins, Dipanjan Das, Slav Petrov, Gaurav Singh Tomar, Iulia Turc, and David ", "page_idx": 10, "bbox": [70, 741.7105712890625, 290, 775.6334838867188], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 142, "type": "text", "text": "Reitter. 2021. Measuring attribution in natu- ral language generation models. arXiv preprint arXiv:2112.12870 . ", "page_idx": 10, "bbox": [316, 72.61956787109375, 526, 106.54253387451172], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 143, "type": "text", "text": "Revanth Gangi Reddy, Yi R Fung, Qi Zeng, Manling Li, Ziqi Wang, Paul Sullivan, et al. 2023. Smartbook: Ai- assisted situation report generation.  arXiv preprint arXiv:2303.14337 . ", "page_idx": 10, "bbox": [306, 117.611572265625, 526, 162.4924774169922], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 144, "type": "text", "text": "Michele Salvagno, Fabio Silvio Taccone, Alberto Gio- vanni Gerli, et al. 2023. Can artificial intelligence help for scientific writing?  Critical care , 27(1):1–5. ", "page_idx": 10, "bbox": [306, 173.56158447265625, 526, 207.48448181152344], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 145, "type": "text", "text": "Tal Schuster, Adam Fisch, and Regina Barzilay. 2021. Get your vitamin C! robust fact verification with contrastive evidence . In  Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies , pages 624–643, Online. As- sociation for Computational Linguistics. ", "page_idx": 10, "bbox": [306, 218.55255126953125, 526, 296.3115234375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 146, "type": "text", "text": "Özge Sevgili, Artem Shelmanov, Mikhail Arkhipov, Alexander Panchenko, and Chris Biemann. 2022. Neural entity linking: A survey of models based on deep learning.  Semantic Web , (Preprint):1–44. ", "page_idx": 10, "bbox": [306, 307.37957763671875, 526, 352.2615051269531], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 147, "type": "text", "text": "Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. 2021. Retrieval augmentation reduces hallucination in conversation.  arXiv preprint arXiv:2104.07567 . ", "page_idx": 10, "bbox": [306, 363.3295593261719, 526, 408.2115173339844], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 148, "type": "text", "text": "Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto. 2023. Alpaca: A strong, replicable instruction-following model.  Stan- ford Center for Research on Foundation Models. https://crfm. stanford. edu/2023/03/13/alpaca. html , 3(6):7.", "page_idx": 10, "bbox": [306, 419.2795715332031, 526, 497.03851318359375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 149, "type": "text", "text": "James Thorne, Andreas Vlachos, Christos Christo dou lo poul os, and Arpit Mittal. 2018. FEVER: a large-scale dataset for fact extraction and VERification . In  Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers) , pages 809–819, New Orleans, Louisiana. Association for Computational Linguistics. ", "page_idx": 10, "bbox": [306, 508.1065673828125, 526, 607.7835083007812], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 150, "type": "text", "text": "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023.  Llama: Open and efficient foundation language models . ", "page_idx": 10, "bbox": [306, 618.8515625, 526, 685.6515502929688], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 151, "type": "text", "text": "Denny Vrandeˇ ci´ c and Markus Krötzsch. 2014. Wiki- data: a free collaborative knowledge base.  Communi- cations of the ACM, 57(10):78–85.", "page_idx": 10, "bbox": [306, 696.62060546875, 526, 730.6425170898438], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 152, "type": "text", "text": "Adina Williams, Nikita Nangia, and Samuel Bowman. 2018.  A broad-coverage challenge corpus for sen- tence understanding through inference . ", "page_idx": 10, "bbox": [306, 741.7105712890625, 526, 775.6334838867188], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 153, "type": "text", "text": "Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. 2023.  Wizardlm: Empowering large language models to follow complex instructions . Wei Yang, Yuqing Xie, Aileen Lin, Xingyu Li, Luchen Tan, Kun Xiong, Ming Li, and Jimmy Lin. 2019. End-to-end open-domain question answering with bertserini.  arXiv preprint arXiv:1902.01718 . Xiang Yue, Boshi Wang, Kai Zhang, Ziru Chen, Yu Su, and Huan Sun. 2023. Automatic evaluation of at- tribution by large language models.  arXiv preprint arXiv:2305.06311 . Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Wein- berger, and Yoav Artzi. 2019a. Bertscore: Eval- uating text generation with bert. arXiv preprint arXiv:1904.09675 . Yuan Zhang, Jason Baldridge, and Luheng He. 2019b. PAWS: Paraphrase adversaries from word scrambling . In  Proceedings of the 2019 Conference of the North American Chapter of the Association for Computa- tional Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) , pages 1298–1308, Minneapolis, Minnesota. Association for Computa- tional Linguistics. Guido Zuccon and Bevan Koopman. 2023. Dr chatgpt, tell me what i want to hear: How prompt knowledge impacts health answer correctness.  arXiv preprint arXiv:2302.13793 . ", "page_idx": 11, "bbox": [71, 72.61956787109375, 290, 425.3475036621094], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 154, "type": "text", "text": "A Dataset Construction ", "text_level": 1, "page_idx": 11, "bbox": [306, 73, 435, 84], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 155, "type": "text", "text": "In this section, we will explain the detailed process and algorithms for the automatic dataset construc- tion pipeline. Using this pipeline, we are able to construct datasets with a greater scale or in other domains. ", "page_idx": 11, "bbox": [305, 93.47101593017578, 526, 160.81344604492188], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 156, "type": "text", "text": "A.1 Person Selection ", "text_level": 1, "page_idx": 11, "bbox": [306, 171, 410, 184], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 157, "type": "text", "text": "To improve the complexity of the questions and dif- ficulty to LLMs, we involve more than one person in each question. In addition, we need high qual- ity paragraphs for subsequent dataset generation steps. Therefore, we utilize name pairs and para- graphs from the  biographical database , which is a database specifically designed for the relation ex- traction (RE) task. Each piece of data from the bio- graphical database includes a short paragraph, and a relation triple extracted from the paragraph. The relation triple consists of two people and their re- lationship such as <William Shakespeare, Spouse, Anne Hathaway>. The biographical database in- cludes an automatically extracted set and a human annotated set. We specifically choose the human annotated set from the database to ensure high- quality name pairs. To avoid potential ambiguities, we filter out data if any name in the triple is incom- plete. In practice, we consider a name complete if it has at least a family name and a surname. ", "page_idx": 11, "bbox": [305, 189.31898498535156, 526, 459.8984680175781], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 158, "type": "text", "text": "A.2 Name Disambiguation ", "text_level": 1, "page_idx": 11, "bbox": [306, 471, 437, 483], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 159, "type": "text", "text": "Due to the presence of duplicate names (e.g., Anne Hathaway: the actress, or the wife of William Shakespeare), we perform name disambiguation to map each name in the triple to a unique entity from the knowledge graph. We utilize WikiData 4   ( Vran- deˇ ci´ c and Krötzsch ,  2014 ) as the knowledge base and employ SPARQL ( Pérez et al. ,  2009 ) queries to retrieve all entities associated with the name. Wiki- Data assigns a unique QID to each entity which distinguishes between entities with the same name. In WikiData, each entity represents a node in the knowledge graph. Since each triple consists of two names and one relation, we select the two entities obtained from the query if they are connected to each other on WikiData. Additionally, the connect- ing edge should align with the relation specified in the triple. Subsequently, we extract the one- hop sub-graph centered around each person node, which provides properties related to the person, such as gender, birth date, occupation, and more. We convert ambiguous person names from previous steps to unique QID from WikiData. The extracted sub-graphs contain all knowledge from WikiData about the selected people. We call the extracted graphs “knowledge pool”. ", "page_idx": 11, "bbox": [305, 488.40399169921875, 526, 745.4344482421875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 160, "type": "text", "text": "", "page_idx": 12, "bbox": [70, 71.74500274658203, 291, 152.63650512695312], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 161, "type": "text", "text": "A.3 Evolutionary Question Generation ", "text_level": 1, "page_idx": 12, "bbox": [70, 162, 260, 175], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 162, "type": "text", "text": "We employ an “evolutionary question generation” approach inspired by WizardLM ( Xu et al. ,  2023 ) and DKGen ( Qian et al. ,  2023 ), where we gradually increase the set of knowledge required by injecting knowledge through iterations. In each iteration, LLMs extend the paragraph with one sentence by incorporating the additional knowledge. After the last iteration, LLMs propose two questions accord- ing to the extended paragraph, one is a general version, and the other is a specific version. The general question is more concise, and the specific question is more detailed. Both questions target the same set of knowledge. All injected knowl- edge form a “ minimum knowledge set ”, which includes the least knowledge required to answer the proposed question (Table  1 ). We do not throw all knowledge to LLM at once to form a paragraph because extending the paragraph and knowledge set increment ally allow us to select the appropriate knowledge after each iteration. ", "page_idx": 12, "bbox": [70, 180.01698303222656, 291, 450.59747314453125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 163, "type": "text", "text": "In the first iteration, LLMs annotate the orig- inal paragraph from Biographical Database with the knowledge from the “knowledge pool”. For in- stance, the sentence “Artemisia was born in Rome.” is annotated with knowledge [Artemisia , place of birth, Rome]. In each subsequent iteration, we se- lect a piece of appropriate knowledge according to the existing paragraph. A sentence with appropriate knowledge should have good  specificity  and  coher- ence . Specificity refers to the significance of the knowledge, such that it is not too general or trivial. Coherence refers to the naturalness of the addi- tional knowledge. The added knowledge should not deviate from the existing paragraph and should be coherent when reading. During knowledge se- lection, each piece of knowledge is assigned a score by adding the specificity score and coherence score. The specificity score measures the uniqueness of the knowledge. We discourage the system from selecting too frequent relation types like “gender” or \"date of birth\" which may be less informative. A less frequent relation tend to provide a knowl- edge specific to the person. Derived from IDF, we calculate the  number of occurrences  $C o u n t_{r}$   for each  relation  $r$   in the  dataset with size    $N$  . The coherence score is calculated through perplexity. We convert each piece of knowledge to a simple sentence by applying a template. For instance, the knowledge [Artemisia, place of birth, Rome] is converted to “Artemisia’s place of birth is Rome”. There are three templates depending on the POS of the relation. We append each sentence to the orig- inal paragraph and calculate normalized inverse perplexity to obtain coherence score. The overall score is a weighted sum of specificity score and coherence score: ", "page_idx": 12, "bbox": [70, 451.12298583984375, 291, 777.5364379882812], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 164, "type": "text", "text": "", "page_idx": 12, "bbox": [305, 71.35227966308594, 526, 233.93148803710938], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 165, "type": "equation", "text": "\n$$\n\\begin{array}{r l}{S c o r e_{r}=\\alpha\\cdot\\log(2\\cdot N/C o u n t_{r})}&{{}}\\\\ {+\\left(1-\\alpha\\right)\\cdot\\mathrm{softmax}(1/p e r p_{r})}&{{}}\\end{array}\n$$\n ", "text_format": "latex", "page_idx": 12, "bbox": [318, 242, 501, 276], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 166, "type": "text", "text": "In each iteration, we leverage the “text-davinci-  $_{003}{}^{\\circ}$   model for annotation or generation with in- context learning. We provide separate instructions and demonstrations for general and specific ques- tions. The detailed prompt templates used is pro- vided in the appendix  D . We provide one human written demonstration. Some examples of full ques- tion evolution process are provided in appendix  E . In practice, we employ five iterations to ensure suf- ficient complexity in the questions without making them overly tedious. ", "page_idx": 12, "bbox": [305, 286.197021484375, 526, 434.83447265625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 167, "type": "text", "text": "B Experiment Details ", "text_level": 1, "page_idx": 12, "bbox": [305, 447, 426, 460], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 168, "type": "text", "text": "B.1 Main Experiment ", "text_level": 1, "page_idx": 12, "bbox": [306, 470, 416, 483], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 169, "type": "text", "text": "For the main experiments, we run reach model with different seeds for three times. The OpenAI fam- ily models are implemented using OpenAI APIs. Running one round of experiment with ChatGPT model takes approximately 1 hour, and costs about 3 USD. Running one round of experiment with GPT4 model takes approximately 1.5 to 2 hours, and costs about 60 USD. Each LLaMA family model is run on one TESLA V100 GPU, where each run takes about 6 to 8 hours for Alpaca-7B and Vicuna-13B, and about 12-16 hours for LLaMA-7B and LLaMA-13B. ", "page_idx": 12, "bbox": [305, 489.0069885253906, 526, 651.1934814453125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 170, "type": "text", "text": "B.2 Text Quality Evaluation ", "text_level": 1, "page_idx": 12, "bbox": [305, 663, 445, 676], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 171, "type": "text", "text": "For text quality evaluation, we use the model “text- davinci-  $.003^{\\circ}$   with temperature 0 to ensure stability and reproducibility of the results. We randomly sample 100 outputs from each baseline and take three runs to report mean. We do not report stan- dard deviation since most of them are mostly in- significantly small (below 0.1). ", "page_idx": 12, "bbox": [305, 681.458984375, 526, 775.8994140625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 172, "type": "text", "text": "B.3 NLI ", "text_level": 1, "page_idx": 13, "bbox": [70, 72, 118, 83], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 173, "type": "text", "text": "For the automatic evaluation of text citation align- ment and evaluation of the known unknown cita- tions, we implement the TRUE model from Hug- gingFace 5 , which was trained on SNLI ( Bow- man et al. ,  2015 ), MNLI ( Williams et al. ,  2018 ), Fever ( Thorne et al. ,  2018 ), Scitail ( Khot et al. , 2018 ), PAWS ( Zhang et al. ,  2019b ), and Vita- minC ( Schuster et al. ,  2021 ). The model uses the prompt of “premise: {PREMISE} hypothesis: {HYPOTHESIS}”. For each sentence citation pair, we place the sentence in the “PREMISE”, and the citation to the “HYPOTHESIS”, like the following: “premise: {Hertwig served as a professor at the Uni- versity of Jena for the last 40 years of his career.} hypothesis: {employer: University of Jena}” ", "page_idx": 13, "bbox": [69, 89.22199249267578, 291, 292.0564880371094], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 174, "type": "text", "text": "C Human Evaluation ", "text_level": 1, "page_idx": 13, "bbox": [70, 302, 190, 315], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 175, "type": "text", "text": "C.1 Dataset Evaluation ", "text_level": 1, "page_idx": 13, "bbox": [70, 323, 187, 335], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 176, "type": "text", "text": "To evaluate the dataset quality, we have two indi- vidual annotators who are proficient in the English language. Below are the exact method for evaluat- ing each metric: ", "page_idx": 13, "bbox": [69, 339.8599853515625, 291, 393.6534729003906], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 177, "type": "text", "text": "•  Authenticity . We ask the annotators to check from WikiPedia and understand the back- ground stories of the mentioned people, and decide if the generated question matches the background story. Each question is assigned score 1 if it matches the background story, and score 0 if there is contradiction. ", "page_idx": 13, "bbox": [83, 399.572265625, 291, 494.4054870605469], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 178, "type": "text", "text": "•  Relevance . After understanding the back- ground stories, we ask the annotators to label each piece of knowledge from the minimum knowledge set. A piece of knowledge is la- beled 1 if the annotator thinks it is necessary to answer the question, and 0 if it is redun- dant. The relevance score of a question is the ratio of number of necessary knowledge to the number of knowledge in the minimum set. ", "page_idx": 13, "bbox": [83, 502.1592712402344, 291, 624.0914306640625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 179, "type": "text", "text": "•  Naturalness . We ask the annotators to give an integer score 1 to 5 to label each question. 5 means the question can be easily understand- able, and is concise. 1 means the question is not written in natural English language or is extremely tedious. ", "page_idx": 13, "bbox": [83, 631.8452758789062, 291, 713.1294555664062], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 180, "type": "text", "text": "•  Significance . We ask the annotators to give an integer score 1 to 5 to label each question. ", "page_idx": 13, "bbox": [83, 720.88330078125, 291, 747.970458984375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 181, "type": "text", "text": "5 means the annotator feels that he or she may be interested in this question under some circumstances, and 1 means the opposite. ", "page_idx": 13, "bbox": [327, 71.74500274658203, 526, 111.98947143554688], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 182, "type": "text", "text": "The agreement between the two annotators are as follow: the agreement between them is   $100\\%$   for authenticity and   $86\\%$   for relevance. Since the eval- uation for naturalness and significance are score based, in   $92\\%$   and  $90\\%$   of the evaluated datasets respectively, the score difference between the two annotators is no larger than 1. ", "page_idx": 13, "bbox": [305, 131.60499572753906, 526, 226.04550170898438], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 183, "type": "text", "text": "C.2 Generated Text Evaluation ", "text_level": 1, "page_idx": 13, "bbox": [305, 244, 460, 257], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 184, "type": "text", "text": "Among text quality evaluation, citation quality eval- uation, and text-citation alignment, we conduct hu- man evaluation on text-citation alignment. Text quality evaluation is conducted using G-Eval. We acknowledge this is not a perfect metric, but the hu- man evaluation is conducted in ( Liu et al. ,  2023b ). The focus is this paper is not to improve G-Eval. Citation quality evaluation is conducted with look- ing for exact match between generated citations and minimum knowledge set, which is an objective evaluation. The text-citation alignment evaluation is conducted using NLI, which we are not certain if entailment means providing support. In addition, whether a knowledge supports a sentence can be subjective. Therefore, we conduct human evalua- tion on alignment. ", "page_idx": 13, "bbox": [305, 266.719970703125, 526, 483.1034851074219], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 185, "type": "text", "text": "We present the Human Evaluation Instructions provided to the annotators in Table  8 . We follow the implementation from ( Clark et al. ,  2021 ), and provide detailed instructions and examples to im- prove evaluation accuracy. For this human evalu- ation, there are four individual annotators in total. We arrange different annotators for different base- lines, and each baseline has two annotators. The Inter-Annotator Agreement for ChatGPT, LLaMA- 7B, and Vicuna-13B are reported as follows:   $90\\%$  ,  $97\\%$  , and   $89\\%$   respectively. ", "page_idx": 13, "bbox": [305, 486.0679931640625, 526, 634.7054443359375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 186, "type": "text", "text": "D Prompts ", "text_level": 1, "page_idx": 13, "bbox": [305, 654, 371, 668], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 187, "type": "text", "text": "We present the prompts and instructions we used in this section. We present the prompts for the evolutionary question construction in Table  9 ,  10 , 11 , and  12 . We present the prompt for the answer generation in Table  13 . We present the prompts we use for text evaluation with G-Eval in Table  14 ,  15 , 16 , and  17 . ", "page_idx": 13, "bbox": [305, 681.458984375, 526, 775.8994140625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 188, "type": "text", "text": "Each evaluation content includes a sentence and a piece of knowledge. Our task is to determine whether this sentence contains the given knowledge, i.e., whether this knowledge provides support for the sentence. If the sentence does not mention the given knowledge or if the content of the sentence does not align with the knowledge, it is considered unsupported. We use 1 to indicate support and 0 to indicate lack of support. ", "page_idx": 14, "bbox": [77, 90.89225006103516, 282, 219.84103393554688], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 189, "type": "text", "text": "Here are some examples: ", "page_idx": 14, "bbox": [77, 230.36924743652344, 201.6892547607422, 239.76602172851562], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 190, "type": "text", "text": "Sentence: Stephen Crane was an American writer born on November 1, 1871, in Newark, and died on June 5, 1900, in Baden weil er. Knowledge: date of birth: 1871-11-01 Result: 1, because the sentence’s date of birth matches the knowledge’s date of birth. ", "page_idx": 14, "bbox": [77, 250.2942352294922, 277, 339.39202880859375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 191, "type": "text", "text": "Sentence: Merton died on December 10, 1968, in Bangkok, Thailand. Knowledge: country of citizenship: United States of America Result: 0, because the sentence does not mention Merton’s nationality. ", "page_idx": 14, "bbox": [77, 349.9212341308594, 282, 409.1310729980469], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 192, "type": "text", "text": "Table 8: Instruction we provide to the human annotators. ", "page_idx": 14, "bbox": [70, 409.55657958984375, 290, 421.5615234375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 193, "type": "text", "text": "E Evolutionary Question Generation ", "text_level": 1, "page_idx": 14, "bbox": [70, 443, 268, 456], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 194, "type": "text", "text": "We provide an example of evolutionary question generation in Table  18 . ", "page_idx": 14, "bbox": [70, 464.06500244140625, 290, 490.7604675292969], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 195, "type": "text", "text": "F Examples ", "text_level": 1, "page_idx": 14, "bbox": [70, 500, 140, 514], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 196, "type": "text", "text": "We show examples of the attributed answers gener- ated by the LLMs in Table  19  and  20 . ", "page_idx": 14, "bbox": [70, 522.2930297851562, 290, 548.9874267578125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 197, "type": "text", "text": "Instruction: Your objective is to select relevant knowledge to label the sentence and generate a question ", "page_idx": 15, "bbox": [77, 133.48573303222656, 490, 158.46844482421875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 198, "type": "text", "text": "sentence: Artemisia Gentiles chi was born Artemisia Gentiles chi Lomi in Rome on July 8 1593 although her birth certificate from the Archivio di Stato indicated she was born in 1590 the eldest child of the Tuscan painter Orazio Gentiles chi and Prudenzia di Ottaviano Montoni. ", "page_idx": 15, "bbox": [77, 174.1337432861328, 516, 239.76348876953125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 199, "type": "text", "text": "knowledge: {qid: Q367360, name: Orazio Gentiles chi, sex or gender: male, place of birth: Pisa, place of death: London, instance of: human, occupation: painter, child: Artemisia Gentiles chi, described by source: The Great Theatre of Dutch Painters, notable works: Diana the Huntress, given name: Orazio, topic’s main category: Category:Orazio Gentiles chi, surname: Gentiles chi, genre: portrait, languages spoken: Italian, movement: mannerism, work location: Rome, ethnic group: Italians, date of birth: 1563-07-19, date of death: 1639-02-07} {qid: Q212657, name: Artemisia Gentiles chi, sex or gender: female, place of birth: Rome, place of death: Naples, instance of: human, occupation: painter, member of: Accademia delle Arti del Disegno, father: Orazio Gentiles chi, described by source: The Great Theatre of Dutch Painters, notable works: Judith Slaying Holofernes, topic’s main category: Category:Artemisia Gentiles chi, movement: Car ava gg is ti, ethnic group: Italians, work location: Florence, depicted by: Artemisia, field of work: painting, surname: Gentiles chi, genre: portrait, languages spoken: Italian, position held: court painter, student of: Orazio Gentiles chi, spouse: Pier antonio Stiattesi, given name: Artemisia, mother: Prudenzia di Ottaviano Montoni, date of birth: 1596-07-08, date of death: 1654-01-01} ", "page_idx": 15, "bbox": [77, 241.87977600097656, 516, 536.420654296875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 200, "type": "text", "text": "Generated Answer: Artemisia Gentiles chi [qid: Q212657, name: Artemisia Gentiles chi] was born Artemisia Gentiles chi Lomi in Rome [qid: Q212657, place of birth: Rome] on July 8 1593 [qid: Q212657, date of birth: 1596-07-08] although her birth certificate from the Archivio di Stato indicated she was born in 1590 the eldest child of the Tuscan painter Orazio Gentiles chi [qid: Q212657, father: Orazio Gentiles chi] [qid: Q367360, name: Orazio Gentiles chi, occupation: painter] and Prudenzia di Ottaviano Montoni. ", "page_idx": 15, "bbox": [77, 552.0869140625, 516, 671.9126586914062], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 201, "type": "text", "text": "Instruction: Your objective is to extend the original paragraph by adding one sentence that includes the given knowledge ", "page_idx": 16, "bbox": [77, 234.39271545410156, 516, 259.37445068359375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 202, "type": "text", "text": "answer: Artemisia Gentiles chi [qid: Q212657, name: Artemisia Gentiles chi] was born Artemisia Gentiles chi Lomi in Rome [qid: Q212657, place of birth: Rome] on July 8 1593 [qid: Q212657, date of birth: 1596-07-08] although her birth certificate from the Archivio di Stato indicated she was born in 1590 the eldest child of the Tuscan painter Orazio Gentiles chi [qid: Q212657, father: Orazio Gentiles chi] [qid: Q367360, name: Orazio Gentiles chi, occupation: painter] and Prudenzia di Ottaviano Montoni. knowledge: {qid: Q212657, name: Artemisia Gentiles chi, notable works: Judith Slaying Holofernes} ", "page_idx": 16, "bbox": [77, 275.04071044921875, 516, 408.41552734375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 203, "type": "text", "text": "Generated Answer: Artemisia Gentiles chi [qid: Q212657, name: Artemisia Gentiles chi] was born Artemisia Gentiles chi Lomi in Rome [qid: Q212657, place of birth: Rome] on July 8 1593 [qid: Q212657, date of birth: 1596-07-08] although her birth certificate from the Archivio di Stato indicated she was born in 1590 the eldest child of the Tuscan painter Orazio Gentiles chi [qid: Q212657, father: Orazio Gentiles chi] [qid: Q367360, name: Orazio Gentiles chi, occupation: painter] and Prudenzia di Ottaviano Montoni. Under the influence of her father, Artemisia Gentiles chi created her iconic painting Judith Slaying Holofernes [qid: Q212657, notable works: Judith Slaying Holofernes] when she was around twenty years old. ", "page_idx": 16, "bbox": [77, 424.081787109375, 516, 584.5556030273438], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 204, "type": "text", "text": "Table 10: Instruction and demonstration for evolutionary question construction for round 2 to 5. We use brown color for the knowledge, and blue color for the demonstration of expected output. ", "page_idx": 16, "bbox": [70, 585.5965576171875, 524.406005859375, 609.5565185546875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 205, "type": "text", "text": "Instruction: Your objective is to ask a question whose answer is the given paragraph. There should be only one question when possible, if not, make sure the question is as concise as possible. ", "page_idx": 17, "bbox": [77, 80.61975860595703, 516, 119.15052032470703], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 206, "type": "text", "text": "Paragraph: Artemisia Gentiles chi was born Artemisia Gentiles chi Lomi in Rome on July 8 1593 although her birth certificate from the Archivio di Stato indicated she was born in 1590 the eldest child of the Tuscan painter Orazio Gentiles chi and Prudenzia di Ottaviano Montoni. Her life and work were later depicted in the film Ärtemisia¨ , which brought her story to a wider audience. Her father, Orazio, was a prominent figure in the Mannerism art movement, which likely influenced Artemisia’s own artistic style. However, Artemisia herself was a part of the Car ava gg is ti movement, a group of artists who followed the style of Caravaggio. She was also a student of her father, Orazio Gentiles chi, which further shaped her artistic development. Orazio’s influence on Artemisia’s development as a prominent Baroque painter can be seen in her highly naturalistic portrayal of figures, dramatic scenes and the use of chiaroscuro technique ", "page_idx": 17, "bbox": [77, 134.81581115722656, 516, 335.9376220703125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 207, "type": "text", "text": "Generated Question: Who was Artemisia Gentiles chi and what influences shaped her artistic style? ", "page_idx": 17, "bbox": [77, 351.6028747558594, 463, 376.5856018066406], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 208, "type": "text", "text": "Table 11: Instruction and demonstration for general question generation. We use blue color for the demonstration of expected output. ", "page_idx": 17, "bbox": [70, 377.6265563964844, 524.4063110351562, 401.5865173339844], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 209, "type": "text", "text": "Instruction: Your objective is to ask a question whose answer is the given paragraph. The question should not be too tedious. ", "page_idx": 17, "bbox": [77, 428.813720703125, 503, 453.79547119140625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 210, "type": "text", "text": "Paragraph: Artemisia Gentiles chi was born Artemisia Gentiles chi Lomi in Rome on July 8 1593 although her birth certificate from the Archivio di Stato indicated she was born in 1590 the eldest child of the Tuscan painter Orazio Gentiles chi and Prudenzia di Ottaviano Montoni. Her life and work were later depicted in the film Ärtemisia¨ , which brought her story to a wider audience. Her father, Orazio, was a prominent figure in the Mannerism art movement, which likely influenced Artemisia’s own artistic style. However, Artemisia herself was a part of the Car ava gg is ti movement, a group of artists who followed the style of Caravaggio. She was also a student of her father, Orazio Gentiles chi, which further shaped her artistic development. Orazio’s influence on Artemisia’s development as a prominent Baroque painter can be seen in her highly naturalistic portrayal of figures, dramatic scenes and the use of chiaroscuro technique ", "page_idx": 17, "bbox": [77, 469.46173095703125, 516, 670.58251953125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 211, "type": "text", "text": "Generated Question: What were the key artistic influences and characteristics that shaped Artemisia Gentiles chi’s unique Baroque style, and how did her relationship with her father, Orazio Gentiles chi, impact her artistic development? ", "page_idx": 17, "bbox": [77, 686.248779296875, 509.66973876953125, 738.3284912109375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 212, "type": "text", "text": "Instruction: You answer the question based on your knowledge, with the given information for annotation, following the given format. Use [NA] for claims that need annotation but is unprovided. ", "page_idx": 18, "bbox": [77, 127.42371368408203, 516, 165.9554443359375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 213, "type": "text", "text": "Question: Considering the information: ", "text_level": 1, "page_idx": 18, "bbox": [76, 180, 333, 192], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 214, "type": "text", "text": "{name: Orazio Gentiles chi, place of death: London, child: Artemisia Gentiles chi, notable works: Diana the Huntress, given name: Orazio, surname: Gentiles chi, languages spoken: Italian, movement: mannerism, work location: Rome, ethnic group: Italians, date of birth: 1563-07-19, date of death: 1639-02-07, qid: Q367360} {name: Artemisia Gentiles chi, place of birth: Rome, place of death: Naples, occupation: painter, member of: Accademia delle Arti del Disegno, father: Orazio Gentiles chi, notable works: Judith Slaying Holofernes, movement: Car ava gg is ti, ethnic group: Italians, work location: Florence, depicted by: Artemisia, surname: Gentiles chi, languages spoken: Italian, student of: Orazio Gentiles chi, given name: Artemisia, mother: Prudenzia di Ottaviano Montoni, date of death: 1654-01-01, qid: Q212657} How did Orazio Gentiles chi’s influence on Artemisia’s life and career shape her development as a prominent Baroque painter, despite facing significant challenges as a female artist in a male-dominated field? ", "page_idx": 18, "bbox": [77, 193.74476623535156, 503, 434.089599609375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 215, "type": "text", "text": "Answer: Artemisia Gentiles chi was an Italian painter born on July 8, 1596 [NA] in Rome [Q212657, ethnic group: Italians, occupation: painter, place of birth: Rome]. She was a member of the Accademia delle Arti del Disegno and is best known for her work Judith Slaying Holofernes [Q212657, member of: Accademia delle Arti del Disegno, notable works: Judith Slaying Holofernes]. She was the eldest child of the Tuscan painter Orazio Gentiles chi [Q212657, father: Orazio Gentiles chi]. Orazio Gentiles chi was an Italian painter [NA] born in 1563 and died in 1639 [Q367360, ethnic group: Italians, date of birth: 1563-07-19, date of death: 1639-02-07]. He was born in Pisa and died in London [Q367360, place of death: London]. Orazio’s influence on Artemisia’s development as a prominent Baroque [NA] painter can be seen in her highly naturalistic portrayal of figures, dramatic scenes and the use of chiaroscuro technique [NA]. He also provided her with the opportunity to study with him and learn from his experience and expertise. She became an important second-generation proponent of Caravaggio’s dramatic realism [Q212657, movement: Car ava gg is ti]. ", "page_idx": 18, "bbox": [77, 449.7548522949219, 516, 691.524658203125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 216, "type": "text", "text": "Table 13: Full instruction and demonstration for answer generation with citaion. We use brown color for the knowledge pool, and blue color for the expected outcome provided by the demonstration. ", "page_idx": 18, "bbox": [70, 692.5655517578125, 524.4059448242188, 716.5255126953125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 217, "type": "text", "text": "Instruction: You will be given one question and answer. Your task is to rate the answer on one metric. Please make sure you read and understand these instructions carefully. Please keep this document open while reviewing, and refer to it as needed. ", "page_idx": 19, "bbox": [77, 113.69574737548828, 516, 165.7755126953125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 218, "type": "text", "text": "Evaluation Criteria: ", "page_idx": 19, "bbox": [77, 181.44178771972656, 208, 192.87451171875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 219, "type": "text", "text": "Coherence (1-5) - the collective quality of all sentences. We align this dimension with the DUC quality question of structure and coherence whereby the answer should be well-structured and well-organized. The answer should not just be a heap of related information, but should build from sentence to sentence to a coherent body of information about a topic. ", "page_idx": 19, "bbox": [77, 194.9907989501953, 516, 274.1695556640625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 220, "type": "text", "text": "Evaluation Steps: ", "page_idx": 19, "bbox": [77, 289.8348388671875, 188.94180297851562, 301.267578125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 221, "type": "text", "text": "1. Read the questions carefully and identify the main topic and key points. 2. Read the answer and compare it to the question. Check if the answer covers the main topic and key points of the question, and if it presents them in a clear and logical order. 3. Assign a score for coherence on a scale of 1 to 5, where 1 is the lowest and 5 is the highest based on the Evaluation Criteria. ", "page_idx": 19, "bbox": [77, 303.38385009765625, 516, 396.11163330078125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 222, "type": "text", "text": "Instruction: You will be given one question and answer. Your task is to rate the answer on one metric. Please make sure you read and understand these instructions carefully. Please keep this document open while reviewing, and refer to it as needed. ", "page_idx": 19, "bbox": [77, 502.5377197265625, 516, 554.6174926757812], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 223, "type": "text", "text": "Evaluation Criteria: Consistency (1-5) - the answer should be consistent with the given knowledge. The answer should also be self-consistent, without any contradiction to itself.\n\n ", "page_idx": 19, "bbox": [77, 570.2837524414062, 509, 622.3634643554688], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 224, "type": "text", "text": "Evaluation Steps: ", "text_level": 1, "page_idx": 19, "bbox": [76, 638, 189, 648.75], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 225, "type": "text", "text": "1. Read the question and knowledge carefully.\n\n 2. Read the answer and compare it to the knowledge. Check if the answer is consistent with the give knowledge. 3. Assign a score for consistency on a scale of 1 to 5, where 1 is the lowest and 5 is the highest based on the Evaluation Criteria. ", "page_idx": 19, "bbox": [77, 651.5787353515625, 516, 717.2074584960938], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 226, "type": "text", "text": "Instruction: You will be given one question and answer. Your task is to rate the answer on one metric. Please make sure you read and understand these instructions carefully. Please keep this document open while reviewing, and refer to it as needed. ", "page_idx": 20, "bbox": [77, 127.24475860595703, 516, 179.32452392578125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 227, "type": "text", "text": "", "page_idx": 20, "bbox": [76, 194, 210, 201.75], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 228, "type": "text", "text": "Fluency (1-5) - the answer should be written in fluent language. The answer should use appropriate vocabulary, grammar, and sentence structures that enable readers or listeners to comprehend the content effortlessly. ", "page_idx": 20, "bbox": [77, 208.53981018066406, 516, 260.62054443359375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 229, "type": "text", "text": "Evaluation Steps:\n\n ", "page_idx": 20, "bbox": [77, 276.28582763671875, 188.94180297851562, 287.71856689453125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 230, "type": "text", "text": "1. Read the question carefully.\n\n 2. Read the answer and check if the language in the answer is fluent. ", "page_idx": 20, "bbox": [77, 289.8348388671875, 483.48785400390625, 328.3656005859375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 231, "type": "text", "text": "3. Assign a score for fluency on a scale of 1 to 5, where 1 is the lowest and 5 is the highest based on the Evaluation Criteria. ", "page_idx": 20, "bbox": [77, 330.48284912109375, 516, 355.464599609375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 232, "type": "text", "text": "Table 16: Instruction for text evaluation with GPT-EVAL - Fluency ", "page_idx": 20, "bbox": [163.66600036621094, 356.5055847167969, 431.30126953125, 368.5105285644531], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 233, "type": "text", "text": "Instruction: You will be given one question and answer. Your task is to rate the answer on one metric. Please make sure you read and understand these instructions carefully. Please keep this document open while reviewing, and refer to it as needed. ", "page_idx": 20, "bbox": [77, 488.98773193359375, 516, 541.0684814453125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 234, "type": "text", "text": "Evaluation Criteria: Relevance (1-5) - the answer should be relevant to the question. The answer should directly answers the question, without providing any irrelevant information.\n\n ", "page_idx": 20, "bbox": [77, 556.7337646484375, 509.3427429199219, 608.814453125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 235, "type": "text", "text": "Evaluation Steps: ", "text_level": 1, "page_idx": 20, "bbox": [76, 625, 189, 636], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 236, "type": "text", "text": "1. Read the question carefully.\n\n 2. Read the answer and compare with the question to check if it fully answers the question and have no redundancies. 3. Assign a score for relevance on a scale of 1 to 5, where 1 is the lowest and 5 is the highest based on the Evaluation Criteria. ", "page_idx": 20, "bbox": [77, 638.0297241210938, 503, 703.658447265625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 237, "type": "text", "text": "Round 1: ", "page_idx": 21, "bbox": [77, 160.1042938232422, 120, 169.50106811523438], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 238, "type": "text", "text": "Annotated Knowledge ", "text_level": 1, "page_idx": 21, "bbox": [76, 171, 183, 179], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 239, "type": "text", "text": "[qid: Q258115, name: Diego Simeone, occupation: association football player] [qid: Q6439494, name: Giovanni Simeone, father: Diego Simeone] [qid: Q6439494, name: Giovanni Simeone, place of birth: Madrid] [qid: Q258115, name: Diego Simeone, member of sports team: Atletico Madrid] ", "page_idx": 21, "bbox": [77, 178.03623962402344, 502.6767883300781, 217.32101440429688], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 240, "type": "text", "text": "Paragraph : The son of former Argentine international footballer Diego Simeone and Carolina Baldini Giovanni Simeone was born in Madrid while his father was playing for Atletico Madrid. ", "page_idx": 21, "bbox": [77, 217.34925842285156, 497, 247.20901489257812], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 241, "type": "text", "text": "Round 2: ", "text_level": 1, "page_idx": 21, "bbox": [76, 258, 119, 267], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 242, "type": "text", "text": "Input Knowledge : [qid: Q258115, name: Diego Simeone, position played on team: midfielder] ", "page_idx": 21, "bbox": [77, 267.5613098144531, 508.05609130859375, 287.4580383300781], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 243, "type": "text", "text": "Paragraph : The son of former Argentine international footballer Diego Simeone and Carolina Baldini Giovanni Simeone was born in Madrid while his father was playing as a midfielder for Atletico Madrid. ", "page_idx": 21, "bbox": [77, 287.4862976074219, 497, 317.3460388183594], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 244, "type": "text", "text": "Round 3: ", "text_level": 1, "page_idx": 21, "bbox": [76, 328, 119, 337], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 245, "type": "text", "text": "Input Knowledge : [qid: Q258115, Diego Simeone, member of sports team: S.S. Lazio] ", "page_idx": 21, "bbox": [77, 337.69830322265625, 491, 357.5950622558594], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 246, "type": "text", "text": "Paragraph : The son of former Argentine international footballer Diego Simeone and Carolina Baldini Giovanni Simeone was born in Madrid while his father was playing as a midfielder for Atletico Madrid. Before his stint at Atletico Madrid, Diego Simeone had also played for S.S. Lazio. ", "page_idx": 21, "bbox": [77, 357.6233215332031, 518, 397.44610595703125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 247, "type": "text", "text": "Round 4: ", "text_level": 1, "page_idx": 21, "bbox": [76, 408, 120, 417], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 248, "type": "text", "text": "Input Knowledge : [qid: Q258115, Diego Simeone, medical condition: COVID-19] Paragraph : The son of former Argentine international footballer Diego Simeone and Carolina Baldini Giovanni Simeone was born in Madrid while his father was playing as a midfielder for Atletico Madrid. Before his stint at Atletico Madrid, Diego Simeone had also played for S.S. Lazio. Later in life, Diego Simeone contracted the COVID-19 virus, but managed to recover. ", "page_idx": 21, "bbox": [77, 418.0823059082031, 518, 477.8301086425781], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 249, "type": "text", "text": "Round 5: ", "page_idx": 21, "bbox": [77, 489.0422668457031, 120, 498.4390563964844], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 250, "type": "text", "text": "Input Knowledge : [qid: Q6439494, Giovanni Simeone, position played on team: forward] ", "page_idx": 21, "bbox": [77, 498.4673156738281, 491, 518.3640747070312], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 251, "type": "text", "text": "Paragraph : The son of former Argentine international footballer Diego Simeone and Carolina Baldini Giovanni Simeone was born in Madrid while his father was playing as a midfielder for Atletico Madrid. Before his stint at Atletico Madrid, Diego Simeone had also played for S.S. Lazio. Later in life, Diego Simeone contracted the COVID-19 virus, but managed to recover. Following in his father’s footsteps but with a different role on the pitch, Giovanni Simeone grew up to be a forward in the game of football. ", "page_idx": 21, "bbox": [77, 518.3922729492188, 518, 588.1031494140625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 252, "type": "text", "text": "General Question : ", "text_level": 1, "page_idx": 21, "bbox": [75, 599, 168, 607.75], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 253, "type": "text", "text": "How did the career of Diego Simeone influence his son’s path in football? Specific Question : ", "page_idx": 21, "bbox": [77, 607.2843017578125, 470.397705078125, 629.0046997070312], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 254, "type": "text", "text": "Who are Diego Simeone and Giovanni Simeone, and how are there career path in football different from each other? ", "page_idx": 21, "bbox": [77, 627.2102661132812, 486.5372619628906, 646.569091796875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 255, "type": "text", "text": "Table 18: An example of the generated questions after each round in evolutionary question construction. We use brown color for the knowledge, and blue color for the generated questions. In each round we underline the texts to the newly injected knowledge. ", "page_idx": 21, "bbox": [70, 646.9945678710938, 524, 682.9105224609375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 256, "type": "text", "text": "Question : ", "text_level": 1, "page_idx": 22, "bbox": [75, 115, 135, 126], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 257, "type": "text", "text": "What were some of Stephen Crane’s notable works that reflected his views on religion and the harsh realities of life, and how did his experiences at Syracuse University influence his writing? Also, what was the name of the sports team he was a member of at Syracuse University? ", "page_idx": 22, "bbox": [75, 126.71173858642578, 516, 192.34149169921875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 258, "type": "text", "text": "Retrieved Knowledge : ", "text_level": 1, "page_idx": 22, "bbox": [76, 208, 208, 219], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 259, "type": "text", "text": "[qid: Q206534, name: Stephen Crane, sex or gender: male, place of birth: Newark, place of death: Baden weil er, country of citizenship: United States of America, cause of death: Tuberculosis, place of burial: Evergreen Cemetery, alma mater: Syracuse University, instance of: human, given name: Stephen, medical condition: Tuberculosis, occupation: writer, described by source: Great Soviet Encyclopedia (1969âC“1978), manner of death: natural causes, movement: literary realism, notable works: The Red Badge of Courage, surname: Crane’, \"topic’s main category\": ’Category:Stephen Crane, religion: atheism, languages spoken: English, mother: Mary Helen Peck Crane, sport: baseball, member of sports team: Syracuse Orange baseball, father: Jonathan Townley Crane, name: Stephen Crane, date of birth: 1871-11-01, date of death: 1900-06-05] ", "page_idx": 22, "bbox": [75, 220.1308135986328, 516, 407.7034912109375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 260, "type": "text", "text": "Generated Output : ", "text_level": 1, "page_idx": 22, "bbox": [76, 424, 188, 434], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 261, "type": "text", "text": "Stephen Crane was a male writer born on November 1, 1871, in Newark, United States of America, and died on June 5, 1900, in Baden weil er due to tuberculosis [Q206534, sex or gender: male, place of birth: Newark, place of death: Baden weil er, country of citizenship: United States of America, cause of death: Tuberculosis, medical condition: Tuberculosis, date of birth: 1871-11-01, date of death: 1900-06-05]. Crane was a prominent literary realist writer [Q206534, movement: literary realism] who wrote about the harsh realities of life and his atheistic views on religion [Q206534, religion: atheism]. His notable works include \"The Red Badge of Courage\" [Q206534, notable works: The Red Badge of Courage]. ", "page_idx": 22, "bbox": [75, 435.4937438964844, 510, 595.967529296875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 262, "type": "text", "text": "Crane attended Syracuse University and was a member of the Syracuse Orange baseball team [Q206534, alma mater: Syracuse University, sport: baseball, member of sports team: Syracuse Orange baseball]. His experiences at Syracuse University influenced his writing, and he wrote several stories about college life, including \"The Eighty-Yard Run,\" which was based on his own experiences as a football player at Syracuse [NA]. ", "page_idx": 22, "bbox": [75, 611.6328125, 516, 704.3605346679688], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 263, "type": "text", "text": "Table 19: An example of the attributed answer generated by ChatGPT (temperature 0.5). We use blue color for the question and brown color for the retrieved knowledge. ", "page_idx": 22, "bbox": [70, 705.4015502929688, 524.4133911132812, 729.3624877929688], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 264, "type": "text", "text": "Question : ", "text_level": 1, "page_idx": 23, "bbox": [75, 122, 135, 132.75], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 265, "type": "text", "text": "What were some of Stephen Crane’s notable works that reflected his views on religion and the harsh realities of life, and how did his experiences at Syracuse University influence his writing? Also, what was the name of the sports team he was a member of at Syracuse University? ", "page_idx": 23, "bbox": [75, 133.48573303222656, 516, 199.115478515625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 266, "type": "text", "text": "Retrieved Knowledge : ", "text_level": 1, "page_idx": 23, "bbox": [76, 215, 207, 226], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 267, "type": "text", "text": "[qid: Q206534, name: Stephen Crane, sex or gender: male, place of birth: Newark, place of death: Baden weil er, country of citizenship: United States of America, cause of death: Tuberculosis, place of burial: Evergreen Cemetery, alma mater: Syracuse University, instance of: human, given name: Stephen, medical condition: Tuberculosis, occupation: writer, described by source: Great Soviet Encyclopedia (1969âC“1978), manner of death: natural causes, movement: literary realism, notable works: The Red Badge of Courage, surname: Crane’, \"topic’s main category\": ’Category:Stephen Crane, religion: atheism, languages spoken: English, mother: Mary Helen Peck Crane, sport: baseball, member of sports team: Syracuse Orange baseball, father: Jonathan Townley Crane, name: Stephen Crane, date of birth: 1871-11-01, date of death: 1900-06-05] ", "page_idx": 23, "bbox": [75, 226.90577697753906, 516, 414.478515625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 268, "type": "text", "text": "Generated Output : ", "text_level": 1, "page_idx": 23, "bbox": [76, 431, 188, 441], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 269, "type": "text", "text": "Stephen Crane was an American writer born on November 1, 1871, in Newark, and died on June 5, 1900, in Baden weil er [Q206534, date of birth: 1871-11-01, date of death: 1900-06-05, place of birth: Newark, place of death: Baden weil er]. He was a part of the literary realism movement and is best known for his notable work, The Red Badge of Courage [Q206534, movement: literary realism, notable works: The Red Badge of Courage]. Crane’s views on religion were atheistic [Q206534, religion: atheism]. His works often reflected the harsh realities of life, such as war, poverty, and human suffering [NA]. ", "page_idx": 23, "bbox": [75, 442.26776123046875, 510, 575.6435546875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 270, "type": "text", "text": "Crane attended Syracuse University [Q206534, alma mater: Syracuse University], where his experiences likely influenced his writing. He was exposed to a diverse range of ideas and perspectives, which may have contributed to his realistic approach to literature [NA]. ", "page_idx": 23, "bbox": [75, 591.309814453125, 510, 643.3895263671875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 271, "type": "text", "text": "While at Syracuse University, Stephen Crane was a member of the Syracuse Orange baseball team [Q206534, member of sports team: Syracuse Orange baseball]. ", "page_idx": 23, "bbox": [75, 659.0548095703125, 490, 697.5865478515625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 272, "type": "text", "text": "Table 20: An example of the attributed answer generated by GPT-4. We use blue color for the question and brown color for the retrieved knowledge. ", "page_idx": 23, "bbox": [70, 698.6275634765625, 524.4073486328125, 722.5875244140625], "page_size": [595.2760009765625, 841.8900146484375]}
