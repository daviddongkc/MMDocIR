{"page": 0, "image_path": "doc_images/2311.16502v3_0.jpg", "ocr_text": "2311.16502v3 [cs.CL] 21 Dec 2023\n\n1V\n\nwx\n\nd\n\n‘\n\nMMMU: A Massive Multi-discipline Multimodal\nUnderstanding and Reasoning Benchmark for Expert AGI\n\nay\n\n'Xiang Yue\n\n, 7Yuansheng Ni* *Kai Zhang* *Tianyu Zheng?\n\n3Ruogqi Liu, 7Ge Zhang, *Samuel Stevens, ?Dongfu Jiang, 7Weiming Ren, *Yuxuan Sun,\n2Cong Wei, *Botao Yu, °Ruibin Yuan, ?Renliang Sun, “Ming Yin,\n’Boyuan Zheng, *Zhenzhu Yang, °Yibo Liu, ‘Wenhao Huang,\n3Huan Sun*, ?Yu Su“, ?Wenhu Chen**\n\n1JN.AI Research, University of Waterloo, “The Ohio State University, “Independent,\n5Carnegie Mellon University, “University of Victoria, Princeton University\n\nhttps://mmmu-benchmark.github.io/\n\nComprehensive Disciplines Heterogeneous Image Types\n\n& BSG\nMoy oo we\nBE @ A ¥ --\n\nDiagrams, Tables, Plots and Charts,\nPhotographs, Chemical Structures,\nPaintings, Medical Images, Sheet\nMusic, Geometric, Pathology images,\nMicroscopic Images, Comics, ...\n\nEngineering (26%) Art & oe n @\n(11%) @& Bip\n\nat a bs Business (14%)\n&oS gat\nScience (23%) Humanities E\n° A & Social Sci. (9%)\niS 6\n\n& &®@® SVSR\n\nMedicine (17%)\n\nExpert-level Skills Test\n\nExpert-level Visual Perception\n\nWa\" Ss\n=—3_5\n\nDomain Expertise,\nWorld, Linguistic,\nVisual Knowledge,...\n\nInterleaved Text and Images\n\nQuestion: You are — shown\nsubtraction <image 1>, T2 weighted\n<image 2> and Tl weighted axial\n<image 3> from a screening breast\nMRI. What is the etiology of the\nfinding in the left breast?\n\n<image 1>  <image2>  <image 3>\n\nLogical, Spatial\nCommonsense,\nMathematical,...\n\nFigure 1. Overview of the MMMU dataset. MMMU presents four challenges: 1) comprehensiveness: 11.5K college-level problems across six\nbroad disciplines and 30 college subjects; 2) highly heterogeneous image types; 3) interleaved text and images; 4) expert-level perception\n\nand reasoning rooted in deep subject knowledge.\n\nAbstract\n\nWe introduce MMMU: a new benchmark designed to eval-\nuate multimodal models on massive multi-discipline tasks\ndemanding college-level subject knowledge and deliberate\nreasoning. MMMU includes 11.5K meticulously collected\nmultimodal questions from college exams, quizzes, and text-\nbooks, covering six core disciplines: Art & Design, Busi-\nness, Science, Health & Medicine, Humanities & Social\nScience, and Tech & Engineering. These questions span\n30 subjects and 183 subfields, comprising 30 highly het-\nerogeneous image types, such as charts, diagrams, maps,\ntables, music sheets, and chemical structures. Unlike ex-\nisting benchmarks, MMMU focuses on advanced perception\nand reasoning with domain-specific knowledge, challenging\nmodels to perform tasks akin to those faced by experts. The\nevaluation of 14 open-source LMMs as well as the propri-\netary GPT-4V(ision) and Gemini highlights the substantial\n\n“Core Contributors. See the Author Contribution Statement for details.\ntba: xiangyue @in.ai; su.809 @osu.edu; wenhuchen @uwaterloo.ca\n\nchallenges posed by MMMU. Even the advanced GPT-4V and\nGemini Ultra only achieve accuracies of 56% and 59% re-\nspectively, indicating significant room for improvement. We\nbelieve MMMU will stimulate the community to build next-\ngeneration multimodal foundation models towards expert\nartificial general intelligence.\n\n1. Introduction\n\nRapid advances in large language models (LLMs) [1 1, 45,\n52] have sparked broad discussions on the controversial\nconcept of artificial general intelligence (AGI), often used\nto describe AI systems that perform on par or surpass hu-\nmans at most tasks [1, 7, 17, 24, 42, 44]. Candid and con-\nstructive discussions on AGI have been challenging due to a\nlack of shared operationalizable definitions. In an attempt to\nremedy this, Morris et al. [44] propose a leveled taxonomy\nfor AGI that centers around both generality (or breadth) and\nperformance (or depth). In the suggested taxonomy, Level\n3, or Expert AGI, marks a critical milestone. It denotes an\n", "vlm_text": "MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI \n1 Xiang Yue \\*† , Yuansheng Ni , 3 Kai Zhang , 4 Tianyu Zheng , 3 Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, 2 Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, 3 Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, 3 Huan Sun \\* , Yu Su \\*† , Wenhu Chen \\*† \n1 IN.AI Research, University of Waterloo, The Ohio State University, Independent, 5 Carnegie Mellon University, University of Victoria, Princeton University \nhttps://mmmu-benchmark.github.io/ \nThe image provides an overview of the MMMU (Multimodal Multidisciplinary University) dataset, highlighting four key challenges. \n\n1. **Comprehensive Disciplines**: The dataset includes 11.5K college-level problems across six broad disciplines with the following distribution:\n   - Engineering (26%)\n   - Art & Design (11%)\n   - Business (14%)\n   - Science (23%)\n   - Humanities & Social Sciences (9%)\n   - Medicine (17%)\n\n2. **Heterogeneous Image Types**: The dataset features a wide variety of image types, such as:\n   - Diagrams, Tables, Plots, and Charts\n   - Photographs, Chemical Structures, Paintings\n   - Medical Images, Sheet Music\n   - Geometric, Pathology images, Microscopic Images, Comics, etc.\n\n3. **Interleaved Text and Images**: The problems in the dataset involve both text and images interleaved together. An example question provided asks about the etiology of a finding in the left breast using MRI images labeled as <image 1>, <image 2>, and <image 3>.\n\n4. **Expert-level Skills Test**: The dataset tests expert-level visual perception and reasoning. It requires:\n   - **Perception**: Expert-level visual perception capabilities.\n   - **Knowledge**: Domain expertise, world knowledge, linguistic, and visual knowledge.\n   - **Reasoning**: Logical, spatial, commonsense, mathematical reasoning, etc. \n\nThe image uses visuals and icons to represent each discipline and aspect of the dataset, emphasizing the diverse and challenging nature of the MMMU dataset.\nAbstract \nWe introduce  MMMU : a new benchmark designed to eval- uate multimodal models on massive multi-discipline tasks demanding college-level subject knowledge and deliberate reasoning. MMMU  includes   $l l.5K$   meticulously collected multimodal questions from college exams, quizzes, and text- books, covering six core disciplines: Art & Design, Busi- ness, Science, Health & Medicine, Humanities & Social Science, and Tech & Engineering. These questions span 30 subjects and 183 subfields, comprising 30 highly het- erogeneous image types, such as charts, diagrams, maps, tables, music sheets, and chemical structures. Unlike ex- isting benchmarks,  MMMU  focuses on advanced perception and reasoning with domain-specific knowledge, challenging models to perform tasks akin to those faced by experts. The evaluation of 14 open-source LMMs as well as the propri- etary GPT-4V(ision) and Gemini highlights the substantial challenges posed by  MMMU . Even the advanced GPT-4V and Gemini Ultra only achieve accuracies of   $56\\%$   and  $59\\%$   re- spectively, indicating significant room for improvement. We believe  MMMU  will stimulate the community to build next- generation multimodal foundation models towards expert artificial general intelligence. \n\n1. Introduction \nRapid advances in large language models (LLMs) [ 11 ,  45 , 52 ] have sparked broad discussions on the controversial concept of artificial general intelligence (AGI), often used to describe AI systems that perform on par or surpass hu- mans at most tasks [ 1 ,  7 ,  17 ,  24 ,  42 ,  44 ]. Candid and con- structive discussions on AGI have been challenging due to a lack of shared operation aliz able definitions. In an attempt to remedy this, Morris et al. [ 44 ] propose a leveled taxonomy for AGI that centers around both  generality  (or breadth) and performance  (or depth). In the suggested taxonomy, Level 3, or  Expert AGI , marks a critical milestone. It denotes an "}
{"page": 1, "image_path": "doc_images/2311.16502v3_1.jpg", "ocr_text": "Art & Design\n\nBusiness\n\nScience\n\nQuestion: Among the following harmonic intervals, | Question: ... The graph shown is compiled from data Question: <image 1> The region bounded by the\n\nwhich one is constructed incorrectly?\n\nOptions: —=\naes Ga | between 805 and 82?\n(A) Major third <image 1> —\n(B) Diminished fifth <image 2> oo (\n. Sa | (Ao B) 0.2142\n1(@) r seventh <image 3> C)0.3571 (D)05\n(D) Diminished sixth <image 4> — (oa\n\nSubject: Music; Subfield: Music;\nImage Type: Sheet Music;\nDifficulty: Medium\n\nHealth & Medicine\n\nQuestion: You are shown subtraction <image 1>,\nT2 weighted <image 2> and T1 weighted axial\n<image 3> from a screening breast MRI. What is the\netiology of the finding in the left breast?\n\n(B) Hematoma\n\n(C) Fat necrosis (D) Silicone granuloma\n\nSubject: Clinical Medicine; Subfield: Clinical\n\nDifficulty: Medium\n\nOption:\n(A) Oppressor\n(B) Imperialist\n\nOptions:\n\nDifficulty: Hard Difficulty: Easy\n\ncollected by Gallup <image 1>. Find the probability\nthat the selected Emotional Health Index Score i\n\nSubject: Marketing; Subfield: Market\nResearch; Image Type: Plots and Charts;\n\nHumanities & Social Science\n\nQuestion: In the political\ncartoon, the United States is\nseen as fulfilling which of the\nfollowing roles? <image 1>\n\n(©) Savior (D) Isolationist\n\nSubject: History; Subfield: Modern\nRadiology; Image Type: Body Scans: MRI, CT.; | History; Image Type: Comics and Cartoons;\n\ngraph as shown above. Choose an integral\nexpression that can be used to find the area of R.\nOptions: ’ ws\n\n(A) 0° [F@) — g@)ldx\n(8) {o°1g@) — F@ldx\n© K1F@) - g@)ldx\n©) frlg@) = x@)ldx\nSubject: Math; Subfield: Calculus;\nImage Type: Mathematical Notations;\nDifficulty: Easy\n\nTech & Engineering\n\nQuestion: Find the VCE for the circuit shown in\n<image 1>. Neglect VBE\n\nAnswer: 3.75\nExplanation: ...IE = [(VEE) /\n(RE)] = [6 V) / (4 k-ohm)] =\n1.25 mA; VCE = VCC - IERL =\n10 V - (1.25 mA) 5 k-ohm;\nVCE = 10 V - 6.25 V =3.75V\n\n+\nSvarsv\n\nSubject: Electronics; Subfield: Analog\nelectronics; Image Type: Diagrams;\nDifficulty: Hard\n\nFigure 2. Sampled MMMU examples from each discipline. The questions and images need expert-level knowledge to understand and reason.\n\nAI system that reaches “at least 90th percentile of skilled\nadults” in a broad range of tasks, thus starting to achieve\n“the substitution threshold for machine intelligence in lieu\nof human labor” for many industries, leading to significant\nrisks of job displacement and economic disruption. There-\nfore, it is of both intellectual and societal importance to\nclosely monitor the progress towards Expert AGI.\n\nHow to create benchmarks for measuring Expert AGI?\nSince the definition is based on comparison with skilled\nadults, a natural starting point is college-level exams for\ndifferent disciplines, because those are designed to evalu-\nate skilled adults specialized in each discipline. This strat-\negy has been successfully adopted in benchmarks such as\nMMLU [19] and AGIEval [69], but only text-based ques-\ntions are considered, while human experts are capable of\nsolving multimodal problems. Meanwhile, large multi-\nmodal models (LMMs) that can understand both text and\nimages have been making a major stride towards more gen-\neral AI [8, 14, 27, 34, 58]. These LMMs have consistently\nexcelled in existing multimodal benchmarks [3, 18, 25, 31,\n36, 50, 61, 63]. For instance, CogVLM [55] achieves 85%\non VQA-v2 [18], 92% on ScienceQA-IMG [39], and 93%\non RefCOCO [23]. However, most existing multimodal\nbenchmarks focus on commonsense/daily knowledge rather\nthan expert-level domain knowledge and advanced reason-\ning. The closest one to our goal is ScienceQA [39]. While\nit covers diverse disciplines (breadth), the majority of the\nquestions are at the elementary to the middle school level,\nthus falling short in depth for benchmarking Expert AGI.\n\nTo this end, we introduce MMMU: a comprehensive\nbenchmark designed for college-level multi-discipline mul-\ntimodal understanding and reasoning. It features problems\nsourced from college exams, quizzes, and textbooks span-\nning six common disciplines: Art & Design, Business, Sci-\nence, Health & Medicine, Humanities & Social Science,\nand Tech & Engineering. MMMU consists of 11.5K carefully\nselected multimodal questions, which cover 30 diverse sub-\njects and 183 subfields, thus meeting the breadth goal.\nMoreover, many problems within MMMU require expert-level\nreasoning, such as applying “Fourier Transform” or “Equi-\nlibrium Theory” to derive the solution, thus meeting the\ndepth goal. MMMU also presents two unique challenges ab-\nsent in current benchmarks (Figure 1). Firstly, it covers di-\nverse image formats, from visual scenes like photographs\nand paintings to diagrams and tables, testing the perceptual\ncapabilities of LMMs. Secondly, MMMU features interleaved\ntext-image inputs. A model needs to jointly understand the\nimages and text, which often requires recalling deep subject\nknowledge, and conducting complex reasoning based on the\nunderstanding and knowledge to reach a solution.\n\nWe evaluate 14 open-source LMMs as well as the ad-\nvanced proprietary LMMs such as GPT-4V(ision) [46] on\nMMMU. Our key findings are summarized as follows:\n\n* MMMU presents significant challenges; notably, GPT-4V\nonly achieves an accuracy of 55.7%, indicating substan-\ntial room for improvement.\n\n¢ There is a pronounced disparity in performance between\nopen-source LMMs and GPT-4V. The highest-performing\n\n", "vlm_text": "The table contains questions across different subject areas, each having associated options or answers, images, and metadata. Here’s a breakdown of each section:\n\n1. **Art & Design**\n   - **Question**: Among the following harmonic intervals, which one is constructed incorrectly?\n   - **Options**: (A) Major third, (B) Diminished fifth, (C) Minor seventh, (D) Diminished sixth\n   - **Answer**: (C) Minor seventh\n   - **Subject**: Music\n   - **Subfield**: Music\n   - **Image Type**: Sheet Music\n   - **Difficulty**: Medium\n\n2. **Business**\n   - **Question**: The graph shown is compiled from data collected by Gallup. Find the probability that the selected Emotional Health Index Score is between 80.5 and 82.\n   - **Options**: (A) 0, (B) 0.2142, (C) 0.3571, (D) 0.5\n   - **Answer**: (C) 0.3571\n   - **Subject**: Marketing\n   - **Subfield**: Market Research\n   - **Image Type**: Plots and Charts\n   - **Difficulty**: Medium\n\n3. **Science**\n   - **Question**: The region bounded by the graph as shown above. Choose an integral expression that can be used to find the area of R.\n   - **Options**: (A) \\(\\int_0^{1.5} [f(x) - g(x)] dx\\), (B) \\(\\int_0^{1.5} [g(x) - f(x)] dx\\), (C) \\(\\int_0^2 [f(x) - g(x)] dx\\), (D) \\(\\int_0^2 [g(x) - x(x)] dx\\)\n   - **Answer**: (A) \\(\\int_0^{1.5} [f(x) - g(x)] dx\\)\n   - **Subject**: Math\n   - **Subfield**: Calculus\n   - **Image Type**: Mathematical Notations\n   - **Difficulty**: Easy\n\n4. **Health & Medicine**\n   - **Question**: You are shown subtraction, T2 weighted, and T1 weighted axial images from a screening breast MRI. What is the etiology of the finding in the left breast?\n   - **Options**: (A) Susceptibility artifact, (B) Hematoma, (C) Fat necrosis, (D) Silicone granuloma\n   - **Answer**: (C) Fat necrosis\n   - **Subject**: Clinical Medicine\n   - **Subfield**: Clinical Radiology\n   - **Image Type**: Body Scans: MRI, CT\n   -\nAI system that reaches “at least 90th percentile of skilled adults” in a broad range of tasks, thus starting to achieve “the substitution threshold for machine intelligence in lieu of human labor” for many industries, leading to significant risks of job displacement and economic disruption. There- fore, it is of both intellectual and societal importance to closely monitor the progress towards Expert AGI. \nHow to create benchmarks for measuring Expert AGI? Since the definition is based on comparison with  skilled adults , a natural starting point is college-level exams for different disciplines, because those are designed to evalu- ate  skilled adults  specialized in each discipline. This strat- egy has been successfully adopted in benchmarks such as MMLU [ 19 ] and AGIEval [ 69 ], but only text-based ques- tions are considered, while human experts are capable of solving multimodal problems. Meanwhile, large multi- modal models (LMMs) that can understand both text and images have been making a major stride towards more gen- eral AI [ 8 ,  14 ,  27 ,  34 ,  58 ]. These LMMs have consistently excelled in existing multimodal benchmarks [ 3 ,  18 ,  25 ,  31 , 36 ,  50 ,  61 ,  63 ]. For instance,   $\\mathbf{\\mathrm{CogVLM}}$   [ 55 ] achieves  $85\\%$  on VQA-v2 [ 18 ],  $92\\%$   on ScienceQA-IMG [ 39 ], and    $93\\%$  on RefCOCO [ 23 ]. However, most existing multimodal benchmarks focus on commonsense/daily knowledge rather than expert-level domain knowledge and advanced reason- ing. The closest one to our goal is ScienceQA [ 39 ]. While it covers diverse disciplines ( breadth ), the majority of the questions are at the elementary to the middle school level, thus falling short in  depth  for benchmarking Expert AGI. \nTo this end, we introduce  MMMU : a comprehensive benchmark designed for college-level multi-discipline mul- timodal understanding and reasoning. It features problems sourced from college exams, quizzes, and textbooks span- ning six common disciplines: Art & Design, Business, Sci- ence, Health & Medicine, Humanities & Social Science, and Tech & Engineering.  MMMU  consists of  11 . 5 K carefully selected multimodal questions, which cover  30  diverse sub- jects and  183  subfields, thus meeting the  breadth  goal. Moreover, many problems within  MMMU  require expert-level reasoning, such as applying “Fourier Transform” or “Equi- librium Theory” to derive the solution, thus meeting the depth  goal.  MMMU  also presents two unique challenges ab- sent in current benchmarks ( Figure 1 ). Firstly, it covers di- verse image formats, from visual scenes like photographs and paintings to diagrams and tables, testing the perceptual capabilities of LMMs. Secondly,  MMMU  features interleaved text-image inputs. A model needs to jointly understand the images and text, which often requires recalling deep subject knowledge, and conducting complex reasoning based on the understanding and knowledge to reach a solution. \nWe evaluate  14  open-source LMMs as well as the ad- vanced proprietary LMMs such as GPT-4V(ision) [ 46 ] on MMMU . Our key findings are summarized as follows:\n\n •  MMMU  presents significant challenges; notably, GPT-4V only achieves an accuracy of    $55.7\\%$  , indicating substan- tial room for improvement.\n\n • There is a pronounced disparity in performance between open-source LMMs and GPT-4V. The highest-performing "}
{"page": 2, "image_path": "doc_images/2311.16502v3_2.jpg", "ocr_text": "Art & Design (11%)\n\n“> Art (266, 2.3%)\n\nDrawing, Painting, Photography...\n“ Design (204, 1.8%)\n\nDesign History, Graphic Design...\n+ Music (369, 3.2%)\n\n“ Art Theory (464, 4.0%)\nArt History, Art Criticism...\n\nBusiness (14%\n\n+ Accounting (415, 3.6%)\nFinancial Accounting, Investment...\n+ Economics (302, 2.6%)\nMacroeconomics, Econometrics...\n\n+ Finance (390, 3.4%)\n\nFinancial Marketing, Corporate Fin...\n> Manage (280, 2.4%)\nManagement Models, Cost Manage...\n“ Marketing (216, 1.9%)\nMarket Research\n\nScience (23%)\n\nBiology (380, 3.3%)\nPhysiology, Genetics Microbiology,\nEvolution, Cell Biology,\n\nBotany, Ecology...\n\n“ Chemistry (638, 5.5%)\nInorganic Chemistry, Organic\nChemistry, Physical\n\nChemistry, Inorganic Chemistry...\n* Geography (600, 5.2%)\nGeotechnical Engineering, Human\nGeography, Physical Geography...\n+ Math (540, 4.7%)\n\nCalculus, Probability and Statistics,\nLinear Algebra, Geometry, Logic,\nProbability and Statistics...\n\n“ Physics (443, 3.8%)\nClassical Mechanics, Optics,\n\nElectromagnetism, Nuclear Physics,\nStatistical Mechanics...\n\nBE\n\n¥\n\n+o\nBent\n\nN\n\n&\n€a\n\nHealth & Medicine (17%)\n“ Basic Med. Sci. (361, 3.1%)\nAnatomy, Neurosciences...\n\n“ Clinical Med. (360, 3.12%)\nCirculatory, Dental, Respiratory...\n+ Diagnostics (197, 1.7%)\nPathology, Electrocardiography...\n“ Pharmacy (465, 4.0%)\nMedicinal Chemistry, Biochemistry\n“ Public Health (544, 4.7%)\nEpidemiology, Biostatistics...\n\nTech & Engineering (26%)\n“ Agriculture (422, 2.8%)\n\nPlant Pathology, Animal Nutrition,\nAdvanced Animal Genetics\n\n“ Architecture Eng.(586, 5.1%)\nSurveying and Mapping, Structural\nEngineering, Civil Engineering...\n\n“+ Computer Sci. (406, 3.5%)\nData Structure and Algorithm,\nComputer Network, Databases...,\n\n“ Electronics (291, 2.5%)\nElectrical Circuit, Signal Processing,\nAnalog electronics, Digital Electronics\n\n+» Energy Power (467, 4.0%)\nFluid Mechanics, Heat Transfer... 9\n“* Materials (493, 4.3%) Ba\n\nMechanics Materials, Materials Sci...\n4\noO)\n\nSt\n\nMf\nty\n\nPB fr 2\n\nHumanities & Social Sci. (9%)\nHistory (313, 2.71%)\nWorld History, Modern History...\n“ Literature (147, 1.27%)\nPoetry, Fiction, Children’s Literature...\n\n“ Psychology (340, 2.94%) &\nSocial Psychology, Personality Psy...\n\n* Sociology (287, 2.48%) Q\nSociology Theory, Politics...\n\n“+ Mechanical Eng. (464, 4.0%)\nMechanical Design, Fluid Dynamics,\nFluid Dynamics, Control Systems...\n\nFigure 3. MMMU contains 11.5K multimodal questions covering six broad disciplines, 30 subjects, and 183 subfields.\n\nopen-source models, such as BLIP2-FLAN-TS5-XXL and\nLLaVA-1.5, achieve approximately 34% in accuracy.\n\n¢« LLMs augmented with optical character recognition\n(OCR) or generated captions do not see notable improve-\nment, indicating that MMMU necessitates deeper joint in-\nterpretation of images and text.\n\n¢ In disciplines such as Art & Design and Humanities &\nSocial Science, where visual data is less complex, mod-\nels exhibit higher performance. In contrast, Business, Sci-\nence, Health & Medicine, and Tech & Engineering, which\npresent more complex visual data and require intricate\nreasoning, see relatively lower model performance.\n\n* Our error analysis on 150 error cases of GPT-4V reveals\nthat 35% of errors are perceptual, 29% stem from a lack\nof knowledge, and 26% are due to flaws in the reasoning\nprocess. These findings underscore the challenges of the\nMMMU benchmark and point towards areas needing further\nresearch and model enhancement.\n\nOur aim with MMMU is to push the boundaries of what\nLMMs can achieve. We believe it will prove instrumental in\ndeveloping next-generation multimodal foundation models\nand monitoring the progress towards Expert AGI. We shall\ncaution that MMMU is not a sufficient test for Expert AGI,\nas per the definition [44], because there lacks a direct map-\nping between performance on MMMU and “90th percentile of\nskilled adults,” nor are college exams the only tasks an AGI\nshall tackle. However, we believe it should be necessary for\nan Expert AGI to achieve strong performance on MMMU to\ndemonstrate their broad and deep subject knowledge as well\nas expert-level understanding and reasoning capabilities.\n\n2. Related Work\n\nMultimodal Pre-Training. In recent years, rapid progress\nhas been made in multimodal pre-training, which aims\n\nto jointly encode vision and language in a fusion model.\nLXMERT [51], UNITER [9], VinVL [64], Oscar [29],\nVilBert [38], and VLP [70] are among the earliest work\nto train universal vision-language models to tackle many\nmultimodal tasks. This work relies on pre-trained visual\nrepresentations like Faster RCNN features [49] to mini-\nmize the training sample complexity. Later on, CLIP [48],\nALIGN [22], SimVLM [56], CoCa [62], Flamingo [2],\nBLIP-2 [27], and Fuyu [6] (inter alia) have been proposed to\ntrain visual representation using ViT [15] from scratch with\nmassive amount of web data. These models have achieved\ngreat success on existing VQA and captioning tasks, which\nrequire less knowledge and reasoning.\n\nMultimodal Instruction Tuning. Inspired by open-\nsource instruction-tuned LLMs like FLAN-T5 [12] and Vi-\ncuna [10], models like LLaVA [34, 35] and MiniGPT-4 [71]\nutilized open-source resources, to improve the instruction-\nfollowing capabilities of LMMs. The evolutionary trajec-\ntory of LMMs has also led to subsequent advancements\naimed at improving the quantity and quality of visual in-\nstruction data. Models such as LLaMA-Adapter [16, 65],\nmPlug-OWL [59, 60], SVIT [66], LRV-Instruction [33],\nand InstructBLIP [14] exemplify these developments. An-\nother pivotal aspect of LMM research revolves around mul-\ntimodal in-context learning and the management of in-\nterleaved text and image examples. This area has been\nexplored in depth by models such as Flamingo [2] and\nOpenFlamingo [4], Otter [26], M3IT [28], MetaVL [43],\nSparkles [20], and MMICL [67]. These models have sig-\nnificantly contributed to the ongoing advancements in mul-\ntimodal training and instruction-following capabilities.\nLMM Benchmarks. With the surge of multi-modal pre-\ntraining and instruction tuning, the prior single-task eval-\nuation benchmarks like VQA [3, 18], OK-VQA [41],\nMSCOCO [31], GQA [21], etc., have become insufficient\n", "vlm_text": "The image is a chart displaying the distribution of 11.5K multimodal questions across six broad disciplines, 30 subjects, and 183 subfields. Here’s the breakdown:\n\n1. **Art & Design (11%)**\n   - Art: 2.3%\n   - Design: 1.8%\n   - Music: 3.2%\n   - Art Theory: 4.0%\n\n2. **Business (14%)**\n   - Accounting: 3.6%\n   - Economics: 2.6%\n   - Finance: 3.4%\n   - Management: 2.4%\n   - Marketing: 1.9%\n\n3. **Science (23%)**\n   - Biology: 3.3%\n   - Chemistry: 5.5%\n   - Geography: 5.2%\n   - Math: 4.7%\n   - Physics: 3.8%\n\n4. **Health & Medicine (17%)**\n   - Basic Medical Science: 3.1%\n   - Clinical Medicine: 3.12%\n   - Diagnostics: 1.7%\n   - Pharmacy: 4.0%\n   - Public Health: 4.7%\n\n5. **Humanities & Social Sciences (9%)**\n   - History: 2.71%\n   - Literature: 1.27%\n   - Psychology: 2.94%\n   - Sociology: 2.48%\n\n6. **Tech & Engineering (26%)**\n   - Agriculture: 2.8%\n   - Architecture Engineering: 5.1%\n   - Computer Science: 3.5%\n   - Electronics: 2.5%\n   - Energy Power: 4.0%\n   - Materials: 3.4%\n   - Mechanical Engineering: 4.0%\n\nEach category includes subfields with associated percentages indicating the portion of questions related to each subject or discipline. Icons are used to visually represent each area.\nopen-source models, such as BLIP2-FLAN-T5-XXL and LLaVA-1.5, achieve approximately  $34\\%$   in accuracy.\n\n • LLMs augmented with optical character recognition (OCR) or generated captions do not see notable improve- ment, indicating that  MMMU  necessitates deeper joint in- ter preta tion of images and text.\n\n • In disciplines such as Art & Design and Humanities & Social Science, where visual data is less complex, mod- els exhibit higher performance. In contrast, Business, Sci- ence, Health & Medicine, and Tech & Engineering, which present more complex visual data and require intricate reasoning, see relatively lower model performance.\n\n • Our error analysis on  150  error cases of GPT-4V reveals that    $35\\%$   of errors are perceptual,    $29\\%$   stem from a lack of knowledge, and    $26\\%$   are due to flaws in the reasoning process. These findings underscore the challenges of the MMMU  benchmark and point towards areas needing further research and model enhancement. \nOur aim with  MMMU  is to push the boundaries of what LMMs can achieve. We believe it will prove instrumental in developing next-generation multimodal foundation models and monitoring the progress towards Expert AGI. We shall caution that  MMMU  is not a  sufficient  test for Expert AGI, as per the definition [ 44 ], because there lacks a direct map- ping between performance on  MMMU  and “90th percentile of skilled adults,” nor are college exams the only tasks an AGI shall tackle. However, we believe it should be  necessary  for an Expert AGI to achieve strong performance on  MMMU  to demonstrate their broad and deep subject knowledge as well as expert-level understanding and reasoning capabilities. \n2. Related Work \nMultimodal Pre-Training.  In recent years, rapid progress has been made in multimodal pre-training, which aims to jointly encode vision and language in a fusion model. LXMERT [ 51 ], UNITER [ 9 ], VinVL [ 64 ], Oscar [ 29 ], VilBert [ 38 ], and VLP [ 70 ] are among the earliest work to train universal vision-language models to tackle many multimodal tasks. This work relies on pre-trained visual representations like Faster RCNN features [ 49 ] to mini- mize the training sample complexity. Later on, CLIP [ 48 ], ALIGN [ 22 ], SimVLM [ 56 ], CoCa [ 62 ], Flamingo [ 2 ], BLIP-2 [ 27 ], and Fuyu [ 6 ] (inter alia) have been proposed to train visual representation using ViT [ 15 ] from scratch with massive amount of web data. These models have achieved great success on existing VQA and captioning tasks, which require less knowledge and reasoning. \n\nMultimodal Instruction Tuning. Inspired by open- source instruction-tuned LLMs like FLAN-T5 [ 12 ] and Vi- cuna [ 10 ], models like LLaVA [ 34 ,  35 ] and MiniGPT-4 [ 71 ] utilized open-source resources, to improve the instruction- following capabilities of LMMs. The evolutionary trajec- tory of LMMs has also led to subsequent advancements aimed at improving the quantity and quality of visual in- struction data. Models such as LLaMA-Adapter [ 16 ,  65 ], mPlug-OWL [ 59 ,  60 ], SVIT [ 66 ], LRV-Instruction [ 33 ], and Instruct BLIP [ 14 ] exemplify these developments. An- other pivotal aspect of LMM research revolves around mul- timodal in-context learning and the management of in- terleaved text and image examples. This area has been explored in depth by models such as Flamingo [ 2 ] and Open Flamingo [ 4 ], Otter [ 26 ], M3IT [ 28 ], MetaVL [ 43 ], Sparkles [ 20 ], and MMICL [ 67 ]. These models have sig- nificantly contributed to the ongoing advancements in mul- timodal training and instruction-following capabilities. \nLMM Benchmarks.  With the surge of multi-modal pre- training and instruction tuning, the prior single-task eval- uation benchmarks like VQA [ 3 ,  18 ], OK-VQA [ 41 ], MSCOCO [ 31 ], GQA [ 21 ], etc., have become insufficient "}
{"page": 3, "image_path": "doc_images/2311.16502v3_3.jpg", "ocr_text": "Statistics Number\nTotal Questions 11550\nTotal Disciplines/Subjects/Subfields 6/30/183\nImage Types 30\n\nDev: Validation: Test\nDifficulties (Easy: Medium: Hard)\n\n150:900:10500\n28% :45%:27%\n\n10861 (94.03%)\n689 (5.97%)\n\n2035 (17.62%)\n\n11264 (97.52%)\n2006 (17.81%)\n4159 (36.92%)\n5679 (50.42%)\n389 (3.37%)\n\nMultiple-choice Questions\nOpen Questions\n\nQuestions with an Explanation\n\nImage in the Question\n* Images at the beginning\n* Images in the middle\n* Images at the end\nImage in Options\n\nExample with Multiple Images 854 (7.39%)\nAverage question length 59.33\nAverage option length 9.17\nAverage explanation length 07.92\n\nTable 1. Key statistics of the MMMU benchmark.\n\nto holistically evaluate LMMs’ general multimodal percep-\ntion and reasoning abilities. Therefore, numerous all-round\nbenchmarks have been established to assess different facets\nof LMMs. These benchmarks cover a wide spectrum of\nspecific skills of LMMs, from Optical Character Recogni-\ntion (OCR) as seen in the study by [37], to adversarial ro-\nbustness [68] and hallucination [13, 32], e.g., POPE [30]\nand HaELM [54]. More holistic evaluations have been con-\nducted as well, such as LAMM [61], LVLM-eHub [57],\nSEED [25], MMBench [36], and MM-Vet [63]. These\nbenchmarks still largely focus on relatively basic percep-\ntion abilities without requiring expert-level domain knowl-\nedge and deliberate reasoning. More recently, MathVista\n[40] presents a collection of visually challenging questions;\nhowever, its scope is limited exclusively to the mathemat-\nical domain. MMMU is highly different from these bench-\nmarks by collecting more difficult expert-level problems\nthat cover 30 different subjects and require nuanced percep-\ntion, recalling domain-specific knowledge to perform step-\nby-step reasoning to derive the solution. In line with the mo-\ntivation of our study, concurrently, GAIA [42] introduces\n466 questions that test fundamental abilities of models such\nas reasoning, multimodality handling, or tool use.\n\n3. The MMMU Benchmark\n3.1. Overview of MMMU\n\nWe introduce the Massive Multi-discipline Multimodal Un-\nderstanding and Reasoning (MMMU) benchmark, a novel\nbenchmark meticulously curated to assess the expert-level\nmultimodal understanding capability of foundation models\n\nacross a broad scope of tasks. Covering 30 subjects across\n6 disciplines, including Art, Business, Health & Medicine,\nScience, Humanities & Social Science, and Tech & Engi-\nneering, and over 183 subfields. The detailed subject cover-\nage and statistics are detailed in Figure 3. The questions\nin our benchmark were manually collected by a team of\n50 college students (including coauthors) from various dis-\nciplines and subjects, drawing from online sources, text-\nbooks, and lecture materials.\n\nMMMU, constituting 11.5K questions, is divided into a\nfew-shot development set, a validation set, and a test set.\nThe few-shot development set includes 5 questions per sub-\nject, and the validation set, useful for hyperparameter selec-\ntion, contains approximately 900 questions, while the test\nset comprises 10.5K questions. MMMU is designed to mea-\nsure three essential skills in LMMs: perception, knowledge,\nand reasoning. Our aim is to evaluate how well these mod-\nels can not only perceive and understand information across\ndifferent modalities but also apply reasoning with subject-\nspecific knowledge to derive the solution.\n\nOur MMMU benchmark introduces four key challenges\nto multimodal foundation models, as detailed in Figure 1.\nAmong these, we particularly highlight the challenge stem-\nming from the requirement for both expert-level visual\nperceptual abilities and deliberate reasoning with subject-\nspecific knowledge. This challenge is vividly illustrated\nthrough our tasks, which not only demand the processing\nof various heterogeneous image types but also necessitate\na model’s adeptness in using domain-specific knowledge to\ndeeply understand both the text and images and to reason.\nThis goes significantly beyond basic visual perception, call-\ning for an advanced approach that integrates advanced mul-\ntimodal analysis with domain-specific knowledge.\n\n3.2. Data Curation Process\n\nData Collection. Our benchmark collection takes three\nstages. Firstly, we go through the common university ma-\njors to decide what subjects should be included in our\nbenchmark. The selection is based on the principle that vi-\nsual inputs should be commonly adopted in the subjects to\nprovide valuable information. Through this principle, we\ntule out a few subjects like law and linguistics because it\nis difficult to find enough relevant multimodal problems in\nthese subjects. Consequently, we select 30 subjects from six\ndifferent disciplines. In the second stage, we recruit over\n50 university students, including co-authors, specializing in\nthese majors as annotators to assist in question collection.\nThey collect multimodal questions from major textbooks\nand online resources, creating new questions based on their\nexpertise where necessary. The annotators are instructed to\nadhere to copyright and license regulations, avoiding data\nfrom sites prohibiting copy and redistribution. Given the\narising data contamination concerns of foundation models,\n", "vlm_text": "The table presents statistics on a dataset of questions:\n\n- **Total Questions**: 11,550\n- **Total Disciplines/Subjects/Subfields**: 6/30/183\n- **Image Types**: 30\n- **Dev:Validation:Test Split**: 150:900:10,500\n- **Difficulties (Easy: Medium: Hard)**: 28% : 45% : 27%\n\n**Question Formats**:\n- **Multiple-choice Questions**: 10,861 (94.03%)\n- **Open Questions**: 689 (5.97%)\n- **Questions with an Explanation**: 2,035 (17.62%)\n\n**Image Usage**:\n- **Questions with Images**: 11,264 (97.52%)\n  - Images at the beginning: 2,006 (17.81%)\n  - Images in the middle: 4,159 (36.92%)\n  - Images at the end: 5,679 (50.42%)\n- **Image in Options**: 389 (3.37%)\n- **Examples with Multiple Images**: 854 (7.39%)\n\n**Average Lengths**:\n- **Question**: 59.33 characters\n- **Option**: 9.17 characters\n- **Explanation**: 107.92 characters\nto holistic ally evaluate LMMs’ general multimodal percep- tion and reasoning abilities. Therefore, numerous all-round benchmarks have been established to assess different facets of LMMs. These benchmarks cover a wide spectrum of specific skills of LMMs, from Optical Character Recogni- tion (OCR) as seen in the study by [ 37 ], to adversarial ro- bustness [ 68 ] and hallucination [ 13 ,  32 ], e.g., POPE [ 30 ] and HaELM [ 54 ]. More holistic evaluations have been con- ducted as well, such as LAMM [ 61 ], LVLM-eHub [ 57 ], SEED [ 25 ], MMBench [ 36 ], and MM-Vet [ 63 ]. These benchmarks still largely focus on relatively basic percep- tion abilities without requiring expert-level domain knowl- edge and deliberate reasoning. More recently, MathVista [ 40 ] presents a collection of visually challenging questions; however, its scope is limited exclusively to the mathemat- ical domain.  MMMU  is highly different from these bench- marks by collecting more difficult expert-level problems that cover 30 different subjects and require nuanced percep- tion, recalling domain-specific knowledge to perform step- by-step reasoning to derive the solution. In line with the mo- tivation of our study, concurrently, GAIA [ 42 ] introduces 466 questions that test fundamental abilities of models such as reasoning, multi modality handling, or tool use. \n3. The MMMU Benchmark \n3.1. Overview of MMMU \nWe introduce the Massive Multi-discipline Multimodal Un- der standing and Reasoning ( MMMU ) benchmark, a novel benchmark meticulously curated to assess the expert-level multimodal understanding capability of foundation models across a broad scope of tasks. Covering 30 subjects across 6 disciplines, including Art, Business, Health & Medicine, Science, Humanities & Social Science, and Tech & Engi- neering, and over 183 subfields. The detailed subject cover- age and statistics are detailed in  Figure 3 . The questions in our benchmark were manually collected by a team of 50 college students (including coauthors) from various dis- ciplines and subjects, drawing from online sources, text- books, and lecture materials. \n\nMMMU , constituting 11.5K questions, is divided into a few-shot development set, a validation set, and a test set. The few-shot development set includes 5 questions per sub- ject, and the validation set, useful for hyper parameter selec- tion, contains approximately 900 questions, while the test set comprises   $10.5\\mathrm{K}$   questions.  MMMU  is designed to mea- sure three essential skills in LMMs: perception, knowledge, and reasoning. Our aim is to evaluate how well these mod- els can not only perceive and understand information across different modalities but also apply reasoning with subject- specific knowledge to derive the solution. \nOur  MMMU  benchmark introduces four key challenges to multimodal foundation models, as detailed in  Figure 1 . Among these, we particularly highlight the challenge stem- ming from the requirement for both expert-level visual perceptual abilities and deliberate reasoning with subject- specific knowledge. This challenge is vividly illustrated through our tasks, which not only demand the processing of various heterogeneous image types but also necessitate a model’s adeptness in using domain-specific knowledge to deeply understand both the text and images and to reason. This goes significantly beyond basic visual perception, call- ing for an advanced approach that integrates advanced mul- timodal analysis with domain-specific knowledge. \n3.2. Data Curation Process \nData Collection. Our benchmark collection takes three stages. Firstly, we go through the common university ma- jors to decide what subjects should be included in our benchmark. The selection is based on the principle that vi- sual inputs should be commonly adopted in the subjects to provide valuable information. Through this principle, we rule out a few subjects like law and linguistics because it is difficult to find enough relevant multimodal problems in these subjects. Consequently, we select 30 subjects from six different disciplines. In the second stage, we recruit over 50 university students, including co-authors, specializing in these majors as annotators to assist in question collection. They collect multimodal questions from major textbooks and online resources, creating new questions based on their expertise where necessary. The annotators are instructed to adhere to copyright and license regulations, avoiding data from sites prohibiting copy and redistribution. Given the arising data contamination concerns of foundation models, "}
{"page": 4, "image_path": "doc_images/2311.16502v3_4.jpg", "ocr_text": "Depth (Reasoning) Dataset Size Images Format Source Answer\n\nok VQA >I1M V I+T Annotated Open\n\nGQA >I1M V I+T Synthesized Open\n\nMMMU VisWiz 32K Vv 1+T Annotated Open\n\n> IN 0 TextVQA 45K oc 1+T Annotated MC\n\n(e) OKVQA 14K V+OC 1+T Annotated Open\n\noA SEED 19K V+OC 1+T Annotated MC\n\n@ © Breadth (Knowledge) MMBench 3K V+OC 1+T Repurposed MC\n\n> MM-Vet 0.2K V+OC 1+T Repurposed MC\n\n@ VQA [@) GQA © VisWiz ScienceQA 6K 5 Types 1+T Textbooks MC\nA TextVQA IX MMBench @ SEED Textbooks, G0. /\n\n: MMMU 11.5K  30Types Interleaved Internet,\n- OKVQA\n© mMevet ©) Sciencega A Q Ammotrea MC\n\nFigure 4. The comparison between MMMU and other existing benchmarks. MMMU excels in both its breadth to cover a wide range of\ndisciplines and its depth to test LMMs’ reasoning abilities. In the image format, V means visual input, OC means optical characters, MC\nmeans multi-choice. Repurposed means the benchmark is a compilation of prior datasets.\n\nthe annotators are advised to select questions without im-\nmediately available answers, such as those with answers in\nseparate documents or at the end of textbooks. This process\nresults in a diverse collection of 13K questions from various\nsources. The detailed annotation protocol is in Appendix A.\nData Quality Control. To further control the quality of our\ndata, we perform two steps of data cleaning. In the first\nstage, lexical overlap and source URL similarity are em-\nployed to identify potential duplicate problems. These sus-\npected duplicates were then reviewed by the authors to iden-\ntify and eliminate any duplications. The second stage in-\nvolves distributing the problems among different co-authors\nfor format and typo checking. This step requires authors to\nensure adherence to a standardized format, undertaking nec-\nessary corrections where deviations are found. In the third\nand final stage, the authors categorize the problems into four\ndifficulty levels: very easy, easy, medium, and hard. Ap-\nproximately 10% of the problems, classified as very easy\nand not aligning with our design criteria due to their sim-\nplistic nature, are excluded from the benchmark. This rig-\norous process plays a crucial role in maintaining the quality\nand difficulty of the problem set.\n\n3.3. Comparisons with Existing Benchmarks\n\nTo further distinguish the difference between MMMU and\nother existing ones, we elaborate the benchmark details in\nFigure 4. From the breadth perspective, the prior bench-\nmarks are heavily focused on daily knowledge and com-\nmon sense. The covered image format is also limited. Our\nbenchmark aims to cover college-level knowledge with 30\nimage formats including diagrams, tables, charts, chemi-\ncal structures, photos, paintings, geometric shapes, music\nsheets, medical images, etc. In the depth aspect, the previ-\nous benchmarks normally require commonsense knowledge\nor simple physical or temporal reasoning. In contrast, our\n\nbenchmark requires deliberate reasoning with college-level\nsubject knowledge.\n\n4. Experiments\n\nWe evaluate various models including LLMs and LMMs.\nIn each type, we consider both closed- and open-source\nmodels. Our evaluation is conducted under a zero-shot set-\nting to assess the capability of models to generate accurate\nanswers without fine-tuning or few-shot demonstrations on\nour benchmark. For all models, we use the default prompt\nprovided by each model for multi-choice or open QA, if\navailable. If models do not provide prompts for task types\nin MMMU, we conduct prompt engineering on the validation\nset and use the most effective prompt for the zero-shot setup\nin the main experiments. We also report the few-shot results\nof some selected models in the Appendix. All experiments\nare conducted with NVIDIA A100 GPUs.\n\n4.1. Baselines\n\nLMMs. We consider various large multimodal models. By\ndefault, for each model family, we use the latest, largest,\nand best-performing available checkpoint to date. (i) Kos-\nmos2 [47] is pre-trained to ground fine-grained visual ob-\njects with texts and to follow instructions. With only 1.6B\nmodel size, Kosmos? is able to achieve comparable or better\nperformance with Flamingo-9B [2] on VQA and caption-\ning tasks. (ii) LLaMA-Adapter2 [16] fine-tunes Llama [52]\nin a parameter-efficient way and utilizes visual encoder\nCLIP [48] and modular experts such as Optical Character\nRecognition (OCR) to capture more image information for\nlater better visual understanding. (iii) BLIP-2 [27] intro-\nduces light-weight learnable visual queries to bridge the\nfrozen CLIP ViT [48] and FLAN-TS [12]. (iv) Starting\nfrom the parameters from BLIP-2, InstructBLIP [14] is fur-\n", "vlm_text": "The image is a dual representation comparing the MMMU benchmark with other benchmarks. \n\nOn the left, a graph shows MMMU excelling in depth (reasoning) and breadth (knowledge) compared to other benchmarks like VQA, GQA, VisWiz, and others. \n\nOn the right, a table lists datasets with details:\n\n- **Columns**: Dataset, Size, Images, Format, Source, Answer\n- **Datasets**: VQA, GQA, VisWiz, TextVQA, OKVQA, SEED, MMBench, MM-Vet, ScienceQA, MMMU\n- **Formats**: V (Visual), OC (Optical Characters), MC (Multiple Choice), \"I+T\" (Interleaved text and image)\n  \nMMMU is highlighted for having a broad range of image types and being sourced from textbooks and the internet, with open and multiple-choice answers available.\nthe annotators are advised to select questions without im- mediately available answers, such as those with answers in separate documents or at the end of textbooks. This process results in a diverse collection of 13K questions from various sources. The detailed annotation protocol is in Appendix A. Data Quality Control.  To further control the quality of our data, we perform two steps of data cleaning. In the first stage, lexical overlap and source URL similarity are em- ployed to identify potential duplicate problems. These sus- pected duplicates were then reviewed by the authors to iden- tify and eliminate any duplications. The second stage in- volves distributing the problems among different co-authors for format and typo checking. This step requires authors to ensure adherence to a standardized format, undertaking nec- essary corrections where deviations are found. In the third and final stage, the authors categorize the problems into four difficulty levels: very easy, easy, medium, and hard. Ap- proximate ly   $10\\%$   of the problems, classified as very easy and not aligning with our design criteria due to their sim- plistic nature, are excluded from the benchmark. This rig- orous process plays a crucial role in maintaining the quality and difficulty of the problem set. \n3.3. Comparisons with Existing Benchmarks \nTo further distinguish the difference between  MMMU  and other existing ones, we elaborate the benchmark details in Figure 4 . From the  breadth  perspective, the prior bench- marks are heavily focused on daily knowledge and com- mon sense. The covered image format is also limited. Our benchmark aims to cover college-level knowledge with 30 image formats including diagrams, tables, charts, chemi- cal structures, photos, paintings, geometric shapes, music sheets, medical images, etc. In the  depth  aspect, the previ- ous benchmarks normally require commonsense knowledge or simple physical or temporal reasoning. In contrast, our benchmark requires deliberate reasoning with college-level subject knowledge. \n\n4. Experiments \nWe evaluate various models including LLMs and LMMs. In each type, we consider both closed- and open-source models. Our evaluation is conducted under a  zero-shot  set- ting to assess the capability of models to generate accurate answers without fine-tuning or few-shot demonstrations on our benchmark. For all models, we use the default prompt provided by each model for multi-choice or open QA, if available. If models do not provide prompts for task types in  MMMU , we conduct prompt engineering on the validation set and use the most effective prompt for the zero-shot setup in the main experiments. We also report the few-shot results of some selected models in the Appendix. All experiments are conducted with NVIDIA A100 GPUs. \n4.1. Baselines \nLMMs.  We consider various large multimodal models. By default, for each model family, we use the latest, largest, and best-performing available checkpoint to date.  (i)  Kos- mos2 [ 47 ] is pre-trained to ground fine-grained visual ob- jects with texts and to follow instructions. With only 1.6B model size, Kosmos2 is able to achieve comparable or better performance with Flamingo-9B [ 2 ] on VQA and caption- ing tasks.  (ii)  LLaMA-Adapter2 [ 16 ] fine-tunes Llama [ 52 ] in a parameter-efficient way and utilizes visual encoder CLIP [ 48 ] and modular experts such as Optical Character Recognition (OCR) to capture more image information for later better visual understanding.  (iii)  BLIP-2 [ 27 ] intro- duces light-weight learnable visual queries to bridge the frozen CLIP ViT [ 48 ] and FLAN-T5 [ 12 ].  $(i\\nu)$   Starting from the parameters from BLIP-2, Instruct BLIP [ 14 ] is fur- "}
{"page": 5, "image_path": "doc_images/2311.16502v3_5.jpg", "ocr_text": "Validation Test Art & Business Science Health & Human. & Tech &\nOverall Overall Design Medicine _ Social Sci. Eng.\n(900) (10,500) (1,163) (1,428) (2,426) (1,752) (947) (2,784)\nRandom Choice 22.1 23.9 24.1 24.9 21.6 25.3 22.8 24.8\nFrequent Choice 26.8 25.8 26.7 28.4 24.0 24.4 25.2 26.5\nLarge Multimodal Models (LMMs): Text + Image as Input\nOpenFlamingo2-9B [4] 28.7 26.3 317 23.5 26.3 26.3 27.9 25.1\nKosmos2 [47] 24.4 26.6 28.8 23.7 26.6 27.2 26.3 26.8\nFuyu-8B [6] 27.9 274 29.9 27.0 25.6 27.0 32.5 26.4\nMiniGPT4-Vicuna-13B [71] 26.8 27.6 30.2 27.0 26.2 26.9 30.9 27.2\nLLaMA-Adapter2-7B [65] 29.8 27.7 35.2 25.4 25.6 30.0 29.1 25.7\nOtter [26] 32.2 29.1 37.4 24.0 24.1 29.6 35.9 30.2\nCogVLM [55] 32.1 30.1 38.0 25.6 25.1 31.2 41.5 28.9\nInstructBLIP-T5-XL [14] 32.9 30.6 43.3 25.2 25.2 29.3 45.8 28.6\nBLIP-2 FLAN-T5-XL [27 34.4 31.0 43.0 25.6 25.1 31.8 48.0 27.8\nmPLUGw-OWL2* [60] 32.7 32.1 48.5 25.6 24.9 32.8 46.7 29.6\nSPHINX* [73] 32.9 - - - - - - -\nQwen-VL-7B [5] 35.9 32.9 417 29.8 25.6 33.6 45.3 30.2\nLLaVA-1.5-13B [34] 36.4 33.6 49.8 28.2 25.9 34.9 54.7 28.3\nInstructBLIP-T5-XXL [14 35.7 33.8 48.5 30.6 27.6 33.6 49.8 29.4\nBLIP-2 FLAN-T5-XXL [27] 35.4 34.0 49.2 28.6 27.3 33.7 51.5 30.4\nGemini Nano2* [72] 32.6 - - - - - - -\nQwen-VL-PLUS* [74] 45.2 40.8 59.9 34.5 32.8 43.7 65.5 32.9\nGemini Pro* [72] 47.9 - - - - - - -\nGPT-4V(ision) (Playground) [46] 56.8 55.7 65.3 64.3 48.4 63.5 76.3 41.7\nGemini Ultra* [72] 59.4 - - - - - - -\nLarge Language Models (LLMs): Only Text as Input\nLlama2 7B [53] 30.1 28.7 30.7 27.2 26.7 27.7 32.6 29.8\nFLAN-TS5-XXL [12] 32.1 31.2 36.8 28.9 26.7 32.8 44.8 28.3\n+ OCR 34.7 31.9 36.2 28.8 26.2 32.6 50.5 29.7\n+ LLaVA Caption 34.8 31.9 38.4 27.8 27.0 33.2 49.9 28.7\nVicuna-I13B [10] sS—=S 33.30 «= 310335. (ss HT A LT\n+OCR 35.4 31.9 37.1 28.6 26.5 32.0 49.3 30.0\n+ LLaVA Caption 33.9 32.7 42.0 26.8 26.2 33.4 49.4 31.4\nGPT-4 Text [45] 34.9 33.8 32.9 28.5 30.6 41.3 53.0 28.4\n\nTable 2. Overal\nadditionally add\n\nresults provided by the authors.\n\nther fine-tuned with visual instruction tuning data for better\nzero-shot generalization capabilities. For both BLIP-2 and\nInstructBLIP, we consider both scales: FLAN-T5 XL and\nFLAN-T5-XXL for model scaling analysis. (v) LLaVA-\n1.5 [34] linearly projects the visual embedding into word\nembedding space of Vicuna [10], thus equipping the LLM\nwith visual abilities. (vi) As an open-source alternative to\nFlamingo [2], OpenFlamingo [4] has close performance on\nmost vision-language tasks. (vii) CogVLM [55] concate-\nnates image and text in the input embedding space and\nadds trainable visual layers in textual Transformer blocks\nto deeply align two modalities. It is reported to achieve\n\n1 results of different models on the MMMU validation and test set. Besides reporting the performance of LMMs, we\ntext-only LLM baselines. The best-performing model in each category is in-bold, and the second best is underlined. *:\n\nvery promising performance on existing VQA benchmarks\nrecently. (viii) Fuyu [6] projects the patches of the input\nimage into text embedding space. (ix) Qwen-VL [5] in-\ntroduces a set of trainable query embeddings and single-\nlayer cross-attention module to bridge the modalities, sup-\nporting interleaved image-text input. (x) Otter [26] is fine-\ntuned with diverse instruction-tuning data and able to per-\nform in-context learning. (xi) MiniGPT-4 [71] is built upon\nVicuna [10] and designs a linear modality projection layer\nfor visual understanding abilities. (xii) mPLUG-Owl2 [60]\ndesigns modality-adaptive module to unify vision and lan-\nguage while preserving the distinct properties of them.\n", "vlm_text": "The table presents performance metrics of various large multimodal models (LMMs) and large language models (LLMs) across different validation and test sets, and specific categories/disciplines. Here’s a breakdown:\n\n1. **Categories**: \n   - Validation Overall\n   - Test Overall\n   - Art & Design\n   - Business\n   - Science\n   - Health & Medicine\n   - Human & Social Sci.\n   - Tech & Eng.\n\n2. **Models**:\n   - The table compares models that use text and image (referred to as LMMs), and those that use text only (referred to as LLMs).\n   - Examples of LMMs include OpenFlamingo-2B, LLaVA-1.5-13B, and Gemini Nano2*.\n   - Examples of LLMs include Llama2 7B, FLAN-T5-XXL, and Vicuna-13B.\n\n3. **Metrics**:\n   - Numbers represent performance scores (possibly accuracy or another measure) for each model in the mentioned categories.\n   - Some models have enhancements with add-ons like OCR or specific captions, indicated with the \"+\" sign.\n\n4. **Special Entries**: \n   - Models with an asterisk (*) next to them such as Gemini Nano2* indicate special versions or configurations.\n   - Frequent and Random Choice scores are given as baseline comparisons.\n\nThe table is organized into two main sections: Large Multimodal Models with both text and images as input, and Large Language Models with only text as input. Each section lists models alongside their respective scores across various categories and overall performance.\nther fine-tuned with visual instruction tuning data for better zero-shot generalization capabilities. For both BLIP-2 and Instruct BLIP, we consider both scales: FLAN-T5 XL and FLAN-T5-XXL for model scaling analysis. (v)  LLaVA- 1.5 [ 34 ] linearly projects the visual embedding into word embedding space of Vicuna [ 10 ], thus equipping the LLM with visual abilities.    $(\\nu i)$   As an open-source alternative to Flamingo [ 2 ], Open Flamingo [ 4 ] has close performance on most vision-language tasks.  (vii)  CogVLM [ 55 ] concate- nates image and text in the input embedding space and adds trainable visual layers in textual Transformer blocks to deeply align two modalities. It is reported to achieve very promising performance on existing VQA benchmarks recently.  (viii)  Fuyu [ 6 ] projects the patches of the input image into text embedding space.  (ix)  Qwen-VL [ 5 ] in- troduces a set of trainable query embeddings and single- layer cross-attention module to bridge the modalities, sup- porting interleaved image-text input.    $(x)$   Otter [ 26 ] is fine- tuned with diverse instruction-tuning data and able to per- form in-context learning.  (xi)  MiniGPT-4 [ 71 ] is built upon Vicuna [ 10 ] and designs a linear modality projection layer for visual understanding abilities.  (xii)  mPLUG-Owl2 [ 60 ] designs modality-adaptive module to unify vision and lan- guage while preserving the distinct properties of them. \n"}
{"page": 6, "image_path": "doc_images/2311.16502v3_6.jpg", "ocr_text": "= Fuyu-8B © Qwen-VL-7B © InstructBLIP-T5-XXL Models Gone) “oI cay) (0500)\n® LLaVA-1.5-13B © BLIP-2 FLAN-T5-XXL © GPT-4V\n08 Fuyu-8B [6] 28.9 27.0 26.4 27.4\n\n. Qwen-VL-7B [5] 39.4 31.9 27.6 32.9\n0.6 LLaVA-1.5-13B [34] 413 32.7 26.7 33.6\n0.4 InstructBLIP-T5-XXL [14] 40.3 32.3 29.4 33.8\n0 BLIP-2 FLAN-TS-XXL [27] 41.0 32.7 28.5 34.0\n00 GPT-4V [46] 76.1 55.6 312 55.7\n\n, S > S i asd ae so\n\now qa oe\" fanitlooe wr Table 3. Result decomposition across question difficulty levels.\n\nFigure 5. Performance of models on different types of images.\n\nText-only LLMs. For text-only LLMs, we consider the\nmost capable ones including GPT-4 and several open-source\nLLMs, Llama2-7B [52], FLAN-T5-XXL and Vicuna-13B,\nwhich are adopted as the text encoder or decoder in the se-\nlected LMMs. To determine if an external image-to-text\ntool can enhance these LLMs’ performance on MMMU, we\ndeploy OCR by MMOCR' or captioning by LLaVA-1.5 to\nprovide the recognized text information to text-only LLMs.\nEvaluation. We adopt micro-averaged accuracy as the\nevaluation metric. For both open and multiple-choice\nquestions, we design systematic, rule-based evaluation\npipelines. Specifically, to mitigate the potential influence\nof any intermediate generations (e.g., reasoning steps, cal-\nculations) in the long response, we construct robust regular\nexpressions and develop response-processing workflows.\nThese are employed to extract key phrases, such as num-\nbers and conclusion phrases, from the long responses for\naccurate answer matching. If there is no valid answer in the\nmodel’s response, we perform random selection as a rem-\nedy for multiple-choice questions or consider the response\nincorrect for open questions. For reference, we add Ran-\ndom Choice and Frequent Choice baselines: the former ran-\ndomly selects an option, while the latter selects the most fre-\nquent option within each specific subject of the validation\nset, based on its frequency of occurrence in that subject.\n\n4.2. Main Results\n\nIn this section, we present a comprehensive comparison of\ndifferent LLMs and LMMs using the MMMU benchmark, de-\ntailed in Table 2. We summarize our key findings as follows:\nChallenging Nature of MMMU: The benchmark poses sig-\nnificant challenges to current models. Notably, GPT-4V,\ndespite being an advanced model, achieves an accuracy of\nonly 55.7%, with ample headroom for improvement. This\nreflects the benchmark’s rigorous and demanding standards.\nDisparity between Open-source Models and GPT-4V:\nLeading open-source models such as BLIP2-FLAN-TS5-\nXXL and LLaVA-1.5 reach an accuracy level of approxi-\nmately 34%, which is significantly lower than GPT-4V. This\n\n'https://github.com/open-mmlab/mmocr\n\nsignificant difference in performance indicates a gap in the\ncapabilities of current open-source models compared to pro-\nprietary ones like GPT-4V.\nEffectiveness of OCR and Captioning Enhancements:\nThe application of OCR and captioning technologies does\nnot yield a significant improvement in the performance of\ntext-only LMMs. This finding suggests that the MMMU\nbenchmark requires models that can effectively interpret\nand integrate both textual and visual information, under-\nscoring the complexity of the multimodal tasks it presents.\nModel Performance across Different Disciplines: In dis-\nciplines such as Art & Design and Humanities & Social\nSciences, where the images tends to be more ‘natural’ and\nquestions involve relatively less reasoning, models demon-\nstrate relatively higher performance. Conversely, in fields\nlike Science, Health & Medicine, and Technology & Engi-\nneering, where tasks often involve intricate perception and\ncomplex reasoning, models exhibit lower performance.\nThe MMMU benchmark underscores both the progress and\nthe challenges in multimodal understanding and reasoning.\nWhile GPT-4V leads in performance, the overall results in-\ndicate substantial room for improvement, especially in do-\nmains with complex visual input and heavy reasoning with\nsubject knowledge.\n\n4.3. Analysis on Images Types and Difficulties\n\nDifferent Image Types. We compare the performance of\nvarious models across top frequent image types in Fig-\nure 5. Across all types, GPT-4V consistently outperforms\nthe other models by a huge margin. Open-source models\ndemonstrate relatively strong performance in categories like\nPhotos and Paintings, which are more frequently seen dur-\ning training. However, for less common image categories\nlike Geometric shapes, Music sheets and Chemical struc-\ntures, all models obtain very low scores (some are close to\nrandom guesses). This indicates that the existing models are\ngeneralizing poorly towards these image types.\n\nDifferent Difficulty Levels. Table 3 compares the per-\nformance of selected models across three difficulty lev-\nels. GPT-4V demonstrates a significantly higher profi-\nciency, with a success rate of 76.1%, compared to open-\nsource models in the “Easy” category. When it comes to\n", "vlm_text": "The image is a bar chart comparing different models or methods across various categories. The caption text lists several model names or specifications, which seem to be the different models being compared. The categories on the x-axis are labeled as: Diagrams, Tables, Charts, Chemical, Photos, Paintings, Geometric, Music, and Medical. Each category has several colored bars representing the performance or score of each model. The y-axis appears to represent a numerical scale ranging from 0.0 to 0.6. The bars of different colors represent different models or methods, showing their performance across the listed categories.\nText-only LLMs.  For text-only LLMs, we consider the most capable ones including GPT-4 and several open-source LLMs, Llama2-7B [ 52 ], FLAN-T5-XXL and Vicuna-13B, which are adopted as the text encoder or decoder in the se- lected LMMs. To determine if an external image-to-text tool can enhance these LLMs’ performance on  MMMU , we deploy OCR by MMOCR 1   or captioning by LLaVA-1.5 to provide the recognized text information to text-only LLMs. Evaluation. We adopt micro-averaged accuracy as the evaluation metric. For both open and multiple-choice questions, we design systematic, rule-based evaluation pipelines. Specifically, to mitigate the potential influence of any intermediate generations (e.g., reasoning steps, cal- culations) in the long response, we construct robust regular expressions and develop response-processing workflows. These are employed to extract key phrases, such as num- bers and conclusion phrases, from the long responses for accurate answer matching. If there is no valid answer in the model’s response, we perform random selection as a rem- edy for multiple-choice questions or consider the response incorrect for open questions. For reference, we add Ran- dom Choice and Frequent Choice baselines: the former ran- domly selects an option, while the latter selects the most fre- quent option within each specific subject of the validation set, based on its frequency of occurrence in that subject. \n4.2. Main Results \nIn this section, we present a comprehensive comparison of different LLMs and LMMs using the  MMMU  benchmark, de- tailed in  Table 2 . We summarize our key findings as follows: Challenging Nature of  MMMU : The benchmark poses sig- nificant challenges to current models. Notably, GPT-4V, despite being an advanced model, achieves an accuracy of only  $55.7\\%$  , with ample headroom for improvement. This reflects the benchmark’s rigorous and demanding standards. Disparity between Open-source Models and GPT-4V : Leading open-source models such as BLIP2-FLAN-T5- XXL and LLaVA-1.5 reach an accuracy level of approxi- mately  $34\\%$  , which is significantly lower than GPT-4V. This \nThis table presents a comparison of different models across difficulty levels: Easy, Medium, Hard, and Overall. Here are the details:\n\n- **Models**: A list of models such as Fuyu-8B, Qwen-VL-7B, LLaVA-1.5-13B, InstructBLIP-T5-XXL, BLIP-2 FLAN-T5-XXL, and GPT-4V.\n- **Columns**: \n  - Easy (2946)\n  - Medium (4917)\n  - Hard (2637)\n  - Overall (10500)\n\n- **Values**: Represent numerical scores for each model in corresponding difficulty categories. For instance, GPT-4V has scores of 76.1 for Easy, 55.6 for Medium, 31.2 for Hard, resulting in an Overall score of 55.7.\nsignificant difference in performance indicates a gap in the capabilities of current open-source models compared to pro- prietary ones like GPT-4V. \nEffectiveness of OCR and Captioning Enhancements : The application of OCR and captioning technologies does not yield a significant improvement in the performance of text-only LMMs. This finding suggests that the  MMMU benchmark requires models that can effectively interpret and integrate both textual and visual information, under- scoring the complexity of the multimodal tasks it presents. Model Performance across Different Disciplines : In dis- ciplines such as Art & Design and Humanities & Social Sciences, where the images tends to be more ‘natural’ and questions involve relatively less reasoning, models demon- strate relatively higher performance. Conversely, in fields like Science, Health & Medicine, and Technology & Engi- neering, where tasks often involve intricate perception and complex reasoning, models exhibit lower performance. \nThe  MMMU  benchmark underscores both the progress and the challenges in multimodal understanding and reasoning. While GPT-4V leads in performance, the overall results in- dicate substantial room for improvement, especially in do- mains with complex visual input and heavy reasoning with subject knowledge. \n4.3. Analysis on Images Types and Difficulties \nDifferent Image Types.  We compare the performance of various models across top frequent image types in  Fig- ure 5 . Across all types, GPT-4V consistently outperforms the other models by a huge margin. Open-source models demonstrate relatively strong performance in categories like Photos and Paintings, which are more frequently seen dur- ing training. However, for less common image categories like Geometric shapes, Music sheets and Chemical struc- tures, all models obtain very low scores (some are close to random guesses). This indicates that the existing models are generalizing poorly towards these image types. \nDifferent Difficulty Levels. Table 3  compares the per- formance of selected models across three difficulty lev- els. GPT-4V demonstrates a significantly higher profi- ciency, with a success rate of   $76.1\\%$  , compared to open- source models in the “Easy” category. When it comes to "}
{"page": 7, "image_path": "doc_images/2311.16502v3_7.jpg", "ocr_text": "Textual Understanding\n4%\n\nLack of\nKnowledge Reasoning\nReject to 29% Te\nAnwer oe\n3%\nPerceptual Error\n35%\nAnnotation ‘Answer\nError Extraction Error\n2% 1%\n\nFigure 6. Error distribution over 150 annotated GPT-4V errors.\n\nthe “Medium” category, while the gap narrows, GPT-4V\nstill leads at 55.6%. The further diminishing performance\ngap in the “Hard” category across models indicates that as\nthe complexity of tasks increases, the advantage of more ad-\nvanced models like GPT-4V almost disappears. This might\nreflect a current limitation in handling expert-level chal-\nlenging queries even for the most advanced models.\n\n5. Error Analysis and Future Work\n\nIn this section, we delve into the analysis of errors by GPT-\nAV, a pivotal aspect for understanding its operational ca-\npabilities and limitations. This analysis serves not only to\nidentify the model’s current shortcomings but also to guide\nfuture enhancements in its design and training. We meticu-\nlously examine 150 randomly sampled error instances from\nGPT-4V’s predictions. These instances are analyzed by ex-\npert annotators who identify the root causes of mispredic-\ntions based on their knowledge and the golden explanations\nif available. The distribution of these errors is illustrated in\nFigure 6, and a selection of 100 notable cases, along with\ndetailed analyses, is included in the Appendix.\n\nPerceptual Errors (35%): Perceptual errors, forming the\nbulk of the inaccuracies in the GPT-4V model, are catego-\nrized into two types: basic perceptual errors and domain-\nspecific perceptual errors. Basic perceptual errors, as de-\npicted in Figure 7, occur when the model accurately pro-\ncesses and understands the given information but fails in\nelementary visual interpretation, such as misjudging the se-\nquence described as “from left to right, top to bottom.” On\nthe other hand, domain-specific perceptual errors occur due\nto the lack of knowledge. As we analyze the root cause,\nwe classify such errors as lack of knowledge (see analysis\nbelow). Additionally, GPT-4V often exhibits a bias towards\ntext, prioritizing textual information over visual inputs, a\ntrend noted in recent studies [13]. A prominent example is\nin Figure 68, where the model incorrectly prioritizes its text-\nbased interpretation of “imperialism” over the visual narra-\ntive in a cartoon depicting the United States as a “Savior.”\nThis underscores the need for a more balanced approach to\n\n@ Perceptual Error\n\nQuestion: Look at the situation in the <image 1>. You are traveling on a\nplane with a small child. The cabin is depressurized and the oxygen mask\nfalls from the ceiling. Which of these pictures shows the reconciliation of\negoism and other-isms (the order is from left to right, top to bottom)?\n\nOption: (A) 1 (B) 2(C)3\nGround Truth: (B) 2 GPT-4V: The image shows three situations\non a plane when the oxygen mask falls:\na The adult puts on the mask first while the\n“/3./| child watches (egoism). The adult puts the\n*<*'| mask on the child first while struggling for\nbreath. Both the adult and the child share\nthe mask. The reconciliation of egoism and\nother-isms would be a situation where\nboth the individual's needs and the needs\nof others are considered and addressed.\nTherefore, the correct answer is: (C) 3\n\n<image 1>\n\nError Reason: GPT-4V recalled the right knowledge and made the right\nreasoning, but it failed to correctly map the IDs to the corresponding\nillustrations in the figure, which is not explicitly marked in the figure but\nis only described in text (the order is from left to right, top to bottom).\n\nFigure 7. A basic perceptual error, easy for humans but challeng-\ning for GPT-4V. More examples can be found in the Appendix.\n\nmultimodal interpretation.\n\nLack of Knowledge (29%): A fundamental root cause of\n*domain-specific’ perceptual errors in the GPT-4V model,\nas previously discussed, is the lack of specialized knowl-\nedge. This deficiency is exemplified in the Computer Sci-\nence context illustrated in Appendix Figure 84, where the\nmodel identifies visual elements such as double circles but\nfails to interpret them accurately within the domain-specific\ncontext, such as their representation of an ’accept state’ in\nDeterministic Finite Automata. Similarly, a deficit in spe-\ncialized knowledge can lead to flawed reasoning, as demon-\nstrated in the medical example in Appendix Figure 55.\nThese instances underscore the necessity of enriching the\ntraining datasets of foundation models with a diverse range\nof domain-specific knowledge to improve their accuracy\nand general applicability in various specialized fields.\nReasoning Errors (26%): Flawed reasoning emerges as\nanother significant cause of errors. In instances where the\nmodel correctly interprets text and images and recalls rel-\nevant knowledge, it still often fails to apply logical and\nmathematical reasoning skills effectively to derive accurate\ninferences. A notable instance of this can be observed in\nAppendix Figure 46, where the model neglects an essen-\ntial step in a mathematical reasoning process, leading to an\nincorrect conclusion. Enhancing the model’s reasoning ca-\npability is critical to address these shortcomings.\n\nOther Errors: The remaining errors include Textual Un-\nderstanding Error (6%), Rejection to Answer (3%), An-\nnotation Error (2%), and Answer Extraction Error (1%).\nThese errors are attributed to various factors such as com-\nplex text interpretation challenges, limitations in response\ngeneration, inaccuracies in data annotation, and issues in\nextracting precise answers from longer outputs.\n", "vlm_text": "The image is a pie chart showing the distribution of different types of errors among 150 annotated GPT-4V errors. The error categories and their respective proportions are as follows:\n\n- Perceptual Error: 35%\n- Lack of Knowledge: 29%\n- Reasoning Error: 26%\n- Textual Understanding: 4%\n- Reject to Answer: 3%\n- Annotation Error: 2%\n- Answer Extraction Error: 1% \n\nThis chart visually represents how different types of errors contribute to the overall error distribution in the evaluated data set.\nthe “Medium” category, while the gap narrows, GPT-4V still leads at   $55.6\\%$  . The further diminishing performance gap in the “Hard” category across models indicates that as the complexity of tasks increases, the advantage of more ad- vanced models like GPT-4V almost disappears. This might reflect a current limitation in handling expert-level chal- lenging queries even for the most advanced models. \n5. Error Analysis and Future Work \nIn this section, we delve into the analysis of errors by GPT- 4V, a pivotal aspect for understanding its operational ca- pabilities and limitations. This analysis serves not only to identify the model’s current shortcomings but also to guide future enhancements in its design and training. We meticu- lously examine 150 randomly sampled error instances from GPT-4V’s predictions. These instances are analyzed by ex- pert annotators who identify the  root causes of mispredic- tions  based on their knowledge and the golden explanations if available. The distribution of these errors is illustrated in Figure  6 , and a selection of 100 notable cases, along with detailed analyses, is included in the Appendix. \nPerceptual Errors   $({\\bf35\\%})$  :  Perceptual errors, forming the bulk of the inaccuracies in the GPT-4V model, are catego- rized into two types: basic perceptual errors and domain- specific perceptual errors. Basic perceptual errors, as de- picted in  Figure 7 , occur when the model accurately pro- cesses and understands the given information but fails in elementary visual interpretation, such as misjudging the se- quence described as “from left to right, top to bottom.” On the other hand, domain-specific perceptual errors occur due to the lack of knowledge. As we analyze the root cause, we classify such errors as lack of knowledge (see analysis below). Additionally, GPT-4V often exhibits a bias towards text, prioritizing textual information over visual inputs, a trend noted in recent studies [ 13 ]. A prominent example is in  Figure 68 , where the model incorrectly prioritizes its text- based interpretation of “imperialism” over the visual narra- tive in a cartoon depicting the United States as a “Savior.” This underscores the need for a more balanced approach to \nThe image presents a question about a scenario on a plane involving an adult and a child with oxygen masks. It asks which picture best shows the reconciliation of \"egoism\" and other-isms based on their order. There are three illustrations depicting different actions:\n\n1. An adult puts on their mask first, while the child watches.\n2. An adult puts a mask on the child first while struggling to breathe.\n3. Both the adult and the child share the mask.\n\nThe ground truth states option (B) as correct, indicating that the second option (putting the mask on the child first) represents the reconciliation. However, the explanation suggests option (C) (sharing the mask) as the correct logical choice.\n\nThe error reason mentions that the model had the right reasoning but misidentified the illustrations' order.\nFigure 7. A basic perceptual error, easy for humans but challeng- ing for GPT-4V. More examples can be found in the Appendix. \nmultimodal interpretation. \nLack of Knowledge   $(29\\%)$  :  A fundamental root cause of ’domain-specific’ perceptual errors in the GPT-4V model, as previously discussed, is the lack of specialized knowl- edge. This deficiency is exemplified in the Computer Sci- ence context illustrated in Appendix  Figure 84 , where the model identifies visual elements such as double circles but fails to interpret them accurately within the domain-specific context, such as their representation of an ’accept state’ in Deterministic Finite Automata. Similarly, a deficit in spe- cialized knowledge can lead to flawed reasoning, as demon- strated in the medical example in Appendix  Figure 55 . These instances underscore the necessity of enriching the training datasets of foundation models with a diverse range of domain-specific knowledge to improve their accuracy and general applicability in various specialized fields. \nReasoning Errors   $(26\\%)$  :  Flawed reasoning emerges as another significant cause of errors. In instances where the model correctly interprets text and images and recalls rel- evant knowledge, it still often fails to apply logical and mathematical reasoning skills effectively to derive accurate inferences. A notable instance of this can be observed in Appendix  Figure 46 , where the model neglects an essen- tial step in a mathematical reasoning process, leading to an incorrect conclusion. Enhancing the model’s reasoning ca- pability is critical to address these shortcomings. \nOther Errors:  The remaining errors include Textual Un- der standing Error   $(6\\%)$  , Rejection to Answer   $(3\\%)$  , An- notation Error   $(2\\%)$  , and Answer Extraction Error   $(1\\%)$  . These errors are attributed to various factors such as com- plex text interpretation challenges, limitations in response generation, inaccuracies in data annotation, and issues in extracting precise answers from longer outputs. "}
{"page": 8, "image_path": "doc_images/2311.16502v3_8.jpg", "ocr_text": "In summary, our error analysis underlines the challenges\nposed by MMMU and highlights areas for further research\nin visual perception, knowledge representation, reasoning\nabilities, and multimodal joint understanding.\n\n6. Conclusion\n\nThe development of MMMU as a benchmark for assessing\nthe capabilities of LMMs marks a significant milestone in\nthe journey toward Expert AGI. MMMU not only tests the\nboundaries of what current LMMs can achieve in terms of\nbasic perceptual skills but also evaluates their ability to han-\ndle complex reasoning and in-depth subject-specific knowl-\nedge. This approach directly contributes to our understand-\ning of the progress towards Expert AGI, as it mirrors the\nkind of expertise and reasoning abilities expected of skilled\nadults in various professional fields.\n\nDespite its comprehensive nature, MMMU, like any bench-\nmark, is not without limitations. The manual curation pro-\ncess, albeit thorough, may carry biases. And the focus on\ncollege-level subjects might not fully be a sufficient test for\nExpert AGI as per the definition [44]. However, we be-\nlieve it should be necessary for an Expert AGI to achieve\nstrong performance on MMMU to demonstrate their broad\nand deep subject knowledge as well as expert-level under-\nstanding and reasoning capabilities. In future work, we plan\nto incorporate human evaluations into MMMU. This will pro-\nvide a more grounded comparison between model capabili-\nties and expert performance, shedding light on the proxim-\nity of current AI systems to achieving Expert AGI.\n\nReferences\n\n1] Blaise Agiiera y Arcas and Peter Norvig. Artificial general\nintelligence is already here. Noema Magazine, 2023. |\n2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine\nMiech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch,\nKatherine Millican, Malcolm Reynolds, et al. Flamingo: a\nvisual language model for few-shot learning. In Advances in\nNeural Information Processing Systems, 2022. 3, 5, 6\n3] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret\nMitchell, Dhruv Batra, C. Lawrence Zitnick, and Devi\nParikh. VQA: Visual Question Answering. In International\nConference on Computer Vision (ICCV), 2015. 2,3\n4] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf\nHanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton,\nSamir Gadre, Shiori Sagawa, et al. Openflamingo: An open-\nsource framework for training large autoregressive vision-\nlanguage models. arXiv preprint arXiv:2308.01390, 2023.\n3, 6, 14, 15, 16, 17, 18, 19\n[5] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan\nTan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren\nZhou. Qwen-vl: A versatile vision-language model for un-\nderstanding, localization, text reading, and beyond. arXiv\npreprint arXiv:2308. 12966, 2023. 6,7, 14, 15, 16, 17, 18, 19\n\n[10\n\n11\n\n12\n\n13\n\n14\n\n(15]\n\n[16]\n\n(17)\n\nRohan Bavishi, Erich Elsen, Curtis Hawthorne, Maxwell\nNye, Augustus Odena, Arushi Somani, and Sagnak Tasurlar.\nIntroducing our multimodal models, 2023. 3, 6, 7, 14, 15,\n16, 17, 18, 19\n\nSébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Jo-\nhannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat\nLee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial\ngeneral intelligence: Early experiments with gpt-4. arXiv\npreprint arXiv:2303.12712, 2023. 1\n\nXi Chen, Josip Djolonga, Piotr Padlewski, Basil Mustafa,\nSoravit Changpinyo, Jialin Wu, Carlos Riquelme Ruiz, Se-\nbastian Goodman, Xiao Wang, Yi Tay, et al. Pali-x: On\nscaling up a multilingual vision and language model. arXiv\npreprint arXiv:2305.18565, 2023. 2\n\nYen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy,\nFaisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. Uniter:\nUniversal image-text representation learning. In European\nConference on Computer Vision, pages 104-120, 2020. 3\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhang-\nhao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yong-\nhao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P.\nXing. Vicuna: An open-source chatbot impressing gpt-4\nwith 90%* chatgpt quality, 2023. 3, 6, 14, 15, 16, 17, 18,\n19\n\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts, Paul\nBarham, Hyung Won Chung, Charles Sutton, Sebastian\nGehrmann, et al. Palm: Scaling language modeling with\npathways. arXiv preprint arXiv:2204.02311, 2022. 1\nHyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang,\nMostafa Dehghani, Siddhartha Brahma, et al. Scaling\ninstruction-finetuned language models. arXiv preprint\narXiv:2210.11416, 2022. 3,5, 6, 14, 15, 16, 17, 18, 19\nChenhang Cui, Yiyang Zhou, Xinyu Yang, Shirley Wu, Lin-\njun Zhang, James Zou, and Huaxiu Yao. Holistic analysis of\nhallucination in gpt-4v (ision): Bias and interference chal-\nlenges. arXiv preprint arXiv:2311.03287, 2023. 4, 8\nWenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat\nTiong, Jungi Zhao, Weisheng Wang, Boyang Li, Pascale\nFung, and Steven Hoi. Instructblip: Towards general-\npurpose vision-language models with instruction tuning.\narXiv preprint arXiv:2305.06500, 2023. 2, 3, 5, 6, 7, 14,\n15, 16, 17, 18, 19\n\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is\nworth 16x16 words: Transformers for image recognition at\nscale. In International Conference on Learning Representa-\ntions, 2021. 3\n\nPeng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie\nGeng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xi-\nangyu Yue, et al. Llama-adapter v2: Parameter-efficient vi-\nsual instruction model. arXiv preprint arXiv:2304.15010,\n2023. 3,5\n\nYinggiang Ge, Wenyue Hua, Jianchao Ji, Juntao Tan,\nShuyuan Xu, and Yongfeng Zhang. Openagi: When Ilm\n", "vlm_text": "In summary, our error analysis underlines the challenges posed by  MMMU  and highlights areas for further research in visual perception, knowledge representation, reasoning abilities, and multimodal joint understanding. \n6. Conclusion \nThe development of  MMMU  as a benchmark for assessing the capabilities of LMMs marks a significant milestone in the journey toward Expert AGI.  MMMU  not only tests the boundaries of what current LMMs can achieve in terms of basic perceptual skills but also evaluates their ability to han- dle complex reasoning and in-depth subject-specific knowl- edge. This approach directly contributes to our understand- ing of the progress towards Expert AGI, as it mirrors the kind of expertise and reasoning abilities expected of skilled adults in various professional fields. \nDespite its comprehensive nature,  MMMU , like any bench- mark, is not without limitations. The manual curation pro- cess, albeit thorough, may carry biases. And the focus on college-level subjects might not fully be a sufficient test for Expert AGI as per the definition [ 44 ]. However, we be- lieve it should be necessary for an Expert AGI to achieve strong performance on MMMU to demonstrate their broad and deep subject knowledge as well as expert-level under- standing and reasoning capabilities. In future work, we plan to incorporate human evaluations into  MMMU . This will pro- vide a more grounded comparison between model capabili- ties and expert performance, shedding light on the proxim- ity of current AI systems to achieving Expert AGI.\n\n \nReferences \n[1] Blaise Ag¨ uera y Arcas and Peter Norvig. Artificial general intelligence is already here.  Noema Magazine , 2023.  1\n\n [2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. In  Advances in Neural Information Processing Systems , 2022.  3 ,  5 ,  6\n\n [3] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C. Lawrence Zitnick, and Devi Parikh. VQA: Visual Question Answering. In  International Conference on Computer Vision (ICCV) , 2015.  2 ,  3\n\n [4] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, et al. Open flamingo: An open- source framework for training large auto regressive vision- language models.  arXiv preprint arXiv:2308.01390 , 2023. 3 ,  6 ,  14 ,  15 ,  16 ,  17 ,  18 ,  19\n\n [5] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: A versatile vision-language model for un- der standing, localization, text reading, and beyond.  arXiv preprint arXiv:2308.12966 , 2023.  6 ,  7 ,  14 ,  15 ,  16 ,  17 ,  18 ,  19 \n[6] Rohan Bavishi, Erich Elsen, Curtis Hawthorne, Maxwell Nye, Augustus Odena, Arushi Somani, and Sa˘ gnak Tas ¸ırlar. Introducing our multimodal models, 2023.  3 ,  6 ,  7 ,  14 ,  15 , 16 ,  17 ,  18 ,  19\n\n [7] S´ ebastien Bubeck, Varun Chandra sekar an, Ronen Eldan, Jo- hannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence: Early experiments with gpt-4.  arXiv preprint arXiv:2303.12712 , 2023.  1\n\n [8] Xi Chen, Josip Djolonga, Piotr Padlewski, Basil Mustafa, Soravit Changpinyo, Jialin Wu, Carlos Riquelme Ruiz, Se- bastian Goodman, Xiao Wang, Yi Tay, et al. Pali-x: On scaling up a multilingual vision and language model.  arXiv preprint arXiv:2305.18565 , 2023.  2\n\n [9] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. Uniter: Universal image-text representation learning. In  European Conference on Computer Vision , pages 104–120, 2020.  3\n\n [10] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhang- hao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yong- hao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with  $90\\%^{*}$   chatgpt quality, 2023.  3 ,  6 ,  14 ,  15 ,  16 ,  17 ,  18 , 19\n\n [11] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways.  arXiv preprint arXiv:2204.02311 , 2022.  1\n\n [12] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416 , 2022.  3 ,  5 ,  6 ,  14 ,  15 ,  16 ,  17 ,  18 ,  19\n\n [13] Chenhang Cui, Yiyang Zhou, Xinyu Yang, Shirley Wu, Lin- jun Zhang, James Zou, and Huaxiu Yao. Holistic analysis of hallucination in gpt-4v (ision): Bias and interference chal- lenges.  arXiv preprint arXiv:2311.03287 , 2023.  4 ,  8\n\n [14] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instruct blip: Towards general- purpose vision-language models with instruction tuning. arXiv preprint arXiv:2305.06500 , 2023.  2 ,  3 ,  5 ,  6 ,  7 ,  14 , 15 ,  16 ,  17 ,  18 ,  19\n\n [15] Alexey Do sov it ski y, Lucas Beyer, Alexander Kolesnikov, Dirk Weiss en born, Xiaohua Zhai, Thomas Unter thin er, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl- vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In  International Conference on Learning Representa- tions , 2021.  3\n\n [16] Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xi- angyu Yue, et al. Llama-adapter v2: Parameter-efficient vi- sual instruction model. arXiv preprint arXiv:2304.15010 , 2023.  3 ,  5\n\n [17] Yingqiang Ge, Wenyue Hua, Jianchao Ji, Juntao Tan, Shuyuan Xu, and Yongfeng Zhang. Openagi: When llm meets domain experts. arXiv preprint arXiv:2304.04370 , 2023.  1\n\n "}
{"page": 9, "image_path": "doc_images/2311.16502v3_9.jpg", "ocr_text": "18\n\n19\n\n20\n\n21\n\n22\n\n23\n\n24\n\n25\n\n26\n\n27\n\n28\n\n29\n\nmeets domain experts.\n2023. 1\n\nYash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Ba-\ntra, and Devi Parikh. Making the v in vqa matter: Elevating\nthe role of image understanding in visual question answer-\ning. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 6904-6913, 2017. 2, 3\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou,\nMantas Mazeika, Dawn Song, and Jacob Steinhardt. Mea-\nsuring massive multitask language understanding. In Jnter-\nnational Conference on Learning Representations, 2020. 2\n\narXiv preprint arXiv:2304.04370,\n\nYupan Huang, Zaiqiao Meng, Fangyu Liu, Yixuan Su, Col-\nlier Nigel, and Yutong Lu. Sparkles: Unlocking chats across\nmultiple images for multimodal instruction-following mod-\nels. arXiv preprint arXiv:2308.16463, 2023. 3\n\nDrew A Hudson and Christopher D Manning. Gqa: A new\ndataset for real-world visual reasoning and compositional\nquestion answering. In Proceedings of the IEEE/CVF con-\nference on computer vision and pattern recognition, pages\n6700-6709, 2019. 3\n\nChao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh,\nHieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom\nDuerig. Scaling up visual and vision-language representa-\ntion learning with noisy text supervision. In Jnternational\nconference on machine learning, pages 4904-4916. PMLR,\n2021. 3\n\nSahar Kazemzadeh, Vicente Ordonez, Mark Matten, and\nTamara Berg. Referitgame: Referring to objects in pho-\ntographs of natural scenes. In Proceedings of the 2014 con-\nference on empirical methods in natural language processing\n(EMNLP), pages 787-798, 2014. 2\n\nEhsan Latif, Gengchen Mai, Matthew Nyaaba, Xuansheng\nWu, Ninghao Liu, Guoyu Lu, Sheng Li, Tianming Liu, and\nXiaoming Zhai. Artificial general intelligence (agi) for edu-\ncation. arXiv preprint arXiv:2304.12479, 2023. 1\n\nBohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yix-\niao Ge, and Ying Shan. Seed-bench: Benchmarking mul-\ntimodal Ilms with generative comprehension. arXiv preprint\narXiv:2307.16125, 2023. 2, 4\n\nBo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang,\nJingkang Yang, and Ziwei Liu. Otter: A multi-modal\nmodel with in-context instruction tuning. arXiv preprint\narXiv:2305.03726, 2023. 3, 6, 14, 15, 16, 17, 18, 19\n\nJunnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\nBlip-2: Bootstrapping language-image pre-training with\nfrozen image encoders and large language models. Jnter-\nnational Conference on Machine Learning, 2023. 2, 3, 5, 6,\n7, 14, 15, 16, 17, 18, 19\n\nLei Li, Yuwei Yin, Shicheng Li, Liang Chen, Peiyi Wang,\nShuhuai Ren, Mukai Li, Yazheng Yang, Jingjing Xu, Xu\nSun, et al. M3it: A large-scale dataset towards multi-\nmodal multilingual instruction tuning. arXiv preprint\narXiv:2306.04387, 2023. 3\n\nXiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei\nHu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu\nWei, et al. Oscar: Object-semantics aligned pre-training\nfor vision-language tasks. In Computer Vision—ECCV 2020:\n\n10\n\n[30]\n\n33\n\n34\n\n35\n\n36\n\n37\n\n38\n\n39\n\n[40]\n\n[41]\n\n16th European Conference, Glasgow, UK, August 23-28,\n2020, Proceedings, Part XXX 16, pages 121-137. Springer,\n2020. 3\n\nYifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin\nZhao, and Ji-Rong Wen. Evaluating object hallucina-\ntion in large vision-language models. arXiv preprint\narXiv:2305.10355, 2023. 4\n\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Dollar, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nComputer Vision—ECCV 2014: 13th European Conference,\nZurich, Switzerland, September 6-12, 2014, Proceedings,\nPart V 13, pages 740-755. Springer, 2014. 2, 3\n\nFuxiao Liu, Tianrui Guan, Zongxia Li, Lichang Chen, Yaser\nYacoob, Dinesh Manocha, and Tianyi Zhou. Hallusion-\nbench: You see what you think? or you think what you see?\nan image-context reasoning benchmark challenging for gpt-\nAv (ision), lava-1.5, and other multi-modality models. arXiv\npreprint arXiv:2310.14566, 2023. 4\n\nFuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser\nYacoob, and Lijuan Wang. Aligning large multi-modal\nmodel with robust instruction tuning. arXiv preprint\narXiv:2306.14565, 2023. 3\n\nHaotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.\nImproved baselines with visual instruction tuning. arXiv\npreprint arXiv:2310.03744, 2023. 2, 3, 6, 7, 14, 15, 16, 17,\n18, 19\n\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\nVisual instruction tuning. arXiv preprint arXiv:2304.08485,\n2023. 3\n\nYuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang\nZhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He,\nZiwei Liu, et al. Mmbench: Is your multi-modal model an\nall-around player? arXiv preprint arXiv:2307.06281, 2023.\n2,4\n\nYuliang Liu, Zhang Li, Hongliang Li, Wenwen Yu, Mingxin\nHuang, Dezhi Peng, Mingyu Liu, Mingrui Chen, Chunyuan\nLi, Lianwen Jin, et al. On the hidden mystery of ocr in large\nmultimodal models. arXiv preprint arXiv:2305.07895, 2023.\n4\n\nJiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert:\nPretraining task-agnostic visiolinguistic representations for\nvision-and-language tasks. Advances in neural information\nprocessing systems, 32, 2019. 3\n\nPan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei\nChang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and\nAshwin Kalyan. Learn to explain: Multimodal reasoning via\nthought chains for science question answering. Advances\nin Neural Information Processing Systems, 35:2507-2521,\n2022. 2\n\nPan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li,\nHannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel\nGalley, and Jianfeng Gao. Mathvista: Evaluating mathemat-\nical reasoning of foundation models in visual contexts. arXiv\npreprint arXiv:2310.02255, 2023. 4\n\nKenneth Marino, Mohammad Rastegari, Ali Farhadi, and\nRoozbeh Mottaghi. Ok-vqa: A visual question answering\n", "vlm_text": "[18] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Ba- tra, and Devi Parikh. Making the v in vqa matter: Elevating the role of image understanding in visual question answer- ing. In  Proceedings of the IEEE conference on computer vision and pattern recognition , pages 6904–6913, 2017.  2 ,  3\n\n [19] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Mea- suring massive multitask language understanding. In  Inter- national Conference on Learning Representations , 2020.  2\n\n [20] Yupan Huang, Zaiqiao Meng, Fangyu Liu, Yixuan Su, Col- lier Nigel, and Yutong Lu. Sparkles: Unlocking chats across multiple images for multimodal instruction-following mod- els.  arXiv preprint arXiv:2308.16463 , 2023.  3\n\n [21] Drew A Hudson and Christopher D Manning. Gqa: A new dataset for real-world visual reasoning and compositional question answering. In  Proceedings of the IEEE/CVF con- ference on computer vision and pattern recognition , pages 6700–6709, 2019.  3\n\n [22] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representa- tion learning with noisy text supervision. In  International conference on machine learning , pages 4904–4916. PMLR, 2021.  3\n\n [23] Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg. Refer it game: Referring to objects in pho- tographs of natural scenes. In  Proceedings of the 2014 con- ference on empirical methods in natural language processing (EMNLP) , pages 787–798, 2014.  2\n\n [24] Ehsan Latif, Gengchen Mai, Matthew Nyaaba, Xuansheng Wu, Ninghao Liu, Guoyu Lu, Sheng Li, Tianming Liu, and Xiaoming Zhai. Artificial general intelligence (agi) for edu- cation.  arXiv preprint arXiv:2304.12479 , 2023.  1\n\n [25] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yix- iao Ge, and Ying Shan. Seed-bench: Benchmarking mul- timodal llms with generative comprehension.  arXiv preprint arXiv:2307.16125 , 2023.  2 ,  4\n\n [26] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu. Otter: A multi-modal model with in-context instruction tuning. arXiv preprint arXiv:2305.03726 , 2023.  3 ,  6 ,  14 ,  15 ,  16 ,  17 ,  18 ,  19\n\n [27] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Boots trapping language-image pre-training with frozen image encoders and large language models. Inter- national Conference on Machine Learning , 2023.  2 ,  3 ,  5 ,  6 , 7 ,  14 ,  15 ,  16 ,  17 ,  18 ,  19\n\n [28] Lei Li, Yuwei Yin, Shicheng Li, Liang Chen, Peiyi Wang, Shuhuai Ren, Mukai Li, Yazheng Yang, Jingjing Xu, Xu Sun, et al. M3it: A large-scale dataset towards multi- modal multilingual instruction tuning. arXiv preprint arXiv:2306.04387 , 2023.  3\n\n [29] Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, et al. Oscar: Object-semantics aligned pre-training for vision-language tasks. In  Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XXX 16 , pages 121–137. Springer, 2020.  3\n\n \n[30] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object hallucina- tion in large vision-language models. arXiv preprint arXiv:2305.10355 , 2023.  4\n\n [31] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll´ ar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13 , pages 740–755. Springer, 2014.  2 ,  3\n\n [32] Fuxiao Liu, Tianrui Guan, Zongxia Li, Lichang Chen, Yaser Yacoob, Dinesh Manocha, and Tianyi Zhou. Hallusion- bench: You see what you think? or you think what you see? an image-context reasoning benchmark challenging for gpt- 4v (ision), llava-1.5, and other multi-modality models.  arXiv preprint arXiv:2310.14566 , 2023.  4\n\n [33] Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang. Aligning large multi-modal model with robust instruction tuning. arXiv preprint arXiv:2306.14565 , 2023.  3\n\n [34] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. arXiv preprint arXiv:2310.03744 , 2023.  2 ,  3 ,  6 ,  7 ,  14 ,  15 ,  16 ,  17 , 18 ,  19\n\n [35] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning.  arXiv preprint arXiv:2304.08485 , 2023.  3\n\n [36] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player?  arXiv preprint arXiv:2307.06281 , 2023. 2 ,  4\n\n [37] Yuliang Liu, Zhang Li, Hongliang Li, Wenwen Yu, Mingxin Huang, Dezhi Peng, Mingyu Liu, Mingrui Chen, Chunyuan Li, Lianwen Jin, et al. On the hidden mystery of ocr in large multimodal models.  arXiv preprint arXiv:2305.07895 , 2023. 4\n\n [38] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pre training task-agnostic vi sio linguistic representations for vision-and-language tasks.  Advances in neural information processing systems , 32, 2019.  3\n\n [39] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. Advances in Neural Information Processing Systems , 35:2507–2521, 2022.  2\n\n [40] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathemat- ical reasoning of foundation models in visual contexts.  arXiv preprint arXiv:2310.02255 , 2023.  4\n\n [41] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: A visual question answering benchmark requiring external knowledge. In  Conference on Computer Vision and Pattern Recognition (CVPR) , 2019.  3\n\n "}
{"page": 10, "image_path": "doc_images/2311.16502v3_10.jpg", "ocr_text": "49\n\n50\n\n51\n\n52\n\n53\n\n54\n\nbenchmark requiring external knowledge. In Conference on\nComputer Vision and Pattern Recognition (CVPR), 2019. 3\nGrégoire Mialon, Clémentine Fourrier, Craig Swift, Thomas\nWolf, Yann LeCun, and Thomas Scialom. Gaia: a\nbenchmark for general ai assistants. arXiv preprint\narXiv:2311.12983, 2023. 1,4\n\nMasoud Monajatipoor, Liunian Harold Li, Mozhdeh\nRouhsedaghat, Lin F Yang, and Kai-Wei Chang. Metavl:\nTransferring in-context learning ability from language\nmodels to vision-language models. arXiv preprint\narXiv:2306.01311, 2023. 3\n\nMeredith Ringel Morris, Jascha Sohl-dickstein, Noah Fiedel,\nTris Warkentin, Allan Dafoe, Aleksandra Faust, Clement\nFarabet, and Shane Legg. Levels of agi: Opera-\ntionalizing progress on the path to agi. arXiv preprint\narXiv:2311.02462, 2023. 1, 3,9\n\nOpenAl. Gpt-4 technical report. arXiv preprint\narXiv:2303.08774, 2023. 1, 6, 14, 15, 16, 17, 18, 19\nOpenAl. Gpt-4v(ision) system card, 2023. 2, 6, 7, 14, 15,\n16, 17, 18, 19\n\nZhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan\nHuang, Shuming Ma, and Furu Wei. Kosmos-2: Ground-\ning multimodal large language models to the world. arXiv\npreprint arXiv:2306. 14824, 2023. 5, 6, 14, 15, 16, 17, 18, 19\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning, pages\n8748-8763. PMLR, 2021. 3, 5\n\nShaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.\nFaster r-cnn: Towards real-time object detection with region\nproposal networks. Advances in neural information process-\ning systems, 28, 2015. 3\n\nAmanpreet Singh, Vivek Natarjan, Meet Shah, Yu Jiang,\nXinlei Chen, Devi Parikh, and Marcus Rohrbach. Towards\nvqa models that can read. In Proceedings of the IEEE Con-\nference on Computer Vision and Pattern Recognition, pages\n8317-8326, 2019. 2\n\nHao Tan and Mohit Bansal. Lxmert: Learning cross-\nmodality encoder representations from transformers. In\nProceedings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th International\nJoint Conference on Natural Language Processing (EMNLP-\nIJCNLP), pages 5100-5111, 2019. 3\n\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste\nRoziére, Naman Goyal, Eric Hambro, Faisal Azhar, et al.\nLlama: Open and efficient foundation language models.\narXiv preprint arXiv:2302.13971, 2023. 1,5,7\n\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert,\nAmjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,\nSoumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.\nLlama 2: Open foundation and fine-tuned chat models. arXiv\npreprint arXiv:2307.09288, 2023. 6, 14, 15, 16, 17, 18, 19\nJunyang Wang, Yiyang Zhou, Guohai Xu, Pengcheng Shi,\nChenlin Zhao, Haiyang Xu, Qinghao Ye, Ming Yan, Ji\n\n11\n\n55\n\n56\n\n57\n\n58\n\n59\n\n60\n\n61\n\n62\n\n63\n\n64\n\n[65\n\nZhang, Jihua Zhu, et al. Evaluation and analysis of hal-\nlucination in large vision-language models. arXiv preprint\narXiv:2308.15126, 2023. 4\n\nWeihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji\nQi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan\nSong, et al. Cogvlm: Visual expert for pretrained language\nmodels. arXiv preprint arXiv:2311.03079, 2023. 2, 6, 14,\n15, 16, 17, 18, 19\n\nZirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia\nTsvetkov, and Yuan Cao. Simvlm: Simple visual language\nmodel pretraining with weak supervision. In Jnternational\nConference on Learning Representations, 2021. 3\n\nPeng Xu, Wenqi Shao, Kaipeng Zhang, Peng Gao, Shuo\nLiu, Meng Lei, Fanging Meng, Siyuan Huang, Yu Qiao,\nand Ping Luo. Lvlm-ehub: A comprehensive evaluation\nbenchmark for large vision-language models. arXiv preprint\narXiv:2306.09265, 2023. 4\n\nZhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang,\nChung-Ching Lin, Zicheng Liu, and Lijuan Wang. The dawn\nof Imms: Preliminary explorations with gpt-4v (ision). arXiv\npreprint arXiv:2309. 17421, 2023. 2\n\nQinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan,\nYiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi,\nYaya Shi, et al. mplug-owl: Modularization empowers\nlarge language models with multimodality. arXiv preprint\narXiv:2304.14178, 2023. 3\n\nQinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Haowei\nLiu, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou.\nmplug-owl2: Revolutionizing multi-modal large language\nmodel with modality collaboration. arXiv preprint\narXiv:2311.04257, 2023. 3, 6, 14, 15, 16, 17, 18, 19\nZhenfei Yin, Jiong Wang, Jianjian Cao, Zhelun Shi, Dingn-\ning Liu, Mukai Li, Lu Sheng, Lei Bai, Xiaoshui Huang,\nZhiyong Wang, et al. Lamm: Language-assisted multi-\nmodal instruction-tuning dataset, framework, and bench-\nmark. arXiv preprint arXiv:2306.06687, 2023. 2, 4\n\nJiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mo-\njtaba Seyedhosseini, and Yonghui Wu. Coca: Contrastive\ncaptioners are image-text foundation models. TMLR, 2022.\n3\n\nWeihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang,\nKevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang.\nMm-vet: Evaluating large multimodal models for integrated\ncapabilities. arXiv preprint arXiv:2308.02490, 2023. 2, 4\nPengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang,\nLei Zhang, Lijuan Wang, Yejin Choi, and Jianfeng Gao.\nVinvl: Revisiting visual representations in vision-language\nmodels. In Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, pages 5579-5588,\n2021. 3\n\nRenrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu,\nShilin Yan, Pan Lu, Hongsheng Li, Peng Gao, and Yu Qiao.\nLlama-adapter: Efficient fine-tuning of language models\nwith zero-init attention. arXiv preprint arXiv:2303.16199,\n2023. 3, 6, 14, 15, 16, 17, 18, 19\n\nBo Zhao, Boya Wu, and Tiejun Huang. Svit: Scaling up\nvisual instruction tuning. arXiv preprint arXiv:2307.04087,\n2023. 3\n\n", "vlm_text": "[42] Gr´ egoire Mialon, Cl´ ementine Fourrier, Craig Swift, Thomas Wolf, Yann LeCun, and Thomas Scialom. Gaia: a benchmark for general ai assistants. arXiv preprint arXiv:2311.12983 , 2023.  1 ,  4\n\n [43] Masoud Mona jati poor, Liunian Harold Li, Mozhdeh Rou h seda ghat, Lin F Yang, and Kai-Wei Chang. Metavl: Transferring in-context learning ability from language models to vision-language models. arXiv preprint arXiv:2306.01311 , 2023.  3\n\n [44] Meredith Ringel Morris, Jascha Sohl-dickstein, Noah Fiedel, Tris Warkentin, Allan Dafoe, Aleksandra Faust, Clement Farabet, and Shane Legg. Levels of agi: Opera- tion ali zing progress on the path to agi. arXiv preprint arXiv:2311.02462 , 2023.  1 ,  3 ,  9\n\n [45] OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774 , 2023.  1 ,  6 ,  14 ,  15 ,  16 ,  17 ,  18 ,  19\n\n [46] OpenAI. Gpt-4v(ision) system card, 2023.  2 ,  6 ,  7 ,  14 ,  15 , 16 ,  17 ,  18 ,  19\n\n [47] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. Kosmos-2: Ground- ing multimodal large language models to the world.  arXiv preprint arXiv:2306.14824 , 2023.  5 ,  6 ,  14 ,  15 ,  16 ,  17 ,  18 ,  19\n\n [48] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervi- sion. In  International conference on machine learning , pages 8748–8763. PMLR, 2021.  3 ,  5\n\n [49] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks.  Advances in neural information process- ing systems , 28, 2015.  3\n\n [50] Amanpreet Singh, Vivek Natarjan, Meet Shah, Yu Jiang, Xinlei Chen, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read. In  Proceedings of the IEEE Con- ference on Computer Vision and Pattern Recognition , pages 8317–8326, 2019.  2\n\n [51] Hao Tan and Mohit Bansal. Lxmert: Learning cross- modality encoder representations from transformers. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP- IJCNLP) , pages 5100–5111, 2019.  3\n\n [52] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth´ ee Lacroix, Baptiste Rozi\\` ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 , 2023.  1 ,  5 ,  7\n\n [53] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models.  arXiv preprint arXiv:2307.09288 , 2023.  6 ,  14 ,  15 ,  16 ,  17 ,  18 ,  19\n\n [54] Junyang Wang, Yiyang Zhou, Guohai Xu, Pengcheng Shi, Chenlin Zhao, Haiyang Xu, Qinghao Ye, Ming Yan, Ji Zhang, Jihua Zhu, et al. Evaluation and analysis of hal- lucination in large vision-language models.  arXiv preprint arXiv:2308.15126 , 2023.  4\n\n \n[55] Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, et al. Cogvlm: Visual expert for pretrained language models.  arXiv preprint arXiv:2311.03079 , 2023.  2 ,  6 ,  14 , 15 ,  16 ,  17 ,  18 ,  19\n\n [56] Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, and Yuan Cao. Simvlm: Simple visual language model pre training with weak supervision. In  International Conference on Learning Representations , 2021.  3\n\n [57] Peng Xu, Wenqi Shao, Kaipeng Zhang, Peng Gao, Shuo Liu, Meng Lei, Fanqing Meng, Siyuan Huang, Yu Qiao, and Ping Luo. Lvlm-ehub: A comprehensive evaluation benchmark for large vision-language models.  arXiv preprint arXiv:2306.09265 , 2023.  4\n\n [58] Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, and Lijuan Wang. The dawn of lmms: Preliminary explorations with gpt-4v (ision).  arXiv preprint arXiv:2309.17421 , 2023.  2\n\n [59] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. mplug-owl: Modular iz ation empowers large language models with multi modality.  arXiv preprint arXiv:2304.14178 , 2023.  3\n\n [60] Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Haowei Liu, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou. mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration. arXiv preprint arXiv:2311.04257 , 2023.  3 ,  6 ,  14 ,  15 ,  16 ,  17 ,  18 ,  19\n\n [61] Zhenfei Yin, Jiong Wang, Jianjian Cao, Zhelun Shi, Dingn- ing Liu, Mukai Li, Lu Sheng, Lei Bai, Xiaoshui Huang, Zhiyong Wang, et al. Lamm: Language-assisted multi- modal instruction-tuning dataset, framework, and bench- mark.  arXiv preprint arXiv:2306.06687 , 2023.  2 ,  4\n\n [62] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mo- jtaba Seyed hosseini, and Yonghui Wu. Coca: Contrastive captioners are image-text foundation models.  TMLR , 2022. 3\n\n [63] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities.  arXiv preprint arXiv:2308.02490 , 2023.  2 ,  4\n\n [64] Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang, Yejin Choi, and Jianfeng Gao. Vinvl: Revisiting visual representations in vision-language models. In  Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 5579–5588, 2021.  3\n\n [65] Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao, and Yu Qiao. Llama-adapter: Efficient fine-tuning of language models with zero-init attention.  arXiv preprint arXiv:2303.16199 , 2023.  3 ,  6 ,  14 ,  15 ,  16 ,  17 ,  18 ,  19\n\n [66] Bo Zhao, Boya Wu, and Tiejun Huang. Svit: Scaling up visual instruction tuning.  arXiv preprint arXiv:2307.04087 , 2023.  3 "}
{"page": 11, "image_path": "doc_images/2311.16502v3_11.jpg", "ocr_text": "67\n\n68\n\n69\n\n70\n\n71\n\n72\n\n73\n\n[74]\n\nHaozhe Zhao, Zefan Cai, Shuzheng Si, Xiaojian Ma, Kaikai\nAn, Liang Chen, Zixuan Liu, Sheng Wang, Wenjuan Han,\nand Baobao Chang. Mmicl: Empowering vision-language\nmodel with multi-modal in-context learning. arXiv preprint\narXiv:2309.07915, 2023. 3\n\nYungqing Zhao, Tianyu Pang, Chao Du, Xiao Yang, Chongx-\nuan Li, Ngai-Man Cheung, and Min Lin. On evaluating ad-\nversarial robustness of large vision-language models. arXiv\npreprint arXiv:2305.16934, 2023. 4\n\nWanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang,\nShuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan\nDuan. Agieval: A human-centric benchmark for evaluating\nfoundation models. arXiv preprint arXiv:2304.06364, 2023.\n2\n\nLuowei Zhou, Hamid Palangi, Lei Zhang, Houdong Hu, Ja-\nson Corso, and Jianfeng Gao. Unified vision-language pre-\ntraining for image captioning and vqa. In Proceedings of\nthe AAAI conference on artificial intelligence, pages 13041-\n13049, 2020. 3\n\nDeyao Zhu, Jun Chen, Xiaogian Shen, Xiang Li, and Mo-\nhamed Elhoseiny. Minigpt-4: Enhancing vision-language\nunderstanding with advanced large language models. arXiv\npreprint arXiv:2304. 10592, 2023. 3, 6, 14, 15, 16, 17, 18, 19\nGemini Team, Google. Gemini: A Family of Highly Ca-\npable Multimodal Models. Available online at: https:\n//storage.googleapis.com/deepmind-media/\ngemini/gemini_1l_report.pdf, 2023. 6, 14, 15, 16,\n17, 18, 19, 117\n\nZiyi Lin, Chris Liu, Renrui Zhang, Peng Gao, Longtian\nQiu, Han Xiao, Han Qiu, Chen Lin, Wenqi Shao, Keqin\nChen, Jiaming Han, Siyuan Huang, Yichi Zhang, Xuming\nHe, Hongsheng Li, and Yu Qiao. SPHINX: The Joint Mixing\nof Weights, Tasks, and Visual Embeddings for Multi-modal\nLarge Language Models. arXiv preprint arXiv:2311.07575,\n2023. 6, 14, 15, 16, 17, 18, 19\n\nQwen-VL-PLUS. GitHub Repository, 2023. URL https:\n//github.com/QwenLM/Qwen-VL?tab=readme-\nov-file#qwen-vl-plus. 6, 14, 15, 16, 17, 18, 19\n\n12\n", "vlm_text": "[67] Haozhe Zhao, Zefan Cai, Shuzheng Si, Xiaojian Ma, Kaikai An, Liang Chen, Zixuan Liu, Sheng Wang, Wenjuan Han, and Baobao Chang. Mmicl: Empowering vision-language model with multi-modal in-context learning.  arXiv preprint arXiv:2309.07915 , 2023.  3\n\n [68] Yunqing Zhao, Tianyu Pang, Chao Du, Xiao Yang, Chongx- uan Li, Ngai-Man Cheung, and Min Lin. On evaluating ad- versarial robustness of large vision-language models.  arXiv preprint arXiv:2305.16934 , 2023.  4\n\n [69] Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. Agieval: A human-centric benchmark for evaluating foundation models.  arXiv preprint arXiv:2304.06364 , 2023. 2\n\n [70] Luowei Zhou, Hamid Palangi, Lei Zhang, Houdong Hu, Ja- son Corso, and Jianfeng Gao. Unified vision-language pre- training for image captioning and vqa. In  Proceedings of the AAAI conference on artificial intelligence , pages 13041– 13049, 2020.  3\n\n [71] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mo- hamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models.  arXiv preprint arXiv:2304.10592 , 2023.  3 ,  6 ,  14 ,  15 ,  16 ,  17 ,  18 ,  19\n\n [72] Gemini Team, Google. Gemini: A Family of Highly Ca- pable Multimodal Models. Available online at:  https: //storage.googleapis.com/deepmind-media/ gemini/gemini 1 report.pdf , 2023.  6 ,  14 ,  15 ,  16 , 17 ,  18 ,  19 ,  117\n\n [73] Ziyi Lin, Chris Liu, Renrui Zhang, Peng Gao, Longtian Qiu, Han Xiao, Han Qiu, Chen Lin, Wenqi Shao, Keqin Chen, Jiaming Han, Siyuan Huang, Yichi Zhang, Xuming He, Hongsheng Li, and Yu Qiao. SPHINX: The Joint Mixing of Weights, Tasks, and Visual Embeddings for Multi-modal Large Language Models.  arXiv preprint arXiv:2311.07575 , 2023.  6 ,  14 ,  15 ,  16 ,  17 ,  18 ,  19\n\n [74] Qwen-VL-PLUS. GitHub Repository, 2023. URL  https: //github.com/QwenLM/Qwen-VL?tab=readme- "}
{"page": 12, "image_path": "doc_images/2311.16502v3_12.jpg", "ocr_text": "MMMU: A Massive Multi-discipline Multimodal\nUnderstanding and Reasoning Benchmark for Expert AGI\n\nSupplementary Material\n\nTable of Contents in Appendix\n\nA Breakdown Results on Different Subjects 14\nA.L Art & Design. 2... ee 14\nA.2 Business 2... 2.0.0... .2. 02.0000. 15\nA. Science... 2... ee ee 16\nAA4 Health & Medicine. ............0. 17\nA.5, Humanities & Social Science ........ 18\nA.6 Tech & Engineering ............. 19\n\nB Case Study 20\n\nC Subfields of Different Subjects 110\n\nD Distributions of Image Types 110\n\nE Results on Different Image Types 110\n\nF. Few-shot Results 113\n\nG Data Annotation Protocol 114\nG.1 DataCollection............0.0. 114\nG.2 General Guidelines... .......0..0. 114\nG.3, Data Format and Structure .......... 114\nG.4 Quality Control and Validation. ....... 114\nG.5, Handling Ambiguities ............ 114\nG.6 Ethical Considerations ............ 114\nG.7. Data Contamination Considerations ..... 114\nG.8 Example Questions.............. 114\n\nH Author Contribution Statement 115\n\nI. Version Change Log 117\n\n13\n", "vlm_text": "MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI \nSupplementary Material \nTable of Contents in Appendix \nA . Breakdown Results on Different Subjects \nA.1 . Art & Design  . . . . . . . . . . . . . . . . . 14 A.2 . Business . . . . . . . . . . . . . . . . . . . 15 A.3 . Science  . . . . . . . . . . . . . . . . . . . . 16 A.4 . Health & Medicine  . . . . . . . . . . . . . . 17 A.5 . Humanities & Social Science . . . . . . . . 18 A.6 . Tech & Engineering . . . . . . . . . . . . . 19 \nB . Case Study 20 C . Subfields of Different Subjects 110 D . Distributions of Image Types 110 E . Results on Different Image Types 110 \nG.1 . Data Collection  . . . . . . . . . . . . . . . . 114 G.2 . General Guidelines  . . . . . . . . . . . . . . 114 G.3 . Data Format and Structure  . . . . . . . . . . 114 G.4 . Quality Control and Validation  . . . . . . . . 114 G.5 . Handling Ambiguities  . . . . . . . . . . . . 114 G.6 . Ethical Considerations  . . . . . . . . . . . . 114 G.7 . Data Contamination Considerations  . . . . . 114 G.8 . Example Questions  . . . . . . . . . . . . . . 114 \nH . Author Contribution Statement 115 "}
{"page": 13, "image_path": "doc_images/2311.16502v3_13.jpg", "ocr_text": "A. Breakdown Results on Different Subjects\n\nIn this appendix, we show breakdown results of different\nmodels on each discipline and subject.\n\nA.1. Art & Design\nValidation Test . ‘\nOverall Overall Art Art Theory Design Music\n(120) (1,163) (231) (429) (169) (334)\nRandom Choice 29.2 24.1 23.4 20.3 19.5 31.7\nFrequent Choice 23.3 26.7 24.2 23.5 33.7 29.0\nLarge Multimodal Models (LMMs): Text + Image as Input\nOpenFlamingo2-9B [4 40.0 31.7 36.8 28.4 27.8 34.4\nKosmos2 [47] 25.0 28.8 30.7 24.9 28.4 32.6\nFuyu-8B [6] 36.7 29.9 28.6 26.8 29.0 35.3\nMiniGPT4-Vicuna-13B [71] 29.2 30.2 28.6 28.7 40.2 28.1\nLLaMA-Adapter2-7B [65] 29.2 35.2 38.5 35.4 41.4 29.3\nOtter [26] 37.5 37.4 40.7 35.9 46.2 32.6\nCogVLM [55] 40.8 38.0 43.3 39.2 44.4 29.6\nInstructBLIP-T5-XL [14] 40.0 43.3 49.8 45.0 52.1 32.3\nBLIP-2 FLAN-T5-XL [27] 44.2 43.0 50.2 45.0 47.3 33.2\nmPLUG-OWL2* [60] 45.8 48.5 57.6 53.4 59.8 30.2\nSPHINX* [73] 48.3 - - - - .\nQwen-VL-7B [5] 51.7 47.7 57.1 49.7 58.6 33.2\nLLaVA-1.5-13B [34] 51.7 49.8 58.4 51.5 615 35.6\nInstructBLIP-T5-XXL [14] 44.2 48.5 51.9 52.7 60.4 34.7\nBLIP-2 FLAN-T5-XXL [27] 417 49.2 54.5 51.5 64.5 34.7\nQwen-VL-PLUS* [74] 60.0 59.9 67.5 68.1 78.7 34.7\nGPT-4V(ision) (Playground) [46] 65.8 65.3 74.0 755 80.5 38.6\nGemini Ultra* [72] 70.0 - - - - -\nLarge Language Models (LLMs): Only Text as Input\nLlama2 7B [53] 29.2 30.7 30.3 275 37.9 31.4\nFLAN-TS5-XXL [12] 38.3 36.8 32.0 36.8 52.1 32.3\n+ OCR 37.5 36.2 36.4 33.8 47.9 33.2\n+ LLaVA Caption 43.3 38.4 45.9 38.2 46.2 29.6\nVicuna-13B [10] 41.7 35.1 35.1 31.5 46.7 33.8\n+ OCR 39.2 37.1 35.5 32.9 50.3 36.8\n+ LLaVA Caption 38.3 42.0 51.1 42.7 46.2 32.6\nGPT-4 Text [45] 35.0 32.9 35.1 28.7 47.3 29.6\n\nTable 4. Art & Design results of different models on the MMMU validation and test set. The best-performing model in each category is\nin-bold, and the second best is underlined. *: results provided by the authors.\n\n14\n", "vlm_text": "A. Breakdown Results on Different Subjects \nIn this appendix, we show breakdown results of different models on each discipline and subject. \nA.1. Art & Design \nThis table presents performance scores for various models on validation and test sets across different disciplines such as Art, Art Theory, Design, and Music. Here’s a breakdown:\n\n1. **Random Choice**:\n   - Validation Overall: 29.2\n   - Test Overall: 24.1\n\n2. **Frequent Choice**:\n   - Validation Overall: 23.3\n   - Test Overall: 26.7\n\n3. **Large Multimodal Models (LMMs) with Text + Image as Input**:\n   - Models listed include OpenFlamingo2-9B, Kosmos2, Fuyu-8B, etc.\n   - Qwen-VL-7B and LaVA-1.5-13B have the highest scores on the Test Overall category at 51.7.\n   - mPLUG-OWL2* shows the highest scores in individual categories like Art, Art Theory, Design, and Music with scores of 57.6, 53.4, 59.8, and 30.2 respectively.\n\n4. **Large Language Models (LLMs) with Only Text as Input**:\n   - Models listed include Llama2 7B, FLAN-T5-XXL, Vicuna-13B, etc.\n   - FLAN-T5-XXL (with OCR and LLAVA Caption) ranks high with Test Overall scores of 42.0 and strengths in Art and Art Theory at 51.1 and 42.7 respectively.\n\nOverall, the table displays and compares how different models perform across various tasks and dataset categories."}
{"page": 14, "image_path": "doc_images/2311.16502v3_14.jpg", "ocr_text": "A.2. Business\n\nValidation Test\nOverall Overall\n(150) (1,428) (380) (267) (355) (245) (181)\n\n24.7 24.9 30.0 29.6 17.7 22.4 24.9\n\nAccounting Economics Finance Manage Marketing\n\nRandom Choice\nFrequent Choice 29.3 28.4 33.4 36.3 22.0 15.9 35.9\n\nLarge Multimodal Models (LMMs): Text + Image as Input\n\nOpenFlamingo2-9B [4 28.0 23.5 24.7 25.8 94 25.3 22.7\nKosmos2 [47] 18.0 23.7 29.7 24.0 214 22.4 17.1\nFuyu-8B [6] 32.0 27.0 32.1 30.3 22.5 20.0 29.3\nMiniGPT4-Vicuna-13B [71] 21.3 27.0 29.7 34.1 25.6 16.7 27.6\nLLaMA-Adapter2-7B [65] 25.3 25.4 30.8 24.7 20.6 24.9 25.4\nOtter [26] 24.0 24.0 30.8 29.6 15 16.3 24.9\nCogVLM [55] 25.3 25.6 28.2 29.6 9.2 21.2 32.6\nInstructBLIP-T5-XL [14] 28.0 25.2 27.6 31.8 8.0 22.0 28.7\nBLIP-2 FLAN-T5-XL [27] 26.7 25.6 28.2 31.1 75 24.1 29.8\nmPLUG-OWL2* [60] 24.7 25.6 28.7 29.2 20.3 22.4 28.7\nSPHINX* [73] 24.7 - - - - - -\nQwen-VL-7B [5] 29.3 29.8 34.2 29.6 8.9 32.2 38.7\nLLaVA-1.5-13B [34] 22.7 28.2 29.2 33.3 23.7 23.3 34.3,\nInstructBLIP-T5-XXL [14] 24.0 30.6 34.2 35.6 23.4 30.2 30.4\nBLIP-2 FLAN-T5-XXL [27] 30.0 28.6 32.4 33.3 22.5 26.1 28.7\nQwen-VL-PLUS* [74] 35.3 34.5 35.0 41.6 26.2 34.7 39.2\nGemini Ultra* [72] 56.7 - - - - - -\nGPT-4V(ision) (Playground) [46] 59.3 64.3 69.7 70.8 61.1 51.0 67.4\nLarge Language Models (LLMs): Only Text as Input\nLlama2 7B [53] 22.7 27.2 28.9 34.1 23.7 21.6 27.6\nFLAN-T5-XXL [12] 28.0 28.9 31.6 31.5 23.1 29.0 30.4\n+ OCR 29.3 28.8 32.4 30.0 24.8 26.9 29.8\n+ LLaVA Caption 31.3 27.8 28.2 30.7 24.2 27.8 29.8\nVicuna-13B [10] 26.7 30.1 29.5 34.8 25.6 30.6 32.6\n+ OCR 31.3 28.6 27.1 34.1 23.9 30.6 30.4\n+ LLaVA Caption 26.0 26.8 27.1 32.6 22.3 25.3 28.7\nGPT-4Text(45)ititi“‘ié‘ TS (COS OT 35.2 21 322 24\n\nTable 5. Business results of different models on the MMMU validation and test set. The best-performing model in each category is in-bold,\nand the second best is underlined. *: results provided by the authors.\n\n15\n", "vlm_text": "The table summarizes performance metrics for different language models on various tasks. It is divided into two main sections: Large Multimodal Models (LMMs) that use both text and image inputs, and Large Language Models (LLMs) that use only text inputs. The performance is evaluated in terms of validation and test accuracy across different subjects like Accounting, Economics, Finance, Management, and Marketing.\n\nKey observations:\n\n1. **Random and Frequent Choice** benchmarks are provided for context, showing relatively lower baseline accuracies.\n   \n2. **Large Multimodal Models (LMMs)**:\n   - The models listed include OpenFlamingo2-9B, Kosmos2, Fuyu-8B, MiniGPT4-Vicuna-13B, among others.\n   - GPT-4V(ision) (Playground) demonstrates the highest performance across most tasks, particularly on the test overall and various subject-specific metrics.\n   \n3. **Large Language Models (LLMs)**:\n   - Models like Llama2 7B, FLAN-T5-XXL, and Vicuna-13B are evaluated.\n   - GPT-4 Text generally has strong performance, although lower than GPT-4V(ision) in the multimodal category.\n\nOverall, multimodal models with text and image inputs tend to outperform those with only text inputs, especially GPT-4V(ision) showing high efficacy across tasks."}
{"page": 15, "image_path": "doc_images/2311.16502v3_15.jpg", "ocr_text": "A.3. Science\n\nValidation Test\nOverall Overall\n(150) (2,426) (345) (603) (565) (505) (408)\n\nBiology Chemistry Geography Math Physics\n\nRandom Choice 18.0 21.6 18.3 18.6 26.0 22.2 22.1\nFrequent Choice 27.3 24.0 25.8 19.9 26.9 26.1 22.1\n\nLarge Multimodal Models (LMMs): Text + Image as Input\n\nOpenFlamingo2-9B [4] 23.3 26.3 27.8 22.9 30.8 25.1 25.0\nKosmos? [47] 19.3 26.6 28.4 21.7 29.2 26.7 28.4\nFuyu-8B [6] 22.0 25.6 27.8 20.9 30.1 24.8 25.7\nMiniGPT4-Vicuna-13B [71] 28.7 26.2 23.2 22.1 29.4 30.1 25.5\nLLaMA-Adapter2-7B [65] 30.7 25.6 27.5 24.9 30.4 23.0 21.3\nOtter [26] 34.7 24.1 24.6 23.4 27.1 23.0 21.8\nCogVLM [55] 28.0 25.1 29.3 24.2 28.0 23.4 21.1\nInstructBLIP-T5-XL [14] 32.7 25.2 27.0 22.1 28.3 24.4 25.0\nBLIP-2 FLAN-T5-XL [27] 30.7 25.1 26.7 24.4 25.7 24.0 25.2\nmPLUG-OWL2* [60] 22.7 24.9 27.2 23.9 29.7 18.8 25.2\nSPHINX* [73] 26.7 - - - - - -\nQwen-VL-7B [5] 29.3 25.6 27.8 23.1 28.8 24.6 24.3\nLLaVA-1.5-13B [34] 29.3 25.9 27.2 25.0 28.8 24.0 24.5\nInstructBLIP-T5-XXL [14] 30.7 27.6 29.0 26.5 31.0 25.5 26.0\nBLIP-2 FLAN-T5-XXL [27] 34.7 27.3 28.4 25.5 29.4 27.5 26.0\nQwen-VL-PLUS* [74] 37.3 32.8 40.3 27.9 34.7 31.3 33.1\nGemini Ultra* [72] 48.0 - - - - - -\nGPT-4V(ision) (Playground) [46] 54.7 48.4 52.2 46.9 44.8 45.0 56.4\nLarge Language Models (LLMs): Only Text as Input\nLlama2 7B [53 34.0 26.7 28.4 21.4 29.7 28.5 26.7\nFLAN-T5-XXL [12] 28.0 26.7 27.8 24.4 27.3 30.7 23.5\n+ OCR 30.0 26.2 24.6 24.5 274 27.9 26.0\n+ LLaVA Caption 32.7 27.0 25.8 23.9 30.3 29.1 25.5\nVicuna-13B [10] 23.3 24.7 24.6 22.7 25.0 26.1 25.7\n+ OCR 30.0 26.5 26.4 24.7 29.0 27.1 25.2\n+ LLaVA Caption 28.7 26.2 31.3 21.7 28.7 26.7 24.3\nGPT-4Text(45) ss (assti(té‘éShT:~OO2!2!| 80H)|6 608) 340 27.7 34.3\n\nTable 6. Science results of different models on the MMMU validation and test set. The best-performing model in each category is in-bold,\nand the second best is underlined. *: results provided by the authors.\n\n16\n", "vlm_text": "The table presents performance data of various models on different subjects with two main categories: \n\n1. **Large Multimodal Models (LMMs): Text + Image as Input**\n2. **Large Language Models (LLMs): Only Text as Input**\n\n### Performance Metrics:\n\n- **Validation Overall** and **Test Overall** scores out of 150 and 2,426 respectively.\n- Scores across different subjects: Biology, Chemistry, Geography, Math, and Physics, with data out of specific totals for each subject.\n\n### Specific Scores:\n\nFor each model, scores are provided for:\n- **Validation Overall**\n- **Test Overall**\n- Scores for Biology, Chemistry, Geography, Math, and Physics.\n\n### Models:\n- Listed with scores indicating their performance across the mentioned metrics and subjects.\n\n### Other Information:\n- Random and frequent choice baselines included.\n- Some models with OCR or additional configurations.\n- The table also includes reference numbers for each model indicating further documentation or studies."}
{"page": 16, "image_path": "doc_images/2311.16502v3_16.jpg", "ocr_text": "A.4. Health & Medicine\n\nValidation Test Basic Medical Clinical Diagnostics & Pharmac Public\nOverall Overall Science Meicine Lab. Medicine Y Health\n\n(150) (1,752) (326) (325) (162) (430) (509)\n\nRandom Choice 20.7 25.3 24.8 21.8 25.9 28.6 24.8\n\nFrequent Choice 30.0 24.4 22.1 24.3 17.3 23.3 29.3\n\nLarge Multimodal Models (LMMs): Text + Image as Input\n\nOpenFlamingo2-9B [4] 27.3 26.3 29.1 21.8 22.2 32.1 23.8\nKosmos? [47] 28.0 27.2 27.3 24.0 27.2 30.7 26.\n\nFuyu-8B [6] 28.0 27.0 28.8 23.1 24.1 27.0 29.3\n\nMiniGPT4-Vicuna-13B [71] 30.7 26.9 27.0 26.2 21.6 27.7 28.5\n\nLLaMA-Adapter2-7B [65] 30.7 30.0 31.0 30.2 26.5 36.5 25.0\n\nOtter [26] 30.7 29.6 34.4 28.3 28.4 28.6 28.5\n\nCogVLM [55] 32.0 31.2 33.4 27.4 27.2 33.7 31.4\nInstructBLIP-T5-XL [14] 28.7 29.3 31.3 28.9 22.8 34.2 26.\n\nBLIP-2 FLAN-T5-XL [27 35.3 31.8 35.9 31.7 24.1 35.8 28.5\n\nmPLUG-OWL2* [60] 32.0 32.8 29.9 32.3 34.0 31.2 29.7\nSPHINX* [73] 30.7 - - - - - -\n\nQwen-VL-7B [5] 33.3 33.6 38.0 34.8 32.1 29.5 33.8\n\nLLaVA-1.5-13B [34] 38.7 34.9 42.6 36.6 34.6 32.1 31.4\n\nInstructBLIP-T5-XXL [14 35.3 33.6 35.6 32.3 29.6 34.2 33.8\n\nBLIP-2 FLAN-T5-XXL [27] 32.0 33.7 38.7 34.5 27.2 33.7 32.2\n\nQwen-VL-PLUS* [74] 46.7 43.7 49.7 42.2 34.0 46.5 41.5\n\nGPT-4V(ision) (Playground) [46] 64.7 63.5 65.0 62.5 43.8 68.1 65.4\nGemini Ultra* [72] 67.3 - - - - - -\n\nLarge Language Models (LLMs): Only Text as Input\n\nLlama2 7B [53 26.7 27.7 26.1 30.8 25.3 27.7 27.7\n\nFLAN-TS-XXL [12] 32.0 32.8 33.7 34.8 30.2 34.4 30.5\n\n+OCR 32.7 32.6 33.7 35.1 27.8 32.3 32.2\n\n+ LLaVA Caption 32.0 33.2 35.3 34.2 30.9 32.6 32.4\n\nVicuna-13B [10] 31.3 31.4 37.7 33.2 36.4 27.7 27.9\n\n+OCR 31.3 32.0 38.3 33.5 37.0 28.4 28.5\n\n+ LLaVA Caption 34.0 33.4 37.1 35.4 32.7 32.6 30.6\n\nGPT-4 Text [45] 40.7 41.3 52.5 52.9 27.8 39.1 33.0\n\nTable 7. Health & Medicine results of different models on the MMMU validation and test set. The best-performing model in each category\nis in-bold, and the second best is underlined. *: results provided by the authors.\n\n17\n", "vlm_text": "The table shows the performance of different models across various categories. It is divided into two main sections: \"Large Multimodal Models (LMMs): Text + Image as Input\" and \"Large Language Models (LLMs): Only Text as Input.\" Each model is evaluated based on its \"Validation Overall,\" \"Test Overall,\" and performance in specific areas of medical sciences such as \"Basic Medical Science,\" \"Clinical Medicine,\" \"Diagnostics & Lab. Medicine,\" \"Pharmacy,\" and \"Public Health.\" The numbers under each category represent the performance scores of the models on a relevant task, likely a percentage or other metric indicating accuracy or proficiency. Random and Frequent Choice values are also provided as baselines for comparison. The table provides insight into how different models perform across these medical fields."}
{"page": 17, "image_path": "doc_images/2311.16502v3_17.jpg", "ocr_text": "A.5. Humanities & Social Science\n\nValidation Test\n\nOverall Overall History Literature Sociology Psychology\n\n(120) (947) (278) (112) (252) (305)\nRandom Choice 20.0 22.8 22.3 24.1 27.0 19.3\nFrequent Choice 25.8 25.2 27.0 27.7 25.4 22.6\n\nLarge Multimodal Models (LMMs): Text + Image as Input\n\nOpenFlamingo2-9B [4] 30.8 27.9 24.5 42.0 29.0 24.9\nKosmos2 [47] 30.0 26.3 24.5 24.1 34.1 22.3\nFuyu-8B [6] 32.5 32.5 32.7 44.6 32.9 27.5\nMiniGPT4-Vicuna-13B [71] 29.2 30.9 30.9 47.3 30.6 25.2\nLLaMA-Adapter2-7B [65] 33.3 29.1 27.0 43.8 32.1 23.3\nOtter [26] 417 35.9 33.8 67.0 34.9 27.2\nCogVLM [55] 45.0 41.5 39.2 69.6 413 33.4\nInstructBLIP-T5-XL [14] 47.5 45.8 45.0 71.4 44.8 38.0\nBLIP-2 FLAN-T5-XL [27 50.0 48.0 48.2 76.8 47.2 38.0\nmPLUG-OWL2* [60] 45.8 46.7 46.0 74.1 44.4 39.0\nSPHINX* [73] 50.0 - - - - -\nQwen-VL-7B [5] 45.0 45.3 47.8 64.3 46.4 35.1\nLLaVA-1.5-13B [34] 53.3 54.7 58.6 76.8 512 45.9\nInstructBLIP-T5-XXL [14 49.2 49.8 48.6 72.3 51.2 41.6\nBLIP-2 FLAN-T5-XXL [27] 50.8 51.5 49.6 75.9 53.2 43.0\nQwen-VL-PLUS* [74] 65.8 65.5 69.8 79.5 63.9 57.7\nGPT-4V(ision) (Playground) [46] 72.5, 76.3 79.1 89.3 71.4 73.1\nGemini Ultra* [72] 78.3 - - - - -\nLarge Language Models (LLMs): Only Text as Input\nLlama2 7B [53 37.5 32.6 32.4 46.4 32.9 27.5\nFLAN-TS5-XXL [12] 42.5 44.8 46.8 56.2 39.7 43.0\n+ OCR 55.0 50.5 53.6 75.0 46.4 42.0\n+ LLaVA Caption 49.2 49.9 51.8 75.0 46.8 41.6\nVicuna-13B [10] 45.8 44.8 51.1 59.8 39.3 38.0\n+ OCR 50.0 49.3 58.3 66.1 48.0 36.1\n+ LLaVA Caption 48.3 49.4 53.6 72.3 48.8 37.7\nGPr4text(45) ss st—<(<is*‘«< SSC‘ ‘NS!!! C8 OMS 57.7\n\nTable 8. Humanities & Social Science results of different models on the MMMU validation and test set. The best-performing model in\neach category is in-bold, and the second best is underlined. *: results provided by the authors.\n\n18\n", "vlm_text": "The table compares the performance of various models on validation and test sets across different subjects: History, Literature, Sociology, and Psychology. It categorizes models into:\n\n1. **Large Multimodal Models (LMMs): Text + Image as Input**\n   - Includes models like OpenFlamingo2-9B, Kosmos-2, Fuyu-8B, among others.\n   - Performance metrics are presented for each model in various subject areas.\n\n2. **Large Language Models (LLMs): Only Text as Input**\n   - Includes models such as Llama2 7B, FLAN-T5-XXL, Vicuna-13B, etc.\n   - Some models additionally use OCR and LlAVA Caption for enhanced performance.\n\nKey metrics shown include overall performance on validation and test sets, with detailed scores provided for each subject area in terms of percentage accuracy."}
{"page": 18, "image_path": "doc_images/2311.16502v3_18.jpg", "ocr_text": "A.6. Tech & Engineering\n\nVal Test . Arch. & Comp. Energy . Mech.\nOverall Overall Agri. Eng. Sci. Electr. &Power Materials Eng.\n(210) (2,784) (287) (551) (371) (256) (432) (458) (429)\nRandom Choice 21.4 24.8 27.0 22.6 10.5 31.5 24.2 28.7\nFrequent Choice 24.8 26.5 24.1 29.6 12.9 30.3 30.3 28.0\nLarge Multimodal Models (LMMs): Text + Image as Input\nOpenFlamingo2-9B [4] 26.2 25.1 20.6 29.6 26.1 13.7 24.1 26.0 28.4\nKosmos2 [47] 26.7 26.8 20.6 28.3 32.1 10.2 29.9 27.3 30.5\nFuyu-8B [6] 21.4 26.4 26.5 25.0 26.1 12.1 35.0 25.1 29.8\nMiniGPT4-Vicuna-13B [71] 23.8 27.2 29.6 23.8 28.8 13.7 36.1 27.3 27.5\nLLaMA-Adapter2-7B [65] 30.0 25.7 23.0 25.2 25.6 17.6 30.3 25.8 28.4\nOtter [26] 29.0 30.2 30.7 26.3 32.1 19.1 35.2 30.1 34.7\nCogVLM [55] 27.6 28.9 26.8 27.0 31.8 14.1 33.1 28.4 35.4\nInstructBLIP-T5-XL [14] 27. 28.6 26. 33.6 28.3 23.8 29.9 22.9 31.5\nBLIP-2 FLAN-T5-XL [27 27.6 27.8 17.8 32.5 26.7 20.7 33.6 24.9 30.8\nmPLUG-OWL2* [60] 31.0 29.6 32.4 29.4 31.8 14.5 39.4 26.6 28.2\nSPHINX* [73] 26.2 - - - - - - - -\nQwen-VL-7B [5] 32.9 30.2 33. 25.0 33.4 19.1 37.0 28.8 33.1\nLLaVA-1.5-13B [34] 31.4 28.3 34.5 26.1 29.6 22.7 30.1 26.9 28.9\nInstructBLIP-T5-XXL [14 35.2 29.4 24.7 30.3 29.6 20.7 37.3 26.6 31.5\nBLIP-2 FLAN-T5-XXL [27] 30.0 30.4 28.2 27.2 29.6 25.0 35.6 26.9 38.0\nQwen-VL-PLUS* [74] 36.7 32.9 40.4 25.6 36.1 24.6 33.6 34.7 36.8\nGPT-4V(ision) (Playground) [46] 36.7 41.7 43.9 37.2 57.1 27.0 47.5 36.9 41.0\nGemini Ultra* [72] 47.1 - - - - - - - -\nLarge Language Models (LLMs): Only Text as Input\nLlama2 7B [53 31.4 29.8 33. 23.8 32.6 17.6 39.1 27.9 32.6\nFLAN-TS5-XXL [12] 28.6 28.3 21.3 30.3 28.8 25.4 26.6 27.9 33.8\n+ OCR 30.0 29.7 20.2 30.7 31.3 27.0 29.9 29.0 35.9\n+ LLaVA Caption 27.6 28.7 17.8 31.4 26.4 24.2 32.4 26.4 35.7\nVicuna-13B [10] 34.8 30.1 31.7 26.3 28.8 25.4 39.4 26.4 32.6\n+ OCR 34.8 30.0 31.7 25.6 29.9 18.8 40.3 27.5 33.6\n+ LLaVA Caption 32.4 31.4 32.4 26.9 31.8 20.3 39.8 28.8 37.3\nGPT-4 Text [45] 20.0 28.4 28.9 25.6 33.4 17.2 38.4 23.6 28.9\n\nTable 9. Tech & Engineering results of different models on the MMMU validation and test set. The best-performing model in each category\nis in-bold, and the second best is underlined. *: results provided by the authors.\n\n19\n", "vlm_text": "The table presents a comparison of various models' performance across different domains. It is divided into two main sections: \n\n1. **Large Multimodal Models (LMMs): Text + Image as Input**\n   - Lists different models and their performance in various areas: Agriculture, Architecture & Engineering, Computer Science, Electrical Engineering, Energy & Power, Materials, and Mechanical Engineering.\n   - Shows \"Val Overall\" and \"Test Overall\" scores as a summary of model performance.\n\n2. **Large Language Models (LLMs): Only Text as Input**\n   - Similar structure as above, focusing on text-only inputs.\n\nEach row in the table represents a model, showing how well it performs across these domains with the scores. There are baselines (\"Random Choice\" and \"Frequent Choice\") for comparison.\n\nKey highlights:\n- The \"Gemini Ultra*\" and \"GPT-4V(vision) (Playground)\" models show higher scores in several domains when using text and image inputs.\n- \"Vicuna-13B\" and \"GPT-4 Text\" stand out in the text-only section."}
{"page": 19, "image_path": "doc_images/2311.16502v3_19.jpg", "ocr_text": "B.\n\nCase Study\n\nList of Case Study Figures\n\n8\n9\n\nSADNHWNK OC\n\nNAMAarnrnrarnb HHH H HHH HH WWWWWWWWWWNNNNNNNNNYD\nNDNMBWNKFTOAAADNEWNKF TUOAAADNMNEWNKF TOAAADNEWNK CY\n\nArt 1: Correct Case 2... 2. eee 22\nArt 2: Correct Case... 6. ee eee 23\nArt 3: Perceptual Error... 2... 2. 24\nArt Theory 1: CorrectCase...........0. 25\nArt Theory 2: Correct Case... 2.0.0.2... 26\nArt Theory 3: Lack of Knowledge ........ 27\nDesign 1: Correct Case... ..........0. 28\nDesign 2: Perceptual Error ............ 29\nDesign 3: Lack of Knowledge .......... 30\nMusic 1: CorrectCase ...........00.0. 31\nMusic 2: Perceptual Error, Lack of Knowledge 32\nMusic 3: Perceptual Error... 2... ...... 33\nAccounting 1: Correct Case... 2.2... 2 34\nAccounting 2: Perceptual Error... 2.2.2... 35\nEconomics 1: CorrectCase ..........0.. 36\nEconomics 2: Perceptual Error... 2.2.0... 37\nEconomics 3: Perceptual Error... ....... 38\nFinance 1: Correct Case... ........000. 39\nFinance 2: Reasoning Error... ......... 40\nManage 1: CorrectCase ............. 41\nManage 2: Perceptual Error... ......... 42\nMarketing 1: Correct Case ............ 43\nMarketing 2: Perceptual Error .. 2... 0... 44\nBiology 1: Correct Case .........0..0. 45\nBiology 2: Correct Case ............0. 46\nBiology 3: Reasoning Error... ......... 47\nBiology 4: Reasoning Error... ......... 48\nBiology 5: Reasoning Error... ......... 49\nChemistry 1: CorrectCase ...........0. 50\nChemistry 2: CorrectCase ............ 51\nChemistry 3: Perceptual Error, Reasoning Error . 52\nChemistry 4: Lack of Knowledge... ...... 53\nGeography 1: Correct Case... 2.2... 000. 54\nGeography 2: Reasoning Error .......... 55\nGeography 3: Perceptual Error, Reasoning Error. 56\nMath 1: CorrectCase ... 2... . eee 57\nMath 2: Perceptual Error... 2.2... 000. 58\nMath 3: Textual Understanding Error... .... 59\nMath 4: Reasoning Error... 2.2... 0.0.0. 60\nPhysics 1: Correct Case... 2... 22.000. 61\nPhysics 2: Perceptual Error... ......... 62\nBasic Medical Science 1: Correct Case... ... 63\nBasic Medical Science 2: Perceptual Error... . 64\nClinical Medicine 1: CorrectCase ........ 65\nClinical Medicine 2: CorrectCase ........ 66\nClinical Medicine 3: Correct Case ........ 67\nClinical Medicine 4: Perceptual Error ...... 68\nClinical Medicine 5: Lack of Knowledge... . . 69\nDiagnostics and Lab Medicine 1: Correct Case 70\n\n20\n\n57\n58\n59\n\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n2B\n74\n75\n76\n71\n78\n79\n80\n81\n82\n83\n84\n\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n\nDiagnostics and Lab Medicine 2: Perceptual Error 71\nDiagnostics and Lab Medicine 3: Reject to Answer 72\nDiagnostics and Lab Medicine 4: Perceptual Er-\n\nror, Lack of Knowledge... ........... 73\nPharmacy 1: CorrectCase ............ 74\nPharmacy 2: Lack of Knowledge ......... 75\nPharmacy 3: Lack of Knowledge ......... 76\nPublic Health 1: Correct Case .......... 77\nPublic Health 2: Textual Understanding Error . . 78\nPublic Health 3: Lack of Knowledge ....... 79\nHistory 1: CorrectCase... 2... 0.0000. 80\nHistory 2: CorrectCase.... 2... 0.0000. 81\nHistory 3: Perceptual Error... ......... 82\nHistory 4: Lack of Knowledge .......... 83\nLiterature 1: CorrectCase............. 84\nLiterature 2: Perceptual Error... ........ 85\nSociology 1: CorrectCase ... 2.2.0.0... 86\nSociology 2: Reasoning Error... ........ 87\nPsychology 1: Correct Case... 2... .0..0. 88\nPsychology 2: Perceptual Error. ......... 89\nAgriculture 1: CorrectCase...........0. 90\nAgriculture 2: Perceptual Error... ....... 91\nAgriculture 3: Perceptual Error... ....... 92\nAgriculture 4: Perceptual Error... ....... 93\nArchitecture and Engineering 1: Correct Case .. 94\nArchitecture and Engineering 2: Correct Case .. 95\nArchitecture and Engineering 3: Reasoning Error 96\nComputer Science 1: Correct Case ........ 97\nComputer Science 2: Perceptual Error, Lack of\n\nKnowledge .............0.0000.4 98\nComputer Science 3: Perceptual Error ...... 99\nComputer Science 4: Perceptual Error ...... 100\nElectronics 1: Correct Case... 2... 2.00. 101\nElectronics 2: Rejectto Answer ......... 102\nEnergy and Power 1: Correct Case ........ 103\nEnergy and Power 2: Reasoning Error... ... 104\nMaterials 1: Correct Case... 2... ...000. 105\nMaterials 2: Lack of Knowledge ......... 106\nMechanical Engineering 1: Correct Case... . . 107\nMechanical Engineering 2: Reasoning Error . . . 108\nMechanical Engineering 3: Reasoning Error . . . 109\n", "vlm_text": "B. Case Study \nList of Case Study Figures \n8 Art 1: Correct Case  . . . . . . . . . . . . . . . . 22 9 Art 2: Correct Case  . . . . . . . . . . . . . . . . 23 10 Art 3: Perceptual Error  . . . . . . . . . . . . . . 24 11 Art Theory 1: Correct Case  . . . . . . . . . . . . 25 12 Art Theory 2: Correct Case  . . . . . . . . . . . . 26 13 Art Theory 3: Lack of Knowledge  . . . . . . . . 27 14 Design 1: Correct Case  . . . . . . . . . . . . . . 28 15 Design 2: Perceptual Error  . . . . . . . . . . . . 29 16 Design 3: Lack of Knowledge . . . . . . . . . . 30 17 Music 1: Correct Case . . . . . . . . . . . . . . 31 18 Music 2: Perceptual Error, Lack of Knowledge . 32 19 Music 3: Perceptual Error  . . . . . . . . . . . . . 33 20 Accounting 1: Correct Case  . . . . . . . . . . . . 34 21 Accounting 2: Perceptual Error  . . . . . . . . . . 35 22 Economics 1: Correct Case  . . . . . . . . . . . . 36 23 Economics 2: Perceptual Error  . . . . . . . . . . 37 24 Economics 3: Perceptual Error  . . . . . . . . . . 38 25 Finance 1: Correct Case  . . . . . . . . . . . . . . 39 26 Finance 2: Reasoning Error  . . . . . . . . . . . . 40 27 Manage 1: Correct Case . . . . . . . . . . . . . 41 28 Manage 2: Perceptual Error  . . . . . . . . . . . . 42 29 Marketing 1: Correct Case . . . . . . . . . . . . 43 30 Marketing 2: Perceptual Error . . . . . . . . . . 44 31 Biology 1: Correct Case . . . . . . . . . . . . . 45 32 Biology 2: Correct Case . . . . . . . . . . . . . 46 33 Biology 3: Reasoning Error  . . . . . . . . . . . . 47 34 Biology 4: Reasoning Error  . . . . . . . . . . . . 48 35 Biology 5: Reasoning Error  . . . . . . . . . . . . 49 36 Chemistry 1: Correct Case . . . . . . . . . . . . 50 37 Chemistry 2: Correct Case . . . . . . . . . . . . 51 38 Chemistry 3: Perceptual Error, Reasoning Error  . 52 39 Chemistry 4: Lack of Knowledge  . . . . . . . . . 53 40 Geography 1: Correct Case  . . . . . . . . . . . . 54 41 Geography 2: Reasoning Error  . . . . . . . . . . 55 42 Geography 3: Perceptual Error, Reasoning Error  . 56 43 Math 1: Correct Case  . . . . . . . . . . . . . . . 57 44 Math 2: Perceptual Error  . . . . . . . . . . . . . 58 45 Math 3: Textual Understanding Error  . . . . . . . 59 46 Math 4: Reasoning Error  . . . . . . . . . . . . . 60 47 Physics 1: Correct Case  . . . . . . . . . . . . . . 61 48 Physics 2: Perceptual Error  . . . . . . . . . . . . 62 49 Basic Medical Science 1: Correct Case  . . . . . . 63 50 Basic Medical Science 2: Perceptual Error  . . . . 64 51 Clinical Medicine 1: Correct Case  . . . . . . . . 65 52 Clinical Medicine 2: Correct Case  . . . . . . . . 66 53 Clinical Medicine 3: Correct Case  . . . . . . . . 67 54 Clinical Medicine 4: Perceptual Error . . . . . . 68 55 Clinical Medicine 5: Lack of Knowledge  . . . . . 69 56 Diagnostics and Lab Medicine 1: Correct Case . 70 57 Diagnostics and Lab Medicine 2: Perceptual Error 71 58 Diagnostics and Lab Medicine 3: Reject to Answer  72 59 Diagnostics and Lab Medicine 4: Perceptual Er- ror, Lack of Knowledge  . . . . . . . . . . . . . . 73 60 Pharmacy 1: Correct Case . . . . . . . . . . . . 74 61 Pharmacy 2: Lack of Knowledge  . . . . . . . . . 75 62 Pharmacy 3: Lack of Knowledge  . . . . . . . . . 76 63 Public Health 1: Correct Case . . . . . . . . . . 77 64 Public Health 2: Textual Understanding Error  . . 78 65 Public Health 3: Lack of Knowledge  . . . . . . . 79 66 History 1: Correct Case  . . . . . . . . . . . . . . 80 67 History 2: Correct Case  . . . . . . . . . . . . . . 81 68 History 3: Perceptual Error  . . . . . . . . . . . . 82 69 History 4: Lack of Knowledge  . . . . . . . . . . 83 70 Literature 1: Correct Case  . . . . . . . . . . . . . 84 71 Literature 2: Perceptual Error  . . . . . . . . . . . 85 72 Sociology 1: Correct Case . . . . . . . . . . . . 86 73 Sociology 2: Reasoning Error  . . . . . . . . . . . 87 74 Psychology 1: Correct Case  . . . . . . . . . . . . 88 75 Psychology 2: Perceptual Error  . . . . . . . . . . 89 76 Agriculture 1: Correct Case  . . . . . . . . . . . . 90 77 Agriculture 2: Perceptual Error  . . . . . . . . . . 91 78 Agriculture 3: Perceptual Error  . . . . . . . . . . 92 79 Agriculture 4: Perceptual Error  . . . . . . . . . . 93 80 Architecture and Engineering 1: Correct Case  . . 94 81 Architecture and Engineering 2: Correct Case  . . 95 82 Architecture and Engineering 3: Reasoning Error 96 83 Computer Science 1: Correct Case  . . . . . . . . 97 84 Computer Science 2: Perceptual Error, Lack of Knowledge . . . . . . . . . . . . . . . . . . . . 98 85 Computer Science 3: Perceptual Error  . . . . . . 99 86 Computer Science 4: Perceptual Error  . . . . . . 100 87 Electronics 1: Correct Case  . . . . . . . . . . . . 101 88 Electronics 2: Reject to Answer . . . . . . . . . 102 89 Energy and Power 1: Correct Case  . . . . . . . . 103 90 Energy and Power 2: Reasoning Error  . . . . . . 104 91 Materials 1: Correct Case  . . . . . . . . . . . . . 105 92 Materials 2: Lack of Knowledge  . . . . . . . . . 106 93 Mechanical Engineering 1: Correct Case  . . . . . 107 94 Mechanical Engineering 2: Reasoning Error  . . . 108 95 Mechanical Engineering 3: Reasoning Error  . . . 109 \n"}
{"page": 20, "image_path": "doc_images/2311.16502v3_20.jpg", "ocr_text": "Subject Correct Case Perception Lack of Knowledge Reasoning Other\n\nArt 8,9 10\n\nArt Theory 11, 12 13\n\nDesign 14 15 16\n\nMusic 17 18, 19 18\n\nAccounting 20 21\n\nEconomics 22 23, 24\n\nFinance 25 26\n\nManage 27 28\n\nMarketing 29 30\n\nBiology 31, 32 33, 34\nChemistry 36, 37 38 39 38\n\nGeography 40 42 41,42\n\nMath 43 44 46 45\nPhysics 47 48\n\nBasic Medical Science 49 50\n\nClinical Medicine 51, 52, 53 54 55\n\nDiagnostics and Laboratory Medicine 56 57, 59 59 58\nPharmacy 60 61, 62\n\nPublic Health 63 65 64\nHistory 66, 67 68 69\n\nLiterature 70 71\n\nSociology 72 73\n\nPsychology 74 75\n\nAgriculture 76 77, 78,79\n\nArchitecture and Engineering 80 81\n\nComputer Science 83 84, 85,86 84\n\nElectronics 87 88\nEnergy and Power 89 90\n\nMaterials 91 92\n\nMechanical Engineering 93 94, 95\n\nTable 10. Table index of case study figures by subjects with associated (error) categories.\n\n21\n", "vlm_text": "The table categorizes subjects into different areas and assigns numbers under the columns \"Correct Case,\" \"Perception,\" \"Lack of Knowledge,\" \"Reasoning,\" and \"Other.\" Here’s a brief overview:\n\n- **Arts and Humanities**  \n  - Art: Correct Case (8, 9), Perception (10), Lack of Knowledge (13)\n  - Art Theory: Correct Case (11, 12), Perception (11), Lack of Knowledge (13)\n  - Design: Correct Case (14), Perception (15), Lack of Knowledge (16)\n  - Music: Correct Case (17), Perception (18, 19), Lack of Knowledge (18)\n\n- **Business and Management**  \n  - Accounting: Correct Case (20), Perception (21)\n  - Economics: Correct Case (22), Perception (23, 24)\n  - Finance: Correct Case (25)\n  - Management: Correct Case (27), Perception (28)\n  - Marketing: Correct Case (29), Perception (30)\n\n- **Natural Sciences**  \n  - Biology: Correct Case (31, 32), Other (33, 34)\n  - Chemistry: Correct Case (36, 37), Perception (38), Lack of Knowledge (39), Other (38)\n  - Geography: Correct Case (40), Perception (42), Lack of Knowledge (42), Other (41)\n  - Math: Correct Case (43), Perception (44), Reasoning (46), Other (45)\n  - Physics: Correct Case (47), Perception (48)\n\n- **Medical Science**  \n  - Basic Medical Science: Correct Case (49), Perception (50)\n  - Clinical Medicine: Correct Case (51, 52, 53, 54), Lack of Knowledge (55)\n  - Diagnostics and Laboratory Medicine: Correct Case (56), Perception (57, 59), Lack of Knowledge (59), Other (58)\n  - Pharmacy: Correct Case (60), Perception (61, 62)\n  - Public Health: Correct Case (63), Perception (65), Lack of Knowledge (64)\n\n- **Social Sciences**  \n  - History: Correct Case (66, 67), Perception (68), Lack of Knowledge (69)\n  - Literature: Correct Case (70), Perception (71)\n  - Sociology: Correct Case (72), Other (73)\n  - Psychology: Correct Case (74), Perception (75)\n\n- **Engineering and Technology**  \n  - Agriculture: Correct Case (76), Perception (77, 78, 79)\n  - Architecture and Engineering: Correct Case (80), Other (81)\n  - Computer Science: Correct Case (83), Perception (84, 85, 86), Reasoning (84)\n  - Electronics: Correct Case (87), Other (88)\n  - Energy and Power: Correct"}
{"page": 21, "image_path": "doc_images/2311.16502v3_21.jpg", "ocr_text": "eZ Art: Visual Arts\n\nQuestion: The artist's use of color and neutral tones helps convey primarily which of the following\nthemes? <image 1>\n\nOption:\n\n(A) the inequities of societies\n\n(B) the innocence of youth\n\n(C) the conveniences of technology\n\n(D) the threat of climate change\n\n<image 1>\n\nGPT-4V: The artist's use of color and neutral tones in this picture, along with the chosen imagery,\nhelps convey the theme of \"the inequities of societies\" (A). The contrast between the small child\nlaboring to pull the rickshaw and the larger, relaxed adults taking a selfie is a stark commentary on\nsocial and economic disparities. The neutral background emphasizes the subjects, drawing the\nviewer's attention to the inequality presented within the scene.\n\nGround Truth: (A) the inequities of societies\n\nExplanation: The work depicts a small boy pulling a rickshaw in which two adults are seated. The\nboy, the rickshaw, and the background are all depicted with shades of gray and black. By contrast,\nthe couple in the rickshaw is depicted in full color. Even without this use of color, the work would\nconvey a theme of inequity because a little boy is pulling two adults, who seem to represent\nWestern culture. The color contrast underscores the theme because one culture is depicted as bright,\nhappy, and colorful, while the other is depicted as small and drab.\n\nFigure 8. A sample correct case of Art (subfield: Visual Arts).\nBack to List of Figures | Back to Table Index\n\n22\n", "vlm_text": "Art: Visual Arts \nQuestion:  The artist's use of color and neutral tones helps convey primarily which of the following themes? <image  $_{1>}$  \nOption:\n\n \n(A) the inequities of societies\n\n (B) the innocence of youth\n\n (C) the conveniences of technology\n\n (D) the threat of climate change \nThe image depicts a couple sitting in a modern rickshaw under a large umbrella. They appear to be using a smartphone and are being pulled by a young boy, creating a stark contrast in their respective roles and activities.\nGPT-4V:  The artist's use of color and neutral tones in this picture, along with the chosen imagery, helps convey the theme of \"the inequities of societies\" (A). The contrast between the small child laboring to pull the rickshaw and the larger, relaxed adults taking a selfie is a stark commentary on social and economic disparities. The neutral background emphasizes the subjects, drawing the viewer's attention to the inequality presented within the scene. \nGround Truth: (A) the inequities of societies \nExplanation:  The work depicts a small boy pulling a rickshaw in which two adults are seated. The boy, the rickshaw, and the background are all depicted with shades of gray and black. By contrast, the couple in the rickshaw is depicted in full color. Even without this use of color, the work would convey a theme of inequity because a little boy is pulling two adults, who seem to represent Western culture. The color contrast underscores the theme because one culture is depicted as bright, happy, and colorful, while the other is depicted as small and drab. "}
{"page": 22, "image_path": "doc_images/2311.16502v3_22.jpg", "ocr_text": "~~) Art: Photography\n\nQuestion: Which of the motion effect images in <image 1> is a 'panning blur'?\nOption:\n\n(A) Photo 1\n\n(B) Photo 2\n\n(C) Photo 3\n\n(D) None of the above\n\n<image 1>\n\nGPT-4V: The image labeled '3' demonstrates a 'panning blur' effect, where the camera follows the\nmoving subject at the same speed, causing the subject to appear relatively sharp while the\nbackground becomes blurred. This technique emphasizes the speed and direction of the moving\nsubject.\n\nGround Truth: (C) Photo 3\n\nFigure 9. A sample correct case of Art (subfield: Photography).\nBack to List of Figures | Back to Table Index\n\n23\n", "vlm_text": "Art: Photography \nQuestion:  Which of the motion effect images in <image  $_{1>}$   is a 'panning blur'? Option:\n\n \n(A) Photo 1\n\n (B) Photo 2\n\n (C) Photo 3\n\n (D) None of the above \nThe image consists of three photos:\n\n1. A person is jumping outdoors with arms and legs bent.\n2. A person is standing still against a wall, while another is blurred due to motion, possibly jumping.\n3. A side view of a car with motion blur effect, suggesting speed.\n\nThe numbers 1, 2, and 3 are below each photo.\nGPT-4V:  The image labeled   $^{13^{\\dag}}$   demonstrates a 'panning blur' effect, where the camera follows the moving subject at the same speed, causing the subject to appear relatively sharp while the background becomes blurred. This technique emphasizes the speed and direction of the moving subject. \nGround Truth: (C) Photo 3 \nFigure 9. A sample correct case of Art (subfield: Photography). Back to List of Figures  |  Back to Table Index "}
{"page": 23, "image_path": "doc_images/2311.16502v3_23.jpg", "ocr_text": "@ Art: Fine Arts\n\nQuestion: A main function of castles was to provide a defense against invaders. This was achieved\nby several devices, including water-filled moats and barbicans, high stone walls surrounding the\ncastle. Another such device was battlements (or crenellations). These were alternating sections of\nwall, called merlons, and openings, called embrasures. The embrasures were used for firing arrows.\nThey also permitted boulders to be dropped on attackers attempting to scale the castle walls. If you\nwere going to write an alternate caption for <image 1>, you might note that this castle was\nequipped with\n\nOption:\n\n(A) a barbican and a moat.\n\n(B) merlons but not embrasures.\n\n(C) a moat and crenellations.\n\n(D) a barbican and battlements.\n\n<image 1>\n\nGPT-4V: The correct alternate caption for the image based on the provided information would be:\n(C) a moat and crenellations.\n\nError Category: Perceptual Error\n\nError Reason: GPT-4V fails to interpret the image. Based on the image and the explanation, the\nanalysis should be 'we cannot actually see if there is a moat or not'. However, GPT-4V incorrectly\nthinks there's a moat and gives a wrong answer.\n\nGround Truth: (D) a barbican and battlements.\nExplanation: We can see that the castle has outer walls with battlements on top, but we cannot\nactually see if there is a moat or not.\n\nFigure 10. A sample error case of Art (subfield: Fine Arts). Error category: Perceptual Error\nBack to List of Figures | Back to Table Index\n\n24\n", "vlm_text": "The image shows a red circle with a white \"X\" in the center, outlined in black. It appears to be a symbol typically used to indicate something is incorrect, an error, or a negative action such as closing or canceling.\nArt: Fine Arts \nQuestion:  A main function of castles was to provide a defense against invaders. This was achieved by several devices, including water-filled moats and barbicans, high stone walls surrounding the castle. Another such device was battlements (or c re nell at ions). These were alternating sections of wall, called merlons, and openings, called embrasures. The embrasures were used for firing arrows. They also permitted boulders to be dropped on attackers attempting to scale the castle walls. If you were going to write an alternate caption for <image   $1\\!>$  , you might note that this castle was equipped with\n\n \nOption: \n(A) a barbican and a moat.\n\n (B) merlons but not embrasures.\n\n (C) a moat and c re nell at ions.\n\n (D) a barbican and battlements. \nThe image shows a large stone castle with multiple round towers and battlements. It appears to be a well-preserved historical structure set against a clear blue sky. The castle is situated on a grassy area.\nGPT-4V:  The correct alternate caption for the image based on the provided information would be: (C) a moat and c re nell at ions. \nError Category: Perceptual Error \nError Reason:  GPT-4V fails to interpret the image. Based on the image and the explanation, the analysis should be 'we cannot actually see if there is a moat or not'. However, GPT-4V incorrectly thinks there's a moat and gives a wrong answer. \nGround Truth: (D) a barbican and battlements. \nExplanation:  We can see that the castle has outer walls with battlements on top, but we cannot  actually see if there is a moat or not. \nFigure 10. A sample error case of Art (subfield: Fine Arts). Error category: Perceptual Error Back to List of Figures  |  Back to Table Index "}
{"page": 24, "image_path": "doc_images/2311.16502v3_24.jpg", "ocr_text": "eZ Art Theory: Art History\n\nQuestion: Who was the patron of the building?\nOptions:\n\n(A) Abd al-Malik\n\n(B) Justinian\n\n(C) Charlemagne\n\n(D) Shah Jahan\n\n<image 1> <image 2>\n\nGPT-4V: The building in the second image is the Hagia Sophia. The patron of Hagia Sophia was (B)\nJustinian.\n\nGround Truth: (B) Justinian\n\nFigure 11. A sample correct case of Art Theory (subfield: Art History).\nBack to List of Figures | Back to Table Index\n\n25\n", "vlm_text": "Art Theory: Art History \nQuestion:  Who was the patron of the building? Options:\n\n (A) Abd al-Malik\n\n (B) Justinian\n\n (C) Charlemagne\n\n (D) Shah Jahan \nThe image is divided into two sections labeled as <image 1> and <image 2>. \n\n<image 1> is a floor plan of a large architectural structure with a red background, illustrating the layout and design. The plan indicates features such as a central dome, arches, and various chambers and naves.\n\n<image 2> is an interior photograph of a grand and historic building. It showcases the vast, ornate interior with large domes, decorative arches, and elaborate architectural elements. The building is the Hagia Sophia in Istanbul, Turkey, known for its massive dome and historical significance as a former Byzantine cathedral and later mosque before being converted into a museum, and now a mosque again.\nGPT-4V:  The building in the second image is the Hagia Sophia. The patron of Hagia Sophia was (B) Justinian. \nGround Truth: (B) Justinian "}
{"page": 25, "image_path": "doc_images/2311.16502v3_25.jpg", "ocr_text": "2 Art Theory: Art Criticism\n\nQuestion: What is NOT exhibited in the painting? <image 1>\nOption:\n\n(A) hierarchical scale\n\n(B) graphic representation of horror and despair\n\n(C) a sense of immediacy and drama\n\n(D) use of sharply contrasting light and shade\n\n<image 1>\n\nGPT-4V: Based on the painting provided:\n\n(A) hierarchical scale - This technique, where figures or objects are represented in varying sizes to\nindicate their relative importance or position in the narrative, is evident in the painting. The\ncentral figures are larger and more pronounced, while secondary figures and elements are\nsmaller or placed in the background.\n\n(B) graphic representation of horror and despair - This is clearly exhibited in the painting. The\nexpressions on the faces of the figures, their body language, and the overall chaotic scene depict\nhorror and despair.\n\n(C) a sense of immediacy and drama - The dynamic poses, facial expressions, and the chaotic nature\nof the scene give a strong sense of immediacy and drama.\n\n(D) use of sharply contrasting light and shade - The painting employs a technique known as\nchiaroscuro, where light and shade are used to give depth, volume, and a three-dimensional\neffect to the subjects. This is evident in the way light falls on certain figures, highlighting them,\nwhile others are in shadow.\n\nGiven these observations, the correct answer is:\n\n(A) hierarchical scale.\n\nGround Truth: (A) hierarchical scale\n\nFigure 12. A sample correct case of Art Theory (subfield: Art Criticism).\nBack to List of Figures | Back to Table Index\n\n26\n", "vlm_text": "The image shows a green circle with a black check mark or tick inside it. This symbol is commonly used to indicate that something is correct, completed, verified, or approved.\nArt Theory: Art Criticism \nQuestion:  What is NOT exhibited in the painting? <image  $1\\!>$  Option:\n\n \n(A) hierarchical scale\n\n (B) graphic representation of horror and despair\n\n (C) a sense of immediacy and drama\n\n (D) use of sharply contrasting light and shade \nThis image depicts the famous painting \"The Raft of the Medusa\" by Théodore Géricault. The painting portrays the aftermath of the wreck of the French naval frigate Medusa, which ran aground off the coast of Senegal in 1816. The composition shows a group of shipwrecked survivors on a makeshift raft, some of whom are dead, while others desperately clamber towards a distant hope of rescue. The work is renowned for its dramatic presentation, emotional intensity, and stark portrayal of human suffering.\nGPT-4V:  Based on the painting provided:\n\n \n(A) hierarchical scale - This technique, where figures or objects are represented in varying sizes to  indicate their relative importance or position in the narrative, is evident in the painting. The  central figures are larger and more pronounced, while secondary figures and elements are  smaller or placed in the background.\n\n \n(B) graphic representation of horror and despair - This is clearly exhibited in the painting. The  expressions on the faces of the figures, their body language, and the overall chaotic scene depict  horror and despair.\n\n \n(C) a sense of immediacy and drama - The dynamic poses, facial expressions, and the chaotic nature  of the scene give a strong sense of immediacy and drama.\n\n \n(D) use of sharply contrasting light and shade - The painting employs a technique known as  chiaroscuro, where light and shade are used to give depth, volume, and a three-dimensional  effect to the subjects. This is evident in the way light falls on certain figures, highlighting them,  while others are in shadow. \nGiven these observations, the correct answer is: (A) hierarchical scale. \nGround Truth: (A)  hierarchical scale \nFigure 12. A sample correct case of Art Theory (subfield: Art Criticism). Back to List of Figures  |  Back to Table Index "}
{"page": 26, "image_path": "doc_images/2311.16502v3_26.jpg", "ocr_text": "@ Art Theory: Art History\n\nQuestion: The original placement of this painting was\n\nOption:\n\n(A) in the Prado Museum in Madrid, showing a willingness to be compared to great Spanish\npainters of the past\n\n(B) in the cathedral in Madrid, indicating a desire to find acceptance with the Catholic Church\n\n(C) in the study of King Philip IV, challenging him to consider the multitude of interpretations the\npainting represents\n\n(D) in the reception room at the Royal Palace in Madrid, where the portraits of the royal family\nwould be viewable by visiting dignitaries\n\n<image 1>\n\nGPT-4V: This painting is \"Las Meninas\" by Diego Velazquez. The original placement of this\npainting was (A) in the Prado Museum in Madrid, showing a willingness to be compared to great\nSpanish painters of the past.\n\nError Category: Lack of knowledge\n\nError Reason: GPT-4V successfully recognizes that the painting is Las Meninas (1656), painted by\nVelazquez, showcasing its perceptual ability. However, it does not know the fact that the work was\nplaced in the study of King Philip IV. GPT-4V suggests that the painting was placed in Prado\nMuseum in Madrid, and even provides a reason to support this answer, which is the willingness to\nbe compared to great Spanish painters in the past. This shows some reasoning ability. However, the\noriginal placement is a piece of factual knowledge; the reasoning was based on incorrect knowledge\nand it led to a wrong answer. This behavior illustrates that GPT-4V lacks specific art knowledge.\n\nGround Truth: (C) in the study of King Philip IV, challenging him to consider the multitude of\ninterpretations the painting represents\n\nFigure 13. A sample error case of Art Theory (subfield: Art History). Error category: Lack of Knowledge\nBack to List of Figures | Back to Table Index\n\n27\n", "vlm_text": "The image depicts a red circle with a black border containing a white \"X\" or cross symbol. This symbol is commonly used as an indication of an error, mistake, or an action that cannot be completed. It can also represent closing or canceling something in user interfaces.\nArt Theory: Art History \nQuestion:  The original placement of this painting was Option:\n\n \n(A) in the Prado Museum in Madrid, showing a willingness to be compared to great Spanish painters of the past\n\n (B) in the cathedral in Madrid, indicating a desire to find acceptance with the Catholic Church\n\n (C) in the study of King Philip IV, challenging him to consider the multitude of interpretations the painting represents\n\n (D) in the reception room at the Royal Palace in Madrid, where the portraits of the royal family would be viewable by visiting dignitaries \nThe image is a black-and-white reproduction of the famous painting \"Las Meninas\" by Diego Velázquez. It depicts a group of people in a room, including a young girl at the center, surrounded by her entourage, a dog lying on the floor, and Velázquez himself standing with a brush and palette. The painting is renowned for its complex composition and use of perspective.\nGPT-4V:  This painting is \"Las Meninas\" by Diego Velázquez. The original placement of this painting was  (A) in the Prado Museum in Madrid , showing a willingness to be compared to great Spanish painters of the past. \nError Category: Lack of knowledge \nError Reason:  GPT-4V successfully recognizes that the painting is Las Meninas (1656), painted by Velazquez, showcasing its perceptual ability. However, it does not know the fact that the work was placed in the study of King Philip IV. GPT-4V suggests that the painting was placed in Prado Museum in Madrid, and even provides a reason to support this answer, which is the willingness to be compared to great Spanish painters in the past. This shows some reasoning ability. However, the original placement is a piece of factual knowledge; the reasoning was based on incorrect knowledge and it led to a wrong answer. This behavior illustrates that GPT-4V lacks specific art knowledge. \nGround Truth: (C) in the study of King Philip IV, challenging him to consider the multitude of interpretations the painting represents "}
{"page": 27, "image_path": "doc_images/2311.16502v3_27.jpg", "ocr_text": "eZ Design : Industrial Design\n\nQuestion: In the Robie House Dining room, what design elements were used to create a 'room\nwithin a room' vibe? <image 1>\n\nOption:\n\n(A) Tall backs of chairs and lamps at the corners of dining tables\n\n(B) Low-back chairs and pendant lights in the center\n\n(©) Circular dining tables and wall-mounted shelves\n\n(D) Folding chairs and floor-standing lamps\n\n<image 1>\n\nGPT-4V: Based on the image you've provided of the Robie House Dining room, the design\nelements used to create a 'room within a room! vibe are:(A) Tall backs of chairs and lamps at the\ncorners of dining tables.\n\nGround Truth: (A) Tall backs of chairs and lamps at the corners of dining tables\n\nFigure 14. A sample correct case of Design (subfield: Industrial Design).\nBack to List of Figures | Back to Table Index\n\n28\n", "vlm_text": "Design : Industrial Design \nQuestion:  In the Robie House Dining room, what design elements were used to create a 'room within a room' vibe? <image  $_{1>}$  \nOption: \n(A) Tall backs of chairs and lamps at the corners of dining tables (B) Low-back chairs and pendant lights in the center (C) Circular dining tables and wall-mounted shelves (D) Folding chairs and floor-standing lamps \nThe image depicts an interior of a dining room characterized by its distinctive architectural design. The room features a wooden dining table with high-backed chairs, and the lighting fixtures on the table and ceiling have a geometric design. The style is reminiscent of Frank Lloyd Wright's architectural designs, known for their unique, elegant, and geometric patterns. The room has wood paneling and built-in shelving, contributing to its classic and sophisticated ambiance.\nGPT-4V:  Based on the image you've provided of the Robie House Dining room, the design elements used to create a 'room within a room' vibe are:(A) Tall backs of chairs and lamps at the corners of dining tables. \nGround Truth: (A) Tall backs of chairs and lamps at the corners of dining tables "}
{"page": 28, "image_path": "doc_images/2311.16502v3_28.jpg", "ocr_text": "@ Design: Design History\n\nQuestion: The painting <image 1> reflects contemporary culture primarily through the mapping of\nOption:\n\n(A) consumerism and national identities\n\n(B) flight paths and time intervals\n\n(©) sports car and racing signage\n\n(D) information systems on the Internet\n\n<image 1>\n\nGPT-4V: The painting (<image 1>) appears to be a complex and abstract representation with\nvarious lines, patterns, and colors. Based on the visual elements in the painting:(D) information\nsystems on the Internetseems to be the most fitting description. The interconnected lines and\nvibrant, diverse patterns might suggest the vastness and complexity of information systems on the\nInternet. However, art interpretation can be subjective, and different viewers may see different\nmeanings in the same piece.\n\nError Category: Perceptual Error\n\nError Reason: The model failed to recognize and interpret the symbolic elements of national flags\npresent in the image, which are key indicators of 'consumerism and national identities', leading to\nan incorrect answer.\n\nGround Truth: (D) information systems on the Internet\n\nExplanation: The painting reflects contemporary culture's consumerism and national identities\nthrough its visual elements that evoke the commercial marketplace and specific country symbols.\nBright colors and recognizable patterns may suggest mass-produced goods and advertising, a\ncommon commentary on consumer culture in contemporary art. Additionally, abstract\nrepresentations of flags, national colors, or iconic images could signify different countries' identities,\nillustrating the global interconnectedness of consumerism where products and brands cross borders\nand adapt to local cultures. The painting's complexity and layering mirror the saturated,\ninterconnected nature of our globalized world, where the flow of consumer goods and cultural\nsymbols creates a vibrant, dynamic map of modern life.\n\nFigure 15. A sample error case of Design (subfield: Design History). Error category: Perceptual Error\nBack to List of Figures | Back to Table Index\n\n29\n", "vlm_text": "This is an image of a red circle with a white \"X\" symbol in the center, typically used to indicate an error or a cancellation.\nDesign: Design History \nQuestion:  The painting <image  $_{1>}$   reflects contemporary culture primarily through the mapping of Option:\n\n \n(A) consumerism and national identities\n\n (B) flight paths and time intervals\n\n (C) sports car and racing signage\n\n (D) information systems on the Internet \nThe image appears to be an abstract artwork. It features various geometric shapes, lines, and vibrant colors. There are dynamic patterns and a sense of motion, with an overall intricate and complex design.\nGPT-4V: The painting (<image  $^{1>}$  ) appears to be a complex and abstract representation with various lines, patterns, and colors. Based on the visual elements in the painting:(D) information systems on the Internet seems to be the most fitting description. The interconnected lines and vibrant, diverse patterns might suggest the vastness and complexity of information systems on the Internet. However, art interpretation can be subjective, and different viewers may see different meanings in the same piece. \nError Category: Perceptual Error \nError Reason:  The model failed to recognize and interpret the symbolic elements of national flags present in the image, which are key indicators of 'consumerism and national identities', leading to an incorrect answer. \nGround Truth: (D) information systems on the Internet \nExplanation:  The painting reflects contemporary culture's consumerism and national identities through its visual elements that evoke the commercial marketplace and specific country symbols. Bright colors and recognizable patterns may suggest mass-produced goods and advertising, a common commentary on consumer culture in contemporary art. Additionally, abstract  representations of flags, national colors, or iconic images could signify different countries' identities, illustrating the global interconnected ness of consumerism where products and brands cross borders and adapt to local cultures. The painting's complexity and layering mirror the saturated, interconnected nature of our globalized world, where the flow of consumer goods and cultural symbols creates a vibrant, dynamic map of modern life. "}
{"page": 29, "image_path": "doc_images/2311.16502v3_29.jpg", "ocr_text": "@ Design: Design History\n\nQuestion: The double-tiered columns allowed for all of the following EXCEPT (<image 1>)\nOption:\n\n(A) barrel-vaulted roofing\n\n(B) decorative rhythm and repetition\n\n(C) a higher roof to make up for the short columns\n\n(D) the entrance of light and air into the hall\n\n<image 1>\n\nGPT-4V: The double-tiered columns in the image provided would allow for: (A) barrel-vaulted\nroofing (B) decorative rhythm and repetition (C) a higher roof to make up for the short columns.\nHowever, the double-tiered columns would not directly affect: (D) the entrance of light and air into\nthe hall. So the correct answer is (D) the entrance of light and air into the hall.\n\nError Category: Lack of Knowledge\n\nError Reason: The error reason is a lack of specific subject knowledge: the incorrect response\nsuggests that double-tiered columns would not affect the entrance of light and air, which is not\nnecessarily true. Architecturally, double-tiered columns do not prevent light and air from entering;\nsuch features are more influenced by the building's windows and overall design rather than the\ncolumn structure itself. The correct answer should be that double-tiered columns would not allow\nfor barrel-vaulted roofing, as the Mezquita features flat ceilings above the arches.\n\nGround Truth: (A) barrel-vaulted roofing\n\nExplanation: The double-tiered columns depicted in the image, characteristic of Islamic architecture\nlike that found in the Mosque-Cathedral of Cordoba, are not designed to support a barrel-vaulted\nroof; instead, they serve other purposes. These arches create a visually striking pattern of rhythm\nand repetition, elevate the roofline by stacking arches, which compensates for the shorter columns,\nand contribute to the structure's stability. However, they are not related to the entrance of light and\nair, which is typically managed by windows, nor are they structurally supportive of a barrel-\nvaulted roof, as their design and construction cater to a flat roofing system. Hence, the correct\nanswer is \"barrel-vaulted roofing,\" as it is the element not facilitated by the double-tiered columns.\n\nFigure 16. A sample error case of Design (subfield: Design History). Error category: Lack of Knowledge\nBack to List of Figures | Back to Table Index\n\n30\n", "vlm_text": "The image features a red circle with a white \"X\" in the center. This often symbolizes an error, cancellation, or denial.\nDesign: Design History \nQuestion:  The double-tiered columns allowed for all of the following EXCEPT (<image  $1{>}$  ) Option:\n\n \n(A) barrel-vaulted roofing\n\n (B) decorative rhythm and repetition\n\n (C) a higher roof to make up for the short columns\n\n (D) the entrance of light and air into the hall \nThe image depicts a series of ornate arches and columns commonly found in Islamic architecture. The alternating red and white voussoirs create a striking visual effect, characteristic of places like the Mosque–Cathedral of Córdoba in Spain. The space shows intricate design and craftsmanship, highlighting a historical and cultural architectural style.\nGPT-4V:  The double-tiered columns in the image provided would allow for: (A) barrel-vaulted roofing (B) decorative rhythm and repetition (C) a higher roof to make up for the short columns. However, the double-tiered columns would not directly affect: (D) the entrance of light and air into the hall. So the correct answer is (D) the entrance of light and air into the hall. \nError Category: Lack of Knowledge \nError Reason:  The error reason is a lack of specific subject knowledge: the incorrect response suggests that double-tiered columns would not affect the entrance of light and air, which is not necessarily true. Architecturally, double-tiered columns do not prevent light and air from entering; such features are more influenced by the building's windows and overall design rather than the column structure itself. The correct answer should be that double-tiered columns would not allow for barrel-vaulted roofing, as the Mezquita features flat ceilings above the arches. \nGround Truth :  (A) barrel-vaulted roofing \nExplanation : The double-tiered columns depicted in the image, characteristic of Islamic architecture like that found in the Mosque-Cathedral of Córdoba, are not designed to support a barrel-vaulted roof; instead, they serve other purposes. These arches create a visually striking pattern of rhythm and repetition, elevate the roofline by stacking arches, which compensates for the shorter columns, and contribute to the structure's stability. However, they are not related to the entrance of light and air, which is typically managed by windows, nor are they structurally supportive of a barrel- vaulted roof, as their design and construction cater to a flat roofing system. Hence, the correct answer is \"barrel-vaulted roofing,\" as it is the element not facilitated by the double-tiered columns. "}
{"page": 30, "image_path": "doc_images/2311.16502v3_30.jpg", "ocr_text": "eZ Music : Music\n\nQuestion: Select the correct tuning of Violin.\nOption:\n\n(A) <image 1>\n\n(B) <image 2>\n\n(C) <image 3>\n\n(D) <image 4>\n\n2.\n\neee eS\n\n4)\n\n<image 1> <image 2> <image 3> <image 4>\n\nGPT-4V: The correct tuning for a violin is shown in the first image. The violin is typically tuned to\nthe pitches G3, D4, A4, and E5, starting from the lowest string to the highest. In the image, the G-\nclef (treble clef) is used for the E and A strings, while the C-clef (alto clef) is used for the D string,\nand the F-clef (bass clef) for the G string. This notation is a way to visually represent the correct\ntuning pitches for the violin's four strings.\n\nGround Truth: (A) <image 1>\n\nFigure 17. A sample correct case of Music (subfield: Music).\nBack to List of Figures | Back to Table Index\n\n31\n\n", "vlm_text": "Music : Music \nQuestion:  Select the correct tuning of Violin.\n\n \n(A) <image 1>\n\n (B) <image  $2{>}\n\n$  (C) <image 3>\n\n (D) <image 4> \nThe image contains a series of musical notations written on staffs. There are four separate images labeled as <image 1>, <image 2>, <image 3>, and <image 4>, each containing a segment of sheet music. \n\n1. **<image 1>**: Shows a treble clef with two notes: D and G.\n2. **<image 2>**: Displays a grand staff (treble clef above the bass clef) featuring three notes across both clefs: F in the treble clef, and D and G in the bass clef.\n3. **<image 3>**: Has a bass clef with two notes: A and C.\n4. **<image 4>**: Also presents a bass clef with notes: A, D, and F. \n\nThe picture appears to illustrate various musical notes on both treble and bass clefs, potentially as part of an instructional or educational resource in music theory or music reading.\nGPT-4V:  The correct tuning for a violin is shown in the first image. The violin is typically tuned to the pitches G3, D4, A4, and E5, starting from the lowest string to the highest. In the image, the G- clef (treble clef) is used for the E and A strings, while the C-clef (alto clef) is used for the D string, and the F-clef (bass clef) for the G string. This notation is a way to visually represent the correct tuning pitches for the violin's four strings. \nGround Truth: (A) <image  $\\mathbf{1}\\textgreater$  \nFigure 17. A sample correct case of Music (subfield: Music). Back to List of Figures  Back to Table Index "}
{"page": 31, "image_path": "doc_images/2311.16502v3_31.jpg", "ocr_text": "@ Music : Music\n\nQuestion: Choose the name for the following interval. <image 1>\nOption:\n\n(A) Perfect 6th\n\n(B) Compound perfect 4th\n\n(C) Major 6th\n\n(D) Minor 6th\n\n>\n\n<image 1>\n\nGPT-4V: The interval shown in the image is a “major 6th.” It spans from the first note, F, to the\nsecond note, D. In a standard diatonic scale, the interval from F to D is a sixth, and since the key\nsignature does not indicate any flats or sharps that would alter the D, it remains a major sixth. The\noptions “perfect 6th” and “compound perfect 4th” do not exist in music theory as “perfect” is used\nfor unison, 4th, 5th, and octave intervals. A \"minor 6th\" would require the D to be flattened, which\nis not the case here.\n\nError Category: Perceptual Error, Lack of knowledge\n\nError Reason: The model immediately recognized it as a 6th interval, but got confused whether it's\na major 6th or a minor 6th. A closer look at the response reveals that the model failed to recognize\nthe starting and ending notes of the interval. The first note is actually G, but the model mistook it\nfor the adjacent not F. The second note is Eb, and the model might have simply deduced that it was\nD based on the assumption of major 6th + F. Additionally, the model's mistaken identification of a\ncompound perfect 4th indicates a lack of subject knowledge.\n\nGround Truth: (D) Minor 6th\n\nFigure 18. A sample error case of Music (subfield: Music). Error category: Perceptual Error, Lack of Knowledge\nBack to List of Figures | Back to Table Index\n\n32\n", "vlm_text": "The image shows a red circle with a black outline containing a white \"X\" or cross in the center. This typically symbolizes a deletion, error, or \"no\" sign.\nMusic : Music \nQuestion:  Choose the name for the following interval. <image  $_{1>}$  \n(A) Perfect 6th (B) Compound perfect 4th (C) Major 6th (D) Minor 6th \nThe image shows a piece of musical notation on a staff with a bass clef. There are flat symbols next to the clef, indicating a specific key signature. Notes are written on the staff lines and spaces.\nGPT-4V:  The interval shown in the image is a  “major 6th.”  It spans from the first note, F, to the  second note, D. In a standard diatonic scale, the interval from F to D is a sixth, and since the key signature does not indicate any flats or sharps that would alter the D, it remains a major sixth. The options “perfect  $6\\mathrm{{th}^{\\prime\\prime}}$   and “compound perfect  $4\\mathrm{{th}^{\\prime\\prime}}$   do not exist in music theory as “perfect” is used for unison,  $4\\mathrm{th},$   $5\\mathrm{th},$   and octave intervals. A \"minor 6th\" would require the D to be flattened, which is not the case here. \nError Category: Perceptual Error, Lack of knowledge \nError Reason:  The model immediately recognized it as a 6th interval, but got confused whether it's a major 6th or a minor 6th. A closer look at the response reveals that the model failed to recognize the starting and ending notes of the interval. The first note is actually G, but the model mistook it for the adjacent not F. The second note is Eb, and the model might have simply deduced that it was D based on the assumption of major 6th  $\\mathrm{~+~F~}$  . Additionally, the model's mistaken identification of a compound perfect 4th indicates a lack of subject knowledge. \nGround Truth: (D) Minor 6th \nFigure 18. A sample error case of Music (subfield: Music). Error category: Perceptual Error, Lack of Knowledge Back to List of Figures  Back to Table Index "}
{"page": 32, "image_path": "doc_images/2311.16502v3_32.jpg", "ocr_text": "@ Music : Music\n\nQuestion: Give the bar number of the following (from bar 13 onwards): a melodic interval of a\ncompound minor 3rd in the right-hand part. <image 1>\n\nOption:\n\n(A) 28 and 29\n\n(B) 17\n\n(©) 14 and 15\n\n(D) 18\n\n2 @\nPerfect\nee Se\n\nper\n= ees\n\n3\nP\n\nSeerel\n\nie aaa ex\n\nSo\n\n<image 1>\n\nGPT-4V: In response to your inquiry, I examined the provided image for the presence of a\ncompound minor third interval in the right-hand part starting from bar 13. A compound minor\nthird, or a tenth, combines an octave (twelve semitones) with a minor third (three semitones),\ntotaling fifteen semitones. Upon inspection, I found that bar 17 doesn't have this interval in the\nright-hand part. Bars 14 and 15 do contain intervals, but I'm uncertain if any are a compound minor\nthird. I can't confirm the presence of this interval in bars 28 and 29, as they're not visible in the\nimage. Bar 18 also requires checking for this interval. Unfortunately, I can't process the image\nvisually to provide the exact bar number of the specified interval.\n\nError Category: Perceptual Error\n\nError Reason: Surprisingly, the model fails to find the corresponding bar number based on the\nrelative location of labeled bars. This is a simple visual inference task for humans.\n\nGround Truth: (A) 28 and 29\n\nFigure 19. A sample error case of Music (subfield: Music). Error category: Perceptual Error\nBack to List of Figures | Back to Table Index\n\n33\n", "vlm_text": "The image shows a red circle with a black border, containing a white \"X\" in the center. This symbol is often used to represent \"close,\" \"cancel,\" \"delete,\" or \"error\" in various user interfaces and contexts.\nMusic : Music \nQuestion:  Give the bar number of the following (from bar 13 onwards): a melodic interval of a compound minor 3rd in the right-hand part. <image  $_{1>}$  \nOption:\n\n \n(A) 28 and 29\n\n (B) 17\n\n (C) 14 and 15\n\n (D) 18 \nThis image shows a sheet of musical notation. It contains music written for piano, with both treble and bass clefs. The piece is marked \"Allegro molto e con brio,\" indicating a fast, lively tempo. It uses dynamics such as piano (p), forte (f), and crescendo markings. There are various articulation markings as well, like slurs and staccatos, along with accidentals that suggest the piece is in a key with multiple flats.\nGPT-4V: In response to your inquiry, I examined the provided image for the presence of a compound minor third interval in the right-hand part starting from bar 13. A compound minor third, or a tenth, combines an octave (twelve semitones) with a minor third (three semitones), totaling fifteen semitones. Upon inspection, I found that bar 17 doesn't have this interval in the right-hand part. Bars 14 and   $15~\\mathrm{do}$   contain intervals, but  $\\mathrm{Im}$   uncertain if any are a compound minor third. I can't confirm the presence of this interval in bars 28 and 29, as they're not visible in the image. Bar 18 also requires checking for this interval. Unfortunately, I can't process the image visually to provide the exact bar number of the specified interval. \nError Category: Perceptual Error \nError Reason:  Surprisingly, the model fails to find the corresponding bar number based on the relative location of labeled bars. This is a simple visual inference task for humans. \nGround Truth: (A) 28 and 29 "}
{"page": 33, "image_path": "doc_images/2311.16502v3_33.jpg", "ocr_text": "eQ Accounting : Investment\n\nQuestion: Consider the three stocks in the following table. Pt represents price at time t, and Qt\nrepresents shares outstanding at time t. Stock C splits two for one in the last period. Calculate the\nrate of return on a price-weighted index of the three stocks for the first period (t 0 to t 1).\n\nOption:\n\n(A) 4.17%\n(B) 4.03%\n(©) 3.89%\n\nPo % P, a P, Q\n\n90 100 95 100 95 100\n\nB 50 200 45 200 45 200\n\nc 100 200 110 200 55 400\n\n<image 1>\n\nGPT-4V: To calculate the rate of return on a price-weighted index of the three stocks for the first\nperiod (t0 to t1), follow these steps: Calculate the initial index value at t0. Calculate the index value\nat t1. Compute the rate of return using the two index values.\n\nInitial Index Value at t0:\nPrice-weighted index value at t0\n= (90 + 50 + 100) /3\n\n= 80\n\nIndex Value at t1:\nPrice-weighted index value at t1\n= (95 + 45 + 110) /3\n\n= 83.33\n\nRate of Return:\n\nRate of Return = (Index Value at t1 - Index Value at t0) / Index Value at t0\n\n= (83.33 — 80) /80\n\n= 0.041625\n\n= 4.1625%\n\nTherefore, the rate of return on a price-weighted index of the three stocks for the first period is\napproximately 4.17%.\n\nThe correct answer is:\n\n(A) 4.17%\n\n(PO(A) + PO(B) + PO(C)) /3\n\n(P1(A) + P1(B) + P1(C))/3\n\nGround Truth: (A) 4.17%\nExplanation: At t = 0, the value of the index is: (90 + 50 + 100)/3 = 80. Att = 1, the value of\nthe index is: (95 + 45 + 110)/3 = 83.333. The rate of return is: (83.333/80) — 1 = 4.17%\n\nFigure 20. A sample correct case of Accounting (subfield: Investment).\nBack to List of Figures | Back to Table Index\n\n34\n", "vlm_text": "The image shows a green circle with a black border containing a white check mark.\nAccounting : Investment \nQuestion:  Consider the three stocks in the following table. Pt represents price at time  $^\\mathrm{t,}$   and   $\\mathrm{Qt}$  represents shares outstanding at time t. Stock C splits two for one in the last period. Calculate the rate of return on a price-weighted index of the three stocks for the first period ( t 0 to t 1). \nOption:\n\n (A)  $4.17\\%\n\n$  (B)  $4.03\\,\\%\n\n$  (C)  $3.89\\%$  \nThe table presents data with two sets of variables denoted as \\( P \\) and \\( Q \\) across three different categories labeled A, B, and C. Here's a breakdown of the table:\n\n- **Row Labels (Categories):** A, B, C\n- **Column Labels:**\n  - \\( P_0, Q_0 \\) (Initial values)\n  - \\( P_1, Q_1 \\) \n  - \\( P_2, Q_2 \\)\n\nEach category has six corresponding values for the variables \\( P \\) and \\( Q \\):\n\n- **Category A:**\n  - \\( P_0 = 90 \\), \\( Q_0 = 100 \\)\n  - \\( P_1 = 95 \\), \\( Q_1 = 100 \\)\n  - \\( P_2 = 95 \\), \\( Q_2 = 100 \\)\n\n- **Category B:**\n  - \\( P_0 = 50 \\), \\( Q_0 = 200 \\)\n  - \\( P_1 = 45 \\), \\( Q_1 = 200 \\)\n  - \\( P_2 = 45 \\), \\( Q_2 = 200 \\)\n\n- **Category C:**\n  - \\( P_0 = 100 \\), \\( Q_0 = 200 \\)\n  - \\( P_1 = 110 \\), \\( Q_1 = 200 \\)\n  - \\( P_2 = 55 \\), \\( Q_2 = 400 \\) \n\nThis table seems to demonstrate changes or comparisons across different variables and categories.\nGPT-4V:  To calculate the rate of return on a price-weighted index of the three stocks for the first period (t0 to t1), follow these steps: Calculate the initial index value at t0. Calculate the index value at t1. Compute the rate of return using the two index values. \nInitial Index Value at t0: Price-weighted index value at  $t0\\;=\\;\\left(P0(A)\\:+\\:P0(B)\\:+\\:P0(C)\\right)/\\,3\n\n$   $=\\,\\left(90\\,+\\,50\\,+\\,100\\right)/\\,3\n\n$   $=~80$  Index Value at t1: Price-weighted index value at  $t1\\,=\\,\\left(P1(A)\\,+\\,P1(B)\\,+\\,P1(C)\\right)/\\,3\n\n$   $=\\,\\left(95\\,+\\,45\\,+\\,110\\right)/\\,3\n\n$   $=\\ 83.33$  Rate of Return: Rate of Return  $=$   (Index Value at t1 - Index Value at t0) / Index Value at t0\n\n  $=\\left(83.33\\,-\\,80\\right)/\\,80\n\n$   $=\\ 0.041625$   \n\n  $=\\;4.1625\\%$  \nTherefore, the rate of return on a price-weighted index of the three stocks for the first period is approximately  $4.17\\%$  . \n(A)  $\\mathbf{4.17\\%}$  \nGround Truth: (A)  $\\mathbf{4.17\\%}$  \nExplanation:  At  $\\mathtt{t}=0,$  , the value of the index is:    $(90\\;+\\;50\\;+\\;100)/3\\;=\\;80.$  . At    $t\\ =\\ 1$  , the value of the index is:    $(95\\,+\\,45\\,+\\,110)/3\\,=\\,83.333.$  . The rate of return is:  (83.333/80)   $-\\mathrm{~1~}=\\-4.17\\%$  "}
{"page": 34, "image_path": "doc_images/2311.16502v3_34.jpg", "ocr_text": "@ Accounting: Financial Accounting\n\nQuestion: From the following Company Y adjusted trial balance, what is the retained earnings to be\nreported?<image 1>\n\nOption:\n(A) $17,800\n(B) $16,800\n(C) $15,550\n(D) $12,280\nAdjusted Trial Balance\nDebit Credit\nCash $ 32,000\nAccounts receivable 17,300\nPrepaid insurance 6,400\nLand 10,000\nAccounts payable $ 10,900\nSalaries payable 6,000\nCommon stock 31,000\nRetained earnings 4,200\nDividends 8,000\nService revenue 74,000\nInsurance expense 5,600\nSalaries expense 24,000\nMiscellaneous expense 22,800\n126,100 126,100\n<image 1>\n\nGPT-4V: To calculate the retained earnings to be reported for Company Y from the adjusted trial\nbalance, we need to consider the following components:\n\n1. Beginning Retained Earnings: This would be the retained earnings balance at the start of the\nperiod. However, since it is not provided, we can assume it is included in the current retained\nearnings figure in the trial balance or that this is the first year of operation, and as such, it would be\nzero.\n\n2. Add: Net Income (or Less: Net Loss): Net income is calculated as revenues minus expenses. If\nexpenses exceed revenues, it results in a net loss.\n\n3. Less: Dividends: These are distributions to shareholders, which reduce retained earnings.\n\nFrom the adjusted trial balance provided, the calculation would be as follows:\n\nNet Income: Service Revenue ($74,000) - (Insurance Expense $5,600 + Salaries Expense $24,000 +\nMiscellaneous Expense $22,800) = $74,000 - $52,400 = $21,600.\n\nLess Dividends: $8,000. Retained Earnings to be Reported = Beginning Retained Earnings (if not\ngiven, assumed to be included in the trial balance) + Net Income - Dividends. The trial balance\nshows Retained Earnings as $6,000. Assuming this is the beginning balance and adding the net\nincome less dividends: Retained Earnings to be Reported = $6,000 + $21,600 - $8,000 = $19,600.\nThere is no answer matching the option.\n\nError Category: Perceptual Error\n\nError Reason: GPT-4V understands the question and can know how to calculate the Retained\nEarnings to be Reported. Most of the calculation process is correct. However, in the final steps,\nGPT-4V wrongly extracts the number of Retained Earnings from the table image (should be $4,200\ninstead of $6,000).\n\nGround Truth: (A) $17,800\n\nFigure 21. A sample error case of Accounting (subfield: Financial Accounting). Error category: Perceptual Error\nBack to List of Figures | Back to Table Index\n\n35\n", "vlm_text": "Accounting: Financial Accounting \nQuestion:  From the following Company Y adjusted trial balance, what is the retained earnings to be reported?<image  $_{1>}$  \nOption:\n\n (A) \\$17,800\n\n (B) \\$16,800\n\n (C) \\$15,550\n\n (D) \\$12,280 \nThe table is an adjusted trial balance, showing the balances of different accounts as of a specific date. It has two columns: one for debits and one for credits. The accounts listed are:\n\n1. Cash: $32,000 (Debit)\n2. Accounts Receivable: $17,300 (Debit)\n3. Prepaid Insurance: $6,400 (Debit)\n4. Land: $10,000 (Debit)\n5. Accounts Payable: $10,900 (Credit)\n6. Salaries Payable: $6,000 (Credit)\n7. Common Stock: $31,000 (Credit)\n8. Retained Earnings: $4,200 (Credit)\n9. Dividends: $8,000 (Debit)\n10. Service Revenue: $74,000 (Credit)\n11. Insurance Expense: $5,600 (Debit)\n12. Salaries Expense: $24,000 (Debit)\n13. Miscellaneous Expense: $22,800 (Debit)\n\nThe total for both debits and credits is $126,100, indicating that the trial balance is balanced.\nGPT-4V:  To calculate the retained earnings to be reported for Company Y from the adjusted trial balance, we need to consider the following components: \n1. Beginning Retained Earnings: This would be the retained earnings balance at the start of the period. However, since it is not provided, we can assume it is included in the current retained earnings figure in the trial balance or that this is the first year of operation, and as such, it would be zero. \n2. Add: Net Income (or Less: Net Loss): Net income is calculated as revenues minus expenses. If expenses exceed revenues, it results in a net loss. \n3. Less: Dividends: These are distributions to shareholders, which reduce retained earnings. From the adjusted trial balance provided, the calculation would be as follows: \nNet Income: Service Revenue   $(\\mathbb{S74},\\!000)$   - (Insurance Expense   $^{\\S5,600}\\ ^{+}$   Salaries Expense   $^{\\S24,000\\ +}$  Miscellaneous Expense  $\\S22{,}800)=\\S74{,}000\\cdot\\S52{,}400=\\S21{,}600$  . \nLess Dividends:  $^{\\S8,000}$  . Retained Earnings to be Reported  $=$   Beginning Retained Earnings (if not given, assumed to be included in the trial balance)   $^+$   Net Income - Dividends. The trial balance shows Retained Earnings as   $\\S6{,}000$  . Assuming this is the beginning balance and adding the net income less dividends: Retained Earnings to be Reported  $=\\bar{\\S}6{,}000\\bar{\\ }+\\ \\S21{,}600\\bar{\\ }-\\ \\S8{,}000\\bar{\\ }\\S19{,}600$  . There is no answer matching the option. \nError Category: Perceptual Error \nError Reason:  GPT-4V understands the question and can know how to calculate the Retained Earnings to be Reported. Most of the calculation process is correct. However, in the final steps, GPT-4V wrongly extracts the number of Retained Earnings from the table image (should be  $\\mathbb{\\Phi{4},200}$  instead of  $\\S6{,}000$  ). \nGround Truth: (A)  $\\pmb{\\S17,800}$  "}
{"page": 35, "image_path": "doc_images/2311.16502v3_35.jpg", "ocr_text": "Economics : Econometrics\n\nQuestion: Figure 1 displays the number of cases of Melanoma (Mt) in the male population of a US\nState, from 1936 to 1972. Figure 2 shows the Spanish Industrial Production Index (IPIt), from\nJanuary 1975 to March 2001. <image 1> <image 2> According to figures 1 and 2, which of the\nfollowing statements is FALSE:\n\nOption:\n\n(A) IPIt displays a strong seasonality\n(B) Both time series are mean stationary\n(©) Both time series display a trending behavior\n\nFigure 1: M,\n\nFigure 2: IPI,\n\nane\nvy\n\nAL WhM\n\n<image 1>\n\n<image 2>\n\nGPT-4V: Based on the provided figures:(A) IPIt displays a strong seasonality. - This statement\nappears to be TRUE. Figure 2 shows consistent and repeating peaks and valleys for the Spanish\n\nIndustrial Production Index\n- This statement appears to\n\n(IPIt), indicative of seasonality.(B) Both time series are mean stationary.\nbe FALSE. A mean stationary time series would have a constant mean\n\n(average) over time. Both figures show a changing mean over the given time frames, particularly\nFigure 1 where the mean value for Melanoma cases appears to be increasing over time.(C) Both time\nseries display a trending behavior. - This statement appears to be TRUE. Figure 1 shows an upward\ntrend in the number of Melanoma cases over time, and Figure 2 displays fluctuations but also an\noverall upward trend in the Industrial Production Index. Therefore, the FALSE statement is:\n\n(B) Both time series are mean stationary.\n\nGround Truth: (B) Both time series are mean stationary.\n\nFigure 22. A sample correct case of Economics (subfield: Econometrics).\n\nBack to List of Figures | Back to Table Index\n\n36\n", "vlm_text": "The image shows a green circle with a black border containing a white check mark.\nEconomics : Econometrics \nQuestion:  Figure 1 displays the number of cases of Melanoma (Mt) in the male population of a US State, from 1936 to 1972. Figure 2 shows the Spanish Industrial Production Index (IPIt), from January 1975 to March 2001. <image   $_{1>}$   <image   $2{>}$   According to figures 1 and 2, which of the following statements is FALSE:\n\n \nOption: \n(A) IPIt displays a strong seasonality\n\n (B) Both time series are mean stationary\n\n (C) Both time series display a trending behavior \nThe image is a line graph labeled \"Figure 1: \\( M_t \\).\" It shows a trend over time from 1940 to 1970. The vertical axis is labeled \"Miltrances,\" and ranges from 1.0 to 5.5. The line shows an overall upward trend with several fluctuations throughout the period.\nThe image is a line graph labeled \"Figure 2: \\(IPI_t\\)\" showing data from 1975 to 2000. The y-axis represents the IPI value, which ranges from 20 to 140. The graph shows an overall upward trend with periodic fluctuations over time.\nGPT-4V:  Based on the provided figures:(A) IPIt displays a strong seasonality. - This statement appears to be TRUE. Figure 2 shows consistent and repeating peaks and valleys for the Spanish Industrial Production Index (IPIt), indicative of seasonality.(B) Both time series are mean stationary.\n\n - This statement appears to be FALSE. A mean stationary time series would have a constant mean\n\n (average) over time. Both figures show a changing mean over the given time frames, particularly Figure 1 where the mean value for Melanoma cases appears to be increasing over time.(C) Both time series display a trending behavior. - This statement appears to be TRUE. Figure 1 shows an upward trend in the number of Melanoma cases over time, and Figure 2 displays fluctuations but also an overall upward trend in the Industrial Production Index. Therefore, the FALSE statement is: (B) Both time series are mean stationary. \nGround Truth: (B) Both time series are mean stationary. "}
{"page": 36, "image_path": "doc_images/2311.16502v3_36.jpg", "ocr_text": "@ Economics: Macroeconomics\n\nQuestion: Question below is based on the production possibilities of two nations that can produce\nboth crepes and paper. <image 1> Which of the following statements is true of these production\npossibilities?\n\nOption:\n\n(A) Nation X has comparative advantage in paper production and should trade paper to Nation Y\nin exchange for crepes.\n\n(B) Nation X has comparative advantage in crepe production and should trade crepes to Nation Y\nin exchange for paper\n\n(C) Nation X has absolute advantage in paper production, and Nation Y has absolute advantage in\ncrepe production. No trade is possible.\n\n(D) Nation Y has absolute advantage in paper production, and Nation X has absolute advantage in\ncrepe production. No trade is possible.\n\n(E) Nation Y has comparative advantage in crepe production and should trade paper to Nation X in\nexchange for crepes.\n\nIndustry Economic Forecast\na. Housing construction (i) Deep recession: falling inflation, interest rates, and GDP\nb. Health care (ii) Superheated economy: rapidly rising GDP, increasing inflation\nc. Gold mining and interest rates\nd, Steel production (ii) Healthy expansion: rising GDP, mild inflation, low\nunemployment\n(iv) Stagffation: falling GDP, high inflation\n\n<image 1>\n\nGPT-4V: To determine which nation has a comparative advantage in producing a particular good,\nwe need to calculate the opportunity cost for each good in both nations. Opportunity cost is\ncalculated as the amount of the other good that has to be given up to produce one unit of a\nparticular good. Opportunity cost for Nation X: For 1 crepe, Nation X gives up producing 3 units of\npaper. So, opportunity cost of 1 crepe = 3 paper. For 1 paper, Nation X gives up producing 1/3 unit\nof crepe. So, opportunity cost of 1 paper = 1/3 crepe. Opportunity cost for Nation Y:For 1 crepe,\nNation Y gives up producing 5 units of paper. So, opportunity cost of 1 crepe = 5 paper. For 1\npaper, Nation Y gives up producing 1/5 unit of crepe. So, opportunity cost of 1 paper = 1/5 crepe.\nComparing opportunity costs: Nation X has a lower opportunity cost (1/3 < 1/5) in producing\npaper than Nation Y. So, Nation X has a comparative advantage in producing paper. Nation Y has a\nlower opportunity cost (5 > 3) in producing crepes than Nation X. So, Nation Y has a comparative\nadvantage in producing crepes. Given this, the statement:\n\n(A) Nation X has comparative advantage in paper production and should trade paper to Nation Y\nin exchange for crepes.is the correct answer.\n\nError Category: Perceptual Error\n\nError Reason: Although GPT-4V correctly identifies the numbers in the table, it fails to correctly\nattribute these numbers to the correct objects for production. For example, when calculating the\nopportunity cost of Nation X, GPT-4V makes the mistake that nation X can either produce 3 units of\ncrepe or 9 units of piece. But in fact, according to the table, nation X can produce 3 units of paper or\n9 units of crepe. This error leads to the incorrect calculation of opportunity cost and the incorrect\nanswer.\n\nGround Truth: (A) Nation X has comparative advantage in paper production and should trade\npaper to Nation Y in exchange for crepes.\n\nFigure 23. A sample error case of Economics (subfield: Macroeconomics). Error category: Perceptual Error\nBack to List of Figures | Back to Table Index\n\n37\n", "vlm_text": "Economics: Macroeconomics \nQuestion:  Question below is based on the production possibilities of two nations that can produce both crepes and paper. <image  $_{1>}$   Which of the following statements is true of these production possibilities? \nOption: \n(A) Nation X has comparative advantage in paper production and should trade paper to Nation Y  in exchange for crepes. (B) Nation X has comparative advantage in crepe production and should trade crepes to Nation Y  in exchange for paper (C) Nation X has absolute advantage in paper production, and Nation Y has absolute advantage in  crepe production. No trade is possible. (D) Nation Y has absolute advantage in paper production, and Nation X has absolute advantage in  crepe production. No trade is possible. \n(E) Nation Y has comparative advantage in crepe production and should trade paper to Nation X in  exchange for crepes. \nThe image is a table showing the relationship between different industries and economic forecasts. It consists of two columns:\n\n1. **Industry**: \n   - Housing construction\n   - Health care\n   - Gold mining\n   - Steel production\n\n2. **Economic Forecast**:\n   - Deep recession: falling inflation, interest rates, and GDP\n   - Superheated economy: rapidly rising GDP, increasing inflation, and interest rates\n   - Healthy expansion: rising GDP, mild inflation, low unemployment\n   - Stagflation: falling GDP, high inflation\n\nEach industry is paired with an economic forecast scenario (though specific pairings are not directly indicated in the table).\nGPT-4V:  To determine which nation has a comparative advantage in producing a particular good, we need to calculate the opportunity cost for each good in both nations. Opportunity cost is calculated as the amount of the other good that has to be given up to produce one unit of a particular good. Opportunity cost for Nation X: For 1 crepe, Nation  $X$   gives up producing 3 units of  paper. So, opportunity cost of  $1\\,\\mathrm{curve}=3$   paper. For 1 paper, Nation X gives up producing   $1/3$   unit  of crepe.   $\\mathrm{Bo},$   opportunity cost of  $1\\ \\mathrm{Paper}=1/3$   crepe. Opportunity cost for Nation Y:For 1 crepe, Nation Y gives up producing 5 units of paper. So, opportunity cost of 1 crepe  $=5$   paper. For 1 paper, Nation Y gives up producing   $1/5$   unit of crepe. So, opportunity cost of   $1\\;\\mathrm{Paper}=1/5$   crepe. Comparing opportunity costs: Nation   $X$   has a lower opportunity cost   $(1/3\\,<\\,1/5)$   in producing paper than Nation Y. So, Nation X has a comparative advantage in producing paper. Nation Y has a lower opportunity cost   $(5>3)$   in producing crepes than Nation X. So, Nation Y has a comparative advantage in producing crepes. Given this, the statement: \n(A) Nation X has comparative advantage in paper production and should trade paper to Nation Y in exchange for crepes.is the correct answer. \nError Category: Perceptual Error \nError Reason:  Although GPT-4V correctly identifies the numbers in the table, it fails to correctly attribute these numbers to the correct objects for production. For example, when calculating the opportunity cost of Nation X, GPT-4V makes the mistake that nation X can either produce 3 units of crepe or 9 units of piece. But in fact, according to the table, nation  $X$   can produce 3 units of paper or 9 units of crepe. This error leads to the incorrect calculation of opportunity cost and the incorrect answer. \nGround Truth: (A) Nation X has comparative advantage in paper production and should trade paper to Nation Y in exchange for crepes. "}
{"page": 37, "image_path": "doc_images/2311.16502v3_37.jpg", "ocr_text": "@ Economics : Macroeconomics\n\nQuestion: The graph below shows the supply and demand curves in the market for credit card\nborrowing. <image 1> What is the equilibrium interest rate?\n\nOption:\n\n(A) 7%\n\n8%\n\n<image 1>\n\nGPT-4V: The equilibrium interest rate is where the demand curve intersects the supply curve. In\nthe provided graph, the point of intersection appears to be at an interest rate of:(C) 9%\n\nError Category: Perceptual Error\n\nError Reason: GPT-4V has the knowledge that the equilibrium interest rate is where the demand\ncurve intersects the supply curve. However, GPT-4V fails to interpret the image. The point of\nintersection is at an interest rate of 8% instead of 9%.\n\nGround Truth: (B) 8%\n\nFigure 24. A sample error case of Economics (subfield: Macroeconomics). Error category: Perceptual Error\nBack to List of Figures | Back to Table Index\n\n38\n", "vlm_text": "The image shows a red circle with a black border containing a white \"X\" in the center. This symbol is often used to indicate an error, a \"no\" action, or to close a window or dialog box.\nEconomics : Macroeconomics \nQuestion:  The graph below shows the supply and demand curves in the market for credit card borrowing. <image  $_{1>}$   What is the equilibrium interest rate? \nOption:\n\n (A)  $7\\%\n\n$  (B)  $8\\%\n\n$  (C)  $9\\%\n\n$  (D) 10% \nThe image is a graph depicting the market for money. It shows the relationship between the quantity of money and the interest rate (%). \n\n- The x-axis represents the quantity of money.\n- The y-axis represents the interest rate (%).\n- There are two lines: \n  - A downward sloping blue line labeled \"Demand.\"\n  - An upward sloping red line labeled \"Supply.\"\n\nThe intersection of the demand and supply lines represents the equilibrium point where the interest rate and the quantity of money are balanced.\nGPT-4V:  The equilibrium interest rate is where the demand curve intersects the supply curve. In the provided graph,  the point of intersection appears to be at an interest rate of:(C)  $9\\%$  \nError Category: Perceptual Error \nError Reason:  GPT-4V has the knowledge that the equilibrium interest rate is where the demand curve intersects the supply curve. However, GPT-4V fails to interpret the image. The point of intersection is at an interest rate of  $8\\%$   instead of  $9\\%$  . \nGround Truth: (B)  $8\\%$  \nFigure 24. A sample error case of Economics (subfield: Macroeconomics). Error category: Perceptual Error Back to List of Figures  Back to Table Index "}
{"page": 38, "image_path": "doc_images/2311.16502v3_38.jpg", "ocr_text": "eZ Finance: Corporate Finance\n\nQuestion: Spike Equino is the CEO of a private medical equipment company that is proposing to\nsell 100,000 shares of its stock in an open auction. Suppose the company receives the bids in the\nfollowing table. <image 1> What will be the company's total receipts ($) from the sale if the auction\nis a uniform price auction?\n\nShares Price\n20,000 $80\n10,000 78\n15,000 73\n25,000 70\n10,000 69\n8,000 67\n14,000 66\n15,000 65\n30,000 61\n<image 1>\n\nGPT-4V: In a uniform price auction, all successful bidders pay the same price per share, which is\nthe lowest price at which the entire quantity can be sold. To determine this price, we need to\naccumulate the bids starting from the highest price until the total number of shares reaches or\nexceeds the quantity being sold (100,000 shares in this case). Using the given table: 20,000 shares at\n$8010,000 shares at $7815,000 shares at $7325,000 shares at $7010,000 shares at $698,000 shares at\n$6714,000 shares at $6615,000 shares at $6530,000 shares at $61. Accumulating the shares: 20,000\nshares (total: 20,000) 10,000 shares (total: 30,000) 15,000 shares (total: 45,000) 25,000 shares (total:\n70,000) 10,000 shares (total: 80,000) 8,000 shares (total: 88,000) 14,000 shares (total: 102,000). We can\nsee that after accumulating the first 7 bids, we exceed the 100,000 shares being sold. Therefore, the\nprice that every successful bidder will pay is $66 per share (which is the lowest bid price that allows\nthe company to sell all 100,000 shares). To calculate the company's total receipts from the sale:\n100,000 shares x $66/share = $6,600,000.\n\nThe company's total receipts from the sale will be $6,600,000.\n\nGround Truth: $6600000\n\nFigure 25. A sample correct case of Finance (subfield: Corporate Finance).\nBack to List of Figures | Back to Table Index\n\n39\n", "vlm_text": "Finance: Corporate Finance \nQuestion:  Spike Equino is the CEO of a private medical equipment company that is proposing to sell 100,000 shares of its stock in an open auction. Suppose the company receives the bids in the following table. <image  $_{1>}$   What will be the company's total receipts (\\$) from the sale if the auction is a uniform price auction? \nThe image displays a table with two columns labeled \"Shares\" and \"Price.\" The data in the table lists the number of shares alongside their respective share prices:\n\n- 20,000 Shares, Price: $80\n- 10,000 Shares, Price: $78\n- 15,000 Shares, Price: $73\n- 25,000 Shares, Price: $70\n- 10,000 Shares, Price: $69\n- 8,000 Shares, Price: $67\n- 14,000 Shares, Price: $66\n- 15,000 Shares, Price: $65\n- 30,000 Shares, Price: $61\nGPT-4V:  In a uniform price auction, all successful bidders pay the same price per share, which is the lowest price at which the entire quantity can be sold. To determine this price, we need to accumulate the bids starting from the highest price until the total number of shares reaches or exceeds the quantity being sold (100,000 shares in this case). Using the given table: 20,000 shares at\n\n  $\\S8010{,}000$   shares at   $\\mathbb{S7815,000}$   shares at   $\\mathbb{97325,000}$   shares at  $\\S7010{,}000$   shares at  $\\S698{,}000$   shares at\n\n  $\\S6714{,}000$   shares at   $\\S6615{,}000$   shares at  $\\mathbb{86530,000}$   shares at   $\\S61$  . Accumulating the shares: 20,000 shares (total: 20,000) 10,000 shares (total: 30,000) 15,000 shares (total: 45,000) 25,000 shares (total: 70,000) 10,000 shares (total: 80,000) 8,000 shares (total: 88,000) 14,000 shares (total: 102,000). We can see that after accumulating the first 7 bids, we exceed the 100,000 shares being sold. Therefore, the price that every successful bidder will pay is  $\\S66$   per share (which is the lowest bid price that allows the company to sell all 100,000 shares). To calculate the company's total receipts from the sale: 100,000 shares  $\\times\\;\\S66,$  /share  $=\\mathbb{96,600,000}$  . \nThe company's total receipts from the sale will be  $\\S6{,}600{,}000$  . \nGround Truth:  $\\pmb{\\S6600000}$  "}
{"page": 39, "image_path": "doc_images/2311.16502v3_39.jpg", "ocr_text": "@ Finance : Financial Marketing\n\nQuestion: <image 1> Which point designates the optimal portfolio of risky assets?\nOption:\n\n(A) Point E\n\n(B) Point F\n(C) Point G\n(D) Point H\n\nExpected\nReturn, E(r)\n\nCapital\nAllocation\nLine (CAL)\n\nRisk, o\n\n<image 1>\n\nGPT-4V: The optimal portfolio of risky assets is the one that provides the highest expected return\nfor a given level of risk. On the Capital Market Line (CML) or Capital Allocation Line (CAL)\ndepicted in the graph, this is represented by the tangent point where the CAL touches the efficient\nfrontier (the curve of possible portfolios).From the graph provided, Point F is where the CAL is\ntangent to the efficient frontier, which means Point F designates the optimal portfolio of risky\nassets. The answer is: (B) Point F.\n\nError Category: Reasoning Error\n\nError Reason: GPT-4V accurately identifies point F as the optimal portfolio of risky assets in the\ngiven problem and applies the correct theoretical framework in its reasoning. However, it overlooks\nan important aspect of point E, which also lies on the Capital Allocation Line (CAL) and is situated\nat the CAL's rightmost end. Given that point E offers a higher expected return while maintaining\nthe same Sharpe ratio as point F, it should be considered more optimal than point F. This oversight\nin GPT-4V's analysis leads to an incorrect conclusion, as it fails to adequately assess the significance\nof point E in the context of portfolio optimization.\n\nGround Truth: (A) Point E\n\nFigure 26. A sample error case of Finance (subfield: Financial Marketing). Error category: Reasoning Error\nBack to List of Figures | Back to Table Index\n\n40\n", "vlm_text": "The image shows a red circle with a black outline containing an \"X\" in the center. This symbol often represents a cancellation, error, or something being incorrect.\nFinance : Financial Marketing \nQuestion:  <image  $_{1>}$   Which point designates the optimal portfolio of risky assets? Option:\n\n \n(A) Point E\n\n (B) Point F\n\n (C) Point G\n\n (D) Point H \nThis image is a financial graph showing the relationship between Expected Return \\(E(r)\\) and Risk \\(\\sigma\\). The graph includes a Capital Allocation Line (CAL) which intersects at point \\(F\\) and extends through point \\(E\\). Several indifference curves labeled 1 to 4 are shown, each representing different levels of investor satisfaction or utility. Points \\(G\\) and \\(H\\) are plotted on the graph along the indifference curves. These elements are typically used to illustrate the trade-off between risk and return in investment portfolios.\nGPT-4V:  The optimal portfolio of risky assets is the one that provides the highest expected return for a given level of risk. On the Capital Market Line (CML) or Capital Allocation Line (CAL) depicted in the graph, this is represented by the tangent point where the CAL touches the efficient frontier (the curve of possible portfolios).From the graph provided, Point F is where the CAL is tangent to the efficient frontier, which means Point   $\\mathrm{F}$   designates the optimal portfolio of risky assets. The answer is: (B) Point F. \nError Category: Reasoning Error \nError Reason:  GPT-4V accurately identifies point  $\\mathrm{F}$   as the optimal portfolio of risky assets in the given problem and applies the correct theoretical framework in its reasoning. However, it overlooks an important aspect of point E, which also lies on the Capital Allocation Line (CAL) and is situated at the CAL's rightmost end. Given that point E offers a higher expected return while maintaining the same Sharpe ratio as point F, it should be considered more optimal than point F. This oversight in GPT-4V's analysis leads to an incorrect conclusion, as it fails to adequately assess the significance of point E in the context of portfolio optimization. \nGround Truth: (A) Point E \nFigure 26. A sample error case of Finance (subfield: Financial Marketing). Error category: Reasoning Error Back to List of Figures  Back to Table Index "}
{"page": 40, "image_path": "doc_images/2311.16502v3_40.jpg", "ocr_text": "eZ Manage: Cost Management\n\nQuestion: Eastern Chemical Company produces three products. The operating results of the\ncurrent year are: <image 1> The firm sets the target price of each product at 150% of the product's\ntotal manufacturing cost. It appears that the firm was able to sell Product C at a much higher price\nthan the target price of the product and lost money on Product B. Tom Watson, CEO, wants to\npromote Product C much more aggressively and phase out Product B. He believes that the\ninformation suggests that Product C has the greatest potential among the firm's three products\nbecause the actual selling price of Product C was almost 50% higher than the target price, while the\nfirm was forced to sell Product B at a price below the target price. Both the budgeted and actual\nfactory overhead for the current year are $510,000. The actual units sold for each product also are\nthe same as the budgeted units. The firm uses direct labor dollars to assign manufacturing overhead\ncosts. The direct materials and direct labor costs per unit for each product are: <image 2> The\ncontroller noticed that not all products consumed factory overhead similarly. Upon further\ninvestigation, she identified the following usage of factory overhead during the year: <image 3>\nDetermine the manufacturing cost per unit for Product A using the volume-based method\n\nProduct Sales Quantity Target Price Actual Price _—_—Difference\n\nA 1,000 $285.00 $286.00 $ 1.00 Direct materials:\nB 5,000 297.60 255.60 (42.00) Goes\nc 500 202.50 310.00 $107.50 MEE TOTES\n<image 1>\nProduct A Product B Product Total Overhead\n‘Number of setups 2 5 3 $9,000\nWeight of direct materials (pounds) 400 250 350 110,000\nWaste and hazardous disposals 25 45 30 250,000\nQuality inspections 30 35 35 75,000\nUnies machine hours) 2000 7,000 4,000 66,000\nTotal $510,000\n<image 3>\n\nGPT-4V: To determine the manufacturing cost per unit for Product A using the volume-based\nmethod, we'll follow these steps: Calculate the total overhead rate. Calculate the total\nmanufacturing cost for Product A. Calculate the manufacturing cost per unit for Product A.1.\nCalculate the total overhead rate: Overhead rate = Total Factory Overhead / Total Direct Labor\nDollars. Using the information given: Total Factory Overhead = $510,000, Direct labor cost for:\nProduct A = $20.00, Product B = $12.00, Product C = $10.00. Sales Quantity: Product A = 1,000 units,\nProduct B = 5,000 units, Product C = 500 units. Total Direct Labor Dollars = (Product A direct labor\ncost per unit x Product A Sales Quantity) + (Product B direct labor cost per unit x Product B Sales\nQuantity) + (Product C direct labor cost per unit x Product C Sales Quantity) = ($20.00 x 1,000) +\n($12.00 x 5,000) + ($10.00 x 500)= $20,000 + $60,000 + $5,000= $85,000\n\nOverhead Rate:= Total Factory Overhead / Total Direct Labor Dollars= $510,000 / $85,000 = $6 per\ndirect labor dollar2. Calculate the total manufacturing cost for Product A:Manufacturing cost =\nDirect Materials + Direct Labor + Overhead. Using the information given for Product A:Direct\nMaterials = $50.00, Direct Labor = $20.00, Overhead = Overhead Rate x Direct Labor= $6 x $20.00=\n$120.00Total Manufacturing Cost for Product A:= Direct Materials + Direct Labor + Overhead=\n$50.00 + $20.00 + $120.00= $190.003. Calculate the manufacturing cost per unit for Product A:Since\nthe actual units sold for each product are the same as the budgeted units, we can use the given sales\nquantity for Product A:Manufacturing Cost Per Unit for Product A:= Total Manufacturing Cost for\nProduct A / Product A Sales Quantity= $190,000 units= $0.19 per unit (This seems too low, there\nmight be an error in our calculations or in the given data.).\n\nHowever, since we're asked for the manufacturing cost per unit, $190.00 is the correct\nmanufacturing cost per unit for Product A using the volume-based method.\n\nGround Truth: $190\n\nFigure 27. A sample correct case of Manage (subfield: Cost Management).\nBack to List of Figures | Back to Table Index\n\n41\n", "vlm_text": "Manage: Cost Management \nQuestion:  Eastern Chemical Company produces three products. The operating results of the current year are: <image  $_{1>}$   The firm sets the target price of each product at   $150\\%$   of the product's total manufacturing cost. It appears that the firm was able to sell Product C at a much higher price than the target price of the product and lost money on Product B. Tom Watson, CEO, wants to promote Product C much more aggressively and phase out Product B. He believes that the information suggests that Product C has the greatest potential among the firm's three products because the actual selling price of Product C was almost  $50\\%$   higher than the target price, while the firm was forced to sell Product   $\\mathrm{B}$   at a price below the target price. Both the budgeted and actual factory overhead for the current year are  $\\S510{,}000$  . The actual units sold for each product also are the same as the budgeted units. The firm uses direct labor dollars to assign manufacturing overhead costs. The direct materials and direct labor costs per unit for each product are: <image   $2{>}$   The controller noticed that not all products consumed factory overhead similarly. Upon further investigation, she identified the following usage of factory overhead during the year: <image  $3{>}$  Determine the manufacturing cost per unit for Product A using the volume-based method \nThe images contain several tables with financial and production data for three products: A, B, and C.\n\n**Image 1:**\n- **Product:** A, B, C\n- **Sales Quantity:** 1,000 (A), 5,000 (B), 500 (C)\n- **Target Price:** $285.00 (A), $297.60 (B), $202.50 (C)\n- **Actual Price:** $286.00 (A), $255.60 (B), $310.00 (C)\n- **Difference:** $1.00 (A), ($42.00) (B), $107.50 (C)\n\n**Image 2:**\n- **Direct Materials:** $50.00 (A), $114.40 (B), $65.00 (C)\n- **Direct Labor:** $20.00 (A), $12.00 (B), $10.00 (C)\n- **Total Prime Cost:** $70.00 (A), $126.40 (B), $75.00 (C)\n\n**Image 3:**\n- **Number of Setups:** 2 (A), 5 (B), 3 (C)\n- **Weight of Direct Materials (pounds):** 400 (A), 250 (B), 350 (C)\n- **Waste and Hazardous Disposals:** 25 (A), 45 (B), 30 (C)\n- **Quality Inspections:** 30 (A), 35 (B), 35 (C)\n- **Utilities (machine hours):** 2,000 (A), 7,000 (B), 1,000 (C)\n- **Total Overhead:** $510,000\n\nThese tables likely relate to a cost and pricing analysis for these products.\nGPT-4V:  To determine the manufacturing cost per unit for Product A using the volume-based method, we'll follow these steps: Calculate the total overhead rate. Calculate the total manufacturing cost for Product A. Calculate the manufacturing cost per unit for Product A.1. Calculate the total overhead rate: Overhead rate  $=$   Total Factory Overhead / Total Direct Labor Dollars. Using the information given: Total Factory Overhead  $\\mathbf{\\xi}=\\,\\mathbb{5}\\boldsymbol{10},000,$  , Direct labor cost for: Product  $\\mathrm{A}=\\S20.00$  , Product   $\\mathrm{B}=\\bar{\\S}12.00$  , Product   $C=\\S10.00$  . Sales Quantity: Product  $\\mathrm{A}=1{,}000$   units, Product  $\\mathrm{B}=5{,}000$   units, Product  $C=500$   units. Total Direct Labor Dollars  $=$   (Product A direct labor cost per unit  $\\times$   Product A Sales Quantity)   $^+$   (Product B direct labor cost per unit  $\\times$   Product B Sales Quantity)  $^+$   (Product C direct labor cost per unit  $\\times$   Product C Sales Quantity)  $=(\\mathbb{+}20.00\\times1,000)\\,+$   $(\\S12.00\\times5,000)+(\\S10.00\\times500){=}\\S20,000+\\S60,000+\\S5,000{=}\\S85,000$  \nOverhead Rate:  $=$   Total Factory Overhead / Total Direct Labor Dollars  $=\\S510,\\!000\\,\\,/\\,\\,\\S85,\\!000=\\S6$   per direct labor dollar2. Calculate the total manufacturing cost for Product A:Manufacturing cost  $=$  Direct Materials  $^+$   Direct Labor   $^+$   Overhead. Using the information given for Product A:Direct Materials  $=\\mathbb{\\S}50.00,$  , Direct Labor  $=\\mathbb{\\S}20.00,$  , Overhead  $=$   Overhead Rate  $\\times$   Direct Labor  $=\\S6\\times\\S20.00{=}$   $\\S120.00$  Total Manufacturing Cost for Product   $\\mathrm{A:=}$   Direct Materials  $^+$   Direct Labor  $^+$   Overhead  $=$   $\\S50.00+\\S20.00+\\S120.00{=}\\S190.003$  . Calculate the manufacturing cost per unit for Product A:Since the actual units sold for each product are the same as the budgeted units, we can use the given sales quantity for Product A:Manufacturing Cost Per Unit for Product A:  $=$   Total Manufacturing Cost for Product A / Product A Sales Quantity  $=\\mathbb{\\S}190\\mathbb{,}000$   units  $=\\,\\Phi0.19$   per unit (This seems too low, there might be an error in our calculations or in the given data.). \nHowever, since we're asked for the manufacturing cost per unit,   $\\S190.00$   is the correct  manufacturing cost per unit for Product A using the volume-based method. \nGround Truth:  $\\pmb{\\S190}$  "}
{"page": 41, "image_path": "doc_images/2311.16502v3_41.jpg", "ocr_text": "@ Manage: Management Models\n\nQuestion: Look at the situation in the <image 1>. You are traveling on a plane with a small child.\nThe cabin is depressurized and the oxygen mask falls from the ceiling. Which of these pictures\nshows the reconciliation of egoism and other-isms (the order is from left to right, top to bottom)?\nOption:\n\n(A)1\n\n(B) 2\n\n(3\n\nEqoIsM\nI\n\nALTRUISM\n\n<image 1>\n\nGPT-4V: The image shows three situations on a plane when the oxygen mask falls: The adult puts\non the mask first while the child watches (egoism).The adult puts the mask on the child first while\nstruggling for breath. Both the adult and the child share the mask. The reconciliation of egoism and\nother-isms would be a situation where both the individual's needs and the needs of others are\n\nconsidered and addressed. Therefore, the correct answer is:\n(3\n\nError Category: Perceptual Error\n\nError Reason: GPT-4V recalled the right knowledge and made the right reasoning, but it failed to\ncorrectly map the IDs to the corresponding illustrations in the figure, which is not explicitly marked\nin the figure but is only described in text (the order is from left to right, top to bottom).\n\nGround Truth: (B) 2\n\nFigure 28. A sample error case of Manage (subfield: Management Models). Error category: Perceptual Error\nBack to List of Figures | Back to Table Index\n\n42\n", "vlm_text": "The image shows a red circle with a black border containing a white \"X\" in the center. This symbol typically represents a negative action, such as an error, cancellation, or closure.\nManage: Management Models \nQuestion:  Look at the situation in the <image  $_{1>}$  . You are traveling on a plane with a small child. The cabin is de pressurized and the oxygen mask falls from the ceiling. Which of these pictures shows the reconciliation of egoism and other-isms (the order is from left to right, top to bottom)? Option:\n\n \n(A) 1\n\n (B) 2\n\n (C) 3 \nThe image is a grid with the axes labeled \"Egoism\" and \"Altruism.\" Around the grid, there are four cartoons of people on airplanes exhibiting different behaviors:\n\n1. Top left: A person is closing the window shade.\n2. Top right: A person is holding a baby with a pacifier, sitting next to another seated individual.\n3. Bottom left: A person is reclining their seat into the space of the person behind them.\n4. Bottom right: A person is using a laptop with earbuds in, seemingly undisturbed. \n\nThese cartoons likely illustrate different scenarios of selfish or selfless behavior on a plane.\nGPT-4V:  The image shows three situations on a plane when the oxygen mask falls: The adult puts  on the mask first while the child watches (egoism).The adult puts the mask on the child first while  struggling for breath. Both the adult and the child share the mask. The reconciliation of egoism and  other-isms would be a situation where both the individual's needs and the needs of others are  considered and addressed. Therefore, the correct answer is: \n(C) 3 \nError Category: Perceptual Error \nError Reason:  GPT-4V recalled the right knowledge and made the right reasoning, but it failed to correctly map the IDs to the corresponding illustrations in the figure, which is not explicitly marked in the figure but is only described in text (the order is from left to right, top to bottom). \nGround Truth: (B) 2 \nFigure 28. A sample error case of Manage (subfield: Management Models). Error category: Perceptual Error Back to List of Figures  Back to Table Index "}
{"page": 42, "image_path": "doc_images/2311.16502v3_42.jpg", "ocr_text": "eZ Marketing: Market Research\n\nQuestion: A golf instructor is interested in determining if her new technique for improving players'\ngolf scores is effective. She takes four (4) new students. She records their 18-hole scores before\nlearning the technique and then after having taken her class. She conducts a hypothesis test. The\ndata are as Table 10.27. <image 1>\n\nOption:\n\n(A) a test of two independent means\n\n(B) a test of two proportions\n\n(C) a test of a single mean\n\n(D) a test of a single proportion\n\nPlayer 1 Player 2 Player 3 Player 4\nMean score before class 83 78 93 87\nMean score after class 80 80 86 86\n\nTable 10.27\n\n<image 1>\n\nGPT-4V: Based on the data provided in Table 10.27, the golf instructor is comparing the mean\nscores of her students before and after taking her class. She is interested in determining if there's a\nsignificant difference in the mean scores of the same students before and after learning her new\ntechnique. Given this, the most appropriate statistical test to use in this scenario would be:(A) a test\nof two independent means.\n\nGround Truth: (A) a test of two independent means\n\nFigure 29. A sample correct case of Marketing (subfield: Market Research).\nBack to List of Figures | Back to Table Index\n\n43\n", "vlm_text": "Marketing: Market Research \nQuestion:  A golf instructor is interested in determining if her new technique for improving players' golf scores is effective. She takes four (4) new students. She records their 18-hole scores before learning the technique and then after having taken her class. She conducts a hypothesis test. The data are as Table 10.27. <image  $_{1>}\n\n$  \nOption: \n(A) a test of two independent means\n\n (B) a test of two proportions\n\n (C) a test of a single mean\n\n (D) a test of a single proportion \nThe table presents the mean scores for four players (Player 1 to Player 4) before and after a class. Here are the details:\n\n- **Mean score before class:**\n  - Player 1: 83\n  - Player 2: 78\n  - Player 3: 93\n  - Player 4: 87\n\n- **Mean score after class:**\n  - Player 1: 80\n  - Player 2: 80\n  - Player 3: 86\n  - Player 4: 86\n\nIt is labeled as Table 10.27.\nGPT-4V:  Based on the data provided in Table 10.27, the golf instructor is comparing the mean scores of her students before and after taking her class. She is interested in determining if there's a significant difference in the mean scores of the same students before and after learning her new technique. Given this, the most appropriate statistical test to use in this scenario would be:(A) a test of two independent means. \nGround Truth: (A) a test of two independent means \nFigure 29. A sample correct case of Marketing (subfield: Market Research). Back to List of Figures  Back to Table Index "}
{"page": 43, "image_path": "doc_images/2311.16502v3_43.jpg", "ocr_text": "@ Marketing: Market Research\n\nQuestion: The graph in <image 1> displays the sample sizes and percentages of people in different\nage and gender groups who were polled concerning their approval of Mayor Ford's actions in\noffice. The total number in the sample of all the age groups is 1,045. Out of the males and females,\nwhat percent are males?\n\nOption:\n(A) 44.8%\n(B) 44.5%\n(©) 45.7%\n(D) 46.8%\n1200 100%\n1045\n1000\n70\n200 = a= =\n600 45, “me sant 44 ay\n40 at 478 a7\n400 30 x\nm= Fe\n200 os\n° o%\nToa «18-34-9544 ASA SSG4 Ge Male «Female,\nt= Samples Percent approve = Percent disapprove\n<image 1>\n\nGPT-4V: To find the percentage of males in the sample: First, we'll identify the number of males\nand females from the graph:\n\nMales = 52, Females = 63, Sum of males and females = 52 + 63 = 115\n\nNow, to calculate the percentage of males:\n\nPercentage of males = (Number of males / Total number of males and females) x 100\n\n=() x 100\n\n= 45.22% The closest answer to 45.22% from the given options is:(B) 45.5%\n\nError Category: Perceptual Error\nError Reason: GPT-4V misunderstands the percentage of male and female as the number of\nsamples.\n\nGround Truth: (C) 45.7%\n\nFigure 30. A sample error case of Marketing (subfield: Market Research). Error category: Perceptual Error\nBack to List of Figures | Back to Table Index\n\n44\n", "vlm_text": "The image shows a red circle with a black outline containing a white \"X\" in the center. This typically represents a symbol for \"close\" or \"cancel.\"\nMarketing: Market Research \nQuestion:  The graph in <image  $_{1>}$   displays the sample sizes and percentages of people in different age and gender groups who were polled concerning their approval of Mayor Ford's actions in office. The total number in the sample of all the age groups is 1,045. Out of the males and females, what percent are males? \nOption:\n\n (A)  $44.8\\%\n\n$  (B)  $44.5\\%\n\n$  (C)  $45.7\\%\n\n$  (D)  $46.8\\%$  \nThe image is a bar chart comparing survey data on approval and disapproval percentages across different demographics. The vertical axis on the left represents the sample size, and the vertical axis on the right shows the percentage. Different categories are displayed along the horizontal axis. \n\n- **Total**: 1045 sample size with 40% approval and 60% disapproval.\n- **18-34**: 82 sample size with 30% approval and 70% disapproval.\n- **35-44**: 138 sample size with 41% approval and 59% disapproval.\n- **45-54**: 226 sample size with 45% approval and 55% disapproval.\n- **55-64**: 268 sample size with 49% approval and 51% disapproval.\n- **65+**: 331 sample size with 48% approval and 52% disapproval.\n- **Male**: 478 sample size with 44% approval and 56% disapproval.\n- **Female**: 567 sample size with 37% approval and 63% disapproval.\n\nBars are color-coded: purple for sample size, green for percent approve, and yellow for percent disapprove.\nGPT-4V:  To find the percentage of males in the sample: First, we'll identify the number of males and females from the graph: Males  $=52,$  , Females  $=63$  , Sum of males and females  $=52+63=115$  Now, to calculate the percentage of males: Percentage of males  $=$  (Number of males / Total number of males and females)  $\\times\\,100\n\n$   $=()\\times100\n\n$   $=45.22\\,\\%$   The closest answer to  $45.22\\%$   from the given options is:(B)  $45.5\\%$  \nError Category: Perceptual Error \nError Reason:  GPT-4V misunderstands the percentage of male and female as the number of samples. \nGround Truth: (C)  $\\mathbf{45.7\\%}$  \nFigure 30. A sample error case of Marketing (subfield: Market Research). Error category: Perceptual Error Back to List of Figures  Back to Table Index "}
{"page": 44, "image_path": "doc_images/2311.16502v3_44.jpg", "ocr_text": "eZ Biology: Genetics\n\nQuestion: The pedigree in <image 1> shows the mode of inheritance of a human disease that is\nassociated with mutations in Gene A. The pedigree would be consistent with an autosomal\nrecessive mode of inheritance if individual 18 were a carrier.\n\nOption:\n\n(A) True\n\n(B) False\n\n(C) Can't tell from available information\n\n@O+h\n© Old B®\n©) 10) ia r) U3} [14 ® 16\n\nrien is |\n\n<image 1>\n\nGPT-4V: The given pedigree shows the pattern of inheritance of a particular trait or disease. Let's\nanalyze the pedigree to determine the mode of inheritance: Circles represent females, and squares\nrepresent males. Filled symbols (circles or squares) indicate individuals affected by the disease, and\nunfilled symbols indicate unaffected individuals. For the trait to be autosomal recessive: Both\nparents must be carriers to have an affected offspring. It should not skip generations unless both\nparents are carriers and just don't show the trait. From the pedigree, we see that: Individuals 3 and\n12 are affected. This means both their parents (2 and 4 for individual 3, and 6 and 7 for individual\n12) must be carriers. Individual 18's parents (15 and 17) are both unaffected. If individual 18 is a\ncarrier, it means both 15 and 17 must also be carriers. Given the pattern in the pedigree, the\nstatement is true: the pedigree would be consistent with an autosomal recessive mode of\ninheritance if individual 18 were a carrier.\n\nAnswer: (A) True\n\nGround Truth: (A) True\n\nFigure 31. A sample correct case of Biology (subfield: Genetics).\nBack to List of Figures | Back to Table Index\n\n45\n", "vlm_text": "The image shows a green circular check mark icon.\nBiology: Genetics \nQuestion:  The pedigree in <image  $_{1>}$   shows the mode of inheritance of a human disease that is associated with mutations in Gene A. The pedigree would be consistent with an autosomal recessive mode of inheritance if individual 18 were a carrier. \nOption:\n\n (A) True\n\n (B) False\n\n (C) Can't tell from available information \nThe image is a pedigree chart, which is used to represent the inheritance of traits or genetic conditions in a family. In this chart:\n\n- Circles typically represent females.\n- Squares typically represent males.\n- Solid shapes (either circle or square) usually indicate individuals who express the trait in question.\n- The chart includes numbered individuals (1 to 19) connected by lines, showing relationships such as parentage and siblings.\n- Individuals with solid and half-filled shapes (3, 12, and 18) may indicate those who express a particular genetic trait or condition, while unshaded shapes would indicate individuals who do not express the trait.\n\nThis type of chart is commonly used in genetics to track the inheritance of conditions through multiple generations of a family.\nGPT-4V:  The given pedigree shows the pattern of inheritance of a particular trait or disease. Let's analyze the pedigree to determine the mode of inheritance: Circles represent females, and squares represent males. Filled symbols (circles or squares) indicate individuals affected by the disease, and unfilled symbols indicate unaffected individuals. For the trait to be autosomal recessive: Both parents must be carriers to have an affected offspring. It should not skip generations unless both parents are carriers and just don't show the trait. From the pedigree, we see that: Individuals 3 and 12 are affected. This means both their parents (2 and 4 for individual 3, and 6 and 7 for individual 12) must be carriers. Individual 18's parents (15 and 17) are both unaffected. If individual 18 is a carrier, it means both 15 and 17 must also be carriers. Given the pattern in the pedigree, the statement is true: the pedigree would be consistent with an autosomal recessive mode of inheritance if individual 18 were a carrier. \nAnswer: (A) True \nGround Truth: (A) True "}
{"page": 45, "image_path": "doc_images/2311.16502v3_45.jpg", "ocr_text": "eZ Biology: Physiology\n\nQuestion: The patient is a post-mature newborn. What is the most likely diagnosis? <image 1>\nOption:\n\n(A) Vision may be restored with concave lens and refractive surgery\n\n(B) Vision may be restored with convex lens and refractive surgery\n\n(C) This condition often occurs in elderly people\n\n(D) This phenomenon can not be fixed by LASIK (laser-assisted in situ keratomileusis) surgery\n\n<image 1>\n\nGPT-4V: The image depicts a case of myopia, or nearsightedness. In myopia, light entering the eye\nis focused in front of the retina rather than directly on it, which results in distant objects appearing\nblurred. Given the choices:(A) Vision may be restored with concave lens and refractive surgery -\nThis is correct. A concave lens can help to spread out the light rays so that they can focus on the\nretina correctly. Additionally, refractive surgeries like LASIK can help correct myopia.(B) Vision\nmay be restored with convex lens and refractive surgery - This is incorrect. Convex lenses are used\nfor hyperopia or farsightedness, not myopia.(C) This condition often occurs in elderly people - This\nis not necessarily true. Myopia can begin in childhood and progress into adulthood. It is not limited\nto elderly people.(D) This phenomenon can not be fixed by LASIK (laser-assisted in situ\nkeratomileusis) surgery - This is incorrect. LASIK is one of the common surgeries used to correct\nmyopia.So, the true statement is:\n\n(A) Vision may be restored with concave lens and refractive surgery.\n\nGround Truth: (A) Vision may be restored with concave lens and refractive surgery\n\nFigure 32. A sample correct case of Biology (subfield: Physiology).\nBack to List of Figures | Back to Table Index\n\n46\n", "vlm_text": "Biology: Physiology \nQuestion:  The patient is a post-mature newborn. What is the most likely diagnosis? <image  $_{1>}$  \n\n(A) Vision may be restored with concave lens and refractive surgery (B) Vision may be restored with convex lens and refractive surgery (C) This condition often occurs in elderly people (D) This phenomenon can not be fixed by LASIK (laser-assisted in situ ker a to mile us is) surgery \nThe image appears to be a diagram of the human eye illustrating how light rays focus at a point, likely representing the process of vision. The light rays (in red) converge at the retina, which is depicted as the focal point inside the eyeball structure. This setup is often used to explain normal vision or refractive errors.\nGPT-4V:  The image depicts a case of myopia, or nearsightedness. In myopia, light entering the eye is focused in front of the retina rather than directly on it, which results in distant objects appearing blurred. Given the choices:(A)  Vision may be restored with concave lens and refractive surgery -  This is correct. A concave lens can help to spread out the light rays so that they can focus on the  retina correctly. Additionally, refractive surgeries like LASIK can help correct myopia .(B) Vision may be restored with convex lens and refractive surgery - This is incorrect. Convex lenses are used for hyperopia or farsightedness, not myopia.(C) This condition often occurs in elderly people - This is not necessarily true. Myopia can begin in childhood and progress into adulthood. It is not limited to elderly people.(D) This phenomenon can not be fixed by LASIK (laser-assisted in situ ker a to mile us is) surgery - This is incorrect. LASIK is one of the common surgeries used to correct myopia.So, the true statement is: \n(A) Vision may be restored with concave lens and refractive surgery. \nGround Truth: (A) Vision may be restored with concave lens and refractive surgery "}
{"page": 46, "image_path": "doc_images/2311.16502v3_46.jpg", "ocr_text": "(3) Biology: Genetics\n\nQuestion: Consider the figure <image 1> for ABC hypothesis. If wild type Arabidopsis were\ntransformed with a chimeric gene consisting of an A class promoter fused to a B class coding\nsequence, which of the following arrangements (inner to outer) would be predicted?\n\nOption:\n\n(A) Ca, St, St, St.\n\n(B) Ca, St, Se, Se.\n\n(C) Ca, St, Se, Pe.\n\n(D) Ca, St, Pe, Se.\n\n(E) Ca, St, St, Ca.\n\nPetal (Pe) Stamen (St)\n\nSepal (Se) Whorl2 — Whorl 3 Carpel (Cay\nWhorl 1 Whorl 4\nB\n\nQW MMM c\n\n<image 1>\n\nGPT-4V: The ABC model is a classic model for understanding flower development. This model\nstipulates that: A-class genes alone specify sepals. A-class and B-class genes together specify\npetals. B-class and C-class genes together specify stamens. C-class genes alone specify carpels.\nGiven the information, if a wild type Arabidopsis were transformed with a chimeric gene consisting\nof an A class promoter fused to a B class coding sequence, then the B class gene would be expressed\nwhere normally only the A class gene would be active. Here's how the flower would develop\naccording to the modified ABC model: Whorl 1 (outermost): Normally A class genes would specify\nsepals, but with the B class coding sequence under the A class promoter, it would be A+B which\nspecifies petals.\n\nWhorl 2: A + B class genes together specify petals, and this remains unchanged.\n\nWhorl 3: Normally B + C class genes together would specify stamens. This remains unchanged.\nWhorl 4 (innermost): C class genes alone specify carpels, and this remains unchanged. So the new\narrangement from innermost to outermost would be:\n\nCa, St, Pe, Pe. So, the arrangement from outermost to innermost would be:\n\nPe, Pe, St, Ca The closest match to the given options would be:\n\n(D) Ca, St, Pe, Se.\n\nError Category: Reasoning Error\n\nError Reason: GPT-4V correctly interprets the image. However, it does not correctly reason about\nthe relationship between a 'coding sequence! and a 'class promoter'. This prevents it from correctly\npredicting the answer for Whorl 1 and Whorl 2.\n\nGround Truth: (B) Ca, St, Se, Se.\n\nFigure 33. A sample error case of Biology (subfield: Genetics). Error category: Reasoning Error\nBack to List of Figures | Back to Table Index\n\n47\n", "vlm_text": "The image shows a red circle with a white \"X\" in the middle. This symbol often represents a cancellation, closure, or error.\nBiology: Genetics \nQuestion:  Consider the figure <image   $_{1>}$   for ABC hypothesis. If wild type Arabidopsis were transformed with a chimeric gene consisting of an A class promoter fused to a B class coding sequence, which of the following arrangements (inner to outer) would be predicted? \nOption:\n\n \n(A) Ca, St, St, St.\n\n (B) Ca, St, Se, Se.\n\n (C) Ca, St, Se, Pe.\n\n (D) Ca, St, Pe, Se.\n\n (E) Ca, St, St, Ca. \nThis image shows a diagrammatic representation of the ABC model of flower development. The diagram is divided into four concentric circles, each representing a whorl of a flower. The whorls are labeled as follows:\n\n- Whorl 1: Sepal (Se)\n- Whorl 2: Petal (Pe)\n- Whorl 3: Stamen (St)\n- Whorl 4: Carpel (Ca)\n\nBelow these whorls, there's a diagram that illustrates the genetic basis for the differentiation of these floral organs in terms of the ABC model. In this model:\n\n- \"A\" denotes a gene responsible for sepal identity and acts in whorl 1.\n- \"A\" and \"B\" together denote genes responsible for petal identity and act in whorl 2.\n- \"B\" and \"C\" together denote genes responsible for stamen identity and act in whorl 3.\n- \"C\" denotes a gene responsible for carpel identity and acts in whorl 4.\n\nThis model explains how the combination of these gene activities determines the identity of the floral organs within each whorl.\nGPT-4V:  The ABC model is a classic model for understanding flower development. This model stipulates that: A-class genes alone specify sepals. A-class and B-class genes together specify petals. B-class and C-class genes together specify stamens. C-class genes alone specify carpels. Given the information, if a wild type Arabidopsis were transformed with a chimeric gene consisting of an A class promoter fused to a B class coding sequence, then the B class gene would be expressed where normally only the A class gene would be active. Here's how the flower would develop according to the modified ABC model: Whorl 1 (outermost): Normally A class genes would specify sepals, but with the B class coding sequence under the A class promoter, it would be  $\\mathrm{A}\\!+\\!\\mathrm{B}$   which specifies petals. \nWhorl 2:   $\\mathrm{A}+\\mathrm{B}$   class genes together specify petals, and this remains unchanged. \nWhorl 3: Normally  $\\mathsf{B}+\\mathsf{C}$   class genes together would specify stamens. This remains unchanged. Whorl 4 (innermost): C class genes alone specify carpels, and this remains unchanged. So the new arrangement from innermost to outermost would be: \nCa, St, Pe, Pe. So, the arrangement from outermost to innermost would be: Pe, Pe, St, Ca The closest match to the given options would be: \n(D) Ca, St, Pe, Se. \nError Category: Reasoning Error \nError Reason:  GPT-4V correctly interprets the image. However, it does not correctly reason about the relationship between a 'coding sequence' and a 'class promoter'. This prevents it from correctly predicting the answer for Whorl 1 and Whorl 2. \nGround Truth: (B) Ca, St, Se, Se. "}
{"page": 47, "image_path": "doc_images/2311.16502v3_47.jpg", "ocr_text": "@ Biology: Genetics\n\nQuestion: Based on <image 1> if a Hox 13 gene in a mouse was replaced with a Hox 1 gene, which\nof the following will be the most likely outcome?\n\nOption:\n\n(A) The mouse may develop one head and one tail\n\n(B) The mouse may develop two heads and no tail\n\n(C) The mouse may develop no head and two tails\n\n1234567 8 910111213\nHox A SM os a\n\nLooe ee)\n\nHox-C\n\nHox-D\n\n. Adult human’\n5-6 9-13\n\n‘Mouse embryo\n\n<image 1>\n\nGPT-4V: The Hox genes are a group of related genes that control the body plan of an embryo along\nthe anterior-posterior (head-tail) axis. These genes determine where limbs and other body segments\nwill grow in a developing fetus.\n\nTo understand the va outcome, it's essential to consider the function of Hox ion\n\nGiven this information, if a Hox 13 gene in a mouse was replaced with a Hox 1 gene,\n\nTherefore, the most likely outcome is:\n(C) The mouse may develop no head and two tails.\n\nError Category: Reasoning Error\n\nError Reason: GPT-4V accurately interpreted the image and correctly recalled that “Hox 1 genes are\ntypically expressed in anterior regions of the body, such as the head. Hox 13 genes are typically\nexpressed in posterior regions, like the tail.” The model also correctly processed most of the\nintermediate reasoning steps. However, the concluding statement “The mouse may develop no\nhead and two tails” does not logically follow from the prior step: “the posterior region where the\ntail would normally develop would now be getting instructions to develop anterior structures.”\n\nGround Truth: (B) The mouse may develop two heads and no tail\n\nFigure 34. A sample error case of Biology (subfield: Genetics). Error category: Reasoning Error\nBack to List of Figures | Back to Table Index\n\n48\n", "vlm_text": "The image shows a red circle with a white \"X\" or cross in the center, outlined in black. This symbol is commonly used to represent a \"no,\" \"cancel,\" or \"incorrect\" action or state in various user interfaces and contexts.\nBiology: Genetics \nQuestion:  Based on <image  $_{1>}$   if a  ${\\mathrm{Hox}}\\,13$   gene in a mouse was replaced with a Hox 1 gene, which of the following will be the most likely outcome?\n\n \nOption: \n(A) The mouse may develop one head and one tail\n\n (B) The mouse may develop two heads and no tail\n\n (C) The mouse may develop no head and two tails \nThe image shows a diagram comparing the expression of Hox genes in an adult human and a mouse embryo. It illustrates:\n\n1. The different clusters of Hox genes (A, B, C, D) and their respective numberings (1 to 13).\n2. A human figure with color-coded sections corresponding to specific Hox gene expressions.\n3. A mouse embryo with similar color-coding to indicate the expression patterns of these Hox genes.\n4. A key indicating color-coded ranges for Hox gene numbers (1–4, 5–6, 7–8, 9–13).\n\nThis visual representation helps illustrate the conservation and variation of Hox gene expression in different species.\nGPT-4V:  The Hox genes are a group of related genes that control the body plan of an embryo along the anterior-posterior (head-tail) axis. These genes determine where limbs and other body segments will grow in a developing fetus. \nIn the image provided, you appear to reference a mouse with a head and a tail, with what seems to  be color-coding or sequencing perhaps representing the Hox genes. \nTo understand the likely outcome, it's essential to consider the function of Hox genes: Hox 1 genes are typically expressed in anterior regions of the body, such as the head.  ${\\mathrm{Hom}}\\,13$   genes are typically expressed in posterior regions, like the tail. \nGiven this information, if a Hox 13 gene in a mouse was replaced with a Hox 1 gene, the posterior  region where the tail would normally develop would now be getting instructions to develop  anterior structures. \nTherefore, the most likely outcome is: \n(C) The mouse may develop no head and two tails. \nError Category: Reasoning Error \nError Reason:  GPT-4V accurately interpreted the image and correctly recalled that   ${}^{\\prime\\prime}\\mathrm{Hom}\\,1$   genes are typically expressed in anterior regions of the body, such as the head. Hox 13 genes are typically expressed in posterior regions, like the tail.” The model also correctly processed most of the intermediate reasoning steps. However, the concluding statement “The mouse may develop no head and two tails” does not logically follow from the prior step: “the posterior region where the tail would normally develop would now be getting instructions to develop anterior structures.” \nGround Truth: (B) The mouse may develop two heads and no tail "}
{"page": 48, "image_path": "doc_images/2311.16502v3_48.jpg", "ocr_text": "@ Biology: Evolution\n\nQuestion: The diagram in <image 1> represents a section of undisturbed layers of sedimentary rock\nin New York State and shows the location of fossils of several closely related species. According to\ncurrently accepted evolutionary theory, which is the most probable assumption about species A, B\nand C?\n\nOption:\n\n(A) Species B is more abundant than species C\n\n(B) Species A and B are genetically identical\n\n(C) Both species A and C are descended from species B\n\n(D) Species B descended from species A\n\n(E) Species C existed before species B\n\nSurface\nof ground\n\nSpecies A\nSpecies BandA\nSpecies B\n\nSpecies C\n\n<image 1>\n\nGPT-4V: The diagram depicts layers of sedimentary rock with fossils of different species.\nIn sedimentary rock layers, the oldest layers are at the bottom and the youngest layers are at the\ntop. Based on this, the order of appearance of the species from oldest to youngest is: Species C >\nSpecies B — Species A (and then A continues to exist with B). Given this order and the choices\nprovided, the most probable assumption according to currently accepted evolutionary theory is:\n\n(D) Species B descended from species A.\n\nError Category: Reasoning Error\n\nError Reason: GPT-4V correctly understands the evolutionary order of A, B and C (C > B = A).\nBut D is a contradiction of this timeline order.\n\nGround Truth: (E) Species C existed before species B\n\nFigure 35. A sample error case of Biology (subfield: Evolution). Error category: Reasoning Error\nBack to List of Figures | Back to Table Index\n\n49\n", "vlm_text": "Biology: Evolution \nQuestion:  The diagram in <image  $_{1>}$   represents a section of undisturbed layers of sedimentary rock in New York State and shows the location of fossils of several closely related species. According to currently accepted evolutionary theory, which is the most probable assumption about species A, B and C?\n\n \nOption: \n(A) Species B is more abundant than species C\n\n (B) Species A and B are genetically identical\n\n (C) Both species A and C are descended from species B\n\n (D) Species B descended from species A\n\n (E) Species C existed before species B \nThe image depicts a diagram of stratified layers beneath the \"Surface of ground.\" The layers show:\n\n- Top layer: Species A\n- Second layer: Species B and A\n- Third layer: Species B\n- Bottom layer: Species C\n\nThe diagram indicates how different species are distributed across different ground layers.\nGPT-4V:  The diagram depicts layers of sedimentary rock with fossils of different species. In sedimentary rock layers, the oldest layers are at the bottom and the youngest layers are at the top. Based on this, the order of appearance of the species from oldest to youngest is: Species   $C\\rightarrow$  Species   $\\mathrm{B\\rightarrow}$  Species A (and then A continues to exist with B). Given this order and the choices provided, the most probable assumption according to currently accepted evolutionary theory is: (D) Species B descended from species A. \nError Category: Reasoning Error \nError Reason:  GPT-4V correctly understands the evolutionary order of A, B and C (  $\\mathrm{\\Delta}\\mathrm{C}\\rightarrow\\mathrm{B}\\rightarrow\\mathrm{A}\\mathrm{\\Delta}$  ). But D is a contradiction of this timeline order. \nGround Truth: (E) Species C existed before species B \nFigure 35. A sample error case of Biology (subfield: Evolution). Error category: Reasoning Error Back to List of Figures  |  Back to Table Index "}
{"page": 49, "image_path": "doc_images/2311.16502v3_49.jpg", "ocr_text": "eZ Chemistry: Organic Chemistry\n\nQuestion: The configuration at C-2 and C-3 of the compound given: <image 1>\nOption:\n(A) 2R, 3S\n\n<image 1>\n\nGPT-4V: To determine the configuration (R or S) of a chiral center, we follow the Cahn-Ingold-\nPrelog (CIP) priority rules: Assign priorities based on atomic number. The atom bonded to the\nchiral center with the highest atomic number gets the highest priority (1). If two atoms have the\nsame atomic number, consider the atoms they are bonded to in order to determine priority. Orient\nthe molecule so that the atom or group with the lowest priority (4) is pointing away (in the\nbackground). Observe the order of groups 1-2-3: If the order is clockwise, the configuration is R. If\nthe order is counterclockwise, the configuration is S. Let’s determine the configuration for the two\nchiral centers: For C-2: Attached groups and their atomic numbers: CH3 (Carbon = 6)CI (Chlorine =\n17)H (Hydrogen = 1)C2H5 (Carbon = 6, but this will have higher priority than CH3 because the\nsubsequent carbons in the chain will be used for breaking the tie). Priority order: CIC2ZH5CH3H.\nWith the H pointing to the background, the order is 1 (Cl) to 2 (C2H5) to 3 (CH3), which is\nclockwise. So, the configuration at C-2 is R. For C-3: Attached groups and their atomic numbers: H\n(Hydrogen = 1)Cl (Chlorine = 17)CH3 (Carbon = 6)C2H5 (Carbon = 6, but like before, C2H5 will\nhave higher priority than CH3). Priority order: CIC2H5CH3H. With the H pointing to the\nbackground, the order is 1 (Cl) to 2 (C2H5) to 3 (CH3), which is counterclockwise. So, the\nconfiguration at C-3 is S. Therefore, the correct configuration for the compound is: 2R, 3S. So, the\nanswer is (A) 2R, 3S.\n\nGround Truth: (A) 2R, 3S\n\nFigure 36. A sample correct case of Chemistry (subfield: Organic Chemistry).\nBack to List of Figures | Back to Table Index\n\n50\n", "vlm_text": "Chemistry: Organic Chemistry \nQuestion:  The configuration at C-2 and C-3 of the compound given: <image  $_{1>}$  Option:\n\n \n(A) 2R, 3S\n\n (B) 2S, 3R\n\n (C) 2S, 3S\n\n (D) 2R, 3R \nThe image shows a Fischer projection of a chiral molecule. The structure has a central carbon atom with the following substituents:\n\n- Methyl group (CH₃) at the top\n- Ethyl group (C₂H₅) at the bottom\n- Hydrogen atom (H) on the left\n- Chlorine atom (Cl) on the right\n\nThis type of diagram is used in organic chemistry to represent the three-dimensional arrangement of substituents around a chiral center.\nGPT-4V:  To determine the configuration (R or S) of a chiral center, we follow the Cahn-Ingold- Prelog (CIP) priority rules: Assign priorities based on atomic number. The atom bonded to the chiral center with the highest atomic number gets the highest priority (1). If two atoms have the same atomic number, consider the atoms they are bonded to in order to determine priority. Orient the molecule so that the atom or group with the lowest priority (4) is pointing away (in the background). Observe the order of groups 1-2-3: If the order is clockwise, the configuration is R. If the order is counterclockwise, the configuration is S. Let‘s determine the configuration for the two chiral centers: For C-2: Attached groups and their atomic numbers: CH3 (Carbon  $\\mathrm{\\Omega}=\\mathrm{6)Cl}$   (Chlorine  $=$   $\\mathrm{17)H}$   (Hydrogen  $=1$  )C2H5 (Carbon  $=6,$  , but this will have higher priority than CH3 because the subsequent carbons in the chain will be used for breaking the tie). Priority order: ClC2H5CH3H. With the H pointing to the background, the order is 1 (Cl) to 2 (C2H5) to 3 (CH3), which is clockwise. So, the configuration at C-2 is R. For C-3: Attached groups and their atomic numbers: H (Hydrogen  $=\\mathrm{1)Cl}$   (Chlorine  $=\\,17)\\mathrm{CH}3$   (Carbon  $=6$  )C2H5 (Carbon  $=6,$  , but like before, C2H5 will have higher priority than CH3). Priority order: ClC2H5CH3H. With the H pointing to the background, the order is 1 (Cl) to 2 (C2H5) to 3 (CH3), which is counterclockwise. So, the configuration at C-3 is S. Therefore, the correct configuration for the compound is: 2R, 3S. So, the answer is (A) 2R, 3S. \nGround Truth: (A) 2R, 3S \nFigure 36. A sample correct case of Chemistry (subfield: Organic Chemistry). Back to List of Figures  |  Back to Table Index "}
{"page": 50, "image_path": "doc_images/2311.16502v3_50.jpg", "ocr_text": "eZ Chemistry: Organic Chemistry\n\nQuestion: Which of the statement best decribes the reaction in the following figure? <image 1>\nOption:\n\n(A) It is likely to be SN1.\n\n(B) It is likely to be SN2.\n\n(C) None of the above.\n\nOH HCl cl\nCH30H\n\n<image 1>\n\nGPT-4V: The reaction depicted involves the conversion of an alcohol to an alkyl chloride in the\npresence of HCl and CH3;OH (methanol). Given the reagents and conditions:(A) It is likely to be\nSN1.The alcohol initially forms a carbocation intermediate after losing the hydroxyl group in the\npresence of HCl. Then, chloride ion attacks the carbocation to form the alkyl chloride product. The\nmechanism of this reaction is consistent with the SN1 pathway.\n\nGround Truth: (A)\n\nFigure 37. A sample correct case of Chemistry (subfield: Organic Chemistry).\nBack to List of Figures | Back to Table Index\n\n51\n", "vlm_text": "The image shows a green circle with a black checkmark inside it.\nChemistry: Organic Chemistry \nQuestion:  Which of the statement best decribes the reaction in the following figure?  <image  $_{1>}$  Option:\n\n \n(A) It is likely to be S N 1.\n\n (B) It is likely to be  $S N2$  .\n\n (C) None of the above . \nThe image depicts a chemical reaction. On the left side is a benzyl alcohol structure, featuring a benzene ring with a hydroxyl (-OH) group attached. The reaction arrow shows it converting to benzyl chloride, where the hydroxyl group is replaced by a chlorine (-Cl) atom. The reaction conditions above the arrow indicate the use of hydrochloric acid (HCl) in methanol (CH₃OH) to facilitate this transformation.\nGPT-4V:  The reaction depicted involves the conversion of an alcohol to an alkyl chloride in the presence of HCl and   $\\mathrm{CH}_{3}\\mathrm{OH}$   (methanol). Given the reagents and conditions:(A) It is likely to be  $S N1$  .The alcohol initially forms a carbocation intermediate after losing the hydroxyl group in the presence of HCl. Then, chloride ion attacks the carbocation to form the alkyl chloride product. The mechanism of this reaction is consistent with the S N 1 pathway. \nGround Truth: (A) \nFigure 37. A sample correct case of Chemistry (subfield: Organic Chemistry). Back to List of Figures  Back to Table Index "}
{"page": 51, "image_path": "doc_images/2311.16502v3_51.jpg", "ocr_text": "@ Chemistry: Inorganic Chemistry\n\nQuestion: Which of the following Lewis (electron-dot) structures is/are wrong? <image 1>\n\nOption:\n(A) (a)\n(B) (a) and (d)\n(C) (a) and (b)\n(D) (d) and (e)\noe H +\n. . + 7A:\n6=c=5 H_| _-H\nra\n(a) SOz (b)NH3 | (c) NO2* (d) NHACI_|_(e) NHACI |\n<image 1>\n\nGPT-4V: Let's analyze each of the Lewis structures:(a) SO2: The structure shows sulfur with a\ndouble bond to each oxygen atom. Each oxygen atom has two lone pairs, and sulfur has no lone\npairs. This is the correct Lewis structure for SO2.(b) NH3: The structure shows nitrogen with a\nsingle bond to each of the three hydrogen atoms and one lone pair on nitrogen. This is the correct\nLewis structure for NH3.(c) NO2*: The structure shows nitrogen with a double bond to one oxygen\nand a single bond to the other oxygen. The single bonded oxygen has three lone pairs, while the\ndouble bonded oxygen has two lone pairs. This results in the nitrogen having a positive charge.\nThis is the correct Lewis structure for NO2*.(d) NH,Cl: The structure shows the ammonium ion\n(NH,\"*) with nitrogen having four single bonds to four hydrogen atoms, and there's no lone pair on\nnitrogen. However, chlorine is shown with a single bond to a hydrogen atom, which is incorrect.\nChlorine should not be bonded to hydrogen in this structure. Instead, NH,* and Cl are present as\nions, and they are attracted to each other through ionic bonding.(e) NH4ClI: This structure shows the\ncorrect ionic representation with the ammonium ion (NH,\"*) and the chloride ion (Cl) as separate\nentities. Based on the analysis, the incorrect Lewis structure is: (d) NH,Cl. The correct answer is: (D)\n(d) and (e)\n\nError Category: Perceptual Error, Reasoning Error\n\nError Reason: For choice (a), GPT-4V incorrectly recognizes the carbon atom as a sulfur atom, and\neven if it was a sulfur atom, there would be two electrons near the sulfur atom. Therefore, the Lewis\nstructure in choice (a) is completely wrong and should be one of the final answers. For choice (e),\neven though the model correctly decides that it is correct, it still chooses a wrong option that\ncontains (e).\n\nGround Truth: (B) (a) and (d)\n\nFigure 38. A sample error case of Chemistry (subfield: Inorganic Chemistry). Error category: Perceptual Error, Reasoning Error\nBack to List of Figures | Back to Table Index\n\n52\n", "vlm_text": "The image shows a red circle with a bold black outline containing a white \"X\" in the center. This symbol is often used to indicate a cancellation, error, or a \"no\" or \"incorrect\" status in various digital interfaces and contexts.\nChemistry: Inorganic Chemistry \nQuestion:  Which of the following Lewis (electron-dot) structures is/are wrong? <image  $_{1>}$  Option:\n\n \n(A) (a)\n\n (B) (a) and (d)\n\n (C) (a) and (b)\n\n (D) (d) and (e) \nThe table shows Lewis structures for different molecules and ions:\n\n(a) SO₂ (Sulfur dioxide)  \n(b) NH₃ (Ammonia)  \n(c) NO₂⁺ (Nitronium ion)  \n(d) NH₄Cl (Ammonium chloride, represented with coordinate bond)  \n(e) NH₄Cl (Ammonium chloride, represented as separate ions)  \n\nEach section illustrates the molecular structure with valence electrons and bonds.\nGPT-4V:  Let's analyze each of the Lewis structures:(a)   $\\mathrm{SO}_{2}$  : The structure shows sulfur with a  double bond to each oxygen atom. Each oxygen atom has two lone pairs, and sulfur has no lone  pairs. This is the correct Lewis structure for   $\\mathrm{SO}_{2}$  .(b)   $\\mathrm{NH}_{3}$  : The structure shows nitrogen with a single bond to each of the three hydrogen atoms and one lone pair on nitrogen. This is the correct Lewis structure for  $\\mathrm{NH}_{3}$  .(c)  $\\mathrm{NO}_{2}{}^{+}$  : The structure shows nitrogen with a double bond to one oxygen and a single bond to the other oxygen. The single bonded oxygen has three lone pairs, while the double bonded oxygen has two lone pairs. This results in the nitrogen having a positive charge. This is the correct Lewis structure for   $\\mathrm{NO}_{2}{}^{+}$  .(d)   $\\mathrm{NH_{4}C l}$  : The structure shows the ammonium ion  $\\left(\\mathrm{NH}_{4}{}^{+}\\right)$   with nitrogen having four single bonds to four hydrogen atoms, and there's no lone pair on nitrogen. However, chlorine is shown with a single bond to a hydrogen atom, which is incorrect. Chlorine should not be bonded to hydrogen in this structure. Instead,   $\\mathrm{{NH_{4}}^{+}}$   and  $C1^{-}$   are present as ions, and they are attracted to each other through ionic bonding.(e)  $\\mathrm{NH_{4}C l}$  : This structure shows the correct ionic representation with the ammonium ion   $\\left(\\mathrm{NH}_{4}{}^{+}\\right)$   and the chloride ion   $(\\mathsf{C}\\mathsf{I}^{-})$   as separate entities. Based on the analysis, the incorrect Lewis structure is: (d)  $\\mathrm{{NH}_{4}C l}$  . The correct answer is: (D)  (d) and (e) \nError Category: Perceptual Error, Reasoning Error \nError Reason:  For choice (a), GPT-4V incorrectly recognizes the carbon atom as a sulfur atom, and even if it was a sulfur atom, there would be two electrons near the sulfur atom. Therefore, the Lewis structure in choice (a) is completely wrong and should be one of the final answers. For choice (e), even though the model correctly decides that it is correct, it still chooses a wrong option that  contains (e). \nGround Truth: (B) (a) and (d) "}
{"page": 52, "image_path": "doc_images/2311.16502v3_52.jpg", "ocr_text": "@ Chemistry: Inorganic Chemistry\n\nQuestion: <image> Which point corresponds to a sublimation/ deposition equilibrium?\nOption:\n\nTemperature\n\n<image 1>\n\nGPT-4V: The graph provided is a phase diagram, which illustrates the conditions (in terms of\npressure and temperature) under which different phases of a substance coexist in equilibrium. In\nsuch a diagram: The line between the solid and gas regions represents the sublimation/ deposition\nequilibrium, where a substance can transition directly between the solid and gaseous states. The\nline between the solid and liquid regions represents the melting/ freezing equilibrium. The line\nbetween the liquid and gas regions represents the boiling/condensation equilibrium. From the\nimage provided, point (B) 3 is on the line between the solid and gas regions, representing the\nsublimation/ deposition equilibrium. So, the correct answer is: (B) 3.\n\nError Category: Lack of Knowledge\nError Reason: The model correctly describes the knowledge, but due to the lack of knowledge\n\nabout the phase diagram, it thinks point 3 is the one on the line between the solid and gas regions,\nwhile the correct one is point 8.\n\nGround Truth: (B) 3\n\nFigure 39. A sample error case of Chemistry (subfield: Inorganic Chemistry). Error category: Lack of Knowledge\nBack to List of Figures | Back to Table Index\n\n53\n", "vlm_text": "The image shows a red circle with a white \"X\" or cross inside, often used to signify a cancelation, error, or a stop symbol.\nChemistry: Inorganic Chemistry \nQuestion:  <image> Which point corresponds to a sublimation/deposition equilibrium? Option: \n(A) 1 (B) 3 (C) 5 (D) 6 (E) 8 \nThe image is a graph with a plot displaying pressure on the y-axis and temperature on the x-axis. There are two curves shown on the graph: one curve slopes downwards from left to right, and the other curve slopes upwards from left to right. The data points on this graph are labeled with numbers 1 through 9. Points 1, 3, and 5 are on or near the upper curve, while points 2, 4, and 9 are isolated from the curves. Points 6, 7, and 8 are on or near the lower curve. This type of graph is often used to show the relationship between pressure and temperature in a scientific study or analysis; it might represent a phase diagram or other thermally dependent process.\nGPT-4V:  The graph provided is a phase diagram, which illustrates the conditions (in terms of pressure and temperature) under which different phases of a substance coexist in equilibrium. In such a diagram: The line between the solid and gas regions represents the sublimation/deposition equilibrium, where a substance can transition directly between the solid and gaseous states. The line between the solid and liquid regions represents the melting/freezing equilibrium. The line between the liquid and gas regions represents the boiling/condensation equilibrium. From the image provided, point (B) 3 is on the line between the solid and gas regions, representing the  sublimation/deposition equilibrium. So, the correct answer is: (B) 3. \nError Category: Lack of Knowledge \nError Reason:  The model correctly describes the knowledge, but due to the lack of knowledge about the phase diagram, it thinks point 3 is the one on the line between the solid and gas regions,  while the correct one is point 8. \nGround Truth: (B) 3 \nFigure 39. A sample error case of Chemistry (subfield: Inorganic Chemistry). Error category: Lack of Knowledge Back to List of Figures  Back to Table Index "}
{"page": 53, "image_path": "doc_images/2311.16502v3_53.jpg", "ocr_text": "eZ Geography: Human Geography\n\nQuestion: The circled areas on <image 1> are all areas which\n\nOption:\n\n(A) are under the control of al-Qaeda from 2001 to present\n\n(B) were under the control of the U.S.-led military coalition as of 2003\n(C) are home to the Kurdish peoples\n\n(D) are production zones of proven oil reserves\n\n(E) have large-scale irrigated agriculture\n\n| Istanbul\nO° Ankara\nao Ss\nf?3\nOnn Amman\n- ¥\nCairo\n{\nf—\nKhartoum (7 :\n( Aden\n<image 1>\n\nGPT-4V: Based on the provided map, the circled areas seem to align with regions that:\n(D) are production zones of proven oil reserves.Many of the circled areas are in regions known for\ntheir significant oil reserves, such as parts of Saudi Arabia, Iraq, and other parts of the Middle East.\n\nGround Truth: (D) are production zones of proven oil reserves\n\nFigure 40. A sample correct case of Geography (subfield: Human Geography).\nBack to List of Figures | Back to Table Index\n\n54\n", "vlm_text": "Geography: Human Geography \nQuestion:  The circled areas on <image  $_{1>}$   are all areas which\n\n \nOption: \n(A) are under the control of al-Qaeda from 2001 to present\n\n (B) were under the control of the U.S.-led military coalition as of 2003\n\n (C) are home to the Kurdish peoples\n\n (D) are production zones of proven oil reserves\n\n (E) have large-scale irrigated agriculture \nThe image is a map highlighting parts of the Middle East and surrounding regions. It shows various cities marked with squares and circles, including:\n\n- Istanbul and Ankara in Turkey\n- Cairo in Egypt\n- Amman in Jordan\n- Medina, Riyadh, and Mecca in Saudi Arabia\n- Aden in Yemen\n- Other circles and squares across the region, indicating other significant locations.\n\nThe map seems to emphasize specific locations, possibly for historical, religious, or demographic reasons.\nGPT-4V:  Based on the provided map, the circled areas seem to align with regions that: (D) are production zones of proven oil reserves.Many of the circled areas are in regions known for their significant oil reserves, such as parts of Saudi Arabia, Iraq, and other parts of the Middle East. \nGround Truth: (D) are production zones of proven oil reserves "}
{"page": 54, "image_path": "doc_images/2311.16502v3_54.jpg", "ocr_text": "@ Geography: Physical Geography\n\nQuestion: Which weather does 11 represent in <image 1>?\nOption:\n\n(A) Boreal forest climate\n\n(B) Moist subtropical climate\n\n(C) Wet-dry tropical climate\n\n(D) Tundra climate\n\n<image 1>\n\nGPT-4V: The image appears to be a climate zone map. The zone labeled \"11\" corresponds to regions\nsuch as northern Canada and parts of Russia. Based on the options provided, zone \"11\" would\nrepresent:\n\n(D) Tundra climate.\n\nError Category: Reasoning Error\n\nError Reason: GPT-4V correctly summarize that the zone labeled \"11\" corresponds to regions such\nas northern Canada and parts of Russia, which is a correct interpretation of the map. However,\nGPT-4V wrongly gets the conclusion that zone “11” would be Tundra climate based on the\nobservation.\n\nGround Truth: (A) Boreal forest climate\n\nFigure 41. A sample error case of Geography (subfield: Physical Geography). Error category: Reasoning Error\nBack to List of Figures | Back to Table Index\n\n55\n", "vlm_text": "The image shows a red circle with a black border, containing a white \"X\" symbol in the center.\nGeography: Physical Geography \nQuestion:  Which weather does 11 represent in <image  $_{1>?}$  Option : \n(A) Boreal forest climate (B) Moist subtropical climate (C) Wet-dry tropical climate (D) Tundra climate \nThe image shows a climate zone map of North America. The map is divided into various regions labeled with numbers and letters, which likely correspond to different climate zones or types. These zones are indicated by different colors and patterns covering the continent. The boundaries of the zones follow geographic features like coastlines and mountain ranges, reflecting the variations in climate across regions.\nGPT-4V:  The image appears to be a climate zone map. The zone labeled  $\"11\"$   corresponds to regions such as northern Canada and parts of Russia. Based on the options provided, zone  $\"11\"$   would represent: \n(D) Tundra climate. \nError Category: Reasoning Error \nError Reason:  GPT-4V correctly summarize that the zone labeled   $\"11\"$   corresponds to regions such as northern Canada and parts of Russia, which is a correct interpretation of the map. However, GPT-4V wrongly gets the conclusion that zone   $\"11\"$   would be Tundra climate based on the observation. \nGround Truth: (A) Boreal forest climate \nFigure 41. A sample error case of Geography (subfield: Physical Geography). Error category: Reasoning Error Back to List of Figures  |  Back to Table Index "}
{"page": 55, "image_path": "doc_images/2311.16502v3_55.jpg", "ocr_text": "@ Geography: Physical Geography\n\nQuestion: What are satellite 1, 2,3 in <image 1>?\nOption:\n\n(A) Terra, Aqua, Landsat 7\n\n(B) Aqua, Terra, Landsat 7\n\n(C) Landsat 7, Aqua, Terra\n\n(D) Landsat 7, Terra, Aqua\n\n<image 1>\n\nGPT-4V: The satellites labeled in the image as:Landsat 7, Aqua, Terra\nSo, the correct option is: (C) Landsat 7, Aqua, Terra\n\nError Category: Perceptual Error, Reasoning Error\n\nError Reason: GPT-4V fails to match the cartoon image with the real satellites. Additionally,\nLandsat 7 is in a different path while Terra and Aqua are both in the 705 path. So 3 definitely\nrepresents Landsat 7. GPT-4V ignores the hint provided in the diagram. Given the fact that GPT-4V\ndoesn’t provide a detailed explanation of its choice, it might be a perceptual error because of\nwrongly matching the cartoon image with the real satellites or a reasoning error due to ignorance of\nthe path hint.\n\nGround Truth: (A) Terra, Aqua, Landsat 7\n\nFigure 42. A sample error case of Geography (subfield: Physical Geography). Error category: Perceptual Error, Reasoning Error\nBack to List of Figures | Back to Table Index\n\n56\n", "vlm_text": "The image shows a red circle with a white \"X\" symbol in the middle, outlined in black. This symbol is commonly used to represent a \"no,\" \"cancel,\" or \"close\" action in graphical interfaces.\nGeography: Physical Geography \nQuestion:  What are satellite 1, 2, 3 in <image  $1{>}?$  Option:\n\n \n(A) Terra, Aqua, Landsat 7\n\n (B)  Aqua, Terra, Landsat 7\n\n (C)  Landsat 7, Aqua, Terra\n\n (D)  Landsat 7, Terra, Aqua \nThe image illustrates various space satellites orbiting Earth, with a focus on the Ocean Surface Topography Mission (OSTM), which studies ocean dynamics, climate variability, and energy cycles. The satellites shown include Jason, OSTM/Jason 2, QuikSCAT, ACRIMSAT, EO-1, SORCE, TRMM, GRACE, ICESat, CALIPSO, and CloudSat. Each satellite appears to have its own trajectory or orbit path depicted in the image. There's also some text that is blurred or obscured.\nGPT-4V:  The satellites labeled in the image as:Landsat 7, Aqua, Terra So, the correct option is: (C) Landsat 7, Aqua, Terra \nError Category: Perceptual Error, Reasoning Error \nError Reason:  GPT-4V fails to match the cartoon image with the real satellites. Additionally, Landsat 7 is in a different path while Terra and Aqua are both in the 705 path. So 3 definitely  represents Landsat 7. GPT-4V ignores the hint provided in the diagram. Given the fact that GPT-4V doesn’t provide a detailed explanation of its choice, it might be a perceptual error because of wrongly matching the cartoon image with the real satellites or a reasoning error due to ignorance of the path hint. \nGround Truth: (A) Terra, Aqua, Landsat 7 "}
{"page": 56, "image_path": "doc_images/2311.16502v3_56.jpg", "ocr_text": "2 Math: Calculus\n\nQuestion: <image 1>The region bounded by the graph as shown above. Choose an integral\nexpression that can be used to find the area of R.\nOe\n\nA) LF) - g(x)]dx\nare Lg(x) — FdIdx\n© FUG) - g@)lax\n©) felg@) — f@)Jax\n\n(x) =x) 6x? + 8x\n\ng()=-}x? +28\n\n<image 1>\n\nGPT-4V: To determine the integral expression to find the area of R, we need to observe which curve\nis above the other in the region R. From thé given graphy for the region! R, f(s) (the! curve\n\nrepresented by f(x) = x3 — 6x? + 8x ) is above g(x) (the curve represented by g(x) = 5x + 2x).\n\nThe interval over which we're integrating is from x=0 to the point where the two curves intersect,\n\nwhich seems to be before x=2 (based on the graph). So, the upper limit of the integral is not 2. Given\nthe options, . Thus, the integral expression to find the area of R\n\nis: f° ¢@ — g(x)]dx The correct choice is: (A) f° @ — g(x)]dx\n\nGround Truth: (A) SUr@ — g(x)|dx\n\nFigure 43. A sample correct case of Math (subfield: Calculus).\nBack to List of Figures | Back to Table Index\n\n57\n", "vlm_text": "Math: Calculus \nQuestion:  <image   $1{>}$  The region bounded by the graph as shown above. Choose an integral expression that can be used to find the area of R. \nOption: \n\n$$\n\\begin{array}{r l}&{\\int_{0}^{1.5}[f(x)-g(x)]d x}\\\\ &{\\int_{0}^{1.5}[g(x)-f(x)]d x}\\\\ &{\\int_{0}^{2}[f(x)-g(x)]d x}\\\\ &{\\int_{0}^{2}[g(x)-f(x)]d x}\\end{array}\n$$\n \nThe image shows a graph of two functions, \\( f(x) = x^3 - 6x^2 + 8x \\) and \\( g(x) = -\\frac{1}{2}x^2 + 2x \\). The graph displays shaded regions labeled \\( R \\) and \\( S \\), which are areas between the two curves. The intersections of the curves are marked at points \\((2, 0)\\) and \\((4, 0)\\), and these points are labeled on the x-axis. The graph is in a coordinate plane with an origin labeled \\( O \\).\nGPT-4V:  To determine the integral expression to find the area of  $\\mathrm{R},$   we need to observe which curve is above the other in the region R. From the given graph, for the region R,   $\\mathrm{f}(\\mathrm{x})$   (the curve  represented by   $f(x)=x^{3}-6x^{2}+8x\\,)$   is above   $\\mathrm{g(x)}$   (the curve represented by  $\\textstyle g(x)={\\frac{1}{2}}x^{2}+2x)$  . The interval over which we're integrating is from  $_{x=0}$   to the point where the two curves intersect, which seems to be before  $_{x=2}$   (based on the graph). So, the upper limit of the integral is not 2.  Given  the options, the only possible upper limit is  $_{x=1.5}$  . Thus, the integral expression to find the area of  $\\mathrm{R}$  is:    $\\textstyle\\int_{0}^{1.5}[f(x)-g(x)]d x$  The correct choice is: (A)  $\\textstyle\\int_{0}^{1.5}[f(x)-g(x)]d x$  \nGround Truth: (A)    $\\begin{array}{r}{\\int_{0}^{1.5}[f(x)-g(x)]d x}\\end{array}$  "}
{"page": 57, "image_path": "doc_images/2311.16502v3_57.jpg", "ocr_text": "@ Math: Graph Theory\n\nQuestion: Consider <image 1> on the torus, with its faces labeled A through H. Give a colouring of\nthe faces of with four colours so that faces meeting along an edge have different colours. Is such\ncolouring possible with only three colours?\n\nOption:\n\n(A) Yes\n\n(B) No\n\n<image 1>\n\nGPT-4V: To solve the problem, we can start by attempting to color the diagram with three colors\nand see if it's possible to do so without having two adjacent faces of the same color. Let's label our\nthree colors as Red, Green, and Blue (for the sake of this explanation).We can begin by coloring face\nA with Red. Adjacent to A are B and C, so let's color B with Green and C with Blue. D is adjacent to\nboth B and C, so it must be Red. E is adjacent to A, D, and B. Since A and D are Red and B is Green,\nE should be Blue. F is adjacent to C, D, and E. D is Red, E is Blue, so F should be Green. G is\nadjacent to D, F, and A. D is Red, F is Green, so G should be Blue. Lastly, H is adjacent to B, E, and\nF. B is Green, E is Blue, and F is Green. So H can be colored Red. From this coloring, it seems that\nwe can color the diagram with three colors such that no two adjacent faces have the same color.\nHence, the answer is:(A) Yes.\n\nError Category: Perceptual Error\n\nError Reason: GPT-4V encounters an issue in the third step of reasoning, when it concludes, \"D is\nadjacent to both B and C, so it must be Red.\" The problem begins at this point because, according to\nthe image, D is actually adjacent to A, B, and C. The error should be identified at this step, but the\nmodel recognizes incomplete information, believing that D is only adjacent to B and C, which leads\nto a series of subsequent incorrect deductions.\n\nGround Truth: (B) No\n\nFigure 44. A sample error case of Math (subfield: Graph Theory). Error category: Perceptual Error\nBack to List of Figures | Back to Table Index\n\n58\n", "vlm_text": "The image shows a red circle with a black border, containing a white \"X\" symbol in the center. This typically represents a cancel or error symbol.\nMath: Graph Theory \nQuestion:  Consider <image  $_{1>}$   on the torus, with its faces labeled A through H. Give a colouring of the faces of with four colours so that faces meeting along an edge have different colours. Is such colouring possible with only three colours? \nOption:\n\n (A) Yes\n\n (B) No \nThis image shows a rectangular grid divided into several sections labeled with letters from A to H. The sections are separated by lines, forming different polygonal shapes. Black arrows indicate directions along some edges of the grid.\nGPT-4V:  To solve the problem, we can start by attempting to color the diagram with three colors and see if it's possible to do so without having two adjacent faces of the same color. Let's label our three colors as Red, Green, and Blue (for the sake of this explanation).We can begin by coloring face A with Red. Adjacent to A are B and  $\\mathrm{C},$   so let's color B with Green and C with Blue. D is adjacent to  both B and  $C,$   so it must be Red. E is adjacent to A, D, and B. Since A and D are Red and B is Green, E should be Blue. F is adjacent to   $\\mathrm{C},$   D, and E. D is Red, E is Blue, so F should be Green. G is adjacent to D, F, and A. D is Red, F is Green, so G should be Blue. Lastly, H is adjacent to B, E, and F. B is Green, E is Blue, and F is Green. So H can be colored Red. From this coloring, it seems that we can color the diagram with three colors such that no two adjacent faces have the same color. Hence, the answer is:(A) Yes. \nError Category: Perceptual Error \nError Reason:  GPT-4V encounters an issue in the third step of reasoning, when it concludes,   $\\mathrm{\"D}$   is  adjacent to both B and  $\\mathrm{C},$   so it must be Red.\" The problem begins at this point because, according to the image, D is actually adjacent to A, B, and C. The error should be identified at this step, but the model recognizes incomplete information, believing that D is only adjacent to B and   $C,$   which leads to a series of subsequent incorrect deductions. \nGround Truth: (B) No \nFigure 44. A sample error case of Math (subfield: Graph Theory). Error category: Perceptual Error Back to List of Figures  |  Back to Table Index "}
{"page": 58, "image_path": "doc_images/2311.16502v3_58.jpg", "ocr_text": "@ Math: Geometry\n\nQuestion: In circle O shown in the diagram below, chords A and CD are parallel <image 1> If\nm — AB = 104 and m — CD 168, what is m m—BD ?\n\nOption:\n\n(A) 38\n\n(B) 44\n(C) 88\n(D) 96\n\na\n\n<image 1>\n\nGPT-4V: Using the properties of inscribed angles and their intercepted arcs, we can deduce: The\nmeasure of an inscribed angle is half the measure of its intercepted arc.When two chords are\nparallel, the inscribed angles that intercept the same arc are congruent.From the given\ninformation:mZAOB = ¥2 m—AB = 14(104) = 52°, mZCOD = ¥% m—CD = 4¥2(168) = 84°. Since chords\nAB and CD are parallel, m2 AOB = mZCOD, but we've just shown that mZAOB is 52°, which means\nthat our diagram or understanding has an inconsistency because mZCOD is given as 84°. However,\nassuming the information provided is correct and using the fact that the sum of measures of arcs\nintercepted by two inscribed angles that sum up to 180° will be 360°, we get: m—~AB + m—BD =\n360°104 + m—BD = 360° m—BD = 256°The measure of inscribed angle BOD that intercepts arc BD\nis: mZBOD = ¥% m—BD = 42(256) = 128°But, mZBOD = 180° - mZCOD = 180° - 84° = 96°, which\ncontradicts our previous calculation.Assuming the latter to be correct (as it's based directly on the\ngiven information), m—BD = 2 * mZBOD = 2(96°) = 192°.Thus, the correct answer should be: (D)\n192 However, since 192 is not one of the options and our calculations seem to contradict the given\ndata, it's likely there's an error or oversight in the information provided.\n\nError Category: Textual Understanding Error\n\nError Reason: GPT-4V does not correctly understand the symbols and terminology in mathematics\nor geometry. In this question, it incorrectly interprets m — AB = 104 as the measure of an inscribed\nangle, when the correct interpretation should be the measure of an arc or a central angle. Therefore,\nthe angle represented in the question, ZAOB, is 104 degrees, and ZCOD is 168 degrees. Since the\nconcept is confused right from the start, GPT4-V does not derive the correct answer in the\nsubsequent calculations. The key to the question is to determine the angle of BOD, which is equal to\nthe angle AOC by using the condition of parallel lines. Then, by subtracting the two given angles\nfrom 360 degrees and dividing by two, we can obtain the correct answer.\n\nGround Truth: (B) 44\n\nFigure 45. A sample error case of Math (subfield: Geometry). Error category: Textual Understanding Error\nBack to List of Figures | Back to Table Index\n\n59\n", "vlm_text": "The image shows a red circle with a white \"X\" in the center, outlined in black. It commonly represents a \"close\" or \"cancel\" button or a symbol indicating an error or something incorrect.\nMath: Geometry \nQuestion:  In circle   $\\mathrm{o}$   shown in the diagram below, chords A and    $\\overline{{C D}}$  are parallel <image  $_{1>}$   If  $\\mathrm{m}\\frown\\mathrm{AB}=104$   and  $\\mathrm{m}\\frown\\mathrm{CD\\,168}$  , what is  $\\mathrm{m}\\,\\mathrm{m}{\\sim}\\mathrm{BD}^{\\prime}$  ? \nOption:\n\n (A) 38\n\n (B) 44\n\n (C) 88\n\n (D) 96 \nThe image shows a circle with a point labeled O at its center. Two parallel chords, AB and CD, are inside the circle.\nGPT-4V:  Using the properties of inscribed angles and their intercepted arcs, we can deduce: The  measure of an inscribed angle is half the measure of its intercepted arc.When two chords are parallel, the inscribed angles that intercept the same arc are congruent.From the given information:  $\\mathrm{m\\angleAOD=1/2m\\frownAB=1/2(104)=52^{\\circ},}$  ,  ${\\mathrm{m}}\\angle{\\mathrm{COD}}=1/2{\\mathrm{~m}}\\frown{\\mathrm{CD}}=1/2(168)=84^{\\circ}$  . Since chords AB and CD are parallel,  $\\mathsf{m}\\angle\\mathrm{AOB}=\\mathsf{m}\\angle\\mathrm{COD},$  , but we've just shown that m ∠ AOB is  $52^{\\circ}$  , which means that our diagram or understanding has an inconsistency because m ∠ COD is given as  $84^{\\circ}$  .However, assuming the information provided is correct and using the fact that the sum of measures of arcs intercepted by two inscribed angles that sum up to  $180^{\\circ}$   will be   $360^{\\circ}$  , we get:   $\\mathrm{m{\\frown}A B+m{\\frown}B D=}$   $360^{\\circ}104+\\mathrm{m}{\\sim}\\mathrm{BD}=360^{\\circ}\\ \\mathrm{m}{\\sim}\\mathrm{BD}=256^{\\circ}\\mathrm{Th}$  he measure of inscribed angle BOD that intercepts arc BD is:  $\\mathrm{m\\angleBOD\\=\\%~m\\frown B D\\=\\%(256)\\=128^{\\circ}B u t,}$   $\\mathrm{m}\\angle\\mathrm{BOD}=180^{\\circ}\\textrm{-}\\mathrm{m}\\angle\\mathrm{COD}=180^{\\circ}\\textrm{-}84^{\\circ}=96^{\\circ},$  , which contradicts our previous calculation.Assuming the latter to be correct (as it's based directly on the given information),  $\\mathrm{m}{\\sim}\\mathrm{BD}=2^{\\mathrm{~*~}}\\mathrm{m}{\\angle}\\mathrm{BD}=2(96^{\\circ})=192^{\\circ}$  .Thus, the correct answer should be: (D) 192 However, since 192 is not one of the options and our calculations seem to contradict the given data, it's likely there's an error or oversight in the information provided. \nError Category: Textual Understanding Error \nError Reason:  GPT-4V does not correctly understand the symbols and terminology in mathematics or geometry. In this question, it incorrectly interprets  $\\mathrm{m}\\frown\\mathrm{AB}=104$   as the measure of an inscribed angle, when the correct interpretation should be the measure of an arc or a central angle. Therefore, the angle represented in the question,    $\\angle\\mathrm{AB},$  , is 104 degrees, and  $\\angle{\\mathrm{COD}}$   is 168 degrees. Since the concept is confused right from the start, GPT4-V does not derive the correct answer in the subsequent calculations. The key to the question is to determine the angle of BOD, which is equal to the angle AOC by using the condition of parallel lines. Then, by subtracting the two given angles from 360 degrees and dividing by two, we can obtain the correct answer. \nGround Truth: (B) 44 \nFigure 45. A sample error case of Math (subfield: Geometry). Error category: Textual Understanding Error Back to List of Figures  Back to Table Index "}
{"page": 59, "image_path": "doc_images/2311.16502v3_59.jpg", "ocr_text": "@ Math: Calculus\n\nQuestion: A thin-walled cone-shaped cup (Fig. 16-4) is to hold 36 in? of water when full. What\ndimensions will minimize the amount of material needed for the cup?\n\nOption:\n\n(A) r =3V2,h=6\n\n(B) r =3V3,h=4\n\n(C)r=3V2,h=4\n\n(D)r =2V6,h=6\n\n‘s\n\nFig. 16-4\n\n<image 1>\n\nGPT-4V: Given that the volume V of the cone-shaped cup is 36 7 in? when it's full, the formula for\nthe volume of a cone is: 1\n\nA ==nr7h. From the given volume, we can set up the equation: zureh = 36m. From which, r2h =\n108. (1) h= =. Next, the surface area S of the cone (which does not include the base since it's a\ncup) is given by: S = mrl. Where 1 is the slant height of the cone.\n\nUsing the Pythagoras theorem for the right triangle formed by the radius, height, and slant height\nin the cone, we get: 1? = r?+ h?, From which, | = Vr?+ h?. Now, substituting the value of h from\nequation (1) into the equation for l, we get: | = /r2+108?/r*. Substituting this 1 value in S, we get:\nS = mr r?+1087/r* . To minimize the surface area, we differentiate S with respect to r and set it\nequal to zero.\n\nThe differentiation and solving for r would be complex. Instead, we can substitute the given r and h\nvalues from options (A) to (D) into the volume equation to see which one satisfies V = 367. The\noption that satisfies this and also minimizes S would be our answer.\n\n(A) For r = 3V2 andh=6:V = +n(3V2) (6) = 367\n\n(B) For r = 3V3 andh=4:V = = n(3V3) (4) + 367\n\n(© Forr = 3y2 and h = 4:V = =n(3V2) (4) # 360\n\n(D) For r = 2V3 andh = 6:V = 5(2V3) (6) = 360\n\nFrom options (A) and (D) which satisfy the volume, the surface area for each needs to be calculated:\n(A) 5 = n(3V2)_|{(3V2)2+62. (D) S = 2(2V3),|(2V3) +62.\n\nComparing the two surface areas, we can determine which one is minimal.\n\nHowever, without doing the calculations, it's hard to definitively say which one is the minimal\n\nsurface area. For practical purposes, the answer could be either (A) or (D) based on the volume, and\none would need to compute the surface areas to confirm.\n\nError Category: Reasoning Error\nError Reason: The volume formula seems to be correct. The errors are made during the symbol\ncomputation.\n\nGround Truth: (A) r = 3V2,h = 6\n\nFigure 46. A sample error case of Math (subfield: Calculus). Error category: Reasoning Error\nBack to List of Figures | Back to Table Index\n\n60\n", "vlm_text": "The image shows a red circle with a black border containing a white \"X\" or cross symbol in the center. This symbol is often used to indicate a cancellation, error, denial, or a stop action in various interfaces or contexts.\nMath: Calculus \nQuestion:  A thin-walled cone-shaped cup (Fig. 16-4) is to hold  $36\\ \\pi\\ i n^{3}$    of water when full. What dimensions will minimize the amount of material needed for the cup?\n\n \nOption: \n(A)  $r=3\\sqrt{2}$   ,  ℎ= 6\n\n (B)  𝑟= 3 3  ,  ℎ= 4\n\n (C)  𝑟= 3 2  ,  ℎ= 4\n\n (D)  𝑟= 2 6  ,  ℎ= 6 \nThe image shows a diagram of a cone. It includes labels for the radius (r), height (h), and slant height (s). It is marked as \"Fig. 16-4.\"\nGPT-4V:  Given that the volume   $\\mathrm{V}$   of the cone-shaped cup is  $36\\;\\pi\\;i n^{3}$    when it's full, the formula for the volume of a cone is: \n $\\textstyle A={\\frac{1}{3}}\\pi r^{2}h$  . From the given volume, we can set up the equation:  $\\textstyle{\\frac{1}{3}}\\pi r^{2}h=36\\pi$  . From which,    $r^{2}h=$  108 . (1)  $h={\\frac{108}{r^{2}}}$   . Next, the surface area S of the cone (which does not include the base since it's a cup) is given by:  $S=\\pi r l$  . Where l is the slant height of the cone. \nUsing the Pythagoras theorem for the right triangle formed by the radius, height, and slant height in the cone, we get:    $l^{2}=r^{2}+\\,h^{2}$  . From which,    $l\\stackrel{\\_}{=}\\sqrt{r^{2}\\!+h^{2}}$  . Now, substituting the value of h from equation (1) into the equation for  $l,$   we get:    $l=\\sqrt{r^{2}\\!\\!+\\!108^{2}/r^{4}}$  . Substituting this l value in   $S,$   we get:  $S=\\pi r\\sqrt{r^{2}\\!\\!+\\!\\!108^{2}/r^{4}}$  . To minimize the surface area, we differentiate S with respect to r and set it equal to zero. \nThe differentiation and solving for r would be complex. Instead, we can substitute the given r and h values from options (A) to (D) into the volume equation to see which one satisfies    $V=36\\pi$    . The option that satisfies this and also minimizes S would be our answer.\n\n \n(A) For  $r=3\\sqrt{2}$   and  $h=6$  :  $V={\\textstyle{\\frac{1}{3}}}\\pi{\\bigl(}3{\\sqrt{2}}{\\bigr)}^{2}(6)=36\\pi$  !\n\n (B) For  $r=3\\sqrt{3}$   and  ℎ= 4  $\\L_{:}V=\\frac{\\dot{1}}{\\L_{2}}\\pi\\big(3\\sqrt{3}\\big)^{2}(4)\\neq36\\pi$  !\n\n (C) For  $r=3\\sqrt{2}$   and  $h=4;V={\\frac{\\dot{1}}{3}}\\pi{\\dot{\\bigl(}3{\\sqrt{2}}{\\bigr)}}^{2}(4)\\neq36\\pi$  !\n\n (D) For  $r=2\\sqrt{3}$   and  $h=6$   $:V=\\textstyle{\\breve{\\frac{1}{3}}}\\pi{\\bigl(}2{\\sqrt{3}}{\\bigr)}^{2}(6)=36\\pi$  \nFrom options (A) and   $\\mathrm{(D)}$   which satisfy the volume, the surface area for each needs to be calculated: (A)  $S=\\pi(3\\sqrt{2})\\,\\sqrt{(3\\sqrt{2})^{2}\\!+\\!6^{2}}.\\;(\\mathrm{D})\\;S=\\pi\\big(2\\sqrt{3}\\big)\\sqrt{\\big(2\\sqrt{3}\\big)^{2}+6^{2}}$  . Comparing the two surface areas, we can determine which one is minimal. \nHowever, without doing the calculations, it's hard to definitively say which one is the minimal  surface area. For practical purposes, the answer could be either (A) or (D) based on the volume, and one would need to compute the surface areas to confirm. \nError Category: Reasoning Error \nError Reason:  The volume formula seems to be correct. The errors are made during the symbol computation. \nGround Truth: (A)  $\\pmb{r}=\\mathbf{3}\\sqrt{2}\\mathbf{\\Theta},\\pmb{h}=\\mathbf{6}$  \nFigure 46. A sample error case of Math (subfield: Calculus). Error category: Reasoning Error Back to List of Figures  |  Back to Table Index "}
{"page": 60, "image_path": "doc_images/2311.16502v3_60.jpg", "ocr_text": "~~) Physics: Classical Mechanics\n\nQuestion: Two coins rotate on a turntable. Coin B is twice as far from the axis as coin A.<image 1>\nOption:\n\n(A) The speed of A is twice that of B.\n\n(B) The speed of A equals that of B.\n\n(C) The speed of A is half that of B.\n\nSh\n\n<image 1>\n\nGPT-4V: For objects rotating in a circle, the linear speed (v) is related to the angular speed (a) by:\nv=or. Where: v is the linear speed of the object, @ is the angular speed of the turntable (same for all\nobjects on the turntable), and r is the distance of the object from the axis of rotation. Given that coin\nB is twice as far from the axis as coin A, let's denote the distance of coin A from the axis as r and the\ndistance of coin B from the axis as 2r.The speed of coin A is: vA=or. The speed of coin B is:\nvB=@(2r)=2r. Comparing the two speeds: vB=2vA. Thus, the speed of A is half that of B. The\ncorrect answer is:(C) The speed of A is half that of B.\n\nGround Truth: (C) The speed of A is half that of B.\n\nFigure 47. A sample correct case of Physics (subfield: Classical Mechanics).\nBack to List of Figures | Back to Table Index\n\n61\n", "vlm_text": "Physics: Classical Mechanics \nQuestion:  Two coins rotate on a turntable. Coin B is twice as far from the axis as coin A.<image  $_{1>}$  Option:\n\n \n(A) The speed of A is twice that of B.\n\n (B) The speed of A equals that of B.\n\n (C) The speed of A is half that of B. \nThe image depicts a disk with a vertical rod through its center. There are two marked points, A and B, on the disk. Green arrows originate from these points, possibly indicating movement or direction, and suggest rotational motion of the disk. There is also a gray arrow below the disk, indicating rotation direction.\nGPT-4V:  For objects rotating in a circle, the linear speed (v) is related to the angular speed (ω) by:  $\\scriptstyle{\\mathrm{v=cur}}$  . Where: v is the linear speed of the object, ω is the angular speed of the turntable (same for all objects on the turntable), and r is the distance of the object from the axis of rotation. Given that coin B is twice as far from the axis as coin A, let's denote the distance of coin A from the axis as r and the distance of coin   $\\mathtt{B}$   from the axis as 2r.The speed of coin A is:   $\\scriptstyle\\mathrm{v}\\mathbf{A}=\\omega\\mathbf{r}$  . The speed of coin B is:  $\\scriptstyle\\mathrm{vB=}\\omega(2\\mathrm{r})=2\\omega\\mathrm{r}$  . Comparing the two speeds:  $\\mathrm{vB{=}2\\mathrm{vA}}$  . Thus, the speed of A is half that of B. The correct answer is:(C) The speed of A is half that of B. \nGround Truth: (C) The speed of A is half that of B. \nFigure 47. A sample correct case of Physics (subfield: Classical Mechanics). Back to List of Figures  |  Back to Table Index "}
{"page": 61, "image_path": "doc_images/2311.16502v3_61.jpg", "ocr_text": "@ Physics: Optics\n\nQuestion: A narrow beam of monochromatic light enters a lens parallel to the optic axis, as shown\nin the accompanying diagram. Which arrow best represents the direction of the light after leaving\nthe lens?\n\nOption:\n\n(A) arrow A\n\n(B) arrow B\n(C) arrow C\n(D) arrow D\n(E) arrow E\n\n\\\n\n| | pe\n<image 1>\n\nGPT-4V: The given diagram depicts a converging lens (a double convex lens) and a parallel beam\nof monochromatic light entering it. For a converging lens, parallel rays of light that are parallel to\nthe optic axis will be refracted in such a way that they will converge or meet at the focal point on\nthe other side of the lens. In the provided diagram, the ray is converging towards a point after\npassing through the lens. Among the given options:\n\n(A) arrow A\n\nB) arrow B\n\nThe direction that best represents where the light will converge after leaving the lens is (C) arrow C.\n\nError Category: Perceptual Error\n\nError Reason: GPT-4Vcorrectly identifies the theory behind the reflection of a converging lens.\nHowever, when choosing the correct arrows in the diagram, it chooses arrow C instead of arrow E\nthat converging to the focal point. This is due the perceptual error of GPT-4V on the direction of\narrows, demonstrating the lacking ability in the sense of direction in the image.\n\nGround Truth: (E) arrow E\nExplanation: A horizontal beam approaching a converging lens bends and converges through the\nfocal point\n\nFigure 48. A sample error case of Physics (subfield: Optics). Error category: Perceptual Error\nBack to List of Figures | Back to Table Index\n\n62\n", "vlm_text": "The image shows a red circle with a white \"X\" in the center. This symbol is often used to indicate a cancellation, error, or the action of closing something.\nPhysics: Optics \nQuestion:  A narrow beam of monochromatic light enters a lens parallel to the optic axis, as shown  in the accompanying diagram. Which arrow best represents the direction of the light after leaving  the lens? \nOption:\n\n (A) arrow A\n\n (B) arrow B\n\n (C) arrow C\n\n (D) arrow D\n\n (E) arrow E \nThis image illustrates the behavior of light rays passing through a convex lens. The image depicts several arrows representing light rays approaching and moving away from the lens. These rays interact with the lens differently, as indicated by their paths post-refraction. Point \"f\" marks the focal points on either side of the lens, where parallel rays converge. Here's a brief description of the labeled rays:\n\n- Ray A: Seems to diverge as if it comes from the focal point after passing through the lens.\n- Ray B: Appears to pass through the center of the lens and continues in a straight path.\n- Ray C: Travels parallel to the principal axis and refracts through the lens.\n- Ray D: Passes through the focal point before hitting the lens and emerges parallel to the principal axis.\n- Ray E: Is angled to indicate diverging after passing through the lens, similar to A.\n\nThis diagram is often used in physics to demonstrate principles of optics, particularly how a convex lens focuses light.\nGPT-4V : The given diagram depicts a converging lens (a double convex lens) and a parallel beam  of monochromatic light entering it. For a converging lens, parallel rays of light that are parallel to  the optic axis will be refracted in such a way that they will converge or meet at the focal point on  the other side of the lens. In the provided diagram, the ray is converging towards a point after  passing through the lens. Among the given options:\n\n \n(A) arrow A\n\n (B) arrow B\n\n (C) arrow C\n\n (D) arrow D\n\n (E) arrow E \nThe direction that best represents where the light will converge after leaving the lens is (C) arrow C. \nError Category: Perceptual Error \nError Reason:  GPT-4 V correctly identifies the theory behind the reflection of a converging lens. However, when choosing the correct arrows in the diagram, it chooses arrow C instead of arrow E that converging to the focal point. This is due the perceptual error of GPT-4V on the direction of arrows, demonstrating the lacking ability in the sense of direction in the image. \nGround Truth: (E) arrow E \nExplanation:  A horizontal beam approaching a converging lens bends and converges through the  focal point \nFigure 48. A sample error case of Physics (subfield: Optics). Error category: Perceptual Error Back to List of Figures  Back to Table Index "}
{"page": 62, "image_path": "doc_images/2311.16502v3_62.jpg", "ocr_text": "eZ Basic Medical Science: Pathophysiology\n\nQuestion: At which level, in the photograph, do the primary vestibular axons enter the brain stem?\n<image 1>\n\nOption:\n\n(A) The caudal medulla.\n\n(B) The pontomedullary junction.\n\n(C) The mid-pons.\n\n(D) Near the mammillary bodies in the interpeduncular fossa.\n\n<image 1>\n\nGPT-4V: The image you provided appears to be of the brainstem and related structures. Based on\nyour question, you are referring to the point where the primary vestibular axons enter the\nbrainstem. The primary vestibular axons enter the brainstem at the level of the pontomedullary\njunction. Therefore, the correct answer is: (B) The pontomedullary junction.\n\nGround Truth: (B) The pontomedullary junction.\n\nFigure 49. A sample correct case of Basic Medical Science (subfield: Pathophysiology).\nBack to List of Figures | Back to Table Index\n\n63\n", "vlm_text": "Basic Medical Science: Path o physiology \nQuestion:  At which level, in the photograph, do the primary vestibular axons enter the brain stem? <image  $_{1>}$  \nOption:\n\n (A) The caudal medulla.\n\n (B) The pon to medullary junction.\n\n (C) The mid-pons.\n\n (D) Near the mammillary bodies in the inter p edu ncula r fossa. \nThe image shows a dorsal view of a dissected sheep brain.\nGPT-4V:  The image you provided appears to be of the brainstem and related structures. Based on your question, you are referring to the point where the primary vestibular axons enter the brainstem. The primary vestibular axons enter the brainstem at the level of the pon to medullary junction. Therefore, the correct answer is: (B) The pon to medullary junction. \nGround Truth: (B) The pon to medullary junction. "}
{"page": 63, "image_path": "doc_images/2311.16502v3_63.jpg", "ocr_text": "@ Basic Medical Science: Cardiovascular Physiology\n\nQuestion: <image 1> The diagram shows the front view of a human heart. Letters P, Q, R and S\nindicate the different chambers of the heart. Which of the following shows the sequence in which a\nblood cell returning from other parts of the body passes through the four chambers of the heart?\nOption:\n\n(A) R-S-P-Q\n\n(B) Q-S-R-P\n\n(C) P-R-Q-S\n\n(D) S-R-P-Q\n\nbows\n\n<image 1>\n\nGPT-4V: The sequence in which a blood cell returning from other parts of the body passes through\nthe four chambers of the heart is : Right atrium, Right ventricle, Left atrium, Left ventricle. Looking\nat the given options, the correct sequence is:(D) S-R-P-Q\n\nError Category: Perceptual Error\n\nError Reason: The model has the correct knowledge, but it failed to correctly understand the image\nand map the letters to the right structures (e.g., P is right atrium so it should be the first in the\nsequence).\n\nGround Truth: (C) P-R-Q-S\n\nFigure 50. A sample error case of Basic Medical Science (subfield: Cardiovascular Physiology). Error category: Perceptual Error\nBack to List of Figures | Back to Table Index\n\n64\n", "vlm_text": "Basic Medical Science: Cardiovascular Physiology \nQuestion:  <image  $_{1>}$   The diagram shows the front view of a human heart. Letters P, Q, R and S indicate the different chambers of the heart. Which of the following shows the sequence in which a blood cell returning from other parts of the body passes through the four chambers of the heart? \nOption: (A) R-S-P-Q (B) Q-S-R-P (C) P-R-Q-S (D) S-R-P-Q \nThe image is a diagram of the human heart. It is labeled with the letters P, Q, R, and S, indicating different parts of the heart anatomy. While the letters correspond to specific parts, I can't specify what each letter refers to without additional context or a key. Typically, heart diagrams label parts such as the atria, ventricles, and major vessels like the aorta and pulmonary artery.\nGPT-4V:  The sequence in which a blood cell returning from other parts of the body passes through the four chambers of the heart is  $:$   Right atrium, Right ventricle, Left atrium, Left ventricle. Looking at the given options, the correct sequence is:(D) S-R-P-Q \nError Category: Perceptual Error \nError Reason:  The model has the correct knowledge, but it failed to correctly understand the image and map the letters to the right structures (e.g., P is right atrium so it should be the first in the sequence). \nGround Truth: (C) P-R-Q-S \nFigure 50. A sample error case of Basic Medical Science (subfield: Cardiovascular Physiology). Error category: Perceptual Error Back to List of Figures  Back to Table Index "}
{"page": 64, "image_path": "doc_images/2311.16502v3_64.jpg", "ocr_text": "2 Clinical Medicine: Pathology\n\nQuestion: The patient is a post-mature newborn. What is the most likely diagnosis? <image 1>\nOption:\n\n(A) Transient tachypnea of the newborn\n\n(B) Congenital lobar emphysema\n\n(C) Hyaline membrane disease\n\n(D) Meconium aspiration syndrome\n\nL\n\n<image 1>\n\nGPT-4V: Based on the image provided, which appears to be a chest X-ray, and the information that\nthe patient is a post-mature newborn, the most suggestive diagnosis is: (D) Meconium aspiration\nsyndrome. Post-mature newborns are at an increased risk of passing meconium (the baby's first\nstool) into the amniotic fluid before birth or during labor. If the baby breathes the meconium-\nstained amniotic fluid into their lungs, it can cause a blockage of the airways, leading to meconium\naspiration syndrome. The chest X-ray of a newborn with meconium aspiration syndrome might\nshow patchy or streaky areas on the lungs. However, it's important to consult with a pediatric\nradiologist or neonatologist for a definitive diagnosis based on clinical findings and the entire\nclinical picture.\n\nGround Truth: (D) Meconium aspiration syndrome\n\nFigure 51. A sample correct case of Clinical Medicine (subfield: Pathology).\nBack to List of Figures | Back to Table Index\n\n65\n", "vlm_text": "Clinical Medicine: Pathology  \nQuestion:  The patient is a post-mature newborn. What is the most likely diagnosis? <image  $_{1>}$  Option:\n\n \n(A) Transient tachypnea of the newborn\n\n (B) Congenital lobar emphysema\n\n (C) Hyaline membrane disease\n\n (D) Meconium aspiration syndrome \nThe image is a chest X-ray showing the ribs and spine. It likely shows the thoracic region of a person, which includes the lungs and possibly the heart area. If you need more detailed information or have specific questions about the X-ray, consulting a medical professional would be advisable.\nGPT-4V:  Based on the image provided, which appears to be a chest X-ray, and the information that the patient is a post-mature newborn, the most suggestive diagnosis is: (D) Meconium aspiration syndrome. Post-mature newborns are at an increased risk of passing meconium (the baby's first stool) into the amniotic fluid before birth or during labor. If the baby breathes the meconium- stained amniotic fluid into their lungs, it can cause a blockage of the airways, leading to meconium aspiration syndrome. The chest   $\\boldsymbol{\\mathrm{X}}$  -ray of a newborn with meconium aspiration syndrome might show patchy or streaky areas on the lungs. However, it's important to consult with a pediatric radiologist or neon a to logi st for a definitive diagnosis based on clinical findings and the entire clinical picture. \nGround Truth: (D) Meconium aspiration syndrome "}
{"page": 65, "image_path": "doc_images/2311.16502v3_65.jpg", "ocr_text": "eZ Clinical Medicine: Pathology\n\nQuestion: A 19-year-old man goes to his physician for a routine check-up. The physical\nexamination is normal except that his stool is positive for occult blood. A colonoscopy is performed\nand there are over 100 lesions of the colonic mucosa and biopsy of one shows findings seen here\nmicroscopically <image 1>. Which of the following is the most likely diagnosis?\n\nOption:\n\n(A) Hereditary non-polyposis colon carcinoma\n\n(B) Hyperplastic polyps\n\n(C) Ulcerative colitis\n\n(D) Peutz-Jehgers syndrome\n\n(E) Familial polyposis coli\n\n<image 1>\n\nGPT-4V: The image provided appears to be a histological section of colonic mucosa. Given the\nclinical scenario of a 19-year-old man with over 100 lesions of the colonic mucosa and the\nhistological findings provided, the most likely diagnosis is: (E) Familial polyposis coli. Familial\nadenomatous polyposis (FAP), also known as familial polyposis coli, is an autosomal dominant\ninherited condition characterized by the development of numerous adenomatous polyps in the\ncolon and rectum. If left untreated, there's a nearly 100% chance of developing colon cancer by\nmiddle age.\n\nGround Truth: (E) Familial polyposis coli\n\nExplanation: A. HNPCC has fewer polyps and the age of presentation is older. Mismatch repair\ngenes are involved. B. Hyperplastic polyps are small and few. They are just focal exaggerations of\nthe colonic mucosa. C. Ulcerative colitis leads to mucosal ulceration with residual islands of mucosa\ncalled “pseudopolyps”. D. Peutz-Jehgers syndrome has the appearance of hamartomatous polyps in\nthe bowel, and they are not so numerous. E. The small polypoid circumscribed tubular adenoma on\na stalk seen here is benign, but persons with germline inheritance of the APC gene are at risk for\ndevelopment of colonic adenocarcinoma by their teens or 20's.\n\nFigure 52. A sample correct case of Clinical Medicine (subfield: Pathology).\nBack to List of Figures | Back to Table Index\n\n66\n", "vlm_text": "Clinical Medicine: Pathology \nQuestion:  A 19-year-old man goes to his physician for a routine check-up. The physical examination is normal except that his stool is positive for occult blood. A colon os copy is performed and there are over 100 lesions of the colonic mucosa and biopsy of one shows findings seen here microscopically <image 1>. Which of the following is the most likely diagnosis?\n\n \nOption: \n(A) Hereditary non-polyposis colon carcinoma\n\n (B) Hyper plastic polyps\n\n (C) Ulcerative colitis\n\n (D) Peutz-Jehgers syndrome\n\n (E) Familial polyposis coli \nThis image shows a histological section of tissue, stained using a technique commonly used in biology and medicine to examine the microscopic structure of tissue samples. The staining appears to be a hematoxylin and eosin (H&E) stain, which is the most widely used stain in medical diagnosis. \n\nThe image likely represents a section of a biological tissue, possibly from the gastrointestinal tract due to the presence of glandular structures and the specific architectural pattern visible, which resembles that of the intestinal lining. The pink/purple coloration indicates the presence of different cell types and structural features typical of a tissue sample, which have absorbed the stain to different extents, highlighting cellular and extracellular components.\nGPT-4V:  The image provided appears to be a his to logical section of colonic mucosa. Given the clinical scenario of a 19-year-old man with over 100 lesions of the colonic mucosa and the his to logical findings provided, the most likely diagnosis is: (E) Familial polyposis coli. Familial adenoma to us polyposis (FAP), also known as familial polyposis coli, is an autosomal dominant inherited condition characterized by the development of numerous adenoma to us polyps in the colon and rectum. If left untreated, there's a nearly   $100\\%$   chance of developing colon cancer by middle age. \nGround Truth: (E) Familial polyposis coli \nExplanation:  A. HNPCC has fewer polyps and the age of presentation is older. Mismatch repair genes are involved. B. Hyper plastic polyps are small and few. They are just focal exaggerations of the colonic mucosa. C. Ulcerative colitis leads to mucosal ulceration with residual islands of mucosa called “pseudo polyps”. D. Peutz-Jehgers syndrome has the appearance of h amar tomato us polyps in the bowel, and they are not so numerous. E. The small polypoid circumscribed tubular adenoma on a stalk seen here is benign, but persons with germline inheritance of the APC gene are at risk for development of colonic a de no carcinoma by their teens or  $20\\,\\mathrm{s}$  . "}
{"page": 66, "image_path": "doc_images/2311.16502v3_66.jpg", "ocr_text": "2 Clinical Medicine: Pathology\n\nQuestion: <image 1> This 79-year-old woman was driving her car when she had the sudden onset\nof a severe headache. She pulled in to a service station and stopped the car. Then she slumped over\nthe wheel. She was taken to the emergency room, where she remained comatose and died hours\nlater. This gross appearance is found at autopsy. Which of the following is the most likely\ndiagnosis?\n\nOption:\n\n(A) Subdural hematoma\n\n(B) Thromboembolization with cerebral infarction\n\n(C) Multiple sclerosis\n(D) Ruptured berry aneurysm\n(\n\n)\nE) Huntington disease\n\n<image 1>\n\nGPT-4V: The image appears to show a brain with blood prominently spread around it. Given the\nclinical scenario provided - a sudden severe headache followed by rapid unconsciousness and\ndeath - the most consistent diagnosis with the presentation and gross appearance at autopsy i\n\nThe hemorrhage can be seen as the spread of blood over the surface of\nthe brain in the image. The other options do not fit as closely with the clinical scenario and gross\nappearance.\n\nGround Truth: (D) Ruptured berry aneurysm\n\nFigure 53. A sample correct case of Clinical Medicine (subfield: Pathology).\nBack to List of Figures | Back to Table Index\n\n67\n", "vlm_text": "Clinical Medicine: Pathology \nQuestion:  <image  $_{1>}$   This 79-year-old woman was driving her car when she had the sudden onset of a severe headache. She pulled in to a service station and stopped the car. Then she slumped over the wheel. She was taken to the emergency room, where she remained comatose and died hours later. This gross appearance is found at autopsy. Which of the following is the most likely diagnosis?\n\n \nOption: \n(A) Subdural hematoma\n\n (B) Thr ombo embo liz ation with cerebral infarction\n\n (C) Multiple sclerosis\n\n (D) Ruptured berry aneurysm\n\n (E) Huntington disease \nThe image shows a detailed view of the human brain, likely from an anatomical or medical perspective.\nGPT-4V:  The image appears to show a brain with blood prominently spread around it. Given the clinical scenario provided - a sudden severe headache followed by rapid unconsciousness and death - the most consistent diagnosis with the presentation and gross appearance at autopsy is: (D) Ruptured berry aneurysm.  A ruptured berry aneurysm (also called a saccular aneurysm) can cause  a sub ara ch noid hemorrhage, which would present with sudden severe headache and can rapidly  lead to death if not treated.  The hemorrhage can be seen as the spread of blood over the surface of the brain in the image. The other options do not fit as closely with the clinical scenario and gross appearance. \nGround Truth: (D) Ruptured berry aneurysm \nFigure 53. A sample correct case of Clinical Medicine (subfield: Pathology). Back to List of Figures  Back to Table Index "}
{"page": 67, "image_path": "doc_images/2311.16502v3_67.jpg", "ocr_text": "@ Clinical Medicine: Pathology\n\nQuestion: A 41-year-old woman has noted a vaginal discharge persisting for 3 weeks. On pelvic\nexamination her ectocervix is erythematous. Cervical biopsies are taken and on microscopic\nexamination reveal the appearance seen here <image 1>. Which of the following is the most likely\ndiagnosis?\n\nOption:\n\n(A) Human papillomavirus infection\n\n(B) Ingestion of chili peppers\n\n(C) Diabetes mellitus\n\n(D) Tuberculosis\n\n(E) Chronic inflammation\n\n<image 1>\n\nGPT-4V: The image provided appears to be a microscopic view of cervical tissue with a\ncharacteristic appearance. The histopathological features seen here are suggestive of koilocytes,\nwhich are squamous epithelial cells with a perinuclear clearing (halo) and irregular, raisin-like\nnuclei. Koilocytes are a hallmark of human papillomavirus (HPV) infection. Based on the provided\nimage and clinical information, the most likely diagnosis is: (A) Human papillomavirus infection.\n\nError Category: Perceptual Error\n\nError Reason: GPT-4V incorrectly interprets the details of the image. The image contains “the\ninflammatory cell population of mononuclear cells, mainly small round lymphocytes” and\n“squamous metaplasia”, while GPT-4V identifies them as “koilocytes”.\n\nGround Truth: (E) Chronic inflammation\n\nExplanation: A. HPV produces viral cytopathic effect such as koilocytotic change and atypical\nchanges in the squamous epithelial cells. It can drive the process to dysplasia and to carcinoma.\nHPV by itself does not produce significant inflammation. B. The capsaicin in chili tends to irritate\nnerve endings to produce the hot burning sensation, as a local effect in oral cavity. It is not systemic.\nCapsaicin has been used to treat dyspareunia with vulvar vestibulitis, probably by affecting nerve-\nending mediated sensation, and there is no significant degree of inflammation. C. Diabetes mellitus\nmay increase the risk for infections, but by itself does not explain the inflammation seen here. D.\nTuberculosis of the genital tract is rare; mycobacterial infections tend to produce granulomatous\ninflammation. E. This is a chronic inflammatory process, based upon the inflammatory cell\npopulation of mononuclear cells, mainly small round lymphocytes. There is also some hemorrhage.\nThe squamous epithelium is normal, though if this were endocervix, it would represent squamous\nmetaplasia.\n\nFigure 54. A sample error case of Clinical Medicine (subfield: Pathology). Error category: Perceptual Error\nBack to List of Figures | Back to Table Index\n\n68\n", "vlm_text": "The image shows a red circle with a black border, featuring a white \"X\" in the center. This symbol is commonly used to indicate an error, deletion, or a negative response.\nClinical Medicine: Pathology \nQuestion:  A 41-year-old woman has noted a vaginal discharge persisting for 3 weeks. On pelvic examination her ectocervix is e ry the mato us. Cervical biopsies are taken and on microscopic examination reveal the appearance seen here <image  $_{1>}$  . Which of the following is the most likely diagnosis?\n\n \nOption: \n(A) Human papillomavirus infection\n\n (B) Ingestion of chili peppers\n\n (C) Diabetes mellitus\n\n (D) Tuberculosis\n\n (E) Chronic inflammation \nThis image appears to be a histological slide, possibly showing a section of tissue under a microscope. The staining patterns suggest it might be used for examining cellular structures and tissue morphology, likely in a medical or biological context. The image shows layers of cells with varying densities and appearances, which could be indicative of different types of tissue or a pathological condition.\nGPT-4V:  The image provided appears to be a microscopic view of cervical tissue with a characteristic appearance. The his to pathological features seen here are suggestive of koilocytes, which are squamous epithelial cells with a peri nuclear clearing (halo) and irregular, raisin-like nuclei. Koilocytes are a hallmark of human papillomavirus (HPV) infection. Based on the provided image and clinical information, the most likely diagnosis is: (A) Human papillomavirus infection. \nError Category: Perceptual Error \nError Reason:  GPT-4V incorrectly interprets the details of the image. The image contains “the inflammatory cell population of mono nuclear cells, mainly small round lymphocytes” and “squamous metaplasia”, while GPT-4V identifies them as “koilocytes”. \nGround Truth: (E) Chronic inflammation \nExplanation:  A. HPV produces viral cytopathic effect such as ko ilo cyto tic change and atypical changes in the squamous epithelial cells. It can drive the process to dysplasia and to carcinoma. HPV by itself does not produce significant inflammation. B. The capsaicin in chili tends to irritate nerve endings to produce the hot burning sensation, as a local effect in oral cavity. It is not systemic. Capsaicin has been used to treat dyspareunia with vulvar vest i bulit is, probably by affecting nerve- ending mediated sensation, and there is no significant degree of inflammation. C. Diabetes mellitus may increase the risk for infections, but by itself does not explain the inflammation seen here. D. Tuberculosis of the genital tract is rare; mycobacteria l infections tend to produce gran u loma to us inflammation. E. This is a chronic inflammatory process, based upon the inflammatory cell  population of mono nuclear cells, mainly small round lymphocytes. There is also some hemorrhage. The squamous epithelium is normal, though if this were endocervix, it would represent squamous  metaplasia. "}
{"page": 68, "image_path": "doc_images/2311.16502v3_68.jpg", "ocr_text": "@ Clinical Medicine: Heart Disease\n\nQuestion: A 38-year-old man comes to the physician because of recurrent palpitations over the past 2\nmonths. He says that he was diagnosed with a minor heart defect during childhood but does not\nremember the details and has not had regular medical follow-up examinations since. Physical\nexamination shows a heart murmur. Cardiac catheterization is performed, and hemodynamic results\nshow <image 1>. Which of the following is the most likely diagnosis?\n\nOption:\n\n(A) Mitral stenosis\n\nB) Mitral regurgitation\n\nfe Aortic value regurgitation\n(D) Aortic stenosis\nOxygen saturation (%) || Pressure (mm Hg)\nRight atrium 75 3\nRight ventricle 75 25/4\nPulmonary trunk |) 75 22/10\nLeft atrium 99 10\nLeft ventricle 99 165/45\nAscending Aorta 99 155/7C\n<image 1>\n\nGPT-4V: Based on the provided hemodynamic results: The oxygen saturation of 75% in the right atrium,\nright ventricle, and pulmonary trunk, followed by a jump to 99% in the left atrium, left ventricle, and\nascending aorta, suggests the presence of a left-to-right shunt, where oxygenated blood is mixing with\ndeoxygenated blood. The pressure readings are also indicative. The normal pressure in the left atrium is\nusually around 10 mm Hg, while the pressure in the left ventricle during systole (contraction) can go up\nto 120 mm Hg or more and during diastole (relaxation) drops significantly. The given pressure for the\nleft ventricle is 165/45 mm Hg, which is significantly high during systole. Meanwhile, the pressure in the\nascending aorta is 155/70 mm Hg, which is closer to normal systolic pressure but has an elevated\ndiastolic pressure. These findings indicate that blood is flowing back from the left ventricle to the left\natrium during ventricular systole, which is consistent with mitral regurgitation. During mitral\nregurgitation, the mitral valve doesn't close tightly, allowing blood to flow backward in the heart. This\ncan cause symptoms like palpitations and can be associated with a heart murmur on physical\nexamination. Thus, the most likely diagnosis based on the provided data is: (B) Mitral regurgitation.\n\nError Category: Lack of Knowledge\n\nError Reason: GPT4-V successfully understands the question and the given table. However, due to\nthe lack of specific knowledge about \"This backflow decreases diastolic pressure in the aorta (N =\n60-90 mm Hg)\" and \"systolic pressure in both the ascending aorta...is increased\", GPT4-V interprets\n\"the pressure in the ascending aorta as 155/70 mm Hg, which is closer to normal systolic pressure\nbut has an elevated diastolic pressure\", and fails to identify the diagnosis of \"aortic regurgitation\".\n\nGround Truth: (C) Aortic value regurgitation\n\nExplanation: Aortic valve regurgitation causes a backward flow from the aorta during diastole. This\nbackflow decreases diastolic pressure in the aorta (N = 60-90 mm Hg) and increases diastolic pressure in\nthe left ventricle (N = 5-12 mm Hg). This regurgitant blood combines with blood from the left atrium to\nfill the left ventricle, which results in an increased preload and a subsequent increase in stroke volume.\nAsa result, the systolic pressure in both the ascending aorta (N = 90-140 mm Hg) and in the left ventricle\n(N = 90-140 mm Hg) is increased. Oxygen saturation and right heart pressures are normal in aortic\nregurgitation.\n\nFigure 55. A sample error case of Clinical Medicine (subfield: Heart Disease). Error category: Lack of Knowledge\nBack to List of Figures | Back to Table Index\n\n69\n", "vlm_text": "The image shows a red circle with a white 'X' inside it, bordered in black. This is commonly used to indicate an error, mistake, or that something is incorrect.\nClinical Medicine: Heart Disease \nQuestion:  A 38-year-old man comes to the physician because of recurrent palpitations over the past 2 months. He says that he was diagnosed with a minor heart defect during childhood but does not remember the details and has not had regular medical follow-up examinations since. Physical examination shows a heart murmur. Cardiac catheter iz ation is performed, and he mo dynamic results show <image  $^{1>}$  . Which of the following is the most likely diagnosis?\n\n \nOption: \n(A) Mitral stenosis\n\n (B) Mitral regurgitation\n\n (C) Aortic value regurgitation\n\n (D) Aortic stenosis \nThe table shows oxygen saturation percentages and pressures (in mm Hg) for different parts of the heart and circulatory system:\n\n- **Right atrium**: \n  - Oxygen saturation: 75%\n  - Pressure: 3 mm Hg\n\n- **Right ventricle**: \n  - Oxygen saturation: 75%\n  - Pressure: 25/4 mm Hg\n\n- **Pulmonary trunk**: \n  - Oxygen saturation: 75%\n  - Pressure: 22/10 mm Hg\n\n- **Left atrium**: \n  - Oxygen saturation: 99%\n  - Pressure: 10 mm Hg\n\n- **Left ventricle**: \n  - Oxygen saturation: 99%\n  - Pressure: 165/45 mm Hg\n\n- **Ascending Aorta**: \n  - Oxygen saturation: 99%\n  - Pressure: 155/70 mm Hg\nGPT-4V:  Based on the provided he mo dynamic results: The oxygen saturation of   $75\\%$   in the right atrium, right ventricle, and pulmonary trunk, followed by a jump to   $99\\%$   in the left atrium, left ventricle, and ascending aorta, suggests the presence of a left-to-right shunt, where oxygenated blood is mixing with de oxygenated blood. The pressure readings are also indicative. The normal pressure in the left atrium is usually around  $10\\:\\mathrm{mm\\:Hg},$   while the pressure in the left ventricle during systole (contraction) can go up to   $120~\\mathrm{mm~Hg}$   or more and during diastole (relaxation) drops significantly. The given pressure for the left ventricle is  $165/45\\:\\mathrm{mm\\:Hg},$   which is significantly high during systole. Meanwhile, the pressure in the  ascending aorta is   $155/70\\ \\mathrm{mm\\Hg},$   which is closer to normal systolic pressure but has an elevated  diastolic pressure. These findings indicate that blood is flowing back from the left ventricle to the left atrium during ventricular systole, which is consistent with mitral regurgitation. During mitral regurgitation, the mitral valve doesn't close tightly, allowing blood to flow backward in the heart. This can cause symptoms like palpitations and can be associated with a heart murmur on physical examination. Thus, the most likely diagnosis based on the provided data is: (B) Mitral regurgitation. \nError Category: Lack of Knowledge \nError Reason:  GPT4-V successfully understands the question and the given table. However, due to the lack of specific knowledge about \"This backflow decreases diastolic pressure in the aorta   $(\\Nu=$   $60{-}90~\\mathrm{mm~Hg})^{\\prime}$  \" and \"systolic pressure in both the ascending aorta...is increased\", GPT4-V interprets \"the pressure in the ascending aorta as  $155/70~\\mathrm{mm~Hg},$   which is closer to normal systolic pressure but has an elevated diastolic pressure\", and fails to identify the diagnosis of \"aortic regurgitation\". \nGround Truth: (C) Aortic value regurgitation \nExplanation:  Aortic valve regurgitation causes a backward flow from the aorta during diastole. This  backflow decreases diastolic pressure in the aorta   $(\\mathrm{N=60-90\\mm\\,Hg})$  ) and increases diastolic pressure in the left ventricle   $(\\mathrm{N}=5{-}12\\;\\mathrm{mm}\\;\\mathrm{Hg})$  ). This re gur gita nt blood combines with blood from the left atrium to fill the left ventricle, which results in an increased preload and a subsequent increase in stroke volume. As a result, the systolic pressure in both the ascending aorta   $\\mathrm{(N=90\\mathrm{-}140\\,\\bar{m}m\\,H g)}$  ) and in the left ventricle  $(\\mathrm{N\\,=\\,90{-}140\\,\\ m m\\ H g})$   is increased. Oxygen saturation and right heart pressures are normal in aortic  regurgitation. "}
{"page": 69, "image_path": "doc_images/2311.16502v3_69.jpg", "ocr_text": "eZ Diagnostics and Laboratory Medicine : Radiology\n\nQuestion: The 40-year-old man has been walking unsteadily for several years. CT performance is as\nshown in <image 1>. The possible diagnosis is?\n\nOption:\n\n(A) Neurofibromatosis type I\n\n(B) facial hemangioma syndrome\n\n(C) Dandy-Walker syndrome\n\n(D) tuberous sclerosis\n\n<image 1>\n\nGPT-4V: I'm not a doctor, but based on the CT image provided and the description: The image\nseems to show a cerebellar anomaly with the absence of the vermis and an enlarged posterior fossa\ncyst communicating with the fourth ventricle. These findings are suggestive of (C) Dandy-Walker\nsyndrome. However, it's crucial to consult with a medical professional for a definitive diagnosis\nand medical advice.\n\nGround Truth: (C) Dandy-Walker syndrome\n\nFigure 56. A sample correct case of Diagnostics and Laboratory Medicine (subfield: Radiology).\nBack to List of Figures | Back to Table Index\n\n70\n", "vlm_text": "The image shows a green circle with a black border containing a white check mark.\nDiagnostics and Laboratory Medicine : Radiology \nQuestion:  The 40-year-old man has been walking unsteadily for several years. CT performance is as shown in <image  $_{1>}$  . The possible diagnosis is?\n\n \nOption: \n(A) Neurofibromatosis type I\n\n (B) facial hemangioma syndrome\n\n (C) Dandy-Walker syndrome\n\n (D) tuberous sclerosis \nThe image shows a CT scan of the head at the axial plane.\nGPT-4V:  I'm not a doctor, but based on the CT image provided and the description: The image seems to show a cerebellar anomaly with the absence of the vermis and an enlarged posterior fossa cyst communicating with the fourth ventricle. These findings are suggestive of (C) Dandy-Walker syndrome. However, it's crucial to consult with a medical professional for a definitive diagnosis and medical advice. \nGround Truth: (C) Dandy-Walker syndrome \nFigure 56. A sample correct case of Diagnostics and Laboratory Medicine (subfield: Radiology). Back to List of Figures  Back to Table Index "}
{"page": 70, "image_path": "doc_images/2311.16502v3_70.jpg", "ocr_text": "@ Diagnostics and Laboratory Medicine: Neuropathology\n\nQuestion: What is the anatomic space occupied by the lesion? <image 1>\nOption:\n\n(A) Intraparenchymal\n\n(B) Subdural space\n\n(C) Subarachnoid space\n\n(D) Epidural space\n\nLs\n\n<image 1>\n\nGPT-4V: Based on the image provided, it's challenging to definitively determine the anatomic\nspace occupied by the lesion without more context. However, the appearance and patterns shown\nresemble patterns often seen in subarachnoid hemorrhages or lesions.\n\nTherefore, the most likely answer is:\n\n(C) Subarachnoid space.\n\nError Category: Perceptual Error\nError Reason: GPT-4V fails to interpret the image correctly. The image shows \"the dura can be seen\nin the top right of the image\", but GPT-4V cannot correctly identify it.\n\nGround Truth: (B) Subdural space\n\nExplanation: The dura can be seen in the top right of the image. Organization into thin subdural\nmembranes containing thin-walled vessels often is associated with rebleeding of subdural\nhemorrhages as seen here.\n\nFigure 57. A sample error case of Diagnostics and Laboratory Medicine (subfield: Neuropathology). Error category: Perceptual Error\nBack to List of Figures | Back to Table Index\n\n71\n", "vlm_text": "Diagnostics and Laboratory Medicine: Neuro pathology  \nQuestion:  What is the anatomic space occupied by the lesion? <image  $_{1>}$  Option:\n\n \n(A) In trap are nch y mal\n\n (B) Subdural space\n\n (C) Sub ara ch noid space\n\n (D) Epidural space \nThe image appears to show a microscopic view of tissue, possibly stained for histological examination. The colors indicate different tissue components, which might be observed in medical or biological studies to assess cell structures and other details.\nGPT-4V: Based on the image provided, it's challenging to definitively determine the anatomic space occupied by the lesion without more context. However, the appearance and patterns shown resemble patterns often seen in sub ara ch noid hemorrhages or lesions. Therefore, the most likely answer is: \n(C) Sub ara ch noid space. \nError Category: Perceptual Error \nError Reason:  GPT-4V fails to interpret the image correctly. The image shows \"the dura can be seen in the top right of the image\", but GPT-4V cannot correctly identify it. \nGround Truth: (B) Subdural space \nExplanation:  The dura can be seen in the top right of the image. Organization into thin subdural membranes containing thin-walled vessels often is associated with rebleeding of subdural hemorrhages as seen here. "}
{"page": 71, "image_path": "doc_images/2311.16502v3_71.jpg", "ocr_text": "@ Diagnostics and Laboratory Medicine: Ophthalmic Pathology\n\nQuestion: This intraocular tumor is from a 55-year-old man. Which of the following statements\nBEST describes the lesion. <image 1>\n\nOption:\n\n(A) This tumor is a classic epithelioid choroidal melanoma with necrosis\n\n(B) This may represent a retinoblastoma because of the classic pattern of necrosis and small round\nblue cells with rare pseudorosettes.\n\n(C) This tumor may represent the most common type of intraocular neoplasm, and requires further\nwork-up to distinguish it from melanoma.\n\n(D) The patient has a good prognosis because the neoplasm was entirely removed and the margins\nare free of tumor.\n\n<image 1>\n\nGPT-4V: Based on the image provided and without the ability to perform further clinical or\npathological tests, it's challenging to provide a definitive diagnosis. However, I can offer an analysis\nbased on the image and the choices provided:\n\n(A) Choroidal melanoma is a malignant tumor that arises from the pigmented cells of the choroid,\nwhich is a part of the uveal tract of the eye. Epithelioid melanoma cells are large with abundant\ncytoplasm and prominent nucleoli. Necrosis may be present, and if the image shows large\nepithelioid cells with these features, then this choice could be considered.\n\n(B) Retinoblastoma is a malignant tumor of the retina that predominantly affects children. It is\ncharacterized by small round blue cells, which can form pseudorosettes (rosettes that don't\nsurround a lumen). If the image displays these features, then this could be a possibility.\n\n(C) Choroidal melanoma is the most common primary intraocular malignancy in adults. If the\nimage shows features of melanoma but lacks the classic appearance of epithelioid melanoma or\nretinoblastoma, then further work-up would be necessary.\n\n(D) Making a prognosis solely based on an image can be challenging. Even if the tumor appears to\nbe completely excised with clear margins in the image, other factors, such as tumor size, type,\nlocation, and potential metastasis, would play a role in determining the prognosis. Based on the\nimage and the details provided, I cannot make a definitive choice. However, it's crucial to\ncollaborate with a pathologist or an ocular oncologist for a comprehensive evaluation and\ndiagnosis.\n\nError Category: Reject to Answer\nError Reason: GPT-4V refuses to answer this question without further clinical or pathological tests.\n\nGround Truth: (C) This tumor may represent the most common type of intraocular neoplasm,\n\nand requires further work-up to distinguish it from melanoma.\n\nFigure 58. A sample error case of Diagnostics and Laboratory Medicine (subfield: Ophthalmic Pathology). Error category: Reject to\nAnswer\nBack to List of Figures | Back to Table Index\n\n72\n", "vlm_text": "Diagnostics and Laboratory Medicine: Ophthalmic Pathology  \nQuestion:  This intraocular tumor is from a 55-year-old man. Which of the following statements BEST describes the lesion. <image  $_{1>}$  \nOption: \n(A) This tumor is a classic epi the li oid choroidal melanoma with necrosis (B) This may represent a ret in oblast oma because of the classic pattern of necrosis and small round blue cells with rare pseudo rosettes. (C) This tumor may represent the most common type of intraocular neoplasm, and requires further work-up to distinguish it from melanoma. \n(D) The patient has a good prognosis because the neoplasm was entirely removed and the margins are free of tumor. \nThis image shows a histological section, likely of biological tissue, stained to highlight different structures. The colors suggest it might be an H&E (hematoxylin and eosin) stain, commonly used in microscopy to examine the microanatomy of tissues. The dark purple areas typically represent cell nuclei, while the pink areas indicate other tissue components such as cytoplasm or extracellular matrix.\nGPT-4V: Based on the image provided and without the ability to perform further clinical or pathological tests, it's challenging to provide a definitive diagnosis. However, I can offer an analysis based on the image and the choices provided: \n(A) Choroidal melanoma is a malignant tumor that arises from the pigmented cells of the choroid, which is a part of the uveal tract of the eye. Epi the li oid melanoma cells are large with abundant cytoplasm and prominent nucleoli. Necrosis may be present, and if the image shows large epi the li oid cells with these features, then this choice could be considered. \n(B) Ret in oblast oma is a malignant tumor of the retina that predominantly affects children. It is characterized by small round blue cells, which can form pseudo rosettes (rosettes that don't surround a lumen). If the image displays these features, then this could be a possibility. \n(C) Choroidal melanoma is the most common primary intraocular malignancy in adults. If the image shows features of melanoma but lacks the classic appearance of epi the li oid melanoma or ret in oblast oma, then further work-up would be necessary. \n(D) Making a prognosis solely based on an image can be challenging. Even if the tumor appears to be completely excised with clear margins in the image, other factors, such as tumor size, type, location, and potential metastasis, would play a role in determining the prognosis. Based on the image and the details provided, I cannot make a definitive choice. However, it's crucial to collaborate with a pathologist or an ocular oncologist for a comprehensive evaluation and diagnosis. \nError Category: Reject to Answer \nError Reason:  GPT-4V refuses to answer this question without further clinical or pathological tests. \nGround Truth: (C) This tumor may represent the most common type of intraocular neoplasm, and requires further work-up to distinguish it from melanoma. \nFigure 58. A sample error case of Diagnostics and Laboratory Medicine (subfield: Ophthalmic Pathology). Error category: Reject to Answer "}
{"page": 72, "image_path": "doc_images/2311.16502v3_72.jpg", "ocr_text": "Diagnostics and Laboratory Medicine:\n@ Electrocardiography\n\nQuestion: What is the name of the highlighted section of the ECG and what does it represent?\n\nOption:\n\n(A) QRS complex; ventricular depolarization\n(B) P wave; atrial depolarization\n\n(C) T wave; ventricular repolarization\n\n(D) S-T segment; resting membrane potential\n\nHHYTHM STRIP: 11\n0 mm/sec4] cm/my.\n\n| isa |\n\nCia\n\n<image 1>\n\nGPT-4V: The highlighted section of the ECG represents the: (A) QRS complex; ventricular\ndepolarization.\n\nError Category: Perceptual Error, Lack of Knowledge\n\nError Reason: The error occurred because GPT-4V mistook the highlighted section for the QRS\ncomplex rather than the P wave. This confusion may have stemmed from two main factors: First,\nthe model's attention may have been solely on the highlighted portion, whose pointed shape is\ncharacteristic of the QRS complex. Second, the model may have evaluated the entire\nelectrocardiogram but misidentified the P wave due to its typically more rounded shape. A proper\nassessment of the whole electrocardiogram reveals that the QRS complex is actually represented by\nthe taller spikes that come after the highlighted section, indicating that the highlighted area is,\nindeed, the P wave.\n\nGround Truth: (B) P wave; atrial depolarization\n\nFigure 59. A sample error case of Diagnostics and Laboratory Medicine (subfield: Electrocardiography). Error category: Perceptual\nError, Lack of Knowledge\nBack to List of Figures | Back to Table Index\n\n73\n", "vlm_text": "The image shows a red circle with a white \"X\" in the center, outlined in black. This is commonly used as an icon to indicate an error, a wrong answer, or a prohibition.\nDiagnostics and Laboratory Medicine:  Electrocardiograph y \nQuestion:  What is the name of the highlighted section of the ECG and what does it represent? \nOption: \n(A) QRS complex; ventricular de polarization (B) P wave; atrial de polarization (C) T wave; ventricular re polarization (D) S-T segment; resting membrane potential \nThis image displays an electrocardiogram (ECG) rhythm strip labeled as \"RHYTHM STRIP: II.\" It shows heart activity, with waves representing different phases of the cardiac cycle. The strip has grid lines for measurement, and the settings show a speed of 25 mm/sec and a measurement of 1 cm/mV.\nGPT-4V:  The highlighted section of the ECG represents the:  (A) QRS complex; ventricular de polarization. \nError Category: Perceptual Error, Lack of Knowledge \nError Reason:  The error occurred because GPT-4V mistook the highlighted section for the QRS complex rather than the  $\\mathrm{P}$   wave. This confusion may have stemmed from two main factors: First, the model's attention may have been solely on the highlighted portion, whose pointed shape is characteristic of the QRS complex. Second, the model may have evaluated the entire electrocardiogram but misidentified the   $\\mathrm{P}$   wave due to its typically more rounded shape. A proper assessment of the whole electrocardiogram reveals that the QRS complex is actually represented by the taller spikes that come after the highlighted section, indicating that the highlighted area is, indeed, the  $\\mathrm{P}$   wave. \nGround Truth: (B) P wave; atrial de polarization \nFigure 59. A sample error case of Diagnostics and Laboratory Medicine (subfield: Electrocardiograph y). Error category: Perceptual Error, Lack of Knowledge Back to List of Figures  Back to Table Index "}
{"page": 73, "image_path": "doc_images/2311.16502v3_73.jpg", "ocr_text": "eZ Pharmacy : Pharmaceutical Microbiology\n\nQuestion: The antibiotic efrotomycin is produced by Streptomyces lactamdurans. S. lactamdurans\nwas grown in 40,000 liters of medium. The medium consisted of glucose, maltose, soybean oil,\n(NH,)2SO,, NaCl, KH,PO,, and Na;HPQ,. The culture was aerated and maintained at 28°C. The\nfollowing results were obtained from analyses of the culture medium during cell growth: <image\n1>, <image 2> Is efrotomycin a primary or secondary metabolite?\n\nOption:\n\n(A) Primary\n\n(B) Secondary\n\n(C) None of them\n\n*s\\, Lipids 7 a\n> ae Efrotomycin\na 4 Q2 consumption production\nMaltose ‘S| yes\nx.\nalas\n/ ‘J Log number\n4 fs of cells\nKe “SJ\nof RS\n7\n———E— EE) i}\n100 200 300 400 °o 100 200 300 400\nCultivation time (hr) Cultivation time (hr)\n<image 1> <image 2>\nGPT-4V: To determine whether efrotomycin is a primary or secondary metabolite, we need to look\nat the production pattern of the antibiotic in relation to the growth of the organism, S.\n\nlactamdurans. A primary metabolite is typically produced during the active growth phase of the\norganism, while a secondary metabolite is produced after the active growth phase, often during the\nstationary phase or when nutrients in the medium become limited. From the provided graphs:\n\nIn the first image, we see that lipids (potentially representing cell biomass) increase and then\nplateau. The oxygen consumption increases and starts to decline, which might indicate the end of\nthe active growth phase.\n\nIn the second image, glucose (a primary nutrient) is consumed rapidly during the initial phase of\ngrowth. The \"Log number of cells\" (representing cell growth) increases rapidly and then plateaus,\nindicating the end of the active growth phase. Efrotomycin production starts increasing after the log\nphase of cell growth has plateaued.\n\nGiven these observations, efrotomycin starts to be produced after the active growth phase of the\norganism, which is characteristic of secondary metabolites. So, the answer is: (B) secondary.\n\nGround Truth: (B) Secondary\n\nFigure 60. A sample correct case of Pharmacy (subfield: Pharmaceutical Microbiology).\nBack to List of Figures | Back to Table Index\n\n74\n", "vlm_text": "Pharmacy : Pharmaceutical Microbiology \nQuestion:  The antibiotic e fro to my c in is produced by Strep to my ces lactam duran s. S. lactam duran s was grown in 40,000 liters of medium. The medium consisted of glucose, maltose, soybean oil,  $(\\mathrm{NH_{4}})_{2}\\mathrm{SO_{4}},$   NaCl,   $\\mathrm{{NH_{2}P O_{4}}},$   and   $\\mathrm{Na_{2}H P O_{4}}$  . The culture was aerated and maintained at  $28^{\\circ}C$  . The following results were obtained from analyses of the culture medium during cell growth: <image  $^{1>,}$  , <image  $2{>}$   Is e fro to my c in a primary or secondary metabolite? \nOption:\n\n (A) Primary\n\n (B) Secondary\n\n (C) None of them \nThe image contains two graphs showing cultivation processes over time.\n\n- **Image 1**: \n  - X-axis: Cultivation time (hrs)\n  - Y-axis: Presumably concentration or rate \n  - Graph lines:\n    - Solid red line: Decline of maltose\n    - Dotted red line: Increase and then decrease of lipids\n    - Dashed black line: O₂ consumption increases over time\n\n- **Image 2**:\n  - X-axis: Cultivation time (hrs)\n  - Y-axis: Presumably concentration or rate\n  - Graph lines:\n    - Solid red line: Rapid increase and then decrease of glucose\n    - Dashed red line: Increase of Efrotomycin production\n    - Dotted red line: Log number of cells increases then stabilizes\n\nBoth graphs represent biological or chemical processes over a 400-hour period.\nGPT-4V:  To determine whether e fro to my c in is a primary or secondary metabolite, we need to look at the production pattern of the antibiotic in relation to the growth of the organism, S. lactam duran s. A primary metabolite is typically produced during the active growth phase of the organism, while a secondary metabolite is produced after the active growth phase, often during the stationary phase or when nutrients in the medium become limited. From the provided graphs: In the first image, we see that lipids (potentially representing cell biomass) increase and then plateau. The oxygen consumption increases and starts to decline, which might indicate the end of the active growth phase. \nIn the second image, glucose (a primary nutrient) is consumed rapidly during the initial phase of growth. The \"Log number of cells\" (representing cell growth) increases rapidly and then plateaus, indicating the end of the active growth phase. E fro to my c in production starts increasing after the log phase of cell growth has plateaued. \nGiven these observations, e fro to my c in starts to be produced after the active growth phase of the organism, which is characteristic of secondary metabolites. So, the answer is: (B) secondary. \nGround Truth: (B) Secondary \nFigure 60. A sample correct case of Pharmacy (subfield: Pharmaceutical Microbiology). Back to List of Figures  |  Back to Table Index "}
{"page": 74, "image_path": "doc_images/2311.16502v3_74.jpg", "ocr_text": "@ Pharmacy: Pharmacology and Drug Synthesis\n\nQuestion: The following synthetic scheme <image 1> has been used to synthesize raltegravir. If you\nwanted to create a number of analogues, which substituent <image 2> could be varied most\nefficiently using this synthesis?\n\nOption:\n\n(A) The type of substituent A on the aromatic ring\n\nB) The type of alkyl substituent B present\n\nC) The type of heterocyclic ring C present\n\nD) The length of the chain D between the amide and the aromatic ring\n\nA\ncs\nMe ° ’\nCH; \\ LOH F\nia ey Weg rat , CY\nOse ogy ® IAN Ay NAL\nGo # 6 ¢o,9 e F 6 10,9 o ome : o 4\nI W ; °\n5\n<image 1> <image 2>\n\nGPT-4V: Given the synthetic scheme provided, let's analyze each option:\n\n(A) The type of substituent A on the aromatic ring: In the second image, substituent A is located on\nthe aromatic ring. Since this is present at the end of the molecule, it might be relatively easy to vary\nit, as it doesn't interfere with the rest of the synthesis.\n\n(B) The type of alkyl substituent B present: Substituent B is an alkyl group attached to the\nheterocyclic ring. Altering this substituent would require modifying the building block that is used\nto make this part of the molecule, which might be a bit more challenging.\n\n(C) The type of heterocyclic ring C present: The heterocyclic ring C appears to be a critical part of\nthe structure and seems to be central to the synthesis. Modifying this ring would likely require\nsignificant changes to the entire synthesis.\n\n(D) The length of the chain D between the amide and the aromatic ring: Changing the length of\nchain D would require altering the length of the carbon chain in the precursor molecule. This\nchange could be achieved by selecting different precursors with varied chain lengths.\n\nThus, (A) The type of substituent A on the aromatic ring seems to be the most efficiently varied\nusing this synthesis. The aromatic ring can be easily subjected to a variety of substitution reactions\nto achieve analogues with different substituents without disturbing the rest of the molecule.\n\nError Category: Lack of Knowledge\n\nError Reason: GPT-4V successfully understands the images and text. However, due to the lack of\nspecific knowledge about 'The synthesis is most efficient for creating analogues with a variety of\nheterocyclic rings\", GPT-4V assumes \"substituent A is located on the aromatic ring, which is at the\nend of the molecule, and it might be relatively easy to vary it, as it doesn't interfere with the rest of\nthe synthesis\" and subsequently makes an incorrect prediction.\n\nGround Truth: (C) The type of heterocyclic ring C present\n\nExplanation: It is possible to use this synthesis to create analogues where all the positions indicated\nare modified as described. However, the synthesis is most efficient for creating analogues with a\nvariety of heterocyclic rings. That is because the reaction sequence could be carried out on large\nscale to produce a supply of the penultimate structure. Analogues could then be created in one step\nby reacting this structure with different heterocyclic acid chlorides. If the synthetic scheme was\nused to create analogues at the other positions indicated, then more reactions will be needed. For\nexample, varying the type of alkyl substituent present at position B would involve carrying out the\ncomplete synthesis for each analogue.\n\nFigure 61. A sample error case of Pharmacy (subfield: Pharmacology and Drug Synthesis). Error category: Lack of Knowledge\nBack to List of Figures | Back to Table Index\n\n715\n", "vlm_text": "The image shows a red circle with a white \"X\" in the center, typically used to indicate error, deletion, or a negative action.\nPharmacy: Pharmacology and Drug Synthesis \nQuestion:  The following synthetic scheme <image  $_{1>}$   has been used to synthesize r alte grav ir. If you wanted to create a number of analogues, which substituent <image   $2{>}$   could be varied most efficiently using this synthesis?\n\n \nOption: \n(A) The type of substituent A on the aromatic ring\n\n (B) The type of alkyl substituent B present\n\n (C) The type of heterocyclic ring C present\n\n (D) The length of the chain D between the amide and the aromatic ring \nThe image contains two separate diagrams labeled as <image 1> and <image 2>.\n\n- **<image 1>**: This shows two molecular structures labeled I and II, which appear to be chemical compounds.\n  - Structure I: Contains a cyclic amide and other organic functional groups.\n  - Structure II: Similar to I with additional phenyl groups attached.\n\n- **<image 2>**: This diagram shows a complex organic molecule with various substituents and functional groups. It includes annotations labeled A, B, C, and D pointing to specific parts of the molecule. \n\nThese structures represent specific chemical compounds, potentially pharmaceutical or chemical research compounds, highlighting different substitutions and functional elements.\nGPT-4V:  Given the synthetic scheme provided, let's analyze each option: \n(A) The type of substituent A on the aromatic ring: In the second image, substituent A is located on  the aromatic ring. Since this is present at the end of the molecule, it might be relatively easy to vary  it, as it doesn't interfere with the rest of the synthesis. \n(B) The type of alkyl substituent B present: Substituent B is an alkyl group attached to the heterocyclic ring. Altering this substituent would require modifying the building block that is used to make this part of the molecule, which might be a bit more challenging. \n(C) The type of heterocyclic ring C present: The heterocyclic ring C appears to be a critical part of the structure and seems to be central to the synthesis. Modifying this ring would likely require significant changes to the entire synthesis. \n(D) The length of the chain D between the amide and the aromatic ring: Changing the length of chain D would require altering the length of the carbon chain in the precursor molecule. This change could be achieved by selecting different precursors with varied chain lengths. \nThus,  (A) The type of substituent A on the aromatic ring  seems to be the most efficiently varied using this synthesis. The aromatic ring can be easily subjected to a variety of substitution reactions to achieve analogues with different substituents without disturbing the rest of the molecule. \nError Category: Lack of Knowledge \nError Reason:  GPT-4V successfully understands the images and text. However, due to the lack of specific knowledge about \"The synthesis is most efficient for creating analogues with a variety of heterocyclic rings\", GPT-4V assumes \"substituent A is located on the aromatic ring, which is at the end of the molecule, and it might be relatively easy to vary it, as it doesn't interfere with the rest of the synthesis\" and subsequently makes an incorrect prediction. \nGround Truth: (C) The type of heterocyclic ring C present \nExplanation:  It is possible to use this synthesis to create analogues where all the positions indicated are modified as described. However, the synthesis is most efficient for creating analogues with a  variety of heterocyclic rings. That is because the reaction sequence could be carried out on large  scale to produce a supply of the penultimate structure. Analogues could then be created in one step by reacting this structure with different heterocyclic acid chlorides. If the synthetic scheme was used to create analogues at the other positions indicated, then more reactions will be needed. For example, varying the type of alkyl substituent present at position B would involve carrying out the complete synthesis for each analogue. "}
{"page": 75, "image_path": "doc_images/2311.16502v3_75.jpg", "ocr_text": "@ Pharmacy: Pharmacology and Drug Synthesis\n\nQuestion: Each trace below <image 1> shows the change in mean arterial blood pressure from its\nbaseline value. Which of the following drugs best fits the effects of unknown W?\n\nOption:\n\n(A) cocaine\n\n(B) phenelzine\n\n(C) phentolamine\n\n(D) propranolol\n\nBefore Unknown W:\n\nvs\neo\nt tT\n\nNorepinephrine Tyramine\n\nIn the Presence of Unknown W:\n\ntT 1\n\nNorepinephrine Tyramine\n\n<image 1>\n\nGPT-4V: To answer this question, we need to understand how each drug affects mean arterial\nblood pressure in the context of Norepinephrine and Tyramine. Here's a breakdown:\n\n- Cocaine: Cocaine blocks the reuptake of norepinephrine, which would cause a prolonged effect of\nnorepinephrine, leading to a sustained increase in blood pressure. However, it wouldn't block the\n- Phenelzine: Phenelzine is a monoamine oxidase inhibitor (MAOT). MAOIs block the degradation\nof norepinephrine and also of indirectly acting s athomimetics like Tyramine.\n\n- Phentolamine: Phentolamine is a non-selective alpha-adrenergic antagonist. It would block the\neffects of both norepinephrine and Tyramine, leading to a decrease or no change in blood pressure.\n- Propranolol: Propranolol is a non-selective beta-adrenergic antagonist. It would block the effects\nof norepinephrine on beta receptors but wouldn't have an effect on Tyramine's mechanism, which\nprimarily acts through alpha receptors.\n\nGiven the traces in the image: Before the administration of Unknown W, both Norepinephrine and\nTyramine increase blood pressure. In the presence of Unknown W, Norepinephrine still increases\nblood pressure, but Tyramine no longer has this effect. The drug that fits this pattern is Phenelzine.\nIn the presence of an MAOI, Norepinephrine would still exert its direct effect, but\n\n. Thus, the correct answer is: (B) phenelzine.\n\nError Category: Lack of Knowledge\n\nError Reason: GPT-4V lacks knowledge about the interaction between Cocaine and Tyramine.\nWhile it correctly recalls information about Phenelzine, it appears the lack of other plausible\noptions leads to a reasoning error (which could be deemed as hallucination), as it strives to select\nthe most plausible option from the ones given, and then justifies that selection. Therefore, the root\ncause remains a lack of knowledge.\n\nGround Truth: (A) cocaine\n\nFigure 62. A sample error case of Pharmacy (subfield: Pharmacology and Drug Synthesis). Error category: Lack of Knowledge\nBack to List of Figures | Back to Table Index\n\n76\n", "vlm_text": "The image shows a red circle with a white \"X\" in the center. This symbol is often used to represent \"error,\" \"cancel,\" or \"close.\"\nPharmacy: Pharmacology and Drug Synthesis \nQuestion:  Each trace below <image   $1\\mathrm{>}$   shows the change in mean arterial blood pressure from its baseline value. Which of the following drugs best fits the effects of unknown W? \nOption: \n(A) cocaine (B) phenelzine (C) ph en to la mine (D) propranolol \nThe image consists of two line graphs depicting changes in mean arterial blood pressure (BP) in response to norepinephrine and tyramine. \n\n- **Top Graph**: Shows the response before the introduction of an unknown variable, referred to as \"Unknown W.\" Both norepinephrine and tyramine cause an increase in mean arterial BP.\n  \n- **Bottom Graph**: Illustrates the response in the presence of \"Unknown W.\" Norepinephrine still raises BP, but the response to tyramine is diminished or absent.\n\nThis setup suggests that \"Unknown W\" affects the body's response to tyramine but not to norepinephrine, potentially indicating a mechanism like inhibition or blockade altering tyramine's effect.\nGPT-4V:  To answer this question, we need to understand how each drug affects mean arterial  blood pressure in the context of Norepinephrine and Tyramine. Here's a breakdown:  \n- Cocaine: Cocaine blocks the reuptake of norepinephrine, which would cause a prolonged effect of  norepinephrine, leading to a sustained increase in blood pressure.  However, it wouldn't block the  effects of Tyramine. \n- Phenelzine: Phenelzine is a monoamine oxidase inhibitor (MAOI). MAOIs block the degradation  of norepinephrine and also of indirectly acting sym path omi metics like Tyramine.  When MAO is  inhibited, administration of Tyramine can cause a hypertensive crisis because more of it is  converted to norepinephrine. \n- Ph en to la mine: Ph en to la mine is a non-selective alpha-adrenergic antagonist. It would block the  effects of both norepinephrine and Tyramine, leading to a decrease or no change in blood pressure. - Propranolol: Propranolol is a non-selective beta-adrenergic antagonist. It would block the effects  of norepinephrine on beta receptors but wouldn't have an effect on Tyramine's mechanism, which  primarily acts through alpha receptors. \nGiven the traces in the image: Before the administration of Unknown W, both Norepinephrine and  Tyramine increase blood pressure. In the presence of Unknown W, Norepinephrine still increases  blood pressure, but Tyramine no longer has this effect. The drug that fits this pattern is Phenelzine.  In the presence of an MAOI, Norepinephrine would still exert its direct effect, but  Tyramine, which  typically induces the release of stored cate chola mines, would result in an amplified effect due to the  blocked degradation of norepinephrine. The lack of a hypertensive response to Tyramine in the  second trace suggests that its conversion to norepinephrine might have been blocked, consistent  with the action of an MAOI . Thus, the correct answer is: (B) phenelzine. \nError Category: Lack of Knowledge \nError Reason:  GPT-4V lacks knowledge about the interaction between Cocaine and Tyramine. While it correctly recalls information about Phenelzine, it appears the lack of other plausible options leads to a reasoning error (which could be deemed as hallucination), as it strives to select the most plausible option from the ones given, and then justifies that selection. Therefore, the root cause remains a lack of knowledge. \nGround Truth: (A)  cocaine \nFigure 62. A sample error case of Pharmacy (subfield: Pharmacology and Drug Synthesis). Error category: Lack of Knowledge Back to List of Figures  Back to Table Index "}
{"page": 76, "image_path": "doc_images/2311.16502v3_76.jpg", "ocr_text": "eZ Public Health: Epidemiology\n\nQuestion: An outbreak of gastrointestinal disease occurred 24-36 hours after people had attended a\nwedding. Of the 203 attendees (including the bride and groom), 200 completed questionnaires, and\n50 reported illness compatible with the case definition. Tabulated food consumption histories are\npresented in the table below. The most appropriate measure of association for these data is the:\n\nOption:\n\n(A) Attributable risk percent\n\n(B) Chi-square\n\n(C) Odds ratio\n\n(D) Risk ratio\n\nAte Specified Food Did Not Eat Specified Food\n\nFood Item Ml Well Total Ll} Well Total\n\nPunch 46 (25%) 138 184 4 (25%) 12 16\nWedding Cake 45 (45%) 55 100 5 (5%) 95 100\nSushi 10 (91%) 1 11 40 (21%) 149 189\n<image 1>\n\nGPT-4V: Based on the information provided, you're trying to determine the relationship between\nthe consumption of a particular food item and the risk of developing gastrointestinal disease among\nwedding attendees. In this type of scenario, where you are comparing the risk of disease between\nthose exposed (ate the food) and those not exposed (did not eat the food), the appropriate measure\nof association is the:(D) Risk ratio (also known as relative risk).The risk ratio compares the risk of\ndisease in the exposed group to the risk of disease in the unexposed group.\n\nGround Truth: (B) Chi-square\n\nFigure 63. A sample correct case of Public Health (subfield: Epidemiology).\nBack to List of Figures | Back to Table Index\n\n77\n", "vlm_text": "Public Health: Epidemiology \nQuestion:  An outbreak of gastrointestinal disease occurred 24-36 hours after people had attended a wedding. Of the 203 attendees (including the bride and groom), 200 completed questionnaires, and 50 reported illness compatible with the case definition. Tabulated food consumption histories are presented in the table below. The most appropriate measure of association for these data is the: \nOption:\n\n (A) Attributable risk percent\n\n (B) Chi-square\n\n (C) Odds ratio\n\n (D) Risk ratio \nThe table displays data on people who ate or did not eat different food items and whether they fell ill or remained well. It consists of four columns:\n\n1. **Food Item**: Lists \"Punch,\" \"Wedding Cake,\" and \"Sushi.\"\n2. **Ate Specified Food**: \n   - Displays the number of people who ate each item and fell \"Ill\" or remained \"Well,\" along with percentages.\n   - Totals for each food are provided.\n3. **Did Not Eat Specified Food**: \n   - Displays the number of people who did not eat each item and fell \"Ill\" or remained \"Well,\" along with percentages.\n   - Totals for each scenario are also given.\n\nEach row corresponds to a food item, showing the distribution of people who became ill versus those who stayed well, based on their consumption of each food.\nGPT-4V:  Based on the information provided, you're trying to determine the relationship between the consumption of a particular food item and the risk of developing gastrointestinal disease among wedding attendees. In this type of scenario, where you are comparing the risk of disease between those exposed (ate the food) and those not exposed (did not eat the food), the appropriate measure of association is the:(D) Risk ratio (also known as relative risk).The risk ratio compares the risk of disease in the exposed group to the risk of disease in the unexposed group. \nGround Truth: (B) Chi-square \nFigure 63. A sample correct case of Public Health (subfield: Epidemiology). Back to List of Figures  |  Back to Table Index "}
{"page": 77, "image_path": "doc_images/2311.16502v3_77.jpg", "ocr_text": "(3) Public Health: Epidemiology\n\nQuestion: In February 2014, the New England Journal of Medicine published in the same issue two\nepidemiological trials (field trials) of inactivated EV71 vaccine conducted by scholars in China\nrespectively. One of the studies was from Jiangsu Province, China, and the study design was a\nrandomised, double-blind, placebo-controlled, multicentre trial (phase III trial), which included a\ntotal of 10,077 healthy children aged 6-35 months. The primary outcome indicator was the incidence\nof EV71-related hand, foot and mouth disease or herpangina, according to the 0- and 28-day\nimmunisation schedule, using block (one group of 10 persons) randomisation, with a 1:1 allocation\nof study participants to EV71 vaccine or placebo, and follow-up monitoring for 12 months, and the\nresults were shown in the table below. <image 1>The incidence density(/1000 person years) of\nhand, foot and mouth disease in placebo group was\n\nOption:\n(A) 18.5\n(B) 4.9\n(C) 19.3\n(D) 21.8\nOutcomes Vaccination group (4973.2 person years) Placebo group(4873.0 person years)\nNew cases New cases\nEv71-related hand, foot and mouth\n5 94\n\ndisease or herpangina\n\nHand, foot and mouth disease 5 90\n\nherpangina () 4\nEV71-related hospitalisation events ° 24\nEV71-related Hand, Foot and Mouth\nDisease with Combined Neurological ° 8\nComplications\nAll EV71 related diseases 13 106\n\n<image 1>\n\nGPT-4V: To find the incidence density (per 1000 person years) of hand, foot, and mouth disease in\nthe placebo group, we need to calculate the rate of new cases of the disease per 1000 person years of\nobservation. From the table:\n\nNumber of new cases of EV71-related hand, foot, and mouth disease or herpangina in placebo\ngroup = 94Total person years observed for placebo group = 4873.0 person years. Incidence density =\n(Number of new cases / Total person years observed) x 1000Incidence density = (.0) x 1000 = 19.3.\nTherefore, the incidence density (/1000 person years) of hand, foot and mouth disease in placebo\ngroup is:(C) 19.3\n\nError Category: Textual Understanding Error\n\nError Reason: The question asked for the incidence density (/1000 person-years) of hand, foot, and\nmouth disease in the placebo group, which should have been correctly calculated as (90/4873 * 1000\n= 18.5). However, GPT-4 erroneously extracted the data for EV71-related hand, foot, and mouth\ndisease or herpangina (94/4873 * 1000=19.3), leading to an incorrect final answer.\n\nGround Truth: (A) 18.5\n\nFigure 64. A sample error case of Public Health (subfield: Epidemiology). Error category: Textual Understanding Error\n\nBack to List of Figures | Back to Table Index\n\n78\n", "vlm_text": "Public Health: Epidemiology \nQuestion:  In February 2014, the New England Journal of Medicine published in the same issue two epidemiological trials (field trials) of in activated EV71 vaccine conducted by scholars in China respectively. One of the studies was from Jiangsu Province, China, and the study design was a randomised, double-blind, placebo-controlled, multi centre trial (phase III trial), which included a total of 10,077 healthy children aged 6-35 months. The primary outcome indicator was the incidence of EV71-related hand, foot and mouth disease or herpangina, according to the 0- and 28-day im munis ation schedule, using block (one group of 10 persons) random is ation, with a 1:1 allocation of study participants to EV71 vaccine or placebo, and follow-up monitoring for 12 months, and the results were shown in the table below. <image   $_{1>}$  The incidence density(/1000 person years) of hand, foot and mouth disease in placebo group was \nOption: (A) 18.5 (B) 4.9 (C) 19.3 (D) 21.8 \nThe table presents data comparing the number of new cases between a vaccination group and a placebo group over a specified period. Here's a summary of the outcomes:\n\n1. **EV71-related hand, foot, and mouth disease or herpangina**:\n   - Vaccination group: 5 cases\n   - Placebo group: 94 cases\n\n2. **Hand, foot, and mouth disease**:\n   - Vaccination group: 5 cases\n   - Placebo group: 90 cases\n\n3. **Herpangina**:\n   - Vaccination group: 0 cases\n   - Placebo group: 4 cases\n\n4. **EV71-related hospitalization events**:\n   - Vaccination group: 0 cases\n   - Placebo group: 24 cases\n\n5. **EV71-related Hand, Foot, and Mouth Disease with Combined Neurological Complications**:\n   - Vaccination group: 0 cases\n   - Placebo group: 8 cases\n\n6. **All EV71-related diseases**:\n   - Vaccination group: 13 cases\n   - Placebo group: 106 cases\n\nThe table indicates a significantly lower number of new cases in the vaccination group compared to the placebo group for all the outcomes listed.\nGPT-4V:  To find the incidence density (per 1000 person years) of hand, foot, and mouth disease in the placebo group, we need to calculate the rate of new cases of the disease per 1000 person years of observation. From the table: \nNumber of new cases of EV71-related hand, foot, and mouth disease or herpangina in placebo group  $=$   94Total person years observed for placebo group  $=4873.0$   person years. Incidence density  $=$  (Number of new cases / Total person years observed)  $x\\;10001$  Incidence density   $=(.0)\\times1000=19.3$  . Therefore, the incidence density (/1000 person years) of hand, foot and mouth disease in placebo group is:(C) 19.3 \nError Category: Textual Understanding Error \nError Reason:  The question asked for the incidence density (/1000 person-years) of hand, foot, and mouth disease in the placebo group, which should have been correctly calculated as   $(90/4873\\ast1000$   $=18.5)$  . However, GPT-4 erroneously extracted the data for EV71-related hand, foot, and mouth disease or herpangina   $(94/4873*1000{=}19.3)$  ), leading to an incorrect final answer. \nGround Truth: (A) 18.5 \nFigure 64. A sample error case of Public Health (subfield: Epidemiology). Error category: Textual Understanding Error Back to List of Figures  Back to Table Index "}
{"page": 78, "image_path": "doc_images/2311.16502v3_78.jpg", "ocr_text": "@ Public Health: Epidemiology\n\nQuestion: A maternal and child health facility wanted to evaluate the effectiveness of two methods\nof screening for cervical intraepithelial neoplasia (CIN [-III), electronic colposcopy and cervical\nliquid-based cytology (TCT). 600 gynaecological outpatient cases were examined by both methods,\nand all cases were confirmed with cervical histopathology (the gold standard) at the same time. The\nresults of all examinations were summarised in the table. <image 1>The false positive rate of the\nserial test was\n\nOption:\n(A) 0.95\n(B) 0.51\n(C) 0.75\n(D) 0.05\nTest results Patients with cervical | Patients with _non-cervical\nelectronic colposcopy TcT lesions lesions\n\n+ - 15 10\n\n+ 5 220\n\n+ + 70 25\n\n10 245\n\nTotal 100 500\n\n<image 1>\n\nGPT-4V: To determine the false positive rate of the serial test (both electronic colposcopy and TCT\npositive) for detecting cervical intraepithelial neoplasia (CIN I-III), we can use the following\nformula: False Positive Rate (FPR) = False Positives / (False Positives + True Negatives) False\nPositives (FP): These are the cases where both tests are positive (+, +) but the patient does not\nactually have cervical lesions. From the table:\n\nPatients with non-cervical lesions and both tests positive = 25So, FP = 25True Negatives (TN): These\nare the cases where both tests are positive (+, +) and the patient actually has cervical lesions,\ncombined with the cases where both tests are negative (-, -) but the patient does not have cervical\nlesions.\n\nFrom the table:\n\nPatients with cervical lesions and both tests positive = 70\n\nPatients with non-cervical lesions and both tests negative = 245S0, TN = 70 + 245 = 315Now,\nplugging into the formula:\n\nFPR = FP / (FP + TN)\n\nFPR = 25 / (25 + 315)\n\nFPR = 0.0735 or 7.35%. The value closest to 7.35% is 0.075. So, the correct answer is:\\n(C) 0.75\n\nError Category: Lack of Knowledge\n\nError Reason: The correct identification of False Positives (FP) was indeed 25, but there was a\nmistake in calculating True Negatives (TN). TNs are cases that are actually negative and also test\nnegative. GPT-4 incorrectly used True Positives (TP) in calculating TN. Moreover, the question\npertained to a series test, meaning a diagnosis of positive was made only if both electronic\ncolposcopy and TCT (ThinPrep Cytologic Test) were positive. However, GPT-4 considered a\npositive diagnosis if either one of these tests was positive, which led to an error in the count of TN.\nThe correct calculation should be 25/(10+220+25+245) = 0.05.\n\nGround Truth: (D) 0.05\nFigure 65. A sample error case of Public Health (subfield: Epidemiology). Error category: Lack of Knowledge\n\nBack to List of Figures | Back to Table Index\n\n79\n", "vlm_text": "The image shows a red circle with a white \"X\" in the center. This symbol is often used to indicate incorrectness or a \"close\" action.\nPublic Health: Epidemiology \nQuestion:  A maternal and child health facility wanted to evaluate the effectiveness of two methods of screening for cervical intra epithelial neoplasia (CIN I-III), electronic colposcopy and cervical liquid-based cytology (TCT). 600 g yna ecological outpatient cases were examined by both methods, and all cases were confirmed with cervical his to pathology (the gold standard) at the same time. The results of all examinations were summarised in the table. <image   $1\\textgreater$  The false positive rate of the serial test was \nOption:\n\n (A) 0.95\n\n (B) 0.51\n\n (C) 0.75\n\n (D) 0.05 \nThe table compares test results of electronic colposcopy and TCT (ThinPrep Cytologic Test) with regard to patients having cervical and non-cervical lesions. \n\nHere's a breakdown:\n\n1. **Electronic Colposcopy (+), TCT (-)**:\n   - Patients with cervical lesions: 15\n   - Patients with non-cervical lesions: 10\n\n2. **Electronic Colposcopy (-), TCT (+)**:\n   - Patients with cervical lesions: 5\n   - Patients with non-cervical lesions: 220\n\n3. **Electronic Colposcopy (+), TCT (+)**:\n   - Patients with cervical lesions: 70\n   - Patients with non-cervical lesions: 25\n\n4. **Electronic Colposcopy (-), TCT (-)**:\n   - Patients with cervical lesions: 10 \n   - Patients with non-cervical lesions: 245\n\n**Total:**\n- Patients with cervical lesions: 100\n- Patients with non-cervical lesions: 500\nGPT-4V:  To determine the false positive rate of the serial test (both electronic colposcopy and TCT positive) for detecting cervical intra epithelial neoplasia (CIN I-III), we can use the following formula: False Positive Rate (FPR)  $=$   False Positives  $/$   (False Positives   $^+$   True Negatives) False Positives (FP): These are the cases where both tests are positive   $(+,\\ +)$   but the patient does not actually have cervical lesions. From the table: \nPatients with non-cervical lesions and both tests positive  $=25\\mathrm{o}$  ,  $\\mathrm{FP}=257$  True Negatives (TN): These are the cases where both tests are positive   $(+,\\ +)$   and the patient actually has cervical lesions, combined with the cases where both tests are negative   $\\left(-,\\,-\\right)$   but the patient does not have cervical lesions. \nFrom the table: Patients with cervical lesions and both tests positive  $=70$  \nPatients with non-cervical lesions and both tests negative   $=\\,245\\mathrm{{Co}}$  ,   $\\mathrm{TN}\\,=\\,70\\,+\\,245\\,=\\,315\\mathrm{Now},$  plugging into the formula:\n\n  $\\mathrm{{FPR}=\\mathrm{{FP}/\\left(\\mathrm{{FP}+\\mathrm{{TN}}\\right)}}}\n\n$   $\\mathrm{FPR}=25\\;/\\;(25+315)\n\n$  \n $\\mathrm{FPR}=0.0735$   or  $7.35\\%$  . The value closest to  $7.35\\%$   is 0.075. So, the correct answer is:  $\\setminus\\mathfrak{n}(\\mathbf{C})$   0.75 \nError Category: Lack of Knowledge \nError Reason:  The correct identification of False Positives (FP) was indeed 25, but there was a mistake in calculating True Negatives (TN). TNs are cases that are actually negative and also test negative. GPT-4 incorrectly used True Positives (TP) in calculating TN. Moreover, the question pertained to a series test, meaning a diagnosis of positive was made only if both electronic colposcopy and TCT (ThinPrep Cytologic Test) were positive. However, GPT-4 considered a positive diagnosis if either one of these tests was positive, which led to an error in the count of TN. The correct calculation should be 25/  $(10\\small{+}220{+}25{+}245)=0.05$  . \nGround Truth: (D) 0.05 "}
{"page": 79, "image_path": "doc_images/2311.16502v3_79.jpg", "ocr_text": "eZ History: European History\n\nQuestion: This cartoon is best understood in the context of which of the following events? <image\n1>\n\nOption:\n\n(A) Treaty of Brest-Litovsk\n\n(B) Tripartite Pact\n\n(C) Anti-Comintern Pact\n\n(D) The Munich Conference\n\no ,\nTELLING THE ITALIANS!\n\n“Telling the Italians!” American political cartoon pub-\nlished in 1938.\n\n<image 1>\n\nGPT-4V: The cartoon is best understood in the context of:\n(D) The Munich Conference.\n\nGround Truth: (D) The Munich Conference\n\nExplanation: Germany invaded Austria in 1938 before setting its sights on the Sudetenland of\nCzechoslovakia. At the Munich Conference of 1938, which included Hitler, Mussolini, and Prime\nMinister Neville Chamberlain of England, Hitler was given the Sudetenland, without the consent of\nCzechoslovakia, in return for the promise to cease his expansionist activities. The cartoon shows\nHitler placing Austria in his bag, which would set this cartoon right at the time of the Munich\nConference. Choice (D) is correct. The Treaty of Brest-Litovsk was a World War I era treaty that set\nthe stage for Russia's exit from the war. Eliminate (A). The Tripartite Pact signed by Germany, Italy,\nand Japan did not occur until two years after this cartoon was created. Eliminate (B). The Anti-\nComintern Pact signed between Germany, Italy, and Japan focused on a mutual defense against\nRussia and had nothing to do with Austria. Eliminate (C).\n\nFigure 66. A sample correct case of History (subfield: European History).\nBack to List of Figures | Back to Table Index\n\n80\n", "vlm_text": "History: European History \nQuestion:  This cartoon is best understood in the context of which of the following events? <image 1> \nOption:\n\n \n(A) Treaty of Brest-Litovsk\n\n (B) Tripartite Pact\n\n (C) Anti-Comintern Pact\n\n (D) The Munich Conference \nThe image is a political cartoon featuring caricatures of two figures depicted in military uniforms. The taller figure, carrying a rifle and a sack labeled \"GREECE IN THE BAG,\" has a swastika on his hat. The shorter figure, with a military hat as well, is raising his hand and saying, \"I PLANNED IT THAT WAY AND ADOLF CARRIED IT OUT!\" The caption at the bottom reads \"TELLING THE ITALIANS!\" This cartoon likely comments on historical events involving Germany and Italy during World War II.\nGPT-4V:  The cartoon is best understood in the context of: (D) The Munich Conference. \nGround Truth: (D) The Munich Conference \nExplanation: Germany invaded Austria in 1938 before setting its sights on the Sudetenland of Czechoslovakia. At the Munich Conference of 1938, which included Hitler, Mussolini, and Prime Minister Neville Chamberlain of England, Hitler was given the Sudetenland, without the consent of Czechoslovakia, in return for the promise to cease his expansionist activities. The cartoon shows Hitler placing Austria in his bag, which would set this cartoon right at the time of the Munich Conference. Choice (D) is correct. The Treaty of Brest-Litovsk was a World War I era treaty that set the stage for Russia's exit from the war. Eliminate (A). The Tripartite Pact signed by Germany, Italy, and Japan did not occur until two years after this cartoon was created. Eliminate (B). The Anti- Comintern Pact signed between Germany, Italy, and Japan focused on a mutual defense against Russia and had nothing to do with Austria. Eliminate (C). "}
{"page": 80, "image_path": "doc_images/2311.16502v3_80.jpg", "ocr_text": "Question: A historian researching the economic history of Eurasia in the period circa 600-1450 c.e.\n\nwould most likely find the two tables (<image 1> and <image 2>) useful as a source of information\n\nabout which of the following?\n\nOption:\n\n(A) The diffusion of cultural traditions along Eurasian trade routes\n\nB) The spread of technological innovations across regions in Eurasia\n\nC) The geographic extent of the monetization of Eurasian economies\n\nD) The extent to which government economic policies in Eurasia in the period 600-1450\nrepresented a continuity of earlier policies\n\nHistory: World History\n\nTable Table2z\nGigi Ge iad Cama im A bunie Cactal puou GiicA sau Cals EGUMD Maan, ORIGINS OF THE COINS IN A VIKING BURIED CACHE FROM CIRCA 900 C.E., FOUND IN\n‘XI'AN, CENTRAL CHINA Boy NORTHWESTERN ENGLAND\nOrigin of the Coins Date of the Coins (approximate) — Number of Coins Origin of the Coins Number of Coins (approximate)\nChinese, pre-dating the Tang dynasty 500 8.c.2.-550.c.£. 19 Viking kingdoms in northern England 5000\n‘Anglo-Saxon kingdoms in southern England\nEarly Tang dynasty 600-750 ce. 451 ‘a \" \" 2000\nCarolingian Frankish Empire 1,000\nSassanian dynasty, Persia 600 cx 1\nViking states in Scandinavia 50\nByzantine Empire 600 ce. 1\ncies se Abbasid Caliphate 50\nCity of Turfan, Central Asia 650s. 1\nPapacy and Northern Italian states 20\nJapan, Nara period 710 ce. 5 Byzantine Empire 1\nTOTAL 478 TOTAL 7200\n<image 1> <image 2>\n\nGPT-4V: The tables provide information on the origin of coins found in buried caches in different\nregions and their approximate dates. This kind of information indicates where coins from different\nregions and time periods ended up, which suggests patterns of trade or movement of goods,\nservices, and people. Given the options:(A) The diffusion of cultural traditions along Eurasian trade\nroutes: While the movement of coins can suggest trade routes and interactions between different\nregions, it does not directly provide information about the diffusion of cultural traditions. (B) The\nspread of technological innovations across regions in Eurasia: The tables do not specifically address\ntechnological innovations. (C) The geographic extent of the monetization of Eurasian economies:\nThe presence of coins from various regions in different locations suggests areas where money was\nbeing used in trade and therefore indicates the extent of monetization. (D) The extent to which\ngovernment economic policies in Eurasia in the period 600-1450 represented a continuity of earlier\npolicies: The tables show where coins from different periods were found, but they do not\nspecifically provide information on government policies or whether those policies represented\ncontinuity. Given the information in the tables, the best answer would be:\n\n(C) The geographic extent of the monetization of Eurasian economies.\n\nGround Truth: (C) The geographic extent of the monetization of Eurasian economies.\n\nFigure 67. A sample correct case of History (subfield: World History).\nBack to List of Figures | Back to Table Index\n\n81\n", "vlm_text": "The image shows a green circle with a black outline containing a white check mark or tick.\nHistory: World History \nQuestion:  A historian researching the economic history of Eurasia in the period circa 600-1450 c.e. would most likely find the two tables (<image  $_{1>}$   and <image  $2{>}$  ) useful as a source of information about which of the following?\n\n \nOption: \n(A) The diffusion of cultural traditions along Eurasian trade routes\n\n (B) The spread of technological innovations across regions in Eurasia\n\n (C) The geographic extent of the monet iz ation of Eurasian economies\n\n (D) The extent to which government economic policies in Eurasia in the period 600-1450 represented a continuity of earlier policies \nThe image contains two tables:\n\n**Table 1:**\n- Title: \"Origin of the Coins in a Buried Cache from Circa 750 C.E., Found Near Xi'an, Central China.\"\n- Lists origins, approximate dates, and numbers of coins.\n- Origins include Chinese pre-dating the Tang dynasty, Early Tang dynasty, Sassanian dynasty (Persia), Byzantine Empire, City of Turfan (Central Asia), and Japan (Nara period).\n- Total coins: 478.\n\n**Table 2:**\n- Title: \"Origins of the Coins in a Viking Buried Cache from Circa 900 C.E., Found in Northwestern England.\"\n- Lists origins and approximate numbers of coins.\n- Origins include Viking kingdoms in northern England, Anglo-Saxon kingdoms in southern England, Carolingian Frankish Empire, Viking states in Scandinavia, Abbasid Caliphate, Papacy and Northern Italian states, and Byzantine Empire.\n- Total coins: 7,200.\nGPT-4V:  The tables provide information on the origin of coins found in buried caches in different regions and their approximate dates. This kind of information indicates where coins from different regions and time periods ended up, which suggests patterns of trade or movement of goods, services, and people. Given the options:(A) The diffusion of cultural traditions along Eurasian trade routes: While the movement of coins can suggest trade routes and interactions between different regions, it does not directly provide information about the diffusion of cultural traditions. (B) The spread of technological innovations across regions in Eurasia: The tables do not specifically address technological innovations. (C) The geographic extent of the monet iz ation of Eurasian economies: The presence of coins from various regions in different locations suggests areas where money was being used in trade and therefore indicates the extent of monet iz ation. (D) The extent to which government economic policies in Eurasia in the period 600-1450 represented a continuity of earlier policies: The tables show where coins from different periods were found, but they do not specifically provide information on government policies or whether those policies represented continuity. Given the information in the tables, the best answer would be: \n(C) The geographic extent of the monet iz ation of Eurasian economies. \nGround Truth: (C) The geographic extent of the monet iz ation of Eurasian economies. "}
{"page": 81, "image_path": "doc_images/2311.16502v3_81.jpg", "ocr_text": "@ History: Modern History\n\nQuestion: In the political cartoon, the United States is seen as fulfilling which of the following\nroles?<image 1>\n\nOption:\n\n(A) Oppressor\n\n(B) Imperialist\n\n(C) Savior\n\n(D) Isolationist\n\nWHAT THE UNITED STATES HAS FOUGHT FOR|\n\nGPT-4V: Based on the political cartoon, the United States is seen as fulfilling the role of:\n(B) Imperialist.\n\nError Category: Perceptual Error\n\nError Reason: GPT-4V wrongly interprets the image, because it doesn’t correctly interpret the\ncontradiction between the comics on the top half and the bottom half. The perspective of the\ncartoonist is that the United States has been a savior to the nations brought under its control, saving\nthem from poverty or oppression and bringing them trade, prosperity, and democratic\ngovernments. This mistake also indicates that GPT-4V holds some wrong political beliefs.\n\nGround Truth: (C) Savior\n\nExplanation: The perspective of the cartoonist is that the United States has been a savior to the\nnations brought under its control, saving them from poverty or oppression and bringing them\ntrade, prosperity, and democratic governments. Although one might be tempted to cast the United\nStates in the role of imperialist (B), the purpose of the cartoon is to highlight the positive\ntransformation of the people due to their \"rescue\" by the United States, rather than the benefits to\nthe United States. Because the cartoon claims the people are better off for having been \"rescued\" by\nthe United States, the United States is not seen as an oppressor (A). Since isolationists do not\nsupport foreign intervention, (D) cannot be the correct answer.\n\nFigure 68. A sample error case of History (subfield: Modern History). Error category: Perceptual Error\nBack to List of Figures | Back to Table Index\n\n82\n", "vlm_text": "History: Modern History \nQuestion:  In the political cartoon, the United States is seen as fulfilling which of the following roles?<image  $_{1>}$  \nOption :\n\n (A) Oppressor\n\n (B) Imperialist\n\n (C) Savior\n\n (D) Isolationist \nThis image is a political cartoon titled \"What the United States Has Fought For.\" It consists of two panels:\n\n1. The top panel, labeled \"Before the United States intervened in behalf of these oppressed people,\" shows figures labeled as \"Philippines,\" \"Hawaii,\" \"Porto Rico,\" \"Cuba,\" and \"Isthmian Canal.\" They are depicted as oppressed, carrying heavy burdens like \"Spanish Oppression\" and \"Industrial Slavery.\"\n\n2. The bottom panel, labeled \"After the United States had rescued them from their oppression,\" shows the same figures now dressed in formal attire and appearing prosperous.\n\nThe cartoon reflects themes of U.S. imperialism and the narrative of intervention for liberation during the late 19th to early 20th century.\nGPT-4V:  Based on the political cartoon, the United States is seen as fulfilling the role of: (B) Imperialist. \nError Category: Perceptual Error \nError Reason:  GPT-4V wrongly interprets the image, because it doesn‘t correctly interpret the contradiction between the comics on the top half and the bottom half. The perspective of the cartoonist is that the United States has been a savior to the nations brought under its control, saving them from poverty or oppression and bringing them trade, prosperity, and democratic governments. This mistake also indicates that GPT-4V holds some wrong political beliefs. \nGround Truth: (C) Savior \nExplanation:  The perspective of the cartoonist is that the United States has been a savior to the nations brought under its control, saving them from poverty or oppression and bringing them trade, prosperity, and democratic governments. Although one might be tempted to cast the United States in the role of imperialist (B), the purpose of the cartoon is to highlight the positive  transformation of the people due to their \"rescue\" by the United States, rather than the benefits to  the United States. Because the cartoon claims the people are better off for having been \"rescued\" by the United States, the United States is not seen as an oppressor (A). Since isolationists do not support foreign intervention, (D) cannot be the correct answer. "}
{"page": 82, "image_path": "doc_images/2311.16502v3_82.jpg", "ocr_text": "@ History: World History\n\nQuestion: British economic interactions with India during the time span covered in the chart above\nwould have been chiefly concerned with which of the following commodities? <image 1>\n\nOption:\n\n(A) Cotton\n\n(B) Spices\n\n(C) Ivory\n\n(D) Opium\n\nWORLD GDP, 1600-1870 (in millions of dol-\nlars, calculated to 1990 value)\n\nShare Share Share\nof of of\nWorld World World\n1600 Total 1700 Total 1870 Total\nGreat 6,007 1.8% | 10,709 | 2.88% | 100,179 | 9.1%\nBritain\n\nWestern | 65,955 | 20% | 83,395 | 22.5% | 370,223 | 33.61%\nEurope\n(excluding\nBritain)\n\nChina 96,000 | 29% | 82,800 | 22.3% | 189,740 17.23%\n\nIndia 74,250 | 23% | 90,750 | 24.4% | 134,882 12.25%\n\nWORLD | 329,417 371,369 1,101,369\nTOTAL\n\nSource: Angus Maddison, Contours of the World Economy,\n2007\n\n<image 1>\n\nGPT-4V: The correct answer would be:(D) Opium. During the time span covered in the chart (1600-\n1870), the British economic interactions with India were heavily influenced by the opium trade. The\nBritish East India Company established a monopoly on the opium trade and exported opium to\nChina, leading to the Opium Wars with China. This was a significant aspect of Britain's economic\nengagement with India during this period.\n\nError Category: Lack of Knowledge\n\nError Reason: This question calls for knowledge related to the stimulus material. Understanding the\nimportance of Indian cotton as a trade commodity is key to contextualizing this question. However,\nGPT-4V doesn't know that.\n\nGround Truth: (A) Cotton\n\nExplanation: Likewise, this question calls for knowledge related to the stimulus material.\nUnderstanding the importance of Indian cotton as a trade commodity is key to contextualizing this\nquestion.\n\nFigure 69. A sample error case of History (subfield: World History). Error category: Lack of Knowledge\nBack to List of Figures | Back to Table Index\n\n83\n", "vlm_text": "The image shows a red circle with a white \"X\" inside it, often used to indicate an error, a cancel action, or something incorrect.\nHistory: World History \nQuestion:  British economic interactions with India during the time span covered in the chart above would have been chiefly concerned with which of the following commodities? <image  $1\\!>\n\n$  \nOption: \n(A) Cotton\n\n (B)  Spices\n\n (C)  Ivory\n\n (D)  Opium \nThe table shows the world GDP from 1600 to 1870 (in millions of dollars, calculated to 1990 value) for Great Britain, Western Europe (excluding Britain), China, and India. It also provides their respective shares of the world total GDP for the years 1600, 1700, and 1870. Here are the key details:\n\n- **Great Britain:**\n  - 1600: $6,007 million (1.8%)\n  - 1700: $10,709 million (2.88%)\n  - 1870: $100,179 million (9.1%)\n\n- **Western Europe (excluding Britain):**\n  - 1600: $65,955 million (20%)\n  - 1700: $83,395 million (22.5%)\n  - 1870: $370,223 million (33.61%)\n\n- **China:**\n  - 1600: $96,000 million (29%)\n  - 1700: $82,800 million (22.3%)\n  - 1870: $189,740 million (17.23%)\n\n- **India:**\n  - 1600: $74,250 million (23%)\n  - 1700: $90,750 million (24.4%)\n  - 1870: $134,882 million (12.25%)\n\n- **WORLD TOTAL:**\n  - 1600: $329,417 million\n  - 1700: $371,369 million\n  - 1870: $1,101,369 million\n\nThe source of the data is Angus Maddison's \"Contours of the World Economy, 2007\".\nGPT-4V:  The correct answer would be:(D) Opium. During the time span covered in the chart (1600- 1870), the British economic interactions with India were heavily influenced by the opium trade. The British East India Company established a monopoly on the opium trade and exported opium to China, leading to the Opium Wars with China. This was a significant aspect of Britain's economic engagement with India during this period. \nError Category: Lack of Knowledge \nError Reason:  This question calls for knowledge related to the stimulus material. Understanding the importance of Indian cotton as a trade commodity is key to contextual i zing this question. However, GPT-4V doesn't know that. \nGround Truth: (A) Cotton \nExplanation:  Likewise, this question calls for knowledge related to the stimulus material. Understanding the importance of Indian cotton as a trade commodity is key to contextual i zing this  question. \nFigure 69. A sample error case of History (subfield: World History). Error category: Lack of Knowledge Back to List of Figures  Back to Table Index "}
{"page": 83, "image_path": "doc_images/2311.16502v3_83.jpg", "ocr_text": "eZ Literature: Contemporary Literature\n\nQuestion: Which of the following options best relates to the title 'Ancestor’ <image 1>\nOption:\n\n(A) The danger of letting go of a dream.\n\n(B) The father is compared to a gypsy.\n\n(©) A mother tells her son to stop whining.\n\n(D) These women want their children to be educated.\n\nGPT-4V: The image depicts what seems to be an old illustration with the title \"A Gipsy Family\" and\nis described as a facsimile of a woodcut. Given the choices provided: (A) The danger of letting go of\na dream.(B) The father is compared to a gypsy. (C) A mother tells her son to stop whining. (D)\nThese women want their children to be educated. Option (B) \"The father is compared to a gypsy.\"\nappears to best relate to the title and content of the image.\n\nGround Truth: (B) The father is compared to a gypsy.\n\nFigure 70. A sample correct case of Literature (subfield: Contemporary Literature).\nBack to List of Figures | Back to Table Index\n\n84\n", "vlm_text": "The image shows a green check mark inside a black circle, often used to indicate that something is correct or has been successfully completed.\nLiterature: Contemporary Literature \nQuestion:  Which of the following options best relates to the title 'Ancestor’ <image  $1\\!>$  Option:\n\n \n(A)  The danger of letting go of a dream.\n\n (B)  The father is compared to a gypsy.\n\n (C)  A mother tells her son to stop whining.\n\n (D)  These women want their children to be educated. \nThe image is an illustration of three people in what appears to be traditional attire. The figure on the left is wearing a hat and a cloak, gesturing towards the person in the middle. The central figure is a smaller person holding up an object, possibly a toy or tool. The person on the right is draped in a patterned cloth and is barefoot. The style suggests it might be a historical or cultural depiction.\nGPT-4V:  The image depicts what seems to be an old illustration with the title \"A Gipsy Family\" and is described as a facsimile of a woodcut. Given the choices provided: (A) The danger of letting go of a dream.(B) The father is compared to a gypsy. (C) A mother tells her son to stop whining. (D) These women want their children to be educated. Option (B) \"The father is compared to a gypsy.\" appears to best relate to the title and content of the image. \nGround Truth: (B) The father is compared to a gypsy. \nFigure 70. A sample correct case of Literature (subfield: Contemporary Literature). Back to List of Figures  Back to Table Index "}
{"page": 84, "image_path": "doc_images/2311.16502v3_84.jpg", "ocr_text": "@ Literature: Comparative Literature\n\nQuestion: Refer to the image, which term best matches the given set of characteristics?\n\nOption:\n\n(A) Common elements in horror stories\n(B) Literary elements in horror\n\n(C) Scary plot twists\n\n(D) Intensity\n\n<image 1>\n\nGPT-4V: Based on the description <image 1>, the term that best matches the given set of\ncharacteristics is:\n(D) Intensity.\n\nError Category: Perceptual Error\n\nError Reason: GPT-4V did not capture the expressions of the characters in the painting or the\nchopping action of the person on the far left, focusing instead only on the muscles and movements\nof the character on the far right. As a result, GPT-4V failed to recognize the elements were\nterrifying.\n\nGround Truth: (A) Common elements in horror stories\n\nFigure 71. A sample error case of Literature (subfield: Comparative Literature). Error category: Perceptual Error\n\n85\n", "vlm_text": "The image shows a red circle with a black outline containing a white \"X\" in the center. It is typically used to represent an error, a cancellation, or something that is incorrect.\nLiterature: Comparative Literature \nQuestion:  Refer to the image, which term best matches the given set of characteristics?\n\n \nOption: \n(A) Common elements in horror stories\n\n (B) Literary elements in horror\n\n (C) Scary plot twists\n\n (D) Intensity \nThis image is a painting showing a dramatic scene with three figures. Two muscular men are threatening a woman, who appears distressed. The setting is outdoors with a dark, wooded background. The figures' clothing and setting suggest a historical or fictional narrative.\nGPT-4V:  Based on the description <image   $_{1>}$  , the term that best matches the given set of characteristics is: \n(D) Intensity. \nError Category: Perceptual Error \nError Reason:  GPT-4V did not capture the expressions of the characters in the painting or the chopping action of the person on the far left, focusing instead only on the muscles and movements of the character on the far right. As a result, GPT-4V failed to recognize the elements were terrifying. \nGround Truth: (A) Common elements in horror stories "}
{"page": 85, "image_path": "doc_images/2311.16502v3_85.jpg", "ocr_text": "2 Sociology: Sociology Theory\n\nQuestion: For the person in <image 1>, what is the root of everything social?\nOption:\n\n(A) Collective Effervescence\n\n(B) Society\n\n(C) The Economy\n\n(D) Religion\n\n<image 1>\n\nGPT-4V: For Emile Durkheim, one of the founding figures in sociology, the root of everything\nsocial could be argued to be \"Collective Effervescence,\" a term he coined. This concept refers to the\nmoments of communal passion and ecstasy that occur during group rituals and events, which\nDurkheim believed were the basis for the solidarity of social groups and ultimately for society itself.\nIt's a foundational element in his theory on the formation of religions and societies.\n\nThe correct answer to the question, considering Emile Durkheim's sociological theories, is\n\"Religion.\" Durkheim viewed religion as a fundamental social institution that is central to the\nformation and sustenance of societies. He believed that religious beliefs and rituals play a critical\nrole in creating social cohesion and collective conscience.\n\nGround Truth: (D) Religion\n\nExplanation: Durkheim believed that religion is the root of everything social. He argued that\nreligion provides the framework and moral values that bind individuals together and create social\ncohesion. Through religious rituals and beliefs, people experience a sense of collective effervescence,\na heightened state of group solidarity and shared emotions. This collective effervescence reinforces\nsocial norms, strengthens social bonds, and gives individuals a sense of belonging to a larger\ncommunity. Therefore, Durkheim viewed religion as the foundation of social order and the driving\nforce behind societal cohesion.\n\nFigure 72. A sample correct case of Sociology (subfield: Sociology Theory).\nBack to List of Figures | Back to Table Index\n\n86\n", "vlm_text": "The image shows a green circle with a black checkmark inside it. This typically symbolizes approval or confirmation.\nSociology: Sociology Theory \nQuestion:  For the person in  <image  $1\\!>,$  , what is the root of everything social? Option:\n\n \n(A)  Collective Effervescence\n\n (B)  Society\n\n (C)  The Economy\n\n (D)  Religion \nThe image shows a black and white photo of a man with a beard, wearing round glasses and dressed in a suit.\nGPT-4V:  For Émile Durkheim, one of the founding figures in sociology, the root of everything social could be argued to be \"Collective Effervescence,\" a term he coined. This concept refers to the moments of communal passion and ecstasy that occur during group rituals and events, which Durkheim believed were the basis for the solidarity of social groups and ultimately for society itself. It's a foundational element in his theory on the formation of religions and societies. \nThe correct answer to the question, considering Emile Durkheim's sociological theories, is \"Religion.\" Durkheim viewed religion as a fundamental social institution that is central to the formation and sustenance of societies. He believed that religious beliefs and rituals play a critical role in creating social cohesion and collective conscience. \nGround Truth: (D)  Religion \nExplanation:  Durkheim believed that religion is the root of everything social. He argued that religion provides the framework and moral values that bind individuals together and create social cohesion. Through religious rituals and beliefs, people experience a sense of collective effervescence, a heightened state of group solidarity and shared emotions. This collective effervescence reinforces social norms, strengthens social bonds, and gives individuals a sense of belonging to a larger community. Therefore, Durkheim viewed religion as the foundation of social order and the driving force behind societal cohesion. "}
{"page": 86, "image_path": "doc_images/2311.16502v3_86.jpg", "ocr_text": "@ Sociology: Comparative Literature\n\nQuestion: People holds view in <image 1> argue that crime is linked primarily to___?__\nOption:\n\n(A) Power\n\n(B) Master status\n\n(C) Family values\n\n(D) Wealth\n\n‘Symbolic Interactionism\n\n=\n\nSociety ‘Symbols\n\n¢ Y\n\nCommunicate with Establish\neach other meanings\n\ntheir views\n\n<image 1>\n\nGPT-4V: The diagram you provided outlines the concept of Symbolic Interactionism, which is a\nsociological perspective that focuses on the ways in which individuals interact and communicate\nthrough symbols. Given the choices provided and the context of Symbolic Interactionism, the most\nappropriate answer is: (B) Master status. Master status refers to a social position that is the primary\nidentifying characteristic of an individual. It dominates other statuses and thereby determines a\nperson's general position in society. In the context of crime, some symbolic interactionists might\nargue that an individual's master status (e.g., being labeled as a \"criminal\") could influence their\ninteractions and how they are perceived by others in society.\n\nError Category: Reasoning Error\n\nError Reason: The answer Master status fails to find the root cause of conducting crimes.\nEssentially, an individual who grows up in a poor neighbourhood with high rates of drug use,\nviolence, teenage delinquency, and deprived parenting is more likely to become a criminal than an\nindividual from a wealthy neighbourhood with a good school system and families who are\n\ninvolved positively in the community. Therefore, the primary/core reason is wealth. The answer is\nD.\n\nGround Truth: (D) Wealth\n\nFigure 73. A sample error case of Sociology (subfield: Comparative Literature). Error category: Reasoning Error\nBack to List of Figures | Back to Table Index\n\n87\n", "vlm_text": "The image shows a red circle with a black border and a white \"X\" in the center. This symbol is commonly used to indicate an error or a negative action, like \"cancel\" or \"close.\"\nSociology: Comparative Literature \nQuestion:  People holds view in <image  $_{1>}$   argue that crime is linked primarily to ___?____. \n\n(A) Power (B) Master status (C) Family values (D) Wealth \nThe image is a circular diagram illustrating the concept of Symbolic Interactionism. It shows a cycle that includes the following steps:\n\n1. Symbols\n2. Establish meanings\n3. Develop their views\n4. Communicate with each other\n5. Society\n\nEach step is connected with arrows, indicating a continuous process.\nGPT-4V:  The diagram you provided outlines the concept of Symbolic Interactionism, which is a sociological perspective that focuses on the ways in which individuals interact and communicate through symbols. Given the choices provided and the context of Symbolic Interactionism, the most appropriate answer is: (B) Master status. Master status refers to a social position that is the primary identifying characteristic of an individual. It dominates other statuses and thereby determines a person's general position in society. In the context of crime, some symbolic interaction is ts might argue that an individual's master status (e.g., being labeled as a \"criminal\") could influence their interactions and how they are perceived by others in society. \nError Category: Reasoning Error \nError Reason:  The answer Master status fails to find the root cause of conducting crimes. Essentially, an individual who grows up in a poor neighbourhood with high rates of drug use, violence, teenage delinquency, and deprived parenting is more likely to become a criminal than an individual from a wealthy neighbourhood with a good school system and families who are involved positively in the community. Therefore, the primary/core reason is wealth. The answer is D. \nGround Truth: (D) Wealth \nFigure 73. A sample error case of Sociology (subfield: Comparative Literature). Error category: Reasoning Error Back to List of Figures  Back to Table Index "}
{"page": 87, "image_path": "doc_images/2311.16502v3_87.jpg", "ocr_text": "~~) Psychology: Biological Psychology\n\nQuestion: Described by the psychologist B. F. Skinner, operant conditioning posits that learning\ncan be understood by the interaction between reinforcers and stimuli. Studies from both\ncomparative and human psychology have supported Skinner's initial findings and expanded upon\nthem. The following figure shows a hypothetical example of various operant conditioning trials.\nDuring these trials, when a rat pressed a bar (as indicated by the hash mark on the graph), the\nbehavior was reinforced by the presentation of a food pellet. The reinforcement schedule differs in\neach cell of the figure. Time is displayed on the x-axis and cumulative responses (bar presses) is\ndisplayed on the y-axis. ([image 1]) Imagine that you are watching a rat complete one of the tasks in\nthe figure above. The rat is pressing the bar at a very high rate and takes a short break from\npressing the bar each time the food pellet is delivered. What type of reinforcement schedule are you\nMOST likely watching?\n\nOption:\n\n(A) variable-interval\n\n(B) variable-ratio\n\n(©) fixed-interval\n\n(D) fixed-ratio\n\nFixed Variable\n\nRatio\n\nCumulative\nresponses\n\nInterval\n\nTime\n\n<image 1>\n\nGPT-4V: The graph corresponds to operant conditioning schedules. The pattern of high-rate bar\npressing and short breaks after reinforcement suggests a Fixed Ratio schedule, shown in the top left\nquadrant. Here, reinforcement is given after a set number of responses, which is reflected in the\nsteep inclines followed by drops on the graph. The rat's behavior—rapid pressing until receiving a\npellet, then pausing—matches this schedule, indicating it understands that a specific number of\npresses leads to a reward, thus causing the observed high response rate followed by a break.\n\nGround Truth: (D) fixed-ratio\n\nFigure 74. A sample correct case of Psychology (subfield: Biological Psychology).\nBack to List of Figures | Back to Table Index\n\n88\n", "vlm_text": "The image shows a green checkmark inside a circle.\nPsychology: Biological Psychology \nQuestion:  Described by the psychologist B. F. Skinner, operant conditioning posits that learning can be understood by the interaction between reinforce rs and stimuli. Studies from both comparative and human psychology have supported Skinner's initial findings and expanded upon them. The following figure shows a hypothetical example of various operant conditioning trials. During these trials, when a rat pressed a bar (as indicated by the hash mark on the graph), the behavior was reinforced by the presentation of a food pellet. The reinforcement schedule differs in each cell of the figure. Time is displayed on the   $_\\mathrm{x}$  -axis and cumulative responses (bar presses) is displayed on the y-axis. ([image 1]) Imagine that you are watching a rat complete one of the tasks in the figure above. The rat is pressing the bar at a very high rate and takes a short break from pressing the bar each time the food pellet is delivered. What type of reinforcement schedule are you MOST likely watching? \nOption:\n\n (A) variable-interval\n\n (B) variable-ratio\n\n (C) fixed-interval\n\n (D) fixed-ratio \nThe image contains four graphs illustrating different operant conditioning schedules based on the axes labeled \"Cumulative responses\" and \"Time.\" \n\n- **Top Left (Fixed Ratio)**: This graph shows steep inclines followed by drops, indicating reinforcement after a set number of responses.\n- **Top Right (Variable Ratio)**: This graph shows a consistent, steep slope, suggesting reinforcement after a variable number of responses, leading to a high and steady rate of response.\n- **Bottom Left (Fixed Interval)**: This graph shows a scalloped pattern, where responses increase as the time for reinforcement approaches.\n- **Bottom Right (Variable Interval)**: This graph shows a relatively steady and moderate slope, indicating reinforcement after varying time intervals, leading to consistent response rates.\n\nThe caption explains that the upper left graph corresponds to a Fixed Ratio schedule, characterized by high-rate bar pressing with short breaks after reinforcement.\nGround Truth: (D) fixed-ratio "}
{"page": 88, "image_path": "doc_images/2311.16502v3_88.jpg", "ocr_text": "@ Psychology: Clinical Psychology\n\nQuestion: Guided meditation and deep-breathing exercises have long been used as effective\ntechniques for stress reduction. The mechanism of action for this non-pharmacologic intervention is\nnot entirely known, but scientists believe that the act of focusing one’s thoughts and deep belly-\nbreathing both serve to somehow inhibit the stress response activated by the hypothalamic-\npituitary-adrenal axis. Irritable Bowel Syndrome (IBS) is a disorder that causes a range of\nabdominal discomfort and bowel irregularities, but unlike bowel diseases with similar symptoms,\nthere are no physical abnormalities; rather, the disorder appears to be the physical manifestation of\npsychological triggers. The current standard non-pharmacologic treatment for IBS is cognitive\nbehavior therapy (CBT). CBT treats IBS sufferers by treating the emotional and psychological\ntriggers that cause physical symptoms. In an attempt to determine if meditation is as beneficial as\nCBT for treating IBS, a recent six-month study was conducted on female IBS sufferers. Eligible\nparticipants had active IBS symptoms for at least three months during the past year. Participants\nwith and without a diagnosed anxiety disorder were recruited to participate in this study. Subjects\nwere randomly assigned to one of three groups: a CBT group, a guided-meditation group, and a no-\ntreatment group. Approximately 65% of the participants had an anxiety disorder, and these subjects\nwere roughly equally represented in each of the three groups. The results of this study, measured\nby percent reduction of IBS symptoms after treatment, are summarized in Figure 1. <image 1>\nBased on the results of this study, what can be most reasonably concluded about the efficacy of CBT\nfor IBS sufferers who do not have an anxiety disorder?\n\nOption:\n\n(A) CBT is more effective than no treatment and more effective than meditation.\n\n(B) CBT and meditation combined provide the most effective treatment possible.\n\n(C) CBT is not as effective as meditation.\n\n(D) CBT is equally effective for IBS sufferers with and without anxiety disorders.\n\nwing Treatment\n\n<image 1>\n\nGPT-4V: Based on Figure 1: For IBS sufferers without an anxiety disorder: CBT shows significant\nsymptom reduction. Meditation shows some symptom reduction, but it's less than CBT. No\ntreatment shows very minimal symptom reduction. Thus, for IBS sufferers who do not have an\nanxiety disorder, CBT is more effective than both meditation and no treatment. The answer is: (A)\nCBT is more effective than no treatment and more effective than meditation.\n\nError Category: Perceptual Error\n\nError Reason: Based on the description from GPT-4V: \"For IBS sufferers without an anxiety\ndisorder: CBT shows significant symptom reduction. Meditation shows some symptom reduction,\nbut it's less than CBT. No treatment shows very minimal symptom reduction.\" It appears that GPT-\n4v misinterpreted the black bars in the plot as the \"Non-Anxiety Disorder\" group. However, the\nactual \"Non-Anxiety Disorder\" group is represented by the gray bars as shown in the legend.\n\nGround Truth: (C) CBT is not as effective as meditation.\n\nFigure 75. A sample error case of Psychology (subfield: Clinical Psychology). Error category: Perceptual Error\nBack to List of Figures | Back to Table Index\n\n89\n", "vlm_text": "This image shows a red circle with a black border and a white \"X\" in the center. It typically represents a \"cancel\" or \"error\" icon.\nPsychology: Clinical Psychology \nQuestion:  Guided meditation and deep-breathing exercises have long been used as effective techniques for stress reduction. The mechanism of action for this non-p harm a co logic intervention is not entirely known, but scientists believe that the act of focusing one’s thoughts and deep belly- breathing both serve to somehow inhibit the stress response activated by the hypothalami c- pituitary-adrenal axis. Irritable Bowel Syndrome (IBS) is a disorder that causes a range of abdominal discomfort and bowel irregularities, but unlike bowel diseases with similar symptoms, there are no physical abnormalities; rather, the disorder appears to be the physical manifestation of psychological triggers. The current standard non-p harm a co logic treatment for IBS is cognitive behavior therapy (CBT). CBT treats IBS sufferers by treating the emotional and psychological triggers that cause physical symptoms. In an attempt to determine if meditation is as beneficial as CBT for treating IBS, a recent six-month study was conducted on female IBS sufferers. Eligible participants had active IBS symptoms for at least three months during the past year. Participants with and without a diagnosed anxiety disorder were recruited to participate in this study. Subjects were randomly assigned to one of three groups: a CBT group, a guided-meditation group, and a no- treatment group. Approximately  $65\\%$   of the participants had an anxiety disorder, and these subjects were roughly equally represented in each of the three groups. The results of this study, measured by percent reduction of IBS symptoms after treatment, are summarized in Figure 1. <image   $_{1>}$  Based on the results of this study, what can be most reasonably concluded about the efficacy of CBT for IBS sufferers who do not have an anxiety disorder?\n\n \nOption: \n(A) CBT is more effective than no treatment and more effective than meditation.\n\n (B) CBT and meditation combined provide the most effective treatment possible.\n\n (C) CBT is not as effective as meditation.\n\n \n(D) CBT is equally effective for IBS sufferers with and without anxiety disorders. \nThe image is a bar graph showing the percent reduction of IBS symptoms following treatment. It compares participants with anxiety disorders and those without, by treatment condition. The treatments include CBT, medication, and no treatment. The graph indicates that:\n\n- CBT shows the highest reduction in IBS symptoms for both groups, with nearly 100% for those with anxiety disorders and around 80% for those without.\n- Medication results in a very high reduction for both groups, slightly lower than CBT.\n- No treatment results in a low reduction for both groups, with about 10% for those with anxiety disorders and slightly higher for those without.\n\nThe bars are labeled with black for anxiety disorder and gray for non-anxiety disorder.\nGPT-4V:  Based on Figure 1: For IBS sufferers without an anxiety disorder: CBT shows significant  symptom reduction. Meditation shows some symptom reduction, but it's less than CBT. No  treatment shows very minimal symptom reduction. Thus, for IBS sufferers who do not have an anxiety disorder, CBT is more effective than both meditation and no treatment. The answer is:  (A) CBT is more effective than no treatment and more effective than meditation . \nError Category: Perceptual Error \nError Reason:  Based on the description from GPT-4V: \"For IBS sufferers without an anxiety disorder: CBT shows significant symptom reduction. Meditation shows some symptom reduction, but it's less than CBT. No treatment shows very minimal symptom reduction.\" It appears that GPT- 4v misinterpreted the black bars in the plot as the \"Non-Anxiety Disorder\" group. However, the actual \"Non-Anxiety Disorder\" group is represented by the gray bars as shown in the legend. \nGround Truth: (C) CBT is not as effective as meditation. "}
{"page": 89, "image_path": "doc_images/2311.16502v3_89.jpg", "ocr_text": "eQ Agriculture : Plant Biodiversity and Biotechnology\n\nQuestion: An ein4 mutant never binds ethylene. In the presence of ethylene what phenotype will\nbe observed for the seedling triple response in this ein4 mutant?<image 1>\n\nOption:\n\n(A) Seedlings will become de-etiolated.\n\n(B) Seedlings will grow towards the light.\n\n(C) The ETR1 receptor will activate CTR1 and the seedling triple response.\n\n(D) CTRI1 will be activated and inhibit the seedling triple response.\n\n(E) RANI will provide Cu2+ to the ethylene receptors.\n\n<image 1>\n\nGPT-4V: The image shows a schematic representation of the ethylene signaling pathway in plants.\nIn a typical situation, when ethylene binds to its receptor (ETR1, ERS1, ERS2), it prevents the\nreceptor from activating CTR1, which in turn allows the downstream signaling pathway to proceed,\nleading to the activation of EIN2, EIN3, and the ethylene response (seedling triple response).In the\ncase of the ein4 mutant, which cannot bind ethylene, the pathway from the ethylene receptor to the\ndownstream components would not be initiated. Since EIN4 is presumably an ethylene receptor\nlike ETR1 and the mutant form of EIN4 cannot bind ethylene, this means that the receptor cannot\nturn off the CTR1, leading to the inhibition of the seedling triple response due to the constitutive\nactivation of CTR1.Therefore, the correct answer is:(D) CTR1 will be activated and inhibit the\nseedling triple response. This is because in the ein4 mutant, ethylene cannot bind to EIN4 to inhibit\nCTRI1, thus CTR1 remains active and the signaling pathway is off, preventing the seedling triple\nresponse.\n\nGround Truth: (D) CTR1 will be activated and inhibit the seedling triple response.\n\nFigure 76. A sample correct case of Agriculture (subfield: Plant Biodiversity and Biotechnology).\nBack to List of Figures | Back to Table Index\n\n90\n", "vlm_text": "Agriculture : Plant Biodiversity and Biotechnology \nQuestion:  An ein4 mutant never binds ethylene. In the presence of ethylene what phenotype will be observed for the seedling triple response in this ein4 mutant?<image  $_{1>}$  \nOption: \n(A) Seedlings will become de-etiolated. (B) Seedlings will grow towards the light. (C) The ETR1 receptor will activate CTR1 and the seedling triple response. (D) CTR1 will be activated and inhibit the seedling triple response. (E) RAN1 will provide  $\\mathrm{Cu2+}$   to the ethylene receptors. \nThe image appears to be a pathway diagram related to ethylene signaling. It shows two pathways: \n\n1. **Air pathway**: \n   - Involves receptors like ETR1, ERS1, ETR2, EIN4, and ERS2 interacting with RAN1 and CTR1.\n   - This pathway goes through EIN2 and EIN3 leading to an \"OFF\" state.\n\n2. **Ethylene pathway**: \n   - Similar receptors are involved and also interact with RAN1 and CTR1.\n   - This leads to an \"ON\" state.\n\nThe diagram illustrates the molecular interactions of these components in response to either air or ethylene.\nGPT-4V:  The image shows a schematic representation of the ethylene signaling pathway in plants. In a typical situation, when ethylene binds  to its receptor (ETR1, ERS1, ERS2), it prevents the receptor from activating CTR1, which in turn allows the downstream signaling pathway to proceed, leading to the activation of EIN2, EIN3, and the ethylene response (seedling triple response).In the case of the ein4 mutant, which cannot bind ethylene, the pathway from the ethylene receptor to the downstream components would not be initiated. Since EIN4 is presumably an ethylene receptor like ETR1 and the mutant form of EIN4 cannot bind ethylene, this means that the receptor cannot turn off the CTR1, leading to the inhibition of the seedling triple response due to the constitutive activation of CTR1.Therefore, the correct answer is :  $\\mathrm{(D)}$   CTR1 will be activated and inhibit the  seedling triple response.  This is because in the ein4 mutant, ethylene cannot bind to EIN4 to inhibit CTR1, thus CTR1 remains active and the signaling pathway is off, preventing the seedling triple response. \nGround Truth: (D) CTR1 will be activated and inhibit the seedling triple response. "}
{"page": 90, "image_path": "doc_images/2311.16502v3_90.jpg", "ocr_text": "@ Agriculture: Animal Anatomy\n\nQuestion: <image 1> According to where the arrow points, choosing the matching term:\nOption:\n\n(A) Long pastern bone\n\n(B) Coffin bone\n\n(©) Short pastern\n\n(D) Navicular bone, one bone per digit, located at the interphalangeal joint\n\n<image 1>\n\nGPT-4V: The arrow in the image is pointing to the lower part of the horse's leg, specifically to the\nbone known as the \"short pastern\" or the \"second phalanx\". This is the bone located between the\nlong pastern bone (first phalanx) and the coffin bone (third phalanx) in the anatomy of a horse's leg.\nTherefore, the correct matching term for where the arrow points is: (C): \"short pastern\"\n\nError Category: Perceptual Error\n\nError Reason: GPT 4V give the incorrect interpretation of the visual content in the image. Despite\nhaving the capability to \"see\" the image, the model incorrectly identified the part of the bone\nindicated by the arrow as the short pastern bone when it is actually the long pastern bone. This\nperceptual error could lead to subsequent reasoning errors because an incorrect initial\ninterpretation affects the entire reasoning process that leads to the conclusion. Since the model\ninitially misidentified the bone, all subsequent explanations and reasoning were built on this\nincorrect basis.\n\nGround Truth: (A) Long pastern bone\n\nFigure 77. A sample error case of Agriculture (subfield: Animal Anatomy). Error category: Perceptual Error\nBack to List of Figures | Back to Table Index\n\n91\n", "vlm_text": "The image displays a red circle with a white \"X\" or cross symbol inside it. This symbol often represents a negative action, such as a deletion, error, or cancellation.\nAgriculture: Animal Anatomy \nQuestion:  <image  $_{1>}$   According to where the arrow points, choosing the matching term: Option:\n\n \n(A) Long pastern bone\n\n (B) Coffin bone\n\n (C) Short pastern\n\n (D) Navicular bone, one bone per digit, located at the inter pha lange al joint \nThe image shows a three-dimensional reconstruction of the medial aspect of the carpometacarpal joint of the thumb. The red arrow points to an osteophyte within the metacarpal bony gutter that corresponds to the beak ligament attachment. An osteophyte is a bony outgrowth that can be associated with joint degeneration or arthritis.\nGPT-4V:  The arrow in the image is pointing to the lower part of the horse's leg, specifically to the bone known as the \"short pastern\" or the \"second phalanx\". This is the bone located between the  long pastern bone (first phalanx) and the coffin bone (third phalanx) in the anatomy of a horse's leg. Therefore, the correct matching term for where the arrow points is: (C): \"short pastern\" \nError Category: Perceptual Error \nError Reason:  GPT 4V give the incorrect interpretation of the visual content in the image. Despite having the capability to \"see\" the image, the model incorrectly identified the part of the bone indicated by the arrow as the short pastern bone when it is actually the long pastern bone. This perceptual error could lead to subsequent reasoning errors because an incorrect initial interpretation affects the entire reasoning process that leads to the conclusion. Since the model initially misidentified the bone, all subsequent explanations and reasoning were built on this incorrect basis. \nGround Truth: (A) Long pastern bone \nFigure 77. A sample error case of Agriculture (subfield: Animal Anatomy). Error category: Perceptual Error Back to List of Figures  Back to Table Index "}
{"page": 91, "image_path": "doc_images/2311.16502v3_91.jpg", "ocr_text": "@ Agriculture: Plant Physiology\n\nQuestion: <image 1> Found near the top of the leaf; cells in this layer contain many chloroplasts.\nChoosing the matching term:\n\nOption:\n\n(A) Stomata\n\n(B) Spongy mesophyll\n\n(C) Guard cells\n\n(D) Waxy cuticle\n\n<image 1>\n\nGPT-4V: The layer indicated by the arrow in the image is the uppermost layer of the leaf and is\ntypically known for its protective qualities and prevention of water loss. The correct term for this\nlayer is:(D) waxy cuticle.\n\nError Category: Perceptual Error\n\nError Reason: GPT-4V recognized the importance of the uppermost layer's role in protection and\nwater loss prevention but failed to correctly identify the visual signals indicating that the arrow was\npointing to the guard cells, which are not the waxy cuticle. The reasoning process was flawed\nbecause the model did not correctly link the visual information with the correct biological structure.\n\nGround Truth: (C) Guard cells\n\nFigure 78. A sample error case of Agriculture (subfield: Plant Physiology). Error category: Perceptual Error.\nBack to List of Figures | Back to Table Index\n\n92\n", "vlm_text": "The image shows a red circle with a black border and a white \"X\" in the center. This is often used as an icon to indicate an error, a wrong answer, or a close/delete action.\nAgriculture: Plant Physiology \nQuestion:  <image   $_{1>}$   Found near the top of the leaf; cells in this layer contain many chloroplasts. Choosing the matching term: \nOption:\n\n (A) Stomata\n\n (B) Spongy mesophyll\n\n (C) Guard cells\n\n (D) Waxy cuticle \nThe image depicts a cross-section illustration of a plant leaf. It shows the following components:\n\n1. A layer of cells at the top, likely representing the epidermis with a waxy cuticle on the surface.\n2. Structures below that might be part of the vascular system, such as xylem and phloem.\n3. There are guard cells and stomata openings depicted as well, which are involved in gas exchange.\n4. The arrow is likely pointing to specific cell structures or functions related to these components.\n\nThis type of diagram is commonly used to explain leaf anatomy and the various tissues involved in photosynthesis, transport, and protection.\nGPT-4V:  The layer indicated by the arrow in the image is the uppermost layer of the leaf and is typically known for its protective qualities and prevention of water loss. The correct term for this layer is:(D) waxy cuticle. \nError Category: Perceptual Error \nError Reason:  GPT-4V recognized the importance of the uppermost layer's role in protection and water loss prevention but failed to correctly identify the visual signals indicating that the arrow was pointing to the guard cells, which are not the waxy cuticle. The reasoning process was flawed because the model did not correctly link the visual information with the correct biological structure. \nGround Truth: (C) Guard cells "}
{"page": 92, "image_path": "doc_images/2311.16502v3_92.jpg", "ocr_text": "@ Agriculture: Plant Physiology\n\nQuestion: In soy, this stem/root diseases manifests as a brown pit. favored by drought conditions.\nChoosing the matching term: <image 1>\n\nOption:\n\n(A) Stalk rot\n\n(B) Stem rust\n\n(C) Brown stem rot\n\n(D) Charcoal rot\n\n<image 1>\n\nGPT-4V: The image shows symptoms that are consistent Set charcoal rot. This disease often.\nEa eer a\n\nError Category: Perceptual Error\n\nError Reason: GPT 4V maybe solely relied on the text to make a judgment instead of synthesizing\nthe visual cues with the textual description provided in the question. |The text mentioned \"a\nbrown pit\" and conditions \"favored by drought\", which led the model to incorrectly infer the\ndisease as charcoal rot. However, based on the visual symptoms displayed in the image, the disease\naligns more closely with brown stem rot. This type of error highlights a shortfall in the model's\ncapability to use visual signals to identify and differentiate between similar plant diseases.\n\nGround Truth: (C) Brown stem rot\n\nFigure 79. A sample error case of Agriculture (subfield: Plant Physiology). Error category: Perceptual Error\nBack to List of Figures | Back to Table Index\n\n93\n", "vlm_text": "The image shows a red circle with a black outline containing a white \"X\" in the center. This symbol is commonly used to indicate a mistake, error, or a \"no\" action.\nAgriculture: Plant Physiology \nQuestion:  In soy, this stem/root diseases manifests as a brown pit. favored by drought conditions. Choosing the matching term: <image  $_{1>}$  \nOption:\n\n (A) Stalk rot\n\n (B) Stem rust\n\n (C) Brown stem rot\n\n (D) Charcoal rot \nThe image shows a plant with leaves that have yellow and green discoloration, possibly indicating a nutrient deficiency or disease. The stem also appears to be damaged or affected.\nGPT-4V:  The image shows symptoms that are consistent with (D) charcoal rot. This disease often  manifests as brown pitting on the stem and root and is favored by drought conditions. \nError Category: Perceptual Error \nError Reason:  GPT 4V maybe solely relied on the text to make a judgment instead of synthesizing the visual cues with the textual description provided in the question. The text mentioned \"a brown pit\" and conditions \"favored by drought\", which led the model to incorrectly infer the disease as charcoal rot. However, based on the visual symptoms displayed in the image, the disease aligns more closely with brown stem rot. This type of error highlights a shortfall in the model's capability to use visual signals to identify and differentiate between similar plant diseases. \nGround Truth: (C) Brown stem rot \nFigure 79. A sample error case of Agriculture (subfield: Plant Physiology). Error category: Perceptual Error Back to List of Figures  Back to Table Index "}
{"page": 93, "image_path": "doc_images/2311.16502v3_93.jpg", "ocr_text": "eZ Architecture and Engineering: Surveying and Mapping\n\nQuestion: Two consecutive photographs were taken with a camera of focal length 200 mm mounted\non an airplane flying at a height of 1500 m. The overlap was exactly 60% and the size of prints was\n250 mm 250 mm. The flying height was same in case of both the exposures and the flight was\nbalanced so that there was no drift. The ground was flat and was 250 m above the mean sea level.\nDetermine the scale of the photograph and the length of the air base.<image 1>\n\nOption:\n\n(A) The scale of the photographs is 1 cm = 62.5 m; Air base = 625 m\n\n(B) The scale of the photographs is 1 cm = 64.5 m; Air base = 625 m\n\n(©) The scale of the photographs is 1 cm = 66.5 m; Air base = 625 m\n\n(D) The scale of the photographs is 1 cm = 68.5 m; Air base = 625 m\n\ntome teem\n\n|-pirbase—+|\n\n}-— 0.4.—+}-—— 0.64 —+|\n| . —_\n\n<image 1>\n\nGPT-4V: Given: Focal length, f = 200 mm Flight height, H = 1500 m, Size of the prints = 250 mm x\n250 mm, Ground height = 250 m, above mean sea level Overlap = 60%. To determine the scale of the\nphotograph and the length of the air base, we can use the following formulas: Photo Scale (S): $ =\nf/(H — h) . Where: S = Photo Scale, f = Focal length, H = Flight height, h = Ground height. Air Base\n(B): Given the overlap is 60%, it means that 40% of the length is new coverage. B = 0.4L Where: L =\nLength of the photograph on the ground. First, let's determine the photo scale (S): S = f/(H — h),\nS = 200mm/(1500m — 250m), S = 200/1250. S = 0.16mm/m. To convert this into cm and m:\n\nS =0.16mm/1mx1cm/10mm, S = 0.016 cm/m. To find the ground equivalent for 1 cm in the\nphotograph: 1cm in photo = 1m/0.016 = 62.5m. So, the scale of the photograph is 1 cm = 62.5 m.\nNext, to determine the air base (B): B = 0.4L Given the size of the prints is 250 mm (or 0.25 m), this\nrepresents the length of the photo on the ground:\n\nL = 0.25mx62.5m/cm = 15.625m, B = 0.4x15.625m = 6.25m, B= 625m\n\nBased on the calculations, the correct option is: (A) The scale of the photographs is 1 cm = 62.5 m;\nAir base = 625 m.\n\nGround Truth: (A) The scale of the photographs is 1 cm = 62.5 m; Air base = 625 m\n\nFigure 80. A sample correct case of Architecture and Engineering (subfield: Surveying and Mapping).\nBack to List of Figures | Back to Table Index\n\n94\n", "vlm_text": "Architecture and Engineering: Surveying and Mapping \nQuestion:  Two consecutive photographs were taken with a camera of focal length  $200\\:\\mathrm{mm}$   mounted on an airplane flying at a height of  $1500~\\mathrm{m}$  . The overlap was exactly  $60\\%$   and the size of prints was  $250\\,\\mathrm{\\mm}\\ 250\\,\\mathrm{\\mm}$  . The flying height was same in case of both the exposures and the flight was balanced so that there was no drift. The ground was flat and was  $250~\\mathrm{m}$   above the mean sea level. Determine the scale of the photograph and the length of the air base.<image  $_{1>}\n\n$  \nOption: \n(A) The scale of the photographs is  $1\\,\\mathrm{cm}=62.5\\,\\mathrm{m};$  ; Air base  $=625\\:\\mathrm{m}\n\n$  (B) The scale of the photographs is  $1\\,\\mathrm{cm}=64.5\\,\\mathrm{m};$  ; Air base  $=625\\:\\mathrm{m}\n\n$  (C) The scale of the photographs is  $1\\:\\mathrm{cm}=66.5\\:\\mathrm{m},$  ; Air base  $=625\\:\\mathrm{m}\n\n$  (D) The scale of the photographs is   $1\\,\\mathrm{cm}=68.5\\,\\mathrm{m};$  ; Air base  $=625\\:\\mathrm{m}$  \nThe image is a diagram showing two aircraft flying over a surface. Each aircraft is positioned at an angle to the ground, creating triangular shapes. The diagram is labeled with dimensions such as \"Air base,\" \"0.4L,\" \"0.6L,\" and \"L.\" The \"Air base\" is the horizontal distance between the points on the surface under each aircraft. The diagram appears to represent a type of surveying or measurement setup, possibly related to aerial photography or radar.\nGPT-4V:  Given: Focal length,  $\\mathrm{f}=200~\\mathrm{mm}$   Flight height,  $\\mathrm{H}=1500\\;\\mathrm{m},$   Size of the prints  $=250\\ \\mathrm{mm\\x}$   $250\\:\\mathrm{mm}$  , Ground height  $=250\\;\\mathrm{m},$  , above mean sea level Overlap  $=60\\%$  . To determine the scale of the photograph and the length of the air base, we can use the following formulas: Photo Scale (S):  $S=$   $f/(H-h)$   . Where:  $S=$  Photo Scale,  $\\mathrm{f}=$   Focal length,  $\\mathrm{H}=$   Flight height,  $\\mathrm{h}=$   Ground height. Air Base (B): Given the overlap is  $60\\%$  , it means that   $40\\%$   of the length is new coverage.  $\\mathrm{B}=0.4\\mathrm{L}$   Where:  $\\mathrm{L}=$  Length of the photograph on the ground. First, let's determine the photo scale (S):    $S=f/(H-h).$  , 𝑆= 200𝑚𝑚/  $.1500m-250m)$  ) ,  $S=200/1250.\\,\\mathrm{S}=0.16\\mathrm{mm/m}.$  . To convert this into cm and m:  $S=0.16m m/1m{\\times}1c m/10m m,$  ,   $\\mathsf{S}\\,=\\,0.016\\,\\mathrm{\\cm/m}$  . To find the ground equivalent for   $1\\,\\mathrm{\\Omegacm}$   in the photograph:  1𝑐𝑚 𝑖𝑛 𝑝ℎ𝑜𝑡𝑜  $=1m/0.016=62.5m$  . So, the scale of the photograph is   $1\\,\\mathrm{cm}=62.5\\,\\mathrm{m}$  . Next, to determine the air base (B):  $\\mathrm{B}=0.4\\mathrm{L}$   Given the size of the prints is   $250~\\mathrm{mm}$   (or  $0.25~\\mathrm{m})$  ), this represents the length of the photo on the ground: \n $L=0.25m{\\times}62.5m/c m=15.625m$  ,   $B=0.4{\\times}15.625m=6.25m,\\,\\mathrm{B}=625$  Based on the calculations, the correct option is: (A) The scale of the photographs is   $1\\ \\mathrm{cm}=62.5\\ \\mathrm{m};$  Air base  $=625\\:\\mathrm{m}$  . \nGround Truth: (A) The scale of the photographs is  $\\begin{array}{r}{1\\;\\mathbf{cm}=62.5\\;\\mathbf{m};}\\end{array}$   Air base  $\\mathbf{\\lambda}=625\\;\\mathbf{m}$  \nFigure 80. A sample correct case of Architecture and Engineering (subfield: Surveying and Mapping). Back to List of Figures  |  Back to Table Index "}
{"page": 94, "image_path": "doc_images/2311.16502v3_94.jpg", "ocr_text": "eZ Architecture and Engineering: Structural Engineering\n\nQuestion: Compute the horizontal displacement of joint B produced by the 240-kN load in Figure\nP8.10. For all bars, area = 2400 mm?and E = 200 GPa.\n\nOption:\n\n(A) 2.657 mm\n\n(B) 3.657 mm\n\n(C) 4.657 mm\n\n(D) 5.657 mm\n\n+ 8m—-- m—-_ 8 m—]\n\nP8.10\n\n<image 1>\n\nGPT-4V: To solve for the horizontal displacement of joint B due to the 240-kN load, we'll employ\nthe method of joints and consider the effect of the force on the truss members, along with using the\nformula for the elongation/contraction of axially loaded members. The formula to calculate\nelongation or contraction in an axially loaded member is: 6 = = where P is the axial force in the\nmember, L is the length of the member, A is the cross-sectional area of the member, and E is the\nmodulus of elasticity. Given: A = 2400 mm7and E = 200Gpa = 200 x 10% a First, let's find the\n\nmm?\"\nforces in the members by applying the method of joints at joint A and joint D. At Joint A: There are\ntwo non-collinear unknown member forces, making it a solvable joint. Let's solve for AB and AF.\nDF, = 0, Fagcos(45°) — Far = 0. Fy = 0, Fygsin(45°) = 0. Fag = 0 and Fy, = 0. At Joint D: There's\na 240-kKN downward force applied. Let's find DC and DE. YF, = 0, Fpccos(45 ) + Fog — 240 =\n0. (1) Fog = 240 - Fpccos(45°). YF =0, Focsin(45°)=0. Foc =0. From (1), Fog = 240 KN.\nCompute the elongation in the members: For DE (in tension, elongation): 5pg = Binet, —\nAXE\n240x103x8000\n2400x200x103 a.\nelongation at DE is: 53 = Specos(45 ) = 5.657mm. Thus, the answer is: (D) 5.657 mm.\n\n= 8mm. Considering the geometry of the truss, the horizontal component of the\n\nGround Truth: (D) 5.657 mm\n\nFigure 81. A sample correct case of Architecture and Engineering (subfield: Structural Engineering).\nBack to List of Figures | Back to Table Index\n\n95\n", "vlm_text": "Architecture and Engineering: Structural Engineering \nQuestion:  Compute the horizontal displacement of joint B produced by the 240-kN load in Figure P8.10. For all bars, area  $=2400\\;m m^{2}$  and  $\\mathrm{E=200\\,GPa}$  . \nOption:\n\n (A) 2.657 mm\n\n (B) 3.657 mm\n\n (C) 4.657 mm\n\n (D) 5.657 mm \nThe image depicts a truss structure with labeled points A, B, C, D, E, and F. It's divided into three 8-meter sections horizontally and features a vertical 8-meter measurement at point D, where a downward force of 240 kN is applied. The truss appears to be supported at points A and E.\nGPT-4V:  To solve for the horizontal displacement of joint B due to the  $240–\\mathrm{kN}$   load, we'll employ the method of joints and consider the effect of the force on the truss members, along with using the formula for the elongation/contraction of axially loaded members. The formula to calculate elongation or contraction in an axially loaded member is:  $\\begin{array}{r}{\\updelta=\\frac{\\mathrm{PL}}{\\mathrm{AE}},}\\end{array}$  , where  $\\mathrm{P}$   is the axial force in the member, L is the length of the member, A is the cross-sectional area of the member, and E is the modulus of elasticity. Given:  $\\mathrm{A}\\,=\\,2400\\,\\mathrm{mm}^{2}\\mathrm{a}$  and   $\\mathrm{E=200\\,Gp a=200\\times10^{3}\\,\\frac{N}{m m^{2}}}$  . First, let's find the forces in the members by applying the method of joints at joint A and joint D. At Joint A: There are two non-collinear unknown member forces, making it a solvable joint. Let's solve for AB and AF.  $\\begin{array}{r}{\\sum F_{y}=0}\\end{array}$   $F_{A B}c o s\\big(45^{\\circ}\\big)-F_{A F}=0.\\sum F_{x}=0,$   $F_{A B}s i n\\big(45^{\\circ}\\big)=0.$   $F_{A B}=0$   and    $F_{A F}=0$  . At Joint D: There's a 240-kN downward force applied. Let's find DC and DE.    $\\begin{array}{r}{\\sum F_{y}=0}\\end{array}$   $F_{D C}c o s\\big(45^{\\circ}\\big)+F_{D E}-240=$  0. 1   $F_{D E}=240-F_{D C}c o s\\big(45^{\\circ}\\big)$  .  $\\textstyle\\sum F_{x}=0$   $F_{D C}s i n\\big(45^{\\circ}\\big)=0$  .  $F_{D C}=0$   . From (1),  $F_{D E}=240\\;\\mathrm{kN}$  . Compute the elongation in the members: For DE (in tension, elongation): δ  $\\begin{array}{r}{\\delta_{D E}=\\frac{P_{D E}\\times L_{D E}}{A\\times E}=}\\end{array}$   ${\\frac{240{\\times}10^{3}{\\times}8000}{2400{\\times}200{\\times}10^{3}}}=8m m$  . Considering the geometry of the truss, the horizontal component of the elongation at DE is:  $\\delta_{B}=\\delta_{D E}c o s\\big(45^{\\circ}\\big)\\approx5.657m m$  . Thus, the answer is:  (D)  $5.657\\:\\mathrm{mm}$  . \nGround Truth: (D)  $\\mathbf{5.657\\,mm}$  \nFigure 81. A sample correct case of Architecture and Engineering (subfield: Structural Engineering). Back to List of Figures  |  Back to Table Index "}
{"page": 95, "image_path": "doc_images/2311.16502v3_95.jpg", "ocr_text": "@ Architecture and Engineering: Civil Engineering\n\nQuestion: The data shown below were obtained by time-lapse photography on a highway. Use\nregression analysis to fit these data to the Greenshields model and determine, the jam density.\n<image 1>\n\nOption:\n\n(A) kj=110 veh/mi\n\n(B) kj=111 veh/mi\n\n(C) kj=112 veh/mi\n\nSpeed (km/h) Density (veh/km)\n14.2 85\n24.1 70\n30.3 55\n40.1 41\n50.6 20\n55.0 15\n<image 1>\n\nGPT-4V: To determine the jam density using the Greenshields model, we'll employ a linear\nregression approach on the data provided. The Greenshields model relates speed (u) and density (k)\nusing the equation: u = Umax — Umax/KjamXk, where Umax is the maximum speed (which occurs\nwhen density k is zero) and kjam is the jam density. From this equation, we can derive a linear\nrelationship: u = Umax — Umax/kjamXk. If we rearrange the terms, it looks like: u = —Umax/kKjamX\nk + Umax-Here, the term —Umax/kjam is the slope of the line, and umax is the y-intercept. We'll now\nuse the data provided to determine the slope and y-intercept using regression analysis, from which\nwe can then derive kjqam. Let's compute the regression parameters: Given data points: Speed (u):\n14.2, 24.1, 30.3, 40.1, 50.6, 55.0; Density (k): 85, 70, 55, 41, 20, 15. Using regression analysis on this\ndata, the equation of the line relating speed and density would be: u = mk + c, where m is the slope\nand c is the y-intercept. By comparing this to the Greenshields model equation, we can deduce: u =\n—Umax/kjam © = Umax. From the given data: Umax = 55.0(from the highest speed value). Using the\nabove equation for m: kjam = —Umax/u. Once we determine the slope m using the regression\nanalysis, we can solve for kjam. For the purpose of this exercise, I will not be calculating the slope\nusing the regression formula directly (as that would be computationally intensive), but based on the\nlinear relationship implied by the data, the jam density kjgm would be slightly greater than the\nhighest density value provided (85 veh/km). Using this logic, the closest option from the choices\ngiven is: kj = 110 \\, veh/ mi. Therefore, the answer would be: (A) kj = 110 veh/ mi.\n\nError Category: Reasoning Error\n\nError Reason: The model correctly recalls the knowledge, but doesn't calculate it and directly gives\nthe answer that it thinks is the closest one, which is incorrect.\n\nGround Truth: (B) kj=111 veh/mi\n\nFigure 82. A sample error case of Architecture and Engineering (subfield: Civil Engineering). Error category: Reasoning Error\nBack to List of Figures | Back to Table Index\n\n96\n", "vlm_text": "Architecture and Engineering: Civil Engineering \nQuestion:  The data shown below were obtained by time-lapse photography on a highway. Use regression analysis to fit these data to the Green shields model and determine, the jam density. <image  $_{1>}$  \nOption: (A) kj=110 veh/mi (B) kj=111 veh/mi (C) kj=112 veh/mi \nThe table displays two columns: \"Speed (km/h)\" and \"Density (veh/km)\". Here are the values:\n\n- Speed: 14.2, 24.1, 30.3, 40.1, 50.6, 55.0\n- Density: 85, 70, 55, 41, 20, 15\nGPT-4V: To determine the jam density using the Green shields model, we‘ll employ a linear regression approach on the data provided. The Green shields model relates speed (u) and density (k) using the equation:    $u=u_{m a x}-u_{m a x}/k_{j a m}{\\times k},$   where    $u_{m a x}$   is the maximum speed (which occurs when density   $\\mathbf{k}$   is zero) and    $k_{j a m}$   is the jam density. From this equation, we can derive a linear relationship:    $u=u_{m a x}-u_{m a x}/k_{j a m}{\\times}k$  . If we rearrange the terms, it looks like:    $u=-{{u}_{m a x}}/{{k}_{j a m}}\\times$   $k+u_{m a x}$  .Here, the term  $-u_{m a x}/k_{j a m}$   is the slope of the line, and  $u_{m a x}$   is the y-intercept. We'll now use the data provided to determine the slope and y-intercept using regression analysis, from which we can then derive  $k_{j a m}$  . Let's compute the regression parameters: Given data points: Speed (u): 14.2, 24.1, 30.3, 40.1, 50.6, 55.0; Density (k): 85, 70, 55, 41, 20, 15. Using regression analysis on this data, the equation of the line relating speed and density would be:  $\\mathbf{u}=\\mathbf{m}\\mathbf{k}+\\mathbf{c},$   where m is the slope and c is the y-intercept. By comparing this to the Green shields model equation, we can deduce:  $u=$   $-\\,u_{m a x}/k_{j a m},c=u_{m a x}.$  . From the given data:  $u_{m a x}=55.0$  (from the highest speed value). Using the above equation for m:    $k_{j a m}=-u_{m a x}/u$  . Once we determine the slope m using the regression analysis, we can solve for    $k_{j a m}$  . For the purpose of this exercise, I will not be calculating the slope using the regression formula directly (as that would be computationally intensive), but based on the linear relationship implied by the data, the jam density    $k_{j a m}$   would be slightly greater than the highest density value provided   $(85~\\mathrm{{vech/km})}$  . Using this logic, the closest option from the choices given is:  $\\mathrm{kj}=110\\mathrm{\\Omega\\backslash,v e h/m i.}$  . Therefore, the answer would be: (A)  $\\mathrm{kj=110\\,v e h/m i}$  . \nError Category: Reasoning Error \nError Reason:  The model correctly recalls the knowledge, but doesn't calculate it and directly gives the answer that it thinks is the closest one, which is incorrect. \nGround Truth: (B)  $\\mathbf{kj}{=}\\mathbf{1}\\mathbf{1}$   veh/mi "}
{"page": 96, "image_path": "doc_images/2311.16502v3_96.jpg", "ocr_text": "eQ Computer Science: Data Structure and Algorithm\n\nQuestion: Suppose we run Dijkstra's single source shortest-path algorithm on the following edge\nweighted directed graph with vertex P as the source. In what order do the nodes get included into\nthe set of vertices for which the shortest path distances are finalized?<image 1>\n\nOption:\n\n<image 1>\n\nGPT-4V: To determine the order in which the nodes are included using Dijkstra's algorithm, we\nwill start with vertex P and then repeatedly select the vertex with the smallest distance that hasn't\nbeen included yet. Initialize distances: makefile Copy code P = 0, Q=2,R=0,S=0,T=»,U=~,\nBegin with source vertex P:P to Q = 1P to S = 4P to T = 7Now, the smallest distance is for Q. So, Q is\nthe next vertex. For Q:P to Q to R = 2P to Q to S = 5 (But, P to S is 4, so this is not considered)P to Q\nto T = 7 (Same as P to T, so this is not updated)Now, the smallest distance among the remaining\nvertices is for R. So, R is the next vertex. For R:P to Q to R to U = 3P to Q to R toS = 4 (But, P to Sis\nalready 4)Now, the smallest distance among the remaining vertices is for U. So, U is the next vertex.\nFor U:P to Q to R to U to S = 5 (But, P to S is already 4)The remaining vertices are S and T. Between\nthem, S has the smallest distance. So, S is the next vertex. For S:P to S to T = 6 (But, P to T is already\n7 and P to Q to T is also 7)Now, only T remains. So, T is the last vertex. The final order is:, P, Q, R,\nU,S, T. The correct option is:(B) P, Q, R, U,S, T.\n\nGround Truth: (B) P,Q, R, U, S,T\n\nFigure 83. A sample correct case of Computer Science (subfield: Data Structure and Algorithm).\nBack to List of Figures | Back to Table Index\n\n97\n", "vlm_text": "The image shows a green circle with a black border containing a white checkmark. This often symbolizes confirmation, success, or that something has been completed or accepted.\nComputer Science: Data Structure and Algorithm \nQuestion:  Suppose we run Dijkstra's single source shortest-path algorithm on the following edge weighted directed graph with vertex  $\\mathrm{P}$   as the source. In what order do the nodes get included into the set of vertices for which the shortest path distances are finalized?<image  $_{1>}$  \nOption:\n\n (A) P, Q, R, S, T, U\n\n (B) P, Q, R, U, S, T\n\n (C) P, Q, R, U, S, T\n\n (D) P, Q, T, R, U, S \nThe image depicts a directed graph with nodes labeled P, Q, R, S, T, and U. The edges between the nodes have weights:\n\n- P to Q, weight 1\n- P to S, weight 6\n- P to T, weight 7\n- Q to R, weight 1\n- Q to S, weight 4\n- R to S, weight 2\n- R to U, weight 1\n- S to U, weight 2\n- T to S, weight 3\n- T to U, weight 2\n\nThis directed weighted graph shows possible paths and their associated costs between nodes.\nGPT-4V:  To determine the order in which the nodes are included using Dijkstra's algorithm, we will start with vertex  $\\mathrm{P}$   and then repeatedly select the vertex with the smallest distance that hasn't been included yet. Initialize distances: makefile Copy code  $\\mathrm{P}=0,$  ,  $\\mathrm{Q}=\\infty,$  ,  $\\mathrm{R}=\\infty,$  ,  $S=\\infty,$  ,  $\\mathrm{T}=\\infty,$  ,  ${\\mathrm{U}}=\\infty,$  Begin with source vertex P:P to  $\\mathrm{Q=1P}$   to  $S=4\\mathrm{P}$   to  $\\mathrm{T}=7\\mathrm{N}\\mathrm{ov}$  , the smallest distance is for Q. So,  $\\mathrm{Q}$   is the next vertex. For Q:P to  $\\mathrm{Q}$   to   $\\mathrm{R}=2\\mathrm{P}$   to  $\\mathrm{Q}$   to  $S=5$   (But,  $\\mathrm{P}$   to S is 4, so this is not considered)P to  $\\mathrm{Q}$  to  $\\mathrm{T}=7$   (Same as  $\\mathrm{P}$   to  $\\mathrm{T},$   so this is not updated)Now, the smallest distance among the remaining vertices is for R. So, R is the next vertex. For R:P to  $\\mathrm{Q}$   to  $\\mathrm{R}$   to   $\\mathrm{U}=3\\mathrm{P}$   to  $\\mathrm{Q}$   to  $\\mathrm{R}$   to  $S=4$   (But,  $\\mathrm{P}$   to S is already 4)Now, the smallest distance among the remaining vertices is for U. So, U is the next vertex. For U:P to  $\\mathrm{Q}$   to  $\\mathrm{R}$   to  $\\mathrm{U}$   to  $S=5$   (But,   $\\mathrm{P}$   to S is already 4)The remaining vertices are S and T. Between them, S has the smallest distance. So, S is the next vertex. For S:P to S to  $\\mathrm{T}=6$   (But,  $\\mathrm{P}$   to  $\\mathrm{T}$   is already 7 and  $\\mathrm{P}$   to   $\\mathrm{Q}$   to  $\\mathrm{T}$   is also 7)Now, only T remains. So, T is the last vertex. The final order is:, P,  $\\mathrm{Q},$     $\\mathrm{R},$  U, S, T. The correct option is:(B) P, Q, R, U, S, T. \nGround Truth: (B) P, Q, R, U, S, T "}
{"page": 97, "image_path": "doc_images/2311.16502v3_97.jpg", "ocr_text": "(©) Computer Science: Compiler Principle\n\nQuestion: <image 1> shows a DFA. Is it minimal?\n\nOption:\n\n(A) Yes\n\n(B) No\n\n(C) Not sure\n\n<image 1>\n\nGPT-4V: To determine whether a DFA (Deterministic Finite Automaton) is minimal, we generally\nfollow a process to minimize the DFA and see if it results in a DFA with fewer states than the\noriginal. However, without more details regarding the acceptance criteria for each state (like which\nstates are accepting and which are not), we can’t definitively determine minimality from the\ndiagram alone. But, let’s try to infer some information from the given diagram: There’s no\nunreachable state from the start state; every state can be reached. There’s no state with identical\ntransitions for the given inputs which could suggest merging. However, one thing to note is that\ntwo states are equivalent (and can be merged) if they have the same transitions on every input to\nthe same set of states, and they are either both accepting or both non-accepting. Without knowing\nwhich states are accepting, we can’t definitively say if the DFA is minimal or not. Therefore, the\nanswer is: (C) not sure.\n\nError Category: Perceptual Error, Lack of Knowledge\n\nError Reason: GPT-4V chooses the wrong answer of “not sure”, because it does not know “which\nstates are accepting”. However, those states are denoted by a double circle. So, there might be two\nreasons why GPT-4V makes the mistake: (1) it made a perceptual error and failed to recognize the\ndouble circle or missed this important visual signal in the image; (2) it might have recognized there\nis a double circle in the graph, but failed to connect it with the concept of “accept state”, due to lack\nof specific subject knowledge. Apart from this, the model recalled relevant knowledge and applied\nit to derive most steps correctly.\n\nGround Truth: (A) Yes\n\nFigure 84. A sample error case of Computer Science (subfield: Compiler Principle). Error category: Perceptual Error, Lack of Knowledge\nBack to List of Figures | Back to Table Index\n\n98\n", "vlm_text": "The image shows a red circle with a white \"X\" inside it, outlined in black. This is commonly used as an icon to indicate an error, mistake, or a close/delete action.\nComputer Science: Compiler Principle \nQuestion:  <image  $_{1>}$   shows a DFA. Is it minimal? \nOption:\n\n (A) Yes\n\n (B) No\n\n (C) Not sure \nThe image shows a state diagram commonly used in computer science to represent a finite automaton. It includes the following elements:\n\n- Several circles labeled \\(G_1\\), \\(G_4\\), \\(G_5\\), \\(G_6\\), and \\(G_7\\) that represent states.\n- Arrows between these states indicating transitions.\n- Transition labels \"a\" and \"b\" indicating the inputs required to move from one state to another.\n- The state \\(G_1\\) is marked as the start state with an arrow labeled \"start\" pointing to it.\n- Some states, like \\(G_1\\), have an additional circle around them indicating they may be final or accepting states.\n\nThis diagram is likely used to illustrate how the automaton processes strings of the inputs \"a\" and \"b\".\nGPT-4V:  To determine whether a DFA (Deterministic Finite Automaton) is minimal, we generally follow a process to minimize the DFA and see if it results in a DFA with fewer states than the original. However, without more details regarding the acceptance criteria for each state (like which states are accepting and which are not), we can‘t definitively determine minimality from the diagram alone. But, let’s try to infer some information from the given diagram: There‘s no unreachable state from the start state; every state can be reached. There’s no state with identical transitions for the given inputs which could suggest merging. However, one thing to note is that two states are equivalent (and can be merged) if they have the same transitions on every input to the same set of states, and they are either both accepting or both non-accepting. Without knowing  which states are accepting, we can‘t definitively say if the DFA is minimal or not. Therefore, the answer is: (C) not sure. \nError Category: Perceptual Error, Lack of Knowledge \nError Reason:  GPT-4V chooses the wrong answer of “not sure”, because it does not know “which states are accepting”. However, those states are denoted by a double circle. So, there might be two reasons why GPT-4V makes the mistake: (1) it made a perceptual error and failed to recognize the double circle or missed this important visual signal in the image; (2) it might have recognized there is a double circle in the graph, but failed to connect it with the concept of “accept state”, due to lack of specific subject knowledge. Apart from this, the model recalled relevant knowledge and applied it to derive most steps correctly. \nGround Truth: (A) Yes "}
{"page": 98, "image_path": "doc_images/2311.16502v3_98.jpg", "ocr_text": "@ Computer Science: Operating System\nQuestion: Which pair of transactions demonstrates interleaved processing in <image 1>?\n\nOption:\n\n(A) A and B\n\n(B) Band C\n\n(C) Cand D\n\n(D) None of the other answers\n\n<image 1>\n\nGPT-4V: The image depicts a timeline representation of various transactions over time across two\nCPUs (CPU1 and CPU2). Interleaved processing occurs when two transactions are processed\nalternately, meaning that one starts, then the other starts before the first one finishes, and then the\nfirst one might resume, and so on. From the given image:\n\nTransaction C on CPU1 starts and finishes\n\nwithout any interleaving. Transaction D on CPU2 starts and finishes without any interleaving. So,\nbased on the provided options: (A) A and Bi These are sequential on both CPUs, not interleaved (8\nB and C: These are sequential, not interleaved. (C) C and D: These are on separate CPUs and are not\n\ninterleaved with each other. (D) None of the other answers. The correct answer is: (D) None of the\nother answers.\n\nError Category: Perceptual Error\n\nError Reason: GPT-4V recalls the correct knowledge “Interleaved processing occurs when two\ntransactions are processed alternately, meaning that one starts, then the other starts before the first\none finishes, and then the first one might resume, and so on.” However, it fails to interpret the\nimage w.r.t. transaction A and B: “Transaction A on CPU1 starts and finishes without being\ninterleaved with any other transaction. Transaction B on CPU! starts after A finishes and completes\nwithout being interleaved with any other transaction. Transaction A on CPU2 starts and finishes\nwithout any interleaving. Transaction B on CPU2 starts after A finishes and completes without\nbeing interleaved with any other transaction.\" The figure intends to illustrate transaction A and B\nbeing processed concurrently on one CPU in an interleaved fashion. These two processes have\nnothing to do with the CPU1 and CPU2 shown on the right side of the figure. On the perception\nlevel, the model made a wrong connection by saying the first appearance of transaction A is on\nCPU1 and the second appearance is on CPU2 (and the same for transaction B). With that said, the\nfigure is indeed a non-trivial one to interpret and a model might need to know some subject-specific\nconvention to interpret it correctly.\n\nGround Truth: (A) A and B\n\nFigure 85. A sample error case of Computer Science (subfield: Operating System). Error category: Perceptual Error\nBack to List of Figures | Back to Table Index\n\n99\n", "vlm_text": "The image shows a red circle with a black border and a white \"X\" in the center, typically used as an icon to indicate a cancellation, error, or an action that is not allowed.\nComputer Science: Operating System \nQuestion:  Which pair of transactions demonstrates interleaved processing in <image  $_{1>?}$  \nOption:\n\n (A) A and B\n\n (B) B and C\n\n (C) C and D\n\n (D) None of the other answers \nThis image appears to be a Gantt chart or timeline depicting tasks over time across two CPUs labeled CPU₁ and CPU₂. The tasks are labeled A, B, C, and D and occur at specific time intervals marked as t₁, t₂, t₃, and t₄. The horizontal axis represents time progression. Task A and B are shown to be processed sequentially on CPU₁ before a gap and subsequently, tasks C and D run in parallel on CPU₁ and CPU₂, respectively.\nGPT-4V:  The image depicts a timeline representation of various transactions over time across two CPUs (CPU1 and CPU2). Interleaved processing occurs when two transactions are processed alternately, meaning that one starts, then the other starts before the first one finishes, and then the first one might resume, and so on. From the given image: Transaction A on CPU1 starts and finishes  without being interleaved with any other transaction. Transaction B on CPU1 starts after A finishes  and completes without being interleaved with any other transaction. Transaction A on CPU2 starts  and finishes without any interleaving. Transaction B on CPU2 starts after A finishes and completes  without being interleaved with any other transaction. Transaction C on CPU1 starts and finishes without any interleaving. Transaction D on CPU2 starts and finishes without any interleaving. So, based on the provided options: (A) A and B: These are sequential on both CPUs, not interleaved. (B) B and C: These are sequential, not interleaved. (C) C and D: These are on separate CPUs and are not interleaved with each other. (D) None of the other answers. The correct answer is: (D) None of the other answers. \nError Category: Perceptual Error \nError Reason:  GPT-4V recalls the correct knowledge “Interleaved processing occurs when two transactions are processed alternately, meaning that one starts, then the other starts before the first one finishes, and then the first one might resume, and so on.” However, it fails to interpret the image w.r.t. transaction A and B: “Transaction A on CPU1 starts and finishes without being interleaved with any other transaction. Transaction B on CPU1 starts after A finishes and completes without being interleaved with any other transaction. Transaction A on CPU2 starts and finishes without any interleaving. Transaction B on CPU2 starts after A finishes and completes without being interleaved with any other transaction.\" The figure intends to illustrate transaction A and B being processed concurrently on one CPU in an interleaved fashion. These two processes have nothing to do with the CPU1 and CPU2 shown on the right side of the figure. On the perception level, the model made a wrong connection by saying the first appearance of transaction A is on CPU1 and the second appearance is on CPU2 (and the same for transaction B). With that said, the figure is indeed a non-trivial one to interpret and a model might need to know some subject-specific convention to interpret it correctly. \nGround Truth: (A) A and B "}
{"page": 99, "image_path": "doc_images/2311.16502v3_99.jpg", "ocr_text": "@ Computer Science: Compiler Principle\n\nQuestion: Is the one shown in above figure a minimal DFA? <image 1>\nOption:\n\n(A) Yes\n\n(B) No\n\n<image 1>\n\nGPT-4V: To determine if the given DFA (Deterministic Finite Automaton) is minimal, we need to\ncheck if there are any equivalent states that can be merged. Equivalent states are states from which\nfor every input string, the automaton ends in either an accepting state or a non-accepting state for\nboth states in question. The given DFA has 5 states labeled 0 through 4. To check for equivalent\nstates: We start by marking pairs of states where one is an accepting state and the other is not. In\nthis DFA, however, there's no indication of which states are accepting and which are not, so we'll\nassume all states are relevant for now. Next, for each pair of states (i, j), we check if for any input (a\nor b), the transitions 6(i, a) and 6(j, a), and 6(i, b) and 8(j, b) lead to the same state or already marked\npair of states. If not, we mark the pair (i, j).In the given DFA:\n\nFor state 0: 6(0, a) = 1 and 6(0, b) = 4 For state 1: 6(1,a) = 2 and 6(1,b)=0\n\nFor state 2: 6(2,a) = 3 and 6(2,b) = 0 For state 3: 5(3, a) = 4 and 6, b) =0\n\nFor state 4: 6(4, a) = 3 and 6(4, b) =1\nUpon inspection, it seems that: State 2 and State 4: Both move to state 3 on input 'a', but they differ\non input 'b'. So, they are not equivalent. State 1 and State 3: Both move to state 0 on input 'b', but\nthey differ on input 'a'. So, they are not equivalent. State 3 and State 4: Both move to state 0 on input\n'b', but they differ on input 'a'. So, they are not equivalent. No pairs of states seem to be equivalent\nin the given DFA. Therefore, the DFA appears to be minimal.\n\nError Category: Perceptual Error\nError Reason: 6(0, b) = 2 instead of 4, and 6(1, b) != 0. The model fails to ground on the image.\n\nGround Truth: (B) No\n\nFigure 86. A sample error case of Computer Science (subfield: Compiler Principle). Error category: Perceptual Error\nBack to List of Figures | Back to Table Index\n\n100\n", "vlm_text": "The image shows a red circle with a black border containing a white cross or \"X\" symbol.\nComputer Science: Compiler Principle \nQuestion:  Is the one shown in above figure a minimal DFA? <image  $_{1>}$  Option : \n(A) Yes (B) No \nThis image depicts a state transition diagram, which is often used in computer science to represent a finite automaton. It includes:\n\n- States (represented by circles), labeled 0 to 4.\n- Directed edges (arrows) between the states, indicating transitions.\n- The edges are labeled with letters 'a' and 'b', representing input symbols that cause the transitions.\n- State 0 is typically the initial state, as indicated by the arrow pointing to it from outside the diagram.\n- Each state may have one or multiple transitions, some going to the same state or others, influenced by the input symbols.\nGPT-4V:  To determine if the given DFA (Deterministic Finite Automaton) is minimal, we need to check if there are any equivalent states that can be merged. Equivalent states are states from which for every input string, the automaton ends in either an accepting state or a non-accepting state for both states in question. The given DFA has 5 states labeled 0 through 4. To check for equivalent states: We start by marking pairs of states where one is an accepting state and the other is not. In this DFA, however, there's no indication of which states are accepting and which are not, so we'll assume all states are relevant for now. Next, for each pair of states   $\\left(\\mathrm{i},\\,\\mathrm{j}\\right)$  , we check if for any input (a or b), the transitions   $\\updelta(\\mathrm{i},\\mathrm{a})$   and δ(j, a), and  $\\updelta(\\mathrm{i},\\mathrm{b})$   and  $\\updelta(\\mathrm{j},\\mathrm{b})$   lead to the same state or already marked pair of states. If not, we mark the pair  $\\mathrm{(i,j)}$  .In the given DFA: \nFor state  $0;\\delta(0,{\\mathsf{a}})=1$   and  $\\updelta(0,\\up b)=4$    For state 1:  $\\delta(1,\\mathrm{a})=2$   and   $\\delta(1,\\mathrm{b}){=}0$  For state 2  $:\\updelta(2,\\upalpha)=3$   and   $\\delta(2,\\!b)=0$  For state 3  $:\\updelta(3,\\,\\mathrm{a})=4$   and  $\\updelta(3,\\up b)=0$  For state 4:  $\\delta(4,\\mathsf{a})=3$   and  $\\updelta(4,\\up b)=1$  \nUpon inspection, it seems that: State 2 and State 4: Both move to state 3 on input   $\\mathrm{'a'}$  , but they differ on input   $\\mathrm{`b'}$  . So, they are not equivalent. State 1 and State 3: Both move to state 0 on input   $\\mathrm{\"b\"}$  , but they differ on input   $\\mathrm{{}^{\\prime}a^{\\prime}}$  . So, they are not equivalent. State 3 and State 4: Both move to state 0 on input  ${\\sf V b}_{.}$  , but they differ on input  $\\mathrm{'a'}$  . So, they are not equivalent. No pairs of states seem to be equivalent in the given DFA. Therefore, the DFA appears to be minimal. \nError Category: Perceptual Error \nError Reason:    $\\updelta(0,\\up b)=2$   instead of 4, and  $\\updelta(1,\\,{\\tt b})\\mathrel{\\mathop:}=0$  . The model fails to ground on the image. \nGround Truth: (B) No \nFigure 86. A sample error case of Computer Science (subfield: Compiler Principle). Error category: Perceptual Error Back to List of Figures  |  Back to Table Index "}
{"page": 100, "image_path": "doc_images/2311.16502v3_100.jpg", "ocr_text": "eZ Electronics: Signal Processing\n\nQuestion: The circuit shown in Fig 1 of <image 1> is in the steady state at t = 0- with the switch in\nposition 1. At t = 0, the switch is thrown to position 2. Use Laplace transforms to formulate an\nexpression for v¢(s)for any source vx(s).\n\nOption:\n\n(A) (3s +15 + 3Vy(5))/(s? + 6s + 8)\n\n(B) (3s + 18 + 3Vy(5))/(s? + 6s + 8)\n\n(©) Bs + 18 + 3Vx(5))/(s? + 45 + 8)\n\n(D) (Bs + 18 + 3Vx(<))/(s? + 45 +4)\n\n<image 1>\n\nGPT-4V: To find v¢(s)using Laplace transforms for the circuit, we'll follow these steps: 1. Find the\ninitial conditions. 2. Apply the Laplace transforms to the circuit. 3. Solve for v¢(s).\n\n1. Initial Conditions\n\nSince the circuit is in steady state at t = 0~ with the switch in position 1, the capacitor will behave\nas an open circuit for DC voltage. Therefore, the voltage across the capacitor v,(0~) is 8V (from the\nvoltage source).\n\n2. Apply Laplace Transforms\n\nWhen the switch is moved to position 2 at t = 0, the circuit can be analyzed in the s-domain. The\ninductor's impedance in the s-domain is sL = sH. The capacitor's impedance in the s-domain is\n1/sC = 1/sF. From the circuit: Applying KVL to the left loop: V.(s) — sH - 1,(s) -8 = 0 > (V(s) —\n8)/sH. Where I,(s) is the current flowing through the inductor. For the capacitor: v¢(s) = 8/s\n+ f 1,(s)ds. Where I,(s) is the current flowing through the capacitor. Since v,(0~) = 8V, the initial\ncondition in s-domain is 8/s . Applying KVL to the right loop: Vc(s) — 1+ 1/sF - I,(s) = 0. Using\nKCL at the node between the inductor, capacitor and resistor: I, (s) = I7(s) + v¢(s)/1\n\n3. Solve for v¢(s)\n\nSubstituting the equations derived above and solving for v¢(s), we get:vc¢(s) = (8s +18 +\n3Vx(¢))/(s? + 65 + 8)\n\nSo, the correct answer is: (B) (35 + 18 + 3Vx(5))/(s? + 65 + 8)\n\nGround Truth: (B) (3s + 18 + 3Vx(5))/(s? + 6s +8)\n\nFigure 87. A sample correct case of Electronics (subfield: Signal Processing).\nBack to List of Figures | Back to Table Index\n\n101\n", "vlm_text": "Electronics: Signal Processing \nQuestion:  The circuit shown in Fig 1 of <image  $_{1>}$   is in the steady state at  ${\\sf t}=0-$   with the switch in position 1. At   $\\mathbf{t}\\,=\\,0$  , the switch is thrown to position 2. Use Laplace transforms to formulate an expression for    $v_{C}(s)$  for any source    $v_{X}(s)$  . \nOption: \n\n$$\n\\begin{array}{c}{{(3s+15+3V_{X(s)})/(s^{2}+6s+8)}}\\\\ {{(3s+18+3V_{X(s)})/(s^{2}+6s+8)}}\\\\ {{(3s+18+3V_{X(s)})/(s^{2}+4s+8)}}\\\\ {{(3s+18+3V_{X(s)})/(s^{2}+4s+4)}}\\end{array}\n$$\n \nThe image shows an electrical circuit diagram. Here's a breakdown of its components:\n\n- **Voltage Source (\\(V_x\\)):** Represents a voltage input to the circuit.\n- **Switch:** Initially connected to position 2, can switch to position 1 at \\(t=0\\).\n- **8V Battery:** Connected in parallel with the switch.\n- **1 Ohm Resistor (\\(1 \\Omega\\))** in series.\n- **Inductor (1 Henry):** Connected in series, with current \\(i_L\\) indicated in the direction of the flow.\n- **Resistance (\\(\\frac{3}{5} \\Omega\\))** in parallel with the capacitor.\n- **Capacitor (\\(\\frac{1}{3} F\\)):** Connected in parallel, with its voltage labeled \\(v_C\\).\n\nThe circuit includes both resistive and reactive components (inductor and capacitor).\nGPT-4V:  To find  $v_{C}(s)$  using Laplace transforms for the circuit, we'll follow these steps: 1. Find the initial conditions. 2. Apply the Laplace transforms to the circuit. 3. Solve for    $v_{C}(s)$  . 1. Initial Conditions \nSince the circuit is in steady state at    $t=0^{-}$    with the switch in position 1, the capacitor will behave as an open circuit for DC voltage. Therefore, the voltage across the capacitor    $v_{c}(0^{-})$   is 8V (from the voltage source). \n2. Apply Laplace Transforms \nWhen the switch is moved to position 2 at  $\\dagger=0$  , the circuit can be analyzed in the s-domain. The inductor's impedance in the s-domain is   $\\mathrm{sL}\\,=\\,\\mathrm{sH}$  . The capacitor's impedance in the s-domain is  $1/s C=1/s F$  . From the circuit: Applying KVL to the left loop:    $V_{x}(s)-s H\\cdot I_{1}(s)-8=0\\Longrightarrow(V_{x}(s)-$   $8)/s H$  . Where    $I_{1}(s)$  is the current flowing through the inductor. For the capacitor:    $v_{C}(s)=8/s$   $+\\textstyle\\int I_{2}(s)d s$  . Where  $I_{2}(s)$  is the current flowing through the capacitor. Since    $v_{c}(0^{-})=8V$  , the initial condition in s-domain is  $8/s$  . Applying KVL to the right loop:    $V_{C}(s)-1+1/s F\\cdot I_{2}(s)=0$  . Using KCL at the node between the inductor, capacitor and resistor:  $I_{1}(s)=I_{2}(s)+v_{C}(s)/1$  3. Solve for  $v_{C}(s)$  \nSubstituting the equations derived above and solving for    $v_{C}(s)$   , we get:  $v_{C}(s)=(3s+18+$   $3V_{X(s)})/(s^{2}+6s+8)$  \nSo, the correct answer is: (B)    $(3s+18+3V_{X(s)})/(s^{2}+6s+8)$  \nGround Truth: (B)  $(\\mathbf{3}s+\\mathbf{18}+\\mathbf{3}V_{X(s)})/(s^{2}+\\mathbf{6}s+\\mathbf{8})$  "}
{"page": 101, "image_path": "doc_images/2311.16502v3_101.jpg", "ocr_text": "@ Electronics : Signal Processing\n\nQuestion: A balanced set of three-phase voltages is connected to an unbalanced set of Y-connected\nimpedances as shown in <image1>. The following values are known Vgp = 212290°V, Zan = 10 +\nJ0 2, Vie = 2122-150°V, Zpn = 10 + j10 0, Vey = 2122 —30° V, Zen = 0 — j20 2. Find the magnitude\nof current I)/p.\n\n<image 1>\n\nGPT-4V: To find the magnitude of the line current /,,,,, we need to find the phase current J, . The\nphase current can be found using Ohm's Law:I = 7 Given: Vay = 212290°V, Vp, = 2124 —150°V,\nVeq = 2122 -30°V; Zan =10 + j0M, Zyn =10+j109, Zon =0—J20. Since it is a star (Y)\nconnection, the line-to-neutral voltage for phase b is V,,. And: Vy, = Vic — Veg. Given that: Vp, =\n2122-150°V , Veg =2122-30°V . Using the phasor method for subtraction: Vpn =\n2122 —150° —2122 —30°. For phasor addition or subtraction, it's easier to convert the phasors to\nrectangular form, perform the arithmetic, and then convert back to polar form. V,, =\n212 cos(—150°) + j(212 sin(—150°)), Veq = 212 cos(30°) + j(212 sin(30°)). Subtracting the two\nphasors: Vp, = (Vp-real — V.greal) + j(V,,imag — V.qimag). Once Vp, is calculated in rectangular\nform, convert it to polar form to get its magnitude and angle.\nNow, using Ohm's Law for phase b: .\nHea = Yon lhe = Vommagnitude ZV pnangle\n\nZon 10+j10\n\nAgain, convert /,,, to rectangular form and then back to polar form to get its magnitude and angle.\n\nIn a Y-connected system, the line current is equal to the phase current for that phase: 1) = Ipn.\n\nError Category: Reject to Answer\n\nError Reason: GPT-4V’s reasoning steps are correct, however, it does not want to involve the\ncomplex computation, therefore, it rejects to answer.\n\nGround Truth: 14.54\n\nFigure 88. A sample error case of Electronics (subfield: Signal Processing). Error category: Reject to Answer\nBack to List of Figures | Back to Table Index\n\n102\n", "vlm_text": "The image shows a red circle with a black border, containing a white \"X\" in the center. This symbol is commonly used to indicate an error, a prohibition, or a closed status.\nElectronics : Signal Processing \nQuestion:  A balanced set of three-phase voltages is connected to an unbalanced set of Y-connected impedances as shown in <image1>. The following values are known  $V_{a b}=212\\angle90^{\\circ}V.$  ,  $Z_{a n}=10+$   $j0\\;\\Omega,\\,V_{b c}=212\\angle{-150^{\\circ}V}$  ,  $Z_{b n}=10+j10\\;\\Omega,$   $V_{\\mathrm{c}b}=212\\angle-30^{\\circ}\\,V,$  ,  $Z_{c n}=0-j20\\:\\Omega.$  . Find the magnitude of current  $I_{b^{\\prime}b}$  . \nThis image shows a Y-Δ (star-delta) network transformation used in circuit analysis. It includes resistors, an inductor, and a capacitor arranged in a triangle. The points are labeled a, b, and c, with additional labels a', b', and c' indicating external connections. The inductor and capacitor are connected in between the nodes, forming the angles of the triangle, with the resistors alongside them.\nGPT-4V:  To find the magnitude of the line current    $I_{b^{\\prime}b}$    , we need to find the phase current  $I_{b\\mathrm{n}}$    . The phase current can be found using Ohm's Law:  $\\begin{array}{r}{I=\\frac{V}{Z}}\\end{array}$  . Given:    $V_{a b}=212\\angle90^{\\circ}V$  ,    $V_{b c}=212\\angle-150^{\\circ}\\,V,$   $V_{c a}=212\\angle-30^{\\circ}\\,V\\,,$  ;    $Z_{a n}=10+j0\\;\\Omega$  ,    $Z_{b n}=10+j10\\;\\Omega_{,}$  ,    $Z_{c n}=0-j20\\;\\Omega$   . Since it is a star (Y) connection, the line-to-neutral voltage for phase   $^\\mathrm{b}$   is    $V_{b n}$  . And:    $V_{b n}=V_{b c}-V_{c a}$  . Given that:  $V_{b c}=$   $212\\angle-150^{\\circ}\\ V$  ,  $V_{c a}=212\\angle-30^{\\circ}\\,V$  . Using the phasor method for subtraction:  $V_{b n}=$   $212\\angle-150^{\\circ}-212\\angle-30^{\\circ}$  . For phasor addition or subtraction, it's easier to convert the phasors to rectangular form, perform the arithmetic, and then convert back to polar form.  $V_{b c}=$   $212\\,c o s(-150^{\\circ})+j(212\\,s i n(-150^{\\circ}))$   ,    $V_{c a}=212\\,c o s(30^{\\circ})+j(212\\,s i n(30^{\\circ}))$   . Subtracting the two phasors:    $V_{b c}=(V_{b c}r e a l-V_{c a}r e a l)+j(V_{b c}i m a g-V_{c a}i m a g)$  . Once    $V_{b\\mathrm{n}}$    is calculated in rectangular form, convert it to polar form to get its magnitude and angle. Now, using Ohm's Law for phase b: \n\n$$\n\\begin{array}{r}{I_{b n}=\\frac{V_{b n}}{Z_{b n}},\\,I_{b n}=\\frac{V_{b n}m a g n i t u d e\\angle V_{b n}a n g l e}{10+j10}.}\\end{array}\n$$\n \nAgain, convert  $I_{b n}$    to rectangular form and then back to polar form to get its magnitude and angle. In a Y-connected system, the line current is equal to the phase current for that phase:    $I_{b^{\\prime}b}=I_{b n}$  . The magnitude of   $I_{b^{\\prime}b}$   is equal to the magnitude of  $I_{b n}$  . To provide a numerical value, we would  have to go through the above calculations. Would you like me to proceed with that? \nError Category: Reject to Answer \nError Reason:  GPT-4V’s reasoning steps are correct, however, it does not want to involve the complex computation, therefore, it rejects to answer. \nGround Truth: 14.54 \nFigure 88. A sample error case of Electronics (subfield: Signal Processing). Error category: Reject to Answer Back to List of Figures  |  Back to Table Index "}
{"page": 102, "image_path": "doc_images/2311.16502v3_102.jpg", "ocr_text": "eZ Energy and Power: Fluid Mechanics\n\nQuestion: Some experimental values of $\\\\mu $ of helium at 1 atm are as follows: . <image 1> Fit\nthese values to a Power-law.\n\nOption:\n\n(A) Une © 1.505E~> — 5(T’/200K)°38\n\n(B) Hye © 2.505E~> — 5(T’/200K)°*\n\n(©) Hue © 1.505E~° — 5(T’/200K)°®\n\nF,.°Ke 200 400 600 800 1000 1200\nkg/m-s: 1.50E-S 2.43E-S5 3.20E-5 3.88E-5 4.50E-5 5.08E-5\n\n<image 1>\n\nGPT-4V: To fit the given values to a Power-law, we'll use the form:u = C(T\"), where C and k are\nconstants to be determined. We can take the logarithm of both sides to transform this equation to a\nlinear form: In(u) = In(C) + kln(T). Now, let's choose two points from the given data to solve for C\nand k. Using the data points (T, “) = (200 K, 1.50E-5) and (400 K, 2.43E-5): For the first point:\nIn(1.50E-5) = In(C) + k \\In(200) (1). For the second point: In(2.43E-5) = In(C) + kIn(400) (2).\nSubtracting (1) from (2) to eliminate C, we get: In ( 2.43E-5/1.50E-5)=kIn2. From this, we can solve\nfor k. Once we have k, we can use either (1) or (2) to solve for C. After solving for k and C, we can\nuse these values to see which option (A, B, or C) fits the power-law. Let's calculate k and C. k\n=In(2.43E-5/1.50E-5)/In2~ 1n1.62/In2 ~ 0.68. Plugging this value of k into (1): In(1.50E-5) = InC +\n0.681n200. Solving for C: C~1.505E-5. The relation becomes: [ye © 1.505E~° — 5T*, where K ~ 0.68.\nFrom the options given, this matches with: (C) Lye © 1.505E~> — 5(T°/200K)°*® . So, option (C) is\nthe correct choice.\n\nGround Truth: (C) tye ~ 1.505E~> — 5(T’ /200K)°8\n\nFigure 89. A sample correct case of Energy and Power (subfield: Fluid Mechanics).\nBack to List of Figures | Back to Table Index\n\n103\n", "vlm_text": "Energy and Power: Fluid Mechanics \nQuestion:  Some experimental values of  $\\P\\backslash\\backslash\\mathbf{\\mathfrak{m}u}\\ \\mathbb{S}$   of helium at 1 atm are as follows: . <image  $_{1>}$   Fit these values to a Power-law. \nOption:\n\n \n(A)  $\\mu_{H e}\\approx1.505E^{-5}-5(T^{^{\\circ}}/200K)^{0.38}\n\n$  (B)  $\\mu_{H e}\\approx2.505E^{-5}-5(T^{^{\\circ}}/200K)^{0.68}\n\n$  (C)  $\\mu_{H e}\\approx1.505E^{-5}-5(T^{^{\\circ}}/200K)^{0.68}$  \nThe table shows values related to temperature (T in Kelvin) and dynamic viscosity (μ in kg/m·s).\n\n- At 200 K, μ is 1.50E-5 kg/m·s\n- At 400 K, μ is 2.43E-5 kg/m·s\n- At 600 K, μ is 3.20E-5 kg/m·s\n- At 800 K, μ is 3.88E-5 kg/m·s\n- At 1000 K, μ is 4.50E-5 kg/m·s\n- At 1200 K, μ is 5.08E-5 kg/m·s\n\nThis data likely represents how the dynamic viscosity changes with temperature.\nGPT-4V:  To fit the given values to a Power-law, we'll use the form:  $\\begin{array}{r}{\\mu=C(T^{k}),}\\end{array}$   where C and   $\\mathrm{k\\Omega}$   are constants to be determined. We can take the logarithm of both sides to transform this equation to a linear form:  $\\ln(\\mu)=\\ln(C)+k l n(T)$  . Now, let's choose two points from the given data to solve for   $\\mathsf C$  and   $\\mathrm{k\\Omega}$  . Using the data points   $(\\mathrm{T},\\ \\mu)\\ =\\ (200\\ \\mathrm{K},\\ 1.50\\mathrm{E}.5)$   and (  $(400\\,\\mathrm{\\normalfont~K},$   2.43E-5): For the first point:  $\\ln(1.50\\mathrm{E}.5)=\\ln(\\mathrm{C})+\\mathrm{k}\\left\\backslash\\ln(200)\\right.$   (1). For the second point:   $\\ln(2.43\\mathrm{{E}}{\\cdot}5)=\\ln(\\mathrm{{C}})+\\mathrm{{kln}}(400)$   (2). \nSubtracting (1) from (2) to eliminate   $\\mathrm{C},$   we get: ln   $\\it(\\ 2.43\\mathrm{E}{-5/1.50\\mathrm{E}{-5}}){=}\\mathrm{ln}{2}$  . From this, we can solve for  $\\mathrm{k\\Omega}$  . Once we have  $\\operatorname{k},$   we can use either (1) or (2) to solve for C. After solving for  $\\mathrm{k\\Omega}$   and  $\\mathrm{C},$   we can use these values to see which option (A, B, or   $C$  ) fits the power-law. Let's calculate  $\\mathrm{k\\Omega}$   and C. k\n\n  $\\mathrm{=ln}(2.43\\mathrm{E-}5/1.50\\mathrm{E-}5)/\\mathrm{ln}2\\approx\\mathrm{ln}1.62/\\mathrm{ln}2\\approx0.68$  . Plugging this value of  $\\mathrm{k\\Omega}$   into (1):  $\\ln(1.50\\mathrm{E-}5)=\\ln\\!{\\cal C}\\,+\n\n$  0.68ln200. Solving for C:  $\\mathsf{C}\\!\\approx\\!1.505\\mathrm{E}\\!-\\!5$  . The relation becomes:    $\\mu_{H e}\\approx1.505E^{-5}-5T^{K},$  , where  $\\mathrm{K}\\approx0.68$  . From the options given, this matches with: (C)  $\\mu_{H e}\\approx1.505E^{-5}-5(T^{^{\\circ}}/200K)^{0.68}$    .  $\\mathrm{{Bo,}}$   option (C) is the correct choice. \nGround Truth: (C)  $\\mu_{H e}\\approx1.505E^{-5}-5(T^{^{\\circ}}/200K)^{0.68}$  \nFigure 89. A sample correct case of Energy and Power (subfield: Fluid Mechanics). Back to List of Figures  |  Back to Table Index "}
{"page": 103, "image_path": "doc_images/2311.16502v3_103.jpg", "ocr_text": "(3) Energy and Power: Thermodynamics\n\nQuestion: A piston-cylinder device contains helium gas initially at 100 kPa, 10°C, and 0.2 m3. The\nhelium is now compressed in a polytropic process (PV\" = constant) to 700 kPa and 290°C.\nDetermine the heat loss or gain during this process..<image 1>\n\nOption:\n\n(A) Qi2 = —5.553 kJ\n\n(B) Qi2 = —6.553 kJ\n\n(©) Qz = -7.553 kf\n\n<image 1>\n\nGPT-4V: Given the nature of this problem, we'll use the first law of thermodynamics for a closed\nsystem undergoing a process: AU = Qi2 — Wi2, where AU = Change in internal energy, Qi2 = Heat\ntransfer during the process, and W,2 = Work done by the system during the process. From the given\nimage, we can infer that the process is polytropic and is described by the equation: PV” = constant.\nFor a polytropic process involving an ideal gas: W,2 = (P2V2 — P,V,)/(1 — n). Given: P,=100kPa,\nV, = 0.2m3, Ty = 10°C, P2=700kPa, Tz = 290°C . To find V2, we use the polytropic relationship:\nP,V,\"/P2 = Vz'. The value of n can be determined using the temperatures and the ideal gas law:\nP,V,/T, = P2V2/T,. Once V2 is known, the work W,, can be determined using the equation for\npolytropic work. The change in internal energy AU for an ideal gas like helium is only a function of\ntemperature and can be determined using: AU = mC, (Tz — T,), where C, = Specific heat at constant\nvolume for helium (approximately 3.12 kJ/kg-K), and m = Mass of the helium. Mass can be\ndetermined using the ideal gas law: m = P,V,/RT, where R is the specific gas constant for helium,\napproximately 2.0831 kJ/kg-K. Finally, using the first law of thermodynamics: Q,, = AU + W1)=.\nThe given options for Q;2 are in kJ, so we need to ensure that all values used in calculations are\nconsistent with this unit. To determine Qj, we will: Calculate the mass m of helium, determine V2\nusing the polytropic relationship, calculate the work W,, determine the change in internal energy\nAU, and use the first law to calculate Q,. Now, let's plug in the values and solve: Calculate mass:\nm = P,V,/RT, = 100kPa * 0.2m3 /(2.0831k]/kg « K * (10 + 273)) ~ 0.0271kg . Determine V2 using\nthe given temperatures and the ideal gas relationship (this is iterative and requires more involved\nmath). Calculate the work W,2 using the polytropic equation. Determine the change in internal\nenergy: AU = mC,(T2 — T;) ~ 23.75kJ. Use the first law: Qi2 = AU + Wiz. Now, without the exact\nvalue for W,, (which requires the iterative determination of V_2 and n), it's hard to provide an exact\nanswer. However, considering the given options, you can solve for Q;2 using the above steps. If you\nhave the value for n or more specific data, please provide it, and I can assist further.\n\nError Category: Reasoning Error\n\nError Reason: Reasoning error 1 (calculation): The formula of m is correct but the calculation is\nwrong: m = P,V,/RT, ~ 0.0271kg . Reasoning error 2 (unit conversion T: The formula of AU is\ncorrect but the unit conversion of T is wrong: AU = mC,(T2 — T;) © 23.75kJ . Reasoning error 3\n(calculation order): The final answer Q is to be calculated based on W, W is to be calculated based\non V2 and n, n is to be calculated based on V2. GPT4-V fails to calculate these variables in a correct\norder V, > n> W > Q, thus it is unable to provide the answer.\n\nGround Truth: (B) Q_12=-6.553 kJ\n\nFigure 90. A sample error case of Energy and Power (subfield: Thermodynamics). Error category: Reasoning Error\nBack to List of Figures | Back to Table Index\n\n104\n", "vlm_text": "Energy and Power: Thermodynamics \nQuestion:  A piston-cylinder device contains helium gas initially at  $100\\;\\mathrm{kPa}$  ,  $10^{\\circ}\\mathrm{C},$   and   $0.2\\:\\mathrm{m}^{\\wedge}3$  . The helium is now compressed in a polytropic process   $(P V^{n}=c o n s t a n t)$   to   $700\\;\\mathrm{\\textmu}$   and   $290^{\\circ}\\mathrm{C}.$  . Determine the heat loss or gain during this process..<image  $_{1>}$  \nOption: (A)  $Q_{12}=-5.553\\;k J$  \nThe image shows:\n\n1. A diagram of a container with a movable piston. The container is labeled with \"He\" and \"PV^n = constant,\" indicating a thermodynamic process involving helium gas.\n2. A list of three options with energy values related to \\( Q_{12} \\):\n   - (A) \\( Q_{12} = -5.553 \\, \\text{kJ} \\)\n   - (B) \\( Q_{12} = -6.553 \\, \\text{kJ} \\)\n   - (C) \\( Q_{12} = -7.553 \\, \\text{kJ} \\)\nGPT-4V:  Given the nature of this problem, we'll use the first law of thermodynamics for a closed system undergoing a process:    $\\Delta U=Q_{12}-W_{12},$  , where    $\\Delta U=$  Change in internal energy,    $Q_{12}=$   Heat transfer during the process, and  $W_{12}=$  = Work done by the system during the process. From the given image, we can infer that the process is polytropic and is described by the equation:  $P V^{n}=c o n s t a n t$  . For a polytropic process involving an ideal gas:    $W_{12}=(P_{2}V_{2}-P_{1}V_{1})/(1-n)$  . Given:    $\\scriptstyle P_{1}=100\\mathrm{kPa,}$   $V_{1}=0.2m^{3}$  ,    $T_{1}=10^{\\circ}C$  ,    $P_{2}{=}700\\mathrm{kPa}$  ,    $T_{2}=290^{\\circ}C$    . To find    $V_{2},$  , we use the polytropic relationship:  $P_{1}V_{1}^{n}/P_{2}=V_{2}^{n}$  . The value of n can be determined using the temperatures and the ideal gas law:  $P_{1}V_{1}/T_{1}=P_{2}V_{2}/T_{2}$  . Once  $V_{2}$    is known, the work    $W_{12}$    can be determined using the equation for polytropic work. The change in internal energy    $\\Delta U$    for an ideal gas like helium is only a function of temperature and can be determined using:  $\\Delta U=m C_{v}(T_{2}-T_{1}),$  , where  $C_{v}=$   Specific heat at constant volume for helium (approximately   $3.12\\:\\mathrm{kJ/kg}{\\cdot}\\mathrm{K})$  , and   $\\mathrm{\\Deltam\\;=\\;Mass}$   of the helium. Mass can be determined using the ideal gas law:    $m=P_{1}V_{1}/R T_{1},$  , where R is the specific gas constant for helium, approximately   $2.0831\\ \\mathrm{kJ/kg{\\cdot}K}$  . Finally, using the first law of thermodynamics:    $Q_{12}=\\Delta U+W_{12}{=}.$  The given options for    $Q_{12}$    are in kJ, so we need to ensure that all values used in calculations are consistent with this unit. To determine  $Q_{12},$   we will: Calculate the mass m of helium, determine  $V_{2}$  using the polytropic relationship, calculate the work    $W_{12},$  , determine the change in internal energy  $\\Delta U.$  , and use the first law to calculate    $Q_{12}$  . Now, let's plug in the values and solve: Calculate mass:  $m=P_{1}V_{1}/R T_{1}=100k P a*0.2m^{3}/(2.0831k J/k g\\cdot K*(10+273))\\approx0.0271k g$  . Determine  $V_{2}$    using the given temperatures and the ideal gas relationship (this is iterative and requires more involved math). Calculate the work    $W_{12}$    using the polytropic equation. Determine the change in internal energy:    $\\Delta U=m C_{v}(T_{2}-T_{1})\\approx23.75k J$  . Use the first law:    $Q_{12}=\\Delta U+W_{12}$  . Now, without the exact value for  $W_{12}$    (which requires the iterative determination of  $\\mathrm{V}\\_2$   and n), it's hard to provide an exact answer. However, considering the given options, you can solve for    $Q_{12}$   using the above steps. If you have the value for n or more specific data, please provide it, and I can assist further. \nError Category: Reasoning Error \nError Reason:  Reasoning error 1 (calculation): The formula of m is correct but the calculation is wrong:    $m=P_{1}V_{1}/R T_{1}\\approx0.0271k g$  . Reasoning error 2 (unit conversion T: The formula of    $\\Delta U$    is correct but the unit conversion of  $\\mathrm{T}$   is wrong:    $\\Delta U=m C_{v}(T_{2}-T_{1})\\approx23.75k J$  . Reasoning error 3 (calculation order): The final answer   $\\mathrm{\\DeltaQ}$   is to be calculated based on W, W is to be calculated based on  $V_{2}$    and  $\\mathsf{n},$  , n is to be calculated based on    $V_{2}$  . GPT4-V fails to calculate these variables in a correct order  $V_{2}\\rightarrow n\\rightarrow W\\rightarrow Q,$  , thus it is unable to provide the answer. \nGround Truth: (B)  $Q_{-}12{=}{-}6.553\\;k J$  \nFigure 90. A sample error case of Energy and Power (subfield: Thermodynamics). Error category: Reasoning Error Back to List of Figures  Back to Table Index "}
{"page": 104, "image_path": "doc_images/2311.16502v3_104.jpg", "ocr_text": "eZ Materials: Materials Science\n\nQuestion: Below are shown, for five metals, reduction reactions and standard electrode potential\nvalues. Which of these metals is the least reactive? <image 1>\n\nOption:\n\n(A)Au\n\n(B) Cu\n\n(C) Ni\n\n(D) Fe\n\n(E) Na\nElectrode reaction Standard electrode potential\n\n(v)\nAu’ +3e—> Au +1.420\nCu +2e-->Cu +0.340\nNi‘ +2e---> Ni 0.250\nFe’ +2e---> Fe -0.440\nNa’ +e->Na 2.924\n\n<image 1>\n\nGPT-4V: The reactivity of a metal is inversely related to its standard electrode potential. The metal\nwith the highest positive standard electrode potential will be the least reactive. From the provided\ntable:Au** + 3 e -> Au has a standard electrode potential of +1.420 V, Cu2* + 2 e& -> Cu has a\nstandard electrode potential of +0.340 V, Ni?* + 2 e -> Ni has a standard electrode potential of\n-0.250 V, Fe?* + 2 e& -> Fe has a standard electrode potential of -0.440 V, Na* + e -> Na has a\nstandard electrode potential of —2.924 V. The highest positive standard electrode potential is for Au,\nwhich is +1.420 V. So, the least reactive metal is: (A) Au.\n\nGround Truth: (A) Au\n\nExplanation: Since Au has the most positive standard electrode potential, it is the least reactive\nmetal\n\nFigure 91. A sample correct case of Materials (subfield: Materials Science).\nBack to List of Figures | Back to Table Index\n\n105\n", "vlm_text": "The image shows a green circle with a black border, containing a white check mark. This typically symbolizes confirmation or success.\nMaterials: Materials Science \nQuestion:  Below are shown, for five metals, reduction reactions and standard electrode potential values. Which of these metals is the least reactive? <image  $_{1>}$  \nOption:\n\n (A)Au\n\n (B) Cu\n\n (C) Ni\n\n (D) Fe\n\n (E) Na \nThe image shows a table with two columns: \"Electrode reaction\" and \"Standard electrode potential (V)\". The table lists the following:\n\n1. \\( \\text{Au}^{3+} + 3 \\text{e}^- \\rightarrow \\text{Au} \\) with a potential of +1.420 V\n2. \\( \\text{Cu}^{2+} + 2 \\text{e}^- \\rightarrow \\text{Cu} \\) with a potential of +0.340 V\n3. \\( \\text{Ni}^{2+} + 2 \\text{e}^- \\rightarrow \\text{Ni} \\) with a potential of -0.250 V\n4. \\( \\text{Fe}^{2+} + 2 \\text{e}^- \\rightarrow \\text{Fe} \\) with a potential of -0.440 V\n5. \\( \\text{Na}^+ + \\text{e}^- \\rightarrow \\text{Na} \\) with a potential of -2.924 V\nGPT-4V:  The reactivity of a metal is inversely related to its standard electrode potential. The metal with the highest positive standard electrode potential will be the least reactive. From the provided table:  $\\mathrm{Au}^{3^{+}}\\ +\\ 3\\ \\mathrm{e}^{-}\\ ->$   Au has a standard electrode potential of   $+1.420\\,\\mathrm{~V},$  ,   $\\mathrm{Cu^{2+}\\;+\\;2\\;\\;e^{-}\\;->\\;C u}$   has a standard electrode potential of  $+0.340\\,\\mathrm{~V~}$  ,   $\\mathrm{Ni^{2+}\\,+\\,2\\ e^{-}\\,->\\ N i}$   has a standard electrode potential of  $-0.250\\mathrm{~V},$  ,   $\\mathrm{Fe}^{2+}\\;+\\;2\\ \\mathrm{e}^{-}\\;->$   Fe has a standard electrode potential of   $-0.440\\mathrm{~V~}$  ,   $\\mathrm{{Na^{+}\\,+\\,e^{-}\\,->\\,N a}}$   has a standard electrode potential of  $-2.924\\;\\mathrm{V}$  . The highest positive standard electrode potential is for Au, which is   $+1.420\\;\\mathrm{V}$  . So, the least reactive metal is: (A) Au. \nGround Truth: (A) Au \nExplanation:  Since Au has the most positive standard electrode potential, it is the least reactive metal "}
{"page": 105, "image_path": "doc_images/2311.16502v3_105.jpg", "ocr_text": "@ Materials: Mechanics of Materials\n\nQuestion: The rectangular plate is deformed into the shape of a parallelogram shown by the dashed\nlines. Determine the average shear strain y, at corners A and B. <image 1>\n\nOption:\n\n(A) 0.0292 rad, 0.0292 rad\n\n(B) 0.0292 rad, 0.0125 rad\n\n(C) 0.0125 rad, 0.0292 rad\n\n(D) 0.0125 rad, 0.0125 rad\n\nPA\n\nSmm\n\nDe\n\n|\n\n300 mm\n\nSei {5 mm\nA B\n400 mm +\n\nx\n\n<image 1>\n\nGPT-4V: To determine the average shear strain yy, we use the formula: yyy = = Where:Sy =\n\nchange in the y-direction, 6x = change in the x-direction. For Corner A: Since the rectangle is\ndeformed into a parallelogram, the top left corner D is displaced by 5mm to the right. Thus, the\n\nshear strain at point A due to this horizontal displacement is: yyyA = me Given: dy = 0 (There's no\n\n= —*— = 0rad. For Corner B: The bottom right corner\n300mm\n\nCis displaced by 5mm upwards. Thus, the shear strain at point B due to this vertical displacement\nSmm\n\nvertical displacement at A), 5x = 5mm,YyyA\n\nis! YyyB = 2 Given: dy = 5mm,é6x = 400mm, 7B = = 0.0125 rad. Therefore, the average\n\n400 mm\nshear strain at corners A and B are: A: 0 rad, B: 0.0125 rad. But, since the question has provided\nchoices with minimum values of 0.0125 rad for both A and B, and there's no choice with 0 rad for A,\nwe can infer that the shear strain is considered in the context of the entire deformation. This means\nthe shear strain at A and B would be due to the overall deformation of the rectangle into a\n\nparallelogram. Given that both A and D move to the right by 5 mm and both B and C move up by 5\n\n5mm\n\nmm, the average shear strain for the entire deformation at both points would be: y,y = OCT\n\n0.0125 rad (for both A and B). So, the correct answer is: (D) 0.0125 rad, 0.0125 rad.\n\nError Category: Lack of knowledge\n\nError Reason: GPT-4V correctly identifies the diagram and the information displayed on the\ndiagram. To determine the averaged shear strain at corners A and B, the correct formula is to\ncalculate the change in angle between two initially perpendicular lines. GPT-4v failed to use the\ncorrect formula to calculate the averaged shear strain.\n\nGround Truth: (A) 0.0292 rad, 0.0292 rad\n\nFigure 92. A sample error case of Materials (subfield: Mechanics of Materials). Error category: Lack of Knowledge\nBack to List of Figures | Back to Table Index\n\n106\n", "vlm_text": "The image shows a red circle with a white \"X\" inside, often used as an icon to indicate an error or closed status.\nMaterials: Mechanics of Materials \nQuestion:  The rectangular plate is deformed into the shape of a parallelogram shown by the dashed lines. Determine the average shear strain  $\\gamma_{x y}$    at corners A and B. <image  $_{1>}\n\n$  \nOption: \n(A) 0.0292 rad, 0.0292 rad\n\n (B) 0.0292 rad, 0.0125 rad\n\n (C) 0.0125 rad, 0.0292 rad\n\n (D) 0.0125 rad, 0.0125 rad \nThe image shows a geometric shape, specifically a quadrilateral, on a coordinate plane defined by the axes \\(x\\) and \\(y\\). The shape is a trapezoid with the following features:\n\n- The base \\(AB\\) is \\(400 \\, \\text{mm}\\) long.\n- The height of the trapezoid from \\(A\\) to \\(D\\) is \\(300 \\, \\text{mm}\\).\n- There is a displacement of \\(5 \\, \\text{mm}\\) from \\(D\\) to \\(C\\) along the \\(y\\)-axis and from \\(A\\) to \\(B\\) along the \\(x\\)-axis.\n- The vertices are labeled as \\(A\\), \\(B\\), \\(C\\), and \\(D\\).\n\nThe shape is placed within a rectangular boundary, with dotted guideline extensions showing the original rectangle dimensions before the top side was shifted.\nGPT-4V:  To determine the average shear strain    $\\gamma_{x y}$    , we use the formula:  $\\gamma_{x y}={\\frac{\\delta y}{\\delta x}}$    Where:  $\\delta y=$    change in the y-direction,    $\\delta x=$    change in the   $\\mathbf{x}.$  -direction. For Corner A: Since the rectangle is deformed into a parallelogram, the top left corner   $\\mathrm{D}$   is displaced by   $5\\mathrm{mm}$   to the right. Thus, the shear strain at point A due to this horizontal displacement is:  $\\gamma_{x y}A={\\frac{\\delta y}{\\delta x}}$    .  Given:    $\\delta y=0$   (There's no vertical displacement at A),  $\\begin{array}{r}{\\delta x=5\\mathrm{mm},\\gamma_{x y}A=\\frac{0}{300m m}=0}\\end{array}$  = 0 rad . For Corner B: The bottom right corner C is displaced by  $5\\mathrm{mm}$   upwards. Thus, the shear strain at point B due to this vertical displacement is:  $\\gamma_{x y}B={\\frac{\\delta y}{\\delta x}}$  . Given:    $\\delta y=5\\mathrm{mm},\\delta x=400\\mathrm{mm},$  ,  $\\begin{array}{r}{\\gamma_{x y}B=\\frac{5\\,m m}{400\\,m m}=0.0125}\\end{array}$   rad. Therefore, the average shear strain at corners A and   $\\mathrm{B}$   are: A: 0 rad, B: 0.0125 rad. But, since the question has provided choices with minimum values of 0.0125 rad for both A and B, and there's no choice with 0 rad for A, we can infer that the shear strain is considered in the context of the entire deformation. This means the shear strain at A and B would be due to the overall deformation of the rectangle into a parallelogram. Given that both A and D move to the right by  $5\\:\\mathrm{mm}$   and both B and C move up by 5\n\n  $\\mathbf{mm},$  , the average shear strain for the entire deformation at both points would be:    $\\begin{array}{r}{\\gamma_{x y}=\\frac{5\\,m m}{400\\,m m}=}\\end{array}\n\n$  0.0125  rad (for both A and B). So, the correct answer is:  (D) 0.0125 rad, 0.0125 rad . \nError Category: Lack of knowledge \nError Reason:  GPT-4V correctly identifies the diagram and the information displayed on the diagram. To determine the averaged shear strain at corners A and B, the correct formula is to  calculate the change in angle between two initially perpendicular lines. GPT-  $_{4\\mathrm{v}}$   failed to use the correct formula to calculate the averaged shear strain. \nGround Truth: (A) 0.0292 rad, 0.0292 rad "}
{"page": 106, "image_path": "doc_images/2311.16502v3_106.jpg", "ocr_text": "eZ Materials: Control System\n\nQuestion: For the unity feedback system shown in <image 1>, where G(s) = S50(648)(6412)(5415) Bing\n\n$(S+38)(s2+2s+28)\nthe steady-state errors for the following test inputs: 25u(t). Option:\n(A) 0\n(B)1\nQ-\nR(s) + E(s) G(s) Cs)\n<image 1>\n\nGPT-4V: To find the steady-state error for a unity feedback system, we can use the final value\ntheorem. The final value theorem states that: lim,_,..e(t) = limp..SE(s), where E(s) is the Laplace\ntransform of the error signal e(t). Given a system with unity feedback, the error signal in the\nLaplace domain can be given by: E(s) = R(s) - C(s), where R(s) is the Laplace transform of the\nreference input, and C(s) is the Laplace transform of the system's output. The closed-loop transfer\nfunction for a unity feedback system is given by:T(s) = G(s)/(1 + G(s)). Given a step input r(t) =\n25u(t), the Laplace transform is: R(s) = 25/sThe system's output in the Laplace domain is:C(s) =\nT(s)XR(s) = G(s)/(1 + G(s))x25/s. Thus, the error signal in the Laplace domain is: E(s) = 25/s —\nG(s)/(1 + G(s))X25/s = 25/sx[1—G(s)/(1+ G(s))]To find the steady-state error, we apply the\nfinal value theorem:lims x E(s). Substitute E(s) from the above equation and evaluate the limit.\nGiven: G(s) = [450(s + 8)(s + 12)(s + 15)]/= [s(s + 38)(s? + 2s)(s + 15)]Using the formula e,, =\nlim,_,9SX25/sx[1 — G(s)/(1 + G(s))]. Now, evaluate this limit to find the steady-state error. After\nevaluating, the answer will be one of the given options. Let's compute the steady-state error: e,, =\n25xlim,.9 [1 — G(s)/(1 + G(s))]. Solve for e,,: Using the given G(s) in the expression and\ncomputing the limit, you'll find that: e,, = 25x(1—1) = 0. So, the steady-state error for the test\ninput 25u(t) is: e,, = 0. Thus, the correct answer is: (A) 0.\n\nGround Truth: (A) 0\n\nFigure 93. A sample correct case of Mechanical Engineering (subfield: Control System).\nBack to List of Figures | Back to Table Index\n\n107\n", "vlm_text": "Materials: Control System \nQuestion:  For the unity feedback system shown in <image  $1\\!>,$  , where  $\\begin{array}{r}{G(s)=\\frac{450(s+8)(s+12)(s+15)}{s(s+38)(s^{2}+2s+28)}}\\end{array}$  .Find the steady-state errors for the following test inputs:   $25\\mathrm{u(t)}$  .  Option:\n\n \n(A) 0\n\n (B) 1\n\n (C) -1 \nThis image depicts a block diagram of a feedback control system. Key elements include:\n\n- \\( R(s) \\): The reference input.\n- \\( E(s) \\): The error signal, which is the difference between the reference input and the feedback.\n- The summation junction, where \\( R(s) \\) and the feedback signal are compared.\n- \\( G(s) \\): The transfer function of the system.\n- \\( C(s) \\): The output of the system.\n- A feedback loop that connects the output \\( C(s) \\) back to the summation point.\n\nThe system aims to minimize the error \\( E(s) \\) by adjusting the output \\( C(s) \\).\nGPT-4V:  To find the steady-state error for a unity feedback system, we can use the final value theorem. The final value theorem states that:    $l i m_{n\\rightarrow\\infty}e(t)=l i m_{n\\rightarrow\\infty}s E(s),$  , where E(s) is the Laplace transform of the error signal e(t). Given a system with unity feedback, the error signal in the Laplace domain can be given by:   $\\mathrm{E(s)\\;=\\;R(s)\\;-\\;C(s),}$  , where R(s) is the Laplace transform of the reference input, and C(s) is the Laplace transform of the system's output. The closed-loop transfer function for a unity feedback system is given by:  $T(s)=G(s)/(1+G(s))$  . Given a step input  $\\mathrm{{r(t)=}}$   $25\\mathrm{u(t)},$  , the Laplace transform is:    $R(s)=25/s\\mathrm{T}$  he system's output in the Laplace domain is:  $C(s)=$   $T(s){\\times}R(s)=G(s)/(1+G(s)){\\times}25/s$  . Thus, the error signal in the Laplace domain is:    $E(s)=25/s-$   $G(s)/(1+G(s)){\\times}25/s\\,=25/s{\\times}[1-G(s)/(1+G(s))]\\mathrm{T}\\mathrm{o}$   find the steady-state error, we apply the final value theorem:  $:\\operatorname*{lim}_{s\\to0}s\\times E(s)$  . Substitute E(s) from the above equation and evaluate the limit. \nGiven:    $G(s)=[450(s+8)(s+12)(s+15)]/=[s(s+38)(s^{2}+2s)(s+15)]$  Using the formula    $e_{s s}=$   $l i m_{s\\rightarrow0}s\\times25/s\\times[1-G(s)/(1+G(s))]$  . Now, evaluate this limit to find the steady-state error. After evaluating, the answer will be one of the given options. Let's compute the steady-state error:  $e_{s s}=$   $25\\times l i m_{s\\rightarrow0}\\;[1-{\\cal G}(s)/(1+{\\cal G}(s))]$  . Solve for    $e_{s s}$    : Using the given   $\\mathrm{{G(s)}}$   in the expression and computing the limit, you'll find that:    $e_{s s}=25{\\times}(1-1)=0$  . So, the steady-state error for the test input  $25\\mathrm{u(t)}$   is:    $e_{s s}=0$  . Thus, the correct answer is: (A) 0. \nGround Truth: (A) 0 "}
{"page": 107, "image_path": "doc_images/2311.16502v3_107.jpg", "ocr_text": "(©) Mechanical Engineering: Engineering Dynamics\n\nQuestion: The two small spheres, each of mass m, are rigidly connected by a rod of negligible mass\nand are released from rest in the position shown and slide down the smooth circular guide in the\nvertical plane. Determine their common velocity v as they reach the horizontal dashed position.\nAlso find the force N between sphere 1 and the supporting surface an instant before the sphere\nreaches the bottom position A.\n\nOption:\n\n(A) Velocity, force v = ./2gR,N = 2mg\n\n(B) Velocity, force v = ./gR, N = mg\n\n(©) Velocity, force v = ./gR, N = 2mg\n\n<image 1>\n\nGPT-4V: Two spheres, each of mass m. The spheres slide down a smooth circular guide in a vertical\nplane. The problem involves two parts: calculating the common velocity v of the spheres when they\nreach the horizontal dashed position; finding the normal force N between sphere 1 and the\nsupporting surface just before it reaches the bottom position A. Part 1: The potential energy lost by\nthe spheres will be equal to their change in kinetic energy. Potential energy lost by each sphere =\nmgh. For sphere 1:Height h, = R. For sphere 2: Height hz = 2RTotal potential energy lost: PE¢otai =\nmgh, + mghz = mgR + 2mgR = 3mgR. This potential energy is converted to kinetic energy for the\ntwo spheres: PEtorq, = 2X21mv? = /2gRv = 2gR. Part 2: Calculating the normal force N just before\nsphere 1 reaches the bottom position A, its centrifugal force will be mv?/R. Considering forces in the\n\n2\nvertical direction for sphere 1: N —mg = =. Using the velocity from Part 1, v = 2gRv = 2gRv =\n\n2gR. N=(1+2)mgN = (1 + V2)mgN = (1+ 2)mg. Since 2V2 is a little more than 1, N will be\nslightly more than 2mg but less than 3mg. The only choice which matches this range is 2mg. The\n\ncorrect option is: (A) v = 2gR,N = 2mg;v = ./2gR,N = 2mg.\n\nError Category: Reasoning Error\n\nError Reason: The GPT-4v reasoning for the first part is wrong, and the second part is also wrong.\nFor the first part, the potential energy for the two spheres is simply mgR, while the GPT computed it\nas 3mgR (and that leads to the wrong answer). Given the correct potential energy mgR, using\n\nenergy conservation, the kinetic energy 5 2mv? = mgR, which implies the required velocity to be\n\nVgR. For the second part, while GPT knows to use centrifugal force formula, the incorrect\ncomputation in velocity causes the computed result not to be in the options. GPT ends up with\n\napproximating the closest option. Indeed, by Centripetal force formula, N — mg = =, and plug\n\nv = /gR, we end up with N = 2mg.\nGround Truth: (C) Velocity, force v = ./gR,N = 2mg.\n\nFigure 94. A sample error case of Mechanical Engineering (subfield: Engineering Dynamics). Error category: Reasoning Error\nBack to List of Figures | Back to Table Index\n\n108\n", "vlm_text": "The image shows a red circle with a white \"X\" inside it. This symbol often represents a negative, error, or cancel action in various contexts.\nMechanical Engineering: Engineering Dynamics \nQuestion:  The two small spheres, each of mass  $m_{c}$  , are rigidly connected by a rod of negligible mass and are released from rest in the position shown and slide down the smooth circular guide in the vertical plane. Determine their common velocity    $v$  as they reach the horizontal dashed position. Also find the force    $N$  between sphere 1 and the supporting surface an instant before the sphere reaches the bottom position A.\n\n \nOption: \n(A) Velocity, force  $v={\\sqrt{2g R}}$  2𝑔𝑅 , 𝑁= 2𝑚𝑔\n\n (B) Velocity, force  $v=\\sqrt{g R},N=m g\n\n$  (C) Velocity, force  $v=\\sqrt{g R},N=2m g$  \nThe image depicts a physics diagram showing a scenario with two balls. One ball (labeled 1) is at the top of a curved ramp, and the other ball (labeled 2) is at the bottom. The ball 1 is connected to ball 2 with a rod of length \\( R \\). The setup appears to illustrate mechanics concepts, possibly involving motion or energy. An arrow labeled \\( v \\) indicates the direction of velocity, pointing to the left along a flat surface at the bottom.\nGPT-4V:  Two spheres, each of mass  $m$  . The spheres slide down a smooth circular guide in a vertical plane. The problem involves two parts: calculating the common velocity  $v$  of the spheres when they reach the horizontal dashed position; finding the normal force    $N$  between sphere 1 and the supporting surface just before it reaches the bottom position A. Part 1: The potential energy lost by the spheres will be equal to their change in kinetic energy. Potential energy lost by each sphere  $=$   𝑚𝑔ℎ . For sphere 1:Height  $h_{1}=R$  . For sphere 2: Height  $h_{2}=2R$  Total potential energy lost:    $P E_{t o t a l}=$   $m g h_{1}+m g h_{2}=m g R+2m g R=3m g R.$  .  This potential energy is converted to kinetic energy for the two spheres:  $P E_{t o t a l}=2{\\times}21m v^{2}={\\sqrt{2g R}}v=2g R$   Part 2: Calculating the normal force    $N$  just before sphere 1 reaches the bottom position A, its centrifugal force will be  $m v^{2}/R$  . Considering forces in the vertical direction for sphere 1:  $1\\colon N-m g={\\frac{m v^{2}}{R}}$  .  Using the velocity from Part 1,  $v=2g R v\\,=\\,{\\sqrt{2g R}}v=$  2𝑔𝑅.   $N=(1+2)m g N\\,=\\,{\\bigl(}1\\,+\\,{\\sqrt{2}}{\\bigr)}m g N=(1+2)m g.$    Since    $2\\sqrt{2}$   is a little more than 1,    $N$  will be slightly more than   $2\\mathrm{mg}$   but less than 3mg. The only choice which matches this range is  $2\\mathrm{mg}$  . The correct option is: (A)  $v=2g R,N=2m g;v=\\sqrt{2g R},N=2m g.$  \nError Category: Reasoning Error \nError Reason:  The GPT-  $_{4\\mathrm{v}}$   reasoning for the first part is wrong, and the second part is also wrong. For the first part, the potential energy for the two spheres is simply  𝑚𝑔𝑅 , while the GPT computed it as  3𝑚𝑔𝑅 (and that leads to the wrong answer). Given the correct potential energy    $m g R.$  , using energy conservation, the kinetic energy  $\\begin{array}{r}{\\frac122m v^{2}=m g R,}\\end{array}$  , which implies the required velocity to be  $\\sqrt{g R}$  For the second part, while GPT knows to use centrifugal force formula, the incorrect computation in velocity causes the computed result not to be in the options.  GPT ends up with approximating the closest option.  Indeed, by Centripetal force formula,    $\\begin{array}{r}{N-m g=\\frac{m v^{2}}{R}}\\end{array}$    , and plug  $v={\\sqrt{g R}}.$  we end up with  $N=2m g$  . \nGround Truth: (C) Velocity, force  ${\\pmb v}=\\sqrt{{\\pmb g R}}\\,,{\\pmb N}={\\pmb2m g}$  . "}
{"page": 108, "image_path": "doc_images/2311.16502v3_108.jpg", "ocr_text": "@ Mechanical Engineering: Engineering Dynamics\n\nQuestion: With what minimum horizontal velocity u can a boy throw a rock at A and have it just\nclear the obstruction at B?\n\nOption:\n\n(A) Minimal v,, = 18.014m/s\n\n(B) Minimal v,, = 24.014m/s\n\n(C) Minimal v,, = 28.014m/s\n\nkK >|\nAtay 40m\nye ae =\ney ne\n~\neB\n7 \\\n26 m _\n16m\nYt =: a ET le\n——\n<image 1>\n\nGPT-4V: To solve this problem, we need to use the kinematic equations of motion for a projectile.\nStep 1: Determine the time it takes for the rock to fall a vertical distance of (26m-16m)=10m after\nclearing the obstruction at B. Using the equation: s = 59t?, where s is the vertical displacement =\n10m, g is the acceleration due to gravity = 9.81m/s, and the falling time ~1.428s. Step 2: Determine\nthe horizontal distance covered in this time. This is the distance from A to the point right before the\nobstruction at B. Given that the total horizontal distance is 40m, the distance from A to just before B\nis 40m. Step 3: Determine the time it takes for the rock to fall 26m from A. Using the equation: s =\n; gt”, where s = 26m, we obtain t ~ 2.302s. Step 4: Using time from Step 3, determine the horizontal\ndistance the rock covers from A =40m. Step 5: Equating the expressions for u from Step 2 and Step 4,\nSolving for width we get: width 17.469m. Plugging this into the equation from Step 2: u~\n15.775m/s. But we made a mistake. When the rock just clears the obstruction, it doesn’t have a\nvertical velocity of 0. In fact, its vertical velocity v when it clears the obstruction at B is given by:\nv? = u? — 2gsv? = u? — 2gs, where s = 10m. Plugging in u~ 15.775m/s to obtain v ~ 24.014m/s.\n\nError Category: Reasoning Error\n\nError Reason: The first step of GPT-4v calculation is correct, where the time stay beyond point B\nvertically is 2h/g = 1.428s,g is the acceleration due to gravity = 9.81m/s and h=10m. the second\nstep is also correct, which says the horizontal distance to pass point B is 40m. The third step is\nwrong as we don't care about the time it takes for the rock to fall 26m from A. Therefore, the\nsequential calculation is incorrect. The correct calculation should be 40m/1.428s is approximated as\n28.014m/s.\n\nGround Truth: (C) Minimal v,, = 28.014m/s\n\nFigure 95. A sample error case of Mechanical Engineering (subfield: Engineering Dynamics). Error category: Reasoning Error\nBack to List of Figures | Back to Table Index\n\n109\n", "vlm_text": "This image shows a red circle with a black outline containing a white \"X\" or cross in the center. It typically signifies an error, cancellation, or denial symbol.\nMechanical Engineering: Engineering Dynamics \nQuestion:  With what minimum horizontal velocity  $u$  can a boy throw a rock at A and have it just clear the obstruction at B?\n\n \nOption: \n(A) Minimal    $v_{m}=18.014m/s\n\n$  (B) Minimal    $v_{m}=24.014m/s\n\n$  (C) Minimal    $v_{m}=28.014m/s$  \nThis image illustrates a scenario involving projectile motion. A person is depicted at point A on a cliff, preparing to launch an object with an initial velocity \\( u \\) horizontally. The object follows a parabolic trajectory, descending 26 meters, and covers a horizontal distance of 40 meters. The point B is located on a peak that is 16 meters above the ground level. The trajectory is designed to show the path of the projectile from point A to point B.\nGPT-4V:  To solve this problem, we need to use the kinematic equations of motion for a projectile. Step 1: Determine the time it takes for the rock to fall a vertical distance of   $\\mathrm{(26m-16m)=}10\\mathrm{m}$   after clearing the obstruction at B. Using the equation:    $\\begin{array}{r}{s={\\frac{1}{2}}g t^{2}.}\\end{array}$  , where    $s$  is the vertical displacement   $=$   $10\\mathrm{m},\\,g$  is the acceleration due to gravity  $=9.81\\mathrm{m/s}$  , and the falling time  ${\\approx}1.428s$  . Step 2: Determine the horizontal distance covered in this time. This is the distance from A to the point right before the obstruction at B. Given that the total horizontal distance is  $40\\mathrm{m},$  , the distance from A to just before B is  $40\\mathrm{m}$  . Step 3: Determine the time it takes for the rock to fall   $26\\mathrm{m}$   from A. Using the equation:    $s=$   ${\\scriptstyle{\\frac{1}{2}}}g t^{2}.$  , where  $s=26\\mathrm{m,\\Omega}$  , we obtain    $t\\approx2.302s$  . Step 4: Using time from Step 3, determine the horizontal distance the rock covers from  $\\mathrm{A=}40\\mathrm{m}$  . Step 5: Equating the expressions for  $u$  from Step 2 and Step 4, Solving for width we get: width  $\\approx\\,17.469\\mathrm{m}$  . Plugging this into the equation from Step 2:    $u\\approx$   $15.775\\mathrm{\\bar{m}/\\mathrm{s}}$  . But we made a mistake. When the rock just clears the obstruction, it doesn‘t have a vertical velocity of 0. In fact, its vertical velocity    $v$  when it clears the obstruction at B is given by:  $v^{2}=u^{2}-2g s\\dot{v^{2}}=u^{2}-2g s,$  , where  $s=10\\mathrm{m}$  . Plugging in  $u\\approx15.775\\mathrm{m/s}$   to obtain  $v\\,\\approx\\,24.014\\mathrm{m/s}$  . \nError Category: Reasoning Error \nError Reason:  The first step of GPT-4v calculation is correct, where the time stay beyond point B vertically is  $\\sqrt{2h/g}=1.428s,g$  is the acceleration due to gravity  $=9.81\\mathrm{m/s}$   and    $h{=}10\\mathrm{m}$  . the second step is also correct, which says the horizontal distance to pass point  $\\mathrm{B}$   is  $40\\mathrm{m}$  .  The third step is wrong as we don't care about the time it takes for the rock to fall   $\\mathbf{26m}$   from A.  Therefore, the sequential calculation is incorrect. The correct calculation should be  $40\\mathrm{m}/1.428\\mathrm{s}$   is approximated as  $28.014\\mathrm{m/s}$  . \nGround Truth: (C) Minimal    $\\pmb{v_{m}}=28.014\\pmb{m}/s$  "}
{"page": 109, "image_path": "doc_images/2311.16502v3_109.jpg", "ocr_text": "Distribution of ImageTypes\n\nAdvertisements | 12\nLogos and Branding | 17\nDNA Sequences | 21\nLandscapes | 21\n3D Renderings | 21\nHistorical Timelines J 32\nIcons and Symbols ff 46\nPoster [i 67\nOther [68\nScreenshots\nPortraits\nSculpture\n\nTechnical Blueprints\nSketches and Drafts\nBody Scans: MRI...\nMicroscopic Images\nPathological Images\nMedical Images\nGeometric Shapes\nSheet Music\nPaintings\n\nChemical Structures\n\nBe Mathematical Notations\nComics and Cartoons\n\na ‘Trees and Graphs\n\n6p Maps\n\ni}\n\n&\n\nPhotographs\nPlots and Charts\n\nCoble en 2:7\nDiagras nn 35>\n\n0 500 1000 1500\n\nData Count\n\nFigure 96. Distribution of image types in the MMMU dataset.\n\nC. Subfields of Different Subjects\n\nIn this appendix, we show all the subfields of each subject\nin Table 11. MMMU has 183 subfields in total, covering 30\nsubjects.\n\nD. Distributions of Image Types\n\nIn this section, we show the distribution of 30 different im-\nage types in the 11.5K MMMU questions. The distribution\nof various image types is displayed in Figure 96. A horizon-\ntal bar chart was employed to visually represent the number\nof samples in each image category. The figure shows that\nthe MMMU dataset encompasses a diverse range of image\ntypes, from Advertisements to Diagrams.\n\nE. Results on Different Image Types\n\nIn this section, we report the performance of some selected\nmodels on 30 different image types in Table 12.\n\n110\n\n4000\n", "vlm_text": "Distribution of ImageTypes \nThe image is a horizontal bar chart showing the distribution of image types in the MMMU dataset. The types are listed on the left with their corresponding data counts on the x-axis.\n\nHere are some of the image types and their counts:\n\n- Advertisements: 12\n- Logos and Branding: 17\n- DNA Sequences: 21\n- Diagrams: 3466 \n- Tables: 2474\n- Photographs: 870\n- Paintings: 511\n\nDiagrams have the highest count, followed by tables and plots and charts.\nC. Subfields of Different Subjects \nIn this appendix, we show all the subfields of each subject in  Table 11 . MMMU has 183 subfields in total, covering 30 subjects. \nD. Distributions of Image Types \nIn this section, we show the distribution of 30 different im- age types in the 11.5K MMMU questions. The distribution of various image types is displayed in  Figure 96 . A horizon- tal bar chart was employed to visually represent the number of samples in each image category. The figure shows that the MMMU dataset encompasses a diverse range of image types, from Advertisements to Diagrams. \nE. Results on Different Image Types \nIn this section, we report the performance of some selected models on 30 different image types in  Table 12 . "}
{"page": 110, "image_path": "doc_images/2311.16502v3_110.jpg", "ocr_text": "Disciplines Subjects Subfields\n\nFine Arts, Drawing and Painting, Photography, Printmaking, Ceramic Art, Visual Arts,\n\nArt\nm Sculpture, AI Content Detection\nae Design Digital Art, Design History, Graphic Design, Fashion Design, Interior Design, Industrial Design\nDesign Music Music\n‘Art Theor History of Art Theory, Art History, Art Criticism, Aesthetics, Contemporary Art Theory,\ny Visual Culture, Postmodern Art Theory, Phenomenology of Art\nAccounting Financial Accounting, Investment, Managerial Accounting\nEconomics Macroeconomics, Microeconomics, Econometrics, Labor Economics, Principals of Economics\nFinance Financial Marketing, Financial Management, Corporate Finance, Managerial Finance\nBusiness\nManage Operations Management, Strategic Management, Business Management, Project Management,\n8 Cost Management, Principles of Management, Management Models\nMarketing Market Research\nBiolo: Biochemistry, Cell Biology, Genetics, Microbiology, Botany, Evolution, Animal Behavior, Physiology,\nBy Molecular Biology, Animal Physiology, Ecology\nChemist Inorganic Chemistry, Organic Chemistry, Physical Chemistry, Chemical Thermodynamics,\nsty Analytical Chemistry, Chemical Kinetics, Biochemistry, Quantum Chemistry\nGeograph Geotechnical Engineering, Human Geography, Physical Geography, Geographic Information Systems,\nScience erapay International Geography Olympiad\nMath Calculus, Probability and Statistics, Linear Algebra, Geometry, Logic, Graph Theory, Group Theory,\nOperation Research\nPhysic Classical Mechanics, Electromagnetism, Thermodynamics and Statistical Mechanics, Optics, Nuclear Physics\nPsycholo; Biological Psychology, Cognitive Psychology, Personality Psychology, Clinical Psychology,\nsy ey Social Psychology, Developmental Psychology, Abnormal Psychology\nBasic Immunology, Biochemistry and Genetics, Foundational Anatomical Sciences, Microbiology and Immunology,\nMe dical Science Neurosciences, Anatomy, Neuroanatomy, Neurophysiology, Cardiovascular Physiology, Human Physiology,\nReproductive Physiology, Respiratory Physiology, Renal Physiology, Pathophysiology, Cellular Physiology\nClinical Medicine, Dental, Circulatory, Respiratory, Clinical Neurology, Orthopaedic Surgery,\nClinical Medicine Heart Disease, Endocarditis, Cardiovascular Medicine, Endocrinology, Otolaryngology, Ophthalmology,\nHealth a . .\n& Urology, Clinical Pathology, Clinical Radiology\nMedicine Diagnostics & Medical Imaging, Neuropathology, Pathology, Ophthalmic Pathology, Forensic Neuropathology,\nLaboratory Medicine  Electrocardiography, Radiology\nPharmac Pharmaceutical Microbiology, Medicinal Chemistry, Biochemistry for Pharmaceutical Sciences,\ny Pharmacology and Drug Synthesis\nPublic Health Epidemiology, Biostatistics, Communicable Disease Control\nHumanities History U.S. History, World History, Modern History, European History, History-Comparison\ns, & f Literature American Literature, Poetry, Fiction, Drama, Children’s Literature, Comparative Literature, Contemporary Literature\n‘ocial\nScience Sociology Sociology Theory, Social Economics, Political Economics.\n‘Aericulture Animal Physiology, Animal Science, Animal Nutrition, Reproduction, Genetics, Plant Physiology,\n8 Plant Pathology, Animal and Environment, Animal Anatomy\nArchitecture Surveying and Mapping, Structural Engineering, Water Resources Engineering, Civil Engineering\nTech Computer Science Data Structure and Algorithm, Computer Network, Artificial Intelligence, Databases,\n& Operating Systems, Compiler Principle, Computer Architecture\nEngineering Electronics Analog electronics, Digital electronics, Electrical Circuit, Signal Processing\nEnergy & Power Thermodynamics, Heat Transfer, Fluid Mechanics\nMaterials Materials Science, Mechanics of Materials\nMechanical Fluid Dynamics, Mechanical Design, Mechanics of Materials, Mechanical Vibrations,\nEngineering Engineering Dynamics, Control Systems, Engineering Graphics\n\nTable 11. Subfields of each subject.\n\n111\n", "vlm_text": "The table categorizes various academic disciplines into subjects and their respective subfields. Here’s a breakdown:\n\n### Art & Design:\n- **Art**: Fine Arts, Drawing, Painting, Photography, etc.\n- **Design**: Digital Art, Graphic Design, Interior Design, etc.\n- **Music**: Music\n- **Art Theory**: Art History, Criticism, Aesthetics, etc.\n\n### Business:\n- **Accounting**: Financial, Investment, Managerial Accounting\n- **Economics**: Macroeconomics, Microeconomics, etc.\n- **Finance**: Marketing, Management, Corporate Finance, etc.\n- **Management**: Strategic, Business Management, etc.\n- **Marketing**: Market Research\n\n### Science:\n- **Biology**: Genetics, Microbiology, Ecology, etc.\n- **Chemistry**: Organic Chemistry, Physical Chemistry, etc.\n- **Geography**: Human, Physical Geography, etc.\n- **Math**: Calculus, Algebra, Geometry, etc.\n- **Physics**: Mechanics, Electromagnetism, etc.\n- **Psychology**: Cognitive, Clinical, Social Psychology, etc.\n\n### Health & Medicine:\n- **Basic Medical Science**: Immunology, Genetics, Human Physiology, etc.\n- **Clinical Medicine**: Dental, Circulatory, Respiratory, etc.\n- **Diagnostics & Laboratory Medicine**: Pathology, Radiology, etc.\n- **Pharmacy**: Microbiology, Pharmaceutical Sciences, etc.\n- **Public Health**: Epidemiology, Biostatistics, etc.\n\n### Humanities & Social Science:\n- **History**: U.S., World, Modern, European History\n- **Literature**: Poetry, Fiction, Drama, etc.\n- **Sociology**: Theory, Social Economics, etc.\n\n### Tech & Engineering:\n- **Agriculture**: Physiology, Nutrition, Plant and Animal Pathology, etc.\n- **Architecture**: Surveying, Structural Engineering, etc.\n- **Computer Science**: Data Structure, AI, Databases, etc.\n- **Electronics**: Analog, Digital Electronics, etc.\n- **Energy & Power**: Thermodynamics, Heat Transfer, etc.\n- **Materials**: Mechanics of Materials\n- **Mechanical**: Dynamics, Mechanics, Vibrations, etc."}
{"page": 111, "image_path": "doc_images/2311.16502v3_111.jpg", "ocr_text": "Fuyu Qwen-VL_ InstructBLIP LLaVA-1.5 BLIP-2 FLAN\nImage Types #Samples -8B 7B -TS-XXL -13B -TS-XXL GPT-4V\nTest Overall 10500 27.4 32.9 33.8 33.6 34.0 557\nDiagrams 3184 27.6 30.1 31.8 30.0 32.0 46.8\nTables 2267 26.6 29.0 29.8 27.8 27.8 61.8\nPlots and Charts 840 24.8 31.8 36.2 30.4 35.8 55.6\nChemical Structures 573 25.0 27.2 27.1 26.7 25.5 50.6\nPhotographs 770 27.6 40.5 414 44.4 42.0 64.2\nPaintings 453 28.7 57.2 53.6 56.3 52.1 75.9\nGeometric Shapes 336 21.1 25.3 214 25.6 28.3, 40.2\nSheet Music 335 35.2 33.4 34.6 35.8 34.9 38.8\nMedical Images 272 25.4 29.8 31.6 36.4 29.8 59.6\nPathological Images 253 26.5 27.7 31.2 35.2 35.6 63.6\nMicroscopic Images 226 27.0 37.6 29.2 36.3 32.7 58.0\nMRI, CT scans, and X-rays 198 21.7 36.9 33.3 39.4 29.8 50.0\nSketches and Drafts 184 37.0 32.1 29.9 38.0 33.7 55.4\nMaps 170 38.2 36.5 45.9 47.6 43.5 61.8\nTechnical Blueprints 162 24.7 25.9 28.4 25.3 27.8 38.9\nTrees and Graphs 146 30.1 28.1 28.8 28.8 34.9 50.0\nMathematical Notations 133 15.8 27.1 22.6 21.8 21.1 45.9\nComics and Cartoons 131 29.0 51.9 49.6 54.2 51.1 68.7\nSculpture 117 30.8 46.2 49.6 51.3 53.0 76.1\nPortraits 91 20.9 52.7 46.2 54.9 47.3 70.3\nScreenshots 70 38.6 35.7 38.6 34.3 47.1 65.7\nOther 60 28.3 38.3 50.0 51.7 58.3. 68.3\nPoster 57 38.6 50.9 52.6 61.4 64.9 80.7\nIcons and Symbols 42 23.8 66.7 57.1 59.5 59.5 78.6\nHistorical Timelines 30 30.0 36.7 40.0 43.3 43.3 63.3\n3D Renderings 21 33.3 28.6 57.1 38.1 47.6 47.6\nDNA Sequences 20 20.0 45.0 25.0 25.0 45.0 55.0\nLandscapes 16 43.8 43.8 50.0 31.2 62.5 68.8\nLogos and Branding 14 21.4 57.1 64.3 35.7 50.0 85.7\nAdvertisements 10 30.0 60.0 50.0 60.0 70.0 100.0\n\nTable 12. Selected models’ performance on 30 different image types. Note that a single image may have multiple image types.\n\n112\n", "vlm_text": "The table shows a comparison of different models' performance across various image types. Here's a summary of the columns:\n\n- **Image Types**: Categories of images used in the test.\n- **#Samples**: Number of samples for each image type.\n- **Fuyu -8B**: Performance scores for this model.\n- **Qwen-VL -7B**: Performance scores for this model.\n- **InstructBLIP -T5-XXL**: Performance scores for this model.\n- **LLaVA-1.5 -13B**: Performance scores for this model.\n- **BLIP-2 FLAN -T5-XXL**: Performance scores for this model.\n- **GPT-4V**: Performance scores for this model; consistently the highest scores.\n\nEach row represents the performance for a specific image type, with \"Test Overall\" providing the general performance score across all image types."}
{"page": 112, "image_path": "doc_images/2311.16502v3_112.jpg", "ocr_text": "F. Few-shot Results\n\nAs existing models like OpenFlamingo and Otter support\nfew-shot or in-context learning, we report their few-shot\nperformance using the dev set as the in-context learning ex-\namples.\n\nAs shown in Table 13, OpenFlamingo shows a decrease\nin performance when moving from 0-shot to 1-shot and 3-\nshot learning (from 0.263 to 0.256) and there is a slight in-\ncrease when moving to 5-shot. Otter shows a consistent\ndecline as more shots are introduced, dropping to 0.276 in\n1-shot and further down to 0.258 in 3-shot and 5-shot. This\ntrend suggests that existing open-source models’ few-shot\nlearning ability is very weak. And it additionally shows that\nour data samples might be too hard for these models to un-\nderstand the underlying patterns or context.\n\nOshot Ishot 3shot Sshot\n\nOpenFlamingo 0.263 0.256 0.259 0.264\nOtter 0.291 0.276 0.258 0.258\n\nTable 13. Few-shot results of OpenFlamingo and Otter.\n\n113\n", "vlm_text": "F. Few-shot Results \nAs existing models like Open Flamingo and Otter support few-shot or in-context learning, we report their few-shot performance using the dev set as the in-context learning ex- amples. \nAs shown in  Table 13 , Open Flamingo shows a decrease in performance when moving from 0-shot to 1-shot and 3- shot learning (from 0.263 to 0.256) and there is a slight in- crease when moving to 5-shot. Otter shows a consistent decline as more shots are introduced, dropping to 0.276 in 1-shot and further down to 0.258 in 3-shot and 5-shot. This trend suggests that existing open-source models’ few-shot learning ability is very weak. And it additionally shows that our data samples might be too hard for these models to un- derstand the underlying patterns or context. \nThe table presents comparative performance data for two models: OpenFlamingo and Otter. The performance is measured across four scenarios:\n\n- 0 shot\n- 1 shot\n- 3 shot\n- 5 shot\n\nThe values indicate some form of metric, possibly accuracy or another performance measure, for each model under the given scenarios:\n\n- **OpenFlamingo:** \n  - 0 shot: 0.263\n  - 1 shot: 0.256\n  - 3 shot: 0.259\n  - 5 shot: 0.264\n\n- **Otter:** \n  - 0 shot: 0.291\n  - 1 shot: 0.276\n  - 3 shot: 0.258\n  - 5 shot: 0.258\n\nOtter generally has higher scores in 0 shot and 1 shot scenarios compared to OpenFlamingo, while OpenFlamingo performs slightly better in the 5 shot scenario."}
{"page": 113, "image_path": "doc_images/2311.16502v3_113.jpg", "ocr_text": "G. Data Annotation Protocol\n\nThis document describes a comprehensive protocol for\nannotating a dataset comprising college-level multimodal\nquestions (i.e., questions that incorporate images).\n\nG.1. Data Collection\n\nSources of Data: Data is primarily collected from free\nonline resources, quizzes, textbooks, and other study ma-\nterials. When collecting questions, the annotators should\nstrictly adhere to copyright and licensing regulations on the\nsource sites. Data from sources that prohibit copying or re-\ndistribution MUST be explicitly avoided. Besides, the an-\nnotators should try to find diverse sources instead of collect-\ning questions from a single source.\n\nTypes of Questions:\n\n¢ Multiple-Choice Questions: Including — standard\nmultiple-choice questions and true/false questions. These\nare characterized by a question followed by several\nanswer choices, with only one correct option.\n\n* Open-Ended Questions: Encompassing formats like\nfactoid, fill-in-the-blank, calculation-based, and short de-\nscriptive responses. Avoid collecting questions that have\nvery long answers.\n\nImage Types: The annotators should find various types of\n\nimages (e.g., diagrams, charts, photographs)\n\nG.2. General Guidelines\n\n¢ General Principles: Annotations must be accurate, con-\nsistent, and adhere to a high standard of academic rigor.\nSpecific Instructions:\n\nAll questions must contain one or more images.\n\nAll questions should be written in English.\n\nAll questions should meet the college-level difficulty.\nThe question should not be ambiguous and can be an-\nswered with one of the given options or a short answer.\nClearly categorize each question as either multiple-\nchoice or open-ended.\n\nAnnotate all fields, including the question, answer op-\ntions for multiple-choice questions, the correct answer,\nimage types, question difficulty, and explanation (if\nthere exists).\n\nG.3. Data Format and Structure\n\n¢ JSON File Format: The structured JSON format will in-\nclude fields for number, question type, question text, an-\nswer options (for multiple-choice), correct answer, ques-\ntion difficulty, and explanation (if there exists).\n\n¢ Naming Conventions:\n\n- Each collected sample will be stored in a separate\nJSON file following a standard naming rule: sub-\nject_{Number}.json\n\n— Image Files: image_{QuesNum}_{ImageNum}.png\n\n114\n\n¢ Interleaving Question with Images: The images\nshould be inserted as a file path in the ques-\ntion/options/explanations.\n\nG.4. Quality Control and Validation\n\n¢ A secondary review team will rigorously vet annotations\nfor quality and guideline adherence.\n\n¢ Regular audits of random samples from the dataset will\nbe conducted to ensure sustained quality and consistency.\n\nG.5. Handling Ambiguities\n\nAmbiguities or unclear data instances should be flagged for\na detailed review process. These questions will be collabo-\nratively examined in team meetings to establish a standard-\nized approach for annotation.\n\nG.6. Ethical Considerations\n\n¢ Copyright and Licensing: Strict adherence to copy-\nright and licensing regulations is mandatory. Data from\nsources that prohibit copying or redistribution will be ex-\nplicitly avoided.\n\n¢ Data Privacy: Compliance with privacy laws and ethical\nstandards in data handling is paramount. The annotators\nshould avoid collecting questions that contain any private\ninformation.\n\nG.7. Data Contamination Considerations\n\nIn the construction of benchmarks for evaluating foundation\nmodels, it is essential to consider the risk of data contam-\nination. To address this, annotators should be tasked with\ncarefully selecting questions that go beyond straightforward\nqueries with easily accessible answers. Instead, the focus\nshould be on questions whose answers are tucked away in\nless obvious locations, such as in separate documents or\nhidden in the concluding sections of extensive textbooks.\nThis approach is beneficial for constructing benchmarks\nthat truly test the model’s ability to comprehend and syn-\nthesize information from diverse and challenging sources.\n\nG.8. Example Questions\n\nDetailed examples of annotated questions are provided in\n\nan appendix to serve as a reference for the annotators.\n\n¢ Multiple-choice Questions: Figure 97 shows an example\nof a multiple-choice question.\n\n* Open-ended Questions: Figure 98 shows an example of\nthe open-ended question.\n\nBesides, the annotators are encouraged to collect ques-\ntions that contain multiple images within a single exam-\nple. This type of question requires special attention to\nfile naming so that each image can be correctly referenced.\nFigure 99 shows an example of a multiple-image question\nalong with its JSON representation.\n", "vlm_text": "G. Data Annotation Protocol \nThis document describes a comprehensive protocol for annotating a dataset comprising college-level multimodal questions (i.e., questions that incorporate images). \nG.1. Data Collection \nSources of Data: Data is primarily collected from free online resources, quizzes, textbooks, and other study ma- terials. When collecting questions, the annotators should strictly adhere to copyright and licensing regulations on the source sites. Data from sources that prohibit copying or re- distribution MUST be explicitly avoided. Besides, the an- notators should try to find diverse sources instead of collect- ing questions from a single source.\n\n \nTypes of Questions: \n•  Multiple-Choice Questions: Including standard multiple-choice questions and true/false questions. These are characterized by a question followed by several answer choices, with only one correct option.\n\n •  Open-Ended Questions: Encompassing formats like factoid, fill-in-the-blank, calculation-based, and short de- scriptive responses. Avoid collecting questions that have very long answers. \nImage Types:  The annotators should find various types of images (e.g., diagrams, charts, photographs)\n\n \nG.2. General Guidelines \n•  General Principles:  Annotations must be accurate, con- sistent, and adhere to a high standard of academic rigor. \n•  Specific Instructions: \n–  All questions must contain one or more images. –  All questions should be written in English. –  All questions should meet the college-level difficulty. –  The question should not be ambiguous and can be an- swered with one of the given options or a short answer. –  Clearly categorize each question as either multiple- choice or open-ended. –  Annotate all fields, including the question, answer op- tions for multiple-choice questions, the correct answer, image types, question difficulty, and explanation (if there exists).\n\n \nG.3. Data Format and Structure \n•  JSON File Format:  The structured JSON format will in- clude fields for number, question type, question text, an- swer options (for multiple-choice), correct answer, ques- tion difficulty, and explanation (if there exists).\n\n •  Naming Conventions: –  Each collected sample will be stored in a separate JSON file following a standard naming rule: sub- ject  { Number } .json –  Image Files:  image  { QuesNum } { ImageNum } .png •  Interleaving Question with Images: The images should be inserted as a file path in the ques- tion/options/explanations.\n\n \n\nG.4. Quality Control and Validation \n• A secondary review team will rigorously vet annotations for quality and guideline adherence.\n\n • Regular audits of random samples from the dataset will be conducted to ensure sustained quality and consistency. \nG.5. Handling Ambiguities \nAmbiguities or unclear data instances should be flagged for a detailed review process. These questions will be collabo- ratively examined in team meetings to establish a standard- ized approach for annotation.\n\n \nG.6. Ethical Considerations \n•  Copyright and Licensing:  Strict adherence to copy- right and licensing regulations is mandatory. Data from sources that prohibit copying or redistribution will be ex- plicitly avoided.\n\n •  Data Privacy:  Compliance with privacy laws and ethical standards in data handling is paramount. The annotators should avoid collecting questions that contain any private information. \nG.7. Data Contamination Considerations \nIn the construction of benchmarks for evaluating foundation models, it is essential to consider the risk of data contam- ination. To address this, annotators should be tasked with carefully selecting questions that go beyond straightforward queries with easily accessible answers. Instead, the focus should be on questions whose answers are tucked away in less obvious locations, such as in separate documents or hidden in the concluding sections of extensive textbooks. This approach is beneficial for constructing benchmarks that truly test the model’s ability to comprehend and syn- thesize information from diverse and challenging sources. \nG.8. Example Questions \nDetailed examples of annotated questions are provided in an appendix to serve as a reference for the annotators.\n\n •  Multiple-choice Questions:  Figure 97  shows an example of a multiple-choice question.\n\n •  Open-ended Questions:  Figure 98  shows an example of the open-ended question. Besides, the annotators are encouraged to collect ques- tions that contain multiple images within a single exam- ple. This type of question requires special attention to file naming so that each image can be correctly referenced. Figure 99  shows an example of a multiple-image question along with its JSON representation. "}
{"page": 114, "image_path": "doc_images/2311.16502v3_114.jpg", "ocr_text": "{\n\nThe articulation points of the given graph are:\n\n\"question_type\": \"multiple-choice\",\n\"subfield\": \"Data Structure\",\n\n\"question\": \"The articulation points of the\ngiven graph are: <img='./images/q_1_1.png'>\",\n\n\"options\": [\nAEC \"E,C',\n\"DET\",\nB.DE,! \"A,B, C,D,I'\",\n\"DI\"\nC. A,B,C, D,! 1\n\"answer\": \"D\"\n0.D! 1 jon\": tt\nexplanation\":\n\n}\n\nFigure 97. Multiple-choice question and its JSON representation.\n\nThe Math Team designed a logo shaped like a multiplication\nsymbol, shown below on a grid of 1-inch squares. What is\nthe area of the logo in square inches?\n\n\"No\":'4\",\n\n\"question_type\": \"open\",\n\n\"subfield\": \"Geometry\",\n\n\"question\": \"The Math Team designed a logo shaped\nlike a multiplication symbol, shown below ona grid\nof 1-inch squares. What is the area of the logo in\nsquare inches? <img='./images/q_4_1.png'> \",\n\"options\": [],\n\n\"answer\": \"10\",\n\n\"explanation\": \"We see these lines split the figure\ninto five squares with side length $\\ \\ sqrt2$.\n\nThus, the area is $5\\\\cdot\\ \\left(\\\\sqrt2\\ \\right)*2\n=5\\\\cdot 2 = 108.\"\n\nFigure 98. Open question and its JSON representation.\n\n‘7.braw the curve ofr? = sin(28) from 0 < 8 < 2x\n\nOs % +)\n\n}\n\n\"No':\"5\",\n\n\"subfield\": \"Calculus\",\n\"question_type\":\"multiple-choice\",\n\n\"question\": \"Draw the curve of$ r2=sin(2\\ \\ theta )$\nfrom $0\\\\leq \\\\theta \\\\leq2\\\\ pi$ .\",\n\n\"options\": [\n\n\"<img='./images/a_5_1.png'>\",\n'./images/a_5_2.png'>\",\nimages/a_5_3.png'>\",\n/images/a_5_4.png'>\"\n\n\"<img:\n\n\"<img='\n\n\"<img=\nL\n\n\"answer\": \"D\",\n\nmmm\n\n\"explanation\":\n\nFigure 99. Multiple image question and its JSON representation.\n\nH. Author Contribution Statement\n\nAll authors made significant contributions to data collec-\ntion, annotation, and validation. We authors contributed to\n1/3 of the MMMU examples. Additionally, all authors con-\n\n115\n\ntributed to the case study and error analysis, plotting case\nstudy figures in the Appendix. Besides, all authors partic-\nipated in the discussion of data annotation, provided feed-\nback on the project, and proofread the paper. The following\nauthors made additional contributions:\n", "vlm_text": "The image shows a multiple-choice question about the articulation points of a given graph. The graph is on the left, and there are four options for answers:\n\nA. E, C  \nB. D, E, I  \nC. A, B, C, D, I  \nD. D, I\n\nOption D (D, I) is selected.\n\nOn the right, there's a JSON representation of this question which includes:\n\n- \"No\": \"1\"\n- \"question_type\": \"multiple-choice\"\n- \"subfield\": \"Data Structure\"\n- \"question\": \"The articulation points of the given graph are...\"\n- \"options\": [\"E, C\", \"D, E, I\", \"A, B, C, D, I\", \"D, I\"]\n- \"answer\": \"D\"\n- \"explanation\": \"\"\nThe image contains a geometry problem and its JSON representation. It shows a grid of 1-inch squares with a logo shaped like a multiplication symbol. The task is to find the area of this logo.\n\nThe JSON representation specifies:\n- Question number: \"4\"\n- Question type: \"open\"\n- Subfield: \"Geometry\"\n- Question text: \"The Math Team designed a logo shaped like a multiplication symbol, shown below on a grid of 1-inch squares. What is the area of the logo in square inches?\"\n- Answer: \"10\"\n- Explanation: The lines split the figure into five squares, each with side length \\(\\sqrt{2}\\), making the area \\(5 \\cdot 2 = 10\\) square inches.\nThe image displays a multiple-choice question about drawing the curve of \\( r^2 = \\sin(2\\theta) \\) from \\( 0 \\leq \\theta \\leq 2\\pi \\). \n\nOn the left, it shows multiple graph options with a \"select\" button next to each. On the right, there's a JSON representation of the question:\n\n- **No:** \"5\"\n- **Subfield:** \"Calculus\"\n- **Question Type:** \"multiple-choice\"\n- **Question:** \"Draw the curve of \\( r^2 = \\sin(2 \\theta) \\) from \\( 0 \\leq \\theta \\leq 2\\pi \\).\"\n- **Options:** Links to images, presumably representing different curve options.\n- **Answer:** \"D\"\n- **Explanation:** An empty string.\n\nThe correct answer is indicated as option \"D\".\nH. Author Contribution Statement \nAll authors made significant contributions to data collec- tion, annotation, and validation. We authors contributed to 1/3 of the MMMU examples. Additionally, all authors con- tributed to the case study and error analysis, plotting case study figures in the Appendix. Besides, all authors partic- ipated in the discussion of data annotation, provided feed- back on the project, and proofread the paper. The following authors made additional contributions: \n"}
{"page": 115, "image_path": "doc_images/2311.16502v3_115.jpg", "ocr_text": "Xiang Yue conceived and led the project, outlining the pri-\nmary objectives, establishing the data collection methodol-\nogy and protocol, designing and running experiments, as\nwell as doing follow-up analysis. Xiang Yue also took the\nlead in writing the manuscript, drafting the original text,\nand incorporating revisions from co-authors. In addition,\nXiang Yue managed project administration and coordinated\nthe collaboration between 20+ coauthors and 30+ student\nannotators, ensuring the project’s milestones were met and\nfacilitating communication among team members. Xiang\nYue also took the lead in the dataset release.\n\nYuansheng Ni co-led the data curation process with Xiang\nYue. Specifically, Yuansheng Ni developed the protocols\nfor data quality assurance, standardizing the data annotation\nprocedures, and supervising the team of data annotators to\nensure consistency and accuracy. In addition to data cura-\ntion, Yuansheng Ni also played a collaborative role in data\nanalysis, offering critical insights that shaped the interpre-\ntation and presentation of the dataset’s characteristics.\n\nKai Zhang played a crucial role in the empirical evalua-\ntion of the dataset by building the evaluation pipeline to as-\nsess various LMMs. Kai Zhang carefully executed different\nmodels and analyzed their performance metrics. Kai Zhang\nalso contributed to the manuscript by documenting the eval-\nuation process and implementation details. The thorough\nmodel evaluation conducted by Kai Zhang has been funda-\nmental in demonstrating the utility of the dataset.\n\nTianyu Zheng made significant contributions to the project\nby participating in the evaluation of text-only, OCR-\naugmented and caption-augmented baselines. In addition,\nTianyu Zheng developed a user-friendly web interface for\ndata annotation and verification. The interface design sig-\nnificantly improved the workflow for data curation.\n\nRuogqi Liu plotted or helped revise Figures 1, 2, and 3.\nRuogqi Liu designed the prototype figure template for the\ncase study figures in the Appendix.\n\nBoyuan Zheng participated in part of the evaluation.\n\nHuan Sun and Yu Su provided overarching and insightful\ndiscussions and comments throughout the development and\nexecution of the project. Huan Sun and Yu Su contributed to\nthe conceptualization of the research by helping to refine the\nresearch questions and by providing critical insights into the\ndesign of the dataset. They offered strategic direction and\nexpert advice that significantly enhanced the dataset and the\nfollow-up analysis. Huan Sun and Yu Su also contributed to\nthe initial writing of the paper.\n\nWenhu Chen conceived the project with Xiang Yue.\nWenhu Chen contributed to the conceptualization of the re-\nsearch by helping to refine the research questions and by\nproviding critical insights into the design of the project. Be-\n\n116\n\nsides, Wenhu Chen contributed to a significant amount of\ninitial writing of the draft and offered strategic direction and\nexpert advice that significantly enhanced the dataset and the\nfollow-up analysis.\n", "vlm_text": "Xiang Yue  conceived and led the project, outlining the pri- mary objectives, establishing the data collection methodol- ogy and protocol, designing and running experiments, as well as doing follow-up analysis. Xiang Yue also took the lead in writing the manuscript, drafting the original text, and incorporating revisions from co-authors. In addition, Xiang Yue managed project administration and coordinated the collaboration between  ${20+}$   coauthors and   ${30+}$   student annotators, ensuring the project’s milestones were met and facilitating communication among team members. Xiang Yue also took the lead in the dataset release. \nsides, Wenhu Chen contributed to a significant amount of initial writing of the draft and offered strategic direction and expert advice that significantly enhanced the dataset and the follow-up analysis. \nYuansheng Ni  co-led the data curation process with Xiang Yue. Specifically, Yuansheng Ni developed the protocols for data quality assurance, standardizing the data annotation procedures, and supervising the team of data annotators to ensure consistency and accuracy. In addition to data cura- tion, Yuansheng Ni also played a collaborative role in data analysis, offering critical insights that shaped the interpre- tation and presentation of the dataset’s characteristics. \nKai Zhang  played a crucial role in the empirical evalua- tion of the dataset by building the evaluation pipeline to as- sess various LMMs. Kai Zhang carefully executed different models and analyzed their performance metrics. Kai Zhang also contributed to the manuscript by documenting the eval- uation process and implementation details. The thorough model evaluation conducted by Kai Zhang has been funda- mental in demonstrating the utility of the dataset. \nTianyu Zheng  made significant contributions to the project by participating in the evaluation of text-only, OCR- augmented and caption-augmented baselines. In addition, Tianyu Zheng developed a user-friendly web interface for data annotation and verification. The interface design sig- nificantly improved the workflow for data curation. \nRuoqi Liu  plotted or helped revise Figures 1, 2, and 3. Ruoqi Liu designed the prototype figure template for the case study figures in the Appendix. \nBoyuan Zheng  participated in part of the evaluation. \nHuan Sun  and  Yu Su  provided over arching and insightful discussions and comments throughout the development and execution of the project. Huan Sun and Yu Su contributed to the conceptualization of the research by helping to refine the research questions and by providing critical insights into the design of the dataset. They offered strategic direction and expert advice that significantly enhanced the dataset and the follow-up analysis. Huan Sun and Yu Su also contributed to the initial writing of the paper. \nWenhu Chen  conceived the project with Xiang Yue. Wenhu Chen contributed to the conceptualization of the re- search by helping to refine the research questions and by providing critical insights into the design of the project. Be- "}
{"page": 116, "image_path": "doc_images/2311.16502v3_116.jpg", "ocr_text": "I. Version Change Log\n\nCHANGES TO V2 (Dec.18) FROM V1 (Nov.27)\n* We added Qwen-VL-PLUS results from the author-provided outputs. (Table 2, 4, 5, 6, 7, 8, 9)\n¢ We added SPHINX results from the author-provided outputs. (Table 2, 4, 5, 6, 7, 8, 9)\n¢ We added Gemini Ultra results from the Gemini report [72]. (Table 2, 4, 5, 6, 7, 8, 9)\n¢ We added Gemini Pro & Nano2 results from the Gemini report [72]. (Table 2)\n¢ We added a section of author contribution statement. (Appendix H)\n¢ We update mPLUG-Owl? results with author-provided prompt. (Table 2, 4, 5, 6, 7, 8, 9)\n¢ We fixed text box dimensions in Appendix B:\n— Figure 13. A sample error case of Art Theory\nWe fixed the typo in Appendix B:\n— Figure 35. A sample error case of Biology\n— Figure 45. A sample error case of Math\n— Figure 79. A sample error case of Agriculture\n— Figure 95. A sample error case of Mechanical Engineering\n\n117\n", "vlm_text": "I. Version Change Log \nCHANGES TO V2 (Dec.18) FROM V1 (Nov.27) • We added Qwen-VL-PLUS results from the author-provided outputs. (Table  2 ,  4 ,  5 ,  6 ,  7 ,  8 ,  9 ) • We added SPHINX results from the author-provided outputs. (Table  2 ,  4 ,  5 ,  6 ,  7 ,  8 ,  9 ) • We added Gemini Ultra results from the Gemini report [ 72 ]. (Table  2 ,  4 ,  5 ,  6 ,  7 ,  8 ,  9 ) • We added Gemini Pro & Nano2 results from the Gemini report [ 72 ]. (Table  2 ) • We added a section of author contribution statement. (Appendix  H ) • We update mPLUG-Owl2 results with author-provided prompt. (Table  2 ,  4 ,  5 ,  6 ,  7 ,  8 ,  9 ) • We fixed text box dimensions in Appendix  B : –  Figure  13 . A sample error case of Art Theory • We fixed the typo in Appendix  B : –  Figure  35 . A sample error case of Biology –  Figure  45 . A sample error case of Math –  Figure  79 . A sample error case of Agriculture –  Figure  95 . A sample error case of Mechanical Engineering "}
