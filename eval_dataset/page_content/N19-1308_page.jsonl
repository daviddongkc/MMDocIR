{"page": 0, "image_path": "doc_images/N19-1308_0.jpg", "ocr_text": "A General Framework for Information Extraction\nusing Dynamic Span Graphs\n\nYi Luan‘\n\nDave Wadden‘\n\nLuheng He! Amy Shahi\n\nMari Ostendorf' Hannaneh Hajishirzi™*\nUniversity of Washington\n*Allen Institute for Artificial Intelligence\n?Google AI Language\n{luanyi, dwadden, amyshah, ostendor, hannaneh} @uw.edu\nluheng @ google.com\n\nAbstract\n\nWe introduce a general framework for sev-\neral information extraction tasks that share\nspan representations using dynamically con-\nstructed span graphs. The graphs are con-\nstructed by selecting the most confident entity\nspans and linking these nodes with confidence-\nweighted relation types and coreferences. The\ndynamic span graph allows coreference and re-\nlation type confidences to propagate through\nthe graph to iteratively refine the span rep-\nresentations. This is unlike previous multi-\ntask frameworks for information extraction in\nwhich the only interaction between tasks is in\nthe shared first-layer LSTM. Our framework\nsignificantly outperforms the state-of-the-art\non multiple information extraction tasks across\nmultiple datasets reflecting different domains.\nWe further observe that the span enumeration\napproach is good at detecting nested span enti-\nties, with significant F1 score improvement on\nthe ACE dataset.!\n\n1 Introduction\n\nMost Information Extraction (IE) tasks require\nidentifying and categorizing phrase spans, some\nof which might be nested. For example, entity\nrecognition involves assigning an entity label to\na phrase span. Relation Extraction (RE) involves\nassigning a relation type between pairs of spans.\nCoreference resolution groups spans referring to\nthe same entity into one cluster. Thus, we might\nexpect that knowledge learned from one task might\nbenefit another.\n\nMost previous work in IE (e.g., (Nadeau and\nSekine, 2007; Chan and Roth, 2011)) employs a\npipeline approach, first detecting entities and then\nusing the detected entity spans for relation extrac-\ntion and coreference resolution. To avoid cascading\n\n'Code and pre-trained models are publicly available at\nhttps://github.com/luanyi/DyGIE.\n\nPER-SOC\nPHYS PHYS\nf YS 1|\nTom’s car broke down as he arrived at Starbucks to meet Mike.\nPER VEH PER Loc PER\ncorer]\n\nCOREF )\n\n“This thing’s useless!” Tom exclaimed as it gave off smoke.\nVEH PER VEH\n\nFigure 1: A text passage illustrating interactions be-\ntween entities, relations and coreference links. Some\nrelation and coreference links are omitted.\n\nerrors introduced by pipeline-style systems, recent\nwork has focused on coupling different IE tasks as\nin joint modeling of entities and relations (Miwa\nand Bansal, 2016; Zhang et al., 2017), entities and\ncoreferences (Hajishirzi et al., 2013; Durrett and\nKlein, 2014), joint inference (Singh et al., 2013)\nor multi-task (entity/relation/coreference) learn-\ning (Luan et al., 2018a). These models mostly\nrely on the first layer LSTM to share span repre-\nsentations between different tasks and are usually\ndesigned for specific domains.\n\nIn this paper, we introduce a general framework\nDynamic Graph IE (DYGIE) for coupling multiple\ninformation extraction tasks through shared span\nrepresentations which are refined leveraging con-\nextualized information from relations and coref-\nerences. Our framework is effective in several do-\nmains, demonstrating a benefit from incorporating\nbroader context learned from relation and corefer-\nence annotations.\n\nFigure 1 shows an example illustrating the po-\nential benefits of entity, relation, and coreference\ncontexts. It is impossible to predict the entity la-\nbels for This thing and it from within-sentence con-\next alone. However, the antecedent car strongly\nsuggests that these two entities have a VEH type.\nSimilarly, the fact that Tom is located at Starbucks\nand Mike has a relation to Tom provides support for\n\n3036\n\nProceedings of NAACL-HLT 2019, pages 3036-3046\nMinneapolis, Minnesota, June 2 - June 7, 2019. ©2019 Association for Computational Linguistics\n", "vlm_text": "A General Framework for Information Extraction using Dynamic Span Graphs \nYi Luan † Dave Wadden † Luheng  $\\mathbf{H}\\mathbf{e}^{\\ddag}$  Amy Shah † Mari Ostendorf † Hannaneh Hajishirzi †∗ \n† University of Washington ∗ Allen Institute for Artiﬁcial Intelligence ‡ Google AI Language { luanyi, dwadden, amyshah, ostendor, hannaneh }  $@$  uw.edu luheng@google.com \nAbstract \nWe introduce a general framework for sev- eral information extraction tasks that share span representations using dynamically con- structed span graphs. The graphs are con- structed by selecting the most conﬁdent entity spans and linking these nodes with conﬁdence- weighted relation types and coreferences. The dynamic span graph allows coreference and re- lation type conﬁdences to propagate through the graph to iteratively reﬁne the span rep- resentations. This is unlike previous multi- task frameworks for information extraction in which the only interaction between tasks is in the shared ﬁrst-layer LSTM. Our framework signiﬁcantly outperforms the state-of-the-art on multiple information extraction tasks across multiple datasets reﬂecting different domains. We further observe that the span enumeration approach is good at detecting nested span enti- ties, with signiﬁcant F1 score improvement on the ACE dataset. \n1 Introduction \nMost Information Extraction (IE) tasks require identifying and categorizing phrase spans, some of which might be nested. For example, entity recognition involves assigning an entity label to a phrase span. Relation Extraction (RE) involves assigning a relation type between pairs of spans. Coreference resolution groups spans referring to the same entity into one cluster. Thus, we might expect that knowledge learned from one task might beneﬁt another. \nMost previous work in IE (e.g., ( Nadeau and Sekine ,  2007 ;  Chan and Roth ,  2011 )) employs a pipeline approach, ﬁrst detecting entities and then using the detected entity spans for relation extrac- tion and coreference resolution. To avoid cascading \nThe image is a diagram illustrating interactions between entities, relations, and coreference links within a text passage. It contains the following components:\n\n1. **Entities and Their Labels:**\n   - \"Tom's car\" is marked as a person (PER) and vehicle (VEH).\n   - \"he\" is marked as a person (PER) and serves as a coreference to \"Tom\".\n   - \"Starbucks\" is marked as a location (LOC).\n   - \"Mike\" is marked as a person (PER).\n   - \"This thing\" is marked as a vehicle (VEH) and serves as a coreference to \"Tom's car\".\n   - \"Tom\" is marked as a person (PER).\n   - \"it\" is marked as a vehicle (VEH) and serves as a coreference to \"Tom's car\".\n\n2. **Relations:**\n   - \"Tom's car\" has a physical (PHYS) relationship with both \"he\" and \"Starbucks\".\n   - \"Tom's car\" has a personal-social (PER-SOC) relationship with \"Tom\".\n   - \"he\" has a physical (PHYS) relationship with \"Starbucks\".\n   - \"Tom\" and \"Mike\" have a personal-social (PER-SOC) relationship.\n  \n3. **Coreference Links:**\n   - \"he\" is linked to \"Tom\".\n   - \"This thing\" and \"it\" are both linked to \"Tom's car\".\n\nThis diagram represents a detailed analysis of the syntactic and semantic relationships within the passage, indicating how different entities connect and refer to one another through coreference.\nerrors introduced by pipeline-style systems, recent work has focused on coupling different IE tasks as in joint modeling of entities and relations ( Miwa and Bansal ,  2016 ;  Zhang et al. ,  2017 ), entities and coreferences ( Hajishirzi et al. ,  2013 ;  Durrett and Klein ,  2014 ), joint inference ( Singh et al. ,  2013 ) or multi-task (entity/relation/coreference) learn- ing ( Luan et al. ,  2018a ). These models mostly rely on the ﬁrst layer LSTM to share span repre- sentations between different tasks and are usually designed for speciﬁc domains. \nIn this paper, we introduce a general framework Dynamic Graph IE (D Y GIE) for coupling multiple information extraction tasks through shared span representations which are reﬁned leveraging con- textualized information from relations and coref- erences. Our framework is effective in several do- mains, demonstrating a beneﬁt from incorporating broader context learned from relation and corefer- ence annotations. \nFigure  1  shows an example illustrating the po- tential beneﬁts of entity, relation, and coreference contexts. It is impossible to predict the entity la- bels for  This thing  and  it  from within-sentence con- text alone. However, the antecedent  car  strongly suggests that these two entities have a VEH type. Similarly, the fact that  Tom  is located at  Starbucks and  Mike  has a relation to  Tom  provides support for the fact that  Mike  is located at  Starbucks . "}
{"page": 1, "image_path": "doc_images/N19-1308_1.jpg", "ocr_text": "the fact that Mike is located at Starbucks.\n\nDYGIE uses multi-task learning to identify en-\ntities, relations, and coreferences through shared\nspan representations using dynamically constructed\nspan graphs. The nodes in the graph are dynam-\nically selected from a beam of highly-confident\nmentions, and the edges are weighted according\nto the confidence scores of relation types or coref-\nerences. Unlike the multi-task method that only\nshares span representations from the local con-\ntext (Luan et al., 2018a), our framework leverages\nrich contextual span representations by propagat-\ning information through coreference and relation\nlinks. Unlike previous BIO-based entity recogni-\ntion systems (Collobert and Weston, 2008; Lample\net al., 2016; Ma and Hovy, 2016) that assign a text\nspan to at most one entity, our framework enumer-\nates and represents all possible spans to recognize\narbitrarily overlapping entities.\n\nWe evaluate DYGIE on several datasets span-\nning many domains (including news, scientific arti-\ncles, and wet lab experimental protocols), achiev-\ning state-of-the-art performance across all tasks and\ndomains and demonstrating the value of coupling\nrelated tasks to learn richer span representations.\nFor example, DYGIE achieves relative improve-\nments of 5.7% and 9.9% over state of the art on the\nACES entity and relation extraction tasks, and an\n11.3% relative improvement on the ACEOS over-\nlapping entity extraction task.\n\nThe contributions of this paper are threefold.\n1) We introduce the dynamic span graph frame-\nwork as a method to propagate global contextual\ninformation, making the code publicly available.\n2) We demonstrate that our framework significantly\noutperforms the state-of-the-art on joint entity and\nrelation detection tasks across four datasets: ACE\n2004, ACE 2005, SciERC and the Wet Lab Proto-\ncol Corpus. 3) We further show that our approach\nexcels at detecting entities with overlapping spans,\nachieving an improvement of up to 8 F1 points on\nthree benchmarks annotated with overlapped spans:\nACE 2004, ACE 2005 and GENIA.\n\n2 Related Work\n\nPrevious studies have explored joint model-\ning (Miwa and Bansal, 2016; Zhang et al., 2017;\nSingh et al., 2013; Yang and Mitchell, 2016)) and\nmulti-task learning (Peng and Dredze, 2015; Peng\net al., 2017; Luan et al., 2018a, 2017a) as methods\nto share representational strength across related in-\n\nformation extraction tasks. The most similar to\nours is the work in Luan et al. (2018a) that takes\na multi-task learning approach to entity, relation,\nand coreference extraction. In this model, the dif-\nferent tasks share span representations that only\nincorporate broader context indirectly via the gra-\ndients passed back to the LSTM layer. In contrast,\nDYGIE uses dynamic graph propagation to explic-\nitly incorporate rich contextual information into the\nspan representations.\n\nEntity recognition has commonly been cast as\na sequence labeling problem, and has benefited\nsubstantially from the use of neural architectures\n(Collobert et al., 2011; Lample et al., 2016; Ma and\nHovy, 2016; Luan et al., 2017b, 2018b). However,\nmost systems based on sequence labeling suffer\nfrom an inability to extract entities with overlap-\nping spans. Recently Katiyar and Cardie (2018)\nand Wang and Lu (2018) have presented methods\nenabling neural models to extract overlapping enti-\nties, applying hypergraph-based representations on\ntop of sequence labeling systems. Our framework\noffers an alternative approach, forgoing sequence\nlabeling entirely and simply considering all possi-\nble spans as candidate entities.\n\nNeural graph-based models have achieved sig-\nnificant improvements over traditional feature-\nbased approaches on several graph modeling tasks.\nKnowledge graph completion (Yang et al., 2015;\nBordes et al., 2013) is one prominent example.\nFor relation extraction tasks, graphs have been\nused primarily as a means to incorporate pipelined\nfeatures such as syntactic or discourse relations\n(Peng et al., 2017; Song et al., 2018; Zhang et al.,\n2018). Christopoulou et al. (2018) models all pos-\nsible paths between entities as a graph, and refines\npair-wise embeddings by performing a walk on the\ngraph structure. All these previous works assume\nthat the nodes of the graph (i.e. the entity candi-\ndates to be considered during relation extraction)\nare predefined and fixed throughout the learning\nprocess. On the other hand, our framework does\nnot require a fixed set of entity boundaries as an\ninput for graph construction. Motivated by state-of-\nthe-art span-based approaches to coreference res-\nolution (Lee et al., 2017, 2018) and semantic role\nlabeling (He et al., 2018), the model uses a beam\npruning strategy to dynamically select high-quality\nspans, and constructs a graph using the selected\nspans as nodes.\n\nMany state-of-the-art RE models rely upon\n\n3037\n", "vlm_text": "\nD Y GIE uses multi-task learning to identify en- tities, relations, and coreferences through shared span representations using dynamically constructed span graphs. The nodes in the graph are dynam- ically selected from a beam of highly-conﬁdent mentions, and the edges are weighted according to the conﬁdence scores of relation types or coref- erences. Unlike the multi-task method that only shares span representations from the local con- text ( Luan et al. ,  2018a ), our framework leverages rich contextual span representations by propagat- ing information through coreference and relation links. Unlike previous BIO-based entity recogni- tion systems ( Collobert and Weston ,  2008 ;  Lample et al. ,  2016 ;  Ma and Hovy ,  2016 ) that assign a text span to at most one entity, our framework enumer- ates and represents all possible spans to recognize arbitrarily overlapping entities. \nWe evaluate D Y GIE on several datasets span- ning many domains (including news, scientiﬁc arti- cles, and wet lab experimental protocols), achiev- ing state-of-the-art performance across all tasks and domains and demonstrating the value of coupling related tasks to learn richer span representations. For example, D Y GIE achieves relative improve- ments of  $5.7\\%$   and  $9.9\\%$   over state of the art on the ACE05 entity and relation extraction tasks, and an  $11.3\\%$   relative improvement on the ACE05 over- lapping entity extraction task. \nThe contributions of this paper are threefold. 1) We introduce the dynamic span graph frame- work as a method to propagate global contextual information, making the code publicly available. 2) We demonstrate that our framework signiﬁcantly outperforms the state-of-the-art on joint entity and relation detection tasks across four datasets: ACE 2004, ACE 2005, SciERC and the Wet Lab Proto- col Corpus. 3) We further show that our approach excels at detecting entities with overlapping spans, achieving an improvement of up to 8 F1 points on three benchmarks annotated with overlapped spans: ACE 2004, ACE 2005 and GENIA. \n2 Related Work \nPrevious studies have explored joint model- ing ( Miwa and Bansal ,  2016 ;  Zhang et al. ,  2017 ; Singh et al. ,  2013 ;  Yang and Mitchell ,  2016 )) and multi-task learning ( Peng and Dredze ,  2015 ;  Peng et al. ,  2017 ;  Luan et al. ,  2018a ,  2017a ) as methods to share representational strength across related in- formation extraction tasks. The most similar to ours is the work in  Luan et al.  ( 2018a ) that takes a multi-task learning approach to entity, relation, and coreference extraction. In this model, the dif- ferent tasks share span representations that only incorporate broader context indirectly via the gra- dients passed back to the LSTM layer. In contrast, D Y GIE uses dynamic graph propagation to explic- itly incorporate rich contextual information into the span representations. \n\nEntity recognition has commonly been cast as a sequence labeling problem, and has beneﬁted substantially from the use of neural architectures ( Collobert et al. ,  2011 ;  Lample et al. ,  2016 ;  Ma and Hovy ,  2016 ;  Luan et al. ,  2017b ,  2018b ). However, most systems based on sequence labeling suffer from an inability to extract entities with overlap- ping spans. Recently  Katiyar and Cardie  ( 2018 ) and  Wang and Lu  ( 2018 ) have presented methods enabling neural models to extract overlapping enti- ties, applying hypergraph-based representations on top of sequence labeling systems. Our framework offers an alternative approach, forgoing sequence labeling entirely and simply considering all possi- ble spans as candidate entities. \nNeural graph-based models have achieved sig- niﬁcant improvements over traditional feature- based approaches on several graph modeling tasks. Knowledge graph completion ( Yang et al. ,  2015 ; Bordes et al. ,  2013 ) is one prominent example. For relation extraction tasks, graphs have been used primarily as a means to incorporate pipelined features such as syntactic or discourse relations ( Peng et al. ,  2017 ;  Song et al. ,  2018 ;  Zhang et al. , 2018 ).  Christopoulou et al.  ( 2018 ) models all pos- sible paths between entities as a graph, and reﬁnes pair-wise embeddings by performing a walk on the graph structure. All these previous works assume that the nodes of the graph (i.e. the entity candi- dates to be considered during relation extraction) are predeﬁned and ﬁxed throughout the learning process. On the other hand, our framework does not require a ﬁxed set of entity boundaries as an input for graph construction. Motivated by state-of- the-art span-based approaches to coreference res- olution ( Lee et al. ,  2017 ,  2018 ) and semantic role labeling ( He et al. ,  2018 ), the model uses a beam pruning strategy to dynamically select high-quality spans, and constructs a graph using the selected spans as nodes. \nMany state-of-the-art RE models rely upon domain-speciﬁc external syntactic tools to con- struct dependency paths between the entities in a sentence ( Li and Ji ,  2014 ;  Xu et al. ,  2015 ;  Miwa and Bansal ,  2016 ;  Zhang et al. ,  2017 ). These sys- tems suffer from cascading errors from these tools and are hard to generalize to different domains. To make the model more general, we combine the multitask learning framework with ELMo em- beddings ( Peters et al. ,  2018 ) without relying on external syntactic tools and risking the cascading errors that accompany them, and improve the inter- action between tasks through dynamic graph prop- agation. While the performance of DyGIE beneﬁts from ELMo, it advances over some systems ( Luan et al. ,  2018a ;  Sanh et al. ,  2019 ) that also incorporate ELMo. The analyses presented here give insights into the beneﬁts of joint modeling. "}
{"page": 2, "image_path": "doc_images/N19-1308_2.jpg", "ocr_text": "domain-specific external syntactic tools to con-\nstruct dependency paths between the entities in\na sentence (Li and Ji, 2014; Xu et al., 2015; Miwa\nand Bansal, 2016; Zhang et al., 2017). These sys-\ntems suffer from cascading errors from these tools\nand are hard to generalize to different domains.\nTo make the model more general, we combine\nthe multitask learning framework with ELMo em-\nbeddings (Peters et al., 2018) without relying on\nexternal syntactic tools and risking the cascading\nerrors that accompany them, and improve the inter-\naction between tasks through dynamic graph prop-\nagation. While the performance of DyGIE benefits\nfrom ELMo, it advances over some systems (Luan\net al., 2018a; Sanh et al., 2019) that also incorporate\nELMo. The analyses presented here give insights\ninto the benefits of joint modeling.\n\n3. Model\n\nProblem Definition The input is a document rep-\nresented as a sequence of words D, from which we\nderive S = {s,,..., sr}, the set of all possible\nwithin-sentence word sequence spans (up to length\nL) in the document. The output contains three\nstructures: the entity types F for all spans S, the\nrelations R for all span pairs S x S within the same\nsentence, and the coreference links C’ for all spans\nin S across sentences. We consider two primary\ntasks. First, Entity Recognition is the task of pre-\ndicting the best entity type labels e; for each span\ns;. Second, Relation Extraction involves predicting\nthe best relation type rj; for all span pairs (s;, s;).\nWe provide additional supervision by also training\nour model to perform a third, auxiliary task: Coref-\nerence resolution. For this task we predict the best\nantecedent c; for each span s;.\n\nOur Model We develop a general information\nextraction framework (DYGIE) to identify and\nclassify entities, relations, and coreference in a\nmulti-task setup. DYGIE first enumerates all text\nspans in each sentence, and computes a locally-\ncontextualized vector space representation of each\nspan. The model then employs a dynamic span\ngraph to incorporate global information into its\nspan representations, as follows. At each training\nstep, the model identifies the text spans that are\nmost likely to represent entities, and treats these\nspans as nodes in a graph structure. It constructs\nconfidence-weighted arcs for each node according\nto its predicted coreference and relation links with\nthe other nodes in the graph. Then, the span repre-\n\nsentations are refined using broader context from\ngated updates propagated from neighboring rela-\ntion types and co-referred entities. These refined\nspan representations are used in a multi-task frame-\nwork to predict entity types, relation types, and\ncoreference links.\n\n3.1 Model Architecture\n\nIn this section, we give an overview of the main\ncomponents and layers of the DyGIE framework,\nas illustrated in Figure 2. Details of the graph con-\nstruction and refinement process will be presented\nin the next section.\n\nToken Representation Layer We apply a bidi-\nrectional LSTM over the input tokens. The input\nfor each token is a concatenation of the character\nreprensetation, GLoVe (Pennington et al., 2014)\nword embeddings, and ELMo embeddings (Peters\net al., 2018). The output token representations are\nobtained by stacking the forward and backward\nLSTM hidden states.\n\nSpan Representation Layer For each span s;,\nits initial vector representation g? is obtained by\nconcatenating BiLSTM outputs at the left and right\nend points of s;, an attention-based soft “head-\nword,” and an embedded span width feature, fol-\nlowing Lee et al. (2017).\n\nCoreference Propagation Layer The propaga-\ntion process starts from the span representations\ng?. At each iteration t, we first compute an update\nvector u(, for each span s;. Then we use ul, to\nupdate the current representation g‘, producing the\nnext span representation git By repeating this\nprocess N times, the final span representations\nshare contextual information across spans that are\nlikely to be antecedents in the coreference graph,\nsimilar to the process in (Lee et al., 2018).\n\nRelation Propagation Layer The outputs eN\nfrom the coreference propagation layer are passed\nas inputs to the relation propagation layer. Similar\nto the coreference propagation process, at each it-\neration t, we first compute the update vectors u‘,\nfor each span s;, then use it to compute git In-\nformation can be integrated from multiple relation\n\npaths by repeating this process M/ times.\n\nFinal Prediction Layer We use the outputs of\nthe relation graph layer git! to predict the entity\nlabels FE and relation labels R. For entities, we\n\npass ght to a feed-forward network (FFNN) to\n\n3038\n", "vlm_text": "\n3 Model \nProblem Deﬁnition The input is a document rep- resented as a sequence of words    $D$  , from which we derive    $S\\,=\\,\\{s_{1},.\\,.\\,.\\,,s_{T}\\}$  , the set of all possible within-sentence word sequence spans (up to length  $L$  ) in the document. The output contains three structures: the entity types    $E$   for all spans    $S$  , the relations    $R$   for all span pairs    $S\\times S$    thin the same sentence, and the coreference links  C  for all spans in    $S$   across sentences. We consider two primary tasks. First,  Entity Recognition  is the task of pre- dicting the best entity type labels    $e_{i}$   for each span  $s_{i}$  . Second,  Relation Extraction  involves predicting the best relation type  $r_{i j}$   for all span pairs    $(s_{i},s_{j})$  . We provide additional supervision by also training our model to perform a third, auxiliary task:  Coref- erence resolution . For this task we predict the best antecedent    $c_{i}$   for each span    $s_{i}$  . \nOur Model We develop a general information extraction framework (D Y GIE) to identify and classify entities, relations, and coreference in a multi-task setup. D Y GIE ﬁrst enumerates all text spans in each sentence, and computes a locally- contextualized vector space representation of each span. The model then employs a  dynamic span graph  to incorporate global information into its span representations, as follows. At each training step, the model identiﬁes the text spans that are most likely to represent entities, and treats these spans as nodes in a graph structure. It constructs conﬁdence-weighted arcs for each node according to its predicted coreference and relation links with the other nodes in the graph. Then, the span repre- sentations are reﬁned using broader context from gated updates propagated from neighboring rela- tion types and co-referred entities. These reﬁned span representations are used in a multi-task frame- work to predict entity types, relation types, and coreference links. \n\n3.1 Model Architecture \nIn this section, we give an overview of the main components and layers of the D Y GIE framework, as illustrated in Figure  2 . Details of the graph con- struction and reﬁnement process will be presented in the next section. \nToken Representation Layer We apply a bidi- rectional LSTM over the input tokens. The input for each token is a concatenation of the character reprensetation, GLoVe ( Pennington et al. ,  2014 ) word embeddings, and ELMo embeddings ( Peters et al. ,  2018 ). The output token representations are obtained by stacking the forward and backward LSTM hidden states. \nSpan Representation Layer For each span    $s_{i}$  , its initial vector representation    $\\mathbf{g}_{i}^{0}$    is obtained by concatenating BiLSTM outputs at the left and right end points of    $s_{i}$  , an attention-based soft “head- word,” and an embedded span width feature, fol- lowing  Lee et al.  ( 2017 ). \nCoreference Propagation Layer The propaga- tion process starts from the span representations  $\\mathbf{g}_{i}^{0}$    . At each iteration    $t$  , we ﬁrst compute an  update vector    $\\mathbf{u}_{C}^{t}$    for each span    $s_{i}$  . Then we use    $\\mathbf{u}_{C}^{t}$    to update the current representation    $\\mathbf{g}_{i}^{t}$  , producing the next span representation    $\\mathbf{g}_{i}^{t+1}$  . By repeating this process    $N$   times, the ﬁnal span representations    $\\mathbf{g}_{i}^{N}$  share contextual information across spans that are likely to be antecedents in the coreference graph, similar to the process in ( Lee et al. ,  2018 ). \nRelation Propagation Layer The outputs    $\\mathbf{g}_{i}^{N}$  from the coreference propagation layer are passed as inputs to the relation propagation layer. Similar to the coreference propagation process, at each it- eration    $t$  , we ﬁrst compute the update vectors  $\\mathbf{u}_{R}^{t}$  for each span    $s_{i}$  , then use it to compute  $\\mathbf{g}_{i}^{t+1}$  . In- formation can be integrated from multiple relation paths by repeating this process    $M$   times. \nFinal Prediction Layer We use the outputs of the relation graph layer  $\\mathbf{g}_{i}^{N+M}$  to predict the entity labels    $E$   and relation labels    $R$  . For entities, we pass  $\\mathbf{g}_{i}^{N+M}$  to a feed-forward network (FFNN) to "}
{"page": 3, "image_path": "doc_images/N19-1308_3.jpg", "ocr_text": "Final prediction PER-SOC PHYS PHYS con\nof entities and\nrelations (i PER} VEH NuLLy (I PER ) Cl Loc ( VEH Jj PER 7 VEH J\nTom car _ arrive at Mike Starbucks this thing Tom it =\n\nand propagation\n\nFinal prediction\n\nCoref Coref of coreference\nCH) CLC _) Cc)\n\niterative inference\nand propagation\nfor coreference\n\nSpan\nenumeration Tom car arrive at Starbucks\nToken A\nrepresentations Sentence-level BILSTM\n\nMike\n\nthis thing Tom it\n\nSentence-level BILSTM\n\nInput document Starbucks to meet Mike.\n\nTom’s car broke down as he arrived at\n\nas it gave off smoke.\n\n“This thing’s useless!” Tom exclaimed |\n\nFigure 2: Overview of our DYGIE model. Dotted arcs indicate confidence weighted graph edges. Solid lines\n\nindicate the final predictions.\n\nproduce per-class scores Pz(i) for span s;. For\nN+M\n\nrelations, we pass the concatenation of g; and\ngt! to a FFNN to produce per-class relation\n\nscores Pr(i,j) between spans s; and s;. Entity\nand relation scores are normalized across the label\nspace, similar to Luan et al. (2018a). For coref-\nerence, the scores between span pairs (s;, s;) are\ncomputed from the coreference graph layer outputs\n(gy, gy), and then normalized across all possible\nantecedents, similar to Lee et al. (2018).\n\n3.2. Dynamic Graph Construction and Span\nRefinement\n\nThe dynamic span graph facilitates propagating\nbroader contexts through soft coreference and rela-\ntion links to refine span representations. The nodes\nin the graph are spans s; with vector representa-\ntions gt € R? for the ¢-th iteration. The edges are\nweighted by the coreference and relation scores,\nwhich are trained according to the neural archi-\ntecture explained in Section 3.1. In this section,\nwe explain how coreference and relation links can\nupdate span representations.\n\nCoreference Propagation Similar to (Luan\net al., 2018a), we define a beam Bo consisting\nof 6, spans that are most likely to be in a corefer-\nence chain. We consider Pu to be a matrix of real\nvalues that indicate coreference confidence scores\nbetween these spans at the t-th iteration. P& is\nof size b. x K, where K is the maximum num-\nber of antecedents considered. For the coreference\n\ngraph, an edge in the graph is single directional,\nconnecting the current span s; with all its poten-\ntial antecedents s; in the coreference beam, where\nj <i. The edge between s; and s; is weighted by\ncoreference confidence score at the current itera-\ntion Pi,(é, 7). The span update vector ul,(i) € R?\nis computed by aggregating the neighboring span\nrepresentations gi. weighted by their coreference\nscores Pi,(i, j):\n\nub(i) = S> Phli,det\n\nJE Bc(i)\n\nqd)\n\nwhere Bo(i) is the set of K spans that are an-\ntecedents of s;,\n\nexp(V&(i, j))\nVyrepew xP(VEt, J)\n\nPolis 3) (2)\nV&(i,j) is a scalar score computed by concate-\nnating the span representations [g', 85,8; © gil,\nwhere © is element-wise multiplication. The con-\ncatenated vector is then fed as input to a FFNN,\nsimilar to (Lee et al., 2018).\n\nRelation Propagation For each sentence, we\ndefine a beam Br consisting of b, entity spans\nthat are mostly likely to be involved in a rela-\ntion. Unlike the coreference graph, the weights\nof relation edges capture different relation types.\nTherefore, for the t-th iteration, we use a tensor\nVi, € R’x*bexLe to capture scores of each of the\nLp relation types. In other words, each edge in the\n\n3039\n", "vlm_text": "The image is a diagram illustrating the DYGIE (Dynamic Graph-based Information Extraction) model, which is used for extracting entities, relations, and coreferences from text. The process begins with an input document, split into sentences, and each sentence is processed through a Sentence-level BiLSTM to obtain token representations. Span enumeration identifies possible spans of interest within these token representations.\n\nThe model progressively performs iterative inference and propagation for coreference and relations. During coreference propagation, spans such as \"car,\" \"this thing,\" and \"it\" are linked, indicating potential coreferences (illustrated by green dotted lines). Similarly, during relation propagation, entities such as \"Tom\" and \"Mike\" are analyzed for possible relations, like \"PER-SOC\" (Person-Social), \"PHYS\" (Physical), etc., as indicated by purple dotted arcs.\n\nThe iterative process occurs M times for relations and N times for coreferences. The final prediction outputs solid connections, outlining the identified entities, relations, and coreferences structured as a graph showing confidence-weighted relationships. The model aims to build coherent and connected interpretations of the input text, capturing detailed semantic associations.\nproduce per-class scores    $\\mathbf{P}_{E}(i)$   for span    $s_{i}$  . For relations, we pass the concatenation of    $\\mathbf{g}_{i}^{N+M}$  and  $\\mathbf{g}_{j}^{N+M}$  to a FFNN to produce per-class relation scores  $\\mathbf{P}_{R}(i,j)$   between spans    $s_{i}$   and    $s_{j}$  . Entity and relation scores are normalized across the label space, similar to  Luan et al.  ( 2018a ). For coref- erence, the scores between span pairs   $(s_{i},s_{j})$   are computed from the coreference graph layer outputs  $(\\mathbf{g}_{i}^{N},\\mathbf{g}_{j}^{N})$  ), and then normalized across all possible antecedents, similar to  Lee et al.  ( 2018 ). \n3.2 Dynamic Graph Construction and Span Reﬁnement \nThe dynamic span graph facilitates propagating broader contexts through soft coreference and rela- tion links to reﬁne span representations. The nodes in the graph are spans  $s_{i}$   with vector representa- tions    $\\mathbf{g}_{i}^{t}\\in\\mathbb{R}^{d}$    ∈  for the    $t$  -th iteration. The edges are weighted by the coreference and relation scores, which are trained according to the neural archi- tecture explained in Section  3.1 . In this section, we explain how coreference and relation links can update span representations. \nCoreference Propagation Similar to ( Luan et al. ,  2018a ), we deﬁne a beam    $B_{C}$   consisting of    $b_{c}$   spans that are most likely to be in a corefer- ence chain. We consider  $\\mathbf{P}_{C}^{t}$    to be a matrix of real values that indicate coreference conﬁdence scores between these spans at the    $t$  -th iteration.    $\\mathbf{P}_{C}^{t}$    is of size  $b_{c}\\times K$  , where    $K$   is the maximum num- ber of antecedents considered. For the coreference graph, an edge in the graph is single directional, connecting the current span    $s_{i}$   with all its poten- tial antecedents    $s_{j}$   in the coreference beam, where  $j<i$  . The edge between  $s_{i}$   and  $s_{j}$   is weighted by coreference conﬁdence score at the current itera- tion    $P_{C}^{t}(i,j)$  . The span update vector    $\\mathbf{u}_{C}^{t}(i)\\in\\mathbb{R}^{d}$   ∈ is computed by aggregating the neighboring span representations    $\\mathbf{g}_{j}^{t}$  , weighted by their coreference scores    $P_{C}^{t}(i,j)$  : \n\n\n$$\n\\mathbf{u}_{C}^{t}(i)=\\sum_{j\\in B_{\\mathbb{C}}(i)}P_{C}^{t}(i,j)\\mathbf{g}_{j}^{t}\n$$\n \nwhere    $B_{C}(i)$   is the set of    $K$   spans that are an- tecedents of  $s_{i}$  , \n\n$$\nP_{C}^{t}(i,j)=\\frac{\\exp(V_{C}^{t}(i,j))}{\\sum_{j^{\\prime}\\in B_{C}(i)}\\exp(V_{C}^{t}(i,j))}\n$$\n \n $V_{C}^{t}(i,j)$   is a scalar score computed by concate- nating the span representations    $[\\mathbf{g}_{i}^{t},\\mathbf{g}_{j}^{t},\\mathbf{g}_{i}^{t}\\odot\\mathbf{g}_{j}^{t}]$    , where  $\\odot$  is element-wise multiplication. The con- catenated vector is then fed as input to a FFNN, similar to ( Lee et al. ,  2018 ). \nRelation Propagation For each sentence, we deﬁne a beam    $B_{R}$   consisting of    $b_{r}$   entity spans that are mostly likely to be involved in a rela- tion. Unlike the coreference graph, the weights of relation edges capture different relation types. Therefore, for the    $t$  -th iteration, we use a tensor  $\\mathbf{V}_{R}^{t}\\in\\mathbb{R}^{b_{R}\\times b_{R}\\times L_{R}}$    ∈  to capture scores of each of the  $L_{R}$   relation types. In other words, each edge in the relation graph connects two entity spans    $s_{i}$   and  $s_{j}$  in the relation beam  $B_{R}$  .    $\\mathbf{V}_{R}^{t}(i,j)$   is a    $L_{R}$  -length vector of relation scores, computed with a FFNN with    $[\\mathbf{g}_{i}^{t},\\mathbf{g}_{j}^{t}]$   as the input. The relation update vec- tor    $\\mathbf{u}_{R}^{t}(i)\\in\\mathbb{R}^{d}$   ∈  is computed by aggregating neigh- boring span representations on the relation graph: "}
{"page": 4, "image_path": "doc_images/N19-1308_4.jpg", "ocr_text": "relation graph connects two entity spans s; and s;\nin the relation beam Br. V‘4,(i, j) is a Lr-length\nvector of relation scores, computed with a FFNN\nwith [g!, 4] as the input. The relation update vec-\ntor u,(i) € R¢ is computed by aggregating neigh-\nboring span representations on the relation graph:\n\nup(i) = S0 fVRD))AROB, G)\nJEBR\n\nwhere Ap € R’**7 js a trainable linear projection\nmatrix, f is a non-linear function to select the most\nimportant relations. Because only a small number\nof entities in the relation beam are actually linked\nto the target span, propagation among all possi-\nble span pairs would introduce too much noise to\nthe new representation. Therefore, we choose f\nto be the ReLU function to remove the effect of\nunlikely relations by setting the all negative rela-\ntion scores to 0. Unlike coreference connections,\ntwo spans linked via a relation are not expected\nto have similar representations, so the matrix Az\nhelps to transform the embedding gi according to\neach relation type.\n\nUpdating Span Representations with Gating\nTo compute the span representations for the next\niteration t € {1,..., N + M}, we define a gating\nvector f/(i) € IR4, where a € {C, R}, to deter-\nmine whether to keep the previous span represen-\ntation gi or to integrate new information from the\ncoreference or relation update vectors u!,(i). For-\nmally,\n\nfii) =\ngt! = f@og+a-\n\n(Wiig. 4, ()]) 4)\n£(i)) © ub(i),\nwhere W!, € R?*?4 are trainable parameters, and\ng is an element-wise sigmoid function.\n\n3.3. Training\n\nThe loss function is defined as a weighted sum of\nthe log-likelihood of all three tasks:\n\n- {re log P(E*|C,R,D)  (S)\n(D,R*,E*,C*)ED\n\n+ Ag log P(R* | C, D) + Ac log P(C* | b)}\n\nwhere E*, R* and C®* are gold structures of the\nentity types, relations and coreference, respec-\ntively. D is the collection of all training documents\nD. The task weights Ag, Ar, and Ac are hyper-\nparameters to control the importance of each task.\n\nDomain Docs Ent Rel Coref\nACE04 News 348 7 7 v\nACE05 News 511 7 6\n\nx\nSciERC Al 500 6 7 v\nWLP Biolab 622 18 13 x\n\nTable 1: Datasets for joint entity and relation extraction\nand their statistics. Ent: Number of entity categories.\nRel: Number of relation categories.\n\nWe use a | layer BiLSTM with 200-dimensional\nhidden layers. All the feed-forward functions have\n2 hidden layers of 150 dimensions each. We use 0.4\nvariational dropout (Gal and Ghahramani, 2016) for\nhe LSTMs, 0.4 dropout for the FFNNs, and 0.5\ndropout for the input embeddings. The hidden layer\ndimensions and dropout rates are chosen based on\nhe development set performance in multiple do-\nmains. The task weights, learning rate, maximum\nspan length, number of propagation iterations and\nbeam size are tuned specifically for each dataset\nusing development data.\n\n4 Experiments\n\nDYGIE is a general IE framework that can be ap-\nplied to multiple tasks. We evaluate the perfor-\nmance of DYGIE against models from two lines of\nwork: combined entity and relation extraction, and\noverlapping entity extraction.\n\n4.1 Entity and relation extraction\n\nFor the entity and relation extraction task, we\ntest the performance of DYGIE on four different\ndatasets: ACE2004, ACE2005, SciERC and the\nWet Lab Protocol Corpus. We include the rela-\ntion graph propagation layer in our models for all\ndatasets. We include the coreference graph propa-\ngation layer on the data sets that have coreference\nannotations available.\n\nData All four data sets are annotated with entity\nand relation labels. Only a small fraction of entities\n(< 3% of total) in these data sets have a text span\nthat overlaps the span of another entity. Statistics\non all four data sets are displayed in Table 1.\n\nThe ACE2004 and ACE2005 corpora provide\nentity and relation labels for a collection of docu-\nments from a variety of domains, such as newswire\nand online forums. We use the same entity and\nrelation types, data splits, and preprocessing as\nMiwa and Bansal (2016) and Li and Ji (2014). Fol-\nlowing the convention established in this line of\nwork, an entity prediction is considered correct\n\n3040\n", "vlm_text": "\n\n$$\n\\mathbf{u}_{R}^{t}(i)=\\sum_{j\\in B_{\\mathrm{R}}}f(\\mathbf{V}_{R}^{t}(i,j))\\mathbf{A}_{R}\\odot\\mathbf{g}_{j}^{t},\n$$\n \nwhere    $\\mathbf{A}_{R}\\in\\mathbb{R}^{L_{R}\\times d}$    is a trainable linear projection matrix,  $f$   is a non-linear function to select the most important relations. Because only a small number of entities in the relation beam are actually linked to the target span, propagation among all possi- ble span pairs would introduce too much noise to the new representation. Therefore, we choose    $f$  to be the ReLU function to remove the effect of unlikely relations by setting the all negative rela- tion scores to 0. Unlike coreference connections, two spans linked via a relation are not expected to have similar representations, so the matrix    ${\\bf A}_{R}$  helps to transform the embedding  $\\mathbf{g}_{j}^{t}$    according to each relation type. \nUpdating Span Representations with Gating To compute the span representations for the next iteration  $t\\in\\{1,.\\,.\\,.\\,,N+M\\}$  , we deﬁne a gating vector  $\\mathbf{f}_{x}^{t}(i)\\,\\in\\,\\mathbb{R}^{d}$   ∈ , where    $x\\,\\in\\,\\{C,R\\}$  , to deter- mine whether to keep the previous span represen- tation  $\\mathbf{g}_{i}^{t}$    or to integrate new information from the coreference or relation update vectors  $\\mathbf{u}_{x}^{t}(i)$  . For- mally, \n\n$$\n\\begin{array}{c c l}{\\mathbf{f}_{x}^{t}(i)}&{=}&{g(\\mathbf{W}_{x}^{\\mathrm{f}}[\\mathbf{g}_{i}^{t},\\mathbf{u}_{x}^{t}(i)])}\\\\ {\\mathbf{g}_{i}^{t+1}}&{=}&{\\mathbf{f}_{x}^{t}(i)\\odot\\mathbf{g}_{i}^{t}+(1-\\mathbf{f}_{x}^{t}(i))\\odot\\mathbf{u}_{x}^{t}(i),}\\end{array}\n$$\n \nwhere    $\\mathbf{W}_{x}^{\\mathrm{f}}\\in\\mathbb{R}^{d\\times2d}$    ∈  are trainable parameters, and  $g$   is an element-wise sigmoid function. \n3.3 Training \nThe loss function is deﬁned as a weighted sum of the log-likelihood of all three tasks: \n\n$$\n\\begin{array}{r l}{\\displaystyle\\sum_{(D,R^{*},E^{*},C^{*})\\in\\mathcal{D}}\\Big\\{\\lambda_{\\mathrm{E}}\\log P(E^{*}\\mid C,R,D)\\;}&{(5}\\\\ {+\\;\\lambda_{\\mathsf{R}}\\log P(R^{*}\\mid C,D)+\\lambda_{\\mathsf{C}}\\log P(C^{*}\\mid D)\\Big\\}}\\end{array}\n$$\n \nwhere    $E^{*},\\,R^{*}$  and    $C^{*}$  are gold structures of the entity types, relations and coreference, respec- ely.    $\\mathcal{D}$   is the collection of all training documents  $D$  . The task weights    $\\lambda_{\\mathrm{E}},\\,\\lambda_{\\mathrm{R}}$  , and    $\\lambda_{\\mathrm{C}}$   are hyper- parameters to control the importance of each task. \nThe table presents a comparison of four datasets: ACE04, ACE05, SciERC, and WLP. Below is a brief explanation of each column in the table:\n\n1. **Dataset Names**: The first column lists the names of the datasets: ACE04, ACE05, SciERC, and WLP.\n\n2. **Domain**: This column indicates the domain to which each dataset belongs:\n   - ACE04 and ACE05 are related to news.\n   - SciERC is related to AI.\n   - WLP is related to the Bio lab.\n\n3. **Docs (Documents)**: This column specifies the number of documents in each dataset:\n   - ACE04 has 348 documents.\n   - ACE05 has 511 documents.\n   - SciERC has 500 documents.\n   - WLP has 622 documents.\n\n4. **Ent (Entities)**: This column represents the number of entity types in each dataset:\n   - Both ACE04 and ACE05 have 7 entity types.\n   - SciERC has 6 entity types.\n   - WLP has 18 entity types.\n\n5. **Rel (Relations)**: This column shows the number of relation types in each dataset:\n   - Both ACE04 and SciERC have 7 relation types.\n   - ACE05 has 6 relation types.\n   - WLP has 13 relation types.\n\n6. **Coref (Coreference Resolution)**: The final column indicates whether coreference resolution is included in each dataset:\n   - ACE04 and SciERC have coreference resolution, as marked by a checkmark (✓).\n   - ACE05 and WLP do not have coreference resolution, as marked by a cross (✗).\nWe use a 1 layer BiLSTM with 200-dimensional hidden layers. All the feed-forward functions have 2 hidden layers of 150 dimensions each. We use 0.4 variational dropout ( Gal and Ghahramani ,  2016 ) for the LSTMs, 0.4 dropout for the FFNNs, and 0.5 dropout for the input embeddings. The hidden layer dimensions and dropout rates are chosen based on the development set performance in multiple do- mains. The task weights, learning rate, maximum span length, number of propagation iterations and beam size are tuned speciﬁcally for each dataset using development data. \n4 Experiments \nD Y GIE is a general IE framework that can be ap- plied to multiple tasks. We evaluate the perfor- mance of D Y GIE against models from two lines of work: combined entity and relation extraction, and overlapping entity extraction. \n4.1 Entity and relation extraction \nFor the entity and relation extraction task, we test the performance of D Y GIE on four different datasets: ACE2004, ACE2005, SciERC and the Wet Lab Protocol Corpus. We include the rela- tion graph propagation layer in our models for all datasets. We include the coreference graph propa- gation layer on the data sets that have coreference annotations available. \nData All four data sets are annotated with entity and relation labels. Only a small fraction of entities  $(<3\\%$   of total) in these data sets have a text span that overlaps the span of another entity. Statistics on all four data sets are displayed in Table  1 . \nThe  ACE2004  and  ACE2005  corpora provide entity and relation labels for a collection of docu- ments from a variety of domains, such as newswire and online forums. We use the same entity and relation types, data splits, and preprocessing as Miwa and Bansal  ( 2016 ) and  Li and Ji  ( 2014 ). Fol- lowing the convention established in this line of work, an entity prediction is considered correct "}
{"page": 5, "image_path": "doc_images/N19-1308_5.jpg", "ocr_text": "Dataset System Entity —_ Relation\nBekoulis et al. (2018) 81.6 47.5\nACE04 = Miwa and Bansal (2016) 81.8 48.4\nDYGIE 87.4 59.7\nMiwa and Bansal (2016) 83.4 55.6\nZhang et al. (2017) 83.6 57.5\nACE05 Sanh et al. (2019) 87.5 62.7\nDYGIE 88.4 63.2\n“ Luan et al. (2018a) 64.2 39.3\nScERC  DYGIE 65.2 41.6\nKulkarni et al. (2018) 78.0 *54.9\nWLPC DYGIE 79.5 64.1\n\nTable 2: F1 scores on the joint entity and relation ex-\ntraction task on each test set, compared against the pre-\nvious best systems. * indicates relation extraction sys-\ntem that takes gold entity boundary as input.\n\nif its type label and head region match those of\na gold entity. We will refer to this version of\nthe ACE2004 and ACE2005 data as ACE04 and\nACEO0S. Since the domain and mention span an-\nnotations in the ACE datasets are very similar to\nthose of OntoNotes (Pradhan et al., 2012), and\nOntoNotes contains significantly more documents\nwith coreference annotations, we use OntoNotes\nto train the parameters for the auxiliary corefer-\nence task. The OntoNotes corpus contains 3493\ndocuments, averaging roughly 450 words in length.\n\nThe SciERC corpus (Luan et al., 2018a) pro-\nvides entity, coreference and relation annotations\nfor a collection of documents from 500 AI paper\nabstracts. The dataset defines scientific term types\nand relation types specially designed for AI domain\nknowledge graph construction. An entity predic-\ntion is considered correct if its label and span match\nwith a gold entity.\n\nThe Wet Lab Protocol Corpus (WLPC) pro-\nvides entity, relation, and event annotations for 622\nwet lab protocols (Kulkarni et al., 2018). A wet\nlab protocol is a series of instructions specifying\nhow to perform a biological experiment. Following\nthe procedure in Kulkarni et al. (2018), we perform\nentity recognition on the union of entity tags and\nevent trigger tags, and relation extraction on the\nunion of entity-entity relations and entity-trigger\nevent roles. Coreference annotations are not avail-\nable for this dataset.\n\nBaselines We compare DYGIE with current state\nof the art methods in different datasets. Miwa and\nBansal (2016) provide the current state of the art\non ACE04. They construct a Tree LSTM using\ndependency parse information, and use the repre-\n\nsentations learned by the tree structure as features\nfor relation classification. Bekoulis et al. (2018)\nuse adversarial training as regularization for a neu-\nral model. Zhang et al. (2017) cast joint entity and\nrelation extraction as a table filling problem and\nbuild a globally optimized neural model incorpo-\nrating syntactic representations from a dependency\nparser. Similar to DYGIE, Sanh et al. (2019) and\nLuan et al. (2018a) use a multi-task learning frame-\nwork for extracting entity, relation and coreference\nlabels. Sanh et al. (2019) improved the state of\nthe art on ACEOS using multi-task, hierarchical\nsupervised training with a set of low level tasks\nat the bottom layers of the model and more com-\nplex tasks at the top layers of the model. Luan\net al. (2018a) previously achieved the state of the\nart on SciERC and use a span-based neural model\nlike our DYGIE. Kulkarni et al. (2018) provide\na baseline for the WLPC data set. They employ\nan LSTM-CRF for entity recognition, following\nLample et al. (2016). For relation extraction, they\nassume the presence of gold entities and train a\nmaximum-entropy classifier using features from\nthe labeled entities.\n\nResults Table 2 shows test set Fl on the joint\nentity and relation extraction task. We observe that\nDYGIE achieves substantial improvements on both\nentity recognition and relation extraction across the\nfour data sets and three domains, all in the realistic\nsetting where no “gold” entity labels are supplied\nat test time. DyGIE achieves 7.1% and 7.0% rela-\ntive improvements over the state of the art on NER\nfor ACE04 and ACE0S, respectively. For the rela-\ntion extraction task, DYGIE attains 25.8% relative\nimprovement over SOTA on ACE04 and 13.7% rel-\native improvement on ACE0S. For ACE0S, the best\nentity extraction performance is obtained by switch-\ning the order between CorefProp and RelProp\n(RelProp first then CorefProp).\n\nOn SciERC, DYGIE advances the state of the\nart by 5.9% and 1.9% for relation extraction and\nNER, respectively. The improvement of DyYGIE\nover the previous SciERC model underscores the\nability of coreference and relation propagation to\nconstruct rich contextualized representations.\n\nThe results from Kulkarni et al. (2018) estab-\nlish a baseline for IE on the WLPC. In that work,\nrelation extraction is performed using gold entity\nboundaries as input. Without using any gold entity\ninformation, DYGIE improves on the baselines by\n16.8% for relation extraction and 2.2% for NER.\n\n3041\n", "vlm_text": "The table presents performance metrics (likely precision, recall, or F1 scores, although not explicitly stated in the table) of various systems on different datasets. The datasets mentioned are ACE04, ACE05, SciERC, and WLPC. These systems are evaluated based on two categories: \"Entity\" and \"Relation\" metrics.\n\nHere are the key points from the table:\n\n1. **ACE04 Dataset**:\n   - Bekoulis et al. (2018): Entity score of 81.6, Relation score of 47.5.\n   - Miwa and Bansal (2016): Entity score of 81.8, Relation score of 48.4.\n   - DyGIE: Entity score of 87.4, Relation score of 59.7 (indicating the best performance on this dataset for both entity and relation metrics).\n\n2. **ACE05 Dataset**:\n   - Miwa and Bansal (2016): Entity score of 83.4, Relation score of 55.6.\n   - Zhang et al. (2017): Entity score of 83.6, Relation score of 57.5.\n   - Sanh et al. (2019): Entity score of 87.5, Relation score of 62.7.\n   - DyGIE: Entity score of 88.4, Relation score of 63.2 (indicating the best performance on this dataset).\n\n3. **SciERC Dataset**:\n   - Luan et al. (2018a): Entity score of 64.2, Relation score of 39.3.\n   - DyGIE: Entity score of 65.2, Relation score of 41.6 (indicating the best performance on this dataset).\n\n4. **WLPC Dataset**:\n   - Kulkarni et al. (2018): Entity score of 78.0, Relation score of 54.9 (with an asterisk indicating a possible note or exception not visible here).\n   - DyGIE: Entity score of 79.5, Relation score of 64.1 (indicating the best performance on this dataset).\n\nOverall, the DyGIE system achieves the highest performance scores in both the entity and relation categories across all the datasets presented in this table.\nif its type label and head region match those of a gold entity. We will refer to this version of the ACE2004 and ACE2005 data as ACE04 and ACE05. Since the domain and mention span an- notations in the ACE datasets are very similar to those of OntoNotes ( Pradhan et al. ,  2012 ), and OntoNotes contains signiﬁcantly more documents with coreference annotations, we use OntoNotes to train the parameters for the auxiliary corefer- ence task. The OntoNotes corpus contains 3493 documents, averaging roughly 450 words in length. \nThe  SciERC  corpus ( Luan et al. ,  2018a ) pro- vides entity, coreference and relation annotations for a collection of documents from 500 AI paper abstracts. The dataset deﬁnes scientiﬁc term types and relation types specially designed for AI domain knowledge graph construction. An entity predic- tion is considered correct if its label and span match with a gold entity. \nThe  Wet Lab Protocol Corpus (WLPC)  pro- vides entity, relation, and event annotations for 622 wet lab protocols ( Kulkarni et al. ,  2018 ). A wet lab protocol is a series of instructions specifying how to perform a biological experiment. Following the procedure in  Kulkarni et al.  ( 2018 ), we perform entity recognition on the union of entity tags and event trigger tags, and relation extraction on the union of entity-entity relations and entity-trigger event roles. Coreference annotations are not avail- able for this dataset. \nBaselines We compare D Y GIE with current state of the art methods in different datasets.  Miwa and Bansal  ( 2016 ) provide the current state of the art on ACE04. They construct a Tree LSTM using dependency parse information, and use the repre- sentations learned by the tree structure as features for relation classiﬁcation.  Bekoulis et al.  ( 2018 ) use adversarial training as regularization for a neu- ral model.  Zhang et al.  ( 2017 ) cast joint entity and relation extraction as a table ﬁlling problem and build a globally optimized neural model incorpo- rating syntactic representations from a dependency parser. Similar to D Y GIE,  Sanh et al.  ( 2019 ) and Luan et al.  ( 2018a ) use a multi-task learning frame- work for extracting entity, relation and coreference labels.  Sanh et al.  ( 2019 ) improved the state of the art on ACE05 using multi-task, hierarchical supervised training with a set of low level tasks at the bottom layers of the model and more com- plex tasks at the top layers of the model.  Luan et al.  ( 2018a ) previously achieved the state of the art on SciERC and use a span-based neural model like our D Y GIE.  Kulkarni et al.  ( 2018 ) provide a baseline for the WLPC data set. They employ an LSTM-CRF for entity recognition, following Lample et al.  ( 2016 ). For relation extraction, they assume the presence of gold entities and train a maximum-entropy classiﬁer using features from the labeled entities. \n\nResults Table  2  shows test set F1 on the joint entity and relation extraction task. We observe that D Y GIE achieves substantial improvements on both entity recognition and relation extraction across the four data sets and three domains, all in the realistic setting where no “gold” entity labels are supplied at test time. D Y GIE achieves   $7.1\\%$   and   $7.0\\%$   rela- tive improvements over the state of the art on NER for ACE04 and ACE05, respectively. For the rela- tion extraction task, D Y GIE attains   $25.8\\%$   relative improvement over SOTA on ACE04 and   $13.7\\%$   rel- ative improvement on ACE05. For ACE05, the best entity extraction performance is obtained by switch- ing the order between  CorefProp  and  RelProp ( RelProp  ﬁrst then  CorefProp ). \nOn SciERC, D Y GIE advances the state of the art by   $5.9\\%$   and   $1.9\\%$   for relation extraction and NER, respectively. The improvement of D Y GIE over the previous SciERC model underscores the ability of coreference and relation propagation to construct rich contextualized representations. \nThe results from  Kulkarni et al.  ( 2018 ) estab- lish a baseline for IE on the WLPC. In that work, relation extraction is performed using gold entity boundaries as input. Without using any gold entity information, D Y GIE improves on the baselines by  $16.8\\%$   for relation extraction and  $2.2\\%$   for NER. "}
{"page": 6, "image_path": "doc_images/N19-1308_6.jpg", "ocr_text": "Domain Docs Ent Overlap Coref Dataset System Entity Fl\n\nACE04-O News 443 7 42% v Katiyar and Cardie (2018) 72.7\nACE05-O News 437 7 32% x ACE04-O Wang and Lu (2018) 75.1\nGENIA Biomed 1999 5 24% v DyYGIE 84.7\nKatiyar and Cardie (2018) 70.5\n\nTable 3: Datasets for overlapping entity extraction and ACE05-O Wang and Lu (2018) 74.5\ntheir statistics. Ent: Number of entity categories. Over- DyYGIE 82.9\nlap: Percentage of sentences that contain overlapping Katiyar and Cardie (2018) 73.8\nentities. GENIA Wang and Lu (2018) 75.1\nDyYGIE 76.2\n\nOn the OntoNotes data set used for the auxiliary\ncoreference task with ACE05, our model achieves\ncoreference test set performance of 70.4 F1, which\nis competitive with the state-of-the-art performance\nreported in Lee et al. (2017).\n\n4.2 Overlapping Entity Extraction\n\nThere are many applications where the correct iden-\ntification of overlapping entities is crucial for cor-\nrect document understanding. For instance, in the\nbiomedical domain, a BRCAI mutation carrier\ncould refer to a patient taking part in a clinical\ntrial, while BRCA/ is the name of a gene.\n\nWe evaluate the performance of DYGIE on\noverlapping entity extraction in three datasets:\nACE2004, ACE2005 and GENIA. Since relation\nannotations are not available for these datasets, we\ninclude the coreference propagation layer in our\nmodels but not the relation layer.”\n\nData Statistics on our three datasets are listed\nin Table 3. All three have a substantial number\n(> 20% of total) of overlapping entities, making\nthem appropriate for this task.\nAs in the joint case, we evaluate our model on\nACE2004 and ACE2005, but here we follow the\nsame data preprocessing and evaluation scheme as\nWang and Lu (2018). We refer to these data sets\nas ACE04-O and ACE0S5-O. Unlike the joint en-\ntity and relation task in Sec. 4.1, where only the\nentity head span need be predicted, an entity pre-\ndiction is considered correct in these experiments\nif both its entity label and its full text span match\na gold prediction. This is a more stringent evalua-\ntion criterion than the one used in Section 4.1. As\nbefore, we use the OntoNotes annotations to train\nthe parameters of the coreference layer.\nThe GENIA corpus (Kim et al., 2003) provides\nentity tags and coreferences for 1999 abstracts from\nthe biomedical research literature. We only use\nthe IDENT label to extract coreference clusters.\n\n?We use the pre-processed ACE dataset from previous\nwork and relation annotation is not available.\n\nTable 4: Performance on the overlapping entity extrac-\ntion task, compared to previous best systems. We re-\nport F1 of extracted entities on the test sets.\n\nEntity Relation\nModel P R_ Fl P R_ Fl\nDyGIE 87.4 86.7 87.1 56.2 60.9 58.4\n—CorefProp 86.2 85.2 85.7 64.3 56.7 60.2\n—RelProp 87.0 86.7 86.9 60.4 55.8 58.0\nBase 86.1 85.7 85.9 59.5 55.7 57.6\n\nTable 5: Ablations on the ACE05 development set with\ndifferent graph propagation setups. —CorefProp\nablates the coreference propagation layers, while\n—RelProp ablates the relation propagation layers.\nBase is the system without any propagation.\n\nWe use the same data set split and preprocessing\nprocedure as Wang and Lu (2018) for overlapping\nentity recognition.\n\nBaselines The current state-of-the-art approach\non all three data sets is Wang and Lu (2018), which\nuses a segmental hypergraph coupled with neural\nnetworks for feature learning. Katiyar and Cardie\n(2018) also propose a hypergraph approach using a\nrecurrent neural network as a feature extractor.\n\nResults Table 4 presents the results of our over-\nlapping entity extraction experiments on the differ-\nent datsets. DYGIE improves 11.6% on the state of\nthe art for ACE04-O and 11.3% for ACE05-O. Dy-\nGIE also advances the state of the art on GENIA,\nalbeit by a more modest 1.5%. Together these re-\nsults suggest that DYGIE can be utilized fruitfully\nfor information extraction across different domains\nwith overlapped entities, such as bio-medicine.\n\n5 Analysis of Graph Propagation\nWe use the dev sets of ACE2005 and SciERC to\n\nanalyze the effect of different model components.\n5.1 Coreference and Relation Graph Layers\n\nTables 5 and 6 show the effects of graph propa-\ngation on entity and relation prediction accuracy,\n\n3042\n", "vlm_text": "The table contains information about three datasets used in various domains. Here's a breakdown of each column in the table:\n\n1. **Domain**: Specifies the name of the dataset. The datasets listed are ACE04-O, ACE05-O, and GENIA.\n\n2. **Domain**: Indicates the type of data the dataset contains. ACE04-O and ACE05-O belong to the \"News\" domain, while GENIA is categorized under \"Biomed.\"\n\n3. **Docs**: Represents the number of documents included in each dataset. ACE04-O has 443 documents, ACE05-O has 437 documents, and GENIA contains 1999 documents.\n\n4. **Ent**: Displays the number of entity types present in the dataset. Both ACE04-O and ACE05-O have 7 entity types, while GENIA has 5 entity types.\n\n5. **Overlap**: Shows the percentage of overlapping entities within the data. ACE04-O has 42% overlap, ACE05-O has 32% overlap, and GENIA has 24% overlap.\n\n6. **Coref**: Indicates whether coreference annotations are available within the dataset. ACE04-O and GENIA have coreference annotations (marked with a check), while ACE05-O does not (marked with a cross).\nOn the OntoNotes data set used for the auxiliary coreference task with ACE05, our model achieves coreference test set performance of 70.4 F1, which is competitive with the state-of-the-art performance reported in  Lee et al.  ( 2017 ). \n4.2 Overlapping Entity Extraction \nThere are many applications where the correct iden- tiﬁcation of overlapping entities is crucial for cor- rect document understanding. For instance, in the biomedical domain, a  BRCA1 mutation carrier could refer to a patient taking part in a clinical trial, while  BRCA1  is the name of a gene. \nWe evaluate the performance of D Y GIE on overlapping entity extraction in three datasets: ACE2004, ACE2005 and GENIA. Since relation annotations are not available for these datasets, we include the coreference propagation layer in our models but not the relation layer. \nData Statistics on our three datasets are listed in Table  3 . All three have a substantial number  $(>20\\%$   of total) of overlapping entities, making them appropriate for this task. \nAs in the joint case, we evaluate our model on ACE2004  and  ACE2005 , but here we follow the same data preprocessing and evaluation scheme as Wang and Lu  ( 2018 ). We refer to these data sets as ACE04-O and ACE05-O. Unlike the joint en- tity and relation task in Sec.  4.1 , where only the entity head span need be predicted, an entity pre- diction is considered correct in these experiments if both its entity label and its full text span match a gold prediction. This is a more stringent evalua- tion criterion than the one used in Section  4.1 . As before, we use the OntoNotes annotations to train the parameters of the coreference layer. \nThe  GENIA  corpus ( Kim et al. ,  2003 ) provides entity tags and coreferences for 1999 abstracts from the biomedical research literature. We only use the IDENT label to extract coreference clusters. \nThe table presents the performance of different systems on various datasets, measured by the Entity F1 score. The datasets listed are ACE04-O, ACE05-O, and GENIA. For each dataset, three systems are evaluated: \"Katiyar and Cardie (2018)\", \"Wang and Lu (2018)\", and \"DyGIE\".\n\n- For the ACE04-O dataset, the Entity F1 scores for the systems are:\n  - Katiyar and Cardie (2018): 72.7\n  - Wang and Lu (2018): 75.1\n  - DyGIE: 84.7\n\n- For the ACE05-O dataset, the Entity F1 scores for the systems are:\n  - Katiyar and Cardie (2018): 70.5\n  - Wang and Lu (2018): 74.5\n  - DyGIE: 82.9\n\n- For the GENIA dataset, the Entity F1 scores for the systems are:\n  - Katiyar and Cardie (2018): 73.8\n  - Wang and Lu (2018): 75.1\n  - DyGIE: 76.2\n\nThe DyGIE system achieves the highest Entity F1 score across all datasets.\nThe table provides performance metrics of different models on entity and relation extraction tasks. For entities and relations, it presents precision (P), recall (R), and F1 scores. The models compared include:\n\n1. **DyGIE**:\n   - Entity: P = 87.4, R = 86.7, F1 = 87.1\n   - Relation: P = 56.2, R = 60.9, F1 = 58.4\n\n2. **DyGIE without Coreference Propagation (−CorefProp)**:\n   - Entity: P = 86.2, R = 85.2, F1 = 85.7\n   - Relation: P = 64.3, R = 56.7, F1 = 60.2\n\n3. **DyGIE without Relation Propagation (−RelProp)**:\n   - Entity: P = 87.0, R = 86.7, F1 = 86.9\n   - Relation: P = 60.4, R = 55.8, F1 = 58.0\n\n4. **Base**:\n   - Entity: P = 86.1, R = 85.7, F1 = 85.9\n   - Relation: P = 59.5, R = 55.7, F1 = 57.6\n\nFrom these results, DyGIE shows the highest F1 score for entities while −CorefProp scores best for relations.\nTable 5: Ablations on the ACE05 development set with different graph propagation setups. − CorefProp ablates the coreference propagation layers, while  $\\mathtt{-R e l P r o p}$   ablates the relation propagation layers. Base  is the system without any propagation. \nWe use the same data set split and preprocessing procedure as  Wang and Lu  ( 2018 ) for overlapping entity recognition. \nBaselines The current state-of-the-art approach on all three data sets is  Wang and Lu  ( 2018 ), which uses a segmental hypergraph coupled with neural networks for feature learning.  Katiyar and Cardie ( 2018 ) also propose a hypergraph approach using a recurrent neural network as a feature extractor. \nResults Table  4  presents the results of our over- lapping entity extraction experiments on the differ- ent datsets. D Y GIE improves   $11.6\\%$   on the state of the art for ACE04-O and   $11.3\\%$   for ACE05-O. D Y - GIE also advances the state of the art on GENIA, albeit by a more modest   $1.5\\%$  . Together these re- sults suggest that D Y GIE can be utilized fruitfully for information extraction across different domains with overlapped entities, such as bio-medicine. \n5 Analysis of Graph Propagation \nWe use the dev sets of ACE2005 and SciERC to analyze the effect of different model components. \n5.1 Coreference and Relation Graph Layers \nTables  5  and  6  show the effects of graph propa- gation on entity and relation prediction accuracy, "}
{"page": 7, "image_path": "doc_images/N19-1308_7.jpg", "ocr_text": "Entity Relation\nModel P R_ Fl P R_ FI\nDYGIE 68.6 67.8 68.2 46.2 38.5 42.0\n—CorefProp 69.2 66.9 68.0 42.0 40.5 41.2\n—RelProp 69.1 66.0 67.5 43.6 37.6 40.4\nBase 70.0 66.3 68.1 45.4 34.9 39.5\n\nTable 6: Ablations on the SciERC development set on\ndifferent graph progation setups. CorefProp has a\nmuch smaller effect on entity Fl compared to ACE0S.\n\n90 64\n\n88\n60\n58\n\n86 a)\n0 1 2 3\n\n84\nNum. iterations Mf\n\nEntity Fl\nRelation Fl\n\n82\n80\n\ni} 1 2 3\nNum. iterations N’\n\n(a) Entity Fl with different\nnumber of CorefProp it-\nerations NV.\n\n(b) Relation F1 with differ-\nent number of Re1P rop it-\nerations M.\n\nFigure 3: F1 score of each layer on ACE development\nset for different number of iterations. N = 0 or M = 0\nindicates no propagation is made for the layer.\n\nwhere —CorefProp and —RelProp denote ab-\nlating the propagation process by setting N = 0\nor M = 0, respectively. Base is the base model\nwithout any propagation. For ACE0S, we observe\nthat coreference propagation is mainly helpful for\nentities; it appears to hurt relation extraction. On\nScilE, coreference propagation gives a small ben-\nefit on both tasks. Relation propagation signifi-\ncantly benefits both entity and relation extraction\nin both domains. In particular, there are a large por-\ntion of sentences with multiple relation instances\nacross different entities in both ACEOS and Sci-\nERC, which is the scenario in which we expect\nrelation propagation to help.\n\nSince coreference propagation has more effect\non entity extraction and relation propagation has\nmore effect on relation extraction, we mainly focus\non ablating the effect of coreference propagation\non entity extraction and relation propagation on\nrelation extraction in the following subsections.\n\n5.2 Coreference Propagation and Entities\n\nA major challenge of ACEO5 is to disambiguate\nthe entity class for pronominal mentions, which\nrequires reasoning with cross-sentence contexts.\nFor example, in a sentence from ACEO0S5 dataset,\n“One of [them]prr, from a very close friend of\n[ours]ora.” It is impossible to identity whether\nthem and ours is a person (PER) or organization\n(ORG) unless we have read previous sentences. We\n\nEntity Perf. on Pronouns P R FL\n\nDYGIE 79.0 77.1 78.0\nDyGIE—CorefProp 73.8 72.6 73.2\n\nTable 7: Entity extraction performance on pronouns in\nACE05. CorefProp significantly increases entity ex-\ntraction F1 on hard-to-disambiguate pronouns by allow-\ning the model to leverage cross-sentence contexts.\n\nhypothesize that this is a context where coreference\npropagation can help. Table 7 shows the effect\nof the coreference layer for entity categorization\nof pronouns.* DYGIE has 6.6% improvement on\npronoun performance, confirming our hypothesis.\nLooking further, Table 8 shows the impact on all\nentity categories, giving the difference between\nhe confusion matrix entries with and without\nCorefProp. The frequent confusions associated\nwith pronouns (GPE/PER and PER/ORG, where\nGPE is a geopolitical entity) greatly improve, but\nhe benefit of CorefProp extends to most cate-\ngories.\n\nOf course, there are a few instances where\nCorefProp causes errors in entity extraction. For\nexample, in the sentence “[They]oR¢ might have\nbeen using Northshore...”, DYGIE predicted They\nto be of ORG type because the most confident an-\ntecedent is those companies in the previous sen-\ntence: “The money was invested in those compa-\nnies.” However, They is actually referring to these\nfund managers earlier in the document, which be-\nlongs to PER category.\n\nIn the SciERC dataset, the pronouns are uni-\nformly assigned with a Generic label, which ex-\nplains why CorefProp does not have much ef-\nfect on entity extraction performance.\n\nThe Figure 3a shows the effect of number of\niterations for coreference propagation in the entity\nextraction task. The figure shows that coreference\nlayer obtains the best performance on the second\niteration (N = 2).\n\n5.3. Relation Propagation Impact\n\nFigure 4 shows relation scores as a function of num-\nber of entities in sentence for DYGIE and DYGIE\nwithout relation propagation on ACEOS. The figure\nindicates that relation propagation achieves signifi-\ncant improvement in sentences with more entities,\nwhere one might expect that using broader context\n\n>Pronouns included:\n\nitself, one, our,\nthem, themselves,\n\nanyone, everyone, it,\nours, their, theirs,\nthey, us, we, who\n\n3043\n", "vlm_text": "The table presents the performance metrics for different models on entity recognition and relation extraction tasks. \n\n- The models listed are DyGIE, DyGIE without CorefProp, DyGIE without RelProp, and a Base model.\n- For each model, three metrics are reported: \n  - P (Precision)\n  - R (Recall)\n  - F1 (F1 Score)\n\n**Entity Task:**\n- DyGIE achieves a precision of 68.6, a recall of 67.8, and an F1 score of 68.2.\n- DyGIE without CorefProp achieves a precision of 69.2, a recall of 66.9, and an F1 score of 68.0.\n- DyGIE without RelProp achieves a precision of 69.1, a recall of 66.0, and an F1 score of 67.5.\n- The Base model achieves a precision of 70.0, a recall of 66.3, and an F1 score of 68.1.\n\n**Relation Task:**\n- DyGIE achieves a precision of 46.2, a recall of 38.5, and an F1 score of 42.0.\n- DyGIE without CorefProp achieves a precision of 42.0, a recall of 40.5, and an F1 score of 41.2.\n- DyGIE without RelProp achieves a precision of 43.6, a recall of 37.6, and an F1 score of 40.4.\n- The Base model achieves a precision of 45.4, a recall of 34.9, and an F1 score of 39.5.\n\nThe bold numbers indicate the highest F1 scores in the specific category within each model.\ndifferent graph progation setups.  CorefProp  has a much smaller effect on entity F1 compared to ACE05. \nThe image shows two line graphs comparing the performance of entity and relation extraction with different numbers of iterations for two processes: CorefProp and RelProp. \n\n- The left graph is titled \"Entity F1\" and represents the F1 score of entity extraction across four iteration counts (0, 1, 2, and 3) for CorefProp. The scores are plotted as a blue line with circular markers. The highest score appears at two iterations.\n\n- The right graph is titled \"Relation F1\" and represents the F1 score of relation extraction across four iteration counts (0, 1, 2, and 3) for RelProp. The scores are plotted as a red line with square markers. The highest score occurs at two iterations.\n\nThe graphs indicate the intended impact of iterative processes on F1 scores for both entity and relation extraction tasks.\nFigure 3: F1 score of each layer on ACE development set for different number of iterations.    $N=0$   or  $M=0$  indicates no propagation is made for the layer. \nwhere  − CorefProp  and  − RelProp  de lating the propagation process by setting  $N=0$  or    $M=0$  , respectively.  Base  is the base model without any propagation. For ACE05, we observe that coreference propagation is mainly helpful for entities; it appears to hurt relation extraction. On SciIE, coreference propagation gives a small ben- eﬁt on both tasks. Relation propagation signiﬁ- cantly beneﬁts both entity and relation extraction in both domains. In particular, there are a large por- tion of sentences with multiple relation instances across different entities in both ACE05 and Sci- ERC, which is the scenario in which we expect relation propagation to help. \nSince coreference propagation has more effect on entity extraction and relation propagation has more effect on relation extraction, we mainly focus on ablating the effect of coreference propagation on entity extraction and relation propagation on relation extraction in the following subsections. \n5.2 Coreference Propagation and Entities \nA major challenge of ACE05 is to disambiguate the entity class for pronominal mentions, which requires reasoning with cross-sentence contexts. For example, in a sentence from ACE05 dataset,\n\n “One of    $[\\mathbf{them}]_{\\mathrm{PER}}$  , from a very close friend of\n\n [ours] ORG .” It is impossible to identity whether them  and  ours  is a person ( PER ) or organization ( ORG ) unless we have read previous sentences. We hypothesize that this is a context where coreference propagation can help. Table  7  shows the effect of the coreference layer for entity categorization of pronouns.   D Y GIE has  $6.6\\%$   improvement on pronoun performance, conﬁrming our hypothesis. \n\nLooking further, Table  8  shows the impact on all entity categories, giving the difference between the confusion matrix entries with and without CorefProp . The frequent confusions associated with pronouns ( GPE/PER  and  PER/ORG , where  $G P E$   is a geopolitical entity) greatly improve, but the beneﬁt of  CorefProp  extends to most cate- gories. \nOf course, there are a few instances where CorefProp  causes errors in entity extraction. For example, in the sentence “[They] ORG   might have PER been using Northshore...”, D Y GIE predicted  They to be of    $O R G$   type because the most conﬁdent an- tecedent is  those companies  in the previous sen- tence: “The money was invested in  those compa- nies .” However,  They  is actually referring to  these fund managers  earlier in the document, which be- longs to  PER  category. \nIn the SciERC dataset, the pronouns are uni- formly assigned with a  Generic  label, which ex- plains why  CorefProp  does not have much ef- fect on entity extraction performance. \nThe Figure  3a  shows the effect of number of iterations for coreference propagation in the entity extraction task. The ﬁgure shows that coreference layer obtains the best performance on the second iteration   $(N=2)$  ). \n5.3 Relation Propagation Impact \nFigure  4  shows relation scores as a function of num- ber of entities in sentence for D Y GIE and D Y GIE without relation propagation on ACE05. The ﬁgure indicates that relation propagation achieves signiﬁ- cant improvement in sentences with more entities, where one might expect that using broader context "}
{"page": 8, "image_path": "doc_images/N19-1308_8.jpg", "ocr_text": "| LOC WEA GPE PER FAC ORG_ VEH\n\nLOC 5 0 -2 -1 2 -1 0\nWEA 0 3 0 0 1 -3 -1\nGPE -3 0 1 -26 3 -7 0\nPER 0) -2 -3 18 -1 -26 4\nFAC 4 -1 2 -3 2) -5 1\nORG 0) 0 0 -8 -1 6 0\nVEH 0 -2 -1 2 5 -1 1\nTable 8: Difference in the confusion matrix counts\n\nfor ACE0S5 entity extraction associated with adding\nCorefProp.\n\n70 - DyYGIE\na —6+ DyGIE—RelProp\n[say\n=\n& 60\n5\n3\n4\n50\n\n2 3 4-5 6-11 12-max\n\nNum. entities in sentence\nFigure 4: Relation Fl broken down by number of enti-\nties in each sentence. The performance of relation ex-\ntraction degrades on sentences containing more entities.\nAdding relation propagation alleviates this problem.\n\ncould have more impact.\n\nFigure 3b shows the effect of number of itera-\ntions for relation propagation in the relation extrac-\ntion task. Our model achieves the best performance\non the second iteration (IM = 2).\n\n6 Conclusion\n\nWe have introduced DYGIE as a general informa-\ntion extraction framework, and have demonstrated\nthat our system achieves state-of-the art results\non entity recognition and relation extraction tasks\nacross a diverse range of domains. The key con-\ntribution of our model is the dynamic span graph\napproach, which enhance interaction across tasks\nthat allows the model to learn useful information\nfrom broader context. Unlike many IE frameworks,\nour model does not require any preprocessing using\nsyntactic tools, and has significant improvement\nacross different IE tasks including entity, relation\nextraction and overlapping entity extraction. The\naddition of co-reference and relation propagation\nacross sentences adds only a small computation\ncost to inference; the memory cost is controlled by\nbeam search. These added costs are small relative\nto those of the baseline span-based model. We wel-\ncome the community to test our model on different\ninformation extraction tasks. Future directions in-\nclude extending the framework to encompass more\nstructural IE tasks such as event extraction.\n\nAcknowledgments\n\nThis research was supported by the Office of Naval\nResearch under the MURI grant NO0014-18-1-\n2670, NSF (IIS 1616112, III 1703166), Allen Dis-\ntinguished Investigator Award, Samsung GRO and\ngifts from Allen Institute for AI, Google, Amazon,\nand Bloomberg. We also thank the anonymous re-\nviewers and the UW-NLP group for their helpful\ncomments.\n\nReferences\n\nGiannis Bekoulis, Johannes Deleu, Thomas Demeester,\nand Chris Develder. 2018. Adversarial training for\nmulti-context joint entity and relation extraction. In\nProc. Conf. Empirical Methods Natural Language\nProcess. (EMNLP), pages 2830-2836.\n\nAntoine Bordes, Nicolas Usunier, Alberto Garcia-\nDuran, Jason Weston, and Oksana Yakhnenko.\n2013. Translating embeddings for modeling multi-\nrelational data. In Advances in neural information\nprocessing systems.\n\nYee Seng Chan and Dan Roth. 2011. Exploiting\nsyntactico-semantic structures for relation extrac-\ntion. In Proc. Annu. Meeting Assoc. for Computa-\ntional Linguistics (ACL).\n\nFenia Christopoulou, Makoto Miwa, and Sophia Ana-\nniadou. 2018. A walk-based model on entity graphs\nfor relation extraction. In Proc. Annu. Meeting As-\nsoc. for Computational Linguistics (ACL), volume 2,\npages 81-88.\n\nRonan Collobert and Jason Weston. 2008. A unified\narchitecture for natural language processing: Deep\nneural networks with multitask learning. In Proc.\nInt. Conf. Machine Learning (ICML), pages 160-\n167.\n\nRonan Collobert, Jason Weston, Léon Bottou, Michael\nKarlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011.\nNatural language processing (almost) from scratch.\nJ. Machine Learning Research, 12(Aug):2493-\n2537.\n\nGreg Durrett and Dan Klein. 2014. A joint model\nfor entity analysis: Coreference, typing, and linking.\nTrans. Assoc. for Computational Linguistics (TACL),\n2:477-490.\n\nYarin Gal and Zoubin Ghahramani. 2016. A theoret-\nically grounded application of dropout in recurrent\nneural networks. In Proc. Annu. Conf. Neural In-\nform. Process. Syst. (NIPS).\n\nHannaneh Hajishirzi, Leila Zilles, Daniel S Weld, and\nLuke Zettlemoyer. 2013. Joint coreference res-\nolution and named-entity linking with multi-pass\nsieves. In Proc. Conf. Empirical Methods Natural\nLanguage Process. (EMNLP), pages 289-299.\n\n3044\n", "vlm_text": "This table appears to display a matrix of some form of interaction or relationship between different categories, possibly related to Named Entity Recognition (NER) types in text processing. The column and row headers are abbreviations commonly used in NER:\n\n- LOC: Location\n- WEA: Weapon\n- GPE: Geopolitical Entity\n- PER: Person\n- FAC: Facility\n- ORG: Organization\n- VEH: Vehicle\n\nThe diagonal of the table is shaded, possibly indicating that it represents the frequency, strength, or a score of the entity interacting with itself. The numbers off the diagonal could reflect some interaction, misclassification rates, or co-occurrence frequency between the entities. Negative values might indicate conflicts, errors, or other statistical measures, while positive values could indicate cooperation or other positive attributes. The specific meaning of the numbers would depend on the context in which this table is used.\nThe image is a line graph showing the performance of relation extraction systems, measured by F1 score, as a function of the number of entities present in a sentence. There are two compared systems: \"DyGIE\" and \"DyGIE-RelProp.\"\n\n- The x-axis represents the number of entities in each sentence, categorized into groups (2, 3, 4-5, 6-11, 12-max).\n- The y-axis represents the Relation F1 score, ranging from 50 to 70.\n\nKey Observations:\n- For sentences with 2 entities, both systems perform similarly with high F1 scores.\n- As the number of entities in the sentence increases, the performance of both systems decreases.\n- \"DyGIE\" (represented by a blue line with circle markers) generally outperforms \"DyGIE-RelProp\" (represented by a red line with square markers) across all categories.\n- Despite the decrease in performance with more entities, the addition of \"relation propagation\" (in \"DyGIE-RelProp\") attempts to address this decline, although not as effectively as \"DyGIE\" without relation propagation.\ncould have more impact. \nFigure  3b  shows the effect of number of itera- tions for relation propagation in the relation extrac- tion task. Our model achieves the best performance on the second iteration (  $M=2$  ). \n6 Conclusion \nWe have introduced D Y GIE as a general informa- tion extraction framework, and have demonstrated that our system achieves state-of-the art results on entity recognition and relation extraction tasks across a diverse range of domains. The key con- tribution of our model is the dynamic span graph approach, which enhance interaction across tasks that allows the model to learn useful information from broader context. Unlike many IE frameworks, our model does not require any preprocessing using syntactic tools, and has signiﬁcant improvement across different IE tasks including entity, relation extraction and overlapping entity extraction. The addition of co-reference and relation propagation across sentences adds only a small computation cost to inference; the memory cost is controlled by beam search. These added costs are small relative to those of the baseline span-based model. We wel- come the community to test our model on different information extraction tasks. Future directions in- clude extending the framework to encompass more structural IE tasks such as event extraction. \nAcknowledgments \nThis research was supported by the Ofﬁce of Naval Research under the MURI grant N00014-18-1- 2670, NSF (IIS 1616112, III 1703166), Allen Dis- tinguished Investigator Award, Samsung GRO and gifts from Allen Institute for AI, Google, Amazon, and Bloomberg. We also thank the anonymous re- viewers and the UW-NLP group for their helpful comments. \nReferences \nand Chris Develder. 2018. Adversarial training for multi-context joint entity and relation extraction. In Proc. Conf. Empirical Methods Natural Language Process. (EMNLP) , pages 2830–2836. Antoine Bordes, Nicolas Usunier, Alberto Garcia- Duran, Jason Weston, and Oksana Yakhnenko. 2013. Translating embeddings for modeling multi- relational data. In  Advances in neural information processing systems . Yee Seng Chan and Dan Roth. 2011. Exploiting syntactico-semantic structures for relation extrac- tion. In  Proc. Annu. Meeting Assoc. for Computa- tional Linguistics (ACL) . Fenia Christopoulou, Makoto Miwa, and Sophia Ana- niadou. 2018. A walk-based model on entity graphs for relation extraction. In  Proc. Annu. Meeting As- soc. for Computational Linguistics (ACL) , volume 2, pages 81–88. Ronan Collobert and Jason Weston. 2008. A uniﬁed architecture for natural language processing: Deep neural networks with multitask learning. In  Proc. Int. Conf. Machine Learning (ICML) , pages 160– 167. Ronan Collobert, Jason Weston, L´ eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. J. Machine Learning Research , 12(Aug):2493– 2537. Greg Durrett and Dan Klein. 2014. A joint model for entity analysis: Coreference, typing, and linking. Trans. Assoc. for Computational Linguistics (TACL) , 2:477–490. Yarin Gal and Zoubin Ghahramani. 2016. A theoret- ically grounded application of dropout in recurrent neural networks. In  Proc. Annu. Conf. Neural In- form. Process. Syst. (NIPS) . Hannaneh Hajishirzi, Leila Zilles, Daniel S Weld, and Luke Zettlemoyer. 2013. Joint coreference res- olution and named-entity linking with multi-pass sieves. In  Proc. Conf. Empirical Methods Natural Language Process. (EMNLP) , pages 289–299. "}
{"page": 9, "image_path": "doc_images/N19-1308_9.jpg", "ocr_text": "Luheng He, Kenton Lee, Omer Levy, and Luke Zettle-\nmoyer. 2018. Jointly predicting predicates and argu-\nments in neural semantic role labeling. In ACL.\n\nArzoo Katiyar and Claire Cardie. 2018. Nested\nnamed entity recognition revisited. In Proc. Conf.\nNorth American Assoc. for Computational Linguis-\ntics (NAACL).\n\nJin-Dong Kim, Tomoko Ohta, Yuka Tateisi, and\nJun’ichi Tsujii. 2003. Genia corpus - a semantically\nannotated corpus for bio-textmining. Bioinformat-\nics, 19 Suppl 1:i180-2.\n\nChaitanya Kulkarni, Wei Xu, Alan Ritter, and Raghu\nMachiraju. 2018. An annotated corpus for machine\nreading of instructions in wet lab protocols. In\nNAACL-HLT.\n\nGuillaume Lample, Miguel Ballesteros, Sandeep Sub-\nramanian, Kazuya Kawakami, and Chris Dyer. 2016.\nNeural architectures for named entity recognition.\nIn Proc. Conf. North American Assoc. for Compu-\ntational Linguistics (NAACL).\n\nKenton Lee, Luheng He, Mike Lewis, and Luke S.\nZettlemoyer. 2017. End-to-end neural coreference\nresolution. In EMNLP.\n\nKenton Lee, Luheng He, and Luke Zettlemoyer. 2018.\nHigher-order coreference resolution with coarse-to-\nfine inference. In NAACL.\n\nQi Li and Heng Ji. 2014. Incremental joint extrac-\ntion of entity mentions and relations. In Proc.\nAnnu. Meeting Assoc. for Computational Linguistics\n(ACL), volume 1, pages 402-412.\n\nYi Luan, Chris Brockett, Bill Dolan, Jianfeng Gao,\nand Michel Galley. 2017a. Multi-task learning for\nspeaker-role adaptation in neural conversation mod-\nels. In Proc. LJICNLP.\n\nYi Luan, Luheng He, Mari Ostendorf, and Hannaneh\nHajishirzi. 2018a. Multi-task identification of enti-\nties, relations, and coreference for scientific knowl-\nedge graph construction. In Proc. Conf. Empirical\nMethods Natural Language Process. (EMNLP).\n\nYi Luan, Mari Ostendorf, and Hannaneh Hajishirzi.\n2017b. Scientific information extraction with semi-\nsupervised neural tagging. In Proc. Conf. Empirical\nMethods Natural Language Process. (EMNLP).\n\nYi Luan, Mari Ostendorf, and Hannaneh Hajishirzi.\n2018b. The uwnlp system at semeval-2018 task 7:\nNeural relation extraction model with selectively in-\ncorporated concept embeddings. In Proc. Int. Work-\nshop on Semantic Evaluation (SemEval), pages 788—\n792.\n\nXuezhe Ma and Eduard Hovy. 2016. End-to-end\nsequence labeling via bi-directional LSTM-CNNs-\nCRF. In Proc. Annu. Meeting Assoc. for Computa-\ntional Linguistics (ACL).\n\nMakoto Miwa and Mohit Bansal. 2016. End-to-end re-\nlation extraction using Istms on sequences and tree\nstructures. In Proc. Annu. Meeting Assoc. for Com-\nputational Linguistics (ACL), pages 1105-1116.\n\nDavid Nadeau and Satoshi Sekine. 2007. A survey of\nnamed entity recognition and classification. Lingvis-\nticae Investigationes, 30(1):3-26.\n\nNanyun Peng and Mark Dredze. 2015. Named en-\ntity recognition for chinese social media with jointly\ntrained embeddings. In Proc. Conf. Empirical Meth-\nods Natural Language Process. (EMNLP), pages\n548-554.\n\nNanyun Peng, Hoifung Poon, Chris Quirk, Kristina\nToutanova, and Wen-tau Yih. 2017. Cross-sentence\nn-ary relation extraction with graph Istms. Trans. As-\nsoc. for Computational Linguistics (TACL), 5:101-\n115.\n\nJeffrey Pennington, Richard Socher, and Christopher D\nManning. 2014. Glove: Global vectors for word rep-\nresentation. In Proc. Conf. Empirical Methods Natu-\nral Language Process. (EMNLP), volume 14, pages\n1532-1543.\n\nMatthew E. Peters, Mark Neumann, Mohit Lyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. In NAACL.\n\nSameer Pradhan, Alessandro Moschitti, Nianwen Xue,\nOlga Uryupina, and Yuchen Zhang. 2012. Conll-\n2012 shared task: Modeling multilingual unre-\nstricted coreference in ontonotes. In Joint Confer-\nence on EMNLP and CoNLL-Shared Task, pages 1-\n40. Association for Computational Linguistics.\n\nVictor Sanh, Thomas Wolf, and Sebastian Ruder. 2019.\nA hierarchical multi-task approach for learning em-\nbeddings from semantic tasks. AAAI.\n\nSameer Singh, Sebastian Riedel, Brian Martin, Jiaping\nZheng, and Andrew McCallum. 2013. Joint infer-\nence of entities, relations, and coreference. In Proc.\nof the 2013 workshop on Automated knowledge base\nconstruction, pages 1-6. ACM.\n\nLinfeng Song, Yue Zhang, Zhiguo Wang, and Daniel\nGildea. 2018. N-ary relation extraction using graph-\nstate Istm. In Proc. Conf. Empirical Methods Natu-\nral Language Process. (EMNLP), pages 2226-2235.\n\nBailin Wang and Wei Lu. 2018. Neural segmental hy-\npergraphs for overlapping mention recognition. In\nEMNLP.\n\nKun Xu, Yansong Feng, Songfang Huang, and\nDongyan Zhao. 2015. Semantic relation classifica-\ntion via convolutional neural networks with simple\nnegative sampling. In Proc. Conf. Empirical Meth-\nods Natural Language Process. (EMNLP), pages\n536-540.\n\n3045\n", "vlm_text": "Luheng He, Kenton Lee, Omer Levy, and Luke Zettle- moyer. 2018. Jointly predicting predicates and argu- ments in neural semantic role labeling. In  ACL . Arzoo Katiyar and Claire Cardie. 2018. Nested named entity recognition revisited. In  Proc. Conf. North American Assoc. for Computational Linguis- tics (NAACL) . Jin-Dong Kim, Tomoko Ohta, Yuka Tateisi, and Jun’ichi Tsujii. 2003. Genia corpus - a semantically annotated corpus for bio-textmining.  Bioinformat- ics , 19 Suppl 1:i180–2. Chaitanya Kulkarni, Wei Xu, Alan Ritter, and Raghu Machiraju. 2018. An annotated corpus for machine reading of instructions in wet lab protocols. In NAACL-HLT . Guillaume Lample, Miguel Ballesteros, Sandeep Sub- ramanian, Kazuya Kawakami, and Chris Dyer. 2016. Neural architectures for named entity recognition. In  Proc. Conf. North American Assoc. for Compu- tational Linguistics (NAACL) . Kenton Lee, Luheng He, Mike Lewis, and Luke S. Zettlemoyer. 2017. End-to-end neural coreference resolution. In  EMNLP . Kenton Lee, Luheng He, and Luke Zettlemoyer. 2018. Higher-order coreference resolution with coarse-to- ﬁne inference. In  NAACL . Qi Li and Heng Ji. 2014. Incremental joint extrac- tion of entity mentions and relations. In  Proc. Annu. Meeting Assoc. for Computational Linguistics (ACL) , volume 1, pages 402–412. Yi Luan, Chris Brockett, Bill Dolan, Jianfeng Gao, and Michel Galley. 2017a. Multi-task learning for speaker-role adaptation in neural conversation mod- els. In  Proc. IJCNLP . Yi Luan, Luheng He, Mari Ostendorf, and Hannaneh Hajishirzi. 2018a. Multi-task identiﬁcation of enti- ties, relations, and coreference for scientiﬁc knowl- edge graph construction. In  Proc. Conf. Empirical Methods Natural Language Process. (EMNLP) . Yi Luan, Mari Ostendorf, and Hannaneh Hajishirzi. 2017b. Scientiﬁc information extraction with semi- supervised neural tagging. In  Proc. Conf. Empirical Methods Natural Language Process. (EMNLP) . Yi Luan, Mari Ostendorf, and Hannaneh Hajishirzi. 2018b. The uwnlp system at semeval-2018 task 7: Neural relation extraction model with selectively in- corporated concept embeddings. In  Proc. Int. Work- shop on Semantic Evaluation (SemEval) , pages 788– 792. Xuezhe Ma and Eduard Hovy. 2016. End-to-end sequence labeling via bi-directional LSTM-CNNs- CRF. In  Proc. Annu. Meeting Assoc. for Computa- tional Linguistics (ACL) . \nMakoto Miwa and Mohit Bansal. 2016. End-to-end re- lation extraction using lstms on sequences and tree structures. In  Proc. Annu. Meeting Assoc. for Com- putational Linguistics (ACL) , pages 1105–1116. David Nadeau and Satoshi Sekine. 2007. A survey of named entity recognition and classiﬁcation.  Lingvis- ticae Investigationes , 30(1):3–26. Nanyun Peng and Mark Dredze. 2015. Named en- tity recognition for chinese social media with jointly trained embeddings. In  Proc. Conf. Empirical Meth- ods Natural Language Process. (EMNLP) , pages 548–554. Nanyun Peng, Hoifung Poon, Chris Quirk, Kristina Toutanova, and Wen-tau Yih. 2017. Cross-sentence n-ary relation extraction with graph lstms.  Trans. As- soc. for Computational Linguistics (TACL) , 5:101– 115. Jeffrey Pennington, Richard Socher, and Christopher D Manning. 2014. Glove: Global vectors for word rep- resentation. In  Proc. Conf. Empirical Methods Natu- ral Language Process. (EMNLP) , volume 14, pages 1532–1543. Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word repre- sentations. In  NAACL . Sameer Pradhan, Alessandro Moschitti, Nianwen Xue, Olga Uryupina, and Yuchen Zhang. 2012. Conll- 2012 shared task: Modeling multilingual unre- stricted coreference in ontonotes. In  Joint Confer- ence on EMNLP and CoNLL-Shared Task , pages 1– 40. Association for Computational Linguistics. Victor Sanh, Thomas Wolf, and Sebastian Ruder. 2019. A hierarchical multi-task approach for learning em- beddings from semantic tasks.  AAAI . Sameer Singh, Sebastian Riedel, Brian Martin, Jiaping Zheng, and Andrew McCallum. 2013. Joint infer- ence of entities, relations, and coreference. In  Proc. of the 2013 workshop on Automated knowledge base construction , pages 1–6. ACM. Linfeng Song, Yue Zhang, Zhiguo Wang, and Daniel Gildea. 2018. N-ary relation extraction using graph- state lstm. In  Proc. Conf. Empirical Methods Natu- ral Language Process. (EMNLP) , pages 2226–2235. Bailin Wang and Wei Lu. 2018. Neural segmental hy- pergraphs for overlapping mention recognition. In EMNLP . Kun Xu, Yansong Feng, Songfang Huang, and Dongyan Zhao. 2015. Semantic relation classiﬁca- tion via convolutional neural networks with simple negative sampling. In  Proc. Conf. Empirical Meth- ods Natural Language Process. (EMNLP) , pages 536–540. "}
{"page": 10, "image_path": "doc_images/N19-1308_10.jpg", "ocr_text": "Bishan Yang and Tom M Mitchell. 2016. Joint extrac-\ntion of events and entities within a document context.\nIn Proceedings of the 2016 Conference of the North\nAmerican Chapter of the Association for Computa-\n\ntional Linguistics: Human Language Technologies,\npages 289-299.\n\nBishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng\nGao, and Li Deng. 2015. Embedding entities and\nrelations for learning and inference in knowledge\nbases. In Proc. Int. Conf. Learning Representations\n(ICLR).\n\nMeishan Zhang, Yue Zhang, and Guohong Fu. 2017.\nEnd-to-end neural relation extraction with global op-\ntimization. In Proc. Conf. Empirical Methods Natu-\nral Language Process. (EMNLP), pages 1730-1740.\n\nYuhao Zhang, Peng Qi, and Christopher D Man-\nning. 2018. Graph convolution over pruned depen-\ndency trees improves relation extraction. In Proc.\nConf. Empirical Methods Natural Language Pro-\ncess. (EMNLP).\n\n3046\n", "vlm_text": "Bishan Yang and Tom M Mitchell. 2016. Joint extrac- tion of events and entities within a document context. In  Proceedings of the 2016 Conference of the North American Chapter of the Association for Computa- tional Linguistics: Human Language Technologies , pages 289–299. Bishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng Gao, and Li Deng. 2015. Embedding entities and relations for learning and inference in knowledge bases. In  Proc. Int. Conf. Learning Representations (ICLR) . Meishan Zhang, Yue Zhang, and Guohong Fu. 2017. End-to-end neural relation extraction with global op- timization. In  Proc. Conf. Empirical Methods Natu- ral Language Process. (EMNLP) , pages 1730–1740. Yuhao Zhang, Peng Qi, and Christopher D Man- ning. 2018. Graph convolution over pruned depen- dency trees improves relation extraction. In  Proc. Conf. Empirical Methods Natural Language Pro- cess. (EMNLP) . "}
