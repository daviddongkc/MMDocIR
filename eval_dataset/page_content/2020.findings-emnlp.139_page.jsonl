{"page": 0, "image_path": "doc_images/2020.findings-emnlp.139_0.jpg", "ocr_text": "CodeBERT:\nA Pre-Trained Model for Programming and Natural Languages\n\nZhangyin Feng’; Daya Guo’; Duyu Tang*, Nan Duan’, Xiaocheng Feng!\nMing Gong’, Linjun Shou’, Bing Qin’, Ting Liu!, Daxin Jiang’, Ming Zhou*\n\n1 Research Center for Social Computing and Information Retrieval, Harbin Institute of Technology, China\n\n? The School of Data and Computer Science, Sun Yat-sen University, China\n3 Microsoft Research Asia, Beijing, China\n4 Microsoft Search Technology Center Asia, Beijing, China\n{zyfeng, xcfeng, qinb, tliu}@ir.hit.edu.cn\nguody5@mail2.sysu.edu.cn\n{dutang, nanduan,migon, lisho,djiang, mingzhou}@microsoft .com\n\nAbstract\n\nWe present CodeBERT, a bimodal pre-trained\nmodel for programming language (PL) and\nnatural language (NL). CodeBERT learns\ngeneral-purpose representations that support\ndownstream NL-PL applications such as nat-\nural language code search, code documen-\ntation generation, etc. We develop Code-\nBERT with Transformer-based neural architec-\nture, and train it with a hybrid objective func-\ntion that incorporates the pre-training task of\nreplaced token detection, which is to detect\nplausible alternatives sampled from generators.\nThis enables us to utilize both “bimodal” data\nof NL-PL pairs and “unimodal” data, where\nthe former provides input tokens for model\ntraining while the latter helps to learn bet-\nter generators. We evaluate CodeBERT on\ntwo NL-PL applications by fine-tuning model\nparameters. Results show that CodeBERT\nachieves state-of-the-art performance on both\nnatural language code search and code docu-\nmentation generation. Furthermore, to inves-\ntigate what type of knowledge is learned in\nCodeBERT, we construct a dataset for NL-PL\nprobing, and evaluate in a zero-shot setting\nwhere parameters of pre-trained models are\nfixed. Results show that CodeBERT performs\nbetter than previous pre-trained models on NL-\nPL probing.!\n\n1 Introduction\n\nLarge pre-trained models such as ELMo (Peters\net al., 2018), GPT (Radford et al., 2018), BERT\n(Devlin et al., 2018), XLNet (Yang et al., 2019)\n\n“Work done while this author was an intern at Microsoft\nResearch Asia.\n'AIl the codes and data are available at https: //\ngithub.com/microsoft/CodeBERT\n\nand RoBERTa (Liu et al., 2019) have dramati-\ncally improved the state-of-the-art on a variety of\nnatural language processing (NLP) tasks. These\npre-trained models learn effective contextual repre-\nsentations from massive unlabeled text optimized\nby self-supervised objectives, such as masked\nlanguage modeling, which predicts the original\nmasked word from an artificially masked input\nsequence. The success of pre-trained models in\nNLP also drives a surge of multi-modal pre-trained\nmodels, such as ViLBERT (Lu et al., 2019) for\nlanguage-image and VideoBERT (Sun et al., 2019)\nfor language-video, which are learned from bi-\nmodal data such as language-image pairs with bi-\nmodal self-supervised objectives.\n\nIn this work, we present CodeBERT, a bimodal\npre-trained model for natural language (NL) and\nprogramming language (PL) like Python, Java,\nJavaScript, etc. CodeBERT captures the seman-\ntic connection between natural language and pro-\ngramming language, and produces general-purpose\nrepresentations that can broadly support NL-PL\nunderstanding tasks (e.g. natural language code\nsearch) and generation tasks (e.g. code documen-\ntation generation). It is developed with the mullti-\nlayer Transformer (Vaswani et al., 2017), which is\nadopted in a majority of large pre-trained models.\nIn order to make use of both bimodal instances\nof NL-PL pairs and large amount of available uni-\nmodal codes, we train CodeBERT with a hybrid\nobjective function, including standard masked lan-\nguage modeling (Devlin et al., 2018) and replaced\ntoken detection (Clark et al., 2020), where uni-\nmodal codes help to learn better generators for\nproducing better alternative tokens for the latter\nobjective.\n\nWe train CodeBERT from Github code reposito-\n\n1536\n\nFindings of the Association for Computational Linguistics: EMNLP 2020, pages 1536-1547\nNovember 16 - 20, 2020. ©2020 Association for Computational Linguistics\n", "vlm_text": "CodeBERT: A Pre-Trained Model for Programming and Natural Languages \nZhangyin  $\\mathbf{Feng}^{1;}$  , Daya  $\\mathbf{{G u o}^{2}}$  , Duyu Tang 3 , Nan Duan 3 , Xiaocheng Feng 1 Ming   $\\mathbf{G o n g^{4}}$  , Linjun Shou 4 , Bing  $\\mathbf{Q}\\mathbf{in}^{1}$  , Ting   $\\mathbf{L}\\mathbf{i}\\mathbf{u}^{1}$  , Daxin Jiang 4 , Ming Zhou 3 \n1  Research Center for Social Computing and Information Retrieval, Harbin Institute of Technology, China 2  The School of Data and Computer Science, Sun Yat-sen University, China 3  Microsoft Research Asia, Beijing, China \n{ zyfeng,xcfeng,qinb,tliu } @ir.hit.edu.cn guody5@mail2.sysu.edu.cn dutang,nanduan,migon,lisho,djiang,mingzhou @microsoft.com \nAbstract \nWe present CodeBERT, a  bimodal  pre-trained model for programming language (PL) and natural language (NL). CodeBERT learns general-purpose representations that support downstream NL-PL applications such as nat- ural language code search, code documen- tation generation, etc. We develop Code- BERT with Transformer-based neural architec- ture, and train it with a hybrid objective func- tion that incorporates the pre-training task of replaced token detection, which is to detect plausible alternatives sampled from generators. This enables us to utilize both “ bimodal ” data of NL-PL pairs and “ unimodal ” data, where the former provides input tokens for model training while the latter helps to learn bet- ter generators. We evaluate CodeBERT on two NL-PL applications by ﬁne-tuning model parameters. Results show that CodeBERT achieves state-of-the-art performance on both natural language code search and code docu- mentation generation. Furthermore, to inves- tigate what type of knowledge is learned in CodeBERT, we construct a dataset for NL-PL probing, and evaluate in a zero-shot setting where parameters of pre-trained models are ﬁxed. Results show that CodeBERT performs better than previous pre-trained models on NL- PL probing. \n1 Introduction \nLarge pre-trained models such as ELMo ( Peters et al. ,  2018 ), GPT ( Radford et al. ,  2018 ), BERT ( Devlin et al. ,  2018 ), XLNet ( Yang et al. ,  2019 ) \nand RoBERTa ( Liu et al. ,  2019 ) have dramati- cally improved the state-of-the-art on a variety of natural language processing (NLP) tasks. These pre-trained models learn effective contextual repre- sentations from massive unlabeled text optimized by self-supervised objectives, such as masked language modeling, which predicts the original masked word from an artiﬁcially masked input sequence. The success of pre-trained models in NLP also drives a surge of multi-modal pre-trained models, such as ViLBERT ( Lu et al. ,  2019 ) for language-image and VideoBERT ( Sun et al. ,  2019 ) for language-video, which are learned from  bi- modal  data such as language-image pairs with  bi- modal  self-supervised objectives. \nIn this work, we present CodeBERT, a  bimodal pre-trained model for natural language (NL) and programming language (PL) like Python, Java, JavaScript, etc. CodeBERT captures the seman- tic connection between natural language and pro- gramming language, and produces general-purpose representations that can broadly support NL-PL understanding tasks (e.g. natural language code search) and generation tasks (e.g. code documen- tation generation). It is  developed  with the multi- layer Transformer ( Vaswani et al. ,  2017 ), which is adopted in a majority of large pre-trained models. In order to make use of both  bimodal  instances of NL-PL pairs and large amount of available  uni- modal  codes, we train CodeBERT with a hybrid objective function, including standard masked lan- guage modeling ( Devlin et al. ,  2018 ) and replaced token detection ( Clark et al. ,  2020 ), where  uni- modal  codes help to learn better generators for producing better alternative tokens for the latter objective. \nWe train CodeBERT from Github code reposito- ries in 6 programming languages, where  bimodal datapoints are codes that pair with function-level natural language documentations ( Husain et al. , 2019 ). Training is conducted in a setting similar to that of multilingual BERT ( Pires et al. ,  2019 ), in which case one pre-trained model is learned for 6 programming languages with no explicit mark- ers used to denote the input programming lan- guage. We evaluate CodeBERT on two down- stream NL-PL tasks, including natural language code search and code documentation generation. Results show that ﬁne-tuning the parameters of CodeBERT achieves state-of-the-art performance on both tasks. To further investigate what type of knowledge is learned in CodeBERT, we construct a dataset for NL-PL probing, and test CodeBERT in a zero-shot scenario, i.e. without ﬁne-tuning the parameters of CodeBERT. We ﬁnd that CodeBERT consistently outperforms RoBERTa, a purely natu- ral language-based pre-trained model. The contri- butions of this work are as follows: "}
{"page": 1, "image_path": "doc_images/2020.findings-emnlp.139_1.jpg", "ocr_text": "ries in 6 programming languages, where bimodal\ndatapoints are codes that pair with function-level\nnatural language documentations (Husain et al.,\n2019). Training is conducted in a setting similar\nto that of multilingual BERT (Pires et al., 2019),\nin which case one pre-trained model is learned for\n6 programming languages with no explicit mark-\ners used to denote the input programming lan-\nguage. We evaluate CodeBERT on two down-\nstream NL-PL tasks, including natural language\ncode search and code documentation generation.\nResults show that fine-tuning the parameters of\nCodeBERT achieves state-of-the-art performance\non both tasks. To further investigate what type of\nknowledge is learned in CodeBERT, we construct\na dataset for NL-PL probing, and test CodeBERT\nin a zero-shot scenario, i.e. without fine-tuning the\nparameters of CodeBERT. We find that CodeBERT\nconsistently outperforms RoBERTa, a purely natu-\nral language-based pre-trained model. The contri-\nbutions of this work are as follows:\n\n* CodeBERT is the first large NL-PL pre-\ntrained model for multiple programming lan-\nguages.\n\n¢ Empirical results show that CodeBERT is ef-\nfective in both code search and code-to-text\ngeneration tasks.\n\n¢ We further created a dataset which is the first\none to investigate the probing ability of the\ncode-based pre-trained models.\n\n2 Background\n\n2.1 Pre-Trained Models in NLP\n\nLarge pre-trained models (Peters et al., 2018; Rad-\nford et al., 2018; Devlin et al., 2018; Yang et al.,\n2019; Liu et al., 2019; Raffel et al., 2019) have\nbrought dramatic empirical improvements on al-\nmost every NLP task in the past few years. Suc-\ncessful approaches train deep neural networks on\nlarge-scale plain texts with self-supervised learning\nobjectives. One of the most representative neural\narchitectures is the Transformer (Vaswani et al.,\n2017), which is also the one used in this work. It\ncontains multiple self-attention layers, and can be\nconventionally learned with gradient decent in an\nend-to-end manner as every component is differen-\ntiable. The terminology “self-supervised” means\nthat supervisions used for pre-training are auto-\nmatically collected from raw data without manual\n\nannotation. Dominant learning objectives are lan-\nguage modeling and its variations. For example,\nin GPT (Radford et al., 2018), the learning objec-\ntive is language modeling, namely predicting the\nnext word w;, given the preceding context words\n{wy}, we, ..., Wei}. As the ultimate goal of pre-\ntraining is not to train a good language model, it is\ndesirable to consider both preceding and following\ncontexts to learn better general-purpose contextual\nrepresentations. This leads us to the masked lan-\nguage modeling objective used in BERT (Devlin\net al., 2018), which learns to predict the masked\nwords of a randomly masked word sequence given\nsurrounding contexts. Masked language modeling\nis also used as one of the two learning objectives\nfor training CodeBERT.\n\n2.2 Multi-Modal Pre-Trained Models\n\nThe remarkable success of the pre-trained model\nin NLP has driven the development of multi-modal\npre-trained model that learns implicit alignment\nbetween inputs of different modalities. These mod-\nels are typically learned from bimodal data, such\nas pairs of language-image or pairs of language-\nvideo. For example, ViLBERT (Lu et al., 2019)\nlearns from image caption data, where the model\nlearns by reconstructing categories of masked im-\nage region or masked words given the observed\ninputs, and meanwhile predicting whether the cap-\ntion describes the image content or not. Simi-\nlarly, VideoBERT (Sun et al., 2019) learns from\nlanguage-video data and is trained by video and\ntext masked token prediction. Our work belongs\nto this line of research as we regard NL and PL\nas different modalities. Our method differs from\nprevious works in that the fuels for model train-\ning include not only bimodal data of NL-PL pairs,\nbut larger amounts of unimodal data such as codes\nwithout paired documentations.\n\nA concurrent work (Kanade et al., 2019) uses\nmasked language modeling and next sentence pre-\ndiction as the objective to train a BERT model on\nPython source codes, where a sentence is a log-\nical code line as defined by the Python standard.\nIn terms of the pre-training process, CodeBERT\ndiffers from their work in that (1) CodeBERT is\ntrained in a cross-modal style and leverages both\nbimodal NL-PL data and unimodal PL/NL data, (2)\nCodeBERT is pre-trained over six programming\nlanguages, and (3) CodeBERT is trained with a\nnew learning objective based on replaced token\n\n1537\n", "vlm_text": "\n•  CodeBERT is the ﬁrst large NL-PL pre- trained model for multiple programming lan- guages. •  Empirical results show that CodeBERT is ef- fective in both code search and code-to-text generation tasks. •  We further created a dataset which is the ﬁrst one to investigate the probing ability of the code-based pre-trained models. \n2 Background \n2.1 Pre-Trained Models in NLP \nLarge pre-trained models ( Peters et al. ,  2018 ;  Rad- ford et al. ,  2018 ;  Devlin et al. ,  2018 ;  Yang et al. , 2019 ;  Liu et al. ,  2019 ;  Raffel et al. ,  2019 ) have brought dramatic empirical improvements on al- most every NLP task in the past few years. Suc- cessful approaches train deep neural networks on large-scale plain texts with self-supervised learning objectives. One of the most representative neural architectures is the Transformer ( Vaswani et al. , 2017 ), which is also the one used in this work. It contains multiple self-attention layers, and can be conventionally learned with gradient decent in an end-to-end manner as every component is differen- tiable. The terminology “self-supervised” means that supervisions used for pre-training are auto- matically collected from raw data without manual annotation. Dominant learning objectives are lan- guage modeling and its variations. For example, in  GPT  ( Radford et al. ,  2018 ), the learning objec- tive is language modeling, namely predicting the next word    $w_{k}$   given the preceding context words  $\\{w_{1},w_{2},...,w_{k-1}\\}$  . As the ultimate goal of pre- training is not to train a good language model, it is desirable to consider both preceding and following contexts to learn better general-purpose contextual representations. This leads us to the masked lan- guage modeling objective used in BERT ( Devlin et al. ,  2018 ), which learns to predict the masked words of a randomly masked word sequence given surrounding contexts. Masked language modeling is also used as one of the two learning objectives for training CodeBERT. \n\n2.2 Multi-Modal Pre-Trained Models \nThe remarkable success of the pre-trained model in NLP has driven the development of multi-modal pre-trained model that learns implicit alignment between inputs of different modalities. These mod- els are typically learned from  bimodal  data, such as pairs of language-image or pairs of language- video. For example, ViLBERT ( Lu et al. ,  2019 ) learns from image caption data, where the model learns by reconstructing categories of masked im- age region or masked words given the observed inputs, and meanwhile predicting whether the cap- tion describes the image content or not. Simi- larly, VideoBERT ( Sun et al. ,  2019 ) learns from language-video data and is trained by video and text masked token prediction. Our work belongs to this line of research as we regard NL and PL as different modalities. Our method differs from previous works in that the fuels for model train- ing include not only  bimodal  data of NL-PL pairs, but larger amounts of  unimodal  data such as codes without paired documentations. \nA concurrent work ( Kanade et al. ,  2019 ) uses masked language modeling and next sentence pre- diction as the objective to train a BERT model on Python source codes, where a sentence is a log- ical code line as deﬁned by the Python standard. In terms of the pre-training process, CodeBERT differs from their work in that (1) CodeBERT is trained in a cross-modal style and leverages both bimodal NL-PL data and unimodal PL/NL data, (2) CodeBERT is pre-trained over six programming languages, and (3) CodeBERT is trained with a new learning objective based on replaced token "}
{"page": 2, "image_path": "doc_images/2020.findings-emnlp.139_2.jpg", "ocr_text": "detection.\n\n3  CodeBERT\n\nWe describe the details about CodeBERT in this\nsection, including the model architecture, the input\nand output representations, the objectives and data\nused for training CodeBERT, and how to fine-tune\nCodeBERT when it is applied to downstream tasks.\n\n3.1 Model Architecture\n\nWe follow BERT (Devlin et al., 2018) and\nRoBERTa (Liu et al., 2019), and use multi-layer\nbidirectional Transformer (Vaswani et al., 2017) as\nthe model architecture of CodeBERT. We will not\nreview the ubiquitous Transformer architecture in\ndetail. We develop CodeBERT by using exactly the\nsame model architecture as ROBERTa-base. The\ntotal number of model parameters is 125M.\n\n3.2. Input/Output Representations\n\nIn the pre-training phase, we set the input as the\nconcatenation of two segments with a special sepa-\nrator token, namely [CLS], wi, wo, ..Wn, [SEP],\nC1, C2, -.-;€m; [EOS]. One segment is natural lan-\nguage text, and another is code from a certain pro-\ngramming language. [C'LS] is a special token in\nfront of the two segments, whose final hidden repre-\nsentation is considered as the aggregated sequence\nrepresentation for classification or ranking. Follow-\ning the standard way of processing text in Trans-\nformer, we regard a natural language text as a se-\nquence of words, and split it as WordPiece (Wu\net al., 2016). We regard a piece of code as a se-\nquence of tokens.\n\nThe output of CodeBERT includes (1) contextual\nvector representation of each token, for both natural\nlanguage and code, and (2) the representation of\n[CLS], which works as the aggregated sequence\nrepresentation.\n\n3.3. Pre-Training Data\n\nWe train CodeBERT with both bimodal data, which\nrefers to parallel data of natural language-code\npairs, and unimodal data, which stands for codes\nwithout paired natural language texts and natural\nlanguage without paired codes.\n\nWe use datapoints from Github repositories,\nwhere each bimodal datapoint is an individual\nfunction with paired documentation, and each uni-\nmodal code is a function without paired documen-\ntation. Specifically, we use a recent large dataset\n\nTRAINING DATA — bimodal DATA unimodal CODES\n\nGo 319,256 726,768\nJAVA 500,754 1,569,889\nJAVASCRIPT 143,252 1,857,835\nPHP 662,907 977,821\nPYTHON 458,219 1,156,085\nRUBY 52,905 164,048\nALL 2,137,293 6,452,446\n\nTable 1: Statistics of the dataset used for training Code-\nBERT.\n\nprovided by Husain et al. (2019), which includes\n2.1M bimodal datapoints and 6.4M unimodal codes\nacross six programming languages (Python, Java,\nJavaScript, PHP, Ruby, and Go). Data statistics is\nshown in Table 1.7\n\nThe data comes from publicly available open-\nsource non-fork GitHub repositories and are fil-\ntered with a set of constraints and rules. For ex-\nample, (1) each project should be used by at least\none other project, (2) each documentation is trun-\ncated to the first paragraph, (3) documentations\nshorter than three tokens are removed, (4) func-\ntions shorter than three lines are removed, and (5)\nfunction names with substring “test” are removed.\nAn example of the data is given in Figure 1 >.\n\nif s[-1].lower() not in u\nraise ValueError(\"invalid format: \" + s)\nreturn int(float(s[:-1]) * units[s[-1].lower()])\n\nFigure 1: An example of the NL-PL pair, where NL is\nthe first paragraph (filled in red) from the documenta-\ntion (dashed line in black) of a function.\n\n3.4 Pre-Training CodeBERT\n\nWe describe the two objectives used for training\nCodeBERT here. The first objective is masked\nlanguage modeling (MLM), which has proven ef-\nfective in literature (Devlin et al., 2018; Liu et al.,\n\nSince we will evaluate on the natural language code\nsearch task, we only use the training data of Husain et al.\n(2019) to train CodeBERT with no access to the dev and test-\ning data.\n\n3The source of the illustrating example comes from\nhttps://github.com/apache/spark/blob/\n618d6bf££71073c8c93501ab7392c3cc579730£0b/\npython/pyspark/rdd.py#L125-L138\n\n1538\n", "vlm_text": "3 CodeBERT \nWe describe the details about CodeBERT in this section, including the model architecture, the input and output representations, the objectives and data used for training CodeBERT, and how to ﬁne-tune CodeBERT when it is applied to downstream tasks. \n3.1 Model Architecture \nWe follow BERT ( Devlin et al. ,  2018 ) and RoBERTa ( Liu et al. ,  2019 ), and use multi-layer bidirectional Transformer ( Vaswani et al. ,  2017 ) as the model architecture of CodeBERT. We will not review the ubiquitous Transformer architecture in detail. We develop CodeBERT by using exactly the same model architecture as RoBERTa-base. The total number of model parameters is 125M. \n3.2 Input/Output Representations \nIn the pre-training phase, we set the input as the concatenation of two segments with a special sepa- rator token, namely    $[C L S],w_{1},w_{2},..w_{n},[S E P].$  ,  $c_{1},c_{2},...,c_{m},[E O S]$  . One segment is natural lan- guage text, and another is code from a certain pro- gramming language.    $[C L S]$   is a special token in front of the two segments, whose ﬁnal hidden repre- sentation is considered as the aggregated sequence representation for classiﬁcation or ranking. Follow- ing the standard way of processing text in Trans- former, we regard a natural language text as a se- quence of words, and split it as WordPiece ( Wu et al. ,  2016 ). We regard a piece of code as a se- quence of tokens. \nThe output of CodeBERT includes (1) contextual vector representation of each token, for both natural language and code, and (2) the representation of  $[C L S]$  , which works as the aggregated sequence representation. \n3.3 Pre-Training Data \nWe train CodeBERT with both  bimodal  data, which refers to parallel data of natural language-code pairs, and  unimodal  data, which stands for codes without paired natural language texts and natural language without paired codes. \nWe use datapoints from Github repositories, where each  bimodal  datapoint is an individual function with paired documentation, and each  uni- modal  code is a function without paired documen- tation. Speciﬁcally, we use a recent large dataset \nThe table presents information about training data split into two categories: bimodal data and unimodal codes, for various programming languages including Go, Java, JavaScript, PHP, Python, and Ruby. \n\n- For Go, the table lists 319,256 bimodal data points and 726,768 unimodal codes.\n- For Java, there are 500,754 bimodal data points and 1,569,889 unimodal codes.\n- For JavaScript, 143,252 bimodal data points and 1,857,835 unimodal codes are shown.\n- For PHP, the table lists 662,907 bimodal data points and 977,821 unimodal codes.\n- For Python, there are 458,219 bimodal data points and 1,156,085 unimodal codes.\n- For Ruby, the numbers are 52,905 bimodal data points and 164,048 unimodal codes.\n\nIn total (labeled as All), there are 2,137,293 bimodal data points and 6,452,446 unimodal codes across all these languages.\nprovided by  Husain et al.  ( 2019 ), which includes 2.1M  bimodal  datapoints and 6.4M  unimodal  codes across six programming languages (Python, Java, JavaScript, PHP, Ruby, and Go). Data statistics is shown in Table  1 . \nThe data comes from publicly available open- source non-fork GitHub repositories and are ﬁl- tered with a set of constraints and rules. For ex- ample, (1) each project should be used by at least one other project, (2) each documentation is trun- cated to the ﬁrst paragraph, (3) documentations shorter than three tokens are removed, (4) func- tions shorter than three lines are removed, and (5) function names with substring “ test ” are removed. An example of the data is given in Figure    $1\\,\\,{^3}$  . \nThe image shows a snippet of Python code inside a documentation block for a function named `_parse_memory`. The code is intended to parse a memory string in the format supported by Java (e.g., \"1g\", \"200m\") and return its value in mebibytes (MiB). The documentation paragraph is highlighted with a red-filled box, which says: \"Parse a memory string in the format supported by Java (e.g., 1g, 200m) and return the value in MiB.\"\n\nThe function converts input strings like \"256m\" and \"2g\" into their equivalent memory values in MiB, returning 256 and 2048, respectively, as shown in the example usage code below the documentation paragraph. The implementation uses a dictionary `units` to map suffixes ('g', 'm', 't', 'k') to their respective conversion factors with 'g' being 1024. It checks if the last character of the input (`s[-1]`) is in the dictionary keys, raises a `ValueError` for invalid formats, and calculates the memory in MiB using the conversion factor.\n\nThe overall layout of this image illustrates how documentation (natural language, NL) is paired with the corresponding code implementation (programming language, PL) for clarity.\n3.4 Pre-Training CodeBERT \nWe describe the two objectives used for training CodeBERT here. The ﬁrst objective is masked language modeling (MLM), which has proven ef- fective in literature ( Devlin et al. ,  2018 ;  Liu et al. , "}
{"page": 3, "image_path": "doc_images/2020.findings-emnlp.139_3.jpg", "ocr_text": "w, ——> [MASK], ——>\nw2 ——— wz\n\nw3; ———> w3 NL Generator\nWe — WwW\n\nWs ——> [MASK]y ——>\n\nCy > 1\nC, ——> [MASK], ——>\nC3 > C3\nCode Generator\nC4 > C4\ncs ——~ Cs\n\nC4 ——> [MASK], ——>\n\n—— replaced\n— original\n— original\n— original\n\n—— original\n\nNL-Code\n\nDiscriminator\n—— original\n\n—— replaced\n—— original\n—— original\noriginal\n\n—— replaced\n\nFigure 2: An illustration about the replaced token detection objective. Both NL and code generators are language\nmodels, which generate plausible tokens for masked positions based on surrounding contexts. NL-Code discrimi-\nnator is the targeted pre-trained model, which is trained via detecting plausible alternatives tokens sampled from\nNL and PL generators. NL-Code discriminator is used for producing general-purpose representations in the fine-\ntuning step. Both NL and code generators are thrown out in the fine-tuning step.\n\n2019; Sun et al., 2019). We apply masked language\nmodeling on bimodal data of NL-PL pairs. The sec-\nond objective is replaced token detection (RTD),\nwhich further uses a large amount of unimodal data,\nsuch as codes without paired natural language texts.\nDetailed hyper-parameters for model pre-training\nare given in Appendix B.1.\n\nObjective #1: Masked Language Modeling\n(MLM) _ Given a datapoint of NL-PL pair (a =\n{w, c}) as input, where w is a sequence of NL\nwords and c is a sequence of PL tokens, we first\nselect a random set of positions for both NL and PL\nto mask out (i.e. m™ and m°, respectively), and\nthen replace the selected positions with a special\n[MASK] token. Following Devlin et al. (2018),\n15% of the tokens from a are masked out.\n\nmy ~ unif{1, |w]|} for i = 1 to |w| qd)\n\nm§ ~ unif{1, |e|} for i = 1 to |e| (2)\nwmmsked — REPLACE(w,m™”, [MASK]) (3)\nc™sked — REPLACE(c,m°,[MASK]) (4)\nx=wte (5)\n\nThe MLM objective is to predict the original to-\nkens which are masked out, formulated as follows,\n\nwhere p”! is the discriminator which predicts a\ntoken from a large vocabulary.\n\nLum (9) = S- —log p?! (a; pomasked masked)\n\n1Emv UUme\n(6)\n\nObjective #2: Replaced Token Detection (RTD)\nIn the MLM objective, only bimodal data (i.e. data-\npoints of NL-PL pairs) is used for training. Here we\npresent the objective of replaced token detection.\nThe RTD objective (Clark et al., 2020) is origi-\nnally developed for efficiently learning pre-trained\nmodel for natural language. We adapt it in our sce-\nnario, with the advantage of using both bimodal\nand unimodal data for training. Specifically, there\nare two data generators here, an NL generator pC\nand a PL generator p@, both for generating plau-\nsible alternatives for the set of randomly masked\n\npositions.\nSw (w;;wmke4) fori em” (7)\n\nmasked) for i € m° (8)\n\nWw, ~ p\n\né, ~ p@* (eile\n\nwemrt — REPLACE(w,m”,w) (9)\ncoorupt REPLACE(c, m*, é) (10)\n\ngp comtupt _ ycorupt + ceomupt (11)\n\nThe discriminator is trained to determine whether\na word is the original one or not, which is a binary\nclassification problem. It is worth noting that the\nRTD objective is applied to every position in the\ninput, and it differs from GAN (generative adver-\nsarial network) in that if a generator happens to\nproduce the correct token, the label of that token\nis “real” instead of “fake” (Clark et al., 2020). The\nloss function of RTD with regard to the discrimina-\ntor parameterized by @ is given below, where 6(7) is\n\n1539\n", "vlm_text": "The image depicts a schematic illustrating the replaced token detection objective, highlighting the roles of natural language (NL) and code (PL) generators, as well as a discriminator. The process is as follows:\n\n1. **Masked Token Inputs:** \n   - Natural language (NL) inputs (`w_1, w_2, ..., w_5`) and code inputs (`c_1, c_2, ..., c_6`) have certain tokens masked (`[MASK]_w`, `[MASK]_c`).\n\n2. **Token Generation:**\n   - The NL Generator tries to fill in the masked tokens in the natural language inputs. For example, `[MASK]_w` gets replaced with tokens like `w_51` and another `[MASK]_w` might be replaced with `w_5`.\n   - Similarly, the Code Generator fills masked positions within the code sequence, replacing `[MASK]_c` with tokens like `c_29` or `c_162`.\n\n3. **Discriminator:**\n   - Both the generated sequences from the NL and code generators are passed to the NL-Code Discriminator.\n   - This discriminator is trained to identify whether the tokens at the masked positions in both NL and code sequences are \"replaced\" or \"original.\"\n\n4. **Output Labels:**\n   - The discriminator outputs labels indicating whether a token at the masked position is originally from the sequence or replaced. This is shown on the right side of the image with labels such as \"replaced\" or \"original.\"\n\n5. **Training and Fine-Tuning:** \n   - The focus is on training the NL-Code discriminator to produce general-purpose representations by detecting tokens from the NL and code generators.\n   - In the fine-tuning step, the NL and code generators are discarded.\n\nThis figure emphasizes the model's training process to detect modifications within sequences, enhancing the model's ability to create robust representations.\n2019 ;  Sun et al. ,  2019 ). We apply masked language modeling on  bimodal  data of NL-PL pairs. The sec- ond objective is replaced token detection (RTD), which further uses a large amount of  unimodal  data, such as codes without paired natural language texts. Detailed hyper-parameters for model pre-training are given in Appendix B.1. \nObjective #1: Masked Language Modeling\n\n (MLM) Given a datapoint of NL-PL pair   $({\\pmb x}=\n\n$   $\\{\\pmb{w},\\,\\pmb{c}\\})$   as nput, where    $\\mathbfit{w}$   is a sequence of  $\\mathrm{NL}$  words and  c  is a sequence of PL tokens, we ﬁrst select a random set of positions for both  $\\mathrm{NL}$   and PL to mask out (i.e.    $_m w$    and  $_m\\!^{c}$  , respectively), and then replace the selected positions with a special  $[M A S K]$   token. Following  Devlin et al.  ( 2018 ),  $15\\%$   of the tokens from    $_{_{\\pmb{x}}}$   are masked out. \n\n$$\n\\begin{array}{r}{m_{i}^{w}\\sim\\mathrm{unif}\\{1,|w|\\}\\;\\mathrm{for}\\;i=1\\;\\mathrm{to}\\;|w|\\;\\;\\;\\;\\;\\;\\;\\;}\\\\ {m_{i}^{c}\\sim\\mathrm{unif}\\{1,|c|\\}\\;\\mathrm{for}\\;i=1\\;\\mathrm{to}\\;|c|\\;\\;\\;\\;\\;\\;\\;}\\\\ {{w}^{\\mathrm{smoke}}=\\mathrm{REPLACE}(w,m^{w},[M A S K])\\;\\;}\\\\ {c^{\\mathrm{smoke}}=\\mathrm{REPLACE}(c,m^{c},[M A S K])\\;\\;\\;\\;\\;\\;\\;}\\\\ {{x}=w+c\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;}\\end{array}\n$$\n \nThe MLM objective is to predict the original to- kens which are masked out, formulated as follows, where    $p^{D_{1}}$    is the discriminator which predicts a token from a large vocabulary. \n\n$$\n\\mathcal{L}_{\\mathrm{MLM}}(\\theta)\\!=\\!\\sum_{i\\in m^{w}\\cup m^{c}}\\!-\\!\\log p^{D_{1}}(x_{i}|\\pmb{w}^{\\mathrm{smoke}},\\!c^{\\mathrm{mascade}})\n$$\n \nObjective #2: Replaced Token Detection (RTD) In the MLM objective, only  bimodal  data (i.e. data- points of NL-PL pairs) is used for training. Here we present the objective of replaced token detection. The RTD objective ( Clark et al. ,  2020 ) is origi- nally developed for efﬁciently learning pre-trained model for natural language. We adapt it in our sce- nario, with the advantage of using both  bimodal and  unimodal  data for training. Speciﬁcally, there are two data generators here, an  $\\mathrm{NL}$   generator    $\\boldsymbol{p}^{G_{w}}$  and a PL generator    $p^{G_{c}}$  , both for generating plau- sible alternatives for the set of randomly masked positions. \n\n$$\n\\begin{array}{r}{\\hat{w}_{i}\\sim p^{G_{w}}(w_{i}|\\pmb{w}^{\\mathrm{unshock}})\\mathrm{~for~}i\\in\\pmb{m}^{w}}\\\\ {\\hat{c}_{i}\\sim p^{G_{c}}(c_{i}|\\pmb{c}^{\\mathrm{unshock}})\\mathrm{~for~}i\\in\\pmb{m}^{c}}\\end{array}\n$$\n \n\n$$\n\\begin{array}{r l}&{\\pmb{w}^{\\mathrm{corrupt}}=\\mathrm{REPLACE}(\\pmb{w},\\pmb{m}^{\\pmb{w}},\\hat{\\pmb{w}})}\\\\ &{\\pmb{c}^{\\mathrm{corrupt}}=\\mathrm{REPLACE}(\\pmb{c},\\pmb{m}^{c},\\hat{\\pmb{c}})}\\\\ &{\\pmb{x}^{\\mathrm{corrupt}}=\\pmb{w}^{\\mathrm{corrupt}}+\\pmb{c}^{\\mathrm{corrupt}}}\\end{array}\n$$\n \nThe discriminator is trained to determine whether a word is the original one or not, which is a binary classiﬁcation problem. It is worth noting that the RTD objective is applied to every position in the input, and it differs from GAN (generative adver- sarial network) in that if a generator happens to produce the correct token, the label of that token is “real” instead of “fake” ( Clark et al. ,  2020 ). The loss function of RTD with regard to the discrimina- tor parameterized by  $\\theta$   is given below, where    $\\delta(i)$   is an indicator function and  $p^{D_{2}}$    is the discriminator that predicts the probability of the  $i$  -th word being original. "}
{"page": 4, "image_path": "doc_images/2020.findings-emnlp.139_4.jpg", "ocr_text": "an indicator function and p”? is the discriminator\nthat predicts the probability of the i-th word being\noriginal.\n\n|w|+le|\n\nLerp(8) = >\n\ni=1\n\n(1 - o(i)) (1 — Jog p22 (aso, ))\n\n(12)\n\nThere are many different ways to implement the\ngenerators. In this work, we implement two ef-\nficient n-gram language models (Jurafsky, 2000)\nwith bidirectional contexts, one for NL and one\nfor PL, and learn them from corresponding uni-\nmodel datapoints, respectively. The approach is\neasily generalized to learn bimodal generators or\nuse more complicated generators like Transformer-\nbased neural architecture learned in a joint manner.\nWe leave these to future work. The PL training data\nis the unimodal codes as shown in Table 1, and the\nNL training data comes from the documentations\nfrom bimodal data. One could easily extend these\ntwo training datasets to larger amount. The final\nloss function are given below.\n\n(i108 pP2 (grt 5) 4\n\n+e corrupt\nif x; = X%. (13)\notherwise.\n\nmin Lum (9) + L£erp(9) (14)\n\n3.5 Fine-Tuning CodeBERT\n\nWe have different settings to use CodeBERT in\ndownstream NL-PL tasks. For example, in natural\nlanguage code search, we feed the input as the\nsame way as the pre-training phase and use the\nrepresentation of [C'L.S] to measure the semantic\nrelevance between code and natural language query,\nwhile in code-to-text generation, we use an encoder-\ndecoder framework and initialize the encoder of\na generative model with CodeBERT. Details are\ngiven in the experiment section.\n\n4 Experiment\n\nWe present empirical results in this section to verify\nthe effectiveness of CodeBERT. We first describe\nthe use of CodeBERT in natural language code\nsearch (§4.1), in a way that model parameters of\nCodeBERT are fine-tuned. After that, we present\nthe NL-PL probing task (§4.2), and evaluate Code-\nBERT in a zero-shot setting where the parameters\n\nof CodeBERT are fixed. Finally, we evaluate Code-\nBERT on a generation problem, i.e. code documen-\ntation generation (§4.3), and further evaluate on a\nprogramming language which is never seen in the\ntraining phase (§4.4).\n\n4.1 Natural Language Code Search\n\nGiven a natural language as the input, the objec-\ntive of code search is to find the most semantically\nrelated code from a collection of codes. We con-\nduct experiments on the CodeSearchNet corpus\n(Husain et al., 2019) +. We follow the official evalu-\nation metric to calculate the Mean Reciprocal Rank\n(MRR) for each pair of test data (c, w) over a fixed\nset of 999 distractor codes. We further calculate the\nmacro-average MRR for all languages as an overall\nevaluation metric. It is helpful to note that this met-\nric differs from the AVG metric in the original pa-\nper, where the answer is retrieved from candidates\nfrom all six languages. We fine-tune a language-\nspecific model for each programming language.\nWe train each model with a binary classification\nloss function, where a softmaz layer is connected\nto the representation of [CLS]. Both training and\nvalidation datasets are created in a way that posi-\ntive and negative samples are balanced. Negative\nsamples consist of balanced number of instances\nwith randomly replaced NL (i.e. (c, w)) and PL\n(ie. (é, w)). Detailed hyper-parameters for model\nfine-tuning are given in Appendix B.2.\n\nModel Comparisons Table 2 shows the results\nof different approaches on the CodeSearchNet cor-\npus. The first four rows are reported by Husain\net al. (2019), which are joint embeddings of NL and\nPL (Gu et al., 2018; Mitra et al., 2018). NBOW\nrepresents neural bag-of-words. CNN, BIRNN\nand SELFATT stand for 1D convolultional neu-\nral network (Kim, 2014), bidirectional GRU-based\nrecurrent neural network (Cho et al., 2014), and\nmulti-head attention (Vaswani et al., 2017), respec-\ntively.\n\nWe report the remaining numbers in Table 2.\nWe train all these pre-trained models by regarding\ncodes as a sequence of tokens. We also continu-\nously train RoBERTa only on codes from Code-\nSearchNet with masked language modeling. Re-\nsults show that CodeBERT consistently performs\n\n‘More details about the dataset are given in Appendix A.\n\n>We have fine-tuned a multi-lingual model for six program-\nming languages, but find that it performs worse that fine-tuning\na language-specific model for each programming language.\n\n1540\n", "vlm_text": "\n\n$$\n\\begin{array}{r l}&{\\mathcal{L}_{\\mathrm{RTD}}(\\theta)=\\displaystyle\\sum_{i=1}^{|w|+|c|}\\bigg(\\delta(i){\\log p^{D_{2}}}({\\mathbf{x}}^{\\mathrm{corrupt}},i)+}\\\\ &{\\qquad\\qquad\\Big(1-\\delta(i)\\Big)\\Big(1-\\log p^{D_{2}}({\\mathbf{x}}^{\\mathrm{corrupt}},i)\\Big)\\bigg)}\\end{array}\n$$\n \n\n$$\n\\delta(i)={\\left\\{\\begin{array}{l l}{1,}&{{\\mathrm{if~}}x_{i}^{\\mathrm{corrupt}}=x_{i}.}\\\\ {0,}&{{\\mathrm{otherwise}}.}\\end{array}\\right.}\n$$\n \nThere are many different ways to implement the generators. In this work, we implement two ef- ﬁcient  $\\mathbf{n}$  -gram language models ( Jurafsky ,  2000 ) with bidirectional contexts, one for NL and one for PL, and learn them from corresponding uni- model datapoints, respectively. The approach is easily generalized to learn  bimodal  generators or use more complicated generators like Transformer- based neural architecture learned in a joint manner. We leave these to future work. The PL training data is the  unimodal  codes as shown in Table  1 , and the NL training data comes from the documentations from  bimodal  data. One could easily extend these two training datasets to larger amount. The ﬁnal loss function are given below. \n\n$$\n\\operatorname*{min}_{\\theta}\\mathcal{L}_{\\mathrm{MLM}}(\\theta)+\\mathcal{L}_{\\mathrm{RTD}}(\\theta)\n$$\n \n3.5 Fine-Tuning CodeBERT \nWe have different settings to use CodeBERT in downstream NL-PL tasks. For example, in natural language code search, we feed the input as the same way as the pre-training phase and use the representation of    $[C L S]$   to measure the semantic relevance between code and natural language query, while in code-to-text generation, we use an encoder- decoder framework and initialize the encoder of a generative model with CodeBERT. Details are given in the experiment section. \n4 Experiment \nWe present empirical results in this section to verify the effectiveness of CodeBERT. We ﬁrst describe the use of CodeBERT in natural language code search ( § 4.1 ), in a way that model parameters of CodeBERT are ﬁne-tuned. After that, we present the NL-PL probing task ( § 4.2 ), and evaluate Code- BERT in a zero-shot setting where the parameters of CodeBERT are ﬁxed. Finally, we evaluate Code- BERT on a generation problem, i.e. code documen- tation generation ( § 4.3 ), and further evaluate on a programming language which is never seen in the training phase (§ 4.4 ). \n\n4.1 Natural Language Code Search \nGiven a natural language as the input, the objec- tive of code search is to ﬁnd the most semantically related code from a collection of codes. We con- duct experiments on the CodeSearchNet corpus ( Husain et al. ,  2019 )   4 . We follow the ofﬁcial evalu- ation metric to calculate the Mean Reciprocal Rank (MRR) for each pair of test data   $(c,w)$   over a ﬁxed set of 999 distractor codes. We further calculate the macro-average MRR for all languages as an overall evaluation metric. It is helpful to note that this met- ric differs from the  AVG  metric in the original pa- per, where the answer is retrieved from candidates from all six languages. We ﬁne-tune a language- speciﬁc model for each programming language 5 . We train each model with a binary classiﬁcation loss function, where a  softmax  layer is connected to the representation of    $[C L S]$  . Both training and validation datasets are created in a way that posi- tive and negative samples are balanced. Negative samples consist of balanced number of instances with randomly replaced NL (i.e.   $(c,\\,\\hat{w}))$  )) and PL (i.e.   $(\\hat{c},w)_{\\perp}$  )). Detailed hyper-parameters for model ﬁne-tuning are given in Appendix B.2. \nModel Comparisons Table  2  shows the results of different approaches on the CodeSearchNet cor- pus. The ﬁrst four rows are reported by  Husain et al.  ( 2019 ), which are joint embeddings of NL and PL ( Gu et al. ,  2018 ;  Mitra et al. ,  2018 ).  NB O W represents neural bag-of-words.  CNN ,  B I RNN and  S ELF ATT  stand for 1D convolultional neu- ral network ( Kim ,  2014 ), bidirectional GRU-based recurrent neural network ( Cho et al. ,  2014 ), and multi-head attention ( Vaswani et al. ,  2017 ), respec- tively. \nWe report the remaining numbers in Table  2 . We train all these pre-trained models by regarding codes as a sequence of tokens. We also continu- ously train RoBERTa only on codes from Code- SearchNet with masked language modeling. Re- sults show that CodeBERT consistently performs "}
{"page": 5, "image_path": "doc_images/2020.findings-emnlp.139_5.jpg", "ocr_text": "MODEL RUBY JAVASCRIPT GO PYTHON JAVA PHP MaA-AvG\nNBow 0.4285 0.4607 0.6409 0.5809 0.5140 0.4835 0.5181\nCNN 0.2450 0.3523 0.6274 0.5708 0.5270 0.5294 0.4753\nBIRNN 0.0835 0.1530 0.4524 0.3213 0.2865 0.2512 0.2580\nSELFATT 0.3651 0.4506 0.6809 0.6922 0.5866 0.6011 0.5628\nROBERTA 0.6245 0.6060 0.8204 0.8087 0.6659 0.6576 0.6972\nPT w/ CODE ONLY (INIT=S) 0.5712 0.5557 0.7929 0.7855 0.6567 0.6172 0.6632\nPT w/ CODE ONLY (INIT=R) 0.6612 0.6402 0.8191 0.8438 0.7213. 0.6706 0.7260\nCODEBERT (MLM, INIT=s) 0.5695 0.6029 0.8304 0.8261 0.7142 0.6556 0.6998\nCODEBERT (MLM, INIT=R) 0.6898 0.6997 0.8383 0.8647 0.7476 0.6893 0.7549\nCODEBERT (RTD, INIT=R) 0.6414 0.6512 0.8285 0.8263 0.7150 0.6774 0.7233\nCODEBERT (MLM+RTD, INIT=R) 0.6926 0.7059 0.8400 0.8685 0.7484 0.7062 0.7603\n\nTable 2: Results on natural language code retrieval. Baselines include four joint embeddings (first group) of NL\nand PL, RoBERTa, and RoBERTa which is continuously trained with masked language modeling on codes only\n(second group). PT stands for pre-training. We train CodeBERT (third group) with different settings, including\nusing different initialization (from scratch (INIT=S) or initialized with the parameters of ROBERTa (INIT=R)) and\nusing different learning objectives (MLM, RTD, or the combination of both).\n\nbetter than RoBERTa and the model pre-trained\nwith code only. CodeBERT (MLM) learned from\nscratch performs better than RoBERTa. Unsur-\nprisingly, initializing CodeBERT with RoBERTa\n\nimproves the performance °.\n\n4.2 NL-PL Probing\n\nIn the previous subsection, we show the empirical\neffectiveness of CodeBERT in a setting that the\nparameters of CodeBERT are fine-tuned in down-\nstream tasks. In this subsection, we further inves-\ntigate what type of knowledge is learned in Code-\nBERT without modifying the parameters.\n\nTask Formulation and Data Construction Fol-\nlowing the probing experiments in NLP (Petroni\net al., 2019; Talmor et al., 2019), we study NL-\nPL probing here. Since there is no existing work\ntowards this goal, we formulate the problem of\nNL-PL probing and create the dataset by ourselves.\nGiven an NL-PL pair (c, w), the goal of NL-PL\nprobing is to test model’s ability to correctly pre-\ndict/recover the masked token of interest (either a\ncode token c; or word token w;) among distractors.\nThere are two major types of distractors: one is the\nwhole target vocabulary used for the masked lan-\nguage modeling objective (Petroni et al., 2019), and\nanother one has fewer candidates which are filter or\ncurated based on experts’ understanding about the\nability to be tested (Talmor et al., 2019). We follow\nthe second direction and formulate NL-PL probing\nas a multi-choice question answering task, where\nthe question is cloze-style in which a certain token\n\n°We further give a learning curve of different pre-trained\nmodels in the fine-tuning process in Appendix C.\n\nis replaced by [MASK] and distractor candidate\nanswers are curated based on our expertise.\nSpecifically, we evaluate on the NL side and PL\nside, respectively. To ease the effort of data col-\nlection, we collect data automatically from NL-PL\npairs in both validation and testing sets of Code-\nSearchNet, both of which are unseen in the pre-\ntraining phase. To evaluate on the NL side, we\nselect NL-PL pairs whose NL documentations in-\nclude one of the six keywords (max, maximize, min,\nminimize, less, greater), and group them to four\ncandidates by merging first two keywords and the\nmiddle two keywords. The task is to ask pre-trained\nmodels to select the correct one instead of three\nother distractors. That is to say, the input in this\nsetting includes the complete code and a masked\nNL documentation. The goal is to select the correct\nanswer from four candidates. For the PL side, we\nselect codes containing keywords max and min, and\nformulate the task as a two-choice answer selection\nproblem. Here, the input includes complete NL\ndocumentation and a masked PL code, and the goal\nis to select the correct answer from two candidates.\nSince code completion is an important scenario,\nwe would like to test model’s ability in predicting\nthe correct token merely based on preceding PL\ncontexts. Therefore, we add an additional setting\nfor PL side, where the input includes the complete\nNL documentation and preceding PL codes. Data\nstatistics is given in the top two rows in Table 3.\n\nModel Comparisons Results are given in Table\n3. We report accuracy, namely the number of cor-\nrectly predicted instances over the number of all\ninstances, for each programming language. Since\n\n1541\n", "vlm_text": "This table presents performance metrics of different models across various programming languages including Ruby, JavaScript, Go, Python, Java, and PHP. The models compared are NBow, CNN, BiRNN, SelfAtt, RoBERTa, PT with Code Only (two initialization strategies), and various configurations of CodeBERT. \n\nEach column (except for the model column and MA-Avg) represents the performance score of these models on a specific language. The MA-Avg column shows the mean average performance score of each model across all languages.\n\nThe best scores for each language are highlighted in bold, with \"CodeBERT (MLM+RTD, init=R)\" generally showing the highest performance across most languages and the mean average.\nbetter than RoBERTa and the model pre-trained with code only. CodeBERT (MLM) learned from scratch performs better than RoBERTa. Unsur- prisingly, initializing CodeBERT with RoBERTa improves the performance   6 . \n4.2 NL-PL Probing \nIn the previous subsection, we show the empirical effectiveness of CodeBERT in a setting that the parameters of CodeBERT are ﬁne-tuned in down- stream tasks. In this subsection, we further inves- tigate what type of knowledge is learned in Code- BERT without modifying the parameters. \nTask Formulation and Data Construction Fol- lowing the probing experiments in NLP ( Petroni et al. ,  2019 ;  Talmor et al. ,  2019 ), we study NL- PL probing here. Since there is no existing work towards this goal, we formulate the problem of NL-PL probing and create the dataset by ourselves. Given an NL-PL pair   $(c,\\,\\pmb{w})$  , the goal of NL-PL probing is to test model’s ability to correctly pre- dict/recover the masked token of interest (either a code token    $c_{i}$   or word token  $w_{j}.$  ) among distractors. There are two major types of distractors: one is the whole target vocabulary used for the masked lan- guage modeling objective ( Petroni et al. ,  2019 ), and another one has fewer candidates which are ﬁlter or curated based on experts’ understanding about the ability to be tested ( Talmor et al. ,  2019 ). We follow the second direction and formulate NL-PL probing as a multi-choice question answering task, where the question is cloze-style in which a certain token is replaced by    $[M A S K]$   and distractor candidate answers are curated based on our expertise. \n\nSpeciﬁcally, we evaluate on the NL side and PL side, respectively. To ease the effort of data col- lection, we collect data automatically from NL-PL pairs in both validation and testing sets of Code- SearchNet, both of which are unseen in the pre- training phase. To evaluate on the NL side, we select NL-PL pairs whose NL documentations in- clude one of the six keywords ( max ,  maximize ,  min , minimize ,  less ,  greater ), and group them to four candidates by merging ﬁrst two keywords and the middle two keywords. The task is to ask pre-trained models to select the correct one instead of three other distractors. That is to say, the input in this setting includes the complete code and a masked NL documentation. The goal is to select the correct answer from four candidates. For the PL side, we select codes containing keywords  max  and  min , and formulate the task as a two-choice answer selection problem. Here, the input includes complete NL documentation and a masked PL code, and the goal is to select the correct answer from two candidates. Since code completion is an important scenario, we would like to test model’s ability in predicting the correct token merely based on preceding PL contexts. Therefore, we add an additional setting for PL side, where the input includes the complete NL documentation and preceding PL codes. Data statistics is given in the top two rows in Table  3 . \nModel Comparisons Results are given in Table 3 . We report accuracy, namely the number of cor- rectly predicted instances over the number of all instances, for each programming language. Since "}
{"page": 6, "image_path": "doc_images/2020.findings-emnlp.139_6.jpg", "ocr_text": "RUBY JAVASCRIPT GO PYTHON JAVA PHP ALL\nNUMBER OF DATAPOINTS FOR PROBING\nPL (2 CHOICES) 38 272 152 1,264 482 407 2,615\nNL (4 CHOICES) 20 65 159 216 323 73 856\nPL PROBING\nROBERTA 73.68 63.97 72.37 59.18 59.96 69.78 62.45\nPRE-TRAIN W/ CODE ONLY 71.05 77.94 89.47 70.41 70.12 82.31 74.11\nCODEBERT (MLM) 86.84 86.40 90.79 82.20 90.46 88.21 85.66\nPL PROBING WITH PRECEDING CONTEXT ONLY\nROBERTA 73.68 53.31 51.32 55.14 42.32 52.58 52.24\nPRE-TRAIN W/ CODE ONLY 63.16 48.53 61.84 56.25 58.51 58.97 56.71\nCODEBERT (MLM) 65.79 50.74 59.21 62.03 54.98 59.95 59.12\nNL PROBING\nROBERTA 50.00 72.31 54.72 61.57 61.61 65.75 61.21\nPRE-TRAIN W/ CODE ONLY 55.00 67.69 60.38 68.06 65.02 68.49 65.19\nCODEBERT (MLM) 65.00 89.23 66.67 76.85 73.37 79.45 74.53\n\nTable 3: Statistics of the data for NL-PL probing and the performance of different pre-trained models. Accuracies\n\n(%) are reported. Best results in each group are in bold.\n\ndatasets in different programming languages are\nextremely unbalanced, we report the accumulated\nmetric with the same way. We use CodeBERT\n(MLM) here because its output layer naturally fits\nfor probing. Results show that CodeBERT per-\nforms better than baselines on almost all languages\non both NL and PL probing. The numbers with\nonly preceding contexts are lower than that with\nbidirectional contexts, which suggests that code\ncompletion is challenging. We leave it as a future\nwork.\n\nWe further give a case study on PL-NL probing.\nWe mask NL token and PL token separately, and\nreport the predicted probabilities of ROBERTa and\nCodeBERT. Figure 3 illustrates the example of a\npython code’. We can see that ROBERTa fails in\nboth cases, whereas CodeBERT makes the correct\nprediction in both NL and PL settings.\n\n4.3 Code Documentation Generation\n\nAlthough the pre-training objective of Code-\nBERT does not include generation-based objectives\n(Lewis et al., 2019), we would like to investigate\nto what extent does CodeBERT perform on gen-\neration tasks. Specifically, we study code-to-NL\ngeneration, and report results for the documenta-\ntion generation task on CodeSearchNet Corpus in\nsix programming languages. Since the generated\ndocumentations are short and higher order n-grams\nmay not overlap, we remedy this problem by using\nsmoothed BLEU score (Lin and Och, 2004).\n\n\"The example comes from https://\ngithub.com/peri-source/peri/blob/\n6 lbeedSdeaaf978ab31led716e8470d86ba639867/\nperi/comp/psfcalc.py#L994-L1002\n\nmasked NL token\n\"Transforms a vector np.arange(-N, M, dx) to np.arange((min\\(|vec/),\n‘max(N,M),dx)]\"\n\ndef vec_to_halfvec(vec):\n\nd=vec{1:] - vec{:-1]\nif ((d/d.mean()).std() > 1e-14) or (d.mean() < 0):\nraise ValueError('vec must be np.arange() in increasing order’)\n\ndx = d.mean() masked PL token\n\nlowest = np.abs(vec).\nhighest = np.abs(vec).max()\nreturn np.arange(lowest, highest + 0.1*dx, dx).astype(vec.dtype)\n\nmax min less greater\nNL Roberta 96.24% | 3.73% 0.02% 0.01%\nCodeBERT (MLM) | 39.38% | 60.60% | 0.02% | 0.0003%\naL Roberta 95.85% | 4.15%\nCodeBERT (MLM) | 0.001% | 99.999%\n\nFigure 3: Case study on python language. Masked to-\nens in NL (in blue) and PL (in yellow) are separately\napplied. Predicted probabilities of ROBERTa and Code-\nBERT are given.\n\nModel Comparisons We compare our model\nwith several baselines, including a RNN-based\nmodel with attention mechanism (Sutskever et al.,\n2014), the Transformer (Vaswani et al., 2017),\nRoBERTa and the model pre-trained on code only.\nTo demonstrate the effectiveness of CodeBERT\non code-to-NL generation tasks, we adopt various\npre-trained models as encoders and keep the hyper-\nparameters consistent. Detailed hyper-parameters\nare given in Appendix B.3.\n\nTable 4 shows the results with different mod-\nels for the code-to-documentation generation task.\nAs we can see, models pre-trained on program-\nming language outperform RoBERTa, which illus-\ntrates that pre-trainning models on programming\n\n1542\n", "vlm_text": "This table presents the results of several experiments evaluating the performance of different models on probing tasks related to programming languages (PL) and natural languages (NL) across different programming languages: Ruby, JavaScript, Go, Python, Java, and PHP.\n\nHere's a breakdown of the table's contents:\n\n1. **Number of Datapoints for Probing:**\n   - **PL (2 choices):** It lists the number of data points for each programming language used in probing tasks that have two choices: Ruby (38), JavaScript (272), Go (152), Python (1,264), Java (482), PHP (407), and a total of (2,615).\n   - **NL (4 choices):** It lists the number of data points for each programming language used in probing tasks that have four choices: Ruby (20), JavaScript (65), Go (159), Python (216), Java (323), PHP (73), and a total of (856).\n\n2. **PL Probing:**\n   - Evaluates the performance of different models on programming language probing tasks.\n   - **Roberta:** Shows performance metrics for Ruby (73.68), JavaScript (63.97), Go (72.37), Python (59.18), Java (59.96), PHP (69.78), with an overall score of (62.45).\n   - **Pre-Train w/ Code Only:** Performance scores are Ruby (71.05), JavaScript (77.94), Go (89.47), Python (70.41), Java (70.12), PHP (82.31), with an overall score of (74.11).\n   - **CodeBERT (MLM):** Performance scores are Ruby (86.84), JavaScript (86.40), Go (90.79), Python (82.20), Java (90.46), PHP (88.21), with an overall score of (85.66).\n\n3. **PL Probing with Preceding Context Only:**\n   - Measures model performance when preceding context is considered.\n   - **Roberta:** Ruby (73.68), JavaScript (53.31), Go (51.32), Python (55.14), Java (42.32), PHP (52.58), overall (52.24).\n   - **Pre-Train w/ Code Only:** Ruby (63.16), JavaScript (48.53), Go (61.84), Python (56.25), Java (58.51), PHP (58.97), overall (56.71).\n   - **CodeBERT (MLM):** Ruby (65.79), JavaScript (50.74), Go (59.21), Python (62.03), Java (54.98), PHP (59.95), overall (59.12).\n\n4. **NL Probing:**\n   - Evaluates the models on natural language tasks.\n\ndatasets in different programming languages are extremely unbalanced, we report the accumulated metric with the same way. We use CodeBERT (MLM) here because its output layer naturally ﬁts for probing. Results show that CodeBERT per- forms better than baselines on almost all languages on both NL and PL probing. The numbers with only preceding contexts are lower than that with bidirectional contexts, which suggests that code completion is challenging. We leave it as a future work. \nWe further give a case study on PL-NL probing. We mask NL token and PL token separately, and report the predicted probabilities of RoBERTa and CodeBERT. Figure  3  illustrates the example of a python code 7 . We can see that RoBERTa fails in both cases, whereas CodeBERT makes the correct prediction in both NL and PL settings. \n4.3 Code Documentation Generation \nAlthough the pre-training objective of Code- BERT does not include generation-based objectives ( Lewis et al. ,  2019 ), we would like to investigate to what extent does CodeBERT perform on gen- eration tasks. Speciﬁcally, we study code-to-NL generation, and report results for the documenta- tion generation task on CodeSearchNet Corpus in six programming languages. Since the generated documentations are short and higher order n-grams may not overlap, we remedy this problem by using smoothed BLEU score ( Lin and Och ,  2004 ). \nThe image is a code snippet from a Python function named `vec_to_halfvec`. The function takes a vector `vec` and transforms `np.arange(-N, M, dx)` to `np.arange(min(|vec|), max(N, M), dx)`. \n\nHighlighted elements in the image include:\n- A masked natural language (NL) token, `min`, found in the caption string above the function definition.\n- The line of code `lowest = np.abs(vec).min()` has a masked programming language (PL) token, `min`.\n\nThe function:\n1. Calculates the difference `d` between consecutive elements of the input vector `vec`.\n2. Checks if the standard deviation of `d/d.mean()` is greater than `1e-14` or if `d.mean()` is less than `0` to raise a `ValueError` if the conditions are met.\n3. Computes `dx` as the mean of `d`.\n4. Determines the `lowest` and `highest` values as the minimum and maximum of the absolute values of `vec`, respectively.\n5. Returns a range using `np.arange(lowest, highest + 0.1*dx, dx)` with the same data type as `vec`.\nThe table compares the performance of two models, Roberta and CodeBERT (MLM), across two different contexts, NL (Natural Language) and PL (Programming Language). It presents data in terms of four metrics: max, min, less, and greater.\n\n### For NL:\n- **Roberta**\n  - Max: 96.24%\n  - Min: 3.73%\n  - Less: 0.02%\n  - Greater: 0.01%\n\n- **CodeBERT (MLM)**\n  - Max: 39.38%\n  - Min: 60.60%\n  - Less: 0.02%\n  - Greater: 0.0003%\n\n### For PL:\n- **Roberta**\n  - Max: 95.85%\n  - Min: 4.15%\n  - Less: -\n  - Greater: -\n\n- **CodeBERT (MLM)**\n  - Max: 0.001%\n  - Min: 99.999%\n  - Less: -\n  - Greater: -\n\nThe table uses two colors to differentiate between NL and PL rows, with the former in blue and the latter in light yellow.\nFigure 3: Case study on python language. Masked to- kens in NL (in blue) and PL (in yellow) are separately applied. Predicted probabilities of RoBERTa and Code- BERT are given. \nModel Comparisons We compare our model with several baselines, including a RNN-based model with attention mechanism ( Sutskever et al. , 2014 ), the Transformer ( Vaswani et al. ,  2017 ), RoBERTa and the model pre-trained on code only. To demonstrate the effectiveness of CodeBERT on code-to-NL generation tasks, we adopt various pre-trained models as encoders and keep the hyper- parameters consistent. Detailed hyper-parameters are given in Appendix B.3. \nTable  4  shows the results with different mod- els for the code-to-documentation generation task. As we can see, models pre-trained on program- ming language outperform RoBERTa, which illus- trates that pre-trainning models on programming "}
{"page": 7, "image_path": "doc_images/2020.findings-emnlp.139_7.jpg", "ocr_text": "MODEL RUBY JAVASCRIPT Go PYTHON JAVA PHP OVERALL\nSEQ2SEQ 9.64 10.21 13.98 15.93 15.09 21.08 14.32\nTRANSFORMER 11.18 11.59 16.38 15.81 16.26 22.12 15.56\nROBERTA 11.17 11.90 17.72 18.14 16.47 24.02 16.57\nPRE-TRAIN W/ CODE ONLY 11.91 13.99 17.78 18.58 17.50 24.34 17.35\nCODEBERT (RTD) 11.42 13.27 17.53 18.29 17.35 24.10 17.00\nCODEBERT (MLM) 11.57 14.41 17.78 18.77 17.38 24.85 17.46\nCODEBERT (RTD+MLM) 12.16 14.90 18.07 19.06 17.65 25.16 17.83\n\nTable 4: Results on Code-to-Documentation generation, evaluated with smoothed BLEU-4 score.\n\nlanguage could improve code-to-NL generation.\nBesides, results in the Table 4 show that CodeBERT\npre-trained with RTD and MLM objectives brings\na gain of 1.3 BLEU score over RoOBERTa overall\nand achieve the state-of-the-art performance’.\n4.4 Generalization to Programming\nLanguages NOT in Pre-training\n\nWe would like to evaluate CodeBERT on the pro-\ngramming language which is never seen in the pre-\ntraining step. To this end, we study the task of gen-\nerating a natural language summary of a C# code\nsnippet. We conduct experiments on the dataset\nof CodeNN (Iyer et al., 2016)°, which consists of\n66,015 pairs of questions and answers automati-\ncally collected from StackOverflow. This dataset\nis challenging since the scale of dataset is orders\nof magnitude smaller than CodeSearchNet Corpus.\nWe evaluate models using smoothed BLEU-4 score\nand use the same evaluation scripts as Iyer et al.\n(2016).\n\nMODEL BLEU\nMOSES (KOEHN ET AL., 2007) 11.57\nIR 13.66\nSUM-NN (RUSH ET AL., 2015) 19.31\n2-LAYER BILSTM 19.78\nTRANSFORMER (VASWANIET AL., 2017) 19.68\nTREELSTM (TAI ET AL., 2015) 20.11\nCODENN (IYER ET AL., 2016) 20.53\nCODE2SEQ (ALON ET AL., 2019) 23.04\nROBERTA 19.81\nPRE-TRAIN W/ CODE ONLY 20.65\nCopEBERT (RTD) 22.14\nCoDEBERT (MLM) 22.32\nCODEBERT (MLM+RTD) 22.36\n\nTable 5: Code-to-NL generation on C# language.\n\nModel Comparisons Table 5 shows that our\nmodel with MLM and RTD pre-training objectives\nachieves 22.36 BLEU score and improves by 2.55\npoints over ROBERTa, which illustrates CodeBERT\n\n’We further give some output examples in Appendix E.\n°https://github.com/sriniiyer/codenn\n\ncould generalize better to other programming lan-\nguage which is never seen in the pre-training step.\nHowever, our model achieve slightly lower results\nthan code2seq (Alon et al., 2019). The main reason\ncould be that code2seq makes use of compositional\npaths in its abstract syntax tree (AST) while Code-\nBERT only takes original code as the input. We\nhave trained a version of CodeBERT by traversing\nthe tree structure of AST following a certain order,\nbut applying that model does not bring improve-\nments on generation tasks. This shows a potential\ndirection to improve CodeBERT by incorporating\nAST.\n\n5 Conclusion\n\nIn this paper, we present CodeBERT, which to the\nbest of our knowledge is the first large bimodal\npre-trained model for natural language and pro-\ngramming language. We train CodeBERT on both\nbimodal and unimodal data, and show that fine-\ntuning CodeBERT achieves state-of-the-art perfor-\nmance on downstream tasks including natural lan-\nguage code search and code-to-documentation gen-\neration. To further investigate the knowledge em-\nbodied in pre-trained models, we formulate the task\nof NL-PL probing and create a dataset for probing.\nWe regard the probing task as a cloze-style answer\nselection problem, and curate distractors for both\nNL and PL parts. Results show that, with model\nparameters fixed, CodeBERT performs better than\nRoBERTa and a continuously trained model using\ncodes only.\n\nThere are many potential directions for further\nresearch on this field. First, one could learn better\ngenerators with bimodal evidence or more compli-\ncated neural architecture to improve the replaced to-\nken detection objective. Second, the loss functions\nof CodeBERT mainly target on NL-PL understand-\ning tasks. Although CodeBERT achieves strong\nBLEU scores on code-to-documentation genera-\ntion, the CodeBERT itself could be further im-\nproved by generation-related learning objectives.\n\n1543\n", "vlm_text": "The table presents a comparison of different models evaluated on specific programming languages, including Ruby, JavaScript, Go, Python, Java, and PHP. The models listed are:\n\n1. SEQ2SEQ\n2. Transformer\n3. RoBERTa\n4. Pre-trained with Code Only\n5. CodeBERT (RTD)\n6. CodeBERT (MLM)\n7. CodeBERT (RTD+MLM)\n\nEach model is evaluated based on its performance across the aforementioned programming languages, and an \"Overall\" score is provided, which likely represents a weighted or averaged performance metric across all languages.\n\nFrom the table, it is observed that CodeBERT (RTD+MLM) generally achieves the highest scores across all columns, indicating superior performance in comparison to the other models for each of the programming languages and overall.\nlanguage could improve code-to-NL generation. Besides, results in the Table  4  show that CodeBERT pre-trained with RTD and MLM objectives brings a gain of 1.3 BLEU score over RoBERTa overall and achieve the state-of-the-art performance 8 . \n4.4 Generalization to Programming Languages NOT in Pre-training \nWe would like to evaluate CodeBERT on the pro- gramming language which is never seen in the pre- training step. To this end, we study the task of gen- erating a natural language summary of a C# code snippet. We conduct experiments on the dataset of CodeNN ( Iyer et al. ,  $2016)^{9}$  , which consists of 66,015 pairs of questions and answers automati- cally collected from StackOverﬂow. This dataset is challenging since the scale of dataset is orders of magnitude smaller than CodeSearchNet Corpus. We evaluate models using smoothed BLEU-4 score and use the same evaluation scripts as  Iyer et al. ( 2016 ). \nThis table compares different models based on their BLEU scores. Here are the details:\n\n- **MOSES** (Koehn et al., 2007) - 11.57\n- **IR** - 13.66\n- **SUM-NN** (Rush et al., 2015) - 19.31\n- **2-layer BiLSTM** - 19.78\n- **Transformer** (Vaswani et al., 2017) - 19.68\n- **TreeLSTM** (Tai et al., 2015) - 20.11\n- **CodeNN** (Iyer et al., 2016) - 20.53\n- **Code2Seq** (Alon et al., 2019) - 23.04\n\nAdditional models and variations:\n\n- **RoBERTa** - 19.81\n- Pre-train w/ code only - 20.65\n- **CodeBERT (RTD)** - 22.14\n- **CodeBERT (MLM)** - 22.32\n- **CodeBERT (MLM+RTD)** - 22.36\n\nThe highest BLEU score is achieved by CodeBERT (MLM+RTD) with a score of 22.36.\nModel Comparisons Table  5  shows that our model with MLM and RTD pre-training objectives achieves 22.36 BLEU score and improves by 2.55 points over RoBERTa, which illustrates CodeBERT could generalize better to other programming lan- guage which is never seen in the pre-training step. However, our model achieve slightly lower results than code2seq ( Alon et al. ,  2019 ). The main reason could be that code2seq makes use of compositional paths in its abstract syntax tree (AST) while Code- BERT only takes original code as the input. We have trained a version of CodeBERT by traversing the tree structure of AST following a certain order, but applying that model does not bring improve- ments on generation tasks. This shows a potential direction to improve CodeBERT by incorporating AST. \n\n5 Conclusion \nIn this paper, we present CodeBERT, which to the best of our knowledge is the ﬁrst large  bimodal pre-trained model for natural language and pro- gramming language. We train CodeBERT on both bimodal  and  unimodal  data, and show that ﬁne- tuning CodeBERT achieves state-of-the-art perfor- mance on downstream tasks including natural lan- guage code search and code-to-documentation gen- eration. To further investigate the knowledge em- bodied in pre-trained models, we formulate the task of NL-PL probing and create a dataset for probing. We regard the probing task as a cloze-style answer selection problem, and curate distractors for both NL and PL parts. Results show that, with model parameters  ﬁxed, CodeBERT performs better than RoBERTa and a continuously trained model using codes only. \nThere are many potential directions for further research on this ﬁeld. First, one could learn better generators with  bimodal  evidence or more compli- cated neural architecture to improve the replaced to- ken detection objective. Second, the loss functions of CodeBERT mainly target on NL-PL understand- ing tasks. Although CodeBERT achieves strong BLEU scores on code-to-documentation genera- tion, the CodeBERT itself could be further im- proved by generation-related learning objectives. "}
{"page": 8, "image_path": "doc_images/2020.findings-emnlp.139_8.jpg", "ocr_text": "How to successfully incorporate AST into the pre-\ntraining step is also an attractive direction. Third,\nwe plan to apply CodeBERT to more NL-PL re-\nlated tasks, and extend it to more programming\nlanguages. Flexible and powerful domain/language\nadaptation methods are necessary to generalize\nwell.\n\nAcknowledgments\n\nXiaocheng Feng is the corresponding author of this\nwork. We thank the anonymous reviewers for their\ninsightful comments. Zhangyin Feng, Xiaocheng\nFeng, Bing Qin and Ting Liu are supported by the\nNational Key R&D Program of China via grant\n2018YFB1005103 and National Natural Science\nFoundation of China (NSFC) via grant 61632011\nand 61772156.\n\nReferences\n\nUri Alon, Shaked Brody, Omer Levy, and Eran Yahav.\n2019. code2seq: Generating sequences from struc-\ntured representations of code. International Confer-\nenceon Learning Representations.\n\nKyunghyun Cho, Bart Van Merriénboer, Caglar Gul-\ncehre, Dzmitry Bahdanau, Fethi Bougares, Holger\nSchwenk, and Yoshua Bengio. 2014. Learning\nphrase representations using mn encoder-decoder\nfor statistical machine translation. arXiv preprint\narXiv:1406.1078.\n\nKevin Clark, Minh-Thang Luong, Quoc V. Le, and\nChristopher D. Manning. 2020. {ELECTRA}: Pre-\ntraining text encoders as discriminators rather than\ngenerators. In Jnternational Conference on Learn-\ning Representations.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv: 1810.04805.\n\nXiaodong Gu, Hongyu Zhang, and Sunghun Kim. 2018.\nDeep code search. In 2018 IEEE/ACM 40th Interna-\ntional Conference on Software Engineering (ICSE),\npages 933-944. IEEE.\n\nHamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis\nAllamanis, and Marc Brockschmidt. 2019. Code-\nsearchnet challenge: Evaluating the state of seman-\ntic code search. arXiv preprint arXiv: 1909.09436.\n\nSrinivasan Iyer, Ioannis Konstas, Alvin Cheung, and\nLuke Zettlemoyer. 2016. Summarizing source code\nusing a neural attention model. In Proceedings\nof the 54th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 2073-2083.\n\nDan Jurafsky. 2000. Speech & language processing.\nPearson Education India.\n\nAditya Kanade, Petros Maniatis, Gogul Balakrish-\nnan, and Kensen Shi. 2019. Pre-trained contex-\ntual embedding of source code. arXiv preprint\narXiv:2001.00059.\n\nYoon Kim. 2014. Convolutional neural net-\nworks for sentence classification. arXiv preprint\narXiv: 1408.5882.\n\nPhilipp Koehn, Hieu Hoang, Alexandra Birch, Chris\nCallison-Burch, Marcello Federico, Nicola Bertoldi,\nBrooke Cowan, Wade Shen, Christine Moran,\nRichard Zens, et al. 2007. Moses: Open source\ntoolkit for statistical machine translation. In Pro-\nceedings of the 45th annual meeting of the associ-\nation for computational linguistics companion vol-\nume proceedings of the demo and poster sessions,\npages 177-180.\n\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Ves Stoyanov, and Luke Zettlemoyer. 2019.\nBart: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and\ncomprehension. arXiv preprint arXiv:1910.13461.\n\nChin-Yew Lin and Franz Josef Och. 2004. Orange: a\nmethod for evaluating automatic evaluation metrics\nfor machine translation. In Proceedings of the 20th\ninternational conference on Computational Linguis-\ntics, page 501. Association for Computational Lin-\nguistics.\n\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Dangi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv: 1907.11692.\n\nJiasen Lu, Dhruv Batra, Devi Parikh, and Stefan\nLee. 2019. Vilbert: Pretraining task-agnostic visi-\nolinguistic representations for vision-and-language\ntasks. In Advances in Neural Information Process-\ning Systems, pages 13-23.\n\nBhaskar Mitra, Nick Craswell, et al. 2018. An intro-\nduction to neural information retrieval. Foundations\nand Trends® in Information Retrieval, 13(1):1-126.\n\nMatthew E Peters, Mark Neumann, Mohit lyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. arXiv preprint arXiv: 1802.05365.\n\nFabio Petroni, Tim Rocktischel, Patrick Lewis, Anton\nBakhtin, Yuxiang Wu, Alexander H Miller, and Se-\nbastian Riedel. 2019. Language models as knowl-\nedge bases? arXiv preprint arXiv: 1909.01066.\n\nTelmo Pires, Eva Schlinger, and Dan Garrette. 2019.\nHow multilingual is multilingual bert? arXiv\npreprint arXiv: 1906.01502.\n\n1544\n", "vlm_text": "How to successfully incorporate AST into the pre- training step is also an attractive direction. Third, we plan to apply CodeBERT to more NL-PL re- lated tasks, and extend it to more programming languages. Flexible and powerful domain/language adaptation methods are necessary to generalize well. \nAcknowledgments \nXiaocheng Feng is the corresponding author of this work. We thank the anonymous reviewers for their insightful comments. Zhangyin Feng, Xiaocheng Feng, Bing Qin and Ting Liu are supported by the National Key R&D Program of China via grant 2018YFB1005103 and National Natural Science Foundation of China (NSFC) via grant 61632011 and 61772156. \nReferences \nUri Alon, Shaked Brody, Omer Levy, and Eran Yahav. 2019. code2seq: Generating sequences from struc- \nenceon Learning Representations . Kyunghyun Cho, Bart Van Merri¨ enboer, Caglar Gul- cehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using rnn encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078 . Kevin Clark, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning. 2020.    $\\{{\\mathrm{ELECT}}_{\\mathrm{IRA}}\\}$  : Pre- training text encoders as discriminators rather than generators. In  International Conference on Learn- ing Representations . Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understand- ing.  arXiv preprint arXiv:1810.04805 . Xiaodong Gu, Hongyu Zhang, and Sunghun Kim. 2018. Deep code search. In  2018 IEEE/ACM 40th Interna- tional Conference on Software Engineering (ICSE) , pages 933–944. IEEE. Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc Brockschmidt. 2019. Code- searchnet challenge: Evaluating the state of seman- tic code search.  arXiv preprint arXiv:1909.09436 . Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer. 2016. Summarizing source code using a neural attention model. In  Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 2073–2083. \nDan Jurafsky. 2000. Speech & language processing . Pearson Education India. \nAditya Kanade, Petros Maniatis, Gogul Balakrish- nan, and Kensen Shi. 2019. Pre-trained contex- tual embedding of source code. arXiv preprint arXiv:2001.00059 . \nYoon Kim. 2014. Convolutional neural net- works for sentence classiﬁcation. arXiv preprint arXiv:1408.5882 . \nPhilipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, et al. 2007. Moses: Open source toolkit for statistical machine translation. In  Pro- ceedings of the 45th annual meeting of the associ- ation for computational linguistics companion vol- ume proceedings of the demo and poster sessions , pages 177–180. \nMike Lewis, Yinhan Liu, Naman Goyal, Mar- jan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. 2019. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension.  arXiv preprint arXiv:1910.13461 . \nChin-Yew Lin and Franz Josef Och. 2004. Orange: a method for evaluating automatic evaluation metrics for machine translation. In  Proceedings of the 20th international conference on Computational Linguis- tics , page 501. Association for Computational Lin- guistics. \nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man- dar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining ap- proach.  arXiv preprint arXiv:1907.11692 . \nJiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. 2019. Vilbert: Pretraining task-agnostic visi- olinguistic representations for vision-and-language tasks. In  Advances in Neural Information Process- ing Systems , pages 13–23. \nBhaskar Mitra, Nick Craswell, et al. 2018. An intro- duction to neural information retrieval.  Foundations and Trends® in Information Retrieval , 13(1):1–126. \nMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word repre- sentations.  arXiv preprint arXiv:1802.05365 . \nFabio Petroni, Tim Rockt¨ aschel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H Miller, and Se- bastian Riedel. 2019. Language models as knowl- edge bases?  arXiv preprint arXiv:1909.01066 . \nTelmo Pires, Eva Schlinger, and Dan Garrette. 2019. How multilingual is multilingual bert? arXiv preprint arXiv:1906.01502 . "}
{"page": 9, "image_path": "doc_images/2020.findings-emnlp.139_9.jpg", "ocr_text": "Alec Radford, Karthik Narasimhan, Tim Salimans,\nand Ilya Sutskever. 2018. Improving language\nunderstanding by generative pre-training. URL\nhttps://s3-us-west-2._ amazonaws. _ com/openai-\nassets/researchcovers/languageunsupervised/language\nunderstanding paper. pdf.\n\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2019. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. arXiv preprint arXiv: 1910.10683.\n\nAlexander M Rush, Sumit Chopra, and Jason We-\nston. 2015. A neural attention model for ab-\nstractive sentence summarization. arXiv preprint\narXiv: 1509.00685.\n\nChen Sun, Austin Myers, Carl Vondrick, Kevin Mur-\nphy, and Cordelia Schmid. 2019. Videobert: A joint\nmodel for video and language representation learn-\ning. arXiv preprint arXiv: 1904.01766.\n\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.\nSequence to sequence learning with neural networks.\n\nIn Advances in neural information processing sys-\ntems, pages 3104-3112.\n\nKai Sheng Tai, Richard Socher, and Christopher D\nManning. 2015. Improved semantic representations\nfrom tree-structured long short-term memory net-\nworks. arXiv preprint arXiv: 1503.00075.\n\nAlon Talmor, Yanai Elazar, Yoav Goldberg, and\nJonathan Berant. 2019. | olmpics—on what lan-\nguage model pre-training captures. arXiv preprint\narXiv:1912.13283.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998-6008.\n\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V\nLe, Mohammad Norouzi, Wolfgang Macherey,\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus\nMacherey, et al. 2016. Google’s neural machine\ntranslation system: Bridging the gap between hu-\nman and machine translation. arXiv preprint\narXiv:1609.08144.\n\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Ruslan Salakhutdinov, and Quoc V Le.\n2019. XInet: Generalized autoregressive pretrain-\ning for language understanding. arXiv preprint\narXiv: 1906.08237.\n\nA Data Statistic\n\nData statistics of the training/validation/testing data\nsplits for six programming languages are given in\nTable 6.\n\nCODE SEARCH TRAINING DEV TESTING\nGo 635,635 28,483 14,291\nJAVA 908,886 30,655 26,909\nJAVASCRIPT 247,773 16,505 6,483\nPHP 1,047,406 52,029 28,391\nPYTHON 824,342 46,213 22,176\nRUBY 97,580 4,417 2,279\n\nTable 6: Data statistics about the CodeSearchNet Cor-\npus for natural language code search.\n\nB_ Train Details\n\nB.1_ Pre-training\n\nWe train CodeBERT on one NVIDIA DGX-2 ma-\nchine using FP16. It combines 16 interconnected\nNVIDIA Tesla V100 with 32GB memory. We use\nthe following set of hyper-parameters to train mod-\nels: batchsize is 2,048 and learning rate is Se-4. We\nuse Adam to update the parameters and set the num-\nber of warmup steps as 10K. We set the max length\nas 512 and the max training step is 100K. Training\n1,000 batches of data costs 600 minutes with MLM\nobjective, 120 minutes with RTD objective.\n\nB.2. CodeSearch\n\nIn the fine-turning step, we set the learning rate as\nle-5, the batch size as 64, the max sequence length\nas 200 and the max fine-tuning epoch as 8. As the\nsame with pre-training, We use Adam to update the\nparameters. We choose the model performed best\non the development set, and use that to evaluate on\nthe test set.\n\nB.3 Code Summarization on Six\nProgramming Languages\n\nWe use Transformer with 6 layers, 768 dimensional\nhidden states and 12 attention heads as our decoder\nin all settings. We set the max length of input\nand inference as 256 and 64, respectively. We use\nthe Adam optimizer to update model parameters.\nThe learning rate and the batch size are 5e-5 and\n64, respectively. We tune hyperparameters and\nperform early stopping on the development set.\n\nB.4. Code Summarization on C#\n\nSince state-of-the-art methods use RNN as their de-\ncoder, we choose a 2-layer GRU with an attention\nmechanism as our decoder for a comparison. We\nfine-tune models using a grid search with the fol-\nlowing set of hyper-parameters: batchsize is in {32,\n64} and learning rate is in {2e-5, 5e-5}. We report\n\n1545\n", "vlm_text": "Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language understanding by generative pre-training. URL https://s3-us-west-2. amazonaws. com/openai- assets/researchcovers/language unsupervised/language understanding paper. pdf . \nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2019. Exploring the limits of transfer learning with a uniﬁed text-to-text trans- former.  arXiv preprint arXiv:1910.10683 . \nAlexander M Rush, Sumit Chopra, and Jason We- ston. 2015. A neural attention model for ab- stractive sentence summarization. arXiv preprint arXiv:1509.00685 . \nChen Sun, Austin Myers, Carl Vondrick, Kevin Mur- phy, and Cordelia Schmid. 2019. Videobert: A joint model for video and language representation learn- ing.  arXiv preprint arXiv:1904.01766 . \nIlya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to sequence learning with neural networks. In  Advances in neural information processing sys- tems , pages 3104–3112. \nKai Sheng Tai, Richard Socher, and Christopher D Manning. 2015. Improved semantic representations from tree-structured long short-term memory net- works.  arXiv preprint arXiv:1503.00075 . \nAlon Talmor, Yanai Elazar, Yoav Goldberg, and Jonathan Berant. 2019. olmpics–on what lan- guage model pre-training captures.  arXiv preprint arXiv:1912.13283 . \nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In  Advances in neural information pro- cessing systems , pages 5998–6008. \nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. 2016. Google’s neural machine translation system: Bridging the gap between hu- man and machine translation. arXiv preprint arXiv:1609.08144 . \nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car- bonell, Ruslan Salakhutdinov, and Quoc V Le. 2019. Xlnet: Generalized autoregressive pretrain- ing for language understanding. arXiv preprint arXiv:1906.08237 . \nA Data Statistic \nData statistics of the training/validation/testing data splits for six programming languages are given in Table  6 . \nThe table presents data on code search for various programming languages across three categories: Training, Dev (Development), and Testing. Here are the details:\n\n- **Go**\n  - Training: 635,635\n  - Dev: 28,483\n  - Testing: 14,291\n\n- **Java**\n  - Training: 908,886\n  - Dev: 30,655\n  - Testing: 26,909\n\n- **JavaScript**\n  - Training: 247,773\n  - Dev: 16,505\n  - Testing: 6,483\n\n- **PHP**\n  - Training: 1,047,406\n  - Dev: 52,029\n  - Testing: 28,391\n\n- **Python**\n  - Training: 824,342\n  - Dev: 46,213\n  - Testing: 22,176\n\n- **Ruby**\n  - Training: 97,580\n  - Dev: 4,417\n  - Testing: 2,279\nB Train Details \nB.1 Pre-training \nWe train CodeBERT on one NVIDIA DGX-2 ma- chine using FP16. It combines 16 interconnected NVIDIA Tesla V100 with 32GB memory. We use the following set of hyper-parameters to train mod- els: batchsize is 2,048 and learning rate is 5e-4. We use Adam to update the parameters and set the num- ber of warmup steps as 10K. We set the max length as 512 and the max training step is 100K. Training 1,000 batches of data costs 600 minutes with MLM objective, 120 minutes with RTD objective. \nB.2 CodeSearch \nIn the ﬁne-turning step, we set the learning rate as 1e-5, the batch size as 64, the max sequence length as 200 and the max ﬁne-tuning epoch as 8. As the same with pre-training, We use Adam to update the parameters. We choose the model performed best on the development set, and use that to evaluate on the test set. \nB.3 Code Summarization on Six Programming Languages \nWe use Transformer with 6 layers, 768 dimensional hidden states and 12 attention heads as our decoder in all settings. We set the max length of input and inference as 256 and 64, respectively. We use the Adam optimizer to update model parameters. The learning rate and the batch size are 5e-5 and 64, respectively. We tune hyperparameters and perform early stopping on the development set. \nB.4 Code Summarization on C# \nSince state-of-the-art methods use RNN as their de- coder, we choose a 2-layer GRU with an attention mechanism as our decoder for a comparison. We ﬁne-tune models using a grid search with the fol- lowing set of hyper-parameters: batchsize is in    $\\{32$  , 64 }  and learning rate is in    $\\{2\\mathrm{e}.5,5\\mathrm{e}.5\\}$  . We report the number when models achieve best performance on the development set. "}
{"page": 10, "image_path": "doc_images/2020.findings-emnlp.139_10.jpg", "ocr_text": "the number when models achieve best performance\non the development set.\n\nC_ Learning Curve of CodeSearch\n\nFrom Figure 4, we can see that CodeBERT per-\nforms better at the early stage, which reflects that\nCodeBERT provides good initialization for learn-\ning downstream tasks.\n\n885-7 Roberta 80) =o Roberta\n\n—® CodeseRT ~® CodeBeRT\n\n© Prestrainw/code only, 82.5 2 retrain w/ code only\n875 oe Ht\n\n5 820\n\nZero\n\nBas, 73.5)\nThe\n\nnber of Epoch The\n\nFigure 4: Learning curve of different pre-trained mod-\nels in the fine-tuning step. We show results on Python\nand Java.\n\nD_ Late Fusion\n\nIn section §4.1 , we show that CodeBERT per-\nforms well in the setting where natural languages\nand codes have early interactions. Here, we in-\nvestigate whether CodeBERT is good at working\nas a unified encoder. We apply CodeBERT for\nnatural language code search in a later fusion set-\nting, where CodeBERT first encodes NL and PL\nseparately, and then calculates the similarity by dot-\nproduct. In this way, code search is equivalent to\nfind the nearest codes in the shared vector space.\nThis scenario also facilitates the use of CodeBERT\nin an online system, where the representations of\ncodes are calculated in advance. In the runtime, a\nsystem only needs to compute the representation\nof NL and vector-based dot-product.\n\nWe fine-tune CodeBERT with the following ob-\njective, which maximizes the dot-product of the\nground truth while minimizing the dot-product of\ndistractors.\n\n1 exp(Enc(c;)'Enc(w;))\n1\nN » °s ( yj exp(Enc(c;)TEnc(wy)) )\n(15)\n\nResults are given in Table 7. We just do this\nsetting on two languages with a relatively small\namount of data.\n\nWe can see that CodeBERT performs better than\nRoBERTa and the model pre-trained with codes\n\nMODEL RUBY Go\n\nROBERTA 0.0043 0.0030\nPRE-TRAIN W/CODEONLY 0.1648 0.4179\nCoDEBERT 0.6870 0.8372\n\nTable 7: Results on natural language code search by\nlate fusion.\n\nonly. And late fusion performs comparable with\nthe standard way. What’s more, late fusion is more\nefficient and this setting could be used in an online\nsystem.\n\nE_ Case Study\n\nTo qualitatively analyze the effectiveness of Code-\nBERT, we give some cases for code search and\ncode documentation generation tasks.\n\nConsidering the limited space, we only give the\ntop2 results of the query for python programming\nlanguage. As show in Figure 5, search results are\nvery relevant with query.\n\nFigure 6 and Figure 7 show the outputs with\ndifferent models for the code documentation gen-\neration task. As we can see, CodeBERT performs\nbetter than all baselines.\n\n1546\n", "vlm_text": "\nC Learning Curve of CodeSearch \nFrom Figure  4 , we can see that CodeBERT per- forms better at the early stage, which reﬂects that CodeBERT provides good initialization for learn- ing downstream tasks. \nThis image consists of two line graphs side by side, comparing the development accuracy of three different models—Roberta, CodeBERT, and a model pre-trained with code only—across a varying number of epochs. \n\n- The left graph depicts the development accuracy for Python:\n  - CodeBERT (orange line) exhibits consistently high accuracy across epochs, peaking around 87.5% before slightly declining.\n  - The model pre-trained with code only (green line) has a higher starting accuracy than Roberta, showing some fluctuation with a high around the second epoch, eventually stabilizing around 86%.\n  - Roberta (blue line) shows a lower accuracy, starting around 85.4% and exhibits slight fluctuations across epochs.\n\n- The right graph illustrates the development accuracy for Java:\n  - CodeBERT (orange line) starts with accuracy over 82% and shows a decreasing trend over the epochs, ending slightly below 81%.\n  - The model pre-trained with code only (green line) starts just over 80%, peaks around the second epoch, and stabilizes around 80.5%.\n  - Roberta (blue line) starts with a similar trend as the pre-trained model but generally shows lower accuracy, starting just under 80.5% and demonstrating some fluctuation.\n\nOverall, CodeBERT consistently achieves the highest accuracy for both Python and Java during model training across epochs, while Roberta tends to have the lowest accuracy.\nFigure 4: Learning curve of different pre-trained mod- els in the ﬁne-tuning step. We show results on Python and Java. \nD Late Fusion \nIn section    $\\S4.1$   , we show that CodeBERT per- forms well in the setting where natural languages and codes have early interactions. Here, we in- vestigate whether CodeBERT is good at working as a uniﬁed encoder. We apply CodeBERT for natural language code search in a later fusion set- ting, where CodeBERT ﬁrst encodes NL and PL separately, and then calculates the similarity by dot- product. In this way, code search is equivalent to ﬁnd the nearest codes in the shared vector space. This scenario also facilitates the use of CodeBERT in an online system, where the representations of codes are calculated in advance. In the runtime, a system only needs to compute the representation of NL and vector-based dot-product. \nThe table compares the performance of three different models—RoBERTa, a model pretrained with code only, and CodeBERT—on tasks related to the Ruby and Go programming languages. The performance is indicated by numerical scores:\n\n1. **RoBERTa**:\n   - Ruby: 0.0043\n   - Go: 0.0030\n\n2. **Pre-Train w/ code only**:\n   - Ruby: 0.1648\n   - Go: 0.4179\n\n3. **CodeBERT**:\n   - Ruby: 0.6870\n   - Go: 0.8372\n\nThese scores suggest that CodeBERT outperforms the other models significantly in both Ruby and Go tasks.\nonly. And late fusion performs comparable with the standard way. What’s more, late fusion is more efﬁcient and this setting could be used in an online system. \nE Case Study \nTo qualitatively analyze the effectiveness of Code- BERT, we give some cases for code search and code documentation generation tasks. \nConsidering the limited space, we only give the top2 results of the query for python programming language. As show in Figure  5 , search results are very relevant with query. \nFigure  6  and Figure  7  show the outputs with different models for the code documentation gen- eration task. As we can see, CodeBERT performs better than all baselines. \nWe ﬁne-tune CodeBERT with the following ob- jective, which maximizes the dot-product of the ground truth while minimizing the dot-product of distractors. \n\n$$\n-\\frac{1}{N}\\sum_{i}\\log\\bigg(\\frac{\\exp\\big(E n c(c_{i})^{\\intercal}E n c(w_{i})\\big)}{\\sum_{j}\\exp\\big(E n c(c_{j})^{\\intercal}E n c(w_{i})\\big)}\\bigg)\n$$\n \nResults are given in Table  7 . We just do this setting on two languages with a relatively small amount of data. \nWe can see that CodeBERT performs better than RoBERTa and the model pre-trained with codes "}
{"page": 11, "image_path": "doc_images/2020.findings-emnlp.139_11.jpg", "ocr_text": "Query\n\ncreate file and write something\n\nSearch Results (top2)\n\nhttps://github.com/darknessomi/musicbox/blob/master/NEMbox/utils.py#L37-L40\n\ndef create_file(path, default=\"\\n\"):\nif not os.path.exists(path):\nwith open(path, \"w\") as f:\nf.write(default)\n\nhttps://github.com/datakortet/yamldirs/blob/master/yamldirs/filemaker.py#L114-L118\n\ndef make_file(self, filename, content):\n\"\"Create a new file with name *‘filename™* and content ‘‘content*’.\nwith open(filename, 'w') as fp:\nfp.write(content)\n\nFigure 5: Python CodeSearch example. The results are searched from 1,156,085 python code data. We only give\nthe top2 results because space is limited.\n\npublic void addWriteErrorResult(final Bulk WriteError writeError,\nfinal IndexMap indexMap) {\nnotNull(\"writeError\", writeError);\nmergeWriteErrors(asList(writeError), indexMap);\n\n}\n\nGold: Add a write error result\n\nCodeBERT: Add a write error result .\n\nPRE-TRAIN W/ CODEONLY : Merges the given write error .\nRoberta: Add a write operation to the map .\n\nTransformer: Adds an error to the write map .\n\nRNN: Add an error map .\n\nFigure 6: Java code documentation generation output example.\n\ndef create_or_update(self, list_id, subscriber_hash, data):\n\nsubscriber_hash = check_subscriber_hash(subscriber_hash)\n\nself.list_id = list_id\n\nself.subscriber_hash = subscriber_hash\n\nif 'email_address' not in data:\nraise KeyError('The list member must have an email_address')\n\ncheck_email(data['email_address'])\n\nif 'status_if_new' not in data:\nraise KeyError('The list member must have a status_if_new’)\n\nif data['status_if_new'] not in ['subscribed’, 'unsubscribed’, 'cleaned’, 'pending’\nraise ValueError('The list member status_if_new must be one of\n\"subscribed\", \"unsubscribed\", \"cleaned\", \"pending\", or \"transactional\"')\n\nreturn self._mc_client._put(url=self._build_path(list_id, 'members', subscriber_hash), data=data)\n\n', 'transactional']:\n\nGold: Add or update a list member .\n\nCodeBERT: Create or update a list member .\n\nPRE-TRAIN W/ CODEONLY: Create or update a subscriber .\nRoberta: Create or update an existing record .\n\nTransformer: Create or update a subscription .\n\nRNN: Creates or updates an email address .\n\nFigure 7: Python code documentation generation output example.\n\n1547\n", "vlm_text": "The image contains a search query and its corresponding search results. The query appears to be \"create file and write something.\"\n\nThe results shown are two code snippets from GitHub repositories with their respective links.\n\n1. The first result is from the repository \"darknessomi/musicbox\" at the file \"NEMbox/utils.py\" lines 37-40. The code snippet defines a function `create_file(path, default=\"\\n\")` that checks if a file at the given path exists and if not, it creates one with the default content.\n\n2. The second result is from the repository \"datakortet/yamldirs\" at the file \"yamldirs/filemaker.py\" lines 114-118. The code snippet defines a method `make_file(self, filename, content)`, which creates a new file with the specified filename and writes the given content to it.\nFigure 5: Python CodeSearch example. The results are searched from 1,156,085 python code data. We only give the top2 results because space is limited. \nThe image contains two sections. The first section is a code snippet in Java, with a method definition `addWriteErrorResult`. This method takes two parameters, `BulkWriteError writeError` and `IndexMap indexMap`. The method calls two functions: `notNull` to check that `writeError` is not null, and `mergeWriteErrors`, which appears to merge the write error into an index map using `asList`.\n\nThe second section provides various model-generated captions or descriptions for the code snippet:\n\n1. **Gold**: Add a write error result\n2. **CodeBERT**: Add a write error result.\n3. **PRE-TRAIN W/ CODENOILY**: Merges the given write error.\n4. **Roberta**: Add a write operation to the map.\n5. **Transformer**: Adds an error to the write map.\n6. **RNN**: Add an error map.\n\nThese descriptions seem to be generated by different models attempting to summarize or describe the purpose of the Java code method provided in the first section.\nFigure 6: Java code documentation generation output example. \nThe image contains a code snippet and the output of various models attempting to caption the snippet.\n\nThe code snippet defines a Python function `create_or_update` which is intended to add or update a list member. The function checks for the presence of 'email_address' and 'status_if_new' in the input data and raises a `KeyError` if they are missing. It also validates the value of 'status_if_new' and raises a `ValueError` if it's not among the accepted values. Finally, it performs a PUT request using `_mc_client`.\n\nUnderneath the code, various models provide their caption predictions for the code snippet:\n\n1. **Gold**: Add or update a list member.\n2. **CodeBERT**: Create or update a list member.\n3. **PRE-TRAIN W/ CODEONLY**: Create or update a subscriber.\n4. **Roberta**: Create or update an existing record.\n5. **Transformer**: Create or update a subscription.\n6. **RNN**: Creates or updates an email address."}
