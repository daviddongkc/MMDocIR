{"layout": 0, "type": "text", "text": "Tied Multitask Learning for Neural Speech Translation ", "text_level": 1, "page_idx": 0, "bbox": [129, 67, 470, 87], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 1, "type": "text", "text": "Antonios Anastasopoulos  and  David Chiang Department of Computer Science and Engineeering University of Notre Dame { aanastas,dchiang } @nd.edu ", "page_idx": 0, "bbox": [176.5909881591797, 113.22607421875, 423.94403076171875, 177.831787109375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 2, "type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0, "bbox": [159, 223, 204, 236], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 3, "type": "text", "text": "We explore multitask models for neural trans- lation of speech, augmenting them in order to reﬂect two intuitive notions. First, we in- troduce a model where the second task de- coder receives information from the decoder of the ﬁrst task, since higher-level intermediate representations should provide useful infor- mation. Second, we apply regularization that encourages  transitivity  and  invertibility . We show that the application of these notions on jointly trained models improves performance on the tasks of low-resource speech transcrip- tion and translation. It also leads to better per- formance when using attention information for word discovery over unsegmented input. ", "page_idx": 0, "bbox": [89, 244.9586181640625, 273, 424.3355407714844], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 4, "type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0, "bbox": [72, 433, 155, 447], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 5, "type": "text", "text": "Recent e ﬀ orts in endangered language documen- tation focus on collecting spoken language re- sources, accompanied by spoken translations in a high resource language to make the resource in- terpretable ( Bird et al. ,  2014a ). For example, the BULB project ( Adda et al. ,  2016 ) used the LIG- Aikuma mobile app ( Bird et al. ,  2014b ;  Blachon et al. ,  2016 ) to collect parallel speech corpora be- tween three Bantu languages and French. Since it’s common for speakers of endangered languages to speak one or more additional languages, collec- tion of such a resource is a realistic goal. ", "page_idx": 0, "bbox": [72, 454.8030090332031, 290, 616.989501953125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 6, "type": "text", "text": "Speech can be interpreted either by transcrip- tion in the original language or translation to an- other language. Since the size of the data is ex- tremely small, multitask models that jointly train a model for both tasks can take advantage of both signals. Our contribution lies in improv- ing the sequence-to-sequence multitask learning paradigm, by drawing on two intuitive notions: that higher-level representations are more useful than lower-level representations, and that transla- tion should be both transitive and invertible. ", "page_idx": 0, "bbox": [72, 617.39306640625, 290, 766.030517578125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 7, "type": "text", "text": "Higher-level intermediate representations , such as transcriptions, should in principle carry infor- mation useful for an end task like speech transla- tion. A typical multitask setup ( Weiss et al. ,  2017 ) shares information at the level of encoded frames, but intuitively, a human translating speech must work from a higher level of representation, at least at the level of phonemes if not syntax or semantics. Thus, we present a novel architecture for  tied  mul- titask learning with sequence-to-sequence models, in which the decoder of the second task receives information not only from the encoder, but also from the decoder of the ﬁrst task. ", "page_idx": 0, "bbox": [307, 223.4190216064453, 525, 399.1545715332031], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 8, "type": "text", "text": "In addition,  transitivity  and  invertibility  are two properties that should hold when mapping be- tween levels of representation or across languages. We demonstrate how these two notions can be im- plemented through regularization of the attention matrices, and how they lead to further improved performance. ", "page_idx": 0, "bbox": [307, 399.9071044921875, 525, 494.3476257324219], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 9, "type": "text", "text": "We evaluate our models in three experiment settings: low-resource speech transcription and translation, word discovery on unsegmented in- put, and high-resource text translation. Our high- resource experiments are performed on English, French, and German. Our low-resource speech ex- periments cover a wider range of linguistic diver- sity: Spanish-English, Mboshi-French, and Ainu- English. ", "page_idx": 0, "bbox": [307, 495.10113525390625, 525, 616.6395874023438], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 10, "type": "text", "text": "In the speech transcription and translation tasks, our proposed model leads to improved perfor- mance against all baselines as well as previous multitask architectures. We observe improvements of up to  $5\\%$   character error rate in the transcrip- tion task, and up to  $2.8\\%$   character-level BLEU in the translation task. However, we didn’t observe similar improvements in the text translation exper- iments. Finally, on the word discovery task, we im- prove upon previous work by about  $3\\%$   F-score on both tokens and types. ", "page_idx": 0, "bbox": [307, 617.3931274414062, 525, 766.0305786132812], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 11, "type": "text", "text": "Our models are based on a sequence-to-sequence model with attention ( Bahdanau et al. ,  2015 ). In general, this type of model is composed of three parts: a recurrent encoder, the attention, and a re- current decoder (see Figure  1 a). ", "page_idx": 1, "bbox": [72, 83.90503692626953, 290, 151.24655151367188], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 12, "type": "text", "text": "The encoder transforms an input sequence of words or feature frames    $\\mathbf{x}_{1},\\ldots,\\mathbf{x}_{N}$   into a sequence of  input states  ${\\bf h}_{1},\\dots,{\\bf h}_{N}$  : ", "page_idx": 1, "bbox": [72, 151.6510772705078, 290, 191.89456176757812], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 13, "type": "equation", "text": "\n$$\n\\mathbf h_{n}=\\mathsf{e n c}(\\mathbf h_{n-1},\\mathbf x_{n}).\n$$\n ", "text_format": "latex", "page_idx": 1, "bbox": [137, 198, 223, 212], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 14, "type": "text", "text": "The attention transforms the input states into a se- quence of  context vectors  via a matrix of  attention weights : ", "page_idx": 1, "bbox": [72, 219.3091583251953, 290, 259.5526428222656], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 15, "type": "equation", "text": "\n$$\n\\mathbf{c}_{m}=\\sum_{n}\\alpha_{m n}\\mathbf{h}_{n}.\n$$\n ", "text_format": "latex", "page_idx": 1, "bbox": [144, 256.25, 217, 284], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 16, "type": "text", "text": "Finally, the decoder computes a sequence of  out- put states  from which a probability distribution over output words can be computed. ", "page_idx": 1, "bbox": [72, 286.01116943359375, 290, 326.2556457519531], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 17, "type": "equation", "text": "\n$$\n\\begin{array}{c}{\\mathbf{s}_{m}=\\operatorname*{det}(\\mathbf{s}_{m-1},\\mathbf{c}_{m},\\mathbf{y}_{m-1})}\\\\ {P(\\mathbf{y}_{m})=\\mathrm{softmax}(\\mathbf{s}_{m}).}\\end{array}\n$$\n ", "text_format": "latex", "page_idx": 1, "bbox": [118, 332, 244, 364], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 18, "type": "text", "text": "In a standard encoder-decoder  multitask  model (Figure  1 b) ( Dong et al. ,  2015 ;  Weiss et al. ,  2017 ), we jointly model two output sequences using a shared encoder, but separate attentions and de- coders: ", "page_idx": 1, "bbox": [72, 370.2071533203125, 290, 437.5496520996094], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 19, "type": "equation", "text": "\n$$\n\\begin{array}{l}{\\displaystyle\\mathbf{c}_{m}^{1}=\\sum_{n}\\alpha_{m n}^{1}\\mathbf{h}_{n}}\\\\ {\\displaystyle\\quad\\mathbf{s}_{m}^{1}=\\operatorname*{det}^{1}(\\mathbf{s}_{m-1}^{1},\\mathbf{c}_{m}^{1},\\mathbf{y}_{m-1}^{1})}\\\\ {\\displaystyle P(\\mathbf{y}_{m}^{1})=\\operatorname{softmax}(\\mathbf{s}_{m}^{1})}\\end{array}\n$$\n ", "text_format": "latex", "page_idx": 1, "bbox": [116, 439, 246, 505], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 20, "type": "text", "text": "and ", "page_idx": 1, "bbox": [72, 512.9141235351562, 87.75377655029297, 526.0595703125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 21, "type": "equation", "text": "\n$$\n\\begin{array}{l l}{{\\displaystyle{\\bf c}_{m}^{2}=\\sum_{n}\\alpha_{m n}^{2}{\\bf h}_{n}}}\\\\ {{\\displaystyle~~{\\bf s}_{m}^{2}=\\mathrm{dec}^{2}({\\bf s}_{m-1}^{2},{\\bf c}_{m}^{2},{\\bf y}_{m-1}^{2})}}\\\\ {{\\displaystyle P({\\bf y}_{m}^{2})=\\mathrm{softmax}({\\bf s}_{m}^{2})}.}\\end{array}\n$$\n ", "text_format": "latex", "page_idx": 1, "bbox": [116, 530, 247, 597], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 22, "type": "text", "text": "We can also arrange the decoders in a  cascade (Figure  1 c), in which the second decoder attends only to the output states of the ﬁrst decoder: ", "page_idx": 1, "bbox": [72, 601.4241333007812, 290, 641.6675415039062], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 23, "type": "equation", "text": "\n$$\n\\begin{array}{c}{\\displaystyle\\mathbf{c}_{m}^{2}=\\sum_{m^{\\prime}}\\alpha_{m m^{\\prime}}^{12}\\mathbf{s}_{m^{\\prime}}^{1}}\\\\ {\\displaystyle\\mathbf{s}_{m}^{2}=\\mathrm{dec}^{2}(\\mathbf{s}_{m-1}^{2},\\mathbf{c}_{m}^{2},\\mathbf{y}_{m-1}^{2})}\\\\ {\\displaystyle P(\\mathbf{y}_{m}^{2})=\\mathrm{softmax}(\\mathbf{s}_{m}^{2}).}\\end{array}\n$$\n ", "text_format": "latex", "page_idx": 1, "bbox": [116, 643, 246, 709], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 24, "type": "text", "text": "Tu et al.  ( 2017 ) use exactly this architecture to train on bitext by setting the second output se- quence to be equal to the input sequence   $(\\mathbf{y}_{i}^{2}=\\mathbf{x}_{i})$  ). ", "page_idx": 1, "bbox": [307, 63.68604278564453, 525, 105], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 25, "type": "text", "text": "In our proposed  triangle  model (Figure  1 d), the ﬁrst decoder is as above, but the second decoder has two attentions, one for the input states of the encoder and one for the output states of the ﬁrst decoder: ", "page_idx": 1, "bbox": [307, 104.41797637939453, 525, 171.76046752929688], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 26, "type": "equation", "text": "\n$$\n\\begin{array}{r}{\\begin{array}{c}{\\mathbf{c}_{m}^{2}=\\left[\\sum_{m^{\\prime}}\\alpha_{m m^{\\prime}}^{12}\\mathbf{s}_{m^{\\prime}}^{1}\\quad\\sum_{n}\\alpha_{m n}^{2}\\mathbf{h}_{n}\\right]}\\\\ {\\mathbf{s}_{m}^{2}=\\operatorname*{det}\\mathbf{c}^{2}(\\mathbf{s}_{m-1}^{2},\\mathbf{c}_{m}^{2},\\mathbf{y}_{m-1}^{2})}\\\\ {P(\\mathbf{y}_{m}^{2})=\\mathrm{softmax}(\\mathbf{s}_{m}^{2}).}\\end{array}}\\end{array}\n$$\n ", "text_format": "latex", "page_idx": 1, "bbox": [338, 179, 496, 236], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 27, "type": "text", "text": "Note that the context vectors resulting from the two attentions are concatenated, not added. ", "page_idx": 1, "bbox": [307, 244.9709014892578, 525, 271.6653747558594], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 28, "type": "text", "text": "3 Learning and Inference ", "text_level": 1, "page_idx": 1, "bbox": [306, 283, 446, 296], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 29, "type": "text", "text": "For compactness, we will write    $\\mathbf{X}$   for the matrix whose rows are the    ${\\bf x}_{n}$  , and similarly    $\\mathbf{H},\\,\\mathbf{C}$  , and so on. We also write  A  for the matrix of attention weights:   $[\\mathbf{A}]_{i j}=\\alpha_{i j}$  . ", "page_idx": 1, "bbox": [307, 303.3041687011719, 525, 358], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 30, "type": "text", "text": "Let  $\\theta$   be the parameters of our model, which we train on sentence triples   $({\\bf X},{\\bf Y}^{1},{\\bf Y}^{2})$  . ", "page_idx": 1, "bbox": [307, 357.9779052734375, 525, 384.6723937988281], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 31, "type": "text", "text": "3.1 Maximum likelihood estimation ", "text_level": 1, "page_idx": 1, "bbox": [306, 394, 481, 407], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 32, "type": "text", "text": "Deﬁne the score of a sentence triple to be a log- linear interpolation of the two decoders’ probabil- ities: ", "page_idx": 1, "bbox": [307, 411.8459167480469, 525, 452.08941650390625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 33, "type": "equation", "text": "\n$$\n\\begin{array}{r}{\\begin{array}{l}{\\mathrm{score}(\\mathbf{Y}^{1},\\mathbf{Y}^{2}\\mid\\mathbf{X};\\theta)=\\lambda\\log P(\\mathbf{Y}^{1}\\mid\\mathbf{X};\\theta)\\;+}\\\\ {(1-\\lambda)\\log P(\\mathbf{Y}^{2}\\mid\\mathbf{X},\\mathbf{S}^{1};\\theta)}\\end{array}}\\end{array}\n$$\n ", "text_format": "latex", "page_idx": 1, "bbox": [308, 460, 525, 497], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 34, "type": "text", "text": "where    $\\lambda$   is a parameter that controls the impor- tance of each sub-task. In all our experiments, we set    $\\lambda$   to 0 . 5. We then train the model to maximize ", "page_idx": 1, "bbox": [307, 506.3509521484375, 525, 546.5944213867188], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 35, "type": "equation", "text": "\n$$\n\\mathcal{L}(\\boldsymbol{\\theta})=\\sum\\mathrm{score}(\\mathbf{Y}^{1},\\mathbf{Y}^{2}\\mid\\mathbf{X};\\boldsymbol{\\theta}),\n$$\n ", "text_format": "latex", "page_idx": 1, "bbox": [345, 553, 486, 576], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 36, "type": "text", "text": "where the summation is over all sentence triples in the training data. ", "page_idx": 1, "bbox": [307, 582.968994140625, 525, 609.6634521484375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 37, "type": "text", "text": "3.2 Regularization ", "text_level": 1, "page_idx": 1, "bbox": [307, 619, 402, 632], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 38, "type": "text", "text": "We can optionally add a regularization term to the objective function, in order to encourage our atten- tion mechanisms to conform to two intuitive prin- ciples of machine translation:  transitivity  and  in- vertibility . ", "page_idx": 1, "bbox": [307, 636.8369750976562, 525, 704.179443359375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 39, "type": "text", "text": "Transitivity  attention regularizer To a ﬁrst ap- proximation, the translation relation should be transitive ( Wang et al. ,  2006 ;  Levinboim and Chi- ang ,  2015 ): If source word  $\\mathbf{X}_{i}$   aligns to target word ", "page_idx": 1, "bbox": [307, 711.8006591796875, 525, 767.6663818359375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 40, "type": "image", "page_idx": 2, "img_path": "layout_images/N18-1008_0.jpg", "img_caption": "Figure 1: Variations on the standard attentional model. In the standard  single-task  model, the decoder attends to the encoder’s states. In a typical  multitask  setup, two decoders attend to the encoder’s states. In the  cascade  ( Tu et al. , 2017 ), the second decoder attends to the ﬁrst decoder’s states. In our proposed  triangle  model, the second decoder attends to both the encoder’s states and the ﬁrst decoder’s states. Note that for clarity’s sake there are dependencies not shown. ", "bbox": [71, 62, 527, 311], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "Pi ++ Ym)\n\nT softmax\nSi---S\n\nT decoder\nety\n\nT attention\nhy ---hy\n\nTencoder\nXx\n\n(a) single-task\n\nPU *Vig) PY, Yep)\n\nT softmax T softmax\n1 1 Pears\nBre Sul Sesh\nT decoder T decoder\n1 1 2 2\nCr Sit Se\nattention ——_-—“attention\nhy ---hy\nT encoder\n\nX,-0Xy\n\n(b) multitask\n\nPOL Yip)\nT softmax\nT decoder\n\nPUY Yip) Ch Ge\n\nT softmax x attention\ncare\nae\n\nT decoder\n\n1 Sy\n\nattention ——_\n\nhy ---hy\nTencoder\nXo Xy\n\n(c) cascade\n\nPUT Yio)\n\nT softmax\nDanae\nSpo Sie\nT decoder\n1 1 2 2\nPoly!) Gee,\nToottmax attentions\n1 1\nSia eu!\nT decoder\n1 1\nCo Sat\nattention —_\nhy --- hy\nTencoder\nXp- Xy\n\n(d) triangle\n", "vlm_text": "The image provides a comparison of different variations on the standard attentional model used in sequence-to-sequence tasks. Here's a breakdown of each model shown in the image:\n\n1. **(a) Single-task**: \n   - This is the standard model where a single decoder attends to the states of an encoder. \n   - The process follows this order: input sequence (`x_1 ... x_N`) is encoded into hidden states (`h_1 ... h_N`), an attention mechanism produces context vectors (`c_1 ... c_M`) which inform the decoder states (`s_1 ... s_M`), and a softmax function is applied for output prediction (`P(y_1 ... y_M)`).\n\n2. **(b) Multitask**:\n   - In this setup, the model possesses two decoders that both attend to the states of the same encoder. \n   - Similarly, inputs are encoded into hidden states, and each decoder has its own attention mechanism, context vectors, and predictions.\n\n3. **(c) Cascade** (referenced from Tu et al., 2017):\n   - Here, the second decoder does not directly attend to the encoder states. Instead, it attends to the states of the first decoder.\n   - This approach creates a sequential setup where the output of the first decoder informs the second decoder, resulting in a cascading attention.\n\n4. **(d) Triangle**:\n   - The innovative aspect of the triangle model is that the second decoder attends to both the encoder’s states and the first decoder’s states.\n   - This dual-attention setup might allow for more nuanced and context-aware predictions, as the second decoder is directly influenced by both prior stages.\n\nEach model variation utilizes an attention mechanism to align and focus on different parts of input sequences, enhancing the performance of tasks like machine translation or summarization."}
{"layout": 41, "type": "text", "text": " $\\mathbf{y}_{j}^{1}$    and  $\\mathbf{y}_{j}^{1}$    aligns to target word  $\\mathbf{y}_{k}^{2}$  , then  $\\mathbf{X}_{i}$   should also probably align to  $\\mathbf{y}_{k}^{2}$  . To encourage the model to preserve this relationship, we add the following transitivity  regularizer to the loss function of the triangle  models with a small weight  $\\lambda_{\\mathrm{trans}}=0.2$  : ", "page_idx": 2, "bbox": [71, 332, 290, 402.5356750488281], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 42, "type": "equation", "text": "\n$$\n\\mathcal{L}_{\\mathrm{trans}}=\\mathrm{score}(\\mathbf{Y}^{1},\\mathbf{Y}^{2})-\\lambda_{\\mathrm{trans}}\\left\\|\\mathbf{A}^{12}\\mathbf{A}^{1}-\\mathbf{A}^{2}\\right\\|_{2}^{2}\\!.\n$$\n ", "text_format": "latex", "page_idx": 2, "bbox": [78, 408, 283, 427], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 43, "type": "text", "text": "Invertibility  attention regularizer The transla- tion relation also ought to be roughly invertible ( Levinboim et al. ,  2015 ): if, in the  reconstruc- tion  version of the  cascade  model, source word  $\\mathbf{X}_{i}$   aligns to target word    $\\mathbf{y}_{j}^{1}$  , then it stands to rea- son that  $\\mathbf{y}_{j}$   is likely to align to    $\\mathbf{X}_{i}$  . So, whereas  Tu et al.  ( 2017 ) let the attentions of the translator and the reconstructor be unrelated, we try adding the following  invertibility  regularizer to encourage the attentions to each be the inverse of the other, again with a weight  $\\lambda_{\\mathrm{inv}}=0.2$  : ", "page_idx": 2, "bbox": [71, 433.00384521484375, 290, 582.0776977539062], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 44, "type": "equation", "text": "\n$$\n\\mathcal{L}_{\\mathrm{inv}}=\\mathrm{score}(\\mathbf{Y}^{1},\\mathbf{Y}^{2})-\\lambda_{\\mathrm{inv}}\\left\\|\\mathbf{A}^{1}\\mathbf{A}^{12}-\\mathbf{I}\\right\\|_{2}^{2}.\n$$\n ", "text_format": "latex", "page_idx": 2, "bbox": [88, 587, 274, 607], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 45, "type": "text", "text": "3.3 Decoding ", "text_level": 1, "page_idx": 2, "bbox": [71, 613, 141, 626], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 46, "type": "text", "text": "Since we have two decoders, we now need to em- ploy a two-phase beam search, following  Tu et al. ( 2017 ): ", "page_idx": 2, "bbox": [71, 630.458251953125, 290, 670.70166015625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 47, "type": "text", "text": "1. The  ﬁrst decoder  produces, through standard beam search, a set of triples each consist- ing of a candidate transcription  $\\hat{\\mathbf{Y}}^{1}$  , a score  $P(\\hat{\\mathbf{Y}}^{1})$  ), and a hidden state sequence  $\\hat{\\mathbf{S}}$  . 2. For each transcription candidate from the  ﬁrst decoder , the  second decoder  now produces ", "page_idx": 2, "bbox": [80, 677.2872314453125, 290, 766.0307006835938], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 48, "type": "table", "page_idx": 2, "img_path": "layout_images/N18-1008_1.jpg", "table_footnote": "Table 1: Statistics on our speech datasets. ", "bbox": [306, 330, 527, 419], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "Corpus Speakers Segments Hours\nAinu-English 1 2,668 2.5\n\nMboshi-French 3 5,131 4.4\nSpanish-English 240 17,394 20\n\n", "vlm_text": "The table lists information about three different language corpora: Ainu-English, Mboshi-French, and Spanish-English. Each row provides details on the number of speakers, segments, and duration in hours for each corpus. Specifically:\n\n- **Ainu-English**: This corpus has 1 speaker, 2,668 segments, and comprises 2.5 hours.\n- **Mboshi-French**: This corpus features 3 speakers, 5,131 segments, and spans 4.4 hours.\n- **Spanish-English**: This corpus includes 240 speakers, 17,394 segments, and totals 20 hours."}
{"layout": 49, "type": "text", "text": "through beam search a set of candidate trans- lations  $\\hat{\\mathbf{Y}}^{2}$  , each with a score    $P(\\hat{\\mathbf{Y}}^{2})$  ). 3. We then output the combination that yields the highest total score  $(\\mathbf{Y}^{1},\\mathbf{Y}^{2})$  . ", "page_idx": 2, "bbox": [315.9320373535156, 441.23101806640625, 525, 503.9494934082031], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 50, "type": "text", "text": "3.4 Implementation ", "text_level": 1, "page_idx": 2, "bbox": [307, 513, 407, 526], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 51, "type": "text", "text": "All our models are implemented in DyNet ( Neubig et al. ,  2017 ).   We use a dropout of 0.2, and train using Adam with initial learning rate of 0 . 0002 for a maximum of 500 epochs. For testing, we select the model with the best performance on dev. At inference time, we use a beam size of 4 for each decoder (due to GPU memory constraints), and the beam scores include length normalization ( Wu et al. ,  2016 ) with a weight of 0.8, which  Nguyen and Chiang  ( 2017 ) found to work well for low- resource NMT. ", "page_idx": 2, "bbox": [307, 530.6920166015625, 525, 679.3294677734375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 52, "type": "text", "text": "4 Speech Transcription and Translation ", "text_level": 1, "page_idx": 2, "bbox": [306, 690, 519, 703], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 53, "type": "text", "text": "We focus on speech transcription and translation of endangered languages, using three di ﬀ erent cor- ", "page_idx": 2, "bbox": [307, 710.822021484375, 525, 737.5164794921875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 54, "type": "table", "page_idx": 3, "img_path": "layout_images/N18-1008_2.jpg", "table_footnote": "Table 2: The multitask models outperform the baseline single-task model and the pivot approach (auto / text) on all language pairs tested. The  triangle  model also outperforms the simple multitask models on both tasks in almost all cases. The best results for each dataset and task are highlighted. ", "bbox": [71, 61, 526, 286], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "Model Search Mboshi French | Ainu English | Spanish English\nASR MT ASR MT CER BLEU | CER BLEU CER BLEU\n\n(1) | auto text 1-best 1-best | 42.3 21.4 | 44.0 16.4 63.2 24.2\n(2) | gold text —_— 1-best 0.0 31.2 0.0 19.3 0.0 51.3\n(3) single-task 1-best _ 20.8 _ 12.0 _ 21.6\n(4) multitask 4-best 1-best | 36.9 21.0 | 40.1 18.3 57.4 26.0\n\n(5) cascade 4-best 1-best | 39.7 24.3, | 42.1 19.8 58.1 26.8\n\n(6) triangle 4-best 1-best | 32.3 24.1 39.9 19.2 58.9 28.6\n(7) | triangle+Lirans | 4-best 1-best | 33.0 24.7 | 43.3 20.2 59.3 28.6\n\n(8) triangle l-best 1-best | 31.8 19.7 | 38.9 19.8 58.4 28.8\n(9) | triangle+Lirans | 1-best 1-best | 32.1 20.9 | 43.0 20.3 59.1 28.5\n\n", "vlm_text": "The table presents a comparison of different models evaluated on a set of metrics for various languages. Here's a breakdown of the table's content:\n\n1. **Columns:**\n   - \"Model\": Lists different models being compared.\n     - \"ASR\" and \"MT\": Indicate the components related to Automatic Speech Recognition (ASR) and Machine Translation (MT) within the model.\n   - \"Search\": Specifies the search strategies used for ASR and MT: \"1-best\" or \"4-best\".\n   - Performance metrics for different language pairs:\n     - \"Mboshi CER\" (Character Error Rate)\n     - \"French BLEU\" (Bilingual Evaluation Understudy Score)\n     - \"Ainu CER\"\n     - \"English BLEU\"\n     - \"Spanish CER\"\n     - \"English BLEU\"\n\n2. **Rows:**\n   - Model types include variations like \"auto\", \"gold\", \"single-task\", \"multitask\", \"cascade\", \"triangle\", and \"triangle+L_trans\".\n   - Each row provides the CER or BLEU scores achieved by these models for different language tasks. Scores in bold indicate presumably the best performance for a specific metric across the models.\n\n3. **Performance:**\n   - CER measures the error rate in transcriptions.\n   - BLEU measures the quality of translated text.\n\nThe table aims to show how different model configurations and search strategies impact performance for speech recognition and translation across several languages."}
{"layout": 55, "type": "text", "text": "pora on three di ﬀ erent language directions: Span- ish (es) to English (en), Ainu (ai) to English, and Mboshi (mb) to French (fr). ", "page_idx": 3, "bbox": [72, 307.06707763671875, 290, 347.3105773925781], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 56, "type": "text", "text": "4.1 Data ", "text_level": 1, "page_idx": 3, "bbox": [71, 361, 119, 372], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 57, "type": "text", "text": "Spanish is, of course, not an endangered language, but the availability of the CALLHOME Spanish Speech dataset (LDC2014T23) with English trans- lations ( Post et al. ,  2013 ) makes it a convenient language to work with, as has been done in almost all previous work in this area. It consists of tele- phone conversations between relatives (about 20 total hours of audio) with more than 240 speak- ers. We use the original train-dev-test split, with the training set comprised of 80 conversations and dev and test of 20 conversations each. ", "page_idx": 3, "bbox": [72, 379.35211181640625, 290, 527.9896240234375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 58, "type": "text", "text": "Hokkaido Ainu is the sole surviving member of the Ainu language family and is generally consid- ered a language isolate. As of 2007, only ten native speakers were alive. The Glossed Audio Corpus of Ainu Folklore provides 10 narratives with au- dio (about 2.5 hours of audio) and translations in Japanese and English.   Since there does not exist a standard train-dev-test split, we employ a cross validation scheme for evaluation purposes. In each fold, one of the 10 narratives becomes the test set, with the previous one (mod 10) becoming the dev set, and the remaining 8 narratives becoming the training set. The models for each of the 10 folds are trained and tested separately. On average, for each fold, we train on about 2000 utterances; the dev and test sets consist of about 270 utterances. ", "page_idx": 3, "bbox": [72, 529.3711547851562, 290, 745.754638671875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 59, "type": "text", "text": "We report results on the concatenation of all folds. The Ainu text is split into characters, except for the equals  $(=)$   and underscore ( ) characters, which are used as phonological or structural markers and are thus merged with the following character. ", "page_idx": 3, "bbox": [307, 307.0670166015625, 525, 374.40948486328125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 60, "type": "text", "text": "Mboshi (Bantu C25 in the Guthrie classiﬁca- tion) is a language spoken in Congo-Brazzaville, without standard orthography. We use a corpus\n\n ( Godard et al. ,  2017 ) of 5517 parallel utterances\n\n (about 4.4 hours of audio) collected from three na- tive speakers. The corpus provides non-standard grapheme transcriptions (close to the language phonology) produced by linguists, as well as French translations. We sampled 100 segments from the training set to be our dev set, and used the original dev set (514 sentences) as our test set. ", "page_idx": 3, "bbox": [307, 375.9200134277344, 525, 524.5574951171875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 61, "type": "text", "text": "4.2 Implementation ", "text_level": 1, "page_idx": 3, "bbox": [307, 538, 407, 551], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 62, "type": "text", "text": "We employ a 3-layer speech encoding scheme similar to that of  Duong et al.  ( 2016 ). The ﬁrst bidirectional layer receives the audio sequence in the form of 39-dimensional Perceptual Linear Pre- dictive (PLP) features ( Hermansky ,  1990 ) com- puted over overlapping 25ms-wide windows ev- ery   $10\\mathrm{ms}$  . The second and third layers consist of LSTMs with hidden state sizes of 128 and 512 re- spectively. Each layer encodes every second out- put of the previous layer. Thus, the sequence is downsampled by a factor of 4, decreasing the com- putation load for the attention mechanism and the decoders. In the speech experiments, the decoders output the sequences at the grapheme level, so the output embedding size is set to 64. ", "page_idx": 3, "bbox": [307, 557.30908203125, 525, 733.0444946289062], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 63, "type": "text", "text": "", "page_idx": 4, "bbox": [72, 63.68604278564453, 290, 90.38150024414062], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 64, "type": "text", "text": "We found that this simpler speech encoder works well for our extremely small datasets. Ap- plying our models to larger datasets with many more speakers would most likely require a more sophisticated speech encoder, such as the one used by  Weiss et al.  ( 2017 ). ", "page_idx": 4, "bbox": [72, 92.11005401611328, 290, 173.00155639648438], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 65, "type": "text", "text": "4.3 Results ", "text_level": 1, "page_idx": 4, "bbox": [71, 187, 131, 199], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 66, "type": "text", "text": "In Table  2 , we present results on three small datasets that demonstrate the e ﬃ cacy of our mod- els. We compare our proposed models against three baselines and one “skyline.” The ﬁrst base- line is a traditional pivot approach (line 1), where the ASR output, a sequence of characters, is the input to a character-based NMT system (trained on gold transcriptions). The “skyline” model (line 2) is the same NMT system, but tested on gold transcriptions instead of ASR output. The second baseline is translation directly from source speech to target text (line 3). The last baseline is the stan- dard  multitask  model (line 4), which is similar to the model of  Weiss et al.  ( 2017 ). ", "page_idx": 4, "bbox": [72, 206.94105529785156, 290, 396.2256164550781], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 67, "type": "text", "text": "The  cascade  model (line 5) outperforms the baselines on the translation task, while only falling behind the  multitask  model in the transcription task. On all three datasets, the  triangle  model (lines 6, 7) outperforms all baselines, including the standard  multitask  model. On Ainu-English, we even obtain translations that are comparable to the “skyline” model, which is tested on gold Ainu transcriptions. ", "page_idx": 4, "bbox": [72, 397.95513916015625, 290, 519.49365234375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 68, "type": "text", "text": "Comparing the performance of all models across the three datasets, there are two notable trends that verify common intuitions regarding the speech transcription and translation tasks. First, an increase in the number of speakers hurts the per- formance of the speech transcription tasks. The character error rates for Ainu are smaller than the CER in Mboshi, which in turn are smaller than the CER in CALLHOME. Second, the character-level BLEU scores increase as the amount of training data increases, with our smallest dataset (Ainu) having the lowest BLEU scores, and the largest dataset (CALLHOME) having the highest BLEU scores. This is expected, as more training data means that the translation decoder learns a more informed character-level language model for the target language. ", "page_idx": 4, "bbox": [72, 521.2232055664062, 290, 751.1556396484375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 69, "type": "text", "text": "Note that  Weiss et al.  ( 2017 ) report much higher BLEU scores on CALLHOME: our model un- derperforms theirs by almost 9  word-level  BLEU points. However, their model has signiﬁcantly more parameters and is trained on 10 times more data than ours. Such an amount of data would never be available in our endangered lan- guages scenario. When calculated on the word- level, all our models’ BLEU scores are between 3 and 7 points for the extremely low resource datasets (Mboshi-French and Ainu-English), and between 7 and 10 for CALLHOME. Clearly, the size of the training data in our experiments is not enough for producing high quality speech transla- tions, but we plan to investigate the performance of our proposed models on larger datasets as part of our future work. ", "page_idx": 4, "bbox": [82, 752.8851928710938, 290, 766.0306396484375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 70, "type": "text", "text": "", "page_idx": 4, "bbox": [307, 63.68622589111328, 525, 280.0697937011719], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 71, "type": "text", "text": "To evaluate the e ﬀ ect of using the combined score from both decoders at decoding time, we evaluated the  triangle  models using only the 1-best output from the speech model (lines 8, 9). One would expect that this would favor speech at the expense of translation. In transcription accu- racy, we indeed observed improvements across the board. In translation accuracy, we observed a sur- prisingly large drop on Mboshi-French, but sur- prisingly little e ﬀ ect on the other language pairs – in fact, BLEU scores tended to go up slightly, but not signiﬁcantly. ", "page_idx": 4, "bbox": [307, 281.3353271484375, 525, 443.5218811035156], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 72, "type": "text", "text": "Finally, Figure  2  visualizes the attention ma- trices for one utterance from the baseline multi- task model and our proposed  triangle  model. It is clear that our intuition was correct: the transla- tion decoder receives most of its context from the transcription decoder, as indicated by the higher attention weights of    $\\mathbf{A}^{12}$  . Ideally, the area under the red squares (gold alignments) would account for   $100\\%$   of the attention mass of    $\\mathbf{A}^{12}$  . In our tri- angle model, the total mass under the red squares is  $34\\%$  , whereas the multitask model’s correct at- tentions amount to only  $21\\%$   of the attention mass. ", "page_idx": 4, "bbox": [307, 444.78741455078125, 525, 606.9749145507812], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 73, "type": "text", "text": "5 Word Discovery ", "text_level": 1, "page_idx": 4, "bbox": [306, 620, 408, 634], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 74, "type": "text", "text": "Although the above results show that our model gives large performance improvements, in abso- lute terms, its performance on such low-resource tasks leaves a lot of room for future improvement. A possible more realistic application of our meth- ods is word discovery, that is, ﬁnding word bound- aries in unsegmented phonetic transcriptions. ", "page_idx": 4, "bbox": [307, 643.6294555664062, 525, 738.0698852539062], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 75, "type": "text", "text": "After training an attentional encoder-decoder model between Mboshi unsegmented phonetic se- ", "page_idx": 4, "bbox": [307, 739.33544921875, 525, 766.0299072265625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 76, "type": "image", "page_idx": 5, "img_path": "layout_images/N18-1008_3.jpg", "img_caption": "Figure 2: Attentions in an Mboshi-French sentence, extracted from two of our models. The red squares denote gold alignments. The second decoder of the  triangle  model receives most of its context from the ﬁrst decoder through  $\\mathbf{A}^{12}$    instead of the source. The  $\\mathbf{A}^{2}$    matrix of the  triangle  model is more informed (  $34\\%$   correct attention mass) than the  multitask  one (  $21\\%$   correct), due to the  transitivity  regularizer. ", "bbox": [71, 58, 527, 399], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "Limqgawmsmxow ;ebu\n\nAZ\n\n- — |\n° =\nE\n3\n°\n]\nc\na\ng =\n“i\ncs ia\nol\no\n<\n\n|\n|\n\n(a) multitask\n\n{\n\nHIM agumsmxow ye bu\n\nAZ\n\na4ngl el @ assaiq sins aw af\n\n(b) triangle + transitivity\n\nA\n\nngsimokosamébali\n\n:\n", "vlm_text": "This image visually represents the attention mechanisms in machine learning models dealing with a translation task between the Mboshi and French languages. It is divided into sections showing two models: (a) multitask and (b) triangle with transitivity regularizer. Each section further includes matrices labeled \\( \\mathbf{A}^1 \\) and \\( \\mathbf{A}^2 \\), representing attention weights at different stages of processing, and \\( \\mathbf{A}^{12} \\) for the triangle model.\n\n1. **\\( \\mathbf{A}^1 \\) Matrices**: These matrices depict the initial attention layer for both models, showing alignments between input source tokens (Mboshi) and initial processing steps.\n2. **\\( \\mathbf{A}^2 \\) Matrices**: These matrices depict the attention layer related to the decoder or next stage where the French sentences align with either the Mboshi inputs or previous processing outputs.\n3. **\\( \\mathbf{A}^{12} \\) Matrix** (Right of the triangle model): This matrix illustrates how much attention in the second decoder of the triangle model is derived from the first decoder instead of directly from the source. It shows connections between the outputs of the first layer of decoders and the subsequent decoder.\n4. **Red Squares**: These denote the 'gold' alignments which are the correct or target alignments for the translation task.\n5. **Comparison**: The attention distribution effectiveness between the multitask and triangle models, with triangle + transitivity showing a more \"informed\" attention aligning more accurately by a percentage, presumably improving translation or understanding.\n\nThe waveform below each section relates to the speech or verbal input being translated, likely representing the acoustic features tied into the multitask learning model."}
{"layout": 77, "type": "text", "text": "quences and French word sequences, the atten- tion weights can be thought of as soft alignments, which allow us to project the French word bound- aries onto Mboshi. Although we could in princi- ple perform word discovery directly on speech, we leave this for future work, and only explore single- task and reconstruction models. ", "page_idx": 5, "bbox": [72, 421.052001953125, 290, 515.4924926757812], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 78, "type": "text", "text": "5.1 Data ", "text_level": 1, "page_idx": 5, "bbox": [71, 528, 120, 539], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 79, "type": "text", "text": "We use the same Mboshi-French corpus as in Sec- tion  4 , but with the original training set of 4617 utterances and the dev set of 514 utterances. Our parallel data consist of the unsegmented phonetic Mboshi transcriptions, along with the word-level French translations. ", "page_idx": 5, "bbox": [72, 546.321044921875, 290, 627.2125244140625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 80, "type": "text", "text": "5.2 Implementation ", "text_level": 1, "page_idx": 5, "bbox": [71, 640, 171, 651], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 81, "type": "text", "text": "We ﬁrst replicate the model of  Boito et al.  ( 2017 ), with a single-layer bidirectional encoder and sin- gle layer decoder, using an embedding and hidden size of 12 for the base model, and an embedding and hidden state size of 64 for the reverse model. In our own models, we set the embedding size to 32 for Mboshi characters, 64 for French words, and the hidden state size to 64. We smooth the at- tention weights  A  using the method of  Duong et al. ( 2016 ) with a temperature    $T=10$   for the softmax computation of the attention mechanism. ", "page_idx": 5, "bbox": [72, 658.0401000976562, 290, 766.030517578125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 82, "type": "text", "text": "", "page_idx": 5, "bbox": [307, 420.6593322753906, 526, 461.2955627441406], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 83, "type": "text", "text": "Following  Boito et al.  ( 2017 ), we train mod- els both on the  base  Mboshi-to-French direction, as well as the  reverse  (French-to-Mboshi) direc- tion, with and without this smoothing operation. We further smooth the computed soft alignments of all models so that    $a_{m n}=(a_{m n-1}\\!+\\!a_{m n}\\!+\\!a_{m n+1})/3$  as a post-processing step. From the  single-task models we extract the    $\\mathbf{A}^{1}$    attention matrices. We also train  reconstruction  models on both direc- tions, with and without the  invertibility  regularizer, extracting both  $\\mathbf{A}^{1}$    and  $\\mathbf{A}^{12}$    matrices. The two ma- trices are then combined so that  $\\mathbf{A}=\\mathbf{A}^{1}+(\\mathbf{A}^{12})^{T}$  . ", "page_idx": 5, "bbox": [307, 462.8520812988281, 526, 625.03955078125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 84, "type": "text", "text": "5.3 Results ", "text_level": 1, "page_idx": 5, "bbox": [307, 638, 366, 650], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 85, "type": "text", "text": "Evaluation is done both at the token and the type level, by computing precision, recall, and F- score over the discovered segmentation, with the best results shown in Table  3 . We reimplemented the base (Mboshi-French) and reverse (French- Mboshi) models from  Boito et al.  ( 2017 ), and the performance of the base model was comparable to the one reported. However, we were unable to ", "page_idx": 5, "bbox": [307, 658.0401000976562, 526, 766.0305786132812], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 86, "type": "table", "page_idx": 6, "img_path": "layout_images/N18-1008_4.jpg", "table_footnote": "Table 3: The reconstruction model with the  invertibility  regularizer produces more informed attentions that result in better word discovery for Mboshi with an Mboshi-French model. Scores reported by previous work are in  italics and best scores from our experiments are in  bold . ", "bbox": [71, 62, 526, 272], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": ": : Tokens Types\n\nModel (with smoothing) Precision Recall F-score | Precision Recall F-score\nBoito et al. 2017 base 5.85 6.82 6.30 6.76 15.00 9.32\n(reported) reverse 21.44 16.49 18.64 27.23 15.02 19.36\nBoito et al. 2017 base 6.87 6.33 6.59 6.17 13.02 8.37\n(reimplementation) — reverse 7.58 8.16 7.86 9.22 11.97 10.42\nour sinele-task base 7.99 tSd 7.78 7.59 16.41 10.38\n\n8 reverse 11.31 11.82 11.56 9.29 14.75 11.40\nreconstruction + 0.2Liny 8.93 9.78 9.33 8.66 15.48 11.02\nreconstruction + 0.5Liny 7.42 10.00 8.52 10.46 16.36 12.76\n", "vlm_text": "The table presents an evaluation of different models with smoothing in terms of their precision, recall, and F-score for both tokens and types. The models compared are from Boito et al. 2017 (both reported and reimplemented versions) and a single-task model from the authors of the table. Additionally, the table evaluates models that use reconstruction with two different levels of an inverse loss function (denoted as \\( \\mathcal{L}_{inv} \\)).\n\n1. **Columns:**\n   - The table is divided into two main sections: Tokens and Types.\n   - Each section evaluates three metrics: Precision, Recall, and F-score.\n\n2. **Rows:**\n   - The first two rows present the results from Boito et al. 2017. The \"base\" and \"reverse\" configurations are compared, with the reverse model generally performing better.\n   - The next two rows show reimplementation results of Boito et al. 2017, indicating similar trends but with slight differences in metrics.\n   - \"Our single-task\" row displays the performance of the authors' own model, where the reverse configuration significantly improves metrics in both tokens and types compared to the base version.\n   - The last two rows assess models with reconstruction and varying levels of inverse loss. Performance varies, with an inverse loss of 0.5 achieving the highest F-scores in both tokens and types categories.\n\nThe table demonstrates comparative model performance, emphasizing the impact of different configurations and additional loss functions on the evaluation metrics."}
{"layout": 87, "type": "text", "text": "reproduce the signiﬁcant gains that were reported when using the reverse model ( italicized  in Ta- ble  3 ). Also, our version of both the base and re- verse singletask models performed better than our re implementation of the baseline. ", "page_idx": 6, "bbox": [72, 293.51806640625, 290, 360.8595886230469], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 88, "type": "text", "text": "Furthermore, we found that we were able to obtain even better performance at the type level by combining the attention matrices of a recon- struction model trained with the  invertibility  reg- ularizer.  Boito et al.  ( 2017 ) reported that combin- ing the attention matrices of a base and a reverse model signiﬁcantly reduced performance, but they trained the two models separately. In contrast, we obtain the base   $(\\mathbf{A}^{1})$   and the reverse attention ma- trices   $(\\mathbf{A}^{12})$   from a model that trains them jointly, while also tying them together through the  invert- ibility  regularizer. Using the regularizer is key to the improvements; in fact, we did not observe any improvements when we trained the reconstruction models without the regularizer. ", "page_idx": 6, "bbox": [72, 361.6990966796875, 290, 564.5336303710938], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 89, "type": "text", "text": "6 Negative Results: High-Resource Text Translation ", "text_level": 1, "page_idx": 6, "bbox": [71, 576, 283, 603], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 90, "type": "text", "text": "6.1 Data ", "text_level": 1, "page_idx": 6, "bbox": [71, 613, 119, 625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 91, "type": "text", "text": "For evaluating our models on text translation, we use the Europarl corpus which provides parallel sentences across several European languages. We extracted 1,450,890 three-way parallel sentences on English, French, and German. The concatena- tion of the newstest 2011–2013 sets (8,017 sen- tences) is our dev set, and our test set is the con- catenation of the newstest 2014 and 2015 sets (6,003 sentences). We test all architectures on the six possible translation directions between English (en), French (fr) and German (de). All the se- quences are represented by subword units with byte-pair encoding (BPE) ( Sennrich et al. ,  2016 ) trained on each language with 32000 operations. ", "page_idx": 6, "bbox": [72, 630.9421997070312, 290, 766.0306396484375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 92, "type": "text", "text": "", "page_idx": 6, "bbox": [307, 293.5181884765625, 525, 347.3106994628906], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 93, "type": "text", "text": "6.2 Experimental Setup ", "text_level": 1, "page_idx": 6, "bbox": [306, 358, 426, 370], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 94, "type": "text", "text": "On all experiments, the encoder and the decoder(s) have 2 layers of LSTM units with hidden state size and attention size of 1024, and embedding size of 1024. For this high resource scenario, we only train for a maximum of 40 epochs. ", "page_idx": 6, "bbox": [307, 374.14923095703125, 525, 441.4917297363281], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 95, "type": "text", "text": "6.3 Results ", "text_level": 1, "page_idx": 6, "bbox": [306, 451, 366, 463], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 96, "type": "text", "text": "The accuracy of all the models on all six lan- guage pair directions is shown in Table  4 . In all cases, the best models are the baseline single-task or simple multitask models. There are some in- stances, such as English-German, where the  re- construction  or the  triangle  models are not statis- tically signiﬁcantly di ﬀ erent from the best model. The reason for this, we believe, is that in the case of text translation between so linguistically close languages, the lower level representations (the out- put of the encoder) provide as much information as the higher level ones, without the search errors that are introduced during inference. ", "page_idx": 6, "bbox": [307, 468.3292541503906, 525, 644.0657348632812], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 97, "type": "text", "text": "A notable outcome of this experiment is that we do not observe the signiﬁcant improvements with the  reconstruction  models that  Tu et al.  ( 2017 ) ob- served. A few possible di ﬀ erences between our experiment and theirs are: our models are BPE- based, theirs are word-based; we use Adam for optimization, they use Adadelta; our model has slightly fewer parameters than theirs; we test on less typologically di ﬀ erent language pairs than ", "page_idx": 6, "bbox": [307, 644.4912719726562, 525, 766.0307006835938], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 98, "type": "table", "page_idx": 7, "img_path": "layout_images/N18-1008_5.jpg", "table_footnote": "Table 4: BLEU scores for each model and translation d ection    $s\\ \\rightarrow\\ t.$  . In the multitask, cascade, and triangle models,  $x$   stands for the third language, other than    $s$   and  t . In each column, the best results are highlighted. The non-highlighted results are statistically signiﬁcantly worse than the single-task baseline. ", "bbox": [71, 61, 526, 300], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "SSE\n\nModel en>fr en-de fren frode de-en de-fr\nsingletask 20.92 12.69 20.96 11.24 16.10 15.29\nmultitask s > x,t 20.54 12.79 20.01 1118 16.31 15.07\ncascade s > x > t 15.93 1131 1658 7.60 13.46 13.24\ncascade s > t 3 x 20.34 12.27 19.17 11.09 15.24 14.78\nreconstruction 20.19 12.44 20.63 10.88 15.66 13.44\nreconstruction +Liny 20.72 12.64 20.11 1046 15.43 12.64\ntriangle s 22, 1 20.39 12.70 17.93 10.17 14.94 14.07\ntriangle s 2, ¢+Luans | 20.52 12.64 1834 1042 15.22 14.37\ntriangle s >, x 20.38 1240 18.50 10.22 15.62 14.77\ntriangle s 2°, x+Lions | 20.64 12.42 19.20 10.21 15.87 14.89\n", "vlm_text": "The table presents the results of different models used for translating between several language pairs, specifically English (en), French (fr), and German (de). The table is organized with rows representing various model types and columns representing translation tasks or directions. Each cell contains a number, which likely represents a performance metric like BLEU score, indicating the quality of the translation output for that language pair and model.\n\nHere's a breakdown of what appears in the table:\n\n- **Column Headers**: \n  - `s → t`: Indicates source language `s` to target language `t`.\n  - Language pairs: `en→fr`, `en→de`, `fr→en`, `fr→de`, `de→en`, `de→fr`.\n\n- **Row Labels (Models)**:\n  - `singletask`\n  - `multitask s → x, t`\n  - `cascade s → x → t`\n  - `cascade s → t → x`\n  - `reconstruction`\n  - `reconstruction + L_{inv}`\n  - `triangle s → x → t`\n  - `triangle s → x → t + L_{trans}`\n  - `triangle s → t → x`\n  - `triangle s → t → x + L_{trans}`\n\n- **Performance Scores**: \n  - Each model's performance across different language pairs is indicated by the numerical values in each cell.\n  \n- **Bolded Values**:\n  - Certain values are bolded, perhaps indicating the best performance for that language pair among the models compared.\n\nFrom these details, it's inferred that the table aims to compare translation quality across different modeling approaches for specific language pairs."}
{"layout": 99, "type": "text", "text": "English-Chinese. ", "text_level": 1, "page_idx": 7, "bbox": [71, 321, 146, 333], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 100, "type": "text", "text": "However, we also observe that in most cases our proposed regularizers lead to increased perfor- mance. The  invertibility  regularizer aids the  recon- struction  models in achiev slightly higher BLEU scores in 3 out of the 6 cases. The  transitivity  reg- ularizer is even more e ﬀ ective: in 9 out the 12 source-target language combinations, the  triangle models achieve higher performance when trained using the regularizer. Some of them are statistical signiﬁcant improvements, as in the case of French to English where English is the intermediate target language and German is the ﬁnal target. ", "page_idx": 7, "bbox": [72, 335.31903076171875, 290, 497.5055847167969], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 101, "type": "text", "text": "7 Related Work ", "text_level": 1, "page_idx": 7, "bbox": [71, 511, 161, 525], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 102, "type": "text", "text": "The speech translation problem has been tradi- tionally approached by using the output of an ASR system as input to a MT system. For ex- ample, Ney (1999) and Matusov et al. (2005)use ASR output lattices as input to translation models, integrating speech recognition uncertainty into the translation model. Recent work has fo- cused more on modelling speech translation with- out explicit access to transcriptions.  Duong et al. ( 2016 ) introduced a sequence-to-sequence model for speech translation without transcriptions but only evaluated on alignment, while  Anastasopou- los et al.  ( 2016 ) presented an unsupervised align- ment method for speech-to-translation alignment. Bansal et al.  ( 2017 ) used an unsupervised term discovery system ( Jansen et al. ,  2010 ) to clus- ter recurring audio segments into pseudowords and translate speech using a bag-of-words model. B´ erard et al.  ( 2016 ) translated synthesized speech data using a model similar to the Listen Attend and Spell model ( Chan et al. ,  2016 ). A larger-scale study ( B´ erard et al. ,  2018 ) used an end-to-end neu- ral system system for translating audio books be- tween French and English. On a di ﬀ erent line of work,  Boito et al.  ( 2017 ) used the attentions of a sequence-to-sequence model for word discovery. ", "page_idx": 7, "bbox": [72, 536.0980834960938, 290, 766.030517578125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 103, "type": "text", "text": "", "page_idx": 7, "bbox": [307, 320.58111572265625, 525, 442.1206359863281], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 104, "type": "text", "text": "Multitask learning ( Caruana ,  1998 ) has found extensive use across several machine learning and NLP ﬁelds. For example,  Luong et al.  ( 2016 ) and Eriguchi et al.  ( 2017 ) jointly learn to parse and translate;  Kim et al.  ( 2017 ) combine CTC- and attention-based models using multitask models for speech transcription;  Dong et al.  ( 2015 ) use mul- titask learning for multiple language translation. Toshniwal et al.  ( 2017 ) apply multitask learning to neural speech recognition in a less traditional fashion: the lower-level outputs of the speech en- coder are used for ﬁne-grained auxiliary tasks such as predicting HMM states or phonemes, while the ﬁnal output of the encoder is passed to a character- level decoder. ", "page_idx": 7, "bbox": [307, 448.66314697265625, 525, 651.4976196289062], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 105, "type": "text", "text": "Our work is most similar to the work of  Weiss et al.  ( 2017 ). They used sequence-to-sequence models to transcribe Spanish speech and trans- late it in English, by jointly training the two tasks in a multitask scenario where the decoders share the encoder. In contrast to our work, they use a large corpus for training the model on roughly 163 hours of data, using the Spanish Fisher and CALL- HOME conversational speech corpora. The pa- rameter number of their model is signiﬁcantly larger than ours, as they use 8 encoder layers, and 4 layers for each decoder. This allows their model to adequately learn from such a large amount of data and deal well with speaker variation. How- ever, training such a large model on endangered language datasets would be infeasible. ", "page_idx": 7, "bbox": [307, 658.0402221679688, 525, 766.0306396484375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 106, "type": "text", "text": "", "page_idx": 8, "bbox": [72, 63.68604278564453, 290, 171.67654418945312], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 107, "type": "text", "text": "Our model also bears similarities to the archi- tecture of the model proposed by  Tu et al.  ( 2017 ). They report signiﬁcant gains in Chinese-English translation by adding an additional  reconstruction decoder that attends on the last states of the  trans- lation  decoder, mainly inspired by auto-encoders. ", "page_idx": 8, "bbox": [72, 172.5351104736328, 290, 253.42660522460938], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 108, "type": "text", "text": "8 Conclusion ", "text_level": 1, "page_idx": 8, "bbox": [71, 266, 147, 278], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 109, "type": "text", "text": "We presented a novel architecture for multitask learning that provides the second task with higher- level representations produced from the ﬁrst task decoder. Our model outperforms both the single- task models as well as traditional multitask ar- chitectures. Evaluating on extremely low-resource settings, our model improves on both speech tran- scription and translation. By augmenting our mod- els with regularizers that implement transitivity and invertibility, we obtain further improvements on all low-resource tasks. ", "page_idx": 8, "bbox": [72, 287.66412353515625, 290, 436.3016662597656], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 110, "type": "text", "text": "These results will hopefully lead to new tools for endangered language documentation. Projects like BULB aim to collect about 100 hours of audio with translations, but it may be impractical to tran- scribe this much audio for many languages. For future work, we aim to extend these methods to settings where we don’t necessarily have sentence triples, but where some audio is only transcribed and some audio is only translated. ", "page_idx": 8, "bbox": [72, 437.1601867675781, 290, 558.69970703125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 111, "type": "text", "text": "Acknowledgements This work was generously supported by NSF Award 1464553. We are grate- ful to the anonymous reviewers for their useful comments. ", "page_idx": 8, "bbox": [72, 567.6505737304688, 290, 621.835693359375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 112, "type": "text", "text": "References ", "text_level": 1, "page_idx": 8, "bbox": [71, 647, 128, 658], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 113, "type": "text", "text": "Gilles Adda, Sebastian St¨ uker, Martine Adda-Decker, Odette Ambouroue, Laurent Besacier, David Bla- chon, H´ el\\` ene Bonneau-Maynard, Pierre Godard, Fa- tima Hamlaoui, Dmitry Idiatov, et al. 2016.  Break- ing the unwritten language barrier: The BULB project .  Procedia Computer Science , 81:8–14. ", "page_idx": 8, "bbox": [72, 666.2108154296875, 290, 733.0597534179688], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 114, "type": "text", "text": "Antonios Anastasopoulos, David Chiang, and Long Duong. 2016.  An unsupervised probability model ", "page_idx": 8, "bbox": [72, 742.8008422851562, 290, 765.7647705078125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 115, "type": "text", "text": "for speech-to-translation alignment of low-resource languages . In  Proc. EMNLP . Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben- gio. 2015. Neural machine translation by jointly learning to align and translate . In  Proc. ICLR . Sameer Bansal, Herman Kamper, Adam Lopez, and Sharon Goldwater. 2017. Towards speech-to-text translation without speech recognition . In  Proc. EACL . Alexandre B´ erard, Laurent Besacier, Ali Can Ko- cabiyikoglu, and Olivier Pietquin. 2018. End-to- end automatic speech translation of audiobooks . arXiv:1802.04200. Alexandre B´ erard, Olivier Pietquin, Christophe Servan, and Laurent Besacier. 2016. Listen and translate: A proof of concept for end-to-end speech-to-text translation . In  Proc. NIPS Workshop on End-to-end Learning for Speech and Audio Processing . Steven Bird, Lauren Gawne, Katie Gelbart, and Isaac McAlister. 2014a.  Collecting bilingual audio in re- mote indigenous communities . In  Proc. COLING . Steven Bird, Florian R. Hanke, Oliver Adams, and Hae- joong Lee. 2014b.  Aikuma: A mobile app for col- laborative language documentation . In  Proc. of the 2014 Workshop on the Use of Computational Meth- ods in the Study of Endangered Languages . David Blachon, Elodie Gauthier, Laurent Besacier, Guy-No¨ el Kouarata, Martine Adda-Decker, and An- nie Rialland. 2016. Parallel speech collection for under-resourced language studies using the LIG- Aikuma mobile device app . In  Proc. SLTU (Spoken Language Technologies for Under-Resourced Lan- guages) , volume 81. Marcely Zanon Boito, Alexandre B´ erard, Aline Villav- icencio, and Laurent Besacier. 2017.  Unwritten lan- guages demand attention too! word discovery with encoder-decoder models . arXiv:1709.05631. Rich Caruana. 1998. Multitask learning. In  Learning to learn , pages 95–133. Springer. William Chan, Navdeep Jaitly, Quoc Le, and Oriol Vinyals. 2016. Listen, attend and spell: A neural network for large vocabulary conversational speech recognition. In  Proc. ICASSP , pages 4960–4964. IEEE. Daxiang Dong, Hua Wu, Wei He, Dianhai Yu, and Haifeng Wang. 2015.  Multi-task learning for mul- tiple language translation . In  Proc. ACL-IJCNLP . Long Duong, Antonios Anastasopoulos, David Chiang, Steven Bird, and Trevor Cohn. 2016.  An attentional model for speech translation without transcription . In  Proc. NAACL HLT . Akiko Eriguchi, Yoshimasa Tsuruoka, and Kyunghyun Cho. 2017.  Learning to parse and translate improves neural machine translation . In  Proc. ACL . ", "page_idx": 8, "bbox": [307, 64.56085205078125, 525, 765.7645874023438], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 116, "type": "text", "text": "P. Godard, G. Adda, M. Adda-Decker, J. Benjumea, L. Besacier, J. Cooper-Leavitt, G-N. Kouarata, L. Lamel, H. Maynard, M. Mueller, et al. 2017.  A very low resource language speech corpus for com- putational language documentation experiments . arXiv:1710.03501. Hynek Hermansky. 1990. Perceptual linear predictive (PLP) analysis of speech.  J. Acoustical Society of America , 87(4):1738–1752. Aren Jansen, Kenneth Church, and Hynek Hermansky. 2010.  Towards spoken term discovery at scale with zero resources . In  Proc. INTERSPEECH . Suyoun Kim, Takaaki Hori, and Shinji Watanabe. 2017. Joint CTC-attention based end-to-end speech recog- nition using multi-task learning . In  Proc. ICASSP . Tomer Levinboim and David Chiang. 2015.  Multi-task word alignment triangulation for low-resource lan- guages . In  Proc. NAACL HLT . Tomer Levinboim, Ashish Vaswani, and David Chiang. 2015.  Model invertibility regularization: Sequence alignment with or without parallel data . In  Proc. NAACL HLT . Minh-Thang Luong, Quoc V Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. 2016. Multi-task se- quence to sequence learning . In  Proc. ICLR . Evgeny Matusov, Stephan Kanthak, and Hermann Ney. 2005. On the integration of speech recognition and statistical machine translation. In  Ninth European Conference on Speech Communication and Technol- ogy . Graham Neubig, Chris Dyer, Yoav Goldberg, Austin Matthews, Waleed Ammar, Antonios Anastasopou- los, Miguel Ballesteros, David Chiang, Daniel Clothiaux, Trevor Cohn, et al. 2017.  DyNet: The dynamic neural network toolkit . arXiv:1701.03980. Hermann Ney. 1999. Speech translation: Coupling of recognition and translation. In  Proc. ICASSP , vol- ume 1. Toan Q. Nguyen and David Chiang. 2017. Transfer learning across low-resource related languages for neural machine translation . In  Proc. IJCNLP . Matt Post, Gaurav Kumar, Adam Lopez, Damianos Karakos, Chris Callison-Burch, and Sanjeev Khu- danpur. 2013. Improved speech-to-text transla- tion with the Fisher and Callhome Spanish-English speech translation corpus . In  Proc. IWSLT . Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016.  Neural machine translation of rare words with subword units . In  Proc. ACL . Shubham Toshniwal, Hao Tang, Liang Lu, and Karen Livescu. 2017. Multitask learning with low-level auxiliary tasks for encoder-decoder based speech recognition . In  Proc. Interspeech . ", "page_idx": 9, "bbox": [72, 64.56060791015625, 290, 765.76416015625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 117, "type": "text", "text": "Zhaopeng Tu, Yang Liu, Lifeng Shang, Xiaohua Liu, and Hang Li. 2017.  Neural machine translation with reconstruction . In  Proc. AAAI . Haifeng Wang, Hua Wu, and Zhanyi Liu. 2006.  Word alignment for languages with scarce resources using bilingual corpora of other language pairs . In  Proc. COLING / ACL , pages 874–881. Ron J. Weiss, Jan Chorowski, Navdeep Jaitly, Yonghui Wu, and Zhifeng Chen. 2017. Sequence-to- sequence models can directly transcribe foreign speech . In  Proc. INTERSPEECH . Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. 2016. Google’s neural machine translation system: Bridging the gap between human and machine translation . arXiv:1609.08144. ", "page_idx": 9, "bbox": [307, 64.56024169921875, 525, 278.80694580078125], "page_size": [595.2760009765625, 841.8900146484375]}
