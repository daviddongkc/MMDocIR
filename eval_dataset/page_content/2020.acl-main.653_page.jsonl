{"page": 0, "image_path": "doc_images/2020.acl-main.653_0.jpg", "ocr_text": "MLQA: Evaluating Cross-lingual Extractive Question Answering\n\nPatrick Lewis‘' Barlas Oguz’ Ruty Rinott” Sebastian Riedel“ Holger Schwenk”\n\n“Facebook AI Research\n\nUniversity College London\n\n{plewis, barlaso, ruty, sriedel, schwenk}@ flb.com\n\nAbstract\n\nQuestion answering (QA) models have shown\nrapid progress enabled by the availability of\nlarge, high-quality benchmark datasets. Such\nannotated datasets are difficult and costly to\ncollect, and rarely exist in languages other\nthan English, making building QA systems\nthat work well in other languages challeng-\ning. In order to develop such systems, it is\ncrucial to invest in high quality multilingual\nevaluation benchmarks to measure progress.\nWe present MLQA, a multi-way aligned ex-\ntractive QA evaluation benchmark intended to\nspur research in this area.! MLQA contains\nQA instances in 7 languages, English, Ara-\nbic, German, Spanish, Hindi, Vietnamese and\nSimplified Chinese. MLQA has over 12K in-\nstances in English and 5K in each other lan-\nguage, with each instance parallel between\n4 languages on average. We evaluate state-\nof-the-art cross-lingual models and machine-\ntranslation-based baselines on MLQA. In all\ncases, transfer results are significantly behind\ntraining-language performance.\n\n1 Introduction\n\nQuestion answering (QA) is a central and highly\npopular area in NLP, with an abundance of datasets\navailable to tackle the problem from various angles,\nincluding extractive QA, cloze-completion, and\nopen-domain QA (Richardson, 2013; Rajpurkar\net al., 2016; Chen et al., 2017; Kwiatkowski et al.,\n2019). The field has made rapid advances in recent\nyears, even exceeding human performance in some\nsettings (Devlin et al., 2019; Alberti et al., 2019).\n\nDespite such popularity, QA datasets in lan-\nguages other than English remain scarce, even\nfor relatively high-resource languages (Asai et al.,\n2018), as collecting such datasets at sufficient\nscale and quality is difficult and costly. There\n\n'MLQA is publicly available at https://github.\ncom/facebookresearch/mlga\n\nare two reasons why this lack of data prevents in-\nternationalization of QA systems. First, we can-\nnot measure progress on multilingual QA with-\nout relevant benchmark data. Second, we cannot\neasily train end-to-end QA models on the task,\nand arguably most recent successes in QA have\nbeen in fully supervised settings. Given recent\nprogress in cross-lingual tasks such as document\nclassification (Lewis et al., 2004; Klementiev et al.,\n2012; Schwenk and Li, 2018), semantic role la-\nbelling (Akbik et al., 2015) and NLI (Conneau\net al., 2018), we argue that while multilingual QA\ntraining data might be useful but not strictly neces-\nsary, multilingual evaluation data is a must-have.\n\nRecognising this need, several cross-lingual\ndatasets have recently been assembled (Asai et al.,\n2018; Liu et al., 2019a). However, these gen-\nerally cover only a small number of languages,\ncombine data from different authors and annota-\ntion protocols, lack parallel instances, or explore\nless practically-useful QA domains or tasks (see\nSection 3). Highly parallel data is particularly\nattractive, as it enables fairer comparison across\nlanguages, requires fewer source language annota-\nions, and allows for additional evaluation setups\nat no extra annotation cost. A purpose-built evalua-\ntion benchmark dataset covering a range of diverse\nlanguages, and following the popular extractive QA\nparadigm on a practically-useful domain would be\na powerful testbed for cross-lingual QA models.\n\nWith this work, we present such a benchmark,\nMLQA, and hope that it serves as an accelerator\nor multilingual QA in the way datasets such as\nSQuaAD (Rajpurkar et al., 2016) have done for its\nmonolingual counterpart. MLQA is a multi-way\nparallel extractive QA evaluation benchmark in\nseven languages: English, Arabic, German, Viet-\nnamese, Spanish, Simplified Chinese and Hindi. To\nconstruct MLQA, we first automatically identify\nsentences from Wikipedia articles which have the\nsame or similar meaning in multiple languages. We\n\n7315\n\nProceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7315-7330\nJuly 5 - 10, 2020. ©2020 Association for Computational Linguistics\n", "vlm_text": "MLQA: Evaluating Cross-lingual Extractive Question Answering \nPatrick Lewis \\* † Barlas O˘ guz \\* Ruty Rinott \\* Sebastian Riedel \\* † Holger Schwenk \\* \\* Facebook AI Research † University College London { plewis,barlaso,ruty,sriedel,schwenk } @fb.com \nAbstract \nQuestion answering (QA) models have shown rapid progress enabled by the availability of large, high-quality benchmark datasets. Such annotated datasets are difﬁcult and costly to collect, and rarely exist in languages other than English, making building QA systems that work well in other languages challeng- ing. In order to develop such systems, it is crucial to invest in high quality multilingual evaluation  benchmarks to measure progress. We present MLQA, a multi-way aligned ex- tractive QA evaluation benchmark intended to spur research in this area.   MLQA contains QA instances in 7 languages,  English, Ara- bic, German, Spanish, Hindi, Vietnamese  and Simpliﬁed Chinese . MLQA has over 12K in- stances in English and 5K in each other lan- guage, with each instance parallel between 4 languages on average. We evaluate state- of-the-art cross-lingual models and machine- translation-based baselines on MLQA. In all cases, transfer results are signiﬁcantly behind training-language performance. \n1 Introduction \nQuestion answering (QA) is a central and highly popular area in NLP, with an abundance of datasets available to tackle the problem from various angles, including extractive QA, cloze-completion, and open-domain QA ( Richardson ,  2013 ;  Rajpurkar et al. ,  2016 ;  Chen et al. ,  2017 ;  Kwiatkowski et al. , 2019 ). The ﬁeld has made rapid advances in recent years, even exceeding human performance in some settings ( Devlin et al. ,  2019 ;  Alberti et al. ,  2019 ). \nDespite such popularity, QA datasets in lan- guages other than English remain scarce, even for relatively high-resource languages ( Asai et al. , 2018 ), as collecting such datasets at sufﬁcient scale and quality is difﬁcult and costly. There are two reasons why this lack of data prevents in- ter nationalization of QA systems. First, we can- not  measure  progress on multilingual QA with- out relevant benchmark data. Second, we cannot easily  train  end-to-end QA models on the task, and arguably most recent successes in QA have been in fully supervised settings. Given recent progress in cross-lingual tasks such as document classiﬁcation ( Lewis et al. ,  2004 ;  Klementiev et al. , 2012 ;  Schwenk and Li ,  2018 ), semantic role la- belling ( Akbik et al. ,  2015 ) and NLI ( Conneau et al. ,  2018 ), we argue that while multilingual QA training data might be useful but not strictly neces- sary, multilingual evaluation data is a must-have. \n\nRecognising this need, several cross-lingual datasets have recently been assembled ( Asai et al. , 2018 ;  Liu et al. ,  2019a ). However, these gen- erally cover only a small number of languages, combine data from different authors and annota- tion protocols, lack parallel instances, or explore less practically-useful QA domains or tasks (see Section  3 ). Highly parallel data is particularly attractive, as it enables fairer comparison across languages, requires fewer source language annota- tions, and allows for additional evaluation setups at no extra annotation cost. A purpose-built evalua- tion benchmark dataset covering a range of diverse languages, and following the popular extractive QA paradigm on a practically-useful domain would be a powerful testbed for cross-lingual QA models. \nWith this work, we present such a benchmark, MLQA, and hope that it serves as an accelerator for multilingual QA in the way datasets such as SQuAD ( Rajpurkar et al. ,  2016 ) have done for its monolingual counterpart. MLQA is a multi-way parallel extractive QA evaluation benchmark in seven languages:  English, Arabic, German, Viet- namese, Spanish, Simpliﬁed Chinese  and  Hindi . To construct MLQA, we ﬁrst automatically identify sentences from Wikipedia articles which have the same or similar meaning in multiple languages. We extract the paragraphs that contain such sentences, then crowd-source questions on the English para- graphs, making sure the answer is in the aligned sentence. This makes it possible to answer the ques- tion in all languages in the vast majority of cases. The generated questions are then translated to all target languages by professional translators, and answer spans are annotated in the aligned contexts for the target languages. "}
{"page": 1, "image_path": "doc_images/2020.acl-main.653_1.jpg", "ocr_text": "extract the paragraphs that contain such sentences,\nthen crowd-source questions on the English para-\ngraphs, making sure the answer is in the aligned\nsentence. This makes it possible to answer the ques-\ntion in all languages in the vast majority of cases.”\nThe generated questions are then translated to all\ntarget languages by professional translators, and\nanswer spans are annotated in the aligned contexts\nfor the target languages.\n\nThe resulting corpus has between 5,000 and\n6,000 instances in each language, and more than\n12,000 in English. Each instance has an aligned\nequivalent in multiple other languages (always in-\ncluding English), the majority being 4-way aligned.\nCombined, there are over 46,000 QA annotations.\n\nWe define two tasks to assess performance on\nMLQA. The first, cross-lingual transfer (XLT), re-\nquires models trained in one language (in our case\nEnglish) to transfer to test data in a different lan-\nguage. The second, generalised cross-lingual trans-\nfer (G-XLT) requires models to answer questions\nwhere the question and context language is differ-\nent, e.g. questions in Hindi and contexts in Arabic,\na setting possible because MLQA is highly parallel.\n\nWe provide baselines using state-of-the-art cross-\nlingual techniques. We develop machine transla-\ntion baselines which map answer spans based on\nthe attention matrices from a translation model, and\nuse multilingual BERT (Devlin et al., 2019) and\nXLM (Lample and Conneau, 2019) as zero-shot ap-\nproaches. We use English for our training language\nand adopt SQuAD as a training dataset. We find\nthat zero-shot XLM transfers best, but all models\nlag well behind training-language performance.\n\nIn summary, we make the following contribu-\ntions: 1) We develop a novel annotation pipeline\nto construct large multilingual, highly-parallel ex-\ntractive QA datasets ii) We release MLQA, a 7-\nlanguage evaluation dataset for cross-lingual QA\niii) We define two cross-lingual QA tasks, including\na novel generalised cross-lingual QA task iv) We\nprovide baselines using state-of-the-art techniques,\nand demonstrate significant room for improvement.\n\n2 The MLQA corpus\n\nFirst, we state our desired properties for a cross-\nlingual QA evaluation dataset. We note that whilst\nsome existing datasets exhibit these properties,\n\n>The automatically aligned sentences occasionally differ\nin a named entity or information content, or some questions\n\nmay not make sense without the surrounding context. In these\nrare cases, there may be no answer for some languages.\n\nnone exhibit them all in combination (see section 3).\nWe then describe our annotation protocol, which\nseeks to fulfil these desiderata.\n\nParallel The dataset should consist of instances\nthat are parallel across many languages. First, this\nmakes comparison of QA performance as a func-\ntion of transfer language fairer. Second, additional\nevaluation setups become possible, as questions\nin one language can be applied to documents in\nanother. Finally, annotation cost is also reduced as\nmore instances can be shared between languages.\n\nNatural Documents Building a parallel QA\ndataset in many languages requires access to paral-\nlel documents in those languages. Manually trans-\nlating documents at sufficient scale entails huge\ntranslator workloads, and could result in unnatural\ndocuments. Exploiting existing naturally-parallel\ndocuments is advantageous, providing high-quality\ndocuments without requiring manual translation.\n\nDiverse Languages A primary goal of cross-\nlingual research is to develop systems that work\nwell in many languages. The dataset should en-\nable quantitative performance comparison across\nlanguages with different linguistic resources, lan-\nguage families and scripts.\n\nExtractive QA  Cross-lingual understanding\nbenchmarks are typically based on classifica-\ntion (Conneau et al., 2018). Extracting spans in\ndifferent languages represents a different language\nunderstanding challenge. Whilst there are extrac-\ntive QA datasets in a number of languages (see\nSection 3), most were created at different times by\ndifferent authors with different annotation setups,\nmaking cross-language analysis challenging.\n\nTextual Domain We require a naturally highly\nlanguage-parallel textual domain. Also, it is desir-\nable to select a textual domain that matches existing\nextractive QA training resources, in order to isolate\nthe change in performance due to language transfer.\n\nTo satisfy these desiderata, we identified the\nmethod described below and illustrated in Figure 1.\nWikipedia represents a convenient textual domain,\nas its size and multi-linguality enables collection of\ndata in many diverse languages at scale. It has been\nused to build many existing QA training resources,\nallowing us to leverage these to train QA models,\nwithout needing to build our own training dataset.\nWe choose English as our source language as it has\nthe largest Wikipedia, and to easily source crowd\n\n7316\n", "vlm_text": "\nThe resulting corpus has between 5,000 and 6,000 instances in each language, and more than 12,000 in English. Each instance has an aligned equivalent in multiple other languages (always in- cluding English), the majority being 4-way aligned. Combined, there are over 46,000 QA annotations. \nWe deﬁne two tasks to assess performance on MLQA. The ﬁrst, cross-lingual transfer (XLT), re- quires models trained in one language (in our case English) to transfer to test data in a different lan- guage. The second, generalised cross-lingual trans- fer (G-XLT) requires models to answer questions where the question and context language is  differ- ent , e.g. questions in Hindi and contexts in Arabic, a setting possible because MLQA is highly parallel. \nWe provide baselines using state-of-the-art cross- lingual techniques. We develop machine transla- tion baselines which map answer spans based on the attention matrices from a translation model, and use multilingual BERT ( Devlin et al. ,  2019 ) and XLM ( Lample and Conneau ,  2019 ) as zero-shot ap- proaches. We use English for our training language and adopt SQuAD as a training dataset. We ﬁnd that zero-shot XLM transfers best, but all models lag well behind training-language performance. \nIn summary, we make the following contribu- tions: i) We develop a novel annotation pipeline to construct large multilingual, highly-parallel ex- tractive QA datasets ii) We release MLQA, a 7- language evaluation dataset for cross-lingual QA iii) We deﬁne two cross-lingual QA tasks, including a novel generalised cross-lingual QA task iv) We provide baselines using state-of-the-art techniques, and demonstrate signiﬁcant room for improvement. \n2 The MLQA corpus \nFirst, we state our desired properties for a cross- lingual QA evaluation dataset. We note that whilst some existing datasets exhibit these properties, none exhibit them all in combination (see section  3 ). We then describe our annotation protocol, which seeks to fulﬁl these desiderata. \n\nParallel The dataset should consist of instances that are parallel across many languages. First, this makes comparison of QA performance as a func- tion of transfer language fairer. Second, additional evaluation setups become possible, as questions in one language can be applied to documents in another. Finally, annotation cost is also reduced as more instances can be shared between languages. \nNatural Documents Building a parallel QA dataset in many languages requires access to paral- lel documents in those languages. Manually trans- lating documents at sufﬁcient scale entails huge translator workloads, and could result in unnatural documents. Exploiting existing naturally-parallel documents is advantageous, providing high-quality documents without requiring manual translation. \nDiverse Languages A primary goal of cross- lingual research is to develop systems that work well in many languages. The dataset should en- able quantitative performance comparison across languages with different linguistic resources, lan- guage families and scripts. \nExtractive QA Cross-lingual understanding benchmarks are typically based on classiﬁca- tion ( Conneau et al. ,  2018 ). Extracting spans in different languages represents a different language understanding challenge. Whilst there are extrac- tive QA datasets in a number of languages (see Section  3 ), most were created at different times by different authors with different annotation setups, making cross-language analysis challenging. \nTextual Domain We require a naturally highly language-parallel textual domain. Also, it is desir- able to select a textual domain that matches existing extractive QA training resources, in order to isolate \nthe change in performance due to language transfer. To satisfy these desiderata, we identiﬁed the method described below and illustrated in Figure  1 . Wikipedia represents a convenient textual domain, as its size and multi-linguality enables collection of data in many diverse languages at scale. It has been used to build many existing QA training resources, allowing us to leverage these to train QA models, without needing to build our own training dataset. We choose English as our source language as it has the largest Wikipedia, and to easily source crowd "}
{"page": 2, "image_path": "doc_images/2020.acl-main.653_2.jpg", "ocr_text": "En Wikipedia Article\n\nSoseeeeee eer Eclipses only occur\nExtract paralle| [.]- Solar eclipses\noccur at new moon,\nSentence Pen when the Moon\nwith surrounding is between the Sun\n=. context Con and Earth. In\nOot eee contrast [..] Earth.\n\nCen\n\n_-- Bei einer\noo Sonnenfinsternis,\na die nur bei Neumond\na auftreten kann,\n\nwaz] Extract parallel, stent der\n\nsentence bye Mond zwischen Sonne\nwith surrounding ¥2d Erde. Eine\nSonnenfinsternis\ncontext Cye\n\ns the moon\n\nbetween the\n\nQA - Sun and the\nAnnotation Earth\nAen\nQuestion\nTranslation\nAnswer zwischen\n- Sonne und\nAnnotation\n\nErde.\nde\n\n[..] Erdoberflache.\n\nCue\n\nFigure 1: MLQA annotation pipeline. Only one target language is shown for clarity. Left: We first identify N-way\nparallel sentences be,,, b; ... by —1 in Wikipedia articles on the same topic, and extract the paragraphs that contain\nthem, Cen, Cy ...Cn—1. Middle: Workers formulate questions q,,, from c,,, for which answer a,,, is a span within\nben. Right: English questions qe, are then translated by professional translators into all languages q; and the\nanswer a; is annotated in the target language context c; such that a; is a span within );.\n\nworkers. We choose six other languages which rep-\nresent a broad range of linguistic phenomena and\nhave sufficiently large Wikipedia. Our annotation\npipeline consists of three main steps:\n\nStep 1) We automatically extract paragraphs\nwhich contain a parallel sentence from articles on\nthe same topic in each language (left of Figure 1).\n\nStep 2) We employ crowd-workers to annotate\nquestions and answer spans on the English para-\ngraphs (centre of Figure 1). Annotators must\nchoose answer spans within the parallel source sen-\ntence. This allows annotation of questions in the\nsource language with high probability of being an-\nswerable in the target languages, even if the rest of\nthe context paragraphs are different.\n\nStep 3) We employ professional translators to\ntranslate the questions and to annotate answer spans\nin the target language (right of Figure 1).\n\nThe following sections describe each step in the\ndata collection pipeline in more detail.\n\n2.1 Parallel Sentence Mining\n\nParallel Sentence mining allows us to leverage\nnaturally-written documents and avoid translation,\nwhich would be expensive and result in potentially\nunnatural documents. In order for questions to be\nanswerable in every target language, we use con-\ntexts containing an N-way parallel sentence. Our\napproach is similar to WikiMatrix (Schwenk et al.,\n2019) which extracts parallel sentences for many\nlanguage pairs in Wikipedia, but we limit the search\n\nde es ar zh vi hi\n\n54M LIM = 83.7k 241K 9.2k 1340\n\nTable 1: Incremental alignment with English to obtain\n7-way aligned sentences.\n\nor parallel sentences to documents on the same\nopic only, and aim for N-way parallel sentences.\nTo detect parallel sentences we use the LASER\noolkit,? which achieves state-of-the-art perfor-\nmance in mining parallel sentences (Artetxe and\nSchwenk, 2019). LASER uses multilingual sen-\nence embeddings and a distance or margin cri-\nerion in the embeddings space to detect parallel\nsentences. The reader is referred to Artetxe and\nSchwenk (2018) and Artetxe and Schwenk (2019)\nor a detailed description. See Appendix A.6 for\nurther details and statistics on the number of par-\nallel sentences mined for all language pairs.\nWe first independently align all languages with\nEnglish, then intersect these sets of parallel sen-\nences, forming sets of N-way parallel sentences.\nAs shown in Table 1, starting with 5.4M parallel\nEnglish/German sentences, the number of N-way\nparallel sentences quickly decreases as more lan-\nguages are added. We also found that 7-way par-\nallel sentences lack linguistic diversity, and often\nappear in the first sentence or paragraph of articles.\nAs a compromise between language-parallelism\n\nShttps://github.com/facebookresearch/\nLASER\n\n7317\n", "vlm_text": "The image illustrates the MLQA (Multilingual Question Answering) annotation pipeline, depicting the process of generating and translating question-answer pairs across different languages using content from Wikipedia articles. It consists of several steps:\n\n1. **Extract Parallel Sentences and Contexts:** \n   - Identify parallel sentences in English (`b_en`) and other target languages like German (`b_de`) from Wikipedia articles about the same topic.\n   - Extract surrounding context paragraphs in both languages (`c_en` and `c_de`).\n\n2. **QA Annotation:**\n   - Workers create questions (`q_en`) from the English context (`c_en`) with answers (`a_en`) being spans within the sentence (`b_en`).\n\n3. **Question Translation:**\n   - Translate the English questions (`q_en`) into the target language (e.g., German `q_de`) by professional translators.\n\n4. **Answer Annotation:**\n   - Annotate the answer in the target language (`a_de`), ensuring it corresponds correctly within its respective context (`c_de`) and is a span within the translated sentence (`b_de`).\n\nThe image emphasizes handling multilingual contexts and ensuring the fidelity of information across different languages in the QA process.\nworkers. We choose six other languages which rep- resent a broad range of linguistic phenomena and have sufﬁciently large Wikipedia. Our annotation pipeline consists of three main steps: \nStep 1) We automatically extract paragraphs which contain a parallel sentence from articles on the same topic in each language (left of Figure  1 ). \nStep 2) We employ crowd-workers to annotate questions and answer spans on the English para- graphs (centre of Figure  1 ). Annotators must choose answer spans within the parallel source sen- tence. This allows annotation of questions in the source language with high probability of being an- swerable in the target languages, even if the rest of the context paragraphs are different. \nStep 3) We employ professional translators to translate the questions and to annotate answer spans in the target language (right of Figure  1 ). \nThe following sections describe each step in the data collection pipeline in more detail. \n2.1 Parallel Sentence Mining \nParallel Sentence mining allows us to leverage naturally-written documents and avoid translation, which would be expensive and result in potentially unnatural documents. In order for questions to be answerable in every target language, we use con- texts containing an    $N$  -way parallel sentence. Our approach is similar to WikiMatrix ( Schwenk et al. , 2019 ) which extracts parallel sentences for many language pairs in Wikipedia, but we limit the search \nThe table shows data associated with language codes and numbers:\n\n- **de**: 5.4M\n- **es**: 1.1M\n- **ar**: 83.7k\n- **zh**: 24.1k\n- **vi**: 9.2k\n- **hi**: 1340\n\nThese could represent figures related to language usage, such as numbers of speakers or articles, but the specific context isn't provided.\nfor parallel sentences to documents on the same topic only, and aim for    $N$  -way parallel sentences. \nTo detect parallel sentences we use the LASER toolkit,   which achieves state-of-the-art perfor- mance in mining parallel sentences ( Artetxe and Schwenk ,  2019 ). LASER uses multilingual sen- tence embeddings and a distance or margin cri- terion in the embeddings space to detect parallel sentences. The reader is referred to  Artetxe and Schwenk  ( 2018 ) and  Artetxe and Schwenk  ( 2019 ) for a detailed description. See Appendix  A.6  for further details and statistics on the number of par- allel sentences mined for all language pairs. \nWe ﬁrst independently align all languages with English, then intersect these sets of parallel sen- tences, forming sets of N-way parallel sentences. As shown in Table  1 , starting with 5.4M parallel English/German sentences, the number of N-way parallel sentences quickly decreases as more lan- guages are added. We also found that 7-way par- allel sentences lack linguistic diversity, and often appear in the ﬁrst sentence or paragraph of articles. As a compromise between language-parallelism and both the number and diversity of parallel sen- tences, we use sentences that are 4-way parallel. This yields 385,396 parallel sentences (see Ap- pendix  A.6 ) which were sub-sampled to ensure parallel sentences were evenly distributed in para- graphs. We ensure that each language combination is equally represented, so that each language has many QA instances in common with every other language. Except for any rejected instances later in the pipeline, each QA instance will be parallel between English and three target languages. "}
{"page": 3, "image_path": "doc_images/2020.acl-main.653_3.jpg", "ocr_text": "and both the number and diversity of parallel sen-\ntences, we use sentences that are 4-way parallel.\nThis yields 385,396 parallel sentences (see Ap-\npendix A.6) which were sub-sampled to ensure\nparallel sentences were evenly distributed in para-\ngraphs. We ensure that each language combination\nis equally represented, so that each language has\nmany QA instances in common with every other\nlanguage. Except for any rejected instances later\nin the pipeline, each QA instance will be parallel\nbetween English and three target languages.\n\n2.2 English QA Annotation\n\nWe use Amazon Mechanical Turk to annotate En-\nglish QA instances, broadly following the method-\nology of Rajpurkar et al. (2016). We present work-\ners with an English aligned sentence, b., along\nwith the paragraph that contains it c.,. Workers\nformulate a question ger, and highlight the shortest\nanswer span ae, that answers it. ae, must be be a\nsubspan of ben to ensure gen will be answerable in\nthe target languages. We include a “No Question\nPossible” button when no sensible question could\nbe asked. Screenshots of the annotation interface\ncan be found in Appendix A.1. The first 15 ques-\ntions from each worker are manually checked, after\nwhich the worker is contacted with feedback, or\ntheir work is auto-approved.\n\nOnce the questions and answers have been anno-\ntated, we run another task to re-annotate English\nanswers. Here, workers are presented with qe, and\nCen, and requested to generate an a/,, or to indicate\nthat den is not answerable. Two additional answer\nspan annotations are collected for each question.\nThe additional answer annotations enable us to cal-\nculate an inter-annotator agreement (IAA) score.\nWe calculate the mean token F1 score between the\nthree answer annotations, giving an IAA score of\n82%, comparable to the SQUAD v1.1 development\nset, where this IAA measure is 84%.\n\nRather than provide all three answer annotations\nas gold answers, we select a single representative\nreference answer. In 88% of cases, either two or\nthree of the answers exactly matched, so the major-\nity answer is selected. In the remaining cases, the\nanswer with highest F1 overlap with the other two\nis chosen. This results both in an accurate answer\nspan, and ensures the English results are compara-\nble to those in the target languages, where only one\nanswer is annotated per question.\n\nWe discard instances where annotators marked\n\nthe question as unanswerable as well as instances\nwhere over 50% of the question appeared as a sub-\nsequence of the aligned sentence, as these are too\neasy or of low quality. Finally, we reject questions\nwhere the IAA score was very low (< 0.3) remov-\ning a small number of low quality instances. To\nverify we were not discarding challenging but high\nquality examples in this step, a manual analysis\nof discarded questions was performed. Of these\ndiscarded questions, 38% were poorly specified,\n24% did not make sense/had no answer, 30% had\npoor answers, and only 8% were high quality chal-\nlenging questions.\n\n2.3 Target Language QA Annotation\n\nWe use the One Hour Translation platform to\nsource professional translators to translate the ques-\ntions from English to the six target languages, and\nto find answers in the target contexts. We present\neach translator with the English question qe, En-\nglish answer ae, and the context c, (containing\naligned sentence b,) in target language x. The\ntranslators are only shown the aligned sentence and\nthe sentence on each side (where these exist). This\nincreases the chance of the question being answer-\nable, as in some cases the aligned sentences are\nnot perfectly parallel, without requiring workers to\nread the entire context c,. By providing the English\nanswer we try to minimize cultural and personal\ndifferences in the amount of detail in the answer.\nWe sample 2% of the translated questions for\nadditional review by language experts. Transla-\ntors that did not meet the quality standards were\nremoved from the translator pool, and their transla-\ntions were reallocated. By comparing the distribu-\ntion of answer lengths relative to the context to the\nEnglish distribution, some cases were found where\nsome annotators selected very long answers, espe-\ncially for Chinese. We clarified the instructions\nwith these specific annotators, and send such cases\nfor re-annotation. We discard instances in target\nlanguages where annotators indicate there is no an-\nswer in that language. This means some instances\nare not 4-way parallel. “No Answer” annotations\noccurred for 6.6%-21.9% of instances (Vietnamese\nand German, respectively). We release the “No An-\nswer” data separately as an additional resource, but\ndo not consider it in our experiments or analysis.\n\n2.4 The Resulting MLQA corpus\n\nContexts, questions and answer spans for all the\nlanguages are then brought together to create the\n\n7318\n", "vlm_text": "\n2.2 English QA Annotation \nWe use Amazon Mechanical Turk to annotate En- glish QA instances, broadly following the method- ology of  Rajpurkar et al.  ( 2016 ). We present work- ers with an English aligned sentence,    $b_{e n}$   along with the paragraph that contains it    $c_{e n}$  . Workers formulate a question  $q_{e n}$   and highlight the shortest answer span  $a_{e n}$   that answers it.  $a_{e n}$   must be be a subspan of  $b_{e n}$   to ensure  $q_{e n}$   will be answerable in the target languages. We include a “No Question Possible” button when no sensible question could be asked. Screenshots of the annotation interface can be found in Appendix  A.1 . The ﬁrst 15 ques- tions from each worker are manually checked, after which the worker is contacted with feedback, or their work is auto-approved. \nOnce the questions and answers have been anno- tated, we run another task to re-annotate English answers. Here, workers are presented with  $q_{e n}$   and  $c_{e n}$  , and requested to generate an    $a_{e n}^{\\prime}$    or to indicate that    $q_{e n}$   is not answerable. Two additional answer span annotations are collected for each question. The additional answer annotations enable us to cal- culate an inter-annotator agreement (IAA) score. We calculate the mean token F1 score between the three answer annotations, giving an IAA score of  $82\\%$  , comparable to the SQuAD v1.1 development set, where this IAA measure is  $84\\%$  . \nRather than provide all three answer annotations as gold answers, we select a single representative reference answer. In   $88\\%$   of cases, either two or three of the answers exactly matched, so the major- ity answer is selected. In the remaining cases, the answer with highest F1 overlap with the other two is chosen. This results both in an accurate answer span, and ensures the English results are compara- ble to those in the target languages, where only one answer is annotated per question. \nWe discard instances where annotators marked the question as unanswerable as well as instances where over   $50\\%$   of the question appeared as a sub- sequence of the aligned sentence, as these are too easy or of low quality. Finally, we reject questions where the IAA score was very low   $(<0.3)$   remov- ing a small number of low quality instances. To verify we were not discarding challenging but high quality examples in this step, a manual analysis of discarded questions was performed. Of these discarded questions,   $38\\%$   were poorly speciﬁed,  $24\\%$   did not make sense/had no answer,  $30\\%$   had poor answers, and only   $8\\%$   were high quality chal- lenging questions. \n\n2.3 Target Language QA Annotation \nWe use the One Hour Translation platform to source professional translators to translate the ques- tions from English to the six target languages, and to ﬁnd answers in the target contexts. We present each translator with the English question    $q_{e n}$  , En- glish answer    $a_{e n}$  , and the context    $c_{x}$   (containing aligned sentence    $b_{x}$  ) in target language    $x$  . The translators are only shown the aligned sentence and the sentence on each side (where these exist). This increases the chance of the question being answer- able, as in some cases the aligned sentences are not perfectly parallel, without requiring workers to read the entire context  $c_{x}$  . By providing the English answer we try to minimize cultural and personal differences in the amount of detail in the answer. \nWe sample    $2\\%$   of the translated questions for additional review by language experts. Transla- tors that did not meet the quality standards were removed from the translator pool, and their transla- tions were reallocated. By comparing the distribu- tion of answer lengths relative to the context to the English distribution, some cases were found where some annotators selected very long answers, espe- cially for Chinese. We clariﬁed the instructions with these speciﬁc annotators, and send such cases for re-annotation. We discard instances in target languages where annotators indicate there is no an- swer in that language. This means some instances are not 4-way parallel. “No Answer” annotations occurred for  $6.6\\%–21.9\\%$   of instances (Vietnamese and German, respectively). We release the “No An- swer” data separately as an additional resource, but do not consider it in our experiments or analysis. \n2.4 The Resulting MLQA corpus \nContexts, questions and answer spans for all the languages are then brought together to create the "}
{"page": 4, "image_path": "doc_images/2020.acl-main.653_4.jpg", "ocr_text": "fold en de es ar zh vi hi\n\ndev 1148 512 500 517 504 511 507\ntest 11590 4517 5253 5335 5137 5495 4918\n\nTable 2: Number of instances per language in MLQA.\n\nde es ar zh vi hi\nde 5029\nes 1972 5753\nar 1856 2139 5852\nzh «1811 «2108 +=2100 5641\nvi 1857 2207 2210 2127 6006\nhi 1593 1910 2017) 2124 2124 5425\n\nTable 3: Number of parallel instances between target\nlanguage pairs (all instances are parallel with English).\n\nfinal corpus. MLQA consists of 12,738 extractive\nQA instances in English and between 5,029 and\n6,006 instances in the target languages. 9,019 in-\nstances are 4-way parallel, 2,930 are 3-way parallel\nand 789 2-way parallel. Representative examples\nare shown in Figure 2. MLQA is split into devel-\nopment and test splits, with statistics in Tables 2,\n3 and 4. To investigate the distribution of topics\nin MLQA, a random sample of 500 articles were\nmanually analysed. Articles cover a broad range\nof topics across different cultures, world regions\nand disciplines. 23% are about people, 19% on\nphysical places, 13% on cultural topics, 12% on\nscience/engineering, 9% on organisations, 6% on\nevents and 18% on other topics. Further statistics\nare given in Appendix A.2.\n\nen de es ar zh_svi hi\n\n5530 2806 2762 2627 2673 2682 2255\n10894 4509 5215 5085 4989 5246 4524\n12738 5029 5753 5852 5641 6006 5425\n\n# Articles\n# Contexts\n# Instances\n\nTable 4: Number of Wikipedia articles with a context\nin MLQA.\n\n3 Related Work\n\nMonolingual QA Data There is a great vari-\nety of English QA data, popularized by MCTest\n(Richardson, 2013), CNN/Daily Mail (Hermann\net al., 2015) CBT (Hill et al., 2016), and Wik-\niQA (Yang et al., 2015) amongst others. Large\nspan-based datasets such as SQUAD (Rajpurkar\net al., 2016, 2018), TriviaQA (Joshi et al., 2017),\nNewsQA (Trischler et al., 2017), and Natural Ques-\ntions (Kwiatkowski et al., 2019) have seen extrac-\ntive QA become a dominant paradigm. However,\n\nlarge, high-quality datasets in other languages are\nrelatively rare. There are several Chinese datasets,\nsuch as DUReader (He et al., 2018), CMRC (Cui\net al., 2019b) and DRCD (Shao et al., 2018). More\nrecently, there have been efforts to build corpora in\na wider array of languages, such as Korean (Lim\net al., 2019) and Arabic (Mozannar et al., 2019).\n\nCross-lingual QA Modelling —Cross-lingual QA\nas a discipline has been explored in QA for RDF\ndata for a number of years, such as the QALD-3\nand 5 tracks (Cimiano et al., 2013; Unger et al.,\n2015), with more recent work from Zimina et al.\n(2018). Lee et al. (2018) explore an approach to\nuse English QA data from SQuAD to improve QA\nperformance in Korean using an in-language seed\ndataset. Kumar et al. (2019) study question gener-\nation by leveraging English questions to generate\nbetter Hindi questions, and Lee and Lee (2019) and\nCui et al. (2019a) develop modelling approaches to\nimprove performance on Chinese QA tasks using\nEnglish resources. Lee et al. (2019) and Hsu et al.\n(2019) explore modelling approaches for zero-shot\ntransfer and Singh et al. (2019) explore how train-\ning with cross-lingual data regularizes QA models.\n\nCross-lingual QA Data Gupta et al. (2018) re-\nlease a parallel QA dataset in English and Hindi,\nHardalov et al. (2019) investigate QA transfer\nfrom English to Bulgarian, Liu et al. (2019b) re-\nlease a cloze QA dataset in Chinese and English,\nand Jing et al. (2019) released BiPar, built using\nparallel paragraphs from novels in English and\nChinese. These datasets have a similar spirit to\nMLQA, but are limited to two languages. Asai et al.\n(2018) investigate extractive QA on a manually-\ntranslated set of 327 SQUAD instances in Japanese\nand French, and develop a phrase-alignment mod-\nelling technique, showing improvements over back-\ntranslation. Like us, they build multi-way par-\nallel extractive QA data, but MLQA has many\nmore instances, covers more languages and does\nnot require manual document translation. Liu\net al. (2019a) explore cross-lingual open-domain\nQA with a dataset built from Wikipedia “Did you\nknow?” questions, covering nine languages. Un-\nlike MLQA, it is distantly supervised, the dataset\nsize varies by language, instances are not paral-\nlel, and answer distributions vary by language,\nmaking quantitative comparisons across languages\nchallenging. Finally, in contemporaneous work,\nArtetxe et al. (2019) release XQuAD, a dataset of\n\n7319\n", "vlm_text": "The table presents the number of entries in different data folds for various languages. Here's the breakdown:\n\n- **Fold Types:**\n  - **dev** (development)\n  - **test**\n\n- **Languages:**\n  - **en** (English)\n  - **de** (German)\n  - **es** (Spanish)\n  - **ar** (Arabic)\n  - **zh** (Chinese)\n  - **vi** (Vietnamese)\n  - **hi** (Hindi)\n\n- **Counts:**\n  - **dev:**\n    - en: 1148\n    - de: 512\n    - es: 500\n    - ar: 517\n    - zh: 504\n    - vi: 511\n    - hi: 507\n    \n  - **test:**\n    - en: 11590\n    - de: 4517\n    - es: 5253\n    - ar: 5335\n    - zh: 5137\n    - vi: 5495\n    - hi: 4918\nThe table shows a matrix of numbers with language codes as both the headers and row labels. The language codes are:\n\n- **de** (German)\n- **es** (Spanish)\n- **ar** (Arabic)\n- **zh** (Chinese)\n- **vi** (Vietnamese)\n- **hi** (Hindi)\n\nEach cell represents a numerical value that likely corresponds to data specific to the language pairs indicated by the row and column. For instance, the intersection of \"es\" (Spanish) in the row and \"ar\" (Arabic) in the column is 2139.\nﬁnal corpus. MLQA consists of 12,738 extractive QA instances in English and between 5,029 and 6,006 instances in the target languages. 9,019 in- stances are 4-way parallel, 2,930 are 3-way parallel and 789 2-way parallel. Representative examples are shown in Figure  2 . MLQA is split into devel- opment and test splits, with statistics in Tables  2 , 3  and  4 . To investigate the distribution of topics in MLQA, a random sample of 500 articles were manually analysed. Articles cover a broad range of topics across different cultures, world regions and disciplines.   $23\\%$   are about people,   $19\\%$   on physical places,   $13\\%$   on cultural topics,   $12\\%$   on science/engineering,  $9\\%$   on organisations,  $6\\%$   on events and   $18\\%$   on other topics. Further statistics are given in Appendix  A.2 . \nThe table shows data across different languages indicated by their codes: en (English), de (German), es (Spanish), ar (Arabic), zh (Chinese), vi (Vietnamese), and hi (Hindi). It contains the following information:\n\n- **# Articles**: Number of articles for each language.\n- **# Contexts**: Number of contexts for each language.\n- **# Instances**: Number of instances for each language.\n\nHere are the numbers for each:\n\n- **English (en)**: \n  - Articles: 5530\n  - Contexts: 10894\n  - Instances: 12738\n\n- **German (de)**:\n  - Articles: 2806\n  - Contexts: 4509\n  - Instances: 5029\n\n- **Spanish (es)**:\n  - Articles: 2762\n  - Contexts: 5215\n  - Instances: 5753\n\n- **Arabic (ar)**:\n  - Articles: 2627\n  - Contexts: 5085\n  - Instances: 5852\n\n- **Chinese (zh)**:\n  - Articles: 2673\n  - Contexts: 4989\n  - Instances: 5641\n\n- **Vietnamese (vi)**:\n  - Articles: 2682\n  - Contexts: 5246\n  - Instances: 6006\n\n- **Hindi (hi)**:\n  - Articles: 2255\n  - Contexts: 4524\n  - Instances: 5425\n3 Related Work \nMonolingual QA Data There is a great vari- ety of English QA data, popularized by MCTest ( Richardson ,  2013 ), CNN/Daily Mail ( Hermann et al. ,  2015 ) CBT ( Hill et al. ,  2016 ), and Wik- iQA ( Yang et al. ,  2015 ) amongst others. Large span-based datasets such as SQuAD ( Rajpurkar et al. ,  2016 ,  2018 ), TriviaQA ( Joshi et al. ,  2017 ), NewsQA ( Trischler et al. ,  2017 ), and Natural Ques- tions ( Kwiatkowski et al. ,  2019 ) have seen extrac- tive QA become a dominant paradigm. However, large, high-quality datasets in other languages are relatively rare. There are several Chinese datasets, such as DUReader ( He et al. ,  2018 ), CMRC ( Cui et al. ,  2019b ) and DRCD ( Shao et al. ,  2018 ). More recently, there have been efforts to build corpora in a wider array of languages, such as Korean ( Lim et al. ,  2019 ) and Arabic ( Mozannar et al. ,  2019 ). \n\nCross-lingual QA Modelling Cross-lingual QA as a discipline has been explored in QA for RDF data for a number of years, such as the QALD-3 and 5 tracks ( Cimiano et al. ,  2013 ;  Unger et al. , 2015 ), with more recent work from  Zimina et al. ( 2018 ).  Lee et al.  ( 2018 ) explore an approach to use English QA data from SQuAD to improve QA performance in Korean using an in-language seed dataset.  Kumar et al.  ( 2019 ) study question gener- ation by leveraging English questions to generate better Hindi questions, and  Lee and Lee  ( 2019 ) and Cui et al.  ( 2019a ) develop modelling approaches to improve performance on Chinese QA tasks using English resources.  Lee et al.  ( 2019 ) and  Hsu et al. ( 2019 ) explore modelling approaches for zero-shot transfer and  Singh et al.  ( 2019 ) explore how train- ing with cross-lingual data regularizes QA models. \nCross-lingual QA Data Gupta et al.  ( 2018 ) re- lease a parallel QA dataset in English and Hindi, Hardalov et al.  ( 2019 ) investigate QA transfer from English to Bulgarian,  Liu et al.  ( 2019b ) re- lease a cloze QA dataset in Chinese and English, and  Jing et al.  ( 2019 ) released BiPar, built using parallel paragraphs from novels in English and Chinese. These datasets have a similar spirit to MLQA, but are limited to two languages.  Asai et al. ( 2018 ) investigate extractive QA on a manually- translated set of 327 SQuAD instances in Japanese and French, and develop a phrase-alignment mod- elling technique, showing improvements over back- translation. Like us, they build multi-way par- allel extractive QA data, but MLQA has many more instances, covers more languages and does not require manual document translation. Liu et al.  ( 2019a ) explore cross-lingual open-domain QA with a dataset built from Wikipedia “Did you know?” questions, covering nine languages. Un- like MLQA, it is distantly supervised, the dataset size varies by language, instances are not paral- lel, and answer distributions vary by language, making quantitative comparisons across languages challenging. Finally, in contemporaneous work, Artetxe et al.  ( 2019 ) release XQuAD, a dataset of "}
{"page": 5, "image_path": "doc_images/2020.acl-main.653_5.jpg", "ocr_text": "En | During what time period did the Angles migrate to Great Britain?\n\nWhat are the names given to the campuses on the east side of the\n\nEn | and the university sits on?\n\nThe name \"England' is derived from the Old English name Englaland [...] The\nAngles were one of the Germanic tribes that settled in Great Britain during the\nEarly Middle Ages. [...] The Welsh name for the English language is \"Saesneg\"\n\nWahrend welcher Zeitperiode migrierten die Angeln nach\n\nDe | GroBbritannien?\n\nThe campus is in the residential area of Westwood [...] The campus is informally\ndivided into North Campus and South Campus, which are both on the eastern\nhalf of the university's land. [...] The campus includes [...] a mix of architectural\n\nstyles.\n\n~Cuales son los nombres dados a los campus ubicados en el lado\n\nEs | este del recinto donde se encuentra la universidad?\n\nDer Name England leitet sich vom altenglischen Wort Engaland [...] Die Angeln\nwaren ein germanischer Stamm, der das Land im Friifmittelalter besiedelte.\n[...] ein Verweis auf die weiBen Klippen von Dover.\n\nAr flint Gilly cl} Jacl pale Hie da ol Ga\n\nEl campus incluye [...] una mezcla de estilos arquitectonicos. Informalmente\nesta dividido en Campus Norte y Campus Sur, ambos localizados en la parte\neste del terreno que posee la universidad. [...] El Campus Sur esta enfocado en\nla ciencias fisicas [...] y el Centro Médico Ronald Reagan de UCLA.\n\nZh SIF AF SHARE BES RATA?\n\negy gS LIS yo \"ll\" aul Gy Englalande saaly cals July \"Wasi! Ga I a aly\nLash all olen 8s [...] teu ol! y gunn! 15/9 IAS | las) gi Stl all ale pall Ll) ye\nSY!\n\nvi Trong khoang thai gian nao nguéi Angles di cur dén Anh?\n\nEMERBREMIA A SILA MLE, AMRF AS SREB\n\nTRAERRAP, BAULANRZSANEAA, RPHARRA\n\noe one Library) MAHUS BAREIS. [...] RN MAES\nFOP\n\nHi fagafererrcra oral feaia @, Saree GT fee Hay afkery wr ae ATH fee Te a?\n\nTén goi cla Anh trong ting Viét bat ngudn ter tiéng Trung. [...] Nguoi Angle Ia\nmét trong nhirng bé téc German dinh cur tai Anh trong Tho’ dau Trung C6. [...]\nduéng nhur né lién quan t6i phong tuc goi nguéi German tai Anh la Angli\nSaxones hay Anh - Sachsen.\n\n(a)\n\nwa 1919 4 apiteery > eT azar URE Glen, da see aR SARA eft [...] RAT\n\nsriirenfte wa a sad wit sik alejoh ahaa feontsra &, at tat faeatrarcra Bt\nar © el feed 3 Fc [...] efaroft ofa a titres fata, site fas, scifrafen,\nrife, Woicte farsa, ait care S aafeia aa tk verity AfsHct Bex fee 21\n\n(b)\n\nFigure 2: (a) MLQA example parallel for En-De-Ar-Vi. (b) MLQA example parallel for En-Es-Zh-Hi. Answers\nshown as highlighted spans in contexts. Contexts shortened for clarity with “[...]”.\n\n1190 SQUAD instances from 240 paragraphs man-\nually translated into 10 languages. As shown in\nTable 4, MLQA covers 7 languages, but contains\nmore data per language — over 5k QA pairs from\n5k paragraphs per language. MLQA also uses real\nWikipedia contexts rather than manual translation.\n\nAggregated Cross-lingual Benchmarks Re-\ncently, following the widespread adoption of\nprojects such as GLUE (Wang et al., 2019), there\nhave been efforts to compile a suite of high quality\nmultilingual tasks as a unified benchmark system.\nTwo such projects, XGLUE (Liang et al., 2020) and\nXTREME (Hu et al., 2020) incorporate MLQA as\npart of their aggregated benchmark.\n\n4 Cross-lingual QA Experiments\n\nWe introduce two tasks to assess cross-lingual QA\nperformance with MLQA. The first, cross-lingual\ntransfer (XLT), requires training a model with\n(Cx, Ux; Gx) training data in language 2, in our case\nEnglish. Development data in language x is used\nfor tuning. At test time, the model must extract\nanswer a, in language y given context c, and ques-\ntion q,. The second task, generalized cross-lingual\ntransfer (G-XLT), is trained in the same way, but\nat test time the model must extract a, from c, in\nlanguage z given q, in language y. This evaluation\nsetup is possible because MLQA is highly parallel,\n\nallowing us to swap qz for qy for parallel instances\nwithout changing the question’s meaning.\n\nAs MLQA only has development and test data,\nwe adopt SQuAD v1.1 as training data. We use\nMLQA-en as development data, and focus on zero-\nshot evaluation, where no training or development\ndata is available in target languages. Models were\ntrained with the SQUAD-v1 training method from\nDevlin et al. (2019) and implemented in Pytext (Aly\net al., 2018). We establish a number of baselines to\nassess current cross-lingual QA capabilities:\n\nTranslate-Train We translate instances from the\nSQuAD training set into the target language us-\ning machine-translation.t Before translating, we\nenclose answers in quotes, as in Lee et al. (2018).\nThis makes it easy to extract answers from trans-\nlated contexts, and encourages the translation\nmodel to map answers into single spans. We dis-\ncard instances where this fails (~5%). This corpus\nis then used to train a model in the target language.\n\nTranslate-Test The context and question in the\n\ntarget language is translated into English at test\n\ntime. We use our best English model to produce\n\nan answer span in the translated paragraph. For\n\nall languages other than Hindi,> we use attention\n4We use Facebook’s production translation models.\n\n> Alignments were unavailable for Hindi-English due to\nproduction model limitations. Instead we translate English\n\n7320\n", "vlm_text": "The image contains two sets of multilingual question-answer examples from the MLQA (Multilingual Question Answering) dataset. \n\n- **Part (a)**: Features parallel question and answer examples in English (En), German (De), Arabic (Ar), and Vietnamese (Vi). The topic is about the time period the Angles migrated to Great Britain, with \"Early Middle Ages\" highlighted as the answer.\n\n- **Part (b)**: Shows parallel examples in English (En), Spanish (Es), Chinese (Zh), and Hindi (Hi). The topic is about the names of the campuses on the east side of the university land, with \"North Campus and South Campus\" highlighted as the answer.\n\nContexts have been shortened for clarity with “[...]”.\n1190 SQuAD instances from 240 paragraphs man- ually translated into 10 languages. As shown in Table  4 , MLQA covers 7 languages, but contains more data per language – over 5k QA pairs from 5k paragraphs per language. MLQA also uses real Wikipedia contexts rather than manual translation. \nAggregated Cross-lingual Benchmarks Re- cently, following the widespread adoption of projects such as GLUE ( Wang et al. ,  2019 ), there have been efforts to compile a suite of high quality multilingual tasks as a uniﬁed benchmark system. Two such projects, XGLUE ( Liang et al. ,  2020 ) and XTREME ( Hu et al. ,  2020 ) incorporate MLQA as part of their aggregated benchmark. \n4 Cross-lingual QA Experiments \nWe introduce two tasks to assess cross-lingual QA performance with MLQA. The ﬁrst,  cross-lingual transfer  (XLT), requires training a model with  $(c_{x},q_{x},a_{x})$   training data in language  $x$  , in our case English. Development data in language  $x$   is used for tuning. At test time, the model must extract answer    $a_{y}$   in language  $y$   given context  $c_{y}$   and ques- tion  $q_{y}$  . The second task,  generalized cross-lingual transfer  (G-XLT), is trained in the same way, but at test time the model must extract    $a_{z}$   from    $c_{z}$   in language    $z$   given  $q_{y}$   in language  $y$  . This evaluation setup is possible because MLQA is highly parallel, allowing us to swap  $q_{z}$   for    $q_{y}$   for parallel instances without changing the question’s meaning. \n\nAs MLQA only has development and test data, we adopt SQuAD v1.1 as training data. We use MLQA-en as development data, and focus on zero- shot evaluation, where no training or development data is available in target languages. Models were trained with the SQuAD-v1 training method from Devlin et al.  ( 2019 ) and implemented in Pytext ( Aly et al. ,  2018 ). We establish a number of baselines to assess current cross-lingual QA capabilities: \nTranslate-Train We translate instances from the SQuAD training set into the target language us- ing machine-translation.   Before translating, we enclose answers in quotes, as in  Lee et al.  ( 2018 ). This makes it easy to extract answers from trans- lated contexts, and encourages the translation model to map answers into single spans. We dis- card instances where this fails   $({\\sim}5\\%)$  . This corpus is then used to train a model in the target language. \nTranslate-Test The context and question in the target language is translated into English at test time. We use our best English model to produce an answer span in the translated paragraph. For all languages other than Hindi,   we use attention scores,    $a_{i j}$  , from the translation model to map the answer back to the original language. Rather than aligning spans by attention argmax, as by  Asai et al. ( 2018 ), we identify the span in the original context which maximizes F1 score with the English span: "}
{"page": 6, "image_path": "doc_images/2020.acl-main.653_6.jpg", "ocr_text": "scores, a;;, from the translation model to map the\nanswer back to the original language. Rather than\naligning spans by attention argmax, as by Asai et al.\n(2018), we identify the span in the original context\nwhich maximizes F1 score with the English span:\n\nRC = Vies.jes, ti/ Dies, tix\nPR = ies. jes, i/ Lyes, 9\nFl = 2*RC*PR/RC+PR\n\nanswer = arg max F1(S,)\nSo\n\nd)\n\nwhere S, and 5S, are the English and original spans\nrespectively, aj. = yj ay and a,j = D0; a;\n\nCross-lingual Representation Models We pro-\nduce zero-shot transfer results from multilingual\nBERT (cased, 104 languages) (Devlin et al., 2019)\nand XLM (MLM + TLM, 15 languages) (Lample\nand Conneau, 2019). Models are trained with the\nSQuAD training set and evaluated directly on the\nMLQA test set in the target language. Model se-\nlection is also constrained to be strictly zero-shot,\nusing only English development data to pick hyper-\nparameters. As a result, we end up with a single\nmodel that we test for all 7 languages.\n\n4.1 Evaluation Metrics for Multilingual QA\n\nMost extractive QA tasks use Exact Match (EM)\nand mean token F1 score as performance metrics.\nThe widely-used SQuAD evaluation also performs\nthe following answer-preprocessing operations: i)\nlowercasing, ii) stripping (ASCII) punctuation iii)\nstripping (English) articles and iv) whitespace to-\nkenisation. We introduce the following modifica-\ntions for fairer multilingual evaluation: Instead of\nstripping ASCII punctuation, we strip all unicode\ncharacters with a punctuation General_Category.®\nWhen a language has stand-alone articles (English,\nSpanish, German and Vietnamese) we strip them.\nWe use whitespace tokenization for all MLQA lan-\nguages other than Chinese, where we use the mixed\nsegmentation method from Cui et al. (2019b).\n\n5 Results\n\n5.1 XLT Results\n\nTable 5 shows the results on the XLT task. XLM\nperforms best overall, transferring best in Span-\n\nanswers using another round of translation. Back-translated\nanswers may not map back to spans in the original context, so\nthis Translate-Test performs poorly.\n®http://www.unicode.org/reports/tr44/\ntr44—-4.html#General_Category_Values\n\nen es de vi zh ar hi mean\n\nWho 7 +0.2 +2.8 +0.7 +1.4 +3.9 +1.1 -4.9 +0.7\n\nWhat 7 -0.4 -2.5 -2.1 -2.3 -1.5 -18 -0.2 -16\n\nWhere 7 -0.5 -4.6 -6.5 +0.8 -4.6 42.6 -5.8 -2.7\n\nHow + -1.0 +4.0 +2.4 +0.6 +1.6 41.8 42.1\n\nLanguage\n\nFigure 3: F1 score stratified by English wh* word, rel-\native to overall F1 score for XLM\n\nish, German and Arabic, and competitively with\ntranslate-train+M-BERT for Vietnamese and Chi-\nnese. XLM is however, weaker in English. Even\nfor XLM, there is a 39.8% drop in mean EM score\n(20.9% F1) over the English BERT-large baseline,\nshowing significant room for improvement. All\nmodels generally struggle on Arabic and Hindi.\n\nA manual analysis of cases where XLM failed to\nexactly match the gold answer was carried out for\nall languages. 39% of these errors were completely\nwrong answers, 5% were annotation errors and\n7% were acceptable answers with no overlap with\nthe gold answer. The remaining 49% come from\nanswers that partially overlap with the gold span.\nThe variation of errors across languages was small.\n\nTo see how performance varies by question type,\nwe compute XLM F1 scores stratified by common\nEnglish wh-words. Figure 3 shows that “When”\nquestions are the easiest for all languages, and\n“Where” questions seem challenging in most target\nlanguages. Further details are in Appendix A.3.\n\nTo explore whether questions that were difficult\nfor the model in English were also challenging in\nthe target languages, we split MLQA into two sub-\nsets on whether the XLM model got an English\nFI score of zero. Figure 4 shows that transfer per-\nformance is better when the model answers well\nin English, but is far from zero when the English\nanswer is wrong, suggesting some questions may\nbe easier to answer in some languages than others.\n\n5.2 G-XLT Results\n\nTable 6 shows results for XLM on the G-XLT task.”\nFor questions in a given language, the model per-\nforms best when the context language matches the\nquestion, except for Hindi and Arabic. For con-\n\n7 Additional results may be found in Appendix A.4\n\n7321\n", "vlm_text": "\n\n$$\n\\begin{array}{r}{\\begin{array}{r l}&{\\mathrm{\\bf{R}C}=\\sum_{i\\in S_{e},j\\in S_{o}}a_{i j}\\big/\\sum_{i\\in S_{e}}a_{i*}}\\\\ &{\\mathrm{\\bf{P}R}=\\sum_{i\\in S_{e},j\\in S_{o}}a_{i j}\\big/\\sum_{j\\in S_{o}}a_{*j}}\\\\ &{\\mathrm{\\bf{F}1}=2*\\mathrm{\\bf{R}C}*\\mathrm{\\bf{P}R}\\big/\\mathrm{\\bf{R}C}+\\mathrm{\\bf{P}R}}\\\\ &{\\mathrm{\\bf{a}n s w e r}=\\underset{S_{o}}{\\mathrm{arg\\,max}}~\\mathrm{\\bf{F}}1(S_{o})}\\end{array}}\\end{array}\n$$\n \nwhere  $S_{e}$   and    $S_{o}$   are the English and original spans respectively,    $\\begin{array}{r}{a_{i*}=\\sum_{j}a_{i j}}\\end{array}$   and    $\\begin{array}{r}{a_{*j}=\\sum_{i}a_{*j}}\\end{array}$  . ∗ \nCross-lingual Representation Models We pro- duce zero-shot transfer results from multilingual BERT (cased, 104 languages) ( Devlin et al. ,  2019 ) and XLM   $(\\mathbf{MLM}+\\mathrm{TLM}$  , 15 languages) ( Lample and Conneau ,  2019 ). Models are trained with the SQuAD training set and evaluated directly on the MLQA test set in the target language. Model se- lection is also constrained to be strictly zero-shot, using only English development data to pick hyper- parameters. As a result, we end up with a single model that we test for all 7 languages. \n4.1 Evaluation Metrics for Multilingual QA \nMost extractive QA tasks use Exact Match (EM) and mean token F1 score as performance metrics. The widely-used SQuAD evaluation also performs the following answer-preprocessing operations: i) lowercasing, ii) stripping (ASCII) punctuation iii) stripping (English) articles and iv) whitespace to- kenisation. We introduce the following modiﬁca- tions for fairer multilingual evaluation: Instead of stripping ASCII punctuation, we strip all unicode characters with a punctuation  General Category . When a language has stand-alone articles (English, Spanish, German and Vietnamese) we strip them. We use whitespace tokenization for all MLQA lan- guages other than Chinese, where we use the mixed segmentation method from  Cui et al.  ( 2019b ). \n5 Results \n5.1 XLT Results \nTable  5  shows the results on the XLT task. XLM performs best overall, transferring best in Span- \nThe image is a heatmap displaying F1 score differences stratified by English question words (\"Who\", \"What\", \"When\", \"Where\", \"How\") relative to the overall F1 score for the XLM model. The scores are shown for different languages (English, Spanish, German, Vietnamese, Chinese, Arabic, Hindi) as well as the mean difference.\n\n- The rows correspond to different English question words.\n- The columns represent different languages.\n- The values in the heatmap reflect the difference in F1 score for each question word relative to the overall F1 score for each language.\n- Positive values indicate a higher F1 score for the question word compared to the overall score, while negative values indicate a lower F1 score.\n- The most significant positive difference is for the \"When\" question word in German (+11.1), followed closely by Spanish (+10.9) and Vietnamese (+10.9).\n- The \"Where\" question word shows the most negative differences in German (-6.5).\n\nEach cell is color-coded, presumably to quickly visualize the magnitude and direction (positive or negative) of the differences, with warmer colors indicating higher positive differences and cooler colors indicating negative differences.\nish, German and Arabic, and competitively with translate-train  $+\\mathbf{M}$  -BERT for Vietnamese and Chi- nese. XLM is however, weaker in English. Even for XLM, there is a  $39.8\\%$   drop in mean EM score  $(20.9\\%$   F1) over the English BERT-large baseline, showing signiﬁcant room for improvement. All models generally struggle on Arabic and Hindi. \nA manual analysis of cases where XLM failed to exactly match the gold answer was carried out for all languages.   $39\\%$   of these errors were completely wrong answers,   $5\\%$   were annotation errors and  $7\\%$   were acceptable answers with no overlap with the gold answer. The remaining   $49\\%$   come from answers that partially overlap with the gold span. The variation of errors across languages was small. \nTo see how performance varies by question type, we compute XLM F1 scores stratiﬁed by common English wh-words. Figure  3  shows that “When” questions are the easiest for all languages, and “Where” questions seem challenging in most target languages. Further details are in Appendix  A.3 . \nTo explore whether questions that were difﬁcult for the model in English were also challenging in the target languages, we split MLQA into two sub- sets on whether the XLM model got an English F1 score of zero. Figure  4  shows that transfer per- formance is better when the model answers well in English, but is far from zero when the English answer is wrong, suggesting some questions may be easier to answer in some languages than others. \n5.2 G-XLT Results \nTable  6  shows results for XLM on the G-XLT task. For questions in a given language, the model per- forms best when the context language matches the question, except for Hindi and Arabic. For con- "}
{"page": 7, "image_path": "doc_images/2020.acl-main.653_7.jpg", "ocr_text": "Fl /EM en es ar hi vi zh\nBERT-Large 80.2/67.4 - - - - - -\nMultilingual-BERT 77.7165.2 64.3/46.6 57.9/44.3 45.7/29.8 43.8/29.7 57.1/38.6 57.5/37.3\nXLM 74.91624 68.0/49.8 62.2/47.6 548/363 48.8/27.3 614/418 61.1/39.6\nTranslate test, BERT-L - 65.4/44.0 57.9/41.8  33.6/20.4 23.8/18.9* 58.2/33.2 44.2/20.3\nTranslate train, M-BERT —- 53.9/37.4 62.0/47.5 51.8/33.2 55.0/40.0 62.0/43.1 61.4/39.5\nTranslate train, XLM - 65.2/47.8 61.4/46.7 54.0/344 50.7/33.4  59.3/39.4 59.8/37.9\n\nTable 5: Fl score and Exact Match on the MLQA test set for the cross-lingual transfer task (XLT)\n\n@mm Total Fl Score\n\n0.8 ISM _F1 score given correct English Answer\nSams F1 score given incorrect English Answer\n2 0.6\n°o\n6\na\n4 0.4\nira\n\n°\nuN\n\nes de ar hi vi zh\n\nen\n\nFigure 4: XLM F1 score stratified by English difficulty\n\ntexts in a given language, English questions tend to\nperform best, apart from Chinese and Vietnamese.\n\nc/q en es de ar hi vi zh\n\nen 585 50.8 43.6 55.7 53.9\nes 617 540 49.5 58.1 565\nd 62.2 574 49.9 60.1 57.3\nar 60.0 578 549 548 42.4 S505 43.5\nhi | 59.6 563 50.5 444 48.8 48.9 40.2\nvi | 60.2 59.6 53.2 487 405 614 48.5\nzh 52.9 55.8 50.0 40.9 35.4 46.5 | 611\n\nTable 6: Fl Score for XLM for G-XLT. Columns show\nquestion language, rows show context language.\n\n5.3. English Results on SQUAD 1 and MLQA\n\nThe MLQA-en results in Table 5 are lower than re-\nported results on SQUAD v1.1 in the literature for\nequivalent models. However, once SQUAD scores\nare adjusted to reflect only having one answer an-\nnotation (picked using the same method used to\npick MLQA answers), the discrepancy drops to\n5.8% on average (see Table 7). MLQA-en con-\ntexts are on average 28% longer than SQuAD’s,\nand MLQA covers a much wider set of articles\nthan SQUAD. Minor differences in preprocessing\nand answer lengths may also contribute (MLQA-\nen answers are slightly longer, 3.1 tokens vs 2.9\non average). Question type distributions are very\nsimilar in both datasets (Figure 7 in Appendix A)\n\nModel SQuAD SQuAD* MLQA-en\nBERT-Large 91.0/80.8 84.8/72.9  80.2/67.4\nM-BERT 88.5/81.2 83.0/71.1  77.7/65.1\nXLM 87.6/80.5 82.1/69.7 74.9/62.4\n\nTable 7: English performance comparisons to SQUAD\nusing our models. * uses a single answer annotation.\n\n6 Discussion\n\nIt is worth discussing the quality of context para-\ngraphs in MLQA. Our parallel sentence mining\napproach can source independently-written docu-\nments in different languages, but, in practice, arti-\ncles are often translated from English to the target\nlanguages by volunteers. Thus our method some-\ntimes acts as an efficient mechanism of sourcing\nexisting human translations, rather than sourcing\nindependently-written content on the same topic.\nThe use of machine translation is strongly discour-\naged by the Wikipedia community,® but from exam-\nining edit histories of articles in MLQA, machine\ntranslation is occasionally used as an article seed,\nbefore being edited and added to by human authors.\nOur annotation method restricts answers to come\nfrom specified sentences. Despite being provided\nseveral sentences of context, some annotators may\nbe tempted to only read the parallel sentence\nand write questions which only require a single\nsentence of context to answer. However, single\nsentence context questions are a known issue in\nSQuAD annotation in general (Sugawara et al.,\n2018) suggesting our method would not result in\nless challenging questions, supported by scores on\nMLQA-en being similar to SQUAD (section 5.3).\nMLQA is partitioned into development and test\nsplits. As MLQA is parallel, this means there is de-\nvelopment data for every language. Since MLQA\nwill be freely available, this was done to reduce the\nrisk of test data over-fitting in future, and to estab-\nShttps://en.wikipedia.org/wiki/\n\nWikipedia: Translation#Avoid_machine_\ntranslations\n\n7322\n", "vlm_text": "The table presents F1 and Exact Match (EM) scores for various language models across different languages. The columns list languages (en, es, de, ar, hi, vi, zh), and the rows compare different models and translation strategies. \n\n1. **Models Evaluated**:\n   - BERT-Large\n   - Multilingual-BERT\n   - XLM\n\n2. **Translation Strategies**:\n   - Translate test, BERT-L\n   - Translate train, M-BERT\n   - Translate train, XLM\n\n3. **Languages**:\n   - en: English\n   - es: Spanish\n   - de: German\n   - ar: Arabic\n   - hi: Hindi\n   - vi: Vietnamese\n   - zh: Chinese\n\n4. **Scores**:\n   - F1/EM scores are given for each model and strategy for the respective languages.\n   - For instance, BERT-Large achieves an F1/EM score of 80.2/67.4 in English, but the scores are not calculated (\"-\") for other languages using BERT-Large.\n   - Multilingual-BERT and XLM models have been evaluated in more languages compared to BERT-Large.\n   - The Translation strategies (Translate test/train) are evaluated for combinations like BERT-L, M-BERT, and XLM with various translation approaches.\n\n* An asterisk (*) is noted by the Hindi score for the Translate test, BERT-L, which may denote some special consideration or note in the source document.\n\nThe table effectively compares model performances in cross-lingual contexts using F1 and EM metrics, highlighting how models perform in transferring between languages directly or through translation-based approaches.\nThe image is a bar chart depicting the performance of a model evaluated using the F1 score, stratified by the difficulty of English. The chart displays three different metrics for multiple languages (en, es, de, ar, hi, vi, zh):\n\n1. **Total F1 Score (Blue Bar):** This represents the overall performance of the model for each language. The F1 score is a measure of a model's accuracy that considers both precision and recall.\n\n2. **F1 Score Given Correct English Answer (Orange Striped Bar):** This shows the F1 score when the model's English answer is correct for each language. It measures how well the model performs when its English predictions are accurate.\n\n3. **F1 Score Given Incorrect English Answer (Green Criss-Crossed Bar):** This shows the F1 score when the model's English answer is incorrect for each language. It indicates the model's performance despite making mistakes in English predictions.\n\nEach language (en, es, de, ar, hi, vi, zh) on the x-axis has these three corresponding bars that give insights into the model's capability across different linguistic scenarios.\ntexts in a given language, English questions tend to perform best, apart from Chinese and Vietnamese. \nThe table displays a matrix of numerical values representing scores or data points for combinations of English (en), Spanish (es), German (de), Arabic (ar), Hindi (hi), Vietnamese (vi), and Chinese (zh). The first column and first row indicate the languages involved. Each cell within the table represents the score corresponding to a pair of languages. For instance, the value at the intersection of the first row and first column (en, en) is 74.9, indicating a score or measurement for English when paired with itself. Similarly, each row represents scores associated with translating from or interacting with the language in the first column into languages in the top row. The shading of the cells likely reflects the relative magnitude of the values, with darker shades indicating higher values.\n5.3 English Results on SQuAD 1 and MLQA \nThe MLQA-en results in Table  5  are lower than re- ported results on SQuAD v1.1 in the literature for equivalent models. However, once SQuAD scores are adjusted to reﬂect only having one answer an- notation (picked using the same method used to pick MLQA answers), the discrepancy drops to  $5.8\\%$   on average (see Table  7 ). MLQA-en con- texts are on average   $28\\%$   longer than SQuAD’s, and MLQA covers a much wider set of articles than SQuAD. Minor differences in preprocessing and answer lengths may also contribute (MLQA- en answers are slightly longer, 3.1 tokens vs 2.9 on average). Question type distributions are very similar in both datasets (Figure  7  in Appendix  A ) \nThe image is a table showing the performance of different models on three datasets: SQuAD, SQuAD*, and MLQA-en. The models listed are BERT-Large, M-BERT, and XLM. For each model, two numbers are provided per dataset, which likely correspond to two different evaluation metrics (commonly accuracy and F1 score) for the models' performances. Here are the values for each:\n\n- BERT-Large:\n  - SQuAD: 91.0 / 80.8\n  - SQuAD*: 84.8 / 72.9\n  - MLQA-en: 80.2 / 67.4\n\n- M-BERT:\n  - SQuAD: 88.5 / 81.2\n  - SQuAD*: 83.0 / 71.1\n  - MLQA-en: 77.7 / 65.1\n\n- XLM:\n  - SQuAD: 87.6 / 80.5\n  - SQuAD*: 82.1 / 69.7\n  - MLQA-en: 74.9 / 62.4\n\nThe table helps compare the performance of these models across different datasets.\nTable 7: English performance comparisons to SQuAD using our models. \\* uses a single answer annotation. \n6 Discussion \nIt is worth discussing the quality of context para- graphs in MLQA. Our parallel sentence mining approach can source independently-written docu- ments in different languages, but, in practice, arti- cles are often translated from English to the target languages by volunteers. Thus our method some- times acts as an efﬁcient mechanism of sourcing existing human translations, rather than sourcing independently-written content on the same topic. The use of machine translation is strongly discour- aged by the Wikipedia community,   but from exam- ining edit histories of articles in MLQA, machine translation is occasionally used as an article seed, before being edited and added to by human authors. Our annotation method restricts answers to come from speciﬁed sentences. Despite being provided several sentences of context, some annotators may be tempted to only read the parallel sentence and write questions which only require a single sentence of context to answer. However, single sentence context questions are a known issue in SQuAD annotation in general ( Sugawara et al. , 2018 ) suggesting our method would not result in less challenging questions, supported by scores on MLQA-en being similar to SQuAD (section  5.3 ). \nMLQA is partitioned into development and test splits. As MLQA is parallel, this means there is de- velopment data for every language. Since MLQA will be freely available, this was done to reduce the risk of test data over-ﬁtting in future, and to estab- lish standard splits. However, in our experiments, we only make use of the English development data and study strict zero-shot settings. Other evalua- tion setups could be envisioned, e.g. by exploiting the target language development sets for hyper- parameter optimisation or ﬁne-tuning, which could be fruitful for higher transfer performance, but we leave such “few-shot” experiments as future work. Other potential areas to explore involve training datasets other than English, such as CMRC ( Cui et al. ,  2018 ), or using unsupervised QA techniques to assist transfer ( Lewis et al. ,  2019 ). "}
{"page": 8, "image_path": "doc_images/2020.acl-main.653_8.jpg", "ocr_text": "lish standard splits. However, in our experiments,\nwe only make use of the English development data\nand study strict zero-shot settings. Other evalua-\ntion setups could be envisioned, e.g. by exploiting\nthe target language development sets for hyper-\nparameter optimisation or fine-tuning, which could\nbe fruitful for higher transfer performance, but we\nleave such “‘few-shot” experiments as future work.\nOther potential areas to explore involve training\ndatasets other than English, such as CMRC (Cui\net al., 2018), or using unsupervised QA techniques\nto assist transfer (Lewis et al., 2019).\n\nFinally, a large body of work suggests QA mod-\nels are over-reliant on word-matching between\nquestion and context (Jia and Liang, 2017; Gan and\nNg, 2019). G-XLT represents an interesting test-\nbed, as simple symbolic matching is less straight-\nforward when questions and contexts use different\nlanguages. However, the performance drop from\nXLT is relatively small (8.2 mean F1), suggesting\nword-matching in cross-lingual models is more nu-\nanced and robust than it may initially appear.\n\n7 Conclusion\n\nWe have introduced MLQA, a highly-parallel mul-\ntilingual QA benchmark in seven languages. We\ndeveloped several baselines on two cross-lingual\nunderstanding tasks on MLQA with state-of-the-art\nmethods, and demonstrate significant room for im-\nprovement. We hope that MLQA will help to catal-\nyse work in cross-lingual QA to close the gap be-\ntween training and testing language performance.\n\nAcknowledgements\n\nThe authors would like to acknowledge their crowd-\nworking and translation colleagues for their work\non MLQA. The authors would also like to thank\nYuxiang Wu, Andres Compara Nufiez, Kartikay\nKhandelwal, Nikhil Gupta, Chau Tran, Ahmed\nKishky, Haoran Li, Tamar Lavee, Ves Stoyanov\nand the anonymous reviewers for their feedback\nand comments.\n\nReferences\n\nAlan Akbik, Laura Chiticariu, Marina Danilevsky, Yun-\nyao Li, Shivakumar Vaithyanathan, and Huaiyu Zhu.\n2015. Generating High Quality Proposition Banks\nfor Multilingual Semantic Role Labeling. In Pro-\nceedings of the 53rd Annual Meeting of the Associa-\ntion for Computational Linguistics and the 7th Inter-\nnational Joint Conference on Natural Language Pro-\ncessing (Volume I: Long Papers), pages 397-407,\n\nBeijing, China. Association for Computational Lin-\nguistics.\n\nChris Alberti, Daniel Andor, Emily Pitler, Jacob De-\nvlin, and Michael Collins. 2019. Synthetic QA Cor-\npora Generation with Roundtrip Consistency. In\nProceedings of the 57th Annual Meeting of the\nAssociation for Computational Linguistics, pages\n6168-6173, Florence, Italy. Association for Compu-\ntational Linguistics.\n\nAhmed Aly, Kushal Lakhotia, Shicong Zhao, Mri-\nnal Mohit, Barlas Oguz, Abhinav Arora, Sonal\nGupta, Christopher Dewan, Stef Nelson-Lindall, and\nRushin Shah. 2018. Pytext: A seamless path\nfrom nlp research to production. arXiv preprint\narXiv: 1812.08729.\n\nMikel Artetxe, Sebastian Ruder, and Dani Yogatama.\n2019. On the Cross-lingual Transferability of Mono-\nlingual Representations. arXiv:1910.11856 [cs].\nArXiv: 1910.11856.\n\nMikel Artetxe and Holger Schwenk. 2018. Mas-\nsively Multilingual Sentence Embeddings for\nZero-Shot Cross-Lingual Transfer and Beyond.\narXiv:1812.10464 [cs]. ArXiv: 1812.10464.\n\nMikel Artetxe and Holger Schwenk. 2019. Margin-\nbased Parallel Corpus Mining with Multilingual Sen-\ntence Embeddings. In Proceedings of the 57th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 3197-3203, Florence, Italy. Asso-\nciation for Computational Linguistics.\n\nAkari Asai, Akiko Eriguchi, Kazuma Hashimoto,\nand Yoshimasa Tsuruoka. 2018. Multilingual Ex-\ntractive Reading Comprehension by Runtime Ma-\nchine Translation. arXiv: 1809.03275 [cs]. ArXiv:\n1809.03275.\n\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine\nBordes. 2017. Reading Wikipedia to Answer Open-\nDomain Questions. In Proceedings of the 55th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 1870-\n1879, Vancouver, Canada. Association for Computa-\ntional Linguistics.\n\nPhilipp Cimiano, Vanessa Lopez, Christina Unger,\nElena Cabrio, Axel-Cyrille Ngonga Ngomo, and\nSebastian Walter. 2013. Multilingual Question\nAnswering over Linked Data (QALD-3): Lab\nOverview. In CLEF.\n\nAlexis Conneau, Guillaume Lample, Ruty Rinott,\nAdina Williams, Samuel R. Bowman, Holger\nSchwenk, and Veselin Stoyanov. 2018. XNLI:\nEvaluating Cross-lingual Sentence Representations.\narXiv:1809.05053 [cs]. ArXiv: 1809.05053.\n\nYiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Shijin\nWang, and Guoping Hu. 2019a. Cross-Lingual Ma-\nchine Reading Comprehension. In Proceedings of\nthe 2019 Conference on Empirical Methods in Nat-\nural Language Processing and the 9th International\n\n7323\n", "vlm_text": "\nFinally, a large body of work suggests QA mod- els are over-reliant on word-matching between question and context ( Jia and Liang ,  2017 ;  Gan and Ng ,  2019 ). G-XLT represents an interesting test- bed, as simple symbolic matching is less straight- forward when questions and contexts use different languages. However, the performance drop from XLT is relatively small (8.2 mean F1), suggesting word-matching in cross-lingual models is more nu- anced and robust than it may initially appear. \n7 Conclusion \nWe have introduced MLQA, a highly-parallel mul- tilingual QA benchmark in seven languages. We developed several baselines on two cross-lingual understanding tasks on MLQA with state-of-the-art methods, and demonstrate signiﬁcant room for im- provement. We hope that MLQA will help to catal- yse work in cross-lingual QA to close the gap be- tween training and testing language performance. \nAcknowledgements \nThe authors would like to acknowledge their crowd- working and translation colleagues for their work on MLQA. The authors would also like to thank Yuxiang Wu, Andres Compara Nu nez, Kartikay Khandelwal, Nikhil Gupta, Chau Tran, Ahmed Kishky, Haoran Li, Tamar Lavee, Ves Stoyanov and the anonymous reviewers for their feedback and comments. \nReferences \nAlan Akbik, Laura Chiticariu, Marina Danilevsky, Yun- yao Li, Shivakumar Vaithyanathan, and Huaiyu Zhu. 2015.  Generating High Quality Proposition Banks for Multilingual Semantic Role Labeling . In  Pro- ceedings of the 53rd Annual Meeting of the Associa- tion for Computational Linguistics and the 7th Inter- national Joint Conference on Natural Language Pro- cessing (Volume 1: Long Papers) , pages 397–407, \nBeijing, China. Association for Computational Lin- guistics. \nChris Alberti, Daniel Andor, Emily Pitler, Jacob De- vlin, and Michael Collins. 2019.  Synthetic QA Cor- pora Generation with Roundtrip Consistency . In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 6168–6173, Florence, Italy. Association for Compu- tational Linguistics. \nAhmed Aly, Kushal Lakhotia, Shicong Zhao, Mri- nal Mohit, Barlas Oguz, Abhinav Arora, Sonal Gupta, Christopher Dewan, Stef Nelson-Lindall, and Rushin Shah. 2018. Pytext: A seamless path from nlp research to production. arXiv preprint arXiv:1812.08729 . \nMikel Artetxe, Sebastian Ruder, and Dani Yogatama. 2019.  On the Cross-lingual Transferability of Mono- lingual Representations . arXiv:1910.11856 [cs] . ArXiv: 1910.11856. \nMikel Artetxe and Holger Schwenk. 2018. Mas- sively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond . arXiv:1812.10464 [cs] . ArXiv: 1812.10464. \nMikel Artetxe and Holger Schwenk. 2019. Margin- based Parallel Corpus Mining with Multilingual Sen- tence Embeddings . In  Proceedings of the 57th An- nual Meeting of the Association for Computational Linguistics , pages 3197–3203, Florence, Italy. Asso- ciation for Computational Linguistics. \nAkari Asai, Akiko Eriguchi, Kazuma Hashimoto, and Yoshimasa Tsuruoka. 2018. Multilingual Ex- tractive Reading Comprehension by Runtime Ma- chine Translation .  arXiv:1809.03275 [cs] . ArXiv: 1809.03275. \nDanqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. 2017.  Reading Wikipedia to Answer Open- Domain Questions . In  Proceedings of the 55th An- nual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 1870– 1879, Vancouver, Canada. Association for Computa- tional Linguistics. \nPhilipp Cimiano, Vanessa L´ opez, Christina Unger, Elena Cabrio, Axel-Cyrille Ngonga Ngomo, and Sebastian Walter. 2013. Multilingual Question Answering over Linked Data (QALD-3): Lab Overview. In  CLEF . \nAlexis Conneau, Guillaume Lample, Ruty Rinott, Adina Williams, Samuel R. Bowman, Holger Schwenk, and Veselin Stoyanov. 2018. XNLI: Evaluating Cross-lingual Sentence Representations . arXiv:1809.05053 [cs] . ArXiv: 1809.05053. \nYiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Shijin Wang, and Guoping Hu. 2019a.  Cross-Lingual Ma- chine Reading Comprehension . In  Proceedings of the 2019 Conference on Empirical Methods in Nat- ural Language Processing and the 9th International "}
{"page": 9, "image_path": "doc_images/2020.acl-main.653_9.jpg", "ocr_text": "Joint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 1586-1595, Hong Kong,\nChina. Association for Computational Linguistics.\n\nYiming Cui, Ting Liu, Wanxiang Che, Li Xiao,\nZhipeng Chen, Wentao Ma, Shijin Wang, and Guop-\ning Hu. 2019b. A Span-Extraction Dataset for Chi-\nnese Machine Reading Comprehension. In Proceed-\nings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and 9th Interna-\ntional Joint Conference on Natural Language Pro-\ncessing. Association for Computational Linguistics.\n\nYiming Cui, Ting Liu, Li Xiao, Zhipeng Chen, Wentao\nMa, Wanxiang Che, Shijin Wang, and Guoping Hu.\n2018. A Span-Extraction Dataset for Chinese Ma-\nchine Reading Comprehension. arXiv: 1810.07366\n[cs]. ArXiv: 1810.07366.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\nDeep Bidirectional Transformers for Language Un-\nderstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume I (Long and Short Papers),\npages 4171-4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\n\nWee Chung Gan and Hwee Tou Ng. 2019. Improv-\ning the Robustness of Question Answering Systems\nto Question Paraphrasing. In Proceedings of the\n57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 6065-6075, Florence,\nItaly. Association for Computational Linguistics.\n\nDeepak Gupta, Surabhi Kumari, Asif Ekbal, and Push-\npak Bhattacharyya. 2018. MMQA: A Multi-domain\nMulti-lingual Question-Answering Framework for\nEnglish and Hindi. In LREC.\n\nMomchil Hardalov, Ivan Koychev, and Preslav Nakov.\n2019. Beyond English-only Reading Comprehen-\nsion: Experiments in Zero-Shot Multilingual Trans-\nfer for Bulgarian. arXiv:1908.01519 [cs]. ArXiv:\n1908.01519.\n\nWei He, Kai Liu, Jing Liu, Yajuan Lyu, Shiqi Zhao,\nXinyan Xiao, Yuan Liu, Yizhong Wang, Hua Wu,\nQiaogiao She, Xuan Liu, Tian Wu, and Haifeng\nWang. 2018. DuReader: a Chinese Machine Read-\ning Comprehension Dataset from Real-world Appli-\ncations. In Proceedings of the Workshop on Ma-\nchine Reading for Question Answering, pages 37-\n46, Melbourne, Australia. Association for Computa-\ntional Linguistics.\n\nKarl Moritz Hermann, Tomas Kocisky, Edward Grefen-\nstette, Lasse Espeholt, Will Kay, Mustafa Suley-\nman, and Phil Blunsom. 2015. Teaching Machines\nto Read and Comprehend. In C. Cortes, N. D.\nLawrence, D. D. Lee, M. Sugiyama, and R. Gar-\nnett, editors, Advances in Neural Information Pro-\ncessing Systems 28, pages 1693-1701. Curran Asso-\nciates, Inc.\n\nFelix Hill, Antoine Bordes, Sumit Chopra, and Jason\nWeston. 2016. The Goldilocks Principle: Reading\nChildren’s Books with Explicit Memory Representa-\ntions. In 4th International Conference on Learning\nRepresentations, ICLR 2016, San Juan, Puerto Rico,\nMay 2-4, 2016, Conference Track Proceedings.\n\nMatthew Honnibal and Ines Montani. 2017. spaCy 2:\nNatural language understanding with Bloom embed-\ndings, convolutional neural networks and incremen-\ntal parsing. To appear.\n\nTsung-Yuan Hsu, Chi-Liang Liu, and Hung-yi Lee.\n2019. Zero-shot Reading Comprehension by Cross-\nlingual Transfer Learning with Multi-lingual Lan-\nguage Representation Model. In Proceedings of\nthe 2019 Conference on Empirical Methods in Nat-\nural Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 5935-5942, Hong Kong,\nChina. Association for Computational Linguistics.\n\nJunjie Hu, Sebastian Ruder, Aditya Siddhant, Gra-\nham Neubig, Orhan Firat, and Melvin Johnson.\n2020. Xtreme: A massively multilingual multi-task\nbenchmark for evaluating cross-lingual generaliza-\ntion. ArXiv, abs/2003.11080.\n\nRobin Jia and Percy Liang. 2017. Adversarial Ex-\namples for Evaluating Reading Comprehension Sys-\ntems. In Proceedings of the 2017 Conference on\nEmpirical Methods in Natural Language Processing,\npages 2021-2031, Copenhagen, Denmark. Associa-\ntion for Computational Linguistics.\n\nYimin Jing, Deyi Xiong, and Zhen Yan. 2019. BiPaR:\nA Bilingual Parallel Dataset for Multilingual and\nCross-lingual Reading Comprehension on Novels.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 2452-\n2462, Hong Kong, China. Association for Computa-\ntional Linguistics.\n\nMandar Joshi, Eunsol Choi, Daniel Weld, and Luke\nZettlemoyer. 2017. TriviaQA: A Large Scale Dis-\ntantly Supervised Challenge Dataset for Reading\nComprehension. In Proceedings of the 55th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 1601-1611,\nVancouver, Canada. Association for Computational\nLinguistics.\n\nAlexandre Klementiev, Ivan Titov, and Binod Bhat-\ntarai. 2012. Inducing Crosslingual Distributed Rep-\nresentations of Words. In Proceedings of COLING\n2012, pages 1459-1474, Mumbai, India. The COL-\nING 2012 Organizing Committee.\n\nVishwajeet Kumar, Nitish Joshi, Arijit Mukherjee,\nGanesh Ramakrishnan, and Preethi Jyothi. 2019.\nCross-Lingual Training for Automatic Question\nGeneration. arXiv:1906.02525 [cs]. ArXiv:\n1906.02525.\n\n7324\n", "vlm_text": "Joint Conference on Natural Language Processing (EMNLP-IJCNLP) , pages 1586–1595, Hong Kong, China. Association for Computational Linguistics. \nYiming Cui, Ting Liu, Wanxiang Che, Li Xiao, Zhipeng Chen, Wentao Ma, Shijin Wang, and Guop- ing Hu. 2019b. A Span-Extraction Dataset for Chi- nese Machine Reading Comprehension. In  Proceed- ings of the 2019 Conference on Empirical Methods in Natural Language Processing and 9th Interna- tional Joint Conference on Natural Language Pro- cessing . Association for Computational Linguistics. \nYiming Cui, Ting Liu, Li Xiao, Zhipeng Chen, Wentao Ma, Wanxiang Che, Shijin Wang, and Guoping Hu. 2018.  A Span-Extraction Dataset for Chinese Ma- chine Reading Comprehension .  arXiv:1810.07366 [cs] . ArXiv: 1810.07366. \nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Un- derstanding . In  Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) , pages 4171–4186, Minneapolis, Minnesota. Associ- ation for Computational Linguistics. \nWee Chung Gan and Hwee Tou Ng. 2019. Improv- ing the Robustness of Question Answering Systems to Question Paraphrasing . In  Proceedings of the 57th Annual Meeting of the Association for Com- putational Linguistics , pages 6065–6075, Florence, Italy. Association for Computational Linguistics. \nDeepak Gupta, Surabhi Kumari, Asif Ekbal, and Push- pak Bhattacharyya. 2018. MMQA: A Multi-domain Multi-lingual Question-Answering Framework for English and Hindi. In  LREC . \nMomchil Hardalov, Ivan Koychev, and Preslav Nakov. 2019. Beyond English-only Reading Comprehen- sion: Experiments in Zero-Shot Multilingual Trans- fer for Bulgarian .  arXiv:1908.01519 [cs] . ArXiv: 1908.01519. \nWei He, Kai Liu, Jing Liu, Yajuan Lyu, Shiqi Zhao, Xinyan Xiao, Yuan Liu, Yizhong Wang, Hua Wu, Qiaoqiao She, Xuan Liu, Tian Wu, and Haifeng Wang. 2018.  DuReader: a Chinese Machine Read- ing Comprehension Dataset from Real-world Appli- cations . In  Proceedings of the Workshop on Ma- chine Reading for Question Answering , pages 37– 46, Melbourne, Australia. Association for Computa- tional Linguistics. \nKarl Moritz Hermann, Tomas Kocisky, Edward Grefen- stette, Lasse Espeholt, Will Kay, Mustafa Suley- man, and Phil Blunsom. 2015.  Teaching Machines to Read and Comprehend . In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Gar- nett, editors,  Advances in Neural Information Pro- cessing Systems 28 , pages 1693–1701. Curran Asso- ciates, Inc. \nFelix Hill, Antoine Bordes, Sumit Chopra, and Jason Weston. 2016.  The Goldilocks Principle: Reading Children’s Books with Explicit Memory Representa- tions . In  4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings . \nMatthew Honnibal and Ines Montani. 2017. spaCy 2: Natural language understanding with Bloom embed- dings, convolutional neural networks and incremen- tal parsing. To appear. \nTsung-Yuan Hsu, Chi-Liang Liu, and Hung-yi Lee. 2019.  Zero-shot Reading Comprehension by Cross- lingual Transfer Learning with Multi-lingual Lan- guage Representation Model . In  Proceedings of the 2019 Conference on Empirical Methods in Nat- ural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) , pages 5935–5942, Hong Kong, China. Association for Computational Linguistics. \nJunjie Hu, Sebastian Ruder, Aditya Siddhant, Gra- ham Neubig, Orhan Firat, and Melvin Johnson. 2020. Xtreme: A massively multilingual multi-task benchmark for evaluating cross-lingual generaliza- tion.  ArXiv , abs/2003.11080. \nRobin Jia and Percy Liang. 2017. Adversarial Ex- amples for Evaluating Reading Comprehension Sys- tems . In  Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing , pages 2021–2031, Copenhagen, Denmark. Associa- tion for Computational Linguistics. \nYimin Jing, Deyi Xiong, and Zhen Yan. 2019.  BiPaR: A Bilingual Parallel Dataset for Multilingual and Cross-lingual Reading Comprehension on Novels . In  Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Lan- guage Processing (EMNLP-IJCNLP) , pages 2452– 2462, Hong Kong, China. Association for Computa- tional Linguistics. \nMandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. 2017.  TriviaQA: A Large Scale Dis- tantly Supervised Challenge Dataset for Reading Comprehension . In  Proceedings of the 55th Annual Meeting of the Association for Computational Lin- guistics (Volume 1: Long Papers) , pages 1601–1611, Vancouver, Canada. Association for Computational Linguistics. \nAlexandre Klementiev, Ivan Titov, and Binod Bhat- tarai. 2012.  Inducing Crosslingual Distributed Rep- resentations of Words . In  Proceedings of COLING 2012 , pages 1459–1474, Mumbai, India. The COL- ING 2012 Organizing Committee. \nVishwajeet Kumar, Nitish Joshi, Arijit Mukherjee, Ganesh Ramakrishnan, and Preethi Jyothi. 2019. Cross-Lingual Training for Automatic Question Generation . arXiv:1906.02525 [cs] . ArXiv: 1906.02525. "}
{"page": 10, "image_path": "doc_images/2020.acl-main.653_10.jpg", "ocr_text": "Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nfield, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Matthew Kelcey,\nJacob Devlin, Kenton Lee, Kristina N. Toutanova,\nLlion Jones, Ming-Wei Chang, Andrew Dai, Jakob\nUszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-\nral Questions: a Benchmark for Question Answering\nResearch. Transactions of the Association of Com-\nputational Linguistics.\n\nGuillaume Lample and Alexis Conneau. 2019.\nCross-lingual Language Model _ Pretraining.\narXiv:1901.07291 [cs]. ArXiv: 1901.07291.\n\nChia-Hsuan Lee and Hung-Yi Lee. 2019. — Cross-\nLingual Transfer Learning for Question Answering.\narXiv:1907.06042 [cs]. ArXiv: 1907.06042.\n\nKyungjae Lee, Sunghyun Park, Hojae Han, Jinyoung\nYeo, Seung-won Hwang, and Juho Lee. 2019. Learn-\ning with Limited Data for Multilingual Reading\nComprehension. In Proceedings of the 2019 Con-\nference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 2833-2843, Hong Kong, China. As-\nsociation for Computational Linguistics.\n\nKyungjae Lee, Kyoungho Yoon, Sunghyun Park, and\nSeung-won Hwang. 2018. Semi-supervised Train-\ning Data Generation for Multilingual Question An-\nswering. In Proceedings of the Eleventh Interna-\ntional Conference on Language Resources and Eval-\nuation (LREC 2018), Miyazaki, Japan. European\nLanguage Resources Association (ELRA).\n\nDavid D. Lewis, Yiming yang, Tony G. Rose, and Fan\nLi. 2004. Revl: A new benchmark collection for\ntext categorization research. jmlr, 5:361-397.\n\nPatrick Lewis, Ludovic Denoyer, and Sebastian Riedel.\n2019. Unsupervised Question Answering by Cloze\nTranslation. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 4896-4910, Florence, Italy. Associa-\ntion for Computational Linguistics.\n\nYaobo Liang, Nan Duan, Yeyun Gong, Ning Wu, Fen-\nfei Guo, Weizhen Qi, Ming Gong, Linjun Shou,\nDaxin Jiang, Guihong Cao, Xiaodong Fan, Bruce\nZhang, Rahul Agrawal, Edward Cui, Sining Wei,\nTaroon Bharti, Ying Qiao, Jiun-Hung Chen, Winnie\nWu, Shuguang Liu, Fan Yang, Rangan Majumder,\nand Ming Zhou. 2020. Xglue: A new benchmark\ndataset for cross-lingual pre-training, understanding\nand generation. ArXiv, abs/2004.01401.\n\nSeungyoung Lim, Myungji Kim, and Jooyoul Lee.\n2019. Korquad1.0: Korean qa dataset for ma-\nchine reading comprehension. arXiv: 1909.07005v2\n[cs.CL].\n\nJiahua Liu, Yankai Lin, Zhiyuan Liu, and Maosong\nSun. 2019a. XQA: A Cross-lingual Open-domain\nQuestion Answering Dataset. In Proceedings of\nACL 2019.\n\nPengyuan Liu, Yuning Deng, Chenghao Zhu, and Han\nHu. 2019b. XCMRC: Evaluating Cross-lingual Ma-\nchine Reading Comprehension. arXiv: 1908.05416\n[cs]. ArXiv: 1908.05416.\n\nHussein Mozannar, Karl El Hajal, Elie Maamary, and\nHazem Hajj. 2019. Neural Arabic Question Answer-\ning. arXiv: 1906.05394 [cs]. ArXiv: 1906.05394.\n\nPranav Rajpurkar, Robin Jia, and Percy Liang. 2018.\nKnow What You Don’t Know: Unanswerable Ques-\ntions for SQUAD. In Proceedings of the 56th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 2: Short Papers), pages 784—\n789, Melbourne, Australia. Association for Compu-\ntational Linguistics.\n\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ Questions\nfor Machine Comprehension of Text. In Proceed-\nings of the 2016 Conference on Empirical Methods\nin Natural Language Processing, pages 2383-2392,\nAustin, Texas. Association for Computational Lin-\nguistics.\n\nMatthew Richardson. 2013. MCTest: A Challenge\nDataset for the Open-Domain Machine Comprehen-\nsion of Text. In Proceedings of the 2013 Conference\non Emprical Methods in Natural Language Process-\ning (EMNLP 2013).\n\nHolger Schwenk, Vishrav Chaudhary, Shuo Sun,\nHongyu Gong, and Francisco Guzman. 2019.\nWikimatrix: Mining 135m parallel sentences in\n1620 language pairs from wikipedia. CoRR,\nabs/1907.05791.\n\nHolger Schwenk and Xian Li. 2018. A corpus for mul-\ntilingual document classification in eight languages.\nIn Proceedings of the Eleventh International Confer-\nence on Language Resources and Evaluation (LREC\n2018), Miyazaki, Japan. European Language Re-\nsources Association (ELRA).\n\nChih Chieh Shao, Trois Liu, Yuting Lai, Yiying\nTseng, and Sam Tsai. 2018. DRCD: a Chi-\nnese Machine Reading Comprehension Dataset.\narXiv: 1806.00920 [cs]. ArXiv: 1806.00920.\n\nJasdeep Singh, Bryan McCann, Nitish Shirish\nKeskar, Caiming Xiong, and Richard Socher. 2019.\nXLDA: Cross-Lingual Data Augmentation for Nat-\nural Language Inference and Question Answering.\narXiv:1905.11471 [cs]. ArXiv: 1905.11471.\n\nSaku Sugawara, Kentaro Inui, Satoshi Sekine, and\nAkiko Aizawa. 2018. What Makes Reading Com-\nprehension Questions Easier? In Proceedings of\nthe 2018 Conference on Empirical Methods in Nat-\nural Language Processing, pages 4208-4219, Brus-\nsels, Belgium. Association for Computational Lin-\nguistics.\n\n7325\n", "vlm_text": "Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red- ﬁeld, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Kenton Lee, Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019.  Natu- ral Questions: a Benchmark for Question Answering Research .  Transactions of the Association of Com- putational Linguistics . \nGuillaume Lample and Alexis Conneau. 2019. Cross-lingual Language Model Pretraining arXiv:1901.07291 [cs] . ArXiv: 1901.07291. \nChia-Hsuan Lee and Hung-Yi Lee. 2019. Cross- Lingual Transfer Learning for Question Answering . arXiv:1907.06042 [cs] . ArXiv: 1907.06042. \nKyungjae Lee, Sunghyun Park, Hojae Han, Jinyoung Yeo, Seung-won Hwang, and Juho Lee. 2019.  Learn- ing with Limited Data for Multilingual Reading Comprehension . In  Proceedings of the 2019 Con- ference on Empirical Methods in Natural Language Processing and the 9th International Joint Confer- ence on Natural Language Processing (EMNLP- IJCNLP) , pages 2833–2843, Hong Kong, China. As- sociation for Computational Linguistics. \nKyungjae Lee, Kyoungho Yoon, Sunghyun Park, and Seung-won Hwang. 2018.  Semi-supervised Train- ing Data Generation for Multilingual Question An- swering . In  Proceedings of the Eleventh Interna- tional Conference on Language Resources and Eval- uation (LREC 2018) , Miyazaki, Japan. European Language Resources Association (ELRA). \nDavid D. Lewis, Yiming yang, Tony G. Rose, and Fan Li. 2004. Rcv1: A new benchmark collection for text categorization research.  jmlr , 5:361–397. \nPatrick Lewis, Ludovic Denoyer, and Sebastian Riedel. 2019.  Unsupervised Question Answering by Cloze Translation . In  Proceedings of the 57th Annual Meeting of the Association for Computational Lin- guistics , pages 4896–4910, Florence, Italy. Associa- tion for Computational Linguistics. \nYaobo Liang, Nan Duan, Yeyun Gong, Ning Wu, Fen- fei Guo, Weizhen Qi, Ming Gong, Linjun Shou, Daxin Jiang, Guihong Cao, Xiaodong Fan, Bruce Zhang, Rahul Agrawal, Edward Cui, Sining Wei, Taroon Bharti, Ying Qiao, Jiun-Hung Chen, Winnie Wu, Shuguang Liu, Fan Yang, Rangan Majumder, and Ming Zhou. 2020. Xglue: A new benchmark dataset for cross-lingual pre-training, understanding and generation.  ArXiv , abs/2004.01401. \nSeungyoung Lim, Myungji Kim, and Jooyoul Lee. 2019. Korquad1.0: Korean qa dataset for ma- chine reading comprehension .  arXiv:1909.07005v2 [cs.CL] . \nJiahua Liu, Yankai Lin, Zhiyuan Liu, and Maosong Sun. 2019a. XQA: A Cross-lingual Open-domain Question Answering Dataset. In  Proceedings of ACL 2019 . \nPengyuan Liu, Yuning Deng, Chenghao Zhu, and Han Hu. 2019b.  XCMRC: Evaluating Cross-lingual Ma- chine Reading Comprehension .  arXiv:1908.05416 [cs] . ArXiv: 1908.05416. \nHussein Mozannar, Karl El Hajal, Elie Maamary, and Hazem Hajj. 2019.  Neural Arabic Question Answer- ing .  arXiv:1906.05394 [cs] . ArXiv: 1906.05394. \nPranav Rajpurkar, Robin Jia, and Percy Liang. 2018. Know What You Don’t Know: Unanswerable Ques- tions for SQuAD . In  Proceedings of the 56th An- nual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers) , pages 784– 789, Melbourne, Australia. Association for Compu- tational Linguistics. \nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD:   $100{,}000{+}$   Questions for Machine Comprehension of Text . In  Proceed- ings of the 2016 Conference on Empirical Methods in Natural Language Processing , pages 2383–2392, Austin, Texas. Association for Computational Lin- guistics. \nMatthew Richardson. 2013. MCTest: A Challenge Dataset for the Open-Domain Machine Comprehen- sion of Text . In  Proceedings of the 2013 Conference on Emprical Methods in Natural Language Process- ing (EMNLP 2013) . \nHolger Schwenk, Vishrav Chaudhary, Shuo Sun, Hongyu Gong, and Francisco Guzm´ an. 2019. Wikimatrix: Mining   $135\\mathrm{m}$   parallel sentences in 1620 language pairs from wikipedia . CoRR , abs/1907.05791. \nHolger Schwenk and Xian Li. 2018.  A corpus for mul- tilingual document classiﬁcation in eight languages . In  Proceedings of the Eleventh International Confer- ence on Language Resources and Evaluation (LREC 2018) , Miyazaki, Japan. European Language Re- sources Association (ELRA). \nChih Chieh Shao, Trois Liu, Yuting Lai, Yiying Tseng, and Sam Tsai. 2018. DRCD: a Chi- nese Machine Reading Comprehension Dataset . arXiv:1806.00920 [cs] . ArXiv: 1806.00920. \nJasdeep Singh, Bryan McCann, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. 2019. XLDA: Cross-Lingual Data Augmentation for Nat- ural Language Inference and Question Answering . arXiv:1905.11471 [cs] . ArXiv: 1905.11471. \nSaku Sugawara, Kentaro Inui, Satoshi Sekine, and Akiko Aizawa. 2018.  What Makes Reading Com- prehension Questions Easier? In  Proceedings of the 2018 Conference on Empirical Methods in Nat- ural Language Processing , pages 4208–4219, Brus- sels, Belgium. Association for Computational Lin- guistics. "}
{"page": 11, "image_path": "doc_images/2020.acl-main.653_11.jpg", "ocr_text": "Adam Trischler, Tong Wang, Xingdi Yuan, Justin Har-\nris, Alessandro Sordoni, Philip Bachman, and Ka-\nheer Suleman. 2017. NewsQA: A Machine Compre-\nhension Dataset. In Proceedings of the 2nd Work-\nshop on Representation Learning for NLP, pages\n191-200, Vancouver, Canada. Association for Com-\nputational Linguistics.\n\nChristina Unger, Corina Forescu, Vanessa Lopez, Axel-\nCyrille Ngonga Ngomo, Elena Cabrio, Philipp Cimi-\nano, and Sebastian Walter. 2015. Question Answer-\ning over Linked Data (QALD-5). In CLEF.\n\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2019.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In Inter-\nnational Conference on Learning Representations.\n\nYi Yang, Wen-tau Yih, and Christopher Meek. 2015.\nWikiQA: A Challenge Dataset for Open-Domain\nQuestion Answering. In Proceedings of the 2015\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 2013-2018, Lisbon, Portu-\ngal. Association for Computational Linguistics.\n\nElizaveta  Zimina, Jyrki Nummenmaa, Kalervo\nJarvelin, Jaakko Peltonen, and Kostas Stefani-\ndis. 2018. MuG-QA: Multilingual Grammatical\nQuestion Answering for RDF Data. 20/8 IEEE\nInternational Conference on Progress in Informatics\nand Computing (PIC), pages 57-61.\n\n7326\n", "vlm_text": "Adam Trischler, Tong Wang, Xingdi Yuan, Justin Har- ris, Alessandro Sordoni, Philip Bachman, and Ka- heer Suleman. 2017.  NewsQA: A Machine Compre- hension Dataset . In  Proceedings of the 2nd Work- shop on Representation Learning for NLP , pages 191–200, Vancouver, Canada. Association for Com- putational Linguistics. Christina Unger, Corina Forescu, Vanessa Lopez, Axel- Cyrille Ngonga Ngomo, Elena Cabrio, Philipp Cimi- ano, and Sebastian Walter. 2015. Question Answer- ing over Linked Data (QALD-5). In  CLEF . Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. 2019. GLUE: A multi-task benchmark and analysis plat- form for natural language understanding . In  Inter- national Conference on Learning Representations . Yi Yang, Wen-tau Yih, and Christopher Meek. 2015. WikiQA: A Challenge Dataset for Open-Domain Question Answering . In  Proceedings of the 2015 Conference on Empirical Methods in Natural Lan- guage Processing , pages 2013–2018, Lisbon, Portu- gal. Association for Computational Linguistics. Elizaveta Zimina, Jyrki Nummenmaa, Kalervo Jarvelin, Jaakko Peltonen, and Kostas Stefani- dis. 2018. MuG-QA: Multilingual Grammatical Question Answering for RDF Data. 2018 IEEE International Conference on Progress in Informatics and Computing (PIC) , pages 57–61. "}
{"page": 12, "image_path": "doc_images/2020.acl-main.653_12.jpg", "ocr_text": "HIT Instructions (Click to expand)\n\nWhen asking questions, avoid using the same words/phrases as the paragraph, be specific, and\nyou are encouraged to ask hard questions. Rembember the answer must come from the\nsentence in bold. Refer the instructions if having difficulty operating the HIT\n\nQuestion 1/5\n\nDioxygen is used in cellular respiration and many major classes of organic molecules in\nliving organisms contain oxygen, such as proteins, nucleic acids, carbohydrates, and fats,\nas do the major constituent inorganic compounds of animal shells, teeth, and bone. Most of\nthe mass of living organisms is oxygen as a component of water, the major constituent of\nlifeforms. Oxygen is continuously replenished in Earth's atmosphere by\nphotosynthesis, which uses the energy of sunlight to produce oxygen from water and\ncarbon dioxide. Oxygen is too chemically reactive to remain a free element in air without\nbeing continuously replenished by the photosynthetic action of living organisms. Another\nform (allotrope) of oxygen, ozone (03), strongly absorbs ultraviolet UVB radiation and the\nhigh-altitude ozone layer helps protect the biosphere from ultraviolet radiation.\n\nYour Question: Ask a question here. Try using your own words MRM ter\n\nFigure 5: English QA annotation interface screenshot\n\nen de es ar zh* vi hi\nContext 157.5 102.2 103.4 116.8 222.9 195.1 141.5\nQuestion 84 7.7 86 76 143 106 9.3\n\nAnswer 3.1 3.2 41 34 82 45 3.6\n\nTable 8: Mean Sequence lengths (tokens) in MLQA.\n*calculated with mixed segmentation (section 4.1)\n\nA Appendices\n\nA.1 Annotation Interface\n\nFigure 5 shows a screenshot of the annotation inter-\nface. Workers are asked to write a question in the\nbox, and highlight an answer using the mouse in the\nsentence that is in bold. There are a number of data\ninput validation features to assist workers, as well\nas detailed instructions in a drop-down window,\nwhich are shown in Figure 6\n\nA.2 Additional MLQA Statistics\n\nFigure 7 shows the distribution of wh words in ques-\ntions in both MLQA-en and SQuAD v.1.1. The\ndistributions are very similar, suggesting training\non SQuAD data is an appropriate training dataset\nchoice.\n\nTable 4 shows the number of Wikipedia articles\nthat feature at least one of their paragraphs as a con-\ntext paragraph in MLQA, along with the number of\nunique context paragraphs in MLQA. There are 1.9\ncontext paragraphs from each article on average.\nThis is in contrast to SQUAD, which instead fea-\ntures a small number of curated articles, but more\ndensely annotated, with 43 context paragraphs per\narticle on average. Thus, MLQA covers a much\nbroader range of topics than SQUAD.\n\nTable 8 shows statistics about the lengths of con-\n\nHIT Instructions (Click to collapse)\n\nSpend around 6 minutes in total to ask one question on each of these five\nparagraphs.\n\nSelect the answer from the paragraph by clicking on ‘select answer' and\nthen highlight the smallest segment of the paragraph that answers the\nquestion.\n\nThe answer must come from the sentence in bold (the interface will not\nallow answers to be selected from other sentences)\n\nWhen asking questions:\n+ Avoid using the same words/phrases as in the paragraph.\n+ You are encouraged to pose hard questions\n+ Be specific, and avoid pronouns\n+ Do not ask ‘fill in the blank style’ questions\n+ Select the shortest answer to your question (NOT the entire bold\nsentence)\n\nIf you cant think of a question for a particular paragraph, click the \"no\nquestion possible\" button. You should only do this if there is no question\nyou could possibly ask, so please try hard to think of questions.\n\nAn example is provided below:\n\nMost of the mass of living organisms is oxygen as a component of\nwater, the major constituent of lifeforms. Oxygen is continuously\nreplenished in Earth's atmosphere by photosynthesis, which\nuses th gy of sunlight to produce oxygen from water and\ncarbon dioxide. Oxygen is too chemically reactive to remain a free\nelement in air without being continuously replenished by the\nphotosynthetic action of living organisms.\n\nYour Question: What is the name of the biol JUSTO Car Oyd\nnswer Saved. Click To Change\n\nYour Answer:\n\nphotosynthesis\n\nDetailed instructions for using the interface:\n\nFirst, click on the \"Your Question\" field, and write your question. Then,\nclick the \"Select Answer\" button, and highlight your answer from the\nsentence in bold in the paragraph using the mouse/cursor. The \"Your\nAnswer\" field will automatically populate with your highlighted answer. If\nyou made a mistake highlighting your answer, click \"Answer Saved. Click\nto change” and highlight a new answer using the mouse/cursor. Once you\nhave provided a question and answer for the first paragraph, the second\nwill appear. Once you have provided questions and answers for five\nparagraphs, click the submit button to end the HIT.\n\nIf you encounter problems when performing this HIT, you can contact us\nHere.\n\nFigure 6: English annotation instructions screenshot\n\ntexts, questions and answers in MLQA. Vietnamese\nhas the longest contexts on average and German\nare shortest, but all languages have a substantial\ntail of long contexts. Other than Chinese, answers\nare on average 3 to 4 tokens.\n\nA.3 QA Performance stratified by question\nand answer types\n\nTo examine how performance varies across lan-\nguages for different types of questions, we stratify\nMLQA with three criteria — By English Wh-word,\nby answer Named-Entity type and by English Ques-\ntion Difficulty\n\n7327\n", "vlm_text": "\n\n\n\nThe image appears to be a screenshot of a user interface, likely from some sort of application or website. It contains a field labeled \"Your Question\" where users are prompted to \"Ask a question here. Try using your own words.\" There is also a button labeled \"No question possible?\" and another section labeled \"Select Answer\" below the prompt.\nThe table presents data organized in rows and columns. The columns represent different language codes: \"en\" (English), \"de\" (German), \"es\" (Spanish), \"ar\" (Arabic), \"zh*\" (likely Mandarin Chinese), \"vi\" (Vietnamese), and \"hi\" (Hindi).\n\nThe rows represent categories labeled \"Context,\" \"Question,\" and \"Answer.\" Each cell contains a numeric value that corresponds to a particular category and language. Here is a breakdown of the values:\n\n- Context:\n  - en: 157.5\n  - de: 102.2\n  - es: 103.4\n  - ar: 116.8\n  - zh*: 222.9\n  - vi: 195.1\n  - hi: 141.5\n\n- Question:\n  - en: 8.4\n  - de: 7.7\n  - es: 8.6\n  - ar: 7.6\n  - zh*: 14.3\n  - vi: 10.6\n  - hi: 9.3\n\n- Answer:\n  - en: 3.1\n  - de: 3.2\n  - es: 4.1\n  - ar: 3.4\n  - zh*: 8.2\n  - vi: 4.5\n  - hi: 3.6\n\nThe context or meaning of these numerical values is not provided in the image, so further information is needed to interpret them correctly.\nTable 8: Mean Sequence lengths (tokens) in MLQA. \\*calculated with mixed segmentation (section  4.1 ) \nA Appendices \nA.1 Annotation Interface \nFigure  5  shows a screenshot of the annotation inter- face. Workers are asked to write a question in the box, and highlight an answer using the mouse in the sentence that is in bold. There are a number of data input validation features to assist workers, as well as detailed instructions in a drop-down window, which are shown in Figure  6 \nA.2 Additional MLQA Statistics \nFigure  7  shows the distribution of wh words in ques- tions in both MLQA-en and SQuAD v.1.1. The distributions are very similar, suggesting training on SQuAD data is an appropriate training dataset choice. \nTable  4  shows the number of Wikipedia articles that feature at least one of their paragraphs as a con- text paragraph in MLQA, along with the number of unique context paragraphs in MLQA. There are 1.9 context paragraphs from each article on average. This is in contrast to SQuAD, which instead fea- tures a small number of curated articles, but more densely annotated, with 43 context paragraphs per article on average. Thus, MLQA covers a much broader range of topics than SQuAD. \nTable  8  shows statistics about the lengths of con- \n\n\n\n\n\nThe image is a screenshot of a section from a digital interface or software. It seems to depict a Q&A or testing module. The section titled \"Your Question\" has an unfinished question cut off after \"What is the name of the biol\" and a note saying \"No question possible?\" In the \"Your Answer\" section, the response is \"photosynthesis.\" There is also a blue button-like area stating \"Answer Saved. Click To Change,\" suggesting that the answer can be modified.\n\n\n\nFigure 6: English annotation instructions screenshot \ntexts, questions and answers in MLQA. Vietnamese has the longest contexts on average and German are shortest, but all languages have a substantial tail of long contexts. Other than Chinese, answers are on average 3 to 4 tokens. \nA.3 QA Performance stratiﬁed by question and answer types \nTo examine how performance varies across lan- guages for different types of questions, we stratify MLQA with three criteria — By English Wh-word, by answer Named-Entity type and by English Ques- tion Difﬁculty "}
{"page": 13, "image_path": "doc_images/2020.acl-main.653_13.jpg", "ocr_text": "lm MLQA-English\nlm SQUAD dev-v1.1\n\n40\n30\n\n20\n\nLL ts an ot.» _.\n& S§ € & s S)\n& bS ws é\n\nProportion of dataset (%)\n\n°\n\n& & St\nFigure 7: Question type distribution (by “wh” word)\nin MLQA-en and SQUAD V1.1. The distributions are\n\nstrikingly similar\n\nen es de vi zh ar hi mean 20\n\nNot\nEntities | “9 20 aw 6 2.8 -7.0 -8.3 -7.2 1\n\nAll 142.6 44.4 $4.8 +3.5 +1.7 +4.4 +4.9 +3.7\nEntities\n\nGpe 7+0.1 -1.4 -0.5 -0.5 -0.8 467 #31 +10\n\nLoc 7-2.9 +0.9 -4.3 -6.1 +0.2 +2.8 -3.1 -18\n\nMisc 4-0.4 -2.5 -1.3 -4.4 +1.7 +2.2 -0.6 -0.8 0\n\nNumeric {#10 47.0 #62 +38 00 +39 474 +42 ;\nOrg 7-0.8 -3.6 -1.9 -0.1 22 456 17 -0.8\n\nPerson 7 -0.4 +3.6 +0.2 -0.4 +1.0 +2.8 +0.5 +1.0\n\n-10\n\nTemporal +4.3 715\n\nlanguage\n-20\n\nFigure 8: Fl score stratified by named entity types in\nanswer spans, relative to overall Fl score for XLM\n\nBy wh-word: First, we split by the English Wh*\nword in the question. This resulting change in F1\nscore compared to the overall F1 score is shown\nin Figure 3, and discussed briefly in the main text.\nThe English wh* word provides a clue as to the type\nof answer the questioner is expecting, and thus acts\nas a way of classifying QA instances into types.\nWe chose the 5 most common wh* words in the\ndataset for this analysis. We see that “when” ques-\ntions are consistently easier than average across\nthe languages, but the pattern is less clear for other\nquestion types. ”’Who” questions also seem easier\nthan average, except for Hindi, where the perfor-\nmance is quite low for these questions. “How”-type\nquestions (such as “how much”, “how many” or\n“how long” ) are also more challenging to answer\nthan average in English compared to the other lan-\nguages. “Where” questions also seem challenging\n\nfor Spanish, German, Chinese and Hindi, but this\nis not true for Arabic or Vietnamese.\n\nBy Named-Entity type We create subsets of\nMLQA by detecting which English named enti-\nties are contained in the answer span. To achieve\nthis, we run Named Entity Recognition using\nSPaCy (Honnibal and Montani, 2017), and de-\ntect where named entity spans overlap with an-\nswer spans. The F1 scores for different answer\ntypes relative to overall Fl score are shown for\nvarious Named Entity types in Figure 8. There\nare some clear trends: Answer spans that contain\nnamed entities are easier to answer than those that\ndo not (the first two rows) for all the languages,\nbut the difference is most pronounced for Ger-\nman. Secondly,“Temporal” answer types (DATE\nand TIME entity labels) are consistently easier\nthan average for all languages, consistent with the\nhigh scores for “when” questions in the previous\nsection. Again, this result is most pronounced\nin German, but is also very strong for Spanish,\nHindi, and Vietnamese. Arabic also performs\nwell for ORG, GPE and LOC answer types, unlike\nmost of the other languages. Numeric questions\n(CARDINAL, ORDINAL, PERCENT, QUANTITY\nand MONEY entity labels) also seem relatively easy\nfor the model in most languages.\n\nBy English Question Difficulty Here, we split\nMLQA into two subsets, according to whether the\nXLM model got the question completely wrong (no\nword overlap with the correct answer). We then\nevaluated the mean F1 score for each language on\nthe two subsets, with the results shown in Figure\n4. We see that questions that are “easy” in English\nalso seem to be easier in the target languages, but\nthe drop in performance for the “hard” subset is\nnot as dramatic as one might expect. This suggests\nthat not all questions that are hard in English in\nMLQA are hard in the target languages. This could\nbe due to the grammar and morphology of differ-\nent languages leading to questions being easier or\nmore difficult to answer, but an another factor is\nthat context documents can be shorter in target lan-\nguages for questions the model struggled to answer\ncorrectly in English, effectively making them eas-\nier. Manual inspection suggests that whilst context\ndocuments are often shorter for when the model\nis correct in the target language, this effect is not\nsufficient to explain the difference in performance.\n\n7328\n", "vlm_text": "The image is a bar chart comparing the distribution of question types based on \"wh\" words in two datasets: MLQA-English and SQuAD V1.1. The \"wh\" words include \"what,\" \"how,\" \"who,\" \"when,\" \"where,\" \"which,\" \"in,\" \"the,\" \"why,\" and \"other.\" The chart shows the proportion of each question type in percentage (%) for both datasets. The bars are color-coded, with blue representing MLQA-English and orange representing SQuAD V1.1. The caption notes that the distributions between the two datasets are strikingly similar. The most common question type in both datasets is \"what,\" which constitutes the largest proportion, followed by question types like \"how,\" \"who,\" \"when,\" and others, with \"other\" types also having a notable presence.\nThe image is a heatmap that presents the F1 score stratified by named entity types in answer spans, relative to the overall F1 score for XLM (presumably a language model). The heatmap displays variations across different languages (en, es, de, vi, zh, ar, hi) and named entity types (Not Entities, All Entities, Gpe, Loc, Misc, Numeric, Org, Person, Temporal).\n\nEach cell in the heatmap shows the relative difference in F1 score, with positive values indicating a higher score and negative values indicating a lower score compared to the overall F1 score. The color gradient ranges from blue (lower relative F1 scores, up to -20) to red (higher relative F1 scores, up to +20), helping to visualize the variations across languages and entity types.\n\nKey observations include:\n- The \"Temporal\" entity type often has the highest positive relative F1 score, especially in languages like German (de) and Vietnamese (vi).\n- \"Not Entities\" generally shows a negative relative F1 score across most languages.\n- \"All Entities\" typically has a positive relative F1 score.\n- For most entity types, scores vary between positive and negative values across languages, indicating that the effectiveness of named entity recognition for different types varies depending on the language.\nBy wh-word: First, we split by the English  $\\mathrm{Wh^{*}}$  word in the question. This resulting change in F1 score compared to the overall F1 score is shown in Figure  3 , and discussed brieﬂy in the main text. The English wh\\* word provides a clue as to the type of answer the questioner is expecting, and thus acts as a way of classifying QA instances into types. We chose the 5 most common wh\\* words in the dataset for this analysis. We see that “when” ques- tions are consistently easier than average across the languages, but the pattern is less clear for other question types. ”Who” questions also seem easier than average, except for Hindi, where the perfor- mance is quite low for these questions. “How”-type questions (such as “how much”, “how many” or “how long” ) are also more challenging to answer than average in English compared to the other lan- guages. “Where” questions also seem challenging for Spanish, German, Chinese and Hindi, but this is not true for Arabic or Vietnamese. \n\nBy Named-Entity type We create subsets of MLQA by detecting which English named enti- ties are contained in the answer span. To achieve this, we run Named Entity Recognition using SPaCy ( Honnibal and Montani ,  2017 ), and de- tect where named entity spans overlap with an- swer spans. The F1 scores for different answer types relative to overall F1 score are shown for various Named Entity types in Figure  8 . There are some clear trends: Answer spans that contain named entities are easier to answer than those that do not (the ﬁrst two rows) for all the languages, but the difference is most pronounced for Ger- man. Secondly,“Temporal” answer types ( DATE and  TIME  entity labels) are consistently easier than average for all languages, consistent with the high scores for “when” questions in the previous section. Again, this result is most pronounced in German, but is also very strong for Spanish, Hindi, and Vietnamese. Arabic also performs well for  ORG ,  GPE  and  LOC  answer types, unlike most of the other languages. Numeric questions ( CARDINAL ,  ORDINAL ,  PERCENT ,  QUANTITY and  MONEY  entity labels) also seem relatively easy for the model in most languages. \nBy English Question Difﬁculty Here, we split MLQA into two subsets, according to whether the XLM model got the question completely wrong (no word overlap with the correct answer). We then evaluated the mean F1 score for each language on the two subsets, with the results shown in Figure 4 . We see that questions that are “easy” in English also seem to be easier in the target languages, but the drop in performance for the “hard” subset is not as dramatic as one might expect. This suggests that not all questions that are hard in English in MLQA are hard in the target languages. This could be due to the grammar and morphology of differ- ent languages leading to questions being easier or more difﬁcult to answer, but an another factor is that context documents can be shorter in target lan- guages for questions the model struggled to answer correctly in English, effectively making them eas- ier. Manual inspection suggests that whilst context documents are often shorter for when the model is correct in the target language, this effect is not sufﬁcient to explain the difference in performance. "}
{"page": 14, "image_path": "doc_images/2020.acl-main.653_14.jpg", "ocr_text": "A.4 Additional G-XLT results\n\nTable 6 in the main text shows for XLM on the\nG-XLT task, and Table 9 for Multilingual-BERT\nrespectively. XLM outperforms M-BERT for most\nlanguage pairs, with a mean G-XLT performance of\n53.4 Fl compared to 47.2 Fl (mean of off-diagonal\nelements of Tables 6 and 9). Multilingual BERT ex-\nhibits more of a preference for English than XLM\nfor G-XLT, and exhibits a bigger performance drop\ngoing from XLT to G-XLT (10.5 mean drop in F1\ncompared to 8.2).\n\nc/q en es de ar hi vi zh\n\nen RINCNNONTN 45.7 40.1 5229542\nes (674 643 585 44.1 381 482 511\nde | 628 574 57.9 388 355 447 463\nar 512 45.3 46.4 45.6 32.1 37.3 40.0\nhi S518 43.2 46.2 36.9 43.8 384 40.5\nvi (614) 521 514 344 35.1 S57) 47.1\nzh (58.0 49.1 49.6 40.5 36.0 44.6 9575\n\nTable 9: Fl Score for M-BERT for G-XLT. Columns\nshow question language, rows show context language.\n\nA.5 Additional preprocessing Details\n\nOpenCC (https: //github.com/BYVoid/OpenCc)\nis used to convert all Chinese contexts to Simplified\nChinese, as wikipedia dumps generally consist of a\nmixture of simplified and traditional Chinese text.\n\nA.6 Further details on Parallel Sentence\nmining\n\nTable 10 shows the number of mined parallel sen-\ntences found in each language, as function of how\nmany languages the sentences are parallel between.\nAs the number of languages that a parallel sen-\ntence is shared between increases, the number of\nsuch sentences decreases. When we look for 7-way\naligned examples, we only find 1340 sentences\nfrom the entirety of the 7 Wikipedia. Additionally,\nmost of these sentences are the first sentence of\nthe article, or are uninteresting. However, if we\nchoose 4-way parallel sentences, there are plenty\nof sentences to choose from. We sample evenly\nfrom each combination of English and 3 of the 6\ntarget languages. This ensures that we have an even\ndistribution over all the target languages, as well as\nensuring we have even numbers of instances that\nwill be parallel between target language combina-\ntions.\n\n7329\n", "vlm_text": "A.4 Additional G-XLT results \nTable  6  in the main text shows for XLM on the G-XLT task, and Table  9  for Multilingual-BERT respectively. XLM outperforms M-BERT for most language pairs, with a mean G-XLT performance of  $53.4\\,\\mathrm{F}1$   compared to 47.2 F1 (mean of off-diagonal elements of Tables  6  and  9 ). Multilingual BERT ex- hibits more of a preference for English than XLM for G-XLT, and exhibits a bigger performance drop going from XLT to G-XLT (10.5 mean drop in F1 compared to 8.2). \nThe table appears to present a matrix of numerical values associated with language codes: \"en\" for English, \"es\" for Spanish, \"de\" for German, \"ar\" for Arabic, \"hi\" for Hindi, \"vi\" for Vietnamese, and \"zh\" for Chinese. The table is structured with these language codes both as column headers and row headers, indicating some form of comparative values between each pair of languages.\n\nEach cell contains a numerical value, which might indicate a score or percentage representing a specific metric involving the two languages that intersect at that cell. The diagonal cells (where the row and column headers are the same language) likely represent comparisons within the same language, which are notably the highest relative to the others in the same row, likely indicating a baseline or maximum reference point.\n\nThe shades of gray in each cell might denote intensity or significance, with darker shades potentially marking higher values. Without specific context or additional captioning, it's unclear what these particular numbers signify—whether they represent translation accuracy, similarity scores, linguistic distances, or some other metric—but they are structured to compare specific relationships between pairs of languages.\nA.5 Additional preprocessing Details \nOpenCC ( https://github.com/BYVoid/OpenCC ) is used to convert all Chinese contexts to Simpliﬁed Chinese, as wikipedia dumps generally consist of a mixture of simpliﬁed and traditional Chinese text. \nA.6 Further details on Parallel Sentence mining \nTable  10  shows the number of mined parallel sen- tences found in each language, as function of how many languages the sentences are parallel between. As the number of languages that a parallel sen- tence is shared between increases, the number of such sentences decreases. When we look for 7-way aligned examples, we only ﬁnd 1340 sentences from the entirety of the 7 Wikipedia. Additionally, most of these sentences are the ﬁrst sentence of the article, or are uninteresting. However, if we choose 4-way parallel sentences, there are plenty of sentences to choose from. We sample evenly from each combination of English and 3 of the 6 target languages. This ensures that we have an even distribution over all the target languages, as well as ensuring we have even numbers of instances that will be parallel between target language combina- tions. "}
{"page": 15, "image_path": "doc_images/2020.acl-main.653_15.jpg", "ocr_text": "N-way en de es ar zh vi hi\n\n2 12219436 3925542 4957438 1047977 1174359 904037 210083\n3. 2143675 1157009 1532811 427609 603938 482488 83495\n4 385396 249022 319902 148348 223513 181353 34050\n5\n6\n7\n\n73918 56756 67383 44684 58814 54884 = 13151\n12333 11171 11935 11081 11485 11507 4486\n1340 1340 1340 1340 1340 1340 1340\n\nTable 10: Number of mined parallel sentences as a function of how many languages the sentences are parallel\nbetween\n\n7330\n", "vlm_text": "The table presents a distribution of elements across different categories based on an N-way classification. The columns are labeled with language codes ('en' for English, 'de' for German, 'es' for Spanish, 'ar' for Arabic, 'zh' for Chinese, 'vi' for Vietnamese, and 'hi' for Hindi). The rows correspond to different N-way combinations, ranging from 2 to 7.\n\nFor each language and each N-way classification:\n- The number in the cell represents a count or frequency associated with that category.\n\nHere's a summary of the data:\n\n- For the 2-way classification, the counts are: \n  - en: 12,219,436\n  - de: 3,925,542\n  - es: 4,957,438\n  - ar: 1,047,977\n  - zh: 11,743,59\n  - vi: 904,037\n  - hi: 210,083\n\n- For the 3-way classification:\n  - en: 2,143,675\n  - de: 1,157,009\n  - es: 1,532,811\n  - ar: 427,609\n  - zh: 603,938\n  - vi: 482,488\n  - hi: 83,495\n\n- For the 4-way classification:\n  - en: 385,396\n  - de: 249,022\n  - es: 319,902\n  - ar: 148,348\n  - zh: 223,513\n  - vi: 181,353\n  - hi: 34,050\n\n- For the 5-way classification:\n  - en: 73,918\n  - de: 56,756\n  - es: 67,383\n  - ar: 44,684\n  - zh: 58,814\n  - vi: 54,884\n  - hi: 13,151\n\n- For the 6-way classification:\n  - en: 12,333\n  - de: 11,171\n  - es: 11,935\n  - ar: 11,081\n  - zh: 11,485\n  - vi: 11,507\n  - hi: 4,486\n\n- For the 7-way classification:\n  - en: 1,340\n  - de: 1,340\n  - es: 1,340\n  - ar: 1,340\n  - zh: 1,340\n  - vi: 1,340\n  - hi: 1,340\n\nThe table appears to depict frequency data or distribution counts related to each language for the N-way categories."}
