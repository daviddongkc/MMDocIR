{"layout": 0, "type": "text", "text": "Learning To Teach Large Language Models Logical Reasoning ", "text_level": 1, "page_idx": 0, "bbox": [63, 81, 548, 101], "page_size": [612.0, 792.0]}
{"layout": 1, "type": "text", "text": "Kaitao Song ", "page_idx": 0, "bbox": [441.30999755859375, 105.99241638183594, 499.9861145019531, 122.5145034790039], "page_size": [612.0, 792.0]}
{"layout": 2, "type": "text", "text": "Peking University S-Lab meiqichen@stu.pku.edu.cn Nanyang Technological University ", "page_idx": 0, "bbox": [87.29499816894531, 120.1891098022461, 377.23187255859375, 145.91342163085938], "page_size": [612.0, 792.0]}
{"layout": 3, "type": "text", "text": "Dongsheng Li Microsoft Research Asia ", "page_idx": 0, "bbox": [420.9090270996094, 152.4224090576172, 519.389404296875, 180.38742065429688], "page_size": [612.0, 792.0]}
{"layout": 4, "type": "text", "text": "Yan Zhang Peking University ", "page_idx": 0, "bbox": [269.5820007324219, 152.4224090576172, 342.67755126953125, 180.38742065429688], "page_size": [612.0, 792.0]}
{"layout": 5, "type": "text", "text": "Yixin Cao Singapore Management University ", "page_idx": 0, "bbox": [71.21098327636719, 152.4224090576172, 212.74961853027344, 180.38742065429688], "page_size": [612.0, 792.0]}
{"layout": 6, "type": "text", "text": "ABSTRACT ", "text_level": 1, "page_idx": 0, "bbox": [53, 188, 112, 200], "page_size": [612.0, 792.0]}
{"layout": 7, "type": "text", "text": "User ", "text_level": 1, "page_idx": 0, "bbox": [357, 192, 374, 203], "page_size": [612.0, 792.0]}
{"layout": 8, "type": "text", "text": "Large language models (LLMs) have gained enormous attention from both academia and industry, due to their exceptional ability in language generation and extremely powerful generalization. How- ever, current LLMs still output unreliable content in practical reason- ing tasks due to their inherent issues (e.g., hallucination). To better disentangle this problem, in this paper, we conduct an in-depth investigation to systematically explore the capability of LLMs in logical reasoning. More in detail, we first investigate the deficiency of LLMs in logical reasoning on different tasks, including event re- lation extraction and deductive reasoning. Our study demonstrates that LLMs are not good reasoners in solving tasks with rigorous reasoning and will produce counter factual answers, which require us to iterative ly refine. Therefore, we comprehensively explore different strategies to endow LLMs with logical reasoning ability, and thus enable them to generate more logically consistent an- swers across different scenarios. Based on our approach, we also contribute a synthesized dataset ( LLM-LR ) involving multi-hop reasoning for evaluation and pre-training. Extensive quantitative and qualitative analyses on different tasks also validate the effec- tiveness and necessity of teaching LLMs with logic and provide insights for solving practical tasks with LLMs in future work. Codes will be available at https://github.com/chenmeiqii/Teach-LLM-LR. ", "page_idx": 0, "bbox": [53, 201.4237823486328, 295, 443.9523620605469], "page_size": [612.0, 792.0]}
{"layout": 9, "type": "text", "text": "Text: A large  FIRE  broke out at the Waitrose supermarket in  Wellingtonâ€™s High Street, half of the roof at the entrance of  the store  collapsed  during the blaze.  ", "page_idx": 0, "bbox": [337, 227.31478881835938, 533, 263.8429870605469], "page_size": [612.0, 792.0]}
{"layout": 10, "type": "image", "page_idx": 0, "img_path": "layout_images/2310.09158v1_0.jpg", "bbox": [324, 265, 540, 355], "page_size": [612.0, 792.0], "ocr_text": ";â€” Causal Relation: C\nSubevent Relation: NO_SUBEVENT\n\nCoreference Relatio\nTemporal Relation\n\nLogical Inconsistency (LI) =\n\nChatGPT\n: NO_COREFERENCE ---\n\nÂ©: conflicts @\n\nNumber of Conflicts (Â®)\n\nNumber of Combinations (@+@)\n\nQe Pairwise\n\nConflicts\nDetection\n\n: No Conflicts\n\n22 216.7%\n2G 716.\n", "vlm_text": "The image appears to explain a method for detecting logical inconsistencies in relationships determined by ChatGPT. It involves the following:\n\n1. **Coreference Relation**: NO_COREFERENCE\n2. **Temporal Relation**: SIMULTANEOUS (with a conflict indicated)\n3. **Causal Relation**: CAUSE (no conflict)\n4. **Subevent Relation**: NO_SUBEVENT (no conflict)\n\nThe process uses \"Pairwise Conflicts Detection\" with indications for conflicts (red circles with exclamation marks) and no conflicts (green check marks).\n\nThe **Logical Inconsistency (LI)** is calculated as:\n- LI = (Number of Conflicts) / (Number of Combinations)\n- Example given: 1/6, which equals approximately 16.7%."}
{"layout": 11, "type": "text", "text": "This answer  SIMULTANEOUS  is  logically inconsistent . ", "page_idx": 0, "bbox": [337, 366.2491455078125, 516, 374.1545104980469], "page_size": [612.0, 792.0]}
{"layout": 12, "type": "text", "text": "", "text_level": 1, "page_idx": 0, "bbox": [334, 384, 359, 389.75], "page_size": [612.0, 792.0]}
{"layout": 13, "type": "text", "text": "â€¢  If event A causes event B, then event A must happen either before or overlap with event B. â€¢  If event A and B happens simultaneously, they wonâ€™t have a causal relation. ", "page_idx": 0, "bbox": [337, 393.5943603515625, 533, 431.4002380371094], "page_size": [612.0, 792.0]}
{"layout": 14, "type": "text", "text": "Figure 1: An example of LLM in generating logically inconsis- tent answers. We let LLM (e.g., ChatGPT) answer the relations between events  â€œFIREâ€  and  â€œcollapsedâ€  from the given pas- sage. We can find that LLM predicts an incorrect answer (i.e., SIMULTANEOUS) because it ignores some prior logic in this scenario, leading to logical inconsistency. ", "text_level": 1, "page_idx": 0, "bbox": [316, 448, 560, 513], "page_size": [612.0, 792.0]}
{"layout": 15, "type": "text", "text": "CCS CONCEPTS ", "text_level": 1, "page_idx": 0, "bbox": [53, 455, 135, 466], "page_size": [612.0, 792.0]}
{"layout": 16, "type": "text", "text": "â€¢  Computing methodologies  $\\rightarrow$  Knowledge representation and reasoning . ", "text_level": 1, "page_idx": 0, "bbox": [53, 471, 294, 491], "page_size": [612.0, 792.0]}
{"layout": 17, "type": "text", "text": "KEYWORDS ", "text_level": 1, "page_idx": 0, "bbox": [53, 502, 116, 513], "page_size": [612.0, 792.0]}
{"layout": 18, "type": "text", "text": "Large Language Models, Event Relation Extraction, Logical Reason- ing ", "page_idx": 0, "bbox": [53, 515.0667724609375, 295, 538.4174194335938], "page_size": [612.0, 792.0]}
{"layout": 19, "type": "text", "text": "However, LLMs sometimes output counter factual s when dealing with practical tasks that require rigorous logical reasoning. As showcased in Figure 1, ChatGPT predicts the temporal and causal relations between events    $\"F I R E\"$   and  â€œcollapsedâ€  being  â€œsimulta- neousâ€  and  â€œcauseâ€ . According to the prior logical constraints, we could readily claim the predictions are not fully correct even before reading the context, because  \"simultaneous\"  and  \"cause\"  conflict with each other in terms of semantics. Some works [ 26 ,  33 ,  48 ] attribute these phenomena to their inherent deficiencies (e.g., hallu- cination, unfaithfulness), however, how to disentangle and improve the capability of LLMs in these tasks is still an open problem. ", "page_idx": 0, "bbox": [317, 532.8767700195312, 559, 654.8573608398438], "page_size": [612.0, 792.0]}
{"layout": 20, "type": "text", "text": "1 INTRODUCTION ", "text_level": 1, "page_idx": 0, "bbox": [53, 552, 157, 564], "page_size": [612.0, 792.0]}
{"layout": 21, "type": "text", "text": "Recently, Large Language Models (LLMs) have made incredible progress in many different downstream tasks, such as GPT-3 [ 3 ], ChatGPT [ 32 ], and LLaMA [ 39 ]. These models are typically trained on a combination of filtered web data and curated high-quality corpora (e.g., social media conversations, books, or technical pub- lications) [ 34 ]. Studies have indicated that the emergent abilities of LLMs can exhibit promising reasoning capabilities [45] and the curation process is necessary to produce their zero-shot generaliza- tion abilities [34]. ", "page_idx": 0, "bbox": [53, 565.7537841796875, 295, 665.8163452148438], "page_size": [612.0, 792.0]}
{"layout": 22, "type": "text", "text": "To deeply understand the deficiencies of LLMs in logical rea- soning and explore the corresponding solutions, in this paper, we conduct an in-depth investigation of LLMs in solving reasoning tasks from multiple dimensions. We first evaluate the capacity of LLMs in two practical scenarios including event relation extraction ", "page_idx": 0, "bbox": [317, 653.4247436523438, 559, 709.6513671875], "page_size": [612.0, 792.0]}
{"layout": 23, "type": "text", "text": "Despite these notable achievements, current LLMs still have some issues in producing high-quality content with fluency and reliability. A good content generator should produce logically con- sistent answers that are reasonable for given or prior constraints. ", "page_idx": 0, "bbox": [53, 664.3837890625, 295, 709.6513671875], "page_size": [612.0, 792.0]}
{"layout": 24, "type": "text", "text": "and deductive reasoning tasks, both of which demand rigorous reasoning ability to infer [ 38 ,  42 ]. Our experimental results show that:  1)  Even the cutting-edge LLMs still generate large amounts of inconsistent answers, e.g., over  $60\\%$   of the answers from Chat- GPT on the MAVEN-ERE [ 42 ] dataset are logically inconsistent as shown in Figure 2;  2)  Chain-of-thought (CoT) prompting [ 47 ], like â€œ Letâ€™s think step by step \" could stimulate the reasoning abili- ties for LLMs. However, some inherent issues (e.g., hallucination, unfaithfulness) in the LLM will cause such generated rationale to be unreliable or inconsistent;  3)  Providing relevant logic to LLMs improves performance, but injecting irrelevant logic introduces fluctuations in results. Therefore, how to obtain the relevant logic and inject its information into LLMs is a non-trivial problem, de- serving further exploration;  4)  To verify the capacity of LLM for more complex reasoning, we contribute a synthesized dataset (i.e., LLM-LR ) for evaluation, which involves multiple hops of logical reasoning.  LLM-LR  is automatically constructed by applying logic programming [ 11 ,  22 ] on our collected logical constraints, which could provide logical reasoning instances with any number of hops. Results show that as the number of logical hops increases   $(2{\\sim}10$  hops), LLMs struggle to output correct answers, and the propor- tion of logically inconsistent answers steadily rises. This indicates that LLMs will perform worse when the reasoning becomes more abstract and complicated. Therefore, how to alleviate the afore- mentioned issues and enable LLMs with a more powerful ability of logical reasoning is the critical point of our paper. ", "page_idx": 1, "bbox": [53, 83.56178283691406, 295, 369.92535400390625], "page_size": [612.0, 792.0]}
{"layout": 25, "type": "text", "text": "Based on these findings, we put forward a series of solutions to teach LLMs to generate answers with better logical consistency. Here, we divide the procedure for teaching LLMs logical reasoning into three different kinds of approaches according to the ways of logic acquisition: 1)  Generative-based approach , which encourages LLMs to generate reasoning rationale themselves, inspired by CoT prompting. In this paradigm, we find that incorporating logical constraints into LLM instruction will bring substantial improve- ments, but the uncertainty of the generated rationales may also bring some biases, leading to an incorrect subsequent answer; 2) Retrieval-based approach , which provides our manually designed logical constraints, then retrieves relevant contents and adds them to the LLM instruction. This kind of approach ensures the correct- ness of logical constraints and significantly improves performance, but requires some hand-crafted engineering; 3)  Pre training-based approach , which uses our curated dataset  LLM-LR  introduced be- fore to train LLMs to perform complex logical reasoning. The pre- training dataset consists of 6776 instances containing  $2{\\sim}5$   hops of logical reasoning. This strategy encodes logic in model parameters inherently, while also requiring additional training time. Therefore, how to choose the most suitable strategy can be a trade-off based on the practical scenario. ", "page_idx": 1, "bbox": [53, 368.4927978515625, 295, 611.0213623046875], "page_size": [612.0, 792.0]}
{"layout": 26, "type": "text", "text": "Furthermore, based on the above framework, we also conduct extensive quantitative and qualitative analyses on different tasks to validate the effectiveness of teaching LLMs with logic and provide insights for future work:  1)  We investigate whether to add logical constraints before obtaining results or later, and find that directly conveying constraints to LLMs is more effective than adding post- processing operations based on the results;  2)  Compared with the setting that uses more demonstrations, incorporating logical con- straints into prompts can achieve better performance with fewer demonstrations. This phenomenon further indicates that it is im- portant to teach LLMs to balance demonstrations and logical con- straints;  3)  Benefits from LLMsâ€™ powerful interactive ability, we can further improve the performance through multi-turn conversation enhanced by iterative retrievals. However, when there are too many iterations, LLMs may have the problem of over thinking â€” more useless and redundant information interferes with their predic- tions;  4)  When trained on  LLM-LR , LLMs such as LlaMA2-13B [ 39 ] can achieve better performance, even surpassing that of greater LLMs (e.g., ChatGPT, 175B), which validates the effectiveness of our curated dataset. ", "page_idx": 1, "bbox": [53, 609.5888061523438, 295, 709.6513671875], "page_size": [612.0, 792.0]}
{"layout": 27, "type": "text", "text": "", "page_idx": 1, "bbox": [316, 83.56178283691406, 559, 205.54234313964844], "page_size": [612.0, 792.0]}
{"layout": 28, "type": "text", "text": "Overall, the contributions of our paper can be summarized as follows: ", "page_idx": 1, "bbox": [316, 204.1098175048828, 559, 227.4603729248047], "page_size": [612.0, 792.0]}
{"layout": 29, "type": "text", "text": " We provide an in-depth investigation of the logical inconsistency\n\n â€¢ problems of current LLMs in solving practical tasks, and indicate the deficiency of LLMs in utilizing logic.  To enhance the reliability of the content generated by LLMs, we\n\n â€¢ propose several solutions to incorporate relevant logic. Based on our approach, we construct a synthesized dataset ( LLM-LR ) involving multi-hop reasoning. By leveraging the  LLM-LR , we endow specialized LLMs with logical reasoning ability, which enhances LLMs to generate more logically consistent answers.  Experimental results on different tasks with quantitative and\n\n â€¢ qualitative analyses verify the importance of our investigation in empowering LLMs with logical reasoning. ", "page_idx": 1, "bbox": [316, 229.58778381347656, 559, 362.52734375], "page_size": [612.0, 792.0]}
{"layout": 30, "type": "text", "text": "2 PRELIMINARIES ", "text_level": 1, "page_idx": 1, "bbox": [317, 373, 421, 385], "page_size": [612.0, 792.0]}
{"layout": 31, "type": "text", "text": "In this section, we first introduce two tasks that this paper mainly explores. ", "page_idx": 1, "bbox": [316, 386.7118225097656, 559, 410.0623474121094], "page_size": [612.0, 792.0]}
{"layout": 32, "type": "text", "text": "2.1 Event Relation Extraction ", "text_level": 1, "page_idx": 1, "bbox": [317, 421, 471, 433], "page_size": [612.0, 792.0]}
{"layout": 33, "type": "text", "text": "Event relation extraction (ERE) [ 21 ,  42 ] aims to identify relations (i.e., Co reference, Temporal, Causal, and Subevent) between two events in the text. Traditionally, it can be formulated as a multi- label classification problem, determining one label for each relation type. Compared with other common tasks, ERE tasks should take more considerations about the logical constraints between event relations (e.g., the constraints in Figure 1), and guarantee the predic- tions should conform to those constraints to avoid counter factual s. Therefore, we need to rigorously consider the logical constraints between each event pair during the prediction. To better evaluate the capability of LLMs on the ERE task, we formulate the logical consistency for evaluation. ", "page_idx": 1, "bbox": [316, 434.246826171875, 559, 567.1864013671875], "page_size": [612.0, 792.0]}
{"layout": 34, "type": "text", "text": "Logical consistency plays a crucial role in understanding the re- lations between events. To assess the logical consistency, we collect a comprehensive set including 11 logical constraints for all relations between two events, as shown in Table 4. Based on these logical constraints, we introduce a logical inconsistency metric (i.e., LI) to measure LLMsâ€™ ability on ERE tasks. Specifically, for the answers of LLMs, logical inconsistency is calculated as the ratio of the number of conflicts (i.e., the answers that conflict with the given logical constraints) to the total number of combinations (i.e., all combina- tions of each two relations). To better illustrate the computation of logical inconsistency, here we introduce an example (as shown in Figure 1): if an LLM outputs the relations between two events as â€œNO CO REFERENCE, SIMULTANEOUS, CAUSE, NO SUB EVENTâ€. ", "page_idx": 1, "bbox": [316, 565.7537841796875, 559, 709.6513671875], "page_size": [612.0, 792.0]}
{"layout": 35, "type": "image", "page_idx": 2, "img_path": "layout_images/2310.09158v1_1.jpg", "img_caption": "Figure 2: Performance of ChatGPT in the pilot study. ", "bbox": [56, 88, 301, 201], "page_size": [612.0, 792.0], "ocr_text": "=== MicroF1  â€”eâ€” Inconsistent Answers\n\n-60>\n60- 50\n= -40 8\ni 40- <\nra -30=\nEy 3\n& -202\nial 5\no- <0\nVanilla ChatGPT + Irrelevant logic + Relevant logic\n\nMAVEN-ERE\n\nMicro-F4 (%)\n\n=== Micro-F1 â€”eâ€” Inconsistent Answers\n\n100 - ~ 80\n2\n\n80- -602\n3\n\n60- â‚¬\n-40%\n\nâ€œo 3\n2\n\n8\n\nÂ£\n\nâ€œi\n\nVanilla ChatGPT + Irrelevant logic + Relevant logic\nProofWriter\n", "vlm_text": "The image contains two bar graphs with accompanying line plots showing the performance of ChatGPT in a pilot study across two different datasets: MAVEN-ERE and ProofWriter.\n\n1. MAVEN-ERE (left graph):\n   - The bar graph represents the Micro-F1 score in percentage (shown in purple).\n   - The line plot represents the percentage of inconsistent answers (shown with blue markers and line).\n   - Three conditions are compared: Vanilla ChatGPT, ChatGPT with Irrelevant Logic, and ChatGPT with Relevant Logic.\n     - For Vanilla ChatGPT, the Micro-F1 score is low, and the percentage of inconsistent answers is high.\n     - When irrelevant logic is applied, the Micro-F1 score remains low, but the inconsistent answers percentage decreases significantly.\n     - With relevant logic, the Micro-F1 score increases significantly, and the percentage of inconsistent answers remains low.\n\n2. ProofWriter (right graph):\n   - Similar to the MAVEN-ERE graph, the bar represents the Micro-F1 score and the line shows the inconsistent answers percentage.\n   - Conditions: Vanilla ChatGPT, ChatGPT with Irrelevant Logic, and ChatGPT with Relevant Logic.\n     - Vanilla ChatGPT shows a low Micro-F1 score and a moderate percentage of inconsistent answers.\n     - With irrelevant logic, the Micro-F1 score increases significantly, and inconsistent answers decrease.\n     - With relevant logic, the Micro-F1 score remains high, but the percentage of inconsistent answers increases slightly from the irrelevant logic condition.\n\nOverall, both graphs illustrate the impact of using relevant and irrelevant logic on the performance of ChatGPT, highlighting improvements in Micro-F1 scores and variations in inconsistent answer percentages."}
{"layout": 36, "type": "text", "text": "Among these, \"SIMULTANEOUS\" and \"CAUSE\" are identified as conflicting with each other based on the logical constraints we have defined, resulting in a single conflict. Now, regarding the total number of combinations: for each pair of events, we have 4 types of relations to determine. The total combinations between these rela- tions are calculated using the combinatorial formula:  $4{*}(4{-}1)/2=6$  . So, there are 6 possible combinations between the relations for two events. Hence, the logical inconsistency in this example is com- puted as  $\\mathrm{{{LI}}}=1/6$   (or approximately   $16.7\\%$  ). Obviously, given the logical constraints, an algorithm can be designed to automatically detect conflicts and calculate the value of logical inconsistency. ", "page_idx": 2, "bbox": [53, 220.6468048095703, 295, 342.6273498535156], "page_size": [612.0, 792.0]}
{"layout": 37, "type": "text", "text": "Overall, intuitively, the smaller the value of logical inconsistency is, the more self-consistent and reasonable answer that LLM can produce. More descriptions about this task are in Appendix A. ", "page_idx": 2, "bbox": [53, 341.19482421875, 295, 375.5043640136719], "page_size": [612.0, 792.0]}
{"layout": 38, "type": "text", "text": "2.2 Deductive Reasoning ", "text_level": 1, "page_idx": 2, "bbox": [53, 389, 183, 400], "page_size": [612.0, 792.0]}
{"layout": 39, "type": "text", "text": "Deductive reasoning typically begins with known facts and rules, then iterative ly makes new inferences until the desired statement can be either confirmed or refuted [ 35 ]. To ensure the accuracy of these inferences, each step in deductive reasoning must adhere to the known logical constraints (rules). More specifically, the logical constraints in deductive reasoning are usually specific to individ- ual cases rather than being universally applicable like that in the ERE task. Consequently, when engaging in deductive reasoning, it is essential to assess and apply logical constraints based on the distinct circumstances and known facts of each example to arrive at accurate conclusions. For the calculation of logical inconsistency of deductive reasoning, we need to manually count the number of reasoning processes generated by LLMs that are inconsistent with known facts or rules, and then calculate the proportion. ", "page_idx": 2, "bbox": [53, 400.7718200683594, 295, 555.62939453125], "page_size": [612.0, 792.0]}
{"layout": 40, "type": "text", "text": "3 UNVEILING LLMS IN LOGICAL REASONING In this section, we conduct a pilot study to investigate how current LLMs exhibit in reasoning tasks and how logic benefits LLMs. ", "page_idx": 2, "bbox": [53, 566.2232055664062, 295, 604.2463989257812], "page_size": [612.0, 792.0]}
{"layout": 41, "type": "text", "text": "3.1 How Is LLM Performing Practical Reasoning Tasks? ", "text_level": 1, "page_idx": 2, "bbox": [54, 616, 244, 641], "page_size": [612.0, 792.0]}
{"layout": 42, "type": "text", "text": "3.1.1 Data Source .  We conduct a manual evaluation on MAVEN- ERE [ 42 ] and Proof Writer [ 38 ]. MAVEN-ERE is a unified large-scale dataset for the ERE task, which needs to identify four types of relations. Proof Writer is a commonly used dataset for deductive logical reasoning, where each example is a pair of (problem, goal) and the label is selected from {Proved, Disproved, Unknown}. To ", "page_idx": 2, "bbox": [53, 642.4657592773438, 295, 709.6513671875], "page_size": [612.0, 792.0]}
{"layout": 43, "type": "image", "page_idx": 2, "img_path": "layout_images/2310.09158v1_2.jpg", "bbox": [322, 86, 561, 208], "page_size": [612.0, 792.0], "ocr_text": "CE2\nVanilla ChatGPT\n\nCE - + Irrelevant Logic\n+ Relevant Logic\n* 60\n40 50\n20 30\n10\ncE1\nFE1\nFE2\nMAVEN-ERE\n\nCE3\n\nFE1\n\nCE2\n\nVanilla ChatGPT\n+ + Irrelevant Logic\n+ +Relevant Logic\n\n598070\n40\nOrt\n, (CE1\n\nFE2\nProofWriter\n", "vlm_text": "The image contains two radar charts comparing three variations: \"Vanilla ChatGPT,\" \"+ Irrelevant Logic,\" and \"+ Relevant Logic.\" Each chart has labeled axes (CE1, CE2, CE3, FE1, FE2, FE3) representing different criteria, with varying scores indicated in concentric circles. The left chart is for \"MAVEN-ERE\" and the right is for \"ProofWriter.\" The charts visually compare how each variation performs across these criteria. The colors used for the variations are blue, orange, and green."}
{"layout": 44, "type": "text", "text": "Figure 3: Error analysis of ChatGPT in the pilot study by human evaluation. CE and FE denote incorrectness and un- faithfulness errors, respectively. ", "text_level": 1, "page_idx": 2, "bbox": [317, 225, 559, 257], "page_size": [612.0, 792.0]}
{"layout": 45, "type": "text", "text": "employ our investigation, we randomly choose 100 samples (50 from MAVEN-ERE and 50 from Proof Writer). ", "page_idx": 2, "bbox": [317, 277.5088195800781, 559, 300.8593444824219], "page_size": [612.0, 792.0]}
{"layout": 46, "type": "text", "text": "3.1.2 Experimental Setup .  Our experiments are conducted as a multi-turn conversation for zero-shot reasoning, to leverage LLMâ€™s interaction ability. Given a task input    $(X)$  , we also write a prompt\n\n  describing the task, and let LLM generate output  by an-\n\n ( ð‘‡ )  ( ð‘Œ ) swering the given query. We also add  â€œLetâ€™s think step by stepâ€ before each answer for prediction generation, which is a simple but effective trick to improve zero-shot reasoning for LLMs [ 19 ]. We adopt ChatGPT as the backbone and manually check its generated rationales under the following three settings:\n\n ", "page_idx": 2, "bbox": [317, 305.9128112792969, 559, 405.975341796875], "page_size": [612.0, 792.0]}
{"layout": 47, "type": "text", "text": " Vanilla LLM (i.e., ChatGPT) without any additional information;\n\n â€¢\n\n  LLM (i.e., ChatGPT) plus the most relevant (i.e., ground truth)\n\n â€¢ logic; ", "page_idx": 2, "bbox": [317, 407.4298095703125, 559, 441.73834228515625], "page_size": [612.0, 792.0]}
{"layout": 48, "type": "text", "text": "", "text_level": 1, "page_idx": 2, "bbox": [318, 446.25, 525, 452], "page_size": [612.0, 792.0]}
{"layout": 49, "type": "text", "text": "The prompt examples can be found in Figure  $10{\\sim}13$  ", "page_idx": 2, "bbox": [317, 454.1518249511719, 506, 466.5433654785156], "page_size": [612.0, 792.0]}
{"layout": 50, "type": "text", "text": "3.1.3 Analysis .  As shown in Figure 2, we visualize the micro-F1 and the proportion of logically inconsistent answers generated by ChatGPT. We find that no matter whether on MAVEN-ERE or Proof Writer, Vanilla ChatGPT always achieves a bad result with low micro-F1 scores and high inconsistency values (e.g.,  $15\\%$   micro- F1 and  $63\\%$   inconsistent answers on MAVEN-ERE), which indicates the deficiency of LLM in solving complex reasoning tasks. To inves- tigate this issue in depth, we conduct analyses from the following two aspects. ", "page_idx": 2, "bbox": [317, 471.5968017578125, 559, 571.6593627929688], "page_size": [612.0, 792.0]}
{"layout": 51, "type": "text", "text": "What Is The Relation Between Logical Consistency And Model Performance?  From Figure 2, we find that: 1) The model directly receives significant improvements on both MAVEN-ERE and Proof Writer when adding relevant logic; 2) When adding some irrelevant logic, the results show some fluctuations (exaltation in MAVEN-ERE and degeneration in Proof Writer). That means di- rectly adding logic without any constraints will bring some uncer- tainty; 3) Typically, a higher logical inconsistency corresponds to a poorer micro-F1, however, rectifying logical inconsistency does not necessarily lead to the same degree of increase in micro-F1. Gen- erally, an intuitive observation is that incorporating relevant logic into the LLM instruction will be very helpful in solving reasoning tasks. So, the challenges are how to obtain these relevant logic and how to utilize them for LLMs. ", "page_idx": 2, "bbox": [317, 577.7080688476562, 559, 709.6513671875], "page_size": [612.0, 792.0]}
{"layout": 52, "type": "text", "text": "", "page_idx": 3, "bbox": [53, 83.56178283691406, 295, 106.91233825683594], "page_size": [612.0, 792.0]}
{"layout": 53, "type": "text", "text": "What Types of Errors Does LLM Usually Make?  To delve into a deep understanding of the failures that Vanilla LLM encounters in logical reasoning, we also conduct a detailed error analysis for this. Here, we divide the error types into two aspects: 1)  Incorrectness to the Constraint  (CE): whether the rationale generated by LLM is wrong (CE1), incomplete (CE2), or redundant (CE3) compared with the true logical constraints. 2)  Unfaithfulness to the Reasoning Process  (FE): where LLM does not correctly use the constraints. We define two types of errors upon FE, i.e., i) Wrong start, LLM begins with an irrelevant fact or focuses on an improper perspective for the correct answer (FE1). ii) Wrong process, LLM starts from a proper point, but makes mistakes during the reasoning process (FE2). Annotators are asked to review 100 predictions generated by ChatGPT and mark the error types. Results in Figure 3 show that: 1) The quality of constraints produced by the Vanilla ChatGPT is not high enough, which limits its subsequent reasoning ability. 2) Incorporating relevant logical constraints could guarantee the correctness of constraints and thus greatly improve the generation quality of ChatGPT in faithfulness. ", "page_idx": 3, "bbox": [53, 113.28376770019531, 295, 322.93536376953125], "page_size": [612.0, 792.0]}
{"layout": 54, "type": "text", "text": "3.2 How Is LLM Performing Abstract Multi-hop Reasoning? ", "text_level": 1, "page_idx": 3, "bbox": [53, 334, 295, 360], "page_size": [612.0, 792.0]}
{"layout": 55, "type": "text", "text": "Based on the above analyses, we can confirm the deficiency of LLMs in solving complex reasoning tasks and the effectiveness of incorporating logical constraints. Nevertheless, we also want to explore how LLMs exhibit in more challenging settings. ", "page_idx": 3, "bbox": [53, 360.6888122558594, 295, 405.9573669433594], "page_size": [612.0, 792.0]}
{"layout": 56, "type": "text", "text": "3.2.1 Data Source .  Considering that existing datasets lack multi- hop instances, we construct a synthesized dataset ( LLM-LR ) to evaluate the ability of LLMs to perform multi-hop reasoning. Specif- ically, we first collect 39 additional logical constraints for all the high-order relations among three events, as outlined in Table 7. The collection is based on transitive dependency (i.e., one event may affect another through an intermediate event). For example, BEFORE( ð´ ,  ðµ )  âˆ§ BEF  $\\mathrm{DRE}(B,\\,C)\\rightarrow\\mathrm{BaFORE}(A,\\,C)$   means that â€œ If event  ð´ happens BEFORE event    $B$  , and event  ðµ happens BEFORE event  ð¶ , then event  ð´ happens BEFORE event  $C^{\\ast}$  . Thereby, we obtain a comprehensive set containing a total of 50 logical constraints (along with the 11 constraints between two events we introduced in Section 2.1). ", "page_idx": 3, "bbox": [53, 412.32879638671875, 295, 556.2273559570312], "page_size": [612.0, 792.0]}
{"layout": 57, "type": "text", "text": "As the number of events further increases (i.e.,  ${>}3$  ), there are more complex interactions involved, and it is inefficient to list all the constraints manually at this time. To address this, we introduce logic programming [ 11 ,  22 ] to automatically generate new event relations by inputting the known constraints and relations. We em- ploy a forward- and backward-chaining rule-based method utilizing Prolog [ 10 ] as the foundation for our logic programming approach. For instance, when dealing with temporal relations involving four events   $(A,B,C$  , and  $D_{-}$  ), given the known relations: â€œBEFORE  $(A,B)$  âˆ§ SIMULTANEOUS  $(B,C)$     $\\setminus\\operatorname{OVERLAP}(C,D)^{\\ast}$  â€, our logic program- ming approach can deduce a   $\\mathrm{\"BEGRE}(A,D)\"$   conclusion according to the constraints in Table 7. Then, we provide a task description and use the given relations as the input case to let LLMs reason the relation between events  $(A,D)$  , i.e., a 3-hop query. We could ", "page_idx": 3, "bbox": [53, 554.7947998046875, 295, 709.6513671875], "page_size": [612.0, 792.0]}
{"layout": 58, "type": "image", "page_idx": 3, "img_path": "layout_images/2310.09158v1_3.jpg", "img_caption": "Figure 4: Multi-hop reasoning performance of LLMs evalu- ated on our  LLM-LR  dataset. ", "bbox": [316, 85, 568, 212], "page_size": [612.0, 792.0], "ocr_text": "Micro-F1 (%)\n\n2\no\n\ne\nES\n\nÂ°\nN\n\n0.0\n\n4\n\nâ€”-â€” GPT-turbo\n\nâ€”â€” Text-davinci-003\n\nâ€”â€” GPT-4\n~~ Vicuna-13B\nUama2-13B\n\n5 6 7 8 9\nNumber of Hops\n\ncs es Â¢\nyb Bo Â®\n\nInconsistent Answers(%)\n2\n\n4\n\nâ€”â€” GPT-turbo\n\n~~ Text-davinci-003\nâ€” GPT-4\n\n~*~ Vieuna-138\n= Ulama2-138\n\n5 6 7 8 9 10\nNumber of Hops\n", "vlm_text": "The image shows two line graphs comparing the performance of different language models (GPT-turbo, Text-davinci-003, GPT-4, Vicuna-13B, Llama2-13B) on multi-hop reasoning tasks. \n\n- **Left Graph: Micro-F1 (%) vs. Number of Hops**\n  - Displays how the Micro-F1 score changes as the number of hops increases from 2 to 10. Generally, the performance decreases as the number of hops increases.\n\n- **Right Graph: Inconsistent Answers (%) vs. Number of Hops**\n  - Shows the percentage of inconsistent answers as the number of hops increases from 2 to 10. The inconsistency tends to increase with more hops.\n\nEach line represents a different model, as indicated in the legend."}
{"layout": 59, "type": "text", "text": "use the description text provided in Table 6 to convert the sym- bolic representation into natural language forms. The conclusion deduced by our logic engine will serve as the ground truth to check LLMsâ€™ answers. A pseudo-code can be found in Appendix D.1 and a prompt example is in Figure 14. ", "page_idx": 3, "bbox": [316, 230.38380432128906, 559, 286.6103515625], "page_size": [612.0, 792.0]}
{"layout": 60, "type": "text", "text": "3.2.2 Experimental Setup .  For evaluation, we randomly gen- erate 50 samples for each  $2{\\sim}10$  -hop reasoning. In addition to the three variants of ChatGPT ( gpt-turbo ,  text-davinci-003 , and  gpt4 ), we employ another two open-source LLMs (Vicuna-  $13\\mathrm{B}â€“\\mathrm{v}1.3^{1}$    and Llama2-13B) for evaluation. Note that: 1) for 2-hop reasoning (i.e., high-order relations among three events), there are only 39 samples. 2) Our approach allows for the extension of the reasoning path, but we report results for clarity and due to the length limits of LLMs, covering only 2 to 10 hops. ", "page_idx": 3, "bbox": [316, 291.4508056640625, 559, 391.51336669921875], "page_size": [612.0, 792.0]}
{"layout": 61, "type": "text", "text": "3.2.3 Analysis .  As shown in Figure 4, we visualize the micro-F1 and the proportion of logically inconsistent answers generated by LLMs. We can see that: 1) When the number of hops is relatively small (i.e.,   $2~\\sim~5$   hops), the performance of GPT-4 is outstand- ing compared with other models. 2) With the increase of hops, all the LLMs perform worse when the reasoning becomes more complicated, and the proportion of logically inconsistent answers is gradually increasing. Among them, Vicuna-13B fully fails after 6 hops and could not output any correct answers. This further demonstrates the necessity of teaching LLMs logical reasoning. ", "page_idx": 3, "bbox": [316, 396.3528137207031, 559, 507.3743591308594], "page_size": [612.0, 792.0]}
{"layout": 62, "type": "text", "text": "4 TEACHING LLMS LOGICAL REASONING ", "text_level": 1, "page_idx": 3, "bbox": [317, 517, 543, 530], "page_size": [612.0, 792.0]}
{"layout": 63, "type": "text", "text": "Based on the aforementioned analysis, we expect to explore how to empower LLMsâ€™ capability with logical reasoning. Therefore, in this section, we first introduce the instruction-following technique we use in Section 4.1 and then propose three different approaches to instruct LLMs to generate answers with better logical consis- tency (Section  $4.2\\sim4.4$  ). ", "page_idx": 3, "bbox": [316, 530.6458129882812, 559, 597.8313598632812], "page_size": [612.0, 792.0]}
{"layout": 64, "type": "text", "text": "4.1 In-Context Learning for LLMs ", "text_level": 1, "page_idx": 3, "bbox": [316, 608, 492, 620], "page_size": [612.0, 792.0]}
{"layout": 65, "type": "text", "text": "We deploy LLMs for event relation extraction and deductive reason- ing tasks via in-context learning (ICL, [ 3 ,  32 ]). Given a task input  $(X)$  , we also write a prompt    $(T)$   describing the task, then further provide several demonstrations    $D=\\left\\{D_{i}\\right\\}_{i=1}^{|\\bar{D}|}$    .  $D_{i}=(X_{i},Y_{i})$  are used for few-shot learning. Then, the LLM generates output  ( ð‘Œ ) by completing the prompt    $\\mathcal{(Y}=\\mathcal{M}(T,D,X))$  ) , where  $\\mathcal{M}$   denotes ", "page_idx": 3, "bbox": [316, 621.1017456054688, 559, 696.3392944335938], "page_size": [612.0, 792.0]}
{"layout": 66, "type": "image", "page_idx": 4, "img_path": "layout_images/2310.09158v1_4.jpg", "img_caption": "Figure 5: Incorporate logical constraints to LLMs by using generative, retrieval, and pre training-based approaches. The dashed boxes indicate answers output by LLMs, and the underlined texts indicate the logical constraints. ", "bbox": [53, 83, 559, 405], "page_size": [612.0, 792.0], "ocr_text": "= User\n\nThere is a piece of text with two events, and your task is to identify four types of relations between them...\n\nText:\n\nA large FIRE broke out at the Waitrose supermarket in Wellington's High Street, half of the roof at the entrance\nof the store collapsed during the blaze.\n\nLLM -Thought\n\nâ€œFrom the text, we could first get:\ncauses the store to â€˜â€˜collapsedâ€. i\n\nDue to the logic constraint: if event A CAUSEs\nevent B, then event A happens BEFORE or\nOVERLAP event B, and they won't have\ncoreference and subevent relations...\n\n_We can deduce that the answers are:\n\nle Logical Constraints\n\ny\n\nCoreference Relation -\n\nOVERLAP i\n\nâ€”--------4\n\nCAUSE\n\n(a) Generative-based\n\nTemporal Relation ~\n\nCausal Relation -\n\nSubevent Relation-\n\n_Logical Set\n\nyour answers:\n\nwon't have temporal, causal, and subevent |\n\nhere are some rules among the relations, you |\ncan select some of them to reason or check |\n\n(2) Iftwo events are COREFERENCE, then they _\n\nDetect Conflicts\n\nRetrieval\n\nCea aNEGuE |\n\nSIMULTANEOUS â€˜O-\n1 SIMULTANEOUS }\n\nOriginal Answers\n\nor\nPost-processing\n\n1\n1 J 1\n\nree\n\n(0) Retrieval-based\n\nLLM-LR Dataset\n\nâ€˜ontext\n\nGiven that event A and event B are COREFERENCE,\n\nevent B and event C are COREFERENCE.\n\n| Question:\n| Based on the above information, what are the four\n\ntypes of relations between event A and C?\nAnswers:\n\nCOREFERENCE, NO_TEMPORAL, NO_CAUSAL,\n_NO_SUBEVENT.\n\nPre-training\nSpecialized LLMs\n\nGenerate\n\nee a\n\np NO_COREFERENCE !\n\nSee Se eacear\n\nL | OVERLAP 1\n1 | CAUSE 1\n\na te\n\n1 NO_SUBEVENT 1 Qe\nOutput Answers\n\n(c) Pretraining-based\n\n", "vlm_text": "The image illustrates an approach to incorporating logical constraints into large language models (LLMs) using generative, retrieval, and pre-training methods, as labeled in sections (a), (b), and (c).\n\n1. **User Prompt**: A piece of text with two events is given, where the task is to identify four types of relationsâ€”Coreference, Temporal, Causal, and Subeventâ€”between the two events described: a fire breaking out and a collapse occurring.\n\n2. **Sections**:\n   - **(a) Generative-based (LLM-Thought)**: This approach uses logical reasoning to derive answers. It starts by identifying that the \"FIRE\" causes the store to \"collapsed\". Logical constraints applied (e.g., causation implies certain ordering and lack of coreference/subevent) lead to deduced relations such as no coreference, overlap for temporal relation, cause, and no subevent. The output is aligned with logical constraints.\n   \n   - **(b) Retrieval-based (Logical Set)**: This method involves using predefined logic rules to detect and resolve conflicts in the answers retrieved. Here, the system detects a conflict between simultaneous and overlap, correcting it by replacing \"SIMULTANEOUS\" with \"OVERLAP\", resulting in conflict-free answers.\n   \n   - **(c) Pretraining-based (LLM-LR Dataset)**: Pre-trained specialized LLMs are used to generate answers based on a dataset context where known relationships serve as input. It outputs answers similar to the generative approach, complying with logical constraints.\n\nThe image highlights how logical constraints can refine LLM outputs to more accurately capture relationships between events based on the described approaches."}
{"layout": 67, "type": "text", "text": "the LLM. In such a setting, the LLM can follow the structure of the provided demonstrates to output the expected format of answers for subsequent automatic evaluation. Besides, the whole process does not require any gradient update, allowing LLMs to generate predictions without massive training data. ", "page_idx": 4, "bbox": [53, 420.518798828125, 295, 476.746337890625], "page_size": [612.0, 792.0]}
{"layout": 68, "type": "text", "text": "Compared Models.  We choose three variants of ChatGPT ( gpt- turbo ,  text-davinci-003 , and  gpt4 ), Vicuna-13B-v1.3, and Llama2-13B as the main experimental LLMs for evaluation. We also provide two fine-tuning RoBERTa-large baselines (one-shot and fully fine-tuned) for comparison. The training details of RoBERTa- large can be found in Appendix B.2. ", "page_idx": 4, "bbox": [53, 487.65679931640625, 295, 554.8423461914062], "page_size": [612.0, 792.0]}
{"layout": 69, "type": "text", "text": "Dataset Construction.  Our main experiments are evaluated on MAVEN-ERE, Causal-TimeBank [ 28 ], and Proof Writer. For the ERE task, we focus on relations between two events and conduct sam- pling at the sentence level. The samples of the two events that do not have any relations will be excluded. Here, we randomly sample 500 examples from the test set of MAVEN-ERE and 100 examples from the test set of Causal-TimeBank as our testbed. For the deduc- tive reasoning task, we use the OWA subset of Proof Writer, which is divided into five parts, each part requiring 0, 1, 2, 3, and 5 hops of reasoning, respectively. We evaluate the hardest 5-hop subset. To reduce the computation cost, we randomly sample 200 examples in the test set and ensure a balanced label distribution. Other details can be found in Appendix B.1. ", "page_idx": 4, "bbox": [53, 565.7537841796875, 295, 709.6513671875], "page_size": [612.0, 792.0]}
{"layout": 70, "type": "text", "text": "Evaluation Metrics.  We adopt the averaged micro-F1 score as the evaluation metric and also report the logical inconsistency (defined in Section 2.1) on ERE datasets. The reported value is averaged by the results of three runs to reduce random fluctuation. ", "page_idx": 4, "bbox": [317, 420.518798828125, 559, 465.787353515625], "page_size": [612.0, 792.0]}
{"layout": 71, "type": "text", "text": "4.2 Generative-based Approaches ", "text_level": 1, "page_idx": 4, "bbox": [317, 477, 489, 488], "page_size": [612.0, 792.0]}
{"layout": 72, "type": "text", "text": "Generative-based approaches mean we let LLMs generate logic by using a form of one-shot ICL. Here, we study three variants:\n\n ", "page_idx": 4, "bbox": [317, 489.72882080078125, 559, 513.079345703125], "page_size": [612.0, 792.0]}
{"layout": 73, "type": "text", "text": "(1)  Vanilla ICL:  which utilizes the common prompts consisting of a task description, one demonstration, and the input case.\n\n (2)  Vanilla ICL plus CoT:  which first bootstraps rationales by using chain-of-thought as intermediate reasoning steps following the style of the given demonstration, then output answers. Ratio- nales here do not involve the content of logical constraints.\n\n (3)  CoT with self-generated logical constraints:  which teaches LLMs to generate and utilize logical constraints based on CoT (Fig- ure 5 (a)). Specifically, it will first extract the obvious relation-  $s/$  facts and generate the relevant logical constraints according to the extracted relations/facts, then we enforce LLMs to in- fer the remaining relations/facts based on the constraints and the known relations/facts. The prompt example can be seen in Appendix H.2.\n\n ", "page_idx": 4, "bbox": [317, 514.9738159179688, 559, 669.8313598632812], "page_size": [612.0, 792.0]}
{"layout": 74, "type": "text", "text": "4.2.1 Results .  From Table 1, We could observe that: 1) Compared with a smaller language model (SLM, i.e., RoBERTa-large), the gen- era liz ation ability of vanilla LLMs on both two tasks under the ", "page_idx": 4, "bbox": [317, 675.3427734375, 559, 709.6513671875], "page_size": [612.0, 792.0]}
{"layout": 75, "type": "table", "page_idx": 5, "img_path": "layout_images/2310.09158v1_5.jpg", "table_caption": "Table 1: ChatGPT ( gpt-turbo ,  text-davinci-003 , and  gpt-4 ), Vicuna-13B, and Llama2-13Bâ€™s performance on MAVEN-ERE, Causal- TimeBank, and Proof Writer. We report averaged micro-F1 scores here and \"LI\" denotes the logical inconsistency metric. For each dataset, the best result of each LLM is in bold. RoBERTa-Large (one-shot) fails to output any correct answers on Causal-TimeBank. ", "bbox": [52, 82, 560, 373], "page_size": [612.0, 792.0], "ocr_text": "Model MAVEN-ERE Causal-TimeBank Proof Writer\nMicro-F1 (%) LI (%) | Micro-F1 (%) LI (%) | Micro-F1 (%)\n\nRoBERTa-Large (one-shot) 17.4 54.8 - - 35\n\nRoBERTa-Large (fully fine-tuned) 56.8 6.4 22.2 36.2 63\n\nis vanilla ICL 18.0 53.3 19.0 54.0 39\n\nEj vanilla ICL w. CoT 18.8 49.3 17.0 30.3 42\n\nm CoT w. logical constraints 25.3 37.9 27.0 12.8 45\n\n3 vanilla ICL 21.6 49.1 18.0 58.8 30\n\n5 vanilla ICL w. CoT 20.5 60.5 21.0 64.7 40\n\nfa) CoT w. logical constraints 24.8 5.5 23.0 39.2 49\n\n+ vanilla ICL 29.3 50.7 22.5 30.5 47\n\nss vanilla ICL w. CoT 30.3 36.7 23.0 35.0 67\n\no CoT w. logical constraints 32.3 13.7 24.5 24.0 70\n\nBy vanilla ICL 13.8 25.4 45 84.1 37\n\n& vanilla ICL w. CoT 11.6 47.4 6.0 57.6 40\n\n> CoT w. logical constraints 14.9 21.7 8.0 33.1 42\n\na vanilla ICL 17.0 54.6 11.5 26.7 29\n\n5 vanilla ICL w. CoT 17.8 58.4 10.5 33.6 31\n\n= CoT w. logical constraints 21.5 18.9 13.0 18.1 40\n\n", "vlm_text": "The table presents the evaluation of different models on three tasks: MAVEN-ERE, Causal-TimeBank, and ProofWriter. For each task, it provides Micro-F1 (%) scores and LI (%) scores where applicable. The models include RoBERTa-Large (one-shot and fully fine-tuned), Turbo, Davinci, GPT-4, Vicuna, and Llama2, with variations in vanilla ICL, vanilla ICL with CoT, and CoT with logical constraints.\n\n### Columns:\n1. **Model** - The models and their configurations.\n2. **MAVEN-ERE** - Measures:\n   - Micro-F1 (%)\n   - LI (%)\n3. **Causal-TimeBank** - Measures:\n   - Micro-F1 (%)\n   - LI (%)\n4. **ProofWriter** - Measures:\n   - Micro-F1 (%)\n\n### Key Points:\n- RoBERTa-Large shows baseline scores for comparison.\n- Turbo, Davinci, GPT-4, Vicuna, and Llama2 are evaluated in various configurations.\n- The highest scores for each task and model variation are bolded."}
{"layout": 76, "type": "text", "text": "one-shot setting is remarkable, but there is still a gap with the fully- finetuned baseline. 2) Directly using CoT to infer logic does not help much for ERE tasks, a possible reason is that the inherent issues may lead to the failure of LLM in the precise rationale generation (i.e., a high ratio of logical inconsistency). We give a case study for this in Appendix E. 3) When using generative-based approaches to encourage LLMs to produce logical constraints in the reasoning process, LLMs can significantly improve their performance on both two tasks. It is worth mentioning that the performance of GPT-4 (CoT w. logical constraints) could even surpass that of the fully fine-tuned baseline on the Proof writer dataset. ", "page_idx": 5, "bbox": [53, 389.1168212890625, 295, 511.09735107421875], "page_size": [612.0, 792.0]}
{"layout": 77, "type": "text", "text": "4.3 Retrieval-based Approaches ", "text_level": 1, "page_idx": 5, "bbox": [53, 531, 216, 542], "page_size": [612.0, 792.0]}
{"layout": 78, "type": "text", "text": "Although generative-based approaches enable models to automati- cally generate and utilize logic, the prediction of LLMs is usually un- certain and inaccuracy. Therefore, we also provide retrieval-based approaches, which aim to obtain relevant logic from our pre-defined constraints (Figure 5 (b)). We mainly conduct experiments on the ERE task by utilizing the collected logical constraints. Specifically, We take the collected 11 constraints in Section 2.1 as the retrieval set, and our solutions include:\n\n ", "page_idx": 5, "bbox": [53, 543.4097900390625, 295, 632.514404296875], "page_size": [612.0, 792.0]}
{"layout": 79, "type": "text", "text": "(1)  with all logical constraints:  which directly adds all the 11 logical constraints in the set.\n\n ", "page_idx": 5, "bbox": [53, 642.4657592773438, 295, 665.8163452148438], "page_size": [612.0, 792.0]}
{"layout": 80, "type": "text", "text": "(2)  with retrieved logical constraints:  which means that we first detect logically inconsistent answers based on the prediction of LLMs, and then retrieve the corresponding information if we find any conflict. Finally, we add it to the LLM instruction and let LLMs re-generate the answers. Please see Appendix C.1 for details.\n\n ", "page_idx": 5, "bbox": [53, 664.3837890625, 295, 709.6513671875], "page_size": [612.0, 792.0]}
{"layout": 81, "type": "text", "text": "(3)  with post-processing:  which first obtains the answers of LLMs, then automatically generates some logically consistent candi- dates according to the constraints, and randomly selects one of them as the final answer. This approach ensures that there is no logical conflict   $(\\mathrm{{LI}}=0\\%)$  ). Please see Appendix C.2 for details.\n\n ", "page_idx": 5, "bbox": [317, 389.1168212890625, 559, 467.2613525390625], "page_size": [612.0, 792.0]}
{"layout": 82, "type": "text", "text": "4.3.1 Main Results .  From Table 2, We could observe that: 1) When using retrieval-based approaches to obtain logic constraints and incorporate them into LLM instruction, the logical inconsis- tency of LLMsâ€™ answers is greatly reduced and the overall perfor- mance on both two tasks is further improved. 2) Although our post-processing guarantees the absence of logical conflicts (result- ing in LI of   $0\\%$  ), it may severely affect the quality of the whole generation. On one hand, the semantics of the post-processing an- swer may be far from the ground truth due to the random selection. On the other hand, the size of the candidate set for each case will also affect the performance. It may also need more operations at the post-processing stage, which we leave as future work. ", "page_idx": 5, "bbox": [317, 477.2198181152344, 559, 610.1583862304688], "page_size": [612.0, 792.0]}
{"layout": 83, "type": "text", "text": "4.3.2 Ablation Study .  We conduct an ablation study using Chat- GPT ( gpt-turbo ) in this subsection. ", "text_level": 1, "page_idx": 5, "bbox": [317, 622, 557, 643], "page_size": [612.0, 792.0]}
{"layout": 84, "type": "text", "text": "Demonstrations.  Following previous experiences [ 3 ], we also ap- pend demonstrations into the prompt to investigate how logical constraints will affect when combined with different numbers of demonstrations. Here, we select different numbers of demonstration samples  $K$  from  $\\{1,5,10,20\\}$  . The experiments are tested on vanilla ", "page_idx": 5, "bbox": [317, 653.4247436523438, 559, 709.6513671875], "page_size": [612.0, 792.0]}
{"layout": 85, "type": "table", "page_idx": 6, "img_path": "layout_images/2310.09158v1_6.jpg", "bbox": [110, 82, 500, 301], "page_size": [612.0, 792.0], "ocr_text": "Model MAVEN-ERE Causal-TimeBank\nMicro-F1 (%) LI(%) | Micro-F1 (%) LI (%) J\n2 w. all logical constraints 20.8 30.9 20.0 36.8\nEj w. retrieved logical constraints 22.3 30.2 22.0 11.3\nial w. post-processing 14.0 0 15.0 0\n3 w. all logical constraints 27.0 25.6 31.0 21.8\n5 w. retrieved logical constraints 27.8 30.8 22.0 40.5\n[a] w. post-processing 14.8 0 19.0 0\nÂ¥ w. all logical constraints 37.3 8.3 26.0 20.0\nBE w. retrieved logical constraints 33.5 28.8 24.0 13.5\no w. post-processing 17.0 0 19.0 0\nÂ§ w. all logical constraints 15.2 37.6 11.0 23.5\n& w. retrieved logical constraints 15.7 33.2 10.0 26.7\n> w. post-processing 9.8 0 9.0 0\na w. all logical constraints 19.5 34.6 10.0 23.5\ng w. retrieved logical constraints 18.3 38.2 9.5, 26.7\n= w. post-processing 12.0 0 9.5, 0\n\n", "vlm_text": "The table presents a comparison of model performance on two datasets: MAVEN-ERE and Causal-TimeBank. It reports the Micro-F1 percentages and Logical Inconsistency (LI) percentages for different models (Turbo, Davinci, GPT-4, Vicuna, and Llama2) under three conditions:\n\n1. With all logical constraints\n2. With retrieved logical constraints\n3. With post-processing\n\nKey columns include:\n- Micro-F1 (%) for MAVEN-ERE and Causal-TimeBank, indicating precision and recall balance.\n- LI (%) with a downward arrow, indicating lower is better for logical inconsistency.\n\nEach model has results for these metrics under the specified conditions."}
{"layout": 86, "type": "text", "text": "ICL and ICL plus all logical constraints. From Figure 6, we can ob- serve that: 1) When the number of demonstrations increases from 1 to 5, we can observe an evident improvement, but the subsequent improvements are limited when continue to increase the number of demonstrations (e.g.,  $\\geq10.$  ); 2) Adding logical constraints into LLM instructions can provide stable improvements, especially with more demonstrations. 3) The performance of incorporating logical con- straints with a smaller number of demonstrations can even surpass that of prompts with only a larger number of demonstrations (e.g., the performance of using 5 demonstrations on MAVEN-ERE w. log- ical constraints,  $25.7\\%$  , surpasses that of 10 demonstrations w/o. logical constraints,  $24.5\\%$  ). This indicates that it is important to tell LLMs both \"What\" (demonstrations) and \"How\" (logical con- straints). Overall, these studies further confirm the merits of using logical constraints in solving reasoning tasks. ", "page_idx": 6, "bbox": [53, 340.7668151855469, 295, 506.5833435058594], "page_size": [612.0, 792.0]}
{"layout": 87, "type": "text", "text": "Iterative Retrieval.  Considering the outstanding ability of LLMs in interaction, we further explore whether we can introduce log- ical constraints into the multi-turn conversation (for the prompt design, please see Appendix H.3). Here, we adopt a retrieval-based approach to incorporate logical constraints iterative ly and the re- sults are shown in Figure 6. We find that the logical inconsistency of answers will gradually decrease with the increase of iterations, but the overall micro-F1 seems relatively stable. We guess the main reason for this phenomenon is the over thinking of LLMs, as al- though it can bring more reasoning rationale, it possibly produces correct but more useless or abundant information when inferring multiple iterations. Overall, instructing LLM with logic is beneficial for conversation, but how to support longer information is still challenging. ", "page_idx": 6, "bbox": [53, 506.9638366699219, 295, 661.8203735351562], "page_size": [612.0, 792.0]}
{"layout": 88, "type": "text", "text": "4.4 Pre training-based Approach ", "text_level": 1, "page_idx": 6, "bbox": [52, 673, 219, 685], "page_size": [612.0, 792.0]}
{"layout": 89, "type": "text", "text": "Although the retrieval-based approach guarantees the correctness of logical constraints, it still needs to interact with an external set ", "page_idx": 6, "bbox": [53, 686.3017578125, 295, 709.6513671875], "page_size": [612.0, 792.0]}
{"layout": 90, "type": "image", "page_idx": 6, "img_path": "layout_images/2310.09158v1_7.jpg", "img_caption": "Figure 6: Ablation Study of ChatGPT for demonstrations and iterative retrieval, where  $\\scriptstyle\"{\\mathbf{I}}{\\mathbf{c}}\"$   denotes the logical constraints. ", "bbox": [316, 344, 569, 471], "page_size": [612.0, 792.0], "ocr_text": "yp\nR\n\nMicro-F1 (%)\na\n\n= MAVEN-ERE wio. le\nMAVEN-ERE w. Ic\nCTB wio. Ie\nCTB w. Ic\n\n1 5 10 20\nNumber of Demonstration Samples\n\n3\nNumber of iterations\n\n3\na\n\nBs 8 8\nLogical inconsistency (%)\n\n}\n\n", "vlm_text": "The image consists of two graphs.\n\n1. **Left Graph:**\n   - **Y-Axis:** Micro-F1 (%)\n   - **X-Axis:** Number of Demonstration Samples (1, 5, 10, 20)\n   - **Legend:**\n     - MAVEN-ERE without logical constraints (w/o. lc)\n     - MAVEN-ERE with logical constraints (w. lc)\n     - CTB without logical constraints (w/o. lc)\n     - CTB with logical constraints (w. lc)\n\n   The graph shows the performance (Micro-F1 score) of different models/datasets with and without logical constraints as the number of demonstration samples increases.\n\n2. **Right Graph:**\n   - **Left Y-Axis:** Micro-F1 (%), shown with orange bars\n   - **Right Y-Axis:** Logical Inconsistency (%), shown with a blue line\n   - **X-Axis:** Number of iterations (0, 1, 2, 3, 4)\n\n   The graph demonstrates how the Micro-F1 score and logical inconsistency change over multiple iterations for the MAVEN-ERE and CTB datasets. The MAVEN-ERE results are shown separately with a line, indicating a decreasing trend in logical inconsistency with iterations.\n\nOverall, the figures illustrate an ablation study of ChatGPT for demonstrations and iterative retrieval, focusing on the impact of logical constraints on performance and consistency."}
{"layout": 91, "type": "text", "text": "constantly. Therefore, we provide a pre training-based approach to embed the logical constraints into LLMs themselves. We use the logic programming approach introduced in Section 3.2 to automati- cally generate 6776 instances containing all the  $2\\sim5$  -hop reasoning data. We do not generate longer hops for training here considering the computation complexity and the length limitation of LLMs. The dataset statistic can be found in Table 5. Then, we train LLMs to perform complex logical reasoning based on the curated dataset LLM-LR . Finally, we conduct inference with the trained LLMs. An example of the training data can be seen in Figure 5 (c) or Figure 14. ", "page_idx": 6, "bbox": [317, 488.02581787109375, 561, 599.04736328125], "page_size": [612.0, 792.0]}
{"layout": 92, "type": "text", "text": "4.4.1 Pre training Details .  We adopt Vicuna-13B-v1.3 and Llama2- 13B as the base models and employ the LoRA [ 14 ] technique. During pre-training, only LoRA parameters are optimized. Other Details can be found in Appendix G. ", "page_idx": 6, "bbox": [317, 603.6017456054688, 561, 648.870361328125], "page_size": [612.0, 792.0]}
{"layout": 93, "type": "text", "text": "4.4.2 Results .  As shown in Table 3, we find that: 1) Once trained on  LLM-LR , the performance of LlaMA2-13B and Vicuna-13B im- proves greatly compared with that of Table 1 and 2, especially on the baselines without logical constraints. 2) The performance of LlaMA2-13B-PT could even surpass that of some greater LLMs (e.g., ", "page_idx": 6, "bbox": [317, 653.4247436523438, 561, 709.6513671875], "page_size": [612.0, 792.0]}
{"layout": 94, "type": "table", "page_idx": 7, "img_path": "layout_images/2310.09158v1_8.jpg", "table_caption": "Table 3: Vicuna-13B and Llama2-13Bâ€™s performance on MAVEN-ERE and Causal-TimeBank after pre-training on  LLM-LR . ", "bbox": [58, 82, 554, 281], "page_size": [612.0, 792.0], "ocr_text": "Model MAVEN-ERE Causal-TimeBank\nMicro-F1 (%) LI (%) J Micro-F1 (%) LI(%) |\na vanilla ICL 15.3 (+1.5%) 21.2 (-4.2%) 8.0 (+3.5%) 35.5 (-48.6%)\nFa] vanilla ICL w. CoT 15.8 (+4.2%) 17.8 (-29.6%) 7.5 (+1.5%) 52.5 (-5.1%)\npe CoT w. logical constraints 18.0 (+3.1%) 6.0 (-15.7%) 8.5 (+0.5%) 2.0 (-31.1%)\n4% oe eae\nEt w. all logical constraints 16.3 (+1.1%) 8.7 (-28.9%) 12.1 (+1.1%) 0 (-23.5%)\nFs w. retrieved logical constraints 16.1 (+0.4%) 19.0 (-14.2%) 10.7 (+0.7%) 9.5 (-17.2%)\nw. post-processing 11.0 (+1.2%) 0(-) 8.0 (-1.0%) 0(-)\n& vanilla ICL 19.0 (+2.0%) 45.8 (-8.8%) 12.0 (+0.5%) 22.7 (-4.0%)\n@ vanilla ICL w. CoT 22.1 (+4.3%) 42.9 (-15.5%) 11.5 (+1.0%) 3.0 (-30.6%)\nbr CoT w. logical constraints 26.4 (+4.9%) 15.7 (-3.2%) 13.3 (+0.3%) 13.0 (-5.1%)\na eee ae Ee,\na w. all logical constraints 20.2 (+0.7%) 28.7 (-5.9%) 12.0 (+2.0%) 23.0 (-0.5%)\n4 w. retrieved logical constraints 18.7 (+0.4%) 34.2 (-4.0%) 11.0 (+1.5%) 19.4 (-7.3%)\nw. post-processing 11.0 (-1.0%) 0(-) 11.0 (+1.5%) 0(-)\n", "vlm_text": "The table presents the performance of two models, Vicuna-13B-PT and Llama2-13B-PT, on two datasets: MAVEN-ERE and Causal-TimeBank. The performance metrics shown are Micro-F1 percentage (%) and Logical Inconsistency (LI) percentage (%), with lower LI indicating better performance. The table reports these metrics under various conditions, including \"vanilla ICL,\" \"vanilla ICL with CoT,\" and \"CoT with logical constraints.\" Additional configurations like using all logical constraints, retrieved logical constraints, and post-processing effects are also evaluated.\n\nFor Vicuna-13B-PT on MAVEN-ERE:\n- The best Micro-F1 (18.0%) is achieved with CoT with logical constraints, while the lowest LI (0%) is achieved with post-processing.\n\nFor Vicuna-13B-PT on Causal-TimeBank:\n- The best Micro-F1 (12.1%) is under all logical constraints, and the lowest LI (0%) is obtained with post-processing.\n\nFor Llama2-13B-PT on MAVEN-ERE:\n- The highest Micro-F1 (26.4%) is for CoT with logical constraints, with the lowest LI (0%) from post-processing.\n\nFor Llama2-13B-PT on Causal-TimeBank:\n- The highest Micro-F1 (13.3%) is with logical constraints, while the lowest LI (0%) occurs with post-processing.\n\nEach performance metric shows an improvement indicator (e.g., +1.5%) relative to a baseline, providing a comparison of the relative benefit of different configurations."}
{"layout": 95, "type": "image", "page_idx": 7, "img_path": "layout_images/2310.09158v1_9.jpg", "img_caption": "Figure 7: Case study on Llama-2-13B before and after pre- training (PT). ", "bbox": [53, 295, 296, 546], "page_size": [612.0, 792.0], "ocr_text": "There is a piece of text with events marked by < and > symbols ...\n\ni\ni\n\n, â€˜ext: The exhibition < went on > to show at the Art Institute of :\n; Chicago and then to The Copley Society of Art in Boston, where, _\n| due to a lack of space , all the work by American artists was <\n\\ removed >. :\n| Event Pairs:\n! <went on >and < removed >\n| <removed > and < went on > :\n| Answers:\n| NO_COREFERENCE, BEFORE, NO_CAUSAL, NO_SUBEVENT.\n| NO_COREFERENCE, NO_TEMPORAL, NO_CAUSAL, NO_SUBEVENT. _\nt Demonstration |\ni\n\nt\n\nt\n\nt\n\nt\n\nl\n\ni\n\n1\n\nt\n\nâ€˜\n\nText: The men's ice hockey < tournament > at the 1924 Winter\nOlympics in Chamonix, France , was the 2nd Olympic\nChampionship, also serving as the 2nd World < Championships > .\nEvent Pairs:\n\n< Championships > and < tournament >\n\n< tournament > and < Championships >\n\nAnswet\n\nNO. COREFERENCE, BEFORE, NO_CAUSAL, NO_SUBEVENT. 9\nCOREFERENCE, SIMULTANEOUS, NO_CAUSAL, SUBEVENT.\n\nCOREFERENCE, NO_TEMPORAL, NO_CAUSAL, NO_SUBEVENT. vw\nCOREFERENCE, NO_TEMPORAL, NO_CAUSAL, NO_SUBEVENT. :\n\nLlama-2-13B-PT :\n\n", "vlm_text": "The image shows a case study involving Llama-2-13B before and after pre-training (PT). It is structured as follows:\n\n- **Instruction**: An introductory statement explaining that text events are marked by \"<\" and \">\" symbols.\n\n- **Demonstration**: \n  - **Text**: An example sentence about an exhibition with marked events (\"went on\" and \"removed\").\n  - **Event Pairs**: Lists events and their sequences.\n  - **Answers**: Specifies relationships like NO_COREFERENCE, BEFORE, NO_CAUSAL, NO_SUBEVENT, etc.\n\n- **Input**:\n  - **Text**: Describes an ice hockey tournament at the 1924 Winter Olympics, with marked events.\n  - **Event Pairs**: Lists sequences like \"<Championships>\" and \"<tournament>.\"\n  - **Answers**: Relations like NO_COREFERENCE, BEFORE, NO_CAUSAL, NO_SUBEVENT.\n\n- **Llama-2-13B and Llama-2-13B-PT Responses**:\n  - Shows the model's answers before and after pre-training.\n  - Llama-2-13B makes some errors, marked with an 'X.'\n  - Llama-2-13B-PT provides corrected answers, marked with a 'âœ“.'\n\nThe color-coded sections are used to differentiate the stages and model responses."}
{"layout": 96, "type": "text", "text": "vanilla ChatGPT, 175B), which further validates the importance of teaching LLM with logic in solving reasoning tasks. ", "page_idx": 7, "bbox": [53, 553.36376953125, 295, 576.7144165039062], "page_size": [612.0, 792.0]}
{"layout": 97, "type": "text", "text": "4.4.3 Case Study .  In Figure 7, We conduct a case study of Llama- 2-13Bâ€™s answers to the same input before and after pre-training. From Figure 7 we can see that LlaMA2-13B-PT could output the correct answers after pre-training on  LLM-LR , which validates the effectiveness of our pre-training approach. ", "page_idx": 7, "bbox": [53, 580.853759765625, 295, 637.0803833007812], "page_size": [612.0, 792.0]}
{"layout": 98, "type": "text", "text": "5 RELATED WORK ", "text_level": 1, "page_idx": 7, "bbox": [53, 646, 158, 658], "page_size": [612.0, 792.0]}
{"layout": 99, "type": "text", "text": "5.1 Large Language Models (LLMs) ", "text_level": 1, "page_idx": 7, "bbox": [53, 662, 232, 675], "page_size": [612.0, 792.0]}
{"layout": 100, "type": "text", "text": "We are fortunate to witness the surging development of Large Language Models (LLMs [ 3 ,  8 ,  9 ,  32 ]), and a series of work aiming to leverage the reasoning abilities of LLMs such as chain-of-thought prompting [ 19 ,  46 ,  51 ], self verification [ 18 ,  44 ], self learning [ 15 , 49 ], etc. However, recent studies show LLMs still stumble across generating hallucination and logic inconsistency [ 2 ,  13 ,  16 ,  17 ,  20 ]. To solve such challenges, our work explores teaching LLMs logical reasoning through various approaches. ", "page_idx": 7, "bbox": [53, 675.3427734375, 295, 709.6513671875], "page_size": [612.0, 792.0]}
{"layout": 101, "type": "text", "text": "", "page_idx": 7, "bbox": [317, 297.330810546875, 559, 353.558349609375], "page_size": [612.0, 792.0]}
{"layout": 102, "type": "text", "text": "5.2 Event Relation Extraction (ERE) ", "text_level": 1, "page_idx": 7, "bbox": [317, 369, 502, 381], "page_size": [612.0, 792.0]}
{"layout": 103, "type": "text", "text": "Events play crucial roles in comprehending narratives, and under- standing the complex relationships between events is essential to understanding the text [ 37 ]. Thus Event Relation Extraction (ERE) tasks are fundamental information extraction (IE) tasks and support various downstream applications [ 5 ,  36 ,  50 ]. Extensive studies have been carried out on ERE tasks, including different kinds of relations such as co reference relations [ 24 ,  25 ], temporal relations [ 30 ,  40 ], causal relations [4, 6, 7], and subevent relations [1, 41]. ", "page_idx": 7, "bbox": [317, 382.226806640625, 559, 471.33135986328125], "page_size": [612.0, 792.0]}
{"layout": 104, "type": "text", "text": "There also have been some recent explorations on how to lever- age the power of LLMs on event-related information extraction tasks [ 12 ,  27 ,  43 ]. To the best of our knowledge, however, our work is the first to (1) design elaborate experiments to evaluate the per- formance of LLMs on the ERE task, and (2) analyze the logical reasoning abilities of LLMs using ERE as an intermediate task. ", "page_idx": 7, "bbox": [317, 469.8988037109375, 559, 537.0844116210938], "page_size": [612.0, 792.0]}
{"layout": 105, "type": "text", "text": "6 CONCLUSION ", "text_level": 1, "page_idx": 7, "bbox": [317, 552, 406, 565], "page_size": [612.0, 792.0]}
{"layout": 106, "type": "text", "text": "In this paper, we conduct a detailed investigation on how to enhance LLMs to produce more logically consistent answers. Specifically, we first investigate the existing issues of current LLMs in doing some complex reasoning tasks (e.g., event relation extraction and deductive reasoning). Then, we study multiple strategies to obtain and utilize logic for LLMs, including generative-based, retrieval- based, and pre training-based approaches. Based on our approach, we also contribute a synthesized dataset ( LLM-LR ) involving multi- hop reasoning for evaluation and pre-training. We show that LLMs are not logically consistent reasoners, but their performance could be improved if we explicitly teach them the logical constraints. Comprehensive quantitative and qualitative analyses have been conducted to further provide insights. ", "page_idx": 7, "bbox": [317, 565.7537841796875, 559, 709.6513671875], "page_size": [612.0, 792.0]}
{"layout": 107, "type": "text", "text": "REFERENCES ", "text_level": 1, "page_idx": 8, "bbox": [53, 85, 123, 95], "page_size": [612.0, 792.0]}
{"layout": 108, "type": "text", "text": "[1]  Mohammed Aldawsari and Mark Finlayson. 2019. Detecting Subevents using Discourse and Narrative Features. In  Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics . Association for Computational Linguistics, Florence, Italy, 4780â€“4790. https://doi.org/10.18653/v1/P19-1471\n\n [2]  Yejin Bang, Samuel Cah yaw i jaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, et al .  2023. A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity.  arXiv preprint arXiv:2302.04023  (2023).\n\n [3]  Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neel a kant an, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners. In  Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual , Hugo Larochelle, Marcâ€™Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (Eds.). https://proceedings.neurips.cc/paper/2020/ hash/1457 c 0 d 6 bfc b 4967418 b fb 8 ac 142 f 64 a-Abstract.html\n\n [4]  Tommaso Caselli and Piek Vossen. 2017. The Event StoryLine Corpus: A New Benchmark for Causal and Temporal Relation Extraction. In  Proceedings of the Events and Stories in the News Workshop . Association for Computational Linguis- tics, Vancouver, Canada, 77â€“86. https://doi.org/10.18653/v1/W17-2711\n\n [5]  Snigdha Chaturvedi, Haoruo Peng, and Dan Roth. 2017. Story Comprehension for Predicting What Happens Next. In  Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing . Association for Computational Linguistics, Copenhagen, Denmark, 1603â€“1614. https://doi.org/10.18653/v1/D17- 1168\n\n [6]  Meiqi Chen, Yixin Cao, Kunquan Deng, Mukai Li, Kun Wang, Jing Shao, and Yan Zhang. 2022. ERGO: Event Relational Graph Transformer for Document-level Event Causality Identification. In  Proceedings of the 29th International Conference on Computational Linguistics . International Committee on Computational Lin- guistics, Gyeongju, Republic of Korea, 2118â€“2128. https://a cl anthology.org/2022. coling-1.185\n\n [7]  Meiqi Chen, Yixin Cao, Yan Zhang, and Zhiwei Liu. 2023. CHEER: Centrality- aware High-order Event Reasoning Network for Document-level Event Causality Identification. In  Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) . 10804â€“10816.\n\n [8]  Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Se- bastian Gehrmann, et al .  2022. Palm: Scaling language modeling with pathways. ArXiv preprint  abs/2204.02311 (2022). https://arxiv.org/abs/2204.02311\n\n [9]  Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al .  2022. Scaling instruction-finetuned language models.  ArXiv preprint  abs/2210.11416 (2022). https://arxiv.org/abs/2210.11416\n\n [10]  William F Clocksin and Christopher S Mellish. 2003.  Programming in PROLOG . Springer Science & Business Media.\n\n [11]  Bruce Frederiksen. 2008. Applying expert system technology to code reuse with Pyke.  PyCon: Chicago  (2008).\n\n [12]  Jun Gao, Huan Zhao, Changlong Yu, and Ruifeng Xu. 2023. Exploring the Feasi- bility of ChatGPT for Event Extraction. https://arxiv.org/abs/2303.03836\n\n [13]  Olga Golovneva, Moya Chen, Spencer Poff, Martin Corredor, Luke Z ett le moyer, Maryam Fazel-Zarandi, and Asli Cel i kyi l maz. 2022. ROSCOE: A Suite of Metrics for Scoring Step-by-Step Reasoning. arXiv:2212.07919 [cs.CL]\n\n [14]  Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. LoRA: Low-Rank Adaptation of Large Language Models. In  International Conference on Learning Representations . https: //openreview.net/forum?id=n ZeV Kee FY f 9\n\n [15]  Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han. 2022. Large Language Models Can Self-Improve. arXiv:2210.11610 [cs.CL]\n\n [16]  Myeongjun Jang and Thomas L ukasiewicz. 2023. Consistency Analysis of Chat- GPT. arXiv:2303.06273 [cs.CL]\n\n [17]  Fangkai Jiao, Zhiyang Teng, Shafiq Joty, Bosheng Ding, Aixin Sun, Zhengyuan Liu, and Nancy F Chen. 2023. LogicLLM: Exploring Self-supervised Logic-enhanced Training for Large Language Models.  arXiv preprint arXiv:2305.13718  (2023).\n\n [18]  Jaehun Jung, Lianhui Qin, Sean Welleck, Faeze Brahman, Chandra Bhaga va tula, Ronan Le Bras, and Yejin Choi. 2022. Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations. In  Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing . Association for Computational Linguistics, Abu Dhabi, United Arab Emirates, 1266â€“1279. https://a cl anthology. org/2022.emnlp-main.82\n\n [19]  Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners.  ArXiv preprint abs/2205.11916 (2022). https://arxiv.org/abs/2205.11916\n\n ", "page_idx": 8, "bbox": [53, 96.51348114013672, 295, 704.2928466796875], "page_size": [612.0, 792.0]}
{"layout": 109, "type": "text", "text": "[20]  Hanmeng Liu, Ruoxi Ning, Zhiyang Teng, Jian Liu, Qiji Zhou, and Yue Zhang. 2023. Evaluating the logical reasoning ability of chatgpt and gpt-4.  arXiv preprint arXiv:2304.03439  (2023).\n\n [21]  Kang Liu, Yubo Chen, Jian Liu, Xinyu Zuo, and Jun Zhao. 2020. Extracting Events and Their Relations from Texts: A Survey on Recent Research Progress and Challenges.  AI Open  1 (2020), 22â€“39. https://doi.org/10.1016/j.aiopen.2021.02.004\n\n [22]  John W Lloyd. 2012.  Foundations of logic programming . Springer Science & Business Media.\n\n [23]  Ilya Loshchilov and Frank Hutter. 2019. Decoupled Weight Decay Regular iz ation. In  7th International Conference on Learning Representations, ICLR 2019, New Or- leans, LA, USA, May 6-9, 2019 . OpenReview.net. https://openreview.net/forum? id=Bkg6RiCqY7\n\n[24]  Jing Lu and Vincent Ng. 2021. Conundrums in Event Co reference Resolution: Making Sense of the State of the Art. In  Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing . Association for Computational Linguistics, Online and Punta Cana, Dominican Republic, 1368â€“1380. https: //doi.org/10.18653/v1/2021.emnlp-main.103\n\n [25]  Yaojie Lu, Hongyu Lin, Jialong Tang, Xianpei Han, and Le Sun. 2022. End-to-end neural event co reference resolution.  Artificial Intelligence  303 (2022), 103632.\n\n [26]  Qing Lyu, Shreya Havaldar, Adam Stein, Li Zhang, Delip Rao, Eric Wong, Mar- ianna Apidianaki, and Chris Callison-Burch. 2023. Faithful Chain-of-Thought Reasoning.  ArXiv preprint  abs/2301.13379 (2023). https://arxiv.org/abs/2301.13379\n\n [27]  Yubo Ma, Yixin Cao, YongChing Hong, and Aixin Sun. 2023. Large Language Model Is Not a Good Few-shot Information Extractor, but a Good Reranker for Hard Samples! arXiv:2303.08559 [cs.CL]\n\n [28]  Paramita Mirza, Rachele Sprugnoli, Sara Tonelli, and Manuela Speranza. 2014. Annotating Causality in the TempEval-3 Corpus. In  Proceedings of the EACL 2014 Workshop on Computational Approaches to Causality in Language (CAtoCL) . Association for Computational Linguistics, Gothenburg, Sweden, 10â€“19. https: //doi.org/10.3115/v1/W14-0702\n\n [29]  Paramita Mirza and Sara Tonelli. 2014. An Analysis of Causality between Events and its Relation to Temporal Information. In  Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers . Dublin City University and Association for Computational Linguistics, Dublin, Ireland, 2097â€“2106. https://aclanthology.org/C14-1198\n\n [30]  Qiang Ning, Zhili Feng, Hao Wu, and Dan Roth. 2018. Joint Reasoning for Temporal and Causal Relations. In  Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) . Association for Computational Linguistics, Melbourne, Australia, 2278â€“2288. https://doi.org/10. 18653/v1/P18-1212\n\n [31]  Tim Oâ€™Gorman, Kristin Wright-Bettner, and Martha Palmer. 2016. Richer Event Description: Integrating event co reference with temporal, causal and bridging annotation. In  Proceedings of the 2nd Workshop on Computing News Storylines (CNS 2016) . Association for Computational Linguistics, Austin, Texas, 47â€“56. https://doi.org/10.18653/v1/W16-5706\n\n [32]  Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al .  2022. Training language models to follow instructions with human feedback.  Advances in Neural Information Processing Systems  35 (2022), 27730â€“27744.\n\n [33]  Liangming Pan, Alon Albalak, Xinyi Wang, and William Yang Wang. 2023. Logic- LM: Empowering Large Language Models with Symbolic Solvers for Faithful Logical Reasoning.  ArXiv preprint  abs/2305.12295 (2023). https://arxiv.org/abs/ 2305.12295\n\n [34]  Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. 2023. The RefinedWeb dataset for Falcon LLM: outperforming curated corpora with web data, and web data only.  arXiv preprint arXiv:2306.01116 (2023).\n\n [35]  David L Poole and Alan K Mackworth. 2010.  Artificial Intelligence: foundations of computational agents . Cambridge University Press.\n\n [36]  Maarten Sap, Ronan Le Bras, Emily Allaway, Chandra Bhaga va tula, Nicholas Lourie, Hannah Rashkin, Brendan Roof, Noah A. Smith, and Yejin Choi. 2019. ATOMIC: An Atlas of Machine Commonsense for If-Then Reasoning. In  The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019, Honolulu, Hawaii, USA, January 27 - February 1, 2019 . AAAI Press, 3027â€“3035. https://doi.org/10.1609/aaai.v33i01.33013027\n\n [37]  Beth M. Sundheim. 1991. Evaluating Text Understanding Systems. In  Speech and Natural Language: Proceedings of a Workshop Held at Pacific Grove, California, February 19-22, 1991 . https://a cl anthology.org/H91-1093\n\n [38]  Oyvind Tafjord, Bhavana Dalvi, and Peter Clark. 2021. Proof Writer: Gener- ating Implications, Proofs, and Abductive Statements over Natural Language. In  Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021 . Association for Computational Linguistics, Online, 3621â€“3634. https://doi.org/ 10.18653/v1/2021.findings-acl.317 ", "page_idx": 8, "bbox": [317, 85.80345916748047, 559, 701.1692504882812], "page_size": [612.0, 792.0]}
{"layout": 110, "type": "text", "text": "[39]  Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yas- mine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhos- ale, et al .  2023. Llama 2: Open foundation and fine-tuned chat models.  arXiv preprint arXiv:2307.09288  (2023).\n\n [40]  Haoyu Wang, Muhao Chen, Hongming Zhang, and Dan Roth. 2020. Joint Con- strained Learning for Event-Event Relation Extraction. In  Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) . Asso- ciation for Computational Linguistics, Online, 696â€“706. https://doi.org/10.18653/ v1/2020.emnlp-main.51\n\n [41]  Haoyu Wang, Hongming Zhang, Muhao Chen, and Dan Roth. 2021. Learning Constraints and Descriptive Segmentation for Subevent Detection. In  Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing . Association for Computational Linguistics, Online and Punta Cana, Dominican Republic, 5216â€“5226. https://doi.org/10.18653/v1/2021.emnlp-main.423\n\n [42]  Xiaozhi Wang, Yulin Chen, Ning Ding, Hao Peng, Zimu Wang, Yankai Lin, Xu Han, Lei Hou, Juanzi Li, Zhiyuan Liu, Peng Li, and Jie Zhou. 2022. MAVEN-ERE: A Uni- fied Large-scale Dataset for Event Co reference, Temporal, Causal, and Subevent Relation Extraction. In  Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing . Association for Computational Linguistics, Abu Dhabi, United Arab Emirates, 926â€“941. https://a cl anthology.org/2022.emnlp- main.60\n\n [43]  Xingyao Wang, Sha Li, and Heng Ji. 2022. Code 4 Struct: Code Generation for Few-Shot Structured Prediction from Natural Language. arXiv:2210.12810 [cs.CL]\n\n [44] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. 2022. Self-consistency improves chain of thought reasoning in language models. ArXiv preprint  abs/2203.11171 (2022). https://arxiv.org/abs/2203.11171\n\n [45]  Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. 2022. Emergent Abilities of Large Language Models.  Trans. Mach. Learn. Res.  (2022).\n\n [46]  Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. 2022. Chain of thought prompting elicits reasoning in large language models.  ArXiv preprint  abs/2201.11903 (2022). https://arxiv.org/abs/ 2201.11903\n\n [47]  Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. 2022. Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. In  NeurIPS .\n\n [48]  Fangzhi Xu, Qika Lin, Jiawei Han, Tianzhe Zhao, Jun Liu, and Erik Cambria. 2023. Are Large Language Models Really Good Logical Reasoners? A Comprehensive Evaluation From Deductive, Inductive and Abductive Views.  ArXiv preprint abs/2306.09841 (2023). https://arxiv.org/abs/2306.09841\n\n [49]  Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. 2022. STaR: Bootstrap- ping Reasoning With Reasoning. In  Advances in Neural Information Processing Systems , Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (Eds.). https://openreview.net/forum?id=3 EL Rd g 2 sgI\n\n [50]  Hongming Zhang, Daniel Khashabi, Yangqiu Song, and Dan Roth. 2020. Tran- sOMCS: From Linguistic Graphs to Commonsense Knowledge. In  Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI 2020 , Christian Bessiere (Ed.). ijcai.org, 4004â€“4010. https://doi.org/10.24963/ijcai. 2020/554\n\n [51]  Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. 2022. Automatic chain of thought prompting in large language models.  ArXiv preprint  abs/2210.03493 (2022). https://arxiv.org/abs/2210.03493 ", "page_idx": 9, "bbox": [53, 85.80345916748047, 295, 517.8572387695312], "page_size": [612.0, 792.0]}
{"layout": 111, "type": "text", "text": "A UNDERSTANDING EVENT RELATIONS ", "text_level": 1, "page_idx": 10, "bbox": [53, 84, 271, 95], "page_size": [612.0, 792.0]}
{"layout": 112, "type": "text", "text": "There are four kinds of widely-used event relations: co reference, temporal, causal, and subevent relations [31, 42].\n\n ", "page_idx": 10, "bbox": [53, 97.25978088378906, 295, 120.61039733886719], "page_size": [612.0, 792.0]}
{"layout": 113, "type": "text", "text": "(1)  Co reference relations  between events occur when multiple event mentions in a text refer to the same underlying event. We call these event mentions  cluster .\n\n (2)  Temporal relations  refer to the temporal ordering of events based on their occurrence in time. In this paper, we consider seven different types of temporal relations:  NO_TEMPO AL: i there is no clear temporal relation be- â€¢ tween event  ð´ and  ðµ . â€¢  BEFORE: if event    $A$  appened completely before vent  $B$  . â€¢  OVERLAP: if event    $A$  as an overlap with ev t  ðµ .  CONTAINS: if event â€™s time contains event â€™s time. â€¢  ð´  ðµ â€¢  SIMULTANEOUS: i vents A and  $B$  hap en at the same time.  ENDS-ON: if event ends when e nt starts. â€¢  ð´  ðµ  BEGINS-ON: if event and event start at the same time, â€¢  ð´  ðµ but end at different times. In Figure 8, we list all the types of temporal relations and illus- trate their distinctions on a unified timeline.\n\n (3)  Causal relations  refer to that one event (the cause) brings about or influences the occurrence of another event (the effect). They can be classified into two different types:  CAUSE  relation where the tail event is inevitable given the head event, and  PRECON- DITION  where the tail event would not have happened if the head event had not happened.\n\n (4)  Subevent relations  refer to the connection where one event (the subevent) is a component or a smaller part of another event (the main event). Identifying and understanding subevent relations helps to reveal the underlying hierarchy and organizational structure of events in a given text. ", "page_idx": 10, "bbox": [53, 129.21180725097656, 295, 437.4943542480469], "page_size": [612.0, 792.0]}
{"layout": 114, "type": "text", "text": "Event Relation Extraction.  Event Relation Extraction (ERE) in- cludes identifying co reference, temporal, causal, and subevent rela- tions between every two events in the text. We formulate ERE as a multi-classification problem, determining one label (relation) for each of these four relation types. For co reference relations, the labels {NO CO REFERENCE, CO REFERENCE}; for temporal relations, âˆˆ the labels  $\\in\\{\\mathrm{NO}_{-}$  TEMPORAL, BEFORE, OVERLAP, CONTAINS, SIMULTANEOUS, ENDS-ON, BEGINS-ON}; for causal relations, the labels  $\\in$  {NO_CAUSAL, PRECONDITION, CAUSE}; for subevent relations, the labels  $\\in\\{\\sf N O_{\\tau}}$  _SUBEVENT, SUBEVENT}. ", "page_idx": 10, "bbox": [53, 449.97381591796875, 295, 561], "page_size": [612.0, 792.0]}
{"layout": 115, "type": "text", "text": "B TRAINING DETAILS OF ROBERTA-LARGE ON TWO TASKS ", "text_level": 1, "page_idx": 10, "bbox": [53, 578, 286, 603], "page_size": [612.0, 792.0]}
{"layout": 116, "type": "text", "text": "B.1 Dataset Construction ", "text_level": 1, "page_idx": 10, "bbox": [53, 608, 185, 619], "page_size": [612.0, 792.0]}
{"layout": 117, "type": "text", "text": "MAVEN-ERE.  contains 4,480 documents, 103,193 events corefer- ence chains, 1,216,217 temporal relations, 57,992 causal relations, and 15,841 subevent relations, which is larger than existing datasets of all the ERE tasks by at least an order of magnitude [ 42 ]. MAVEN- ERE has released the train and valid set, but does not release the ground-truth test set, so we randomly split its train set into train/- valid sets with a ratio of 8:2, and then use its original valid set as the new test set. ", "page_idx": 10, "bbox": [53, 620.5477905273438, 295, 709.6513671875], "page_size": [612.0, 792.0]}
{"layout": 118, "type": "table", "page_idx": 10, "img_path": "layout_images/2310.09158v1_10.jpg", "bbox": [337, 84, 538, 213], "page_size": [612.0, 792.0], "ocr_text": "Relation(A, B)\n\nBEFORE\n\nOVERLAP\n\nCONTAINS\n\nSIMULTANEOUS\n\nENDS-ON\n\nBEGINS-ON\n\n", "vlm_text": "The table visually represents temporal relationships between two events or intervals, A and B, along a timeline. It shows different ways in which the interval B can relate to interval A in terms of timing. Here are the temporal relations depicted in the table:\n\n1. **BEFORE:** Interval B occurs completely before interval A on the timeline.\n2. **OVERLAP:** Interval B begins before interval A ends, creating an overlap between the two.\n3. **CONTAINS:** Interval A completely contains interval B, meaning B occurs entirely within the timeframe of A.\n4. **SIMULTANEOUS:** Intervals A and B occur simultaneously, starting and ending at the same time.\n5. **ENDS-ON:** Interval B ends at the same time as interval A but starts before A.\n6. **BEGINS-ON:** Interval B starts at the same time as interval A but ends after A.\n\nThese relationships help in understanding the sequencing and interaction of events over time."}
{"layout": 119, "type": "text", "text": "Figure 8: Interpretations of the temporal relation between two events A and B. Brackets represent time intervals along the time axis. ", "text_level": 1, "page_idx": 10, "bbox": [317, 226, 559, 259], "page_size": [612.0, 792.0]}
{"layout": 120, "type": "text", "text": "Causal-TimeBank.  contains 184 documents, 6,813 events, and 7,608 event pairs [ 29 ]. Among them, 318 event pairs are annotated with causal relations, and 6,115 event pairs are annotated with temporal relations. Due to Causal-TimeBank does not split train/- valid/test sets, we randomly split it to train/valid/test sets with a ratio of 6:1:3. We do not evaluate co reference and subevent relations in the Causal-TimeBank dataset since there are no annotations for these two relation types. ", "page_idx": 10, "bbox": [317, 290.0768127441406, 559, 379.18035888671875], "page_size": [612.0, 792.0]}
{"layout": 121, "type": "text", "text": "Proof Writer.  We use the OWA subset of Proof Writer and con- sider the hardest 5-hop subset. The training, valid, and test sets contain 3000, 600, and 600 samples, respectively. ", "page_idx": 10, "bbox": [317, 387.2828063964844, 559, 421.59234619140625], "page_size": [612.0, 792.0]}
{"layout": 122, "type": "text", "text": "B.2 Experimental Setup ", "text_level": 1, "page_idx": 10, "bbox": [317, 435, 442, 447], "page_size": [612.0, 792.0]}
{"layout": 123, "type": "text", "text": "Our experiments include two settings. (1) fully fine-tuned: we fine- tune SLMs with complete and abundant samples. This setting is for reference to see the performance limit of SLMs. (2) one-shot: we sample only one example for each label and construct a tiny train- ing set. This setting is for direct comparison with our experiments on LLMs (similar training/demonstration sample number). ", "page_idx": 10, "bbox": [317, 448.12481689453125, 559, 515.3103637695312], "page_size": [612.0, 792.0]}
{"layout": 124, "type": "text", "text": "We implement vanilla fine-tuning approaches on three datasets and use RoBERTa-Large as backbones. We run each experiment on a single NVIDIA V100 GPU. We adopt the AdamW [ 23 ] optimizer with a linear scheduler and 0.1 warm-up steps. We set the weight- decay coefficient as 1e-5 and maximum gradient norms as 1.0. We set the batch size as 16 with 20 or 50 epochs. We set the maximum input length as 256 and the learning rate as 2e-5. ", "page_idx": 10, "bbox": [317, 513.8777465820312, 559, 592.0223999023438], "page_size": [612.0, 792.0]}
{"layout": 125, "type": "text", "text": "C LOGICAL CONSTRAINTS BETWEEN TWO EVENTS ", "text_level": 1, "page_idx": 10, "bbox": [317, 606, 550, 629], "page_size": [612.0, 792.0]}
{"layout": 126, "type": "text", "text": "In Table 4, we provide a comprehensive set of logical constraints for the relations between two events to assess their logical consistency. We also manually design description text for each constraint to let LLMs follow the prompt. As shown in Table 6, CO REFERENCE( ð´ ,  $B)\\to\\lnot\\mathrm{EMPORAL}(A,\\,B)$  ,    $\\neg\\mathrm{{CASAL}}(A,\\,B)$  ,  Â¬ SUBEVENT( ð´ ,  ðµ ) indicates that \"if event  ð´ and event  ðµ have a co reference relation, they will not have temporal, causal, and subevent relations\". ", "page_idx": 10, "bbox": [317, 631.5067749023438, 559, 709.6513671875], "page_size": [612.0, 792.0]}
{"layout": 127, "type": "table", "page_idx": 11, "img_path": "layout_images/2310.09158v1_11.jpg", "table_footnote": "Table 4: Logical Constraints of relations between two events, where  denotes \"NOT\", denotes \"OR\". ", "bbox": [100, 82, 512, 224], "page_size": [612.0, 792.0], "ocr_text": "If Relation(A, B)\n\nThen Relation (A, B)\n\nThen Relation (B, A)\n\nCOREFERENCE\naTEMPORAL\nBEFORE\nOVERLAP\nCONTAINS\nSIMULTANEOUS\nENDS-ON\nBEGINS-ON\nCAUSE\nPRECONDITION\nSUBEVENT\n\naTEMPORAL, =CAUSAL, -SUBEVENT\n=CAUSAL, -SUBEVENT\nâ€”=COREFERENCE, =SUBEVENT\n=COREFERENCE, =SUBEVENT\n=COREFERENCE, =CAUSAL\n=COREFERENCE, =CAUSAL, ~SUBEVENT\nâ€”=COREFERENCE, =CAUSAL, =SUBEVENT\n=COREFERENCE, =CAUSAL, =SUBEVENT\nâ€”=COREFERENCE, BEFORE V OVERLAP, -SUBEVENT\n=COREFERENCE, BEFORE V OVERLAP, ~-SUBEVENT\n=COREFERENCE, CONTAINS =CAUSAL\n\nCOREFERENCE\n/\naTEMPORAL\naTEMPORAL\naTEMPORAL\nSIMULTANEOUS\naTEMPORAL\nBEGINS-ON\naTEMPORAL\naTEMPORAL\naTEMPORAL\n", "vlm_text": "The table presents relational logic for events between entities A and B, organized into three columns:\n\n1. **If Relation(A, B)**: Lists initial conditions or event relationships between A and B, such as:\n   - COREFERENCE\n   - Â¬TEMPORAL\n   - BEFORE\n   - OVERLAP\n   - CONTAINS\n   - SIMULTANEOUS\n   - ENDS-ON\n   - BEGINS-ON\n   - CAUSE\n   - PRECONDITION\n   - SUBEVENT\n\n2. **Then Relation (A, B)**: Lists resulting relations when the initial conditions are met, denoted by:\n   - Negatives of conditions (Â¬) like TEMPORAL, CAUSAL, SUBEVENT, COREFERENCE\n   - Combinations with logical operators like v (or), such as Â¬CORE, BEFORE v OVERLAP\n\n3. **Then Relation (B, A)**: Represents the resultant relations from the perspective of B to A:\n   - Includes conditions like COREFERENCE, SIMULTANEOUS, BEGINS-ON\n   - Negatives like Â¬TEMPORAL\n\nThis table essentially describes logical outcomes for certain types of event relations between two entities."}
{"layout": 128, "type": "text", "text": " NO CO REFERENCE, SIMULTANEOUS, NO_CAUSAL, â€¢ NO SUB EVENT  NO CO REFERENCE, OVERLAP, CAUSE, NO SUB EVENT â€¢  NO CO REFERENCE, BEFORE, CAUSE, NO SUB EVENT â€¢  NO CO REFERENCE, NO TEMPORAL, NO_CAUSAL, â€¢ NO SUB EVENT ", "page_idx": 11, "bbox": [333, 252.00978088378906, 559, 319.1962585449219], "page_size": [612.0, 792.0]}
{"layout": 129, "type": "text", "text": "C.1 An Example of Detecting Conflicts and Retrieving Relevant Constraints ", "text_level": 1, "page_idx": 11, "bbox": [53, 252, 275, 277], "page_size": [612.0, 792.0]}
{"layout": 130, "type": "text", "text": "As described above, for the ERE task, we meticulously collect 11 logical constraints covering all relations between two events. These constraints serve as our benchmark to identify inconsistencies in the predictions made by LLMs. ", "page_idx": 11, "bbox": [53, 278.6598205566406, 295, 323.9283447265625], "page_size": [612.0, 792.0]}
{"layout": 131, "type": "text", "text": "Let us consider an illustrative example. If LLM produces an answer such as â€œNO CO REFERENCE, SIMULTANEOUS, CAUSE, NO SUB EVENTâ€ (refer to Figure 5), we could detect the inconsis- tency between â€œSIMULTANEOUSâ€ and â€œCAUSEâ€, as shown in Table 4: ", "page_idx": 11, "bbox": [53, 322.4958190917969, 295, 378.72235107421875], "page_size": [612.0, 792.0]}
{"layout": 132, "type": "text", "text": "as the ultimate answer, thus ensuring that the results must be logically consistent (i.e.,  $\\mathrm{LI}=0)$  ). ", "page_idx": 11, "bbox": [317, 329.1177978515625, 559, 352.4683532714844], "page_size": [612.0, 792.0]}
{"layout": 133, "type": "text", "text": "D LOGICAL CONSTRAINTS AMONG THREE EVENTS ", "text_level": 1, "page_idx": 11, "bbox": [317, 372, 550, 396], "page_size": [612.0, 792.0]}
{"layout": 134, "type": "text", "text": " A â€œSIMULTANEOUSâ€ relation implies a â€œNO_CAUSALâ€ (Â¬CAUSAL) â€¢ relation.  Conversely, a â€œCAUSEâ€ relation suggests the presence of â€¢ either a â€œBEFOREâ€ or an â€œOVERLAPâ€ relation. ", "page_idx": 11, "bbox": [69, 385.21380615234375, 318.9826354980469, 430.48236083984375], "page_size": [612.0, 792.0]}
{"layout": 135, "type": "text", "text": "We provide a comprehensive set of 39 logical constraints for the relations among three events in Table 7. We also manually design prompt for each constraint, as shown in Table 8. ", "page_idx": 11, "bbox": [317, 397.7018127441406, 559, 432.0113525390625], "page_size": [612.0, 792.0]}
{"layout": 136, "type": "text", "text": "Given this, â€œSIMULTANEOUSâ€ and â€œCAUSEâ€ are inherently con- tradictory, and they cannot coexist in a consistent prediction. To rectify this, we retrieve the associated textual description from Ta- ble 6. Specifically, the statements â€œIf event  ð´ CAUSEs event  $B$  , then event  $A$  happens BEFORE or OVERLAP event  ðµ ...â€ and â€œIf event  $A$  and event  $B$  happen SIMULTANEOUSly, then they wonâ€™t have co reference, causal, and subevent relations ...â€ are integrated into the LLMâ€™s instruction. ", "page_idx": 11, "bbox": [53, 436.9728088378906, 295, 526.077392578125], "page_size": [612.0, 792.0]}
{"layout": 137, "type": "text", "text": "D.1 Pseudo Code of Logic Programming ", "text_level": 1, "page_idx": 11, "bbox": [317, 451, 522, 464], "page_size": [612.0, 792.0]}
{"layout": 138, "type": "text", "text": "Once obtain 11 constraints between two events and 39 constraints among three events, we apply logic programming to automatically reason new event relations by inputting known constraints and relations. The pseudo-code mentioned in the main context is shown in Algorithm 1. ", "page_idx": 11, "bbox": [317, 464.2928161621094, 559, 520.5203857421875], "page_size": [612.0, 792.0]}
{"layout": 139, "type": "text", "text": "E CASE STUDY ON SELF-GENERATED LOGICAL CONSTRAINTS ", "text_level": 1, "page_idx": 11, "bbox": [317, 539, 518, 565], "page_size": [612.0, 792.0]}
{"layout": 140, "type": "text", "text": "C.2 An Example of Post-processing ", "text_level": 1, "page_idx": 11, "bbox": [53, 542, 235, 554], "page_size": [612.0, 792.0]}
{"layout": 141, "type": "text", "text": "As shown in Figure 5, if LLMs predict the relations between two events as â€œNO CO REFERENCE, SIMULTANEOUS, CAUSE, NO SUB EVENTâ€, we can detect that â€œSIMULTANEOUSâ€ and â€œCAUSEâ€ are in conflict according to the logical constraints. In order to elim- inate conflicts, one relation can be fixed first, and then the other relation can be randomly decided by the candidates that do not conflict with the current relation. For example, when the fixed tem- poral relation is â€œSIMULTANEOUSâ€, the causal relations can only be â€œNO_CAUSALâ€, while when the fixed causal relation is â€œCAUSEâ€, the temporal relation can be either â€œBEFOREâ€ or â€œOVERLAPâ€. We also add a negative option â€œNO CO REFERENCE, NO TEMPORAL, NO_CAUSAL, NO SUB EVENTâ€ to the candidate set because it is possible that neither relation exits. Finally, we randomly select one option from: ", "page_idx": 11, "bbox": [53, 554.7947998046875, 303.06976318359375, 709.6513671875], "page_size": [612.0, 792.0]}
{"layout": 142, "type": "text", "text": "In the main context, we have found that directly using CoT to infer logic does not help much for ERE tasks. One possible reason is that the inherent issues may lead to the failure of LLM in the precise rationale generation. To further illustrate an intuitive impression, we conduct a case study on MAVEN-ERE and find that the logical constraints generated by LLMs themselves are often inaccurate in content. As shown in Figure 9, ChatGPT could follow the logical constraint provided in the demonstration to a certain extent. How- ever, it wrongly applies this to other relations â€” knowing that event ð´ is event    $B^{\\dagger}$  â€™s precondition, it is wrong to think that event  $B$  will cause event  ð´ . Actually, according to the logical constraints in Ta- ble 4, the relations between  $(B,A)$   should be â€œNO CO REFERENCE, NO TEMPORAL, NO_CAUSAL, NO SUB EVENTâ€. ", "page_idx": 11, "bbox": [317, 565.7537841796875, 559, 709.6513671875], "page_size": [612.0, 792.0]}
{"layout": 143, "type": "table", "page_idx": 12, "img_path": "layout_images/2310.09158v1_12.jpg", "bbox": [54, 83, 295, 298], "page_size": [612.0, 792.0], "ocr_text": "Algorithm 1 An Example of 3-hop Reasoning\n\nInitialize the knowledge base with facts and rules\nKnowledge Base:\nFact: BEFORE(A, B)\nFact: SIMULTANEOUS(B, C)\nFact: OVERLAP(C, D)\n\nRule: BEFORE BEFORE A SIMULTANEOUS\nRule: BEFORE BEFORE A OVERLAP\nRule: OVERLAP â€” SIMULTANEOUS A OVERLAP\n\nInitialize the logic engine with the query\nQuery: BEFORE(A, D)?\n\nwhile obtain new facts do\nfor each rule r of the Knowledge Base do\nif râ€™s premise is satisfied by the current known facts then\nAdd râ€™s conclusion to the knowledge base\nend if\nend for\nend while\n", "vlm_text": "The image shows \"Algorithm 1 An Example of 3-hop Reasoning,\" which outlines a process for logic-based reasoning using facts and rules. Here's a text breakdown:\n\n1. **Initialize the Knowledge Base with Facts and Rules:**\n\n   - **Facts:**\n     - BEFORE(A, B)\n     - SIMULTANEOUS(B, C)\n     - OVERLAP(C, D)\n\n   - **Rules:**\n     - BEFORE â† BEFORE âˆ§ SIMULTANEOUS\n     - BEFORE â† BEFORE âˆ§ OVERLAP\n     - OVERLAP â† SIMULTANEOUS âˆ§ OVERLAP\n\n2. **Initialize the Logic Engine with the Query:**\n\n   - Query: BEFORE(A, D)?\n\n3. **While Loop for Obtaining New Facts:**\n\n   - For each rule \\( r \\) of the knowledge base:\n     - If \\( r \\)'s premise is satisfied by the current known facts, then:\n       - Add \\( r \\)'s conclusion to the knowledge base.\n\nThis algorithm demonstrates the process of logic inference through a series of rules and queries in a knowledge base."}
{"layout": 144, "type": "table", "page_idx": 12, "img_path": "layout_images/2310.09158v1_13.jpg", "table_caption": "Table 5: Statistics of  LLM-LR . ", "bbox": [347, 83, 529, 160], "page_size": [612.0, 792.0], "ocr_text": "Hop # Count # Average Tokens\n\n2 39 71\n3 179 83\n4 945 97\n5 5613 109\n", "vlm_text": "The table shows data across three columns:\n\n1. **Hop**: Lists the levels or steps from 2 to 5.\n2. **# Count**: Shows the number of instances or occurrences for each hop level:\n   - 2: 39\n   - 3: 179\n   - 4: 945\n   - 5: 5613\n3. **# Average Tokens**: Displays the average number of tokens for each hop level:\n   - 2: 71\n   - 3: 83\n   - 4: 97\n   - 5: 109"}
{"layout": 145, "type": "text", "text": "G IMPLEMENTATION DETAILS OF PRE TRAINING-BASED APPROACH", "text_level": 1, "page_idx": 12, "bbox": [317, 187, 518, 213], "page_size": [612.0, 792.0]}
{"layout": 146, "type": "text", "text": "We set the rank of LoRA modules to be 64. Our model is optimized with a learning rate of 2e-4 and a linear warm-up for the first  $3\\%$  steps. We clip the gradients of model parameters to a max norm of 0.3. The batch size is 8 and the number of epochs is 3. All the LoRA parameters are trained on an NVIDIA A100 GPU with 80GB memory. ", "page_idx": 12, "bbox": [317, 213.9027862548828, 559, 281.0883483886719], "page_size": [612.0, 792.0]}
{"layout": 147, "type": "text", "text": "Query result: BEFORE  $(A,D)$   is satisfied with BEFORE  $(A,C)$   and OVERLAP( ðµ ,  ð· ) ", "page_idx": 12, "bbox": [62, 303.4028015136719, 294, 326.75335693359375], "page_size": [612.0, 792.0]}
{"layout": 148, "type": "image", "page_idx": 12, "img_path": "layout_images/2310.09158v1_14.jpg", "img_caption": "Figure 9: A case study that ChatGPT generates inaccurate logical constraints. ", "bbox": [51, 362, 296, 633], "page_size": [612.0, 792.0], "ocr_text": "\\ There is a piece of text with events marked by < and > symbols .\n\nText: the scalp of Mrs. Halloway was <retrieved > and <\nreattached >, she lived for several years more but eventually\nbecame insane.\n\nEvent Pairs:\n< retrieved > and < reattached >\n< reattached > and < retrieved >\n\n< retrieved > happens BEFORE < reattached >, and < reattached >\nhappens BEFORE < became >.\n\nDue to the logical constraints: If event A happens BEFORE event\nB, then they won't have coreference and subevent relations,\n\nand event B has NO_TEMPORAL relation with event A.\n\nWe can deduce that the answers are:\n\nNO_COREFERENCE, BEFORE, NO_CAUSAL, NO_SUBEVENT.\nNO_COREFERENCE, NO_TEMPORAL, NO_CAUSAL, NO_SUBEVENT.\n\ni\nt\n1\nt\nt\nt\nt\n1\nt\n{\nt\n] From the text, we could first get:\nt\n(\n'\n1\n(\n1\nt\nt\n1\n(\n\nText: General Miaja and the political leaders who remained\n< formed > the * Junta de Defensa de Madrid â€ ( Committee for\nthe Defence of Madrid ) to < organize > the republican defenders.\n\nEvent Pairs:\n< formed > and < organise >\n< organise > and < formed >\n\nFrom the text, we could first get\n\n< formed > happens before < organise >, < formed >is the\nPRECONDITION of < organise >.\n\nDue to the logical constraints: If event A happens BEFORE event B, and\nevent A is event B's PRECONDITION, then event B causes event A.\nAnswers\n\nNO_COREFERENCE, BEFORE, PRECONDITION, NO_SUBEVENT.\nNO_COREFERENCE, NO_TEMPORAL, CAUSE, NO_SUBEVENT. 9%\n\nInstruction\n\nDemonstration\n\nInput\n", "vlm_text": "The image contains a case study illustrating how ChatGPT generates logical constraints inaccurately. It is divided into sections labeled Instruction, Demonstration, Input, and Output.\n\n1. **Instruction**: Describes marking events in a text with specific symbols.\n\n2. **Demonstration**: \n   - **Text**: Discusses the sequence of events involving Mrs. Halloway's scalp being retrieved and reattached.\n   - **Event Pairs**: Lists \"< retrieved > and < reattached >\" and \"< reattached > and < retrieved >\".\n   - **Analysis**: Concludes logical relations such as \"NO_COREFERENCE\" and \"BEFORE\".\n\n3. **Input**:\n   - **Text**: Refers to General Miaja and political leaders forming and organizing defense.\n   - **Event Pairs**: Lists \"< formed > and < organise >\" and \"< organise > and < formed >\".\n   - **Analysis**: States \"< formed > happens before < organise >\".\n\n4. **Output**:\n   - Lists supposed logical relations but with errors: the study indicates differences such as \"PRECONDITION\" instead of \"CAUSE\".\n\nThe case study highlights discrepancies in ChatGPT's logical deductions from given event sequences."}
{"layout": 149, "type": "text", "text": "F STATISTICS OF  LLM-LR ", "text_level": 1, "page_idx": 12, "bbox": [52, 672, 191, 685], "page_size": [612.0, 792.0]}
{"layout": 150, "type": "text", "text": "As shown in Table 5, we provide the statistics of our  LLM-LR dataset. ", "page_idx": 12, "bbox": [53, 686.3017578125, 294, 709.6513671875], "page_size": [612.0, 792.0]}
{"layout": 151, "type": "table", "page_idx": 13, "img_path": "layout_images/2310.09158v1_15.jpg", "table_caption": "Table 6: Prompt text of relations between two events. ", "bbox": [138, 80, 474, 544], "page_size": [612.0, 792.0], "ocr_text": "If Relation(A, B)\n\nPrompt Text\n\nCOREFERENCE\n\nIf event A and event B are COREFERENCE,\nthen they won't have temporal, causal, and subevent relations,\nand COREFERENCE relation is bidirectional.\n\nNO_TEMPORAL\n\nIf event A and event B do not have a temporal relation,\nthen they won't have causal and subevent relations.\n\nBEFORE\n\nIf event A happens BEFORE event B,\nthen they wonâ€™t have coreference and subevent relations,\nand event B has NO_TEMPORAL relation with event A.\n\nOVERLAP\n\nIf event A happens OVERLAP with event B,\nthen they wonâ€™t have coreference and subevent relations,\nand event B has NO_TEMPORAL relation with event A.\n\nCONTAINS\n\nIf event Aâ€™s time CONTAINS event Bâ€™s time,\nthen they won't have coreference and causal relations,\nand event B has NO_TEMPORAL relation with event A.\n\nSIMULTANEOUS\n\nIf event A and event B happen SIMULTANEOUSly,\nthen they wonâ€™t have coreference, causal, and subevent relations,\nand SIMULTANEOUS relation is bidirectional.\n\nENDS-ON\n\nIf event A ENDS-ON event B,\nthen they wonâ€™t have coreference, causal and subevent relations,\nand event B has NO_TEMPORAL relation with event A.\n\nBEGINS-ON\n\nIf event A BEGINS-ON event B,\nthen they won't have coreference, causal and subevent relations\nand BEGINS-ON relation is bidirectional.\n\nCAUSE\n\nIf event A CAUSEs event B,\nthen event A happens BEFORE or OVERLAP event B,\nand they wonâ€™t have coreference and subevent relations,\nand event B has NO_TEMPORAL relation with event A.\n\nPRECONDITION\n\nIf event A is event Bâ€™s PRECONDITION,\nthen event A happens BEFORE or OVERLAP event B,\nand they wonâ€™t have coreference and subevent relations,\nand event B has NO_TEMPORAL relation with event A.\n\nies}\n\nSUBEVENT\n\nIf event B is a SUBEVENT of event A,\nthen they wonâ€™t have coreference and causal relations,\nand event Aâ€™s time should CONTAINS event Bâ€™s time,\nand event B has NO_TEMPORAL relation with event A.\n\n", "vlm_text": "The table presents different relational categories between two events, A and B, along with corresponding prompt texts detailing the implications and constraints associated with each type of relation. Here is a summary of the table content:\n\n1. **COREFERENCE**: If events A and B are coreferential, they won't have temporal, causal, or subevent relationships, and the coreference relation is bidirectional.\n\n2. **NO_TEMPORAL**: If events A and B do not have a temporal relationship, they won't have causal or subevent relationships either.\n\n3. **BEFORE**: If event A happens before event B, they won't have coreference or subevent relations, and event B has no temporal relation with event A.\n\n4. **OVERLAP**: If event A overlaps with event B, they won't have coreference or subevent relations, and event B has no temporal relation with event A.\n\n5. **CONTAINS**: If the time of event A contains event B's time, they won't have coreference or causal relations, and event B has no temporal relation with event A.\n\n6. **SIMULTANEOUS**: If events A and B happen simultaneously, they won't have coreference, causal, or subevent relations, and the simultaneous relation is bidirectional.\n\n7. **ENDS-ON**: If event A ends-on event B, they won't have coreference, causal, or subevent relations, and event B has no temporal relation with event A.\n\n8. **BEGINS-ON**: If event A begins-on event B, they won't have coreference, causal, or subevent relations, and the begins-on relation is bidirectional.\n\n9. **CAUSE**: If event A causes event B, event A happens before or overlaps with event B, and they won't have coreference or subevent relations. Event B has no temporal relation with event A.\n\n10. **PRECONDITION**: If event A is a precondition of event B, then event A happens before or overlaps with event B, they won't have coreference or subevent relations, and event B has no temporal relation with event A.\n\n11. **SUBEVENT**: If event B is a subevent of event A, they won't have coreference or causal relations, event A's time should contain event B's time, and event B has no temporal relation with event A.\n\nThe table effectively maps the potential interactions (or lack thereof) between events based on their relational context."}
{"layout": 152, "type": "table", "page_idx": 14, "img_path": "layout_images/2310.09158v1_16.jpg", "table_footnote": "Table 7: Logical Constraints of relations among three events, where  âˆ§ denotes \"AND\",  Â¬  denotes \"NOT\",  âˆ¨ denotes \"OR\". ", "bbox": [63, 83, 548, 544], "page_size": [612.0, 792.0], "ocr_text": "If Relation(A, B) A Relation(B, C)\n\nThen Relation (A, C)\n\nCOREFERENCE A COREFERENCE\nCOREFERENCE A BEFORE\nCOREFERENCE A OVERLAP\nCOREFERENCE (A CONTAINS\nCOREFERENCE A SIMULTANEOUS\nCOREFERENCE A ENDS-ON\nCOREFERENCE A BEGINS-ON\nCOREFERENCE A CAUSE\nCOREFERENCE A PRECONDITION\nCOREFERENCE A SUBEVENT\nBEFORE ( BEFORE\n\nBEFORE \\ OVERLAP\n\nBEFORE A CONTAINS\n\nBEFORE A SIMULTANEOUS\nBEFORE (A ENDS-ON\n\nBEFORE A BEGINS-ON\n\nOVERLAP A BEFORE\n\nOVERLAP A SIMULTANEOUS\nCONTAINS A CONTAINS\nCONTAINS A SIMULTANEOUS\n[MULTANEOUS A BEFORE\nIMULTANEOUS A OVERLAP\nIMULTANEOUS A CONTAINS\nIMULTANEOUS A SIMULTANEOUS\nIMULTANEOUS A ENDS-ON\nMULTANEOUS A BEGINS-ON\nMULTANEOUS A COREFERENCE\nNDS-ON A CONTAINS\n\nNDS-ON A BEGINS-ON\n\nNDS-ON A SIMULTANEOUS\nEGINS-ON A SIMULTANEOUS\nEGINS-ON A BEGINS-ON\nEGINS-ON A COREFERENCE\nCAUSE A CAUSE\n\nCAUSE A PRECONDITION\n\nCAUSE A SUBEVENT\nPRECONDITION A PRECONDITION\nPRECONDITION A SUBEVENT\nSUBEVENT A SUBEVENT\n\nleo Me coi coll cols co Mico Meo Meo Meomes|\n\nzo ics Mes Wes Mics Mcs ics Mes Mes Mes!\n\nDHmMnnnnnnn\n\nDow\n\nCOREFERENCE, ~TEMPORAL, =CAUSAL, ~=SUBEVENT\n\nBEFORE, ~COREFERENCE, ~SUBEVENT\n\nOVERLAP, ~COREFERENCE, ~SUBEVENT\n\nCONTAINS, ~COREFERENCE, =CAUSAL\nSIMULTANEOUS, ~COREFERENCE, ~CAUSAL, -SUBEVENT\nENDS-ON, ~CAUSAL, ~SUBEVENT\nBEGINS-ON, =CAUSAL, -SUBEVENT\n\nCAUSE, =COREFERENCE, BEFORE V OVERLAP, =SUBEVENT\n\nPRECONDITION, =COREFERENCE, BEFORE V OVERLAP, -SUBEVENT\n\nSUBEVENT, =COREFERENCE, CONTAINS =CAUSAL\n\nBE) ERENCE, ~SUBEVENT\nBEF REFERENCE, -SUBEVENT\nBE REFERENCE, -SUBEVENT\nBEF REFERENCE, -SUBEVENT\nBE) REFERENCE, -SUBEVENT\nBE) REFERENCE, -SUBEVENT\nBEFORE, ~COREFERENCE, ~SUBEVENT\n\nOVERLAP, ~COREFERENCE, ~SUBEVENT\nCONTAINS, ~COREFERENCE, =CAUSAL\nCONTAINS, ~COREFERENCE, =CAUSAL\nBEFORE, ~COREFERENCE, ~SUBEVENT\nOVERLAP, ~COREFERENCE, ~SUBEVENT\nCONTAINS, ~COREFERENCE, =CAUSAL\nSIMULTANEOUS, ~COREFERENCE, ~CAUSAL, -SUBEVENT\nENDS-ON, ~COREFERENCE, â€”=SUBEVENT\nBEGINS-ON, ~COREFERENCE, =SUBEVENT\nSIMULTANEOUS, ~COREFERENCE, ~CAUSAL, -SUBEVENT\nBEFORE, ~COREFERENCE, ~SUBEVENT\nENDS-ON, ~COREFERENCE, =CAUSAL, =SUBEVENT\nENDS-ON, ~COREFERENCE, =CAUSAL, =SUBEVENT\nBEGINS-ON, ~COREFERENCE, ~CAUSAL, ~SUBEVENT\nBEGINS-ON, ~COREFERENCE, ~CAUSAL, ~SUBEVENT\nBEGINS-ON, =CAUSAL, -SUBEVENT\nCAUSE, ~COREFERENCE, BEFORE V OVERLAP, -SUBEVENT\nPRECONDITION, =COREFERENCE, BEFORE V OVERLAP, -SUBEVENT\nCAUSE, =COREFERENCE, BEFORE V OVERLAP, =SUBEVENT\nPRECONDITION, ~COREFERENCE, BEFORE V OVERLAP, -SUBEVENT\nPRECONDITION, ~COREFERENCE, BEFORE V OVERLAP, -SUBEVENT\nSUBEVENT, =COREFERENCE, CONTAINS =CAUSAL\n\n", "vlm_text": "The table presents logical relationships between three entities \\( A \\), \\( B \\), and \\( C \\), denoted by various relations such as \"COREFERENCE,\" \"BEFORE,\" \"OVERLAP,\" \"CONTAINS,\" \"SIMULTANEOUS,\" \"ENDS-ON,\" \"BEGINS-ON,\" \"CAUSE,\" \"PRECONDITION,\" and \"SUBEVENT.\" The format of the table is structured as rules:\n\n- The left side of each row, under the column \"If Relation(A, B) âˆ§ Relation(B, C),\" specifies a conjunction of two relations: one between \\( A \\) and \\( B \\) and another between \\( B \\) and \\( C \\).\n\n- The right side of each row, under the column \"Then Relation (A, C),\" indicates the deduced relations between \\( A \\) and \\( C \\), often preceded by negations (Â¬), suggesting a lack of that relation or specific logical constraints.\n\nThe table functions as a logic-based system for deducing the relationships between three entities when given specific pairwise relationships between them. For example, if \\( A \\) and \\( B \\) have a \"COREFERENCE\" and \\( B \\) and \\( C \\) have a \"BEFORE\" relation, then the deduced relation between \\( A \\) and \\( C \\) is \"BEFORE, Â¬COREFERENCE, Â¬SUBEVENT.\""}
{"layout": 153, "type": "table", "page_idx": 15, "img_path": "layout_images/2310.09158v1_17.jpg", "bbox": [120, 81, 492, 760], "page_size": [612.0, 792.0], "ocr_text": "If Relation(A, B) A Relation(B, C)\n\nPrompt Text\n\nCOREFERENCE A COREFERENCE\nCOREFERENCE A BEFORE\nCOREFERENCE A OVERLAP\nCOREFERENCE A CONTAINS\nCOREFERENCE A SIMULTANEOUS\nCOREFERENCE A ENDS-ON\nCOREFERENCE A BEGINS-ON\nCOREFERENCE A CAUSE\nCOREFERENCE A PRECONDITION\nCOREFERENCE A SUBEVENT\n\nIf event A and event B are COREFERENCE,\nthen the relations between event B and event C\n\nshould be the same as that between event A and event C.\n\nBEFORE A BEFORE\nBEFORE A OVERLAP\nBEFORE A CONTAINS\nBEFORE A SIMULTANEOUS\nBEFORE A ENDS-ON\nBEFORE A BEGINS-ON\n\nIf event A happens BEFORE event B, and Relation(B, C),\n\nthen event A happens BEFORE event C.\n\nOVERLAP A BEFORE\n\nIf event A happens OVERLAP with event B,\nand event B happens BEFORE event C,\nthen event A happens BEFORE event C.\n\nOVERLAP A SIMULTANEOUS\n\nIf event A happens OVERLAP with event B,\nand event B and event C happen SIMULTANEOUSly,\nthen event A happens BEFORE event C.\n\nCONTAINS A CONTAINS\n\nIf event Aâ€™s time CONTAINS event Bâ€™s time,\nand event Bâ€™s time CONTAINS event Câ€™s time,\nthen event Aâ€™s time CONTAINS event Câ€™s time.\n\nCONTAINS A SIMULTANEOUS\n\nIf event Aâ€™s time CONTAINS event Bâ€™s time,\nand event B and event C happen SIMULTANEOUSly,\nthen event Aâ€™s time CONTAINS event Câ€™s time.\n\nSIMULTANEOUS A BEFORE\nSIMULTANEOUS A OVERLAP\nSIMULTANEOUS A CONTAINS\nSIMULTANEOUS A SIMULTANEOUS\nSIMULTANEOUS A ENDS-ON\nSIMULTANEOUS A BEGINS-ON\n\nIf events A and B happen SIMULTANEOUSIy, and Relation(B, C),\n\nthen event Aâ€™s time CONTAINS event Câ€™s time.\n\nENDS-ON A CONTAINS\n\nIf event A ENDS-ON event B,\nand event Bâ€™s time CONTAINS event Câ€™s time,\nthen event A happens BEFORE event C.\n\nENDS-ON A BEGINS-ON\nENDS-ON A SIMULTANEOUS\n\nIf event A ENDS-ON event B, and Relation(B, C),\nthen event A ENDS-ON event C.\n\nBEGINS-ON A SIMULTANEOUS\nBEGINS-ON A BEGINS-ON\n\nIf event A BEGINS-ON event B, and Relation(B, C),\nthen event A BEGINS-ON event C.\n\nCAUSE A CAUSE\n\nIf event A CAUSEs event B,\nand event B CAUSEs event C,\nthen event A CAUSEs event C.\n\nCAUSE A PRECONDITION\n\nIf event A CAUSEs event B,\nand event B is event Câ€™s PRECONDITION,\nthen event A is event Câ€™s PRECONDITION.\n\nCAUSE A SUBEVENT\n\nIf event A CAUSEs event B,\nand event C is a SUBEVENT of event B,\nthen event A CAUSEs event C.\n\nPRECONDITION A PRECONDITION\n\nIf event A is event Bâ€™s PRECONDITION,\nand event B is event Câ€™s PRECONDITION,\nthen event A is event Câ€™s PRECONDITION.\n\nPRECONDITION A SUBEVENT\n\nIf event A is event Bâ€™s PRECONDITION,\nand event C is a SUBEVENT of event B,\nthen event A is event Câ€™s PRECONDITION.\n\nSUBEVENT A SUBEVENT\n\nIf event B is a SUBEVENT of event A,\nand event C is a SUBEVENT of event B,\nthen event C is a SUBEVENT of event A.\n", "vlm_text": "The table lists logical relationships between events A, B, and C, detailing different scenarios of temporal relationships. It includes columns for \"If Relation(A, B) âˆ§ Relation(B, C)\" and \"Prompt Text.\" The \"Prompt Text\" explains the outcome or condition based on the given relations.\n\n### Hereâ€™s a summary of some key rows:\n\n1. **COREFERENCE Relations**: If events A and B are COREFERENCE, then relations between B and C should mirror those between A and C.\n\n2. **BEFORE Relations**: If event A happens BEFORE event B, then event A also happens BEFORE event C.\n\n3. **OVERLAP and SIMULTANEOUS Relations**:\n   - If A OVERLAPS B, and B and C are SIMULTANEOUS, then A happens BEFORE C.\n   - If A's time contains B's, and B and C are SIMULTANEOUS, A's time contains C's time.\n\n4. **SIMULTANEOUS Relations**: If A and B are SIMULTANEOUS, and B relates to C, Aâ€™s time encompasses Câ€™s time.\n\n5. **END-ON and BEGINS-ON Relations**:\n   - If A ENDS-ON B, and B contains C, A happens before C.\n   - If A BEGINS-ON B, and B relates to C, A BEGINS-ON C.\n\n6. **CAUSE and PRECONDITION Relations**:\n   - If A CAUSES B, and B causes C, A causes C.\n   - If A is Bâ€™s PRECONDITION, and B is Câ€™s, A is Câ€™s PRECONDITION.\n\n7. **SUBEVENT Relations**: If B is a SUBEVENT of A, and C is a subevent of B, then C is a subevent of A.\n\nThe table is designed to show how different event relations can determine the overall relationship between multiple events."}
{"layout": 154, "type": "text", "text": "H PROMPT EXAMPLES ", "text_level": 1, "page_idx": 16, "bbox": [53, 81, 181, 94], "page_size": [612.0, 792.0]}
{"layout": 155, "type": "text", "text": "In this section, we provide examples of prompts used for each task and approach. ", "page_idx": 16, "bbox": [53, 94.55775451660156, 351.05224609375, 106.94932556152344], "page_size": [612.0, 792.0]}
{"layout": 156, "type": "text", "text": "H.1 Pilot Case Study ", "text_level": 1, "page_idx": 16, "bbox": [53, 116, 163, 128], "page_size": [612.0, 792.0]}
{"layout": 157, "type": "text", "text": "This section includes prompt examples of: ", "page_idx": 16, "bbox": [53, 129.4267120361328, 207.6524658203125, 141.8182830810547], "page_size": [612.0, 792.0]}
{"layout": 158, "type": "text", "text": " MAVEN-ERE w. relevant logic constraints (Figure 10); â€¢  MAVEN-ERE w. irrelevant logic constraints (Figure 11); â€¢  Proof Writer w. relevant logic constraints (Figure 12); â€¢  Proof Writer w. irrelevant logic constraints (Figure 13). â€¢  Multi-hop reasoning (Figure 14) â€¢ ", "page_idx": 16, "bbox": [69, 142.30372619628906, 280.86566162109375, 204.69017028808594], "page_size": [612.0, 792.0]}
{"layout": 159, "type": "image", "page_idx": 16, "img_path": "layout_images/2310.09158v1_18.jpg", "bbox": [59, 227, 81, 259], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image is a simple, black and white icon representing a user. It features a stylized silhouette of a person, often used to signify a generic profile or user account, accompanied by the word \"User\" written below the icon."}
{"layout": 160, "type": "image", "page_idx": 16, "img_path": "layout_images/2310.09158v1_19.jpg", "bbox": [296.25, 202, 324, 224], "page_size": [612.0, 792.0], "ocr_text": "fg\nD-\n4\n", "vlm_text": "This image shows an illustration of a lightbulb with lines around it, which often symbolizes an idea or inspiration. It has a green interior and is encased in a simple, stylized design."}
{"layout": 161, "type": "text", "text": "MAVEN-ERE w. relevant logic constraints ( ", "text_level": 1, "page_idx": 16, "bbox": [82, 208, 295.75, 222], "page_size": [612.0, 792.0]}
{"layout": 162, "type": "text", "text": "Task Description: ", "text_level": 1, "page_idx": 16, "bbox": [88, 228, 160, 237], "page_size": [612.0, 792.0]}
{"layout": 163, "type": "text", "text": "There is a piece of text with two events marked by   $<$   and  $>$   symbols, and your task is  to identify four types of relations between them  (Â·Â·Â· more context here Â·Â·Â· ) ", "page_idx": 16, "bbox": [89, 236.54759216308594, 451, 255.1150665283203], "page_size": [612.0, 792.0]}
{"layout": 164, "type": "text", "text": "Text:  ", "text_level": 1, "page_idx": 16, "bbox": [89, 263, 109, 270.75], "page_size": [612.0, 792.0]}
{"layout": 165, "type": "text", "text": "The 2013 Lahad Datu   $<$   standoff  $>$   ( also known as the Lahad Datu incursion ) was a  military  $<$   conflict  $>$   that started on 11 February 2013 and fully ended on 24 March  2013 .\n\n ", "page_idx": 16, "bbox": [87, 270.91656494140625, 445.5982360839844, 298.07623291015625], "page_size": [612.0, 792.0]}
{"layout": 166, "type": "text", "text": "Event Pairs:  ", "text_level": 1, "page_idx": 16, "bbox": [88, 305, 139, 314], "page_size": [612.0, 792.0]}
{"layout": 167, "type": "text", "text": " $<$   standoff  $>$   and   $<$   conflict >\n\n  $<$   conflict  $>$   and   $<$   standoff  $>$  ", "page_idx": 16, "bbox": [87, 313.87774658203125, 214, 332.4451904296875], "page_size": [612.0, 792.0]}
{"layout": 168, "type": "text", "text": "Answers:  ", "text_level": 1, "page_idx": 16, "bbox": [88, 340, 123, 349], "page_size": [612.0, 792.0]}
{"layout": 169, "type": "text", "text": "Event Pair 1: < standoff  $>$   and < conflict >\n\n (1) Co reference relation : CO REFERENCE  (2) Temporal relation:  CONTAINS\n\n (3) Causal relation: NO_CAUSAL  (4) Subevent relation: NO SUB EVENT Event Pair 2: < conflict  $>$   and < standoff >\n\n (1) Co reference relation:  NO CO REFERENCE (2) Temporal relation: NO TEMPORAL \n\n (3) Causal relation: NO_CAUSAL  (4) Subevent relation: NO SUB EVENT ", "page_idx": 16, "bbox": [87, 367.462158203125, 432.7189025878906, 428.99072265625], "page_size": [612.0, 792.0]}
{"layout": 170, "type": "text", "text": "User ", "text_level": 1, "page_idx": 16, "bbox": [59, 454, 81, 485], "page_size": [612.0, 792.0]}
{"layout": 171, "type": "text", "text": "Letâ€™s think step by step. You have identified that   $<$   standoff  $>$   and   $<$   conflict  $>$   are  co references.  The co reference should be bidirectional. And if two events are  CO REFERENCE, they wonâ€™t have temporal relations.  So please modify your Co reference and Temporal answers. ", "page_idx": 16, "bbox": [88, 454.79052734375, 454.1989440917969, 490.5423583984375], "page_size": [612.0, 792.0]}
{"layout": 172, "type": "image", "page_idx": 16, "img_path": "layout_images/2310.09158v1_20.jpg", "bbox": [85, 504, 497, 587], "page_size": [612.0, 792.0], "ocr_text": "answers:\n\nEvent Pair 1: < standoff > and < conflict >\n(1) Coreference relation : COREFERENCE (2) Temporal relation: NO_TEMPORAL ChatGP\n(3) Causal relation: NO_CAUSAL (4) Subevent relation: NO_SUBEVENT\n\nApologies for the confusion. Based on the revised information, here are the corrected Â©\n\nEvent Pair 2: < conflict > and < standoff >\n(1) Coreference relation: COREFERENCE oY (2) Temporal relation: NO_TEMPORAL\n(3) Causal relation: NO_CAUSAL (4) Subevent relation: NO_SUBEVENT\n", "vlm_text": "The image shows text providing corrected answers for two event pairs regarding their relationships:\n\n**Event Pair 1:**  \n- Events: <standoff> and <conflict>\n  1. Coreference relation: COREFERENCE\n  2. Temporal relation: NO_TEMPORAL\n  3. Causal relation: NO_CAUSAL\n  4. Subevent relation: NO_SUBEVENT\n\n**Event Pair 2:**\n- Events: <conflict> and <standoff>\n  1. Coreference relation: COREFERENCE\n  2. Temporal relation: NO_TEMPORAL\n  3. Causal relation: NO_CAUSAL\n  4. Subevent relation: NO_SUBEVENT\n\nThere's a checkmark next to the COREFERENCE and NO_TEMPORAL relations in both pairs."}
{"layout": 173, "type": "image", "page_idx": 17, "img_path": "layout_images/2310.09158v1_21.jpg", "bbox": [59, 109, 84, 144], "page_size": [612.0, 792.0], "ocr_text": "of\n\nca\n\n=)\n", "vlm_text": "The image shows a simple black silhouette icon of a person with the text \"User\" written below it."}
{"layout": 174, "type": "image", "page_idx": 17, "img_path": "layout_images/2310.09158v1_22.jpg", "bbox": [332.25, 83, 361, 106], "page_size": [612.0, 792.0], "ocr_text": "Â®)\n", "vlm_text": "The image shows a blue circle surrounding a light green light bulb icon that has a diagonal line crossing through it, possibly indicating \"no ideas\" or \"no power.\" The blue circle is flanked by two black curved lines."}
{"layout": 175, "type": "text", "text": "MAVEN-ERE w. irrelevant logic constraints ( ", "text_level": 1, "page_idx": 17, "bbox": [85, 87, 331.75, 103], "page_size": [612.0, 792.0]}
{"layout": 176, "type": "text", "text": "Task Description: ", "text_level": 1, "page_idx": 17, "bbox": [91, 110, 174, 117.75], "page_size": [612.0, 792.0]}
{"layout": 177, "type": "text", "text": "There is a piece of text with two events marked by   $<$   and  $>$   symbols, and your task is  to identify four types of relations between them  (Â·Â·Â· more context here Â·Â·Â· ) ", "page_idx": 17, "bbox": [92, 118.905517578125, 494, 139.5360107421875], "page_size": [612.0, 792.0]}
{"layout": 178, "type": "text", "text": "Text:  ", "text_level": 1, "page_idx": 17, "bbox": [92, 148, 116, 156.75], "page_size": [612.0, 792.0]}
{"layout": 179, "type": "text", "text": "The 2013 Lahad Datu  $<$   standoff  $>$   ( also known as the Lahad Datu incursion ) was a  military  $<$   conflict   $>$   that started on 11 February 2013 and fully ended on 24 March  2013 .\n\n ", "page_idx": 17, "bbox": [92, 157.0933837890625, 489.1328125, 187.2708740234375], "page_size": [612.0, 792.0]}
{"layout": 180, "type": "text", "text": "Event Pairs:  ", "text_level": 1, "page_idx": 17, "bbox": [91, 195, 149, 205.75], "page_size": [612.0, 792.0]}
{"layout": 181, "type": "text", "text": "< standoff  $>$   and   $<$   conflict >\n\n < conflict  $>$   and   $<$   standoff > ", "page_idx": 17, "bbox": [92, 204.82818603515625, 231, 225.45867919921875], "page_size": [612.0, 792.0]}
{"layout": 182, "type": "text", "text": "Answers:  ", "page_idx": 17, "bbox": [92, 233.3417205810547, 135.9124298095703, 244.2079620361328], "page_size": [612.0, 792.0]}
{"layout": 183, "type": "text", "text": "Event Pair 1: < standoff  $>$   and < conflict >\n\n (1) Co reference relation : CO REFERENCE  (2) Temporal relation:  CONTAINS\n\n (3) Causal relation: NO_CAUSAL  (4) Subevent relation: NO SUB EVENT ", "page_idx": 17, "bbox": [92, 263.672119140625, 460.55712890625, 293.849609375], "page_size": [612.0, 792.0]}
{"layout": 184, "type": "text", "text": "Event Pair 2: < conflict  $>$   and < standoff >\n\n (1) Co reference relation:  NO CO REFERENCE (2) Temporal relation: NO TEMPORAL \n\n (3) Causal relation: NO_CAUSAL  (4) Subevent relation: NO SUB EVENT ", "page_idx": 17, "bbox": [92, 301.8599853515625, 474.82244873046875, 332.0374755859375], "page_size": [612.0, 792.0]}
{"layout": 185, "type": "image", "page_idx": 17, "img_path": "layout_images/2310.09158v1_23.jpg", "bbox": [59, 354, 84, 389], "page_size": [612.0, 792.0], "ocr_text": "of\n\n.~\n\n=)\n", "vlm_text": "The image is an icon depicting a generic person or user symbol, often used to represent a user profile."}
{"layout": 186, "type": "image", "page_idx": 17, "img_path": "layout_images/2310.09158v1_24.jpg", "bbox": [93, 359, 499, 393], "page_size": [612.0, 792.0], "ocr_text": "Letâ€™s think step by step. If < standoff > happens BEFORE < conflict >, then they\nwonâ€™t have coreference and subevent relations, and < conflict > will have NO_TEMPORAL\nrelation with < standoff > . fe)\n\n", "vlm_text": "The image contains text that reads: \"Let's think step by step. If <standoff> happens BEFORE <conflict>, then they won't have coreference and subevent relations, and <conflict> will have NO_TEMPORAL relation with <standoff>.\" The text is written in a blue italic font. There is also a small icon in the image that resembles a green circular arrow surrounding the head of a green character that resembles Pac-Man, placed near the bottom center of the image. "}
{"layout": 187, "type": "image", "page_idx": 17, "img_path": "layout_images/2310.09158v1_25.jpg", "bbox": [90, 407, 546, 500], "page_size": [612.0, 792.0], "ocr_text": "Yes, you are correct. Based on the information, here are the answers:\n\nEvent Pair 1: < standoff > and < conflict > ChatGP\n(1) Coreference relation : NO_COREFERENCE x (2) Temporal relation: BEFORE x\n(3) Causal relation: NO_CAUSAL (4) Subevent relation: NO_SUBEVENT\n\nEvent Pair 2: < conflict > and < standoff >\n(1) Coreference relation: NO_COREFERENCE xâ€œ (2) Temporal relation: NO_TEMPORAL\n(3) Causal relation: NO_CAUSAL (4) Subevent relation: NO_SUBEVENT\n", "vlm_text": "The image appears to be a segment from a text-based interaction, possibly with an AI model like ChatGPT, providing information about various relationships between two pairs of events. The text indicates:\n\n- For Event Pair 1, `< standoff > and < conflict >`:\n  - Coreference relation: NO_COREFERENCE (highlighted in red and marked with 'X')\n  - Temporal relation: BEFORE (highlighted in red and marked with 'X')\n  - Causal relation: NO_CAUSAL\n  - Subevent relation: NO_SUBEVENT\n\n- For Event Pair 2, `< conflict > and < standoff >`:\n  - Coreference relation: NO_COREFERENCE (highlighted in red and marked with 'X')\n  - Temporal relation: NO_TEMPORAL\n  - Causal relation: NO_CAUSAL\n  - Subevent relation: NO_SUBEVENT\n\nThe red highlights with 'X' seem to indicate where a user might have made an incorrect assumption or selection. The ChatGPT logo is present on the right side of the image, suggesting that the feedback or evaluation comes from a ChatGPT interaction."}
{"layout": 188, "type": "text", "text": "Proof Writer w. relevant logic constraints ( ", "text_level": 1, "page_idx": 18, "bbox": [85, 92, 313, 106], "page_size": [612.0, 792.0]}
{"layout": 189, "type": "image", "page_idx": 18, "img_path": "layout_images/2310.09158v1_26.jpg", "bbox": [60, 113, 84, 148], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image is a simple black-and-white icon of a person with the word \"User\" written below it."}
{"layout": 190, "type": "text", "text": "Task Description: ", "text_level": 1, "page_idx": 18, "bbox": [92, 114, 173, 122.75], "page_size": [612.0, 792.0]}
{"layout": 191, "type": "text", "text": "Given a problem statement as contexts, the task is to answer a logical reasoning  question.  ", "page_idx": 18, "bbox": [92, 122.8978271484375, 476, 143.52838134765625], "page_size": [612.0, 792.0]}
{"layout": 192, "type": "text", "text": "Text:  ", "text_level": 1, "page_idx": 18, "bbox": [92, 152, 116, 160.75], "page_size": [612.0, 792.0]}
{"layout": 193, "type": "text", "text": "Bob is smart. Bob is young. Charlie is cold. Charlie is round. Charlie is white.  Charlie is young. Gary is smart. Gary is white. Harry is rough. Harry is smart. Harry  is white. All red people are round. Round, smart people are cold. If someone is cold  and young then they are white. Red people are round. All round, young people are red.  If someone is smart then they are rough. Rough, young people are red. If Bob is round  then Bob is smart. If Charlie is white and Charlie is rough then Charlie is young. ", "page_idx": 18, "bbox": [92, 161.085693359375, 503, 219.904052734375], "page_size": [612.0, 792.0]}
{"layout": 194, "type": "text", "text": "Question:  ", "text_level": 1, "page_idx": 18, "bbox": [92, 228, 134, 237.75], "page_size": [612.0, 792.0]}
{"layout": 195, "type": "text", "text": "Based on the above information, is the following statement true, false, or unknown?  Harry is not round. ", "page_idx": 18, "bbox": [92, 237.46136474609375, 493.90606689453125, 257.9183349609375], "page_size": [612.0, 792.0]}
{"layout": 196, "type": "text", "text": "Based on the given information, the statement \"Harry is not round\" is  true . The information states that all red people are round, and Harry is described as  white, smart, and rough,  but not round . Therefore, Harry does not fit the criteria of  being round, and the statement is true. ", "page_idx": 18, "bbox": [92, 278.2529296875, 503, 317.97735595703125], "page_size": [612.0, 792.0]}
{"layout": 197, "type": "image", "page_idx": 18, "img_path": "layout_images/2310.09158v1_27.jpg", "bbox": [60, 330, 84, 365], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image is a simple black and white icon of a person or user figure, which is typically used to represent a user profile or account. Below the figure, the word \"User\" is written."}
{"layout": 198, "type": "text", "text": "Letâ€™s think step by step. From the text, we could first get relevant facts:\n\n â€¢ Harry is smart.\n\n â€¢ Harry is white. ", "page_idx": 18, "bbox": [92, 337.27044677734375, 451.0107421875, 367.44793701171875], "page_size": [612.0, 792.0]}
{"layout": 199, "type": "text", "text": "And  the relevant rule :\n\n â€¢ All red people are round.\n\n â€¢ Round, smart people are cold.\n\n â€¢ All round, young people are red. ", "page_idx": 18, "bbox": [92, 375.45831298828125, 259.365966796875, 415.01165771484375], "page_size": [612.0, 792.0]}
{"layout": 200, "type": "text", "text": "Given these statements, we can conclude the following: Harry is smart and white, but there is no direct statement about his being red. ", "page_idx": 18, "bbox": [92, 423.193115234375, 470.08917236328125, 443.8236083984375], "page_size": [612.0, 792.0]}
{"layout": 201, "type": "image", "page_idx": 18, "img_path": "layout_images/2310.09158v1_28.jpg", "bbox": [90, 462, 497, 504], "page_size": [612.0, 792.0], "ocr_text": "Apologies for the confusion. You are correct, there is no direct statement in the\ngiven information that explicitly mentions whether Harry is round or not. Therefore,\n\nthe statement \"Harry is not round\" is unknown based solely on the provided\ninformation. av a\n\n", "vlm_text": "The image contains text that addresses a misunderstanding. The statement mentions that there's no direct information provided to determine whether \"Harry is round\" or not. It concludes that the assertion \"Harry is not round\" is unknown based on the available information. Certain words or phrases like \"there is no direct statement in the given information that explicitly mentions whether Harry is round or not\" and \"unknown\" are highlighted in green, and there is a green check mark at the bottom."}
{"layout": 202, "type": "image", "page_idx": 19, "img_path": "layout_images/2310.09158v1_29.jpg", "bbox": [59, 109, 84, 144], "page_size": [612.0, 792.0], "ocr_text": "of\n\nca\n\n=)\n", "vlm_text": "The image shows a silhouette of a person with the text \"User\" below it. It's a simple icon-style representation."}
{"layout": 203, "type": "image", "page_idx": 19, "img_path": "layout_images/2310.09158v1_30.jpg", "bbox": [319.25, 83, 351, 106], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows a green light bulb with a diagonal line across it, inside a blue circle. There are also black parentheses on either side of the circle. This symbol might represent \"no ideas,\" \"idea off,\" or a similar concept."}
{"layout": 204, "type": "text", "text": "Proof Writer w. irrelevant logic constraints ( ", "text_level": 1, "page_idx": 19, "bbox": [85, 87, 318.75, 103], "page_size": [612.0, 792.0]}
{"layout": 205, "type": "text", "text": "Task Description: ", "text_level": 1, "page_idx": 19, "bbox": [91, 110, 174, 118.75], "page_size": [612.0, 792.0]}
{"layout": 206, "type": "text", "text": "Given a problem statement as contexts, the task is to answer a logical reasoning  question.  ", "page_idx": 19, "bbox": [92, 118.9058837890625, 476, 139.53643798828125], "page_size": [612.0, 792.0]}
{"layout": 207, "type": "text", "text": "Text:  ", "text_level": 1, "page_idx": 19, "bbox": [92, 148, 116, 156.75], "page_size": [612.0, 792.0]}
{"layout": 208, "type": "text", "text": "Bob is smart. Bob is young. Charlie is cold. Charlie is round. Charlie is white.  Charlie is young. Gary is smart. Gary is white. Harry is rough. Harry is smart. Harry  is white. All red people are round. Round, smart people are cold. If someone is cold  and young then they are white. Red people are round. All round, young people are red.  If someone is smart then they are rough. Rough, young people are red. If Bob is round  then Bob is smart. If Charlie is white and Charlie is rough then Charlie is young. ", "page_idx": 19, "bbox": [92, 157.09375, 503, 215.912109375], "page_size": [612.0, 792.0]}
{"layout": 209, "type": "text", "text": "Question:  ", "text_level": 1, "page_idx": 19, "bbox": [92, 224, 134, 234], "page_size": [612.0, 792.0]}
{"layout": 210, "type": "text", "text": "Based on the above information, is the following statement true, false, or unknown?  Harry is not round. ", "page_idx": 19, "bbox": [92, 233.46942138671875, 493.90606689453125, 253.9263916015625], "page_size": [612.0, 792.0]}
{"layout": 211, "type": "text", "text": "Based on the given information, the statement \"Harry is not round\" is  true . The information states that all red people are round, and Harry is described as  white, smart, and rough,  but not round . Therefore, Harry does not fit the criteria of  being round, and the statement is true. ", "page_idx": 19, "bbox": [92, 274.608154296875, 503, 314.3326416015625], "page_size": [612.0, 792.0]}
{"layout": 212, "type": "image", "page_idx": 19, "img_path": "layout_images/2310.09158v1_31.jpg", "bbox": [60, 329, 84, 364], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows a simple black silhouette of a person, often used as a generic icon or avatar, with the word \"User\" written below it."}
{"layout": 213, "type": "text", "text": "Letâ€™s think step by step. From the text, we could get some facts:\n\n â€¢ Harry is smart.\n\n â€¢ Harry is white.  ", "page_idx": 19, "bbox": [92, 333.27850341796875, 403.2757873535156, 363.45599365234375], "page_size": [612.0, 792.0]}
{"layout": 214, "type": "text", "text": "And some rules:\n\n â€¢ If someone is cold and young then they are white. \n\n â€¢ If someone is smart then they are rough. \n\n â€¢ Rough, young people are red. ", "page_idx": 19, "bbox": [92, 371.46636962890625, 345.2352294921875, 411.0197448730469], "page_size": [612.0, 792.0]}
{"layout": 215, "type": "image", "page_idx": 19, "img_path": "layout_images/2310.09158v1_32.jpg", "bbox": [344, 384, 367, 407], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows a blue circle with a green light bulb inside. There is a diagonal line crossing the bulb, likely indicating a prohibition or restriction, possibly related to energy conservation or \"no light bulb\" usage."}
{"layout": 216, "type": "text", "text": "Given these statements, please output your answers. ", "page_idx": 19, "bbox": [92.98324584960938, 419.201171875, 336.4466247558594, 430.28472900390625], "page_size": [612.0, 792.0]}
{"layout": 217, "type": "text", "text": "Figure 13: Proof Writer w. irrelevant logic constraints ", "text_level": 1, "page_idx": 19, "bbox": [197, 505, 415, 516], "page_size": [612.0, 792.0]}
{"layout": 218, "type": "text", "text": "Multi-hop Reasoning ", "text_level": 1, "page_idx": 19, "bbox": [198, 530, 301, 541], "page_size": [612.0, 792.0]}
{"layout": 219, "type": "image", "page_idx": 19, "img_path": "layout_images/2310.09158v1_33.jpg", "bbox": [196, 542, 417, 678], "page_size": [612.0, 792.0], "ocr_text": "Task Description:\nGiven a problem statement as contexts, your task\nis to induce the relations between two events.\n\nContext:\n\nGiven that event A happens BEFORE event B, event B\nand event C happen SIMULTANEOUSlLy, event C happens\nOVERLAP with event D.\n\nQuestion:\nBased on the above information, what are the four\ntypes of relations between event A and D?\n\nAnswers:\n\n", "vlm_text": "The image contains a problem statement about determining the relations between two events. Here's a summary:\n\n- **Task Description:** Induce the relations between two events based on the context.\n- **Context:** \n  - Event A happens before event B.\n  - Events B and C happen simultaneously.\n  - Event C overlaps with event D.\n- **Question:** Determine the four types of relations between event A and event D.\n\nThe image ends with \"Answers:\" but no answers are provided."}
{"layout": 220, "type": "text", "text": "H.2 In cop orating Logical Constraints ", "text_level": 1, "page_idx": 20, "bbox": [52, 82, 246, 95], "page_size": [612.0, 792.0]}
{"layout": 221, "type": "text", "text": "The highlighted parts represent the content generated by LLMs. We omit the demonstration here for clarity. ", "page_idx": 20, "bbox": [108.88800048828125, 95.08375549316406, 504.2256164550781, 107.47538757324219], "page_size": [612.0, 792.0]}
{"layout": 222, "type": "text", "text": "Vanilla ICL ", "text_level": 1, "page_idx": 20, "bbox": [75, 114, 129, 125], "page_size": [612.0, 792.0]}
{"layout": 223, "type": "image", "page_idx": 20, "img_path": "layout_images/2310.09158v1_34.jpg", "bbox": [74, 130, 284, 297], "page_size": [612.0, 792.0], "ocr_text": "Task Description:\n\nThere is a piece of text with two events marked by\n< and > symbols, and your task is to identify four\ntypes of relations between them\n\nText:\n\nBefore her death , Todd posted a video on YouTube\nin which she used a series of flash cards to <\ntell > her experience of being blackmailed into\nexposing her breasts via webcam , and of being <\nbullied > and physically assaulted .\n\nEvent Pairs:\n< tell > and < bullied >\n< bullied > and < tell >\n\nAnswers:\nNO_COREFERENCE, NO_TEMPORAL, NO_CAUSAL, NO_SUBEVENT.\nNO_COREFERENCE, NO_TEMPORAL, PRECONDITION, NO_SUBEVENT.\n\n", "vlm_text": "The image contains a text analysis task relating to event pair relationships within a given passage. The task description outlines the objective: identify four types of relations (co-reference, temporal, causal, and subevent) between events marked by angle brackets \"<>\" in the text. The specific text to be analyzed describes a video posted by Todd using flash cards to recount experiences of blackmail and bullying. The event pairs given for analysis are \"<tell>\" and \"<bullied>\", in both \"tell, bullied\" and \"bullied, tell\" sequences. The corresponding answers indicate there are no relations such as co-reference, temporal, causal, or subevent for the \"<tell>\" and \"<bullied>\" pair, while for the \"<bullied>\" and \"<tell>\" pair, there is a precondition relationship; all other relations are absent. The task description is partially obscured with \"(... more context here ... )\" placeholder text."}
{"layout": 224, "type": "text", "text": "Vanilla ICL plus CoT ", "text_level": 1, "page_idx": 20, "bbox": [74, 319, 172, 330], "page_size": [612.0, 792.0]}
{"layout": 225, "type": "text", "text": "Task Description: There is a piece of text with two events marked by   $<$   and  $>$   symbols, and your task is to identify four types of relations between them  (Â·Â·Â· more context  here Â·Â·Â· ) ", "page_idx": 20, "bbox": [74, 335.6991882324219, 280.64111328125, 377.1091613769531], "page_size": [612.0, 792.0]}
{"layout": 226, "type": "text", "text": "Before her death , Todd posted a video on YouTube  in which she used a series of flash cards to  $<$    tell  $>$   her experience of being blackmailed into  exposing her breasts via webcam , and of being  $<$    bullied  $>$   and physically assaulted . ", "page_idx": 20, "bbox": [74, 391.87890625, 276, 433.3277893066406], "page_size": [612.0, 792.0]}
{"layout": 227, "type": "image", "page_idx": 20, "img_path": "layout_images/2310.09158v1_35.jpg", "bbox": [75, 473, 276, 524], "page_size": [612.0, 792.0], "ocr_text": "Reasoning:\n\n< bullied > happens before < tell >, and being <\nbullied > leads her to < tell >.\n\nAnswers:\n\nNO_COREFERENCE, BEFORE, NO_CAUSAL, NO_SUBEVENT.\nNO_COREFERENCE, BEFORE, PRECONDITION, NO_SUBEVENT.\n", "vlm_text": "The image contains a textual excerpt displaying reasoning and answers related to an analysis of events. The \"Reasoning\" section discusses the sequence and causality of the events \"< bullied >\" and \"< tell >,\" noting that being bullied leads to telling, and the bullying occurs before telling. The \"Answers\" section lists possible relationships: \n- \"NO_COREFERENCE\" indicates no coreference relationship between entities.\n- \"BEFORE\" confirms the chronological order.\n- The two other options, \"NO_CAUSAL\" and \"PRECONDITION,\" suggest differing views on the causal relationship.\n- \"NO_SUBEVENT\" suggests no subevent relationship exists. \n\nThus, the text seems to be examining different logical relationships or event reasoning in a structured manner."}
{"layout": 228, "type": "text", "text": "CoT w. self-generated logical constraints ", "text_level": 1, "page_idx": 20, "bbox": [71, 533, 259, 544], "page_size": [612.0, 792.0]}
{"layout": 229, "type": "image", "page_idx": 20, "img_path": "layout_images/2310.09158v1_36.jpg", "bbox": [71, 557, 283, 779], "page_size": [612.0, 792.0], "ocr_text": "There is a piece of text with two events marked by\n< and > symbols, and your task is to identify four\ntypes of relations between them\n\nText:\n\nBefore her death , Todd posted a video on YouTube\nin which she used a series of flash cards to <\ntell > her experience of being blackmailed into\nexposing her breasts via webcam , and of being <\nbullied > and physically assaulted .\n\nEvent Pairs:\n< tell > and < bullied >\n< bullied > and < tell >\n\nFrom the text, we could first get:\n\n< bullied > happens before < tell >, and being <\nbullied > leads her to < tell >\n\nDue to the logical constraint: If event A is event\nBâ€™s PRECONDITION, then event A happens BEFORE or\nOVERLAP event B, and they wonâ€™t have coreference\nand subevent relations, and event B has\nNO_TEMPORAL relation with event A.\n\nWe can deduce that the answers are:\nNO_COREFERENCE, NO_TEMPORAL, NO_CAUSAL, NO_SUBEVENT.\nNO_COREFERENCE, BEFORE, PRECONDITION, NO_SUBEVENT.\n\n", "vlm_text": "The image contains a text analysis task related to identifying the relationships between two marked events, \"<tell>\" and \"<bullied>\", in a given piece of text. The text describes an event where Todd posted a video before her death, telling about being blackmailed and bullied. The relationships considered are coreference, temporal, causal, and subevent. \n\nThe analysis in the image suggests that \"<bullied>\" happens before \"<tell>\", and that \"<bullied>\" leads her to \"<tell>\". It applies a logical constraint stating that if event A (bullied) is event Bâ€™s (tell) precondition, then event A happens before or overlaps event B, and there won't be coreference and subevent relations. It concludes with the relationship answers: \n\n- For \"<tell>\" and \"<bullied>\": NO_COREFERENCE, NO_TEMPORAL, NO_CAUSAL, NO_SUBEVENT\n- For \"<bullied>\" and \"<tell>\": NO_COREFERENCE, BEFORE, PRECONDITION, NO_SUBEVENT"}
{"layout": 230, "type": "text", "text": "w. all logical constraints ", "text_level": 1, "page_idx": 20, "bbox": [333, 138, 443, 149], "page_size": [612.0, 792.0]}
{"layout": 231, "type": "image", "page_idx": 20, "img_path": "layout_images/2310.09158v1_37.jpg", "bbox": [332, 154, 540, 412.75], "page_size": [612.0, 792.0], "ocr_text": "Task Description:\n\nThere is a piece of text with two events marked by\n< and > symbols, and your task is to identify four\ntypes of relations between them\n\nLogical constraints:\n\nThere are some rules among the relations, you can\nselect some of them to reason or check your\nanswers:\n\n(1) If two events are COREFERENCE, then they wonâ€™t\nhave temporal, causal, and subevent relations, and\nCOREFERENCE relation is bidirectional.\n\n(11) If event B is a SUBEVENT of event A,\nthen they wonâ€™t have coreference and causal\nrelations, and event Aâ€™s time should CONTAINS\nevent Bâ€™s time, and event B has NO_TEMPORAL\nrelation with event A.\n\nText:\n\nBefore her death , Todd posted a video on YouTube\nin which she used a series of flash cards to <\ntell > her experience of being blackmailed into\nexposing her breasts via webcam , and of being <\nbullied > and physically assaulted .\n\nEvent Pairs:\n< tell > and < bullied >\n< bullied > and < tell >\n\nAnswers:\n", "vlm_text": "The image contains a task description about identifying types of relations between two events in a text marked by `<` and `>` symbols. It lists logical constraints to guide the identification of these relations, such as COREFFERENCE and SUBEVENT. The text for analysis describes a person using flashcards in a video to tell about being blackmailed and bullied. Two event pairs for analysis are given: `< tell > and < bullied >` and `< bullied > and < tell >`. The section for answers is blank."}
{"layout": 232, "type": "text", "text": "NO CO REFERENCE, NO TEMPORAL, NO_CAUSAL, NO SUB EVENT. NO CO REFERENCE, BEFORE, PRECONDITION, NO SUB EVENT. ", "page_idx": 20, "bbox": [333, 413.7467346191406, 524, 428.4950256347656], "page_size": [612.0, 792.0]}
{"layout": 233, "type": "text", "text": "w. retrieved logical constraints ", "text_level": 1, "page_idx": 20, "bbox": [333, 460, 473, 470], "page_size": [612.0, 792.0]}
{"layout": 234, "type": "text", "text": "There is a piece of text with two events marked by   $<$   and  $>$   symbols, and your task is to identify four types of relations between them  (Â·Â·Â· more context  here Â·Â·Â· ) ", "page_idx": 20, "bbox": [333, 483.72705078125, 539, 517.1446533203125], "page_size": [612.0, 792.0]}
{"layout": 235, "type": "text", "text": "select some of them to reason or check your  answers: (1) If event A is event Bâ€™s PRECONDITION, then event A happens BEFORE or OVERLAP event B,  and they wonâ€™t have co reference and subevent  relations, and event B has NO TEMPORAL relation  with event A.  (2) If event A and event B do not have a temporal  relation, then they wonâ€™t have causal and subevent  relations. ", "page_idx": 20, "bbox": [333, 539.94580078125, 539, 621.55078125], "page_size": [612.0, 792.0]}
{"layout": 236, "type": "text", "text": "Before her death , Todd posted a video on YouTube  in which she used a series of flash cards to   $<$  tell  $>$   her experience of being blackmailed into  exposing her breasts via webcam , and of being   $<$    bullied  $>$   and physically assaulted . ", "page_idx": 20, "bbox": [333, 636.3206787109375, 535, 677.7694091796875], "page_size": [612.0, 792.0]}
{"layout": 237, "type": "text", "text": "Event Pairs:\n\n  $<$   tell  $>$   and  $<$   bullied  $>\n\n$   $<$   bullied  $>$   and  $<$   tell  $>$  ", "page_idx": 20, "bbox": [333, 684.5469970703125, 432, 709.8944091796875], "page_size": [612.0, 792.0]}
{"layout": 238, "type": "text", "text": "", "text_level": 1, "page_idx": 20, "bbox": [334, 718, 364, 723.75], "page_size": [612.0, 792.0]}
{"layout": 239, "type": "text", "text": "NO CO REFERENCE, NO TEMPORAL, NO_CAUSAL, NO SUB EVENT. NO CO REFERENCE, BEFORE, PRECONDITION, NO SUB EVENT. ", "page_idx": 20, "bbox": [333, 726.819091796875, 524, 741.5673828125], "page_size": [612.0, 792.0]}
{"layout": 240, "type": "image", "page_idx": 21, "img_path": "layout_images/2310.09158v1_38.jpg", "img_caption": "Figure 15: Multi-turn conversation with ChatGPT. We retrieve relevant logical constraints and provide them to ChatGPT. ", "bbox": [59, 79, 551, 585], "page_size": [612.0, 792.0], "ocr_text": "apo\n\nfi)\n5\n\nTask Description:\n\nThere is a piece of text with two events marked by <\nand > symbols, and your task is to identify four\ntypes of relations between them\n\nText:\n\nThis trade gradually < declined > during the early\n19th century as demand for furs in Europe <\ndiminished > .\n\nEvent Pairs:\n< diminished > and < declined >\n< declined > and < diminished >\n\nAnswers:\n\nNO_COREFERENCE, NO_TEMPORAL,><CAUSE, SUBEVENT. ><\nNO_COREFERENCE, NO_TEMPORAL, NO_CAUSAL, NO_SUBEVENT.\n\npe\n\nYour answers are logically inconsistent.\n\nThere are some rules among the relations, you can\nselect some of them to reason or check your answers:\n(1) If event A CAUSEs event B, then event A happens\nBEFORE or OVERLAP event B, and they wonâ€™t have\ncoreference and subevent relations ---\n\n(2) If event A and event B do not have a temporal\nrelation, then they wonâ€™t have causal and subevent\nrelations ---\n\n(3) If event B is a SUBEVENT of event A, then they\nwonâ€™t have coreference and causal relations, and\nevent Aâ€™s time should CONTAINS event Bâ€™s time ---\n\nNO_COREFERENCE, CONTAINS, >XCAUSE, NO_SUBEVENT.\nNO_COREFERENCE, NO_TEMPORAL, NO_CAUSAL, NO_SUBEVENT.\n\napo\n\nfi)\n5\n\nYour answers are logically inconsistent.\n\nThere are some rules among the relations, you can\nselect some of them to reason or check your answers:\n(1) If event A CAUSEs event B, then event A happens\nBEFORE or OVERLAP event B, and they wonâ€™t have\ncoreference and subevent relations ---\n\n(2) If event Aâ€™s time CONTAINS event Bâ€™s time, then\nthey wonâ€™t have coreference and causal relations ---\n\nNO_COREFERENCE, BEFORE,@/CAUSE, NO_SUBEVENT.\nNO_COREFERENCE, NO_TEMPORAL, NO_CAUSAL, NO_SUBEVENT.\n\n| Iteration 0\n\n@ -\n\nChatGPT\n\nIteration 1\n\nChatGPT\n\nIteration 2\n\nChatGPT\n", "vlm_text": "The image depicts a multi-turn conversation with ChatGPT, focusing on identifying relationships between events in a text. It includes several iterations where the user provides logical rules to refine ChatGPT's answers. Here's a breakdown of each iteration:\n\n- **Iteration 0:**\n  - **Task Description:** Identify the relations between marked events in text.\n  - **Text:** Discusses the decline in trade and demand for furs in Europe.\n  - **Event Pairs Identified:** \"<diminished> and <declined>\", \"<declined> and <diminished>\"\n  - **ChatGPT's Initial Answer:** NO_COREFERENCE, NO_TEMPORAL, NO_CAUSAL, SUBEVENT (incorrect for first pair).\n\n- **Iteration 1:**\n  - **User Comments:** Logic rules are introduced for checking answers.\n  - **Rules Provided:** Relationships between causation, coreference, temporal relations, and subevents.\n  - **ChatGPT's Response:** Adjusts to NO_COREFERENCE, CONTAINS (incorrect), NO_CAUSAL, NO_SUBEVENT.\n\n- **Iteration 2:**\n  - **User Feedback:** Continues to highlight logical inconsistencies.\n  - **Refined Rules:** Further specified logic for temporal containment and causality.\n  - **ChatGPT's Final Answer:** NO_COREFERENCE, BEFORE (incorrect), NO_CAUSAL, NO_SUBEVENT.\n\nThe dialogue highlights how retrieving logical constraints assists in refining answers through iteration."}
{"layout": 241, "type": "text", "text": "H.3 Iterative Retrievals ", "text_level": 1, "page_idx": 21, "bbox": [53, 601, 177, 613], "page_size": [612.0, 792.0]}
{"layout": 242, "type": "text", "text": "In this section, we present a prompt example used in Section 4.3.2. As shown in Figure 15, with iterative prompting, ChatGPT finally outputs the correct answers. ", "page_idx": 21, "bbox": [53, 614.0267944335938, 295, 648.3363647460938], "page_size": [612.0, 792.0]}
{"layout": 243, "type": "text", "text": "H.4 Deductive Reasoning ", "text_level": 1, "page_idx": 22, "bbox": [53, 82, 187, 94], "page_size": [612.0, 792.0]}
{"layout": 244, "type": "text", "text": "The highlighted parts represent the content generated by LLMs. We omit the demonstration here for clarity. ", "page_idx": 22, "bbox": [108.88800048828125, 95.08375549316406, 504.2256164550781, 107.47538757324219], "page_size": [612.0, 792.0]}
{"layout": 245, "type": "text", "text": "Vanilla ICL ", "text_level": 1, "page_idx": 22, "bbox": [61, 115, 118, 125], "page_size": [612.0, 792.0]}
{"layout": 246, "type": "text", "text": "Task Description: ", "text_level": 1, "page_idx": 22, "bbox": [61, 133, 136, 140.75], "page_size": [612.0, 792.0]}
{"layout": 247, "type": "text", "text": "Given a problem statement as contexts, the task is  to answer a logical reasoning question. For each  question, choose from the following options: A)  True, B) False, or C) Unknown. ", "page_idx": 22, "bbox": [63, 140.77264404296875, 282, 176.54800415039062], "page_size": [612.0, 792.0]}
{"layout": 248, "type": "text", "text": "Context: ", "text_level": 1, "page_idx": 22, "bbox": [62, 184, 97, 191.75], "page_size": [612.0, 792.0]}
{"layout": 249, "type": "text", "text": "The cow likes the lion. The cow visits the tiger. (Â·Â·Â· more context here Â·Â·Â· ) ", "page_idx": 22, "bbox": [63, 192.35992431640625, 273, 210.93954467773438], "page_size": [612.0, 792.0]}
{"layout": 250, "type": "text", "text": "Question:  ", "text_level": 1, "page_idx": 22, "bbox": [62, 218, 100, 226.75], "page_size": [612.0, 792.0]}
{"layout": 251, "type": "text", "text": "Based on the above information, is the following  statement true, false, or unknown? The tiger is  not young. ", "page_idx": 22, "bbox": [63, 226.75146484375, 273, 253.92892456054688], "page_size": [612.0, 792.0]}
{"layout": 252, "type": "text", "text": "Vanilla ICL plus CoT ", "text_level": 1, "page_idx": 22, "bbox": [61, 303, 166, 315], "page_size": [612.0, 792.0]}
{"layout": 253, "type": "image", "page_idx": 22, "img_path": "layout_images/2310.09158v1_39.jpg", "bbox": [61, 323, 282, 533], "page_size": [612.0, 792.0], "ocr_text": "Task Description:\n\nGiven a problem statement as contexts, the task is\nto answer a logical reasoning question. For each\nquestion, choose from the following options: A)\nTrue, B) False, or C) Unknown.\n\nContext:\nThe cow likes the lion. The cow visits the tiger.\n\nQuestion:\n\nBased on the above information, is the following\nstatement true, false, or unknown? The tiger is\nnot young.\n\nReasoning:\n\nThe tiger likes the cow. The tiger likes the\nsquirrel. If something likes the squirrel and it\nlikes the cow, then it visits the tiger. So the\ntiger visits the tiger. If something visits the\ntiger then it is nice. So the tiger is nice. If\nsomething is nice and it sees the tiger then it is\nyoung. So the tiger is young.\n\nThe correct option is: B.\n\n", "vlm_text": "The image contains a logical reasoning task description and an example. The task involves determining if a given statement is true, false, or unknown, based on provided context and reasoning.\n\nThe task description outlines that for each question, one must choose from the options: A) True, B) False, or C) Unknown.\n\nThe given context in the example includes: \"The cow likes the lion. The cow visits the tiger.\"\n\nThe specific question asks if the statement \"The tiger is not young\" is true, false, or unknown.\n\nIn the reasoning section, it explains that the tiger likes both the cow and the squirrel. It further states a conditional rule that something liking the squirrel and liking the cow would visit the tiger, therefore the tiger is visited by the cow. \n\nThe reasoning concludes with the statement: \"The tiger is nice. If something is nice and it sees the tiger then it is young. So the tiger is young.\"\n\nFinally, based on the reasoning provided, the correct option is stated as B, indicating that the statement \"The tiger is not young\" is false."}
{"layout": 254, "type": "text", "text": "CoT w. self-generated logical constraints ", "text_level": 1, "page_idx": 22, "bbox": [331, 114, 534, 126], "page_size": [612.0, 792.0]}
{"layout": 255, "type": "text", "text": "Task Description: ", "text_level": 1, "page_idx": 22, "bbox": [333, 132, 407, 140.75], "page_size": [612.0, 792.0]}
{"layout": 256, "type": "text", "text": "Given a problem statement as contexts, the task is  to answer a logical reasoning question. For each  question, choose from the following options: A)  True, B) False, or C) Unknown. ", "page_idx": 22, "bbox": [334, 140.77264404296875, 551, 176.54800415039062], "page_size": [612.0, 792.0]}
{"layout": 257, "type": "text", "text": "Context: ", "text_level": 1, "page_idx": 22, "bbox": [332, 183, 368, 192], "page_size": [612.0, 792.0]}
{"layout": 258, "type": "text", "text": "The cow likes the lion. The cow visits the tiger.  (Â·Â·Â· more context here Â·Â·Â· ) ", "page_idx": 22, "bbox": [334, 192.35992431640625, 551, 210.93954467773438], "page_size": [612.0, 792.0]}
{"layout": 259, "type": "text", "text": "Question:  ", "text_level": 1, "page_idx": 22, "bbox": [333, 218, 372, 226.75], "page_size": [612.0, 792.0]}
{"layout": 260, "type": "text", "text": "Based on the above information, is the following  statement true, false, or unknown? The tiger is  not young. ", "page_idx": 22, "bbox": [334, 226.75146484375, 544, 253.92892456054688], "page_size": [612.0, 792.0]}
{"layout": 261, "type": "text", "text": "The tiger likes the cow. The tiger likes the  squirrel. Due to the mentioned rules: if something likes the  squirrel and it likes the cow, then it visits the  tiger. We can deduce the facts: the tiger visits the  tiger. Due to the mentioned rules: if something visits  the tiger then it is nice. We can deduce the facts: the tiger is nice. Due to the mentioned rules: if something is nice  and it sees the tiger then it is young. We can deduce the facts: the tiger is young. Therefore, the correct option is: B. ", "page_idx": 22, "bbox": [334, 269.7408447265625, 551, 391.4949645996094], "page_size": [612.0, 792.0]}
