{"layout": 0, "type": "text", "text": "End-to-End Object Detection with Transformers ", "text_level": 1, "page_idx": 0, "bbox": [30, 28, 381, 48], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 1, "type": "text", "text": "Nicolas Carion  $\\star$  , Francisco Massa  $\\star$  , Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko ", "page_idx": 0, "bbox": [52.16899871826172, 71.6760025024414, 361.6997375488281, 93.59361267089844], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 2, "type": "text", "text": "Facebook AI ", "page_idx": 0, "bbox": [181.39500427246094, 105.25817108154297, 232.45864868164062, 114.22457122802734], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 3, "type": "text", "text": "Abstract.  We present a new method that views object detection as a direct set prediction problem. Our approach streamlines the detection pipeline, eﬀectively removing the need for many hand-designed compo- nents like a non-maximum suppression procedure or anchor generation that explicitly encode our prior knowledge about the task. The main ingredients of the new framework, called DEtection TRansformer or DETR, are a set-based global loss that forces unique predictions via bi- partite matching, and a transformer encoder-decoder architecture. Given a ﬁxed small set of learned object queries, DETR reasons about the re- lations of the objects and the global image context to directly output the ﬁnal set of predictions in parallel. The new model is conceptually simple and does not require a specialized library, unlike many other modern detectors. DETR demonstrates accuracy and run-time perfor- mance on par with the well-established and highly-optimized Faster R- CNN baseline on the challenging COCO object detection dataset. More- over, DETR can be easily generalized to produce panoptic segmentation in a uniﬁed manner. We show that it sign i cant ly outperforms com- petitive baselines. Training code and pretrained models are available at https://github.com/facebook research/detr . ", "page_idx": 0, "bbox": [62, 150.35919189453125, 351, 356.5877685546875], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 4, "type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0, "bbox": [34, 378, 129, 393], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 5, "type": "text", "text": "The goal of object detection is to predict a set of bounding boxes and category labels for each object of interest. Modern detectors address this set prediction task in an indirect way, by deﬁning surrogate regression and class i cation prob- lems on a large set of proposals [ 37 , 5 ], anchors [ 23 ], or window centers [ 53 , 46 ]. Their performances are sign i cant ly inﬂuenced by post processing steps to col- lapse near-duplicate predictions, by the design of the anchor sets and by the heuristics that assign target boxes to anchors [ 52 ]. To simplify these pipelines, we propose a direct set prediction approach to bypass the surrogate tasks. This end-to-end philosophy has led to signiﬁcant advances in complex structured pre- diction tasks such as machine translation or speech recognition, but not yet in object detection: previous attempts [ 43 , 16 , 4 , 39 ] either add other forms of prior knowledge, or have not proven to be competitive with strong baselines on chal- lenging benchmarks. This paper aims to bridge this gap. ", "page_idx": 0, "bbox": [34, 409.44580078125, 379, 562.8704223632812], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 6, "type": "image", "page_idx": 1, "img_path": "layout_images/2005.12872v3_0.jpg", "img_caption": "Fig. 1: DETR directly predicts (in parallel) the ﬁnal set of detections by combining a common CNN with a transformer architecture. During training, bipartite matching uniquely assigns predictions with ground truth boxes. Prediction with no match should yield a “ no object ”   $\\left(\\emptyset\\right)$   class prediction. ", "bbox": [32, 36, 381, 152], "page_size": [413.8580017089844, 615.1179809570312], "ocr_text": "“no object (0) no object (0)\n\ntransformer\n>| — encoder-\ndecoder\n\nset of image features set of box predictions bipartite matching loss\n", "vlm_text": "This image illustrates the concept of DETR (DEtection TRansformer), which combines a Convolutional Neural Network (CNN) with a transformer architecture to predict object detections. \n\n- The process starts with an input image that is processed by a CNN to extract image features.\n- These features are then passed to a transformer encoder-decoder.\n- The transformer outputs a set of box predictions, including classifications for detected objects or \"no object\" predictions represented by empty set symbols ($\\emptyset$).\n- During training, bipartite matching is used to uniquely assign predictions to ground truth boxes, which helps in computing a matching loss to optimize the model. \n\nThis architecture allows DETR to predict object locations and classes directly."}
{"layout": 7, "type": "text", "text": "We streamline the training pipeline by viewing object detection as a direct set prediction problem. We adopt an encoder-decoder architecture based on trans- formers [ 47 ], a popular architecture for sequence prediction. The self-attention mechanisms of transformers, which explicitly model all pairwise interactions be- tween elements in a sequence, make these architectures particularly suitable for speciﬁc constraints of set prediction such as removing duplicate predictions. ", "page_idx": 1, "bbox": [34, 179.31698608398438, 379, 249.05552673339844], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 8, "type": "text", "text": "Our DEtection TRansformer (DETR, see Figure  1 ) predicts all objects at once, and is trained end-to-end with a set loss function which performs bipar- tite matching between predicted and ground-truth objects. DETR simpliﬁes the detection pipeline by dropping multiple hand-designed components that encode prior knowledge, like spatial anchors or non-maximal suppression. Unlike most existing detection methods, DETR doesn’t require any customized layers, and thus can be reproduced easily in any framework that contains standard CNN and transformer classes. . ", "page_idx": 1, "bbox": [34, 251.81393432617188, 379, 345.46246337890625], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 9, "type": "text", "text": "Compared to most previous work on direct set prediction, the main features of DETR are the conjunction of the bipartite matching loss and transformers with (non-auto regressive) parallel decoding [ 29 , 12 , 10 , 8 ]. In contrast, previous work focused on auto regressive decoding with RNNs [ 43 , 41 , 30 , 36 , 42 ]. Our matching loss function uniquely assigns a prediction to a ground truth object, and is invariant to a permutation of predicted objects, so we can emit them in parallel. ", "page_idx": 1, "bbox": [34, 348.2208557128906, 379, 417.95947265625], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 10, "type": "text", "text": "We evaluate DETR on one of the most popular object detection datasets, COCO [ 24 ], against a very competitive Faster R-CNN baseline [ 37 ]. Faster R- CNN has undergone many design iterations and its performance was greatly improved since the original publication. Our experiments show that our new model achieves comparable performances. More precisely, DETR demonstrates sign i cant ly better performance on large objects, a result likely enabled by the non-local computations of the transformer. It obtains, however, lower perfor- mances on small objects. We expect that future work will improve this aspect in the same way the development of FPN [ 22 ] did for Faster R-CNN. ", "page_idx": 1, "bbox": [34, 420.71685791015625, 379, 526.3214721679688], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 11, "type": "text", "text": "Training settings for DETR diﬀer from standard object detectors in mul- tiple ways. The new model requires extra-long training schedule and beneﬁts from auxiliary decoding losses in the transformer. We thoroughly explore what components are crucial for the demonstrated performance. ", "page_idx": 1, "bbox": [34, 529.0789184570312, 379, 550.9974975585938], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 12, "type": "text", "text": "", "page_idx": 2, "bbox": [34, 36.50601577758789, 379, 58.42363357543945], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 13, "type": "text", "text": "The design ethos of DETR easily extend to more complex tasks. In our experiments, we show that a simple segmentation head trained on top of a pre- trained DETR outperfoms competitive baselines on Panoptic Segmentation [ 19 ], a challenging pixel-level recognition task that has recently gained popularity. ", "page_idx": 2, "bbox": [34, 60.77505874633789, 379, 106.60267639160156], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 14, "type": "text", "text": "2 Related work ", "text_level": 1, "page_idx": 2, "bbox": [33, 127, 133, 141], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 15, "type": "text", "text": "Our work build on prior work in several domains: bipartite matching losses for set prediction, encoder-decoder architectures based on the transformer, parallel decoding, and object detection methods. ", "page_idx": 2, "bbox": [34, 156.01010131835938, 379, 189.88368225097656], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 16, "type": "text", "text": "2.1 Set Prediction ", "text_level": 1, "page_idx": 2, "bbox": [34, 210, 132, 222], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 17, "type": "text", "text": "There is no canonical deep learning model to directly predict sets. The basic set prediction task is multilabel class i cation (see e.g., [ 40 , 33 ] for references in the context of computer vision) for which the baseline approach, one-vs-rest, does not apply to problems such as detection where there is an underlying structure between elements (i.e., near-identical boxes). The ﬁrst diﬃculty in these tasks is to avoid near-duplicates. Most current detectors use post processing s such as non-maximal suppression to address this issue, but direct set prediction are post processing-free. They need global inference schemes that model interactions between all predicted elements to avoid redundancy. For constant-size set pre- diction, dense fully connected networks [ 9 ] are suﬃcient but costly. A general approach is to use auto-regressive sequence models such as recurrent neural net- works [ 48 ]. In all cases, the loss function should be invariant by a permutation of the predictions. The usual solution is to design a loss based on the Hungarian al- gorithm [ 20 ], to ﬁnd a bipartite matching between ground-truth and prediction. This enforces permutation-invariance, and guarantees that each target element has a unique match. We follow the bipartite matching loss approach. In contrast to most prior work however, we step away from auto regressive models and use transformers with parallel decoding, which we describe below. ", "page_idx": 2, "bbox": [34, 233.3140869140625, 379, 446.51361083984375], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 18, "type": "text", "text": "2.2 Transformers and Parallel Decoding ", "text_level": 1, "page_idx": 2, "bbox": [33, 465, 241, 479], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 19, "type": "text", "text": "Transformers were introduced by Vaswani  et al . [ 47 ] as a new attention-based building block for machine translation. Attention mechanisms [ 2 ] are neural net- work layers that aggregate information from the entire input sequence. Trans- formers introduced self-attention layers, which, similarly to Non-Local Neural Networks [ 49 ], scan through each element of a sequence and update it by ag- gregating information from the whole sequence. One of the main advantages of attention-based models is their global computations and perfect memory, which makes them more suitable than RNNs on long sequences. Transformers are now replacing RNNs in many problems in natural language processing, speech pro- cessing and computer vision [ 8 , 27 , 45 , 34 , 31 ]. ", "page_idx": 2, "bbox": [34, 489.9439697265625, 379, 583.5925903320312], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 20, "type": "text", "text": "", "page_idx": 3, "bbox": [34, 36.50601577758789, 379, 58.42363357543945], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 21, "type": "text", "text": "Transformers were ﬁrst used in auto-regressive models, following early sequence- to-sequence models [ 44 ], generating output tokens one by one. However, the pro- hibitive inference cost (proportional to output length, and hard to batch) lead to the development of parallel sequence generation, in the domains of audio [ 29 ], machine translation [ 12 , 10 ], word representation learning [ 8 ], and more recently speech recognition [ 6 ]. We also combine transformers and parallel decoding for their suitable trade-oﬀbetween computational cost and the ability to perform the global computations required for set prediction. ", "page_idx": 3, "bbox": [34, 60.41702651977539, 389.45172119140625, 154.0655975341797], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 22, "type": "text", "text": "2.3 Object detection ", "text_level": 1, "page_idx": 3, "bbox": [33, 170, 144, 182], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 23, "type": "text", "text": "Most modern object detection methods make predictions relative to some ini- tial guesses. Two-stage detectors [ 37 , 5 ] predict boxes w.r.t. proposals, whereas single-stage methods make predictions w.r.t. anchors [ 23 ] or a grid of possible object centers [ 53 , 46 ]. Recent work [ 52 ] demonstrate that the ﬁnal performance of these systems heavily depends on the exact way these initial guesses are set. In our model we are able to remove this hand-crafted process and streamline the detection process by directly predicting the set of detections with absolute box prediction w.r.t. the input image rather than an anchor. ", "page_idx": 3, "bbox": [34, 189.72698974609375, 379, 283.3755187988281], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 24, "type": "text", "text": "Set-based loss.  Several object detectors [ 9 , 25 , 35 ] used the bipartite matching loss. However, in these early deep learning models, the relation between diﬀerent prediction was modeled with convolutional or fully-connected layers only and a hand-designed NMS post-processing can improve their performance. More recent detectors [ 37 , 23 , 53 ] use non-unique assignment rules between ground truth and predictions together with an NMS. ", "page_idx": 3, "bbox": [34, 290.1239013671875, 379, 359.8624572753906], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 25, "type": "text", "text": "Learnable NMS methods [ 16 , 4 ] and relation networks [ 17 ] explicitly model relations between diﬀerent predictions with attention. Using direct set losses, they do not require any post-processing steps. However, these methods employ additional hand-crafted context features like proposal box coordinates to model relations between detections eﬃciently, while we look for solutions that reduce the prior knowledge encoded in the model. ", "page_idx": 3, "bbox": [34, 361.8548278808594, 379, 431.59344482421875], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 26, "type": "text", "text": "Recurrent detectors.  Closest to our approach are end-to-end set predictions for object detection [ 43 ] and instance segmentation [ 41 , 30 , 36 , 42 ]. Similarly to us, they use bipartite-matching losses with encoder-decoder architectures based on CNN activation s to directly produce a set of bounding boxes. These approaches, however, were only evaluated on small datasets and not against modern baselines. In particular, they are based on auto regressive models (more precisely RNNs), so they do not leverage the recent transformers with parallel decoding. ", "page_idx": 3, "bbox": [34, 438.3418273925781, 379, 520.0354614257812], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 27, "type": "text", "text": "3 The DETR model ", "text_level": 1, "page_idx": 3, "bbox": [33, 536, 161, 551], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 28, "type": "text", "text": "Two ingredients are essential for direct set predictions in detection: (1) a set prediction loss that forces unique matching between predicted and ground truth ", "page_idx": 3, "bbox": [34, 561.6748657226562, 379, 583.5924682617188], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 29, "type": "text", "text": "boxes; (2) an architecture that predicts (in a single pass) a set of objects and models their relation. We describe our architecture in detail in Figure  2 . ", "page_idx": 4, "bbox": [32, 36.50601577758789, 379, 58.42363357543945], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 30, "type": "text", "text": "3.1 Object detection set prediction loss ", "text_level": 1, "page_idx": 4, "bbox": [33, 76, 239, 89], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 31, "type": "text", "text": "DETR infers a ﬁxed-size set of    $N$   predictions, in a single pass through the decoder, where    $N$   is set to be sign i cant ly larger than the typical number of objects in an image. One of the main diﬃculties of training is to score predicted objects (class, position, size) with respect to the ground truth. Our loss produces an optimal bipartite matching between predicted and ground truth objects, and then optimize object-speciﬁc (bounding box) losses. ", "page_idx": 4, "bbox": [32, 98.38904571533203, 379, 168.12757873535156], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 32, "type": "text", "text": "Let us denote by    $y$   the ground truth set of objects, and   $\\hat{y}\\,=\\,\\{\\hat{y}_{i}\\}_{i=1}^{N}$   { }   the set of    $N$   predictions. Assuming    $N$   is larger than the number of objects in the image, we consider    $y$   also as a set of size    $N$   padded with    $\\emptyset$  (no object). To ﬁnd a bipartite matching between these two sets we search for a permutation of    $N$  elements    $\\sigma\\in\\mathfrak{S}_{N}$   with the lowest cost: ", "page_idx": 4, "bbox": [32, 169, 379, 229.40855407714844], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 33, "type": "equation", "text": "\n$$\n\\hat{\\sigma}=\\underset{\\sigma\\in\\mathfrak{S}_{N}}{\\arg\\operatorname*{min}}\\sum_{i}^{N}\\mathcal{L}_{\\mathrm{match}}\\big(y_{i},\\hat{y}_{\\sigma(i)}\\big),\n$$\n ", "text_format": "latex", "page_idx": 4, "bbox": [135, 237, 277, 269], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 34, "type": "text", "text": "where    $\\mathcal{L}_{\\mathrm{match}}\\big(y_{i},\\hat{y}_{\\sigma(i)}\\big)$  ) is a pair-wise  matching cost  between ground truth    $y_{i}$   and a prediction with index    $\\sigma(i)$  . This optimal assignment is computed eﬃciently with the Hungarian algorithm, following prior work ( e.g . [ 43 ]). ", "page_idx": 4, "bbox": [32, 279.59893798828125, 379, 313.7205810546875], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 35, "type": "text", "text": "The matching cost takes into account both the class prediction and the sim- ilarity of predicted and ground truth boxes. Each element    $i$   of the ground truth set can be seen as a    $y_{i}~=~(c_{i},b_{i})$   where    $c_{i}$   is the target class label (which may be    $\\emptyset$  ) and    $b_{i}~\\in~[0,1]^{4}$    is a vector that deﬁnes ground truth box cen- ter coordinates and its height and width relative to the image size. For the prediction with index    $\\sigma(i)$   we deﬁne probability of class    $c_{i}$   as   $\\hat{p}_{\\sigma(i)}(c_{i})$  ) and the predicted box as   $\\hat{b}_{\\sigma(i)}$  . With these notations we deﬁne    $\\mathcal{L}_{\\mathrm{match}}(y_{i},\\hat{y}_{\\sigma(i)})$  ) as  $-\\mathbb{1}_{\\{c_{i}\\neq\\emptyset\\}}\\hat{p}_{\\sigma(i)}(c_{i})+\\mathbb{1}_{\\{c_{i}\\neq\\emptyset\\}}\\mathcal{L}_{\\mathrm{box}}\\big(b_{i},\\hat{b}_{\\sigma(i)}\\big)$  ). { } ", "page_idx": 4, "bbox": [32, 315.7249755859375, 379, 419.574951171875], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 36, "type": "text", "text": "This procedure of ﬁnding matching plays the same role as the heuristic assign- ment rules used to match proposal [ 37 ] or anchors [ 22 ] to ground truth objects in modern detectors. The main diﬀerence is that we need to ﬁnd one-to-one matching for direct set prediction without duplicates. ", "page_idx": 4, "bbox": [32, 415.6389465332031, 379, 461.467529296875], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 37, "type": "text", "text": "The second step is to compute the loss function, the  Hungarian loss  for all pairs matched in the previous step. We deﬁne the loss similarly to the losses of common object detectors,  i.e . a linear combination of a negative log-likelihood for class prediction and a box loss deﬁned later: ", "page_idx": 4, "bbox": [32, 463.4709167480469, 379, 509.299560546875], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 38, "type": "equation", "text": "\n$$\n\\mathcal{L}_{\\mathrm{Hunstar}}(y,\\hat{y})=\\sum_{i=1}^{N}\\left[-\\log\\hat{p}_{\\hat{\\sigma}(i)}(c_{i})+\\mathbb{1}_{\\{c_{i}\\neq\\emptyset\\}}\\mathcal{L}_{\\mathrm{box}}(b_{i},\\hat{b}_{\\hat{\\sigma}}(i))\\right]\\,,\n$$\n ", "text_format": "latex", "page_idx": 4, "bbox": [67, 519, 345, 552], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 39, "type": "text", "text": "where σ  is the optimal assignment computed in the ﬁrst step ( 1 ). In practice, we down-weight the log-probability term when    $c_{i}=\\emptyset$  by a factor 10 to account for class imbalance. This is analogous to how Faster R-CNN training procedure bal- ances positive/negative proposals by sub sampling [ 37 ]. Notice that the matching cost between an object and    $\\emptyset$  doesn’t depend on the prediction, which means that in that case the cost is a constant. In the matching cost we use probabil- ities   $\\hat{p}_{\\hat{\\sigma}(i)}(c_{i})$  ) instead of log-probabilities. This makes the class prediction term commensurable to    $\\mathcal{L}_{\\mathrm{box}}(\\cdot,\\cdot)$   (described below), and we observed better empirical performances. ", "page_idx": 4, "bbox": [32, 561.6749877929688, 379, 583.5925903320312], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 40, "type": "text", "text": "", "page_idx": 5, "bbox": [33, 36.50601577758789, 379, 118.19963073730469], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 41, "type": "text", "text": "Bounding box loss.  The second part of the matching cost and the Hungarian loss is    $\\mathcal{L}_{\\mathrm{{box}}}(\\cdot)$   that scores the bounding boxes. Unlike many detectors that do box predictions as a    $\\varDelta$  w.r.t. some initial guesses, we make box predictions directly. While such approach simplify the implementation it poses an issue with relative scaling of the loss. The most commonly-used    $\\ell_{1}$   loss will have diﬀerent scales for small and large boxes even if their relative errors are similar. To mitigate this issue we use a linear combination of the    $\\ell_{1}$   loss and the generalized IoU loss [ 38 ]\n\n  $\\mathcal{L}_{\\mathrm{ion}}(\\cdot,\\cdot)$   that is scale-invariant. Overall, our box loss is    $\\mathcal{L}_{\\mathrm{box}}(b_{i},\\hat{b}_{\\sigma(i)})$  ) deﬁned as\n\n  $\\lambda_{\\mathrm{ion}}\\mathcal{L}_{\\mathrm{ion}}\\big(b_{i},\\hat{b}_{\\sigma(i)}\\big)+\\lambda_{\\mathrm{L1}}||b_{i}-\\hat{b}_{\\sigma(i)}||_{1}$   where    $\\lambda_{\\mathrm{ion}},\\lambda_{\\mathrm{L1}}\\,\\in\\,\\mathbb{R}$   are hyper parameters. These two losses are normalized by the number of objects inside the batch. ", "page_idx": 5, "bbox": [33, 125.0490493774414, 379, 245.8185577392578], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 42, "type": "text", "text": "3.2 DETR architecture ", "text_level": 1, "page_idx": 5, "bbox": [33, 263, 157, 275], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 43, "type": "text", "text": "The overall DETR architecture is surprisingly simple and depicted in Figure  2 . It contains three main components, which we describe below: a CNN backbone to extract a compact feature representation, an encoder-decoder transformer, and a simple feed forward network (FFN) that makes the ﬁnal detection prediction. ", "page_idx": 5, "bbox": [33, 283.3509521484375, 379, 329.1795349121094], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 44, "type": "text", "text": "Unlike many modern detectors, DETR can be implemented in any deep learn- ing framework that provides a common CNN backbone and a transformer archi- tecture implementation with just a few hundred lines. Inference code for DETR can be implemented in less than 50 lines in PyTorch [ 32 ]. We hope that the sim- plicity of our method will attract new researchers to the detection community. ", "page_idx": 5, "bbox": [33, 331.1719055175781, 379, 388.95550537109375], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 45, "type": "text", "text": "Backbone.  Starting from the initial image    $\\boldsymbol{x}_{\\mathrm{img}}\\;\\in\\;\\mathbb{R}^{3\\times H_{0}\\times W_{0}}$    (with 3 color channels 2 ), a conventional CNN backbone generates a lower-resolution activation map    $f\\in\\mathbb{R}^{C\\times H\\times W}$    . Typical values we use are    $C=2048$   and    $\\begin{array}{r}{H,W=\\frac{H_{0}}{32},\\frac{W_{0}}{32}}\\end{array}$  . ", "page_idx": 5, "bbox": [33, 393.1849060058594, 379, 431], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 46, "type": "text", "text": "Transformer encoder.  First, a 1x1 convolution reduces the channel dimension of the high-level activation map    $f$   from    $C$   to a smaller dimension    $d$  . creating a new feature map    $z_{0}\\in\\mathbb{R}^{d\\times H\\times W}$  . The encoder expects a sequence as inpu we collapse the spatial dimensions of    $z_{0}$   into one dimension, resulting in a  $d\\!\\times\\!H\\,W$  × feature map. Each encoder layer has a standard architecture and consists of a multi-head self-attention module and a feed forward network (FFN). Since the transformer architecture is permutation-invariant, we supplement it with ﬁxed positional encodings [ 31 , 3 ] that are added to the input of each attention layer. We defer to the supplementary material the detailed deﬁnition of the architecture, which follows the one described in [ 47 ]. ", "page_idx": 5, "bbox": [33, 436.5260314941406, 379, 554.0856323242188], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 47, "type": "image", "page_idx": 6, "img_path": "layout_images/2005.12872v3_1.jpg", "img_caption": "Fig. 2: DETR uses a conventional CNN backbone to learn a 2D representation of an input image. The model ﬂattens it and supplements it with a positional encoding before passing it into a transformer encoder. A transformer decoder then takes as input a small ﬁxed number of learned positional embeddings, which we call  object queries , and additionally attends to the encoder output. We pass each output embedding of the decoder to a shared feed forward network (FFN) that predicts either a detection (class and bounding box) or a “ no object ” class. ", "bbox": [32, 33, 382, 208], "page_size": [413.8580017089844, 615.1179809570312], "ocr_text": "backbone\nset of image features!\n\n_ positional encoding\n\nencoder\n\ndecoder \"! prediction heads\n\niNew\n\ntransformer\nencoder\n\ntransformer\ndecoder\n\nobject queries\n", "vlm_text": "The image is an illustration of the DETR (Detection Transformer) model architecture, which is used for object detection tasks. It consists of several key components:\n\n1. **Backbone**: It uses a conventional CNN (Convolutional Neural Network) backbone to extract a set of image features from the input image. These features are augmented with positional encodings to retain the spatial information.\n\n2. **Encoder**: The transformer encoder processes the image features and the positional encodings to create a comprehensive 2D representation.\n\n3. **Decoder**: The transformer decoder uses a set of small fixed number of learned positional embeddings known as object queries. The decoder attends to the output of the transformer encoder to refine its predictions.\n\n4. **Prediction Heads**: For each output embedding of the decoder, a shared feed forward network (FFN) predicts either the class and bounding box of a detected object or classifies it as “no object.” The predictions are visualized as bounding boxes with associated class labels overlaid on an image of birds, showing the model's capability to detect objects in the scene."}
{"layout": 48, "type": "text", "text": "Transformer decoder.  The decoder follows the standard architecture of the transformer, transforming    $N$   embeddings of size    $d$   using multi-headed self- and encoder-decoder attention mechanisms. The diﬀerence with the original trans- former is that our model decodes the    $N$   objects in parallel at each decoder layer, while Vaswani et al. [ 47 ] use an auto regressive model that predicts the output sequence one element at a time. We refer the reader unfamiliar with the concepts to the supplementary material. Since the decoder is also permutation-invariant, the    $N$   input embeddings must be diﬀerent to produce diﬀerent results. These in- put embeddings are learnt positional encodings that we refer to as  object queries , and similarly to the encoder, we add them to the input of each attention layer. The    $N$   object queries are transformed into an output embedding by the decoder. They are then  independently  decoded into box coordinates and class labels by a feed forward network (described in the next subsection), resulting    $N$   ﬁnal predictions. Using self- and encoder-decoder attention over these embeddings, the model globally reasons about all objects together using pair-wise relations between them, while being able to use the whole image as context. ", "page_idx": 6, "bbox": [34, 234.58401489257812, 379, 423.87451171875], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 49, "type": "text", "text": "Prediction feed-forward networks (FFNs).  The ﬁnal prediction is com- puted by a 3-layer perceptron with ReLU activation function and hidden dimen- sion    $d$  , and a linear projection layer. The FFN predicts the normalized center coordinates, height and width of the box w.r.t. the input image, and the lin- ear layer predicts the class label using a softmax function. Since we predict a ﬁxed-size set of    $N$   bounding boxes, where    $N$   is usually much larger than the actual number of objects of interest in an image, an additional special class la- bel    $\\emptyset$  is used to represent that no object is detected within a slot. This class plays a similar role to the “background” class in the standard object detection approaches. ", "page_idx": 6, "bbox": [34, 433.9949035644531, 379, 551.5545043945312], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 50, "type": "text", "text": "Auxiliary decoding losses.  We found helpful to use auxiliary losses [ 1 ] in decoder during training, especially to help the model output the correct number of objects of each class. We add prediction FFNs and Hungarian loss after each decoder layer. All predictions FFNs share their parameters. We use an additional shared layer-norm to normalize the input to the prediction FFNs from diﬀerent decoder layers. ", "page_idx": 6, "bbox": [34, 561.6749267578125, 379, 583.592529296875], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 51, "type": "text", "text": "", "page_idx": 7, "bbox": [34, 36.50601577758789, 379, 82.33464050292969], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 52, "type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 7, "bbox": [33, 102, 129, 116], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 53, "type": "text", "text": "We show that DETR achieves competitive results compared to Faster R-CNN in quantitative evaluation on COCO. Then, we provide a detailed ablation study of the architecture and loss, with insights and qualitative results. Fi- nally, to show that DETR is a versatile and extensible model, we present results on panoptic segmentation, training only a small extension on a ﬁxed DETR model. We provide code and pretrained models to reproduce our experiments at https://github.com/facebook research/detr . ", "page_idx": 7, "bbox": [34, 130.53103637695312, 379, 212.23260498046875], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 54, "type": "text", "text": "Dataset.  We perform experiments on COCO 2017 detection and panoptic seg- mentation datasets [ 24 , 18 ], containing 118k training images and 5k validation images. Each image is annotated with bounding boxes and panoptic segmenta- tion. There are 7 instances per image on average, up to 63 instances in a single image in training set, ranging from small to large on the same images. If not speciﬁed, we report AP as bbox AP, the integral metric over multiple thresholds. For comparison with Faster R-CNN we report validation AP at the last training epoch, for ablations we report median over validation results from the last 10 epochs. ", "page_idx": 7, "bbox": [34, 220.45596313476562, 379, 326.0594787597656], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 55, "type": "text", "text": "Technical details.  We train DETR with AdamW [ 26 ] setting the initial trans- former’s learning rate to   $10^{-4}$  , the backbone’s to   $10^{-5}$  , and weight decay to   $10^{-4}$  . All transformer weights are initialized with Xavier init [ 11 ], and the backbone is with ImageNet-pretrained ResNet model [ 15 ] from  torch vision  with frozen batchnorm layers. We report results with two diﬀerent backbones: a ResNet- 50 and a ResNet-101. The corresponding models are called respectively DETR and DETR-R101. Following [ 21 ], we also increase the feature resolution by adding a dilation to the last stage of the backbone and removing a stride from the ﬁrst convolution of this stage. The corresponding models are called respec- tively DETR-DC5 and DETR-DC5-R101 (dilated C5 stage). This modi cation increases the resolution by a factor of two, thus improving performance for small objects, at the cost of a 16x higher cost in the self-attentions of the encoder, leading to an overall 2x increase in computational cost. A full comparison of FLOPs of these models and Faster R-CNN is given in Table  1 . ", "page_idx": 7, "bbox": [34, 334.28985595703125, 379, 499.6694641113281], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 56, "type": "text", "text": "We use scale augmentation, resizing the input images such that the shortest side is at least 480 and at most 800 pixels while the longest at most 1333 [ 50 ]. To help learning global relationships through the self-attention of the encoder, we also apply random crop augmentations during training, improving the per- formance by approximately 1 AP. Spec i call y, a train image is cropped with probability 0.5 to a random rectangular patch which is then resized again to 800-1333. The transformer is trained with default dropout of 0.1. At inference ", "page_idx": 7, "bbox": [34, 501.8988342285156, 379, 583.5924682617188], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 57, "type": "table", "page_idx": 8, "img_path": "layout_images/2005.12872v3_2.jpg", "table_caption": "Table 1: Comparison with Faster R-CNN with a ResNet-50 and ResNet-101 backbones on the COCO validation set. The top section shows results for Faster R-CNN models in Detectron2 [ 50 ], the middle section shows results for Faster R-CNN models with GIoU [ 38 ], random crops train-time augmentation, and the long  9x  training schedule. DETR models achieve comparable results to heavily tuned Faster R-CNN baselines, having lower AP S  but greatly improved AP  $\\mathbf{L}$  . We use torch script Faster R-CNN and DETR models to measure FLOPS and FPS. Results without R101 in the name corre- spond to ResNet-50. ", "bbox": [32, 43, 383, 274], "page_size": [413.8580017089844, 615.1179809570312], "ocr_text": "Model GFLOPS/FPS #params AP APs0 AP75 APs APm APL\nFaster RCNN-DC5 320/16 166M 39.0 60.5 42.3 21.4 43.5 52.5\nFaster RCNN-FPN 180/26 42M 40.2 61.0 43.8 24.2 43.5 52.0\nFaster RCNN-R101-FPN 246/20 60M 42.0 62.5 45.9 25.2 45.6 54.6\nFaster RCNN-DC5+ 320/16 166M 41.1 61.4 44.3 22.9 45.9 55.0\nFaster RCNN-FPN+ 180/26 42M 42.0 62.1 45.5 26.6 45.4 53.4\nFaster RCNN-R101-FPN+ 246/20 60M 44.0 63.9 47.8 27.2 48.1 56.0\nDETR 86/28 4IM 42.0 62.4 44.2 20.5 45.8 61.1\nDETR-DC5 187/12 4IM = 43.3 63.1 45.9 22.5 47.3 61.1\nDETR-R101 152/20 60M 43.5 63.8 46.4 21.9 48.0 61.8\nDETR-DC5-R101 253/10 60M 44.9 64.7 47.7 23.7 49.5 62.3\n\n", "vlm_text": "This table compares different detection models based on several metrics:\n\n- **Model:** Lists the model names.\n- **GFLOPS/FPS:** Indicates the computational cost (GFLOPS) and speed (frames per second, FPS).\n- **#params:** Number of parameters in millions (M).\n- **AP (Average Precision):** Overall performance measure.\n- **AP<sub>50</sub> and AP<sub>75</sub>:** Performance at specific Intersection over Union (IoU) thresholds of 50% and 75%.\n- **AP<sub>S</sub>, AP<sub>M</sub>, AP<sub>L</sub>:** Performance on small, medium, and large objects.\n\nEach row provides detailed metrics for a specific model configuration."}
{"layout": 58, "type": "text", "text": "time, some slots predict empty class. To optimize for AP, we override the predic- tion of these slots with the second highest scoring class, using the corresponding conﬁdence. This improves AP by 2 points compared to ﬁltering out empty slots. Other training hyper parameters can be found in section  A.4 . For our ablation experiments we use training schedule of 300 epochs with a learning rate drop by a factor of 10 after 200 epochs, where a single epoch is a pass over all train- ing images once. Training the baseline model for 300 epochs on 16 V100 GPUs takes 3 days, with 4 images per GPU (hence a total batch size of 64). For the longer schedule used to compare with Faster R-CNN we train for 500 epochs with learning rate drop after 400 epochs. This schedule adds 1.5 AP compared to the shorter schedule. ", "page_idx": 8, "bbox": [34, 298.53302001953125, 379, 428.047607421875], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 59, "type": "text", "text": "4.1 Comparison with Faster R-CNN ", "text_level": 1, "page_idx": 8, "bbox": [34, 445, 222, 458], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 60, "type": "text", "text": "Transformers are typically trained with Adam or Adagrad optimizers with very long training schedules and dropout, and this is true for DETR as well. Faster R-CNN, however, is trained with SGD with minimal data augmentation and we are not aware of successful applications of Adam or dropout. Despite these diﬀerences we attempt to make a Faster R-CNN baseline stronger. To align it with DETR, we add generalized IoU [ 38 ] to the box loss, the same random crop augmentation and long training known to improve results [ 13 ]. Results are presented in Table  1 . In the top section we show Faster R-CNN results from Detectron2 Model Zoo [ 50 ] for models trained with the  3x  schedule. In the middle section we show results (with a “+”) for the same models but trained ", "page_idx": 8, "bbox": [34, 466.0339660644531, 379, 583.5925903320312], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 61, "type": "table", "page_idx": 9, "img_path": "layout_images/2005.12872v3_3.jpg", "table_caption": "Table 2: Eﬀect of encoder size. Each row corresponds to a model with varied number of encoder layers and ﬁxed number of decoder layers. Performance gradually improves with more encoder layers. ", "bbox": [32, 43, 381, 143], "page_size": [413.8580017089844, 615.1179809570312], "ocr_text": "#layers GFLOPS/FPS #params AP APso APs APM  APzt\n0 76/28 33.4M 36.7 57.4 16.8 39.6 54.2\n3 81/25 37.4M 40.1 60.6 18.5 43.8 58.6\n6 86/23 41.3M 40.6 61.6 19.9 44.3 60.2\n12 95/20 49.2M 41.6 62.1 19.8 44.9 61.9\n\n", "vlm_text": "The table contains various data metrics related to network layers:\n\n- **#layers**: Number of layers (0, 3, 6, 12)\n- **GFLOPS/FPS**: Gigaflops per second / Frames per second (76/28, 81/25, 86/23, 95/20)\n- **#params**: Number of parameters (33.4M, 37.4M, 41.3M, 49.2M)\n- **AP**: Average Precision (36.7, 40.1, 40.6, 41.6)\n- **AP₅₀**: Average Precision at IoU=0.50 (57.4, 60.6, 61.6, 62.1)\n- **AₚS**: Average Precision for small objects (16.8, 18.5, 19.9, 19.8)\n- **AₚM**: Average Precision for medium objects (39.6, 43.8, 44.3, 44.9)\n- **AₚL**: Average Precision for large objects (54.2, 58.6, 60.2, 61.9) \n\nThis table is likely from a machine learning or computer vision context, detailing the performance and characteristics of models with different numbers of layers."}
{"layout": 62, "type": "text", "text": "with the  9x  schedule (109 epochs) and the described enhancements, which in total adds 1-2 AP. In the last section of Table  1  we show the results for multiple DETR models. To be comparable in the number of parameters we choose a model with 6 transformer and 6 decoder layers of width 256 with 8 attention heads. Like Faster R-CNN with FPN this model has 41.3M parameters, out of which 23.5M are in ResNet-50, and 17.8M are in the transformer. Even though both Faster R-CNN and DETR are still likely to further improve with longer training, we can conclude that DETR can be competitive with Faster R-CNN with the same number of parameters, achieving 42 AP on the COCO val subset. The way DETR achieves this is by improving AP  $\\mathrm{L}$   (+7.8), however note that the model is still lagging behind in AP S  (-5.5). DETR-DC5 with the same number of parameters and similar FLOP count has higher AP, but is still sign i cant ly behind in AP S  too. Faster R-CNN and DETR with ResNet-101 backbone show comparable results as well. ", "page_idx": 9, "bbox": [34, 168.36602783203125, 379, 333.7464904785156], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 63, "type": "text", "text": "4.2 Ablations ", "text_level": 1, "page_idx": 9, "bbox": [34, 352, 108, 363], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 64, "type": "text", "text": "Attention mechanisms in the transformer decoder are the key components which model relations between feature representations of diﬀerent detections. In our ablation analysis, we explore how other components of our architecture and loss inﬂuence the ﬁnal performance. For the study we choose ResNet-50-based DETR model with 6 encoder, 6 decoder layers and width 256. The model has 41.3M parameters, achieves 40.6 and 42.0 AP on short and long schedules respectively, and runs at 28 FPS, similarly to Faster R-CNN-FPN with the same backbone. ", "page_idx": 9, "bbox": [34, 372.5028991699219, 379, 454.1965026855469], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 65, "type": "text", "text": "Number of encoder layers.  We evaluate the importance of global image- level self-attention by changing the number of encoder layers (Table  2 ). Without encoder layers, overall AP drops by 3.9 points, with a more signiﬁcant drop of 6.0 AP on large objects. We hypothesize that, by using global scene reasoning, the encoder is important for disentangling objects. In Figure  3 , we visualize the attention maps of the last encoder layer of a trained model, focusing on a few points in the image. The encoder seems to separate instances already, which likely simpliﬁes object extraction and localization for the decoder. ", "page_idx": 9, "bbox": [34, 461.1108703613281, 379, 554.760498046875], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 66, "type": "text", "text": "Number of decoder layers.  We apply auxiliary losses after each decoding layer (see Section  3.2 ), hence, the prediction FFNs are trained by design to pre- ", "page_idx": 9, "bbox": [34, 561.6749267578125, 379, 583.592529296875], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 67, "type": "image", "page_idx": 10, "img_path": "layout_images/2005.12872v3_4.jpg", "img_caption": "Fig. 3: Encoder self-attention for a set of reference points. The encoder is able to sep- arate individual instances. Predictions are made with baseline DETR model on a vali- dation set image. ", "bbox": [33, 34, 380, 196], "page_size": [413.8580017089844, 615.1179809570312], "ocr_text": "self-attention(430, 600) self-attention(450, 830)\n\nself-attention(520, 450) selfattention(440, 1200)\n\n", "vlm_text": "The image shows a photograph of a group of cows in a field. The central panel displays the actual image with several instances of cows. On either side, there are visualizations of self-attention from an encoder in a model (likely the DETR model), which highlight how specific reference points within the image are attended to by the model. The highlighted points focus on different cows, demonstrating the model's ability to separate individual instances of cows through self-attention mechanisms. The self-attention maps are shown as blue squares with yellow highlights indicating areas of focus at specific coordinates (e.g., (430, 600), (520, 450), etc.)."}
{"layout": 68, "type": "text", "text": "dict objects out of the outputs of every decoder layer. We analyze the importance of each decoder layer by evaluating the objects that would be predicted at each stage of the decoding (Fig.  4 ). Both AP and AP  $^{50}$   improve after every layer, totalling into a very signiﬁcant +8.2/9.5 AP improvement between the ﬁrst and the last layer. With its set-based loss, DETR does not need NMS by design. To verify this we run a standard NMS procedure with default parameters [ 50 ] for the outputs after each decoder. NMS improves performance for the predictions from the ﬁrst decoder. This can be explained by the fact that a single decoding layer of the transformer is not able to compute any cross-correlations between the output elements, and thus it is prone to making multiple predictions for the same object. In the second and subsequent layers, the self-attention mechanism over the activation s allows the model to inhibit duplicate predictions. We ob- serve that the improvement brought by NMS diminishes as depth increases. At the last layers, we observe a small loss in AP as NMS incorrectly removes true positive predictions. ", "page_idx": 10, "bbox": [34, 223.4150390625, 379, 400.75054931640625], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 69, "type": "text", "text": "Similarly to visualizing encoder attention, we visualize decoder attentions in Fig.  6 , coloring attention maps for each predicted object in diﬀerent colors. We observe that decoder attention is fairly local, meaning that it mostly attends to object extremities such as heads or legs. We hypothesis e that after the encoder has separated instances via global attention, the decoder only needs to attend to the extremities to extract the class and object boundaries. ", "page_idx": 10, "bbox": [34, 403.2169189453125, 379, 472.9555358886719], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 70, "type": "text", "text": "Importance of FFN.  FFN inside tr an former s can be seen as   $1\\times1$   convo- lutional layers, making encoder similar to attention augmented convolutional networks [ 3 ]. We attempt to remove it completely leaving only attention in the transformer layers. By reducing the number of network parameters from 41.3M to 28.7M, leaving only 10.8M in the transformer, performance drops by 2.3 AP, we thus conclude that FFN are important for achieving good results. ", "page_idx": 10, "bbox": [34, 482.4459228515625, 379, 552.1845703125], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 71, "type": "text", "text": "Importance of positional encodings.  There are two kinds of positional en- codings in our model: spatial positional encodings and output positional encod- ", "page_idx": 10, "bbox": [34, 561.6749877929688, 379, 583.592529296875], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 72, "type": "image", "page_idx": 11, "img_path": "layout_images/2005.12872v3_5.jpg", "img_caption": "Fig. 4: AP and AP 50  performance after each de- Fig. 5: Out of distribution gen- coder layer. A single long schedule baseline model era liz ation for rare classes. is evaluated. DETR does not need NMS by de- Even though no image in the sign, which is validated by this ﬁgure. NMS lowers training set has more than 13 AP in the ﬁnal layers, removing TP predictions, giraﬀes, DETR has no diﬃ- but improves AP in the ﬁrst decoder layers, re- culty generalizing to 24 and moving double predictions, as there is no commu- more instances of the same nication in the ﬁrst layer, and slightly improves class.  $\\mathrm{{AP}_{50}}$  . ", "bbox": [40, 31, 373, 262], "page_size": [413.8580017089844, 615.1179809570312], "ocr_text": "— AP No NMS\n-+- AP NMS=0.7\n— APs) No NMS\n= e+ APs5y NMS=0.7\n\n1 2 3 4 5 6\ndecoder layer\n", "vlm_text": "The image is a graph showing the average precision (AP) and AP at 50% IoU (AP₅₀) performance as a function of decoder layers in a model, likely referring to the DETR model. The curves compare the performance without non-maximum suppression (NMS) and with NMS set to 0.7. The y-axis represents AP and AP₅₀ values, while the x-axis shows the decoder layers from 1 to 6. The performance metrics are represented by solid and dashed lines for both AP and AP₅₀, with separate lines for when NMS is applied and when it is not.\n\nThe caption explains that DETR is evaluated without needing NMS by design, and shows how NMS affects AP across decoder layers, improving early AP but reducing AP in later layers due to removing true positive predictions. The caption also refers to DETR’s generalization capabilities for recognizing multiple instances of a rare class."}
{"layout": 73, "type": "text", "text": "ings (object queries). We experiment with various combinations of ﬁxed and learned encodings, results can be found in table  3 . Output positional encodings are required and cannot be removed, so we experiment with either passing them once at decoder input or adding to queries at every decoder attention layer. In the ﬁrst experiment we completely remove spatial positional encodings and pass output positional encodings at input and, interestingly, the model still achieves more than 32 AP, losing 7.8 AP to the baseline. Then, we pass ﬁxed sine spatial positional encodings and the output encodings at input once, as in the original transformer [ 47 ], and ﬁnd that this leads to 1.4 AP drop compared to passing the positional encodings directly in attention. Learned spatial encodings passed to the attentions give similar results. Surprisingly, we ﬁnd that not passing any spatial encodings in the encoder only leads to a minor AP drop of 1.3 AP. When we pass the encodings to the attentions, they are shared across all layers, and the output encodings (object queries) are always learned. ", "page_idx": 11, "bbox": [34, 289.09796142578125, 379, 454.4775390625], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 74, "type": "text", "text": "Given these ablations, we conclude that transformer components: the global self-attention in encoder, FFN, multiple decoder layers, and positional encodings, all sign i cant ly contribute to the ﬁnal object detection performance. ", "page_idx": 11, "bbox": [34, 457.1969299316406, 379, 491.06951904296875], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 75, "type": "text", "text": "Loss ablations.  To evaluate the importance of diﬀerent components of the matching cost and the loss, we train several models turning them on and oﬀ. There are three components to the loss: class i cation loss,    $\\ell_{1}$   bounding box distance loss, and GIoU [ 38 ] loss. The class i cation loss is essential for training and cannot be turned oﬀ, so we train a model without bounding box distance loss, and a model without the GIoU loss, and compare with baseline, trained with all three losses. Results are presented in table  4 . GIoU loss on its own accounts ", "page_idx": 11, "bbox": [34, 501.89892578125, 379, 583.592529296875], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 76, "type": "image", "page_idx": 12, "img_path": "layout_images/2005.12872v3_6.jpg", "img_caption": "Fig. 6: Visualizing decoder attention for every predicted object (images from COCO val  set). Predictions are made with DETR-DC5 model. Attention scores are coded with diﬀerent colors for diﬀerent objects. Decoder typically attends to object extremities, such as legs and heads. Best viewed in color. ", "bbox": [33, 32, 381, 236], "page_size": [413.8580017089844, 615.1179809570312], "ocr_text": "", "vlm_text": "The image consists of two main sections showcasing animal objects detected in a scene, likely using an object detection model called DETR-DC5. \n\n1. **Left Side**: There are two elephants labeled with bounding boxes. Each elephant's bounding box is accompanied by the label \"elephant\" and a confidence score of 100%. The decoder's attention visualization highlights specific parts of the elephants, most notably focusing on their extremities such as legs and heads. The attention scores are visualized in different colors indicating varying attention assigned to distinct parts of the elephants.\n\n2. **Right Side**: Two zebras are depicted, each similarly enclosed in bounding boxes and labeled \"zebra\" with confidence values of 99% and 100%, respectively. Again, attention scores are color-coded to show where the model's decoder is focusing, often on the zebras' heads and legs. These areas of focus demonstrate where the attention mechanism is strongest, which is integral for object recognition and classification.\n\nOverall, the caption and visual elements illustrate how the DETR-DC5 model effectively localizes and identifies animals within the given scenes, with a particular emphasis on extremities to aid in accurate classification and localization."}
{"layout": 77, "type": "text", "text": "Table 3: Results for diﬀerent positional encodings compared to the baseline (last row), which has ﬁxed sine pos. encodings passed at every attention layer in both the encoder and the decoder. Learned embeddings are shared between all layers. Not using spatial positional encodings leads to a signiﬁcant drop in AP. Interestingly, passing them in decoder only leads to a minor AP drop. All these models use learned output positional encodings. ", "page_idx": 12, "bbox": [34, 264.30615234375, 379, 328.0674743652344], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 78, "type": "table", "page_idx": 12, "img_path": "layout_images/2005.12872v3_7.jpg", "table_caption": "Table 4: Eﬀect of loss components on AP. We train two models turning oﬀ  $\\ell_{1}$   loss, and GIoU loss, and observe that    $\\ell_{1}$   gives poor results on its own, but when combined with GIoU improves   $\\mathrm{AP_{M}}$   and   $\\mathrm{AP_{L}}$  . Our baseline (last row) combines both losses. ", "bbox": [33, 329, 381, 475], "page_size": [413.8580017089844, 615.1179809570312], "ocr_text": "spatial pos. enc.\n\noutput pos. enc.\n\nencoder decoder decoder AP A | APs0 A\nnone none learned at input | 32.8 -7.8 | 55.2 -6.5\nsine at input sine at input learned at input | 39.2 -1.4 | 60.0  -1.6\nlearned at attn. learned at attn. learned at attn. 39.6 -1.0 | 60.7 -0.9\nnone sine at attn. learned at attn. 39.3 -1.3 | 60.3 -1.4\nsine at attn. sine at attn. 40.6 - 61.6 -\n\nlearned at attn.\n", "vlm_text": "The table presents results for different configurations of positional encoding in an encoder-decoder model. The columns are as follows:\n\n1. **spatial pos. enc.**\n   - **encoder**: Type of positional encoding used in the encoder (none, sine at input, learned at attention).\n   - **decoder**: Type of positional encoding used in the decoder (none, sine at input, learned at attention).\n\n2. **output pos. enc.**\n   - **decoder**: Type of positional encoding for decoder output (learned at input or learned at attention).\n\n3. **AP**: Average Precision score for each configuration.\n\n4. **Δ (Delta) AP**: Change in Average Precision compared to a baseline.\n\n5. **AP<sub>50</sub>**: Average Precision at IoU threshold 50.\n\n6. **Δ (Delta) AP<sub>50</sub>**: Change in AP<sub>50</sub> compared to a baseline.\n\nThe highest AP and AP<sub>50</sub> scores are in bold in the last row."}
{"layout": 79, "type": "table", "page_idx": 12, "img_path": "layout_images/2005.12872v3_8.jpg", "bbox": [37, 477, 373, 532], "page_size": [413.8580017089844, 615.1179809570312], "ocr_text": "class ¢ GIoU | AP A | APs) A | APs APM APL\nv v 35.8 -48 | 57.3 -44 | 13.7 39.8 57.9\nv v 39.9 -0.7 | 616 0 | 199 43.2 57.9\nv v v 406 8 - 61.6 - 19.9 44.3 60.2\n", "vlm_text": "The table summarizes results across different metrics for combinations of `class`, `ℓ₁`, and `GIoU`:\n\n- **Metrics:**\n  - **AP (Average Precision)**\n  - **Δ (Change in AP and AP₅₀)**\n  - **AP₅₀ (Average Precision at IoU=0.5)**\n  - **APS (AP for small objects)**\n  - **APM (AP for medium objects)**\n  - **APL (AP for large objects)**\n\n- **Rows:**\n  1. **First row:** Uses `class` and `ℓ₁`\n     - AP: 35.8\n     - Δ: -4.8\n     - AP₅₀: 57.3\n     - Δ: -4.4\n     - APS: 13.7\n     - APM: 39.8\n     - APL: 57.9\n     \n  2. **Second row:** Uses `class` and `GIoU`\n     - AP: 39.9\n     - Δ: -0.7\n     - AP₅₀: 61.6\n     - Δ: 0\n     - APS: 19.9 (bold)\n     - APM: 43.2\n     - APL: 57.9\n\n  3. **Third row:** Uses `class`, `ℓ₁`, and `GIoU`\n     - AP: 40.6 (bold)\n     - Δ: Not available\n     - AP₅₀: 61.6\n     - Δ: Not available\n     - APS: 19.9\n     - APM: 44.3\n     - APL: 60.2 (bold)"}
{"layout": 80, "type": "text", "text": "for most of the model performance, losing only 0.7 AP to the baseline with combined losses. Using    $\\ell_{1}$   without GIoU shows poor results. We only studied ", "page_idx": 12, "bbox": [34, 561.675048828125, 379, 585.087646484375], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 81, "type": "image", "page_idx": 13, "img_path": "layout_images/2005.12872v3_9.jpg", "bbox": [34, 34, 379, 100], "page_size": [413.8580017089844, 615.1179809570312], "ocr_text": "", "vlm_text": "The image appears to be a collection of scatter plots, each contained within its own panel, organized into two rows. These scatter plots likely display multivariate data across various dimensions. The different colors (predominantly green, purple, and orange) likely represent various categories or clusters of data points. The arrangement into a grid of panels suggests it might be a pair plot or something similar, used for visualizing relationships between multiple pairs of features or variables in a dataset. This type of visualization is commonly used in exploratory data analysis to get insights into the distribution and correlation between variables."}
{"layout": 82, "type": "text", "text": "Fig. 7: Visualization of all box predictions on all images from COCO 2017 val set for 20 out of total    $N=100$   prediction slots in DETR decoder. Each box prediction is represented as a point with the coordinates of its center in the 1-by-1 square normalized by each image size. The points are color-coded so that green color corresponds to small boxes, red to large horizontal boxes and blue to large vertical boxes. We observe that each slot learns to specialize on certain areas and box sizes with several operating modes. We note that almost all slots have a mode of predicting large image-wide boxes that are common in COCO dataset. ", "page_idx": 13, "bbox": [34, 112.5451889038086, 379, 198.22354125976562], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 83, "type": "text", "text": "simple ablations of diﬀerent losses (using the same weighting every time), but other means of combining them may achieve diﬀerent results. ", "page_idx": 13, "bbox": [34, 225.09298706054688, 379, 247.0105743408203], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 84, "type": "text", "text": "4.3 Analysis ", "text_level": 1, "page_idx": 13, "bbox": [34, 268, 102, 279], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 85, "type": "text", "text": "Decoder output slot analysis  In Fig.  7  we visualize the boxes predicted by diﬀerent slots for all images in COCO 2017 val set. DETR learns diﬀerent specialization for each query slot. We observe that each slot has several modes of operation focusing on diﬀerent areas and box sizes. In particular, all slots have the mode for predicting image-wide boxes (visible as the red dots aligned in the middle of the plot). We hypothesize that this is related to the distribution of objects in COCO. ", "page_idx": 13, "bbox": [34, 290.1399841308594, 379, 371.83355712890625], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 86, "type": "text", "text": "Generalization to unseen numbers of instances.  Some classes in COCO are not well represented with many instances of the same class in the same image. For example, there is no image with more than 13 giraﬀes in the training set. We create a synthetic image  $^3$    to verify the generalization ability of DETR (see Figure  5 ). Our model is able to ﬁnd all 24 giraﬀes on the image which is clearly out of distribution. This experiment conﬁrms that there is no strong class-specialization in each object query. ", "page_idx": 13, "bbox": [34, 380.5469055175781, 379, 462.24053955078125], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 87, "type": "text", "text": "4.4 DETR for panoptic segmentation ", "text_level": 1, "page_idx": 13, "bbox": [34, 483, 227, 494], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 88, "type": "text", "text": "Panoptic segmentation [ 19 ] has recently attracted a lot of attention from the computer vision community. Similarly to the extension of Faster R-CNN [ 37 ] to Mask R-CNN [ 14 ], DETR can be naturally extended by adding a mask head on top of the decoder outputs. In this section we demonstrate that such a head can be used to produce panoptic segmentation [ 19 ] by treating stuﬀand thing classes ", "page_idx": 13, "bbox": [34, 505.3699035644531, 379, 563.153564453125], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 89, "type": "image", "page_idx": 14, "img_path": "layout_images/2005.12872v3_10.jpg", "img_caption": "Fig. 8: Illustration of the panoptic head. A binary mask is generated in parallel for each detected object, then the masks are merged using pixel-wise argmax. ", "bbox": [32, 30, 381, 173], "page_size": [413.8580017089844, 615.1179809570312], "ocr_text": "Input image\n\n@xHxw\n\nEncoded image Resnet features\n\n(dx H/32 x W/32) ResS Res Res3_ Res?\n\nuojueye peey HIN\n\nBox embeddings\n(dx)\n\nBIBUBTEOUOD,\n\nXBWIBIE OSIM-[OXIq,\n\nAttention maps FPN-style CNN\n\n(Nx Mx H/32 x W/32)\n\nMasks logits\n(Nx H/4 x Wid)\n\n", "vlm_text": "The image is a diagram showing the process of a panoptic segmentation model. Here’s a breakdown of the components illustrated:\n\n1. **Input Image**: A labeled image with objects such as a cow, tree, sky, and grass.\n\n2. **Multi-head Attention**: Encodes the input image along with box embeddings to produce attention maps.\n\n3. **Encoded Image**: The output from the multi-head attention module.\n\n4. **ResNet Features**: Different layers of a ResNet (Residual Network) provide feature maps for further processing.\n\n5. **FPN-style CNN (Feature Pyramid Network)**: Combines features from different layers of the ResNet to refine the information.\n\n6. **Mask Logits**: Generated for different objects, showing their probability distribution over the image in multiple channels.\n\n7. **Pixel-wise Argmax**: Combines the masks to generate the final panoptic segmentation output, labeling each pixel as part of a specific object or region (e.g., sky, tree, cow, grass) with the highest probability."}
{"layout": 90, "type": "image", "page_idx": 14, "img_path": "layout_images/2005.12872v3_11.jpg", "img_caption": "Fig. 9: Qualitative results for panoptic segmentation generated by DETR-R101. DETR produces aligned mask predictions in a uniﬁed manner for things and stuﬀ. ", "bbox": [32, 190, 380, 318], "page_size": [413.8580017089844, 615.1179809570312], "ocr_text": "cabinet}\n\nmicrowave}\n\n", "vlm_text": "The image shows three panels demonstrating panoptic segmentation using DETR-R101. Each section is labeled with segments for objects or materials:\n\n1. **Left Panel**: A kitchen scene is segmented with labels like \"light,\" \"cabinet,\" \"microwave,\" \"oven,\" \"counter,\" \"sink,\" \"floor,\" \"potted plant,\" \"vase,\" \"shelf,\" \"book,\" and \"wall-stonedoor-stuff.\"\n\n2. **Middle Panel**: Features a bus with segments labeled \"sky,\" \"bus,\" \"pavement,\" \"building,\" and \"truck.\"\n\n3. **Right Panel**: Shows two giraffes with segments labeled \"giraffe,\" \"sky,\" \"tree,\" and \"grass.\"\n\nEach section highlights how DETR aligns mask predictions for different objects and materials within a unified framework."}
{"layout": 91, "type": "text", "text": "in a uniﬁed way. We perform our experiments on the panoptic annotations of the COCO dataset that has 53 stu categories in addition to 80 things categories. ", "page_idx": 14, "bbox": [34, 350.7210388183594, 379, 372.638671875], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 92, "type": "text", "text": "We train DETR to predict boxes around both  stuﬀ and  things  classes on COCO, using the same recipe. Predicting boxes is required for the training to be possible, since the Hungarian matching is computed using distances between boxes. We also add a mask head which predicts a binary mask for each of the predicted boxes, see Figure  8 . It takes as input the output of transformer decoder for each object and computes multi-head (with    $M$   heads) attention scores of this embedding over the output of the encoder, generating    $M$   attention heatmaps per object in a small resolution. To make the ﬁnal prediction and increase the resolution, an FPN-like architecture is used. We describe the architecture in more details in the supplement. The ﬁnal resolution of the masks has stride 4 and each mask is supervised independently using the DICE/F-1 loss [ 28 ] and Focal loss [ 23 ]. ", "page_idx": 14, "bbox": [34, 378.4900207519531, 379, 519.9586791992188], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 93, "type": "text", "text": "The mask head can be trained either jointly, or in a two steps process, where we train DETR for boxes only, then freeze all the weights and train only the mask head for 25 epochs. Experimentally, these two approaches give similar results, we report results using the latter method since it results in a shorter total wall-clock time training. ", "page_idx": 14, "bbox": [34, 525.81005859375, 379, 583.5926513671875], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 94, "type": "table", "page_idx": 15, "img_path": "layout_images/2005.12872v3_12.jpg", "table_caption": "Table 5: Comparison with the state-of-the-art methods UPSNet [ 51 ] and Panoptic FPN [ 18 ] on the COCO  val  dataset We retrained Pan optic FP N with the same data- augmentation as DETR, on a 18x schedule for fair comparison. UPSNet uses the  1x schedule, UPSNet-M is the version with multiscale test-time augmentations. ", "bbox": [33, 43, 381, 178], "page_size": [413.8580017089844, 615.1179809570312], "ocr_text": "Model Backbone| PQ. SQ RQ [PQ SQ* RQ'\"|PQ* SQ* RQ*| AP\nPanopticFPN++ R50 | 424 79.3 51.6] 49.2 82.4 58.8 | 32.3 74.8 40.6 | 37.7\nUPSnet R50 | 42.5 78.0 52.5] 48.6 79.4 59.6 | 33.4 75.9 41.7 | 34.3\nUPSnet-M R50 | 43.0 79.1 52.8] 48.9 79.7 59.7 | 34.1 78.2 42.3 | 34.3\nPanopticFPN++ R101 | 44.1 79.5 53.3]51.0 83.2 60.6 | 33.6 74.0 42.1 |39.7\nDETR R50 | 43.4 79.3 53.8] 48.2 79.8 59.5 | 36.3 78.5 45.3 | 31.1\nDETR-DC5 R50 | 44.6 79.8 55.0] 49.4 80.5 60.6 |37.3 78.7 46.5 | 31.9\nDETR-R101 R101 |45.1 79.9 55.5] 50.5 80.9 46.0 | 33.0\n\n61.7 | 37.0 78.5\n", "vlm_text": "The table presents the performance comparison of different models on various metrics related to image segmentation tasks. Here's a breakdown of the columns in the table:\n\n1. **Model**: The name of the segmentation model being evaluated.\n2. **Backbone**: The neural network architecture used as the backbone for each model.\n3. **PQ**: Panoptic Quality, a metric that combines both segmentation quality and recognition quality across all segments.\n4. **SQ**: Segmentation Quality, assessing the quality of the segmentation.\n5. **RQ**: Recognition Quality, measuring the quality of object recognition.\n6. **PQ^th**: Panoptic Quality for 'thing' classes.\n7. **SQ^th**: Segmentation Quality for 'thing' classes.\n8. **RQ^th**: Recognition Quality for 'thing' classes.\n9. **PQ^st**: Panoptic Quality for 'stuff' classes.\n10. **SQ^st**: Segmentation Quality for 'stuff' classes.\n11. **RQ^st**: Recognition Quality for 'stuff' classes.\n12. **AP**: Average Precision, a common metric used to summarize the precision-recall curve, typically used in object detection.\n\nEach row provides the metrics for a particular model and backbone combination. The metrics indicate how well each model performs in terms of both segmentation and recognition in a panoptic segmentation task, distinguishing between 'thing' classes (countable objects like cars and people) and 'stuff' classes (background materials like grass and sky)."}
{"layout": 95, "type": "text", "text": "To predict the ﬁnal panoptic segmentation we simply use an argmax over the mask scores at each pixel, and assign the corresponding categories to the resulting masks. This procedure guarantees that the ﬁnal masks have no overlaps and, therefore, DETR does not require a heuristic [ 19 ] that is often used to align diﬀerent masks. ", "page_idx": 15, "bbox": [34, 211.48602294921875, 379, 269.2685852050781], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 96, "type": "text", "text": "Training details.  We train DETR, DETR-DC5 and DETR-R101 models fol- lowing the recipe for bounding box detection to predict boxes around stuﬀand things classes in COCO dataset. The new mask head is trained for 25 epochs (see supplementary for details). During inference we ﬁrst ﬁlter out the detection with a conﬁdence below   $85\\%$  , then compute the per-pixel argmax to determine in which mask each pixel belongs. We then collapse diﬀerent mask predictions of the same stu category in one, and ﬁlter the empty ones (less than 4 pixels). ", "page_idx": 15, "bbox": [34, 296.9169616699219, 379, 378.61053466796875], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 97, "type": "text", "text": "Main results.  Qualitative results are shown in Figure  9 . In table  5  we compare our uniﬁed panoptic seg me nation approach with several established methods that treat things and stu di e rent ly. We report the Panoptic Quality (PQ) and the break-down on things (PQ  $^\\mathrm{th}$  ) and stuﬀ(PQ  $^\\mathrm{st}$  ). We also report the mask AP (computed on the things classes), before any panoptic post-treatment (in our case, before taking the pixel-wise argmax). We show that DETR outper- forms published results on COCO-val 2017, as well as our strong Pan optic FP N baseline (trained with same data-augmentation as DETR, for fair comparison). The result break-down shows that DETR is especially dominant on stu classes, and we hypothesize that the global reasoning allowed by the encoder attention is the key element to this result. For things class, despite a severe deﬁcit of up to 8 mAP compared to the baselines on the mask AP computation, DETR obtains competitive PQ  $^\\mathrm{th}$  . We also evaluated our method on the test set of the COCO dataset, and obtained 46 PQ. We hope that our approach will inspire the exploration of fully uniﬁed models for panoptic segmentation in future work. ", "page_idx": 15, "bbox": [34, 406.2579040527344, 379, 583.592529296875], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 98, "type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 16, "bbox": [33, 33, 118, 47], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 99, "type": "text", "text": "We presented DETR, a new design for object detection systems based on trans- formers and bipartite matching loss for direct set prediction. The approach achieves comparable results to an optimized Faster R-CNN baseline on the chal- lenging COCO dataset. DETR is straightforward to implement and has a ﬂexible architecture that is easily extensible to panoptic segmentation, with competitive results. In addition, it achieves sign i cant ly better performance on large objects than Faster R-CNN, likely thanks to the processing of global information per- formed by the self-attention. ", "page_idx": 16, "bbox": [34, 59.66299819946289, 379, 153.3115692138672], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 100, "type": "text", "text": "This new design for detectors also comes with new challenges, in particular regarding training, optimization and performances on small objects. Current detectors required several years of improvements to cope with similar issues, and we expect future work to successfully address them for DETR. ", "page_idx": 16, "bbox": [34, 155.303955078125, 379, 201.1325225830078], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 101, "type": "text", "text": "6 Acknowledgements ", "text_level": 1, "page_idx": 16, "bbox": [33, 220, 165, 232], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 102, "type": "text", "text": "We thank Sainbayar Sukhbaatar, Piotr Bojanowski, Natalia Neverova, David Lopez-Paz, Guillaume Lample, Danielle Rothermel, Kaiming He, Ross Girshick, Xinlei Chen and the whole Facebook AI Research Paris team for discussions and advices without which this work would not be possible.\n\n ", "page_idx": 16, "bbox": [34, 245.45394897460938, 379, 291.2815246582031], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 103, "type": "text", "text": "References ", "text_level": 1, "page_idx": 16, "bbox": [34, 309, 96, 323], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 104, "type": "text", "text": "1. Al-Rfou, R., Choe, D., Constant, N., Guo, M., Jones, L.: Character-level language modeling with deeper self-attention. In: AAAI Conference on Artiﬁcial Intelligence (2019)\n\n 2. Bahdanau, D., Cho, K., Bengio, Y.: Neural machine translation by jointly learning to align and translate. In: ICLR (2015)\n\n 3. Bello, I., Zoph, B., Vaswani, A., Shlens, J., Le, Q.V.: Attention augmented convo- lutional networks. In: ICCV (2019)\n\n 4. Bodla, N., Singh, B., Chellappa, R., Davis, L.S.: Soft-NMS improving object detection with one line of code. In: ICCV (2017)\n\n 5. Cai, Z., Va sconce los, N.: Cascade R-CNN: High quality object detection and in- stance segmentation. PAMI (2019)\n\n 6. Chan, W., Saharia, C., Hinton, G., Norouzi, M., Jaitly, N.: Imputer: Sequence modelling via imputation and dynamic programming. arXiv:2002.08926 (2020)\n\n 7. Cordonnier, J.B., Loukas, A., Jaggi, M.: On the relationship between self-attention and convolutional layers. In: ICLR (2020)\n\n 8. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: BERT: Pre-training of deep bidirectional transformers for language understanding. In: NAACL-HLT (2019)\n\n 9. Erhan, D., Szegedy, C., Toshev, A., Anguelov, D.: Scalable object detection using deep neural networks. In: CVPR (2014)\n\n 10. Ghaz vi nine j ad, M., Levy, O., Liu, Y., Z ett le moyer, L.: Mask-predict: Parallel de- coding of conditional masked language models. arXiv:1904.09324 (2019)\n\n 11. Glorot, X., Bengio, Y.: Understanding the diﬃculty of training deep feed forward neural networks. In: AISTATS (2010) ", "page_idx": 16, "bbox": [34, 335.16510009765625, 379, 583.3434448242188], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 105, "type": "text", "text": "12. Gu, J., Bradbury, J., Xiong, C., Li, V.O., Socher, R.: Non-auto regressive neural machine translation. In: ICLR (2018)\n\n 13. He, K., Girshick, R., Doll´ ar, P.: Rethinking imagenet pre-training. In: ICCV (2019)\n\n 14. He, K., Gkioxari, G., Doll´ ar, P., Girshick, R.B.: Mask R-CNN. In: ICCV (2017)\n\n 15. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: CVPR (2016)\n\n 16. Hosang, J.H., Benenson, R., Schiele, B.: Learning non-maximum suppression. In: CVPR (2017)\n\n 17. Hu, H., Gu, J., Zhang, Z., Dai, J., Wei, Y.: Relation networks for object detection. In: CVPR (2018)\n\n 18. Kirillov, A., Girshick, R., He, K., Doll´ ar, P.: Panoptic feature pyramid networks. In: CVPR (2019)\n\n 19. Kirillov, A., He, K., Girshick, R., Rother, C., Dollar, P.: Panoptic segmentation. In: CVPR (2019)\n\n 20. Kuhn, H.W.: The hungarian method for the assignment problem (1955)\n\n 21. Li, Y., Qi, H., Dai, J., Ji, X., Wei, Y.: Fully convolutional instance-aware semantic segmentation. In: CVPR (2017)\n\n 22. Lin, T.Y., Doll´ ar, P., Girshick, R., He, K., Hariharan, B., Belongie, S.: Feature pyramid networks for object detection. In: CVPR (2017)\n\n 23. Lin, T.Y., Goyal, P., Girshick, R.B., He, K., Doll´ ar, P.: Focal loss for dense object detection. In: ICCV (2017)\n\n 24. Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll´ ar, P., Zitnick, C.L.: Microsoft COCO: Common objects in context. In: ECCV (2014)\n\n 25. Liu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S.E., Fu, C.Y., Berg, A.C.: Ssd: Single shot multibox detector. In: ECCV (2016)\n\n 26. Loshchilov, I., Hutter, F.: Decoupled weight decay regular iz ation. In: ICLR (2017)\n\n 27. L¨ uscher, C., Beck, E., Irie, K., Kitza, M., Michel, W., Zeyer, A., Schl¨ uter, R., Ney, H.: Rwth asr systems for libri speech: Hybrid vs attention - w/o data augmentation. arXiv:1905.03072 (2019)\n\n 28. Milletari, F., Navab, N., Ahmadi, S.A.: V-net: Fully convolutional neural networks for volumetric medical image segmentation. In: 3DV (2016)\n\n 29. Oord, A.v.d., Li, Y., Babuschkin, I., Simonyan, K., Vinyals, O., Ka vuk cuo g lu, K., Driessche, G.v.d., Lockhart, E., Cobo, L.C., Stimberg, F., et al.: Parallel wavenet: Fast high-ﬁdelity speech synthesis. arXiv:1711.10433 (2017)\n\n 30. Park, E., Berg, A.C.: Learning to decompose for object detection and instance segmentation. arXiv:1511.06449 (2015)\n\n 31. Parmar, N., Vaswani, A., Uszkoreit, J., Kaiser, L., Shazeer, N., Ku, A., Tran, D.: Image transformer. In: ICML (2018)\n\n 32. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chi lam kurt hy, S., Steiner, B., Fang, L., Bai, J., Chintala, S.: Pytorch: An imperative style, high-performance deep learning library. In: NeurIPS (2019)\n\n 33. Pineda, L., Salvador, A., Drozdzal, M., Romero, A.: Elucidating image-to-set pre- diction: An analysis of models, losses and datasets. arXiv:1904.05709 (2019)\n\n 34. Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I.: Language models are unsupervised multitask learners (2019)\n\n 35. Redmon, J., Divvala, S., Girshick, R., Farhadi, A.: You only look once: Uniﬁed, real-time object detection. In: CVPR (2016)\n\n 36. Ren, M., Zemel, R.S.: End-to-end instance segmentation with recurrent attention. In: CVPR (2017) ", "page_idx": 17, "bbox": [34, 37.25316619873047, 379, 583.34326171875], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 106, "type": "text", "text": "37. Ren, S., He, K., Girshick, R.B., Sun, J.: Faster R-CNN: Towards real-time object detection with region proposal networks. PAMI (2015)\n\n 38. Rezatoﬁghi, H., Tsoi, N., Gwak, J., Sadeghian, A., Reid, I., Savarese, S.: General- ized intersection over union. In: CVPR (2019)\n\n 39. Rezatoﬁghi, S.H., Kaskman, R., Motlagh, F.T., Shi, Q., Cremers, D., Leal-Taix´ L., Reid, I.: Deep perm-set net: Learn to predict sets with unknown permutation and cardinality using deep neural networks. arXiv:1805.00613 (2018)\n\n 40. Rezatoﬁghi, S.H., Milan, A., Abbasnejad, E., Dick, A., Reid, I., Kaskman, R., Cremers, D., Leal-Taix, l.: Deepsetnet: Predicting sets with deep neural networks. In: ICCV (2017)\n\n 41. Romera-Paredes, B., Torr, P.H.S.: Recurrent instance segmentation. In: ECCV (2015)\n\n 42. Salvador, A., Bellver, M., Baradad, M., Marqu´ es, F., Torres, J., Gir´ o, X.: Recurrent neural networks for semantic instance segmentation. arXiv:1712.00617 (2017)\n\n 43. Stewart, R.J., Andriluka, M., Ng, A.Y.: End-to-end people detection in crowded scenes. In: CVPR (2015)\n\n 44. Sutskever, I., Vinyals, O., Le, Q.V.: Sequence to sequence learning with neural networks. In: NeurIPS (2014)\n\n 45. Synnaeve, G., Xu, Q., Kahn, J., Grave, E., Li k homan en ko, T., Pratap, V., Sri- ram, A., Lip tch in sky, V., Collobert, R.: End-to-end ASR: from supervised to semi- supervised learning with modern architectures. arXiv:1911.08460 (2019)\n\n 46. Tian, Z., Shen, C., Chen, H., He, T.: FCOS: Fully convolutional one-stage object detection. In: ICCV (2019)\n\n 47. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L., Polosukhin, I.: Attention is all you need. In: NeurIPS (2017)\n\n 48. Vinyals, O., Bengio, S., Kudlur, M.: Order matters: Sequence to sequence for sets. In: ICLR (2016)\n\n 49. Wang, X., Girshick, R.B., Gupta, A., He, K.: Non-local neural networks. In: CVPR (2018)\n\n 50. Wu, Y., Kirillov, A., Massa, F., Lo, W.Y., Girshick, R.: Detectron2.  https:// github.com/facebook research/detectron2  (2019)\n\n 51. Xiong, Y., Liao, R., Zhao, H., Hu, R., Bai, M., Yumer, E., Urtasun, R.: Upsnet: A uniﬁed panoptic segmentation network. In: CVPR (2019)\n\n 52. Zhang, S., Chi, C., Yao, Y., Lei, Z., Li, S.Z.: Bridging the gap between anchor-based and anchor-free detection via adaptive training sample selection. arXiv:1912.02424 (2019)\n\n 53. Zhou, X., Wang, D., Kr¨ ahenb¨ uhl, P.: Objects as points. arXiv:1904.07850 (2019) ", "page_idx": 18, "bbox": [34, 37.25316619873047, 379, 440.7402038574219], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 107, "type": "text", "text": "A Appendix ", "text_level": 1, "page_idx": 19, "bbox": [33, 33, 116, 48], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 108, "type": "text", "text": "A.1 Preliminaries: Multi-head attention layers ", "text_level": 1, "page_idx": 19, "bbox": [34, 57, 274, 69], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 109, "type": "text", "text": "Since our model is based on the Transformer architecture, we remind here the general form of attention mechanisms we use for exhaust iv it y. The attention mechanism follows [ 47 ], except for the details of positional encodings (see Equa- tion  8 ) that follows [ 7 ]. ", "page_idx": 19, "bbox": [33, 77.6840591430664, 380, 123.51264953613281], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 110, "type": "text", "text": "Multi-head  The general form of  multi-head attention  with    $M$   heads of dimen- sion    $d$   is a function with the following signature (using    $\\begin{array}{r}{d^{\\prime}\\ =\\ \\frac{d}{M}}\\end{array}$  , and giving matrix/tensors sizes in underbrace) ", "page_idx": 19, "bbox": [33, 130.34304809570312, 380, 164.2156219482422], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 111, "type": "equation", "text": "\n$$\n\\mathrm{mh-attn:}\\,\\underbrace{X_{\\mathrm{q}}}_{d\\times\\,N_{\\mathrm{q}}},\\,\\underbrace{X_{\\mathrm{kv}}}_{d\\times\\,N_{\\mathrm{kv}}},\\,\\underbrace{T}_{M\\times3\\times d^{\\prime}\\times d},\\underbrace{L}_{d\\times d}\\mapsto\\underbrace{\\tilde{X}_{\\mathrm{q}}}_{d\\times\\,N_{\\mathrm{q}}}\n$$\n ", "text_format": "latex", "page_idx": 19, "bbox": [105, 171, 307, 201], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 112, "type": "text", "text": "where    $X_{\\mathrm{q}}$   is the  query sequence  of length    $N_{\\mathrm{q}}$  ,    $X_{\\mathrm{kv}}$   is the  key-value sequence  of length    $N_{\\mathrm{kv}}$   (with the same number of channels    $d$   for simplicity of exposition),    $T$  is the weight tensor to compute the so-called query, key and value embeddings, and    $L$   is a projection matrix. The output is the same size as the query sequence. To ﬁx the vocabulary before giving details, multi-head  self- attention (mh-s-attn) is the special case    $X_{\\mathrm{q}}=X_{\\mathrm{kv}}$  , i.e. ", "page_idx": 19, "bbox": [33, 210.21395874023438, 380, 281], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 113, "type": "equation", "text": "\n$$\n\\operatorname{mh-s-attn}(X,T,L)=\\operatorname{mh-attn}(X,X,T,L)\\,.\n$$\n ", "text_format": "latex", "page_idx": 19, "bbox": [113, 289, 300, 301], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 114, "type": "text", "text": "The multi-head attention is simply the concatenation of    $M$   single attention heads followed by a projection with    $L$  . The common practice [ 47 ] is to use residual connections, dropout and layer normalization. In other words, denoting  $\\ddot{X}_{\\mathrm{q}}\\,=$  mh-attn  $(X_{\\mathrm{q}},X_{\\mathrm{kv}},T,L)$   and  $\\bar{\\bar{X}}^{\\left(q\\right)}$    the concatenation of attention heads, we have ", "page_idx": 19, "bbox": [33, 310.5028991699219, 380, 359], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 115, "type": "equation", "text": "\n$$\n\\begin{array}{r l}&{X_{\\mathrm{q}}^{\\prime}=[\\mathrm{att}(X_{\\mathrm{q}},X_{\\mathrm{kv}},T_{1});...;\\mathrm{att}(X_{\\mathrm{q}},X_{\\mathrm{kv}},T_{M})]}\\\\ &{\\tilde{X}_{\\mathrm{q}}=\\mathrm{layernorm}\\big(X_{\\mathrm{q}}+\\mathrm{droplet}(L X_{\\mathrm{q}}^{\\prime})\\big)\\,,}\\end{array}\n$$\n ", "text_format": "latex", "page_idx": 19, "bbox": [103, 365, 309, 399], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 116, "type": "text", "text": "where   $[;]$   denotes concatenation on the channel axis. ", "page_idx": 19, "bbox": [33, 405.2919006347656, 261.0835266113281, 416], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 117, "type": "text", "text": "Single head  An attention head with weight tensor    $T^{\\prime}\\in\\mathbb{R}^{3\\times d^{\\prime}\\times d}$  , denoted by  $\\mathrm{atan}(X_{\\mathrm{q}},X_{\\mathrm{kv}},T^{\\prime})$  , depends on additional positional encoding    $P_{\\mathrm{q}}\\in\\mathbb{R}^{d\\times N_{\\mathrm{q}}}$    and  $P_{\\mathrm{kv}}\\in\\mathbb{R}^{d\\times N_{\\mathrm{kv}}}$   ∈ . It starts by computing so-called query, key and value embeddings after adding the query and key positional encodings [ 7 ]: ", "page_idx": 19, "bbox": [33, 420, 380, 467.91351318359375], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 118, "type": "equation", "text": "\n$$\n[Q;K;V]=[T_{1}^{\\prime}(X_{\\mathrm{q}}+P_{\\mathrm{q}});T_{2}^{\\prime}(X_{\\mathrm{kv}}+P_{\\mathrm{kv}});T_{3}^{\\prime}X_{\\mathrm{kv}}]\n$$\n ", "text_format": "latex", "page_idx": 19, "bbox": [100, 476, 313, 489], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 119, "type": "text", "text": "where    $T^{\\prime}$    is the concatenation of    $T_{1}^{\\prime},T_{2}^{\\prime},T_{3}^{\\prime}$   ′  ′ . The  attention weights    $\\alpha$   are then computed based on the softmax of dot products between queries and keys, so that each element of the query sequence attends to all elements of the key-value sequence (  $i$   is a query index and    $j$   a key-value index): ", "page_idx": 19, "bbox": [33, 494.848876953125, 380, 545], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 120, "type": "equation", "text": "\n$$\n\\alpha_{i,j}=\\frac{e^{\\frac{1}{\\sqrt{d^{\\prime}}}Q_{i}^{T}K_{j}}}{Z_{i}}\\;\\;\\mathrm{where}\\;Z_{i}=\\sum_{j=1}^{N_{\\mathrm{kv}}}e^{\\frac{1}{\\sqrt{d^{\\prime}}}Q_{i}^{T}K_{j}}\\,.\n$$\n ", "text_format": "latex", "page_idx": 19, "bbox": [109, 552, 304, 587], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 121, "type": "text", "text": "In our case, the positional encodings may be learnt or ﬁxed, but are shared across all attention layers for a given query/key-value sequence, so we do not explicitly write them as parameters of the attention. We give more details on their exact value when describing the encoder and the decoder. The ﬁnal output is the aggregation of values weighted by attention weights: The    $i$  -th row is given by   $\\begin{array}{r}{\\operatorname{atan}_{i}(X_{\\mathrm{q}},X_{\\mathrm{kv}},T^{\\prime})=\\sum_{j=1}^{N_{\\mathrm{kv}}}\\alpha_{i,j}V_{j}}\\end{array}$  . ", "page_idx": 20, "bbox": [33, 36.50601577758789, 379, 109], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 122, "type": "text", "text": "Feed-forward network (FFN) layers  The original transformer alternates multi-head attention and so-called FFN layers [ 47 ], which are eﬀectively multi- layer 1x1 convolutions, which have    $M d$   input and output channels in our case. The FFN we consider is composed of two-layers of 1x1 convolutions with ReLU activation s. There is also a residual connection/dropout/layernorm after the two layers, similarly to equation  6 . ", "page_idx": 20, "bbox": [33, 113.18805694580078, 379, 182.9265899658203], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 123, "type": "text", "text": "A.2 Losses ", "text_level": 1, "page_idx": 20, "bbox": [33, 201, 95, 212], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 124, "type": "text", "text": "For completeness, we present in detail the losses used in our approach. All losses are normalized by the number of objects inside the batch. Extra care must be taken for distributed training: since each GPU receives a sub-batch, it is not suﬃcient to normalize by the number of objects in the local batch, since in general the sub-batches are not balanced across GPUs. Instead, it is important to normalize by the total number of objects in all sub-batches. ", "page_idx": 20, "bbox": [33, 222.20596313476562, 379, 291.94451904296875], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 125, "type": "text", "text": "Box loss  Similarly to [ 41 , 36 ], we use a soft version of Intersection over Union in our loss, together with a    $\\ell_{1}$   loss on   $\\hat{b}$  : ", "page_idx": 20, "bbox": [33, 298.8879089355469, 379, 323.3795166015625], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 126, "type": "equation", "text": "\n$$\n\\mathcal{L}_{\\mathrm{box}}\\big(b_{\\sigma(i)},\\hat{b}_{i}\\big)=\\lambda_{\\mathrm{ion}}\\mathcal{L}_{\\mathrm{ion}}\\big(b_{\\sigma(i)},\\hat{b}_{i}\\big)+\\lambda_{\\mathrm{L1}}||b_{\\sigma(i)}-\\hat{b}_{i}||_{1}\\,,\n$$\n ", "text_format": "latex", "page_idx": 20, "bbox": [90, 330, 322, 345], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 127, "type": "text", "text": "where    $\\lambda_{\\mathrm{ion}},\\lambda_{\\mathrm{L1}}\\in\\mathbb{R}$   are hyper parameters and    $\\mathcal{L}_{\\mathrm{ion}}(\\cdot)$   is the generalized IoU [ 38 ]: ", "page_idx": 20, "bbox": [33, 354.79486083984375, 379, 372.0799560546875], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 128, "type": "equation", "text": "\n$$\n\\mathcal{L}_{\\mathrm{ion}}(b_{\\sigma(i)},\\hat{b}_{i})=1-\\left(\\frac{|b_{\\sigma(i)}\\cap\\hat{b}_{i}|}{|b_{\\sigma(i)}\\cup\\hat{b}_{i}|}-\\frac{|B(b_{\\sigma(i)},\\hat{b}_{i})\\setminus b_{\\sigma(i)}\\cup\\hat{b}_{i}|}{|B(b_{\\sigma(i)},\\hat{b}_{i})|}\\right).\n$$\n ", "text_format": "latex", "page_idx": 20, "bbox": [73, 374, 340, 405], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 129, "type": "text", "text": "| . |  means “area”, and the union and intersection of box coordinates are used as shorthands for the boxes themselves. The areas of unions or intersections are computed by min  /  max of the linear functions of    $b_{\\sigma(i)}$   and   $\\hat{b}_{i}$  i , which makes the loss suﬃciently well-behaved for stochastic gradients.    $B(b_{\\sigma(i)},\\hat{b}_{i})$  ) means the largest box containing    $b_{\\sigma(i)},\\hat{b}_{i}$   (the areas involving    $B$   are also computed based on min  /  max of linear functions of the box coordinates). ", "page_idx": 20, "bbox": [33, 414.93096923828125, 379, 489.1806335449219], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 130, "type": "text", "text": "DICE/F-1 loss [ 28 ]  The DICE coeﬃcient is closely related to the Intersection over Union. If we denote by m  the raw mask logits prediction of the model, and  $m$   the binary target mask, the loss is deﬁned as: ", "page_idx": 20, "bbox": [33, 496.1240234375, 379, 529.9966430664062], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 131, "type": "equation", "text": "\n$$\n\\mathcal{L}_{\\mathrm{DICE}}(m,\\hat{m})=1-\\frac{2m\\sigma(\\hat{m})+1}{\\sigma(\\hat{m})+m+1}\n$$\n ", "text_format": "latex", "page_idx": 20, "bbox": [131, 539, 282, 565], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 132, "type": "text", "text": "where    $\\sigma$   is the sigmoid function. This loss is normalized by the number of objects. ", "page_idx": 20, "bbox": [33, 573.6300659179688, 379, 583.5926513671875], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 133, "type": "text", "text": "A.3 Detailed architecture ", "text_level": 1, "page_idx": 21, "bbox": [34, 34, 169, 46], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 134, "type": "text", "text": "The detailed description of the transformer used in DETR, with positional en- codings passed at every attention layer, is given in Fig.  10 . Image features from the CNN backbone are passed through the transformer encoder, together with spatial positional encoding that are added to queries and keys at every multi- head self-attention layer. Then, the decoder receives queries (initially set to zero), output positional encoding (object queries), and encoder memory, and produces the ﬁnal set of predicted class labels and bounding boxes through multiple multi- head self-attention and decoder-encoder attention. The ﬁrst self-attention layer in the ﬁrst decoder layer can be skipped. ", "page_idx": 21, "bbox": [34, 59.38803482055664, 382, 164.9916229248047], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 135, "type": "image", "page_idx": 21, "img_path": "layout_images/2005.12872v3_13.jpg", "img_caption": "Fig. 10: Architecture of DETR’s transformer. Please, see Section  A.3  for details. ", "bbox": [45, 187, 368, 468], "page_size": [413.8580017089844, 615.1179809570312], "ocr_text": "Encoder\n\nAdd & Norm\n—\n\nImage features\n\nSpatial positional\n\nencoding\n\nBounding Box\n\nClass\n\nDecoder\n\nAdd & Norm\n\nMulti-Head Self-Attention\n\nTa\nRi\n\nObject queries\n", "vlm_text": "The image is a schematic diagram of the architecture of DETR's (DEtection TRansformers) transformer. It is divided into two main parts: the Encoder and the Decoder.\n\n### Encoder:\n- **Components**: It includes several layers that are repeated \\(N\\) times, consisting of:\n  - **Add & Norm**: This refers to residual connections followed by layer normalization.\n  - **FFN (Feed Forward Network)**: Provides additional transformations.\n  - **Multi-Head Self-Attention**: Enables the encoder to attend to different parts of the input sequence.\n\n### Decoder:\n- **Components**: It includes layers repeated \\(M\\) times, consisting of:\n  - **Add & Norm**: Similar to the encoder.\n  - **FFN**: Part of the transformation process.\n  - **Multi-Head Attention**: Allows attention over the encoder's output.\n  - **Multi-Head Self-Attention**: Similar to the encoder, but in the context of the decoder’s processing.\n\n### Other Elements:\n- **Image Features**: The input features of the image.\n- **Spatial Positional Encoding**: Adds information about the position of elements in the image.\n- **Object Queries**: Queries for each object to be detected.\n\n### Outputs:\n- **Class**: Predicted class labels.\n- **Bounding Box**: Predicted bounding boxes for detected objects.\n\nThis diagram visually represents the flow of data and processes within the DETR framework."}
{"layout": 136, "type": "text", "text": "Computational complexity  Every self-attention in the encoder has complex- ity    $\\mathcal{O}(d^{2}H W+d(H W)^{2})\\colon\\mathcal{O}(d^{\\prime}d)$   is the cost of computing a single query/key/value   embeddings (and  $M d^{\\prime}=d$  ), while    $\\mathcal{O}(d^{\\prime}(H W)^{2})$   is the cost of computing the at- tention weights for one head. Other computations are negligible. In the decoder, each self-attention is in    $\\mathcal{O}(d^{2}N\\!+\\!d N^{2})$  , and cross-attention between encoder and decoder is in    $\\mathcal{O}(d^{2}(N+H W)+d N H W)$  , which is much lower than the encoder since    $N\\ll H\\,W$   in practice. ", "page_idx": 21, "bbox": [34, 501.8990173339844, 382, 583.5926513671875], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 137, "type": "text", "text": "FLOPS computation  Given that the FLOPS for Faster R-CNN depends on the number of proposals in the image, we report the average number of FLOPS for the ﬁrst 100 images in the COCO 2017 validation set. We compute the FLOPS with the tool  flop count operators  from Detectron2 [ 50 ]. We use it without modi cations for Detectron2 models, and extend it to take batch matrix multiply ( bmm ) into account for DETR models. ", "page_idx": 22, "bbox": [33, 36.50601577758789, 379, 106.25269317626953], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 138, "type": "text", "text": "A.4 Training hyper parameters ", "text_level": 1, "page_idx": 22, "bbox": [33, 120, 194, 133], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 139, "type": "text", "text": "We train DETR using AdamW [ 26 ] with improved weight decay handling, set to  $10^{-4}$  . We also apply gradient clipping, with a maximal gradient norm of 0 . 1. The backbone and the transformers are treated slightly diﬀerently, we now discuss the details for both. ", "page_idx": 22, "bbox": [33, 138.2530517578125, 379, 184.0816192626953], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 140, "type": "text", "text": "Backbone  ImageNet pretrained backbone ResNet-50 is imported from Torchvi- sion, discarding the last class i cation layer. Backbone batch normalization weights and statistics are frozen during training, following widely adopted practice in ob- ject detection. We ﬁne-tune the backbone using learning rate of   $10^{-5}$  . We observe that having the backbone learning rate roughly an order of magnitude smaller than the rest of the network is important to stabilize training, especially in the ﬁrst few epochs. ", "page_idx": 22, "bbox": [33, 190.63302612304688, 384.40069580078125, 272.3265686035156], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 141, "type": "text", "text": "Transformer  We train the transformer with a learning rate of   $10^{-4}$  . Additive dropout of 0 . 1 is applied after every multi-head attention and FFN before layer normalization. The weights are randomly initialized with Xavier initialization. ", "page_idx": 22, "bbox": [33, 278, 379, 312.7515563964844], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 142, "type": "text", "text": "Losses  We use linear combination of    $\\ell_{1}$   and GIoU losses for bounding box re- gression with  $\\lambda_{\\mathrm{L1}}=5$   and    $\\lambda_{\\mathrm{ionu}}=2$   weights respectively. All models were trained with    $N=100$   decoder query slots. ", "page_idx": 22, "bbox": [33, 319.3029479980469, 379, 353.175537109375], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 143, "type": "text", "text": "Baseline  Our enhanced Faster-RCNN  $^+$   baselines use GIoU [ 38 ] loss along with the standard    $\\ell_{1}$   loss for bounding box regression. We performed a grid search to ﬁnd the best weights for the losses and the ﬁnal models use only GIoU loss with weights 20 and 1 for box and proposal regression tasks respectively. For the baselines we adopt the same data augmentation as used in DETR and train it with 9  $\\times$   schedule (approximately 109 epochs). All other settings are identical to the same models in the Detectron2 model zoo [ 50 ]. ", "page_idx": 22, "bbox": [33, 359.7279357910156, 379, 441.4215393066406], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 144, "type": "text", "text": "Spatial positional encoding  Encoder activation s are associated with corre- sponding spatial positions of image features. In our model we use a ﬁxed absolute encoding to represent these spatial positions. We adopt a generalization of the original Transformer [ 47 ] encoding to the 2D case [ 31 ]. Spec i call y, for both spatial coordinates of each embedding we independently use    $\\frac{d}{2}$    sine and cosine functions with diﬀerent frequencies. We then concatenate them to get the ﬁnal  $d$   channel positional encoding. ", "page_idx": 22, "bbox": [33, 447.9729309082031, 379, 529.6666259765625], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 145, "type": "text", "text": "A.5 Additional results ", "text_level": 1, "page_idx": 22, "bbox": [33, 545, 153, 557], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 146, "type": "text", "text": "Some extra qualitative results for the panoptic prediction of the DETR-R101 model are shown in Fig. 11 . ", "page_idx": 22, "bbox": [33, 561.675048828125, 379, 583.5926513671875], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 147, "type": "image", "page_idx": 23, "img_path": "layout_images/2005.12872v3_14.jpg", "img_caption": "(a) Failure case with overlapping objects. Pan optic FP N misses one plane entirely, while DETR fails to accurately segment 3 of them. ", "bbox": [33, 33, 380, 134], "page_size": [413.8580017089844, 615.1179809570312], "ocr_text": "", "vlm_text": "The image shows three segments, each depicting the same scene with overlapping airplanes. The caption indicates this is a failure case:\n\n1. **Left segment**: Panoptic FP N misses one plane entirely.\n2. **Middle segment**: Shows segmentation attempts with several airplanes overlaid.\n3. **Right segment**: DETR fails to accurately segment three of the airplanes. The segment labeled \"sky\" overlaps with part of the airplanes.\n\nThis illustrates challenges in object detection and segmentation when multiple objects overlap."}
{"layout": 148, "type": "image", "page_idx": 23, "img_path": "layout_images/2005.12872v3_15.jpg", "img_caption": "(b)  Things  masks are predicted at full resolution, which allows sharper boundaries than Pan optic FP N ", "bbox": [33, 135, 380, 238], "page_size": [413.8580017089844, 615.1179809570312], "ocr_text": "", "vlm_text": "The image consists of three panels showing a semantic segmentation of a scene with an airplane. Different parts of the scene are masked with distinct colors to label them. In each panel, the airplane is labeled distinctly, with the background components such as the road, grass, and sky also labeled.\n\n1. **Left panel**: The airplane is in green with labels indicating \"thing\" or object.\n2. **Middle panel**: The airplane is in cyan.\n3. **Right panel**: The airplane is in blue, and the sky, road, and grass are labeled.\n\nThe caption suggests that the \"things\" masks are predicted at full resolution for sharper boundaries compared to another method mentioned (Panoptic FPN)."}
{"layout": 149, "type": "text", "text": "Fig. 11: Comparison of panoptic predictions. From left to right: Ground truth, Panop- ticFPN with ResNet 101, DETR with ResNet 101 ", "page_idx": 23, "bbox": [34, 249.36715698242188, 379, 269.2925720214844], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 150, "type": "text", "text": "Increasing the number of instances  By design, DETR cannot predict more objects than it has query slots, i.e. 100 in our experiments. In this section, we analyze the behavior of DETR when approaching this limit. We select a canonical square image of a given class, repeat it on a   $10\\times10$   grid, and compute the percentage of instances that are missed by the model. To test the model with less than 100 instances, we randomly mask some of the cells. This ensures that the absolute size of the objects is the same no matter how many are visible. To account for the randomness in the masking, we repeat the experiment 100 times with diﬀerent masks. The results are shown in Fig. 12 . The behavior is similar across classes, and while the model detects all instances when up to 50 are visible, it then starts saturating and misses more and more instances. Notably, when the image contains all 100 instances, the model only detects 30 on average, which is less than if the image contains only 50 instances that are all detected. The counter-intuitive behavior of the model is likely because the images and the detections are far from the training distribution. ", "page_idx": 23, "bbox": [34, 297.6100158691406, 379, 474.944580078125], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 151, "type": "text", "text": "Note that this test is a test of generalization out-of-distribution by design, since there are very few example images with a lot of instances of a single class. It is diﬃcult to disentangle, from the experiment, two types of out-of-domain generalization: the image itself vs the number of object per class. But since few to no COCO images contain only a lot of objects of the same class, this type of experiment represents our best eﬀort to understand whether query objects overﬁt the label and position distribution of the dataset. Overall, the experiments suggests that the model does not overﬁt on these distributions since it yields near-perfect detections up to 50 objects. ", "page_idx": 23, "bbox": [34, 477.9889831542969, 379, 583.5925903320312], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 152, "type": "image", "page_idx": 24, "img_path": "layout_images/2005.12872v3_16.jpg", "img_caption": "Fig. 12: Analysis of the number of instances of various classes missed by DETR de- pending on how many are present in the image. We report the mean and the standard deviation. As the number of instances gets close to 100, DETR starts saturating and misses more and more objects ", "bbox": [33, 31, 381, 254], "page_size": [413.8580017089844, 615.1179809570312], "ocr_text": "% of missed instances\n\n70\n60\n50\n40\n30\n20\n10\n\n20 40 60 80 100\nNumber of visible instances\n\n", "vlm_text": "The image is a line graph showing the percentage of missed instances by DETR (a model for object detection) for three different classes: dog, person, and apple. The x-axis represents the number of visible instances, and the y-axis represents the percentage of missed instances. The graph shows that as the number of visible instances approaches 100, the percentage of missed instances increases, indicating that DETR starts to struggle with detecting objects accurately when they are numerous. Each class has a different color line, as indicated in the legend: blue for dog, orange for person, and green for apple, with shaded areas representing the standard deviation."}
{"layout": 153, "type": "text", "text": "A.6 PyTorch inference code ", "text_level": 1, "page_idx": 24, "bbox": [34, 277, 181, 289], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 154, "type": "text", "text": "To demonstrate the simplicity of the approach, we include inference code with PyTorch and Torch vision libraries in Listing  1 . The code runs with Python   $^{3.6+}$  , PyTorch 1.4 and Torch vision 0.5. Note that it does not support batching, hence it is suitable only for inference or training with Distributed Data Parallel with one image per GPU. Also note that for clarity, this code uses learnt positional encodings in the encoder instead of ﬁxed, and positional encodings are added to the input only instead of at each transformer layer. Making these changes requires going beyond PyTorch implementation of transformers, which hampers readability. The entire code to reproduce the experiments will be made available before the conference. ", "page_idx": 24, "bbox": [34, 299.14898681640625, 379, 416.70758056640625], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 155, "type": "text", "text": "import  torch from  torch  import  nn from  torch vision.models  import  resnet50 ", "page_idx": 25, "bbox": [34, 126.84197235107422, 178.4295196533203, 149.7557830810547], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 156, "type": "text", "text": "", "page_idx": 25, "bbox": [33, 158, 114, 162.75], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 157, "type": "text", "text": "def  __init__ ( self , num classes, hidden_dim, nheads, num encoder layers, num decoder layers): super () . __init__ () # We take only convolutional layers from ResNet-50 model self . backbone  $=$   nn . Sequential( \\* list (resnet50(pretrained = True ) . children())[: -2 ]) self . conv    $=$   nn . Conv2d( 2048 , hidden_dim,  1 ) self . transformer  $=$   nn . Transformer(hidden_dim, nheads, num encoder layers, num decoder layers) self . linear class  $=$   nn . Linear(hidden_dim, num classes    $+\\_1$  ) self . linear b box  $=$   nn . Linear(hidden_dim,  4 ) self . query_pos  $=$   nn . Parameter(torch . rand( 100 , hidden_dim)) self . row_embed  $=$   nn . Parameter(torch . rand( 50 , hidden_dim  // 2 )) self . col_embed  $=$   nn . Parameter(torch . rand( 50 , hidden_dim  // 2 )) def  forward ( self , inputs):  $\\tt{x}={}$   self . backbone(inputs)  $\\texttt{h}=$   self . conv  $\\mathbf{\\rho}(\\mathbf{x})$  H,   ${\\tt W}\\ =\\ {\\tt h}$  . shape[ -2 :] pos    $=$   torch . cat([ self . col_embed[:W] . unsqueeze( 0 ) . repeat(H,  1 ,  1 ), self.row_embed[:H].unsqueeze(1).repeat(1, W, 1),], dim $\\scriptstyle{=-1}$ ).flatten(0, 1).unsqueeze(1)h  $=$   self . transformer(pos  $^+$   h . flatten( 2 ) . permute( 2 ,  0 ,  1 ), self . query_pos . unsqueeze( 1 )) return self . linear class(h),  self . linear b box(h) . sigmoid() ", "page_idx": 25, "bbox": [48, 174.66299438476562, 356.18536376953125, 372.9188232421875], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 158, "type": "text", "text": "inputs  $=$   torch . randn( 1 ,  3 ,  800 ,  1200 ) ", "page_idx": 25, "bbox": [34, 397.8260498046875, 171.03030395507812, 404.7998352050781], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 159, "type": "text", "text": "logits, bboxes  $=$   detr(inputs) ", "page_idx": 25, "bbox": [34, 405.7960205078125, 141.40554809570312, 412.7698059082031], "page_size": [413.8580017089844, 615.1179809570312]}
{"layout": 160, "type": "text", "text": "Listing 1: DETR PyTorch inference code. For clarity it uses learnt positional encod- ings in the encoder instead of ﬁxed, and positional encodings are added to the input only instead of at each transformer layer. Making these changes requires going beyond PyTorch implementation of transformers, which hampers readability. The entire code to reproduce the experiments will be made available before the conference. ", "page_idx": 25, "bbox": [34, 433.45318603515625, 379, 486.2555847167969], "page_size": [413.8580017089844, 615.1179809570312]}
