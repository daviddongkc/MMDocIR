{"page": 0, "image_path": "doc_images/PI_2018.11.19_algorithms_FINAL_0.jpg", "ocr_text": "Pew Research Center SK\n\nFOR RELEASE NOVEMBER 16, 2018\n\nPublic Attitudes Toward\nComputer Algorithms\n\nAmericans express broad concerns over the fairness and\neffectiveness of computer programs making important decisions\nin people’s lives\n\nBY Aaron Smith\n\nFOR MEDIA OR OTHER INQUIRIES:\n\nAaron Smith,\nHaley Nolan,\n\nRECOMMENDED CITATION\n", "vlm_text": "\n  \nBY  Aaron Smith  "}
{"page": 1, "image_path": "doc_images/PI_2018.11.19_algorithms_FINAL_1.jpg", "ocr_text": "About Pew Research Center\n\nPew Research Center is a nonpartisan fact tank that informs the public about the issues, attitudes\nand trends shaping America and the world. It does not take policy positions. It conducts public\nopinion polling, demographic research, content analysis and other data-driven social science\nresearch. The Center studies U.S. politics and policy; journalism and media; internet, science and\ntechnology; religion and public life; Hispanic trends; global attitudes and trends; and U.S. social\nand demographic trends. All of the Center’s reports are available at www.pewresearch.org. Pew\nResearch Center is a subsidiary of The Pew Charitable Trusts, its primary funder.\n\n© Pew Research Center 2018\n\nwww.pewresearch.org\n", "vlm_text": "About Pew Research Center   \nPew Research Center is a nonpartisan fact tank that informs the public about the issues, attitudes  and trends shaping America and the world. It does not take policy positions. It conducts public  opinion polling, demographic research, content analysis and other data-driven social science  research. The Center studies U.S. politics and policy; journalism and media; internet, science and  technology; religion and public life; Hispanic trends; global attitudes and trends; and U.S. social  and demographic trends. All of the Center’s reports are available at www.pew research.org. Pew  Research Center is a subsidiary of The Pew Charitable Trusts, its primary funder.   \n $\\copyright$   Pew Research Center 2018  "}
{"page": 2, "image_path": "doc_images/PI_2018.11.19_algorithms_FINAL_2.jpg", "ocr_text": "Public Attitudes Toward Computer\n\nAlgorithms\n\nAmericans express broad concerns over the fairness and\n\neffectiveness of computer programs making\nimportant decisions\n\nAlgorithms are all around us, utilizing massive stores of data\nand complex analytics to make decisions with often significant\nimpacts on humans. They recommend books and movies for us\nto read and watch, surface news stories they think we might find\nrelevant, estimate the likelihood that a tumor is cancerous and\npredict whether someone might be a criminal or a worthwhile\ncredit risk. But despite the growing presence of algorithms in\nmany aspects of daily life, a Pew Research Center survey of U.S.\nadults finds that the public is frequently skeptical of these tools\nwhen used in various real-life situations.\n\nThis skepticism spans several dimensions. At a broad level, 58%\nof Americans feel that computer programs will always reflect\nsome level of human bias — although 40% think these programs\ncan be designed in a way that is bias-free. And in various\ncontexts, the public worries that these tools might violate\nprivacy, fail to capture the nuance of complex situations, or\nsimply put the people they are evaluating in an unfair situation.\nPublic perceptions of algorithmic decision-making are also\noften highly contextual. The survey shows that otherwise similar\ntechnologies can be viewed with support or suspicion depending\non the circumstances or on the tasks they are assigned to do.\n\nTo gauge the opinions of everyday Americans on this relatively\ncomplex and technical subject, the survey presented\nrespondents with four different scenarios in which computers\n\nReal-world examples of the\nscenarios in this survey\n\nAll four of the concepts discussed in the\nsurvey are based on real-life applications\nof algorithmic decision-making and\nartificial intelligence (Al):\n\nNumerous firms now offer\nnontraditional credit scores that build\ntheir ratings using thousands of data\npoints about customers’ activities and\nbehaviors, under the premise that “all\ndata is credit data.”\n\nStates across the country use criminal\nrisk assessments to estimate the\nlikelihood that someone convicted of a\ncrime will reoffend in the future.\n\nSeveral multinational companies are\ncurrently using Al-based systems during\njob interviews to evaluate the honesty,\nemotional state and overall personality\nof applicants.\n\nComputerized resume screening is a\nlongstanding and common HR practice\nfor eliminating candidates who do not\nmeet the requirements for a job posting.\n\nmake decisions by collecting and analyzing large quantities of public and private data. Each of\n\nthese scenarios were based on real-world examples of algorithmic decision-making (see\n\naccompanying sidebar) and included: a personal finance score used to offer consumers deals or\n\ndiscounts; a criminal risk assessment of people up for parole; an automated resume screening\n\nwww.pewresearch.org\n", "vlm_text": "\n\nAlgorithms are all around us, utilizing massive stores of data  and complex analytics to make decisions with often significant  impacts on humans. They recommend books and movies for us  to read and watch, surface news stories they think we might find  relevant, estimate the likelihood that a tumor is cancerous and  predict whether someone might be a criminal or a worthwhile  credit risk. But despite the growing presence of algorithms in  many aspects of daily life, a Pew Research Center survey of U.S.  adults finds that the public is frequently skeptical of these tools  when used in various real-life situations.   \nThis skepticism spans several dimensions. At a broad level,  $58\\%$    of Americans feel that computer programs will always reflect  some level of human bias – although  $40\\%$   think these programs  can be designed in a way that is bias-free. And in various  contexts, the public worries that these tools might violate  privacy, fail to capture the nuance of complex situations, or  simply put the people they are evaluating in an unfair situation.  Public perceptions of algorithmic decision-making are also  often highly contextual. The survey shows that otherwise similar  technologies can be viewed with support or suspicion depending  on the circumstances or on the tasks they are assigned to do.  \nTo gauge the opinions of everyday Americans on this relatively  complex and technical subject, the survey presented  respondents with four different scenarios in which computers  \nReal-world examples of the  scenarios in this survey  \nAll four of the concepts discussed in the  survey are based on real-life applications  of algorithmic decision-making and  artificial intelligence (AI):  \nNumerous firms now offer  non traditional credit scores  that build  their ratings using thousands of data  points about customers’ activities and  behaviors, under the premise that “all  data is credit data.”  \nStates across the country use  criminal  risk assessments  to estimate the  likelihood that someone convicted of a  crime will reoffend in the future.  \nSeveral multinational companies are  currently using AI-based systems  during  job interviews  to evaluate the honesty,  emotional state and overall personality  of applicants.  \nComputerized resume screening is a  longstanding and common HR practice  for eliminating candidates who do not  meet the requirements for a job posting.  \nmake decisions by collecting and analyzing large quantities of public and private data. Each of  these scenarios were based on real-world examples of algorithmic decision-making (see  accompanying sidebar) and included: a personal finance score used to offer consumers deals or  discounts; a criminal risk assessment of people up for parole; an automated resume screening  "}
{"page": 3, "image_path": "doc_images/PI_2018.11.19_algorithms_FINAL_3.jpg", "ocr_text": "3\nPEW RESEARCH CENTER\n\nprogram for job applicants; and a computer-based analysis of job interviews. The survey also\nincluded questions about the content that users are exposed to on social media platforms as a way\nto gauge opinions of more consumer-facing algorithms.\n\nThe following are among the major findings.\n\nThe public expresses broad concerns about the fairness and acceptability of using\ncomputers for decision-making in situations with important real-world consequences\n\nBy and large, the public views these examples of algorithmic decision-making as unfair to the\n\npeople the computer-based\nsystems are evaluating. Most Majorities of Americans find it unacceptable to use\nnotably, only around one-third algorithms to make decisions with real-world\n\nof Americans think that the consequences for humans\n\nvideo job interview and % of U.S. adults who say the following examples of algorithmic decision-\npersonal finance score making are ...\n\nalgorithms would be fair to job Unacceptable —_ Acceptable\n\n. Criminal risk assessment\napplicants and consumers. for people up for parole\nWhen asked directly whether -2---+2+-:-2+-:+2+.:sesesecseseseeseeeeeecseeseseceeeeeeeeceeeeeeseeeeeeseeeesseeeeseseeseeeeseeeees\nthey think the use of these Automated resume screening 57 a\n\n. : of job applicants\nalgorithms is acceptable, a fon ap\n\nmajority of the public says that Automated video analysis fev\nthey are not acceptable. Two- of job interviews\n\nthirds of Americans (68%)\n\nPersonal finance score\n\nfind the personal finance score using many types of ge se\nalgorithm unacceptable, and consumer data\n67% say the computer-aided Note: Respondents who did not give an answer are not shown.\n\n. . . . . Source: Survey of U.S. adults conducted May 29-June 11, 2018.\nvideo job analysis algorithm 1S Public Attitudes Toward Computer Algorithms\n\nunacceptable. PEW RESEARCH CENTER\n\nThere are several themes\ndriving concern among those who find these programs to be unacceptable. Some of the more\nprominent concerns mentioned in response to open-ended questions include the following:\n\n\"They violate privacy. This is the top concern of those who find the personal finance score\nunacceptable, mentioned by 26% of such respondents.\n\nwww.pewresearch.org\n", "vlm_text": "program for job applicants; and a computer-based analysis of job interviews. The survey also  included questions about the content that users are exposed to on social media platforms as a way  to gauge opinions of more consumer-facing algorithms.  \nThe following are among the major findings.  \nThe public expresses broad concerns about the fairness and acceptability of using  computers for decision-making in situations with important real-world consequences  \nBy and large, the public views these examples of algorithmic decision-making as unfair to the  \npeople the computer-based  systems are evaluating. Most  notably, only around one-third  of Americans think that the  video job interview and  personal finance score  algorithms would be fair to job  applicants and consumers.  When asked directly whether  they think the use of these  algorithms is acceptable, a  majority of the public says that  they are not acceptable. Two- thirds of Americans   $(68\\%)$    find the personal finance score  algorithm unacceptable, and   $67\\%$   say the computer-aided  video job analysis algorithm is  unacceptable.   \nMajorities of Americans find it unacceptable to use  algorithms to make decisions with real-world  consequences for humans  \n $\\%$   of U.S. adults who say the following examples of algorithmic decision- making are …  \nThe image is a chart depicting people's opinions on the acceptability of different automated processes. For each process, there are two percentages: one indicating the proportion of people who find the process unacceptable and another for those who find it acceptable. The four automated processes assessed are:\n\n1. Criminal risk assessment for people up for parole: 56% find it unacceptable, and 42% find it acceptable.\n2. Automated resume screening of job applicants: 57% find it unacceptable, and 41% find it acceptable.\n3. Automated video analysis of job interviews: 67% find it unacceptable, and 32% find it acceptable.\n4. Personal finance score using many types of consumer data: 68% find it unacceptable, and 31% find it acceptable. \n\nEach section has a relevant icon and is presented with green indicating the \"Unacceptable\" percentage and blue for the \"Acceptable\" percentage.\nPEW RESEARCH CENTER  \nThere are several themes  driving concern among those who find these programs to be unacceptable. Some of the more  prominent concerns mentioned in response to open-ended questions include the following:  \n▪   They violate privacy.  This is the top concern of those who find the personal finance score  unacceptable, mentioned by  $_{26\\%}$   of such respondents.  "}
{"page": 4, "image_path": "doc_images/PI_2018.11.19_algorithms_FINAL_4.jpg", "ocr_text": "\"They are unfair. Those who worry about the personal finance score scenario, the job interview\nvignette and the automated screening of job applicants often cited concerns about the fairness\nof those processes in expressing their worries.\n\n\"They remove the human element from important decisions. This is the top concern of those\nwho find the automated resume screening concept unacceptable (36% mention this), and it is a\nprominent concern among those who are worried about the use of video job interview analysis\n(16%).\n\n«\" Humans are complex, and these systems are incapable of capturing nuance. This is a\nrelatively consistent theme, mentioned across several of these concepts as something about\nwhich people worry when they consider these scenarios. This concern is especially prominent\namong those who find the use of criminal risk scores unacceptable. Roughly half of these\nrespondents mention concerns related to the fact that all individuals are different, or that a\nsystem such as this leaves no room for personal growth or development.\n\nAttitudes toward algorithmic decision-making can depend heavily on context\n\nDespite the consistencies in some of these responses, the survey also highlights the ways in which\nAmericans’ attitudes toward algorithmic decision-making can depend heavily on the context of\nthose decisions and the characteristics of the people who might be affected.\n\nThis context dependence is especially notable in the public’s contrasting attitudes toward the\ncriminal risk score and personal finance score concepts. Similar shares of the population think\nthese programs would be effective at doing the job they are supposed to do, with 54% thinking the\npersonal finance score algorithm would do a good job at identifying people who would be good\ncustomers and 49% thinking the criminal risk score would be effective at identifying people who\nare deserving of parole. But a larger share of Americans think the criminal risk score would be fair\nto those it is analyzing. Half (50%) think this type of algorithm would be fair to people who are up\nfor parole, but just 32% think the personal finance score concept would be fair to consumers.\n\nWhen it comes to the algorithms that underpin the social media environment, users’ comfort level\nwith sharing their personal information also depends heavily on how and why their data are being\nused. A 75% majority of social media users say they would be comfortable sharing their data with\nthose sites if it were used to recommend events they might like to attend. But that share falls to\njust 37% if their data are being used to deliver messages from political campaigns.\n\nwww.pewresearch.org\n", "vlm_text": "▪   They are unfair.  Those who worry about the personal finance score scenario, the job interview  vignette and the automated screening of job applicants often cited concerns about the fairness  of those processes in expressing their worries.  \n\n ▪   They remove the human element from important decisions.  This is the top concern of those  who find the automated resume screening concept unacceptable (  ${\\it\\Delta}36\\%$   mention this), and it is a  prominent concern among those who are worried about the use of video job interview analysis   $({\\bf16\\%})$  . \n\n ▪   Humans are complex, and these systems are incapable of capturing nuance.  This is a  relatively consistent theme, mentioned across several of these concepts as something about  which people worry when they consider these scenarios. This concern is especially prominent  among those who find the use of criminal risk scores unacceptable. Roughly half of these  respondents mention concerns related to the fact that all individuals are different, or that a  system such as this leaves no room for personal growth or development.  \nAttitudes toward algorithmic decision-making can depend heavily on context  \nDespite the consistencies in some of these responses, the survey also highlights the ways in which  Americans’ attitudes toward algorithmic decision-making can depend heavily on the context of  those decisions and the characteristics of the people who might be affected.   \nThis context dependence is especially notable in the public’s contrasting attitudes toward the  criminal risk score and personal finance score concepts. Similar shares of the population think  these programs would be effective at doing the job they are supposed to do, with  $54\\%$   thinking the  personal finance score algorithm would do a good job at identifying people who would be good  customers and  $49\\%$   thinking the criminal risk score would be effective at identifying people who  are deserving of parole. But a larger share of Americans think the criminal risk score would be fair  to those it is analyzing. Half   $\\left(50\\%\\right)$   think this type of algorithm would be fair to people who are up  for parole, but just  $32\\%$   think the personal finance score concept would be fair to consumers.  \nWhen it comes to the algorithms that underpin the social media environment, users’ comfort level  with sharing their personal information also depends heavily on how and why their data are being  used. A   $75\\%$   majority of social media users say they would be comfortable sharing their data with  those sites if it were used to recommend events they might like to attend. But that share falls to  just  $37\\%$   if their data are being used to deliver messages from political campaigns.   "}
{"page": 5, "image_path": "doc_images/PI_2018.11.19_algorithms_FINAL_5.jpg", "ocr_text": "In other instances, different\ntypes of users offer divergent\nviews about the collection and\nuse of their personal data. For\ninstance, about two-thirds of\nsocial media users younger than\n50 find it acceptable for social\n\nAcross age groups, social media users are comfortable\nwith their data being used to recommend events - but\nwary of that data being used for political messaging\n\n% of social media users who say it is acceptable for social media sites to use\ndata about them and their online activities to ...\n\nRecommend events Ages 65+ 50-64 18-29 30-49\n\nmedia platforms to use their in their area O° @\n67% 72 78 80\npersonal data to recommend\n. . Recommend someone\nconnecting with people they they might want to oO e—e\nmight want to know. But that know 36 SS 6667\nview is shared by fewer than Show them ads for Oe\nhalf of users ages 65 and older. _ products and services 39 5154 60\n: . Show them messages Cow\nSocial media users are from political campaigns 31 35 38.40\n\nexposed to a mix of positive\nand negative content on\nthese sites\n\n. PEW RESEARCH CENTER\nAlgorithms shape the modern\n\nsocial media landscape in\n\nprofound and ubiquitous ways. By determining the specific types of content that might be most\nappealing to any individual user based on his or her behaviors, they influence the media diets of\nmillions of Americans. This has led to concerns that these sites are steering huge numbers of\npeople toward content that is “engaging” simply because it makes them angry, inflames their\nemotions or otherwise serves as intellectual junk food.\n\nOn this front, the survey provides ample evidence that social media users are regularly exposed to\npotentially problematic or troubling content on these sites. Notably, 71% of social media users say\nthey ever see content there that makes them angry — with 25% saying they see this sort of content\nfrequently. By the same token, roughly six-in-ten users say they frequently encounter posts that\nare overly exaggerated (58%) or posts where people are making accusations or starting arguments\nwithout waiting until they have all the facts (59%).\n\nBut as is often true of users’ experiences on social media more broadly, these negative encounters\n\nare accompanied by more positive interactions. Although 25% of these users say they frequently\nencounter content that makes them feel angry, a comparable share (21%) says they frequently\n\nwww.pewresearch.org\n", "vlm_text": "In other instances, different  types of users offer divergent  views about the collection and  use of their personal data. For  instance, about two-thirds of  social media users younger than  50 find it acceptable for social  media platforms to use their  personal data to recommend  connecting with people they  might want to know. But that  view is shared by fewer than  half of users ages 65 and older.  \nSocial media users are  exposed to a mix of positive  and negative content on  these sites  \nAcross age groups, social media users are comfortable  with their data being used to recommend events – but  wary of that data being used for political messaging  \n% of social media users who say it is acceptable for social media sites to use  data about them and their online activities to …  \nThe image is a chart showing the percentage of people within different age groups who find certain actions by social media sites acceptable. The actions are:\n\n1. **Recommend events in their area**\n   - Ages 65+: 67%\n   - Ages 50-64: 72%\n   - Ages 18-29: 78%\n   - Ages 30-49: 80%\n\n2. **Recommend someone they might want to know**\n   - Ages 65+: 36%\n   - Ages 50-64: 53%\n   - Ages 18-29: 66%\n   - Ages 30-49: 67%\n\n3. **Show them ads for products and services**\n   - Ages 65+: 39%\n   - Ages 50-64: 51%\n   - Ages 18-29: 54%\n   - Ages 30-49: 60%\n\n4. **Show them messages from political campaigns**\n   - Ages 65+: 31%\n   - Ages 50-64: 35%\n   - Ages 18-29: 38%\n   - Ages 30-49: 40%\n\nThe chart draws from a survey of U.S. adults conducted from May 29 to June 11, 2018, titled \"Public Attitudes Toward Computer Algorithms.\" The note specifies that these percentages reflect those who said it is \"very\" or \"somewhat\" acceptable for social media platforms to do these activities.\nAlgorithms shape the modern  social media landscape in  \nprofound and ubiquitous ways. By determining the specific types of content that might be most  appealing to any individual user based on his or her behaviors, they influence the media diets of  millions of Americans. This has  led to concerns  that these sites are steering huge numbers of  people toward content that is “engaging” simply because it makes them angry, inflames their  emotions or otherwise serves as intellectual junk food.  \nOn this front, the survey provides ample evidence that social media users are regularly exposed to  potentially problematic or troubling content on these sites. Notably,  $71\\%$   of social media users say  they ever see content there that makes them angry – with   $25\\%$   saying they see this sort of content  frequently. By the same token, roughly six-in-ten users say they frequently encounter posts that  are overly exaggerated   $(58\\%)$   or posts where people are making accusations or starting arguments  without waiting until they have all the facts   $\\left(59\\%\\right)$  .  \nBut as is often true of users’ experiences on social media more broadly, these negative encounters  are accompanied by more positive interactions. Although  $25\\%$   of these users say they frequently  encounter content that makes them feel angry, a comparable share   $\\left(\\boldsymbol{\\it{21\\%}}\\right)$   says they frequently   "}
{"page": 6, "image_path": "doc_images/PI_2018.11.19_algorithms_FINAL_6.jpg", "ocr_text": "6\nPEW RESEARCH CENTER\n\nencounter content that makes them feel connected to others. And an even larger share (44%)\nreports frequently seeing content that makes them amused.\n\nSimilarly, social media users\ntend to be exposed toa mixof | Amusement, anger, connectedness top the emotions\npositive and negative behaviors USers frequently feel when using social media\nfrom other users on these sites. % of social media users in each age group who say they frequently see\nAround half of users (54%) say content on social media that makes them feel ...\n\n6\n\nthey see an equal mix of people\ny q or peop Ages 65+ 50-64 30-49 18-29\n\nbeing mean or bullying and Amused Oe ee\npeople being kind and 30% S951 54\nsupportive. The remaining Angry @\n\nusers are split between those 23 2425 27\n\nwho see more meanness (21%) connected 0 ee\n\nand kindness (24%) on these 415 2023 25\n\nsites. And a majority of users\n(63%) say they see an equal mix _—sPired ° Porn\nof people trying to be deceptive\n\nand people trying to point out Depressed @e\n\ninaccurate information — with 1112 17\nthe remainder being evenly Lonely O8®@ ©\nsplit between those who see 257 15\n\nmore people spreading\ninaccuracies (18%) and more c\npeople trying to correct that PEW RESEARCH CENTER\nbehavior (17%).\n\nOther key findings from this survey of 4,594 U.S. adults conducted May 29-June 11, 2018, include:\n\n* Public attitudes toward algorithmic decision-making can vary by factors related to race and\nethnicity. Just 25% of whites think the personal finance score concept would be fair to\nconsumers, but that share rises to 45% among blacks. By the same token, 61% of blacks think\nthe criminal risk score concept is not fair to people up for parole, but that share falls to 49%\namong whites.\n\n\"Roughly three-quarters of the public (74%) thinks the content people post on social media is\nnot reflective of how society more broadly feels about important issues — although 25% think\nthat social media does paint an accurate portrait of society.\n\nwww.pewresearch.org\n", "vlm_text": "encounter content that makes them feel connected to others. And an even larger share   $(44\\%)$    reports frequently seeing content that makes them amused.  \nSimilarly, social media users  tend to be exposed to a mix of  positive and negative behaviors  from other users on these sites.  Around half of users   $\\left(54\\%\\right)$   say  they see an equal mix of people  being mean or bullying and  people being kind and  supportive. The remaining  users are split between those  who see more meanness   $\\left(\\boldsymbol{\\it{21\\%}}\\right)$    and kindness   $({\\it24\\%})$   on these  sites. And a majority of users   $(63\\%)$   say they see an equal mix  of people trying to be deceptive  and people trying to point out  inaccurate information – with  the remainder being evenly  split between those who see  more people spreading  inaccuracies   $(\\mathbf{1}8\\%)$   and more  people trying to correct that  behavior   $(17\\%)$  .  \nAmusement, anger, connected ness top the emotions  users frequently feel when using social media  \n $\\%$   of social media users in each age group who say they frequently see  content on social media that makes them feel …  \nThe image is a chart comparing emotional responses across different age groups (65+, 50-64, 30-49, 18-29) for various emotions. Each emotion is represented by a horizontal line with dots corresponding to percentages for each age group. \n\nHere are the emotions and percentages shown:\n\n- **Amused**: \n  - Ages 65+: 30%\n  - 50-64: 39%\n  - 30-49: 51%\n  - 18-29: 54%\n\n- **Angry**: \n  - Ages 65+: 23%\n  - 50-64: 24%\n  - 30-49: 25%\n  - 18-29: 27%\n\n- **Connected**: \n  - Ages 65+: 15%\n  - 50-64: 20%\n  - 30-49: 23%\n  - 18-29: 25%\n\n- **Inspired**: \n  - Ages 65+: 9%\n  - 50-64: 16%\n  - 30-49: 17%\n  - 18-29: 19%\n\n- **Depressed**: \n  - Ages 65+: 11%\n  - 50-64: 12%\n  - 30-49: 12%\n  - 18-29: 17%\n\n- **Lonely**: \n  - Ages 65+: 2%\n  - 50-64: 5%\n  - 30-49: 7%\n  - 18-29: 15%\nPEW RESEARCH CENTER  \nOther key findings from this survey of 4,594 U.S. adults conducted May 29-June 11, 2018, include: \n\n \n▪   Public attitudes toward algorithmic decision-making can vary by factors related to race and  ethnicity. Just   $25\\%$   of whites think the personal finance score concept would be fair to  consumers, but that share rises to  $45\\%$   among blacks. By the same token,  $61\\%$   of blacks think  the criminal risk score concept is  not  fair to people up for parole, but that share falls to   $49\\%$    among whites. \n\n ▪   Roughly three-quarters of the public   $\\left(74\\%\\right)$   thinks the content people post on social media is  not reflective of how society more broadly feels about important issues – although  $25\\%$   think  that social media does paint an accurate portrait of society.  "}
{"page": 7, "image_path": "doc_images/PI_2018.11.19_algorithms_FINAL_7.jpg", "ocr_text": "Younger adults are twice as likely to say they frequently see content on social media that makes\nthem feel amused (54%) as they are content that makes them feel angry (27%). But users ages\n65 and older encounter these two types of content with more comparable frequency. The\nsurvey finds that 30% of older users frequently see content on social media that makes them\nfeel amused, while 24% frequently see content that makes them feel angry.\n\nwww.pewresearch.org\n", "vlm_text": "▪   Younger adults are twice as likely to say they frequently see content on social media that makes  them feel amused   $\\left(54\\%\\right)$   as they are content that makes them feel angry   $\\left(27\\%\\right)$  . But users ages  65 and older encounter these two types of content with more comparable frequency. The  survey finds that  $30\\%$   of older users frequently see content on social media that makes them  feel amused, while   $24\\%$   frequently see content that makes them feel angry.  "}
{"page": 8, "image_path": "doc_images/PI_2018.11.19_algorithms_FINAL_8.jpg", "ocr_text": "8\nPEW RESEARCH CENTER\n\n1. Attitudes toward algorithmic decision-making\n\nToday, many decisions that could be made by human beings — from interpreting medical images to\nrecommending books or movies — can now be made by computer algorithms with advanced\nanalytic capabilities and access to huge stores of data. The growing prevalence of these algorithms\nhas led to widespread concerns about their impact on those who are affected by decisions they\nmake. To proponents, these systems promise to increase accuracy and reduce human bias in\nimportant decisions. But others worry that many of these systems amount to “weapons of math\ndestruction” that simply reinforce existing biases and disparities under the guise of algorithmic\nneutrality.\n\nThis survey finds that the public is more broadly\ninclined to share the latter, more skeptical view. Majority of Americans say computer\nRoughly six-in-ten Americans (58%) feel that programs will always reflect human\n\n. . bias; young adults are more split\ncomputer programs will always reflect the biases\n\nof the people who designed them, while 40% feel” of U.S. adults who say that ...\n\nit is possible for computer programs to make It is possible for computer Computer programs\nprograms to make decisions __ will always reflect\nwithout human bias __ bias of designers\n\ndecisions that are free from human bias.\nNotably, younger Americans are more\nsupportive of the notion that computer programs Total\ncan be developed that are free from bias. Half of 18-29\n18- to 29-year-olds and 43% of those ages 30 to\n49 hold this view, but that share falls to 34%\n\namong those ages 50 and older. 50+\n\n30-49\n\nThis general concern about computer programs\nmaking important decisions is also reflected in “ une\npublic attitudes about the use of algorithms and , :\n\nPEW RESEARCH CENTER\nbig data in several real-life contexts.\n\nTo gain a deeper understanding of the public’s views of algorithms, the survey asked respondents\nabout their opinions of four examples in which computers use various personal and public data to\nmake decisions with real-world impact for humans. They include examples of decisions being\nmade by both public and private entities. They also include a mix of personal situations with direct\nrelevance to a large share of Americans (such as being evaluated for a job) and those that might be\nmore distant from many people’s lived experiences (like being evaluated for parole). And all four\nare based on real-life examples of technologies that are currently in use in various fields.\n\nwww.pewresearch.org\n", "vlm_text": "1. Attitudes toward algorithmic decision-making   \nToday, many decisions that could be made by human beings – from interpreting medical images to  recommending books or movies – can now be made by computer algorithms with advanced  analytic capabilities and access to huge stores of data. The growing prevalence of these algorithms  has led to widespread concerns about their impact on those who are affected by decisions they  make. To proponents, these systems promise to increase accuracy and reduce human bias in  important decisions. But others worry that many of these systems amount to “ weapons of math  destruction ” that simply reinforce existing biases and disparities under the guise of algorithmic  neutrality.  \nThis survey finds that the public is more broadly  inclined to share the latter, more skeptical view.  Roughly six-in-ten Americans   $(58\\%)$   feel that  computer programs will always reflect the biases  of the people who designed them, while  $40\\%$   feel  it is possible for computer programs to make  decisions that are free from human bias.  Notably, younger Americans are more  supportive of the notion that computer programs  can be developed that are free from bias. Half of  18- to 29-year-olds and   $43\\%$   of those ages 30 to  49 hold this view, but that share falls to   $34\\%$    among those ages 50 and older.  \nThis general concern about computer programs  making important decisions is also reflected in  public attitudes about the use of algorithms and  big data in several real-life contexts.  \nMajority of Americans say computer  programs will always reflect human  bias; young adults are more split  \n $\\%$   of U.S. adults who say that …  It is possible for computer  Computer programs  \n\nThe image is a bar chart comparing perceptions of different age groups on whether programs can make decisions without human bias versus whether they will always reflect the bias of designers. Here are the details:\n\n- **Total**: 40% believe programs can make decisions without human bias, while 58% think they will always reflect designer bias.\n- **Ages 18-29**: 50% believe decisions can be made without bias, and 48% think bias is inevitable.\n- **Ages 30-49**: 43% believe in unbiased decisions, versus 56% seeing inevitable bias.\n- **Ages 50+**: 34% think decisions can be unbiased, while 63% expect bias.\n\nThe chart shows a general trend of older age groups being more skeptical about the unbiased capability of programs.\nPEW RESEARCH CENTER  \nTo gain a deeper understanding of the public’s views of algorithms, the survey asked respondents  about their opinions of four examples in which computers use various personal and public data to  make decisions with real-world impact for humans. They include examples of decisions being  made by both public and private entities. They also include a mix of personal situations with direct  relevance to a large share of Americans (such as being evaluated for a job) and those that might be  more distant from many people’s lived experiences (like being evaluated for parole). And all four  are based on real-life examples of technologies that are currently in use in various fields.  "}
{"page": 9, "image_path": "doc_images/PI_2018.11.19_algorithms_FINAL_9.jpg", "ocr_text": "9\nPEW RESEARCH CENTER\n\nThe specific scenarios in the survey include the following:\n\n= An automated personal finance score that collects and analyzes data from many different\nsources about people’s behaviors and personal characteristics (not just their financial\nbehaviors) to help businesses decide whether to offer them loans, special offers or other\nservices.\n\n=  Acriminal risk assessment that collects data about people who are up for parole, compares\nthat data with that of others who have been convicted of crimes, and assigns a score that helps\ndecide whether they should be released from prison.\n\n= A program that analyzes videos of job interviews, compares interviewees’ characteristics,\nbehavior and answers to other successful employees, and gives them a score that can help\nbusinesses decide whether job candidates would be a good hire or not.\n\n= Acomputerized resume screening program that evaluates the contents of submitted resumes\nand only forwards those meeting a certain threshold score to a hiring manager for further\nreview about reaching the next stage of the hiring process.\n\nFor each scenario, respondents were asked to indicate whether they think the program would be\nfair to the people being evaluated; if it would be effective at doing the job it is designed to do; and\nwhether they think it is generally acceptable for companies or other entities to use these tools for\nthe purposes outlined.\n\nBroad public concern about the fairness of these\nexamples of algorithmic decision-making\n\n% of U.S. adults who think the following types of computer programs would\nbe to the people being evaluated\n\nSizable shares of Americans\nview each of these scenarios\nas unfair to those being\n\nevaluated\n\n. . Not fair Notvery Somewhat Very\nAmericans are largely skeptical atall fair fair fair\nabout the fairness of these Automated scoring of people 17\n\n- oo. up for parole\nprograms: None is viewed as\n\nAutomated resume screening\n\nfair by a clear majority of the of job applicants 23\npublic. Especially small shares\n: “ Automated video analysis of 27\nthink the “personal finance job interviews\n» Gq .\nscore” and “video job interview Automated personal 33\nanalysis” concepts would be fair finance score\n\nto consumers or job applicants Note: Respondents who did not give an answer are not shown.\n\n9 9 . Source: Survey of U.S. adults conducted May 29-June 11, 2018.\n(32% and 33%, respectively). Public Attitudes Toward Computer Algorithms\n\nThe automated criminal risk PEW RESEARCH CENTER\n\nscore concept is viewed as fair\n\nwww.pewresearch.org\n", "vlm_text": "The specific scenarios in the survey include the following: \n\n \n▪   An automated  personal finance score  that collects and analyzes data from many different  sources about people’s behaviors and personal characteristics (not just their financial  behaviors) to help businesses decide whether to offer them loans, special offers or other  services. \n\n ▪   A  criminal risk assessment  that collects data about people who are up for parole, compares  that data with that of others who have been convicted of crimes, and assigns a score that helps  decide whether they should be released from prison. \n\n ▪   A program that  analyzes videos of job interviews , compares interviewees’ characteristics,  behavior and answers to other successful employees, and gives them a score that can help  businesses decide whether job candidates would be a good hire or not. \n\n ▪   A  computerized resume screening  program that evaluates the contents of submitted resumes  and only forwards those meeting a certain threshold score to a hiring manager for further  review about reaching the next stage of the hiring process.   \nFor each scenario, respondents were asked to indicate whether they think the program would be  fair to the people being evaluated; if it would be effective at doing the job it is designed to do; and  whether they think it is generally acceptable for companies or other entities to use these tools for  the purposes outlined.  \nSizable shares of Americans  view each of these scenarios  as unfair to those being  evaluated  \nAmericans are largely skeptical  about the fairness of these  programs: None is viewed as  fair by a clear majority of the  public. Especially small shares  think the “personal finance  score” and “video job interview  analysis” concepts would be fair  to consumers or job applicants   $(32\\%$   and  $33\\%$  , respectively).  The automated criminal risk  score concept is viewed as fair  \nBroad public concern about the fairness of these  examples of algorithmic decision-making  \n $\\%$   of U.S. adults who think the following types of computer programs would  be ___ to the people being evaluated  \nThe image is a bar chart comparing perceptions of fairness regarding automated decision-making systems in four scenarios:\n\n1. **Automated scoring of people up for parole:**\n   - Not fair at all: 17%\n   - Not very fair: 32%\n   - Somewhat fair: 41%\n   - Very fair: 10%\n\n2. **Automated resume screening of job applicants:**\n   - Not fair at all: 23%\n   - Not very fair: 34%\n   - Somewhat fair: 35%\n   - Very fair: 8%\n\n3. **Automated video analysis of job interviews:**\n   - Not fair at all: 27%\n   - Not very fair: 39%\n   - Somewhat fair: 27%\n   - Very fair: 6%\n\n4. **Automated personal finance score:**\n   - Not fair at all: 33%\n   - Not very fair: 33%\n   - Somewhat fair: 27%\n   - Very fair: 6%\n\nThe chart uses varying shades of blue to represent different levels of perceived fairness."}
{"page": 10, "image_path": "doc_images/PI_2018.11.19_algorithms_FINAL_10.jpg", "ocr_text": "10\n\nby the largest share of Americans. Even so, only around half the public finds this concept fair —\nand just one-in-ten think this type of program would be very fair to people in parole hearings.\n\nDemographic differences are relatively modest on the question of whether these systems are fair,\nalthough there is some notable attitudinal variation related to race and ethnicity. Blacks and\nHispanics are more likely than whites to find the consumer finance score concept fair to\nconsumers. Just 25% of whites think this type of program would be fair to consumers, but that\nshare rises to 45% among blacks and 47% among Hispanics. By contrast, blacks express much\nmore concern about a parole scoring algorithm than do either whites or Hispanics. Roughly six-in-\nten blacks (61%) think this type of program would not be fair to people up for parole, significantly\nhigher than the share of either whites (49%) or Hispanics (38%) who say the same.\n\nThe public is mostly divided on whether these programs would be effective or not\n\nThe public is relatively split on whether these\nprograms would be effective at doing the job they 54% of Americans think automated\n\nare designed to do. Some 54% think the personal finance scores would be effective - but\n\nfinance score program would be effective at just 32% think they would be fair\n\nidentifying good customers, while around half % of U.S. adults who say the following types of computer\nthink the parole rating (49%) and resume programs would be very/somewhat ...\n\n: : : Effective-\nscreening (47%) algorithms would be effective. fair\nMeanwhile, 39% think the video job interview . Effective Fair difference\n\n: : Automated personal finance 54% 32% +422\nconcept would be a good way to identify score\n* Automated video analysis of\nsuccessful hires. job interviews 39 33 +6\nAutomated resume\n| 47 43 +4\nFor the most part, people’s views of the fairness S°\"enin8 of Job applicants\nAutomated scoring of 49 50 A\n\nand effectiveness of these programs go hand in people up for parole\n\nhand. Similar shares of the public view these\n\nconcepts as fair to those being judged, as say\n\nthey would be effective at producing good\n\ndecisions. But the personal finance score concept PEW RESEARCH CENTER\n\nis a notable exception to this overall trend. Some\n\n54% of Americans think this type of program would do a good job at helping businesses find new\ncustomers, but just 32% think it is fair for consumers to be judged in this way. That 22-percentage-\npoint difference is by far the largest among the four different scenarios.\n\nwww.pewresearch.org\n", "vlm_text": "by the largest share of Americans. Even so, only around half the public finds this concept fair –  and just one-in-ten think this type of program would be  very  fair to people in parole hearings.  \nDemographic differences are relatively modest on the question of whether these systems are fair,  although there is some notable at t it udin al variation related to race and ethnicity. Blacks and  Hispanics are more likely than whites to find the consumer finance score concept fair to  consumers. Just   $25\\%$   of whites think this type of program would be fair to consumers, but that  share rises to  $45\\%$   among blacks and   $47\\%$   among Hispanics. By contrast, blacks express much  more concern about a parole scoring algorithm than do either whites or Hispanics. Roughly six-in- ten blacks   $(61\\%)$   think this type of program would  not  be fair to people up for parole, significantly  higher than the share of either whites   $(49\\%)$   or Hispanics   $(38\\%)$   who say the same.  \nThe public is mostly divided on whether these programs would be effective or not  \nThe public is relatively split on whether these  programs would be effective at doing the job they  are designed to do. Some  $54\\%$   think the personal  finance score program would be effective at  identifying good customers, while around half  think the parole rating  $(49\\%)$   and resume  screening   $(47\\%)$   algorithms would be effective.  Meanwhile,  $39\\%$   think the video job interview  concept would be a good way to identify  successful hires.   \nFor the most part, people’s views of the fairness  and effectiveness of these programs go hand in  hand. Similar shares of the public view these  concepts as fair to those being judged, as say  they would be effective at producing good  decisions. But the personal finance score concept  is a notable exception to this overall trend. Some  \n $\\pmb{54\\%}$   of Americans think automated  finance scores would be effective – but  just  $\\pmb{\\mathscr{\\textbf{s2}}}\\%$   think they would be fair  \n $\\%$   of U.S. adults who say the following types of computer  programs would be very/somewhat …  \nThis table compares the perceived effectiveness and fairness of different automated systems:\n\n1. **Automated personal finance score**\n   - Effective: 54%\n   - Fair: 32%\n   - Effective-fair difference: +22\n\n2. **Automated video analysis of job interviews**\n   - Effective: 39%\n   - Fair: 33%\n   - Effective-fair difference: +6\n\n3. **Automated resume screening of job applicants**\n   - Effective: 47%\n   - Fair: 43%\n   - Effective-fair difference: +4\n\n4. **Automated scoring of people up for parole**\n   - Effective: 49%\n   - Fair: 50%\n   - Effective-fair difference: -1\n\nThe \"Effective-fair difference\" indicates the difference between the percentage of people who find each system effective versus fair.\nPEW RESEARCH CENTER  \n $54\\%$   of Americans think this type of program would do a good job at helping businesses find new  customers, but just  $32\\%$   think it is fair for consumers to be judged in this way. That 22-percentage- point difference is by far the largest among the four different scenarios.  "}
{"page": 11, "image_path": "doc_images/PI_2018.11.19_algorithms_FINAL_11.jpg", "ocr_text": "11\n\nMajorities of Americans think the use of these programs is unacceptable; concerns about\ndata privacy, fairness and overall effectiveness highlight their list of worries\n\nMajorities of the public think it is not acceptable for companies or other entities to use the\nconcepts described in this survey. Most prominently, 68% of Americans think using the personal\nfinance score concept is unacceptable, and 67% think it is unacceptable for companies to conduct\ncomputer-aided video analysis of interviews when hiring job candidates.\n\nThe survey asked respondents to describe in their own words why they feel these programs are\nacceptable or not, and certain themes emerged in these responses. Those who think these\nprograms are acceptable often focus on the fact that they would be effective at doing the job they\npurport to do. Additionally, some argue in the case of private sector examples that the concepts\nsimply represent the company’s prerogative or the free market at work.\n\nMeanwhile, those who find the use of these programs to be unacceptable often worry that they will\nnot do as good a job as advertised. They also express concerns about the fairness of these\nprograms and in some cases worry about the privacy implications of the data being collected and\nshared. The public reaction to each of these concepts is discussed in more detail below.\n\nAutomated personal finance score\n\nAmong the 31% of Americans who think it would be acceptable for companies to use this type of\nprogram, the largest share of respondents (31%) feel it would be effective at helping companies\nfind good customers. Smaller shares say customers have no right to complain about this practice\nsince they are willingly putting their data out in public with their online activities (12%), or that\ncompanies can do what they want and/or that this is simply the free market at work (6%).\n\nHere are some samples of these responses:\n\n\" “TI believe that companies should be able to use an updated, modern effort to judge someone’s\nfiscal responsibility in ways other than if they pay their bills on time.” Man, 28\n\n« “Finances and financial situations are so complex now. A person might have a bad credit score\ndue to a rough patch but doesn't spend frivolously, pays bills, etc. This person might benefit\nfrom an overall look at their trends. Alternately, a person who sides on trends in the opposite\ndirection but has limited credit/good credit might not be a great choice for a company as their\ntrends may indicate that they will default later.” Woman, 32\n\n«  “[It’s] simple economics — if people want to put their info out there....well, sucks to be them.”\nMan, 29\n\nwww.pewresearch.org\n", "vlm_text": "Majorities of Americans think the use of these programs is unacceptable; concerns about  data privacy, fairness and overall effectiveness highlight their list of worries  \nMajorities of the public think it is  not  acceptable for companies or other entities to use the  concepts described in this survey. Most prominently,  $68\\%$   of Americans think using the personal  finance score concept is unacceptable, and  $67\\%$   think it is unacceptable for companies to conduct  computer-aided video analysis of interviews when hiring job candidates.  \nThe survey asked respondents to describe in their own words why they feel these programs are  acceptable or not, and certain themes emerged in these responses. Those who think these  programs are acceptable often focus on the fact that they would be effective at doing the job they  purport to do. Additionally, some argue in the case of private sector examples that the concepts  simply represent the company’s prerogative or the free market at work.  \nMeanwhile, those who find the use of these programs to be unacceptable often worry that they will  not do as good a job as advertised. They also express concerns about the fairness of these  programs and in some cases worry about the privacy implications of the data being collected and  shared. The public reaction to each of these concepts is discussed in more detail below.  \nAutomated personal finance score  \nAmong the  $31\\%$   of Americans who think it would be acceptable for companies to use this type of  program, the largest share of respondents   $\\left(31\\%\\right)$   feel it would be effective at helping companies  find good customers. Smaller shares say customers have no right to complain about this practice  since they are willingly putting their data out in public with their online activities   $\\left(\\mathbf{1}2\\%\\right)$  , or that  companies can do what they want and/or that this is simply the free market at work  $(6\\%)$  .   \nHere are some samples of these responses: \n\n \n▪   “I believe that companies should be able to use an updated, modern effort to judge someone’s  fiscal responsibility in ways other than if they pay their bills on time.”  Man, 28  \n\n ▪   “Finances and financial situations are so complex now. A person might have a bad credit score  due to a rough patch but doesn't spend frivolously, pays bills, etc. This person might benefit  from an overall look at their trends. Alternately, a person who sides on trends in the opposite  direction but has limited credit/good credit might not be a great choice for a company as their  trends may indicate that they will default later.”  Woman, 32  \n\n ▪   “[It’s] simple economics – if people want to put their info out there....well, sucks to be them.”  Man, 29   "}
{"page": 12, "image_path": "doc_images/PI_2018.11.19_algorithms_FINAL_12.jpg", "ocr_text": "= “Tt sounds exactly like a\ncredit card score, which,\nwhile not very fair, is\nconsidered acceptable.”\nWoman, 25\n\n= “Because it’s efficient and\neffective at bringing\nbusinesses information\nthat they can use to\nconnect their services and\nproducts (loans) to\ncustomers. This is a good\nthing. To streamline the\nprocess and make it\ncheaper and more targeted\nmeans less waste of\nresources in advertising\nsuch things.” Man, 33\n\nThe 68% of Americans who\nthink it is unacceptable for\ncompanies to use this type of\nprogram cite three primary\nconcerns. Around one-quarter\n(26%) argue that collecting\nthis data violates people’s\nprivacy. One-in-five say that\nsomeone’s online data does\nnot accurately represent them\n\n12\nPEW RESEARCH CENTER\n\nConcerns over automated personal finance scores\nfocus on privacy, discrimination, failure to represent\npeople accurately\n\n% of U.S. adults\n\nwho say it is__\n\nfor companies to\n\nuse automated -\npersonal finance Among those who say acceptable,\n\nscores % who give these as the main reasons\n\nWould be effective\n\nConsumers willingly make info public\nFree market at work |\nOK if info can be corrected | 4\n\nSame as traditional credit score | 3\n\nExE7) Acceptable\n\nAmong those who say NOT acceptable,\n% who give these as the main reasons\n\nNot\n\nEv acceptable —— Violates privacy 26%\n\nDoesn't represent person accurately\n\nUnfair/discriminatory | 15]\n\nDoesn't reflect creditworthiness | 9]\nNo way to change score i 5\n\nNote: Verbatim responses have been coded into categories. Results may add to more than\n100% because multiple responses were allowed. Respondents who did not give an answer\nor gave other answers are not shown.\n\nSource: Survey of U.S. adults conducted May 29-June 11, 2018.\n\n“Public Attitudes Toward Computer Algorithms”\n\nPEW RESEARCH CENTER\n\nas a person, while 9% make the related point that people’s online habits and behaviors have\nnothing to do with their overall creditworthiness. And 15% feel that it is potentially unfair or\n\ndiscriminatory to rely on this type of score.\n\nHere are some samples of these responses:\n\n= “Opaque algorithms can introduce biases even without intending to. This is more of an issue\nwith criminal sentencing algorithms, but it can still lead to redlining and biases against\n\nwww.pewresearch.org\n", "vlm_text": "▪   “It sounds exactly like a  credit card score, which,  while not very fair, is  considered acceptable.”  Woman, 25  \n\n ▪   “Because it’s efficient and  effective at bringing  businesses information  that they can use to  connect their services and  products (loans) to  customers. This is a good  thing. To streamline the  process and make it  cheaper and more targeted  means less waste of  resources in advertising  such things.”  Man, 33    \nThe  $68\\%$   of Americans who  think it is unacceptable for  companies to use this type of  program cite three primary  concerns. Around one-quarter   $(26\\%)$   argue that collecting  this data violates people’s  privacy. One-in-five say that  someone’s online data does  not accurately represent them  \nConcerns over automated personal finance scores  focus on privacy, discrimination, failure to represent  people accurately  \nThe image is a bar chart illustrating the percentage of U.S. adults who find it acceptable or not acceptable for companies to use automated personal finance scores.\n\n- **31% Acceptable:**  \n  - Reasons include:\n    - Would be effective (31%)\n    - Consumers willingly make info public (12%)\n    - Free market at work (6%)\n    - OK if info can be corrected (4%)\n    - Same as traditional credit score (3%)\n\n- **68% Not Acceptable:**  \n  - Reasons include:\n    - Violates privacy (26%)\n    - Doesn’t represent person accurately (20%)\n    - Unfair/discriminatory (15%)\n    - Doesn’t reflect creditworthiness (9%)\n    - No way to change score (5%)\nPEW RESEARCH CENTER  as a person, while  $9\\%$   make the related point that people’s online habits and behaviors have  nothing to do with their overall credit worthiness. And   $15\\%$   feel that it is potentially unfair or  discriminatory to rely on this type of score.   \n\nHere are some samples of these responses:  \n▪   “Opaque algorithms can introduce biases even without intending to. This is more of an issue  with criminal sentencing algorithms, but it can still lead to redlining and biases against  minority communities. If they were improved to eliminate this I’d be more inclined to accept  their use.”  Man, 46   \n\n "}
{"page": 13, "image_path": "doc_images/PI_2018.11.19_algorithms_FINAL_13.jpg", "ocr_text": "13\n\nminority communities. If they were improved to eliminate this I’d be more inclined to accept\ntheir use.” Man, 46\n\n« “It encroaches on someone’s ability to freely engage in activities online. It makes one want to\nhide what they are buying — whether it is a present for a friend or a book to read. Why should\nanyone have that kind of access to know my buying habits and take advantage of it in some\nway? That kind of monitoring just seems very archaic. I can understand why this would be\ndone, from their point of view it helps to show what I as a customer would be interested in\nbuying. But I feel that there should be a line of some kind, and this crosses that line.” Woman,\n27\n\n* “TI don’t think it is fair for companies to use my info without my permission, even if it would be\na special offer that would interest me. It is like spying, not acceptable. It would also exclude\npeople from receiving special offers that can’t or don’t use social media, including those from\nlower socioeconomic levels.” Woman, 63\n\n« “Algorithms are biased programs adhering to the views and beliefs of whomever is ordering\nand controlling the algorithm ... Someone has made a decision about the relevance of certain\ndata and once embedded in a reviewing program becomes irrefutable gospel, whether it is a\ngood indicator or not.” Man, 80\n\nAutomated criminal risk score\n\nThe 42% of Americans who think the use of this type of program is acceptable mention a range of\nreasons for feeling this way, with no single factor standing out from the others. Some 16% of these\nrespondents think this type of program is acceptable because it would be effective or because it’s\nhelpful for the justice system to have more information when making these decisions. A similar\nshare (13%) thinks this type of program would be acceptable if it is just one part of the decision-\nmaking process, while one-in-ten think it would be fairer and less biased than the current system.\n\nIn some cases, respondents use very different arguments to support the same outcome. For\ninstance, 9% of these respondents think this type of program is acceptable because it offers\nprisoners a second chance at being a productive member of society. But 6% support it because they\nthink it would help protect the public by keeping potentially dangerous individuals in jail who\nmight otherwise go free.\n\nSome examples:\n* “Prison and law enforcement officials have been doing this for hundreds of years already. It is\n\ncommon sense. Now that it has been identified and called a program or a process [that] does\nnot change anything.” Man, 71\n\nwww.pewresearch.org\n", "vlm_text": "\n▪   “It encroaches on someone’s ability to freely engage in activities online. It makes one want to  hide what they are buying – whether it is a present for a friend or a book to read. Why should  anyone have that kind of access to know my buying habits and take advantage of it in some  way? That kind of monitoring just seems very archaic. I can understand why this would be  done, from their point of view it helps to show what I as a customer would be interested in  buying. But I feel that there should be a line of some kind, and this crosses that line.”  Woman,  27   \n\n ▪   “I don’t think it is fair for companies to use my info without my permission, even if it would be  a special offer that would interest me. It is like spying, not acceptable. It would also exclude  people from receiving special offers that can’t or don’t use social media, including those from  lower socioeconomic levels.”  Woman, 63  \n\n ▪   “Algorithms are biased programs adhering to the views and beliefs of whomever is ordering  and controlling the algorithm ... Someone has made a decision about the relevance of certain  data and once embedded in a reviewing program becomes irrefutable gospel, whether it is a  good indicator or not.”  Man, 80   \nAutomated criminal risk score  \nThe  $42\\%$   of Americans who think the use of this type of program is acceptable mention a range of  reasons for feeling this way, with no single factor standing out from the others. Some  $16\\%$   of these  respondents think this type of program is acceptable because it would be effective or because it’s  helpful for the justice system to have more information when making these decisions. A similar  share   $(13\\%)$   thinks this type of program would be acceptable if it is just one part of the decision- making process, while one-in-ten think it would be fairer and less biased than the current system.  \nIn some cases, respondents use very different arguments to support the same outcome. For  instance,  $9\\%$   of these respondents think this type of program is acceptable because it offers  prisoners a second chance at being a productive member of society. But   $6\\%$   support it because they  think it would help protect the public by keeping potentially dangerous individuals in jail who  might otherwise go free.  \nSome examples:  \n▪   “Prison and law enforcement officials have been doing this for hundreds of years already. It is  common sense. Now that it has been identified and called a program or a process [that] does  not change anything.”  Man, 71   "}
{"page": 14, "image_path": "doc_images/PI_2018.11.19_algorithms_FINAL_14.jpg", "ocr_text": "14\nPEW RESEARCH CENTER\n\n“Because the other\noption is to rely Concerns over criminal risk scores focus on lack of\n\nentirely upon human individual focus, people’s ability to change\ndecisions, which are % of U.S. adults\nwho say it is___\nfor the criminal\nJustice system to\n\nAmong those who say acceptable,\n% who give these as the main reasons\n\nthemselves flawed and\nbiased. Both human\n\nintelligence and data use automated Would be effective\n\nshould be used.” Man, criminal risk Should be one-but only one—factor\n\n56 scores Would be more fair/unbiased\n\n“Right now, I think People deserve a second chance\n\nmany of these Need to identify repeat offenders | 6}\n\ndecisions are made Yel\" Acceptable —— People can change in future I 2\n\nsubjectively. If we can Need a human involved in the process | 1\n\nquantify risk by Unfair/could result in bias or profiling | 4\n\nobjective criteria that 7\n\nhave shown validity in Among those who say NOT acceptable,\n\nthe real world, we % who give these as the main reasons\n\nshould use it. Many ; Not Every individual/circumstance is diff.\n\nblack men are in baa acceptable ~~ People can change\n\nprison, it is probable Need a human involved in the process\n\nthat with more Unfair/could result in bias or profiling\n\nobjective criteria they Violates privacy fj 4\n\nwould be eligible for Should be one-but only one-factor fj 2\n\nparole. Similarly, other Would be fair/unbiased | 1\n\nracial/ethnic groups Note: Verbatim responses have been coded into categories. Results may add to more than\n\nmay be getting an 100% because multiple responses were allowed. Respondents who did not give an answer\nor gave other answers are not shown.\n\nundeserved break Source: Survey of U.S. adults conducted May 29-June 11, 2018.\n\nbecause of subjective “Public Attitudes Toward Computer Algorithms”\n\nbias. We need to be as PEW RESEARCH CENTER\n\nfair as possible to all\nindividuals, and this may help.” Man, 81\n\n“While such a program would have its flaws, the current alternative of letting people decide is\nfar more flawed.” Man, 42\n\n“As long as they have OTHER useful info to make their decisions then it would be acceptable.\nThey need to use whatever they have available that is truthful and informative to make such an\nimportant decision!” Woman, 63\n\nwww.pewresearch.org\n", "vlm_text": "▪   “Because the other  option is to rely  entirely upon human  decisions, which are  themselves flawed and  biased. Both human  intelligence and data  should be used.”  Man,  56  \n\n ▪   “Right now, I think  many of these  decisions are made  subjectively. If we can  quantify risk by  objective criteria that  have shown validity in  the real world, we  should use it. Many  black men are in  prison, it is probable  that with more  objective criteria they  would be eligible for  parole. Similarly, other  racial/ethnic groups  may be getting an  undeserved break  because of subjective  bias. We need to be as  fair as possible to all  individuals, and this may help.”  \nConcerns over criminal risk scores focus on lack of  individual focus, people’s ability to change  \nThe image is a chart showing the opinions of U.S. adults on the use of automated criminal risk scores by the criminal justice system. \n\n- 42% find it acceptable, while 56% find it not acceptable.\n  \nFor those who say it's acceptable, reasons include:\n- Would be effective (16%)\n- Should be one of several factors (13%)\n- Would be more fair/unbiased (10%)\n- People deserve a second chance (9%)\n- Need to identify repeat offenders (6%)\n- People can change in future (2%)\n- Need human involvement (1%)\n- Unfair/could result in bias/profiling (1%)\n\nFor those who say it's not acceptable, reasons include:\n- Every individual/circumstance is different (26%)\n- People can change (25%)\n- Need human involvement (12%)\n- Unfair/could result in bias/profiling (9%)\n- Violates privacy (4%)\n- Should be one of several factors (2%)\n- Would be fair/unbiased (1%)\nPEW RESEARCH CENTER  ▪   “While such a program would have its flaws, the current alternative of letting people decide is  far more flawed.”  Man, 42  \n\n \n\n▪   “As long as they have OTHER useful info to make their decisions then it would be acceptable.  They need to use whatever they have available that is truthful and informative to make such an  important decision!”  Woman, 63   "}
{"page": 15, "image_path": "doc_images/PI_2018.11.19_algorithms_FINAL_15.jpg", "ocr_text": "15\n\nThe 56% of Americans who think this type of program is not acceptable tend to focus on the\nefficacy of judging people in this manner. Some 26% of these responses argue that every individual\nor circumstance is different and that a computer program would have a hard time capturing these\nnuances. A similar share (25%) argues that this type of system precludes the possibility of personal\ngrowth or worries that the program might not have the best information about someone when\nmaking its assessment. And around one-in-ten worry about the lack of human involvement in the\nprocess (12%) or express concern that this system might result in unfair bias or profiling (9%).\n\nSome examples:\n\n“People should be looked at and judged as an individual, not based on some compilation of\n\nmany others. We are all very different from one another even if we have the same interests or\n\nideas or beliefs — we are an individual within the whole.” Woman, 71\n\n« “Two reasons: People can change, and data analysis can be wrong.” Woman, 63\n\n= “Because it seems like you’re determining a person’s future based on another person’s\nchoices.” Woman, 46\n\n* “Information about populations are not transferable to individuals. Take BMI [body mass\nindex] for instance. This measure was designed to predict heart disease in large populations\nbut has been incorrectly applied for individuals. So, a 6-foot-tall bodybuilder who weighs 240\nIbs is classified as morbidly obese because the measure is inaccurate. Therefore, information\nabout recidivism of populations cannot be used to judge individual offenders.” Woman, 54\n\n« “Data collection is often flawed and difficult to correct. Algorithms do not reflect the soul. As a\n\ndata scientist, I also know how often these are just wrong.” Man, 36\n\nVideo analysis of job candidates\n\nTwo themes stand out in the responses of the 32% of Americans who think it is acceptable to use\nthis tool when hiring job candidates. Some 17% of these respondents think companies should have\nthe right to hire however they see fit, while 16% think it is acceptable because it’s just one data\npoint among many in the interview process. Another 9% think this type of analysis would be more\nobjective than a traditional person-to-person interview.\n\nSome examples:\n« “Alls fair in commonly accepted business practices.” Man, 76\n« “They are analyzing your traits. I don’t have a problem with that.” Woman, 38\n\n« “Again, in this fast-paced world, with our mobile society and labor market, a semi-scientific\ntool bag is essential to stay competitive.” Man, 71\n\nwww.pewresearch.org\n", "vlm_text": "The  $56\\%$   of Americans who think this type of program is not acceptable tend to focus on the  efficacy of judging people in this manner. Some   $26\\%$   of these responses argue that every individual  or circumstance is different and that a computer program would have a hard time capturing these  nuances. A similar share   $(25\\%)$   argues that this type of system precludes the possibility of personal  growth or worries that the program might not have the best information about someone when  making its assessment. And around one-in-ten worry about the lack of human involvement in the  process   $\\left(\\mathbf{1}2\\%\\right)$   or express concern that this system might result in unfair bias or profiling   $(9\\%)$  .  \nSome examples: \n\n \n▪   “People should be looked at and judged as an individual, not based on some compilation of  many others. We are all very different from one another even if we have the same interests or  ideas or beliefs – we are an individual within the whole.”  Woman, 71  \n\n ▪   “Two reasons: People can change, and data analysis can be wrong.”  Woman, 63  \n\n ▪   “Because it seems like you’re determining a person’s future based on another person’s  choices.”  Woman, 46  \n\n ▪   “Information about populations are not transferable to individuals. Take BMI [body mass  index] for instance. This measure was designed to predict heart disease in large populations  but has been incorrectly applied for individuals. So, a 6-foot-tall bodybuilder who weighs 240  lbs is classified as morbidly obese because the measure is inaccurate. Therefore, information  about recidivism of populations cannot be used to judge individual offenders.”  Woman, 54  \n\n ▪   “Data collection is often flawed and difficult to correct. Algorithms do not reflect the soul. As a  data scientist, I also know how often these are just wrong.”  Man, 36   \nVideo analysis of job candidates  \nTwo themes stand out in the responses of the  $32\\%$   of Americans who think it is acceptable to use  this tool when hiring job candidates. Some  $17\\%$   of these respondents think companies should have  the right to hire however they see fit, while  $16\\%$   think it is acceptable because it’s just one data  point among many in the interview process. Another  $9\\%$   think this type of analysis would be more  objective than a traditional person-to-person interview.  \nSome examples: \n\n \n▪   “All’s fair in commonly accepted business practices.”  Man, 76  \n\n ▪   “They are analyzing your traits. I don’t have a problem with that.”  Woman, 38  \n\n ▪   “Again, in this fast-paced world, with our mobile society and labor market, a semi-scientific  tool bag is essential to stay competitive.”  Man, 71   "}
{"page": 16, "image_path": "doc_images/PI_2018.11.19_algorithms_FINAL_16.jpg", "ocr_text": "16\nPEW RESEARCH CENTER\n\n» “As long as the job\ncandidate agrees to this\nformat, I think it’s\nacceptable. Hiring % of U.S. adults\n\nConcerns over automated job interview analysis focus\non fairness, effectiveness, lack of human involvement\n\nsomeone entails a huge eal oe i Among those who say acceptable,\nfinancial investment and wise ee analysis % who give these as the main reasons\nthis might be a useful when hiring job Companies can hire however they want\ntool.” Woman, 61 candidates Just one data pt. in the process | 16]\n= “T think it’s acceptable to Would be more objective IE\nuse the product during the Acceptable with candidate knowledge [fj 4\ninterview. However, to use Ey Acceptable —— Humans should evaluate humans fj 2\nit as the deciding factor is Would not work/is flawed | 1\nludicrous. Interviews are L Is not fair |1\ntough and make -\ncandidates nervous, Among those who say NOT acceptable,\ntherefore I think that using % who give these as the main reasons\nthis is acceptable but poor Not Would not work/is flawed\nif used for final selection.” ey acceptable —— Humans should evaluate humans\nMan, 23 Is not fair\nNot everyone interviews well\nRespondents who think this Acceptable with candidate knowledge | 1\ntype of process is unacceptable Is weird/uncomfortable | 4\n\ntend to focus on whether It Note: Verbatim responses have been coded into categories. Results may add to more than\n\nwould work as intended. One- 100% because multiple responses were allowed. Respondents who did not give an answer\n. . or gave other answers are not shown.\nin-five argue that this type of Source: Survey of U.S. adults conducted May 29-June 11, 2018.\nanalysis simply won't work or “Public Attitudes Toward Computer Algorithms”\nPEW RESEARCH CENTER\n\nis flawed in some general way.\n\nA slightly smaller share (16%)\nmakes the case that humans should interview other humans, while 14% feel that this process is\njust not fair to the people being evaluated. And 13% feel that not everyone interviews well and that\nthis scoring system might overlook otherwise talented candidates.\n\nSome examples:\n= “T don’t think that characteristics obtained in this manner would be reliable. Great employees\ncan come in all packages.” Woman, 68\n\n« “Individuals may have attributes and strengths that are not evident through this kind of\nanalysis and they would be screened out based on the algorithm.” Woman, 57\n\nwww.pewresearch.org\n", "vlm_text": "▪   “As long as the job  candidate agrees to this  format, I think it’s  acceptable. Hiring  someone entails a huge  financial investment and  this might be a useful  tool.”  Woman, 61  \n\n ▪   “I think it’s acceptable to  use the product during the  interview. However, to use  it as the deciding factor is  ludicrous. Interviews are  tough and make  candidates nervous,  therefore I think that using  this is acceptable but poor  if used for final selection.”  Man, 23   \nRespondents who think this  type of process is unacceptable  tend to focus on whether it  would work as intended. One- in-five argue that this type of  analysis simply won’t work or  is flawed in some general way.  A slightly smaller share   $\\left(16\\%\\right)$    \nConcerns over automated job interview analysis focus  on fairness, effectiveness, lack of human involvement  \nThe image is an infographic representing the opinions of U.S. adults on the acceptability of companies using video analysis when hiring job candidates. \n\nKey points include:\n\n- 32% find it acceptable, while 67% do not.\n- Among those who say it's acceptable, the main reasons given are: \n  - \"Companies can hire however they want\" (17%),\n  - \"Just one data point in the process\" (16%),\n  - \"Would be more objective\" (9%),\n  - \"Acceptable with candidate knowledge\" (4%),\n  - \"Humans should evaluate humans\" (2%),\n  - \"Would not work/is flawed\" (1%),\n  - \"Is not fair\" (1%).\n  \n- Among those who say it's not acceptable, the main reasons are:\n  - \"Would not work/is flawed\" (20%),\n  - \"Humans should evaluate humans\" (16%),\n  - \"Is not fair\" (14%),\n  - \"Not everyone interviews well\" (13%),\n  - \"Acceptable with candidate knowledge\" (1%),\n  - \"Is weird/uncomfortable\" (1%).\nPEW RESEARCH CENTER  makes the case that humans should interview other humans, while  $14\\%$   feel that this process is  just not fair to the people being evaluated. And   $13\\%$   feel that not everyone interviews well and that  this scoring system might overlook otherwise talented candidates.  \n\nSome examples: \n\n \n▪   “I don’t think that characteristics obtained in this manner would be reliable. Great employees  can come in all packages.”  Woman, 68  \n\n \n▪   “Individuals may have attributes and strengths that are not evident through this kind of  analysis and they would be screened out based on the algorithm.”  Woman, 57    "}
{"page": 17, "image_path": "doc_images/PI_2018.11.19_algorithms_FINAL_17.jpg", "ocr_text": "17\n\n« “A person could be great in person but freeze during such an interview (on camera). Hire a\nperson not a robot if you are wanting a person doing a job. Interviews as described should only\nbe used for persons that live far away and can’t come in and then only to narrow down the\ncandidates for the job, then the last interview should require them to have a person to person\ninterview.” Woman, 61\n\n= “Some people do not interview well, and a computer cannot evaluate a person’s personality\nand how they relate to other people.” Man, 75\n\nAutomated resume screening\n\nwww.pewresearch.org\n", "vlm_text": "▪   “A person could be great in person but freeze during such an interview (on camera). Hire a  person not a robot if you are wanting a person doing a job. Interviews as described should only  be used for persons that live far away and can’t come in and then only to narrow down the  candidates for the job, then the last interview should require them to have a person to person  interview.”  Woman, 61   \n\n ▪   “Some people do not interview well, and a computer cannot evaluate a person’s personality  and how they relate to other people.”  Man, 75    \nAutomated resume screening  "}
{"page": 18, "image_path": "doc_images/PI_2018.11.19_algorithms_FINAL_18.jpg", "ocr_text": "18\nPEW RESEARCH CENTER\n\nThe 41% of Americans who think it is acceptable for companies to use this type of program give\nthree major reasons for feeling this way. Around one-in-five (19%) find it acceptable because the\n\ncompany using the process would save a great deal of time and money. An identical share thinks it\nwould be more accurate than screening resumes by hand, and 16% feel that companies can hire\n\nhowever they want to hire.\nSome examples:\n\n= “Have you ever tried to\nsort through hundreds of\napplications? A program\nprovides a non-partial\nmeans of evaluating\napplicants. It may not be\nperfect, but it is efficient.”\nWoman, 65\n\n= “While I wouldn’t do this\nfor my company, I simply\nthink it’s acceptable\nbecause private companies\nshould be able to use\nwhatever methods they\nwant as long as they don’t\nillegally discriminate. I\nhappen to think some\npotentially good\ncandidates would be\npassed over using this\nmethod, but I wouldn’t say\nan organization shouldn’t\nbe allowed to do it this\nway.” Man, 50\n\n= “Tfit eliminates resumes\nthat don’t meet criteria, it\nallows the hiring process\n\nConcerns over automated resume screening focus on\nfairness, lack of human involvement\n\n% of U.S. adults\nwho say it is__\nfor companies to\nuse automated\n\nresume Among those who say acceptable,\nscreening when | % who give these as the main reasons\nhiring job\n\ncandidates Saves time/money EE\n\nWould be more accurate | 19]\n\nCompanies can hire however they want\n\nRemoves human element from process\nWould remove bias\n\nOK as long as it’s not the whole process\n\nvuEA Acceptable —\n\nIs not fair/may not get best person\n\nResumes are bad/system can be gamed lz\n\nAmong those who say NOT acceptable,\n\n% who give these as the main reasons\nNot\n\n<¥i7) acceptable — Removes human element from process\nIs not fair/may not get best person\nResumes are bad/system can be gamed\nWould be more accurate | 1\n\nNote: Verbatim responses have been coded into categories. Results may add to more than\n100% because multiple responses were allowed. Respondents who did not give an answer\nor gave other answers are not shown.\n\nSource: Survey of U.S. adults conducted May 29-June 11, 2018.\n\n“Public Attitudes Toward Computer Algorithms”\n\nPEW RESEARCH CENTER\n\nwww.pewresearch.org\n", "vlm_text": "The  $41\\%$   of Americans who think it is acceptable for companies to use this type of program give  three major reasons for feeling this way. Around one-in-five   $(19\\%)$   find it acceptable because the  company using the process would save a great deal of time and money. An identical share thinks it  would be more accurate than screening resumes by hand, and   $16\\%$   feel that companies can hire  however they want to hire.  \nSome examples: \n\n \n▪   “Have you ever tried to  sort through hundreds of  applications? A program  provides a non-partial  means of evaluating  applicants. It may not be  perfect, but it is efficient.”  Woman, 65   \n\n ▪   “While I wouldn’t do this  for my company, I simply  think it’s acceptable  because private companies  should be able to use  whatever methods they  want as long as they don’t  illegally discriminate. I  happen to think some  potentially good  candidates would be  passed over using this  method, but I wouldn’t say  an organization shouldn’t  be allowed to do it this  way.”  Man, 50   \n\n ▪   “If it eliminates resumes  that don’t meet criteria, it  allows the hiring process  \nConcerns over automated resume screening focus on  fairness, lack of human involvement  \nThe image is a bar chart showing the percentage of U.S. adults who find it acceptable or not acceptable for companies to use automated resume screening when hiring job candidates. \n\n- 41% of respondents say it is acceptable, and among those:\n  - 19% find it acceptable because it saves time/money.\n  - 19% think it would be more accurate.\n  - 16% believe companies can hire however they want.\n  - 6% note it removes the human element from the process.\n  - 5% say it would remove bias.\n  - 5% are okay with it as long as it’s not the whole process.\n  - 4% consider it is not fair/may not get the best person.\n  - 2% think resumes are bad/system can be gamed.\n\n- 57% say it is not acceptable, and among those:\n  - 36% say it removes the human element from the process.\n  - 23% believe it is not fair/may not get the best person.\n  - 16% say resumes are bad/system can be gamed.\n  - 1% think it would be more accurate.\nPEW RESEARCH CENTER  "}
{"page": 19, "image_path": "doc_images/PI_2018.11.19_algorithms_FINAL_19.jpg", "ocr_text": "19\n\nto be more efficient.” Woman, 43\n\nThose who find the process unacceptable similarly focus on three major themes. Around one-third\n(36%) worry that this type of process takes the human element out of hiring. Roughly one-quarter\n(23%) feel that this system is not fair or would not always get the best person for the job. And 16%\nworry that resumes are simply not a good way to choose job candidates and that people could\ngame the system by putting in keywords that appeal to the algorithm.\n\nHere are some samples of these responses:\n\n« “Again, you are taking away the human component. What if a very qualified person couldn’t\nafford to have a professional resume writer do his/her resume? The computer would kick it\nout.” Woman, 72\n\n* “Companies will get only employees who use certain words, phrases, or whatever the\nparameters of the search are. They will miss good candidates and homogenize their\nworkforce.” Woman, 48\n\n« “The likelihood that a program kicks a resume, and the human associated with it, for minor\nquirks in terminology grows. The best way to evaluate humans is with humans.” Man, 54\n\n* “It’s just like taking standardized school tests, such as the SAT, ACT, etc. There are teaching\nprograms to help students learn how to take the exams and how to ‘practice’ with various\nexamples. Therefore, the results are not really comparing the potential of all test takers, but\nrather gives a positive bias to those who spend the time and money learning how to take the\ntest.” Man, 64\n\nwww.pewresearch.org\n", "vlm_text": "to be more efficient.”  Woman, 43 \nThose who find the process unacceptable similarly focus on three major themes. Around one-third \n\n  $(36\\%)$   worry that this type of process takes the human element out of hiring. Roughly one-quarter \n\n  $({\\it23\\%})$   feel that this system is not fair or would not always get the best person for the job. And  $16\\%$    worry that resumes are simply not a good way to choose job candidates and that people could  game the system by putting in keywords that appeal to the algorithm.  \nHere are some samples of these responses: \n\n \n▪   “Again, you are taking away the human component. What if a very qualified person couldn’t  afford to have a professional resume writer do his/her resume? The computer would kick it  out.”  Woman, 72   \n\n ▪   “Companies will get only employees who use certain words, phrases, or whatever the  parameters of the search are. They will miss good candidates and homogenize their  workforce.”  Woman, 48   \n\n ▪   “The likelihood that a program kicks a resume, and the human associated with it, for minor  quirks in terminology grows. The best way to evaluate humans is with humans.”  Man, 54   \n\n ▪   “It’s just like taking standardized school tests, such as the SAT, ACT, etc. There are teaching  programs to help students learn how to take the exams and how to ‘practice’ with various  examples. Therefore, the results are not really comparing the potential of all test takers, but  rather gives a positive bias to those who spend the time and money learning how to take the  test.”  Man, 64   "}
{"page": 20, "image_path": "doc_images/PI_2018.11.19_algorithms_FINAL_20.jpg", "ocr_text": "20\nPEW RESEARCH CENTER\n\n2. Algorithms in action: The content people see on social\n\nmedia\n\nThe social media environment is another prominent example of algorithmic decision-making in\nAmericans’ daily lives. Nearly all the content people see on social media is chosen not by human\neditors but rather by computer programs using massive quantities of data about each user to\n\ndeliver content that he or she might find relevant or engaging. This has led to widespread concerns\nthat these sites are promoting content that is attention-grabbing but ultimately harmful to users —\n\nsuch as misinformation, sensationalism or “hate clicks.”\n\nTo more broadly understand public attitudes toward algorithms\nin this context, the survey asked respondents a series of questions\nabout the content they see on social media, the emotions that\ncontent arouses, and their overall comfort level with these sites\nusing their data to serve them different types of information. And\nlike the questions around the impact of algorithms discussed in\nthe preceding chapter, this portion of the survey led with a broad\nquestion about whether the public thinks social media reflects\noverall public sentiment.\n\nOn this score, a majority of Americans (74%) think the content\npeople post on social media does not provide an accurate picture\nof how society feels about important issues, while one-quarter say\nit does. Certain groups of Americans are more likely than others\nto think that social media paints an accurate picture of society\nwrit large. Notably, blacks (37%) and Hispanics (35%) are more\nlikely than whites (20%) to say this is the case. And the same is\ntrue of younger adults compared with their elders: 35% of 18- to\n29-year-olds think that social media paints an accurate portrait of\nsociety, but that share drops to 19% among those ages 65 and\nolder. Still, despite these differences, a majority of Americans\n\nMost think social media\ndoes not accurately\nreflect society\n\n% of U.S. adults who say the content\non social media____ provide an\naccurate picture of how society feels\nabout important issues\n\nDoes not\n74%\n\nNo\n\nanswer\nSource: Survey of U.S. adults conducted\nMay 29-June 11, 2018.\n\nPublic Attitudes Toward Computer\nAlgorithms\n\nPEW RESEARCH CENTER\n\nacross a wide range of demographic groups feel that social media is not representative of public\n\nopinion more broadly.\n\nwww.pewresearch.org\n", "vlm_text": "2. Algorithms in action: The content people see on social  media  \nThe social media environment is another prominent example of algorithmic decision-making in  Americans’ daily lives. Nearly all the content people see on social media is chosen not by human  editors but rather by computer programs using massive quantities of data about each user to  deliver content that he or she might find relevant or engaging. This has led to widespread concerns  that these sites are promoting content that is attention-grabbing but ultimately harmful to users –  such as misinformation, sensationalism or “hate clicks.”  \nTo more broadly understand public attitudes toward algorithms  in this context, the survey asked respondents a series of questions  about the content they see on social media, the emotions that  content arouses, and their overall comfort level with these sites  using their data to serve them different types of information. And  like the questions around the impact of algorithms discussed in  the preceding chapter, this portion of the survey led with a broad  question about whether the public thinks social media reflects  overall public sentiment.  \nOn this score, a majority of Americans   $\\left(74\\%\\right)$   think the content  people post on social media does  not  provide an accurate picture  of how society feels about important issues, while one-quarter say  it does. Certain groups of Americans are more likely than others  to think that social media paints an accurate picture of society  writ large. Notably, blacks   $\\left(37\\%\\right)$   and Hispanics   $\\left(35\\%\\right)$  ) are more  likely than whites   $\\left(\\boldsymbol{20\\%}\\right)$   to say this is the case. And the same is  true of younger adults compared with their elders:   $35\\%$   of  ${\\bf18-}$   to  29-year-olds think that social media paints an accurate portrait of  society, but that share drops to  $19\\%$   among those ages 65 and  older. Still, despite these differences, a majority of Americans  across a wide range of demographic groups feel that social media is  \nMost think social media  does not accurately  reflect society  \n $\\%$   of U.S. adults who say the content  on social media ___ provide an  accurate picture of how society feels  about important issues  \nThe image is a pie chart that shows the distribution of responses among three categories. The sections are labeled as follows:\n- \"Does\": This section represents 25% and is shown in a lighter blue color.\n- \"Does not\": This section represents 74% and is shown in a darker blue color.\n- \"No answer\": This section represents 1% and is depicted as a narrow gray slice.\nSource: Survey of U.S. adults conducted  May 29-June 11, 2018.  “Public Attitudes Toward Computer  Algorithms”  \nPEW RESEARCH CENTER  \nopinion more broadly.  "}
{"page": 21, "image_path": "doc_images/PI_2018.11.19_algorithms_FINAL_21.jpg", "ocr_text": "21\nPEW RESEARCH CENTER\n\nSocial media users frequently\n\nencounter content that Social media users experience a mix of positive,\nsparks feelings of negative emotions while using these platforms\namusement but also see % of social media users who say they_____ see content on social media that\n\nmaterial that angers them makes them fee\n\nWhen asked about six different Frequently Sometimes NET\nemotions that they might Amused 88\n\nexperience due to the content\n\n. : Angry 71\nthey see on social media, the\nlargest share of users (88% in Connected 72\ntotal) say they see content on Inspired 69\nthese sites that makes them feel\n. Depressed\namused. Amusement is also the p 49\nemotion that the largest share Lonely Wi 24 31\n0)\n\nof users (44%) frequently Note: Respondents who did not give an answer or gave other answers are not shown.\nexperience on these sites. Source: Survey of U.S. adults conducted May 29-June 11, 2018.\n\n“Public Attitudes Toward Computer Algorithms”\n\nPEW RESEARCH CENTER\n\nSocial media also leads many\n\nusers to feel anger. A total of\n\n71% of social media users report encountering content that makes them angry, and one-quarter\nsee this type of content frequently. Similar shares say they encounter content that makes them feel\nconnected (71%) or inspired (69%). Meanwhile, around half (49%) say they encounter content that\nmakes them feel depressed, and 31% indicate that they at least sometimes see content that makes\nthem feel lonely.\n\nIdentical shares of users across a range of age groups say they frequently encounter content on\nsocial media that makes them feel angry. But other emotions exhibit more variation based on age.\nNotably, younger adults are more likely than older adults to say they frequently encounter content\non social media that makes them feel lonely. Some 15% of social media users ages 18 to 29 say this,\ncompared with 7% of those ages 30 to 49 and just 4% of those 50 and older. Conversely, a\nrelatively small share of older adults are frequently amused by content they see on social media. In\nfact, similar shares of social media users ages 65 and older say they frequently see content on these\nplatforms that makes them feel amused (30%) and angry (24%).\n\nwww.pewresearch.org\n", "vlm_text": "Social media users frequently  encounter content that  sparks feelings of  amusement but also see  material that angers them  \nWhen asked about six different  emotions that they might  experience due to the content  they see on social media, the  largest share of users (  $88\\%$   in  total) say they see content on  these sites that makes them feel  amused. Amusement is also the  emotion that the largest share  of users   $(44\\%)$    frequently   experience on these sites.   \nSocial media users experience a mix of positive,  negative emotions while using these platforms  \n $\\%$   of social media users who say they ___ see content on social media that  makes them feel …  \nThe image is a bar chart showing the frequency of different emotions experienced. Each emotion is split into \"Frequently\" and \"Sometimes\", with a total \"NET\" score. \n\n- **Amused**: Frequently 44, Sometimes 44, NET 88\n- **Angry**: Frequently 25, Sometimes 47, NET 71\n- **Connected**: Frequently 21, Sometimes 49, NET 71\n- **Inspired**: Frequently 16, Sometimes 53, NET 69\n- **Depressed**: Frequently 13, Sometimes 36, NET 49\n- **Lonely**: Frequently 7, Sometimes 24, NET 31\nSocial media also leads many  users to feel anger. A total of  \n $71\\%$   of social media users report encountering content that makes them angry, and one-quarter  see this type of content frequently. Similar shares say they encounter content that makes them feel  connected   $(71\\%)$   or inspired   $(69\\%)$  . Meanwhile, around half   $(49\\%)$   say they encounter content that  makes them feel depressed, and  $31\\%$   indicate that they at least sometimes see content that makes  them feel lonely.  \nIdentical shares of users across a range of age groups say they frequently encounter content on  social media that makes them feel angry. But other emotions exhibit more variation based on age.  Notably, younger adults are more likely than older adults to say they frequently encounter content  on social media that makes them feel lonely. Some  $15\\%$   of social media users ages 18 to 29 say this,  compared with  $7\\%$   of those ages 30 to 49 and just  $4\\%$   of those 50 and older. Conversely, a  relatively small share of older adults are frequently amused by content they see on social media. In  fact, similar shares of social media users ages 65 and older say they frequently see content on these  platforms that makes them feel amused   $\\left(30\\%\\right)$   and angry   $({\\it24\\%})$  .  "}
{"page": 22, "image_path": "doc_images/PI_2018.11.19_algorithms_FINAL_22.jpg", "ocr_text": "22\n\nA recent Pew Research Center\n\n: ; Larger share of young social media users say these\nanalysis of congressional\n\nplatforms frequently make them feel amused - but\nalso lonely and depressed\n\n: % of social media users in each age group who say they frequently see\nmost common reaction to posts = content on social media that makes them feel...\n\nFacebook pages found that the\n“anger” emoticon is now the\n\nby members of Congress. And\n\nalthough this survey did not ask Ages 65+ 50-64 30-49 18-29\n\nAmused O—® ee\n\nabout the specific types of 30% 39 515A\n\ncontent that might make people\n\nangry, it does find a modest Angry o: oe\ncorrelation between the\n\nfrequency with which users see Connected on)\ncontent that makes them angry 15 2023 25\n\nand their overall political Inspired 0 @\naffiliation. Some 31% of 9 161719\nconservative Republicans say Depressed @e\nthey frequently feel angry due 4142 17\nto things they see on social\nmedia (compared with 19% of\nmoderate or liberal\nRepublicans), as do 27% of\n\nliberal Democrats (compared\n\nLonely OC} ©\n257 15\n\n: PEW RESEARCH CENTER\nwith 19% of moderate or\n\nconservative Democrats).\n\nSocial media users frequently encounter people being overly dramatic or starting\narguments before waiting for all the facts to emerge\n\nAlong with asking about the emotions social media platforms inspire in users, the survey also\nincluded a series of questions about how often social media users encounter certain types of\nbehaviors and content. These findings indicate that users see two types of content especially\nfrequently: posts that are overly dramatic or exaggerated (58% of users say they see this type of\ncontent frequently) and people making accusations or starting arguments without waiting until\nthey have all the facts (59% see this frequently).\n\nA majority of social media users also say they at least sometimes encounter posts that appear to be\nabout one thing but turn out to be about something else, as well as posts that teach them\n\nwww.pewresearch.org\n", "vlm_text": "A recent Pew Research Center  analysis  of congressional  Facebook pages found that the  “anger” emoticon is now the  most common reaction to posts  by members of Congress. And  although this survey did not ask  about the specific types of  content that might make people  angry, it does find a modest  correlation between the  frequency with which users see  content that makes them angry  and their overall political  affiliation. Some   $31\\%$   of  conservative Republicans say  they frequently feel angry due  to things they see on social  media (compared with  $19\\%$   of  moderate or liberal  Republicans), as do   $27\\%$   of  liberal Democrats (compared  with  $19\\%$   of moderate or  conservative Democrats).  \nLarger share of young social media users say these  platforms frequently make them feel amused – but  also lonely and depressed  \n $\\%$   of social media users in each age group who say they frequently see  content on social media that makes them feel…  \nThe image is a horizontal dot plot displaying survey data about the emotional responses of different age groups to humorous or amusing content. The age groups are coded by color: blue for Ages 65+, light blue for 50-64, dark blue for 30-49, and green for 18-29. Each emotional response (Amused, Angry, Connected, Inspired, Depressed, and Lonely) is plotted with dots representing the percentage of people in each age group who experienced that emotion.\n\nHere is a breakdown of the emotional responses by age group:\n- **Amused:** 30% (65+), 39% (50-64), 51% (30-49), 54% (18-29)\n- **Angry:** 23% (65+), 24% (50-64), 25% (30-49), 27% (18-29)\n- **Connected:** 15% (65+), 20% (50-64), 23% (30-49), 25% (18-29)\n- **Inspired:** 9% (65+), 16% (50-64), 17% (30-49), 19% (18-29)\n- **Depressed:** 11% (65+), 12% (50-64), 12% (30-49), 17% (18-29)\n- **Lonely:** 2% (65+), 5% (50-64), 7% (30-49), 15% (18-29)\n\nThe chart depicts how different age groups react emotionally, particularly highlighting that younger people (18-29) tend to report stronger emotions across all categories compared to older age groups.\nPEW RESEARCH CENTER  \nSocial media users frequently encounter people being overly dramatic or starting  arguments before waiting for all the facts to emerge  \nAlong with asking about the emotions social media platforms inspire in users, the survey also  included a series of questions about how often social media users encounter certain types of  behaviors and content. These findings indicate that users see two types of content especially  frequently: posts that are overly dramatic or exaggerated (  $58\\%$   of users say they see this type of  content frequently) and people making accusations or starting arguments without waiting until  they have all the facts   $(59\\%$   see this frequently).  \nA majority of social media users also say they at least sometimes encounter posts that appear to be  about one thing but turn out to be about something else, as well as posts that teach them   "}
{"page": 23, "image_path": "doc_images/PI_2018.11.19_algorithms_FINAL_23.jpg", "ocr_text": "something useful they hadn’t\nknown before. But in each\ninstance, fewer than half say\nthey see these sorts of posts\nfrequently.\n\nBeyond the emotions they feel\nwhile browsing social media,\nusers are exposed to a mix of\npositive and negative behaviors\nfrom others. Around half (54%)\nof social media users say they\ntypically see an equal mix of\npeople being kind or supportive\nand people being mean or\nbullying. Around one-in-five\n(21%) say they more often see\npeople being kind and\nsupportive on these sites, while\na comparable share (24%) says\n\n23\nPEW RESEARCH CENTER\n\nMajorities of social media users frequently see people\nengaging in drama and exaggeration, jumping into\narguments without having all the facts\n% of social media users who say they_____ see the following types of content\non social media\n\nFrequently Sometimes NET\n\nPosts that are overly dramatic\nor exaggerated\n\n58 31\nPeople making accusations or\nstarting arguments without 59 28 87\nhaving all the facts\n21 57\n33 45\n\n88\n\nPosts that teach you something\n\nuseful you hadn't known before 79\n\nPosts that appear to be about\none thing but turn out to be\nabout something else\n\n78\n\nNote: Respondents who did not give an answer or gave other answers are not shown.\nSource: Survey of U.S. adults conducted May 29-June 11, 2018.\n“Public Attitudes Toward Computer Algorithms”\n\nPEW RESEARCH CENTER\n\nthey more often see people being mean or bullying.\n\nwww.pewresearch.org\n", "vlm_text": "something useful they hadn’t  known before. But in each  instance, fewer than half say  they see these sorts of posts  frequently.   \nBeyond the emotions they feel  while browsing social media,  users are exposed to a mix of  positive and negative behaviors  from others. Around half   $\\left(54\\%\\right)$    of social media users say they  typically see an equal mix of  people being kind or supportive  and people being mean or  bullying. Around one-in-five   $\\left(\\boldsymbol{\\it{21\\%}}\\right)$   say they more often see  people being kind and  supportive on these sites, while  a comparable share   $\\left({\\it24\\%}\\right)$   says  \nMajorities of social media users frequently see people  engaging in drama and exaggeration, jumping into  arguments without having all the facts  \n% of social media users who say they ___ see the following types of content  on social media  \nThe image is a bar chart that displays how frequently certain types of posts occur, based on survey responses. There are four categories of posts, and for each category, participants indicated whether they encounter these posts \"Frequently\" or \"Sometimes.\" The NET column represents the combined percentage of \"Frequently\" and \"Sometimes\" responses for each type.\n\n1. Posts that are overly dramatic or exaggerated:\n   - Frequently: 58%\n   - Sometimes: 31%\n   - NET: 88%\n\n2. People making accusations or starting arguments without having all the facts:\n   - Frequently: 59%\n   - Sometimes: 28%\n   - NET: 87%\n\n3. Posts that teach you something useful you hadn't known before:\n   - Frequently: 21%\n   - Sometimes: 57%\n   - NET: 79%\n\n4. Posts that appear to be about one thing but turn out to be about something else:\n   - Frequently: 33%\n   - Sometimes: 45%\n   - NET: 78%\nPEW RESEARCH CENTER  \nthey more often see people being mean or bullying.  "}
{"page": 24, "image_path": "doc_images/PI_2018.11.19_algorithms_FINAL_24.jpg", "ocr_text": "24\nPEW RESEARCH CENTER\n\nPrevious surveys by the Center\n\nhave found that men are Men somewhat more likely than women to see people\nslightly more likely than women being bullying, deceptive on social media\nto encounter any sort of % of social media users who say they more often see when using these\nharassing or abusive behavior — *““S\nonline. And in this instance, a People being\n\n. People being kind or\nslightly larger share of men mean or bullying —_ supportive Equal mix of both\n\n(29%) than women (19%) say Total\nthey more often see people Men\nbeing mean or bullying content\non social media platforms than\n\nsee kind behavior. Women, on\n\nthe other hand, are slightly People trying\n\nPeople trying to to point out\n\nmore likely than men to say be deceptive inaccurate info Equal mix of both\nthat they more often see people Total\nbeing kind or supportive. But Men\nthe largest shares of both men Women\n\nO, 9,\n(52%) and women (56%) say Note: Respondents who did not give a are not shown.\nthat they typically see an equal —_ Source: Survey of U.S onducted May 29-June 11, 201!\n\n. . . Public Attitudes Toward Computer Algorithms\nmix of supportive and bullying\nPEW RESEARCH CENTER\n\n8\n\nbehavior on social media.\n\nWhen asked about the efforts they see other users making to spread — or correct — misinformation,\naround two-thirds of users (63%) say they generally see an even mix of people trying to be\ndeceptive and people trying to point out inaccurate information. Similar shares more often see one\nof these behaviors than others, with 18% of users saying they more often see people trying to be\ndeceptive and 17% saying they more often see people trying to point out inaccurate information.\nMen are around twice as likely as women to say they more often seeing people being deceptive on\nsocial media (24% vs. 13%). But majorities of both men (58%) and women (67%) see an equal mix\nof deceptiveness and attempts to correct misinformation.\n\nwww.pewresearch.org\n", "vlm_text": "Previous  surveys by the Center   have found that men are  slightly more likely than women  to encounter any sort of  harassing or abusive behavior  online. And in this instance, a  slightly larger share of men   $(29\\%)$   than women   $({\\bf19\\%})$   say  they more often see people  being mean or bullying content  on social media platforms than  see kind behavior. Women, on  the other hand, are slightly  more likely than men to say  that they more often see people  being kind or supportive. But  the largest shares of both men   $\\left(52\\%\\right)$   and women   $(56\\%)$   say  that they typically see an equal  mix of supportive and bullying  behavior on social media.  \nMen somewhat more likely than women to see people  being bullying, deceptive on social media  \n $\\%$   of social media users who say they more often see ___when using these  sites  \nThe image presents two sets of bar graphs comparing perceptions of online behavior between men and women. The first set evaluates perceptions of people being mean or bullying, being kind or supportive, and an equal mix of both. The second set looks at people trying to be deceptive, trying to point out inaccurate information, and an equal mix of both.\n\n1. **People being mean or bullying**:\n   - Total: 24%\n   - Men: 29%\n   - Women: 19%\n\n2. **People being kind or supportive**:\n   - Total: 21%\n   - Men: 17%\n   - Women: 24%\n\n3. **Equal mix of both (mean/kind)**:\n   - Total: 54%\n   - Men: 52%\n   - Women: 56%\n\n4. **People trying to be deceptive**:\n   - Total: 18%\n   - Men: 24%\n   - Women: 13%\n\n5. **People trying to point out inaccurate info**:\n   - Total: 17%\n   - Men: 17%\n   - Women: 17%\n\n6. **Equal mix of both (deceptive/pointing out inaccuracies)**:\n   - Total: 63%\n   - Men: 58%\n   - Women: 67%\n\nThe data suggests differences in perceptions between men and women regarding these online behaviors.\nPEW RESEARCH CENTER  \nWhen asked about the efforts they see other users making to spread – or correct – misinformation,  around two-thirds of users   $(63\\%)$   say they generally see an even mix of people trying to be  deceptive and people trying to point out inaccurate information. Similar shares more often see one  of these behaviors than others, with  $18\\%$   of users saying they more often see people trying to be  deceptive and   $17\\%$   saying they more often see people trying to point out inaccurate information.  Men are around twice as likely as women to say they more often seeing people being deceptive on  social media (  $\\mathbf{\\tilde{2}}4\\%$   vs.  $13\\%$  ). But majorities of both men   $(58\\%)$   and women  $(67\\%)$   see an equal mix  of deceptiveness and attempts to correct misinformation.  "}
{"page": 25, "image_path": "doc_images/PI_2018.11.19_algorithms_FINAL_25.jpg", "ocr_text": "25\nPEW RESEARCH CENTER\n\nUsers’ comfort level with\nsocial media companies Users are relatively comfortable with social platforms\n\nusing their personal data using their data for some purposes, but not others\n\ndepends on how their data % of social media users who say it is____ for social media sites to use data\n\nabout them and their online activities to ...\nare used\n\nwe Not at all Not very Somewhat Very\nThe vast quantities of data that acceptable acceptable acceptable acceptable\n\nsocial media companies possess\n\n. . . Recommend events in\nabout their users — including their area 11\n\n50 25\nbehaviors, likes, clicks and\n. . . Recommend someone they 19 43 14\nother information users provide might want to know\nabout themselves — are Show them ads for products 24 fl “4\nultimately what allows these or services\nplatforms to deliver Show them messages from 7\nindividually targeted content in political campaigns\nan automated fashion. And this _ \\ote: Respondents who did not give an answer are not shown.\n\nsurvey finds that users’ comfort Source: Survey of U adults ted May 29-June 11, 2018.\nPublic Attitudes Toward Computer Algorithms’\n\nPEW RESEARCH CENTER\n\n34 30\n\nlevel with this behavior is\nheavily context-dependent.\n\nThey are relatively accepting of their data being used for certain types of messages, but much less\ncomfortable when it is used for other purposes.\n\nThree-quarters of social media users find it acceptable for those platforms to use data about them\nand their online behavior to recommend events in their area that they might be interested in, while\na smaller majority (57%) thinks it is acceptable if their data are used to recommend other people\nthey might want to be friends with.\n\nOn the other hand, users are somewhat less comfortable with these sites using their data to show\nadvertisements for products or services. Around half (52%) think this behavior is acceptable, but a\nsimilar share (47%) finds it to be not acceptable — and the share that finds it not at all acceptable\n(21%) is roughly double the share who finds it very acceptable (11%). Meanwhile, a substantial\nmajority of users think it is not acceptable for social media platforms to use their data to deliver\nmessages from political campaigns — and 31% say this is not acceptable at all.\n\nRelatively sizable majorities of users across a range of age groups think it is acceptable for social\nmedia sites to use their data to show them events happening in their area. And majorities of users\nacross a range of age categories feel it is not acceptable for social platforms to use their data to\nserve them ads from political campaigns.\n\nwww.pewresearch.org\n", "vlm_text": "Users’ comfort level with  social media companies  using their personal data  depends on how their data  are used  \nThe vast quantities of data that  social media companies possess  about their users – including  behaviors, likes, clicks and  other information users provide  about themselves – are  ultimately what allows these  platforms to deliver  individually targeted content in  an automated fashion. And this  survey finds that users’ comfort  level with this behavior is  heavily context-dependent.  \nUsers are relatively comfortable with social platforms  using their data for some purposes, but not others  \n% of social media users who say it is ___ for social media sites to use data  about them and their online activities to …  \nThis image is a bar chart showing the acceptability of different online recommendations. The categories are:\n\n1. **Recommend events in their area**:\n   - Not at all acceptable: 11%\n   - Not very acceptable: 14%\n   - Somewhat acceptable: 50%\n   - Very acceptable: 25%\n\n2. **Recommend someone they might want to know**:\n   - Not at all acceptable: 19%\n   - Not very acceptable: 24%\n   - Somewhat acceptable: 43%\n   - Very acceptable: 14%\n\n3. **Show them ads for products or services**:\n   - Not at all acceptable: 21%\n   - Not very acceptable: 26%\n   - Somewhat acceptable: 41%\n   - Very acceptable: 11%\n\n4. **Show them messages from political campaigns**:\n   - Not at all acceptable: 31%\n   - Not very acceptable: 31%\n   - Somewhat acceptable: 30%\n   - Very acceptable: 7%\nPEW RESEARCH CENTER  \nThey are relatively accepting of their data being used for certain types of messages, but much less  comfortable when it is used for other purposes.  \nThree-quarters of social media users find it acceptable for those platforms to use data about them  and their online behavior to recommend events in their area that they might be interested in, while  a smaller majority  $\\left(57\\%\\right)$   thinks it is acceptable if their data are used to recommend other people  they might want to be friends with.  \nOn the other hand, users are somewhat less comfortable with these sites using their data to show  advertisements for products or services. Around half  $\\left(52\\%\\right)$   think this behavior is acceptable, but a  similar share   $(47\\%)$   finds it to be not acceptable – and the share that finds it  not at all  acceptable   $\\left(\\boldsymbol{\\it{21\\%}}\\right)$   is roughly double the share who finds it  very  acceptable   $\\left(\\mathbf{11}\\%\\right)$  . Meanwhile, a substantial  majority of users think it is  not  acceptable for social media platforms to use their data to deliver  messages from political campaigns – and  $31\\%$   say this is not acceptable at all.  \nRelatively sizable majorities of users across a range of age groups think it is acceptable for social  media sites to use their data to show them events happening in their area. And majorities of users  across a range of age categories feel it is  not  acceptable for social platforms to use their data to  serve them ads from political campaigns.   "}
{"page": 26, "image_path": "doc_images/PI_2018.11.19_algorithms_FINAL_26.jpg", "ocr_text": "26\nPEW RESEARCH CENTER\n\nBut outside of these specific\n\nsimilarities, older users are Social media users from a range of age groups are\n\nmuch less accepting of social wary of their data being used to deliver political\n\naan . . messages\n\nmedia sites using their data for . i ; ; ;\n\nth This i + % of social media users who say it is acceptable for social media sites to use\nomer reasons. fils 1s mos data about them and their online activities to ...\npronounced when it comes to\nusing that data to recommend Recommend events Ages 65+ 50-64 18-29 30-49\n\n: in their area O-0-@\nother people they might know. 67% 72 78 80\n1 9\nBy a two-to-one margin (66% to Recommend someone\n33%), social media users ages they might want to Oo ee\n: oe know 36 53 6667\n\n18 to 49 think this is an\nacceptable use of their data. But — Show them ads for O—¢0-©\nby a similar 63% to 36% products and services 39 5154 60\nmargin, users ages 65 and older Show them messages 0\nsay this is not acceptable. from political campaigns 31 35 38 40\n\nSimilarly, nearly six-in-ten\nusers ages 18 to 49 think it is\n\nacceptable for these sites to use d Computer Al\ntheir data to show them PEW RESEARCH CENTER\n\nadvertisements for products or\n\nservices, but that share falls to 39% among those 65 and older.\n\nBeyond using their personal data in these specific ways, social media users express consistent and\npronounced opposition to these platforms changing their sites in certain ways for some users but\nnot others. Roughly eight-in-ten social media users think it is unacceptable for these platforms to\ndo things like remind some users but not others to vote on election day (82%), or to show some\nusers more of their friends’ happy posts and fewer of their sad posts (78%). And even the standard\nA/B testing that most platforms engage in on a continuous basis is viewed with much suspicion by\nusers: 78% of users think it is unacceptable for social platforms to change the look and feel of their\nsite for some users but not others.\n\nOlder users are overwhelmingly opposed to these interventions. But even among younger users,\nlarge shares find them problematic even in their most common forms. For instance, 71% of social\nmedia users ages 18 to 29 say it is unacceptable for these sites to change the look and feel for some\nusers but not others.\n\nwww.pewresearch.org\n", "vlm_text": "But outside of these specific  similarities, older users are  much less accepting of social  media sites using their data for  other reasons. This is most  pronounced when it comes to  using that data to recommend  other people they might know.  By a two-to-one margin (  ${}^{66\\%}$   to   $33\\%$  , social media users ages  18 to 49 think this is an  acceptable use of their data. But  by a similar  $63\\%$   to  $36\\%$    margin, users ages 65 and older  say this is  not  acceptable.  Similarly, nearly six-in-ten  users ages 18 to 49 think it is  acceptable for these sites to use  their data to show them  advertisements for products or  \nSocial media users from a range of age groups are  wary of their data being used to deliver political  messages  \n $\\%$   of social media users who say it is acceptable for social media sites to use  data about them and their online activities to …  \nThis image is a chart comparing how different age groups feel about certain types of recommendations and advertisements they receive. The age groups are color-coded as follows:\n\n- Ages 65+ (light blue)\n- Ages 50-64 (dark blue)\n- Ages 18-29 (green)\n- Ages 30-49 (blue-green)\n\nFor each of the four categories listed, dots represent the percentage of people in each age group that approves:\n\n1. **Recommend events in their area:** \n   - 67% of Ages 65+\n   - 72% of Ages 50-64\n   - 78% of Ages 18-29\n   - 80% of Ages 30-49\n\n2. **Recommend someone they might want to know:**\n   - 36% of Ages 65+\n   - 53% of Ages 50-64\n   - 66% of Ages 18-29\n   - 67% of Ages 30-49\n\n3. **Show them ads for products and services:**\n   - 39% of Ages 65+\n   - 51% of Ages 50-64\n   - 54% of Ages 18-29\n   - 60% of Ages 30-49\n\n4. **Show them messages from political campaigns:**\n   - 31% of Ages 65+\n   - 35% of Ages 50-64\n   - 38% of Ages 18-29\n   - 40% of Ages 30-49\n\nEach line has dots indicating the percentage for each age group.\nPEW RESEARCH CENTER  \nservices, but that share falls to  $39\\%$   among those 65 and older.  \nBeyond using their personal data in these specific ways, social media users express consistent and  pronounced opposition to these platforms changing their sites in certain ways for some users but  not others. Roughly eight-in-ten social media users think it is unacceptable for these platforms to  do things like remind some users but not others to vote on election day   $(82\\%)$  , or to show some  users more of their friends’ happy posts and fewer of their sad posts   $(78\\%)$  . And even the standard   $\\mathbf{A}/\\mathbf{B}$   testing that most platforms engage in on a continuous basis is viewed with much suspicion by  users:  $78\\%$   of users think it is unacceptable for social platforms to change the look and feel of their  site for some users but not others.  \nOlder users are overwhelmingly opposed to these interventions. But even among younger users,  large shares find them problematic even in their most common forms. For instance,  $71\\%$   of social  media users ages 18 to 29 say it is unacceptable for these sites to change the look and feel for some  users but not others.  "}
{"page": 27, "image_path": "doc_images/PI_2018.11.19_algorithms_FINAL_27.jpg", "ocr_text": "27\n\nAcknowledgements\n\nThis report is a collaborative effort based on the input and analysis of the following individuals.\nFind related reports online at pewresearch.org/internet.\n\nPrimary researchers\n\nAaron Smith, Associate Director, Research\n\nResearch team\n\nLee Rainie, Director, Internet and Technology Research\nKenneth Olmstead, Research Associate\n\nJingjing Jiang, Research Analyst\n\nAndrew Perrin, Research Analyst\n\nPaul Hitlin, Senior Researcher\n\nMeg Hefferon, Research Analyst\n\nEditorial and graphic design\n\nMargaret Porteus, Information Graphics Designer\nTravis Mitchell, Copy Editor\n\nCommunications and web publishing\n\nHaley Nolan, Communications Assistant\nSara Atske, Assistant Digital Producer\n\nwww.pewresearch.org\n", "vlm_text": "Acknowledgements  \nThis report is a collaborative effort based on the input and analysis of the following individuals.  Find related reports online at  pew research.org/internet .  \nPrimary researchers  \nAaron Smith,  Associate Director, Research  \nResearch team  \nLee Rainie,  Director, Internet and Technology Research Kenneth Olmstead,  Research Associate   Jingjing Jiang,  Research Analyst  Andrew Perrin , Research Analyst  Paul Hitlin , Senior Researcher  Meg Hefferon,  Research Analyst   \nEditorial and graphic design  \nMargaret Porteus,  Information Graphics Designer Travis Mitchell,  Copy Editor   \nCommunications and web publishing  \nHaley Nolan,  Communications Assistant  Sara Atske,  Assistant Digital Producer  "}
{"page": 28, "image_path": "doc_images/PI_2018.11.19_algorithms_FINAL_28.jpg", "ocr_text": "28\nPEW RESEARCH CENTER\n\nThe American Trends Panel Survey Methodology\n\nThe American Trends Panel (ATP), created by Pew Research Center, is a nationally representative\npanel of randomly selected U.S. adults recruited from landline and cellphone random-digit-dial\n(RDD) surveys. Panelists participate via monthly self-administered web surveys. Panelists who do\nnot have internet access are provided with a tablet and wireless internet connection. The panel is\nbeing managed by GfK.\n\nData in this report are drawn from the panel wave conducted May 29-June 11, 2018, among 4,594\nrespondents. The margin of sampling error for the full sample of 4,594 respondents is plus or\nminus 2.4 percentage points.\n\nMembers of the American Trends Panel were recruited from several large, national landline and\ncellphone RDD surveys conducted in English and Spanish. At the end of each survey, respondents\nwere invited to join the panel. The first group of panelists was recruited from the 2014 Political\nPolarization and Typology Survey, conducted Jan. 23 to March 16, 2014. Of the 10,013 adults\ninterviewed, 9,809 were invited to take part in the panel and a total of 5,338 agreed to participate.\nThe second group of panelists was recruited from the 2015 Pew Research Center Survey on\nGovernment, conducted Aug. 27 to Oct. 4, 2015. Of the 6,004 adults interviewed, all were invited\nto join the panel, and 2,976 agreed to participate.? The third group of panelists was recruited from\na survey conducted April 25 to June 4, 2017. Of the 5,012 adults interviewed in the survey or\npretest, 3,905 were invited to take part in the panel and a total of 1,628 agreed to participate.3\n\nThe ATP data were weighted in a multistep process that begins with a base weight incorporating\nthe respondents’ original survey selection probability and the fact that in 2014 some panelists were\nsubsampled for invitation to the panel. Next, an adjustment was made for the fact that the\npropensity to join the panel and remain an active panelist varied across different groups in the\nsample. The final step in the weighting uses an iterative technique that aligns the sample to\npopulation benchmarks on a number of dimensions. Gender, age, education, race, Hispanic origin\nand region parameters come from the U.S. Census Bureau’s 2016 American Community Survey.\nThe county-level population density parameter (deciles) comes from the 2010 U.S. decennial\ncensus. The telephone service benchmark comes from the July-December 2016 National Health\n\n1 When data collection for the 2014 Political Polarization and Typology Survey began, non-internet users were subsampled at a rate of 25%,\nbut a decision was made shortly thereafter to invite all non-internet users to join. In total, 83% of non-internet users were invited to join the\npanel.\n\n2 Respondents to the 2014 Political Polarization and Typology Survey who indicated that they are internet users but refused to provide an\nemail address were initially permitted to participate in the American Trends Panel by mail, but were no longer permitted to join the panel after\nFeb. 6, 2014. Internet users from the 2015 Pew Research Center Survey on Government who refused to provide an email address were not\npermitted to join the panel.\n\n3 White, non-Hispanic college graduates were subsampled at a rate of 50%.\n\nwww.pewresearch.org\n", "vlm_text": "The American Trends Panel Survey Methodology  \nThe American Trends Panel (ATP), created by Pew Research Center, is a nationally representative  panel of randomly selected U.S. adults recruited from landline and cellphone random-digit-dial  (RDD) surveys. Panelists participate via monthly self-administered web surveys. Panelists who do  not have internet access are provided with a tablet and wireless internet connection. The panel is  being managed by GfK.  \nData in this report are drawn from the panel wave conducted May 29-June 11, 2018, among 4,594  respondents. The margin of sampling error for the full sample of 4,594 respondents is plus or  minus 2.4 percentage points.   \nMembers of the American Trends Panel were recruited from several large, national landline and  cellphone RDD surveys conducted in English and Spanish. At the end of each survey, respondents  were invited to join the panel. The first group of panelists was recruited from the 2014 Political  Polarization and Typology Survey, conducted Jan. 23 to March 16, 2014. Of the 10,013 adults  interviewed, 9,809 were invited to take part in the panel and a total of 5,338 agreed to participate. The second group of panelists was recruited from the 2015 Pew Research Center Survey on  Government, conducted Aug. 27 to Oct. 4, 2015. Of the 6,004 adults interviewed, all were invited  to join the panel, and 2,976 agreed to participate.  The third group of panelists was recruited from  a survey conducted April 25 to June 4, 2017. Of the 5,012 adults interviewed in the survey or  pretest, 3,905 were invited to take part in the panel and a total of 1,628 agreed to participate.   \nThe ATP data were weighted in a multistep process that begins with a base weight incorporating  the respondents’ original survey selection probability and the fact that in 2014 some panelists were  subsampled for invitation to the panel. Next, an adjustment was made for the fact that the  propensity to join the panel and remain an active panelist varied across different groups in the  sample. The final step in the weighting uses an iterative technique that aligns the sample to  population benchmarks on a number of dimensions. Gender, age, education, race, Hispanic origin  and region parameters come from the U.S. Census Bureau’s 2016 American Community Survey.  The county-level population density parameter (deciles) comes from the 2010 U.S. decennial  census. The telephone service benchmark comes from the July-December 2016 National Health  "}
{"page": 29, "image_path": "doc_images/PI_2018.11.19_algorithms_FINAL_29.jpg", "ocr_text": "29\nPEW RESEARCH CENTER\n\nInterview Survey and is projected to 2017. The volunteerism benchmark comes from the 2015\nCurrent Population Survey Volunteer Supplement. The party affiliation benchmark is the average\nof the three most recent Pew Research Center general public telephone surveys. The internet\naccess benchmark comes from the 2017 ATP Panel Refresh Survey. Respondents who did not\npreviously have internet access are treated as not having internet access for weighting purposes.\nSampling errors and statistical tests of significance take into account the effect of weighting.\nInterviews are conducted in both English and Spanish, but the Hispanic sample in the ATP is\npredominantly native born and English speaking.\n\nThe following table shows the unweighted sample sizes and the error attributable to sampling that\nwould be expected at the 95% level of confidence for different groups in the survey:\n\nUnweighted\nGroup sample size Plus or minus ...\nTotal sample 4,594 2.4 percentage points\n18-29 469 7.5 percentage points\n30-49 1,343 4.4 percentage points\n50-64 1,451 4.3 percentage points\n65+ 1,326 4.5 percentage points\n\nSample sizes and sampling errors for other subgroups are available upon request.\n\nIn addition to sampling error, one should bear in mind that question wording and practical\ndifficulties in conducting surveys can introduce error or bias into the findings of opinion polls.\n\nThe May 2018 wave had a response rate of 84 % (4,594 responses among 5,486 individuals in the\npanel). Taking account of the combined, weighted response rate for the recruitment surveys\n(10.0%) and attrition from panel members who were removed at their request or for inactivity, the\ncumulative response rate for the wave is 2.4%.4\n\n© Pew Research Center, 2018\n\n4 Approximately once per year, panelists who have not participated in multiple consecutive waves are removed from the panel. These cases\nare counted in the denominator of cumulative response rates.\n\nwww.pewresearch.org\n", "vlm_text": "Interview Survey and is projected to 2017. The volunteer is m benchmark comes from the 2015  Current Population Survey Volunteer Supplement. The party affiliation benchmark is the average  of the three most recent Pew Research Center general public telephone surveys. The internet  access benchmark comes from the 2017 ATP Panel Refresh Survey. Respondents who did not  previously have internet access are treated as not having internet access for weighting purposes.  Sampling errors and statistical tests of significance take into account the effect of weighting.  Interviews are conducted in both English and Spanish, but the Hispanic sample in the ATP is  predominantly native born and English speaking.   \nThe table provides information about sample sizes and margins of error for different age groups in a study or survey:\n\n- **Total sample**: \n  - Unweighted sample size: 4,594\n  - Margin of error: ±2.4 percentage points\n\n- **Age group 18-29**: \n  - Unweighted sample size: 469\n  - Margin of error: ±7.5 percentage points\n\n- **Age group 30-49**: \n  - Unweighted sample size: 1,343\n  - Margin of error: ±4.4 percentage points\n\n- **Age group 50-64**: \n  - Unweighted sample size: 1,451\n  - Margin of error: ±4.3 percentage points\n\n- **Age group 65+**: \n  - Unweighted sample size: 1,326\n  - Margin of error: ±4.5 percentage points\nSample sizes and sampling errors for other subgroups are available upon request.  \nIn addition to sampling error, one should bear in mind that question wording and practical  difficulties in conducting surveys can introduce error or bias into the findings of opinion polls.  \nThe May 2018 wave had a response rate of   $84\\,\\%$   (4,594 responses among 5,486 individuals in the  panel). Taking account of the combined, weighted response rate for the recruitment surveys   $\\left(10.0\\%\\right)$   and attrition from panel members who were removed at their request or for inactivity, the  cumulative response rate for the wave is  $2.4\\%.4$    \n $\\copyright$   Pew Research Center, 2018  "}
