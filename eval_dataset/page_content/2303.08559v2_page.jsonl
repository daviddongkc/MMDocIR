{"page": 0, "image_path": "doc_images/2303.08559v2_0.jpg", "ocr_text": "arXiv:2303.08559v2 [cs.CL] 21 Oct 2023\n\nLarge Language Model Is Not a Good Few-shot Information Extractor,\nbut a Good Reranker for Hard Samples!\n\nLo ies 2 . L Ages 1\n> ’ >\nYubo Ma’, Yixin Cao~, YongChing Hong’, Aixin Sun\n\n1 §-Lab, Nanyang Technological University\n? Singapore Management University\nyubo001@e.ntu.edu.sg\n\nAbstract\n\nLarge Language Models (LLMs) have made\nremarkable strides in various tasks. Whether\nLLMs are competitive few-shot solvers for in-\nformation extraction (IE) tasks, however, re-\nmains an open problem. In this work, we\naim to provide a thorough answer to this ques-\ntion. Through extensive experiments on nine\ndatasets across four IE tasks, we demonstrate\nthat current advanced LLMs consistently ex-\nhibit inferior performance, higher latency, and\nincreased budget requirements compared to\nfine-tuned SLMs under most settings. There-\nfore, we conclude that LLMs are not effec-\ntive few-shot information extractors in gen-\neral ', Nonetheless, we illustrate that with\nappropriate prompting strategies, LLMs can\neffectively complement SLMs and tackle chal-\nlenging samples that SLMs struggle with. And\nmoreover, we propose an adaptive filter-then-\nrerank paradigm to combine the strengths of\nLLMs and SLMs. In this paradigm, SLMs\nserve as filters and LLMs serve as rerankers.\nBy prompting LLMs to rerank a small portion\nof difficult samples identified by SLMs, our pre-\nliminary system consistently achieves promis-\ning improvements (2.4% F1-gain on average)\non various IE tasks, with an acceptable time\nand cost investment. Our code is available at\nhttps: //github.com/mayubo2333/LLM-IE.\n\n1 Introduction\n\nLarge Language Models (LLMs, Brown et al. 2020;\nChowdhery et al. 2022; Touvron et al. 2023) have\nshown remarkable abilities on various NLP applica-\ntions such as factual question answering (Yu et al.,\n2023; Sun et al., 2023), arithmetic reasoning (Chen\net al., 2022a; Qian et al., 2023) and logical rea-\nsoning (Jung et al., 2022; Pan et al., 2023). Given\nthe reasoning, memorization, instruction-following\nand few-shot adaption capabilities emerging from\n\n'A more precise assertion is that current LLMs, with\n\nvanilla prompting setting and without IE-specific fine-tuning,\nare not good few-shot information extractors in general.\n\nLLMs, it prompts a compelling question: Can\nLLMs be used to boost performance in few-shot\ninformation extraction (IE) tasks?\n\nTo answer this question, we conduct an exten-\nsive empirical study to compare the performance\nbetween LLMs using in-context learning 2 (ICL)\nand fine-tuned Small Language Models (SLMs).\nWe fairly evaluate SLMs-based and LLMs-based\nmethods across nine datasets spanning four com-\nmon IE tasks: (1) Named Entity Recognition, (2)\nRelation Extraction, (3) Event Detection and (4)\nEvent Argument Extraction. For each dataset, we\nexplored four to six settings to encompass typi-\ncal low-resource extents, from 1-shot to 20-shot or\neven more. Given the potential sensitivity of LLMs’\nperformance to the prompt context, we meticu-\nlously considered variations in instruction, demon-\nstration number and selection strategy, prompt for-\nmat, etc. Our study reveals that LLMs excel over\nSLMs only when annotations are extremely lim-\nited, i.e., both label types* and the samples* per\nlabel are extremely scarce. With more (e.g., hun-\ndreds of) samples, SLMs significantly outperform\nLLMs. Furthermore, LLMs incur greater inference\nlatency and costs than fine-tuned SLMs. Hence, we\nclaim that current LLMs are not good few-shot\ninformation extractors in general.\n\nWe further investigate whether LLMs and SLMs\nexhibit different abilities to handle various types of\nsamples. We categorize samples according to their\ndifficulty measured by SLMs’ confidence scores,\nand compare LLMs’ and SLMs’ results within each\ngroup. We find that LLMs are good at hard sam-\nples, though bad at easy samples. We posit that\nthe knowledge and reasoning abilities in LLMs en-\nable them to handle hard samples (which are sim-\n\n? All LLMs discussed in this paper are not fine-tuned, and\nresults for LLMs are based on in-context learning.\n\n3Label types denote entity/relation/event/role types in dif-\nferent tasks. We use them interchangeably there-in-after.\n\n“Samples refer to (i) demonstrations in ICL of LLMs, or\n(ii) training samples for SLMs’ fine-tuning.\n", "vlm_text": "Large Language Model Is Not a Good Few-shot Information  Extractor , but a Good  Reranker  for Hard Samples! \nYubo Ma 1 , Yixin   $\\mathbf{Caa0^{2}}$  , YongChing Hong 1 , Aixin Sun 1 \n1  S-Lab, Nanyang Technological University 2  Singapore Management University yubo001@e.ntu.edu.sg \nAbstract \nLarge Language Models (LLMs) have made remarkable strides in various tasks. Whether LLMs are competitive few-shot solvers for in- formation extraction (IE) tasks, however, re- mains an open problem. In this work, we aim to provide a thorough answer to this ques- tion. Through extensive experiments on nine datasets across four IE tasks, we demonstrate that current advanced LLMs consistently ex- hibit inferior performance, higher latency, and increased budget requirements compared to fine-tuned SLMs under most settings. There- fore, we conclude that LLMs are not effec- tive few-shot information extractors in gen- eral   1 . Nonetheless, we illustrate that with appropriate prompting strategies, LLMs can effectively complement SLMs and tackle chal- lenging samples that SLMs struggle with. And moreover, we propose an adaptive  filter-then- rerank  paradigm to combine the strengths of LLMs and SLMs. In this paradigm, SLMs serve as filters and LLMs serve as rerankers. By prompting LLMs to rerank a small portion of difficult samples identified by SLMs, our pre- liminary system consistently achieves promis- ing improvements (  $2.4\\%$   F1-gain on average) on various IE tasks, with an acceptable time and cost investment. Our code is available at https://github.com/mayubo2333/LLM-IE . \n1 Introduction \nLarge Language Models (LLMs,  Brown et al. 2020 ; Chowdhery et al. 2022 ;  Touvron et al. 2023 ) have shown remarkable abilities on various NLP applica- tions such as factual question answering ( Yu et al. , 2023 ;  Sun et al. ,  2023 ), arithmetic reasoning ( Chen et al. ,  2022a ;  Qian et al. ,  2023 ) and logical rea- soning ( Jung et al. ,  2022 ;  Pan et al. ,  2023 ). Given the reasoning, memorization, instruction-following and few-shot adaption capabilities emerging from LLMs, it prompts a compelling question: Can LLMs be used to boost performance in few-shot information extraction (IE) tasks? \n\nTo answer this question, we conduct an exten- sive empirical study to compare the performance between LLMs using  in-context learning   2   (ICL) and  fine-tuned  Small Language Models (SLMs). We fairly evaluate SLMs-based and LLMs-based methods across nine datasets spanning four com- mon IE tasks: (1) Named Entity Recognition, (2) Relation Extraction, (3) Event Detection and (4) Event Argument Extraction. For each dataset, we explored four to six settings to encompass typi- cal low-resource extents, from 1-shot to 20-shot or even more. Given the potential sensitivity of LLMs’ performance to the prompt context, we meticu- lously considered variations in instruction, demon- stration number and selection strategy, prompt for- mat,  etc . Our study reveals that LLMs excel over SLMs only when annotations are extremely lim- ited,  i.e.,  both label types 3   and the samples 4   per label are extremely scarce. With more ( e.g.,  hun- dreds of) samples, SLMs significantly outperform LLMs. Furthermore, LLMs incur greater inference latency and costs than fine-tuned SLMs. Hence, we claim that  current LLMs are not good few-shot information extractors in general . \nWe further investigate whether LLMs and SLMs exhibit different abilities to handle various types of samples. We categorize samples according to their difficulty measured by SLMs’ confidence scores, and compare LLMs’ and SLMs’ results within each group. We find that  LLMs are good at hard sam- ples, though bad at easy samples . We posit that the knowledge and reasoning abilities in LLMs en- able them to handle hard samples (which are sim- ply beyond SLMs’ capabilities) well. Nevertheless, LLMs demonstrate strong predisposition to false- positive predictions on negative samples. Since most negative samples are easy samples (which could be solved readily by SLMs), the performance of LLMs on easy samples sometimes collapses and are usually much worse than fine-tuned SLMs. "}
{"page": 1, "image_path": "doc_images/2303.08559v2_1.jpg", "ocr_text": "ply beyond SLMs’ capabilities) well. Nevertheless,\nLLMs demonstrate strong predisposition to false-\npositive predictions on negative samples. Since\nmost negative samples are easy samples (which\ncould be solved readily by SLMs), the performance\nof LLMs on easy samples sometimes collapses and\nare usually much worse than fine-tuned SLMs.\n\nLeveraging these findings, we pursue an ap-\nproach to incorporate LLMs and SLMs within a\nsingle system and combine their merits. To this end,\nwe propose a novel filter-then-rerank framework.\nThe basic idea is that SLMs serve as a filter and\nLLMs as a reranker. Specifically, SLMs initially\npredict and determine the difficulty of each sample.\nIf the sample is a hard one, we further pass the\ntop-V most-likely candidate labels from SLMs to\nLLMs for reranking. Otherwise we view the predic-\ntion from SLMs as the final decision. By providing\neasy/hard samples with different solution strategies,\nour system utilizes each model’s strengths to com-\nplement each other. Also, it reranks only a small\nsubset of samples and minimizes the extra latency\nand budgets for calling LLMs. With a modest cost\nincrease, our framework yields a consistent F1 im-\nprovement, averaging 2.4% higher than previous\nmethods on various few-shot IE tasks. To the best\nof our knowledge, this is the first successful attempt\nto use LLMs to enhance few-shot IE tasks.\n\n2 Related Work\n\n2.1 LLMs for Information Extraction\n\nRecent studies have increasingly explored Informa-\ntion Extraction (IE) tasks using LLMs. Drawing in-\nspiration from instruction tuning (Wei et al., 2022a),\nseveral methods (Wadhwa et al., 2023; Wang et al.,\n2023a; Lu et al., 2023) transform annotated sam-\nples into instruction-answer pairs and then fine-\ntune LLMs, such as FlanT5 (Chung et al., 2022),\non them. Nonetheless, this method necessitates a\nvast range of samples with diverse schemas and\noften yields suboptimal results in low-resource sce-\nnarios. In the context of few-shot IE tasks, preva-\nlent strategies bifurcate into two main streams. The\nfirst approach perceives LLMs as efficient annota-\ntors (Ding et al., 2023; Josifoski et al., 2023). In\nthese methods, they produce a plethora of pseudo-\nlabeled samples through LLMs and leverage the\nenhanced annotations to train SLMs. Conversely,\nthe latter approach employs LLMs in inference us-\ning the ICL paradigm, which is the focus of our\nsubsequent discussion.\n\n2.2 Few-shot IE with ICL\n\nRegarding few-shot IE tasks, recent studies inten-\nsively compare the performance between SLMs\nand LLMs but yield inconsistent conclusions.\nSome studies favor LLMs as competent few-shot\nextractors (Agrawal et al., 2022; Wang et al.,\n2023b; Li et al., 2023; Zhang et al., 2023a;\nWadhwa et al., 2023), while others dispute this\nclaim (Jimenez Gutierrez et al., 2022; Qin et al.,\n2023; Wei et al., 2023; Gao et al., 2023). This\ndiscrepancy leaves the question of whether LLMs\nperform competitively on few-shot IE tasks unre-\nsolved, thus hindering the advances of this domain.\n\nWe attribute such disagreement to the absence\nof an comprehensive and unified benchmark. Ex-\nisting studies usually vary in tasks, datasets, and\n‘ew-shot settings. Furthermore, some studies rely\non overly simplistic datasets (Jimenez Gutierrez\net al., 2022; Li et al., 2023) and may exaggerate\nhe effectiveness of LLMs. Driven by these find-\nings, our research undertakes comprehensive ex-\nperiments across four IE tasks, nine datasets with\nvarious schema complexities (from coarse-grained\n0 fine-grained) and low-resource settings.\n\nIn addition to the empirical study, we develop an\ninnovative filter-then-rerank paradigm to combine\nhe strengths of both LLMs and SLMs. It utilizes\nprompting strategies akin to QA4RE (Zhang et al.,\n2023a), transforming IE tasks into multi-choice\nquestions. However, our method stands apart by\nintegrating SLMs and LLMs within a single frame-\nwork. This incorporation (1) enables our paradigm\napplicable to various IE tasks by providing candi-\ndate spans in the text and (2) achieves promising\nperformance under low-resource IE scenarios.\n\n3 Large LMs v.s. Small LMs\n\nIn this section, we compare the performance be-\ntween LLMs and SLMs to evaluate whether LLMs\nperform competitively.\n\n3.1 Task, Dataset and Evaluation\n\nWe run experiments on nine widely-used datasets\nacross four IE tasks. (1) Named Entity Recognition\n(NER): CONLLO3 (Tjong Kim Sang and De Meul-\nder, 2003), OntoNotes (Weischedel et al., 2013)\nand FewNERD (Ding et al., 2021). (2) Relation\nExtraction (RE): TACRED (Zhang et al., 2017)\nand TACREV (Alt et al., 2020). (3) Event De-\ntection (ED): ACE05 (Doddington et al., 2004),\nMAVEN (Wang et al., 2020) and ERE (Song et al.,\n", "vlm_text": "\nLeveraging these findings, we pursue an ap- proach to incorporate LLMs and SLMs within a single system and combine their merits. To this end, we propose a novel  filter-then-rerank  framework. The basic idea is that SLMs serve as a filter and LLMs as a reranker. Specifically, SLMs initially predict and determine the difficulty of each sample. If the sample is a hard one, we further pass the top-  $\\cdot N$   most-likely candidate labels from SLMs to LLMs for reranking. Otherwise we view the predic- tion from SLMs as the final decision. By providing easy/hard samples with different solution strategies, our system utilizes each model’s strengths to com- plement each other. Also, it reranks only a small subset of samples and minimizes the extra latency and budgets for calling LLMs. With a modest cost increase, our framework yields a consistent F1 im- provement, averaging  $2.4\\%$   higher than previous methods on various few-shot IE tasks. To the best of our knowledge, this is the first successful attempt to use LLMs to enhance few-shot IE tasks. \n2 Related Work \n2.1 LLMs for Information Extraction \nRecent studies have increasingly explored Informa- tion Extraction (IE) tasks using LLMs. Drawing in- spiration from instruction tuning ( Wei et al. ,  2022a ), several methods ( Wadhwa et al. ,  2023 ;  Wang et al. ,  $2023\\mathrm{a}$  ;  Lu et al. ,  2023 ) transform annotated sam- ples into instruction-answer pairs and then fine- tune LLMs, such as FlanT5 ( Chung et al. ,  2022 ), on them. Nonetheless, this method necessitates a vast range of samples with diverse schemas and often yields suboptimal results in low-resource sce- narios. In the context of few-shot IE tasks, preva- lent strategies bifurcate into two main streams. The first approach perceives LLMs as efficient annota- tors ( Ding et al. ,  2023 ;  Josifoski et al. ,  2023 ). In these methods, they produce a plethora of pseudo- labeled samples through LLMs and leverage the enhanced annotations to train SLMs. Conversely, the latter approach employs LLMs in inference us- ing the ICL paradigm, which is the focus of our subsequent discussion. \n2.2 Few-shot IE with ICL \nRegarding few-shot IE tasks, recent studies inten- sively compare the performance between SLMs and LLMs but yield inconsistent conclusions. Some studies favor LLMs as competent few-shot extractors ( Agrawal et al. ,  2022 ;  Wang et al. , 2023b ;  Li et al. ,  2023 ;  Zhang et al. ,  2023a ; Wadhwa et al. ,  2023 ), while others dispute this claim ( Jimenez Gutierrez et al. ,  2022 ;  Qin et al. , 2023 ;  Wei et al. ,  2023 ;  Gao et al. ,  2023 ). This discrepancy leaves the question of  whether LLMs perform competitively on few-shot IE tasks  unre- solved, thus hindering the advances of this domain. We attribute such disagreement to the absence of an comprehensive and unified benchmark. Ex- isting studies usually vary in tasks, datasets, and few-shot settings. Furthermore, some studies rely on overly simplistic datasets ( Jimenez Gutierrez et al. ,  2022 ;  Li et al. ,  2023 ) and may exaggerate the effectiveness of LLMs. Driven by these find- ings, our research undertakes comprehensive ex- periments across four IE tasks, nine datasets with various schema complexities (from coarse-grained to fine-grained) and low-resource settings. \nIn addition to the empirical study, we develop an innovative  filter-then-rerank  paradigm to combine the strengths of both LLMs and SLMs. It utilizes prompting strategies akin to QA4RE ( Zhang et al. , 2023a ), transforming IE tasks into multi-choice questions. However, our method stands apart by integrating SLMs and LLMs within a single frame- work. This incorporation (1) enables our paradigm applicable to various IE tasks by providing candi- date spans in the text and (2) achieves promising performance under low-resource IE scenarios. \n3 Large LMs v.s. Small LMs \nIn this section, we compare the performance be- tween LLMs and SLMs to evaluate whether LLMs perform competitively. \n3.1 Task, Dataset and Evaluation \nWe run experiments on nine widely-used datasets across four IE tasks. (1) Named Entity Recognition (NER): CONLL03 ( Tjong Kim Sang and De Meul- der ,  2003 ), OntoNotes ( Weischedel et al. ,  2013 ) and FewNERD ( Ding et al. ,  2021 ). (2) Relation Extraction (RE): TACRED ( Zhang et al. ,  2017 ) and TACREV ( Alt et al. ,  2020 ). (3) Event De- tection (ED): ACE05 ( Doddington et al. ,  2004 ), MAVEN ( Wang et al. ,  2020 ) and ERE ( Song et al. , "}
{"page": 2, "image_path": "doc_images/2303.08559v2_2.jpg", "ocr_text": "Named Entity Recognition\n\nRelation Extraction\n\nIdentify the entities then locate each entity to words in\nsentence. The possible entity types are building-hotel, ......\n\nSentence: The 1962 National Football League draft was held on\nDecember 4th, 1961 at the Sheraton Hotel in Chicago, Illinois.\n\nEntities: (type: organization-sportsleague, entity: 1962 National\nFootball League), (type: building-hotel, entity: Sheraton Hotel)\nSentence: Critics noted “The Manual of Detection” combines\n\nelements from several genres, including mystery and fantasy.\nEntities:\n\nIdentify the relation between the entities in the sentence. The\npossible relation types are per:title, org:top_members, ......\nSentence: Five NGOs that form [tstart] TOAID [tend] are\nCompassion International Taiwan, the Eden Social Welfare\nFoundation, the Field Relief Agency of Taiwan, [hstart] NCF\n[hend] and the Taiwan Root Medical Peace Corps.\n\nTriple: (Subj: NCF, Obj: TOAID, Relation: org:member_of)\nSentence: Fuller, 37, has been there before.\n\nTriple: (Subj: Fuller, Obj: 37, Relation:\n\nEvent Detection\n\nIdentify the events then Locate each event to words in sentence.\nThe possible event types are Life.Injure,......\n\nSentence: Injured Russian diplomats were among victims caught in\ncrossfire and friendly fire Sunday.\n\nEvents: (type: Life.Injure, trigger_word:\nConflict.attack, trigger_word: crossfire)\nSentence: Iraqi Kurds fear that Turkish troops could move deep\ninto the enclave in northern Iraq and have threatened clashes.\n\nEvents:\n\n(type: Movement.Transport, trigger_word: move),\n(type: Conflict.attack, trigger_word: clashes)\n\nInjured), (type:\n\nEvent Argument Extraction\n\nGiven a sentence and an event, you need to identify all\narguments of this event, and classify role of each argument.\n\nSentence: Cash-strapped Vivendi wants to sell Universal Studios\nand its TV production company. Event: Transfer-Ownership,\nTrigger_word: sell, Possible roles: Seller, Artifact,......\nArguments: (type: Seller, argument: Vivendi), (type: Artifact,\nargument: Universal Studio and its TV production company)\nSentence: Another appeal is pending in the Federal Court. Event:\nJustice-Appeal, Trigger_word: appeal, Possible roles: Adjudicator,\nPlaintiff, Place...\n\nArguments :\n| (type: Adjudicator, argument: the Federal Court) |\n\nFigure 1: Examples of prompts used. The green, blue and black parts in the top boxes represent the instruction,\ndemonstration (demo) and test sentence in the prompt respectively. The red parts represent the outputs from LLMs.\nWe plot only 1 example for convenience of visualization. The actual demo number is usually much larger than 1.\n\n2015). (4) Event Argument Extraction (EAE):\nACE0S5, ERE and RAMS (Ebner et al., 2020). With\nlabel numbers ranging from 4 to 168, we assess\nLLMs’ performance under different schema com-\nplexities. See their details in Appendix A.1.\nFew-shot Set We construct few-shot datasets from\nthe original datasets above. For training and vali-\ndation set, we adopt /-shot sampling strategy, i.e.,\nsampling A samples for each label type. See more\ndetails in Appendix A.2. For test set, we down-\nsample their original test sets to reduce the cost\nof LLMs. We randomly sample 500 sentences for\nRE tasks, and 250 sentences for other task. We en-\nsure that each label has at least one corresponding\nsample to avoid the absence of rare labels.\nEvaluation We adopt micro-F1 score in NER, RE\nand ED tasks. For EAE task, we follow previous\nwork (Wang et al., 2023b) and adopt head-F1 score,\nwhich merely considers matching of the head word\nrather than the whole content of a text span. We re-\nport averaged score w.r.t 5 sampled train/validation\nsets unless otherwise stated.\n\n3.2. Small Language Models\n\nWe adopt five supervised methods to evaluate the\nabilities of SLMs. (1) Vanilla fine-tuning for all\ntasks, (2) FSLS (Ma et al., 2022a) for NER and ED\ntasks, (3) KnowPrompt (Chen et al., 2022b) for RE\ntask, (4) PAIE (Ma et al., 2022b) for EAE task, and\n\n(5) UIE (Lu et al., 2022c) for all tasks. See their\ndetails in Appendix B.\n\n3.3 Large Language Models\n\nDetailed in Appendix C, we evaluate the ICL abil-\nities of LLMs. Given labeled sentences D =\n{(s;,yi)} and a test sentence s, our goal is to pre-\ndict structured information y from s using a frozen\nLLM £. We feed LLM with prompt P¢,r,7(D, s):\n\nPers(D,8) = [E £(E(D, 8); f(s)\n\nWe give examples of prompts on four IE tasks\nin Figure 1. The prompts consist of three parts: in-\nstruction J (color in green in Figure 1), demonstra-\ntion f(E(D, s)) (demo; color in blue) and the ques-\ntion f(a) (color in black). Here € denotes demo\nselector and €(D,s) C D denotes selected sen-\ntences as the demo to predict s. Prompt format f >\nrefers to the template which converts demo €(D, s)\nand sample s to input context for LLMs. Then\nLLM generates f(y) (color in red) from which we\ncould readily parse the extraction results y.\nModels £: We explore six LLMs from two\nsources. (1) OpenAI models °: we employ Chat-\n\n>We slightly abuse the notation f to allow s, y and {(s, y)}\nas the input for simplicity.\n©The versions of model we use are: gpt-3. 5-turbo-0301,\n", "vlm_text": "The image depicts examples of prompts used for different NLP tasks:\n\n1. **Named Entity Recognition (NER):**\n   - **Instruction:** Identify entities in the sentence and locate them to words.\n   - **Demo Sentence:** \"The 1962 National Football League draft was held at the Sheraton Hotel in Chicago, Illinois.\"\n   - **Entities:** Organization-sportsleague (1962 National Football League), Building-hotel (Sheraton Hotel)\n   - **Test Sentence:** \"Critics noted 'The Manual of Detection' combines elements from several genres, including mystery and fantasy.\"\n   - **Output:** No entities found.\n\n2. **Relation Extraction:**\n   - **Instruction:** Identify the relation between entities in the sentence.\n   - **Demo Sentence:** Features organizations and members, with entities like TOAID and NCF.\n   - **Test Sentence:** \"Fuller, 37, has been there before.\"\n   - **Output:** Per:age\n\n3. **Event Detection:**\n   - **Instruction:** Identify events in the sentence and locate to words.\n   - **Demo Sentence:** \"Injured Russian diplomats were among victims... crossfire.\"\n   - **Events:** Life.Injure (trigger_word: Injured), Conflict.Attack (trigger_word: crossfire)\n   - **Test Sentence:** \"Iraqi Kurds fear that Turkish troops could move deep into the enclave...\"\n   - **Output:** Movement.Transport (trigger_word: move), Conflict.Attack (trigger_word: clashes)\n\n4. **Event Argument Extraction:**\n   - **Instruction:** Identify arguments of an event and classify their roles.\n   - **Demo Sentence:** Vivendi selling Universal Studios, roles like Seller, Artifact.\n   - **Test Sentence:** \"Another appeal is pending in the Federal Court.\"\n   - **Output:** Adjudicator, argument: the Federal Court\n\nEach task involves processing sentences to identify and classify different elements such as entities, relations, events, and arguments.\n2015 ). (4) Event Argument Extraction (EAE): ACE05, ERE and RAMS ( Ebner et al. ,  2020 ). With label numbers ranging from 4 to 168, we assess LLMs’ performance under different schema com- plexities. See their details in Appendix  A.1 . \nFew-shot Set  We construct few-shot datasets from the original datasets above. For training and vali- dation set, we adopt    $K$  -shot sampling strategy,  i.e., sampling    $K$   samples for each label type. See more details in Appendix  A.2 . For test set, we down- sample their original test sets to reduce the cost of LLMs. We randomly sample 500 sentences for RE tasks, and 250 sentences for other task. We en- sure that each label has at least one corresponding sample to avoid the absence of rare labels. \nEvaluation  We adopt micro-F1 score in NER, RE and ED tasks. For EAE task, we follow previous work ( Wang et al. ,  2023b ) and adopt head-F1 score, which merely considers matching of the head word rather than the whole content of a text span. We re- port averaged score w.r.t 5 sampled train/validation sets unless otherwise stated. \n3.2 Small Language Models \nWe adopt five supervised methods to evaluate the abilities of SLMs. (1) Vanilla fine-tuning for all tasks, (2) FSLS ( Ma et al. ,  2022a ) for NER and ED tasks, (3) KnowPrompt ( Chen et al. ,  2022b ) for RE task, (4) PAIE ( Ma et al. ,  2022b ) for EAE task, and (5) UIE ( Lu et al. ,  2022c ) for all tasks. See their details in Appendix  B . \n\n3.3 Large Language Models \nDetailed in Appendix  C , we evaluate the ICL abil- ities of LLMs. Given labeled sentences    $D\\;=\\;$   $\\{(s_{i},y_{i})\\}$   and a test sentence    $s$  , our goal is to pre- dict structured information    $y$   from    $s$   using a frozen LLM    $\\mathcal{L}$  . We feed LLM with prompt    $\\mathcal{P}_{\\mathcal{E},I,f}(D,s)$  : \n\n$$\n\\mathcal{P}_{\\mathcal{E},I,f}(D,s)=[I;f(\\mathcal{E}(D,s));f(s)]\n$$\n \nWe give examples of prompts on four IE tasks in Figure  1 . The prompts consist of three parts: in- struction  $I$   (color in green in Figure  1 ), demonstra- tion    $f(\\mathcal{E}(D,s))$   (demo; color in b e) and the ques- tion    $f(x)$   (color in black). Here  E  denotes demo selector and    ${\\mathcal{E}}(D,s)\\,\\subset\\,D$   denotes selected s tences as the demo to predict    $s$  . Prompt format  $f^{\\ 5}$  refers to the template which converts demo    ${\\mathcal{E}}(D,s)$  and sample    $s$   to input context for LLMs. Then LLM generates    $f(y)$   (color in red) from which we could readily parse the extraction results    $y$  . \nModels    $\\mathcal{L}$  : We explore six LLMs from two sources. (1) OpenAI models   6 : we employ Chat- "}
{"page": 3, "image_path": "doc_images/2303.08559v2_3.jpg", "ocr_text": "= Fine-tuning ~~ FSLS —- UIE\n100 100\n\n©\n$s\n\nChatGPT -e- CODEX\n\nInstructGPT —— LLaMA(13B) —— Vicuna (13B)\n100\n\n©\n$\n\n@ 80 o o\n5 60 5 60 8 60\nPy Py o Py\n® 40 ® gl oeem ® 40\nz z aa z\n\n20 20 as 20\n\nt) t)\n\n4-shot 5-shot 10-shot 20-shot 4-shot 5-shot 10-shot 20-shot 1-shot 5-shot 10-shot 20-shot\nCONLLO3 OntoNotes FewNERD\n(a) Named Entity Recognition (NER)\n\n~~ Fine-tuning ~ - KnowPrompt ~~ UIE ChatGPT -*- CODEX InstructGPT -- LLaMA(13B) —- Vicuna (13B)\n\n10-shot 20-shot 50-shot 100-shot\nTACRED\n\n1-shot 5-shot\n\nj-shot 5-shot 10-shot 20-shot 50-shot 100-shot\nTACREV\n(b) Relation Extraction (RE)\n~~ Fine-tuning ~~ FSLS —- UIE ChatGPT -*- CODEX\n\n80\n\nF1 score\n&.8\n&\n\nnV\n8\n\nInstructGPT —- LLaMA(13B) —— Vicuna (13B)\n80\n\nF1 score\na 2\n$38\n\nRV\n8\n\n1-shot 5-shot 10-shot 20-shot\n\n1-shot 5-shot 10-shot 20-shot 1-shot 5-shot 10-shot 20-shot\nACE05 MAVEN\n(c) Event Detection (ED)\n—- Fine-tuning ~~ PAIE -<- UIE ChatGPT InstructGPT —— LLaMA(13B) —— Vicuna (13B)\n80 80 a 80\n@ 60 @ 60 ——e @ 60\n8 8 ae\n340 340 = => 340\nr r a aote r\n20 WL 20 =, 20\n=\n1-shot 1-shot 1-shot\n\n5-shot 10-shot 20-shot\nACE05\n\n5-shot 10-shot 20-shot\nERE\n\n5-shot 10-shot 20-shot\nRAMS\n\n(d) Event Argument Extraction (EAE)\n\nFigure 2: Overall results of SLM-based methods (dashed lines) and LLM-based methods (solid lines) on nine\ndatasets across four IE tasks. The black, horizontal dashed lines represent the SoTA performance on full dataset.\n\nGPT, CODEX (Chen et al., 2022a) and Instruct-\nGPT (Ouyang et al., 2022) for main experiments.\nWe also evaluate GPT-4 in Appendix D.3. (2)\nOpen-source models: we use LLaMA-13B (Tou-\nvron et al., 2023) and its instruction-tuned counter-\npart, Vicuna-13B (Chiang et al., 2023).\nInstruction J: The instruction (1) describes the\ntask and (2) enumerates all possible labels for ref-\nerence. we adopt instructions shown in Figure 1.\nDemo selector €: The maximum input length of\n\ncode-davinci-002, text-davinci-003 and gpt-4-0314.\nDue to budget constraints, we execute InstructGPT and GPT-\n4 only once per setting. We do not conduct EAE task on\nCODEX since it had been unavailable at that time.\n\nLLMs usually limits the sentence number in de-\nmos even under few-shot settings. Therefore for\neach test sentence s, we demand a demo retriever\n€(D,s) which selects a small subset from D as\nthe sentences in demo. Following previous meth-\nods (Liu et al., 2022; Su et al., 2022), we retrieve\ndemos according to their sentence embedding simi-\nlarity to the test samples.\n\nPrompt format f: We use simple textual tem-\nplates to format the demos and the test sample in\nmain experiments. For example, the template for\nNER is “Sentence: [S], Entities: ([typel],\nCentity1]), (L[type2], Lentity2])...\".\n", "vlm_text": "The image is a set of three line graphs comparing the F1 scores of different models for Named Entity Recognition (NER) tasks across three datasets: CONLL03, OntoNotes, and FewNERD. The models compared include Fine-tuning, FSLS, UIE, ChatGPT, CODEX, InstructGPT, LLaMA (13B), and Vicuna (13B). Performance is shown for different scenarios: 1-shot, 5-shot, 10-shot, and 20-shot learning. Each model's performance is represented by a distinct line style and color. The Y-axis shows the F1 score ranging from 0 to 100.\nThe image contains two line graphs comparing the F1 scores of different models on the TACREV and TACRED datasets. The models displayed include Fine-tuning, KnowPrompt, UIE, ChatGPT, CODEX, InstructGPT, LLaMA (13B), and Vicuna (13B). Each graph shows their performance across different numbers of shots: 1-shot, 5-shot, 10-shot, 20-shot, 50-shot, and 100-shot. The scores indicate the models' performance in relation extraction (RE) tasks.\nThe image shows three line charts comparing F1 scores for different models on the tasks ACE05, ERE, and MAVEN. The models include Fine-tuning, FSLS, UIE, ChatGPT, CODEX, InstructGPT, LLaMA (13B), and Vicuna (13B). The x-axis represents different \"shot\" settings (1-shot, 5-shot, 10-shot, 20-shot), and the y-axis shows the F1 score ranging from 0 to 80. Each model's performance is depicted by different colored lines, allowing for a side-by-side comparison across tasks.\nThis image presents three line graphs that compare the F1 scores of various methods for Event Argument Extraction (EAE) across three datasets: ACE05, ERE, and RAMS. \n\nThe methods compared are:\n\n- Fine-tuning\n- PAIE\n- UIE\n- ChatGPT\n- InstructGPT\n- LLaMA (13B)\n- Vicuna (13B)\n\nEach graph shows the performance (F1 score) across different amounts of training data: 1-shot, 5-shot, 10-shot, and 20-shot scenarios. The F1 score ranges from 0 to 80 on the y-axis. The plots indicate that PAIE and UIE generally achieve higher F1 scores across all datasets compared to other methods.\nFigure 2: Overall results of SLM-based methods (dashed lines) and LLM-based methods (solid lines) on nine datasets across four IE tasks. The black, horizontal dashed lines represent the SoTA performance on full dataset. \nGPT, CODEX ( Chen et al. ,  2022a ) and Instruct- GPT ( Ouyang et al. ,  2022 ) for main experiments. We also evaluate GPT-4 in Appendix  D.3 . (2) Open-source models: we use LLaMA-13B ( Tou- vron et al. ,  2023 ) and its instruction-tuned counter- part, Vicuna-13B ( Chiang et al. ,  2023 ). \nInstruction    $I$  : The instruction (1) describes the task and (2) enumerates all possible labels for ref- erence. we adopt instructions shown in Figure  1 . Demo selector    $\\mathcal{E}$  : The maximum input length of LLMs usually limits the sentence number in de- mos even under few-shot settings. Therefore for each test sentence    $s$  , we demand a demo retriever  ${\\mathcal{E}}(D,s)$   which selects a small subset from    $D$   as the sentences in demo. Following previous meth- ods ( Liu et al. ,  2022 ;  Su et al. ,  2022 ), we retrieve demos according to their sentence embedding simi- larity to the test samples. \n\nPrompt format    $f$  : We use simple textual tem- plates to format the demos and the test sample in main experiments. For example, the template for NER is  “Sentence: [S], Entities: ([type1], [entity1]), ([type2], [entity2])...\" . "}
{"page": 4, "image_path": "doc_images/2303.08559v2_4.jpg", "ocr_text": "F1 score\n\nlo WN 2 8 4 4 8\nInstruction format\n\nae\n\nDemonstration number\n\n—\n@ 57.5\n4\n°\ncs)\n55.0\niL\nChatGPT 52.5\n-*- CODEX ”\n32 64 96 random embed epr\n\nDemonstration selection\n\nFigure 3: LLMs’ performance w.r.t prompt variants on 20-shot FewNERD dataset. See full results on other datasets\nin Appendix E.2- E.5. Left: ChatGPT’s performance (F1 Score) across six instruction variants. Middle: F1 Score\nchanges over varying numbers of demo. Right: ChatGPT’s performance across three demo selection strategies.\nRandom: Random sampling. Embed: Sentence embedding. EPR: Efficient Prompt Retriever (Rubin et al., 2022).\n\n3.4 Main Results\n\nWe summarize the main experimental outcomes in\nFigure 2, indicating that LLMs only outperform\nSLMs in environments with restricted labels and\nsamples. Conversely, SLMs are generally more\neffective. Given (1) the practicality of fine-grained\nIE tasks and the manageable effort of obtaining 10-\n20 annotations per label and (2) the excessive time\nand budget demands of LLM inference, we con-\nclude that LLMs are not as effective as supervised\nSLMs for few-shot IE tasks under real scenarios.\nWe detail our findings as below.\n\nPerformance w.r.t sample number. The perfor-\nmance dynamics of SLMs and LLMs are influenced\nby variations in sample size. Under extremely low-\nresource (1-shot or 5-shot) settings, LLMs some-\ntimes present superior performance than SLMs.\nYet, LLMs tend to reach a performance plateau\nwith only modest increases in sample size. Con-\nversely, SLMs demonstrate marked performance\nenhancement as sample sizes grow. This trend is\nevident in Figure 2, where the SLM trajectories\n(represented by dashed lines) ascend more steeply\ncompared to the LLM ones (solid lines).\n\nPerformance w.r.t label number. Compared with\nSLMs, LLMs tend to struggle on fine-grained\ndatasets. For instance, LLMs perform relatively\nworse on MAVEN and RAMS datasets (with\n168/139 labels) than on CONLL (4 labels only).\nDetailed quantitative results are shown in Ap-\npendix E.1, illustrating a clear negative correlation\nbetween the label number and the result disparity\nbetween LLMs and SLMs across various IE tasks.\nComparisons among LLMs. We observe perfor-\nmance variability among LLMs. (1) Open-source\nmodels, LLaMA and Vicuna, significantly lag be-\nhind proprietary LLMs across all few-shot IE tasks.\n\n(2) Among proprietary LLMs, ChatGPT performs\nbetter on NER and EAE tasks, but poorer so on RE\nand ED tasks. InstructGPT and CODEX demon-\nstrate comparable performance across these tasks.\nLLMs show limited inference speed. We compare\nthe inference speed of different methods and show\ntheir results in Table 1. We observe that LLMs\nis much slower than SLMs since they have much\nmore parameters, longer input contexts and extra\nresponse decay (if external APIs applied).\n\n3.5 Analysis on Prompt Sensitivity\n\nPrevious work (Lu et al., 2022b) indicates that the\nefficacy of LLMs on specific tasks can be signifi-\ncantly influenced by the construction of the prompt.\nTo ensure that LLMs’ suboptimal outcomes are\nnot erroneously ascribed to inappropriate prompt\ndesigns, we meticulously examine the impact of\ndiverse prompt variations from four aspects, i.e., in-\nstruction format, demo number, demo selector and\nprompt format. We leave comprehensive details\nof the variants and their results to Appendix E.2-\nE.5, and illustrate salient findings in Figure 3. Our\nfindings include that (1) diverse instruction strate-\ngies yield comparable results in IE task; (2) in-\ncreasing the number of samples in demonstrations\ndoes not unequivocally enhance performance; and\n(3) The selection strategy of demonstration mat-\nters, and retrieval based on sentence embedding\n\nTable 1: The inference seconds over 500 sentences (run\non single V100 GPU). Here LLaMA is extremely slow\nsince we set batch size as 1 due to memory limit.\n\nDataset (Task) |Roberta TS LLaMA CODEX\nFewNERD (NER)| 2.8 39.4 1135.4 179.4\nTACREV (RE) 14 45.6 1144.9 151.6\nACE05 (ED) 6.6 62.5 733.4 171.7\n", "vlm_text": "The image contains three graphs illustrating the performance of ChatGPT and other models on the 20-shot FewNERD dataset:\n\n1. **Left Graph (Instruction Format)**: Displays F1 scores for different instruction formats (I0 to I5). The performance varies across these formats, with some achieving higher scores than others.\n\n2. **Middle Graph (Demonstration Number)**: Shows how the F1 score changes with varying numbers of demonstrations. It compares ChatGPT and Codex, indicating that ChatGPT generally performs better as the number of demonstrations increases.\n\n3. **Right Graph (Demonstration Selection)**: Compares F1 scores for three demo selection strategies: random sampling, sentence embedding (embed), and Efficient Prompt Retriever (EPR). The EPR strategy shows the highest performance, followed by embedding, with random sampling performing the least effectively.\n3.4 Main Results \nWe summarize the main experimental outcomes in Figure  2 , indicating that LLMs only outperform SLMs in environments with restricted labels and samples. Conversely, SLMs are generally more effective. Given (1) the practicality of fine-grained IE tasks and the manageable effort of obtaining 10- 20 annotations per label and (2) the excessive time and budget demands of LLM inference, we con- clude that LLMs are not as effective as supervised SLMs for few-shot IE tasks under real scenarios. We detail our findings as below. \nPerformance w.r.t sample number.  The perfor- mance dynamics of SLMs and LLMs are influenced by variations in sample size. Under extremely low- resource (1-shot or 5-shot) settings, LLMs some- times present superior performance than SLMs. Yet, LLMs tend to reach a performance plateau with only modest increases in sample size. Con- versely, SLMs demonstrate marked performance enhancement as sample sizes grow. This trend is evident in Figure  2 , where the SLM trajectories (represented by dashed lines) ascend more steeply compared to the LLM ones (solid lines). \nPerformance w.r.t label number.  Compared with SLMs, LLMs tend to struggle on fine-grained datasets. For instance, LLMs perform  relatively worse on MAVEN and RAMS datasets (with 168/139 labels) than on CONLL (4 labels only). Detailed quantitative results are shown in Ap- pendix  E.1 , illustrating a clear negative correlation between the label number and the result disparity between LLMs and SLMs across various IE tasks. \nComparisons among LLMs.  We observe perfor- mance variability among LLMs. (1) Open-source models, LLaMA and Vicuna, significantly lag be- hind proprietary LLMs across all few-shot IE tasks. \n(2) Among proprietary LLMs, ChatGPT performs better on NER and EAE tasks, but poorer so on RE and ED tasks. Instruct GP T and CODEX demon- strate comparable performance across these tasks. LLMs show limited inference speed.  We compare the inference speed of different methods and show their results in Table  1 . We observe that LLMs is much slower than SLMs since they have much more parameters, longer input contexts and extra response decay (if external APIs applied). \n3.5 Analysis on Prompt Sensitivity \nPrevious work ( Lu et al. ,  2022b ) indicates that the efficacy of LLMs on specific tasks can be signifi- cantly influenced by the construction of the prompt. To ensure that LLMs’ suboptimal outcomes are not erroneously ascribed to inappropriate prompt designs, we meticulously examine the impact of diverse prompt variations from four aspects,  i.e.,  in- struction format, demo number, demo selector and prompt format. We leave comprehensive details of the variants and their results to Appendix  E.2 - E.5 , and illustrate salient findings in Figure  3 . Our findings include that (1) diverse instruction strate- gies yield comparable results in IE task; (2) in- creasing the number of samples in demonstrations does not unequivocally enhance performance; and (3) The selection strategy of demonstration mat- ters, and retrieval based on sentence embedding \nThe table compares performance metrics for different models across three datasets and tasks:\n\n### Columns\n- **Models:** Roberta, T5, LLAMA, and CODEX.\n\n### Rows\n- **Datasets (Tasks):**\n  - **FewNERD (NER):** Named Entity Recognition\n    - Roberta: 2.8\n    - T5: 39.4\n    - LLAMA: 1135.4\n    - CODEX: 179.4\n  - **TACREV (RE):** Relation Extraction\n    - Roberta: 1.4\n    - T5: 45.6\n    - LLAMA: 1144.9\n    - CODEX: 151.6\n  - **ACE05 (ED):** Event Detection\n    - Roberta: 6.6\n    - T5: 62.5\n    - LLAMA: 733.4\n    - CODEX: 171.7\n\nThe numbers likely represent performance scores or times, but the specific unit isn't provided."}
{"page": 5, "image_path": "doc_images/2303.08559v2_5.jpg", "ocr_text": "(what we used) proves sufficiently effective. Con-\nsequently, we believe that there unlikely exists a\nlottery prompt that substantially alters our conclu-\nsions that LLMs are not good few-shot IE solver.\n\n3.6 Discussion: Why LLMs Fail to Obtain\nSatisfactory Performance on IE Tasks?\n\nUnderutilized Annotations. We notice that LLMs\nappear to benefit less from additional annotations,\ni.é., more training samples and label types, than\nSLMs. We speculate that LLMs are constrained\nby ICL in two ways. (1) More samples: The num-\nber of effective samples for LLMs, those in de-\nmos, is limited by maximum input length. More-\nover, we also observe LLMs’ performance plateaus\nin some tasks before reaching this limit (see Ap-\npendix E.3). Meanwhile, SLMs can continually\nlearn from more samples through supervised learn-\ning, widening the performance gap as annotated\nsamples increase. (2) More labels: LLMs struggle\nwith fine-grained datasets. It suggests a difficulty\nin understanding numerous labels and their subtle\ninteractions merely from the given instruction and\nexemplars for LLMs. Also, the examples per label\nin demos decrease as label types increase.\nUnexplored Task format. As stated in Zhang\net al. (2023a), IE-related tasks are scarce in the\nwidely-used instruction tuning datasets like Wei\net al. (2022a) and Wang et al. (2022). Furthermore,\nthe highly-flexible format of NER and ED tasks\nimpair the ICL abilities 7. Therefore it is likely that\ninstruction-tuned LLMs are not well-acquainted\nwith such IE-related task formats.\n\n4 LLMs are Good Few-shot Reranker\n\n4.1 Filter-then-rerank Paradigm\n\nRead following sentences and identify what is the entity type\nof ‘The New Yorker” quoted by <t>.\n\nSentence:\n\nIn 2004 Gourevitch was assigned to cover the 2004 U.S.\npresidential election for “<t> The New Yorker <t>”.\n\nCandidate Choices:\n\n(a)The New Yorker does not belong to any known entities.\n(b)The New Yorker is a broadcast program.\n\n(c)The New Yorker is a kind of written art.\n\n(d)The New Yorker is a media/newspaper organization.\nAnalysis:\n\nThe New Yorker is a well-known American magazine that has\nbeen published since 1925, and is primarily known for its\nlong-form journalism, commentary, and satire. It has a\nreputation for publishing high-quality writing on a wide\nvariety of topics, including politics, culture, and the arts.\nSo The New Yorker is a media/newspaper organization.\n\nCorrect Answer: (d)\n\nFigure 4: Multi-choice question (MCQ) prompt.\n\nTThese two tasks require unfixed numbers of (label, span)\ntuple. Furthermore, the length of each span is also unfixed.\n\nTo mitigate LLMs’ drawbacks mentioned above,\nwe propose a filter-then-rerank paradigm to inte-\ngrate both SLMs and LLMs within the same system.\nThis paradigm uses SLMs as filters to select the\ntop-N candidate labels, then LLMs rerank them\nto make final decisions. By using SLM-generated\ncandidate answers, the focus of LLMs shifts from\nsentence-level (i.e., identifying all entities/events\nin the sentence) to sample-level (i.e., determin-\ning single entity/event candidate provided). Each\nquestion now corresponds to a single sample, al-\nlowing us to reframe prompts as multi-choice ques-\nions (MCQ; shown in Figure 4) problem. Un-\nder such format, each candidate label is converted\n0 a choice by pre-defined templates. We claim\nfilter-then-rerank paradigm is more likely to elicit\nhe powers of LLMs and smoothly solve few-shot\nJE tasks because: (1) LLMs are more familiar\nwith MCQ prompts than IE-format prompts (Zhang\net al., 2023a). (2) This paradigm reduces the la-\nbel scopes significantly, since N is usually much\nsmaller than fine-grained label numbers.\n\n4.2 LLMs are Hard Sample Solver\n\nOur filter-then-rerank paradigm, unfortunately,\npresents unsatisfactory performance (and even suf-\nfers longer latency since LLMs rerank candidates\nper sample). Given LLMs’ abilities in memoriza-\ntion and reasoning, however, we still believe that\nLLMs are potential to solve some, if not most, IE\nsamples effectively. We hypothesize that LLMs\nare more proficient than SLMs on hard samples.\nThese samples are characterized by their requisite\nfor external knowledge acquisition or sophisticated\nreasoning strategies, areas where LLMs can lever-\nage their extensive parametric knowledge bases and\ninherent reasoning mechanisms. In contrast, SLMs\noften falter with such samples, constrained by their\nrestricted modeling capacities.\n\nWe leverage an unsupervised metric from SLMs\nto evaluate the difficulty of samples. Given a sample\nx in the sentence s, we define the highest probabil-\nity across all labels as the confidence score:\n\n(2)\n\nconf(x) = max Psp (Ix\nleL\n\nwhere L denotes the label set and Ps7,¢(I|2; s) the\nprobability of a span « (in the sentence s) referring\nto label 1 computed by SLMs. We classify sam-\nples with low confidence scores as hard samples.\nOtherwise we view them as easy samples.\n", "vlm_text": "(what we used) proves sufficiently effective. Con- sequently, we believe that there unlikely exists a lottery  prompt that substantially alters our conclu- sions that LLMs are not good few-shot IE solver. \n3.6 Discussion: Why LLMs Fail to Obtain Satisfactory Performance on IE Tasks? \nUnder utilized Annotations.  We notice that LLMs appear to benefit less from additional annotations, i.e.,  more training samples and label types, than SLMs. We speculate that LLMs are constrained by ICL in two ways. (1) More samples: The num- ber of effective samples for LLMs, those in de- mos, is limited by maximum input length. More- over, we also observe LLMs’ performance plateaus in some tasks before reaching this limit (see Ap- pendix  E.3 ). Meanwhile, SLMs can continually learn from more samples through supervised learn- ing, widening the performance gap as annotated samples increase. (2) More labels: LLMs struggle with fine-grained datasets. It suggests a difficulty in understanding numerous labels and their subtle interactions merely from the given instruction and exemplars for LLMs. Also, the examples per label in demos decrease as label types increase. \nUnexplored Task format.  As stated in  Zhang et al.  ( 2023a ), IE-related tasks are scarce in the widely-used instruction tuning datasets like  Wei et al.  ( 2022a ) and  Wang et al.  ( 2022 ). Furthermore, the highly-flexible format of NER and ED tasks impair the ICL abilities   7 . Therefore it is likely that instruction-tuned LLMs are not well-acquainted with such IE-related task formats. \n4 LLMs are Good Few-shot Reranker 4.1 Filter-then-rerank Paradigm \nThe image shows a text box with instructions to identify the entity type of \"The New Yorker.\" It includes the sentence:\n\n\"In 2004 Gourevitch was assigned to cover the 2004 U.S. presidential election for '<t> The New Yorker <t>'.\"\n\nCandidate choices are:\n(a) The New Yorker does not belong to any known entities.\n(b) The New Yorker is a broadcast program.\n(c) The New Yorker is a kind of written art.\n(d) The New Yorker is a media/newspaper organization.\n\nThe analysis provided explains that The New Yorker is a well-known American magazine known for journalism, commentary, and satire, and concludes that it is a media/newspaper organization. The correct answer is (d).\nFigure 4: Multi-choice question (MCQ) prompt. \nTo mitigate LLMs’ drawbacks mentioned above, we propose a  filter-then-rerank  paradigm to inte- grate both SLMs and LLMs within the same system. This paradigm uses SLMs as filters to select the top-  $\\cdot N$   candidate labels, then LLMs rerank them to make final decisions. By using SLM-generated candidate answers, the focus of LLMs shifts from sentence-level  ( i.e.,  identifying all entities/events in the sentence) to  sample-level  ( i.e.,  determin- ing single entity/event candidate provided). Each question now corresponds to a single sample, al- lowing us to reframe prompts as multi-choice ques- tions (MCQ; shown in Figure  4 ) problem. Un- der such format, each candidate label is converted to a choice by pre-defined templates. We claim filter-then-rerank  paradigm is more likely to elicit the powers of LLMs and smoothly solve few-shot IE tasks because: (1) LLMs are more familiar with MCQ prompts than IE-format prompts ( Zhang et al. ,  2023a ). (2) This paradigm reduces the la- bel scopes significantly, since    $N$   is usually much smaller than fine-grained label numbers. \n4.2 LLMs are  Hard  Sample Solver \nOur  filter-then-rerank  paradigm, unfortunately, presents unsatisfactory performance (and even suf- fers longer latency since LLMs rerank candidates per sample). Given LLMs’ abilities in memoriza- tion and reasoning, however, we still believe that LLMs are potential to solve  some , if not most, IE samples effectively. We hypothesize that LLMs are more proficient than SLMs on  hard  samples. These samples are characterized by their requisite for external knowledge acquisition or sophisticated reasoning strategies, areas where LLMs can lever- age their extensive parametric knowledge bases and inherent reasoning mechanisms. In contrast, SLMs often falter with such samples, constrained by their restricted modeling capacities. \nWe leverage an unsupervised metric from SLMs to evaluate the  difficulty  of samples. Given a sample  $x$   in the sentence    $s$  , we define the highest probabil- ity across all labels as the confidence score: \n\n$$\n\\operatorname{conf}(x)=\\operatorname*{max}_{l\\in L}P_{S L M}(l|x;s)\n$$\n \nwhere  $L$   denotes the label set and    $P_{S L M}(l|x;s)$   the probability of a span    $x$   (in the sentence    $s$  ) referring to label    $l$   computed by SLMs. We classify sam- ples with low confidence scores as  hard  samples. Otherwise we view them as easy samples. "}
{"page": 6, "image_path": "doc_images/2303.08559v2_6.jpg", "ocr_text": "(wo. LLM reranking fll w. LLM reranking\nFewNERD (NER)\n\n75)\n\na\ng 50)\ng\n= 25]\noe 6 o $ 6\nor 9 oo goo\nConfidence Score\nTACREV (RE)\n100\n~ 75\n4\ng 50)\ns\n= 25\noo $ $\noo! 2 oF 9p oF\nConfidence Score\nACEOS (ED)\n100\n~ 75\n“\ng 50)\n8\n= 25\n\n% 48\n\no. ee\n\nof Sa\n\ngt gh °°\n\ncS oF\n\n\"Confidence Score\nFigure 5: Relationship between confidence scores and\nperformance with/without LLM reranking. We adopt\nRoBERTa-large as filter and InstructGPT as reranker.\n\nWe conduct experiments to confirm our hypoth-\nesis that LLMs excel on hard samples. We group\nsamples by confidence scores and compare two\nmethods within each group: (a) SLM-based meth-\nods without LLM reranking, and (b) SLMs as the\nfilter and LLMs as the reranker. Method (b) dif-\nfers from (a) by adding a single LLM to rerank the\ntop-N SLM predictions, using MCQ prompts.\n\nThe results in Figure 5 substantiate our assump-\ntion. (1) LLM-based reranking (blue lines) en-\nhances performance on hard samples (left areas in\nthe figure). We provide a detailed analysis of spe-\ncific challenging instances where LLM rerankers\nprove advantageous in Appendix F.1. These in-\nstances demonstrate the efficacy of LLMs in har-\nnessing external knowledge and complex reason-\ning to rectify erroneous predictions initially made\nby SLMs (red lines). (2) Conversely, LLM-based\nreranking impedes performance on easy samples\n(right areas), resulting in a significant degradation,\nparticularly for very easy samples (rightmost areas).\nIn conclusion, LLMs exhibit greater proficiency in\nhandling hard samples compared to SLMs, yet they\nunderperform relative to SLMs on easy samples.\n\n4.3, Why LLMs Fail on Easy Samples\n\nWe investigate why LLMs (relatively) fail on easy\nsamples in this section. As shown in Table 2, we\nobserve significant higher negative sample ratios\nfor easy samples across diverse IE tasks. In other\n\nTable 2: Comparative ratios of negative to positive sam-\nples across various datasets and subsets. We set fixed\nthreshold 7 here for simplicity.\n\nFewNERD TACREV ACE05\n\nOverall 5.88 3.03 38.2\nEasy samples (rt > 0.9) 9.44 3.21 44.0\nHard samples (r < 0.6) 1.28 2.68 1.36\n\nwords, most negative samples are easy samples for\nSLMs. Here we refer negative samples to those\nlabeled as None. We speculate that the proficiency\nof SLMs with negative samples stems from their\nability to adeptly discern apparent patterns during\nthe fine-tuning stages. Therefore, SLMs could pre-\ndict negative samples with (relatively) high confi-\ndence and accuracy. Due to LLMs’ predisposition\nto false-positive predictions on negative samples,\nhowever, the performance of LLMs on easy sam-\nples collapses. We attribute such false-positive pre-\ndictions to (1) hallucination and (2) span boundary\nmismatch. We detail such two kinds of mistakes\nwith cases in Appendix F.2.\n\n5 Adaptive Filter-then-rerank Paradigm\n\nAbove findings can be summarized as: (1) SLMs\ngenerally outperform LLMs, especially with more\ntraining samples and fine-grained labels. (2) SLMs\nare much more time- and cost-efficient. (3) LLMs\nserve as powerful rerankers on hard samples that\nchallenge SLMs. Based on them, we propose a\nsimple, efficient, and effective adaptive reranker\nthat combines the strengths of SLMs and LLMs.\n\n5.1 Method\n\nOur adaptive filter-then-rerank approach, shown\nin Figure 6, uses supervised SLMs as a filter to\nmake preliminary decisions. Samples with confi-\ndence scores exceeding threshold are viewed as\neasy samples otherwise hard ones. For easy sam-\nples, we retain SLM predictions as final results. For\nhard samples, top-N predictions from SLMs are\nreranked via LLMs using ICL. Here LLMs employ\nMCQ prompts (Figure 4), containing demos and a\nsample to be reranked. The LLMs then generate the\nfinal answer and optionally provide an explanation.\n\n5.2. Experimental Setup\n\nWe conduct experiments on FewNERD for NER\ntask, TACREV for RE task and ACE05 for ED\ntask. We employ top-performing SLM-based meth-\nods from Section 3 (FSLS or KnowPrompt) as the\n", "vlm_text": "The image contains three line graphs comparing the relationship between confidence scores and micro-F1 performance, with and without LLM reranking using RoBERTa-large as a filter and Instruct GPT as a reranker. Each graph represents a different dataset or task:\n\n1. **FewNERD (NER)**: The graph shows performance across different confidence scores, with and without reranking.\n2. **TACREV (RE)**: Similar comparison for the TACREV dataset.\n3. **ACE05 (ED)**: Performance comparison for the ACE05 dataset.\n\nThe graphs indicate how LLM reranking impacts micro-F1 scores at various confidence levels. The color coding is as follows: pink represents performance without LLM reranking, and blue represents performance with LLM reranking.\nWe conduct experiments to confirm our hypoth- esis that LLMs excel on  hard  samples. We group samples by confidence scores and compare two methods within each group: (a) SLM-based meth- ods without LLM reranking, and (b) SLMs as the filter and LLMs as the reranker. Method (b) dif- fers from (a) by adding a single LLM to rerank the top-  $N$   SLM predictions, using MCQ prompts. \nThe results in Figure  5  substantiate our assump- tion. (1) LLM-based reranking (blue lines) en- hances performance on hard samples (left areas in the figure). We provide a detailed analysis of spe- cific challenging instances where LLM rerankers prove advantageous in Appendix  F.1 . These in- stances demonstrate the efficacy of LLMs in har- nessing external knowledge and complex reason- ing to rectify erroneous predictions initially made by SLMs (red lines). (2) Conversely, LLM-based reranking impedes performance on easy samples (right areas), resulting in a significant degradation, particularly for very easy samples (rightmost areas). In conclusion, LLMs exhibit greater proficiency in handling hard samples compared to SLMs, yet they under perform relative to SLMs on easy samples. \n4.3 Why LLMs Fail on Easy Samples \nWe investigate why LLMs (relatively) fail on easy samples in this section. As shown in Table  2 , we observe significant higher negative sample ratios for easy samples across diverse IE tasks. In other Table 2: Comparative ratios of negative to positive sam- ples across various datasets and subsets. We set fixed threshold  $\\tau$  here for simplicity. \n\nThe table displays performance metrics for three datasets: FewNERD, TACREV, and ACE05. It has three rows indicating different sample conditions:\n\n1. **Overall**: \n   - FewNERD: 5.88\n   - TACREV: 3.03\n   - ACE05: 38.2\n\n2. **Easy samples (τ > 0.9)**:\n   - FewNERD: 9.44\n   - TACREV: 3.21\n   - ACE05: 44.0\n   \n3. **Hard samples (τ < 0.6)**:\n   - FewNERD: 1.28\n   - TACREV: 2.68\n   - ACE05: 1.36\n\nThe table compares results for different difficulty levels of samples within each dataset.\nwords, most negative samples are easy samples for SLMs. Here we refer negative samples to those labeled as  None . We speculate that the proficiency of SLMs with negative samples stems from their ability to adeptly discern apparent patterns during the fine-tuning stages. Therefore, SLMs could pre- dict negative samples with (relatively) high confi- dence and accuracy. Due to LLMs’ predisposition to false-positive predictions on negative samples, however, the performance of LLMs on easy sam- ples collapses. We attribute such false-positive pre- dictions to (1) hallucination and (2) span boundary mismatch. We detail such two kinds of mistakes with cases in Appendix  F.2 . \n5 Adaptive Filter-then-rerank Paradigm \nAbove findings can be summarized as: (1) SLMs generally outperform LLMs, especially with more training samples and fine-grained labels. (2) SLMs are much more time- and cost-efficient. (3) LLMs serve as powerful rerankers on  hard  samples that challenge SLMs. Based on them, we propose a simple, efficient, and effective adaptive reranker that combines the strengths of SLMs and LLMs. \n5.1 Method \nOur  adaptive filter-then-rerank  approach, shown in Figure  6 , uses supervised SLMs as a filter to make preliminary decisions. Samples with confi- dence scores exceeding threshold are viewed as easy samples otherwise hard ones. For easy sam- ples, we retain SLM predictions as final results. For hard samples, top-  $\\cdot N$   predictions from SLMs are reranked via LLMs using ICL. Here LLMs employ MCQ prompts (Figure  4 ), containing demos and a sample to be reranked. The LLMs then generate the final answer and optionally provide an explanation. \n5.2 Experimental Setup \nWe conduct experiments on FewNERD for NER task, TACREV for RE task and ACE05 for ED task. We employ top-performing SLM-based meth- ods from Section  3  (FSLS or KnowPrompt) as the "}
{"page": 7, "image_path": "doc_images/2303.08559v2_7.jpg", "ocr_text": "Adrien said he met the\nBaptists’ leader, Laura Silsby\nof Meridian, Idaho, in Port-au-\n\nEugenio Vagni,\n\nPrince on Jan 26.\n\n=\n\nEasy Sample J Hard Sample\n\nFilter\nSmall LM\n\nAnalysis:\n\nS\n\nAdrien said he met the Baptists’ leader, Laura Silsby of\nMeridian, Idaho, in Port-au-Prince on Jan 26.\n\n(a)Laura Silsby lives in the state or province Meridian\n(b)Laura Silsby lives in the city Meridian\n\n(c)Laura Silsby was born in the city Meridian\n\n(@)Laura Silsby has no known relations to Meridian\n\nDemonstration\n\nThe lawyer denied Italian news reports that she\nwept while addressing the court, but said Knox was\nupset as she recounted the pressure, the\naggressiveness of the police who called her a liar,\n(a)she is the other family member of lawyer\n\n(b)she is a lawyer\n\n(c)she has no known relations to lawyer\n\nAnalysis: The word 'she' refers to someone who was\nupset while recounting certain events in court. The\nword ‘lawyer’ refers to someone who denied a news\n\nreport about that same person weeping in court.\n\nReranker\nLarge LM\n\nThere is no information in the sentence to indicate\nthat the two individuals are related in any way.\n\nAnswer: (c)\n\nv The sentence implies that Laura Silsby is associated with the city of Meridian in the state of Idaho, and\n\nper: cities_of_residence\n\ndoes not provide information about her birthplace. So Laura Silsby lives in the city Meridian.\n\nAnswer: (b)\n\nFigure 6: The overall architecture of our adaptive filter-then-rerank paradigm. We color easy samples in orange and\nhard samples in pink. For easy samples, the final predictions are exactly from the SLM-based methods. For hard\nsamples, the top-V predictions from SLMs are fed into LLMs as the format of multiple-choice questions (pink box).\nThe question is paired with demos (green box). LLMs rerank these N candidates and generate the final prediction.\n\nfilter, and Vicuna-13B, InstructGPT or GPT-4 as\nthe reranker. The threshold 7 to determine sam-\nple difficulty is optimized on the valid set. For\nhard sample, the top-3 SLM predictions and None\n(if not included) are feed to LLMs for reranking.\nEach LLM prompt has 4-shot demos. See demo\nexamples in Appendix G.1. We follow templates\nin Lu et al. (2022a) for TACREV and carefully de-\nsign others. See these templates in Appendix G.2.\nWe adopt chain-of-thought reasoning (Wei et al.,\n2022b), i.e., prefacing the answer with an explana-\ntion, to facilitate LLMs’ reranking procedure.\nBaseline We compare our method with two kinds\nof baselines to validate its effectiveness.\n\n(1) LLMs with ICL: We follow the prompts in Sec-\ntion 3.3 and conduct experiments on three LLMs.\n(2) Supervised SLMs: We follow previous SoTA\nmethods shown in Section 3.4 (FSLS or Know-\nPrompt). We additionally combine two SLMs with\nensemble or reranking approach (i.e., replace the\nLLM with another SLM as the reranker) to verify\nthat improvements from our SLM-LLM integrated\nsystem are not solely due to the ensemble effects.\n\n5.3. Main Results\n\nTable 3 shows that our filter-then-rerank method\nconsistently improves performance across three\ndatasets and nine settings. For instance, with In-\nstructGPT, reranking provides an average F1 gain\nof 2.4% without SLM ensemble (Lines 4 vs. 7).\nBased on ensemble SLMs as the filter, our method\nstill achieves 2.1% (Lines 5 vs. 8) gains on av-\n\nerage. This confirms (1) the effectiveness of the\nLLM reranking and (2) its gains are different and\n(almost) orthogonal to the SLM ensemble.\n\n5.4 Analysis\n\nFew makes big difference Our method selectively\nreranks hard samples. Table 4 shows that (1) only a\nminor fraction (0.5%~10%) of samples are deemed\nhard and are reranked by LLMs. (2) Despite their\nlimited quantity, reranking results in a substantial\nperformance boost on these samples (10%~25%\nabsolute F1 gains). This uplift on a small subset\nsignificantly enhances the overall performance.\nGPT-4 is more aggressive From Tables 3 and 4,\nGPT-4 generally improves more on hard samples,\nyet InstructGPT surpasses GPT-4 in NER and RE\ntasks when evaluated overall. This discrepancy\narises from GPT-4’s aggressive reranking which\nintroduces more true positives. InstructGPT, how-\never, focuses more on reducing false positives.\nFew makes small cost Figure 7 demonstrates that\nour method impressively reduces budget and la-\ntency by approximately 80%~90% compared to\ndirect ICL. This reduction is due to (1) fewer LLM\ncallings (only for hard samples) and (2) shorter\nprompts (fewer candidate labels and demos).\n\n5.5 Ablation Study\n\nWe investigate the effectiveness of the modules\nin adaptive filter-then-rerank system by removing\neach of them in turn: (1) CoT: We exclude the\nexplantion for each examples in demo. (2) Demo:\n", "vlm_text": "The image illustrates the architecture of an adaptive filter-then-rerank paradigm. Here's a breakdown:\n\n1. **Easy and Hard Samples**: \n   - Easy samples are colored in orange.\n   - Hard samples are colored in pink.\n\n2. **Processing Flow**:\n   - Easy samples: The final predictions come directly from Small Language Models (SLM).\n   - Hard samples: The top-N predictions from SLMs are converted into multiple-choice questions. These questions, paired with demonstrations, are then fed into Large Language Models (LLMs) for reranking.\n\n3. **Components**:\n   - **Filter (Small LM)**: Processes both easy and hard samples.\n   - **Reranker (Large LM)**: Handles hard samples by reranking predictions and generating the final answer.\n\n4. **Example Details**:\n   - The example question involves determining the relationship of \"Laura Silsby\" with \"Meridian.\"\n   - A demonstration provides a similar analysis to guide the reranking process.\n\nThis approach aims to leverage both SLMs for efficiency and LLMs for accuracy on more complex tasks.\nfilter, and Vicuna-13B, Instruct GP T or GPT-4 as the reranker. The threshold    $\\tau$   to determine sam- ple difficulty is optimized on the valid set. For hard sample, the top-3 SLM predictions and  None (if not included) are feed to LLMs for reranking. Each LLM prompt has 4-shot demos. See demo examples in Appendix  G.1 . We follow templates in  Lu et al.  ( 2022a ) for TACREV and carefully de- sign others. See these templates in Appendix  G.2 . We adopt chain-of-thought reasoning ( Wei et al. , 2022b ),  i.e.,  prefacing the answer with an explana- tion, to facilitate LLMs’ reranking procedure. \nBaseline  We compare our method with two kinds of baselines to validate its effectiveness. \n(1) LLMs with ICL: We follow the prompts in Sec- tion  3.3  and conduct experiments on three LLMs. (2) Supervised SLMs: We follow previous SoTA methods shown in Section  3.4  (FSLS or Know- Prompt). We additionally combine two SLMs with ensemble or reranking approach ( i.e.,  replace the LLM with another SLM as the reranker) to verify that improvements from our SLM-LLM integrated system are not solely due to the ensemble effects. \n5.3 Main Results \nTable  3  shows that our  filter-then-rerank  method consistently improves performance across three datasets and nine settings. For instance, with In- structGPT, reranking provides an average F1 gain of   $2.4\\%$   without SLM ensemble (Lines 4 vs. 7). Based on ensemble SLMs as the filter, our method still achieves  $2.1\\%$   (Lines 5 vs. 8) gains on av- erage. This confirms (1) the effectiveness of the LLM reranking and (2) its gains are different and (almost) orthogonal to the SLM ensemble. \n\n5.4 Analysis \nFew makes big difference  Our method selectively reranks hard samples. Table  4  shows that (1) only a minor fraction   $(0.5\\%{\\sim}10\\%)$   of samples are deemed hard and are reranked by LLMs. (2) Despite their limited quantity, reranking results in a substantial performance boost on these samples (  $(10\\%{\\sim}25\\%$  absolute F1 gains). This uplift on a small subset significantly enhances the overall performance. \nGPT-4 is more aggressive  From Tables  3  and  4 , GPT-4 generally improves more on hard samples, yet Instruct GP T surpasses GPT-4 in NER and RE tasks when evaluated overall. This discrepancy arises from GPT-4’s aggressive reranking which introduces more true positives. Instruct GP T, how- ever, focuses more on reducing false positives. \nFew makes small cost  Figure  7  demonstrates that our method impressively reduces budget and la- tency by approximately   $80\\%{\\sim}90\\%$   compared to direct ICL. This reduction is due to (1) fewer LLM callings (only for hard samples) and (2) shorter prompts (fewer candidate labels and demos). \n5.5 Ablation Study \nWe investigate the effectiveness of the modules in adaptive  filter-then-rerank  system by removing each of them in turn: (1)  CoT : We exclude the explantion for each examples in demo. (2)  Demo : "}
{"page": 8, "image_path": "doc_images/2303.08559v2_8.jpg", "ocr_text": "Table 3: Overall results of LLM-based ICL methods, SLM-based supervised methods, and our proposed filter-then-\nrerank (SLM+LLM) methods. The best results are in bold face and the second best are underlined. All results\nexcept InstructGPT and GPT-4 are averaged over 5 runs, and sample standard deviations are in the round bracket.\n\nFewNERD (NER) TACREV (RE) ACE (ED)\n5-shot 10-shot 20-shot | 20-shot 50-shot 100-shot| 5-shot 10-shot 20-shot\nCODEX 53.8(0.5) 54.0(1.4) 55.9(0.5)|59.1(1.4) 60.3(2.4) 62.4(2.6)|47.1(1.2) 47.7(2.8) 47.9(0.5)\n4 InstructGPT 53.6(-) 54.6(-) 57.2(-) | 60.1(-) 58.3(-) 62.7(-) | 52.9(-) 52.1(-) 49.3(-)\n= GPT-4 - - 57.8(-) - - 59.3(-) - - 52.1(-)\n= Previous SoTA 59.4(1.5) 61.4(0.8) 61.9(1.2)|62.4(3.8) 68.5(1.6) 72.6(1.5)|55.1(4.6) 63.9(0.8) 65.8(2.0)\n+ Ensemble (S) 59.6(1.7) 61.8(1.2) 62.6(1.0)|64.9(1.5) 71.9(2.2) 74.1(1.7)|56.9(4.7) 64.2(2.1) 66.5(1.7)\nM+ Rerank (S) 59.4(1.5) 61.0(1.7) 61.5(1.7)|64.2(2.3) 70.8(2.3) 74.3(2.2)|56.1(0.3) 64.0(1.0) 66.7(1.7)\nVicuna-13B\n+ Rerank (L) 60.0(1.8) 61.9(2.1) 62.2(1.4)|65.2(1.4) 70.8(1.6) 73.8(1.7)|56.9(4.0) 63.5(2.7) 66.0(2.6)\n= + Ensemble (S) + Rerank (L) |59.9(0.7) 62.1(0.7) 62.8(1.1)|66.5(0.5) 73.6(1.4) 75.0(1.5)|57.9(5.2) 64.4(1.2) 66.2(2.4)\n= InstructGPT\n+  +Rerank (L) 60.6(2.1) 62.7(0.8) 63.3(0.6)|66.8(2.6) 72.3(1.4) 75.4(1.5)|57.8(4.6) 65.3(1.7) 67.3(2.2)\n=  +Ensemble (S) + Rerank (L) |61.3(1.9) 63.2(0.9) 63.7(1.8) |68.9(1.3) 74.8(1.3) 76.8(1.2) |59.5(3.7) 65.3(1.9) 67.8(2.1)\nPA GPT-4\n7)\n+ Rerank (L) 60.8(2.3) 62.6 (2.7) 63.0(1.3)|65.9(2.7) 72.3(0.3) 74.5(1.5)|59.6(2.9) 64.9(2.5) 67.1(2.5)\n+ Ensemble (S) + Rerank (L) |61.1(2.2) 62.8(0.9) 63.6(1.2)|68.6(1.3) 73.9(1.4) 75.9(2.4)|60.9(3.9) 65.6(1.5) 67.8(1.7)\n\nTable 4: The Fl-score differences before and after\nreranking on the reranked samples, as well as their pro-\nportion of the total samples.\n\nGPT-4\nbefore after A\n\nInstructGPT\n\nratio |before after A ratio\n\nFewNER\nTACREV\nACE05\n\n31.9 40.7 8.8 3.2%\n25.3 43.0 17.7 9.1%\n31.1 57.9 26.8 1.6%\n\n31.4 28.3 —3.1 3.3%\n33.8 43.4 9.6 7.1%\n35.6 55.7 20.1 0.5%\n\nWe remove all examples, rendering the reranking\na zero-shot problem. (3) LF (label filtering): We\nretain all labels as candidate choices for reranking,\ninstead of only the top-N labels from the SLMs.\n(4) AD (adaptive): We feed all samples, not just\nhard ones, to the LLMs.\n\nWe show their results in Table 5 and see that\n(1) Demos with explanations consistently enhance\nthe reranking ability of LLMs across all datasets.\n(2) Demos without explanations also contribute to\nperformance improvement. (3) Label filtering re-\nsults in gains and notably reduces the demo length,\n\n[Ml Direct ICL (InstructGPT) [ll Filter-then-rerank Ml Fine-tuning (ROBERTa-large)\n\n40\n150\n_ >\nge ¥ 100\n3 3 50\n\n3\n\noe. o.\n\nFewNERD TACREV  ACE05\nTime cost\n\nFewNERD _ TACREV\nFinancial cost\n\n°\n\n— t)\n\n‘ACEOS\n\nFigure 7: The financial and time cost over 500 sentences.\nInstructGPT as the reranker.\n\nTable 5: Ablation study on three datasets. The filter is\n\nensembled SLMs and the reranker is GPT-4.\nFewNERD TACREV ACE05\nCoT Demo LF AD | (20-shot) (100-shot) (20-shot)\nv vv Vv 63.6(1.2) 75.9(2.4) 67.8(1.7)\nx vvv 63.2(1.2) 75.4(2.4) 67.2(1.7)\nx x vv 63.0(1.4)  _74.9(2.2)  66.6(1.5)\nx x xX v 62.4(2.1) 73.8(2.5) 66.5(1.3)\nx x xX x 12.5(2.7) 59.9(6.0) 5.4 (1.1)\nPrevious SoTA methods] 62.6(1.0) 74.1(1.7) 66.5(1.7)\n\nhence cutting inference costs. (4) The performance\ncollapses without a filter to identify sample diffi-\nculty, reiterating the need for an integrated SLM-\nLLM system to complement each other.\n\n6 Conclusion\n\nThrough an extensive empirical study on nine\ndatasets spanning four IE tasks, we find that LLMs,\ndespite their superiority in extreme low-resource\nscenarios, are not effective few-shot information\nextractors in general. They struggle with IE-related\nprompts, have limited demonstration capacity, and\nincur high inference costs. However, LLMs signifi-\ncantly improve the performance on hard samples\nwhen combined with SLM. Building on these in-\nsights, we propose an adaptive filter-then-rerank\nparadigm to leverage the strengths of SLMs and\nLLMs and mitigate their limitations. This approach\nconsistently achieves promising results, with an av-\nerage 2.4% FI gain across multiple few-shot IE\ntasks, while minimizing latency and budget costs.\n", "vlm_text": "The table compares the performance of different language models on various NLP tasks: FewNERD (Named Entity Recognition), TACREV (Relation Extraction), and ACE (Event Detection). The tasks are evaluated with varying numbers of \"shots\" (examples): 5, 10, and 20 for FewNERD and ACE, and 20, 50, and 100 for TACREV.\n\nHere are the key components:\n\n1. **Models Evaluated:**\n   - LLM (Large Language Models): CODEX, InstructGPT, GPT-4\n   - SLM (Small Language Models) and their combinations with LLMs\n\n2. **Performance Metrics:**\n   - Results are presented in various configurations such as \"Previous SoTA,\" \"+ Ensemble (S),\" \"+ Rerank (S),\" and combinations of SLM with LLM.\n   - Numbers in parentheses represent standard deviations.\n\n3. **Models & Tasks:**\n   - Performance is shown for each task under different shot conditions.\n   - Models are combined in various ways to enhance performance.\n\nThe table provides insights into how different models and strategies perform across tasks, highlighting improvements with combinations.\nThe table compares the performance of GPT-4 and InstructGPT on three tasks: FewNER, TACREV, and ACE05. For each model, it shows the performance \"before\" and \"after\" a certain intervention, along with the change (∆) and ratio.\n\n### GPT-4:\n- **FewNER**: \n  - Before: 31.9\n  - After: 40.7\n  - ∆: 8.8\n  - Ratio: 3.2%\n\n- **TACREV**: \n  - Before: 25.3\n  - After: 43.0\n  - ∆: 17.7\n  - Ratio: 9.1%\n\n- **ACE05**: \n  - Before: 31.1\n  - After: 57.9\n  - ∆: 26.8\n  - Ratio: 1.6%\n\n### InstructGPT:\n- **FewNER**: \n  - Before: 31.4\n  - After: 28.3\n  - ∆: -3.1\n  - Ratio: 3.3%\n\n- **TACREV**: \n  - Before: 33.8\n  - After: 43.4\n  - ∆: 9.6\n  - Ratio: 7.1%\n\n- **ACE05**: \n  - Before: 35.6\n  - After: 55.7\n  - ∆: 20.1\n  - Ratio: 0.5%\nWe remove all examples, rendering the reranking a zero-shot problem. (3)  LF  (label filtering): We retain all labels as candidate choices for reranking, instead of only the top-  $\\cdot N$   labels from the SLMs. (4)  AD  (adaptive): We feed all samples, not just hard ones, to the LLMs. \nWe show their results in Table  5  and see that (1) Demos with explanations consistently enhance the reranking ability of LLMs across all datasets. (2) Demos without explanations also contribute to performance improvement. (3) Label filtering re- sults in gains and notably reduces the demo length, \nThe image contains two bar charts comparing financial and time costs over 500 sentences for different approaches using \"InstructGPT\" for reranking:\n\n1. **Financial Cost** (left chart):\n   - **Direct ICL (InstructGPT)** has the highest cost across all datasets (FewNERD, TACREV, ACE05).\n   - **Filter–then–rerank** has a moderate cost.\n   - **Fine-tuning (RoBERTa-large)** has the lowest cost.\n\n2. **Time Cost** (right chart):\n   - **Direct ICL (InstructGPT)** takes the most time across all datasets.\n   - **Filter–then–rerank** takes moderate time.\n   - **Fine-tuning (RoBERTa-large)** is the fastest.\n\nEach dataset (FewNERD, TACREV, ACE05) shows similar trends in both financial and time costs.\nThe table compares different methods using checkmarks (✓) for four settings: CoT, Demo, LF, and AD. It presents results for three datasets: FewNERD (20-shot), TACREV (100-shot), and ACE05 (20-shot). The values are in the format \"score (std)\", representing performance scores and standard deviations, compared against previous state-of-the-art (SoTA) methods.\n\nHere's a summary:\n- When all settings are used (✓), the performance is highest across all datasets.\n- Removing combinations of settings (×) decreases performance.\n- The previous SoTA methods have slightly lower scores for each dataset compared to the best proposed method using all settings.\nhence cutting inference costs. (4) The performance collapses without a filter to identify sample diffi- culty, reiterating the need for an integrated SLM- LLM system to complement each other. \n6 Conclusion \nThrough an extensive empirical study on nine datasets spanning four IE tasks, we find that LLMs, despite their superiority in extreme low-resource scenarios, are not effective few-shot information extractors in general. They struggle with IE-related prompts, have limited demonstration capacity, and incur high inference costs. However, LLMs signifi- cantly improve the performance on  hard  samples when combined with SLM. Building on these in- sights, we propose an adaptive  filter-then-rerank paradigm to leverage the strengths of SLMs and LLMs and mitigate their limitations. This approach consistently achieves promising results, with an av- erage  $2.4\\%$   F1 gain across multiple few-shot IE tasks, while minimizing latency and budget costs. "}
{"page": 9, "image_path": "doc_images/2303.08559v2_9.jpg", "ocr_text": "Limitations\n\nWe do work hard to find better prompts to elicit the\npower of LLMs on few-shot IE tasks in Section 3.5,\nby exploring various kinds of LLMs, demonstra-\ntion strategies and prompt formats. We find that dif-\nferent prompt variants do not significantly impact\nin-context learning abilities. As an empirical study,\nwe acknowledge the potential existence of a lottery\nprompt superior to our explored prompts. However,\nit seems unlikely that an improved prompt would\nsubstantially alter our conclusions.\n\nAnother common risk when evaluating LLMs\non public benchmark is their potential memoriza-\ntion of samples tested. To mitigate such poten-\ntial contamination, we use earlier and stable ver-\nsions of these models rather than the newer and\nupdated ones (for example, gpt-4-0314 instead of\ngpt-4). Even if such contamination makes abilities\nof LLMs overestimated, our primary conclusions\nremain unchanged because we find that LLMs are\nNOT good few-shot information extractors.\n\nRegarding our adaptive _filter-then-rerank\nparadigm, a key limitation lies in how to assess\nsample difficulty. In this work, we employ a\nsimple unsupervised metric, i.e. the maximum\nprobabilities from SLMs. This is predicated on the\nassumption that SLMs are well-calibrated (Guo\net al., 2017). However, it is an obviously imperfect\nassumption. We envision that calibrating SLMs-\nbased filters or developing an advanced difficulty\nmetric could substantially enhance LLM rerankers’\nperformance. We leave them for future work.\n\nAcknowlegement\n\nThis study is supported under the RIE2020 In-\ndustry Alignment Fund — Industry Collaboration\nProjects (IAF-ICP) Funding Initiative, the Singa-\npore Ministry of Education (MOE) Academic Re-\nsearch Fund (AcRF) Tier 1 grant, as well as cash\nand in-kind contribution from the industry part-\nner(s).\n\nReferences\n\nMonica Agrawal, Stefan Hegselmann, Hunter Lang,\nYoon Kim, and David Sontag. 2022. Large language\nmodels are few-shot clinical information extractors.\nIn Proceedings of the 2022 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n1998-2022, Abu Dhabi, United Arab Emirates. Asso-\nciation for Computational Linguistics.\n\nChristoph Alt, Aleksandra Gabryszak, and Leonhard\nHennig. 2020. TACRED revisited: A thorough eval-\nuation of the TACRED relation extraction task. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 1558—\n1569, Online. Association for Computational Linguis-\ntics.\n\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-Voss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems 33:\nAnnual Conference on Neural Information Process-\ning Systems 2020, NeurIPS 2020, December 6-12,\n2020, virtual.\n\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan,\nHenrique Pondé de Oliveira Pinto, Jared Kaplan,\nHarrison Edwards, Yuri Burda, Nicholas Joseph,\nGreg Brockman, Alex Ray, Raul Puri, Gretchen\nKrueger, Michael Petrov, Heidy Khlaaf, Girish Sas-\ntry, Pamela Mishkin, Brooke Chan, Scott Gray,\nNick Ryder, Mikhail Pavlov, Alethea Power, Lukasz\nKaiser, Mohammad Bavarian, Clemens Winter,\nPhilippe Tillet, Felipe Petroski Such, Dave Cum-\nmings, Matthias Plappert, Fotios Chantzis, Eliza-\nbeth Barnes, Ariel Herbert-Voss, William Hebgen\nGuss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie\nTang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,\nWilliam Saunders, Christopher Hesse, Andrew N.\nCarr, Jan Leike, Joshua Achiam, Vedant Misra, Evan\nMorikawa, Alec Radford, Matthew Knight, Miles\nBrundage, Mira Murati, Katie Mayer, Peter Welinder,\nBob McGrew, Dario Amodei, Sam McCandlish, Ilya\nSutskever, and Wojciech Zaremba. 2021. Evaluat-\ning large language models trained on code. ArXiv\npreprint, abs/2107.03374.\n\nTing Chen, Simon Kornblith, Mohammad Norouzi, and\nGeoffrey E. Hinton. 2020. A simple framework for\ncontrastive learning of visual representations. In Pro-\nceedings of the 37th International Conference on\nMachine Learning, ICML 2020, 13-18 July 2020, Vir-\ntual Event, volume 119 of Proceedings of Machine\nLearning Research, pages 1597-1607. PMLR.\n\nWenhu Chen, Xueguang Ma, Xinyi Wang, and\nWilliam W. Cohen. 2022a. Program of thoughts\nprompting: Disentangling computation from reason-\ning for numerical reasoning tasks.\n\nXiang Chen, Ningyu Zhang, Xin Xie, Shumin Deng,\nYunzhi Yao, Chuanqi Tan, Fei Huang, Luo Si, and\nHuajun Chen. 2022b. Knowprompt: Knowledge-\naware prompt-tuning with synergistic optimization\nfor relation extraction. In WWW ’22: The ACM Web\n", "vlm_text": "Limitations \nWe do work hard to find better prompts to elicit the power of LLMs on few-shot IE tasks in Section  3.5 , by exploring various kinds of LLMs, demonstra- tion strategies and prompt formats. We find that dif- ferent prompt variants do not significantly impact in-context learning abilities. As an empirical study, we acknowledge the potential existence of a  lottery prompt superior to our explored prompts. However, it seems unlikely that an improved prompt would substantially alter our conclusions. \nAnother common risk when evaluating LLMs on public benchmark is their potential memoriza- tion of samples tested. To mitigate such poten- tial contamination, we use earlier and stable ver- sions of these models rather than the newer and updated ones (for example,    $\\mathsf{g p t\\!-\\!4\\!-\\!\\!0314}$   instead of gpt-4 ). Even if such contamination makes abilities of LLMs overestimated, our primary conclusions remain unchanged because we find that LLMs are NOT  good few-shot information extractors. \nRegarding our adaptive filter-then-rerank paradigm, a key limitation lies in how to assess sample difficulty. In this work, we employ a simple unsupervised metric,  i.e.,  the maximum probabilities from SLMs. This is predicated on the assumption that SLMs are well-calibrated ( Guo et al. ,  2017 ). However, it is an obviously imperfect assumption. We envision that calibrating SLMs- based filters or developing an advanced difficulty metric could substantially enhance LLM rerankers’ performance. We leave them for future work. \nAc knowle ge ment \nThis study is supported under the RIE2020 In- dustry Alignment Fund – Industry Collaboration Projects (IAF-ICP) Funding Initiative, the Singa- pore Ministry of Education (MOE) Academic Re- search Fund (AcRF) Tier 1 grant, as well as cash and in-kind contribution from the industry part- ner(s). \nReferences \nMonica Agrawal, Stefan Hegselmann, Hunter Lang, Yoon Kim, and David Sontag. 2022.  Large language models are few-shot clinical information extractors . In  Proceedings of the 2022 Conference on Empiri- cal Methods in Natural Language Processing , pages 1998–2022, Abu Dhabi, United Arab Emirates. Asso- ciation for Computational Linguistics. \nChristoph Alt, Aleksandra Gabryszak, and Leonhard Hennig. 2020.  TACRED revisited: A thorough eval- uation of the TACRED relation extraction task . In Proceedings of the 58th Annual Meeting of the Asso- ciation for Computational Linguistics , pages 1558– 1569, Online. Association for Computational Linguis- tics. \nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neel a kant an, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020.  Language models are few-shot learners . In  Ad- vances in Neural Information Processing Systems 33: Annual Conference on Neural Information Process- ing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual . \nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Pondé de Oliveira Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sas- try, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cum- mings, Matthias Plappert, Fotios Chantzis, Eliza- beth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021.  Evaluat- ing large language models trained on code .  ArXiv preprint , abs/2107.03374. \nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. 2020.  A simple framework for contrastive learning of visual representations . In  Pro- ceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Vir- tual Event , volume 119 of  Proceedings of Machine Learning Research , pages 1597–1607. PMLR. \nWenhu Chen, Xueguang Ma, Xinyi Wang, and William W. Cohen. 2022a. Program of thoughts prompting: Disentangling computation from reason- ing for numerical reasoning tasks . \nXiang Chen, Ningyu Zhang, Xin Xie, Shumin Deng, Yunzhi Yao, Chuanqi Tan, Fei Huang, Luo Si, and Huajun Chen. 2022b.  Knowprompt: Knowledge- aware prompt-tuning with synergistic optimization for relation extraction . In  WWW ’22: The ACM Web "}
{"page": 10, "image_path": "doc_images/2303.08559v2_10.jpg", "ocr_text": "Conference 2022, Virtual Event, Lyon, France, April\n25 - 29, 2022, pages 2778-2788. ACM.\n\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion\nStoica, and Eric P. Xing. 2023. Vicuna: An open-\nsource chatbot impressing gpt-4 with 90%* chatgpt\nquality.\n\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben\nHutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\nToju Duke, Anselm Levskaya, Sanjay Ghemawat,\nSunipa Dev, Henryk Michalewski, Xavier Garcia,\nVedant Misra, Kevin Robinson, Liam Fedus, Denny\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,\nBarret Zoph, Alexander Spiridonov, Ryan Sepassi,\nDavid Dohan, Shivani Agrawal, Mark Omernick, An-\ndrew M. Dai, Thanumalayan Sankaranarayana Pil-\nlai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,\nRewon Child, Oleksandr Polozov, Katherine Lee,\nZongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark\nDiaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy\nMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,\nand Noah Fiedel. 2022. Palm: Scaling language mod-\neling with pathways.\n\nHyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, Al-\nbert Webson, Shixiang Shane Gu, Zhuyun Dai,\nMirac Suzgun, Xinyun Chen, Aakanksha Chowdh-\nery, Alex Castro-Ros, Marie Pellat, Kevin Robinson,\nDasha Valter, Sharan Narang, Gaurav Mishra, Adams\nYu, Vincent Zhao, Yanping Huang, Andrew Dai,\nHongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Ja-\ncob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le,\nand Jason Wei. 2022. Scaling instruction-finetuned\nlanguage models.\n\nBosheng Ding, Chengwei Qin, Linlin Liu, Yew Ken\nChia, Boyang Li, Shafiq Joty, and Lidong Bing. 2023.\nIs GPT-3 a good data annotator? In Proceedings\nof the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 11173-11195, Toronto, Canada. Association\nfor Computational Linguistics.\n\nNing Ding, Guangwei Xu, Yulin Chen, Xiaobin Wang,\nXu Han, Pengjun Xie, Haitao Zheng, and Zhiyuan\nLiu. 2021. Few-NERD: A few-shot named entity\nrecognition dataset. In Proceedings of the 59th An-\nnual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Confer-\nence on Natural Language Processing (Volume 1:\nLong Papers), pages 3198-3213, Online. Association\nfor Computational Linguistics.\n\nGeorge Doddington, Alexis Mitchell, Mark Przybocki,\nLance Ramshaw, Stephanie Strassel, and Ralph\nWeischedel. 2004. The automatic content extrac-\ntion (ACE) program — tasks, data, and evaluation. In\nProceedings of the Fourth International Conference\non Language Resources and Evaluation (LREC’04),\nLisbon, Portugal. European Language Resources As-\nsociation (ELRA).\n\nSeth Ebner, Patrick Xia, Ryan Culkin, Kyle Rawlins,\nand Benjamin Van Durme. 2020. Multi-sentence ar-\ngument linking. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 8057-8077, Online. Association for\nComputational Linguistics.\n\nJun Gao, Huan Zhao, Changlong Yu, and Ruifeng Xu.\n2023. Exploring the feasibility of chatgpt for event\nextraction.\n\nTianyu Gao, Xingcheng Yao, and Danqi Chen. 2021.\nSimCSE: Simple contrastive learning of sentence em-\nbeddings. In Proceedings of the 2021 Conference\non Empirical Methods in Natural Language Process-\ning, pages 6894-6910, Online and Punta Cana, Do-\nminican Republic. Association for Computational\nLinguistics.\n\nChuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Wein-\nberger. 2017. On calibration of modern neural net-\nworks. In Proceedings of the 34th International Con-\nference on Machine Learning, ICML 2017, Sydney,\nNSW, Australia, 6-11 August 2017, volume 70 of\nProceedings of Machine Learning Research, pages\n1321-1330. PMLR.\n\nBernal Jimenez Gutierrez, Nikolas McNeal, Clayton\nWashington, You Chen, Lang Li, Huan Sun, and\nYu Su. 2022. Thinking about GPT-3 in-context learn-\ning for biomedical IE? think again. In Findings of the\nAssociation for Computational Linguistics: EMNLP\n2022, pages 4497-4512, Abu Dhabi, United Arab\nEmirates. Association for Computational Linguistics.\n\nMartin Josifoski, Marija Sakota, Maxime Peyrard, and\nRobert West. 2023. Exploiting asymmetry for syn-\nthetic training data generation: Synthie and the case\nof information extraction.\n\nJaehun Jung, Lianhui Qin, Sean Welleck, Faeze Brah-\nman, Chandra Bhagavatula, Ronan Le Bras, and\nYejin Choi. 2022. Maieutic prompting: Logically\nconsistent reasoning with recursive explanations. In\nProceedings of the 2022 Conference on Empirical\nMethods in Natural Language Processing, pages\n1266-1279, Abu Dhabi, United Arab Emirates. Asso-\nciation for Computational Linguistics.\n\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020.\nBART: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\n", "vlm_text": "Conference 2022, Virtual Event, Lyon, France, April 25 - 29, 2022 , pages 2778–2788. ACM. \nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. 2023.  Vicuna: An open- source chatbot impressing gpt-4 with  $90\\%^{*}$   chatgpt quality . \nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsv yash chen ko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vin- odkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Micha lewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, An- drew M. Dai, Than u malayan Sankara narayana Pil- lai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022.  Palm: Scaling language mod- eling with pathways . \nHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Al- bert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdh- ery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Ja- cob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. 2022.  Scaling instruction-finetuned language models . \nBosheng Ding, Chengwei Qin, Linlin Liu, Yew Ken Chia, Boyang Li, Shafiq Joty, and Lidong Bing. 2023. Is GPT-3 a good data annotator? In  Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 11173–11195, Toronto, Canada. Association for Computational Linguistics. \nNing Ding, Guangwei Xu, Yulin Chen, Xiaobin Wang, Xu Han, Pengjun Xie, Haitao Zheng, and Zhiyuan Liu. 2021.  Few-NERD: A few-shot named entity recognition dataset . In  Proceedings of the 59th An- nual Meeting of the Association for Computational Linguistics and the 11th International Joint Confer- ence on Natural Language Processing (Volume 1: Long Papers) , pages 3198–3213, Online. Association for Computational Linguistics. \nGeorge Doddington, Alexis Mitchell, Mark Przybocki, Lance Ramshaw, Stephanie Strassel, and Ralph Weischedel. 2004. The automatic content extrac- tion (ACE) program – tasks, data, and evaluation . In Proceedings of the Fourth International Conference on Language Resources and Evaluation (LREC’04) , Lisbon, Portugal. European Language Resources As- sociation (ELRA). \nSeth Ebner, Patrick Xia, Ryan Culkin, Kyle Rawlins, and Benjamin Van Durme. 2020.  Multi-sentence ar- gument linking . In  Proceedings of the 58th Annual Meeting of the Association for Computational Lin- guistics , pages 8057–8077, Online. Association for Computational Linguistics. \nJun Gao, Huan Zhao, Changlong Yu, and Ruifeng Xu. 2023.  Exploring the feasibility of chatgpt for event extraction . \nTianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. SimCSE: Simple contrastive learning of sentence em- beddings . In  Proceedings of the 2021 Conference on Empirical Methods in Natural Language Process- ing , pages 6894–6910, Online and Punta Cana, Do- minican Republic. Association for Computational Linguistics. \nChuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Wein- berger. 2017.  On calibration of modern neural net- works . In  Proceedings of the 34th International Con- ference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017 , volume 70 of Proceedings of Machine Learning Research , pages 1321–1330. PMLR. \nBernal Jimenez Gutierrez, Nikolas McNeal, Clayton Washington, You Chen, Lang Li, Huan Sun, and Yu Su. 2022.  Thinking about GPT-3 in-context learn- ing for biomedical IE? think again . In  Findings of the Association for Computational Linguistics: EMNLP 2022 , pages 4497–4512, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. \nMartin Josifoski, Marija Sakota, Maxime Peyrard, and Robert West. 2023.  Exploiting asymmetry for syn- thetic training data generation: Synthie and the case of information extraction . \nJaehun Jung, Lianhui Qin, Sean Welleck, Faeze Brah- man, Chandra Bhaga va tula, Ronan Le Bras, and Yejin Choi. 2022.  Maieutic prompting: Logically consistent reasoning with recursive explanations . In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing , pages 1266–1279, Abu Dhabi, United Arab Emirates. Asso- ciation for Computational Linguistics. \nMike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghaz vi nine j ad, Abdel rahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Z ett le moyer. 2020. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and com- prehension . In  Proceedings of the 58th Annual Meet- ing of the Association for Computational Linguistics , "}
{"page": 11, "image_path": "doc_images/2303.08559v2_11.jpg", "ocr_text": "pages 7871-7880, Online. Association for Computa-\ntional Linguistics.\n\nPeng Li, Tianxiang Sun, Qiong Tang, Hang Yan, Yuan-\nbin Wu, Xuanjing Huang, and Xipeng Qiu. 2023.\nCodelE: Large code generation models are better\nfew-shot information extractors. In Proceedings\nof the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 15339-15353, Toronto, Canada. Association\nfor Computational Linguistics.\n\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan,\nLawrence Carin, and Weizhu Chen. 2022. What\nmakes good in-context examples for GPT-3? In\nProceedings of Deep Learning Inside Out (DeeLIO\n2022): The 3rd Workshop on Knowledge Extrac-\ntion and Integration for Deep Learning Architectures,\npages 100-114, Dublin, Ireland and Online. Associa-\ntion for Computational Linguistics.\n\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach.\n\nIlya Loshchilov and Frank Hutter. 2019. Decoupled\nweight decay regularization. In 7th International\nConference on Learning Representations, ICLR 2019,\nNew Orleans, LA, USA, May 6-9, 2019. OpenRe-\nview.net.\n\nKeming Lu, JI-Hung Hsu, Wenxuan Zhou,\nMingyu Derek Ma, and Muhao Chen. 2022a.\nSummarization as indirect supervision for relation\nextraction. In Findings of the Association for\nComputational Linguistics: EMNLP 2022, pages\n6575-6594, Abu Dhabi, United Arab Emirates.\nAssociation for Computational Linguistics.\n\nKeming Lu, Xiaoman Pan, Kaiqiang Song, Hongming\nZhang, Dong Yu, and Jianshu Chen. 2023. Pivoine:\nInstruction tuning for open-world information extrac-\ntion. ArXiv preprint, abs/2305.14898.\n\nYao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel,\nand Pontus Stenetorp. 2022b. Fantastically ordered\nprompts and where to find them: Overcoming few-\nshot prompt order sensitivity. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n8086-8098, Dublin, Ireland. Association for Compu-\ntational Linguistics.\n\nYaojie Lu, Qing Liu, Dai Dai, Xinyan Xiao, Hongyu\nLin, Xianpei Han, Le Sun, and Hua Wu. 2022c. Uni-\nfied structure generation for universal information\nextraction. In Proceedings of the 60th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 5755-5772, Dublin,\nIreland. Association for Computational Linguistics.\n\nJie Ma, Miguel Ballesteros, Srikanth Doss, Rishita\nAnubhai, Sunil Mallya, Yaser Al-Onaizan, and Dan\n\nRoth. 2022a. Label semantics for few shot named\nentity recognition. In Findings of the Association for\nComputational Linguistics: ACL 2022, pages 1956-\n1971, Dublin, Ireland. Association for Computational\nLinguistics.\n\nYubo Ma, Zehao Wang, Yixin Cao, Mukai Li, Meiqi\nChen, Kun Wang, and Jing Shao. 2022b. Prompt\nfor extraction? PAIE: Prompting argument interac-\ntion for event argument extraction. In Proceedings\nof the 60th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 6759-6774, Dublin, Ireland. Association for\nComputational Linguistics.\n\nYubo Ma, Zehao Wang, Yixin Cao, and Aixin Sun. 2023.\nFew-shot event detection: An empirical study and a\nunified view. In Proceedings of the 61st Annual Meet-\ning of the Association for Computational Linguis-\ntics (Volume 1: Long Papers), pages 11211-11236,\nToronto, Canada. Association for Computational Lin-\nguistics.\n\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L. Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder,\nPaul Christiano, Jan Leike, and Ryan Lowe. 2022.\nTraining language models to follow instructions with\nhuman feedback.\n\nLiangming Pan, Alon Albalak, Xinyi Wang, and\nWilliam Yang Wang. 2023. Logic-lm: Empower-\ning large language models with symbolic solvers for\nfaithful logical reasoning.\n\nCheng Qian, Chi Han, Yi R. Fung, Yujia Qin, Zhiyuan\nLiu, and Heng Ji. 2023. Creator: Tool creation for\ndisentangling abstract and concrete reasoning of large\nlanguage models.\n\nChengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao\nChen, Michihiro Yasunaga, and Diyi Yang. 2023. Is\nchatgpt a general-purpose natural language process-\ning task solver?\n\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. J. Mach. Learn. Res., 21:140:1-140:67.\n\nOhad Rubin, Jonathan Herzig, and Jonathan Berant.\n2022. Learning to retrieve prompts for in-context\nlearning. In Proceedings of the 2022 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 2655-2671, Seattle, United States.\nAssociation for Computational Linguistics.\n\nZhiyi Song, Ann Bies, Stephanie Strassel, Tom Riese,\nJustin Mott, Joe Ellis, Jonathan Wright, Seth Kulick,\nNeville Ryant, and Xiaoyi Ma. 2015. From light\nto rich ERE: Annotation of entities, relations, and\n", "vlm_text": "pages 7871–7880, Online. Association for Computa- tional Linguistics. \nPeng Li, Tianxiang Sun, Qiong Tang, Hang Yan, Yuan- bin Wu, Xuanjing Huang, and Xipeng Qiu. 2023. CodeIE: Large code generation models are better few-shot information extractors . In  Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 15339–15353, Toronto, Canada. Association for Computational Linguistics. \nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. 2022. What makes good in-context examples for GPT-3? In Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extrac- tion and Integration for Deep Learning Architectures , pages 100–114, Dublin, Ireland and Online. Associa- tion for Computational Linguistics. \nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man- dar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Z ett le moyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pre training ap- proach . \nIlya Loshchilov and Frank Hutter. 2019.  Decoupled weight decay regular iz ation . In  7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019 . OpenRe- view.net. \nKeming Lu, I-Hung Hsu, Wenxuan Zhou, Mingyu Derek Ma, and Muhao Chen. 2022a. Sum mari z ation as indirect supervision for relation extraction . In  Findings of the Association for Computational Linguistics: EMNLP 2022 , pages 6575–6594, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. \nKeming Lu, Xiaoman Pan, Kaiqiang Song, Hongming Zhang, Dong Yu, and Jianshu Chen. 2023.  Pivoine: Instruction tuning for open-world information extrac- tion .  ArXiv preprint , abs/2305.14898. \nYao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. 2022b.  Fantastically ordered prompts and where to find them: Overcoming few- shot prompt order sensitivity . In  Proceedings of the 60th Annual Meeting of the Association for Compu- tational Linguistics (Volume 1: Long Papers) , pages 8086–8098, Dublin, Ireland. Association for Compu- tational Linguistics. \nYaojie Lu, Qing Liu, Dai Dai, Xinyan Xiao, Hongyu Lin, Xianpei Han, Le Sun, and Hua Wu. 2022c.  Uni- fied structure generation for universal information extraction . In  Proceedings of the 60th Annual Meet- ing of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 5755–5772, Dublin, Ireland. Association for Computational Linguistics. \nJie Ma, Miguel Ball ester os, Srikanth Doss, Rishita Anubhai, Sunil Mallya, Yaser Al-Onaizan, and Dan \nRoth. 2022a.  Label semantics for few shot named entity recognition . In  Findings of the Association for Computational Linguistics: ACL 2022 , pages 1956– 1971, Dublin, Ireland. Association for Computational Linguistics. \nYubo Ma, Zehao Wang, Yixin Cao, Mukai Li, Meiqi Chen, Kun Wang, and Jing Shao. 2022b.  Prompt for extraction? PAIE: Prompting argument interac- tion for event argument extraction . In  Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 6759–6774, Dublin, Ireland. Association for Computational Linguistics. \nYubo Ma, Zehao Wang, Yixin Cao, and Aixin Sun. 2023. Few-shot event detection: An empirical study and a unified view . In  Proceedings of the 61st Annual Meet- ing of the Association for Computational Linguis- tics (Volume 1: Long Papers) , pages 11211–11236, Toronto, Canada. Association for Computational Lin- guistics. \nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car- roll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback . \nLiangming Pan, Alon Albalak, Xinyi Wang, and William Yang Wang. 2023.  Logic-lm: Empower- ing large language models with symbolic solvers for faithful logical reasoning . \nCheng Qian, Chi Han, Yi R. Fung, Yujia Qin, Zhiyuan Liu, and Heng Ji. 2023.  Creator: Tool creation for disentangling abstract and concrete reasoning of large language models . \nChengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro Yasunaga, and Diyi Yang. 2023.  Is chatgpt a general-purpose natural language process- ing task solver? \nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020.  Exploring the limits of transfer learning with a unified text-to-text trans- former .  J. Mach. Learn. Res. , 21:140:1–140:67. \nOhad Rubin, Jonathan Herzig, and Jonathan Berant. 2022.  Learning to retrieve prompts for in-context learning . In  Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech- nologies , pages 2655–2671, Seattle, United States. Association for Computational Linguistics. \nZhiyi Song, Ann Bies, Stephanie Strassel, Tom Riese, Justin Mott, Joe Ellis, Jonathan Wright, Seth Kulick, Neville Ryant, and Xiaoyi Ma. 2015.  From light to rich ERE: Annotation of entities, relations, and "}
{"page": 12, "image_path": "doc_images/2303.08559v2_12.jpg", "ocr_text": "events. In Proceedings of the The 3rd Workshop on\nEVENTS: Definition, Detection, Coreference, and\nRepresentation, pages 89-98, Denver, Colorado. As-\nsociation for Computational Linguistics.\n\nHongjin Su, Jungo Kasai, Chen Henry Wu, Weijia Shi,\nTianlu Wang, Jiayi Xin, Rui Zhang, Mari Ostendorf,\nLuke Zettlemoyer, Noah A. Smith, and Tao Yu. 2022.\nSelective annotation makes language models better\nfew-shot learners.\n\nZhigqing Sun, Xuezhi Wang, Yi Tay, Yiming Yang, and\nDenny Zhou. 2023. Recitation-augmented language\nmodels. In /nternational Conference on Learning\nRepresentations.\n\nErik F. Tjong Kim Sang and Fien De Meulder.\n2003. Introduction to the CoNLL-2003 shared task:\nLanguage-independent named entity recognition. In\nProceedings of the Seventh Conference on Natural\nLanguage Learning at HLT-NAACL 2003, pages 142-\n147.\n\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Roziére, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023. Llama: Open\nand efficient foundation language models.\n\nSomin Wadhwa, Silvio Amir, and Byron Wallace. 2023.\nRevisiting relation extraction in the era of large lan-\nguage models. In Proceedings of the 61st Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 15566-\n15589, Toronto, Canada. Association for Computa-\ntional Linguistics.\n\nXiao Wang, Wei Zhou, Can Zu, Han Xia, Tianze Chen,\nYuan Zhang, Rui Zheng, Junjie Ye, Qi Zhang, Tao\nGui, Jihua Kang, J. Yang, Siyuan Li, and Chunsai\nDu. 2023a. Instructuie: Multi-task instruction tuning\nfor unified information extraction. ArXiv preprint,\nabs/2304.08085.\n\nXiaozhi Wang, Ziqi Wang, Xu Han, Wangyi Jiang, Rong\nHan, Zhiyuan Liu, Juanzi Li, Peng Li, Yankai Lin,\nand Jie Zhou. 2020. MAVEN: A Massive General\nDomain Event Detection Dataset. In Proceedings\nof the 2020 Conference on Empirical Methods in\nNatural Language Processing (EMNLP), pages 1652-\n1671, Online. Association for Computational Linguis-\ntics.\n\nXingyao Wang, Sha Li, and Heng Ji. 2023b.\nCode4Struct: Code generation for few-shot event\nstructure prediction. In Proceedings of the 61st An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 3640-\n3663, Toronto, Canada. Association for Computa-\ntional Linguistics.\n\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc\nLe, Ed Chi, Sharan Narang, Aakanksha Chowdhery,\nand Denny Zhou. 2023c. Self-consistency improves\n\nchain of thought reasoning in language models. In\nThe Eleventh International Conference on Learning\nRepresentations (ICLR 2023).\n\nYizhong Wang, Swaroop Mishra, Pegah Alipoormo-\nlabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva\nNaik, Arjun Ashok, Arut Selvan Dhanasekaran,\nAnjana Arunkumar, David Stap, Eshaan Pathak,\nGiannis Karamanolakis, Haizhi Lai, Ishan Puro-\nhit, Ishani Mondal, Jacob Anderson, Kirby Kuznia,\nKrima Doshi, Kuntal Kumar Pal, Maitreya Patel,\nMehrad Moradshahi, Mihir Parmar, Mirali Purohit,\nNeeraj Varshney, Phani Rohitha Kaza, Pulkit Verma,\nRavsehaj Singh Puri, Rushang Karia, Savan Doshi,\nShailaja Keyur Sampat, Siddhartha Mishra, Sujan\nReddy A, Sumanta Patro, Tanay Dixit, and Xudong\nShen. 2022. Super-NaturalInstructions: Generaliza-\ntion via declarative instructions on 1600+ NLP tasks.\nIn Proceedings of the 2022 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n5085-5109, Abu Dhabi, United Arab Emirates. As-\nsociation for Computational Linguistics.\n\nJason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin\nGuu, Adams Wei Yu, Brian Lester, Nan Du, An-\ndrew M. Dai, and Quoc V. Le. 2022a. Finetuned\nlanguage models are zero-shot learners. In The Tenth\nInternational Conference on Learning Representa-\ntions, ICLR 2022, Virtual Event, April 25-29, 2022.\nOpenReview.net.\n\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed Huai hsin Chi, Quoc Le, and Denny Zhou.\n2022b. Chain of thought prompting elicits reasoning\nin large language models. Proceedings of the 36th\nInternational Conference on Neural Information Pro-\ncessing Systems.\n\nXiang Wei, Xingyu Cui, Ning Cheng, Xiaobin Wang,\nXin Zhang, Shen Huang, Pengjun Xie, Jinan Xu,\nYufeng Chen, Meishan Zhang, Yong Jiang, and Wen-\njuan Han. 2023. Zero-shot information extraction via\nchatting with chatgpt.\n\nRalph Weischedel, Martha Palmer, Mitchell Marcus, Ed-\nuard Hovy, Sameer Pradhan, Lance Ramshaw, Nian-\nwen Xue, Ann Taylor, Jeff Kaufman, Michelle Fran-\nchini, et al. 2013. Ontonotes release 5.0 1dc2013t19.\nLinguistic Data Consortium, Philadelphia, PA.\n\nYi Yang and Arzoo Katiyar. 2020. Simple and effective\nfew-shot named entity recognition with structured\nnearest neighbor learning. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 6365-6375,\nOnline. Association for Computational Linguistics.\n\nWenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu,\nMingxuan Ju, Soumya Sanyal, Chenguang Zhu,\nMichael Zeng, and Meng Jiang. 2023. Generate\nrather than retrieve: Large language models are\nstrong context generators. In International Confer-\nence for Learning Representation (ICLR 2023).\n\nKai Zhang, Bernal Jimenez Gutierrez, and Yu Su. 2023a.\nAligning instruction tasks unlocks large language\n", "vlm_text": "events . In  Proceedings of the The 3rd Workshop on EVENTS: Definition, Detection, Co reference, and Representation , pages 89–98, Denver, Colorado. As- sociation for Computational Linguistics. \nHongjin Su, Jungo Kasai, Chen Henry Wu, Weijia Shi, Tianlu Wang, Jiayi Xin, Rui Zhang, Mari Ostendorf, Luke Z ett le moyer, Noah A. Smith, and Tao Yu. 2022. Selective annotation makes language models better few-shot learners . \nZhiqing Sun, Xuezhi Wang, Yi Tay, Yiming Yang, and Denny Zhou. 2023.  Recitation-augmented language models . In  International Conference on Learning Representations . \nErik F. Tjong Kim Sang and Fien De Meulder. 2003.  Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition . In Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003 , pages 142– 147. \nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023.  Llama: Open and efficient foundation language models . \nSomin Wadhwa, Silvio Amir, and Byron Wallace. 2023. Revisiting relation extraction in the era of large lan- guage models . In  Proceedings of the 61st Annual Meeting of the Association for Computational Lin- guistics (Volume 1: Long Papers) , pages 15566– 15589, Toronto, Canada. Association for Computa- tional Linguistics. \nXiao Wang, Wei Zhou, Can Zu, Han Xia, Tianze Chen, Yuan Zhang, Rui Zheng, Junjie Ye, Qi Zhang, Tao Gui, Jihua Kang, J. Yang, Siyuan Li, and Chunsai Du. 2023a.  Instruct u ie: Multi-task instruction tuning for unified information extraction .  ArXiv preprint , abs/2304.08085. \nXiaozhi Wang, Ziqi Wang, Xu Han, Wangyi Jiang, Rong Han, Zhiyuan Liu, Juanzi Li, Peng Li, Yankai Lin, and Jie Zhou. 2020.  MAVEN: A Massive General Domain Event Detection Dataset . In  Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pages 1652– 1671, Online. Association for Computational Linguis- tics. \nXingyao Wang, Sha Li, and Heng Ji. 2023b. Code 4 Struct: Code generation for few-shot event structure prediction . In  Proceedings of the 61st An- nual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 3640– 3663, Toronto, Canada. Association for Computa- tional Linguistics. \nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023c. Self-consistency improves \nchain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations (ICLR 2023) . \nYizhong Wang, Swaroop Mishra, Pegah Alipoormo- labashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva Naik, Arjun Ashok, Arut Selvan Dhana sekar an, Anjana Arunkumar, David Stap, Eshaan Pathak, Giannis Karam a no lak is, Haizhi Lai, Ishan Puro- hit, Ishani Mondal, Jacob Anderson, Kirby Kuznia, Krima Doshi, Kuntal Kumar Pal, Maitreya Patel, Mehrad Moradshahi, Mihir Parmar, Mirali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh Puri, Rushang Karia, Savan Doshi, Shailaja Keyur Sampat, Siddhartha Mishra, Sujan Reddy A, Sumanta Patro, Tanay Dixit, and Xudong Shen. 2022.  Super-Natural Instructions: Generaliza- tion via declarative instructions on   $1600+$  NLP tasks . In  Proceedings of the 2022 Conference on Empiri- cal Methods in Natural Language Processing , pages 5085–5109, Abu Dhabi, United Arab Emirates. As- sociation for Computational Linguistics. \nJason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, An- drew M. Dai, and Quoc V. Le. 2022a.  Finetuned language models are zero-shot learners . In  The Tenth International Conference on Learning Representa- tions, ICLR 2022, Virtual Event, April 25-29, 2022 . OpenReview.net. \nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Huai hsin Chi, Quoc Le, and Denny Zhou. 2022b. Chain of thought prompting elicits reasoning in large language models.  Proceedings of the 36th International Conference on Neural Information Pro- cessing Systems . \nXiang Wei, Xingyu Cui, Ning Cheng, Xiaobin Wang, Xin Zhang, Shen Huang, Pengjun Xie, Jinan Xu, Yufeng Chen, Meishan Zhang, Yong Jiang, and Wen- juan Han. 2023.  Zero-shot information extraction via chatting with chatgpt . \nRalph Weischedel, Martha Palmer, Mitchell Marcus, Ed- uard Hovy, Sameer Pradhan, Lance Ramshaw, Nian- wen Xue, Ann Taylor, Jeff Kaufman, Michelle Fran- chini, et al. 2013. Ontonotes release 5.0 ldc2013t19. Linguistic Data Consortium, Philadelphia, PA . \nYi Yang and Arzoo Katiyar. 2020.  Simple and effective few-shot named entity recognition with structured nearest neighbor learning . In  Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pages 6365–6375, Online. Association for Computational Linguistics. \nWenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu, Mingxuan Ju, Soumya Sanyal, Chenguang Zhu, Michael Zeng, and Meng Jiang. 2023. Generate rather than retrieve: Large language models are strong context generators. In  International Confer- ence for Learning Representation (ICLR 2023) . \nKai Zhang, Bernal Jimenez Gutierrez, and Yu Su. 2023a. Aligning instruction tasks unlocks large language "}
{"page": 13, "image_path": "doc_images/2303.08559v2_13.jpg", "ocr_text": "models as zero-shot relation extractors. In Find-\nings of the Association for Computational Linguis-\ntics: ACL 2023, pages 794-812, Toronto, Canada.\nAssociation for Computational Linguistics.\n\nYuhao Zhang, Victor Zhong, Danqi Chen, Gabor Angeli,\nand Christopher D. Manning. 2017. Position-aware\nattention and supervised data improve slot filling.\nIn Proceedings of the 2017 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n35-45, Copenhagen, Denmark. Association for Com-\nputational Linguistics.\n\nZhuosheng Zhang, Aston Zhang, Mu Li, and Alex\nSmola. 2023b. Automatic chain of thought prompt-\ning in large language models. In The Eleventh In-\nternational Conference on Learning Representations\n(ICLR 2023).\n\nA Datasets\nA.1_ Full Datasets\n\nWe construct few-shot IE datasets and conduct\nthe empirical study on nine datasets spanning four\ntasks, with varying schema complexities ranging\nfrom 4 to 168. We show their statistics in Table 6.\n\nA.2 Details of Few-shot IE Datasets\n\nSampling Algorithm for Train/Valid Datasets.\nWe downsample sentences from original training\ndataset to construct few-shot training and valid\ndatasets. We adopt K-shot sampling strategy that\neach label has (at least) A samples. We set 6 K-\nvalues (1, 5, 10, 20, 50, 100) for RE tasks and 4\nK-values (1, 5, 10, 20) for other tasks. For RE\ntask, each sentence has exactly one relation and we\nsimply select A’ sentences for each label. For NER,\nED and EAE tasks, each sentences is possible to\ncontain more than one entities/events/arguments.\nSince our sampling is at sentence-level, the algo-\nrithm of accurate sampling , i.e., finding exactly\nK samples for each label, is NP-complete® and\nunlikely to find a practical solution. Therefore we\nfollow Yang and Katiyar (2020) adopting a greedy\nsampling algorithm to select sentences for NER and\nED tasks, as shown in Algorithm 1. Note that the\nactual sample number of each label can be larger\nthan K under this sampling strategy. For all three\ntasks, we additionally sample negative sentences\n(without any defined labels) and make the ratio of\npositive sentences (with at least one label) and neg-\native sentences as 1:1. The statistics of the curated\ndatasets are listed in Table 7.\n\n8The Subset Sum Problem, a classical NP-complete prob-\nlem, can be reduced to this sampling problem.\n\nAlgorithm 1 Greedy Sampling\n\nRequire: shot number K, original full dataset\nD = {(X, Y)} tagged with label set E\n1: Sort E based on their frequencies in {Y} as\nan ascending order\nS < ¢, Counter < dict()\nfor y © Edo\nCounter(y) < 0\nend for\nfor y © Edo\nwhile Counter(y) < A do\nSample (X,Y) € Ds.t.5j, yj = y\nD+«D\\(X,Y)\nUpdate Counter (not only y but all\nevent types in Y)\n1: end while\n12: end for\n13: for s € S do\n14: S © S\\s and update Counter\n15: if Sy € E,s.t. Counter(y) < A then\n\n>\n\n16: S+¢SUs\n17: end if\n18: end for\n\n19: return S\n\nBased on the subsets constructed above, we op-\ntionally further split them into training and valid\nsets. For few-shot datasets with more than 300 sen-\ntences, we additionally split 10% sentences as the\nvalid set and the remaining sentences as training set.\nOtherwise, we do not construct valid set and con-\nduct 5-fold cross validation to avoid overfitting.\n\nB Details on SLMs\n\nWe adopt five representative supervised methods to\nevaluate the ability of SLMs on few-shot IE tasks.\n(1). Fine-tuning (FT): Add a classifier head on\nSLMs to predict the labels of each sentence/word.\n(2). FSLS (Ma et al., 2022a): The state-of-the-art\nextractive-based method for few-shot NER task.\nMa et al. (2023) also validate its competitive per-\nformance on few-shot ED tasks.\n(3). KnowPrompt (Chen et al., 2022b): The best\nextractive-based method for few-shot RE task.\n\n(4). PAIE (Ma et al., 2022b): The best extractive-\nbased method for few-shot EAE task.\n\n(5). UIE (Lu et al., 2022c): A competitive unified\ngeneration-based method for few-shot IE tasks. We\nintroduce their implementation details below:\nFine-tuning/FSLS. We implement these two meth-\n\n", "vlm_text": "models as zero-shot relation extractors . In  Find- ings of the Association for Computational Linguis- tics: ACL 2023 , pages 794–812, Toronto, Canada. Association for Computational Linguistics. \nYuhao Zhang, Victor Zhong, Danqi Chen, Gabor Angeli, and Christopher D. Manning. 2017.  Position-aware attention and supervised data improve slot filling . In  Proceedings of the 2017 Conference on Empiri- cal Methods in Natural Language Processing , pages 35–45, Copenhagen, Denmark. Association for Com- putational Linguistics. \nZhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. 2023b. Automatic chain of thought prompt- ing in large language models. In  The Eleventh In- ter national Conference on Learning Representations (ICLR 2023) . \nA Datasets \nA.1 Full Datasets \nWe construct few-shot IE datasets and conduct the empirical study on nine datasets spanning four tasks,  with varying schema complexities ranging from 4 to 168 . We show their statistics in Table  6 . \nA.2 Details of Few-shot IE Datasets \nSampling Algorithm for Train/Valid Datasets. We downsample sentences from original training dataset to construct few-shot training and valid datasets. We adopt    $K$  -shot sampling strategy that each label has (at least)    $K$   samples. We set 6    $K$  - values (1, 5, 10, 20, 50, 100) for RE tasks and 4  $K$  -values (1, 5, 10, 20) for other tasks. For RE task, each sentence has exactly one relation and we simply select  $K$   sentences for each label. For NER, ED and EAE tasks, each sentences is possible to contain more than one entities/events/arguments. Since our sampling is at sentence-level, the algo- rithm of accurate sampling ,  i.e.,  finding exactly  $K$   samples for each label, is NP-complete 8   and unlikely to find a practical solution. Therefore we follow  Yang and Katiyar  ( 2020 ) adopting a greedy sampling algorithm to select sentences for NER and ED tasks, as shown in Algorithm  1 . Note that the actual sample number of each label can be larger than  $K$   under this sampling strategy. For all three tasks, we additionally sample negative sentences (without any defined labels) and make the ratio of positive sentences (with at least one label) and neg- ative sentences as 1:1. The statistics of the curated datasets are listed in Table  7 . \nRequire:  shot number    $K$  , original full dataset  $\\mathcal{D}=\\{({\\bf X},{\\bf Y})\\}$   tagged with label set    $E$  1:  Sort  E  based on their frequencies in    $\\{\\mathbf{Y}\\}$   as an ascending order 2:    $S\\gets\\phi$  , Counter  $\\leftarrow$  dict () 3:  for    $y\\in E$   do 4:  ${\\mathrm{Counter}}(y)\\gets0$  5:  end for 6:  for    $y\\in E$   do 7: while  Counter  $(y)<K$   do 8: Sample    $(\\mathbf{X},\\mathbf{Y})\\in\\mathcal{D}\\;\\mathrm{s.t.}\\exists j,y_{j}=y$  9:  $\\mathcal{D}\\gets\\mathcal{D}\\backslash({\\mathbf{X}},{\\mathbf{Y}})$  10: Update Counter (not only    $y$   but all event types in  $\\mathbf{Y}$  ) 11: end while 12:  end for 13:  for    $s\\in\\mathcal S$   do 14:  ${\\mathcal{S}}\\gets{\\mathcal{S}}\\backslash s$   and update Counter 15: if    $\\exists y\\in E$  , s.t. Counter  $\\mathopen{}\\mathclose\\bgroup\\left(y\\aftergroup\\egroup\\right)<K$   then 16:  ${\\mathcal{S}}\\gets{\\mathcal{S}}\\bigcup{\\mathcal{S}}$  17: end if 18:  end for 19:  return  $s$  \nBased on the subsets constructed above, we op- tionally further split them into training and valid sets. For few-shot datasets with more than 300 sen- tences, we additionally split   $10\\%$   sentences as the valid set and the remaining sentences as training set. Otherwise, we do not construct valid set and con- duct 5-fold cross validation to avoid over fitting. \nB Details on SLMs \nWe adopt five representative supervised methods to evaluate the ability of SLMs on few-shot IE tasks. (1). Fine-tuning (FT) : Add a classifier head on SLMs to predict the labels of each sentence/word. (2). FSLS  ( Ma et al. ,  2022a ): The state-of-the-art extractive-based method for few-shot NER task. Ma et al.  ( 2023 ) also validate its competitive per- formance on few-shot ED tasks. \n(3). KnowPrompt  ( Chen et al. ,  2022b ): The best extractive-based method for few-shot RE task. \n(4). PAIE  ( Ma et al. ,  2022b ): The best extractive- based method for few-shot EAE task. \n(5). UIE  ( Lu et al. ,  2022c ): A competitive unified generation-based method for few-shot IE tasks. We introduce their implementation details below: \nFine-tuning/FSLS.  We implement these two meth- "}
{"page": 14, "image_path": "doc_images/2303.08559v2_14.jpg", "ocr_text": "Table 6: Statistics of nine datasets used. Note that the #mentions for event detection tasks refers to the number of\ntrigger words, while the #mentions for event argument extraction tasks refers to the number of arguments.\n\nNamed Entity Recognition | Relation Extraction Event Detection Event Arg Extraction\n\nDataset CONLL OntoNotes FewNERD|TACREV TACRED|ACE05 MAVEN ERE |ACE05 RAMS ERE\n#Label Type | 4 18 66 | 41 41 | 33 168 38 | 33 139 38\n\n#Sents Train| 14,041 49,706 131,965 | 68,124 68,124 | 14,024 32,360 14,736] 14,024 7329 14,736\n\n. Test | 3,453 10,348 37,648 15,509 15,509 | 728 8,035 1,163 | 728 871 1,163\n#Mentions Train| 23,499 128,738 340,247 | 13,012 13,012 | 5,349 77,993 6,208 | 4859 17026 8924\nTest | 5,648 12,586 96,902 3,123 3,123 424 18,904 551 576 = =2023 822\n\nTable 7: The statistics of few-shot training sets. We set\ndifferent random seeds and generate 5 training sets for\neach setting. We report their average statistics.\n\nDataset Settings |# Labels #Sent #Sample # Avg shot\nL-shot 48 38 4\n\nsoq S-shot 162 218 55\nCONLL'03 fo shot | 4 292 «426 10.7\n20-shot 656 82.0 20.5\n\nL-shot 200 334 9\n\n_ 5-shot 848 148.0 8.2\nOntoNotes io shor | '8 1586 281.0 15.6\n20-shot 332.8 347.2 30.4\n\nL-shot 898 147.0 22\n\n5-shot 286.2 494.8 75\n\nFewNERD iochot| 538.0 962.0 146\n20-shot 0272 1851.4 28.1\n\nL-shot 816 41.0 0\n\n5-shot 387.6 205.0 5.0\n\n10-shot 741.2 406.0 9.9\n\nTACREV 50-shot| 4! 13672 806.0 19.7\n50-shot 2872.0 1944.0 474\n\n100-shot 4561.0 3520.0 85.9\n\nL-shot 816 41.0 0\n\n5-shot 387.6 205.0 5.0\n\n10-shot 741.2 406.0 9.9\n\nTACRED 50-shot| 4! 13672 806.0 19.7\n50-shot 28712 1944.0 474\n\n100-shot 45752 3520.0 85.9\n\nL-shot 474. 41.0 2\n\n5-shot 1928 165.0 50\n\nACEOS  io-shot| 33 3346 319.4 97\n20-shot 579.4 398.2 18.1\n\nL-shot 157.6 298.0 8\n\n5-shot 540.4 1262.2 75\n\nMAVEN io-shot | 168 — go12 2413.8 144\n20-shot 12864 4611.4 274\n\nL-shot 484. 54.6 4\n\n5-shot 1750 219.2 58\n\nFRE jo-shot}| 38 304.8 © 432.4 114\n20-shot 521.6 806.6 212\n\nL-shot 234 402 2\n\n5-shot 798 178.2 54\n\nACEOS ig-shot| 33 1308 337. 10.2\n20-shot 213.4 630.2 19.1\n\nL-shot 1302 332.6 24\n\n5-shot 514.0 1599.6 115\n\nRAMS fo-shot | '39 795.2 3193.2 23.0\n20-shot 10704 6095.4 43.9\n\nL-shot 216 102.8 27\n\n5-shot 742 © 403.4 10.6\n\nFRE jo-shot} 38 1272 775.6 204\n20-shot 1902 1397.2 368\n\nods by ourselves. We use RoBERTa-large (Liu\net al., 2019) as the backbones. We adopt Auto-\nmatic Mixed Precision (AMP) training strategy? to\nsave memory. We run each experiment on a single\nNVIDIA V100 GPU. We train each model with\nthe AdamW (Loshchilov and Hutter, 2019) opti-\nmizer with linear scheduler and 0.1 warm-up steps.\nWe set the weight-decay coefficient as le-5 and\nmaximum gradient norms as 1.0. We set the batch\nsize as 64, the maximum input length as 192, the\ntraining step as 500 and the learning rate as 5e-5.\nKnowPrompt We implement this method based\non original source code!®, and use ROBERTa-large\nas our backbones. We set 10 maximum epochs for\n50- and 100-shot datasets, and as 50 epochs for\nother datasets. We keep all other hyperparameters\nas default, and run each experiment on a single\nNVIDIA V100 GPU.\n\nPAIE We implement this method on original\nsource code!!, and use BART-large (Lewis et al.,\n2020) as backbones. We keep all hyperparameters\nas default for ACE and RAMS dataset. For ERE\ndataset, we set the training step as 1000, the batch\nsize as 16 and the learning rate as 2e-5. We run\neach experiment on a single NVIDIA V100 GPU.\nUIE We implement this method based on original\nsource code!?, and use T5-large (Raffel et al.,\n2020) as the backbones. We run each experiment\non a single NVIDIA Quadro RTX8000 GPU. We\nset the batch size as 4 with 4000 training steps.\nWe set the maximum input length as 800 and the\nlearning rate as le-4.\n\nC_ LLMs Implementations\n\nRegarding our empirical study, we explore the ICL\nabilities of LLMs on few-shot IE tasks. We mainly\nuse five LLMs from two sources. (1) OpenAI\n\n°https://pytorch.org/docs/stable/amp.html\n‘Ohttps://github.com/zjunlp/KnowPrompt\n\n\" https://github.com/mayubo2333/PAIE\n?https://github.com/universal-ie/UIE\n", "vlm_text": "The table provides statistics on different datasets used for Named Entity Recognition, Relation Extraction, Event Detection, and Event Argument Extraction. Here are the details:\n\n### Columns:\n1. **Dataset Types**\n   - Named Entity Recognition (CONLL, OntoNotes, FewNERD)\n   - Relation Extraction (TACREV, TACRED)\n   - Event Detection (ACE05, MAVEN, ERE)\n   - Event Arg Extraction (ACE05, RAMS, ERE)\n\n### Rows:\n1. **#Label Type**: Number of label types for each dataset.\n   - Named Entity Recognition: 4, 18, 66\n   - Relation Extraction: 41, 41\n   - Event Detection: 33, 168, 38\n   - Event Arg Extraction: 33, 139, 38\n\n2. **#Sents**: Number of sentences in training and test sets.\n   - Named Entity Recognition: \n     - Train: 14,041 (CONLL), 49,706 (OntoNotes), 131,965 (FewNERD)\n     - Test: 3,453 (CONLL), 10,348 (OntoNotes), 37,648 (FewNERD)\n   - Relation Extraction:\n     - Train: 68,124 (TACREV and TACRED)\n     - Test: 15,509 (TACREV and TACRED)\n   - Event Detection:\n     - Train: 14,024 (ACE05), 32,360 (MAVEN), 14,736 (ERE)\n     - Test: 728 (ACE05), 8,035 (MAVEN), 1,163 (ERE)\n   - Event Arg Extraction:\n     - Train: 14,024 (ACE05), 7,329 (RAMS), 14,736 (ERE)\n     - Test: 728 (ACE05), 871 (RAMS), 1,163 (ERE)\n\n3. **#Mentions**: Number of mentions in training and test sets.\n   - Named Entity Recognition: \n     - Train: 23,499 (CONLL), 128,738 (OntoNotes), 340,247 (FewNERD)\n     - Test: 5,648 (CONLL), 12,586 (OntoNotes), 96,902 (FewNERD)\n   - Relation Extraction:\n     - Train: 13,012 (TACREV and TACRED)\n     - Test: 3,123 (TACREV and TACRED)\n   - Event Detection:\n     - Train: 5,349 (ACE05), 77,993 (MAVEN), 6,208 (ERE)\n     - Test: 424 (ACE05), 18,904 (MAVEN), 551 (ERE)\n   - Event Arg Extraction:\n     - Train: 4,859 (ACE\nThe table presents dataset settings with various metrics across different dataset names. Here is the breakdown of its structure:\n\n- **Columns:**\n  - **Dataset Settings:** Name of the dataset.\n  - **# Labels:** Number of labels in the dataset.\n  - **# Sent:** Number of sentences.\n  - **# Sample:** Number of samples.\n  - **# Avg shot:** Average shot size.\n\n- **Rows:** \n  - Each dataset is evaluated under different \"shot\" settings, such as 1-shot, 5-shot, 10-shot, etc.\n  - The values for each metric are provided for each dataset under these shot settings.\n\n- **Datasets Included:**\n  - CONLL'03, OntoNotes, FewNERD, TACREV, TACRED, ACE05, MAVEN, ERE, and RAMS.\n\nThis table is likely from a study involving few-shot learning or similar methodologies applied to natural language processing tasks.\nods by ourselves. We use  RoBERTa-large  ( Liu et al. ,  2019 ) as the backbones. We adopt Auto- matic Mixed Precision (AMP) training strategy 9   to save memory. We run each experiment on a single NVIDIA V100 GPU. We train each model with the AdamW ( Loshchilov and Hutter ,  2019 ) opti- mizer with linear scheduler and 0.1 warm-up steps. We set the weight-decay coefficient as 1e-5 and maximum gradient norms as 1.0. We set the batch size as 64, the maximum input length as 192, the training step as 500 and the learning rate as 5e-5. \nKnowPrompt  We implement this method based on original source code 10 , and use  RoBERTa-large as our backbones. We set 10 maximum epochs for 50- and 100-shot datasets, and as 50 epochs for other datasets. We keep all other hyper parameters as default, and run each experiment on a single NVIDIA V100 GPU. \nPAIE  We implement this method on original source code 11 , and use  BART-large  ( Lewis et al. , 2020 ) as backbones. We keep all hyper parameters as default for ACE and RAMS dataset. For ERE dataset, we set the training step as 1000, the batch size as 16 and the learning rate as 2e-5. We run each experiment on a single NVIDIA V100 GPU. \nUIE  We implement this method based on original source code 12 , and use  T5-large  ( Raffel et al. , 2020 ) as the backbones. We run each experiment on a single NVIDIA Quadro RTX8000 GPU. We set the batch size as 4 with 4000 training steps. We set the maximum input length as 800 and the learning rate as 1e-4. \nC LLMs Implementations \nRegarding our empirical study, we explore the ICL abilities of LLMs on few-shot IE tasks. We mainly use five LLMs from two sources. (1) OpenAI models: CODEX ( code-davinci-002 ;  Chen et al. 2021 ), Instruct GP T ( text-davinci-003 ;  Ouyang et al. 2022 ), and ChatGPT ( gpt-3.5-turbo-0301 ). (2) Open-source models: LLaMA-13B ( Touvron et al. ,  2023 ) and its instruction-tuned counterpart, Vicuna-13B ( Chiang et al. ,  2023 ). We detail their implementation details in the next sections below. "}
{"page": 15, "image_path": "doc_images/2303.08559v2_15.jpg", "ocr_text": "models: CODEX (code-davinci-@2; Chen et al.\n2021), InstructGPT (text-davinci-003; Ouyang\net al. 2022), and ChatGPT (gpt-3.5-turbo-0301).\n(2) Open-source models: LLaMA-13B (Touvron\net al., 2023) and its instruction-tuned counterpart,\nVicuna-13B (Chiang et al., 2023). We detail their\nimplementation details in the next sections below.\n\nC.1 Open-source Models\n\nWe implement multiple ICL approaches on\nLLaMA-13B and Vicuna-13B without fine-tuning.\nWe set the maximum input length as 2048 and the\nbatch size as 1. We run each experiment on a single\nNVIDIA V100 GPU. To achieve this, we leverage\nthe Accelerate !3 framework and fp16 inference\nto save memory. We set maximum output length\nas 96 and sampling temperature as 0 (i.e., greedy\ndecoding). We set both frequency_penalty and\npresence_penalty as 0.\n\nC.2 OpenAI Models\n\nWe implement multiple ICL approaches on Ope-\nnAI models by calling their official APIs '*. We set\nthe maximum input length as 3600 for all tasks and\nmodels. The only exception occurs when we use\nCODEX on RE tasks, where we set the maximum\ninput length as 7000. We unify the maximum out-\nput length as 32 for RE task, and 96 for other three\ntasks. We set the sampling temperature coefficient\nas 0, i.e., greedy decoding.\n\nD Pivot Experiments on LLMs\n\nD.1 Sampling Temperature\n\nExisting prompt-engineering discussion!> suggests\nsetting the sampling temperature t = 0 for tasks\nwith structured outputs, including IE tasks. We\nvalidate this conclusion in Table 8, from which we\ncould see the generated quality when t = 0 is much\nhigher than the quality when t 4 0. Therefore we\nset t = 0 in all main experiments, and do not take\nself-consistency (Wang et al., 2023c) into account.\n\nD.2. Automatic Chain-of-thought\n\nWe additionally investigate whether rationales\ncould facilitate LLMs’ performance on few-shot\nIE tasks. Since there exists no golden rationales in\n\n'Shttps://huggingface.co/docs/accelerate\n\n'4https://openai.com/blog/openai-api\n\n'Shttps://help.openai.com/en/articles/6654000-best-\npractices-for-prompt-engineering-with-openai-api\n\nTable 8: Fl-scores across different t values. Experi-\nments run on 10-shot settings with CODEX.\n\n|FewNERD TACREV ACE0S\n\nt=0 48.5(1.9) 53.7(2.3) 42.9(2.2)\n+ 5-ensemble 53.5(1.3) 58.6(1.5) 46.3(0.8)\nt=0.7 40.9(2.3) 39.9(1.2) 35.6(2.0)\n+ self-consistency| 52.1(0.9) 53.4(1.3) 45.6(3.0)\n\noriginal datasets, we follow Automatic Chain-of-\nthought (Auto-CoT; Zhang et al. 2023b) method as\nbelow. Regarding each sample, we query LLMs\n\nAccording to [sentence], Why [span] is a [label].\n\nFor example, given the sentence “DSC and Trac-\ntion Control on all Speed3 models is also stan-\ndard.”, we would feed LLM the query that “Could\nyou explain why Speed3 is a kind of car”. Then we\ninsert the bootstrapped rationales between the sen-\ntences and ground-truth answers. If a sentence has\nno positive labels, however, we do not ask LLMs\nand keep the original format as the vanilla ICL ap-\nproach. Here we prompt InstructGPT to generate\nthe rationales with temperature t = 0.7. We com-\npare the performance with and without Auto-CoT\nas shown in Table 9.\n\nTable 9: The Fl-score difference between with and\nwithout Auto-CoT. We generate rationales by Instruct-\nGPT, then adopt ICL w. Auto-CoT approach and use\nCODEX as our backbone for inference.\n\n10-shot train set FewNERD TACREV ACE05\n\n(NER) (RE) (ED)\nwo. Auto-CoT 54.0(1.4)  57.3(1.8) 47.7(2.8)\nw. Auto-CoT 36.6(1.7)  22.0(1.2) 43.1(3.4)\n\nWe are frustrated to find Auto-CoT degrades the\nperformance with a large margin. We speculate\nthis degration could be attributed to three main\nreasons. (1) The rationale increase the length of\neach sample and thus decrease the overall example\nnumber in demos. (2) There exists an obvious\ndiscrepancy between sentences with and without\npositive labels. The rationales are only provided for\nsentences with positive labels because it is hard to\nexplain why a sentence dose not contain any label.\n(3) Some auto-generated rationales are low-quality,\nespecially for RE tasks. We would explore better\nstrategy to exploit auto-genertaed rationales in the\nfuture work.\n\n", "vlm_text": "\nC.1 Open-source Models \nWe implement multiple ICL approaches on LLaMA-13B and Vicuna-13B without fine-tuning. We set the maximum input length as 2048 and the batch size as 1. We run each experiment on a single NVIDIA V100 GPU. To achieve this, we leverage the  Accelerate   13   framework and fp16 inference to save memory. We set maximum output length as 96 and sampling temperature as 0 ( i.e.,  greedy decoding). We set both  frequency penalty  and presence penalty  as 0. \nC.2 OpenAI Models \nWe implement multiple ICL approaches on Ope- nAI models by calling their official APIs   14 . We set the maximum input length as 3600 for all tasks and models. The only exception occurs when we use CODEX on RE tasks, where we set the maximum input length as 7000. We unify the maximum out- put length as 32 for RE task, and 96 for other three tasks. We set the sampling temperature coefficient as 0,  i.e.,  greedy decoding. \nD Pivot Experiments on LLMs \nD.1 Sampling Temperature \nExisting prompt-engineering discussion 15   suggests setting the sampling temperature    $t=0$   for tasks with structured outputs, including IE tasks. We validate this conclusion in Table  8 , from which we could see the generated quality when  $t=0$   is much hig an the quality when  $t\\neq0$  . Therefore we set  $t=0$   in all main experiments, and do not take self-consistency ( Wang et al. ,  2023c ) into account. \nD.2 Automatic Chain-of-thought \nWe additionally investigate whether rationales could facilitate LLMs’ performance on few-shot IE tasks. Since there exists no golden rationales in \nThe table presents results for three datasets: FewNERD, TACREV, and ACE05. The results are based on different configurations:\n\n1. **t = 0**:\n   - Without any modifications:\n     - FewNERD: 48.5 (±1.9)\n     - TACREV: 53.7 (±2.3)\n     - ACE05: 42.9 (±2.2)\n   - With 5-ensemble:\n     - FewNERD: 53.5 (±1.3)\n     - TACREV: 58.6 (±1.5)\n     - ACE05: 46.3 (±0.8)\n\n2. **t = 0.7**:\n   - Without any modifications:\n     - FewNERD: 40.9 (±2.3)\n     - TACREV: 39.9 (±1.2)\n     - ACE05: 35.6 (±1.0)\n   - With self-consistency:\n     - FewNERD: 52.1 (±0.9)\n     - TACREV: 53.4 (±1.3)\n     - ACE05: 45.6 (±3.0)\n\nThe numbers seem to represent some performance metrics, such as accuracy or F1 score, with standard deviations in parentheses.\noriginal datasets, we follow Automatic Chain-of- thought (Auto-CoT;  Zhang et al. 2023b ) method as below. Regarding each sample, we query LLMs \nAccording to [sentence], Why [span] is a [label] \nFor example, given the sentence  “DSC and Trac- tion Control on all  Speed3  models is also stan- dard.” , we would feed LLM the query that  “Could you explain why Speed3 is a kind of car” . Then we insert the boots trapped rationales between the sen- tences and ground-truth answers. If a sentence has no positive labels, however, we do not ask LLMs and keep the original format as the vanilla ICL ap- proach. Here we prompt Instruct GP T to generate the rationales with temperature    $t=0.7$  . We com- pare the performance with and without Auto-CoT as shown in Table  9 . \nThe table compares results from a 10-shot train set across three datasets: FewNERD (NER), TACREV (RE), and ACE05 (ED). It contrasts two methods: without Auto-CoT and with Auto-CoT. \n\nFor FewNERD (NER):\n- Without Auto-CoT: 54.0 (±1.4)\n- With Auto-CoT: 36.6 (±1.7)\n\nFor TACREV (RE):\n- Without Auto-CoT: 57.3 (±1.8)\n- With Auto-CoT: 22.0 (±1.2)\n\nFor ACE05 (ED):\n- Without Auto-CoT: 47.7 (±2.8)\n- With Auto-CoT: 43.1 (±3.4)\nWe are frustrated to find Auto-CoT degrades the performance with a large margin. We speculate this degration could be attributed to three main reasons. (1) The rationale increase the length of each sample and thus decrease the overall example number in demos. (2) There exists an obvious discrepancy between sentences with and without positive labels. The rationales are only provided for sentences with positive labels because it is hard to explain why a sentence dose not contain any label. (3) Some auto-generated rationales are low-quality, especially for RE tasks. We would explore better strategy to exploit auto-genertaed rationales in the future work. "}
{"page": 16, "image_path": "doc_images/2303.08559v2_16.jpg", "ocr_text": "Table 10: Fl-scores difference among GPT-4, CODEX and InstructGPT.\n\nNER (20-shot)\nCONLL OntoNotes FewNERD\n\nRE (100-shot)\nTACREV TACRED\n\nED (20-shot) EAE (20-shot)\nACE05 MAVEN ERE|ACE05 RAMS ERE\n\nInstructGPT 772 41.7 57.2 62.7 53.8 49.3 254 40.8| 45.8 42.2 41.9\nCODEX 81.1 55.6 55.9 62.4 53.6 47.9 22.8 39.0 - - -\n\nGPT-4 84.7 65.6 57.8 59.3 50.4 52.1 30.2 40.5] 42.9 38.6 38.2\nSupervised SoTA| 72.3 74.9 61.4 | 72.6 63.1 | 65.8 54.7 56.2| 55.2 57.7 55.6\n\nD.3. GPT-4 v.s. Others\n\nWe tend to minimize the GPT-4 calls due to its high\nprice. Thus we utilize 20-/100-shot settings across\neach dataset to compare GPT-4’s performance with\nother LLMs. Table 10 reveals that GPT-4 does not\noutperform other LLMs significantly, except on\nOntoNotes and MAVEN. However, even on these\ndatasets, GPT-4 still falls behind supervised SLMs\nby a significant margin. Consequently, the exclu-\nsion of GPT-4 does not undermine the conclusions\ndrawn from our main experiments, and we omit it\nfrom our empirical study.\n\nE_ Auxiliary Experiments\n\nE.1 LLMs struggle on Fine-grained Datasets\n\nBased on the results shown in Figure 2, we addi-\ntionally provide a quantitative analysis to show that\nLLMs struggle with fine-grained datasets. Under\nthe 5-shot setting, we compare the performance\ndifference of LLMs (ChatGPT) and SLMs (SoTA\nfew-shot models) among different datasets. For\neach IE task, we observe a clear negative corre-\n\nTable 11: Performance comparison between LLMs\n(ChatGPT) and SLM-based methods among datasets\nwith various schema complexities.\n\nNamed Entity Recognition\nCoNLL OntoNotes FewNERD\n# Entity 4 18 66\n\nMicro-F1 (SLM) 52.5 59.7 59.4\nMicro-F1 (LLM) 71.8 59.4 55.5\nAF1 (LLM, SLM) 25.3 -0.3 -3.9\nEvent Detection\n\nACE05 ERE MAVEN\n# Event 33 38 168\nMicro-F1 (SLM) 55.1 48.0 49.4\nMicro-F1 (LLM) 39.6 33.8 25.3\nAF1 (LLM, SLM) -15.5 -14.2 -24.1\n\nEvent Argument Extraction\n\nACE05 ERE RAMS\n# Event / #Role 33/22 38/26 139/65\nHead-F1 (SLM) 45.9 40.4 54.1\nHead-F1 (LLM) 52.8 40.7 44.2\nAF1(LLM,SLM) 6.9 0.3 -9.9\n\nlation between the label number (row 2) and the\nperformance difference (row 5). In other words,\nwith more label types, LLMs tend to perform rel-\natively worse than SLMs. Therefore we conclude\nthat LLMs struggle on fine-grained datasets.\n\nE.2 Finding Better Instruction\n\nTo investigate whether LLMs would benefit from\ncomplex instructions, we explored six instruction\nvariants from simple to complex. Take NER task\nas an example, we illustrate them as below.\nInstruction0: [empty]\n\nInstruction]: Identify the entities\nexpressed by each sentence, and locate\neach entity to words in the sentence.\nThe possible entity types are: [Type_1],\n[Type_2], ..., [Type_N]. If you do not\nfind any entity in this sentence, just\noutput ‘Answer: No entities found.’\nInstruction2: Identify the entities\nexpressed by each sentence, and locate\neach entity to words in the sentence.\nThe possible entity types are:\n\n¢ [Type_1]: [Definition_1]\n\n¢ [Type_2]: [Definition_2]\n\n¢ [Type_N]: [Definition_N]\n\nIf you do not\nthis sentence,\nentities found.’\n\nfind any\njust output\n\nentity in\n‘Answer: No\n\nInstruction3: Assume you are an\nentity-instance annotator. Given a\nsentence, you need to (1) identify the\n\nword or phrase about the entity in the\nsentence, and (2) classify its entity\ntype. The possible entity types are\nlisted as below: [Type_1], [Type_2],\n\n. , LType_N]. Please note that your\nannotation results must follow such\nformat: ”? Answer: ({Type_1] <SEP>\n", "vlm_text": "The table presents performance metrics of different language models on various NLP tasks, with comparisons to supervised state-of-the-art results. Here's a breakdown:\n\n### NER (20-shot)\n- **Datasets:** CONLL, OntoNotes, FewNERD\n- **Performance (F1 Scores):**\n  - InstructGPT: 77.2, 47.7, 57.2\n  - CODEX: 81.1, 55.6, 55.9\n  - GPT-4: **84.7**, **65.6**, **57.8**\n  - Supervised SoTA: 72.3, 74.9, 61.4\n\n### RE (100-shot)\n- **Datasets:** TACREV, TACRED\n- **Performance (F1 Scores):**\n  - InstructGPT: **62.7**, 53.8\n  - CODEX: 62.4, 53.6\n  - GPT-4: 59.3, 50.4\n  - Supervised SoTA: 72.6, 63.1\n\n### ED (20-shot)\n- **Datasets:** ACE05, MAVEN, ERE\n- **Performance (F1 Scores):**\n  - InstructGPT: 49.3, 25.4, **40.8**\n  - CODEX: 47.9, 22.8, 39.0\n  - GPT-4: **52.1**, **30.2**, 40.5\n  - Supervised SoTA: 65.8, 54.7, 56.2\n\n### EAE (20-shot)\n- **Datasets:** ACE05, RAMS, ERE\n- **Performance (F1 Scores):**\n  - InstructGPT: **45.8**, **42.2**, **41.9**\n  - CODEX: - , - , -\n  - GPT-4: 42.9, 38.6, 38.2\n  - Supervised SoTA: 55.2, 57.7, 55.6\n\n### Highlights\n- GPT-4 generally shows strong performance in most tasks, often outperforming other models.\n- Supervised methods display strong performance across various tasks, sometimes surpassing language models, especially in RE and EAE tasks.\nD.3 GPT-4 v.s. Others \nWe tend to minimize the GPT-4 calls due to its high price. Thus we utilize 20-/100-shot settings across each dataset to compare GPT-4’s performance with other LLMs. Table  10  reveals that GPT-4 does not outperform other LLMs significantly, except on OntoNotes and MAVEN. However, even on these datasets, GPT-4 still falls behind supervised SLMs by a significant margin. Consequently, the exclu- sion of GPT-4 does not undermine the conclusions drawn from our main experiments, and we omit it from our empirical study. \nE Auxiliary Experiments \nE.1 LLMs struggle on Fine-grained Datasets \nBased on the results shown in Figure  2 , we addi- tionally provide a quantitative analysis to show that LLMs struggle with fine-grained datasets. Under the 5-shot setting, we compare the performance difference of LLMs (ChatGPT) and SLMs (SoTA few-shot models) among different datasets. For each IE task, we observe a clear negative corre- \nThe table presents results for three tasks in natural language processing:\n\n1. **Named Entity Recognition (NER)**:\n   - Datasets: CoNLL, OntoNotes, FewNERD\n   - Metrics: Micro-F1 scores for SLM and LLM.\n   - A difference in F1 scores (∆F1) between LLM and SLM is also listed.\n\n2. **Event Detection**:\n   - Datasets: ACE05, ERE, MAVEN\n   - Metrics: Micro-F1 scores for SLM and LLM.\n   - The difference in F1 scores (∆F1) between LLM and SLM is provided.\n\n3. **Event Argument Extraction**:\n   - Datasets: ACE05, ERE, RAMS\n   - Metrics: Head-F1 scores for SLM and LLM.\n   - The difference in F1 scores (∆F1) between LLM and SLM is noted.\n\nEach section includes the number of entities or events, and the tools used are Small Language Model (SLM) and Large Language Model (LLM). The F1 score differences highlight performance gains or losses using LLM over SLM.\nlation between the label number (row 2) and the performance difference (row 5). In other words, with more label types, LLMs tend to perform rel- atively worse than SLMs. Therefore we conclude that LLMs struggle on fine-grained datasets. \nE.2 Finding Better Instruction \nTo investigate whether LLMs would benefit from complex instructions, we explored six instruction variants from simple to complex. Take NER task as an example, we illustrate them as below. \nInstruction 0 :  [empty] \nInstruction 1 : Identify the entities expressed by each sentence, and locate each entity to words in the sentence. The possible entity types are: [Type_1], [Type_2], ..., [Type_N]. If you do not find any entity in this sentence, just output ‘Answer: No entities found.’ \nInstruction 2 : Identify the entities expressed by each sentence, and locate each entity to words in the sentence. The possible entity types are: \n•  [Type_1]: [Definition 1] •  [Type_2]: [Definition 2] •  ... •  [Type_N]: [Definition N] \nIf you do not find any entity in this sentence, just output ‘Answer: No entities found.’ \nInstruction 3 : Assume you are an entity-instance annotator. Given a sentence, you need to (1) identify the word or phrase about the entity in the sentence, and (2) classify its entity type. The possible entity types are listed as below: [Type_1], [Type_2], . . . , [Type_N]. Please note that your annotation results must follow such format: ”’Answer: ([Type_1] <SEP> identified entity:[Entity_1]), ([Type_2]\n\n <SEP> identified entity:[Entity_2]),\n\n ......”’. If you do not find any entity in this sentence, just output ‘Answer: No entities found.’ "}
{"page": 17, "image_path": "doc_images/2303.08559v2_17.jpg", "ocr_text": "identified_entity:[Entity_1]), ({[Type_2]\n<SEP> identified_entity:[Entity_2]),\nsees »? If you do not find any entity\n\nin this sentence, just output ‘Answer:\nNo entities found.’\n\nInstruction4: Assume you are an\nentity-instance annotator. Your\nobjective is to perform a_ series\nof intricate steps for Named Entity\nRecognition. Firstly, you have to\n\nidentify a particular word or phrase\n\nin the sentence that corresponds. to\nan entity. Following this, classify\nthe entity into one of the potential\n\nentity types. The\ntypes are provided as below: [Type_1],\n[Type_2], ..., [CType_N]. Please note\nthat your annotation results must follow\nsuch format: ‘Answer: ([Type_1] <SEP>\nidentified_entity:[Entity_1]), ({[Type_2]\n<SEP> identified_entity:[Entity_2]),\nIf you do not find any entity\nin this sentence, just output ‘Answer:\nNo entities found.’\n\npotential entity\n\nInstruction5: Assume you are an\nentity-instance annotator. Given a\nsentence, you need to (1) identify the\nword or phrase about the entity in the\nsentence, and (2) classify its entity\ntype. The possible entity types are\nlisted as below:\n\n¢ [Type_1]: [Definition_1]\n\n¢ [Type_2]: [Definition_2]\n\n¢ [Type_N]: [Definition_N]\n\nPlease note that your annotation\nresults must follow such\nformat: “Answer: ({Type_1]  <SEP>\nidentified_entity:[Entity_1]), ({[Type_2]\n<SEP> identified_entity:[Entity_2]),\n\nIf you do not find any entity\nin this sentence, just output ‘Answer:\nNo entities found.’\n\nRegarding these six instructions, we evaluate\ntheir performance of ChatGPT on four 20-shot IE\ntasks. As shown in Table 12, there is no signifi-\ncant correlation between the instruction complexity\n\nTable 12: Fl-scores across six instruction formats. Ex-\nperiments run on 20-shot settings with ChatGPT.\n\nFewNERD | TACREV ACE ACE\n\n(NER) (RE) (ED) (EAE)\n10 57.6(2.1) 49.1(2.4) 44.0(1.4) | 50.9¢0.1)\nIl 58.3(0.5) 49.6(1.2) 42.6(1.0) | 51.5(1.1)\n2 57.7(1.0) 50.0(1.5) 41.8(0.9) | 50.3(.5)\nB 57.6(2.3) 52.3(1.8) 42.9(1.3) | 49.2(2.3)\n14 56.8(0.9) 49.6(2.9) 41.6(1.9) | 49.9.2)\n15 57.8(0.5) 47.2(1.8) 43.1(1.8) | 50.6(2.8)\n\nand LLMs’ performance. Even the prompt with-\nout instruction (10) leads to comparable, if not bet-\nter, results than prompt with complex instructions.\nTherefore, we use simple instruction (I1) in our\nmain experiment.\n\nE.3 Do More Samples in Demos Help?\n\nWe wonder whether longer demos bring more pow-\nerful ICL abilities for LLMs. Thus we investigate\nthe impact of increasing the number of demon-\nstrations on LLMs’ performance in Figure 8. We\nobserve that: (1) The performance of the RE task\nconsistently improves with more demos, indicating\nits potential benefiting from additional annotations.\n(2) The NER and ED tasks reach a stable or de-\ngraded performance with increased demo numbers,\nsuggesting that they are limited even before reach-\ning the maximum input length. (3) Open-source\nLLMs, i.e., LLaMA and Vicuna, have more limited\ncapacities in leveraging demos compared to Ope-\nnAI models, with their performance stagnating or\neven collapsing with only a few (2-4) demos.\n\nE.4_ Finding Better Demo Selection Strategy\n\nThe maximum input length of LLMs usually limits\nhe sentence number in demos even under few-\nshot settings. For each test sentence s, we de-\nmand a demo retriever €(D,s) which selects a\nsubset from D as the sentences in demo. Following\nprevious work, we consider three commonly-used\nstrategies. (1) Random sampling. (2) Sentence-\nembedding (Liu et al., 2022; Su et al., 2022): re-\ntrieving the top-K nearest sentences measured by\nsentence embedding. We compute the embeddings\nby SimCSE-RoBERTa-large (Gao et al., 2021).\n\nE(D, s) = arg-topK,,< p[Sent-embed(s’, s)] (3)\n\n(3) Efficient Prompt Retriever (Rubin et al., 2022):\nretrieving by a neural retriever R trained on D.\n\nE(D, s) = arg-topKyep[Rv(s’,s)] (4)\n", "vlm_text": "\nInstruction 4 : Assume you are an \nentity-instance annotator. Your objective is to perform a series of intricate steps for Named Entity Recognition. Firstly, you have to identify a particular word or phrase in the sentence that corresponds to an entity. Following this, classify the entity into one of the potential entity types. The potential entity types are provided as below: [Type_1], [Type_2], . . . , [Type_N]. Please note that your annotation results must follow such format: ‘Answer: ([Type_1] <SEP> identified entity:[Entity_1]), ([Type_2]\n\n <SEP> identified entity:[Entity_2]),\n\n ......’. If you do not find any entity in this sentence, just output ‘Answer: No entities found.’ \nInstruction 5 : Assume you are an entity-instance annotator. Given a sentence, you need to (1) identify the word or phrase about the entity in the sentence, and (2) classify its entity type. The possible entity types are listed as below: \n•  [Type_1]: [Definition 1] •  [Type_2]: [Definition 2] •  ... •  [Type_N]: [Definition N] \nPlease note that your annotation results must follow such format: ‘Answer: ([Type_1] <SEP> identified entity:[Entity_1]), ([Type_2]\n\n <SEP> identified entity:[Entity_2]),\n\n ......’. If you do not find any entity in this sentence, just output ‘Answer: No entities found.’ \nRegarding these six instructions, we evaluate their performance of ChatGPT on four 20-shot IE tasks. As shown in Table  12 , there is no signifi- cant correlation between the instruction complexity \nThe table presents performance results across different datasets and tasks. Here's a brief breakdown:\n\n- **Tasks/Datasets**: \n  - FewNERD (NER)\n  - TACREV (RE)\n  - ACE (ED)\n  - ACE (EAE)\n\n- **Rows** (labeled I0 to I5): Represent different configurations or models.\n\n- **Values**:\n  - Each cell contains a primary value (e.g., 57.6) with a value in parentheses indicating a measure of variability or error (e.g., 2.1).\n\nThese values likely represent performance metrics such as accuracy, F1-score, etc., accompanied by standard deviations or confidence intervals.\nand LLMs’ performance. Even the prompt with- out instruction (I0) leads to comparable, if not bet- ter, results than prompt with complex instructions. Therefore, we use simple instruction (I1) in our main experiment. \nE.3 Do More Samples in Demos Help? \nWe wonder whether longer demos bring more pow- erful ICL abilities for LLMs. Thus we investigate the impact of increasing the number of demon- strations on LLMs’ performance in Figure  8 . We observe that: (1) The performance of the RE task consistently improves with more demos, indicating its potential benefiting from additional annotations. (2) The NER and ED tasks reach a stable or de- graded performance with increased demo numbers, suggesting that they are limited even before reach- ing the maximum input length. (3) Open-source LLMs,  i.e.,  LLaMA and Vicuna, have more limited capacities in leveraging demos compared to Ope- nAI models, with their performance stagnating or even collapsing with only a few (2-4) demos. \nE.4 Finding Better Demo Selection Strategy \nThe maximum input length of LLMs usually limits the sentence number in demos even under few- shot settings. For each test sentence    $s$  , we de- mand a de  retriever    ${\\mathcal{E}}(D,s)$   which selects a subset from  D  as the sentences in demo. Following previous work, we consider three commonly-used strategies. (1) Random sampling. (2) Sentence- embedding ( Liu et al. ,  2022 ;  Su et al. ,  2022 ): re- trieving the top-K nearest sentences measured by sentence embedding. We compute the embeddings by  SimCSE-RoBERTa-large  ( Gao et al. ,  2021 ). \n\n$$\n\\begin{array}{r}{\\mathcal{E}(D,s)=\\mathrm{arg-top}\\mathbf{K}_{s^{\\prime}\\in D}[\\mathrm{Sent-embed}(s^{\\prime},s)]}\\end{array}\n$$\n \n(3) Efficient Prompt Retriever ( Rubin et al. ,  2022 ): retrieving by a neural retriever    $R$   trained on    $D$  . \n\n$$\n\\mathcal{E}(D,s)=\\mathrm{arg-top}{\\bf K}_{s^{\\prime}\\in D}[R_{D}(s^{\\prime},s)]\n$$\n "}
{"page": 18, "image_path": "doc_images/2303.08559v2_18.jpg", "ocr_text": "ChatGPT -e- CODEX\n\n60 60 48\no 56 o 56 o 44\n8 52 8 52 8\n948 9 48 9 40\nL 44 L 44 L 36\n\n40 40\n\n36 36 32\n\n4 8 16 32 64 96 8 16 32 64 8 16 32 64\nFewNERD (NER) TACREV (RE) ACE05 (ED)\n\n(a) OpenAI LLMs\n\n—— LLaMA (13B) —— Vicuna (13B)\n\n28 28\n@ 24 @ 24\n= =\n920 920\n16 16\n12 12\n\n8 8\n\n4 4\n\n2 16\n\n4 8\nFewNERD (NER)\n\n2\n920\n16\nrt\n\n4 8\nTACREV (RE)\n\n16 16\n\n4 8\nACE0S5 (ED)\n\n(b) Open-source LLMs\n\nFigure 8: Relationship between demo number and Fl-score among three datasets. Note that the x-axis in each\nsubfigure represents the number of demos (not the shot value A’) during ICL. We adopt sentence embedding as the\ndemo selection strategy and text prompt in this experiment.\n\nFor each test sentence s, we pre-retrieve M/ sim-\nilar sentences D = {(s',y/)}¥4, C D. Then\nwe score each sentence in D by their likelihoods\nPc(f(y))|f(s,)) where f denotes the prompt for-\n\nmat adopted and £ the scoring LM. We randomly\n(pos)\n\na\n\nfrom the top- p sen-\ntences and hard negative samples s’ (hard-neg)\nthe bottom-/’p ones. Then we train Rp by in-\nbatch contrastive learning (Chen et al., 2020). For\neach sentence si within the batch, there are | posi-\n\n. OS,\ntive sentences sl )\n\nhard-\n{s!( “ mee) B U {sj }Ra- Here we adopt M as\n40, Kp as 5, f as text prompt, the batch size B as\n128, and the scoring LM £ as FLAN-T5-x1.\n\nselect positive samples 5’\nfrom\n\nand 2B —1 negative sentences\n\nTable 13: Fl-scores on three demo-selection strategies.\nExperiments run on 20-shot settings with ChatGPT.\n\nFewNERD TACREV ACE\n\n(NER) (RE) (ED)\nRandom Sampling 53.2(0.4) 43.0(3.3) 38.0(2.5)\nSentence Embedding 57.6(2.3) 49.6(1.2) 42.9(1.3)\nEfficient Prompt Retriever] 57.2(0.6) 48.0(0.8) 43.5(1.4)\n\nTable 13 demonstrates the Fl-score performance\non different selection strategies. We find that both\nthe sentence embedding and EPR surpass random\nsampling by a large margin. Given the simplicity\nof the sentence embedding, we adopt it, rather than\nEPR, as our selection strategy in main experiment.\n\nTable 14: Fl-scores across three prompt formats. Ex-\nperiments run on 20-shot settings with ChatGPT.\n\nFewNERD TACREV ACE ACE\n(NER) (RE) (ED) (EAE)\n\n57.6(2.3) 49.6(1.2) 42.9(1.3) 51.5(1.1)\n53.2(0.9) 50.2(1.8) 44.3(2.0) 47.3.5)\n\nText\nCode\n\nE.5 Finding Better Prompt Format\n\nPrevious studies on LLMs for few-shot IE tasks\nhave explored different prompt formats and high-\nlighted the importance of selecting an appropri-\nate format for achieving competitive performance.\nTherefore, we investigate two commonly-used vari-\nants in previous work: (1) Text prompt as shown in\nFigure 1. (2) Code prompt: We follow Wang et al.\n(2023b); Li et al. (2023) and recast the output of IE\ntasks in the form of code. See more details about\nthis format in their original papers.\n\nTable 14 shows comparable performance across\nall formats. Based on simplicity, we choose the\ntext prompt for our main experiment.\n\nF Case Study\n\nF1 Hard Samples\n\nTable 15 showcases some hard examples which\nbenefits from our LLM reranking. In accordance\nwith our intuition, we observe that the LLM\n", "vlm_text": "The image consists of two sets of graphs comparing F1 scores for language models. \n\n- **Top Row (a) OpenAI LLMs:** Shows graphs for ChatGPT and Codex models.\n  - **FewNERD (NER):** F1 score increases for Codex consistently, while ChatGPT scores fluctuate.\n  - **TACREV (RE):** Both ChatGPT and Codex show improving scores as data increases.\n  - **ACE05 (ED):** Codex maintains a higher score than ChatGPT, with both showing score increments.\n\n- **Bottom Row (b) Open-source LLMs:** Shows graphs for LLaMA and Vicuna models.\n  - **FewNERD (NER):** Initial scores are higher for Vicuna, but both models' scores drop as data increases.\n  - **TACREV (RE):** Both models improve with more data.\n  - **ACE05 (ED):** LLaMA maintains a relatively stable higher score than Vicuna although both decrease with more data.\n\nEach graph measures F1 scores for different natural language processing tasks, indicating performance changes as the amount of data increases.\nFigure 8: Relationship between demo number and F1-score among three datasets. Note that the   $\\mathbf{X}$  -axis in each subfigure represents the number of demos (not the shot value    $K$  ) during ICL. We adopt sentence embedding as the demo selection strategy and text prompt in this experiment. \nFor each test sentence    $s$  , we pre-retrieve    $M$   sim- ilar sentences  $\\bar{D}~=~\\{(s_{i}^{\\prime},y_{i}^{\\prime})\\}_{i=1}^{M}~\\subset~D$    . Then we score each sentence in    $\\bar{D}$   by their likelihoods  $P_{\\mathcal{L}}(f(y_{i}^{\\prime})|f(s_{i}^{\\prime}))$  |  where    $f$   denotes the prompt for- mat adopted and  $\\mathcal{L}$   the scoring LM. We randomly select positive samples    $s_{\\,\\,i}^{\\prime{\\mathrm{(pos)}}}$  from the top-  $K_{D}$   sen- tences and hard negative samples  $s_{\\,\\,i}^{\\prime{\\mathrm{(hard-neg)}}}$  from the bottom-  $\\mathit{K}_{D}$   ones. Then we train    $R_{D}$   by in- batch contrastive learning ( Chen et al. ,  2020 ). For each sentence  $s_{i}^{\\prime}$    within the batch, there are 1 posi- tive sentences    $s_{\\,\\,i}^{\\prime{\\mathrm{(pos)}}}$  and    $2B\\!-\\!1$   negative sentences  $\\{s^{\\prime}{}_{j}^{(\\mathrm{hard-neg})}\\}_{j=1}^{B}\\cup\\{s^{\\prime}{}_{j}\\}_{j\\neq i}^{B}$    . Here we adopt    $M$  as 40,    $K_{D}$   as 5,  $f$   as text prompt, the batch size  B  as 128, and the scoring LM    $\\mathcal{L}$   as  $\\mathsf{F L A N-T5-x1}$  . \nThe table presents a comparison of three methods across three tasks: FewNERD (NER), TACREV (RE), and ACE (ED). The methods compared are:\n\n1. **Random Sampling**\n   - FewNERD: 53.2 (±0.4)\n   - TACREV: 43.0 (±3.3)\n   - ACE: 38.0 (±1.5)\n\n2. **Sentence Embedding**\n   - FewNERD: 57.6 (±2.3)\n   - TACREV: 49.6 (±1.2)\n   - ACE: 42.9 (±1.3)\n\n3. **Efficient Prompt Retriever**\n   - FewNERD: 57.2 (±0.6)\n   - TACREV: 48.0 (±0.8)\n   - ACE: 43.5 (±1.4)\n\nThe numbers indicate performance scores with standard deviations in parentheses.\nTable  13  demonstrates the F1-score performance on different selection strategies. We find that both the sentence embedding and EPR surpass random sampling by a large margin. Given the simplicity of the sentence embedding, we adopt it, rather than EPR, as our selection strategy in main experiment. \nThe table presents results for different tasks across two conditions (Text and Code):\n\n- **Tasks / Datasets:**\n  - FewNERD (NER)\n  - TACREV (RE)\n  - ACE (ED)\n  - ACE (EAE)\n\n- **Text Results:**\n  - FewNERD: 57.6 (±2.3)\n  - TACREV: 49.6 (±1.2)\n  - ACE (ED): 42.9 (±1.3)\n  - ACE (EAE): 51.5 (±1.1)\n\n- **Code Results:**\n  - FewNERD: 53.2 (±0.9)\n  - TACREV: 50.2 (±1.8)\n  - ACE (ED): 44.3 (±2.0)\n  - ACE (EAE): 47.3 (±1.5)\n\nThe numbers in parentheses indicate the standard deviation.\nE.5 Finding Better Prompt Format \nPrevious studies on LLMs for few-shot IE tasks have explored different prompt formats and high- lighted the importance of selecting an appropri- ate format for achieving competitive performance. Therefore, we investigate two commonly-used vari- ants in previous work: (1) Text prompt as shown in Figure  1 . (2) Code prompt: We follow  Wang et al. ( 2023b );  Li et al.  ( 2023 ) and recast the output of IE tasks in the form of code. See more details about this format in their original papers. \nTable  14  shows comparable performance across all formats. Based on simplicity, we choose the text prompt for our main experiment. \nF Case Study \nF.1 Hard Samples \nTable  15  showcases some  hard  examples which benefits from our LLM reranking. In accordance with our intuition, we observe that the LLM rerankers correct two kinds of erroneous predic- tions made by LLMs. (1) The lack of external knowledge, such as the first ( Trip to l emus is a fig- ure in Greek mythology ) and third examples ( Mi- nas Gerais is a state instead of city ). (2) Limited reasoning abilities, such as the second ( His wife’s children are his children ) and the fourth ( The word \"fought\" in this sentence does not involve any phys- ical violence ) examples. "}
{"page": 19, "image_path": "doc_images/2303.08559v2_19.jpg", "ocr_text": "rerankers correct two kinds of erroneous predic-\ntions made by LLMs. (1) The lack of external\nknowledge, such as the first (Triptolemus is a fig-\nure in Greek mythology) and third examples (Mi-\nnas Gerais is a state instead of city). (2) Limited\nreasoning abilities, such as the second (His wife’s\nchildren are his children) and the fourth (The word\n\"fought\" in this sentence does not involve any phys-\nical violence) examples.\n\nF2_ Easy Samples\n\nAs discussed in Section 4.3, we attribute the in-\nferior performance of LLMs on easy samples to\ntheir false-positive predictions for negative samples.\nUpon examining the incorrect cases, we speculate\nthat two main reasons are responsible: (1) LLM\nhallucination and (2) span boundary mismatch. We\nprovide details on them with specific cases below.\n\nLLM Hallucination. The most distinctive mis-\ntakes made by LLMs. Without specific fine-tuning\non IE tasks, LLMs (with in-context learning ap-\nproaches) sometimes hallucinate and provide mean-\ningless, even confusing explanations or answers.\nFor instance, LLMs might predict a verb, preposi-\ntion or even special symbols as an entity.\n\nCase 1: LLMs predict the word die as entity\nperson-other (Ground truth: None)\n\nSentence: Flavius Rumoridus (died 5th cen-\ntury AD) was a Roman soldier who was ap-\npointed consul in AD 403 in the Western Ro-\nman Empire at the same time as the eastern\nemperor Theodosius II served in the same\ncapacity in the East.\n\nCandidate Choices:\n\n(a) died do/does not belong to any known\nentities.\n\n(b) died is a kind of geopolitical entity.\n\n(c) died is a person, but not affiliated with\nfollowing professions: actor, artist, ath-\nlete, author, director, politician, scholar,\nsoldier.\n\nAnalysis: The sentence provides information\nabout Flavius Rumoridus, who was a Roman\nsoldier and appointed consul in AD 403. The\nsentence also mentions that he died in the Sth\ncentury AD, which is a statement about his\ndeath. Therefore, the correct answer is (c)\n\nbecause the sentence does not provide any\ninformation about Flavius Rumoridus being\naffiliated with any of the professions listed in\noption (c).\n\nCorrect Answer: (c)\n\nCase 2: LLMs predict the word of as entity\norganization-other (Ground truth: None)\n\nSentence: During these years he studied at\nthe South Kensington Schools of the Royal\nCollege of Art.\n\nCandidate Choices:\n\n(a) of do/does not belong to any known en-\ntities.\n\n(b) of is an educational institute.\n\n(c) of pertains to an organization that does\nnot fall under the categories of com-\npany, educational institution, govern-\nment, media, political party, religion,\nsports league, sports team, band or mu-\nsical group.\n\nAnalysis: In the given sentence, of is a prepo-\nsition that indicates the relationship between\nSouth Kensington Schools and Royal College\nof Art. None of the candidate choices directly\naddress the correct relationship between these\nentities or state that they are not applicable.\nTherefore, the correct answer is (c): of does\nnot pertain to any known entities mentioned\nin the choices provided.\n\nCorrect Answer: (c)\n\nSpan boundary mismatch. Different from\nsentence-level approaches which take one or mul-\ntiple sentences as an unit and assign each word’s\nlabel globally, LLMs under our filter-then-rerank\n", "vlm_text": "\nF.2 Easy Samples \nAs discussed in Section  4.3 , we attribute the in- ferior performance of LLMs on easy samples to their false-positive predictions for negative samples. Upon examining the incorrect cases, we speculate that two main reasons are responsible: (1) LLM hallucination and (2) span boundary mismatch. We provide details on them with specific cases below. \nLLM Hallucination . The most distinctive mis- takes made by LLMs. Without specific fine-tuning on IE tasks, LLMs (with in-context learning ap- proaches) sometimes hallucinate and provide mean- ingless, even confusing explanations or answers. For instance, LLMs might predict a verb, preposi- tion or even special symbols as an entity. \nCase 1 : LLMs predict the word  die  as entity person-other  (Ground truth:  None ) \nSentence:  Flavius Rumoridus (died 5th cen- tury AD) was a Roman soldier who was ap- pointed consul in AD 403 in the Western Ro- man Empire at the same time as the eastern emperor Theodosius II served in the same capacity in the East. \nCandidate Choices: \n(a)  died do/does not belong to any known entities. (b) died is a kind of geopolitical entity. (c)  died is a person, but not affiliated with following professions: actor, artist, ath- lete, author, director, politician, scholar, soldier. \nAnalysis:  The sentence provides information about Flavius Rumoridus, who was a Roman soldier and appointed consul in AD 403. The sentence also mentions that he died in the 5th century AD, which is a statement about his death. Therefore, the correct answer is (c) because the sentence does not provide any information about Flavius Rumoridus being affiliated with any of the professions listed in option (c). \nCorrect Answer:  (c) \nCase 2 : LLMs predict the word  of  as entity organization-other  (Ground truth:  None ) \nSentence:  During these years he studied at the South Kensington Schools of the Royal College of Art. \nCandidate Choices: \n(a)  of do/does not belong to any known en- tities. \n(b) of is an educational institute. \n(c)  of pertains to an organization that does not fall under the categories of com- pany, educational institution, govern- ment, media, political party, religion, sports league, sports team, band or mu- sical group. \nAnalysis:  In the given sentence,  of  is a prepo- sition that indicates the relationship between South Kensington Schools  and  Royal College of Art . None of the candidate choices directly address the correct relationship between these entities or state that they are not applicable. Therefore, the correct answer is (c):  of  does not pertain to any known entities mentioned in the choices provided. \nCorrect Answer:  (c) \nSpan boundary mismatch . Different from sentence-level approaches which take one or mul- tiple sentences as an unit and assign each word’s label globally, LLMs under our filter-then-rerank paradigm adopt sample-level approaches. In other words, LLMs only consider each word/span candi- date independently. The sample-level approaches enable LLMs to rerank candidates with the help of multi-choice question prompt. However, they also aggravate the span boundary mismatching prob- lem in NER and ED tasks. We empirically find easy samples suffering much more from boundary mismatching problem than hard samples. "}
{"page": 20, "image_path": "doc_images/2303.08559v2_20.jpg", "ocr_text": "paradigm adopt sample-level approaches. In other\nwords, LLMs only consider each word/span candi-\ndate independently. The sample-level approaches\nenable LLMs to rerank candidates with the help of\nmulti-choice question prompt. However, they also\naggravate the span boundary mismatching prob-\nlem in NER and ED tasks. We empirically find\neasy samples suffering much more from boundary\nmismatching problem than hard samples.\n\nCase 3: LLMs predict Baron, a subspan of Baron\nCarl Hurleman, as the entity person-other (The\nground-truth entity type of Baron is None. And the\nground-truth entity type of Baron Carl Hurleman\nis person-artist/author).\n\nSentence: Additions in the 1740s were by\nBaron Carl Hurleman (1700 - 1753).\n\nCandidate Choices:\n\n(a) Baron do/does not belong to any known\nentities.\n\n(b) Baron is a person, but not affiliated with\nfollowing professions: actor, artist, ath-\nlete, author, director, politician, scholar,\nsoldier.\n\n(c) Baron is a kind of geopolitical entity\n\nAnalysis: In the given sentence, Baron is a\ntitle given to a person named Carl Hurleman,\nwho is mentioned as the one who made ad-\n\nditions in the 1740s. The sentence does not\nprovide any information about connections\nto geopolitical entities or any other known\nentities mentioned in the choices provided.\nTherefore, the correct answer is (b): Baron is\na person, but not affiliated with following pro-\nfessions: actor, artist, athlete, author, director,\npolitician, scholar, soldier.\n\nCorrect Answer: (b)\n\nCase 4: LLMs predict 65th, a subspan of\n65th British Academy Film Awards, as the en-\ntity other-awards (The ground-truth entity type\nof 65th is None. And the ground-truth entity\ntype of 65th British Academy Film Awards is\nother-awards).\n\nSentence: Ralph Fiennes was nominated for\nthe BAFTA Award for Outstanding Debut by\na British Writer, Director or Producer at the\n65th British Academy Film Awards.\n\nCandidate Choices:\n\n(a) 65th do/does not belong to any known\nentities.\n\n(b) 65th is a kind of award.\n\n(c) 65th refers to some event except attack,\nelection, natural disaster, protest, revolu-\n\ntion and sports.\n\nAnalysis: The sentence mentions the 65th\nBritish Academy Film Awards, which is an\nevent that recognizes outstanding achieve-\nments in the film industry. Therefore, option\n(c) is incorrect as it excludes events like award\nceremonies. Option (a) is also incorrect as\nthe number 65th is associated with the event.\nHence, the correct answer is (b) as 65th refers\nto the specific edition of the BAFTA Awards.\n\nCorrect Answer: (b)\n\nG_ Details on Adaptive Filter-then-rerank\nParadigm\n\nG.1 Demonstration Examples\n\nWe convert few-shot IE tasks to multiple-choice\nquestions in our filter-then-rerank paradigm. We\nshow 4 examples used in demonstrations for FewN-\nERD dataset in Table 16, for TACREV dataset in\nTable 17, and for ACE05 datasets in Table 18.\n\nG.2 Template\n\nIn our filter-then-rerank paradigm, we utilize tem-\nplates converting candidate labels to question op-\ntions. We list the template for FEewNERD dataset\nin Table 19, for TACREV dataset in Table 20, and\nfor ACEOS datasets in Table 21.\n", "vlm_text": "\nCase 3 : LLMs predict  Baron , a subspan of  Baron Carl Hurleman , as the entity  person-other  (The ground-truth entity type of  Baron  is  None . And the ground-truth entity type of  Baron Carl Hurleman is  person-artist/author ). \nSentence:  Additions in the 1740s were by Baron Carl Hurleman (1700 - 1753). \nCandidate Choices: \n(a)  Baron do/does not belong to any known entities. (b)  Baron is a person, but not affiliated with following professions: actor, artist, ath- lete, author, director, politician, scholar, soldier. (c) Baron is a kind of geopolitical entity \nAnalysis:  In the given sentence,  Baron  is a title given to a person named Carl Hurleman, who is mentioned as the one who made ad- ditions in the 1740s. The sentence does not provide any information about connections to geopolitical entities or any other known entities mentioned in the choices provided. Therefore, the correct answer is (b): Baron is a person, but not affiliated with following pro- fessions: actor, artist, athlete, author, director, politician, scholar, soldier. \nCorrect Answer:  (b) \nSentence:  Ralph Fiennes was nominated for the BAFTA Award for Outstanding Debut by a British Writer, Director or Producer at the 65th British Academy Film Awards. \nCandidate Choices: \n(a)  65th do/does not belong to any known entities. (b) 65th is a kind of award. (c)  65th refers to some event except attack, election, natural disaster, protest, revolu- tion and sports. \nAnalysis:  The sentence mentions the    $65t h$  British Academy Film Awards , which is an event that recognizes outstanding achieve- ments in the film industry. Therefore, option (c) is incorrect as it excludes events like award ceremonies. Option (a) is also incorrect as the number    $65t h$   is associated with the event. Hence, the correct answer is (b) as    $65t h$   refers to the specific edition of the BAFTA Awards. \nCorrect Answer:  (b) \nG Details on Adaptive  Filter-then-rerank Paradigm \nG.1 Demonstration Examples \nWe convert few-shot IE tasks to multiple-choice questions in our  filter-then-rerank  paradigm. We show 4 examples used in demonstrations for FewN- ERD dataset in Table  16 , for TACREV dataset in Table  17 , and for ACE05 datasets in Table  18 . \nG.2 Template \nIn our  filter-then-rerank  paradigm, we utilize tem- plates converting candidate labels to question op- tions. We list the template for FewNERD dataset in Table  19 , for TACREV dataset in Table  20 , and for ACE05 datasets in Table  21 . \nCase 4 : LLMs predict    $65t h$  , a subspan of 65th British Academy Film Awards , as the en- tity  other-awards  (The ground-truth entity type of    $65t h$   is  None . And the ground-truth entity type of  65th British Academy Film Awards  is other-awards ). "}
{"page": 21, "image_path": "doc_images/2303.08559v2_21.jpg", "ocr_text": "Table 15: Examples of the samples corrected by LLM reranking. We sample four examples from NER, RE and ED\ntasks, respectively. Sentences: The sentences in which samples locate. We color the samples (entities or trigger\nwords) to be identified. Before: The prediction before LLM reranking. Based on SLM-based methods. After: The\nreranked prediction using LLMs. Rationales: LLM-generated Explanations.\n\nSentences (entity/event) Before After Rationales\n\nNamed Entity Recognition\nAccording to it, King Eleusis and _ person-other other-god According to the context, Triptolemus is a\nCothonea (Cyntinia), parents of Trip- x v figure in Greek mythology, the son of King\n\ntolemus, are visited by Demeter, who\nrears their son, feeding him divine milk\nby day and placing him into the fire at\nnight, which makes Triptolemus grow\nfaster than mortal children normally do.\n\nEleusis and Cothonea (Cyntinia). He was a\ndemigod who was sent by Demeter to teach\nmankind the art of agriculture. So Triptole-\nmus is a god in some legend/religious story.\n\nRelation Extraction\nSurvivors include his wife, Sandra; four None per:children The sentence states ’Survivors include his\nsons, Jeff, James, Douglas and Harris; a x ov wife, Sandra; four sons, Jeff, James, Douglas\n\ndaughter, Leslie; his mother, Sally; and\ntwo brothers, Guy and Paul.\n\nEliza Samudio, Bruno ’s ex-girlfriend\nand his 4-month-old baby boy’s mother,\ndisappeared from her hometown of Belo\nHorizonte, Minas Gerais.\n\nand Harris; a daughter, Leslie; his mother,\nSally; and two brothers, Guy and Paul.’ This\nindicates his’ is the father of Jeff, James,\nDouglas and Harris, and the husband of San-\ndra. So his is the parent of Jeff.\n\nThe sentence states that “Eliza Samudio,\nBruno’s ex-girlfriend and his 4-month-old\nbaby boy’s mother, disappeared from her\nhometown of Belo Horizonte, Minas Gerais.’\nThis indicates that Eliza Samudio is from the\ncity of Belo Horizonte, which is located in\nthe state of Minas Gerais. So Eliza Samudio\nlives in the state or province Minas Gerais.\n\nEvent Detection\nWe have fought every institution, every  Conflict.Attack None In the given sentence, the word fought is\nlaw firm ... it’s cost us a fortune but now x v used to describe the action of the speaker\nwe ‘re going on a massive expansion. fighting against various institutions and law\n\nfirms. This does not involve any physical\nviolence or court proceedings, so the word\nfought does not trigger any known event.\n", "vlm_text": "This table contains information about text analysis, specifically dealing with named entity recognition, relation extraction, and event detection. Here's a breakdown:\n\n1. **Named Entity Recognition**\n   - **Sentence:** Mentions the story of Triptolemus and Greek mythology.\n   - **Before:** Recognized as \"person-other.\"\n   - **After:** Corrected to \"other-god\" based on context, as Triptolemus is a mythological figure.\n   - **Rationale:** Triptolemus is associated with Demeter in mythology, making him a god-like figure.\n\n2. **Relation Extraction**\n   - **Example 1:**\n     - **Sentence:** Refers to a family structure with names like Jeff and his relatives.\n     - **Before:** No relation recognized.\n     - **After:** Recognizes \"per:children\" relationship.\n     - **Rationale:** Sentence mentions family members, indicating Jeff is a child of the subject.\n   \n   - **Example 2:**\n     - **Sentence:** Talks about Eliza Samudio and her connection to Belo Horizonte, Minas Gerais.\n     - **Before:** Incorrectly labeled as \"per:cities_of_residence.\"\n     - **After:** Corrected to \"per:states_of_residence.\"\n     - **Rationale:** Indicates Eliza's residence in the state of Minas Gerais.\n\n3. **Event Detection**\n   - **Sentence:** Discussion about fighting institutions and law firms.\n   - **Before:** Incorrectly labeled as \"Conflict.Attack.\"\n   - **After:** No event detected.\n   - **Rationale:** The use of \"fought\" is metaphorical, discussing legal and institutional battles, not physical conflict.\n\nThe table uses checkmarks and crosses to indicate post-correction accuracy and includes explanations or rationales for each decision."}
{"page": 22, "image_path": "doc_images/2303.08559v2_22.jpg", "ocr_text": "Table 16: Demo examples used in FEewNERD dataset. We color the entity in blue.\n\nInstruct: Read following sentences and identify what is the entity type of 392 quoted by <t>.\n\nSentence: Powell v. Texas , <t> 392 <t> U.S. 514 ( 1968 ) , was a United States Supreme Court case that ruled that a\nTexas statute criminalizing public intoxication did not violate the Eighth Amendment protection against cruel and unusual\npunishment.\n\n(a) 392 is a legal document, a term or a convention in legal sense.\n\n(b) 392 does not belong to any known entities.\n\n(c) 392 refers to a protest, uprising or revolution event\n\n(d) 392 refers to a government or governmental agency\n\nAnalysis: In the context you provided, 392 refers to the volume number in the United States Reports where the Supreme\nCourt’s decision in Powell v. Texas can be found. However, 392 itself does not refer to a legal document. So 392 do/does\nnot belong to any known entities.\n\nAnswer: (b)\n\nInstruct: Read following sentences and identify what is the entity type of The New Yorker quoted by <t>.\n\nSentence: In 2004 Gourevitch was assigned to cover the 2004 U.S. presidential election for \" <t> The New Yorker <t>\".\n(a) The New Yorker does not belong to any known entities.\n\n(b) The New Yorker is a broadcast program.\n\n(c) The New Yorker is a kind of written art.\n\n(d) The New Yorker is a media/newspaper organization.\n\nAnalysis: The New Yorker is a well-known American magazine that has been published since 1925, and is primarily\nknown for its long-form journalism, commentary, and satire. It has a reputation for publishing high-quality writing on a\nwide variety of topics, including politics, culture, and the arts. So The New Yorker is a media/newspaper organization.\nAnswer: (d)\n\nInstruct: Read following sentence and identify what is the entity type of St. quoted by <t>.\n\nSentence: The May 1980 eruption of Mount <t> St. <t> Helens in the state of Washington seriously affected both 47th Air\nDivision and 92d Bombardment Wing operations at Fairchild AFB , resulting in dispersal of Fairchild ’s B-52 and KC-135\naircraft to various bases while around-the-clock shifts removed the volcanic ash from facilities within the base perimeter. ”\n(a) St. does not belong to any known entities.\n\n(b) St. is a natural disaster event.\n\n(c) St. is a geographic position about mountain.\n\nAnalysis: According to the context, St. is an abbreviation of Saint, used in the name of Mount St. Helens, which is an\nactive volcano in the state of Washington. However, St. itself does not refer to anything. So St. do/does not belong to any\nknown entities.\n\nAnswer: (a)\n\nInstruct: Read following sentence and identify what is the entity type of Ridzuan quoted by <t>.\n\nSentence: <t> Ridzuan <t> was promoted to Harimau Muda A for 2014 season .\n\n(a) Ridzuan does not belong to any known entities.\n\n(b) Ridzuan is a person, but not affiliated with following professions: actor, artist, author, director, politician, scholar,\nsoldier.\n\n(c) Ridzuan is an athlete.\n\nAnalysis: The mention of ’Harimau Muda A’ indicates that it is a sports-related context, and ’promoted’ implies a\nprogression or advancement within the sports team. So Ridzuan is an athlete.\n\nAnswer: (c)\n\n", "vlm_text": "The table contains several sections, each with instructions to identify the entity type of a quoted term in a sentence. The sections include:\n\n1. **Entity:** `392`\n   - **Sentence:** Discusses a U.S. Supreme Court case (Powell v. Texas).\n   - **Options:** Legal document, known entity, protest/revolution event, government agency.\n   - **Answer:** b) 392 does not belong to any known entities.\n\n2. **Entity:** `The New Yorker`\n   - **Sentence:** Mentions 2004 U.S. presidential election coverage.\n   - **Options:** Not a known entity, broadcast program, kind of art, media/newspaper organization.\n   - **Answer:** d) The New Yorker is a media/newspaper organization.\n\n3. **Entity:** `St.`\n   - **Sentence:** Discusses the 1980 eruption of Mount St. Helens.\n   - **Options:** Not a known entity, natural disaster event, geographic position.\n   - **Answer:** a) St. does not belong to any known entities.\n\n4. **Entity:** `Ridzuan`\n   - **Sentence:** Mentions the promotion to Harimau Muda A for 2014 season.\n   - **Options:** Not a known entity, person with various professions, athlete.\n   - **Answer:** c) Ridzuan is an athlete."}
{"page": 23, "image_path": "doc_images/2303.08559v2_23.jpg", "ocr_text": "Table 17: Demo examples used in TACREV dataset. We color the subject and object entities in blue.\n\nInstruct: Read the sentence and determine the relation between she and lawyer quoted by <t>.\n\nSentence: The <t> lawyer <t> denied Italian news reports that she wept while addressing the court, but said Knox was\nupset as <t> she <t> recounted “ the pressure, the aggressiveness of the police who called her a liar . ”\n\n(a) she is the other family member of lawyer\n\n(b) she is a lawyer\n\n(c) she has no known relations to lawyer\n\nAnalysis: In the sentence, the word ’she’ refers to someone who was upset while recounting certain events in court.\nThe word lawyer’ refers to someone who denied a news report about that same person weeping in court. There is no\ninformation in the sentence to indicate that the two individuals are related in any way. So she has no known relations to\nlawyer.\nAnswer: (c)\n\nInstruct: Read the sentence and determine the relation between MEF and Myanmar Equestrian Federation quoted by <t>.\nSentence: YANGON , Dec. 27 -LRB- Xinhua -RRB- — Myanmar will hold a horse race in Yangon to commemorate the\ncountry ’s 63rd Anniversary Independence Day , the <t> Myanmar Equestrian Federation <t> -LRB- <t> MEF <t> -RRB-\nconfirmed to Xinhua on Monday.\n\n(a) MEF is also known as Myanmar Equestrian Federation\n\n(b) MEF has political affiliation with Myanmar Equestrian Federation\n\n(c) MEF has no known relations to Myanmar Equestrian Federation\n\nAnalysis: The symbols -LRB- and -RRB- in the sentence stand for left and right round brackets and are used to enclose\nthe abbreviation MEF’ to indicate that it is a replacement for the longer name ’Myanmar Equestrian Federation. So MEF\nis also known as Myanmar Equestrian Federation.\n\nAnswer: (a)\n\nInstruct: Read the sentence and determine the relation between Douglas Flint and chairman quoted by <t>.\n\nSentence: At the same time , Chief Financial Officer <t> Douglas Flint <t> will become <t> chairman <t> , succeeding\nStephen Green who is leaving to take a government job.\n\n(a) Douglas Flint has no known relations to chairman\n\n(b) Douglas Flint is a chairman\n\n(c) Douglas Flint is the employee of chairman\n\nAnalysis: The sentence s hat Chief Financial Officer Douglas Flint Douglas Flint will succeed Stephen Green as a\nchairman. So Douglas Flint is a chairman.\n\nAnswer: (b)\n\nInstruct: Read the sentence and determine the relation between FAA and U.S. quoted by <t>.\n\nSentence: On its Web site , the <t> U.S. <t> <t> FAA <t> says the Category 2 rating means the country lacks the laws or\nregulations that are needed for the certification and oversight of air carriers , according to minimum international standards.\n(a) FAA is also known as U.S.\n\n(b) FAA has no known relations to U.S.\n\n(c) FAA has a headquarter in the country U.S.\n\nAnalysis: The sentence states that the FAA says the Category 2 rating means the country lacks the laws or regulations\nneeded for the certification and oversight of air carriers, indicating that the FAA is responsible for overseeing aviation\nregulations in the country. Actually the FAA (Federal Aviation Administration) is a U.S. government agency responsible\nfor regulating and overseeing civil aviation in the United States, and it has its headquarters in Washington, D.C.. So FAA\nhas a headquarter in the country U.S..\n\nAnswer: (c)\n\n", "vlm_text": "Instruct : Read the sentence and determine the relation between  she  and  lawyer  quoted by <t>. Sentence : The  ${<}\\mathrm{t}{>}$   lawyer  ${<}\\mathrm{t}{>}$   denied Italian news reports that she wept while addressing the court, but said Knox was upset as   ${<}\\mathrm{t}{>}$   she   ${<}\\mathrm{t}{>}$   recounted “ the pressure, the aggressiveness of the police who called her a liar . ”\n\n (a) she is the other family member of lawyer\n\n (b) she is a lawyer\n\n (c) she has no known relations to lawyer \nAnalysis : In the sentence, the word ’she’ refers to someone who was upset while recounting certain events in court. The word ’lawyer’ refers to someone who denied a news report about that same person weeping in court. There is no information in the sentence to indicate that the two individuals are related in any way. So she has no known relations to lawyer. \nSentence : YANGON , Dec. 27 -LRB- Xinhua -RRB- – Myanmar will hold a horse race in Yangon to commemorate the country ’s 63rd Anniversary Independence Day , the <t> Myanmar Equestrian Federation <t> -LRB- <t> MEF <t> -RRB- confirmed to Xinhua on Monday.\n\n (a) MEF is also known as Myanmar Equestrian Federation\n\n (b) MEF has political affiliation with Myanmar Equestrian Federation\n\n (c) MEF has no known relations to Myanmar Equestrian Federation \nAnalysis : The symbols -LRB- and -RRB- in the sentence stand for left and right round brackets and are used to enclose the abbreviation ’MEF’ to indicate that it is a replacement for the longer name ’Myanmar Equestrian Federation. So MEF is also known as Myanmar Equestrian Federation. Answer : (a) \nInstruct : Read the sentence and determine the relation between  Douglas Flint  and  chairman  quoted by  ${<}\\mathrm{t}{>}$  . \nSentence : At the same time , Chief Financial Officer <t> Douglas Flint <t> will become   ${<}\\mathrm{t}{>}$   chairman  ${<}\\mathrm{t}{>}$   , succeeding Stephen Green who is leaving to take a government job.\n\n (a) Douglas Flint has no known relations to chairman\n\n (b) Douglas Flint is a chairman\n\n (c) Douglas Flint is the employee of chairman \nAnalysis : The sentence states that Chief Financial Officer Douglas Flint Douglas Flint will succeed Stephen Green as a chairman. So Douglas Flint is a chairman. Answer : (b) \nInstruct : Read the sentence and determine the relation between  FAA  and  U.S.  quoted by <t>. Sentence : On its Web site , the <t> U.S. <t> <t> FAA  ${<}\\mathrm{t}{>}$   says the Category 2 rating means the country lacks the laws or regulations that are needed for the certification and oversight of air carriers , according to minimum international standards.\n\n (a) FAA is also known as U.S.\n\n (b) FAA has no known relations to U.S.\n\n (c) FAA has a headquarter in the country U.S. Analysis : The sentence states that the FAA says the Category 2 rating means the country lacks the laws or regulations needed for the certification and oversight of air carriers, indicating that the FAA is responsible for overseeing aviation regulations in the country. Actually the FAA (Federal Aviation Administration) is a U.S. government agency responsible for regulating and overseeing civil aviation in the United States, and it has its headquarters in Washington, D.C.. So FAA has a headquarter in the country U.S.. "}
{"page": 24, "image_path": "doc_images/2303.08559v2_24.jpg", "ocr_text": "Table 18: Demo examples used in ACE0S dataset. We color the trigger word in blue.\n\nInstruct: Read following sentences and identify what event is triggered by the word loan quoted by <t>.\n\nSentence: Separately , former WorldCom CEO Bernard Ebbers failed on April 29 to make a first repayment of 25 million\ndollars, plus interest, on a 400-million - dollar <t> loan <t> from MCI, the Journal said, citing SEC documents.\n\n(a) The word loan does not trigger any known event.\n\n(b) The word loan triggers a TRANSFER-MONEY event: giving, receiving, borrowing, or lending money when it is NOT\nin the context of purchasing something.\n\n(c) The word loan triggers a DECLARE-BANKRUPTCY event: an Entity officially requests legal protection from debt\ncollection due to an extremely negative balance sheet.\n\nAnalysis: In the given sentence, the word loan is used to describe the 400-million-dollar loan from MCI to former\nWorldCom CEO Bernard Ebbers, which he failed to repay on time. This situation clearly involves a transfer of money\nwithout the context of purchasing something, which falls under the TRANSFER-MONEY event. So the word loan\ntriggers a TRANSFER-MONEY event: giving, receiving, borrowing, or lending money when it is NOT in the context of\npurchasing something.\n\nAnswer: (b)\n\nInstruct: Read following sentences and identify what event is triggered by the words treated quoted by <t>.\n\nSentence: When she ’s in Germany , Lynch will be <t> treated <t> for bullet wounds and broken bones .\n\n(a) The word treated triggers an INJURE event: a PERSON gets/got injured whether it occurs accidentally, intentionally\nor even self-inflicted.\n\n(b) The word treated does not trigger any known event.\n\n(c) The word treated triggers a TRANSPORT event: an ARTIFACT (WEAPON or VEHICLE) or a PERSON is moved\nfrom one PLACE (GEOPOLITICAL ENTITY, FACILITY, LOCATION) to another.\n\nAnalysis: The sentence suggests that Lynch has already been injured and will receive medical treatment in Germany for\nher injuries. The word ’treated’ simply describes the medical care she will receive and does not indicate a new event or\naction taking place. So the word treated does not trigger any known event.\n\nAnswer: (b)\n\nInstruct: Read following sentences and identify what event is triggered by the words buy quoted by <t>.\n\nSentence: And I won’t dwell on the irony of an Oracle employee being driven out of Oracle , starting his own company ,\nand forcing Ellison to spend $ 10.3 billion to get his company — but not him — back ( though it does rather delightfully\nremind me of Coca - Cola basically giving away the bottling franchise and then spending billions to <t> buy <t> it back ) .\n(a) The word buy triggers a DECLARE-BANKRUPTCY event: an Entity officially requests legal protection from debt\ncollection due to an extremely negative balance sheet.\n\n(b) The word buy triggers a TRANSFER-OWNERSHIP event: The buying, selling, loaning, borrowing, giving, or\nreceiving of artifacts or organizations by an individual or organization.\n\n(c) The word buy does not trigger any known event.\n\nAnalysis: In the given sentence, the word buy is used to describe the action of Oracle spending $10.3 billion to get a\ncompany back. This clearly involves the transfer of ownership of the company from one entity to another. So the word buy\ntriggers a TRANSFER-OWNERSHIP event: The buying, selling, loaning, borrowing, giving, or receiving of artifacts or\norganizations by an individual or organization.\n\nAnswer: (b)\n\nInstruct: Read following sentences and identify what event is triggered by the words set quoted by <t>.\n\nSentence: British forces also began establishing the country’s first postwar administration Tuesday, granting a local sheik\npower to <t> set <t> up an administrative committee representing the groups in the region.\n\n(a) The word set triggers a START-POSITION event: a PERSON elected or appointed begins working for (or changes\noffices within) an ORGANIZATION or GOVERNMENT.\n\n(b) The word set triggers a START-ORG event: a new ORGANIZATION is created.\n\n(c) The word set does not trigger any known event.\n\nAnalysis: The phrase ’set up’ specifically implies the creation or establishment of a new organization or entity, rather than\nsimply the word ’set’. So the word set does not trigger any known event.\n\nAnswer: (c)\n\n", "vlm_text": "Instruct : Read following sentences and identify what event is triggered by the word  loan  quoted by  ${<}\\mathrm{t}{>}$  . Sentence : Separately , former WorldCom CEO Bernard Ebbers failed on April 29 to make a first repayment of 25 million dollars, plus interest, on a 400-million - dollar  ${<}\\mathrm{t}{>}$   loan   ${<}\\mathrm{t}{>}$   from MCI, the Journal said, citing SEC documents.\n\n (a) The word loan does not trigger any known event.\n\n (b) The word loan triggers a TRANSFER-MONEY event: giving, receiving, borrowing, or lending money when it is NOT in the context of purchasing something. (c) The word loan triggers a DECLARE-BANKRUPTCY event: an Entity officially requests legal protection from debt collection due to an extremely negative balance sheet. Analysis : In the given sentence, the word loan is used to describe the 400-million-dollar loan from MCI to former WorldCom CEO Bernard Ebbers, which he failed to repay on time. This situation clearly involves a transfer of money without the context of purchasing something, which falls under the TRANSFER-MONEY event. So the word loan triggers a TRANSFER-MONEY event: giving, receiving, borrowing, or lending money when it is NOT in the context of purchasing something. Answer : (b) Instruct : Read following sentences and identify what event is triggered by the words  treated  quoted by <t>. Sentence : When she ’s in Germany , Lynch will be   ${<}\\mathrm{t}{>}$   treated  ${<}\\mathrm{t}{>}$   for bullet wounds and broken bones . (a) The word treated triggers an INJURE event: a PERSON gets/got injured whether it occurs accidentally, intentionally or even self-inflicted.\n\n (b) The word treated does not trigger any known event.\n\n (c) The word treated triggers a TRANSPORT event: an ARTIFACT (WEAPON or VEHICLE) or a PERSON is moved from one PLACE (GEOPOLITICAL ENTITY, FACILITY, LOCATION) to another. Analysis : The sentence suggests that Lynch has already been injured and will receive medical treatment in Germany for her injuries. The word ’treated’ simply describes the medical care she will receive and does not indicate a new event or action taking place. So the word treated does not trigger any known event. Answer : (b) Instruct : Read following sentences and identify what event is triggered by the words  buy  quoted by  ${<}\\mathrm{t}{>}$  . Sentence : And I won’t dwell on the irony of an Oracle employee being driven out of Oracle , starting his own company , and forcing Ellison to spend  $\\S~10.3$   billion to get his company – but not him – back ( though it does rather delightfully remind me of Coca - Cola basically giving away the bottling franchise and then spending billions to  ${<}\\mathrm{t}{>}$   buy  ${<}\\mathrm{t}{>}$   it back ) . (a) The word buy triggers a DECLARE-BANKRUPTCY event: an Entity officially requests legal protection from debt collection due to an extremely negative balance sheet. (b) The word buy triggers a TRANSFER-OWNERSHIP event: The buying, selling, loaning, borrowing, giving, or receiving of artifacts or organizations by an individual or organization. (c) The word buy does not trigger any known event. Analysis : In the given sentence, the word buy is used to describe the action of Oracle spending   $\\S10.3$   billion to get a company back. This clearly involves the transfer of ownership of the company from one entity to another. So the word buy triggers a TRANSFER-OWNERSHIP event: The buying, selling, loaning, borrowing, giving, or receiving of artifacts or organizations by an individual or organization. Answer : (b) Instruct : Read following sentences and identify what event is triggered by the words  set  quoted by <t>. Sentence : British forces also began establishing the country’s first postwar administration Tuesday, granting a local sheik power to <t> set <t> up an administrative committee representing the groups in the region. (a) The word set triggers a START-POSITION event: a PERSON elected or appointed begins working for (or changes offices within) an ORGANIZATION or GOVERNMENT.\n\n (b) The word set triggers a START-ORG event: a new ORGANIZATION is created.\n\n (c) The word set does not trigger any known event. Analysis : The phrase ’set up’ specifically implies the creation or establishment of a new organization or entity, rather than simply the word ’set’. So the word set does not trigger any known event. Answer : (c) "}
{"page": 25, "image_path": "doc_images/2303.08559v2_25.jpg", "ocr_text": "Table 19: Templates for FewNERD dataset, where {ent} is the placeholder for entity type.\n\nEntity Template\n\nno-entity {ent} do/does not belong to any known entities.\n\nperson-artist/author {ent} is an artist or author.\n\nperson-actor {ent} is an actor.\n\nart-writtenart {ent} is a kind of writtenart.\n\nperson-director {ent} is a director.\n\nperson-other {ent} is a person, but not affiliated with following professions: actor, artist, athlete,\n\nauthor, director, politician, scholar, soldier.\n\norganization-other\n\n{ent} pertains to an organization that does not fall under the categories of company,\neducational institution, government, media, political party, religion, sports league,\nsports team, band or musical group.\n\norganization-company\n\nent} is a company\n\norganization-sportsteam\n\nent} is a sports team\n\norganization-sportsleague\n\nent} is a sports league\n\nproduct-car\n\nent} is a kind of car\n\nevent-protest\n\nent} refers to a protest, uprising or revolution event\n\norganization-\ngovernment/governmentagency\n\n{\n{\n{\n{\n{\n{\n\nent} refers to a government or governmental agency\n\nother-biologything\n\n{ent} is a special term about biology / life science.\n\nlocation-GPE\n\n{ent} is a kind of geopolitical entity\n\nlocation-other\n\n{ent} is a geographic locaton that does not fall under the categories of geopolitical\nentity, body of water, island, mountain, park, road, railway and transit.\n\nperson-athlete\n\n{ent} is an athlete or coach.\n\nart-broadcastprogram\n\n{ent} is a broadcast program.\n\nproduct-other\n\n{ent} is a kind of product that does not fall under the categories of airplane, train,\nship, car, weapon, food, electronic game and software.\n\nbuilding-other\n\n{ent} is a kind of building that does not fall under the categories of airport, hospital,\nhotel, library, restaurant, sports facility and theater\n\nproduct-weapon\n\nent} is a kind of weapon.\n\nbuilding-airport\n\nent} is an airport.\n\nbuilding-sportsfacility\n\nent} is a sports facility building.\n\nperson-scholar\n\nent} is a scholar.\n\nart-music\n\nent} is a music.\n\nevent-other\n\n{\n{\n{\n{\n{\n{\n\nent} refers to some event except attack, election, natural disaster, protest, revolution\nand sports\n\nother-language\n\nent} is a kind of human language.\n\nother-chemicalthing\n\nent} is some special term about chemical science.\n\nart-film\n\nent} is a film.\n\nbuilding-hospital\n\nis a hospital.\n\nother-law\n\nent} is a legal document, a term or a convention in legal sense.\n\nproduct-airplane\n\nent} is kind of airplane product.\n\nlocation-\nroad/railway/highway/transit\n\nent} is a geographic position about roadways, railways, highways or public transit\nsystems.\n\nperson-soldier\n\nent} is a soldier\n\nlocation-mountain\n\nent} is geographic position about mountain.\n\norganization-education\n\nent} is an educational institute/organization.\n\norganization-media/newspaper\n\nent} is a media/newspaper organization.\n", "vlm_text": "The table outlines different entities and their corresponding templates for classification. Each row contains two columns:\n\n1. **Entity**: The type of entity, such as \"person-actor,\" \"organization-company,\" \"location-GPE,\" etc.\n2. **Template**: A description or template sentence structure for identifying or categorizing the entity, using placeholders like `{ent}` to specify where the entity's name would appear.\n\nThe entities cover a range of categories, including people (e.g., actor, director), organizations (e.g., company, sports team), locations (e.g., geopolitical entity, mountain), products (e.g., car, airplane), events (e.g., protest, other), and other specialized terms for biology, chemistry, language, and law."}
{"page": 26, "image_path": "doc_images/2303.08559v2_26.jpg", "ocr_text": "product-software ent} is a software product.\n\nlocation-island ent} is geographic position about island.\nlocation-bodiesofwater ent} is geographic position situated near a body of water.\nbuilding-library ent} is a library.\n\nother-astronomything ent} is a special term about astronomy.\n\nperson-politician ent} is a politician or lawyer or judge.\n\nbuilding-hotel ent} is a hotel building.\n\nproduct-game ent} is a electronic game product.\n\nother-award ent} is a kind of award.\n\nevent-sportsevent ent} refers to some event related to sports.\norganization-showorganization ent} is a band or musical organization.\nother-educationaldegree ent} is a kind of educational degree.\n\nbuilding-theater ent} is a theater.\n\nother-disease ent} is a kind of disease.\n\nevent-election ent} is an event about election.\norganization-politicalparty ent} is a political party/organization.\n\nother-currency ent} is a kind of currency.\n\nevent- ent} is an event about attack, battle, war or military conflict.\n\nattack/battle/war/militaryconflict\n\nproduct-ship ent} is a ship.\n\nbuilding-restaurant ent} is a restaurant.\n\nother-livingthing ent} is a living animal/creature/organism.\n\nart-other ent} is a work of art, but not belong to the categories of music, film, written art,\n\nroadcast or painting.\n\nevent-disaster ent} is a natural disaster event.\norganization-religion ent} is a religious organization.\nother-medical ent} refers to some kind of medicine.entity\nlocation-park ent} is a park.\n\nother-god ent} is a god in some legend/religious story.\nproduct-food ent} is a kind of food.\n\nproduct-train ent} is a kind of train(vehicle).\n\nart-painting ent} is an art painting.\n", "vlm_text": "The table consists of various categories related to entities, each paired with a corresponding description. Here is a summary of the entity types and their descriptions:\n\n- **product-software**: A software product.\n- **location-island**: A geographic position about an island.\n- **location-bodiesofwater**: A geographic position near a body of water.\n- **building-library**: A library.\n- **other-astronomything**: A special term about astronomy.\n- **person-politician**: A politician, lawyer, or judge.\n- **building-hotel**: A hotel building.\n- **product-game**: An electronic game product.\n- **other-award**: A type of award.\n- **event-sportsevent**: An event related to sports.\n- **organization-showorganization**: A band or musical organization.\n- **other-educationaldegree**: A type of educational degree.\n- **building-theater**: A theater.\n- **other-disease**: A kind of disease.\n- **event-election**: An election event.\n- **organization-politicalparty**: A political party or organization.\n- **other-currency**: A type of currency.\n- **event-attack/battle/war/militaryconflict**: An event about attack, battle, war, or military conflict.\n- **product-ship**: A ship.\n- **building-restaurant**: A restaurant.\n- **other-livingthing**: A living animal, creature, or organism.\n- **art-other**: A work of art, excluding categories like music, film, etc.\n- **event-disaster**: A natural disaster event.\n- **organization-religion**: A religious organization.\n- **other-medical**: A type of medicine entity.\n- **location-park**: A park.\n- **other-god**: A god in some legend or religious story.\n- **product-food**: A kind of food.\n- **product-train**: A kind of train (vehicle).\n- **art-painting**: An art painting."}
{"page": 27, "image_path": "doc_images/2303.08559v2_27.jpg", "ocr_text": "Table 20: Templates for TACREV dataset, where {subj} and {obj} are the placeholders for subject and object\nentities. Copied from (Lu et al., 2022a)\n\nRelation\n\nemplate\n\nno_relation\n\nsu\n\nj} has no known relations to {obj}\n\nper:stateorprovince_of_death\n\nsu\n\nj} died in the state or province {obj}\n\nper:title\n\nsu\n\nij} is a {obj}\n\norg:member_of\n\nsu\n\nyj} is the member of {obj}\n\nper:other_family\n\nsu\n\nj} is the other family member of {obj}\n\norg:country_of_headquarters\n\nsu\n\n)j} has a headquarter in the country {obj}\n\norg:parents\n\nsu\n\n)j} has the parent company {obj}\n\nper:stateorprovince_of_birth\n\nsu\n\n)j} was born in the state or province {obj}\n\nper:spouse\n\nsu\n\nyj} is the spouse of {obj}\n\nper:origin\n\nsu\n\nj} has the nationality {obj}\n\nper:date_of_birth\n\nsu\n\nj} has birthday on {obj}\n\nper:schools_attended\n\nsu\n\nj} studied in {obj}\n\norg:members\n\nsu\n\nj} has the member {obj}\n\norg:founded\n\nsu\n\n4j} was founded in {obj}\n\nper:stateorprovinces_of_residence\n\nsu\n\n)j} lives in the state or province {obj}\n\nper:date_of_death\n\nsu\n\nj} died in the date {obj}\n\norg:shareholders\n\nsu\n\n4j} has shares hold in {obj}\n\norg: website\n\nsu\n\nj} has the website {obj}\n\norg:subsidiaries\n\nsu\n\n4j} owns {obj}\n\nper:charges\n\nsu\n\n4j} is convicted of {obj}\n\norg:dissolved\n\nsu\n\nj} dissolved in {obj}\n\norg:stateorprovince_of_headquarters\n\nsu\n\nper:country_of_birth\n\nsu\n\n4j} was born in the country {obj}\n\nper:siblings\n\nsu\n\nj} is the siblings of {obj}\n\norg:top_members/employees\n\nsu\n\n4j} has the high level member {obj}\n\nper:cause_of_death\n\nsu\n\nj} died because of {obj}\n\nper:alternate_names\n\nsu\n\n4j} has the alternate name {obj}\n\norg:number_of_employees/members\n\nsu\n\n)j} has the number of employees {obj}\n\nper:cities_of_residence\n\nsu\n\nj} lives in the city {obj}\n\norg:city_of_headquarters\n\nsu\n\n)j} has a headquarter in the city {obj}\n\nper:children\n\nsu\n\nj} is the parent of {obj}\n\nper:employee_of\n\nsu\n\nj} is the employee of {obj}\n\norg:political/religious_affiliation\n\nsu\n\n4j} has political affiliation with {obj}\n\nper:parents\n\nsu\n\nj} has the parent {obj}\n\nper:city_of_birth\n\nsu\n\n4j} was born in the city {obj}\n\nper:age\n\nsu\n\nj} has the age {obj}\n\nper:countries_of_residence\n\nsu\n\nj} lives in the country {obj}\n\norg:alternate_names\n\nsu\n\nj} is also known as {obj}\n\nper:religion\n\nsu\n\nj} has the religion {obj}\n\nper:city_of_death\n\nsu\n\nj} died in the city {obj}\n\nper:country_of_death\n\nsu\n\n4j} died in the country {obj}\n\norg:founded_by\n\nsu\n\n}\n} is\nyi\nyi\n}\n}\n}\nyi\n}\n}\n}\n}\n}\n}\n}\n}\n}\n}\nyi\n}\n)j} has a headquarter in the state or province {obj}\n}\nyi\n}\n}\n}\n}\n}\n}\nyi\nyi\n}\n}\n}\n}\n}\nyi\n}\n}\n}\n}\n\n4j} was founded by {obj}\n\n", "vlm_text": "Table 20: Templates for TACREV dataset, where {subj} and   $\\{{\\sf o b j}\\}$   are the placeholder s for subject and object entities. Copied from ( Lu et al. ,  2022a ) \nThe table lists relationships and their corresponding templates. Each row contains a relation type and a sentence template using placeholders `{subj}` and `{obj}`. Here are some examples:\n\n- **Relation:** `per:title`  \n  **Template:** `{subj} is a {obj}`\n\n- **Relation:** `org:country_of_headquarters`  \n  **Template:** `{subj} has a headquarter in the country {obj}`\n\n- **Relation:** `per:date_of_birth`  \n  **Template:** `{subj} has birthday on {obj}`\n\nThis pattern continues for various personal (per) and organizational (org) relationships, defining how to express them in sentence form."}
{"page": 28, "image_path": "doc_images/2303.08559v2_28.jpg", "ocr_text": "Table 21: Templates for ACE0S5 dataset, where {evt} is the placeholder for event type.\n\nEvent Template\nno-event The word {evt} does not trigger any known event.\nMovement.Transport The word {evt} triggers a TRANSPORT event: an ARTIFACT (WEAPON or\n\n<\n\nEHICLE) or a PERSON is moved from one PLACE (GEOPOLITICAL ENTITY,\n‘ACILITY, LOCATION) to another.\n\nles]\n\nPersonnel.Elect\n\ne word {evt} triggers an ELECT event which implies an election.\n\nPersonnel.Start-Position\n\n3/3\n\ne word {evt} triggers a START-POSITION event: a PERSON elected or appointed\nbegins working for (or changes offices within) an ORGANIZATION or GOVERN-\nMENT.\n\nPersonnel.Nominate\n\nThe word {evt} triggers a NOMINATE event: a PERSON is proposed for a position\nthrough official channels.\n\nConflict.Attack\n\nThe word {evt} triggers an ATTACK event: a violent physical act causing harm or\ndamage.\n\nPersonnel.End-Position\n\nThe word {evt} triggers an END-POSITION event: a PERSON stops working for\n(or changes offices within) an ORGANIZATION or GOVERNMENT.\n\nContact.Meet\n\nThe word {evt} triggers a MEET event: two or more entities come together at a\nsingle location and interact with one another face-to-face.\n\nLife.Marry\n\nThe word {evt} triggers a MARRY event: two people are married under the legal\ndefinition.\n\nContact.Phone-Write\n\nThe word {evt} triggers a PHONE-WRITE event: two or more people directly\nengage in discussion which does not take place ’face-to-face’.\n\nTransaction. Transfer-Money\n\nThe word {evt} triggers a TRANSFER-MONEY event: giving, receiving, borrowing,\nor lending money when it is NOT in the context of purchasing something.\n\nJustice.Sue\n\nThe word {evt} triggers a SUE event: a court proceeding has been initiated for the\npurposes of determining the liability of a PERSON, ORGANIZATION or GEOPO-\nLITICAL ENTITY accused of committing a crime or neglecting a commitment\n\nConflict.Demonstrate\n\nThe word {evt} triggers a DEMONSTRATE event: a large number of people come\ntogether in a public area to protest or demand some sort of official action. For eample:\nprotests, sit-ins, strikes and riots.\n\nBusiness.End-Org\n\nThe word {evt} triggers an END-ORG event: an ORGANIZATION ceases to exist\n(in other words, goes out of business).\n\nLife.Injure\n\nThe word {evt} triggers an INJURE event: a PERSON gets/got injured whether it\noccurs accidentally, intentionally or even self-inflicted.\n\nLife.Die\n\nThe word {evt} triggers a DIE event: a PERSON dies/died whether it occurs acci-\ndentally, intentionally or even self-inflicted.\n\nJustice. Arrest-Jail\n\nThe word {evt} triggers a ARREST-JAIL event: a PERSON is sent to prison.\n\nTransaction.Transfer-\nOwnership\n\nThe word {evt} triggers a TRANSFER-OWNERSHIP event: The buying, selling,\nloaning, borrowing, giving, or receiving of artifacts or organizations by an individual\nor organization.\n\nJustice.Execute\n\nThe word {evt} triggers an EXECUTE event: a PERSON is/was executed\n\nJustice.Trial-Hearing\n\nThe word {evt} triggers a TRIAL-HEARING event: a court proceeding has been\ninitiated for the purposes of determining the guilt or innocence of a PERSON,\nORGANIZATION or GEOPOLITICAL ENTITY accused of committing a crime.\n\nJustice.Sentence\n\nThe word {evt} triggers a SENTENCE event: the punishment for the DEFENDANT\nis issued\n\nLife.Be-Born\n\nThe word {evt} triggers a BE-BORN event: a PERSON is given birth to.\n\nJustice.Charge-Indict\n\nThe word {evt} triggers a CHARGE-INDICT event: a PERSON, ORGANIZATION\nor GEOPOLITICAL ENTITY is accused of a crime\n\nBusiness.Start-Org\n\nThe word {evt} triggers a START-ORG event: a new ORGANIZATION is created.\n\nJustice.Convict\n\nThe word {evt} trigges a CONVICT event: a PERSON, ORGANIZATION or\nGEOPOLITICAL ENTITY is convicted whenever it has been found guilty of a\nCRIME.\n\nBusiness.Declare-Bankruptcy\n\nThe word {evt} triggers a DECLARE-BANKRUPTCY event: an Entity officially\nrequests legal protection from debt collection due to an extremely negative balance\nsheet.\n\nJustice.Release-Parole\n\nThe word {evt} triggers a RELEASE-PAROLE event.\n\n", "vlm_text": "The table contains two columns: \"Event\" and \"Template.\" Each row describes an event type and the template for how that event is triggered using the word \"{evt}\". Here are some examples:\n\n1. **Movement.Transport**: The word \"{evt}\" triggers a TRANSPORT event, such as moving an object or person from one place to another.\n\n2. **Personnel.Elect**: Triggers an ELECT event implying an election.\n\n3. **Conflict.Attack**: Triggers an ATTACK event involving violence or harm.\n\n4. **Life.Marry**: Triggers a MARRY event signifying legal marriage.\n\n5. **Justice.Arrest-Jail**: Triggers an ARREST-JAIL event, sending a person to prison.\n\n6. **Justice.Execute**: Triggers an EXECUTE event, indicating a person is or was executed.\n\n7. **Business.Declare-Bankruptcy**: Triggers a DECLARE-BANKRUPTCY event, when an entity seeks legal protection due to negative financial status.\n\nEach event type describes a specific scenario or action that can occur based on the triggering word."}
{"page": 29, "image_path": "doc_images/2303.08559v2_29.jpg", "ocr_text": "Justice.Fine\n\nThe word {evt} triggers a FINE event: a GEOPOLITICAL ENTITY, PERSON or\nORGANIZATION get financial punishment typically as a result of court proceedings.\n\nJustice.Pardon\n\ne word {evt} triggers a PARDON event: a head-of-state or their appointed repre-\nentative lifts a sentence imposed by the judiciary.\n\nPA\n\nJustice.Appeal\n\ne word {evt} triggers a APPEAL event: the decision of a court is taken to a higher\ncourt for review\n\nBusiness.Merge-Org\n\nThe word {evt} triggers a MERGE-ORG event: two or more ORGANIZATION\nEntities come together to form a new ORGANIZATION Entity.\n\nJustice.Extradite\n\nThe word {evt} triggers a EXTRADITE event.\n\nLife.Divorce\n\ne word {evt} triggers a DIVORCE event: two people are officially divorced under\ne legal definition of divorce.\n\nJustice.Acquit\n\ne word {evt} triggers a ACQUIT event: a trial ends but fails to produce a convic-\ntion.\n\n", "vlm_text": "The table lists different event triggers and their descriptions. Each trigger corresponds to a specific type of event that occurs under various circumstances:\n\n1. **Justice.Fine**: A financial punishment involving a geopolitical entity, person, or organization.\n2. **Justice.Pardon**: A head-of-state or representative lifts a judicial sentence.\n3. **Justice.Appeal**: A court decision taken to a higher court for review.\n4. **Business.Merge-Org**: Two or more organizations form a new organization.\n5. **Justice.Extradite**: Triggering an extradition event.\n6. **Life.Divorce**: Two people officially divorced under the law.\n7. **Justice.Acquit**: A trial ends without a conviction."}
