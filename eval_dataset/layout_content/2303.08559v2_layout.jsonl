{"layout": 0, "type": "text", "text": "Large Language Model Is Not a Good Few-shot Information  Extractor , but a Good  Reranker  for Hard Samples! ", "text_level": 1, "page_idx": 0, "bbox": [81, 75, 516, 109], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 1, "type": "text", "text": "Yubo Ma 1 , Yixin   $\\mathbf{Caa0^{2}}$  , YongChing Hong 1 , Aixin Sun 1 ", "page_idx": 0, "bbox": [163.72100830078125, 120, 437.0321350097656, 135.90472412109375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 2, "type": "text", "text": "1  S-Lab, Nanyang Technological University 2  Singapore Management University yubo001@e.ntu.edu.sg ", "page_idx": 0, "bbox": [194.4190216064453, 144.33840942382812, 403.84814453125, 189.73509216308594], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 3, "type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0, "bbox": [157, 214, 202, 225], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 4, "type": "text", "text": "Large Language Models (LLMs) have made remarkable strides in various tasks. Whether LLMs are competitive few-shot solvers for in- formation extraction (IE) tasks, however, re- mains an open problem. In this work, we aim to provide a thorough answer to this ques- tion. Through extensive experiments on nine datasets across four IE tasks, we demonstrate that current advanced LLMs consistently ex- hibit inferior performance, higher latency, and increased budget requirements compared to fine-tuned SLMs under most settings. There- fore, we conclude that LLMs are not effec- tive few-shot information extractors in gen- eral   1 . Nonetheless, we illustrate that with appropriate prompting strategies, LLMs can effectively complement SLMs and tackle chal- lenging samples that SLMs struggle with. And moreover, we propose an adaptive  filter-then- rerank  paradigm to combine the strengths of LLMs and SLMs. In this paradigm, SLMs serve as filters and LLMs serve as rerankers. By prompting LLMs to rerank a small portion of difficult samples identified by SLMs, our pre- liminary system consistently achieves promis- ing improvements (  $2.4\\%$   F1-gain on average) on various IE tasks, with an acceptable time and cost investment. Our code is available at https://github.com/mayubo2333/LLM-IE . ", "page_idx": 0, "bbox": [87, 235.6685791015625, 273, 582.4185180664062], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 5, "type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0, "bbox": [70, 592, 154, 605], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 6, "type": "text", "text": "Large Language Models (LLMs,  Brown et al. 2020 ; Chowdhery et al. 2022 ;  Touvron et al. 2023 ) have shown remarkable abilities on various NLP applica- tions such as factual question answering ( Yu et al. , 2023 ;  Sun et al. ,  2023 ), arithmetic reasoning ( Chen et al. ,  2022a ;  Qian et al. ,  2023 ) and logical rea- soning ( Jung et al. ,  2022 ;  Pan et al. ,  2023 ). Given the reasoning, memorization, instruction-following and few-shot adaption capabilities emerging from LLMs, it prompts a compelling question: Can LLMs be used to boost performance in few-shot information extraction (IE) tasks? ", "page_idx": 0, "bbox": [70, 613.6539916992188, 290, 735.1924438476562], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 7, "type": "text", "text": "", "page_idx": 0, "bbox": [305, 213.47804260253906, 526, 253.72146606445312], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 8, "type": "text", "text": "To answer this question, we conduct an exten- sive empirical study to compare the performance between LLMs using  in-context learning   2   (ICL) and  fine-tuned  Small Language Models (SLMs). We fairly evaluate SLMs-based and LLMs-based methods across nine datasets spanning four com- mon IE tasks: (1) Named Entity Recognition, (2) Relation Extraction, (3) Event Detection and (4) Event Argument Extraction. For each dataset, we explored four to six settings to encompass typi- cal low-resource extents, from 1-shot to 20-shot or even more. Given the potential sensitivity of LLMs’ performance to the prompt context, we meticu- lously considered variations in instruction, demon- stration number and selection strategy, prompt for- mat,  etc . Our study reveals that LLMs excel over SLMs only when annotations are extremely lim- ited,  i.e.,  both label types 3   and the samples 4   per label are extremely scarce. With more ( e.g.,  hun- dreds of) samples, SLMs significantly outperform LLMs. Furthermore, LLMs incur greater inference latency and costs than fine-tuned SLMs. Hence, we claim that  current LLMs are not good few-shot information extractors in general . ", "page_idx": 0, "bbox": [305, 254.53602600097656, 526, 579.9680786132812], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 9, "type": "text", "text": "We further investigate whether LLMs and SLMs exhibit different abilities to handle various types of samples. We categorize samples according to their difficulty measured by SLMs’ confidence scores, and compare LLMs’ and SLMs’ results within each group. We find that  LLMs are good at hard sam- ples, though bad at easy samples . We posit that the knowledge and reasoning abilities in LLMs en- able them to handle hard samples (which are sim- ply beyond SLMs’ capabilities) well. Nevertheless, LLMs demonstrate strong predisposition to false- positive predictions on negative samples. Since most negative samples are easy samples (which could be solved readily by SLMs), the performance of LLMs on easy samples sometimes collapses and are usually much worse than fine-tuned SLMs. ", "page_idx": 0, "bbox": [305, 580.1279907226562, 526, 701.6664428710938], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 10, "type": "text", "text": "", "page_idx": 1, "bbox": [69, 71.74500274658203, 291, 166.18545532226562], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 11, "type": "text", "text": "Leveraging these findings, we pursue an ap- proach to incorporate LLMs and SLMs within a single system and combine their merits. To this end, we propose a novel  filter-then-rerank  framework. The basic idea is that SLMs serve as a filter and LLMs as a reranker. Specifically, SLMs initially predict and determine the difficulty of each sample. If the sample is a hard one, we further pass the top-  $\\cdot N$   most-likely candidate labels from SLMs to LLMs for reranking. Otherwise we view the predic- tion from SLMs as the final decision. By providing easy/hard samples with different solution strategies, our system utilizes each model’s strengths to com- plement each other. Also, it reranks only a small subset of samples and minimizes the extra latency and budgets for calling LLMs. With a modest cost increase, our framework yields a consistent F1 im- provement, averaging  $2.4\\%$   higher than previous methods on various few-shot IE tasks. To the best of our knowledge, this is the first successful attempt to use LLMs to enhance few-shot IE tasks. ", "page_idx": 1, "bbox": [69, 167.0130157470703, 291, 451.1424865722656], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 12, "type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1, "bbox": [70, 463, 161, 476], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 13, "type": "text", "text": "2.1 LLMs for Information Extraction ", "text_level": 1, "page_idx": 1, "bbox": [70, 485, 254, 498], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 14, "type": "text", "text": "Recent studies have increasingly explored Informa- tion Extraction (IE) tasks using LLMs. Drawing in- spiration from instruction tuning ( Wei et al. ,  2022a ), several methods ( Wadhwa et al. ,  2023 ;  Wang et al. ,  $2023\\mathrm{a}$  ;  Lu et al. ,  2023 ) transform annotated sam- ples into instruction-answer pairs and then fine- tune LLMs, such as FlanT5 ( Chung et al. ,  2022 ), on them. Nonetheless, this method necessitates a vast range of samples with diverse schemas and often yields suboptimal results in low-resource sce- narios. In the context of few-shot IE tasks, preva- lent strategies bifurcate into two main streams. The first approach perceives LLMs as efficient annota- tors ( Ding et al. ,  2023 ;  Josifoski et al. ,  2023 ). In these methods, they produce a plethora of pseudo- labeled samples through LLMs and leverage the enhanced annotations to train SLMs. Conversely, the latter approach employs LLMs in inference us- ing the ICL paradigm, which is the focus of our subsequent discussion. ", "page_idx": 1, "bbox": [69, 503.50897216796875, 291, 774.0894775390625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 15, "type": "text", "text": "2.2 Few-shot IE with ICL ", "text_level": 1, "page_idx": 1, "bbox": [305, 70, 434, 85], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 16, "type": "text", "text": "Regarding few-shot IE tasks, recent studies inten- sively compare the performance between SLMs and LLMs but yield inconsistent conclusions. Some studies favor LLMs as competent few-shot extractors ( Agrawal et al. ,  2022 ;  Wang et al. , 2023b ;  Li et al. ,  2023 ;  Zhang et al. ,  2023a ; Wadhwa et al. ,  2023 ), while others dispute this claim ( Jimenez Gutierrez et al. ,  2022 ;  Qin et al. , 2023 ;  Wei et al. ,  2023 ;  Gao et al. ,  2023 ). This discrepancy leaves the question of  whether LLMs perform competitively on few-shot IE tasks  unre- solved, thus hindering the advances of this domain. We attribute such disagreement to the absence of an comprehensive and unified benchmark. Ex- isting studies usually vary in tasks, datasets, and few-shot settings. Furthermore, some studies rely on overly simplistic datasets ( Jimenez Gutierrez et al. ,  2022 ;  Li et al. ,  2023 ) and may exaggerate the effectiveness of LLMs. Driven by these find- ings, our research undertakes comprehensive ex- periments across four IE tasks, nine datasets with various schema complexities (from coarse-grained to fine-grained) and low-resource settings. ", "page_idx": 1, "bbox": [305, 89.81098175048828, 526, 401.33648681640625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 17, "type": "text", "text": "In addition to the empirical study, we develop an innovative  filter-then-rerank  paradigm to combine the strengths of both LLMs and SLMs. It utilizes prompting strategies akin to QA4RE ( Zhang et al. , 2023a ), transforming IE tasks into multi-choice questions. However, our method stands apart by integrating SLMs and LLMs within a single frame- work. This incorporation (1) enables our paradigm applicable to various IE tasks by providing candi- date spans in the text and (2) achieves promising performance under low-resource IE scenarios. ", "page_idx": 1, "bbox": [305, 402.0369873046875, 526, 550.6744384765625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 18, "type": "text", "text": "3 Large LMs v.s. Small LMs ", "text_level": 1, "page_idx": 1, "bbox": [305, 561, 462, 575], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 19, "type": "text", "text": "In this section, we compare the performance be- tween LLMs and SLMs to evaluate whether LLMs perform competitively. ", "page_idx": 1, "bbox": [305, 583.9719848632812, 526, 624.2154541015625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 20, "type": "text", "text": "3.1 Task, Dataset and Evaluation ", "text_level": 1, "page_idx": 1, "bbox": [305, 634, 468, 648], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 21, "type": "text", "text": "We run experiments on nine widely-used datasets across four IE tasks. (1) Named Entity Recognition (NER): CONLL03 ( Tjong Kim Sang and De Meul- der ,  2003 ), OntoNotes ( Weischedel et al. ,  2013 ) and FewNERD ( Ding et al. ,  2021 ). (2) Relation Extraction (RE): TACRED ( Zhang et al. ,  2017 ) and TACREV ( Alt et al. ,  2020 ). (3) Event De- tection (ED): ACE05 ( Doddington et al. ,  2004 ), MAVEN ( Wang et al. ,  2020 ) and ERE ( Song et al. , ", "page_idx": 1, "bbox": [305, 652.5499877929688, 526, 774.0894775390625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 22, "type": "image", "page_idx": 2, "img_path": "layout_images/2303.08559v2_0.jpg", "img_caption": "Figure 1: Examples of prompts used. The green, blue and black parts in the top boxes represent the instruction, demonstration (demo) and test sentence in the prompt respectively. The red parts represent the outputs from LLMs. We plot only 1 example for convenience of visualization. The actual demo number is usually much larger than 1. ", "bbox": [69, 81, 526, 360], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "Named Entity Recognition\n\nRelation Extraction\n\nIdentify the entities then Locate each entity to words in\nsentence. The possible entity types are building-hotel,\n\nSentence: The 1962 National Football League draft was held on\nDecember 4th, 1961 at the Sheraton Hotel in Chicago, Illinois.\n\nEntities: (type: organization-sportsleague, entity: 1962 National\nFootball League), (type: building-hotel, entity: Sheraton Hotel)\n\nSentence: Critics noted “The Manual of Detection” combines\nelements from several genres, including mystery and fantasy.\n\nIdentify the relation between the entities in the sentence. The\npossible relation types are per:title, org:top_members, ......\nSentence: Five NGOs that form [tstart] TOAID [tend] are\nCompassion International Taiwan, the Eden Social Welfare\nFoundation, the Field Relief Agency of Taiwan, [hstart] NCF\n[hend] and the Taiwan Root Medical Peace Corps.\n\nTriple: (Subj: NCF, Obj: TOAID, Relation: org:member_of)\nSentence: Fuller, 37, has been there before.\n\nTriple: (Subj: Fuller, Obj: 37, Relation:\n\nEntities:\n\nNo entities found.\n\n|\n\nPer:age)\n\nEvent Detection\n\nIdentify the events then Locate each event to words in sentence.\nThe possible event types are Life.Injure,.\n\nSentence: Injured Russian diplomats were among victims caught in\ncrossfire and friendly fire Sunday.\n\nEvents: (type: Life.Injure, trigger_word: Injured), (type:\nConflict.attack, trigger_word: crossfire)\n\nSentence: Iraqi Kurds fear that Turkish troops could move deep\ninto the enclave in northern Iraq and have threatened clashes.\n\nEvents:\n\n(type: Movement. Transport, trigger_word: move),\n(type: Conflict.Attack, trigger_word: clashes)\n\nEvent Argument Extraction\n\nGiven a sentence and an event, you need to identify all\narguments of this event, and classify role of each argument.\nSentence: Cash-strapped Vivendi wants to sell Universal Studios\nand its TV production company. Event: Transfer-Ownership,\nTrigger_word: sell, Possible roles: Seller, Artifact,......\nArguments: (type: Seller, argument: Vivendi), (type: Artifact,\nargument: Universal Studio and its TV production company)\nSentence: Another appeal is pending in the Federal Court. Event:\nJustice-Appeal,Trigger_word: appeal, Possible roles: Adjudicator,\nPlaintiff, Place.\n\nArguments:\n(type: Adjudicator, argument: the Federal Court)\n\n", "vlm_text": "The image depicts examples of prompts used for different NLP tasks:\n\n1. **Named Entity Recognition (NER):**\n   - **Instruction:** Identify entities in the sentence and locate them to words.\n   - **Demo Sentence:** \"The 1962 National Football League draft was held at the Sheraton Hotel in Chicago, Illinois.\"\n   - **Entities:** Organization-sportsleague (1962 National Football League), Building-hotel (Sheraton Hotel)\n   - **Test Sentence:** \"Critics noted 'The Manual of Detection' combines elements from several genres, including mystery and fantasy.\"\n   - **Output:** No entities found.\n\n2. **Relation Extraction:**\n   - **Instruction:** Identify the relation between entities in the sentence.\n   - **Demo Sentence:** Features organizations and members, with entities like TOAID and NCF.\n   - **Test Sentence:** \"Fuller, 37, has been there before.\"\n   - **Output:** Per:age\n\n3. **Event Detection:**\n   - **Instruction:** Identify events in the sentence and locate to words.\n   - **Demo Sentence:** \"Injured Russian diplomats were among victims... crossfire.\"\n   - **Events:** Life.Injure (trigger_word: Injured), Conflict.Attack (trigger_word: crossfire)\n   - **Test Sentence:** \"Iraqi Kurds fear that Turkish troops could move deep into the enclave...\"\n   - **Output:** Movement.Transport (trigger_word: move), Conflict.Attack (trigger_word: clashes)\n\n4. **Event Argument Extraction:**\n   - **Instruction:** Identify arguments of an event and classify their roles.\n   - **Demo Sentence:** Vivendi selling Universal Studios, roles like Seller, Artifact.\n   - **Test Sentence:** \"Another appeal is pending in the Federal Court.\"\n   - **Output:** Adjudicator, argument: the Federal Court\n\nEach task involves processing sentences to identify and classify different elements such as entities, relations, events, and arguments."}
{"layout": 23, "type": "text", "text": "2015 ). (4) Event Argument Extraction (EAE): ACE05, ERE and RAMS ( Ebner et al. ,  2020 ). With label numbers ranging from 4 to 168, we assess LLMs’ performance under different schema com- plexities. See their details in Appendix  A.1 . ", "page_idx": 2, "bbox": [70, 381.79498291015625, 290, 449.1374816894531], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 24, "type": "text", "text": "Few-shot Set  We construct few-shot datasets from the original datasets above. For training and vali- dation set, we adopt    $K$  -shot sampling strategy,  i.e., sampling    $K$   samples for each label type. See more details in Appendix  A.2 . For test set, we down- sample their original test sets to reduce the cost of LLMs. We randomly sample 500 sentences for RE tasks, and 250 sentences for other task. We en- sure that each label has at least one corresponding sample to avoid the absence of rare labels. ", "page_idx": 2, "bbox": [70, 449.2232666015625, 290, 584.7034301757812], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 25, "type": "text", "text": "Evaluation  We adopt micro-F1 score in NER, RE and ED tasks. For EAE task, we follow previous work ( Wang et al. ,  2023b ) and adopt head-F1 score, which merely considers matching of the head word rather than the whole content of a text span. We re- port averaged score w.r.t 5 sampled train/validation sets unless otherwise stated. ", "page_idx": 2, "bbox": [70, 584.789306640625, 290, 679.6234130859375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 26, "type": "text", "text": "3.2 Small Language Models ", "text_level": 1, "page_idx": 2, "bbox": [70, 690, 207, 701], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 27, "type": "text", "text": "We adopt five supervised methods to evaluate the abilities of SLMs. (1) Vanilla fine-tuning for all tasks, (2) FSLS ( Ma et al. ,  2022a ) for NER and ED tasks, (3) KnowPrompt ( Chen et al. ,  2022b ) for RE task, (4) PAIE ( Ma et al. ,  2022b ) for EAE task, and (5) UIE ( Lu et al. ,  2022c ) for all tasks. See their details in Appendix  B . ", "page_idx": 2, "bbox": [70, 706.7470092773438, 290, 774.0894775390625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 28, "type": "text", "text": "", "page_idx": 2, "bbox": [305, 381.79498291015625, 526, 408.4894714355469], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 29, "type": "text", "text": "3.3 Large Language Models ", "text_level": 1, "page_idx": 2, "bbox": [305, 420, 444, 431], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 30, "type": "text", "text": "Detailed in Appendix  C , we evaluate the ICL abil- ities of LLMs. Given labeled sentences    $D\\;=\\;$   $\\{(s_{i},y_{i})\\}$   and a test sentence    $s$  , our goal is to pre- dict structured information    $y$   from    $s$   using a frozen LLM    $\\mathcal{L}$  . We feed LLM with prompt    $\\mathcal{P}_{\\mathcal{E},I,f}(D,s)$  : ", "page_idx": 2, "bbox": [305, 437.09100341796875, 526, 511.84075927734375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 31, "type": "equation", "text": "\n$$\n\\mathcal{P}_{\\mathcal{E},I,f}(D,s)=[I;f(\\mathcal{E}(D,s));f(s)]\n$$\n ", "text_format": "latex", "page_idx": 2, "bbox": [333, 529, 497, 545], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 32, "type": "text", "text": "We give examples of prompts on four IE tasks in Figure  1 . The prompts consist of three parts: in- struction  $I$   (color in green in Figure  1 ), demonstra- tion    $f(\\mathcal{E}(D,s))$   (demo; color in b e) and the ques- tion    $f(x)$   (color in black). Here  E  denotes demo selector and    ${\\mathcal{E}}(D,s)\\,\\subset\\,D$   denotes selected s tences as the demo to predict    $s$  . Prompt format  $f^{\\ 5}$  refers to the template which converts demo    ${\\mathcal{E}}(D,s)$  and sample    $s$   to input context for LLMs. Then LLM generates    $f(y)$   (color in red) from which we could readily parse the extraction results    $y$  . ", "page_idx": 2, "bbox": [305, 556.6210327148438, 526, 705.2584228515625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 33, "type": "text", "text": "Models    $\\mathcal{L}$  : We explore six LLMs from two sources. (1) OpenAI models   6 : we employ Chat- ", "page_idx": 2, "bbox": [305, 705.6152954101562, 526, 732.7024536132812], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 34, "type": "image", "page_idx": 3, "img_path": "layout_images/2303.08559v2_1.jpg", "img_caption": "(a) Named Entity Recognition (NER) ", "bbox": [95, 70, 501, 178], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "F1 score\n\n—~ Fine-tuning ~— FSLS -- UIE\n100\n\nyb Da o@\n$6388\n\n1-shot 5-shot 10-shot 20-shot\nCONLLO3\n\nF1 score\n\nChatGPT -*- CODEX\n\n100\n\nby B® e@\n$6568 6\n\n1-shot\n\n5-shot 10-shot 20-shot\nOntoNotes\n\nF1 score\n\n100\n80\n60\n40\n20\n\n1-shot\n\nInstructGPT —— LLaMA(13B) —— Vicuna (13B)\n\n5-shot 10-shot 20-shot\nFewN\n", "vlm_text": "The image is a set of three line graphs comparing the F1 scores of different models for Named Entity Recognition (NER) tasks across three datasets: CONLL03, OntoNotes, and FewNERD. The models compared include Fine-tuning, FSLS, UIE, ChatGPT, CODEX, InstructGPT, LLaMA (13B), and Vicuna (13B). Performance is shown for different scenarios: 1-shot, 5-shot, 10-shot, and 20-shot learning. Each model's performance is represented by a distinct line style and color. The Y-axis shows the F1 score ranging from 0 to 100."}
{"layout": 35, "type": "image", "page_idx": 3, "img_path": "layout_images/2303.08559v2_2.jpg", "img_caption": "(b) Relation Extraction (RE) ", "bbox": [94, 186, 503, 294], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "— = Fine-tuning ~~ KnowPrompt -<- UIE ChatGPT -e- CODEX InstructGPT --- LLaMA(13B) —— Vicuna (13B)\n\n0\n1-shot 5-shot 10-shot 20-shot 50-shot 100-shot 1-shot 5-shot 10-shot 20-shot 50-shot 100-shot\nTACREV TACRED\n", "vlm_text": "The image contains two line graphs comparing the F1 scores of different models on the TACREV and TACRED datasets. The models displayed include Fine-tuning, KnowPrompt, UIE, ChatGPT, CODEX, InstructGPT, LLaMA (13B), and Vicuna (13B). Each graph shows their performance across different numbers of shots: 1-shot, 5-shot, 10-shot, 20-shot, 50-shot, and 100-shot. The scores indicate the models' performance in relation extraction (RE) tasks."}
{"layout": 36, "type": "image", "page_idx": 3, "img_path": "layout_images/2303.08559v2_3.jpg", "img_caption": "(c) Event Detection (ED) ", "bbox": [95, 301, 502, 411], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "F1 score\n\nChatGPT -e- CODEX\n\nInstructGPT ——\n\nLLaMA (13B) —— Vicuna (13B)\n\n~~ Fine-tuning FSLS -- UIE\n80 80\n60 @ 60\n\n=\n\n3\n40 @ 40\n\na\n20 LL 20\n\n1-shot 5-shot 10-shot 20-shot 1-shot 5-shot 10-shot 20-shot\nERE MAVEN\n\n1-shot 5-shot 10-shot 20-shot\n", "vlm_text": "The image shows three line charts comparing F1 scores for different models on the tasks ACE05, ERE, and MAVEN. The models include Fine-tuning, FSLS, UIE, ChatGPT, CODEX, InstructGPT, LLaMA (13B), and Vicuna (13B). The x-axis represents different \"shot\" settings (1-shot, 5-shot, 10-shot, 20-shot), and the y-axis shows the F1 score ranging from 0 to 80. Each model's performance is depicted by different colored lines, allowing for a side-by-side comparison across tasks."}
{"layout": 37, "type": "image", "page_idx": 3, "img_path": "layout_images/2303.08559v2_4.jpg", "img_caption": "(d) Event Argument Extraction (EAE) ", "bbox": [95, 417, 501, 528], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "F1 score\n\ng\n\na\n3\n\nS\nS\n\nNy\nS\n\n1-shot\n\nPAIE -—-\n\n—- Fine-tuning —\n\n5-shot 10-shot 20-shot\n\nUIE\n80\n\nF1 score\na Oo\n& 8\n\nny\nS\n\n1-shot 5-shot\nE\n\nChatGPT InstructGPT —— LLaMA(13B) —— Vicuna (13B)\n80\n\na\n3\n\neeur at\n\n+\nF1 score\nsy\n&\n\nny\nS\n\n1-shot\n\n10-shot 20-shot 5-shot 10-shot 20-shot\nE RAMS\n", "vlm_text": "This image presents three line graphs that compare the F1 scores of various methods for Event Argument Extraction (EAE) across three datasets: ACE05, ERE, and RAMS. \n\nThe methods compared are:\n\n- Fine-tuning\n- PAIE\n- UIE\n- ChatGPT\n- InstructGPT\n- LLaMA (13B)\n- Vicuna (13B)\n\nEach graph shows the performance (F1 score) across different amounts of training data: 1-shot, 5-shot, 10-shot, and 20-shot scenarios. The F1 score ranges from 0 to 80 on the y-axis. The plots indicate that PAIE and UIE generally achieve higher F1 scores across all datasets compared to other methods."}
{"layout": 38, "type": "text", "text": "Figure 2: Overall results of SLM-based methods (dashed lines) and LLM-based methods (solid lines) on nine datasets across four IE tasks. The black, horizontal dashed lines represent the SoTA performance on full dataset. ", "page_idx": 3, "bbox": [70, 540.8985595703125, 524.4099731445312, 564.8585205078125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 39, "type": "text", "text": "GPT, CODEX ( Chen et al. ,  2022a ) and Instruct- GPT ( Ouyang et al. ,  2022 ) for main experiments. We also evaluate GPT-4 in Appendix  D.3 . (2) Open-source models: we use LLaMA-13B ( Tou- vron et al. ,  2023 ) and its instruction-tuned counter- part, Vicuna-13B ( Chiang et al. ,  2023 ). ", "page_idx": 3, "bbox": [70, 586.4500122070312, 291, 667.3404541015625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 40, "type": "text", "text": "Instruction    $I$  : The instruction (1) describes the task and (2) enumerates all possible labels for ref- erence. we adopt instructions shown in Figure  1 . Demo selector    $\\mathcal{E}$  : The maximum input length of LLMs usually limits the sentence number in de- mos even under few-shot settings. Therefore for each test sentence    $s$  , we demand a demo retriever  ${\\mathcal{E}}(D,s)$   which selects a small subset from    $D$   as the sentences in demo. Following previous meth- ods ( Liu et al. ,  2022 ;  Su et al. ,  2022 ), we retrieve demos according to their sentence embedding simi- larity to the test samples. ", "page_idx": 3, "bbox": [70, 668.1372680664062, 291, 730.5147094726562], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 41, "type": "text", "text": "", "page_idx": 3, "bbox": [305, 586.4500122070312, 526, 694.439453125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 42, "type": "text", "text": "Prompt format    $f$  : We use simple textual tem- plates to format the demos and the test sample in main experiments. For example, the template for NER is  “Sentence: [S], Entities: ([type1], [entity1]), ([type2], [entity2])...\" . ", "page_idx": 3, "bbox": [305, 706.3543090820312, 526, 774.0894775390625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 43, "type": "image", "page_idx": 4, "img_path": "layout_images/2303.08559v2_5.jpg", "img_caption": "Figure 3: LLMs’ performance w.r.t prompt variants on 20-shot FewNERD dataset. See full results on other datasets in Appendix  E.2 -  E.5 .  Left : ChatGPT’s performance (F1 Score) across six instruction variants.  Middle : F1 Score changes over varying numbers of demo.  Right : ChatGPT’s performance across three demo selection strategies. Random: Random sampling. Embed: Sentence embedding. EPR: Efficient Prompt Retriever ( Rubin et al. ,  2022 ). ", "bbox": [70, 67, 526, 236], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "F1 score\n\nN12 13 14\nInstruction format\n\nChatGPT\n-e CODEX\n\n4 8 16 32 64 96\nDemonstration number\n\nF1 score\n\na a\nnN id\nau o\n\nry\na\no\n\na\nN\na\n\nrandom\n\nDemonstration selection\n\nembed\n\nepr\n", "vlm_text": "The image contains three graphs illustrating the performance of ChatGPT and other models on the 20-shot FewNERD dataset:\n\n1. **Left Graph (Instruction Format)**: Displays F1 scores for different instruction formats (I0 to I5). The performance varies across these formats, with some achieving higher scores than others.\n\n2. **Middle Graph (Demonstration Number)**: Shows how the F1 score changes with varying numbers of demonstrations. It compares ChatGPT and Codex, indicating that ChatGPT generally performs better as the number of demonstrations increases.\n\n3. **Right Graph (Demonstration Selection)**: Compares F1 scores for three demo selection strategies: random sampling, sentence embedding (embed), and Efficient Prompt Retriever (EPR). The EPR strategy shows the highest performance, followed by embedding, with random sampling performing the least effectively."}
{"layout": 44, "type": "text", "text": "3.4 Main Results ", "text_level": 1, "page_idx": 4, "bbox": [70, 257, 157, 268], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 45, "type": "text", "text": "We summarize the main experimental outcomes in Figure  2 , indicating that LLMs only outperform SLMs in environments with restricted labels and samples. Conversely, SLMs are generally more effective. Given (1) the practicality of fine-grained IE tasks and the manageable effort of obtaining 10- 20 annotations per label and (2) the excessive time and budget demands of LLM inference, we con- clude that LLMs are not as effective as supervised SLMs for few-shot IE tasks under real scenarios. We detail our findings as below. ", "page_idx": 4, "bbox": [70, 279.24200439453125, 291, 427.8794860839844], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 46, "type": "text", "text": "Performance w.r.t sample number.  The perfor- mance dynamics of SLMs and LLMs are influenced by variations in sample size. Under extremely low- resource (1-shot or 5-shot) settings, LLMs some- times present superior performance than SLMs. Yet, LLMs tend to reach a performance plateau with only modest increases in sample size. Con- versely, SLMs demonstrate marked performance enhancement as sample sizes grow. This trend is evident in Figure  2 , where the SLM trajectories (represented by dashed lines) ascend more steeply compared to the LLM ones (solid lines). ", "page_idx": 4, "bbox": [70, 430.38427734375, 291, 592.9634399414062], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 47, "type": "text", "text": "Performance w.r.t label number.  Compared with SLMs, LLMs tend to struggle on fine-grained datasets. For instance, LLMs perform  relatively worse on MAVEN and RAMS datasets (with 168/139 labels) than on CONLL (4 labels only). Detailed quantitative results are shown in Ap- pendix  E.1 , illustrating a clear negative correlation between the label number and the result disparity between LLMs and SLMs across various IE tasks. ", "page_idx": 4, "bbox": [70, 595.46728515625, 291, 717.3994140625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 48, "type": "text", "text": "Comparisons among LLMs.  We observe perfor- mance variability among LLMs. (1) Open-source models, LLaMA and Vicuna, significantly lag be- hind proprietary LLMs across all few-shot IE tasks. ", "page_idx": 4, "bbox": [70, 719.9033203125, 291, 774.0894775390625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 49, "type": "text", "text": "(2) Among proprietary LLMs, ChatGPT performs better on NER and EAE tasks, but poorer so on RE and ED tasks. Instruct GP T and CODEX demon- strate comparable performance across these tasks. LLMs show limited inference speed.  We compare the inference speed of different methods and show their results in Table  1 . We observe that LLMs is much slower than SLMs since they have much more parameters, longer input contexts and extra response decay (if external APIs applied). ", "page_idx": 4, "bbox": [305, 256.81500244140625, 526, 392.4914855957031], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 50, "type": "text", "text": "3.5 Analysis on Prompt Sensitivity ", "text_level": 1, "page_idx": 4, "bbox": [306, 405, 472, 416], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 51, "type": "text", "text": "Previous work ( Lu et al. ,  2022b ) indicates that the efficacy of LLMs on specific tasks can be signifi- cantly influenced by the construction of the prompt. To ensure that LLMs’ suboptimal outcomes are not erroneously ascribed to inappropriate prompt designs, we meticulously examine the impact of diverse prompt variations from four aspects,  i.e.,  in- struction format, demo number, demo selector and prompt format. We leave comprehensive details of the variants and their results to Appendix  E.2 - E.5 , and illustrate salient findings in Figure  3 . Our findings include that (1) diverse instruction strate- gies yield comparable results in IE task; (2) in- creasing the number of samples in demonstrations does not unequivocally enhance performance; and (3) The selection strategy of demonstration mat- ters, and retrieval based on sentence embedding ", "page_idx": 4, "bbox": [305, 422.41400146484375, 526, 652.347412109375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 52, "type": "table", "page_idx": 4, "img_path": "layout_images/2303.08559v2_6.jpg", "table_caption": "Table 1: The inference seconds over 500 sentences (run on single V100 GPU). Here LLaMA is extremely slow since we set batch size as 1 due to memory limit. ", "bbox": [305, 672, 525, 771], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "Dataset (Task) |Roberta T5 LLaMA CODEX\n\nFewNERD (NER)| 2.8 39.4 1135.4 179.4\nTACREV (RE) 14 45.6 1144.9 151.6\nACEO0S (ED) 66 62.5 733.4 171.7\n", "vlm_text": "The table compares performance metrics for different models across three datasets and tasks:\n\n### Columns\n- **Models:** Roberta, T5, LLAMA, and CODEX.\n\n### Rows\n- **Datasets (Tasks):**\n  - **FewNERD (NER):** Named Entity Recognition\n    - Roberta: 2.8\n    - T5: 39.4\n    - LLAMA: 1135.4\n    - CODEX: 179.4\n  - **TACREV (RE):** Relation Extraction\n    - Roberta: 1.4\n    - T5: 45.6\n    - LLAMA: 1144.9\n    - CODEX: 151.6\n  - **ACE05 (ED):** Event Detection\n    - Roberta: 6.6\n    - T5: 62.5\n    - LLAMA: 733.4\n    - CODEX: 171.7\n\nThe numbers likely represent performance scores or times, but the specific unit isn't provided."}
{"layout": 53, "type": "text", "text": "(what we used) proves sufficiently effective. Con- sequently, we believe that there unlikely exists a lottery  prompt that substantially alters our conclu- sions that LLMs are not good few-shot IE solver. ", "page_idx": 5, "bbox": [70, 71.74500274658203, 290, 125.53848266601562], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 54, "type": "text", "text": "3.6 Discussion: Why LLMs Fail to Obtain Satisfactory Performance on IE Tasks? ", "text_level": 1, "page_idx": 5, "bbox": [70, 136, 278, 163], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 55, "type": "text", "text": "Under utilized Annotations.  We notice that LLMs appear to benefit less from additional annotations, i.e.,  more training samples and label types, than SLMs. We speculate that LLMs are constrained by ICL in two ways. (1) More samples: The num- ber of effective samples for LLMs, those in de- mos, is limited by maximum input length. More- over, we also observe LLMs’ performance plateaus in some tasks before reaching this limit (see Ap- pendix  E.3 ). Meanwhile, SLMs can continually learn from more samples through supervised learn- ing, widening the performance gap as annotated samples increase. (2) More labels: LLMs struggle with fine-grained datasets. It suggests a difficulty in understanding numerous labels and their subtle interactions merely from the given instruction and exemplars for LLMs. Also, the examples per label in demos decrease as label types increase. ", "page_idx": 5, "bbox": [70, 168.33030700683594, 290, 412.2054748535156], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 56, "type": "text", "text": "Unexplored Task format.  As stated in  Zhang et al.  ( 2023a ), IE-related tasks are scarce in the widely-used instruction tuning datasets like  Wei et al.  ( 2022a ) and  Wang et al.  ( 2022 ). Furthermore, the highly-flexible format of NER and ED tasks impair the ICL abilities   7 . Therefore it is likely that instruction-tuned LLMs are not well-acquainted with such IE-related task formats. ", "page_idx": 5, "bbox": [70, 412.75128173828125, 290, 521.1343994140625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 57, "type": "text", "text": "4 LLMs are Good Few-shot Reranker 4.1 Filter-then-rerank Paradigm ", "text_level": 1, "page_idx": 5, "bbox": [69, 533, 273, 569], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 58, "type": "image", "page_idx": 5, "img_path": "layout_images/2303.08559v2_7.jpg", "bbox": [82, 585, 277, 720], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "Read following sentences and identify what is the entity type\nof ‘The New Yorker” quoted by <t>.\n\nSentence:\n\nIn 2004 Gourevitch was assigned to cover the 2004 U.S.\npresidential election for “<t> The New Yorker <t>”.\n\nCandidate Choices:\n\n(a)The New Yorker does not belong to any known entities.\n(b)The New Yorker is a broadcast program.\n\n(c)The New Yorker is a kind of written art.\n\n(d)The New Yorker is a media/newspaper organization.\nAnalysis:\n\nThe New Yorker is a well-known American magazine that has\nbeen published since 1925, and is primarily known for its\nlong-form journalism, commentary, and satire. It has a\nreputation for publishing high-quality writing on a wide\nvariety of topics, including politics, culture, and the arts.\nSo The New Yorker is a media/newspaper organization.\n\nCorrect Answer: (d)\n\nyy,\n\n", "vlm_text": "The image shows a text box with instructions to identify the entity type of \"The New Yorker.\" It includes the sentence:\n\n\"In 2004 Gourevitch was assigned to cover the 2004 U.S. presidential election for '<t> The New Yorker <t>'.\"\n\nCandidate choices are:\n(a) The New Yorker does not belong to any known entities.\n(b) The New Yorker is a broadcast program.\n(c) The New Yorker is a kind of written art.\n(d) The New Yorker is a media/newspaper organization.\n\nThe analysis provided explains that The New Yorker is a well-known American magazine known for journalism, commentary, and satire, and concludes that it is a media/newspaper organization. The correct answer is (d)."}
{"layout": 59, "type": "text", "text": "Figure 4: Multi-choice question (MCQ) prompt. ", "page_idx": 5, "bbox": [83.677001953125, 729.4595947265625, 276.32379150390625, 741.4645385742188], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 60, "type": "text", "text": "To mitigate LLMs’ drawbacks mentioned above, we propose a  filter-then-rerank  paradigm to inte- grate both SLMs and LLMs within the same system. This paradigm uses SLMs as filters to select the top-  $\\cdot N$   candidate labels, then LLMs rerank them to make final decisions. By using SLM-generated candidate answers, the focus of LLMs shifts from sentence-level  ( i.e.,  identifying all entities/events in the sentence) to  sample-level  ( i.e.,  determin- ing single entity/event candidate provided). Each question now corresponds to a single sample, al- lowing us to reframe prompts as multi-choice ques- tions (MCQ; shown in Figure  4 ) problem. Un- der such format, each candidate label is converted to a choice by pre-defined templates. We claim filter-then-rerank  paradigm is more likely to elicit the powers of LLMs and smoothly solve few-shot IE tasks because: (1) LLMs are more familiar with MCQ prompts than IE-format prompts ( Zhang et al. ,  2023a ). (2) This paradigm reduces the la- bel scopes significantly, since    $N$   is usually much smaller than fine-grained label numbers. ", "page_idx": 5, "bbox": [305, 71.74500274658203, 526, 369.4234619140625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 61, "type": "text", "text": "4.2 LLMs are  Hard  Sample Solver ", "text_level": 1, "page_idx": 5, "bbox": [305, 382, 474, 395], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 62, "type": "text", "text": "Our  filter-then-rerank  paradigm, unfortunately, presents unsatisfactory performance (and even suf- fers longer latency since LLMs rerank candidates per sample). Given LLMs’ abilities in memoriza- tion and reasoning, however, we still believe that LLMs are potential to solve  some , if not most, IE samples effectively. We hypothesize that LLMs are more proficient than SLMs on  hard  samples. These samples are characterized by their requisite for external knowledge acquisition or sophisticated reasoning strategies, areas where LLMs can lever- age their extensive parametric knowledge bases and inherent reasoning mechanisms. In contrast, SLMs often falter with such samples, constrained by their restricted modeling capacities. ", "page_idx": 5, "bbox": [305, 401.8869934082031, 526, 604.721435546875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 63, "type": "text", "text": "We leverage an unsupervised metric from SLMs to evaluate the  difficulty  of samples. Given a sample  $x$   in the sentence    $s$  , we define the highest probabil- ity across all labels as the confidence score: ", "page_idx": 5, "bbox": [305, 606.1799926757812, 526, 659.972412109375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 64, "type": "equation", "text": "\n$$\n\\operatorname{conf}(x)=\\operatorname*{max}_{l\\in L}P_{S L M}(l|x;s)\n$$\n ", "text_format": "latex", "page_idx": 5, "bbox": [348, 673, 482, 694], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 65, "type": "text", "text": "where  $L$   denotes the label set and    $P_{S L M}(l|x;s)$   the probability of a span    $x$   (in the sentence    $s$  ) referring to label    $l$   computed by SLMs. We classify sam- ples with low confidence scores as  hard  samples. Otherwise we view them as easy samples. ", "page_idx": 5, "bbox": [305, 706, 526, 774.0894775390625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 66, "type": "image", "page_idx": 6, "img_path": "layout_images/2303.08559v2_8.jpg", "img_caption": "Figure 5: Relationship between confidence scores and performance with/without LLM reranking. We adopt RoBERTa-large  as filter and Instruct GP T as reranker. ", "bbox": [70, 70, 290, 339], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "Ss\ns\n\n(iwo. LLM reranking\n\nFewNERD (NER)\n\nfli w. LLM reranking\n\na\n\nMicro-F1\na\ns\n\nMicro-F1\na\nSs\n\nyp\n&.\n\nMicro-F1\n\nof\n\noF of\n\nConfidence Score\n\nTACREV (RE)\n\not\n\no®\n\noF oo\n\nConfidence Score\n\nACEOS (ED)\n\niy\n\noF oo\nConfidence Score\n\n9.\n\n", "vlm_text": "The image contains three line graphs comparing the relationship between confidence scores and micro-F1 performance, with and without LLM reranking using RoBERTa-large as a filter and Instruct GPT as a reranker. Each graph represents a different dataset or task:\n\n1. **FewNERD (NER)**: The graph shows performance across different confidence scores, with and without reranking.\n2. **TACREV (RE)**: Similar comparison for the TACREV dataset.\n3. **ACE05 (ED)**: Performance comparison for the ACE05 dataset.\n\nThe graphs indicate how LLM reranking impacts micro-F1 scores at various confidence levels. The color coding is as follows: pink represents performance without LLM reranking, and blue represents performance with LLM reranking."}
{"layout": 67, "type": "text", "text": "We conduct experiments to confirm our hypoth- esis that LLMs excel on  hard  samples. We group samples by confidence scores and compare two methods within each group: (a) SLM-based meth- ods without LLM reranking, and (b) SLMs as the filter and LLMs as the reranker. Method (b) dif- fers from (a) by adding a single LLM to rerank the top-  $N$   SLM predictions, using MCQ prompts. ", "page_idx": 6, "bbox": [70, 362.2599792480469, 291, 470.25048828125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 68, "type": "text", "text": "The results in Figure  5  substantiate our assump- tion. (1) LLM-based reranking (blue lines) en- hances performance on hard samples (left areas in the figure). We provide a detailed analysis of spe- cific challenging instances where LLM rerankers prove advantageous in Appendix  F.1 . These in- stances demonstrate the efficacy of LLMs in har- nessing external knowledge and complex reason- ing to rectify erroneous predictions initially made by SLMs (red lines). (2) Conversely, LLM-based reranking impedes performance on easy samples (right areas), resulting in a significant degradation, particularly for very easy samples (rightmost areas). In conclusion, LLMs exhibit greater proficiency in handling hard samples compared to SLMs, yet they under perform relative to SLMs on easy samples. ", "page_idx": 6, "bbox": [70, 471.6679992675781, 291, 688.0514526367188], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 69, "type": "text", "text": "4.3 Why LLMs Fail on Easy Samples ", "text_level": 1, "page_idx": 6, "bbox": [70, 701, 252, 714], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 70, "type": "text", "text": "We investigate why LLMs (relatively) fail on easy samples in this section. As shown in Table  2 , we observe significant higher negative sample ratios for easy samples across diverse IE tasks. In other Table 2: Comparative ratios of negative to positive sam- ples across various datasets and subsets. We set fixed threshold  $\\tau$  here for simplicity. ", "page_idx": 6, "bbox": [70, 720.2960205078125, 291, 774.0894775390625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 71, "type": "text", "text": "", "page_idx": 6, "bbox": [305, 70.029541015625, 526, 105.94451141357422], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 72, "type": "table", "page_idx": 6, "img_path": "layout_images/2303.08559v2_9.jpg", "bbox": [310, 114, 521, 170], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "FewNERD TACREV ACE05\n\nOverall 5.88 3.03 38.2\nEasy samples (t > 0.9) 9.44 B21 44.0\nHard samples (7 < 0.6) 1.28 2.68 1.36\n", "vlm_text": "The table displays performance metrics for three datasets: FewNERD, TACREV, and ACE05. It has three rows indicating different sample conditions:\n\n1. **Overall**: \n   - FewNERD: 5.88\n   - TACREV: 3.03\n   - ACE05: 38.2\n\n2. **Easy samples (τ > 0.9)**:\n   - FewNERD: 9.44\n   - TACREV: 3.21\n   - ACE05: 44.0\n   \n3. **Hard samples (τ < 0.6)**:\n   - FewNERD: 1.28\n   - TACREV: 2.68\n   - ACE05: 1.36\n\nThe table compares results for different difficulty levels of samples within each dataset."}
{"layout": 73, "type": "text", "text": "words, most negative samples are easy samples for SLMs. Here we refer negative samples to those labeled as  None . We speculate that the proficiency of SLMs with negative samples stems from their ability to adeptly discern apparent patterns during the fine-tuning stages. Therefore, SLMs could pre- dict negative samples with (relatively) high confi- dence and accuracy. Due to LLMs’ predisposition to false-positive predictions on negative samples, however, the performance of LLMs on easy sam- ples collapses. We attribute such false-positive pre- dictions to (1) hallucination and (2) span boundary mismatch. We detail such two kinds of mistakes with cases in Appendix  F.2 . ", "page_idx": 6, "bbox": [305, 189.6809844970703, 526, 378.9664611816406], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 74, "type": "text", "text": "5 Adaptive Filter-then-rerank Paradigm ", "text_level": 1, "page_idx": 6, "bbox": [305, 390, 521, 402], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 75, "type": "text", "text": "Above findings can be summarized as: (1) SLMs generally outperform LLMs, especially with more training samples and fine-grained labels. (2) SLMs are much more time- and cost-efficient. (3) LLMs serve as powerful rerankers on  hard  samples that challenge SLMs. Based on them, we propose a simple, efficient, and effective adaptive reranker that combines the strengths of SLMs and LLMs. ", "page_idx": 6, "bbox": [305, 410.3680114746094, 526, 518.357421875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 76, "type": "text", "text": "5.1 Method ", "text_level": 1, "page_idx": 6, "bbox": [306, 527, 367, 540], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 77, "type": "text", "text": "Our  adaptive filter-then-rerank  approach, shown in Figure  6 , uses supervised SLMs as a filter to make preliminary decisions. Samples with confi- dence scores exceeding threshold are viewed as easy samples otherwise hard ones. For easy sam- ples, we retain SLM predictions as final results. For hard samples, top-  $\\cdot N$   predictions from SLMs are reranked via LLMs using ICL. Here LLMs employ MCQ prompts (Figure  4 ), containing demos and a sample to be reranked. The LLMs then generate the final answer and optionally provide an explanation. ", "page_idx": 6, "bbox": [305, 545.0079956054688, 526, 693.6454467773438], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 78, "type": "text", "text": "5.2 Experimental Setup ", "text_level": 1, "page_idx": 6, "bbox": [305, 703, 424, 716], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 79, "type": "text", "text": "We conduct experiments on FewNERD for NER task, TACREV for RE task and ACE05 for ED task. We employ top-performing SLM-based meth- ods from Section  3  (FSLS or KnowPrompt) as the ", "page_idx": 6, "bbox": [305, 720.2960205078125, 526, 774.0894775390625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 80, "type": "image", "page_idx": 7, "img_path": "layout_images/2303.08559v2_10.jpg", "img_caption": "Figure 6: The overall architecture of our adaptive  filter-then-rerank  paradigm. We color easy samples in orange and hard samples in pink. For easy samples, the final predictions are exactly from the SLM-based methods. For hard samples, the top-  $N$   predictions from SLMs are fed into LLMs as the format of multiple-choice questions (pink box). The question is paired with demos (green box). LLMs rerank these  $N$   candidates and generate the final prediction. ", "bbox": [69, 69, 527, 314], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "talian! Adrien said he met the\n\nBaptists’ leader, Laura Silsby\nof Meridian, Idaho, in Port-au-\nPrince on Jan 26.\n\nEasy Sample J Hard Sample\n\nFilter\nSmall LM\n\nper: cities_of_residence\n\nAdrien said he met the Baptists’ leader, Laura Silsby of\n\nMeridian, Idaho, in Port-au-Prince on Jan 26.\n(a)Laura Silsby lives in the state or province Meridian\n(b)Laura Silsby lives in the city Meridian\n\n(c)Laura Silsby was born in the city Meridian\n(d)Laura Silsby has no known relations to Meridian\n\nAnalysis: 97\n\nReranker\nLarge LM\n\nDemonstration\n\nThe lawyer denied Italian news reports that she\nwept while addressing the court, but said Knox was\nupset as she recounted the pressure, the\naggressiveness of the police who called her a liar.\n(a)she is the other family member of lawyer\n\n(b)she is a lawyer\n\n(c)she has no known relations to lawyer\n\nAnalysis: The word ‘she' refers to someone who was\nupset while recounting certain events in court. The\nword ‘lawyer’ refers to someone who denied a news\nreport about that same person weeping in court.\nThere is no information in the sentence to indicate\nthat the two individuals are related in any way.\n\n‘Answer: (c)\n\nThe sentence implies that Laura Silsby is associated with the city of Meridian in the state of Idaho, and\n\ndoes not provide information about her birthplace. So Laura Silsby lives in the city Meridian.\n\nAnswer: (b)\n", "vlm_text": "The image illustrates the architecture of an adaptive filter-then-rerank paradigm. Here's a breakdown:\n\n1. **Easy and Hard Samples**: \n   - Easy samples are colored in orange.\n   - Hard samples are colored in pink.\n\n2. **Processing Flow**:\n   - Easy samples: The final predictions come directly from Small Language Models (SLM).\n   - Hard samples: The top-N predictions from SLMs are converted into multiple-choice questions. These questions, paired with demonstrations, are then fed into Large Language Models (LLMs) for reranking.\n\n3. **Components**:\n   - **Filter (Small LM)**: Processes both easy and hard samples.\n   - **Reranker (Large LM)**: Handles hard samples by reranking predictions and generating the final answer.\n\n4. **Example Details**:\n   - The example question involves determining the relationship of \"Laura Silsby\" with \"Meridian.\"\n   - A demonstration provides a similar analysis to guide the reranking process.\n\nThis approach aims to leverage both SLMs for efficiency and LLMs for accuracy on more complex tasks."}
{"layout": 81, "type": "text", "text": "filter, and Vicuna-13B, Instruct GP T or GPT-4 as the reranker. The threshold    $\\tau$   to determine sam- ple difficulty is optimized on the valid set. For hard sample, the top-3 SLM predictions and  None (if not included) are feed to LLMs for reranking. Each LLM prompt has 4-shot demos. See demo examples in Appendix  G.1 . We follow templates in  Lu et al.  ( 2022a ) for TACREV and carefully de- sign others. See these templates in Appendix  G.2 . We adopt chain-of-thought reasoning ( Wei et al. , 2022b ),  i.e.,  prefacing the answer with an explana- tion, to facilitate LLMs’ reranking procedure. ", "page_idx": 7, "bbox": [70, 334.9849853515625, 291, 497.1714782714844], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 82, "type": "text", "text": "Baseline  We compare our method with two kinds of baselines to validate its effectiveness. ", "page_idx": 7, "bbox": [70, 497.978271484375, 291, 525.0654296875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 83, "type": "text", "text": "(1) LLMs with ICL: We follow the prompts in Sec- tion  3.3  and conduct experiments on three LLMs. (2) Supervised SLMs: We follow previous SoTA methods shown in Section  3.4  (FSLS or Know- Prompt). We additionally combine two SLMs with ensemble or reranking approach ( i.e.,  replace the LLM with another SLM as the reranker) to verify that improvements from our SLM-LLM integrated system are not solely due to the ensemble effects. ", "page_idx": 7, "bbox": [70, 526.2649536132812, 291, 648.5984497070312], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 84, "type": "text", "text": "5.3 Main Results ", "text_level": 1, "page_idx": 7, "bbox": [70, 661, 157, 673], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 85, "type": "text", "text": "Table  3  shows that our  filter-then-rerank  method consistently improves performance across three datasets and nine settings. For instance, with In- structGPT, reranking provides an average F1 gain of   $2.4\\%$   without SLM ensemble (Lines 4 vs. 7). Based on ensemble SLMs as the filter, our method still achieves  $2.1\\%$   (Lines 5 vs. 8) gains on av- erage. This confirms (1) the effectiveness of the LLM reranking and (2) its gains are different and (almost) orthogonal to the SLM ensemble. ", "page_idx": 7, "bbox": [70, 679.6489868164062, 291, 774.0894775390625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 86, "type": "text", "text": "", "page_idx": 7, "bbox": [305, 334.9849853515625, 526, 375.2294616699219], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 87, "type": "text", "text": "5.4 Analysis ", "text_level": 1, "page_idx": 7, "bbox": [305, 387, 370, 399], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 88, "type": "text", "text": "Few makes big difference  Our method selectively reranks hard samples. Table  4  shows that (1) only a minor fraction   $(0.5\\%{\\sim}10\\%)$   of samples are deemed hard and are reranked by LLMs. (2) Despite their limited quantity, reranking results in a substantial performance boost on these samples (  $(10\\%{\\sim}25\\%$  absolute F1 gains). This uplift on a small subset significantly enhances the overall performance. ", "page_idx": 7, "bbox": [305, 404.7232666015625, 526, 513.1064453125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 89, "type": "text", "text": "GPT-4 is more aggressive  From Tables  3  and  4 , GPT-4 generally improves more on hard samples, yet Instruct GP T surpasses GPT-4 in NER and RE tasks when evaluated overall. This discrepancy arises from GPT-4’s aggressive reranking which introduces more true positives. Instruct GP T, how- ever, focuses more on reducing false positives. ", "page_idx": 7, "bbox": [305, 513.6992797851562, 526, 608.532470703125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 90, "type": "text", "text": "Few makes small cost  Figure  7  demonstrates that our method impressively reduces budget and la- tency by approximately   $80\\%{\\sim}90\\%$   compared to direct ICL. This reduction is due to (1) fewer LLM callings (only for hard samples) and (2) shorter prompts (fewer candidate labels and demos). ", "page_idx": 7, "bbox": [305, 609.1253051757812, 526, 690.409423828125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 91, "type": "text", "text": "5.5 Ablation Study ", "text_level": 1, "page_idx": 7, "bbox": [306, 702, 401, 714], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 92, "type": "text", "text": "We investigate the effectiveness of the modules in adaptive  filter-then-rerank  system by removing each of them in turn: (1)  CoT : We exclude the explantion for each examples in demo. (2)  Demo : ", "page_idx": 7, "bbox": [305, 720.2960205078125, 526, 774.7440795898438], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 93, "type": "table", "page_idx": 8, "img_path": "layout_images/2303.08559v2_11.jpg", "table_caption": "Table 3: Overall results of LLM-based ICL methods, SLM-based supervised methods, and our proposed  filter-then- rerank    $(\\mathrm{SLM+LLM})$   methods. The best results are in bold face and the second best are underlined. All results except Instruct GP T and GPT-4 are averaged over 5 runs, and sample standard deviations are in the round bracket. ", "bbox": [70, 69, 526, 310], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "FewNERD (NER) TACREV (RE) ACE (ED)\n\n5-shot 10-shot 20-shot | 20-shot 50-shot 100-shot} 5-shot 10-shot 20-shot\n\nCODEX 53.8(0.5) 54.0(1.4) 55.9(0.5)|59.1(1.4) 60.3(2.4) 62.4(2.6)|47.1(1.2) 47.7(2.8) 47.9(0.5)\n\n4 InstructGPT 53.6(-) 54.6(-) 57.2(-) | 60.1(-) 58.3(-) 62.7(-) | 52.9(-) 52.1(-) 49.3(-)\n\n3 GPT-4 - - 57.8(-) - - 59.3(-) - - 52.1(-)\n\ns Previous SoTA 59.4.5) 61.4(0.8) 61.9(1.2)|62.4(3.8) 68.5(1.6) 72.6(1.5)|55.1(4.6) 63.9(0.8) 65.8(2.0)\n\n4 + Ensemble (S) 59.6.7) 61.8(1.2) 62.6(1.0)|64.9(1.5) 71.9(2.2) 74.1(1.7)|56.9(4.7) 64.2(2.1) 66.5(1.7)\n\nM+ Rerank (S) 59.4.5) 61.0(1.7) 61.5(1.7)|64.2(2.3) 70.8(2.3) 74.3(2.2)|56.1(0.3) 64.0(1.0) 66.7(1.7)\nVicuna-13B\n\n+ Rerank (L) 60.0(2.8) 61.9(2.1) 62.2(1.4)|65.2(1.4) 70.8(1.6) 73.8(1.7)|56.9(4.0) 63.5(2.7) 66.0(2.6)\n\n= +Ensemble (S) + Rerank (L) |59.9(0.7) 62.1(0.7) 62.8(1.1)|66.5(0.5) 73.6(1.4) 75.0(1.5)|57.9(5.2) 64.4(1.2) 66.2(2.4)\na EATS\n\n+  +Rerank (L) 60.6(2.1) 62.7(0.8) 63.3(0.6)|66.8(2.6) 72.3(1.4) 75.4(1.5)|57.8(4.6) 65.3(1.7) 67.3(2.2)\n\n= +Ensemble (S) + Rerank (L) |61.3(1.9) 63.2(0.9) 63.7(1.8)|68.9(1.3) 74.8(1.3) 76.8(1.2)|59.5(3.7) 65.3(1.9) 67.8(2.1)\n\nz GPT-4\n+ Rerank (L) 60.8(2.3) 62.6 (2.7) 63.0(1.3)|65.9(2.7) 72.3(0.3) 74.5(1.5)|59.6(2.9) 64.9(2.5) 67.1(2.5)\n+ Ensemble (S) + Rerank (L) |61.1(2.2) 62.8(0.9) 63.6(1.2)|68.6(1.3) 73.9(1.4) 75.9(2.4)|60.9(3.9) 65.6(1.5) 67.8(1.7)\n\n", "vlm_text": "The table compares the performance of different language models on various NLP tasks: FewNERD (Named Entity Recognition), TACREV (Relation Extraction), and ACE (Event Detection). The tasks are evaluated with varying numbers of \"shots\" (examples): 5, 10, and 20 for FewNERD and ACE, and 20, 50, and 100 for TACREV.\n\nHere are the key components:\n\n1. **Models Evaluated:**\n   - LLM (Large Language Models): CODEX, InstructGPT, GPT-4\n   - SLM (Small Language Models) and their combinations with LLMs\n\n2. **Performance Metrics:**\n   - Results are presented in various configurations such as \"Previous SoTA,\" \"+ Ensemble (S),\" \"+ Rerank (S),\" and combinations of SLM with LLM.\n   - Numbers in parentheses represent standard deviations.\n\n3. **Models & Tasks:**\n   - Performance is shown for each task under different shot conditions.\n   - Models are combined in various ways to enhance performance.\n\nThe table provides insights into how different models and strategies perform across tasks, highlighting improvements with combinations."}
{"layout": 94, "type": "table", "page_idx": 8, "img_path": "layout_images/2303.08559v2_12.jpg", "table_caption": "Table 4: The F1-score differences before and after reranking on the reranked samples, as well as their pro- portion of the total samples. ", "bbox": [70, 330, 292, 438], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "GPT-4\nbefore after A ratio\n\n31.9 40.7 8.8 3.2%\n25.3 43.0 17.7 9.1%\n31.1 57.9 26.8 1.6%\n\nInstructGPT\nbefore after A ratio\n\n31.4 28.3 —3.1 3.3%\n33.8 43.4 9.6 7.1%\n35.6 55.7 20.1 0.5%\n\nFewNER\nTACREV\nACEOS\n\n", "vlm_text": "The table compares the performance of GPT-4 and InstructGPT on three tasks: FewNER, TACREV, and ACE05. For each model, it shows the performance \"before\" and \"after\" a certain intervention, along with the change (∆) and ratio.\n\n### GPT-4:\n- **FewNER**: \n  - Before: 31.9\n  - After: 40.7\n  - ∆: 8.8\n  - Ratio: 3.2%\n\n- **TACREV**: \n  - Before: 25.3\n  - After: 43.0\n  - ∆: 17.7\n  - Ratio: 9.1%\n\n- **ACE05**: \n  - Before: 31.1\n  - After: 57.9\n  - ∆: 26.8\n  - Ratio: 1.6%\n\n### InstructGPT:\n- **FewNER**: \n  - Before: 31.4\n  - After: 28.3\n  - ∆: -3.1\n  - Ratio: 3.3%\n\n- **TACREV**: \n  - Before: 33.8\n  - After: 43.4\n  - ∆: 9.6\n  - Ratio: 7.1%\n\n- **ACE05**: \n  - Before: 35.6\n  - After: 55.7\n  - ∆: 20.1\n  - Ratio: 0.5%"}
{"layout": 95, "type": "text", "text": "We remove all examples, rendering the reranking a zero-shot problem. (3)  LF  (label filtering): We retain all labels as candidate choices for reranking, instead of only the top-  $\\cdot N$   labels from the SLMs. (4)  AD  (adaptive): We feed all samples, not just hard ones, to the LLMs. ", "page_idx": 8, "bbox": [70, 463.4150085449219, 291, 544.3064575195312], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 96, "type": "text", "text": "We show their results in Table  5  and see that (1) Demos with explanations consistently enhance the reranking ability of LLMs across all datasets. (2) Demos without explanations also contribute to performance improvement. (3) Label filtering re- sults in gains and notably reduces the demo length, ", "page_idx": 8, "bbox": [70, 546.4039916992188, 291, 627.29541015625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 97, "type": "image", "page_idx": 8, "img_path": "layout_images/2303.08559v2_13.jpg", "img_caption": "Figure 7: The financial and time cost over 500 sentences. Instruct GP T as the reranker. ", "bbox": [70, 650, 291, 770], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "Direct ICL (InstructGPT) Filter-then-rerank [lJ Fine-tuning (ROBERTa-large)\n\n150\n\n3\n8\n\nsecond(s)\n\ng\n\n° =\nFewNERD TACREV  ACEOS FewNERD\nFinancial cost\n\n“TACREV_ACEOS\nTime cost\n\n", "vlm_text": "The image contains two bar charts comparing financial and time costs over 500 sentences for different approaches using \"InstructGPT\" for reranking:\n\n1. **Financial Cost** (left chart):\n   - **Direct ICL (InstructGPT)** has the highest cost across all datasets (FewNERD, TACREV, ACE05).\n   - **Filter–then–rerank** has a moderate cost.\n   - **Fine-tuning (RoBERTa-large)** has the lowest cost.\n\n2. **Time Cost** (right chart):\n   - **Direct ICL (InstructGPT)** takes the most time across all datasets.\n   - **Filter–then–rerank** takes moderate time.\n   - **Fine-tuning (RoBERTa-large)** is the fastest.\n\nEach dataset (FewNERD, TACREV, ACE05) shows similar trends in both financial and time costs."}
{"layout": 98, "type": "table", "page_idx": 8, "img_path": "layout_images/2303.08559v2_14.jpg", "table_caption": "Table 5: Ablation study on three datasets. The filter is ensembled SLMs and the reranker is GPT-4. ", "bbox": [305, 330, 525, 466], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "FewNERD TACREV ACE05\nCoT Demo LF AD | (20-shot) (100-shot) (20-shot)\n\nv Yo vo Vo | 63.6.2) 75.9(2.4)  67.8(1.7)\n\nx rn An A 63.2(1.2) 75.4(2.4) 67.2(1.7)\nx x Vv Vv 63.00.4)  74.9(2. >» 66.6(1.5)\nx x xX Vv 62.4(2.1) 73.8(2.5) 66.5(1.3)\nx x KX x 12.5(2.7) 59.9¢6. : 5.4(1.1)\n\nPrevious SoTA methods | 62.6(1.0) 74.1(1.7) 66.5(1.7)\n\n", "vlm_text": "The table compares different methods using checkmarks (✓) for four settings: CoT, Demo, LF, and AD. It presents results for three datasets: FewNERD (20-shot), TACREV (100-shot), and ACE05 (20-shot). The values are in the format \"score (std)\", representing performance scores and standard deviations, compared against previous state-of-the-art (SoTA) methods.\n\nHere's a summary:\n- When all settings are used (✓), the performance is highest across all datasets.\n- Removing combinations of settings (×) decreases performance.\n- The previous SoTA methods have slightly lower scores for each dataset compared to the best proposed method using all settings."}
{"layout": 99, "type": "text", "text": "hence cutting inference costs. (4) The performance collapses without a filter to identify sample diffi- culty, reiterating the need for an integrated SLM- LLM system to complement each other. ", "page_idx": 8, "bbox": [305, 486.57098388671875, 526, 540.3644409179688], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 100, "type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 8, "bbox": [305, 551, 381, 563], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 101, "type": "text", "text": "Through an extensive empirical study on nine datasets spanning four IE tasks, we find that LLMs, despite their superiority in extreme low-resource scenarios, are not effective few-shot information extractors in general. They struggle with IE-related prompts, have limited demonstration capacity, and incur high inference costs. However, LLMs signifi- cantly improve the performance on  hard  samples when combined with SLM. Building on these in- sights, we propose an adaptive  filter-then-rerank paradigm to leverage the strengths of SLMs and LLMs and mitigate their limitations. This approach consistently achieves promising results, with an av- erage  $2.4\\%$   F1 gain across multiple few-shot IE tasks, while minimizing latency and budget costs. ", "page_idx": 8, "bbox": [305, 571.2550048828125, 526, 774.0894775390625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 102, "type": "text", "text": "Limitations ", "text_level": 1, "page_idx": 9, "bbox": [70, 71, 130, 83], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 103, "type": "text", "text": "We do work hard to find better prompts to elicit the power of LLMs on few-shot IE tasks in Section  3.5 , by exploring various kinds of LLMs, demonstra- tion strategies and prompt formats. We find that dif- ferent prompt variants do not significantly impact in-context learning abilities. As an empirical study, we acknowledge the potential existence of a  lottery prompt superior to our explored prompts. However, it seems unlikely that an improved prompt would substantially alter our conclusions. ", "page_idx": 9, "bbox": [70, 95.12299346923828, 290, 230.21145629882812], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 104, "type": "text", "text": "Another common risk when evaluating LLMs on public benchmark is their potential memoriza- tion of samples tested. To mitigate such poten- tial contamination, we use earlier and stable ver- sions of these models rather than the newer and updated ones (for example,    $\\mathsf{g p t\\!-\\!4\\!-\\!\\!0314}$   instead of gpt-4 ). Even if such contamination makes abilities of LLMs overestimated, our primary conclusions remain unchanged because we find that LLMs are NOT  good few-shot information extractors. ", "page_idx": 9, "bbox": [70, 231.61000061035156, 290, 367.35302734375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 105, "type": "text", "text": "Regarding our adaptive filter-then-rerank paradigm, a key limitation lies in how to assess sample difficulty. In this work, we employ a simple unsupervised metric,  i.e.,  the maximum probabilities from SLMs. This is predicated on the assumption that SLMs are well-calibrated ( Guo et al. ,  2017 ). However, it is an obviously imperfect assumption. We envision that calibrating SLMs- based filters or developing an advanced difficulty metric could substantially enhance LLM rerankers’ performance. We leave them for future work. ", "page_idx": 9, "bbox": [70, 368.09698486328125, 290, 516.7344360351562], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 106, "type": "text", "text": "Ac knowle ge ment ", "text_level": 1, "page_idx": 9, "bbox": [70, 531, 158, 543], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 107, "type": "text", "text": "This study is supported under the RIE2020 In- dustry Alignment Fund – Industry Collaboration Projects (IAF-ICP) Funding Initiative, the Singa- pore Ministry of Education (MOE) Academic Re- search Fund (AcRF) Tier 1 grant, as well as cash and in-kind contribution from the industry part- ner(s). ", "page_idx": 9, "bbox": [70, 554.1780395507812, 290, 648.618408203125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 108, "type": "text", "text": "References ", "text_level": 1, "page_idx": 9, "bbox": [70, 675, 126, 686], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 109, "type": "text", "text": "Monica Agrawal, Stefan Hegselmann, Hunter Lang, Yoon Kim, and David Sontag. 2022.  Large language models are few-shot clinical information extractors . In  Proceedings of the 2022 Conference on Empiri- cal Methods in Natural Language Processing , pages 1998–2022, Abu Dhabi, United Arab Emirates. Asso- ciation for Computational Linguistics. ", "page_idx": 9, "bbox": [70, 696.0645751953125, 290, 773.8235473632812], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 110, "type": "text", "text": "Christoph Alt, Aleksandra Gabryszak, and Leonhard Hennig. 2020.  TACRED revisited: A thorough eval- uation of the TACRED relation extraction task . In Proceedings of the 58th Annual Meeting of the Asso- ciation for Computational Linguistics , pages 1558– 1569, Online. Association for Computational Linguis- tics. ", "page_idx": 9, "bbox": [306, 72.61956787109375, 526, 150.37852478027344], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 111, "type": "text", "text": "Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neel a kant an, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020.  Language models are few-shot learners . In  Ad- vances in Neural Information Processing Systems 33: Annual Conference on Neural Information Process- ing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual . ", "page_idx": 9, "bbox": [306, 160.048583984375, 526, 325.478515625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 112, "type": "text", "text": "Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Pondé de Oliveira Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sas- try, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cum- mings, Matthias Plappert, Fotios Chantzis, Eliza- beth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021.  Evaluat- ing large language models trained on code .  ArXiv preprint , abs/2107.03374. ", "page_idx": 9, "bbox": [306, 335.1485595703125, 526, 566.3314819335938], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 113, "type": "text", "text": "Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. 2020.  A simple framework for contrastive learning of visual representations . In  Pro- ceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Vir- tual Event , volume 119 of  Proceedings of Machine Learning Research , pages 1597–1607. PMLR. ", "page_idx": 9, "bbox": [306, 576.0015869140625, 526, 653.760498046875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 114, "type": "text", "text": "Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W. Cohen. 2022a. Program of thoughts prompting: Disentangling computation from reason- ing for numerical reasoning tasks . ", "page_idx": 9, "bbox": [306, 663.4305419921875, 526, 708.3125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 115, "type": "text", "text": "Xiang Chen, Ningyu Zhang, Xin Xie, Shumin Deng, Yunzhi Yao, Chuanqi Tan, Fei Huang, Luo Si, and Huajun Chen. 2022b.  Knowprompt: Knowledge- aware prompt-tuning with synergistic optimization for relation extraction . In  WWW ’22: The ACM Web ", "page_idx": 9, "bbox": [306, 717.9825439453125, 526, 773.8235473632812], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 116, "type": "text", "text": "Conference 2022, Virtual Event, Lyon, France, April 25 - 29, 2022 , pages 2778–2788. ACM. ", "page_idx": 10, "bbox": [81, 72.61956787109375, 290, 95.58348846435547], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 117, "type": "text", "text": "Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. 2023.  Vicuna: An open- source chatbot impressing gpt-4 with  $90\\%^{*}$   chatgpt quality . ", "page_idx": 10, "bbox": [70, 105.25457763671875, 290, 172.0535125732422], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 118, "type": "text", "text": "Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsv yash chen ko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vin- odkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Micha lewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, An- drew M. Dai, Than u malayan Sankara narayana Pil- lai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022.  Palm: Scaling language mod- eling with pathways . ", "page_idx": 10, "bbox": [70, 181.72357177734375, 290, 434.82452392578125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 119, "type": "text", "text": "Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Al- bert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdh- ery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Ja- cob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. 2022.  Scaling instruction-finetuned language models . ", "page_idx": 10, "bbox": [70, 444.4955749511719, 290, 577.0485229492188], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 120, "type": "text", "text": "Bosheng Ding, Chengwei Qin, Linlin Liu, Yew Ken Chia, Boyang Li, Shafiq Joty, and Lidong Bing. 2023. Is GPT-3 a good data annotator? In  Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 11173–11195, Toronto, Canada. Association for Computational Linguistics. ", "page_idx": 10, "bbox": [70, 586.7185668945312, 290, 664.4765014648438], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 121, "type": "text", "text": "Ning Ding, Guangwei Xu, Yulin Chen, Xiaobin Wang, Xu Han, Pengjun Xie, Haitao Zheng, and Zhiyuan Liu. 2021.  Few-NERD: A few-shot named entity recognition dataset . In  Proceedings of the 59th An- nual Meeting of the Association for Computational Linguistics and the 11th International Joint Confer- ence on Natural Language Processing (Volume 1: Long Papers) , pages 3198–3213, Online. Association for Computational Linguistics. ", "page_idx": 10, "bbox": [70, 674.1475830078125, 290, 773.8235473632812], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 122, "type": "text", "text": "George Doddington, Alexis Mitchell, Mark Przybocki, Lance Ramshaw, Stephanie Strassel, and Ralph Weischedel. 2004. The automatic content extrac- tion (ACE) program – tasks, data, and evaluation . In Proceedings of the Fourth International Conference on Language Resources and Evaluation (LREC’04) , Lisbon, Portugal. European Language Resources As- sociation (ELRA). ", "page_idx": 10, "bbox": [306, 72.61956787109375, 526, 161.33750915527344], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 123, "type": "text", "text": "Seth Ebner, Patrick Xia, Ryan Culkin, Kyle Rawlins, and Benjamin Van Durme. 2020.  Multi-sentence ar- gument linking . In  Proceedings of the 58th Annual Meeting of the Association for Computational Lin- guistics , pages 8057–8077, Online. Association for Computational Linguistics. ", "page_idx": 10, "bbox": [306, 169.72857666015625, 526, 236.5284881591797], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 124, "type": "text", "text": "Jun Gao, Huan Zhao, Changlong Yu, and Ruifeng Xu. 2023.  Exploring the feasibility of chatgpt for event extraction . ", "page_idx": 10, "bbox": [306, 244.9195556640625, 526, 278.842529296875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 125, "type": "text", "text": "Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. SimCSE: Simple contrastive learning of sentence em- beddings . In  Proceedings of the 2021 Conference on Empirical Methods in Natural Language Process- ing , pages 6894–6910, Online and Punta Cana, Do- minican Republic. Association for Computational Linguistics. ", "page_idx": 10, "bbox": [306, 287.23358154296875, 526, 364.9925231933594], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 126, "type": "text", "text": "Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Wein- berger. 2017.  On calibration of modern neural net- works . In  Proceedings of the 34th International Con- ference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017 , volume 70 of Proceedings of Machine Learning Research , pages 1321–1330. PMLR. ", "page_idx": 10, "bbox": [306, 373.3835754394531, 526, 451.1415100097656], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 127, "type": "text", "text": "Bernal Jimenez Gutierrez, Nikolas McNeal, Clayton Washington, You Chen, Lang Li, Huan Sun, and Yu Su. 2022.  Thinking about GPT-3 in-context learn- ing for biomedical IE? think again . In  Findings of the Association for Computational Linguistics: EMNLP 2022 , pages 4497–4512, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. ", "page_idx": 10, "bbox": [306, 459.5335693359375, 526, 537.29150390625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 128, "type": "text", "text": "Martin Josifoski, Marija Sakota, Maxime Peyrard, and Robert West. 2023.  Exploiting asymmetry for syn- thetic training data generation: Synthie and the case of information extraction . ", "page_idx": 10, "bbox": [306, 545.68359375, 526, 590.5645141601562], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 129, "type": "text", "text": "Jaehun Jung, Lianhui Qin, Sean Welleck, Faeze Brah- man, Chandra Bhaga va tula, Ronan Le Bras, and Yejin Choi. 2022.  Maieutic prompting: Logically consistent reasoning with recursive explanations . In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing , pages 1266–1279, Abu Dhabi, United Arab Emirates. Asso- ciation for Computational Linguistics. ", "page_idx": 10, "bbox": [306, 598.95654296875, 526, 687.6735229492188], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 130, "type": "text", "text": "Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghaz vi nine j ad, Abdel rahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Z ett le moyer. 2020. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and com- prehension . In  Proceedings of the 58th Annual Meet- ing of the Association for Computational Linguistics , ", "page_idx": 10, "bbox": [306, 696.0645751953125, 526, 773.8235473632812], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 131, "type": "text", "text": "pages 7871–7880, Online. Association for Computa- tional Linguistics. ", "page_idx": 11, "bbox": [81, 72.61956787109375, 290, 95.58348846435547], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 132, "type": "text", "text": "Peng Li, Tianxiang Sun, Qiong Tang, Hang Yan, Yuan- bin Wu, Xuanjing Huang, and Xipeng Qiu. 2023. CodeIE: Large code generation models are better few-shot information extractors . In  Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 15339–15353, Toronto, Canada. Association for Computational Linguistics. ", "page_idx": 11, "bbox": [70, 104.14459228515625, 290, 192.8614959716797], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 133, "type": "text", "text": "Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. 2022. What makes good in-context examples for GPT-3? In Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extrac- tion and Integration for Deep Learning Architectures , pages 100–114, Dublin, Ireland and Online. Associa- tion for Computational Linguistics. ", "page_idx": 11, "bbox": [70, 201.42156982421875, 290, 290.1395263671875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 134, "type": "text", "text": "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man- dar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Z ett le moyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pre training ap- proach . ", "page_idx": 11, "bbox": [70, 298.6995849609375, 290, 354.54052734375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 135, "type": "text", "text": "Ilya Loshchilov and Frank Hutter. 2019.  Decoupled weight decay regular iz ation . In  7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019 . OpenRe- view.net. ", "page_idx": 11, "bbox": [70, 363.1005554199219, 290, 418.9415283203125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 136, "type": "text", "text": "Keming Lu, I-Hung Hsu, Wenxuan Zhou, Mingyu Derek Ma, and Muhao Chen. 2022a. Sum mari z ation as indirect supervision for relation extraction . In  Findings of the Association for Computational Linguistics: EMNLP 2022 , pages 6575–6594, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. ", "page_idx": 11, "bbox": [70, 427.5015563964844, 290, 505.2605285644531], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 137, "type": "text", "text": "Keming Lu, Xiaoman Pan, Kaiqiang Song, Hongming Zhang, Dong Yu, and Jianshu Chen. 2023.  Pivoine: Instruction tuning for open-world information extrac- tion .  ArXiv preprint , abs/2305.14898. ", "page_idx": 11, "bbox": [70, 513.820556640625, 290, 558.7025146484375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 138, "type": "text", "text": "Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. 2022b.  Fantastically ordered prompts and where to find them: Overcoming few- shot prompt order sensitivity . In  Proceedings of the 60th Annual Meeting of the Association for Compu- tational Linguistics (Volume 1: Long Papers) , pages 8086–8098, Dublin, Ireland. Association for Compu- tational Linguistics. ", "page_idx": 11, "bbox": [70, 567.2625732421875, 290, 655.9805297851562], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 139, "type": "text", "text": "Yaojie Lu, Qing Liu, Dai Dai, Xinyan Xiao, Hongyu Lin, Xianpei Han, Le Sun, and Hua Wu. 2022c.  Uni- fied structure generation for universal information extraction . In  Proceedings of the 60th Annual Meet- ing of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 5755–5772, Dublin, Ireland. Association for Computational Linguistics. ", "page_idx": 11, "bbox": [70, 664.5405883789062, 290, 742.2994995117188], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 140, "type": "text", "text": "Jie Ma, Miguel Ball ester os, Srikanth Doss, Rishita Anubhai, Sunil Mallya, Yaser Al-Onaizan, and Dan ", "page_idx": 11, "bbox": [70, 750.8595581054688, 290, 773.8235473632812], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 141, "type": "text", "text": "Roth. 2022a.  Label semantics for few shot named entity recognition . In  Findings of the Association for Computational Linguistics: ACL 2022 , pages 1956– 1971, Dublin, Ireland. Association for Computational Linguistics. ", "page_idx": 11, "bbox": [316, 72.61956787109375, 526, 128.4604949951172], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 142, "type": "text", "text": "Yubo Ma, Zehao Wang, Yixin Cao, Mukai Li, Meiqi Chen, Kun Wang, and Jing Shao. 2022b.  Prompt for extraction? PAIE: Prompting argument interac- tion for event argument extraction . In  Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 6759–6774, Dublin, Ireland. Association for Computational Linguistics. ", "page_idx": 11, "bbox": [306, 137.02056884765625, 526, 225.73851013183594], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 143, "type": "text", "text": "Yubo Ma, Zehao Wang, Yixin Cao, and Aixin Sun. 2023. Few-shot event detection: An empirical study and a unified view . In  Proceedings of the 61st Annual Meet- ing of the Association for Computational Linguis- tics (Volume 1: Long Papers) , pages 11211–11236, Toronto, Canada. Association for Computational Lin- guistics. ", "page_idx": 11, "bbox": [306, 234.298583984375, 526, 312.0574951171875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 144, "type": "text", "text": "Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car- roll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback . ", "page_idx": 11, "bbox": [306, 320.6175537109375, 526, 409.3345031738281], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 145, "type": "text", "text": "Liangming Pan, Alon Albalak, Xinyi Wang, and William Yang Wang. 2023.  Logic-lm: Empower- ing large language models with symbolic solvers for faithful logical reasoning . ", "page_idx": 11, "bbox": [306, 417.89556884765625, 526, 462.7765197753906], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 146, "type": "text", "text": "Cheng Qian, Chi Han, Yi R. Fung, Yujia Qin, Zhiyuan Liu, and Heng Ji. 2023.  Creator: Tool creation for disentangling abstract and concrete reasoning of large language models . ", "page_idx": 11, "bbox": [306, 471.33758544921875, 526, 516.2195434570312], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 147, "type": "text", "text": "Chengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro Yasunaga, and Diyi Yang. 2023.  Is chatgpt a general-purpose natural language process- ing task solver? ", "page_idx": 11, "bbox": [306, 524.779541015625, 526, 569.6614990234375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 148, "type": "text", "text": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020.  Exploring the limits of transfer learning with a unified text-to-text trans- former .  J. Mach. Learn. Res. , 21:140:1–140:67. ", "page_idx": 11, "bbox": [306, 578.2215576171875, 526, 634.0625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 149, "type": "text", "text": "Ohad Rubin, Jonathan Herzig, and Jonathan Berant. 2022.  Learning to retrieve prompts for in-context learning . In  Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech- nologies , pages 2655–2671, Seattle, United States. Association for Computational Linguistics. ", "page_idx": 11, "bbox": [306, 642.62255859375, 526, 720.3815307617188], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 150, "type": "text", "text": "Zhiyi Song, Ann Bies, Stephanie Strassel, Tom Riese, Justin Mott, Joe Ellis, Jonathan Wright, Seth Kulick, Neville Ryant, and Xiaoyi Ma. 2015.  From light to rich ERE: Annotation of entities, relations, and ", "page_idx": 11, "bbox": [306, 728.9415893554688, 526, 773.8235473632812], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 151, "type": "text", "text": "events . In  Proceedings of the The 3rd Workshop on EVENTS: Definition, Detection, Co reference, and Representation , pages 89–98, Denver, Colorado. As- sociation for Computational Linguistics. ", "page_idx": 12, "bbox": [81, 72.61956787109375, 290, 117.50151824951172], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 152, "type": "text", "text": "Hongjin Su, Jungo Kasai, Chen Henry Wu, Weijia Shi, Tianlu Wang, Jiayi Xin, Rui Zhang, Mari Ostendorf, Luke Z ett le moyer, Noah A. Smith, and Tao Yu. 2022. Selective annotation makes language models better few-shot learners . ", "page_idx": 12, "bbox": [70, 126.06158447265625, 290, 181.9025115966797], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 153, "type": "text", "text": "Zhiqing Sun, Xuezhi Wang, Yi Tay, Yiming Yang, and Denny Zhou. 2023.  Recitation-augmented language models . In  International Conference on Learning Representations . ", "page_idx": 12, "bbox": [70, 190.46258544921875, 290, 235.3445281982422], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 154, "type": "text", "text": "Erik F. Tjong Kim Sang and Fien De Meulder. 2003.  Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition . In Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003 , pages 142– 147. ", "page_idx": 12, "bbox": [70, 243.90557861328125, 290, 310.70452880859375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 155, "type": "text", "text": "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023.  Llama: Open and efficient foundation language models . ", "page_idx": 12, "bbox": [70, 319.26556396484375, 290, 386.06451416015625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 156, "type": "text", "text": "Somin Wadhwa, Silvio Amir, and Byron Wallace. 2023. Revisiting relation extraction in the era of large lan- guage models . In  Proceedings of the 61st Annual Meeting of the Association for Computational Lin- guistics (Volume 1: Long Papers) , pages 15566– 15589, Toronto, Canada. Association for Computa- tional Linguistics. ", "page_idx": 12, "bbox": [70, 394.6255798339844, 290, 472.3835144042969], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 157, "type": "text", "text": "Xiao Wang, Wei Zhou, Can Zu, Han Xia, Tianze Chen, Yuan Zhang, Rui Zheng, Junjie Ye, Qi Zhang, Tao Gui, Jihua Kang, J. Yang, Siyuan Li, and Chunsai Du. 2023a.  Instruct u ie: Multi-task instruction tuning for unified information extraction .  ArXiv preprint , abs/2304.08085. ", "page_idx": 12, "bbox": [70, 480.9435729980469, 290, 547.7434692382812], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 158, "type": "text", "text": "Xiaozhi Wang, Ziqi Wang, Xu Han, Wangyi Jiang, Rong Han, Zhiyuan Liu, Juanzi Li, Peng Li, Yankai Lin, and Jie Zhou. 2020.  MAVEN: A Massive General Domain Event Detection Dataset . In  Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pages 1652– 1671, Online. Association for Computational Linguis- tics. ", "page_idx": 12, "bbox": [70, 556.3035888671875, 290, 645.0215454101562], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 159, "type": "text", "text": "Xingyao Wang, Sha Li, and Heng Ji. 2023b. Code 4 Struct: Code generation for few-shot event structure prediction . In  Proceedings of the 61st An- nual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 3640– 3663, Toronto, Canada. Association for Computa- tional Linguistics. ", "page_idx": 12, "bbox": [70, 653.58154296875, 290, 731.3405151367188], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 160, "type": "text", "text": "Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023c. Self-consistency improves ", "page_idx": 12, "bbox": [70, 739.9005737304688, 290, 773.8235473632812], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 161, "type": "text", "text": "chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations (ICLR 2023) . ", "page_idx": 12, "bbox": [316, 72.61956787109375, 526, 106.54253387451172], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 162, "type": "text", "text": "Yizhong Wang, Swaroop Mishra, Pegah Alipoormo- labashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva Naik, Arjun Ashok, Arut Selvan Dhana sekar an, Anjana Arunkumar, David Stap, Eshaan Pathak, Giannis Karam a no lak is, Haizhi Lai, Ishan Puro- hit, Ishani Mondal, Jacob Anderson, Kirby Kuznia, Krima Doshi, Kuntal Kumar Pal, Maitreya Patel, Mehrad Moradshahi, Mihir Parmar, Mirali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh Puri, Rushang Karia, Savan Doshi, Shailaja Keyur Sampat, Siddhartha Mishra, Sujan Reddy A, Sumanta Patro, Tanay Dixit, and Xudong Shen. 2022.  Super-Natural Instructions: Generaliza- tion via declarative instructions on   $1600+$  NLP tasks . In  Proceedings of the 2022 Conference on Empiri- cal Methods in Natural Language Processing , pages 5085–5109, Abu Dhabi, United Arab Emirates. As- sociation for Computational Linguistics. ", "page_idx": 12, "bbox": [306, 113.5645751953125, 526, 311.87054443359375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 163, "type": "text", "text": "Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, An- drew M. Dai, and Quoc V. Le. 2022a.  Finetuned language models are zero-shot learners . In  The Tenth International Conference on Learning Representa- tions, ICLR 2022, Virtual Event, April 25-29, 2022 . OpenReview.net. ", "page_idx": 12, "bbox": [306, 318.892578125, 526, 396.6505126953125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 164, "type": "text", "text": "Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Huai hsin Chi, Quoc Le, and Denny Zhou. 2022b. Chain of thought prompting elicits reasoning in large language models.  Proceedings of the 36th International Conference on Neural Information Pro- cessing Systems . ", "page_idx": 12, "bbox": [306, 403.67156982421875, 526, 470.4715270996094], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 165, "type": "text", "text": "Xiang Wei, Xingyu Cui, Ning Cheng, Xiaobin Wang, Xin Zhang, Shen Huang, Pengjun Xie, Jinan Xu, Yufeng Chen, Meishan Zhang, Yong Jiang, and Wen- juan Han. 2023.  Zero-shot information extraction via chatting with chatgpt . ", "page_idx": 12, "bbox": [306, 477.4935607910156, 526, 533.3335571289062], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 166, "type": "text", "text": "Ralph Weischedel, Martha Palmer, Mitchell Marcus, Ed- uard Hovy, Sameer Pradhan, Lance Ramshaw, Nian- wen Xue, Ann Taylor, Jeff Kaufman, Michelle Fran- chini, et al. 2013. Ontonotes release 5.0 ldc2013t19. Linguistic Data Consortium, Philadelphia, PA . ", "page_idx": 12, "bbox": [306, 540.3555908203125, 526, 596.1954956054688], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 167, "type": "text", "text": "Yi Yang and Arzoo Katiyar. 2020.  Simple and effective few-shot named entity recognition with structured nearest neighbor learning . In  Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pages 6365–6375, Online. Association for Computational Linguistics. ", "page_idx": 12, "bbox": [306, 603.2175903320312, 526, 670.0165405273438], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 168, "type": "text", "text": "Wenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu, Mingxuan Ju, Soumya Sanyal, Chenguang Zhu, Michael Zeng, and Meng Jiang. 2023. Generate rather than retrieve: Large language models are strong context generators. In  International Confer- ence for Learning Representation (ICLR 2023) . ", "page_idx": 12, "bbox": [306, 677.03857421875, 526, 743.8375244140625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 169, "type": "text", "text": "Kai Zhang, Bernal Jimenez Gutierrez, and Yu Su. 2023a. Aligning instruction tasks unlocks large language ", "page_idx": 12, "bbox": [306, 750.8595581054688, 526, 773.8235473632812], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 170, "type": "text", "text": "models as zero-shot relation extractors . In  Find- ings of the Association for Computational Linguis- tics: ACL 2023 , pages 794–812, Toronto, Canada. Association for Computational Linguistics. ", "page_idx": 13, "bbox": [81, 72.61956787109375, 291, 117.50151824951172], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 171, "type": "text", "text": "Yuhao Zhang, Victor Zhong, Danqi Chen, Gabor Angeli, and Christopher D. Manning. 2017.  Position-aware attention and supervised data improve slot filling . In  Proceedings of the 2017 Conference on Empiri- cal Methods in Natural Language Processing , pages 35–45, Copenhagen, Denmark. Association for Com- putational Linguistics. ", "page_idx": 13, "bbox": [70, 127.237548828125, 291, 204.9965057373047], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 172, "type": "text", "text": "Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. 2023b. Automatic chain of thought prompt- ing in large language models. In  The Eleventh In- ter national Conference on Learning Representations (ICLR 2023) . ", "page_idx": 13, "bbox": [70, 214.7325439453125, 291, 270.573486328125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 173, "type": "text", "text": "A Datasets ", "text_level": 1, "page_idx": 13, "bbox": [71, 284, 136, 296], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 174, "type": "text", "text": "A.1 Full Datasets ", "text_level": 1, "page_idx": 13, "bbox": [71, 306, 160, 318], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 175, "type": "text", "text": "We construct few-shot IE datasets and conduct the empirical study on nine datasets spanning four tasks,  with varying schema complexities ranging from 4 to 168 . We show their statistics in Table  6 . ", "page_idx": 13, "bbox": [70, 324.06298828125, 291, 378.5110168457031], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 176, "type": "text", "text": "A.2 Details of Few-shot IE Datasets ", "text_level": 1, "page_idx": 13, "bbox": [70, 389, 244, 401], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 177, "type": "text", "text": "Sampling Algorithm for Train/Valid Datasets. We downsample sentences from original training dataset to construct few-shot training and valid datasets. We adopt    $K$  -shot sampling strategy that each label has (at least)    $K$   samples. We set 6    $K$  - values (1, 5, 10, 20, 50, 100) for RE tasks and 4  $K$  -values (1, 5, 10, 20) for other tasks. For RE task, each sentence has exactly one relation and we simply select  $K$   sentences for each label. For NER, ED and EAE tasks, each sentences is possible to contain more than one entities/events/arguments. Since our sampling is at sentence-level, the algo- rithm of accurate sampling ,  i.e.,  finding exactly  $K$   samples for each label, is NP-complete 8   and unlikely to find a practical solution. Therefore we follow  Yang and Katiyar  ( 2020 ) adopting a greedy sampling algorithm to select sentences for NER and ED tasks, as shown in Algorithm  1 . Note that the actual sample number of each label can be larger than  $K$   under this sampling strategy. For all three tasks, we additionally sample negative sentences (without any defined labels) and make the ratio of positive sentences (with at least one label) and neg- ative sentences as 1:1. The statistics of the curated datasets are listed in Table  7 . ", "page_idx": 13, "bbox": [70, 406.6542663574219, 291, 745.3734130859375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 178, "type": "text", "text": "Require:  shot number    $K$  , original full dataset  $\\mathcal{D}=\\{({\\bf X},{\\bf Y})\\}$   tagged with label set    $E$  1:  Sort  E  based on their frequencies in    $\\{\\mathbf{Y}\\}$   as an ascending order 2:    $S\\gets\\phi$  , Counter  $\\leftarrow$  dict () 3:  for    $y\\in E$   do 4:  ${\\mathrm{Counter}}(y)\\gets0$  5:  end for 6:  for    $y\\in E$   do 7: while  Counter  $(y)<K$   do 8: Sample    $(\\mathbf{X},\\mathbf{Y})\\in\\mathcal{D}\\;\\mathrm{s.t.}\\exists j,y_{j}=y$  9:  $\\mathcal{D}\\gets\\mathcal{D}\\backslash({\\mathbf{X}},{\\mathbf{Y}})$  10: Update Counter (not only    $y$   but all event types in  $\\mathbf{Y}$  ) 11: end while 12:  end for 13:  for    $s\\in\\mathcal S$   do 14:  ${\\mathcal{S}}\\gets{\\mathcal{S}}\\backslash s$   and update Counter 15: if    $\\exists y\\in E$  , s.t. Counter  $\\mathopen{}\\mathclose\\bgroup\\left(y\\aftergroup\\egroup\\right)<K$   then 16:  ${\\mathcal{S}}\\gets{\\mathcal{S}}\\bigcup{\\mathcal{S}}$  17: end if 18:  end for 19:  return  $s$  ", "page_idx": 13, "bbox": [305, 87.14329528808594, 526, 399.4180603027344], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 179, "type": "text", "text": "Based on the subsets constructed above, we op- tionally further split them into training and valid sets. For few-shot datasets with more than 300 sen- tences, we additionally split   $10\\%$   sentences as the valid set and the remaining sentences as training set. Otherwise, we do not construct valid set and con- duct 5-fold cross validation to avoid over fitting. ", "page_idx": 13, "bbox": [305, 422.77301025390625, 526, 517.2134399414062], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 180, "type": "text", "text": "B Details on SLMs ", "text_level": 1, "page_idx": 13, "bbox": [306, 530, 411, 543], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 181, "type": "text", "text": "We adopt five representative supervised methods to evaluate the ability of SLMs on few-shot IE tasks. (1). Fine-tuning (FT) : Add a classifier head on SLMs to predict the labels of each sentence/word. (2). FSLS  ( Ma et al. ,  2022a ): The state-of-the-art extractive-based method for few-shot NER task. Ma et al.  ( 2023 ) also validate its competitive per- formance on few-shot ED tasks. ", "page_idx": 13, "bbox": [305, 553.2040405273438, 526, 662.6944580078125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 182, "type": "text", "text": "(3). KnowPrompt  ( Chen et al. ,  2022b ): The best extractive-based method for few-shot RE task. ", "page_idx": 13, "bbox": [305, 663.456298828125, 526, 690.54345703125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 183, "type": "text", "text": "(4). PAIE  ( Ma et al. ,  2022b ): The best extractive- based method for few-shot EAE task. ", "page_idx": 13, "bbox": [305, 691.3042602539062, 526, 718.3924560546875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 184, "type": "text", "text": "(5). UIE  ( Lu et al. ,  2022c ): A competitive unified generation-based method for few-shot IE tasks. We introduce their implementation details below: ", "page_idx": 13, "bbox": [305, 719.1533203125, 526, 759.7894287109375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 185, "type": "text", "text": "Fine-tuning/FSLS.  We implement these two meth- ", "page_idx": 13, "bbox": [305, 760.5513305664062, 526, 774.7440795898438], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 186, "type": "table", "page_idx": 14, "img_path": "layout_images/2303.08559v2_15.jpg", "table_caption": "Table 6: Statistics of nine datasets used. Note that the # mentions  for event detection tasks refers to the number of trigger words, while the # mentions  for event argument extraction tasks refers to the number of arguments. ", "bbox": [69, 70, 527, 199], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "Named Entity Recognition | Relation Extraction\n\nEvent Detection Event Arg Extraction\nDataset CONLL OntoNotes FewNERD|TACREV TACRED\n\nACE05 MAVEN ERE |ACE05 RAMS ERE\n\n#Label Type | 4 18 66 | 41 41 33 168 38 | 33 139 38\n\n|\n4Sents Train| 14,041 49,706 131,965 | 68,124 68,124 | 14,024 32,360 14,736] 14,024 7329 14,736\nTest | 3,453 10,348 37,648 15,509 15,509 728 8,035 1,163 | 728 871 = 1,163\n#Mentions Train] 23,499 128,738 340,247 13,012 13,012 | 5,349 77,993 6,208 | 4859 17026 8924\nTest | 5,648 12,586 96,902 3,123 3,123 424 18,904 551 576 ©2023 822\n\n", "vlm_text": "The table provides statistics on different datasets used for Named Entity Recognition, Relation Extraction, Event Detection, and Event Argument Extraction. Here are the details:\n\n### Columns:\n1. **Dataset Types**\n   - Named Entity Recognition (CONLL, OntoNotes, FewNERD)\n   - Relation Extraction (TACREV, TACRED)\n   - Event Detection (ACE05, MAVEN, ERE)\n   - Event Arg Extraction (ACE05, RAMS, ERE)\n\n### Rows:\n1. **#Label Type**: Number of label types for each dataset.\n   - Named Entity Recognition: 4, 18, 66\n   - Relation Extraction: 41, 41\n   - Event Detection: 33, 168, 38\n   - Event Arg Extraction: 33, 139, 38\n\n2. **#Sents**: Number of sentences in training and test sets.\n   - Named Entity Recognition: \n     - Train: 14,041 (CONLL), 49,706 (OntoNotes), 131,965 (FewNERD)\n     - Test: 3,453 (CONLL), 10,348 (OntoNotes), 37,648 (FewNERD)\n   - Relation Extraction:\n     - Train: 68,124 (TACREV and TACRED)\n     - Test: 15,509 (TACREV and TACRED)\n   - Event Detection:\n     - Train: 14,024 (ACE05), 32,360 (MAVEN), 14,736 (ERE)\n     - Test: 728 (ACE05), 8,035 (MAVEN), 1,163 (ERE)\n   - Event Arg Extraction:\n     - Train: 14,024 (ACE05), 7,329 (RAMS), 14,736 (ERE)\n     - Test: 728 (ACE05), 871 (RAMS), 1,163 (ERE)\n\n3. **#Mentions**: Number of mentions in training and test sets.\n   - Named Entity Recognition: \n     - Train: 23,499 (CONLL), 128,738 (OntoNotes), 340,247 (FewNERD)\n     - Test: 5,648 (CONLL), 12,586 (OntoNotes), 96,902 (FewNERD)\n   - Relation Extraction:\n     - Train: 13,012 (TACREV and TACRED)\n     - Test: 3,123 (TACREV and TACRED)\n   - Event Detection:\n     - Train: 5,349 (ACE05), 77,993 (MAVEN), 6,208 (ERE)\n     - Test: 424 (ACE05), 18,904 (MAVEN), 551 (ERE)\n   - Event Arg Extraction:\n     - Train: 4,859 (ACE"}
{"layout": 187, "type": "table", "page_idx": 14, "img_path": "layout_images/2303.08559v2_16.jpg", "table_caption": "Table 7: The statistics of few-shot training sets. We set different random seeds and generate 5 training sets for each setting. We report their average statistics. ", "bbox": [69, 224, 300, 767], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "Dataset Settings\n\n#Labels #Sent #Sample # Avg shot\n\n-shot 48 5.8 14\nang 5-shot 162-218 55\nCONLL'03 ioshot | 4 292 «42.6 10.7\n20-shot 65.6 820 20.5\n\n~shot 20.0 33.4 19\n\n_ 5-shot 848 148.0 82\nOntoNotes ig'shot | '8 1586 281.0 15.6\n20-shot 3328 547.2 30.4\n\n-shot 89.8 147.0 22\n\n5-shot 286.2 © -494'8 75\n\nFewNERD joshot| % 5380 962.0 146\n20-shot 10272 18514 28.1\n\n-shot 816 41.0 1.0\n\n5-shot 387.6 205.0 5.0\n\n10-shot 741.2 406.0 9.9\n\nTACREV 59 shot | 4! 13672 806.0 19.7\n50-shot 2872.0 1944.0 474\n\n100-shot 4561.0 3520.0 85.9\n\n1-shot 816 41.0 1.0\n\n5-shot 387.6 205.0 5.0\n\n10-shot 741.2 406.0 9.9\n\nTACRED 50-shot | 4! 13672. 806.0 19.7\n50-shot 2871.2 1944.0 474\n\n100-shot 4575.2 3520.0 85.9\n\n-shot 474. 41.0 2\n\n5-shot 92.8 165.0 5.0\n\nACE0S jo-shot| 33 334.6 319.4 97\n20-shot 579.4 508.2 18.\n\n-shot 57.6 298.0 8\n\n5-shot 540.4 1262.2 75\n\nMAVEN  jo-shot | 18 — gor.2 2413.8 144\n20-shot 1286.4 4611.4 274\n\n-shot 48.4 54.6 A\n\n5-shot 750 219.2 58\n\nERE jo-shot| 38 3048 © 432.4 4\n20-shot 521.6 806.6 212\n\n-shot 23.4 40.2 2\n\n5-shot 798 178.2 5.4\n\nACE05 — jo-shot | 39 308 3374 10.2\n20-shot 213.4 630.2 19.\n\nshot 30.2 332.6 24\n\n5-shot 514.0 1599.6 115\n\nRAMS jo-shot| 139 795.2 3193.2 23.0\n20-shot 1070.4 6095.4 43.9\n\n-shot 21.6 1028 27\n\n5-shot 742 403.4 10.6\n\nERE jo-shot| 38 1272 = 775.6 20.4\n20-shot 1902 13972 36.8\n\n", "vlm_text": "The table presents dataset settings with various metrics across different dataset names. Here is the breakdown of its structure:\n\n- **Columns:**\n  - **Dataset Settings:** Name of the dataset.\n  - **# Labels:** Number of labels in the dataset.\n  - **# Sent:** Number of sentences.\n  - **# Sample:** Number of samples.\n  - **# Avg shot:** Average shot size.\n\n- **Rows:** \n  - Each dataset is evaluated under different \"shot\" settings, such as 1-shot, 5-shot, 10-shot, etc.\n  - The values for each metric are provided for each dataset under these shot settings.\n\n- **Datasets Included:**\n  - CONLL'03, OntoNotes, FewNERD, TACREV, TACRED, ACE05, MAVEN, ERE, and RAMS.\n\nThis table is likely from a study involving few-shot learning or similar methodologies applied to natural language processing tasks."}
{"layout": 188, "type": "text", "text": "ods by ourselves. We use  RoBERTa-large  ( Liu et al. ,  2019 ) as the backbones. We adopt Auto- matic Mixed Precision (AMP) training strategy 9   to save memory. We run each experiment on a single NVIDIA V100 GPU. We train each model with the AdamW ( Loshchilov and Hutter ,  2019 ) opti- mizer with linear scheduler and 0.1 warm-up steps. We set the weight-decay coefficient as 1e-5 and maximum gradient norms as 1.0. We set the batch size as 64, the maximum input length as 192, the training step as 500 and the learning rate as 5e-5. ", "page_idx": 14, "bbox": [305, 219.28199768066406, 526, 367.9194641113281], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 189, "type": "text", "text": "KnowPrompt  We implement this method based on original source code 10 , and use  RoBERTa-large as our backbones. We set 10 maximum epochs for 50- and 100-shot datasets, and as 50 epochs for other datasets. We keep all other hyper parameters as default, and run each experiment on a single NVIDIA V100 GPU. ", "page_idx": 14, "bbox": [305, 367.9302673339844, 526, 462.76348876953125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 190, "type": "text", "text": "PAIE  We implement this method on original source code 11 , and use  BART-large  ( Lewis et al. , 2020 ) as backbones. We keep all hyper parameters as default for ACE and RAMS dataset. For ERE dataset, we set the training step as 1000, the batch size as 16 and the learning rate as 2e-5. We run each experiment on a single NVIDIA V100 GPU. ", "page_idx": 14, "bbox": [305, 462.7742614746094, 526, 557.6084594726562], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 191, "type": "text", "text": "UIE  We implement this method based on original source code 12 , and use  T5-large  ( Raffel et al. , 2020 ) as the backbones. We run each experiment on a single NVIDIA Quadro RTX8000 GPU. We set the batch size as 4 with 4000 training steps. We set the maximum input length as 800 and the learning rate as 1e-4. ", "page_idx": 14, "bbox": [305, 557.6193237304688, 526, 652.4524536132812], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 192, "type": "text", "text": "C LLMs Implementations ", "text_level": 1, "page_idx": 14, "bbox": [305, 663, 449, 676], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 193, "type": "text", "text": "Regarding our empirical study, we explore the ICL abilities of LLMs on few-shot IE tasks. We mainly use five LLMs from two sources. (1) OpenAI models: CODEX ( code-davinci-002 ;  Chen et al. 2021 ), Instruct GP T ( text-davinci-003 ;  Ouyang et al. 2022 ), and ChatGPT ( gpt-3.5-turbo-0301 ). (2) Open-source models: LLaMA-13B ( Touvron et al. ,  2023 ) and its instruction-tuned counterpart, Vicuna-13B ( Chiang et al. ,  2023 ). We detail their implementation details in the next sections below. ", "page_idx": 14, "bbox": [305, 683.052001953125, 526, 723.29541015625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 194, "type": "text", "text": "", "page_idx": 15, "bbox": [70, 71.74500274658203, 290, 166.18545532226562], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 195, "type": "text", "text": "C.1 Open-source Models ", "text_level": 1, "page_idx": 15, "bbox": [70, 179, 194, 190], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 196, "type": "text", "text": "We implement multiple ICL approaches on LLaMA-13B and Vicuna-13B without fine-tuning. We set the maximum input length as 2048 and the batch size as 1. We run each experiment on a single NVIDIA V100 GPU. To achieve this, we leverage the  Accelerate   13   framework and fp16 inference to save memory. We set maximum output length as 96 and sampling temperature as 0 ( i.e.,  greedy decoding). We set both  frequency penalty  and presence penalty  as 0. ", "page_idx": 15, "bbox": [70, 197.3410186767578, 290, 332.4294738769531], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 197, "type": "text", "text": "C.2 OpenAI Models ", "text_level": 1, "page_idx": 15, "bbox": [71, 345, 172, 356], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 198, "type": "text", "text": "We implement multiple ICL approaches on Ope- nAI models by calling their official APIs   14 . We set the maximum input length as 3600 for all tasks and models. The only exception occurs when we use CODEX on RE tasks, where we set the maximum input length as 7000. We unify the maximum out- put length as 32 for RE task, and 96 for other three tasks. We set the sampling temperature coefficient as 0,  i.e.,  greedy decoding. ", "page_idx": 15, "bbox": [70, 363.583984375, 290, 485.1224670410156], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 199, "type": "text", "text": "D Pivot Experiments on LLMs ", "text_level": 1, "page_idx": 15, "bbox": [70, 498, 237, 511], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 200, "type": "text", "text": "D.1 Sampling Temperature ", "text_level": 1, "page_idx": 15, "bbox": [70, 522, 205, 534], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 201, "type": "text", "text": "Existing prompt-engineering discussion 15   suggests setting the sampling temperature    $t=0$   for tasks with structured outputs, including IE tasks. We validate this conclusion in Table  8 , from which we could see the generated quality when  $t=0$   is much hig an the quality when  $t\\neq0$  . Therefore we set  $t=0$   in all main experiments, and do not take self-consistency ( Wang et al. ,  2023c ) into account. ", "page_idx": 15, "bbox": [70, 536.6300048828125, 290, 648.5794677734375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 202, "type": "text", "text": "D.2 Automatic Chain-of-thought ", "text_level": 1, "page_idx": 15, "bbox": [70, 662, 230, 673], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 203, "type": "text", "text": "We additionally investigate whether rationales could facilitate LLMs’ performance on few-shot IE tasks. Since there exists no golden rationales in ", "page_idx": 15, "bbox": [70, 679.7340087890625, 290, 719.9774169921875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 204, "type": "table", "page_idx": 15, "img_path": "layout_images/2303.08559v2_17.jpg", "table_caption": "Table 8: F1-scores across different    $t$   values. Experi- ments run on 10-shot settings with CODEX. ", "bbox": [305, 70, 526, 174], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "|FewNERD TACREV ACE05\n\nt=0 48.5(1.9) 53.7(2.3) 42.9(2.2)\n+ 5-ensemble 53.5(1.3) 58.6(1.5) 46.3(0.8)\nt=0.7 40.9(2.3) 39.9(1.2) 35.6(1.0)\n\n+ self-consistency| 52.1(0.9) 53.4(1.3) 45.6(3.0)\n", "vlm_text": "The table presents results for three datasets: FewNERD, TACREV, and ACE05. The results are based on different configurations:\n\n1. **t = 0**:\n   - Without any modifications:\n     - FewNERD: 48.5 (±1.9)\n     - TACREV: 53.7 (±2.3)\n     - ACE05: 42.9 (±2.2)\n   - With 5-ensemble:\n     - FewNERD: 53.5 (±1.3)\n     - TACREV: 58.6 (±1.5)\n     - ACE05: 46.3 (±0.8)\n\n2. **t = 0.7**:\n   - Without any modifications:\n     - FewNERD: 40.9 (±2.3)\n     - TACREV: 39.9 (±1.2)\n     - ACE05: 35.6 (±1.0)\n   - With self-consistency:\n     - FewNERD: 52.1 (±0.9)\n     - TACREV: 53.4 (±1.3)\n     - ACE05: 45.6 (±3.0)\n\nThe numbers seem to represent some performance metrics, such as accuracy or F1 score, with standard deviations in parentheses."}
{"layout": 205, "type": "text", "text": "original datasets, we follow Automatic Chain-of- thought (Auto-CoT;  Zhang et al. 2023b ) method as below. Regarding each sample, we query LLMs ", "page_idx": 15, "bbox": [305, 203.7110137939453, 526, 243.95449829101562], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 206, "type": "text", "text": "According to [sentence], Why [span] is a [label] ", "page_idx": 15, "bbox": [317, 249.38502502441406, 526, 262.4104919433594], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 207, "type": "text", "text": "For example, given the sentence  “DSC and Trac- tion Control on all  Speed3  models is also stan- dard.” , we would feed LLM the query that  “Could you explain why Speed3 is a kind of car” . Then we insert the boots trapped rationales between the sen- tences and ground-truth answers. If a sentence has no positive labels, however, we do not ask LLMs and keep the original format as the vanilla ICL ap- proach. Here we prompt Instruct GP T to generate the rationales with temperature    $t=0.7$  . We com- pare the performance with and without Auto-CoT as shown in Table  9 . ", "page_idx": 15, "bbox": [305, 267.96197509765625, 526, 430.1484680175781], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 208, "type": "table", "page_idx": 15, "img_path": "layout_images/2303.08559v2_18.jpg", "table_caption": "Table 9: The F1-score difference between with and without Auto-CoT. We generate rationales by Instruct- GPT, then adopt  ICL w. Auto-CoT  approach and use CODEX as our backbone for inference. ", "bbox": [306, 449, 526, 560], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "FewNERD TACREV ACE05\n\n10-shot train set (NER) (RE) (ED)\n\nwo. Auto-CoT $4.0(1.4) 57.3(1.8) 47.7(2.8)\nw. Auto-CoT 36.6.7) 22.0(1.2) 43.1(3.4)\n", "vlm_text": "The table compares results from a 10-shot train set across three datasets: FewNERD (NER), TACREV (RE), and ACE05 (ED). It contrasts two methods: without Auto-CoT and with Auto-CoT. \n\nFor FewNERD (NER):\n- Without Auto-CoT: 54.0 (±1.4)\n- With Auto-CoT: 36.6 (±1.7)\n\nFor TACREV (RE):\n- Without Auto-CoT: 57.3 (±1.8)\n- With Auto-CoT: 22.0 (±1.2)\n\nFor ACE05 (ED):\n- Without Auto-CoT: 47.7 (±2.8)\n- With Auto-CoT: 43.1 (±3.4)"}
{"layout": 209, "type": "text", "text": "We are frustrated to find Auto-CoT degrades the performance with a large margin. We speculate this degration could be attributed to three main reasons. (1) The rationale increase the length of each sample and thus decrease the overall example number in demos. (2) There exists an obvious discrepancy between sentences with and without positive labels. The rationales are only provided for sentences with positive labels because it is hard to explain why a sentence dose not contain any label. (3) Some auto-generated rationales are low-quality, especially for RE tasks. We would explore better strategy to exploit auto-genertaed rationales in the future work. ", "page_idx": 15, "bbox": [305, 584.8040161132812, 526, 774.0894775390625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 210, "type": "table", "page_idx": 16, "img_path": "layout_images/2303.08559v2_19.jpg", "bbox": [78, 90, 518, 172], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "NER (20-shot) RE (100-shot) ED (20-shot) EAE (20-shot)\nCONLL OntoNotes FewNERD|TACREV TACRED|ACE05 MAVEN ERE|ACE05 RAMS ERE\n\nInstructGPT TLD. 47.7 S72) 62.7 53.8 49.3 25.4 40.8] 45.8 42.2 41.9\nCODEX 81.1 55.6 55.9 62.4 53.6 47.9 22.8 39.0 - - -\nGPT-4 84.7 65.6 57.8 59.3 50.4 52.1 30.2 40.5] 42.9 38.6 38.2\n\nSupervised SoTA| 72.3 74.9 614 | 72.6 63.1 | 65.8 54.7 56.2| 55.2 57.7 55.6\n", "vlm_text": "The table presents performance metrics of different language models on various NLP tasks, with comparisons to supervised state-of-the-art results. Here's a breakdown:\n\n### NER (20-shot)\n- **Datasets:** CONLL, OntoNotes, FewNERD\n- **Performance (F1 Scores):**\n  - InstructGPT: 77.2, 47.7, 57.2\n  - CODEX: 81.1, 55.6, 55.9\n  - GPT-4: **84.7**, **65.6**, **57.8**\n  - Supervised SoTA: 72.3, 74.9, 61.4\n\n### RE (100-shot)\n- **Datasets:** TACREV, TACRED\n- **Performance (F1 Scores):**\n  - InstructGPT: **62.7**, 53.8\n  - CODEX: 62.4, 53.6\n  - GPT-4: 59.3, 50.4\n  - Supervised SoTA: 72.6, 63.1\n\n### ED (20-shot)\n- **Datasets:** ACE05, MAVEN, ERE\n- **Performance (F1 Scores):**\n  - InstructGPT: 49.3, 25.4, **40.8**\n  - CODEX: 47.9, 22.8, 39.0\n  - GPT-4: **52.1**, **30.2**, 40.5\n  - Supervised SoTA: 65.8, 54.7, 56.2\n\n### EAE (20-shot)\n- **Datasets:** ACE05, RAMS, ERE\n- **Performance (F1 Scores):**\n  - InstructGPT: **45.8**, **42.2**, **41.9**\n  - CODEX: - , - , -\n  - GPT-4: 42.9, 38.6, 38.2\n  - Supervised SoTA: 55.2, 57.7, 55.6\n\n### Highlights\n- GPT-4 generally shows strong performance in most tasks, often outperforming other models.\n- Supervised methods display strong performance across various tasks, sometimes surpassing language models, especially in RE and EAE tasks."}
{"layout": 211, "type": "text", "text": "D.3 GPT-4 v.s. Others ", "text_level": 1, "page_idx": 16, "bbox": [70, 192, 181, 204], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 212, "type": "text", "text": "We tend to minimize the GPT-4 calls due to its high price. Thus we utilize 20-/100-shot settings across each dataset to compare GPT-4’s performance with other LLMs. Table  10  reveals that GPT-4 does not outperform other LLMs significantly, except on OntoNotes and MAVEN. However, even on these datasets, GPT-4 still falls behind supervised SLMs by a significant margin. Consequently, the exclu- sion of GPT-4 does not undermine the conclusions drawn from our main experiments, and we omit it from our empirical study. ", "page_idx": 16, "bbox": [70, 210.32704162597656, 290, 358.9644775390625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 213, "type": "text", "text": "E Auxiliary Experiments ", "text_level": 1, "page_idx": 16, "bbox": [70, 371, 207, 384], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 214, "type": "text", "text": "E.1 LLMs struggle on Fine-grained Datasets ", "text_level": 1, "page_idx": 16, "bbox": [70, 394, 287, 407], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 215, "type": "text", "text": "Based on the results shown in Figure  2 , we addi- tionally provide a quantitative analysis to show that LLMs struggle with fine-grained datasets. Under the 5-shot setting, we compare the performance difference of LLMs (ChatGPT) and SLMs (SoTA few-shot models) among different datasets. For each IE task, we observe a clear negative corre- ", "page_idx": 16, "bbox": [70, 412.71099853515625, 290, 507.1514892578125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 216, "type": "table", "page_idx": 16, "img_path": "layout_images/2303.08559v2_20.jpg", "table_caption": "Table 11: Performance comparison between LLMs (ChatGPT) and SLM-based methods among datasets with various schema complexities. ", "bbox": [69, 527, 290, 772], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "Named Entity Recognition\nCoNLL OntoNotes FewNERD\n\n# Entity 4 18 66\nMicro-F1 (SLM) 52.5 59.7 59.4\nMicro-F1 (LLM) 77.8 59.4 55.5\nAFI (LLM, SLM) 25.3 -0.3 -3.9\n\nEvent Detection\nACE05 ERE MAVEN\n\n# Event 33 38 168\nMicro-F1 (SLM) 55:1 48.0 49.4\nMicro-F1 (LLM) 39.6 33.8 25.3.\nAFI (LLM, SLM) -15.5 -14.2 -24.1\n\nEvent Argument Extraction\nACE05 ERE RAMS\n# Event / #Role 33/22 38/26 139/65\nHead-F1 (SLM) 45.9 40.4 54.1\nHead-F1 (LLM) 52.8 40.7 44.2\nAF1(LLM,SLM) 6.9 0.3 -9.9\n", "vlm_text": "The table presents results for three tasks in natural language processing:\n\n1. **Named Entity Recognition (NER)**:\n   - Datasets: CoNLL, OntoNotes, FewNERD\n   - Metrics: Micro-F1 scores for SLM and LLM.\n   - A difference in F1 scores (∆F1) between LLM and SLM is also listed.\n\n2. **Event Detection**:\n   - Datasets: ACE05, ERE, MAVEN\n   - Metrics: Micro-F1 scores for SLM and LLM.\n   - The difference in F1 scores (∆F1) between LLM and SLM is provided.\n\n3. **Event Argument Extraction**:\n   - Datasets: ACE05, ERE, RAMS\n   - Metrics: Head-F1 scores for SLM and LLM.\n   - The difference in F1 scores (∆F1) between LLM and SLM is noted.\n\nEach section includes the number of entities or events, and the tools used are Small Language Model (SLM) and Large Language Model (LLM). The F1 score differences highlight performance gains or losses using LLM over SLM."}
{"layout": 217, "type": "text", "text": "lation between the label number (row 2) and the performance difference (row 5). In other words, with more label types, LLMs tend to perform rel- atively worse than SLMs. Therefore we conclude that LLMs struggle on fine-grained datasets. ", "page_idx": 16, "bbox": [305, 191.6640167236328, 526, 259.0065002441406], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 218, "type": "text", "text": "E.2 Finding Better Instruction ", "text_level": 1, "page_idx": 16, "bbox": [305, 269, 456, 281], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 219, "type": "text", "text": "To investigate whether LLMs would benefit from complex instructions, we explored six instruction variants from simple to complex. Take NER task as an example, we illustrate them as below. ", "page_idx": 16, "bbox": [305, 285.53802490234375, 526, 339.33148193359375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 220, "type": "text", "text": "Instruction 0 :  [empty] ", "page_idx": 16, "bbox": [305, 339.34228515625, 408.31591796875, 353.5350341796875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 221, "type": "text", "text": "Instruction 1 : Identify the entities expressed by each sentence, and locate each entity to words in the sentence. The possible entity types are: [Type_1], [Type_2], ..., [Type_N]. If you do not find any entity in this sentence, just output ‘Answer: No entities found.’ ", "page_idx": 16, "bbox": [305, 352.8912658691406, 526, 446.5899658203125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 222, "type": "text", "text": "Instruction 2 : Identify the entities expressed by each sentence, and locate each entity to words in the sentence. The possible entity types are: ", "page_idx": 16, "bbox": [305, 447.7352600097656, 526, 500.78692626953125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 223, "type": "text", "text": "•  [Type_1]: [Definition 1] •  [Type_2]: [Definition 2] •  ... •  [Type_N]: [Definition N] ", "page_idx": 16, "bbox": [319, 510.35400390625, 458, 589.9214477539062], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 224, "type": "text", "text": "If you do not find any entity in this sentence, just output ‘Answer: No entities found.’ ", "page_idx": 16, "bbox": [305, 599.324951171875, 526, 637.4629516601562], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 225, "type": "text", "text": "Instruction 3 : Assume you are an entity-instance annotator. Given a sentence, you need to (1) identify the word or phrase about the entity in the sentence, and (2) classify its entity type. The possible entity types are listed as below: [Type_1], [Type_2], . . . , [Type_N]. Please note that your annotation results must follow such format: ”’Answer: ([Type_1] <SEP> identified entity:[Entity_1]), ([Type_2]\n\n <SEP> identified entity:[Entity_2]),\n\n ......”’. If you do not find any entity in this sentence, just output ‘Answer: No entities found.’ ", "page_idx": 16, "bbox": [305, 638.6082763671875, 526, 772.9548950195312], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 226, "type": "text", "text": "", "page_idx": 17, "bbox": [70, 72.71591186523438, 291, 137.95294189453125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 227, "type": "text", "text": "Instruction 4 : Assume you are an ", "page_idx": 17, "bbox": [70, 140.0162811279297, 291, 154.20901489257812], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 228, "type": "text", "text": "entity-instance annotator. Your objective is to perform a series of intricate steps for Named Entity Recognition. Firstly, you have to identify a particular word or phrase in the sentence that corresponds to an entity. Following this, classify the entity into one of the potential entity types. The potential entity types are provided as below: [Type_1], [Type_2], . . . , [Type_N]. Please note that your annotation results must follow such format: ‘Answer: ([Type_1] <SEP> identified entity:[Entity_1]), ([Type_2]\n\n <SEP> identified entity:[Entity_2]),\n\n ......’. If you do not find any entity in this sentence, just output ‘Answer: No entities found.’ ", "page_idx": 17, "bbox": [70, 154.92892456054688, 291, 396.3050537109375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 229, "type": "text", "text": "Instruction 5 : Assume you are an entity-instance annotator. Given a sentence, you need to (1) identify the word or phrase about the entity in the sentence, and (2) classify its entity type. The possible entity types are listed as below: ", "page_idx": 17, "bbox": [70, 398.3682556152344, 291, 492.06695556640625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 230, "type": "text", "text": "•  [Type_1]: [Definition 1] •  [Type_2]: [Definition 2] •  ... •  [Type_N]: [Definition N] ", "page_idx": 17, "bbox": [81, 506.24200439453125, 223, 597.9444580078125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 231, "type": "text", "text": "Please note that your annotation results must follow such format: ‘Answer: ([Type_1] <SEP> identified entity:[Entity_1]), ([Type_2]\n\n <SEP> identified entity:[Entity_2]),\n\n ......’. If you do not find any entity in this sentence, just output ‘Answer: No entities found.’ ", "page_idx": 17, "bbox": [70, 611.9559936523438, 291, 717.8399658203125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 232, "type": "text", "text": "Regarding these six instructions, we evaluate their performance of ChatGPT on four 20-shot IE tasks. As shown in Table  12 , there is no signifi- cant correlation between the instruction complexity ", "page_idx": 17, "bbox": [70, 720.2960205078125, 291, 774.0894775390625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 233, "type": "table", "page_idx": 17, "img_path": "layout_images/2303.08559v2_21.jpg", "table_caption": "Table 12: F1-scores across six instruction formats. Ex- periments run on 20-shot settings with ChatGPT. ", "bbox": [305, 70, 527, 198], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "FewNERD | TACREV ACE ACE\n\n| (NER) (RE) | (ED) | (EAE)\na0) 57.6(2.1) A9.1(2.4) 44.0(1.4) | 50.9(0.1)\nIl 58.3(0.5) 49.6(1. | 42.6(1.0) | 51.5(.1)\n2 57.7(1.0) 50.0(2. 41.8(0.9) | 50.3(1.5)\nB 57.6(2.3) 52.3. : 42.9(1.3) | 49.2(2.3)\n14 56.8(0.9) 49 .6(2.9) 41.6(1.9) | 49.9(1.2)\nI5 57.8(0.5) A7.2(1.8) 43.1(.8) | 50.6(1.8)\n", "vlm_text": "The table presents performance results across different datasets and tasks. Here's a brief breakdown:\n\n- **Tasks/Datasets**: \n  - FewNERD (NER)\n  - TACREV (RE)\n  - ACE (ED)\n  - ACE (EAE)\n\n- **Rows** (labeled I0 to I5): Represent different configurations or models.\n\n- **Values**:\n  - Each cell contains a primary value (e.g., 57.6) with a value in parentheses indicating a measure of variability or error (e.g., 2.1).\n\nThese values likely represent performance metrics such as accuracy, F1-score, etc., accompanied by standard deviations or confidence intervals."}
{"layout": 234, "type": "text", "text": "and LLMs’ performance. Even the prompt with- out instruction (I0) leads to comparable, if not bet- ter, results than prompt with complex instructions. Therefore, we use simple instruction (I1) in our main experiment. ", "page_idx": 17, "bbox": [305, 217.3310089111328, 526, 284.6734924316406], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 235, "type": "text", "text": "E.3 Do More Samples in Demos Help? ", "text_level": 1, "page_idx": 17, "bbox": [305, 294, 493, 307], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 236, "type": "text", "text": "We wonder whether longer demos bring more pow- erful ICL abilities for LLMs. Thus we investigate the impact of increasing the number of demon- strations on LLMs’ performance in Figure  8 . We observe that: (1) The performance of the RE task consistently improves with more demos, indicating its potential benefiting from additional annotations. (2) The NER and ED tasks reach a stable or de- graded performance with increased demo numbers, suggesting that they are limited even before reach- ing the maximum input length. (3) Open-source LLMs,  i.e.,  LLaMA and Vicuna, have more limited capacities in leveraging demos compared to Ope- nAI models, with their performance stagnating or even collapsing with only a few (2-4) demos. ", "page_idx": 17, "bbox": [305, 311.26397705078125, 526, 514.0974731445312], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 237, "type": "text", "text": "E.4 Finding Better Demo Selection Strategy ", "text_level": 1, "page_idx": 17, "bbox": [306, 524, 518, 536], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 238, "type": "text", "text": "The maximum input length of LLMs usually limits the sentence number in demos even under few- shot settings. For each test sentence    $s$  , we de- mand a de  retriever    ${\\mathcal{E}}(D,s)$   which selects a subset from  D  as the sentences in demo. Following previous work, we consider three commonly-used strategies. (1) Random sampling. (2) Sentence- embedding ( Liu et al. ,  2022 ;  Su et al. ,  2022 ): re- trieving the top-K nearest sentences measured by sentence embedding. We compute the embeddings by  SimCSE-RoBERTa-large  ( Gao et al. ,  2021 ). ", "page_idx": 17, "bbox": [305, 540.68798828125, 526, 689.325439453125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 239, "type": "equation", "text": "\n$$\n\\begin{array}{r}{\\mathcal{E}(D,s)=\\mathrm{arg-top}\\mathbf{K}_{s^{\\prime}\\in D}[\\mathrm{Sent-embed}(s^{\\prime},s)]}\\end{array}\n$$\n ", "text_format": "latex", "page_idx": 17, "bbox": [311, 698, 507, 714], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 240, "type": "text", "text": "(3) Efficient Prompt Retriever ( Rubin et al. ,  2022 ): retrieving by a neural retriever    $R$   trained on    $D$  . ", "page_idx": 17, "bbox": [305, 723.656005859375, 526, 750.3514404296875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 241, "type": "equation", "text": "\n$$\n\\mathcal{E}(D,s)=\\mathrm{arg-top}{\\bf K}_{s^{\\prime}\\in D}[R_{D}(s^{\\prime},s)]\n$$\n ", "text_format": "latex", "page_idx": 17, "bbox": [335, 759, 495, 775], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 242, "type": "image", "page_idx": 18, "img_path": "layout_images/2303.08559v2_22.jpg", "img_caption": "(b) Open-source LLMs ", "bbox": [81, 67, 514, 303], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "60\n56\n& 52\n3 48\nL 44\n40\n36\n\n28\n@ 24\n£\n\n9 20\n016\n2\n\naa\n\n4\n\n2\n\n8 16 32 64\nFewNERD (NER)\n\n4 8 1\nFewNERD (NER)\n\n96\n\nChatGPT -- CODEX\n60\n56\n& 52\n$3 48\nL 44\n40\n36\n\n8 16 32 64\nTACREV (RE)\n(a) OpenAI LLMs\n\n—- LLaMA (13B) —— Vicuna (13B)\n28\n@ 24\n£\n9 20\n0 16\n2\n8\n4\n16\n\n4 8\nTACREV (RE)\n\nS\no\n\nrN\nB\n\npas\n\n8 16 32 64\nACE05 (ED)\n\nF1 score\no >\n$8\n\noo\nio\n\n28\n@ 24\n£\n\n9 20\na 16\ni\n\n4 8 16\nACEO5 (ED)\n", "vlm_text": "The image consists of two sets of graphs comparing F1 scores for language models. \n\n- **Top Row (a) OpenAI LLMs:** Shows graphs for ChatGPT and Codex models.\n  - **FewNERD (NER):** F1 score increases for Codex consistently, while ChatGPT scores fluctuate.\n  - **TACREV (RE):** Both ChatGPT and Codex show improving scores as data increases.\n  - **ACE05 (ED):** Codex maintains a higher score than ChatGPT, with both showing score increments.\n\n- **Bottom Row (b) Open-source LLMs:** Shows graphs for LLaMA and Vicuna models.\n  - **FewNERD (NER):** Initial scores are higher for Vicuna, but both models' scores drop as data increases.\n  - **TACREV (RE):** Both models improve with more data.\n  - **ACE05 (ED):** LLaMA maintains a relatively stable higher score than Vicuna although both decrease with more data.\n\nEach graph measures F1 scores for different natural language processing tasks, indicating performance changes as the amount of data increases."}
{"layout": 243, "type": "text", "text": "Figure 8: Relationship between demo number and F1-score among three datasets. Note that the   $\\mathbf{X}$  -axis in each subfigure represents the number of demos (not the shot value    $K$  ) during ICL. We adopt sentence embedding as the demo selection strategy and text prompt in this experiment. ", "page_idx": 18, "bbox": [70, 317.841552734375, 524, 353.7565002441406], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 244, "type": "text", "text": "For each test sentence    $s$  , we pre-retrieve    $M$   sim- ilar sentences  $\\bar{D}~=~\\{(s_{i}^{\\prime},y_{i}^{\\prime})\\}_{i=1}^{M}~\\subset~D$    . Then we score each sentence in    $\\bar{D}$   by their likelihoods  $P_{\\mathcal{L}}(f(y_{i}^{\\prime})|f(s_{i}^{\\prime}))$  |  where    $f$   denotes the prompt for- mat adopted and  $\\mathcal{L}$   the scoring LM. We randomly select positive samples    $s_{\\,\\,i}^{\\prime{\\mathrm{(pos)}}}$  from the top-  $K_{D}$   sen- tences and hard negative samples  $s_{\\,\\,i}^{\\prime{\\mathrm{(hard-neg)}}}$  from the bottom-  $\\mathit{K}_{D}$   ones. Then we train    $R_{D}$   by in- batch contrastive learning ( Chen et al. ,  2020 ). For each sentence  $s_{i}^{\\prime}$    within the batch, there are 1 posi- tive sentences    $s_{\\,\\,i}^{\\prime{\\mathrm{(pos)}}}$  and    $2B\\!-\\!1$   negative sentences  $\\{s^{\\prime}{}_{j}^{(\\mathrm{hard-neg})}\\}_{j=1}^{B}\\cup\\{s^{\\prime}{}_{j}\\}_{j\\neq i}^{B}$    . Here we adopt    $M$  as 40,    $K_{D}$   as 5,  $f$   as text prompt, the batch size  B  as 128, and the scoring LM    $\\mathcal{L}$   as  $\\mathsf{F L A N-T5-x1}$  . ", "page_idx": 18, "bbox": [70, 375.3479919433594, 291, 580.6697387695312], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 245, "type": "table", "page_idx": 18, "img_path": "layout_images/2303.08559v2_23.jpg", "table_caption": "Table 13: F1-scores on three demo-selection strategies. Experiments run on 20-shot settings with ChatGPT. ", "bbox": [69, 583, 291, 682], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "FewNERD TACREV ACE\n(NER) (RE) (ED)\n\n53.2(0.4) 43.0(3.3) 38.0(1.5)\n57.6(2.3) 49.6(1.2) 42.9(1.3)\n57.2(0.6) 48.0(0.8) 43.5(1.4)\n\nRandom Sampling\nSentence Embedding\nEfficient Prompt Retriever|\n\n", "vlm_text": "The table presents a comparison of three methods across three tasks: FewNERD (NER), TACREV (RE), and ACE (ED). The methods compared are:\n\n1. **Random Sampling**\n   - FewNERD: 53.2 (±0.4)\n   - TACREV: 43.0 (±3.3)\n   - ACE: 38.0 (±1.5)\n\n2. **Sentence Embedding**\n   - FewNERD: 57.6 (±2.3)\n   - TACREV: 49.6 (±1.2)\n   - ACE: 42.9 (±1.3)\n\n3. **Efficient Prompt Retriever**\n   - FewNERD: 57.2 (±0.6)\n   - TACREV: 48.0 (±0.8)\n   - ACE: 43.5 (±1.4)\n\nThe numbers indicate performance scores with standard deviations in parentheses."}
{"layout": 246, "type": "text", "text": "Table  13  demonstrates the F1-score performance on different selection strategies. We find that both the sentence embedding and EPR surpass random sampling by a large margin. Given the simplicity of the sentence embedding, we adopt it, rather than EPR, as our selection strategy in main experiment. ", "page_idx": 18, "bbox": [70, 693.197998046875, 291, 774.0894775390625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 247, "type": "table", "page_idx": 18, "img_path": "layout_images/2303.08559v2_24.jpg", "table_caption": "Table 14: F1-scores across three prompt formats. Ex- periments run on 20-shot settings with ChatGPT. ", "bbox": [305, 373, 525, 461], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "FewNERD TACREV ACE ACE\n(NER) (RE) (ED) (EAE)\n\nText\nCode\n\n57.6(2.3) 49.6(1.2) 42.9(1.3) 51.5(1.1)\n53.2(0.9) 50.2(1.8) 44.3(2.0) 47.3(1.5)\n\n", "vlm_text": "The table presents results for different tasks across two conditions (Text and Code):\n\n- **Tasks / Datasets:**\n  - FewNERD (NER)\n  - TACREV (RE)\n  - ACE (ED)\n  - ACE (EAE)\n\n- **Text Results:**\n  - FewNERD: 57.6 (±2.3)\n  - TACREV: 49.6 (±1.2)\n  - ACE (ED): 42.9 (±1.3)\n  - ACE (EAE): 51.5 (±1.1)\n\n- **Code Results:**\n  - FewNERD: 53.2 (±0.9)\n  - TACREV: 50.2 (±1.8)\n  - ACE (ED): 44.3 (±2.0)\n  - ACE (EAE): 47.3 (±1.5)\n\nThe numbers in parentheses indicate the standard deviation."}
{"layout": 248, "type": "text", "text": "E.5 Finding Better Prompt Format ", "text_level": 1, "page_idx": 18, "bbox": [305, 483, 477, 496], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 249, "type": "text", "text": "Previous studies on LLMs for few-shot IE tasks have explored different prompt formats and high- lighted the importance of selecting an appropri- ate format for achieving competitive performance. Therefore, we investigate two commonly-used vari- ants in previous work: (1) Text prompt as shown in Figure  1 . (2) Code prompt: We follow  Wang et al. ( 2023b );  Li et al.  ( 2023 ) and recast the output of IE tasks in the form of code. See more details about this format in their original papers. ", "page_idx": 18, "bbox": [305, 502.16802978515625, 526, 637.2564697265625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 250, "type": "text", "text": "Table  14  shows comparable performance across all formats. Based on simplicity, we choose the text prompt for our main experiment. ", "page_idx": 18, "bbox": [305, 638.4359741210938, 526, 678.680419921875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 251, "type": "text", "text": "F Case Study ", "text_level": 1, "page_idx": 18, "bbox": [306, 691, 384, 705], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 252, "type": "text", "text": "F.1 Hard Samples ", "text_level": 1, "page_idx": 18, "bbox": [306, 714, 398, 729], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 253, "type": "text", "text": "Table  15  showcases some  hard  examples which benefits from our LLM reranking. In accordance with our intuition, we observe that the LLM rerankers correct two kinds of erroneous predic- tions made by LLMs. (1) The lack of external knowledge, such as the first ( Trip to l emus is a fig- ure in Greek mythology ) and third examples ( Mi- nas Gerais is a state instead of city ). (2) Limited reasoning abilities, such as the second ( His wife’s children are his children ) and the fourth ( The word \"fought\" in this sentence does not involve any phys- ical violence ) examples. ", "page_idx": 18, "bbox": [305, 733.8450317382812, 526, 774.0894775390625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 254, "type": "text", "text": "", "page_idx": 19, "bbox": [70, 71.74500274658203, 291, 193.28445434570312], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 255, "type": "text", "text": "F.2 Easy Samples ", "text_level": 1, "page_idx": 19, "bbox": [70, 231, 160, 244], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 256, "type": "text", "text": "As discussed in Section  4.3 , we attribute the in- ferior performance of LLMs on easy samples to their false-positive predictions for negative samples. Upon examining the incorrect cases, we speculate that two main reasons are responsible: (1) LLM hallucination and (2) span boundary mismatch. We provide details on them with specific cases below. ", "page_idx": 19, "bbox": [70, 265.04998779296875, 291, 359.4914855957031], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 257, "type": "text", "text": "LLM Hallucination . The most distinctive mis- takes made by LLMs. Without specific fine-tuning on IE tasks, LLMs (with in-context learning ap- proaches) sometimes hallucinate and provide mean- ingless, even confusing explanations or answers. For instance, LLMs might predict a verb, preposi- tion or even special symbols as an entity. ", "page_idx": 19, "bbox": [70, 367.7692565917969, 291, 462.60247802734375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 258, "type": "text", "text": "Case 1 : LLMs predict the word  die  as entity person-other  (Ground truth:  None ) ", "page_idx": 19, "bbox": [70, 471.27301025390625, 291, 497.9684753417969], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 259, "type": "text", "text": "Sentence:  Flavius Rumoridus (died 5th cen- tury AD) was a Roman soldier who was ap- pointed consul in AD 403 in the Western Ro- man Empire at the same time as the eastern emperor Theodosius II served in the same capacity in the East. ", "page_idx": 19, "bbox": [82, 531.2753295898438, 278, 612.5594482421875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 260, "type": "text", "text": "Candidate Choices: ", "text_level": 1, "page_idx": 19, "bbox": [82, 627, 175, 640], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 261, "type": "text", "text": "(a)  died do/does not belong to any known entities. (b) died is a kind of geopolitical entity. (c)  died is a person, but not affiliated with following professions: actor, artist, ath- lete, author, director, politician, scholar, soldier. ", "page_idx": 19, "bbox": [86, 649.6519775390625, 278, 762.0254516601562], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 262, "type": "text", "text": "Analysis:  The sentence provides information about Flavius Rumoridus, who was a Roman soldier and appointed consul in AD 403. The sentence also mentions that he died in the 5th century AD, which is a statement about his death. Therefore, the correct answer is (c) because the sentence does not provide any information about Flavius Rumoridus being affiliated with any of the professions listed in option (c). ", "page_idx": 19, "bbox": [317, 79.86534118652344, 514, 215.34652709960938], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 263, "type": "text", "text": "Correct Answer:  (c) ", "text_level": 1, "page_idx": 19, "bbox": [317, 230, 413, 243], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 264, "type": "text", "text": "Case 2 : LLMs predict the word  of  as entity organization-other  (Ground truth:  None ) ", "page_idx": 19, "bbox": [306, 263.32708740234375, 526, 290.0215148925781], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 265, "type": "text", "text": "Sentence:  During these years he studied at the South Kensington Schools of the Royal College of Art. ", "page_idx": 19, "bbox": [317, 310.0322570800781, 514, 350.6684875488281], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 266, "type": "text", "text": "Candidate Choices: ", "text_level": 1, "page_idx": 19, "bbox": [317, 365, 410, 378], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 267, "type": "text", "text": "(a)  of do/does not belong to any known en- tities. ", "page_idx": 19, "bbox": [322, 387.760986328125, 514, 414.4554748535156], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 268, "type": "text", "text": "(b) of is an educational institute. ", "page_idx": 19, "bbox": [322, 423.82598876953125, 465.6576232910156, 436.9714660644531], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 269, "type": "text", "text": "(c)  of pertains to an organization that does not fall under the categories of com- pany, educational institution, govern- ment, media, political party, religion, sports league, sports team, band or mu- sical group. ", "page_idx": 19, "bbox": [322, 446.34100341796875, 514, 527.232421875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 270, "type": "text", "text": "Analysis:  In the given sentence,  of  is a prepo- sition that indicates the relationship between South Kensington Schools  and  Royal College of Art . None of the candidate choices directly address the correct relationship between these entities or state that they are not applicable. Therefore, the correct answer is (c):  of  does not pertain to any known entities mentioned in the choices provided. ", "page_idx": 19, "bbox": [317, 550.38330078125, 514, 672.3154296875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 271, "type": "text", "text": "Correct Answer:  (c) ", "text_level": 1, "page_idx": 19, "bbox": [317, 687, 413, 699], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 272, "type": "text", "text": "Span boundary mismatch . Different from sentence-level approaches which take one or mul- tiple sentences as an unit and assign each word’s label globally, LLMs under our filter-then-rerank paradigm adopt sample-level approaches. In other words, LLMs only consider each word/span candi- date independently. The sample-level approaches enable LLMs to rerank candidates with the help of multi-choice question prompt. However, they also aggravate the span boundary mismatching prob- lem in NER and ED tasks. We empirically find easy samples suffering much more from boundary mismatching problem than hard samples. ", "page_idx": 19, "bbox": [306, 719.9033813476562, 526, 774.0894775390625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 273, "type": "text", "text": "", "page_idx": 20, "bbox": [70, 71.74500274658203, 290, 193.28445434570312], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 274, "type": "text", "text": "Case 3 : LLMs predict  Baron , a subspan of  Baron Carl Hurleman , as the entity  person-other  (The ground-truth entity type of  Baron  is  None . And the ground-truth entity type of  Baron Carl Hurleman is  person-artist/author ). ", "page_idx": 20, "bbox": [70, 198.0709991455078, 290, 265.4134826660156], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 275, "type": "text", "text": "Sentence:  Additions in the 1740s were by Baron Carl Hurleman (1700 - 1753). ", "page_idx": 20, "bbox": [82, 290.9532775878906, 279, 318.0404968261719], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 276, "type": "text", "text": "Candidate Choices: ", "text_level": 1, "page_idx": 20, "bbox": [82, 332, 174, 345], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 277, "type": "text", "text": "(a)  Baron do/does not belong to any known entities. (b)  Baron is a person, but not affiliated with following professions: actor, artist, ath- lete, author, director, politician, scholar, soldier. (c) Baron is a kind of geopolitical entity ", "page_idx": 20, "bbox": [86, 355.13299560546875, 279, 467.5064697265625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 278, "type": "text", "text": "Analysis:  In the given sentence,  Baron  is a title given to a person named Carl Hurleman, who is mentioned as the one who made ad- ditions in the 1740s. The sentence does not provide any information about connections to geopolitical entities or any other known entities mentioned in the choices provided. Therefore, the correct answer is (b): Baron is a person, but not affiliated with following pro- fessions: actor, artist, athlete, author, director, politician, scholar, soldier. ", "page_idx": 20, "bbox": [82, 490.6572570800781, 279, 639.6874389648438], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 279, "type": "text", "text": "Correct Answer:  (b) ", "text_level": 1, "page_idx": 20, "bbox": [82, 654, 178, 666], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 280, "type": "text", "text": "Sentence:  Ralph Fiennes was nominated for the BAFTA Award for Outstanding Debut by a British Writer, Director or Producer at the 65th British Academy Film Awards. ", "page_idx": 20, "bbox": [317, 79.86528015136719, 514, 134.05044555664062], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 281, "type": "text", "text": "Candidate Choices: ", "text_level": 1, "page_idx": 20, "bbox": [317, 149, 410, 161], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 282, "type": "text", "text": "(a)  65th do/does not belong to any known entities. (b) 65th is a kind of award. (c)  65th refers to some event except attack, election, natural disaster, protest, revolu- tion and sports. ", "page_idx": 20, "bbox": [322, 171.1430206298828, 514, 269.9674987792969], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 283, "type": "text", "text": "Analysis:  The sentence mentions the    $65t h$  British Academy Film Awards , which is an event that recognizes outstanding achieve- ments in the film industry. Therefore, option (c) is incorrect as it excludes events like award ceremonies. Option (a) is also incorrect as the number    $65t h$   is associated with the event. Hence, the correct answer is (b) as    $65t h$   refers to the specific edition of the BAFTA Awards. ", "page_idx": 20, "bbox": [317, 293, 514, 415.05047607421875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 284, "type": "text", "text": "Correct Answer:  (b) ", "text_level": 1, "page_idx": 20, "bbox": [317, 429, 413, 442], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 285, "type": "text", "text": "G Details on Adaptive  Filter-then-rerank Paradigm ", "text_level": 1, "page_idx": 20, "bbox": [305, 469, 522, 497], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 286, "type": "text", "text": "G.1 Demonstration Examples ", "text_level": 1, "page_idx": 20, "bbox": [305, 505, 452, 518], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 287, "type": "text", "text": "We convert few-shot IE tasks to multiple-choice questions in our  filter-then-rerank  paradigm. We show 4 examples used in demonstrations for FewN- ERD dataset in Table  16 , for TACREV dataset in Table  17 , and for ACE05 datasets in Table  18 . ", "page_idx": 20, "bbox": [305, 522.3469848632812, 526, 589.689453125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 288, "type": "text", "text": "G.2 Template ", "text_level": 1, "page_idx": 20, "bbox": [305, 599, 378, 612], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 289, "type": "text", "text": "In our  filter-then-rerank  paradigm, we utilize tem- plates converting candidate labels to question op- tions. We list the template for FewNERD dataset in Table  19 , for TACREV dataset in Table  20 , and for ACE05 datasets in Table  21 . ", "page_idx": 20, "bbox": [305, 616.406005859375, 526, 683.7484130859375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 290, "type": "text", "text": "Case 4 : LLMs predict    $65t h$  , a subspan of 65th British Academy Film Awards , as the en- tity  other-awards  (The ground-truth entity type of    $65t h$   is  None . And the ground-truth entity type of  65th British Academy Film Awards  is other-awards ). ", "page_idx": 20, "bbox": [70, 693, 290, 774.0894775390625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 291, "type": "table", "page_idx": 21, "img_path": "layout_images/2303.08559v2_25.jpg", "table_caption": "Table 15: Examples of the samples corrected by LLM reranking. We sample four examples from NER, RE and ED tasks, respectively.  Sentences : The sentences in which samples locate. We color the samples (entities or trigger words) to be identified.  Before : The prediction before LLM reranking. Based on SLM-based methods.  After : The reranked prediction using LLMs.  Rationales : LLM-generated Explanations. ", "bbox": [70, 221, 526, 621], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "Sentences (entity/event) Before After Rationales\n\nNamed Entity Recognition\nAccording to it, King Eleusis and _ person-other other-god According to the context, Triptolemus is a\nCothonea (Cyntinia), parents of Trip- x v figure in Greek mythology, the son of King\n\ntolemus, are visited by Demeter, who\nrears their son, feeding him divine milk\nby day and placing him into the fire at\nnight, which makes Triptolemus grow\n\nEleusis and Cothonea (Cyntinia). He was a\ndemigod who was sent by Demeter to teach\nmankind the art of agriculture. So Triptole-\nmus is a god in some legend/religious story.\n\nfaster than mortal children normally do.\nRelation Extraction\n\nSurvivors include his wife, Sandra; four None per:children The sentence states Survivors include his\nsons, Jeff, James, Douglas and Harris; a x v wife, Sandra; four sons, Jeff, James, Douglas\ndaughter, Leslie; his mother, Sally; and and Harris; a daughter, Leslie; his mother,\ntwo brothers, Guy and Paul. Sally; and two brothers, Guy and Paul.’ This\n\nindicates ’his’ is the father of Jeff, James,\nDouglas and Harris, and the husband of San-\ndra. So his is the parent of Jeff.\n\nEliza Samudio, Bruno ’s ex-girlfriend The sentence states that \"Eliza Samudio,\nand his 4-month-old baby boy’s mother, ; Bruno’s ex-girlfriend and his 4-month-old\ndisappeared from her hometown of Belo —esidenceX —_residence“ _ aby boy’s mother, disappeared from her\nHorizonte, Minas Gerais. hometown of Belo Horizonte, Minas Gerais.’\nThis indicates that Eliza Samudio is from the\ncity of Belo Horizonte, which is located in\nthe state of Minas Gerais. So Eliza Samudio\nlives in the state or province Minas Gerais.\n\nper:cities_of per:states_of\n\nEvent Detection\n\nWe have fought every institution, every | Conflict.Attack None In the given sentence, the word fought is\nlaw firm ... it’s cost us a fortune but now x v used to describe the action of the speaker\nwe ‘re going on a massive expansion. fighting against various institutions and law\n\nfirms. This does not involve any physical\nviolence or court proceedings, so the word\nfought does not trigger any known event.\n", "vlm_text": "This table contains information about text analysis, specifically dealing with named entity recognition, relation extraction, and event detection. Here's a breakdown:\n\n1. **Named Entity Recognition**\n   - **Sentence:** Mentions the story of Triptolemus and Greek mythology.\n   - **Before:** Recognized as \"person-other.\"\n   - **After:** Corrected to \"other-god\" based on context, as Triptolemus is a mythological figure.\n   - **Rationale:** Triptolemus is associated with Demeter in mythology, making him a god-like figure.\n\n2. **Relation Extraction**\n   - **Example 1:**\n     - **Sentence:** Refers to a family structure with names like Jeff and his relatives.\n     - **Before:** No relation recognized.\n     - **After:** Recognizes \"per:children\" relationship.\n     - **Rationale:** Sentence mentions family members, indicating Jeff is a child of the subject.\n   \n   - **Example 2:**\n     - **Sentence:** Talks about Eliza Samudio and her connection to Belo Horizonte, Minas Gerais.\n     - **Before:** Incorrectly labeled as \"per:cities_of_residence.\"\n     - **After:** Corrected to \"per:states_of_residence.\"\n     - **Rationale:** Indicates Eliza's residence in the state of Minas Gerais.\n\n3. **Event Detection**\n   - **Sentence:** Discussion about fighting institutions and law firms.\n   - **Before:** Incorrectly labeled as \"Conflict.Attack.\"\n   - **After:** No event detected.\n   - **Rationale:** The use of \"fought\" is metaphorical, discussing legal and institutional battles, not physical conflict.\n\nThe table uses checkmarks and crosses to indicate post-correction accuracy and includes explanations or rationales for each decision."}
{"layout": 292, "type": "table", "page_idx": 22, "img_path": "layout_images/2303.08559v2_26.jpg", "bbox": [78, 207, 519, 653], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "Instruct: Read following sentences and identify what is the entity type of 392 quoted by <t>.\n\nSentence: Powell v. Texas , <t> 392 <t> U.S. 514 ( 1968 ) , was a United States Supreme Court case that ruled that a\nTexas statute criminalizing public intoxication did not violate the Eighth Amendment protection against cruel and unusual\npunishment.\n\n(a) 392 is a legal document, a term or a convention in legal sense.\n\n(b) 392 does not belong to any known entities.\n\n(c) 392 refers to a protest, uprising or revolution event\n\n(d) 392 refers to a government or governmental agency\n\nAnalysis: In the context you provided, 392 refers to the volume number in the United States Reports where the Supreme\nCourt’s decision in Powell v. Texas can be found. However, 392 itself does not refer to a legal document. So 392 do/does\nnot belong to any known entities.\n\nAnswer: (b)\n\nInstruct: Read following sentences and identify what is the entity type of The New Yorker quoted by <t>.\n\nSentence: In 2004 Gourevitch was assigned to cover the 2004 U.S. presidential election for '\" <t> The New Yorker <t> \".\n(a) The New Yorker does not belong to any known entities.\n\n(b) The New Yorker is a broadcast program.\n\n(c) The New Yorker is a kind of written art.\n\n(d) The New Yorker is a media/newspaper organization.\n\nAnalysis: The New Yorker is a well-known American magazine that has been published since 1925, and is primarily\nknown for its long-form journalism, commentary, and satire. It has a reputation for publishing high-quality writing on a\nwide variety of topics, including politics, culture, and the arts. So The New Yorker is a media/newspaper organization.\nAnswer: (d)\n\nInstruct: Read following sentence and identify what is the entity type of St. quoted by <t>.\n\nSentence: The May 1980 eruption of Mount <t> St. <t> Helens in the state of Washington seriously affected both 47th Air\nDivision and 92d Bombardment Wing operations at Fairchild AFB , resulting in dispersal of Fairchild ’s B-52 and KC-135\naircraft to various bases while around-the-clock shifts removed the volcanic ash from facilities within the base perimeter. ”\n(a) St. does not belong to any known entities.\n\n(b) St. is a natural disaster event.\n\n(c) St. is a geographic position about mountain.\n\nAnalysis: According to the context, St. is an abbreviation of Saint, used in the name of Mount St. Helens, which is an\nactive volcano in the state of Washington. However, St. itself does not refer to anything. So St. do/does not belong to any\nknown entities.\n\nAnswer: (a)\n\nInstruct: Read following sentence and identify what is the entity type of Ridzuan quoted by <t>.\n\nSentence: <t> Ridzuan <t> was promoted to Harimau Muda A for 2014 season .\n\n(a) Ridzuan does not belong to any known entities.\n\n(b) Ridzuan is a person, but not affiliated with following professions: actor, artist, author, director, politician, scholar,\nsoldier.\n\n(c) Ridzuan is an athlete.\n\nAnalysis: The mention of ’Harimau Muda A’ indicates that it is a sports-related context, and ‘promoted’ implies a\nprogression or advancement within the sports team. So Ridzuan is an athlete.\n\nAnswer: (c)\n\n", "vlm_text": "The table contains several sections, each with instructions to identify the entity type of a quoted term in a sentence. The sections include:\n\n1. **Entity:** `392`\n   - **Sentence:** Discusses a U.S. Supreme Court case (Powell v. Texas).\n   - **Options:** Legal document, known entity, protest/revolution event, government agency.\n   - **Answer:** b) 392 does not belong to any known entities.\n\n2. **Entity:** `The New Yorker`\n   - **Sentence:** Mentions 2004 U.S. presidential election coverage.\n   - **Options:** Not a known entity, broadcast program, kind of art, media/newspaper organization.\n   - **Answer:** d) The New Yorker is a media/newspaper organization.\n\n3. **Entity:** `St.`\n   - **Sentence:** Discusses the 1980 eruption of Mount St. Helens.\n   - **Options:** Not a known entity, natural disaster event, geographic position.\n   - **Answer:** a) St. does not belong to any known entities.\n\n4. **Entity:** `Ridzuan`\n   - **Sentence:** Mentions the promotion to Harimau Muda A for 2014 season.\n   - **Options:** Not a known entity, person with various professions, athlete.\n   - **Answer:** c) Ridzuan is an athlete."}
{"layout": 293, "type": "text", "text": "Instruct : Read the sentence and determine the relation between  she  and  lawyer  quoted by <t>. Sentence : The  ${<}\\mathrm{t}{>}$   lawyer  ${<}\\mathrm{t}{>}$   denied Italian news reports that she wept while addressing the court, but said Knox was upset as   ${<}\\mathrm{t}{>}$   she   ${<}\\mathrm{t}{>}$   recounted “ the pressure, the aggressiveness of the police who called her a liar . ”\n\n (a) she is the other family member of lawyer\n\n (b) she is a lawyer\n\n (c) she has no known relations to lawyer ", "page_idx": 23, "bbox": [81, 208.1372528076172, 514, 269.0776062011719], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 294, "type": "text", "text": "Analysis : In the sentence, the word ’she’ refers to someone who was upset while recounting certain events in court. The word ’lawyer’ refers to someone who denied a news report about that same person weeping in court. There is no information in the sentence to indicate that the two individuals are related in any way. So she has no known relations to lawyer. ", "page_idx": 23, "bbox": [81, 267.9132385253906, 514, 308.9285583496094], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 295, "type": "text", "text": "Sentence : YANGON , Dec. 27 -LRB- Xinhua -RRB- – Myanmar will hold a horse race in Yangon to commemorate the country ’s 63rd Anniversary Independence Day , the <t> Myanmar Equestrian Federation <t> -LRB- <t> MEF <t> -RRB- confirmed to Xinhua on Monday.\n\n (a) MEF is also known as Myanmar Equestrian Federation\n\n (b) MEF has political affiliation with Myanmar Equestrian Federation\n\n (c) MEF has no known relations to Myanmar Equestrian Federation ", "page_idx": 23, "bbox": [81, 333.38824462890625, 514, 394.3296203613281], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 296, "type": "text", "text": "Analysis : The symbols -LRB- and -RRB- in the sentence stand for left and right round brackets and are used to enclose the abbreviation ’MEF’ to indicate that it is a replacement for the longer name ’Myanmar Equestrian Federation. So MEF is also known as Myanmar Equestrian Federation. Answer : (a) ", "page_idx": 23, "bbox": [81, 393.16424560546875, 514, 434.71759033203125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 297, "type": "text", "text": "Instruct : Read the sentence and determine the relation between  Douglas Flint  and  chairman  quoted by  ${<}\\mathrm{t}{>}$  . ", "text_level": 1, "page_idx": 23, "bbox": [82, 438, 470, 447.75], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 298, "type": "text", "text": "Sentence : At the same time , Chief Financial Officer <t> Douglas Flint <t> will become   ${<}\\mathrm{t}{>}$   chairman  ${<}\\mathrm{t}{>}$   , succeeding Stephen Green who is leaving to take a government job.\n\n (a) Douglas Flint has no known relations to chairman\n\n (b) Douglas Flint is a chairman\n\n (c) Douglas Flint is the employee of chairman ", "page_idx": 23, "bbox": [81, 448.67724609375, 514, 499.6556091308594], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 299, "type": "text", "text": "Analysis : The sentence states that Chief Financial Officer Douglas Flint Douglas Flint will succeed Stephen Green as a chairman. So Douglas Flint is a chairman. Answer : (b) ", "page_idx": 23, "bbox": [81, 498.49127197265625, 514, 530.0816040039062], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 300, "type": "text", "text": "Instruct : Read the sentence and determine the relation between  FAA  and  U.S.  quoted by <t>. Sentence : On its Web site , the <t> U.S. <t> <t> FAA  ${<}\\mathrm{t}{>}$   says the Category 2 rating means the country lacks the laws or regulations that are needed for the certification and oversight of air carriers , according to minimum international standards.\n\n (a) FAA is also known as U.S.\n\n (b) FAA has no known relations to U.S.\n\n (c) FAA has a headquarter in the country U.S. Analysis : The sentence states that the FAA says the Category 2 rating means the country lacks the laws or regulations needed for the certification and oversight of air carriers, indicating that the FAA is responsible for overseeing aviation regulations in the country. Actually the FAA (Federal Aviation Administration) is a U.S. government agency responsible for regulating and overseeing civil aviation in the United States, and it has its headquarters in Washington, D.C.. So FAA has a headquarter in the country U.S.. ", "page_idx": 23, "bbox": [81, 534.0783081054688, 514, 644.83251953125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 301, "type": "text", "text": "Instruct : Read following sentences and identify what event is triggered by the word  loan  quoted by  ${<}\\mathrm{t}{>}$  . Sentence : Separately , former WorldCom CEO Bernard Ebbers failed on April 29 to make a first repayment of 25 million dollars, plus interest, on a 400-million - dollar  ${<}\\mathrm{t}{>}$   loan   ${<}\\mathrm{t}{>}$   from MCI, the Journal said, citing SEC documents.\n\n (a) The word loan does not trigger any known event.\n\n (b) The word loan triggers a TRANSFER-MONEY event: giving, receiving, borrowing, or lending money when it is NOT in the context of purchasing something. (c) The word loan triggers a DECLARE-BANKRUPTCY event: an Entity officially requests legal protection from debt collection due to an extremely negative balance sheet. Analysis : In the given sentence, the word loan is used to describe the 400-million-dollar loan from MCI to former WorldCom CEO Bernard Ebbers, which he failed to repay on time. This situation clearly involves a transfer of money without the context of purchasing something, which falls under the TRANSFER-MONEY event. So the word loan triggers a TRANSFER-MONEY event: giving, receiving, borrowing, or lending money when it is NOT in the context of purchasing something. Answer : (b) Instruct : Read following sentences and identify what event is triggered by the words  treated  quoted by <t>. Sentence : When she ’s in Germany , Lynch will be   ${<}\\mathrm{t}{>}$   treated  ${<}\\mathrm{t}{>}$   for bullet wounds and broken bones . (a) The word treated triggers an INJURE event: a PERSON gets/got injured whether it occurs accidentally, intentionally or even self-inflicted.\n\n (b) The word treated does not trigger any known event.\n\n (c) The word treated triggers a TRANSPORT event: an ARTIFACT (WEAPON or VEHICLE) or a PERSON is moved from one PLACE (GEOPOLITICAL ENTITY, FACILITY, LOCATION) to another. Analysis : The sentence suggests that Lynch has already been injured and will receive medical treatment in Germany for her injuries. The word ’treated’ simply describes the medical care she will receive and does not indicate a new event or action taking place. So the word treated does not trigger any known event. Answer : (b) Instruct : Read following sentences and identify what event is triggered by the words  buy  quoted by  ${<}\\mathrm{t}{>}$  . Sentence : And I won’t dwell on the irony of an Oracle employee being driven out of Oracle , starting his own company , and forcing Ellison to spend  $\\S~10.3$   billion to get his company – but not him – back ( though it does rather delightfully remind me of Coca - Cola basically giving away the bottling franchise and then spending billions to  ${<}\\mathrm{t}{>}$   buy  ${<}\\mathrm{t}{>}$   it back ) . (a) The word buy triggers a DECLARE-BANKRUPTCY event: an Entity officially requests legal protection from debt collection due to an extremely negative balance sheet. (b) The word buy triggers a TRANSFER-OWNERSHIP event: The buying, selling, loaning, borrowing, giving, or receiving of artifacts or organizations by an individual or organization. (c) The word buy does not trigger any known event. Analysis : In the given sentence, the word buy is used to describe the action of Oracle spending   $\\S10.3$   billion to get a company back. This clearly involves the transfer of ownership of the company from one entity to another. So the word buy triggers a TRANSFER-OWNERSHIP event: The buying, selling, loaning, borrowing, giving, or receiving of artifacts or organizations by an individual or organization. Answer : (b) Instruct : Read following sentences and identify what event is triggered by the words  set  quoted by <t>. Sentence : British forces also began establishing the country’s first postwar administration Tuesday, granting a local sheik power to <t> set <t> up an administrative committee representing the groups in the region. (a) The word set triggers a START-POSITION event: a PERSON elected or appointed begins working for (or changes offices within) an ORGANIZATION or GOVERNMENT.\n\n (b) The word set triggers a START-ORG event: a new ORGANIZATION is created.\n\n (c) The word set does not trigger any known event. Analysis : The phrase ’set up’ specifically implies the creation or establishment of a new organization or entity, rather than simply the word ’set’. So the word set does not trigger any known event. Answer : (c) ", "page_idx": 24, "bbox": [81, 178.24925231933594, 514, 685.2205810546875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 302, "type": "table", "page_idx": 25, "img_path": "layout_images/2303.08559v2_27.jpg", "table_caption": "Table 19: Templates for FewNERD dataset, where   $\\{{\\mathrm{ent}}\\}$   is the placeholder for entity type. ", "bbox": [77, 70, 520, 781], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "Entity\n\nTemplate\n\nno-entity\n\n{ent} do/does not belong to any known entities.\n\nperson-artist/author\n\n{ent} is an artist or author.\n\nperson-actor\n\n{ent} is an actor.\n\nart-writtenart\n\n{ent} is a kind of writtenart.\n\nperson-director\n\n{ent} is a director.\n\nperson-other\n\n{ent} is a person, but not affiliated with following professions: actor, artist, athlete,\nauthor, director, politician, scholar, soldier.\n\norganization-other\n\n{ent} pertains to an organization that does not fall under the categories of company,\neducational institution, government, media, political party, religion, sports league,\nsports team, band or musical group.\n\norganization-company\n\n{ent} is a company\n\norganization-sportsteam\n\n{ent} is a sports team\n\norganization-sportsleague\n\n{ent} is a sports league\n\nproduct-car\n\n{ent} is a kind of car\n\nevent-protest\n\n{ent} refers to a protest, uprising or revolution event\n\norganization-\ngovernment/governmentagency\n\n{ent} refers to a government or governmental agency\n\nother-biologything\n\n{ent} is a special term about biology / life science.\n\nlocation-GPE\n\n{ent} is a kind of geopolitical entity\n\nlocation-other\n\n{ent} is a geographic locaton that does not fall under the categories of geopolitical\nentity, body of water, island, mountain, park, road, railway and transit.\n\nperson-athlete\n\n{ent} is an athlete or coach.\n\nart-broadcastprogram\n\n{ent} is a broadcast program.\n\nproduct-other\n\n{ent} is a kind of product that does not fall under the categories of airplane, train,\nship, car, weapon, food, electronic game and software.\n\nuilding-other\n\n{ent} is a kind of building that does not fall under the categories of airport, hospital,\nhotel, library, restaurant, sports facility and theater\n\nproduct-weapon\n\n{ent} is a kind of weapon.\n\nbuilding-airport\n\n{ent} is an airport.\n\nuilding-sportsfacility\n\n{ent} is a sports facility building.\n\nperson-scholar\n\n{ent} is a scholar.\n\nart-music\n\n{ent} is a music.\n\nevent-other\n\n{ent} refers to some event except attack, election, natural disaster, protest, revolution\nand sports\n\nother-language\n\n{ent} is a kind of human language.\n\nother-chemicalthing\n\n{ent} is some special term about chemical science.\n\nart-film\n\n{ent} is a film.\n\nbuilding-hospital\n\n{ent} is a hospital.\n\nother-law\n\n{ent} is a legal document, a term or a convention in legal sense.\n\nproduct-airplane\n\n{ent} is kind of airplane product.\n\nlocation-\nroad/railway/highway/transit\n\n{ent} is a geographic position about roadways, railways, highways or public transit\nsystems.\n\nperson-soldier\n\n{ent} is a soldier\n\nlocation-mountain\n\n{ent} is geographic position about mountain.\n\norganization-education\n\n{ent} is an educational institute/organization.\n\norganization-media/newspaper\n\n{ent} is a media/newspaper organization.\n", "vlm_text": "The table outlines different entities and their corresponding templates for classification. Each row contains two columns:\n\n1. **Entity**: The type of entity, such as \"person-actor,\" \"organization-company,\" \"location-GPE,\" etc.\n2. **Template**: A description or template sentence structure for identifying or categorizing the entity, using placeholders like `{ent}` to specify where the entity's name would appear.\n\nThe entities cover a range of categories, including people (e.g., actor, director), organizations (e.g., company, sports team), locations (e.g., geopolitical entity, mountain), products (e.g., car, airplane), events (e.g., protest, other), and other specialized terms for biology, chemistry, language, and law."}
{"layout": 303, "type": "table", "page_idx": 26, "img_path": "layout_images/2303.08559v2_28.jpg", "bbox": [69, 69, 524, 560], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "product-software {ent} is a software product.\n\nlocation-island {ent} is geographic position about island.\nlocation-bodiesofwater {ent} is geographic position situated near a body of water.\nbuilding-library {ent} is a library.\n\nother-astronomything {ent} is a special term about astronomy.\n\nperson-politician {ent} is a politician or lawyer or judge.\n\nbuilding-hotel {ent} is a hotel building.\n\nproduct-game {ent} is a electronic game product.\n\nother-award {ent} is a kind of award.\n\nevent-sportsevent {ent} refers to some event related to sports.\norganization-showorganization {ent} is a band or musical organization.\nother-educationaldegree {ent} is a kind of educational degree.\n\nbuilding-theater {ent} is a theater.\n\nother-disease {ent} is a kind of disease.\n\nevent-election {ent} is an event about election.\norganization-politicalparty {ent} is a political party/organization.\n\nother-currency {ent} is a kind of currency.\n\nevent- {ent} is an event about attack, battle, war or military conflict.\n\nattack/battle/war/militaryconflict\n\nproduct-ship {ent} is a ship.\n\nbuilding-restaurant {ent} is a restaurant.\n\nother-livingthing {ent} is a living animal/creature/organism.\n\nart-other {ent} is a work of art, but not belong to the categories of music, film, written art,\n\nbroadcast or painting.\n\nevent-disaster {ent} is a natural disaster event.\norganization-religion {ent} is a religious organization.\nother-medical {ent} refers to some kind of medicine.entity\nlocation-park {ent} is a park.\n\nother-god {ent} is a god in some legend/religious story.\nproduct-food {ent} is a kind of food.\n\nproduct-train {ent} is a kind of train(vehicle).\n\nart-painting {ent} is an art painting.\n", "vlm_text": "The table consists of various categories related to entities, each paired with a corresponding description. Here is a summary of the entity types and their descriptions:\n\n- **product-software**: A software product.\n- **location-island**: A geographic position about an island.\n- **location-bodiesofwater**: A geographic position near a body of water.\n- **building-library**: A library.\n- **other-astronomything**: A special term about astronomy.\n- **person-politician**: A politician, lawyer, or judge.\n- **building-hotel**: A hotel building.\n- **product-game**: An electronic game product.\n- **other-award**: A type of award.\n- **event-sportsevent**: An event related to sports.\n- **organization-showorganization**: A band or musical organization.\n- **other-educationaldegree**: A type of educational degree.\n- **building-theater**: A theater.\n- **other-disease**: A kind of disease.\n- **event-election**: An election event.\n- **organization-politicalparty**: A political party or organization.\n- **other-currency**: A type of currency.\n- **event-attack/battle/war/militaryconflict**: An event about attack, battle, war, or military conflict.\n- **product-ship**: A ship.\n- **building-restaurant**: A restaurant.\n- **other-livingthing**: A living animal, creature, or organism.\n- **art-other**: A work of art, excluding categories like music, film, etc.\n- **event-disaster**: A natural disaster event.\n- **organization-religion**: A religious organization.\n- **other-medical**: A type of medicine entity.\n- **location-park**: A park.\n- **other-god**: A god in some legend or religious story.\n- **product-food**: A kind of food.\n- **product-train**: A kind of train (vehicle).\n- **art-painting**: An art painting."}
{"layout": 304, "type": "text", "text": "Table 20: Templates for TACREV dataset, where {subj} and   $\\{{\\sf o b j}\\}$   are the placeholder s for subject and object entities. Copied from ( Lu et al. ,  2022a ) ", "page_idx": 27, "bbox": [70, 70, 524.4058227539062, 93.98949432373047], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 305, "type": "table", "page_idx": 27, "img_path": "layout_images/2303.08559v2_29.jpg", "bbox": [85, 101, 512, 781], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "Relation Template\n\nno_relation {subj} has no known relations to {obj}\nper:stateorprovince_of_death {subj} died in the state or province {obj}\nper:title {subj} is a {obj}\n\norg:member_of {subj} is the member of {obj}\nper:other_family {subj} is the other family member of {obj}\norg:country_of_headquarters {subj} has a headquarter in the country {obj}\norg:parents {subj} has the parent company {obj}\nper:stateorprovince_of_birth {subj} was born in the state or province {obj}\nper:spouse {subj} is the spouse of {obj}\n\nper:origin {subj} has the nationality {obj}\nper:date_of_birth {subj} has birthday on {obj}\nper:schools_attended {subj} studied in {obj}\n\norg:members {subj} has the member {obj}\n\norg:founded {subj} was founded in {obj}\nper:stateorprovinces_of_residence {subj} lives in the state or province {obj}\nper:date_of_death {subj} died in the date {obj}\norg:shareholders {subj} has shares hold in {obj}\n\norg: website {subj} has the website {obj}\norg:subsidiaries {subj} owns {obj}\n\nper:charges {subj} is convicted of {obj}\n\norg:dissolved {subj} dissolved in {obj}\norg:stateorprovince_of_headquarters {subj} has a headquarter in the state or province {obj}\nper:country_of_birth {subj} was born in the country {obj}\nper:siblings {subj} is the siblings of {obj}\norg:top_members/employees {subj} has the high level member {obj}\nper:cause_of_death {subj} died because of {obj}\nper:alternate_names {subj} has the alternate name {obj}\norg:number_of_employees/members {subj} has the number of employees {obj}\nper:cities_of_residence {subj} lives in the city {obj}\norg:city_of_headquarters {subj} has a headquarter in the city {obj}\nper:children {subj} is the parent of {obj}\nper:employee_of {subj} is the employee of {obj}\norg:political/religious_affiliation {subj} has political affiliation with {obj}\nper:parents {subj} has the parent {obj}\nper:city_of_birth {subj} was born in the city {obj}\n\nper:age {subj} has the age {obj}\nper:countries_of_residence {subj} lives in the country {obj}\norg:alternate_names {subj} is also known as {obj}\n\nper:religion {subj} has the religion {obj}\nper:city_of_death {subj} died in the city {obj}\nper:country_of_death {subj} died in the country {obj}\norg:founded_by {subj} was founded by {obj}\n\n", "vlm_text": "The table lists relationships and their corresponding templates. Each row contains a relation type and a sentence template using placeholders `{subj}` and `{obj}`. Here are some examples:\n\n- **Relation:** `per:title`  \n  **Template:** `{subj} is a {obj}`\n\n- **Relation:** `org:country_of_headquarters`  \n  **Template:** `{subj} has a headquarter in the country {obj}`\n\n- **Relation:** `per:date_of_birth`  \n  **Template:** `{subj} has birthday on {obj}`\n\nThis pattern continues for various personal (per) and organizational (org) relationships, defining how to express them in sentence form."}
{"layout": 306, "type": "table", "page_idx": 28, "img_path": "layout_images/2303.08559v2_30.jpg", "table_caption": "Table 21: Templates for ACE05 dataset, where {evt} is the placeholder for event type. ", "bbox": [78, 70, 519, 813], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "Event\n\nTemplate\n\nno-event\n\nThe word {evt} does not trigger any known event.\n\nMovement. Transport\n\nThe word {evt} triggers a TRANSPORT event: an ARTIFACT (WEAPON or\nVEHICLE) or a PERSON is moved from one PLACE (GEOPOLITICAL ENTITY,\nFACILITY, LOCATION) to another.\n\nPersonnel.Elect\n\nThe word {evt} triggers an ELECT event which implies an election.\n\nPersonnel.Start-Position\n\nThe word {evt} triggers a START-POSITION event: a PERSON elected or appointed\nbegins working for (or changes offices within) an ORGANIZATION or GOVERN-\nMENT.\n\nPersonnel.Nominate\n\nThe word {evt} triggers a NOMINATE event: a PERSON is proposed for a position\nthrough official channels.\n\nConflict. Attack\n\nThe word {evt} triggers an ATTACK event: a violent physical act causing harm or\ndamage.\n\nPersonnel.End-Position\n\nThe word {evt} triggers an END-POSITION event: a PERSON stops working for\n(or changes offices within) an ORGANIZATION or GOVERNMENT.\n\nContact.Meet\n\nThe word {evt} triggers a MEET event: two or more entities come together at a\nsingle location and interact with one another face-to-face.\n\nLife.Marry\n\nThe word {evt} triggers a MARRY event: two people are married under the legal\ndefinition.\n\nContact.Phone-Write\n\nThe word {evt} triggers a PHONE-WRITE event: two or more people directly\nengage in discussion which does not take place ’ face-to-face’.\n\nTransaction. Transfer-Money\n\nThe word {evt} triggers a TRANSFER-MONEY event: giving, receiving, borrowing,\nor lending money when it is NOT in the context of purchasing something.\n\nJustice.Sue\n\nThe word {evt} triggers a SUE event: a court proceeding has been initiated for the\npurposes of determining the liability of a PERSON, ORGANIZATION or GEOPO-\nLITICAL ENTITY accused of committing a crime or neglecting a commitment\n\nConflict. Demonstrate\n\nThe word {evt} triggers a DEMONSTRATE event: a large number of people come\ntogether in a public area to protest or demand some sort of official action. For eample:\nprotests, sit-ins, strikes and riots.\n\nBusiness.End-Org\n\nThe word {evt} triggers an END-ORG event: an ORGANIZATION ceases to exist\n(in other words, goes out of business).\n\nLife.Injure\n\nThe word {evt} triggers an INJURE event: a PERSON gets/got injured whether it\noccurs accidentally, intentionally or even self-inflicted.\n\nLife.Die\n\nThe word {evt} triggers a DIE event: a PERSON dies/died whether it occurs acci-\ndentally, intentionally or even self-inflicted.\n\nJustice. Arrest-Jail\n\nThe word {evt} triggers a ARREST-JAIL event: a PERSON is sent to prison.\n\nTransaction.Transfer-\nOwnership\n\nThe word {evt} triggers a TRANSFER-OWNERSHIP event: The buying, selling,\nloaning, borrowing, giving, or receiving of artifacts or organizations by an individual\nor organization.\n\nJustice.Execute\n\nThe word {evt} triggers an EXECUTE event: a PERSON is/was executed\n\nJustice. Trial-Hearing\n\nThe word {evt} triggers a TRIAL-HEARING event: a court proceeding has been\ninitiated for the purposes of determining the guilt or innocence of a PERSON,\nORGANIZATION or GEOPOLITICAL ENTITY accused of committing a crime.\n\nJustice.Sentence\n\nThe word {evt} triggers a SENTENCE event: the punishment for the DEFENDANT\nis issued\n\nLife.Be-Born\n\nThe word {evt} triggers a BE-BORN event: a PERSON is given birth to.\n\nJustice.Charge-Indict\n\nThe word {evt} triggers a CHARGE-INDICT event: a PERSON, ORGANIZATION\nor GEOPOLITICAL ENTITY is accused of a crime\n\nBusiness.Start-Org\n\nThe word {evt} triggers a START-ORG event: a new ORGANIZATION is created.\n\nJustice.Convict\n\nThe word {evt} trigges a CONVICT event: a PERSON, ORGANIZATION or\nGEOPOLITICAL ENTITY is convicted whenever it has been found guilty of a\nRIME.\n\nBusiness.Declare-Bankruptcy\n\nCc\nThe word {evt} triggers a DECLARE-BANKRUPTCY event: an Entity officially\nrequests legal protection from debt collection due to an extremely negative balance\nsheet.\n\nJustice.Release-Parole\n\nThe word {evt} triggers a RELEASE-PAROLE event.\n", "vlm_text": "The table contains two columns: \"Event\" and \"Template.\" Each row describes an event type and the template for how that event is triggered using the word \"{evt}\". Here are some examples:\n\n1. **Movement.Transport**: The word \"{evt}\" triggers a TRANSPORT event, such as moving an object or person from one place to another.\n\n2. **Personnel.Elect**: Triggers an ELECT event implying an election.\n\n3. **Conflict.Attack**: Triggers an ATTACK event involving violence or harm.\n\n4. **Life.Marry**: Triggers a MARRY event signifying legal marriage.\n\n5. **Justice.Arrest-Jail**: Triggers an ARREST-JAIL event, sending a person to prison.\n\n6. **Justice.Execute**: Triggers an EXECUTE event, indicating a person is or was executed.\n\n7. **Business.Declare-Bankruptcy**: Triggers a DECLARE-BANKRUPTCY event, when an entity seeks legal protection due to negative financial status.\n\nEach event type describes a specific scenario or action that can occur based on the triggering word."}
{"layout": 307, "type": "table", "page_idx": 29, "img_path": "layout_images/2303.08559v2_31.jpg", "bbox": [70, 72, 508, 244], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "Justice.Fine\n\nThe word {evt} triggers a FINE event: a GEOPOLITICAL ENTITY, PERSON or\nORGANIZATION get financial punishment typically as a result of court proceedings.\n\nJustice.Pardon\n\nThe word {evt} triggers a PARDON event: a head-of-state or their appointed repre-\nsentative lifts a sentence imposed by the judiciary.\n\nJustice.Appeal\n\nThe word {evt} triggers a APPEAL event: the decision of a court is taken to a higher\ncourt for review\n\nBusiness.Merge-Org\n\nThe word {evt} triggers a MERGE-ORG event: two or more ORGANIZATION\nEntities come together to form a new ORGANIZATION Entity.\n\nJustice.Extradite\n\nThe word {evt} triggers a EXTRADITE event.\n\nLife.Divorce\n\nThe word {evt} triggers a DIVORCE event: two people are officially divorced under\nthe legal definition of divorce.\n\nJustice.Acquit\n\nThe word {evt} triggers a ACQUIT event: a trial ends but fails to produce a convic-\ntion.\n\n", "vlm_text": "The table lists different event triggers and their descriptions. Each trigger corresponds to a specific type of event that occurs under various circumstances:\n\n1. **Justice.Fine**: A financial punishment involving a geopolitical entity, person, or organization.\n2. **Justice.Pardon**: A head-of-state or representative lifts a judicial sentence.\n3. **Justice.Appeal**: A court decision taken to a higher court for review.\n4. **Business.Merge-Org**: Two or more organizations form a new organization.\n5. **Justice.Extradite**: Triggering an extradition event.\n6. **Life.Divorce**: Two people officially divorced under the law.\n7. **Justice.Acquit**: A trial ends without a conviction."}
