{"page": 0, "image_path": "doc_images/N18-2072_0.jpg", "ocr_text": "Contextual Augmentation:\nData Augmentation by Words with Paradigmatic Relations\n\nSosuke Kobayashi\nPreferred Networks, Inc., Japan\nsosk@preferred. jp\n\nAbstract\n\nWe propose a novel data augmentation for\nlabeled sentences called contextual augmen-\ntation. We assume an invariance that sen-\ntences are natural even if the words in the\nsentences are replaced with other words with\nparadigmatic relations. We stochastically re-\nplace words with other words that are pre-\ndicted by a bi-directional language model at\nthe word positions. Words predicted accord-\ning to a context are numerous but appropri-\nate for the augmentation of the original words.\nFurthermore, we retrofit a language model\nwith a label-conditional architecture, which al-\nlows the model to augment sentences without\nbreaking the label-compatibility. Through the\nexperiments for six various different text clas-\nsification tasks, we demonstrate that the pro-\nposed method improves classifiers based on\nthe convolutional or recurrent neural networks.\n\n1 Introduction\n\nNeural network-based models for NLP have been\ngrowing with state-of-the-art results in various\ntasks, e.g., dependency parsing (Dyer et al., 2015),\ntext classification (Socher et al., 2013; Kim, 2014),\nmachine translation (Sutskever et al., 2014). How-\never, machine learning models often overfit the\ntraining data by losing their generalization. Gener-\nalization performance highly depends on the size\nand quality of the training data and regulariza-\ntions. Preparing a large annotated dataset is very\ntime-consuming. Instead, automatic data augmen-\ntation is popular, particularly in the areas of vi-\nsion (Simard et al., 1998; Krizhevsky et al., 2012;\nSzegedy et al., 2015) and speech (Jaitly and Hin-\nton, 2015; Ko et al., 2015). Data augmentation is\nbasically performed based on human knowledge\non invariances, rules, or heuristics, e.g., “even if a\npicture is flipped, the class of an object should be\nunchanged”.\n\n452\n\nthe performances are fantastic\nthe films are fantastic\nthe movies are fantastic\nthe stories are fantastic\n\nperformances\nfilms\nmovies\nstories\n\nthe actors are\nthe actors are fantastic po\n\nFigure 1: Contextual augmentation with a bi-\ndirectional RNN language model, when a sentence\n“the actors are fantastic” is augmented by replacing\nonly actors with words predicted based on the context.\n\nHowever, usage of data augmentation for NLP\nhas been limited. In natural languages, it is very\ndifficult to obtain universal rules for transforma-\ntions which assure the quality of the produced data\nand are easy to apply automatically in various do-\nmains. A common approach for such a transfor-\nmation is to replace words with their synonyms se-\nlected from a handcrafted ontology such as Word-\nNet (Miller, 1995; Zhang et al., 2015) or word sim-\nilarity calculation (Wang and Yang, 2015). Be-\ncause words having exactly or nearly the same\nmeanings are very few, synonym-based augmen-\ntation can be applied to only a small percentage\nof the vocabulary. Other augmentation methods\nare known but are often developed for specific do-\nmains with handcrafted rules or pipelines, with the\nloss of generality.\n\nIn this paper, we propose a novel data aug-\n\nProceedings of NAACL-HLT 2018, pages 452-457\nNew Orleans, Louisiana, June 1 - 6, 2018. ©2018 Association for Computational Linguistics\n", "vlm_text": "Contextual Augmentation: Data Augmentation by Words with Paradigmatic Relations \nSosuke Kobayashi Preferred Networks, Inc., Japan sosk@preferred.jp \nAbstract \nWe propose a novel data augmentation for labeled sentences called  contextual augmen- tation . We assume an invariance that sen- tences are natural even if the words in the sentences are replaced with other words with paradigmatic relations. We stochastically re- place words with other words that are pre- dicted by a bi-directional language model at the word positions. Words predicted accord- ing to a context are numerous but appropri- ate for the augmentation of the original words. Furthermore, we retroﬁt a language model with a label-conditional architecture, which al- lows the model to augment sentences without breaking the label-compatibility. Through the experiments for six various different text clas- siﬁcation tasks, we demonstrate that the pro- posed method improves classiﬁers based on the convolutional or recurrent neural networks. \n1 Introduction \nNeural network-based models for NLP have been growing with state-of-the-art results in various tasks, e.g., dependency parsing ( Dyer et al. ,  2015 ), text classiﬁcation ( Socher et al. ,  2013 ;  Kim ,  2014 ), machine translation ( Sutskever et al. ,  2014 ). How- ever, machine learning models often overﬁt the training data by losing their generalization. Gener- alization performance highly depends on the size and quality of the training data and regulariza- tions. Preparing a large annotated dataset is very time-consuming. Instead, automatic data augmen- tation is popular, particularly in the areas of vi- sion ( Simard et al. ,  1998 ;  Krizhevsky et al. ,  2012 ; Szegedy et al. ,  2015 ) and speech ( Jaitly and Hin- ton ,  2015 ;  Ko et al. ,  2015 ). Data augmentation is basically performed based on human knowledge on invariances, rules, or heuristics, e.g., “even if a picture is ﬂipped, the class of an object should be unchanged”. \nThe image illustrates the process of contextual augmentation using a bi-directional RNN language model. It begins with the sentence “the actors are fantastic.” The word \"actors\" is highlighted and is replaced with words like \"performances,\" \"films,\" \"movies,\" and \"stories,\" which are contextually predicted by the model. The sentences generated retain a positive sentiment label. The image visually shows how the RNN processes the context to predict suitable substitutions while maintaining the overall sentiment.\nHowever, usage of data augmentation for NLP has been limited. In natural languages, it is very difﬁcult to obtain universal rules for transforma- tions which assure the quality of the produced data and are easy to apply automatically in various do- mains. A common approach for such a transfor- mation is to replace words with their synonyms se- lected from a handcrafted ontology such as Word- Net ( Miller ,  1995 ;  Zhang et al. ,  2015 ) or word sim- ilarity calculation ( Wang and Yang ,  2015 ). Be- cause words having exactly or nearly the same meanings are very few, synonym-based augmen- tation can be applied to only a small percentage of the vocabulary. Other augmentation methods are known but are often developed for speciﬁc do- mains with handcrafted rules or pipelines, with the loss of generality. \nIn this paper, we propose a novel data aug- mentation method called  contextual augmenta- tion . Our method offers a wider range of sub- stitute words by using words predicted by a bi- directional language model (LM) according to the context, as shown in Figure  1 . This contextual pre- diction suggests various words that have paradig- matic relations ( Saussure and Riedlinger ,  1916 ) with the original words. Such words can also be good substitutes for augmentation. Furthermore, to prevent word replacement that is incompatible with the annotated labels of the original sentences, we retroﬁt the LM with a label-conditional archi- tecture. Through the experiment, we demonstrate that the proposed conditional LM produces good words for augmentation, and contextual augmen- tation improves classiﬁers using recurrent or con- volutional neural networks (RNN or CNN) in var- ious classiﬁcation tasks. "}
{"page": 1, "image_path": "doc_images/N18-2072_1.jpg", "ocr_text": "mentation method called contextual augmenta-\ntion. Our method offers a wider range of sub-\nstitute words by using words predicted by a bi-\ndirectional language model (LM) according to the\ncontext, as shown in Figure 1. This contextual pre-\ndiction suggests various words that have paradig-\nmatic relations (Saussure and Riedlinger, 1916)\nwith the original words. Such words can also be\ngood substitutes for augmentation. Furthermore,\nto prevent word replacement that is incompatible\nwith the annotated labels of the original sentences,\nwe retrofit the LM with a label-conditional archi-\ntecture. Through the experiment, we demonstrate\nthat the proposed conditional LM produces good\nwords for augmentation, and contextual augmen-\ntation improves classifiers using recurrent or con-\nvolutional neural networks (RNN or CNN) in var-\nious classification tasks.\n\n2 Proposed Method\n\nFor performing data augmentation by replac-\ning words in a text with other words, prior\nworks (Zhang et al., 2015; Wang and Yang, 2015)\nused synonyms as substitute words for the origi-\nnal words. However, synonyms are very limited\nand the synonym-based augmentation cannot pro-\nduce numerous different patterns from the origi-\nnal texts. We propose contextual augmentation, a\nnovel method to augment words with more varied\nwords. Instead of the synonyms, we use words that\nare predicted by a LM given the context surround-\ning the original words to be augmented, as shown\nin Figure 1.\n\n2.1 Motivation\n\nFirst, we explain the motivation of our pro-\nposed method by referring to an example with a\nsentence from the Stanford Sentiment Treebank\n(SST) (Socher et al., 2013), which is a dataset of\nsentiment-labeled movie reviews. The sentence,\n“the actors are fantastic.” , is annotated with a pos-\nitive label. When augmentation is performed for\nthe word (position) “actors”, how widely can we\naugment it? According to the prior works, we can\nuse words from a synset for the word actor ob-\ntained from WordNet (histrion, player, thespian,\nand role_player). The synset contains words that\nhave meanings similar to the word actor on aver-\n\nage.! However, for data augmentation, the word\n\n' Actually, the word actor has another synset containing\nother words such as doer and worker. Thus, this synonym-\n\n453\n\nactors can be further replaced with non-synonym\nwords such as characters, movies, stories, and\nsongs or various other nouns, while retaining the\npositive sentiment and naturalness. Considering\nthe generalization, training with maximum pat-\nterns will boost the model performance more.\n\nWe propose using numerous words that have the\nparadigmatic relations with the original words. A\nLM has the desirable property to assign high prob-\nabilities to such words, even if the words them-\nselves are not similar to the original word to be\nreplaced.\n\n2.2 Word Prediction based on Context\n\nFor our proposed method, we requires a LM for\ncalculating the word probability at a position 7\nbased on its context. The context is a sequence of\nwords surrounding an original word w, in a sen-\ntence S, i.e., cloze sentence S\\{w;}. The calcu-\nlated probability is p(-|S\\{w;}). Specifically, we\nuse a bi-directional LSTM-RNN (Hochreiter and\nSchmidhuber, 1997) LM. For prediction at posi-\ntion 7, the model encodes the surrounding words\nindividually rightward and leftward (see Figure 1).\nAs well as typical uni-directional RNN LMs, the\noutputs from adjacent positions are used for cal-\nculating the probability at target position i. The\noutputs from both the directions are concatenated\nand fed into the following feed-forward neural net-\nwork, which produces words with a probability\ndistribution over the vocabulary.\n\nIn contextual augmentation, new substitutes for\nword w; can be smoothly sampled from a given\nprobability distribution, p(-|S\\{w;}), while prior\nworks selected top-K words conclusively. In this\nstudy, we sample words for augmentation at each\nupdate during the training of a model. To control\nthe strength of augmentation, we introduce tem-\nperature parameter 7 and use an annealed distri-\nbution p,(-|S\\{wi}) « p(-|S\\{wi})/7. If the\ntemperature becomes infinity (7 — 00), the words\nare sampled from a uniform distribution. 7 If it\nbecomes zero (rt — 0), the augmentation words\nare always words predicted with the highest prob-\nability. The sampled words can be obtained at one\ntime at each word position in the sentences. We re-\nplace each word simultaneously with a probability\n\nbased approach further requires word sense disambiguation\nor some rules for selecting ideal synsets.\n\n? Bengio et al. (2015) reported that stochastic replace-\nments with uniformly sampled words improved a neural\nencoder-decoder model for image captioning.\n", "vlm_text": "\n2 Proposed Method \nFor performing data augmentation by replac- ing words in a text with other words, prior works ( Zhang et al. ,  2015 ;  Wang and Yang ,  2015 ) used synonyms as substitute words for the origi- nal words. However, synonyms are very limited and the synonym-based augmentation cannot pro- duce numerous different patterns from the origi- nal texts. We propose  contextual augmentation , a novel method to augment words with more varied words. Instead of the synonyms, we use words that are predicted by a LM given the context surround- ing the original words to be augmented, as shown in Figure  1 . \n2.1 Motivation \nFirst, we explain the motivation of our pro- posed method by referring to an example with a sentence from the Stanford Sentiment Treebank (SST) ( Socher et al. ,  2013 ), which is a dataset of sentiment-labeled movie reviews. The sentence, “the actors are fantastic.” , is annotated with a pos- itive label. When augmentation is performed for the word (position)  “actors” , how widely can we augment it? According to the prior works, we can use words from a synset for the word  actor  ob- tained from WordNet ( histrion, player, thespian, and  role player ). The synset contains words that have meanings similar to the word  actor  on aver- age.   However, for data augmentation, the word actors  can be further replaced with non-synonym words such as  characters, movies, stories,  and songs  or various other nouns, while retaining the positive sentiment and naturalness. Considering the generalization, training with maximum pat- terns will boost the model performance more. \n\nWe propose using numerous words that have the paradigmatic relations with the original words. A LM has the desirable property to assign high prob- abilities to such words, even if the words them- selves are not similar to the original word to be replaced. \n2.2 Word Prediction based on Context \nFor our proposed method, we requires a LM for calculating the word probability at a position    $i$  based on its context. The context is a sequence of words surrounding an original word    $w_{i}$   in a sen- tence    $S$  , i.e., cloze sentence    $S\\backslash\\{w_{i}\\}$  . The calcu- lated probability is    $p(\\cdot|S\\backslash\\{w_{i}\\})$  . Speciﬁcally, we use a bi-directional LSTM-RNN ( Hochreiter and Schmidhuber ,  1997 ) LM. For prediction at posi- tion    $i$  , the model encodes the surrounding words individually rightward and leftward (see Figure  1 ). As well as typical uni-directional RNN LMs, the outputs from adjacent positions are used for cal- culating the probability at target position    $i$  . The outputs from both the directions are concatenated and fed into the following feed-forward neural net- work, which produces words with a probability distribution over the vocabulary. \nIn contextual augmentation, new substitutes for word    $w_{i}$   can be smoothly sampled from a given probability distribution,    $p(\\cdot|S\\backslash\\{w_{i}\\})$  , while prior works selected top-K words conclusively. In this study, we sample words for augmentation at each update during the training of a model. To control the strength of augmentation, we introduce tem- perature parameter    $\\tau$   and use an annealed distri- bution    $p_{\\tau}(\\cdot|S\\backslash\\{w_{i}\\})~\\propto~p(\\cdot|S\\backslash\\{w_{i}\\})^{1/\\tau}$  . If the temperature becomes inﬁnity   $(\\tau\\to\\infty)$  ), the words are sampled from a uniform distribution.   2   If it becomes zero   $(\\tau\\,\\rightarrow\\,0)$  , the augmentation words are always words predicted with the highest prob- ability. The sampled words can be obtained at one time at each word position in the sentences. We re- place each word simultaneously with a probability "}
{"page": 2, "image_path": "doc_images/N18-2072_2.jpg", "ocr_text": "as well as Wang and Yang (2015) for efficiency.\n\n2.3. Conditional Constraint\n\nFinally, we introduce a novel approach to address\nthe issue that context-aware augmentation is not\nalways compatible with annotated labels. For un-\nderstanding the issue, again, consider the exam-\nple, “the actors are fantastic.”, which is annotated\nwith a positive label. If contextual augmentation,\nas described so far, is simply performed for the\nword (position of) fantastic, a LM often assigns\nhigh probabilities to words such as bad or terrible\nas well as good or entertaining, although they are\nmutually contradictory to the annotated labels of\npositive or negative. Thus, such a simple augmen-\ntation can possibly generate sentences that are im-\nplausible with respect to their original labels and\nharmful for model training.\n\nTo address this issue, we introduce a condi-\ntional constraint that controls the replacement of\nwords to prevent the generated words from revers-\ning the information related to the labels of the sen-\ntences. We alter a LM to a label-conditional LM,\nie., for position 7 in sentence S with label y, we\naim to calculate p,(-|y, S\\{w;}) instead of the de-\nfault p;(-|S\\{wi}) within the model. Specifically,\nwe concatenate each embedded label y with a hid-\nden layer of the feed-forward network in the bi-\ndirectional LM, so that the output is calculated\nfrom a mixture of information from both the label\nand context.\n\n3 Experiment\n\n3.1 Settings\n\nWe tested combinations of three augmentation\nmethods for two types of neural models through\nsix text classification tasks. The corresponding\ncode is implemented by Chainer (Tokui et al.,\n2015) and available >.\n\nThe benchmark datasets used are as follows:\n(1, 2) SST is a dataset for sentiment classifica-\ntion on movie reviews, which were annotated with\nfive or two labels (SST5, SST2) (Socher et al.,\n2013). (3) Subjectivity dataset (Subj) was anno-\ntated with whether a sentence was subjective or\nobjective (Pang and Lee, 2004). (4) MPQA is an\nopinion polarity detection dataset of short phrases\nrather than sentences (Wiebe et al., 2005). (5) RT\nis another movie review sentiment dataset (Pang\n\nSnttps://github.com/pfnet-research/\ncontextual_augmentation\n\n454\n\nand Lee, 2005). (6) TREC is a dataset for clas-\nsification of the six question types (e.g., person,\nlocation) (Li and Roth, 2002). For a dataset with-\nout development data, we use 10% of its training\nset for the validation set as well as Kim (2014).\n\nWe tested classifiers using the LSTM-RNN or\nCNN, and both have exhibited good performances.\nWe used typical architectures of classifiers based\non the LSTM or CNN with dropout using hyperpa-\nrameters found in preliminary experiments. 4 The\nreported accuracies of the models were averaged\nover eight models trained from different seeds.\n\nThe tested augmentation methods are: (1)\nsynonym-based augmentation, and (2, 3) con-\nextual augmentation with or without a label-\nconditional architecture. The hyperparameters of\nhe augmentation (temperature 7 and probability\nof word replacement) were also selected by a grid-\nsearch using validation set, while retaining the\nhyperparameters of the models. For contextual\naugmentation, we first pretrained a bi-directional\nLSTM LM without the label-conditional architec-\nure, on WikiText-103 corpus (Merity et al., 2017)\nrom a subset of English Wikipedia articles. After\nhe pretraining, the models are further trained on\neach labeled dataset with newly introduced label-\nconditional architectures.\n\n3.2 Results\n\nTable | lists the accuracies of the models with or\nwithout augmentation. The results show that our\ncontextual augmentation improves the model per-\nformances for various datasets from different do-\nmains more significantly than the prior synonym-\nbased augmentation does. Furthermore, our label-\nconditional architecture boosted the performances\non average and achieved the best accuracies. Our\nmethods are effective even for datasets with more\nthan two types of labels, SST5 and TREC.\n\n+ An RNN-based classifier has a single layer LSTM and\nword embeddings, whose output is fed into an output affine\nlayer with the softmax function. A CNN-based classifier\nhas convolutional filters of size {3, 4, 5} and word embed-\ndings (Kim, 2014). The concatenated output of all the fil-\nters are applied with a max-pooling over time and fed into\na two-layer feed-forward network with ReLU, followed by\nthe softmax function. For both the architectures, training was\nperformed by Adam and finished by early stopping with val-\nidation at each epoch.\n\nThe hyperparameters of the models and training were se-\nlected by a grid-search using baseline models without data\naugmentation in each task’s validation set individually. We\nused the best settings from the combinations by changing the\nlearning rate, unit or filter size, embedding dimension, and\ndropout ratio.\n", "vlm_text": "2.3 Conditional Constraint \nFinally, we introduce a novel approach to address the issue that context-aware augmentation is not always compatible with annotated labels. For un- derstanding the issue, again, consider the exam- ple,  “the actors are fantastic.” , which is annotated with a positive label. If contextual augmentation, as described so far, is simply performed for the word (position of)  fantastic , a LM often assigns high probabilities to words such as  bad  or  terrible as well as  good  or  entertaining , although they are mutually contradictory to the annotated labels of positive or negative. Thus, such a simple augmen- tation can possibly generate sentences that are im- plausible with respect to their original labels and harmful for model training. \nTo address this issue, we introduce a condi- tional constraint that controls the replacement of words to prevent the generated words from revers- ing the information related to the labels of the sen- tences. We alter a LM to a label-conditional LM, i.e., for position    $i$   in sentence    $S$   with label    $y$  , we aim to calculate  $p_{\\tau}(\\cdot|y,S\\backslash\\{w_{i}\\})$   instead of the de- fault  $p_{\\tau}(\\cdot|S\\rangle\\{w_{i}\\})$   within the model. Speciﬁcally, we concatenate each embedded label    $y$   with a hid- den layer of the feed-forward network in the bi- directional LM, so that the output is calculated from a mixture of information from both the label and context. \n3 Experiment \n3.1 Settings \nWe tested combinations of three augmentation methods for two types of neural models through six text classiﬁcation tasks. The corresponding code is implemented by Chainer ( Tokui et al. , 2015 ) and available   3 . \nThe benchmark datasets used are as follows: (1, 2) SST is a dataset for sentiment classiﬁca- tion on movie reviews, which were annotated with ﬁve or two labels (SST5, SST2) ( Socher et al. , 2013 ). (3) Subjectivity dataset (Subj) was anno- tated with whether a sentence was subjective or objective ( Pang and Lee ,  2004 ). (4) MPQA is an opinion polarity detection dataset of short phrases rather than sentences ( Wiebe et al. ,  2005 ). (5) RT is another movie review sentiment dataset ( Pang and Lee ,  2005 ). (6) TREC is a dataset for clas- siﬁcation of the six question types (e.g., person, location) ( Li and Roth ,  2002 ). For a dataset with- out development data, we use   $10\\%$   of its training set for the validation set as well as Kim ( 2014 ). \n\nWe tested classiﬁers using the LSTM-RNN or CNN, and both have exhibited good performances. We used typical architectures of classiﬁers based on the LSTM or CNN with dropout using hyperpa- rameters found in preliminary experiments.   4   The reported accuracies of the models were averaged over eight models trained from different seeds. \nThe tested augmentation methods are: (1) synonym-based augmentation, and (2, 3) con- textual augmentation with or without a label- conditional architecture. The hyperparameters of the augmentation (temperature    $\\tau$   and probability of word replacement) were also selected by a grid- search using validation set, while retaining the hyperparameters of the models. For contextual augmentation, we ﬁrst pretrained a bi-directional LSTM LM without the label-conditional architec- ture, on WikiText-103 corpus ( Merity et al. ,  2017 ) from a subset of English Wikipedia articles. After the pretraining, the models are further trained on each labeled dataset with newly introduced label- conditional architectures. \n3.2 Results \nTable  1  lists the accuracies of the models with or without augmentation. The results show that our contextual augmentation improves the model per- formances for various datasets from different do- mains more signiﬁcantly than the prior synonym- based augmentation does. Furthermore, our label- conditional architecture boosted the performances on average and achieved the best accuracies. Our methods are effective even for datasets with more than two types of labels, SST5 and TREC. "}
{"page": 3, "image_path": "doc_images/N18-2072_3.jpg", "ocr_text": "Models STTS5 STT2 Subj MPQA RT TREC| Avg.\nCNN 41.3, 79.5 92.4 86.1 75.9 90.0 |77.53\nw/ synonym] 40.7 80.0 92.4 86.3 76.0 89.6 |77.50\nw/ context | 41.9 80.9 92.7 86.7 75.9 90.0 |78.02\nt+label | 42.1 80.8 93.0 86.7 76.1 90.5 |78.20\nRNN 40.2 80.3 92.4 86.0 76.7 89.0 |77.43\nw/ synonym] 40.5 80.2 92.8 864 76.6 87.9 |77.40\nw/ context | 40.9 79.3 92.8 864 77.0 89.3 |77.62\n+label | 41.1 80.1 92.8 864 774 89.2 |77.83\nTable 1: Accuracies of the models for various bench-\nmarks. The accuracies are averaged over eight models\ntrained from different seeds.\n\nFor investigating our label-conditional bi-\ndirectional LM, we show in Figure 2 the top-10\nword predictions by the model for a sentence from\nthe SST dataset. Each word in the sentence is fre-\nquently replaced with various words that are not\nalways synonyms. We present two types of pre-\ndictions depending on the label fed into the con-\nditional LM. With a positive label, the word “‘fan-\ntastic” is frequently replaced with funny, honest,\ngood, and entertaining, which are also positive ex-\npressions. In contrast, with a negative label, the\nword “fantastic” is frequently replaced with tired,\nforgettable, bad, and dull, which reflect a negative\nsentiment. At another position, the word “the” can\nbe replaced with “no” (with the seventh highest\nprobability), so that the whole sentence becomes\n“no actors are fantastic.”, which seems negative as\na whole. Aside from such inversions caused by\nlabels, the parts unrelated to the labels (e.g., “ac-\ntors”) are not very different in the positive or neg-\native predictions. These results also demonstrated\nthat conditional architectures are effective.\n\n4 Related Work\n\nSome works tried text data augmentation by us-\ning synonym lists (Zhang et al., 2015; Wang and\nYang, 2015), grammar induction (Jia and Liang,\n2016), task-specific heuristic rules (Fiirstenau\nand Lapata, 2009; Kafle et al., 2017; Silfver-\nberg et al., 2017), or neural decoders of au-\ntoencoders (Bergmanis et al., 2017; Xu et al.,\n2017; Hu et al., 2017) or encoder-decoder mod-\nels (Kim and Rush, 2016; Sennrich et al., 2016;\nXia et al., 2017). The works most similar to our\nresearch are Kolomiyets et al. (2011) and Fadaee\net al. (2017). In a task of time expression recog-\nnition, Kolomiyets et al. replaced only the head-\nwords under a task-specific assumption that tem-\nporal trigger words usually occur as headwords.\nThey selected substitute words with top-K scores\n\n455\n\nhis stories get hilarious\nother story have young\nall actors seem compelling\nits two feel enjoyable\nmost performances find engaging\nthose films be fun\nsome movies is entertaining |),\nboth movie were good | Z\nthese film ‘re honest Cad\nthe characters are funny Zz\n‘positive!\nthe actors are fantastic\nnegative:\nthe characters ‘re tired\nsome movie are n't\nthese film were forgettable\nsuch plot seem bad\nits story feel good\nall films is dull\nno themes be unfunny\nhis movies find flat\nboth stories get pretentious\nother songs have bland\n\nFigure 2: Words predicted with the ten highest prob-\nabilities by the conditional bi-directional LM applied\nto the sentence “the actors are fantastic”. The squares\nabove the sentence list the words predicted with a pos-\nitive label. The squares below list the words predicted\nwith a negative label.\n\ngiven by the Latent Words LM (Deschacht and\nMoens, 2009), which is a LM based on fixed-\nlength contexts. Fadaee et al. (2017), focusing\non the rare word problem in machine transla-\ntion, replaced words in a source sentence with\nonly rare words, which both of rightward and left-\nward LSTM LMs independently predict with top-\nK confidences. A word in the translated sentence\nis also replaced using a word alignment method\nand a rightward LM. These two works share the\nidea of the usage of language models with our\nmethod. We used a bi-directional LSTM LM\nwhich captures variable-length contexts with con-\nsidering both the directions jointly. More impor-\ntantly, we proposed a label-conditional architec-\nture and demonstrated its effect both qualitatively\nand quantitatively. Our method is independent\nof any task-specific knowledge, and effective for\nclassification tasks in various domains.\n\nWe use a label-conditional fill-in-the-blank con-\ntext for data augmentation. Neural models us-\ning the fill-in-the-blank context have been invested\nin other applications. Kobayashi et al. (2016,\n2017) proposed to extract and organize informa-\ntion about each entity in a discourse using the con-\ntext. Fedus et al. (2018) proposed GAN (Goodfel-\nlow et al., 2014) for text generation and demon-\nstrated that the mode collapse and training insta-\n", "vlm_text": "The table presents the performance metrics of two types of models, CNN and RNN, along with various modifications, across different tasks or datasets. The columns in the table represent specific evaluation tasks or datasets: S1T5, S1T2, Subj. M, Qui. RP, True, and Avg., which likely denote specific metrics or datasets used in the experiments, though their exact meanings are not provided in the table. The rows show performance scores for each model setup:\n\n1. **CNN**:\n   - Baseline: Shows different performance scores across the tasks, resulting in an average score of 77.53.\n   - `w/ synonym`: Maintains relatively consistent scores with slight variations, averaging at 77.50.\n   - `w/ context`: Again, shows similar results, with an average of 78.02.\n   - `+ label`: This configuration yields the highest average score of 78.20 among the CNN variations.\n\n2. **RNN**:\n   - Baseline: Contains initial performance scores, averaging at 77.43.\n   - `w/ synonym`: Similar to its CNN counterpart, leading to an average score of 77.40.\n   - `w/ context`: Shows performance scores, resulting in an average of 77.62.\n   - `+ label`: This modification leads to the highest average score of 77.83 for RNN models.\n\nIn summary, the table evaluates the effectiveness of different modifications (using synonyms, context, and labels) for CNN and RNN models across various tasks/datasets, showing the average performance score for each configuration.\nFor investigating our label-conditional bi- directional LM, we show in Figure  2  the top-10 word predictions by the model for a sentence from the SST dataset. Each word in the sentence is fre- quently replaced with various words that are not always synonyms. We present two types of pre- dictions depending on the label fed into the con- ditional LM. With a positive label, the word “fan- tastic” is frequently replaced with  funny, honest, good,  and  entertaining , which are also positive ex- pressions. In contrast, with a negative label, the word “fantastic” is frequently replaced with  tired, forgettable, bad , and  dull , which reﬂect a negative sentiment. At another position, the word “the” can be replaced with “no” (with the seventh highest probability), so that the whole sentence becomes “no actors are fantastic.”, which seems negative as a whole. Aside from such inversions caused by labels, the parts unrelated to the labels (e.g., “ac- tors”) are not very different in the positive or neg- ative predictions. These results also demonstrated that conditional architectures are effective. \n4 Related Work \nSome works tried text data augmentation by us- ing synonym lists ( Zhang et al. ,  2015 ;  Wang and Yang ,  2015 ), grammar induction ( Jia and Liang , 2016 ), task-speciﬁc heuristic rules ( F¨ urstenau and Lapata ,  2009 ;  Kaﬂe et al. ,  2017 ;  Silfver- berg et al. ,  2017 ), or neural decoders of au- toencoders ( Bergmanis et al. ,  2017 ;  Xu et al. , 2017 ;  Hu et al. ,  2017 ) or encoder-decoder mod- els ( Kim and Rush ,  2016 ;  Sennrich et al. ,  2016 ; Xia et al. ,  2017 ). The works most similar to our research are Kolomiyets et al. ( 2011 ) and Fadaee et al. ( 2017 ). In a task of time expression recog- nition, Kolomiyets et al. replaced only the head- words under a task-speciﬁc assumption that tem- poral trigger words usually occur as headwords. They selected substitute words with top-K scores \nThe image is a diagram illustrating the predicted words with the ten highest probabilities from a conditional bi-directional language model applied to the sentence \"the actors are fantastic.\" \n\n- The top section lists words predicted with a positive sentiment:\n  - \"the\" - \"funny\" (in order of decreasing probability).\n\n- The bottom section lists words predicted with a negative sentiment:\n  - \"the\" - \"bland\" (in order of decreasing probability).\n\nProbability is represented vertically, with higher probabilities at the top for both positive and negative labels.\ngiven by the Latent Words LM ( Deschacht and Moens ,  2009 ), which is a LM based on ﬁxed- length contexts. Fadaee et al. ( 2017 ), focusing on the rare word problem in machine transla- tion, replaced words in a source sentence with only rare words, which both of rightward and left- ward LSTM LMs independently predict with top- K conﬁdences. A word in the translated sentence is also replaced using a word alignment method and a rightward LM. These two works share the idea of the usage of language models with our method. We used a bi-directional LSTM LM which captures variable-length contexts with con- sidering both the directions jointly. More impor- tantly, we proposed a label-conditional architec- ture and demonstrated its effect both qualitatively and quantitatively. Our method is independent of any task-speciﬁc knowledge, and effective for classiﬁcation tasks in various domains. \nWe use a label-conditional ﬁll-in-the-blank con- text for data augmentation. Neural models us- ing the ﬁll-in-the-blank context have been invested in other applications. Kobayashi et al.  ( 2016 , 2017 ) proposed to extract and organize informa- tion about each entity in a discourse using the con- text.  Fedus et al.  ( 2018 ) proposed GAN ( Goodfel- low et al. ,  2014 ) for text generation and demon- strated that the mode collapse and training insta- bility can be relieved by in-ﬁlling-task training. "}
{"page": 4, "image_path": "doc_images/N18-2072_4.jpg", "ocr_text": "bility can be relieved by in-filling-task training.\n5 Conclusion\n\nWe proposed a novel data augmentation using nu-\nmerous words given by a bi-directional LM, and\nfurther introduced a label-conditional architecture\ninto the LM. Experimentally, our method pro-\nduced various words compatibly with the labels\nof original texts and improved neural classifiers\nmore than the synonym-based augmentation. Our\nmethod is independent of any task-specific knowl-\nedge or rules, and can be generally and easily used\nfor classification tasks in various domains.\n\nOn the other hand, the improvement by our\nmethod is sometimes marginal. Future work will\nexplore comparison and combination with other\ngeneralization methods exploiting datasets deeply\nas well as our method.\n\nAcknowledgments\n\nI would like to thank the members of Preferred\nNetworks, Inc., especially Takeru Miyato and Yuta\nTsuboi, for helpful comments. I would also like to\nthank anonymous reviewers for helpful comments.\n\nReferences\n\nSamy Bengio, Oriol Vinyals, Navdeep Jaitly, and\nNoam Shazeer. 2015. Scheduled sampling for se-\nquence prediction with recurrent neural networks.\nIn NIPS, pages 1171-1179.\n\nToms Bergmanis, Katharina Kann, Hinrich Schiitze,\nand Sharon Goldwater. 2017. Training data aug-\nmentation for low-resource morphological inflec-\ntion. In CoNLL SIGMORPHON, pages 31-39.\n\nKoen Deschacht and Marie-Francine Moens. 2009.\nSemi-supervised semantic role labeling using the la-\ntent words language model. In EMNLP, pages 21—\n29.\n\nChris Dyer, Miguel Ballesteros, Wang Ling, Austin\nMatthews, and Noah A. Smith. 2015. Transition-\nbased dependency parsing with stack long short-\nterm memory. In ACL, pages 334-343.\n\nMarzieh Fadaee, Arianna Bisazza, and Christof Monz.\n2017. Data augmentation for low-resource neural\nmachine translation. In ACL, pages 567-573.\n\nWilliam Fedus, Ian Goodfellow, and Andrew M. Dai.\n2018. MaskGAN: Better text generation via filling\nin the In JCLR.\n\nHagen Fiirstenau and Mirella Lapata. 2009. Semi-\nsupervised semantic role labeling. In EACL, pages\n220-228.\n\n456\n\nlan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,\nBing Xu, David Warde-Farley, Sherjil Ozair, Aaron\nCourville, and Yoshua Bengio. 2014. Generative ad-\nversarial nets. In NIPS, pages 2672-2680.\n\nSepp Hochreiter and Jiirgen Schmidhuber. 1997.\nLong short-term memory. Neural computation,\n9(8):1735-1780.\n\nZhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan\nSalakhutdinov, and Eric P. Xing. 2017. Toward con-\ntrolled generation of text. In ICML, pages 1587-\n1596.\n\nNavdeep Jaitly and Geoffrey E Hinton. 2015. Vo-\ncal tract length perturbation (vtlp) improves speech\nrecognition. In JCML.\n\nRobin Jia and Percy Liang. 2016. Data recombination\nfor neural semantic parsing. In ACL, pages 12-22.\n\nKushal Kafle, Mohammed Yousefhussien, and Christo-\npher Kanan. 2017. Data augmentation for visual\nquestion answering. In JINLG, pages 198-202.\n\nYoon Kim. 2014. Convolutional neural networks for\nsentence classification. In EMNLP, pages 1746—\n1751.\n\nYoon Kim and Alexander M. Rush. 2016. Sequence-\nlevel knowledge distillation. In EMNLP, pages\n1317-1327.\n\nTom Ko, Vijayaditya Peddinti, Daniel Povey, and\nSanjeev Khudanpur. 2015. Audio augmentation\nfor speech recognition. In INTERSPEECH, pages\n3586-3589.\n\nSosuke Kobayashi, Naoaki Okazaki, and Kentaro Inui.\n2017. A neural language model for dynamically rep-\nresenting the meanings of unknown words and enti-\nties in a discourse. In IJCNLP, pages 473-483.\n\nSosuke Kobayashi, Ran Tian, Naoaki Okazaki, and\nKentaro Inui. 2016. Dynamic entity representation\nwith max-pooling improves machine reading. In\nProceedings of NAACL-HLT, pages 850-855.\n\nOleksandr Kolomiyets, Steven Bethard, and Marie-\nFrancine Moens. 2011. Model-portability experi-\nments for textual temporal analysis. In ACL, pages\n271-276.\n\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E Hin-\nton. 2012. Imagenet classification with deep con-\nvolutional neural networks. In NIPS, pages 1097-\n1105.\n\nXin Li and Dan Roth. 2002. Learning question classi-\nfiers. In COLING, pages 1-7.\n\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2017. Pointer sentinel mixture\nmodels. In JCLR.\n\nGeorge A. Miller. 1995. Wordnet: A lexical database\nfor english. Commun. ACM, 38(11):39-41.\n", "vlm_text": "\n5 Conclusion \nWe proposed a novel data augmentation using nu- merous words given by a bi-directional LM, and further introduced a label-conditional architecture into the LM. Experimentally, our method pro- duced various words compatibly with the labels of original texts and improved neural classiﬁers more than the synonym-based augmentation. Our method is independent of any task-speciﬁc knowl- edge or rules, and can be generally and easily used for classiﬁcation tasks in various domains. \nOn the other hand, the improvement by our method is sometimes marginal. Future work will explore comparison and combination with other generalization methods exploiting datasets deeply as well as our method. \nAcknowledgments \nI would like to thank the members of Preferred Networks, Inc., especially Takeru Miyato and Yuta Tsuboi, for helpful comments. I would also like to thank anonymous reviewers for helpful comments. \nReferences \nSamy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. 2015.  Scheduled sampling for se- quence prediction with recurrent neural networks . In  NIPS , pages 1171–1179. Toms Bergmanis, Katharina Kann, Hinrich Sch¨ utze, and Sharon Goldwater. 2017. Training data aug- mentation for low-resource morphological inﬂec- tion . In  CoNLL SIGMORPHON , pages 31–39. Koen Deschacht and Marie-Francine Moens. 2009. Semi-supervised semantic role labeling using the la- tent words language model . In  EMNLP , pages 21– 29. Chris Dyer, Miguel Ballesteros, Wang Ling, Austin Matthews, and Noah A. Smith. 2015.  Transition- based dependency parsing with stack long short- term memory . In  ACL , pages 334–343. Marzieh Fadaee, Arianna Bisazza, and Christof Monz. 2017. Data augmentation for low-resource neural machine translation . In  ACL , pages 567–573. William Fedus, Ian Goodfellow, and Andrew M. Dai. 2018.  MaskGAN: Better text generation via ﬁlling in the . In  ICLR . Hagen F¨ urstenau and Mirella Lapata. 2009. Semi- supervised semantic role labeling . In  EACL , pages 220–228. \nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014.  Generative ad- versarial nets . In  NIPS , pages 2672–2680. Sepp Hochreiter and J¨ urgen Schmidhuber. 1997. Long short-term memory . Neural computation , 9(8):1735–1780. Zhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan Salakhutdinov, and Eric P. Xing. 2017.  Toward con- trolled generation of text . In  ICML , pages 1587– 1596. Navdeep Jaitly and Geoffrey E Hinton. 2015. Vo- cal tract length perturbation (vtlp) improves speech recognition . In  ICML . Robin Jia and Percy Liang. 2016.  Data recombination for neural semantic parsing . In  ACL , pages 12–22. Kushal Kaﬂe, Mohammed Yousefhussien, and Christo- pher Kanan. 2017. Data augmentation for visual question answering . In  INLG , pages 198–202. Yoon Kim. 2014.  Convolutional neural networks for sentence classiﬁcation . In  EMNLP , pages 1746– 1751. Yoon Kim and Alexander M. Rush. 2016.  Sequence- level knowledge distillation . In  EMNLP , pages 1317–1327. Tom Ko, Vijayaditya Peddinti, Daniel Povey, and Sanjeev Khudanpur. 2015. Audio augmentation for speech recognition.  In  INTERSPEECH , pages 3586–3589. Sosuke Kobayashi, Naoaki Okazaki, and Kentaro Inui. 2017.  A neural language model for dynamically rep- resenting the meanings of unknown words and enti- ties in a discourse . In  IJCNLP , pages 473–483. Sosuke Kobayashi, Ran Tian, Naoaki Okazaki, and Kentaro Inui. 2016. Dynamic entity representation with max-pooling improves machine reading. In Proceedings of NAACL-HLT , pages 850–855. Oleksandr Kolomiyets, Steven Bethard, and Marie- Francine Moens. 2011. Model-portability experi- ments for textual temporal analysis . In  ACL , pages 271–276. Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hin- ton. 2012.  Imagenet classiﬁcation with deep con- volutional neural networks . In  NIPS , pages 1097– 1105. Xin Li and Dan Roth. 2002.  Learning question classi- ﬁers . In  COLING , pages 1–7. Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. 2017. Pointer sentinel mixture models.  In  ICLR . George A. Miller. 1995.  Wordnet: A lexical database for english .  Commun. ACM , 38(11):39–41. "}
{"page": 5, "image_path": "doc_images/N18-2072_5.jpg", "ocr_text": "Bo Pang and Lillian Lee. 2004. A sentimental educa-\ntion: Sentiment analysis using subjectivity summa-\nrization based on minimum cuts. In ACL.\n\nBo Pang and Lillian Lee. 2005. Seeing stars: Exploit-\ning class relationships for sentiment categorization\nwith respect to rating scales. In ACL, pages 115-\n124.\n\nCharles Bally Albert Sechehaye Saussure, Ferdi-\nnand de and Albert Riedlinger. 1916. Cours de lin-\nguistique generale. Lausanne: Payot.\n\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Improving neural machine translation models\nwith monolingual data. In ACL, pages 86-96.\n\nMiikka Silfverberg, Adam Wiemerslage, Ling Liu, and\nLingshuang Jack Mao. 2017. Data augmentation\nfor morphological reinflection. In CoNLL SIGMOR-\nPHON, pages 90-99.\n\nPatrice Y. Simard, Yann A. LeCun, John S. Denker, and\nBernard Victorri. 1998. Transformation Invariance\nin Pattern Recognition — Tangent Distance and Tan-\ngent Propagation. Springer Berlin Heidelberg.\n\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D. Manning, Andrew Ng, and\nChristopher Potts. 2013. Recursive deep models\nfor semantic compositionality over a sentiment tree-\nbank. In EMNLP, pages 1631-1642.\n\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.\nSequence to sequence learning with neural net-\nworks. In NIPS, pages 3104-3112.\n\nChristian Szegedy, Wei Liu, Yangqing Jia, Pierre\nSermanet, Scott Reed, Dragomir Anguelov, Du-\nmitru Erhan, Vincent Vanhoucke, and Andrew Ra-\nbinovich. 2015. Going deeper with convolutions. In\nCVPR.\n\nSeiya Tokui, Kenta Oono, Shohei Hido, and Justin\nClayton. 2015. Chainer: a next-generation open\nsource framework for deep learning. In Proceedings\nof Workshop on LearningSys in NIPS 28.\n\nWilliam Yang Wang and Diyi Yang. 2015. That’s\nso annoying!!!: A lexical and frame-semantic em-\nbedding based data augmentation approach to au-\ntomatic categorization of annoying behaviors using\n#petpeeve tweets. In EMNLP, pages 2557-2563.\n\nJanyce Wiebe, Theresa Wilson, and Claire Cardie.\n2005. Annotating expressions of opinions and emo-\ntions in language. Language Resources and Evalu-\nation, 39(2):165-210.\n\nYingce Xia, Tao Qin, Wei Chen, Jiang Bian, Nenghai\nYu, and Tie-Yan Liu. 2017. Dual supervised learn-\ning. In JCML, pages 3789-3798.\n\nWeidi Xu, Haoze Sun, Chao Deng, and Ying Tan.\n2017. Variational autoencoder for semi-supervised\ntext classification. In AAAI, pages 3358-3364.\n\n457\n\nXiang Zhang, Junbo Zhao, and Yann LeCun. 2015.\nCharacter-level convolutional networks for text clas-\nsification. In NJPS, pages 649-657.\n", "vlm_text": "Bo Pang and Lillian Lee. 2004.  A sentimental educa- tion: Sentiment analysis using subjectivity summa- rization based on minimum cuts . In  ACL . Bo Pang and Lillian Lee. 2005.  Seeing stars: Exploit- ing class relationships for sentiment categorization with respect to rating scales . In  ACL , pages 115– 124. Charles Bally Albert Sechehaye Saussure, Ferdi- nand de and Albert Riedlinger. 1916.  Cours de lin- guistique generale . Lausanne: Payot. Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016.  Improving neural machine translation models with monolingual data . In  ACL , pages 86–96. Miikka Silfverberg, Adam Wiemerslage, Ling Liu, and Lingshuang Jack Mao. 2017. Data augmentation for morphological reinﬂection . In  CoNLL SIGMOR- PHON , pages 90–99. Patrice Y. Simard, Yann A. LeCun, John S. Denker, and Bernard Victorri. 1998.  Transformation Invariance in Pattern Recognition — Tangent Distance and Tan- gent Propagation . Springer Berlin Heidelberg. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositional it y over a sentiment tree- bank . In  EMNLP , pages 1631–1642. Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to sequence learning with neural net- works . In  NIPS , pages 3104–3112. Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Du- mitru Erhan, Vincent Vanhoucke, and Andrew Ra- binovich. 2015.  Going deeper with convolutions . In CVPR . Seiya Tokui, Kenta Oono, Shohei Hido, and Justin Clayton. 2015. Chainer: a next-generation open source framework for deep learning . In  Proceedings of Workshop on LearningSys in NIPS 28 . William Yang Wang and Diyi Yang. 2015. That’s so annoying!!!: A lexical and frame-semantic em- bedding based data augmentation approach to au- tomatic categorization of annoying behaviors using #petpeeve tweets . In  EMNLP , pages 2557–2563. Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.  Annotating expressions of opinions and emo- tions in language .  Language Resources and Evalu- ation , 39(2):165–210. Yingce Xia, Tao Qin, Wei Chen, Jiang Bian, Nenghai Yu, and Tie-Yan Liu. 2017.  Dual supervised learn- ing . In  ICML , pages 3789–3798. Weidi Xu, Haoze Sun, Chao Deng, and Ying Tan. 2017.  Variational autoencoder for semi-supervised text classiﬁcation . In  AAAI , pages 3358–3364. \nXiang Zhang, Junbo Zhao, and Yann LeCun. 2015. Character-level convolutional networks for text clas- siﬁcation . In  NIPS , pages 649–657. "}
