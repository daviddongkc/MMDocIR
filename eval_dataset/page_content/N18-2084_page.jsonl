{"page": 0, "image_path": "doc_images/N18-2084_0.jpg", "ocr_text": "When and Why are Pre-trained Word Embeddings Useful\nfor Neural Machine Translation?\n\nYe Qi, Devendra Singh Sachan, Matthieu Felix,\nSarguna Janani Padmanabhan, Graham Neubig\nLanguage Technologies Institute, Carnegie Mellon University, USA\n{yeq, dsachan, matthief, sjpadman, gneubig}@andrew.cmu.edu\n\nAbstract\n\nThe performance of Neural Machine Trans-\nlation (NMT) systems often suffers in low-\nresource scenarios where sufficiently large-\nscale parallel corpora cannot be obtained. Pre-\ntrained word embeddings have proven to be\ninvaluable for improving performance in nat-\nural language analysis tasks, which often suf-\nfer from paucity of data. However, their utility\nfor NMT has not been extensively explored. In\nthis work, we perform five sets of experiments\nthat analyze when we can expect pre-trained\nword embeddings to help in NMT tasks. We\nshow that such embeddings can be surpris-\ningly effective in some cases — providing gains\nof up to 20 BLEU points in the most favorable\nsetting. !\n\n1 Introduction\n\nPre-trained word embeddings have proven to be\nhighly useful in neural network models for NLP\ntasks such as sequence tagging (Lample et al.,\n2016; Ma and Hovy, 2016) and text classifica-\ntion (Kim, 2014). However, it is much less com-\nmon to use such pre-training in NMT (Wt et al.,\n2016), largely because the large-scale training cor-\npora used for tasks such as WMT? tend to be sev-\neral orders of magnitude larger than the annotated\ndata available for other tasks, such as the Penn\nTreebank (Marcus et al., 1993). However, for low-\nresource languages or domains, it is not necessar-\nily the case that bilingual data is available in abun-\ndance, and therefore the effective use of monolin-\ngual data becomes a more desirable option.\nResearchers have worked on a number of meth-\nods for using monolingual data in NMT systems\n(Cheng et al., 2016; He et al., 2016; Ramachan-\ndran et al., 2016). Among these, pre-trained word\nembeddings have been used either in standard\n'Scripts/data to replicate experiments are available at\nhttps://github.com/neulab/word-embeddings- for-nmt\n7http://www.statmt.org/wmt17/\n\n529\n\ntranslation systems (Neishi et al., 2017; Artetxe\net al., 2017) or as a method for learning translation\nlexicons in an entirely unsupervised manner (Con-\nneau et al., 2017; Gangi and Federico, 2017). Both\nmethods show potential improvements in BLEU\nscore when pre-training is properly integrated into\nthe NMT system.\n\nHowever, from these works, it is still not clear\nas to when we can expect pre-trained embeddings\nto be useful in NMT, or why they provide perfor-\nmance improvements. In this paper, we examine\nthese questions more closely, conducting five sets\nof experiments to answer the following questions:\n\nQI Is the behavior of pre-training affected by\nlanguage families and other linguistic fea-\ntures of source and target languages? (§3)\n\nQ2\n\nDo pre-trained embeddings help more when\nthe size of the training data is small? (84)\n\nQ3 How much does the similarity of the source\nand target languages affect the efficacy of us-\n\ning pre-trained embeddings? (85)\n\nQ4 Is it helpful to align the embedding spaces be-\n\ntween the source and target languages? (86)\n\nQ5 Do pre-trained embeddings help more in\nmultilingual systems as compared to bilin-\n\ngual systems? (§7)\n\n2 Experimental Setup\n\nIn order to perform experiments in a controlled,\nmultilingual setting, we created a parallel corpus\nfrom TED talks transcripts.* Specifically, we pre-\npare data between English (EN) and three pairs\nof languages, where the two languages in the\npair are similar, with one being relatively low-\nresourced compared to the other: Galician (GL)\nand Portuguese (PT), Azerbaijani (AZ) and Turk-\nish (TR), and Belarusian (BE) and Russian (RU).\n\n3https://www.ted.com/participate/translate\n\nProceedings of NAACL-HLT 2018, pages 529-535\nNew Orleans, Louisiana, June 1 - 6, 2018. ©2018 Association for Computational Linguistics\n", "vlm_text": "When and Why are Pre-trained Word Embeddings Useful for Neural Machine Translation? \nYe Qi, Devendra Singh Sachan, Matthieu Felix, Sarguna Janani Padmanabhan, Graham Neubig \nLanguage Technologies Institute, Carnegie Mellon University, USA yeq,dsachan,matthief,sjpadman,gneubig @andrew.cmu.edu \nAbstract \nThe performance of Neural Machine Trans- lation (NMT) systems often suffers in low- resource scenarios where sufﬁciently large- scale parallel corpora cannot be obtained. Pre- trained word embeddings have proven to be invaluable for improving performance in nat- ural language analysis tasks, which often suf- fer from paucity of data. However, their utility for NMT has not been extensively explored. In this work, we perform ﬁve sets of experiments that analyze when we can expect pre-trained word embeddings to help in NMT tasks. We show that such embeddings can be surpris- ingly effective in some cases – providing gains of up to 20 BLEU points in the most favorable setting. \n1 Introduction \nPre-trained word embeddings have proven to be highly useful in neural network models for NLP tasks such as sequence tagging ( Lample et al. , 2016 ;  Ma and Hovy ,  2016 ) and text classiﬁca- tion ( Kim ,  2014 ). However, it is much less com- mon to use such pre-training in NMT ( Wu et al. , 2016 ), largely because the large-scale training cor- pora used for tasks such as   $\\mathrm{WMT^{2}}$    tend to be sev- eral orders of magnitude larger than the annotated data available for other tasks, such as the Penn Treebank ( Marcus et al. ,  1993 ). However, for low- resource languages or domains, it is not necessar- ily the case that bilingual data is available in abun- dance, and therefore the effective use of monolin- gual data becomes a more desirable option. \nResearchers have worked on a number of meth- ods for using monolingual data in NMT systems ( Cheng et al. ,  2016 ;  He et al. ,  2016 ;  Ramachan- dran et al. ,  2016 ). Among these, pre-trained word embeddings have been used either in standard translation systems ( Neishi et al. ,  2017 ;  Artetxe et al. ,  2017 ) or as a method for learning translation lexicons in an entirely unsupervised manner ( Con- neau et al. ,  2017 ;  Gangi and Federico ,  2017 ). Both methods show potential improvements in BLEU score when pre-training is properly integrated into the NMT system. \n\nHowever, from these works, it is still not clear as to  when  we can expect pre-trained embeddings to be useful in NMT, or  why  they provide perfor- mance improvements. In this paper, we examine these questions more closely, conducting ﬁve sets of experiments to answer the following questions: \nQ1 Is the behavior of pre-training affected by language families and other linguistic fea- tures of source and target languages? ( § 3 ) Q2 Do pre-trained embeddings help more when the size of the training data is small? ( § 4 ) Q3 How much does the similarity of the source and target languages affect the efﬁcacy of us- ing pre-trained embeddings? ( § 5 ) Q4 Is it helpful to align the embedding spaces be- tween the source and target languages? ( § 6 ) Q5 Do pre-trained embeddings help more in multilingual systems as compared to bilin- gual systems? ( § 7 ) \n2 Experimental Setup \nIn order to perform experiments in a controlled, multilingual setting, we created a parallel corpus from TED talks transcripts.   Speciﬁcally, we pre- pare data between English (E N ) and three pairs of languages, where the two languages in the pair are similar, with one being relatively low- resourced compared to the other: Galician (G L ) and Portuguese (P T ), Azerbaijani (A Z ) and Turk- ish (T R ), and Belarusian (B E ) and Russian (R U ). "}
{"page": 1, "image_path": "doc_images/N18-2084_1.jpg", "ocr_text": "Dataset | train dev test\nGL — EN 10, 017 682 1,007\nPT — EN 51,785 1,193 1,803\nAZ — EN 5, 946 671 903\nTR-— EN 1s 450 4,045 5,029\nBE— EN 4, 509 248 664\nRu — EN 208, 106 4,805 5,476\n\nTable 1: Number of sentences for each language pair.\n\nThe languages in each pair are similar in vocabu-\nlary, grammar and sentence structure (Matthews,\n1997), which controls for language characteristics\nand also improves the possibility of transfer learn-\ning in multi-lingual models (in §7). They also rep-\nresent different language families - GL/PT are Ro-\nmance; AZ/TR are Turkic; BE/RU are Slavic — al-\nlowing for comparison across languages with dif-\nferent caracteristics. Tokenization was done using\nMoses tokenizer* and hard punctuation symbols\nwere used to identify sentence boundaries. Table 1\nshows data sizes.\n\nFor our experiments, we use a standard |-layer\nencoder-decoder model with attention (Bahdanau\net al., 2014) with a beam size of 5 implemented in\nxnmt> (Neubig et al., 2018). Training uses a batch\nsize of 32 and the Adam optimizer (Kingma and\nBa, 2014) with an initial learning rate of 0.0002,\ndecaying the learning rate by 0.5 when devel-\nopment loss decreases (Denkowski and Neubig,\n2017). We evaluate the model’s performance us-\ning BLEU metric (Papineni et al., 2002).\n\nWe use available pre-trained word embed-\ndings (Bojanowski et al., 2016) trained using\nfastText® on Wikipedia’ for each language.\nThese word embeddings (Mikolov et al., 2017)\nincorporate character-level, phrase-level and posi-\ntional information of words and are trained using\nCBOW algorithm (Mikolov et al., 2013). The di-\nmension of word embeddings is set to 300. The\nembedding layer weights of our model are initial-\nized using these pre-trained word vectors. In base-\nline models without pre-training, we use Glorot\nand Bengio (2010)’s uniform initialization.\n\n3 QI: Efficacy of Pre-training\n\nIn our first set of experiments, we examine the ef-\nficacy of pre-trained word embeddings across the\nvarious languages in our corpus. In addition to\n\n4https://github.com/moses-smt/mosesdecoder/blob/\nmaster/scripts/tokenizer/tokenizer.perl\nShttps://github.com/neulab/xnmt/\n°https://github.com/facebookresearch/fastText/\nThttps://dumps.wikimedia.org/\n\n530\n\nSre > std pre std pre\n— Trg | std std pre pre\n\nGL — EN 2.2 13.2 2.8 12.8\nPT EN | 26.2 30.3 26.1 30.8\nAZ — EN 1.3 2.0 1.6 2.0\nTREN | 149 17.6 147 17.9\nBE — EN 2.5 1.3 3.0\nRu — EN 21.2 18.7 21.1\n\nTable 2: Effect of pre-training on BLEU score over six\nlanguages. The systems use either random initializa-\ntion (std) or pre-training (pre) on both the source and\ntarget sides.\n\nproviding additional experimental evidence sup-\nporting the findings of other recent work on us-\ning pre-trained embeddings in NMT (Neishi et al.,\n2017; Artetxe et al., 2017; Gangi and Federico,\n2017), we also examine whether pre-training is\nuseful across a wider variety of language pairs and\nif it is more useful on the source or target side of a\ntranslation pair.\n\nThe results in Table 2 clearly demonstrate that\npre-training the word embeddings in the source\nand/or target languages helps to increase the\nBLEU scores to some degree. Comparing the sec-\nond and third columns, we can see the increase is\nmuch more significant with pre-trained source lan-\nguage embeddings. This indicates that the major-\nity of the gain from pre-trained word embeddings\nresults from a better encoding of the source sen-\nence.\n\nThe gains from pre-training in the higher-\nresource languages are consistent: ~3 BLEU\npoints for all three language pairs. In contrast, for\nhe extremely low-resource languages, the gains\nare either quite small (AZ and BE) or very large,\nas in GL which achieves a gain of up to 11 BLEU\npoints. This finding is interesting in that it indi-\ncates that word embeddings may be particularly\nuseful to bootstrap models that are on the thresh-\nold of being able to produce reasonable transla-\nions, as is the case for GL in our experiments.\n\n4 Q2: Effect of Training Data Size\n\nThe previous experiment had interesting implica-\ntions regarding available data size and effect of\npre-training. Our next series of experiments ex-\namines this effect in a more controlled environ-\nment by down-sampling the training data for the\nhigher-resource languages to 1/2, 1/4 and 1/8 of\ntheir original sizes.\n\nFrom the BLEU scores in Figure 1, we can see\n", "vlm_text": "The table provides information about datasets with different language pairs translating into English (EN). The columns represent the size of the datasets for training (train), development (dev), and testing (test). Here are the details:\n\n- **GL → EN**\n  - Train: 10,017\n  - Dev: 682\n  - Test: 1,007\n\n- **PT → EN**\n  - Train: 51,785\n  - Dev: 1,193\n  - Test: 1,803\n\n- **AZ → EN**\n  - Train: 5,946\n  - Dev: 671\n  - Test: 903\n\n- **TR → EN**\n  - Train: 182,450\n  - Dev: 4,045\n  - Test: 5,029\n\n- **BE → EN**\n  - Train: 4,509\n  - Dev: 248\n  - Test: 664\n\n- **RU → EN**\n  - Train: 208,106\n  - Dev: 4,805\n  - Test: 5,476\nThe languages in each pair are similar in vocabu- lary, grammar and sentence structure ( Matthews , 1997 ), which controls for language characteristics and also improves the possibility of transfer learn- ing in multi-lingual models (in  $\\S7$  ). They also rep- resent different language families – G L /P T  are Ro- mance; A Z /T R  are Turkic; B E /R U  are Slavic – al- lowing for comparison across languages with dif- ferent caracteristics. Tokenization was done using Moses  tokenizer 4   and hard punctuation symbols were used to identify sentence boundaries. Table  1 shows data sizes. \nFor our experiments, we use a standard 1-layer encoder-decoder model with attention ( Bahdanau et al. ,  2014 ) with a beam size of  5  implemented in xnmt 5   ( Neubig et al. ,  2018 ). Training uses a batch size of  32  and the Adam optimizer ( Kingma and Ba ,  2014 ) with an initial learning rate of  0 . 0002 , decaying the learning rate by  0 . 5  when devel- opment loss decreases ( Denkowski and Neubig , 2017 ). We evaluate the model’s performance us- ing BLEU metric ( Papineni et al. ,  2002 ). \nWe use available pre-trained word embed- dings ( Bojanowski et al. ,  2016 ) trained using fastText 6   on Wikipedia 7   for each language. These word embeddings ( Mikolov et al. ,  2017 ) incorporate character-level, phrase-level and posi- tional information of words and are trained using CBOW algorithm ( Mikolov et al. ,  2013 ). The di- mension of word embeddings is set to  300 . The embedding layer weights of our model are initial- ized using these pre-trained word vectors. In base- line models without pre-training, we use  Glorot and Bengio  ( 2010 )’s uniform initialization. \n3 Q1: Efﬁcacy of Pre-training \nIn our ﬁrst set of experiments, we examine the ef- ﬁcacy of pre-trained word embeddings across the various languages in our corpus. In addition to \nThis table appears to display performance metrics for machine translation tasks with different source (Src) and target (Trg) languages. The metrics are likely evaluation scores (such as BLEU scores) that measure the quality of translations from the source language to English (EN). The table is structured to show results under two conditions: \"std\" and \"pre\".\n\nHere's what each row and column represents:\n\n- The first column indicates the source (Src) language and target (Trg) language pair, denoted as Src → Trg. The pairs in the table are:\n  - Galician (GL) to English (EN)\n  - Portuguese (PT) to English (EN)\n  - Azerbaijani (AZ) to English (EN)\n  - Turkish (TR) to English (EN)\n  - Belarusian (BE) to English (EN)\n  - Russian (RU) to English (EN)\n\n- The second and third columns appear under the label \"std\" and represent standard translation metrics in two different contexts:\n  - \"std std\" likely denotes standard conditions without pretraining or other special adjustments.\n  - \"std pre\" could represent a scenario where some preprocessing or preparation was applied.\n\n- The fourth and fifth columns appear under the label \"pre\" and also represent two different contexts:\n  - \"std pre\" might refer to a standard setting but with preprocessing.\n  - \"pre pre\" might indicate both models and data were preprocessed, or some other enhanced condition.\n\n- The numeric values in each cell are likely the evaluation scores under these conditions for each respective language pair.\n\nBold numbers are used to highlight notable results, possibly indicating improvements or significant scores.\n\nIn summary, the table compares translation quality for different language pairs under different processing conditions, emphasizing how preprocessing or other factors might affect performance.\nproviding additional experimental evidence sup- porting the ﬁndings of other recent work on us- ing pre-trained embeddings in NMT ( Neishi et al. , 2017 ;  Artetxe et al. ,  2017 ;  Gangi and Federico , 2017 ), we also examine whether pre-training is useful across a wider variety of language pairs and if it is more useful on the source or target side of a translation pair. \nThe results in Table  2  clearly demonstrate that pre-training the word embeddings in the source and/or target languages helps to increase the BLEU scores to some degree. Comparing the sec- ond and third columns, we can see the increase is much more signiﬁcant with pre-trained source lan- guage embeddings. This indicates that the major- ity of the gain from pre-trained word embeddings results from a better encoding of the source sen- tence. \nThe gains from pre-training in the higher- resource languages are consistent:  ${\\approx}3$   BLEU points for all three language pairs. In contrast, for the extremely low-resource languages, the gains are either quite small (A Z  and B E ) or very large, as in G L  which achieves a gain of up to 11 BLEU points. This ﬁnding is interesting in that it indi- cates that word embeddings may be particularly useful to bootstrap models that are on the thresh- old of being able to produce reasonable transla- tions, as is the case for G L  in our experiments. \n4 Q2: Effect of Training Data Size \nThe previous experiment had interesting implica- tions regarding available data size and effect of pre-training. Our next series of experiments ex- amines this effect in a more controlled environ- ment by down-sampling the training data for the higher-resource languages to 1/2, 1/4 and 1/8 of their original sizes. \nFrom the BLEU scores in Figure  1 , we can see "}
{"page": 2, "image_path": "doc_images/N18-2084_2.jpg", "ocr_text": "— Pt-En (std)\n-4- Pt>En (pre)\n\n30\n\n— TrEn (std) ~—— Ru-En (std)\n\n-4- Run (pre)\n\n-4- Tr>En (pre)\n\n0.2 0.4 0.6\n\nTraining Set Size\n\n0.8 1.0\n\n© Pt-En (increase)\nTr En (increase)\nRu En (increase)\n\n- BLEU(std)\n\nBLEU(pre)\n\n0.2\n\n0.4 0.6\n\nTraining Set Size\n\n0.8\n\nFigure 1: BLEU and BLEU gain by data size.\n\nthat for all three languages the gain in BLEU score\ndemonstrates a similar trend to that found in GL in\nthe previous section: the gain is highest when the\nbaseline system is poor but not too poor, usually\nwith a baseline BLEU score in the range of 3-4.\nThis suggests that at least a moderately effective\nsystem is necessary before pre-training takes ef-\nfect, but once there is enough data to capture the\nbasic characteristics of the language, pre-training\ncan be highly effective.\n\n5 Q3: Effect of Language Similarity\n\nThe main intuitive hypothesis as to why pre-\ntraining works is that the embedding space be-\ncomes more consistent, with semantically simi-\nlar words closer together. We can also make an\nadditional hypothesis: if the two languages in\nthe translation pair are more linguistically simi-\nlar, the semantic neighborhoods will be more sim-\nilar between the two languages (i.e. semantic dis-\ntinctions or polysemy will likely manifest them-\nselves in more similar ways across more simi-\nlar languages). As a result, we may expect that\nthe gain from pre-training of embeddings may be\nlarger when the source and target languages are\nmore similar. To examine this hypothesis, we se-\nlected Portuguese as the target language, which\nwhen following its language family tree from top\nto bottom, belongs to Indo-European, Romance,\n\n531\n\nDataset | Lang. Family | std pre\nEs + PT West-Iberian 17.8 24.8 (+7.0)\nFR — Pr | Western Romance | 12.4 18.1 (+5.7)\nIt > PT Romance 14.5 19.2 (+4.7)\nRu > PT Indo-European 2.4 8.6 (+6.2)\nHE > PT No Common 3.0 11.9 (+8.9)\n\nTable 3: Effect of linguistic similarity and pre-training\non BLEU. The language family in the second column is\nthe most recent common ancestor of source and target\nlanguage.\n\nWestern Romance, and West-Iberian families. We\nthen selected one source language from each fam-\nily above.® To avoid the effects of training set size,\nall pairs were trained on 40,000 sentences.\n\nFrom Table 3, we can see that the BLEU scores\nof Es, FR, and IT do generally follow this hy-\npothesis. As we move to very different languages,\nRu and HE see larger accuracy gains than their\nmore similar counterparts FR and IT. This can be\nlargely attributed to the observation from the pre-\nvious section that systems with larger headroom to\nimprove tend to see larger increases; RU and HE\nhave very low baseline BLEU scores, so it makes\nsense that their increases would be larger.\n\n6 Q4: Effect of Word Embedding\nAlignment\n\nUntil now, we have been using embeddings that\nhave been trained independently in the source and\ntarget languages, and as a result there will not nec-\nessarily be a direct correspondence between the\nembedding spaces in both languages. However,\nwe can postulate that having consistent embedding\nspaces across the two languages may be benefi-\ncial, as it would allow the NMT system to more\neasily learn correspondences between the source\nand target. To test this hypothesis, we adopted\nthe approach proposed by Smith et al. (2017) to\nlearn orthogonal transformations that convert the\nword embeddings of multiple languages to a single\nspace and used these aligned embeddings instead\nof independent ones.\n\nFrom Table 4, we can see that somewhat sur-\nprisingly, the alignment of word embeddings was\nnot beneficial for training, with gains or losses es-\nsentially being insignificant across all languages.\nThis, in a way, is good news, as it indicates that a\npriori alignment of embeddings may not be neces-\n\nEnglish was excluded because the TED talks were orig-\ninally in English, which results in it having much higher\nBLEU scores than the other languages due to it being direct\ntranslation instead of pivoted through English like the others.\n", "vlm_text": "The image consists of two line graphs illustrating the relationship between training set size and BLEU scores for translation tasks from Portuguese (Pt), Turkish (Tr), and Russian (Ru) to English (En). \n\n1. **Top Graph**:\n   - The top graph shows the BLEU scores as a function of training set size, ranging from 0 to 1.0 (representing the proportion of the dataset used).\n   - It compares standard training (std) with pre-trained models (pre).\n   - There are three pairs of lines: \n     - Blue lines represent Pt→En translations.\n     - Red lines represent Tr→En translations.\n     - Green lines represent Ru→En translations.\n   - For each language, the dashed line indicates the pre-trained model's BLEU score, and the solid line indicates the standard model's score.\n\n2. **Bottom Graph**:\n   - The bottom graph shows the gain in BLEU score achieved by using pre-trained models over standard models, calculated as BLEU(pre) - BLEU(std).\n   - It highlights the improvement for each language pair:\n     - Blue dots denote Pt→En.\n     - Red dots denote Tr→En.\n     - Green dots denote Ru→En.\n   - The graph suggests that the gain from pre-training is more significant at smaller training set sizes and decreases as the training set size increases.\n\nOverall, the graphs demonstrate that using pre-training techniques can result in higher BLEU scores, especially when the training data is limited.\nthat for all three languages the gain in BLEU score demonstrates a similar trend to that found in G L  in the previous section: the gain is highest when the baseline system is poor but not too poor, usually with a baseline BLEU score in the range of 3-4. This suggests that at least a moderately effective system is necessary before pre-training takes ef- fect, but once there is enough data to capture the basic characteristics of the language, pre-training can be highly effective. \n5 Q3: Effect of Language Similarity \nThe main intuitive hypothesis as to why pre- training works is that the embedding space be- comes more consistent, with semantically simi- lar words closer together. We can also make an additional hypothesis: if the two languages in the translation pair are more linguistically simi- lar, the semantic neighborhoods will be more sim- ilar between the two languages (i.e. semantic dis- tinctions or polysemy will likely manifest them- selves in more similar ways across more simi- lar languages). As a result, we may expect that the gain from pre-training of embeddings may be larger when the source and target languages are more similar. To examine this hypothesis, we se- lected Portuguese as the target language, which when following its language family tree from top to bottom, belongs to Indo-European, Romance, \nThe table presents data related to translation datasets from various source languages to Portuguese (PT), categorized by language family. It contains the following columns:\n\n1. **Dataset:** Lists the source to target language pairs:\n   - ES → PT (Spanish to Portuguese)\n   - FR → PT (French to Portuguese)\n   - IT → PT (Italian to Portuguese)\n   - RU → PT (Russian to Portuguese)\n   - HE → PT (Hebrew to Portuguese)\n\n2. **Lang. Family:** Shows the language family or common linguistic group of the source language:\n   - Spanish belongs to the West-Iberian family.\n   - French belongs to the Western Romance family.\n   - Italian belongs to the Romance family.\n   - Russian is part of the Indo-European family.\n   - Hebrew is labeled as having \"No Common\" language family with Portuguese.\n\n3. **std:** Represents some standard metric value for the given language pair.\n\n4. **pre:** Represents another metric, likely a pre-optimized or pre-processing metric value, with the improvement shown in parentheses:\n   - ES → PT shows an improvement of +7.0\n   - FR → PT shows an improvement of +5.7\n   - IT → PT shows an improvement of +4.7\n   - RU → PT shows an improvement of +6.2\n   - HE → PT shows an improvement of +8.9\nWestern Romance, and West-Iberian families. We then selected one source language from each fam- ily above.   To avoid the effects of training set size, all pairs were trained on 40,000 sentences. \nFrom Table  3 , we can see that the BLEU scores of E S , F R , and I T  do generally follow this hy- pothesis. As we move to very different languages, R U  and H E  see larger accuracy gains than their more similar counterparts F R  and I T . This can be largely attributed to the observation from the pre- vious section that systems with larger headroom to improve tend to see larger increases; R U  and H E have very low baseline BLEU scores, so it makes sense that their increases would be larger. \n6 Q4: Effect of Word Embedding Alignment \nUntil now, we have been using embeddings that have been trained independently in the source and target languages, and as a result there will not nec- essarily be a direct correspondence between the embedding spaces in both languages. However, we can postulate that having consistent embedding spaces across the two languages may be beneﬁ- cial, as it would allow the NMT system to more easily learn correspondences between the source and target. To test this hypothesis, we adopted the approach proposed by  Smith et al.  ( 2017 ) to learn orthogonal transformations that convert the word embeddings of multiple languages to a single space and used these aligned embeddings instead of independent ones. \nFrom Table  4 , we can see that somewhat sur- prisingly, the alignment of word embeddings was not beneﬁcial for training, with gains or losses es- sentially being insigniﬁcant across all languages. This, in a way, is good news, as it indicates that    $a$  priori  alignment of embeddings may not be neces- "}
{"page": 3, "image_path": "doc_images/N18-2084_3.jpg", "ocr_text": "Dataset | unaligned aligned\nGL — EN 12.8 11.5 (—1.3)\nPr > EN 30.8 30.6 (—0.2)\nAZ — EN 2.0 2.1 (+0.1)\nTR EN 17.9 17.7 (-0.2)\nBE EN 3.0 3.0 (+0.0)\nRu > EN 21.1 21.4 (+0.3)\n\nTable 4: Correlation between word embedding align-\nment and BLEU score in bilingual translation task.\n\nTrain Eval | bi | std pre align\nGL + PT GL 2.2 17.5 20.8 22.4\nAZ+TR AZ 1.3 5.4 5.9 7.5\nBE+Ru_ BE 1.6 | 10.0 7.9 9.6\n\nTable 5: Effect of pre-training on multilingual trans-\nlation into English. bi is a bilingual system trained\non only the eval source language and all others are\nmulti-lingual systems trained on two similar source\nlanguages.\n\nsary in the context of NMT, since the NMT system\ncan already learn a reasonable projection of word\nembeddings during its normal training process.\n\n7 Q5: Effect of Multilinguality\n\nFinally, it is of interest to consider pre-training\nin multilingual translation systems that share an\nencoder or decoder between multiple languages\n(Johnson et al., 2016; Firat et al., 2016), which is\nanother promising way to use additional data (this\ntime from another language) as a way to improve\nNMT. Specifically, we train a model using our\npairs of similar low-resource and higher-resource\nlanguages, and test on only the low-resource lan-\nguage. For those three pairs, the similarity of\nGL/PT is the highest while BE/RU is the lowest.\nWe report the results in Table 5. When applying\npre-trained embeddings, the gains in each transla-\ntion pair are roughly in order of their similarity,\nwith GL/PT showing the largest gains, and BE/RU\nshowing a small decrease. In addition, it is also\ninteresting to note that as opposed to previous sec-\ntion, aligning the word embeddings helps to in-\ncrease the BLEU scores for all three tasks. These\nincreases are intuitive, as a single encoder is used\nfor both of the source languages, and the encoder\nwould have to learn a significantly more compli-\ncated transform of the input if the word embed-\ndings for the languages were in a semantically sep-\narate space. Pre-training and alignment ensures\nthat the word embeddings of the two source lan-\nguages are put into similar vector spaces, allowing\n\n532\n\nthe model to learn in a similar fashion as it would\nif training on a single language.\n\nInterestingly, BE —> EN does not seem to ben-\nefit from pre-training in the multilingual scenario,\nwhich hypothesize is due to the fact that: 1) Be-\nlarusian and Russian are only partially mutually\nintelligible (Corbett and Comrie, 2003), i.e., they\nare not as similar; 2) the Slavic languages have\ncomparatively rich morphology, making sparsity\nin the trained embeddings a larger problem.\n\n8 Analysis\n\n8.1 Qualitative Analysis\n\nFinally, we perform a qualitative analysis of the\ntranslations from GL — EN, which showed one of\nthe largest increases in quantitative numbers. As\ncan be seen from Table 6, pre-training not only\nhelps the model to capture rarer vocabulary but\nalso generates sentences that are more grammat-\nically well-formed. As highlighted in the table\ncells, the best system successfully translates a per-\nson’s name (“chris”) and two multi-word phrases\n(‘big lawyer” and “patent legislation’’), indicat-\ning the usefulness of pre-trained embeddings in\nproviding a better representations of less frequent\nconcepts when used with low-resource languages.\nIn contrast, the bilingual model without pre-\ntrained embeddings substitutes these phrases for\ncommon ones (“i”), drops them entirely, or pro-\nduces grammatically incorrect sentences. The in-\ncomprehension of core vocabulary causes devia-\nion of the sentence semantics and thus increases\nhe uncertainty in predicting next words, gener-\nating several phrasal loops which are typical in\nNMT systems.\n\n8.2 Analysis of Frequently Generated\nn-grams.\n\nWe additionally performed pairwise comparisons\nbetween the top 10 n-grams that each system (se-\nlected from the task GL — EN) is better at gen-\nerating, to further understand what kind of words\npre-training is particularly helpful for.? The re-\nsults displayed in Table 7 demonstrate that pre-\ntraining helps both with words of low frequency in\nhe training corpus, and even with function words\nsuch as prepositions. On the other hand, the im-\nprovements in systems without pre-trained embed-\n\n° Analysis was performed using compare-mt .py from\nttps://github.com/neubig/util-scripts/.\n\n", "vlm_text": "The table shows different datasets with scores for \"unaligned\" and \"aligned\" categories. Here's a breakdown:\n\n- **GL → EN**\n  - Unaligned: 12.8\n  - Aligned: 11.5 (change: −1.3)\n  \n- **PT → EN**\n  - Unaligned: 30.8\n  - Aligned: 30.6 (change: −0.2)\n  \n- **AZ → EN**\n  - Unaligned: 2.0\n  - Aligned: 2.1 (change: +0.1)\n  \n- **TR → EN**\n  - Unaligned: 17.9\n  - Aligned: 17.7 (change: −0.2)\n  \n- **BE → EN**\n  - Unaligned: 3.0\n  - Aligned: 3.0 (change: +0.0)\n  \n- **RU → EN**\n  - Unaligned: 21.1\n  - Aligned: 21.4 (change: +0.3) \n\nThe numbers in parentheses represent the difference between aligned and unaligned scores.\nThe table presents evaluation metrics for different training and evaluation setups involving language pairs. Here are the details:\n\n- **Columns**:\n  - **Train**: Represents the languages used for training.\n  - **Eval**: The language used for evaluation.\n  - **bi**: A metric score, potentially a base or baseline performance.\n  - **std**: A standard metric score.\n  - **pre**: Represents a metric score after some preprocessing or specific processing technique.\n  - **align**: Represents an alignment metric score, apparently the highest in each case, possibly showing improvement.\n\n- **Rows**:\n  1. **GL + PT evaluated on GL**: \n     - bi: 2.2\n     - std: 17.5\n     - pre: 20.8\n     - align: 22.4\n\n  2. **AZ + TR evaluated on AZ**:\n     - bi: 1.3\n     - std: 5.4\n     - pre: 5.9\n     - align: 7.5\n\n  3. **BE + RU evaluated on BE**:\n     - bi: 1.6\n     - std: 10.0\n     - pre: 7.9\n     - align: 9.6\n\nThe table likely compares the effectiveness of using different methods for language model training or alignment and their impact on evaluation scores.\nTable 5: Effect of pre-training on multilingual trans- lation into English.  bi  is a bilingual system trained on only the eval source language and all others are multi-lingual systems trained on two similar source languages. \nsary in the context of NMT, since the NMT system can already learn a reasonable projection of word embeddings during its normal training process. \n7 Q5: Effect of Multilinguality \nFinally, it is of interest to consider pre-training in multilingual translation systems that share an encoder or decoder between multiple languages ( Johnson et al. ,  2016 ;  Firat et al. ,  2016 ), which is another promising way to use additional data (this time from another language) as a way to improve NMT. Speciﬁcally, we train a model using our pairs of similar low-resource and higher-resource languages, and test on only the low-resource lan- guage. For those three pairs, the similarity of G L /P T  is the highest while B E /R U  is the lowest. \nWe report the results in Table  5 . When applying pre-trained embeddings, the gains in each transla- tion pair are roughly in order of their similarity, with G L /P T  showing the largest gains, and B E /R U showing a small decrease. In addition, it is also interesting to note that as opposed to previous sec- tion, aligning the word embeddings helps to in- crease the BLEU scores for all three tasks. These increases are intuitive, as a single encoder is used for both of the source languages, and the encoder would have to learn a signiﬁcantly more compli- cated transform of the input if the word embed- dings for the languages were in a semantically sep- arate space. Pre-training and alignment ensures that the word embeddings of the two source lan- guages are put into similar vector spaces, allowing the model to learn in a similar fashion as it would if training on a single language. \n\nInterestingly,   $\\mathbf{BE}\\rightarrow\\mathbf{EN}$   does not seem to ben- eﬁt from pre-training in the multilingual scenario, which hypothesize is due to the fact that: 1) Be- larusian and Russian are only partially mutually intelligible ( Corbett and Comrie ,  2003 ), i.e., they are not as similar; 2) the Slavic languages have comparatively rich morphology, making sparsity in the trained embeddings a larger problem. \n8 Analysis \n8.1 Qualitative Analysis \nFinally, we perform a qualitative analysis of the translations from   ${\\mathrm{GL}}\\to{\\mathrm{EN}}$  , which showed one of the largest increases in quantitative numbers. As can be seen from Table  6 , pre-training not only helps the model to capture rarer vocabulary but also generates sentences that are more grammat- ically well-formed. As highlighted in the table cells, the best system successfully translates a per- son’s name (“ chris ”) and two multi-word phrases (“ big lawyer ” and “ patent legislation ”), indicat- ing the usefulness of pre-trained embeddings in providing a better representations of less frequent concepts when used with low-resource languages. \nIn contrast, the bilingual model without pre- trained embeddings substitutes these phrases for common ones   $(^{\\ast e}i^{\\ast\\ast})$  , drops them entirely, or pro- duces grammatically incorrect sentences. The in- comprehension of core vocabulary causes devia- tion of the sentence semantics and thus increases the uncertainty in predicting next words, gener- ating several phrasal loops which are typical in NMT systems. \n8.2 Analysis of Frequently Generated  $n$ -grams.\nWe additionally performed pairwise comparisons between the top   $10~\\mathrm{{n}}$  -grams that each system (se- lected from the task   ${\\mathrm{GL}}\\to{\\mathrm{EN}})$  ) is better at gen- erating, to further understand what kind of words pre-training is particularly helpful for.   The re- sults displayed in Table  7  demonstrate that pre- training helps both with words of low frequency in the training corpus, and even with function words such as prepositions. On the other hand, the im- provements in systems without pre-trained embed- "}
{"page": 4, "image_path": "doc_images/N18-2084_4.jpg", "ocr_text": "source (risos ) e é que chris é un grat\n\ne absolutamente nada sobre xenéti\nreference\n\ncertainly nothing about genetics .\nbi:std\n\n0}\n\nado , pero non sabia case nada sobre lexislacién de patentes\n\n( laughter ) now chris is a really brilliant lawyer , but he knew almost nothing about patent law and\n\n( laughter ) andi ’m not a little bit of a little bit of a little bit of and ( laughter ) and i ’m going to be\n\nable to be a lot of years .\nmulti:pre-align\n\n( laughter ) and chris is a big lawyer , but i did n’t know almost anything about patent legislation\n\nand absolutely nothing about genetic .\nTable 6: Example translations of GL — EN.\nbi:std bi:pre multi:std multi:pret+talign\n) so 2/0 | about 0/53 here 6/0 | on the 0/14\n( laughter ) i 2/0 | people 0/49 again , 4/0 | like 1/20\nya 2/0 | or 0/43 several 4/0 | should 0/9\nlaughter ) i 2/0 | these 0/39 you ‘re going 4/0 | court 0/9\n) and 2/0 | with 0/38 \"ve 4/0 | judge 0/7\nthey were 1/0 | because 0/37 we ‘ve 4/0 | testosterone 0/6\nhave to 5/2 | like 0/36 you 're going to 4/0 | patents 0/6\na new 1/0 | could 0/35 people , 4/0 | patent 0/6\nto do, 1/0 | all 0/34 what are 3/0 | test 0/6\n** and then 1/0 | two 0/32 the room 3/0 | with 1/12\n\n(a) Pairwise comparison between two bilingual models\n\n(b) Pairwise comparison between two multilingual models\n\nTable 7: Top 10 n-grams that one system did a better job of producing. The numbers in the figure, separated by a\nslash, indicate how many times each n-gram is generated by each of the two systems.\n\ndings were not very consistent, and largely fo-\ncused on high-frequency words.\n\n8.3 F-measure of Target Words\n\n2 3 4 5:9 10:99 100.999 1000+\n\nFrequency in Training Corpus\n\n08\nmstd\n\npre\n\nF-measure\n\nFigure 2: The f-measure of target words in bilingual\ntranslation task PT + EN\n\nFinally, we performed a comparison of the f-\nmeasure of target words, bucketed by frequency\nin the training corpus. As displayed in Figure 2,\nthis shows that pre-training manages to improve\nthe accuracy of translation for the entire vocabu-\nlary, but particularly for words that are of low fre-\nquency in the training corpus.\n\n9 Conclusion\n\nThis paper examined the utility of considering pre-\ntrained word embeddings in NMT from a number\n\n533\n\nof angles. Our conclusions have practical effects\non the recommendations for when and why pre-\ntrained embeddings may be effective in NMT, par-\nticularly in low-resource scenarios: (1) there is a\nsweet-spot where word embeddings are most ef-\nfective, where there is very little training data but\nnot so little that the system cannot be trained at all,\n(2) pre-trained embeddings seem to be more effec-\ntive for more similar translation pairs, (3) a priori\nalignment of embeddings may not be necessary in\nbilingual scenarios, but is helpful in multi-lingual\ntraining scenarios.\n\nAcknowledgements\n\nParts of this work were sponsored by De-\nfense Advanced Research Projects Agency In-\nformation Innovation Office (120). Program:\nLow Resource Languages for Emergent Incidents\n(LORELEI). Issued by DARPA/I20 under Con-\ntract No. HROO11-15-C-0114. The views and con-\nclusions contained in this document are those of\nthe authors and should not be interpreted as rep-\nresenting the official policies, either expressed or\nimplied, of the U.S. Government. The U.S. Gov-\nernment is authorized to reproduce and distribute\nreprints for Government purposes notwithstanding\nany copyright notation here on.\n", "vlm_text": "The table shows four rows with text related to a translation or transcription task:\n\n1. **Source (in another language):** Describes a lawyer, Chris, who knows nothing about patent law or genetics. It includes the term \"( risos)\" indicating laughter.\n\n2. **Reference:** An English version indicating that Chris is a brilliant lawyer who knows little about patent law and genetics. Includes \"( laughter).\"\n\n3. **bi:std:** An English segment, which appears to deviate significantly and discusses being around for a long time, punctuated by laughter.\n\n4. **multi:pre-align:** Another English segment, stating Chris is a big lawyer who knows almost nothing about patent legislation and genetics, including \"( laughter).\"\n\nThe table contrasts variations of translated or interpreted texts.\nThe table presents word or phrase pairs along with two numerical values separated by a slash (e.g., \"2/0\", \"0/53\"). The table is divided into four columns, each topped by a header: \"bi:std\", \"bi:pre\", \"multi:std\", and \"multi:pre+align\". Here is what each column contains:\n\n1. **bi:std**: \n   - A list of phrases or word sequences is shown with a numerical score formatting style \"numerator/denominator\". The left side seems to reference pairs of expressions separated by brackets or commas.\n\n2. **bi:pre**: \n   - A list of words is presented with corresponding scores formatted as \"numerator/denominator\".\n\n3. **multi:std**: \n   - Contains phrases or word sequences often consisting of several words, each associated with a score in the same formatting style as above.\n\n4. **multi:pre+align**: \n   - This column lists multi-word phrases with their scores next to them, again formatted as \"numerator/denominator\".\n\nThe table seems to be comparing or analyzing word frequencies or associations, possibly in a linguistic or data-driven evaluation context, although the exact purpose is unclear from the provided information.\nTable 7: Top   $10\\;\\mathrm{{n}}$  -grams that one system did a better job of producing. The numbers in the ﬁgure, separated by a slash, indicate how many times each n-gram is generated by each of the two systems. \ndings were not very consistent, and largely fo- cused on high-frequency words. \nThe image is a bar chart comparing F-measure scores of target words based on their frequency in a training corpus. The x-axis represents different frequency ranges: 1, 2, 3, 4, 5-9, 10-99, 100-999, and 1000+. The y-axis represents the F-measure values, ranging from 0 to 0.8.\n\nTwo sets of bars are shown for each frequency range:\n- \"std\" (in blue)\n- \"pre\" (in red).\n\nThe chart shows that both \"std\" and \"pre\" F-measures increase with word frequency, with \"pre\" often outperforming \"std\" in higher frequency ranges.\nFigure 2: The f-measure of target words in bilingual translation task   $\\mathrm{PT}\\rightarrow\\mathrm{EN}$  \nFinally, we performed a comparison of the f- measure of target words, bucketed by frequency in the training corpus. As displayed in Figure  2 , this shows that pre-training manages to improve the accuracy of translation for the entire vocabu- lary, but particularly for words that are of low fre- quency in the training corpus. \n9 Conclusion \nThis paper examined the utility of considering pre- trained word embeddings in NMT from a number of angles. Our conclusions have practical effects on the recommendations for when and why pre- trained embeddings may be effective in NMT, par- ticularly in low-resource scenarios: (1) there is a sweet-spot where word embeddings are most ef- fective, where there is very little training data but not so little that the system cannot be trained at all, (2) pre-trained embeddings seem to be more effec- tive for more similar translation pairs, (3)  a priori alignment of embeddings may not be necessary in bilingual scenarios, but is helpful in multi-lingual training scenarios. \n\nAcknowledgements \nParts of this work were sponsored by De- fense Advanced Research Projects Agency In- formation Innovation Ofﬁce (I2O). Program: Low Resource Languages for Emergent Incidents (LORELEI). Issued by DARPA/I2O under Con- tract No. HR0011-15-C-0114. The views and con- clusions contained in this document are those of the authors and should not be interpreted as rep- resenting the ofﬁcial policies, either expressed or implied, of the U.S. Government. The U.S. Gov- ernment is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation here on. "}
{"page": 5, "image_path": "doc_images/N18-2084_5.jpg", "ocr_text": "References\n\nMikel Artetxe, Gorka Labaka, Eneko Agirre, and\nKyunghyun Cho. 2017. Unsupervised neural ma-\nchine translation. arXiv preprint arXiv:1710.11041\n\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2014. Neural machine translation by jointly\nlearning to align and translate. arXiv e-prints\nabs/1409.0473. https://arxiv.org/abs/1409.0473.\n\nPiotr Bojanowski, Edouard Grave, Armand Joulin,\nand Tomas Mikolov. 2016. Enriching word vec-\ntors with subword information. arXiv preprint\narXiv:1607.04606 .\n\nYong Cheng, Wei Xu, Zhongjun He, Wei He, Hua\nWu, Maosong Sun, and Yang Liu. 2016. Semi-\nsupervised learning for neural machine translation.\narXiv preprint arXiv: 1606.04596 .\n\nAlexis Conneau, Guillaume Lample, Marc’ Aurelio\nRanzato, Ludovic Denoyer, and Hervé Jégou. 2017.\nWord translation without parallel data. arXiv\npreprint arXiv: 1710.04087 .\n\nGreville Corbett and Bernard Comrie. 2003. The\nSlavonic Languages. Routledge.\n\nMichael Denkowski and Graham Neubig. 2017.\nStronger baselines for trustable results in neural ma-\nchine translation. arXiv preprint arXiv: 1706.09733\n\nOrhan Firat, Kyunghyun Cho, and Yoshua Ben-\ngio. 2016. Multi-way, multilingual neural ma-\nchine translation with a shared attention mechanism.\narXiv preprint arXiv:1601.01073 .\n\nMattia Antonino Di Gangi and Marcello Federico.\n2017. Monolingual embeddings for low resourced\nneural machine translation. In International Work-\nshop on Spoken Language Translation (IWSLT).\n\nXavier Glorot and Yoshua Bengio. 2010. Understand-\ning the difficulty of training deep feedforward neu-\nral networks. In Proceedings of the Thirteenth In-\nternational Conference on Artificial Intelligence and\nStatistics. pages 249-256.\n\nDi He, Yingce Xia, Tao Qin, Liwei Wang, Nenghai Yu,\nTieyan Liu, and Wei-Ying Ma. 2016. Dual learn-\ning for machine translation. In Advances in Neural\nInformation Processing Systems. pages 820-828.\n\nMelvin Johnson et al. 2016. Google’s multilingual neu-\nral machine translation system: Enabling zero-shot\ntranslation. arXiv preprint arXiv: 1611.04558 .\n\nYoon Kim. 2014. Convolutional neural networks for\nsentence classification. In In EMNLP. Citeseer.\n\nDiederik Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980 .\n\n534\n\nGuillaume Lample, Miguel Ballesteros, Sandeep Sub-\nramanian, Kazuya Kawakami, and Chris Dyer. 2016.\nNeural architectures for named entity recognition.\nIn ALT-NAACL.\n\nXuezhe Ma and Eduard Hovy. 2016. End-to-end se-\nquence labeling via bi-directional Istm-cnns-crf. In\nProceedings of the 54th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers). Association for Computational Lin-\nguistics, Berlin, Germany, pages 1064-1074. http:\n//www.aclweb.org/anthology/P 16-1101.\n\nMitchell P Marcus, Mary Ann Marcinkiewicz, and\nBeatrice Santorini. 1993. Building a large annotated\ncorpus of english: The penn treebank. Computa-\ntional linguistics 19(2):313-330.\n\nP.H. Matthews. 1997. The Concise Oxford Dictio-\nnary of Linguistics.. Oxford Paperback Reference\n/ Oxford University Press, Oxford. Oxford Univer-\nsity Press, Incorporated. https://books.google.com/\nbooks?id=aYo YAAAAIAAJ.\n\nTomas Mikolov, Edouard Grave, Piotr Bojanowski,\nChristian Puhrsch, and Armand Joulin. 2017. Ad-\nvances in pre-training distributed word representa-\ntions .\n\nTomas Mikolovy, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013. Distributed representa-\ntions of words and phrases and their compositional-\nity. In Advances in neural information processing\nsystems. pages 3111-3119.\n\nMasato Neishi, Jin Sakuma, Satoshi Tohda, Shonosuke\nIshiwatari, Naoki Yoshinaga, and Masashi Toyoda.\n2017. A bag of useful tricks for practical neural\nmachine translation: Embedding layer initialization\nand large batch size. In Proceedings of the 4th Work-\nshop on Asian Translation (WAT2017). Asian Fed-\neration of Natural Language Processing, Taipei, Tai-\nwan, pages 99-109.\n\nGraham Neubig, Matthias Sperber, Xinyi Wang,\nMatthieu Felix, Austin Matthews, Sarguna Pad-\nmanabhan, Ye Qi, Devendra Singh Sachan, Philip\nArthur, Pierre Godard, John Hewitt, Rachid Riad,\nand Liming Wang. 2018. XNMT: The extensible\nneural machine translation toolkit. In Conference\nof the Association for Machine Translation in the\nAmericas (AMTA) Open Source Software Showcase.\nBoston.\n\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic eval-\nuation of machine translation. In Proceedings of\nthe 40th annual meeting on association for compu-\ntational linguistics. Association for Computational\nLinguistics, pages 311-318.\n\nPrajit Ramachandran, Peter J Liu, and Quoc V Le.\n2016. Unsupervised pretraining for sequence to se-\nquence learning. arXiv preprint arXiv: 1611.02683\n", "vlm_text": "References \nMikel Artetxe, Gorka Labaka, Eneko Agirre, and Kyunghyun Cho. 2017. Unsupervised neural ma- chine translation.  arXiv preprint arXiv:1710.11041 . Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben- gio. 2014. Neural machine translation by jointly learning to align and translate . arXiv e-prints abs/1409.0473.  https://arxiv.org/abs/1409.0473 . Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2016. Enriching word vec- tors with subword information. arXiv preprint arXiv:1607.04606  . Yong Cheng, Wei Xu, Zhongjun He, Wei He, Hua Wu, Maosong Sun, and Yang Liu. 2016. Semi- supervised learning for neural machine translation. arXiv preprint arXiv:1606.04596  . Alexis Conneau, Guillaume Lample, Marc’Aurelio Ranzato, Ludovic Denoyer, and Herv´ e J´ egou. 2017. Word translation without parallel data. arXiv preprint arXiv:1710.04087  . Greville Corbett and Bernard Comrie. 2003. The Slavonic Languages . Routledge. Michael Denkowski and Graham Neubig. 2017. Stronger baselines for trustable results in neural ma- chine translation.  arXiv preprint arXiv:1706.09733 . Orhan Firat, Kyunghyun Cho, and Yoshua Ben- gio. 2016. Multi-way, multilingual neural ma- chine translation with a shared attention mechanism. arXiv preprint arXiv:1601.01073  . Mattia Antonino Di Gangi and Marcello Federico. 2017. Monolingual embeddings for low resourced neural machine translation. In  International Work- shop on Spoken Language Translation (IWSLT) . Xavier Glorot and Yoshua Bengio. 2010. Understand- ing the difﬁculty of training deep feedforward neu- ral networks. In  Proceedings of the Thirteenth In- ternational Conference on Artiﬁcial Intelligence and Statistics . pages 249–256. Di He, Yingce Xia, Tao Qin, Liwei Wang, Nenghai Yu, Tieyan Liu, and Wei-Ying Ma. 2016. Dual learn- ing for machine translation. In  Advances in Neural Information Processing Systems . pages 820–828. Melvin Johnson et al. 2016. Google’s multilingual neu- ral machine translation system: Enabling zero-shot translation.  arXiv preprint arXiv:1611.04558  . Yoon Kim. 2014. Convolutional neural networks for sentence classiﬁcation. In  In EMNLP . Citeseer. Diederik Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization.  arXiv preprint arXiv:1412.6980 .\nGuillaume Lample, Miguel Ballesteros, Sandeep Sub- ramanian, Kazuya Kawakami, and Chris Dyer. 2016. Neural architectures for named entity recognition. In  HLT-NAACL . Xuezhe Ma and Eduard Hovy. 2016.  End-to-end se- quence labeling via bi-directional lstm-cnns-crf . In Proceedings of the 54th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1: Long Papers) . Association for Computational Lin- guistics, Berlin, Germany, pages 1064–1074.  http: //www.aclweb.org/anthology/P16-1101 . Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated corpus of english: The penn treebank. Computa- tional linguistics  19(2):313–330. P.H. Matthews. 1997. The Concise Oxford Dictio- nary of Linguistics. . Oxford Paperback Reference / Oxford University Press, Oxford. Oxford Univer- sity Press, Incorporated.  https://books.google.com/ books?id  $=\\!\\!\\mathrm{a}$  YoYAAAAIAAJ . Tomas Mikolov, Edouard Grave, Piotr Bojanowski, Christian Puhrsch, and Armand Joulin. 2017. Ad- vances in pre-training distributed word representa- tions . Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor- rado, and Jeff Dean. 2013. Distributed representa- tions of words and phrases and their compositional- ity. In  Advances in neural information processing systems . pages 3111–3119. Masato Neishi, Jin Sakuma, Satoshi Tohda, Shonosuke Ishiwatari, Naoki Yoshinaga, and Masashi Toyoda. 2017. A bag of useful tricks for practical neural machine translation: Embedding layer initialization and large batch size. In  Proceedings of the 4th Work- shop on Asian Translation (WAT2017) . Asian Fed- eration of Natural Language Processing, Taipei, Tai- wan, pages 99–109. Graham Neubig, Matthias Sperber, Xinyi Wang, Matthieu Felix, Austin Matthews, Sarguna Pad- manabhan, Ye Qi, Devendra Singh Sachan, Philip Arthur, Pierre Godard, John Hewitt, Rachid Riad, and Liming Wang. 2018. XNMT: The extensible neural machine translation toolkit. In  Conference of the Association for Machine Translation in the Americas (AMTA) Open Source Software Showcase . Boston. Kishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu. 2002. Bleu: a method for automatic eval- uation of machine translation. In  Proceedings of the 40th annual meeting on association for compu- tational linguistics . Association for Computational Linguistics, pages 311–318. Prajit Ramachandran, Peter J Liu, and Quoc V Le. 2016. Unsupervised pretraining for sequence to se- quence learning.  arXiv preprint arXiv:1611.02683 . "}
{"page": 6, "image_path": "doc_images/N18-2084_6.jpg", "ocr_text": "Samuel L Smith, David HP Turban, Steven Hamblin,\nand Nils Y Hammerla. 2017. Offline bilingual word\nvectors, orthogonal transformations and the inverted\nsoftmax. arXiv preprint arXiv:1702.03859 .\n\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V.\nLe, Mohammad Norouzi, Wolfgang Macherey,\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus\nMacherey, Jeff Klingner, Apurva Shah, Melvin\nJohnson, Xiaobing Liu, Lukasz Kaiser, Stephan\nGouws, Yoshikiyo Kato, Taku Kudo, Hideto\nKazawa, Keith Stevens, George Kurian, Nishant\nPatil, Wei Wang, Cliff Young, Jason Smith, Jason\nRiesa, Alex Rudnick, Oriol Vinyals, Gregory S.\nCorrado, Macduff Hughes, and Jeffrey Dean. 2016.\nGoogle’s neural machine translation system: Bridg-\ning the gap between human and machine translation.\nCoRR abs/1609.08 144.\n\n535\n", "vlm_text": "Samuel L Smith, David HP Turban, Steven Hamblin, and Nils Y Hammerla. 2017. Ofﬂine bilingual word vectors, orthogonal transformations and the inverted softmax.  arXiv preprint arXiv:1702.03859  . \nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, Lukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Gregory S. Corrado, Macduff Hughes, and Jeffrey Dean. 2016. Google’s neural machine translation system: Bridg- ing the gap between human and machine translation. CoRR  abs/1609.08144. "}
