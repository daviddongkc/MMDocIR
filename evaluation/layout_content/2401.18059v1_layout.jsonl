{"layout": 0, "type": "text", "text": "RAPTOR: R ECURSIVE  A BSTRACTIVE  P ROCESSING FOR  T REE -O RGANIZED  R ETRIEVAL ", "text_level": 1, "page_idx": 0, "bbox": [105, 78, 505, 117], "page_size": [612.0, 792.0]}
{"layout": 1, "type": "text", "text": "Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, Christopher D. Manning Stanford University psarthi@cs.stanford.edu ", "page_idx": 0, "bbox": [113, 135.0118865966797, 531.29150390625, 168.85411071777344], "page_size": [612.0, 792.0]}
{"layout": 2, "type": "text", "text": "A BSTRACT ", "text_level": 1, "page_idx": 0, "bbox": [277, 197, 334, 209], "page_size": [612.0, 792.0]}
{"layout": 3, "type": "text", "text": "Retrieval-augmented language models can better adapt to changes in world state and incorporate long-tail knowledge. However, most existing methods retrieve only short contiguous chunks from a retrieval corpus, limiting holistic under- standing of the overall document context. We introduce the novel approach of recursively embedding, clustering, and summarizing chunks of text, constructing a tree with differing levels of sum mari z ation from the bottom up. At inference time, our RAPTOR model retrieves from this tree, integrating information across lengthy documents at different levels of abstraction. Controlled experiments show that retrieval with recursive summaries offers significant improvements over tra- ditional retrieval-augmented LMs on several tasks. On question-answering tasks that involve complex, multi-step reasoning, we show state-of-the-art results; for example, by coupling RAPTOR retrieval with the use of GPT-4, we can improve the best performance on the QuALITY benchmark by  $20\\%$   in absolute accuracy. ", "page_idx": 0, "bbox": [143, 222.93853759765625, 468, 366.4503173828125], "page_size": [612.0, 792.0]}
{"layout": 4, "type": "text", "text": "1 I N TRO DUCT ION ", "text_level": 1, "page_idx": 0, "bbox": [108, 387, 207, 400], "page_size": [612.0, 792.0]}
{"layout": 5, "type": "text", "text": "Large Language Models (LLMs) have emerged as transformative tools showing impressive perfor- mance on many tasks. With the growing size of LLMs, they can serve standalone as very effective knowledge stores, with facts encoded within their parameters ( Petroni et al. ,  2019 ;  Jiang et al. ,  2020 ; Talmor et al. ,  2020 ;  Rae et al. ,  2021 ;  Hoffmann et al. ,  2022 ;  Chowdhery et al. ,  2022 ;  Bubeck et al. , 2023 ;  Kandpal et al. ,  2023 ) and models can be further improved with fine-tuning on downstream tasks ( Roberts et al. ,  2020 ). Nevertheless, even a large model does not contain sufficient domain- specific knowledge for particular tasks and the world continues to change, invalidating facts in the LLM. Updating the knowledge of these models through additional fine-tuning or editing is difficult, particularly when dealing with vast text corpora ( Lewis et al. ,  2020 ;  Mitchell et al. ,  2022 ). An alter- native approach, pioneered in open domain question answering systems ( Chen et al. ,  2017 ;  Yu et al. , 2018 ), is to index large quantities of text, after splitting it into chunks (paragraphs), in a separate information retrieval system. Retrieved information is then presented to the LLM along with the question as context (“retrieval augmentation”,  Lewis et al. ,  2020 ;  Izacard et al. ,  2022 ;  Min et al. , 2023 ;  Ram et al. ,  2023 ), making it easy to provide a system with current knowledge particular to some domain and enabling easy interpret ability and provenance tracking, whereas the parametric knowledge of LLMs is opaque and difficult to trace back to its source ( Akyurek et al. ,  2022 ). ", "page_idx": 0, "bbox": [107, 412.8103942871094, 504, 589.19921875], "page_size": [612.0, 792.0]}
{"layout": 6, "type": "text", "text": "Nevertheless, existing retrieval-augmented approaches also have flaws. The one we tackle is that most existing methods retrieve only a few short, contiguous text chunks, which limits their ability to represent and leverage large-scale discourse structure. This is particularly relevant for thematic questions that require integrating knowledge from multiple parts of a text, such as understanding an entire book, as in the Narrative QA dataset ( Koˇ cisk\\` y et al. ,  2018 ). Consider the fairy tale of Cinderella, and the question “How did Cinderella reach her happy ending?”. The top-  $\\cdot k$   retrieved short contiguous texts will not contain enough context to answer the question. ", "page_idx": 0, "bbox": [107, 594.1303100585938, 504, 671.8892211914062], "page_size": [612.0, 792.0]}
{"layout": 7, "type": "text", "text": "To address this, we design an indexing and retrieval system that uses a tree structure to capture both high-level and low-level details about a text. As shown in  Figure 1 , our system, RAPTOR, clusters chunks of text, generates text summaries of those clusters, and then repeats, generating a tree from the bottom up. This structure enables RAPTOR to load into an LLM’s context chunks representing the text at different levels so that it can effectively and efficiently answer questions at different levels. ", "page_idx": 0, "bbox": [107, 676.8203125, 504, 732.6612548828125], "page_size": [612.0, 792.0]}
{"layout": 8, "type": "image", "page_idx": 1, "img_path": "layout_images/2401.18059v1_0.jpg", "img_caption": "Figure 1:  Tree construction process:  RAPTOR recursively clusters chunks of text based on their vector embeddings and generates text summaries of those clusters, constructing a tree from the bottom up. Nodes clustered together are siblings; a parent node contains the text summary of that cluster. ", "bbox": [106, 82, 506, 238], "page_size": [612.0, 792.0], "ocr_text": "RAPTOR Tree Formation of one tree layer Contents of a node\nIndex#8 |\nChild Nodes: 2,3 |\n\n7\nRoot layer 9 2. Summarization “f*\nam by LLM\nLo os Text: summary of\n\nTysis nodes 2 and 3\n\n—s\nClustering SS a\n3 4 5\n\nText Embedding\n\n/\n\nLeaf layer\n\nrs\n\nText chunks\n", "vlm_text": "The image illustrates the tree construction process used by RAPTOR to cluster and summarize text. Here's a breakdown of its components:\n\n1. **RAPTOR Tree (Left Panel):**\n   - **Root Layer:** Contains nodes (9 and 10).\n   - **Leaf Layer:** Contains nodes (1 to 5).\n   - Clustering of nodes forms a hierarchical structure from the leaf layer up to the root.\n\n2. **Formation of One Tree Layer (Middle Panel):**\n   - **Step 1: Clustering** - Text chunks (1 through 5) are clustered based on their embeddings.\n   - **Step 2: Summarization by LLM** - Clusters (such as those formed by nodes 2 and 3) are summarized to create another layer in the tree (nodes 6, 7, 8).\n\n3. **Contents of a Node (Right Panel):**\n   - **Index:** Identifies the node (e.g., #8).\n   - **Child Nodes:** Lists nodes contained within a cluster (e.g., 2, 3).\n   - **Text Summary:** Provides a summary of the cluster's contents.\n   - **Text Embedding:** Displays the vector representation used for clustering.\n\nThe image demonstrates how RAPTOR constructs a hierarchical tree by clustering and summarizing text data, working from the bottom (leaf) to the top (root) layers."}
{"layout": 9, "type": "text", "text": "Our main contribution is the idea of using text sum mari z ation to allow retrieval augmentation of context at different scales, and to show its effectiveness in experiments on collections of long doc- uments. Controlled experiments with three language models (UnifiedQA ( Khashabi et al. ,  2020 ), GPT-3 ( Brown et al. ,  2020 ) and GPT-4 ( OpenAI ,  2023 )) show that RAPTOR outperforms current retrieval augmentation. Moreover, RAPTOR coupled with GPT-4, and sometimes even with Uni- fiedQA, gives new state-of-the-art results on three QA tasks: free text response questions on books and movies (Narrative QA,  Koˇ cisk\\` y et al. 2018 ), full-text NLP papers (QASPER,  Dasigi et al. 2021 ), and multiple-choice questions based on medium-length passages (QuALITY,  Pang et al. 2022 ). ", "page_idx": 1, "bbox": [107, 263.4835205078125, 504, 352.20135498046875], "page_size": [612.0, 792.0]}
{"layout": 10, "type": "text", "text": "2 R ELATED  W ORK ", "text_level": 1, "page_idx": 1, "bbox": [107, 371, 213, 384], "page_size": [612.0, 792.0]}
{"layout": 11, "type": "text", "text": "Why Retrieval? Recent advances in hardware and algorithms have indeed expanded the con- text lengths that models can handle, leading to questions about the need for retrieval systems ( Dai et al. ,  2019 ;  Dao et al. ,  2022 ;  Liu et al. ,  2023 ). However, as  Liu et al.  ( 2023 ) and  Sun et al.  ( 2021 ) have noted, models tend to under utilize long-range context and see diminishing performance as con- text length increases, especially when pertinent information is embedded within a lengthy context. Moreover, practically, use of long contexts is expensive and slow. This suggests that selecting the most relevant information for knowledge-intensive tasks is still crucial. ", "page_idx": 1, "bbox": [107, 398.3277587890625, 504, 476.44427490234375], "page_size": [612.0, 792.0]}
{"layout": 12, "type": "text", "text": "Retrieval Methods Retrieval-augmented language models (RALMs) have seen improvements in various components: the retriever, the reader, and end-to-end system training. Retrieval methods have transitioned from traditional term-based techniques like  TF-IDF  ( Sp¨ arck Jones ,  1972 ) and BM25  ( Robertson et al. ,  1995 ;  Roberts et al. ,  2020 ) to deep learning–based strategies ( Karpukhin et al. ,  2020 ;  Khattab & Zaharia ,  2020 ;  Sachan et al. ,  2023 ). Some recent work proposes using large language models as retrievers due to their ability to memorize extensive knowledge ( Yu et al. , 2022 ;  Sun et al. ,  2022 ). Research on the reader component includes  Fusion-in-Decoder (FiD) ( Izacard & Grave ,  2022 ), which employs both DPR and BM25 for retrieval and processes passages independently in the encoder and  RETRO  ( Borgeaud et al. ,  2022 ;  Wang et al. ,  2023 ), which utilizes cross-chunked attention and chunkwise retrieval to generate text grounded on retrieved context. ", "page_idx": 1, "bbox": [107, 491.45367431640625, 504, 602.4472045898438], "page_size": [612.0, 792.0]}
{"layout": 13, "type": "text", "text": "End-to-end system training work includes  Atlas  ( Izacard et al. ,  2022 ), which fine-tunes an encoder- decoder model in conjunction with the retriever;  REALM  ( Guu et al. ,  2020 ), a bidirectional, masked LM fine-tuned for open-domain question answering; and  RAG (Retrieval-Augmented Genera- tion)  ( Lewis et al. ,  2020 ), which integrates pre-trained sequence-to-sequence models with a neural retriever.  Min et al.  ( 2021 ) introduced  Joint Passage Retrieval (JPR)  model which uses a tree- decoding algorithm to handle passage diversity and relevance in multi-answer retrieval.  Dense Hi- erarchical Retrieval (DHR)  and  Hybrid Hierarchical Retrieval (HHR)  represent advancements in retrieval accuracy by combining document and passage level retrievals and integrating sparse and dense retrieval methods, respectively ( Liu et al. ,  2021 ;  Ari vaz hagan et al. ,  2023 ). ", "page_idx": 1, "bbox": [107, 607.0196533203125, 504, 707.0552368164062], "page_size": [612.0, 792.0]}
{"layout": 14, "type": "text", "text": "Despite a diversity in methods, the retrieving components of models predominantly rely on stan- dard approaches, i.e., chunking corpora and encoding with BERT-based retrievers. Although this approach is widely adopted,  Nair et al.  ( 2023 ) highlights a potential shortcoming: contiguous seg- mentation might not capture the complete semantic depth of the text. Reading extracted snippets from technical or scientific documents may lack important context making them difficult to read or even misleading. ( Cohan & Goharian ,  2017 ;  Newman et al. ,  2023 ;  Zhang et al. ,  2023 ). ", "page_idx": 2, "bbox": [108, 82.6185302734375, 504, 149.4183807373047], "page_size": [612.0, 792.0]}
{"layout": 15, "type": "text", "text": "Recursive sum mari z ation as Context Sum mari z ation techniques provide a condensed view of documents, enabling more focused engagement with the content ( Angelidis & Lapata ,  2018 ). The sum mari z ation/snippet model by  Gao et al.  ( 2023 ) uses sum mari zat ions and snippets of passages, which improves correctness on most datasets but can sometimes be a lossy means of compression. The recursive-abstract ive sum mari z ation model by  Wu et al.  ( 2021 ) employs task decomposition to summarize smaller text chunks, which are later integrated to form summaries of larger sections. While this method is effective for capturing broader themes, it can miss granular details. LlamaIndex ( Liu ,  2022 ) mitigates this issue by similarly summarizing adjacent text chunks but also retaining intermediate nodes thus storing varying levels of detail, keeping granular details. However, both methods, due to their reliance on adjacency for grouping or summarizing adjacent nodes, may still overlook distant interdependencies within the text, which we can find and group with RAPTOR. ", "page_idx": 2, "bbox": [108, 161.87278747558594, 504, 283.82525634765625], "page_size": [612.0, 792.0]}
{"layout": 16, "type": "text", "text": "3 M ETHODS ", "text_level": 1, "page_idx": 2, "bbox": [108, 301, 180, 313], "page_size": [612.0, 792.0]}
{"layout": 17, "type": "text", "text": "Overview of RAPTOR Building on the idea that long texts often present subtopics and hierarchi- cal structures ( Cao & Wang ,  2022 ;  Dong et al. ,  2023b ), RAPTOR addresses the issue of semantic depth and connection in reading by building a recursive tree structure that balances broader thematic comprehension with granular details and which allows nodes to be grouped based on semantic sim- ilarity not just order in the text. ", "page_idx": 2, "bbox": [108, 325.86566162109375, 504, 382.064208984375], "page_size": [612.0, 792.0]}
{"layout": 18, "type": "text", "text": "Construction of the RAPTOR tree begins with segmenting the retrieval corpus into short, contiguous texts of length 100, similar to traditional retrieval augmentation techniques. If a sentence exceeds the 100-token limit, we move the entire sentence to the next chunk, rather than cutting it mid-sentence. This preserves the contextual and semantic coherence of the text within each chunk. These texts are then embedded using SBERT, a BERT-based encoder ( multi-qa-mpnet-base-cos-v1 ) ( Reimers & Gurevych ,  2019 ). The chunks and their corresponding SBERT embeddings form the leaf nodes of our tree structure. ", "page_idx": 2, "bbox": [108, 386.99627685546875, 504, 464.754150390625], "page_size": [612.0, 792.0]}
{"layout": 19, "type": "text", "text": "To group similar text chunks, we employ a clustering algorithm. Once clustered, a Language Model is used to summarize the grouped texts. These summarized texts are then re-embedded, and the cycle of embedding, clustering, and sum mari z ation continues until further clustering becomes infeasible, resulting in a structured, multi-layered tree representation of the original documents. An important aspect of RAPTOR is its computational efficiency. The system scales linearly in terms of both build time and token expenditure, making it suitable for processing large and complex corpora. For a comprehensive discussion on RAPTOR’s s cal ability, please refer to the Appendix  A . ", "page_idx": 2, "bbox": [108, 469.68621826171875, 504, 547.444091796875], "page_size": [612.0, 792.0]}
{"layout": 20, "type": "text", "text": "For querying within this tree, we introduce two distinct strategies: tree traversal and collapsed tree. The tree traversal method traverses the tree layer-by-layer, pruning and selecting the most relevant nodes at each level. The collapsed tree method evaluates nodes collectively across all layers to find the most relevant ones. ", "page_idx": 2, "bbox": [108, 552.3761596679688, 504, 597.257080078125], "page_size": [612.0, 792.0]}
{"layout": 21, "type": "text", "text": "Clustering Algorithm Clustering plays a key role in building the RAPTOR tree, organizing text segments into cohesive groups. This step groups related content together, which helps the subse- quent retrieval process. ", "page_idx": 2, "bbox": [108, 609.7115478515625, 504, 643.9931030273438], "page_size": [612.0, 792.0]}
{"layout": 22, "type": "text", "text": "One of the unique aspects of our clustering approach is the use of soft clustering, where nodes can belong to multiple clusters without requiring a fixed number of clusters. This flexibility is essen- tial because individual text segments often contain information relevant to various topics, thereby warranting their inclusion in multiple summaries. ", "page_idx": 2, "bbox": [108, 648.9251708984375, 504, 693.8060913085938], "page_size": [612.0, 792.0]}
{"layout": 23, "type": "text", "text": "Our clustering algorithm is based on Gaussian Mixture Models (GMMs), an approach that offers both flexibility and a probabilistic framework. GMMs assume that data points are generated from a mixture of several Gaussian distributions. ", "page_idx": 2, "bbox": [108, 698.7381591796875, 504, 732.6610717773438], "page_size": [612.0, 792.0]}
{"layout": 24, "type": "text", "text": "Given a set of  $N$   text segments, each represented as a  $d$  -dimensional dense vector embedding, the likelihood of a text vector,    $\\mathbf{x}$  , given its membership in the    $k^{t h}$    Gaussian distribution, is denoted by  $P(\\mathbf{x}|k)=\\mathcal{N}(\\mathbf{x};\\mu_{k},\\Sigma_{k})$  . The overall probability distribution is a weighted combination    $P(\\mathbf{x})\\stackrel{.}{=}$   $\\begin{array}{r}{\\sum_{k=1}^{K}\\pi_{k}\\mathcal{N}(\\mathbf{x};\\mu_{k},\\pmb{\\Sigma}_{k})}\\end{array}$  , where  $\\pi_{k}$   signifies the mixture weight for the    $k^{\\mathrm{{th}}}$   Gaussian distribution. ", "page_idx": 3, "bbox": [106, 82.6185302734375, 505, 131.2974090576172], "page_size": [612.0, 792.0]}
{"layout": 25, "type": "text", "text": "The high dimensionality of vector embeddings presents a challenge for traditional GMMs, as dis- tance metrics may behave poorly when used to measure similarity in high-dimensional spaces ( Ag- garwal et al. ,  2001 ). To mitigate this, we employ Uniform Manifold Approximation and Projection (UMAP), a manifold learning technique for dimensionality reduction ( McInnes et al. ,  2018 ). The number of nearest neighbors parameter,  n neighbors , in UMAP determines the balance between the preservation of local and global structures. Our algorithm varies  n neighbors  to create a hierar- chical clustering structure: it first identifies global clusters and then performs local clustering within these global clusters. This two-step clustering process captures a broad spectrum of relationships among the text data, from broad themes to specific details. ", "page_idx": 3, "bbox": [106, 134.73443603515625, 505, 234.4104461669922], "page_size": [612.0, 792.0]}
{"layout": 26, "type": "text", "text": "Should a local cluster’s combined context ever exceed the sum mari z ation model’s token threshold, our algorithm recursively applies clustering within the cluster, ensuring that the context remains within the token threshold. ", "page_idx": 3, "bbox": [106, 239.342529296875, 505, 273.26544189453125], "page_size": [612.0, 792.0]}
{"layout": 27, "type": "text", "text": "To determine the optimal number of clusters, we employ the Bayesian Information Criterion (BIC) for model selection. BIC not only penalizes model complexity but also rewards goodness of fit ( Schwarz ,  1978 ). The BIC for a given GMM is    $B I C=\\ln(N)k-2\\ln(\\hat{L})$  , where    $N$   is the number of text segments (or data points),    $k$   is the number of model parameters, and  $\\hat{L}$   is the maximized value of the likelihood function of the model. In the context of GMM, the number of parameters  $k$  is a function of the dimensionality of the input vectors and the number of clusters. ", "page_idx": 3, "bbox": [106, 278.1965026855469, 505, 348.59246826171875], "page_size": [612.0, 792.0]}
{"layout": 28, "type": "text", "text": "With the optimal number of clusters determined by BIC, the Expectation-Maximization algorithm is then used to estimate the GMM parameters, namely the means, co variances, and mixture weights. ", "page_idx": 3, "bbox": [106, 353.5235290527344, 505, 376.48748779296875], "page_size": [612.0, 792.0]}
{"layout": 29, "type": "text", "text": "While the Gaussian assumption in GMMs may not perfectly align with the nature of text data, which often exhibits a sparse and skewed distribution, our empirical observations suggest that it offers an effective model for our purpose. We run an ablation comparing GMM Clustering with summarizing contiguous chunks and provide details in Appendix  B . ", "page_idx": 3, "bbox": [106, 381.4185485839844, 505, 426.30047607421875], "page_size": [612.0, 792.0]}
{"layout": 30, "type": "text", "text": "Model-Based Sum mari z ation After clustering the nodes using Gaussian Mixture Models, the nodes in each cluster are sent to a language model for sum mari z ation. This step allows the model to transform large chunks of text into concise, coherent summaries of the selected nodes. For our experiments, we use    $\\mathtt{g p t\\!-\\!3.5\\!-\\!t u r b o}$   to generate the summaries. The sum mari z ation step con- denses the potentially large volume of retrieved information into a manageable size. We provide statistics on the compression due to the sum mari z ation in Appendix  C  and the prompt used for sum mari z ation in Appendix  D . ", "page_idx": 3, "bbox": [106, 438.160888671875, 505, 516.2783813476562], "page_size": [612.0, 792.0]}
{"layout": 31, "type": "text", "text": "While the sum mari z ation model generally produces reliable summaries, a focused annotation study revealed that about   $4\\%$   of the summaries contained minor hallucinations. These did not propagate to parent nodes and had no discernible impact on question-answering tasks. For an in-depth analysis of hallucinations, refer to the appendix  E . ", "page_idx": 3, "bbox": [106, 521.20947265625, 505, 566.0913696289062], "page_size": [612.0, 792.0]}
{"layout": 32, "type": "text", "text": "Querying In this section, we elaborate on the two querying mechanisms employed by RAPTOR: tree traversal and collapsed tree. These methods offer unique ways of traversing the multi-layered RAPTOR tree to retrieve relevant information, each with its own advantages and trade-offs. We provide the pseudocode of both methods in Appendix  F . Note that we embed all nodes using SBERT. ", "page_idx": 3, "bbox": [106, 577.9517822265625, 505, 623.1923828125], "page_size": [612.0, 792.0]}
{"layout": 33, "type": "text", "text": "The  tree traversal  method first selects the top-k most relevant root nodes based on their cosine similarity to the query embedding. The children of these selected nodes are considered at the next layer and the top  $\\cdot\\mathrm{k}$   nodes are selected from this pool again based on their cosine similarity to the query vector. This process is repeated until we reach the leaf nodes. Finally, the text from all selected nodes is concatenated to form the retrieved context. The algorithm’s steps are outlined below: ", "page_idx": 3, "bbox": [106, 627.7647705078125, 505, 683.9644165039062], "page_size": [612.0, 792.0]}
{"layout": 34, "type": "text", "text": "1. Start at the root layer of the RAPTOR tree. Compute the cosine similarity between the query embedding and the embeddings of all nodes present at this initial layer. 2. Choose the top-  $\\cdot k$   nodes based on the highest cosine similarity scores, forming the set  $S_{1}$  . ", "page_idx": 3, "bbox": [131, 694.25244140625, 505, 732.661376953125], "page_size": [612.0, 792.0]}
{"layout": 35, "type": "image", "page_idx": 4, "img_path": "layout_images/2401.18059v1_1.jpg", "img_caption": "Figure 2:  Illustration of the tree traversal and collapsed tree retrieval mechanisms.  Tree traver- sal starts at the root level of the tree and retrieves the top-  $\\cdot k$   (here, top- 1 ) node(s) based on cosine similarity to the query vector. At each level, it retrieves the top-  $\\cdot k$   node(s) from the child nodes of the previous layer’s top-  $k$  . Collapsed tree collapses the tree into a single layer and retrieves nodes until a threshold number of tokens is reached, based on cosine similarity to the query vector. The nodes on which cosine similarity search is performed are highlighted in both illustrations. ", "bbox": [106, 95, 506, 317], "page_size": [612.0, 792.0], "ocr_text": "A. Tree Traversal Retrieval\n\n—\n—\nee) —> o> —S | Ht ucery —— Answer\nEncoder =_eo LLM\nCJ Retrieved Context\n—\n\nTree Structure\n\nB, Collapsed Tree Retrieval\n\n== + OOQ000000000~_o. + = =\nt Encoder -_-—_-_— LLM\n\nCollapsed Tree Structure Retrieved Context\n", "vlm_text": "The image illustrates two mechanisms for retrieving information using tree structures: Tree Traversal Retrieval and Collapsed Tree Retrieval.\n\n1. **Tree Traversal Retrieval (A)**:\n   - Starts with a query that is encoded into a vector.\n   - The tree structure is traversed starting from the root.\n   - At each level, it retrieves the top nodes based on cosine similarity to the query vector.\n   - Retrieves context from selected nodes which is then combined with the query to generate an answer using a language model (LLM).\n\n2. **Collapsed Tree Retrieval (B)**:\n   - Also starts with a query that is encoded.\n   - The tree is collapsed into a single layer.\n   - Nodes are retrieved based on cosine similarity until reaching a threshold number of tokens.\n   - The retrieved context is combined with the query to produce an answer using an LLM.\n\nNodes on which cosine similarity is performed are highlighted in both mechanisms."}
{"layout": 36, "type": "text", "text": "3. Proceed to the child nodes of the elements in set  $S_{1}$  . Compute the cosine similarity between the query vector and the vector embeddings of these child nodes. 4. Select the top  $k$   child nodes with the highest cosine similarity scores to the query, forming the set    $S_{2}$  . 5. Continue this process recursively for  $d$   layers, producing sets  $S_{1},S_{2},.\\,.\\,.\\,,S_{d}$  . 6. Concatenate sets    $S_{1}$   through  $S_{d}$   to assemble the relevant context to the query. ", "page_idx": 4, "bbox": [131, 339.5434875488281, 504, 426.1994323730469], "page_size": [612.0, 792.0]}
{"layout": 37, "type": "text", "text": "By adjusting the depth    $d$   and the number of nodes  $k$   selected at each layer, the tree traversal method offers control over the specificity and breadth of the information retrieved. The algorithm starts with a broad outlook by considering the top layers of the tree and progressively focuses on finer details as it descends through the lower layers. ", "page_idx": 4, "bbox": [107, 436.2204895019531, 504, 481.1014404296875], "page_size": [612.0, 792.0]}
{"layout": 38, "type": "text", "text": "The  collapsed tree  approach offers a simpler way to search for relevant information by considering all nodes in the tree simultaneously, as depicted in Figure  2 . Instead of going layer-by-layer, this method flattens the multi-layered tree into a single layer, essentially bringing all the nodes onto the same level for comparison. The steps for this method are outlined below: ", "page_idx": 4, "bbox": [107, 485.67486572265625, 504, 530.9144287109375], "page_size": [612.0, 792.0]}
{"layout": 39, "type": "text", "text": "1. First, collapse the entire RAPTOR tree into a single layer. This new set of nodes, denoted as    $C$  , contains nodes from every layer of the original tree. 2. Next, calculate the cosine similarity between the query embedding and the embeddings of all nodes present in the collapsed set    $C$  . 3. Finally, pick the top-  $\\cdot k$   nodes that have the highest cosine similarity scores with the query. Keep adding nodes to the result set until you reach a predefined maximum number of tokens, ensuring you don’t exceed the model’s input limitations. ", "page_idx": 4, "bbox": [131, 542.4295043945312, 504, 632.429443359375], "page_size": [612.0, 792.0]}
{"layout": 40, "type": "text", "text": "We tested both approaches on 20 stories from the QASPER dataset. Figure  3  shows the performance of tree traversal with different top- sizes and collapsed tree with different maximum token numbers. The collapsed tree approach consistently performs better. We believe collapsed tree retrieval is better due to offering greater flexibility than tree traversal; i.e., by searching through all the nodes simultaneously, it retrieves information that is at the correct level of granularity for a given question. In comparison, while using tree traversal with the same values of    $d$   and  $k$  , the ratio of nodes from each level of the tree will be constant. So, the ratio of higher-order thematic information to granular details will remain the same regardless of the question. ", "page_idx": 4, "bbox": [107, 643.9434814453125, 504, 732.6614379882812], "page_size": [612.0, 792.0]}
{"layout": 41, "type": "text", "text": "One drawback, however, of the collapsed tree approach is that it requires cosine similarity search to be performed on all nodes in the tree. However, this can be made more efficient with fast    $k$  -nearest neighbor libraries such as FAISS ( Johnson et al. ,  2019 ). ", "page_idx": 5, "bbox": [108, 82.6185302734375, 504, 116.54143524169922], "page_size": [612.0, 792.0]}
{"layout": 42, "type": "image", "page_idx": 5, "img_path": "layout_images/2401.18059v1_2.jpg", "img_caption": "Figure 3:  Comparison of querying methods. Results on 20 stories from the QASPER dataset using tree traversal with different top-k values, and collapsed tree with different context lengths. Collapsed tree with 2000 tokens produces the best results, so we use this querying strategy for our main results. ", "bbox": [106, 124, 307, 361], "page_size": [612.0, 792.0], "ocr_text": "FL\n\n— Collapsed tree\n\n‘Wee Traversal\n\nT T\n1000 1500\n\nContext Length\n\n", "vlm_text": "The image is a graph comparing different querying methods based on the F1 score. The x-axis represents the Context Length, ranging from 0 to 2500. The y-axis shows the F1 score, ranging from 40 to 65.\n\nThere are two lines representing different methods:\n- **Collapsed tree** (green line): Shows results over various context lengths. It peaks at 2000 tokens with the best F1 score, slightly decreasing as the context length increases to 2500.\n- **Tree Traversal** (blue line): Shows a steady increase in F1 score with context length, but overall performs lower than the collapsed tree method.\n\nThe caption mentions using the collapsed tree with 2000 tokens for the best results in the QASPER dataset."}
{"layout": 43, "type": "text", "text": "Overall, given the collapsed tree approach’s greater flexibility and its superior performance on the subset of the QASPER dataset, this is the querying approach with which we proceed. Specifically, we use the collapsed tree with 2000 maximum tokens, which approximately equates to retrieving the top-20 nodes. Using a token-based approach ensures the context does not exceed model context constraints as token counts can vary across nodes. For experiments with the UnifiedQA model, we provide 400 to-kens of context, as UnifiedQA has a max con- text length of 512 tokens. We provide the same amount of tokens of context to RAPTOR and to the baselines. ", "page_idx": 5, "bbox": [315, 121.47344970703125, 503, 286.9022216796875], "page_size": [612.0, 792.0]}
{"layout": 44, "type": "text", "text": "Qualitative Study We conduct a qualitative analysis to understand the benefits of RAP- TOR’s retrieval process compared to Dense Passage Retrieval (DPR) methods. Our study focuses on thematic, multi-hop questions using a 1500-word Cinderella fairytale. As illustrated in Figure  4 , RAPTOR’s tree-based retrieval allows it to choose nodes from different tree layers, matching the question’s detail level. This approach often yields more relevant and comprehensive information for downstream tasks than DPR. For a detailed discussion and examples, including the text retrieved by both RAPTOR and DPR for specific questions, please refer to the appendix  G . ", "page_idx": 5, "bbox": [315, 297.9956359863281, 503, 365.1541748046875], "page_size": [612.0, 792.0]}
{"layout": 45, "type": "text", "text": "", "page_idx": 5, "bbox": [108, 364.10821533203125, 504, 408.9891357421875], "page_size": [612.0, 792.0]}
{"layout": 46, "type": "text", "text": "4 E XPERIMENTS ", "text_level": 1, "page_idx": 5, "bbox": [107, 425, 201, 437], "page_size": [612.0, 792.0]}
{"layout": 47, "type": "text", "text": "Datasets We measure RAPTOR’s performance across three question-answering datasets: Narra- tiveQA, QASPER, and QuALITY. ", "page_idx": 5, "bbox": [108, 448.7705383300781, 504, 472.0931396484375], "page_size": [612.0, 792.0]}
{"layout": 48, "type": "text", "text": "Narrative QA is a dataset that comprises question-answer pairs based on the full texts of books and movie transcripts, totaling 1,572 documents ( Koˇ cisk\\` y et al. ,  2018 ;  Wu et al. ,  2021 ). The Narrative QA-Story task requires a comprehensive understanding of the entire narrative in order to accurately answer its questions, thus testing the model’s ability to comprehend longer texts in the literary domain. We measure performance on this dataset using the standard BLEU (B-1, B-4), ROUGE (R-L), and METEOR (M) metrics. Please see appendix  H  for more details on the Narra- tiveQA evaluation script used in our experiments. ", "page_idx": 5, "bbox": [108, 477.0242004394531, 504, 554.7831420898438], "page_size": [612.0, 792.0]}
{"layout": 49, "type": "text", "text": "The QASPER dataset includes 5,049 questions across 1,585 NLP papers, with each question probing for information embedded within the full text ( Dasigi et al. ,  2021 ). The answer types in QASPER are categorized as Answerable/Unanswerable, Yes/No, Abstract ive, and Extractive. Accuracy is measured using standard F1. ", "page_idx": 5, "bbox": [108, 559.7141723632812, 504, 604.5961303710938], "page_size": [612.0, 792.0]}
{"layout": 50, "type": "text", "text": "Lastly, the QuALITY dataset consists of multiple-choice questions, each accompanied by context passages averaging approximately 5,000 tokens in length ( Pang et al. ,  2022 ). This dataset calls for reasoning over the entire document for QA tasks, enabling us to measure the performance of our re- trieval system on medium-length documents. The dataset includes a challenging subset, QuALITY- HARD, which contains questions that a majority of human annotators answered incorrectly in a speed-setting. We report accuracies for both the entire test set and the HARD subset. ", "page_idx": 5, "bbox": [108, 609.5271606445312, 504, 676.3270874023438], "page_size": [612.0, 792.0]}
{"layout": 51, "type": "text", "text": "Controlled Baseline Comparisons We first present controlled comparisons using the UnifiedQA 3B as the reader, with SBERT ( Reimers & Gurevych ,  2019 ), BM25 ( Robertson et al. ,  1995 ;  2009 ), and DPR ( Karpukhin et al. ,  2020 ) as the embedding models with and without the RAPTOR tree structure, on three datasets: QASPER, Narrative QA, and QuALITY. As shown in Tables  1  and  2 , ", "page_idx": 5, "bbox": [108, 687.4205322265625, 504, 732.6611328125], "page_size": [612.0, 792.0]}
{"layout": 52, "type": "image", "page_idx": 6, "img_path": "layout_images/2401.18059v1_3.jpg", "img_caption": "Figure 4:  Querying Process:  Illustration of how RAPTOR retrieves information for two questions about the Cinderella story: “What is the central theme of the story?” and “How did Cinderella find a happy ending?”. Highlighted nodes indicate RAPTOR’s selections, while arrows point to DPR’s leaf nodes. Notably, RAPTOR’s context often encompasses the information retrieved by DPR, either directly or within higher-layer summaries. ", "bbox": [107, 79, 505, 298], "page_size": [612.0, 792.0], "ocr_text": "EN eis\nGaeOe se\n\npee see eee\n\nC) RAPTOR retrieved for Question 1 ==> DPR retrieved for Question 1\n\ni) RAPTOR retrieved for Question 2 mam DPR retrieved for Question 2\n", "vlm_text": "The image is an illustration of the querying process by RAPTOR, a system for retrieving information. It shows how RAPTOR retrieves information for two questions about the Cinderella story. The diagram features nodes and arrows with different colors representing selections by RAPTOR and DPR (Dense Passage Retrieval). The nodes are arranged in a hierarchical structure with numbers, and the highlighted nodes indicate RAPTOR's selections, differentiated for two distinct questions: \"What is the central theme of the story?\" and \"How did Cinderella find a happy ending?\".\n\nKey components:\n\n- Orange and purple highlighted nodes: Indicate RAPTOR's selections for Question 1 and Question 2.\n- Arrows: Point to DPR’s leaf nodes for each question, with orange arrows for Question 1 and purple arrows for Question 2.\n- RAPTOR's context is shown to often encompass the information retrieved by DPR.\n\nThis diagram visualizes the differing layers of retrieval and how RAPTOR's higher-layer summaries sometimes include DPR's specific retrievals directly or through summarization."}
{"layout": 53, "type": "text", "text": "our results demonstrate that RAPTOR, when combined with any retriever, consistently outperforms the respective retriever across all datasets.   2 ", "page_idx": 6, "bbox": [107, 318.8014831542969, 504, 341.7644348144531], "page_size": [612.0, 792.0]}
{"layout": 54, "type": "text", "text": "Since RAPTOR with SBERT has the best performance, we use it in all subsequent experiments. We now compare RAPTOR with BM25 and DPR, using three different LLMs: GPT-3, GPT-4, and UnifiedQA. As shown in Table  3 , RAPTOR consistently outperforms BM25 and DPR across all three Language Models on the QASPER dataset. RAPTOR’s F-1 Match scores are  $53.1\\%$  ,  $55.7\\%$  , and  $36.6\\%$   when using GPT-3, GPT-4, and UnifiedQA, respectively. These scores surpass DPR by margins of 1.8, 2.7, and 4.5 points, and outdo BM25 by 6.5, 5.5, and 10.2 points across the respective LLMs. QASPER requires synthesizing information within NLP papers, so it is unsurprising that RAPTOR’s higher-level summary nodes would allow it to outperform methods that can only extract the top-  $k$   most similar raw chunks of text, which may not contain the correct response in isolation. ", "page_idx": 6, "bbox": [107, 346.6965026855469, 504, 446.37237548828125], "page_size": [612.0, 792.0]}
{"layout": 55, "type": "table", "page_idx": 6, "img_path": "layout_images/2401.18059v1_4.jpg", "table_caption": "Table 1:  Narrative QA Performance With  $^+$   Without RAPTOR:  Performance comparison of various retrieval methods (SBERT, BM25, DPR) with and without RAPTOR on the Narrative QA dataset, using UnifiedQA-3B as the language model. RAPTOR outperforms baselines of each re- spective retrieval method. ", "bbox": [107, 456, 505, 592], "page_size": [612.0, 792.0], "ocr_text": "Model ROUGE BLEU-1 BLEU-4 METEOR\nSBERT with RAPTOR 30.87% 23.50% 6.42% 19.20%\nSBERT without RAPTOR — 29.26% 22.56% 5.95% 18.15%\nBM25 with RAPTOR 27.93% 21.17% 5.70% 17.03%\nBM25 without RAPTOR 23.52% 17.73% 4.65% 13.98%\nDPR with RAPTOR 30.94% 23.51% 6.45% 19.05%\nDPR without RAPTOR 29.56% 22.84% 6.12% 18.44%\n\n", "vlm_text": "The table presents the evaluation results of different models using various metrics: ROUGE, BLEU-1, BLEU-4, and METEOR. The models listed in the table include different retrieval and scoring configurations, specifically SBERT, BM25, and DPR, each tested with and without the RAPTOR augmentation. The percentage values under each metric column indicate the performance of the models in natural language processing tasks, with higher percentages reflecting better performance.\n\nHere's a summary of the values:\n- **SBERT with RAPTOR** has ROUGE: 30.87%, BLEU-1: 23.50%, BLEU-4: 6.42%, METEOR: 19.20%.\n- **SBERT without RAPTOR** has ROUGE: 29.26%, BLEU-1: 22.56%, BLEU-4: 5.95%, METEOR: 18.15%.\n- **BM25 with RAPTOR** has ROUGE: 27.93%, BLEU-1: 21.17%, BLEU-4: 5.70%, METEOR: 17.03%.\n- **BM25 without RAPTOR** has ROUGE: 23.52%, BLEU-1: 17.73%, BLEU-4: 4.65%, METEOR: 13.98%.\n- **DPR with RAPTOR** has ROUGE: 30.94%, BLEU-1: 23.51%, BLEU-4: 6.45%, METEOR: 19.05%.\n- **DPR without RAPTOR** has ROUGE: 29.56%, BLEU-1: 22.84%, BLEU-4: 6.12%, METEOR: 18.44%.\n\nThe table suggests that models enhanced with RAPTOR generally yield better performance across the metrics compared to those without it."}
{"layout": 56, "type": "text", "text": "Likewise, in the QuALITY dataset as shown in Table  4 , RAPTOR achieves an accuracy of  $62.4\\%$  , which is a  $2\\%$   and   $5.1\\%$   improvement over DPR and BM25. Similar trends are observed when Uni- fiedQA is employed, with RAPTOR outperforming DPR and BM25 by  $2.7\\%$   and   $6.7\\%$  , respectively. ", "page_idx": 6, "bbox": [107, 603.5215454101562, 504, 637.4445190429688], "page_size": [612.0, 792.0]}
{"layout": 57, "type": "text", "text": "Finally, in the Narrative QA dataset, as presented in Table  6 , RAPTOR excels across multiple met- rics. For ROUGE-L, it surpasses BM25 and DPR by 7.3 and 2.7 points, respectively. In other metrics like BLEU-1, BLEU-4, and METEOR, RAPTOR outperforms BM25 and DPR by margins ranging from 1.7 to 5.8 and 0.7 to 2.1 points, respectively. ", "page_idx": 6, "bbox": [107, 642.3755493164062, 504, 687.2575073242188], "page_size": [612.0, 792.0]}
{"layout": 58, "type": "table", "page_idx": 7, "img_path": "layout_images/2401.18059v1_5.jpg", "table_caption": "Table 2:  QuALITY and QASPER Performance With  $^+$   Without RAPTOR:  Performance com- parison across the QuALITY and QASPER datasets of various retrieval methods (SBERT, BM25, DPR) with and without RAPTOR. UnifiedQA-3B is used as the language model. RAPTOR outper- forms baselines of each respective retrieval method for both datasets. ", "bbox": [106, 80, 505, 217], "page_size": [612.0, 792.0], "ocr_text": "Model Accuracy (QUALITY) Answer F1 (QASPER)\nSBERT with RAPTOR 56.6% 36.70%\nSBERT without RAPTOR 54.9% 36.23%\nBM25 with RAPTOR 52.1% 27.00%\nBM25 without RAPTOR 49.9% 26.47%\nDPR with RAPTOR 54.7% 32.23 %\nDPR without RAPTOR 53.1% 31.70%\n", "vlm_text": "The table compares different models based on two metrics: Accuracy (QuALITY) and Answer F1 (QASPER). Here's a breakdown:\n\n- **SBERT with RAPTOR**\n  - Accuracy (QuALITY): 56.6%\n  - Answer F1 (QASPER): 36.70%\n\n- **SBERT without RAPTOR**\n  - Accuracy (QuALITY): 54.9%\n  - Answer F1 (QASPER): 36.23%\n\n- **BM25 with RAPTOR**\n  - Accuracy (QuALITY): 52.1%\n  - Answer F1 (QASPER): 27.00%\n\n- **BM25 without RAPTOR**\n  - Accuracy (QuALITY): 49.9%\n  - Answer F1 (QASPER): 26.47%\n\n- **DPR with RAPTOR**\n  - Accuracy (QuALITY): 54.7%\n  - Answer F1 (QASPER): 32.23%\n\n- **DPR without RAPTOR**\n  - Accuracy (QuALITY): 53.1%\n  - Answer F1 (QASPER): 31.70%\n\nThe models are assessed with and without the RAPTOR component, showing differences in performance across the metrics."}
{"layout": 59, "type": "text", "text": "Table 3: Controlled comparison of F-1 scores on the QASPER dataset, using three different lan- guage models (GPT-3, GPT-4, UnifiedQA 3B) and various retrieval methods. The column ”Title  $^+$  Abstract” reflects performance when only the title and abstract of the papers are used for context. RAPTOR outperforms the established baselines BM25 and DPR across all tested language models. Specifically, RAPTOR’s F-1 scores are at least   $1.8\\%$   points higher than DPR and at least  $5.3\\%$   points higher than BM25. ", "page_idx": 7, "bbox": [108, 227.87957763671875, 505, 294.679443359375], "page_size": [612.0, 792.0]}
{"layout": 60, "type": "table", "page_idx": 7, "img_path": "layout_images/2401.18059v1_6.jpg", "bbox": [143, 303, 467, 367], "page_size": [612.0, 792.0], "ocr_text": "Retriever GPT-3 F-1 Match GPT-4F-1 Match UnifiedQA F-1 Match\n\nTitle + Abstract 25.2 22.2 17.5\nBM25 46.6 50.2 26.4\nDPR 51.3 53.0 32.1\n\nRAPTOR 53.1 55.7 36.6\n\n", "vlm_text": "The table shows F-1 Match scores for different retrievers when combined with different models: GPT-3, GPT-4, and UnifiedQA.\n\n- **Retrievers**: Title + Abstract, BM25, DPR, RAPTOR\n- **Models and Scores**:\n  - **GPT-3 F-1 Match**: Title + Abstract (25.2), BM25 (46.6), DPR (51.3), RAPTOR (53.1)\n  - **GPT-4 F-1 Match**: Title + Abstract (22.2), BM25 (50.2), DPR (53.0), RAPTOR (55.7)\n  - **UnifiedQA F-1 Match**: Title + Abstract (17.5), BM25 (26.4), DPR (32.1), RAPTOR (36.6)\n\nRAPTOR has the highest scores across all models."}
{"layout": 61, "type": "text", "text": "Comparison to State-of-the-art Systems Building upon our controlled comparisons, we examine RAPTOR’s performance relative to other state-of-the-art models. As shown in Table  5 , RAPTOR with GPT-4 sets a new benchmark on QASPER, with a   $55.7\\%$   F-1 score, surpassing the CoLT5 XL’s score of  $53.9\\%$  . ", "page_idx": 7, "bbox": [107, 387.3279113769531, 296, 476.4034423828125], "page_size": [612.0, 792.0]}
{"layout": 62, "type": "text", "text": "In the QuALITY dataset, as shown in Table  7 , RAPTOR paired with GPT-4 sets a new state- of-the-art with an accuracy of   $82.6\\%$  , surpass- ing the previous best result of   $62.3\\%$  . In par- ticular, it outperforms CoLISA by   $21.5\\%$   on QuALITY-HARD, which represents questions that humans took unusually long to correctly answer, requiring rereading parts of the text, difficult reasoning, or both. ", "page_idx": 7, "bbox": [107, 481.33551025390625, 296, 581.0114135742188], "page_size": [612.0, 792.0]}
{"layout": 63, "type": "text", "text": "For the Narrative QA dataset, as represented in Table 6, RAPTOR paired with UnifiedQA setsTable 4: Comparison of accuracies on the QuAL- ITY dev dataset for two different language mod- els (GPT-3, UnifiedQA 3B) using various retrieval methods. RAPTOR outperforms the baselines of BM25 and DPR by at least  $2.0\\%$   in accuracy. ", "page_idx": 7, "bbox": [107, 585.9425048828125, 296, 608.9064331054688], "page_size": [612.0, 792.0]}
{"layout": 64, "type": "text", "text": "", "page_idx": 7, "bbox": [306, 389.08154296875, 503, 444.92144775390625], "page_size": [612.0, 792.0]}
{"layout": 65, "type": "table", "page_idx": 7, "img_path": "layout_images/2401.18059v1_7.jpg", "table_caption": "Table 5: Results on F-1 Match scores of various models on the QASPER dataset. ", "bbox": [305, 454, 505, 537], "page_size": [612.0, 792.0], "ocr_text": "Model GPT-3 Acc. UnifiedQA Acc.\n\nBM25 573 49.9\nDPR 60.4 53.9\nRAPTOR 62.4 56.6\n\n", "vlm_text": "This table compares the performance of three models: BM25, DPR, and RAPTOR. The metrics provided are the accuracy percentages for each model on two different tasks or datasets, labeled \"GPT-3 Acc.\" and \"UnifiedQA Acc.\"\n\n- BM25:\n  - GPT-3 Acc.: 57.3\n  - UnifiedQA Acc.: 49.9\n\n- DPR:\n  - GPT-3 Acc.: 60.4\n  - UnifiedQA Acc.: 53.9\n\n- RAPTOR:\n  - GPT-3 Acc.: 62.4\n  - UnifiedQA Acc.: 56.6\n\nThe RAPTOR model shows the highest accuracy in both categories."}
{"layout": 66, "type": "table", "page_idx": 7, "img_path": "layout_images/2401.18059v1_8.jpg", "bbox": [313, 546, 494, 600], "page_size": [612.0, 792.0], "ocr_text": "Model F-1 Match\n\nLongTS5 XL (Guo et al., 2022) 53.1\nCoLTS5 XL (Ainslie et al., 2023) 53.9\nRAPTOR + GPT-4 55.7\n\n", "vlm_text": "The table compares the F-1 Match scores of different models. \n\n- **LongT5 XL**: Achieved a score of 53.1.\n- **CoLT5 XL**: Achieved a score of 53.9.\n- **RAPTOR + GPT-4**: Achieved the highest score of 55.7.\n\nThe table appears to be comparing the performance of these models based on the F-1 Match metric."}
{"layout": 67, "type": "text", "text": "a new state-of-the-art METEOR score. When compared to the recursively summarizing model by Wu et al.  ( 2021 ), which also employs UnifiedQA, RAPTOR outperforms it on all metrics. While Wu et al.  ( 2021 ) rely solely on the summary in the top root node of the tree structure, RAPTOR benefits from its intermediate layers and clustering approaches, which allows it to capture a range of information, from general themes to specific details, contributing to its overall strong performance. ", "page_idx": 7, "bbox": [108, 607.8604736328125, 504, 663.701416015625], "page_size": [612.0, 792.0]}
{"layout": 68, "type": "text", "text": "4.1 C ON TRI BUT ION OF THE TREE STRUCTURE ", "text_level": 1, "page_idx": 7, "bbox": [107, 678, 310, 689], "page_size": [612.0, 792.0]}
{"layout": 69, "type": "text", "text": "We examine the contribution of each layer of nodes to RAPTOR’s retrieval capabilities. We hy- pothesized that upper nodes play a crucial role in handling thematic or multi-hop queries requiring a broader understanding of the text. ", "page_idx": 7, "bbox": [108, 698.7384643554688, 504, 732.6614379882812], "page_size": [612.0, 792.0]}
{"layout": 70, "type": "text", "text": "Table 6: Performance comparison on the Narrative QA dataset across multiple models, focusing on four metrics: ROUGE-L, BLEU-1, BLEU-4, and METEOR. RAPTOR, when paired with Uni- fiedQA 3B, not only surpasses retrieval methods like BM25 and DPR but also sets a new state-of- the-art in the METEOR metric. ", "page_idx": 8, "bbox": [108, 80.32757568359375, 504, 125.20946502685547], "page_size": [612.0, 792.0]}
{"layout": 71, "type": "table", "page_idx": 8, "img_path": "layout_images/2401.18059v1_9.jpg", "table_caption": "Table 7: Accuracies of the QuALITY dataset on both the overall test set and the more challenging hard subset. GPT-4 with RAPTOR sets a new state-of-the-art. ", "bbox": [106, 133, 505, 246], "page_size": [612.0, 792.0], "ocr_text": "Model\n\nROUGE-L_ BLEU-1\n\nBLEU-4 METEOR\n\nBiDAF (Koéisky et al., 2018)\n\nBM25 + BERT (Mov et al., 2020)\n\nRecursively Summarizing Books (Wu et al., 2021)\nRetriever + Reader (Izacard & Grave, 2022)\nRAPTOR + UnifiedQA\n\n6.2\n21.6\n32.0\n30.8\n\n5.7\n14.5\n22.3\n35.3\n23.5\n\n0.3\n1.4\n4.2\n75\n6.4\n\n3.7\n5.0\n10.6\nieee\n19.1\n", "vlm_text": "The table presents the performance of different models evaluated on metrics ROUGE-L, BLEU-1, BLEU-4, and METEOR. Here are the details:\n\n1. **BiDAF (Kočiskỳ et al., 2018)**\n   - ROUGE-L: 6.2\n   - BLEU-1: 5.7\n   - BLEU-4: 0.3\n   - METEOR: 3.7\n\n2. **BM25 + BERT (Mou et al., 2020)**\n   - ROUGE-L: 15.5\n   - BLEU-1: 14.5\n   - BLEU-4: 1.4\n   - METEOR: 5.0\n\n3. **Recursively Summarizing Books (Wu et al., 2021)**\n   - ROUGE-L: 21.6\n   - BLEU-1: 22.3\n   - BLEU-4: 4.2\n   - METEOR: 10.6\n\n4. **Retriever + Reader (Izacard & Grave, 2022)**\n   - ROUGE-L: 32.0\n   - BLEU-1: 35.3\n   - BLEU-4: 7.5\n   - METEOR: 11.1\n\n5. **RAPTOR + UnifiedQA**\n   - ROUGE-L: 30.8\n   - BLEU-1: 23.5\n   - BLEU-4: 6.4\n   - METEOR: 19.1\n\nThe best scores for each metric are bolded in the table."}
{"layout": 72, "type": "table", "page_idx": 8, "img_path": "layout_images/2401.18059v1_10.jpg", "table_caption": "Table 8: Performance of RAPTOR when querying different tree layers for Story 1 from the QuAL- ITY dataset. Columns represent different starting points (highest layer) and rows represent different numbers of layers queried. ", "bbox": [106, 256, 505, 384], "page_size": [612.0, 792.0], "ocr_text": "Model\n\nAccuracy\n\nTest Set Hard Subset\n\nLongformer-base (Beltagy et al., 2020)\nDPR and DeBERTaV3-large (Pang et al., 2022)\nCoLISA (DeBERTaV3-large) (Dong et al., 2023a)\nRAPTOR + GPT-4\n\n39.5 35.3\n55.4 46.1\n62.3 54.7\n\n82.6 76.2\n", "vlm_text": "The table presents the accuracy of different models on two datasets: the \"Test Set\" and the \"Hard Subset\". The models compared in the table are:\n\n1. Longformer-base (Beltagy et al., 2020)\n   - Test Set Accuracy: 39.5\n   - Hard Subset Accuracy: 35.3\n\n2. DPR and DeBERTaV3-large (Pang et al., 2022)\n   - Test Set Accuracy: 55.4\n   - Hard Subset Accuracy: 46.1\n\n3. CoLISA (DeBERTaV3-large) (Dong et al., 2023a)\n   - Test Set Accuracy: 62.3\n   - Hard Subset Accuracy: 54.7\n\n4. RAPTOR + GPT-4\n   - Test Set Accuracy: 82.6\n   - Hard Subset Accuracy: 76.2\n\nThe RAPTOR + GPT-4 model achieves the highest accuracy on both the Test Set and the Hard Subset compared to the other models listed."}
{"layout": 73, "type": "table", "page_idx": 8, "img_path": "layout_images/2401.18059v1_11.jpg", "bbox": [154, 394, 456, 447], "page_size": [612.0, 792.0], "ocr_text": "Layers Queried /Start Layer Layer 0 (Leaf Nodes) Layer1 Layer 2\n\n1 layer 57.9 57.8 57.9\n2 layers - 52.6 63.15\n3 layers - - 73.68\n\n", "vlm_text": "The table presents data on different layers and their corresponding numeric values under various conditions. It consists of three main columns after the initial descriptive column:\n\n1. **Layers Queried / Start Layer**: This column lists the number of layers queried or the start layer for each row.\n   - 1 layer\n   - 2 layers\n   - 3 layers\n\n2. **Layer 0 (Leaf Nodes)**: This column provides the values associated with Layer 0 for different queries:\n   - For 1 layer: 57.9\n   - No data is provided for 2 layers and 3 layers scenarios for Layer 0.\n\n3. **Layer 1**: This column provides the values associated with Layer 1:\n   - For 1 layer: 57.8\n   - For 2 layers: 52.6\n   - No data is provided for 3 layers scenario for Layer 1.\n\n4. **Layer 2**: This column provides the values associated with Layer 2:\n   - For 1 layer: 57.9\n   - For 2 layers: 63.15\n   - For 3 layers: 73.68 (which is in bold, possibly indicating a significant value or result).\n\nThere is no additional caption or description provided with the table to give context to what these layers specifically refer to, but the format suggests a progression or calculation regarding multiple layers or stages within a system or model."}
{"layout": 74, "type": "text", "text": "We validated this hypothesis both quantitatively and qualitatively. We present qualitative analysis in appendix  G . To quantitatively understand the contribution of the upper-level nodes, we used stories from the QuALITY dataset. The RAPTOR tree is built for each of these stories, as described in Section  3 . However, during retrieval, we limit the search to different subsets of layers. For example, we exclusively retrieve from the leaf nodes and each upper layer, as well as from different contiguous subsets of the layers. We show findings specific to one story in Table  8 , revealing that a full-tree search, utilizing all layers, outperformed retrieval strategies that focused only on specific layers. ", "page_idx": 8, "bbox": [108, 473.9185485839844, 504, 551.6774291992188], "page_size": [612.0, 792.0]}
{"layout": 75, "type": "text", "text": "These findings highlight the importance of the full tree structure in RAPTOR. By providing both the original text and higher-level summaries for retrieval, RAPTOR can effectively handle a wider range of questions, from higher-order thematic queries to detail-oriented questions. Detailed results for additional stories and an ablation study on layer contributions can be found in Appendix  I . ", "page_idx": 8, "bbox": [108, 556.6085205078125, 504, 601.490478515625], "page_size": [612.0, 792.0]}
{"layout": 76, "type": "text", "text": "5 C ONCLUSION ", "text_level": 1, "page_idx": 8, "bbox": [107, 625, 196, 638], "page_size": [612.0, 792.0]}
{"layout": 77, "type": "text", "text": "In this paper, we have presented RAPTOR, a novel tree-based retrieval system that augments the parametric knowledge of large language models with contextual information at various levels of abstraction. By employing recursive clustering and sum mari z ation techniques, RAPTOR creates a hierarchical tree structure that is capable of synthesizing information across various sections of the retrieval corpora. During the query phase, RAPTOR leverages this tree structure for more effective retrieval. Our controlled experiments demonstrated that RAPTOR not only outperforms traditional retrieval methods but also sets new performance benchmarks on several question-answering tasks. ", "page_idx": 8, "bbox": [108, 654.9025268554688, 504, 732.6614379882812], "page_size": [612.0, 792.0]}
{"layout": 78, "type": "text", "text": "Language Models for QA and Sum mari z ation Four language models are used in our RAPTOR experiments: GPT-3 and GPT-4 for QA tasks, and GPT-3.5-turbo for sum mari z ation. The  gpt-3 , gpt-4 , and  gpt-3.5-turbo  models can be accessed via API calls ( OpenAI API ). UnifiedQA, which is used for QA tasks, is publicly available at  Hugging Face . ", "page_idx": 9, "bbox": [107, 106.08086395263672, 504, 151.32041931152344], "page_size": [612.0, 792.0]}
{"layout": 79, "type": "text", "text": "Evaluation Datasets The three evaluation datasets used in our experiments— QuALITY , QASPER , and  Narrative QA —are all publicly accessible. These datasets ensure that the retrieval and QA tests conducted in this study can be replicated. ", "page_idx": 9, "bbox": [107, 162.8838348388672, 504, 197.16539001464844], "page_size": [612.0, 792.0]}
{"layout": 80, "type": "text", "text": "Source Code The source code for RAPTOR will be publicly available  here ", "page_idx": 9, "bbox": [107, 208.7278289794922, 415.4261474609375, 221.68917846679688], "page_size": [612.0, 792.0]}
{"layout": 81, "type": "text", "text": "R EFERENCES ", "text_level": 1, "page_idx": 9, "bbox": [108, 238, 175, 249], "page_size": [612.0, 792.0]}
{"layout": 82, "type": "text", "text": "Charu C Aggarwal, Alexander Hinneburg, and Daniel A Keim. On the Surprising Behavior of Dis- tance Metrics in High Dimensional Space. In  Database Theory—ICDT 2001: 8th International Conference London, UK, January 4–6, 2001 Proceedings 8 , pp. 420–434. Springer, 2001. URL https://link.springer.com/chapter/10.1007/3-540-44503-x_27 . ", "page_idx": 9, "bbox": [107, 256.08648681640625, 504, 300.9674072265625], "page_size": [612.0, 792.0]}
{"layout": 83, "type": "text", "text": "Joshua Ainslie, Tao Lei, Michiel de Jong, Santiago Onta˜ n´ on, Siddhartha Brahma, Yury Zemlyan- skiy, David Uthus, Mandy Guo, James Lee-Thorp, Yi Tay, et al. CoLT5: Faster long-range transformers with conditional computation. arXiv preprint arXiv:2303.09752 , 2023. URL https://arxiv.org/abs/2303.09752 . ", "page_idx": 9, "bbox": [107, 308.3134765625, 504, 353.245361328125], "page_size": [612.0, 792.0]}
{"layout": 84, "type": "text", "text": "Ekin Akyurek, Tolga Bolukbasi, Frederick Liu, Binbin Xiong, Ian Tenney, Jacob Andreas, and Kelvin Guu. Towards tracing knowledge in language models back to the training data. In Findings of the Association for Computational Linguistics: EMNLP 2022 , pp. 2429–2446, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.findings-emnlp.180. URL  https://a cl anthology.org/2022. findings-emnlp.180 . ", "page_idx": 9, "bbox": [107, 360.6404113769531, 504, 427.4403076171875], "page_size": [612.0, 792.0]}
{"layout": 85, "type": "text", "text": "Stefanos Angelidis and Mirella Lapata. Summarizing opinions: Aspect extraction meets sentiment prediction and they are both weakly supervised.  arXiv preprint arXiv:1808.08858 , 2018. URL https://arxiv.org/abs/1808.08858 . ", "page_idx": 9, "bbox": [107, 434.8353576660156, 504, 468.75830078125], "page_size": [612.0, 792.0]}
{"layout": 86, "type": "text", "text": "Manoj Ghuhan Ari vaz hagan, Lan Liu, Peng Qi, Xinchi Chen, William Yang Wang, and Zhiheng Huang. Hybrid hierarchical retrieval for open-domain question answering. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.),  Findings of the Association for Computational Linguistics: ACL 2023 , pp. 10680–10689, Toronto, Canada, July 2023. Association for Computa- tional Linguistics. doi: 10.18653/v1/2023.findings-acl.679. URL  https://a cl anthology. org/2023.findings-acl.679 . ", "page_idx": 9, "bbox": [107, 476.1533508300781, 504, 542.9532470703125], "page_size": [612.0, 792.0]}
{"layout": 87, "type": "text", "text": "Iz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The Long-document Transformer, 2020. URL  https://arxiv.org/abs/2004.05150 . arXiv preprint arXiv:2004.05150. ", "page_idx": 9, "bbox": [107, 550.3483276367188, 504, 573.312255859375], "page_size": [612.0, 792.0]}
{"layout": 88, "type": "text", "text": "Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Milli- can, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al. Improving language models by retrieving from trillions of tokens. In  International conference on machine learning , pp. 2206–2240. PMLR, 2022. URL  https://arxiv.org/abs/2112. 04426 . ", "page_idx": 9, "bbox": [107, 580.707275390625, 504, 636.5482788085938], "page_size": [612.0, 792.0]}
{"layout": 89, "type": "text", "text": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhari- wal, Arvind Neel a kant an, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Rad- ford, Ilya Sutskever, and Dario Amodei. Language Models are Few-Shot Learners. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.),  Advances in Neu- ral Information Processing Systems , volume 33, pp. 1877–1901. Curran Associates, Inc., ", "page_idx": 9, "bbox": [107, 643.9432983398438, 504, 732.6612548828125], "page_size": [612.0, 792.0]}
{"layout": 90, "type": "text", "text": "2020. URL  https://proceedings.neurips.cc/paper files/paper/2020/ file/1457 c 0 d 6 bfc b 4967418 b fb 8 ac 142 f 64 a-Paper.pdf . ", "page_idx": 10, "bbox": [117, 82.6185302734375, 504, 105.58245086669922], "page_size": [612.0, 792.0]}
{"layout": 91, "type": "text", "text": "S´ ebastien Bubeck, Varun Chandra sekar an, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Ka- mar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of Artificial General Intelligence: Early Experiments with GPT-4.  arXiv preprint arXiv:2303.12712 , 2023. URL https://arxiv.org/abs/2303.12712 . ", "page_idx": 10, "bbox": [107, 114.6695556640625, 504, 159.60142517089844], "page_size": [612.0, 792.0]}
{"layout": 92, "type": "text", "text": "Shuyang Cao and Lu Wang. HIBRIDS: Attention with hierarchical biases for structure-aware long document sum mari z ation. In  Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pp. 786–807, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.58. URL  https: //a cl anthology.org/2022.acl-long.58 . ", "page_idx": 10, "bbox": [107, 168.73748779296875, 504, 224.57835388183594], "page_size": [612.0, 792.0]}
{"layout": 93, "type": "text", "text": "Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading Wikipedia to Answer Open-Domain Questions. In  Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pp. 1870–1879, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1171. URL  https: //a cl anthology.org/P17-1171 . ", "page_idx": 10, "bbox": [107, 233.71441650390625, 504, 289.5552978515625], "page_size": [612.0, 792.0]}
{"layout": 94, "type": "text", "text": "Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. PaLM: Scaling Language Modeling with Pathways. arXiv preprint arXiv:2204.02311 , 2022. URL https://arxiv.org/abs/2204.02311 . ", "page_idx": 10, "bbox": [107, 298.6923522949219, 504, 343.57330322265625], "page_size": [612.0, 792.0]}
{"layout": 95, "type": "text", "text": "Arman Cohan and Nazli Goharian. Contextual i zing citations for scientific sum mari z ation using word embeddings and domain knowledge. In  Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval , pp. 1133–1136, 2017. URL https://dl.acm.org/doi/abs/10.1145/3077136.3080740 . ", "page_idx": 10, "bbox": [107, 352.7103576660156, 504, 397.59228515625], "page_size": [612.0, 792.0]}
{"layout": 96, "type": "text", "text": "Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan Salak hut dino v. Transformer-XL: Attentive language models beyond a fixed-length context. In  Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pp. 2978–2988, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1285. URL https://a cl anthology.org/P19-1285 . ", "page_idx": 10, "bbox": [107, 406.72833251953125, 504, 462.5692138671875], "page_size": [612.0, 792.0]}
{"layout": 97, "type": "text", "text": "Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R´ e. Flash Attention: Fast and memory-efficient exact attention with IO-Awareness.  Advances in Neural Information Processing Systems , 35:16344–16359, 2022. URL  https://arxiv.org/abs/2205.14135 . ", "page_idx": 10, "bbox": [107, 471.6562805175781, 504, 505.62823486328125], "page_size": [612.0, 792.0]}
{"layout": 98, "type": "text", "text": "Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A. Smith, and Matt Gardner. A Dataset of Information-Seeking Questions and Answers Anchored in Research Papers. In  Proceed- ings of the 2021 Conference of the North American Chapter of the Association for Computa- tional Linguistics: Human Language Technologies , pp. 4599–4610, Online, June 2021. Asso- ciation for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.365. URL  https: //a cl anthology.org/2021.naacl-main.365 . ", "page_idx": 10, "bbox": [107, 514.7652587890625, 504, 581.5652465820312], "page_size": [612.0, 792.0]}
{"layout": 99, "type": "text", "text": "Mengxing Dong, Bowei Zou, Yanling Li, and Yu Hong. CoLISA: Inner Interaction via Contrastive Learning for Multi-choice Reading Comprehension. In  Advances in Information Retrieval: 45th European Conference on Information Retrieval, ECIR 2023, Dublin, Ireland, April 2–6, 2023, Proceedings, Part I , pp. 264–278. Springer, 2023a. URL  https://link.springer.com/ chapter/10.1007/978-3-031-28244-7_17 . ", "page_idx": 10, "bbox": [107, 590.7012939453125, 504, 646.542236328125], "page_size": [612.0, 792.0]}
{"layout": 100, "type": "text", "text": "Zican Dong, Tianyi Tang, Lunyi Li, and Wayne Xin Zhao. A survey on long text modeling with transformers.  arXiv preprint arXiv:2302.14502 , 2023b. URL  https://arxiv.org/abs/ 2302.14502 . ", "page_idx": 10, "bbox": [107, 655.6782836914062, 504, 689.6012573242188], "page_size": [612.0, 792.0]}
{"layout": 101, "type": "text", "text": "Tianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen. Enabling large language models to generate text with citations.  arXiv preprint arXiv:2305.14627 , 2023. URL  https://arxiv.org/ abs/2305.14627 . ", "page_idx": 10, "bbox": [107, 698.73828125, 504, 732.6612548828125], "page_size": [612.0, 792.0]}
{"layout": 102, "type": "text", "text": "Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung, and Yinfei Yang. LongT5: Efficient text-to-text transformer for long sequences. In  Findings of the Association for Computational Linguistics: NAACL 2022 , pp. 724–736, Seattle, United States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.findings-naacl.55. URL  https://a cl anthology.org/2022.findings-naacl.55 . ", "page_idx": 11, "bbox": [107, 82.6185302734375, 504, 138.4593963623047], "page_size": [612.0, 792.0]}
{"layout": 103, "type": "text", "text": "Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. Retrieval Augmented Language Model Pre-Training. In  International conference on machine learning , pp. 3929–3938. PMLR, 2020. URL  https://doi.org/10.48550/arXiv.2002.08909 . ", "page_idx": 11, "bbox": [107, 147.59649658203125, 504, 181.51841735839844], "page_size": [612.0, 792.0]}
{"layout": 104, "type": "text", "text": "Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buch at s kaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models.  arXiv preprint arXiv:2203.15556 , 2022. URL https://arxiv.org/abs/2203.15556 . ", "page_idx": 11, "bbox": [107, 190.655517578125, 504, 235.5373992919922], "page_size": [612.0, 792.0]}
{"layout": 105, "type": "text", "text": "Gautier Izacard and Edouard Grave. Distilling Knowledge from Reader to Retriever for Ques- tion Answering, 2022. URL  https://arxiv.org/abs/2012.04584 . arXiv preprint arXiv:2012.04584. ", "page_idx": 11, "bbox": [107, 244.6734619140625, 504, 278.59637451171875], "page_size": [612.0, 792.0]}
{"layout": 106, "type": "text", "text": "Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. Few-shot learning with re- trieval augmented language models.  arXiv preprint arXiv:2208.03299 , 2022. URL  https: //arxiv.org/abs/2208.03299 . ", "page_idx": 11, "bbox": [107, 287.7334289550781, 504, 332.6153564453125], "page_size": [612.0, 792.0]}
{"layout": 107, "type": "text", "text": "Zhengbao Jiang, Frank F Xu, Jun Araki, and Graham Neubig. How can we know what language models know?  Transactions of the Association for Computational Linguistics , 8:423–438, 2020. URL  https://arxiv.org/abs/1911.12543 . ", "page_idx": 11, "bbox": [107, 341.75140380859375, 504, 375.67431640625], "page_size": [612.0, 792.0]}
{"layout": 108, "type": "text", "text": "Jeff Johnson, Matthijs Douze, and Herv´ e J´ egou. Billion-Scale Similarity Search with GPUs.  IEEE Transactions on Big Data , 7(3):535–547, 2019. URL  https://arxiv.org/abs/1702. 08734 . ", "page_idx": 11, "bbox": [107, 384.7613830566406, 504, 418.73333740234375], "page_size": [612.0, 792.0]}
{"layout": 109, "type": "text", "text": "Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raffel. Large Language Models struggle to learn Long-Tail Knowledge. In  International Conference on Machine Learn- ing , pp. 15696–15707. PMLR, 2023. URL  https://proceedings.mlr.press/v202/ kandpal23a/kandpal23a.pdf . ", "page_idx": 11, "bbox": [107, 427.8703918457031, 504, 472.7523193359375], "page_size": [612.0, 792.0]}
{"layout": 110, "type": "text", "text": "Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense Passage Retrieval for Open-Domain Question Answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pp. 6769–6781, Online, November 2020. Association for Computational Linguis- tics. doi: 10.18653/v1/2020.emnlp-main.550. URL  https://a cl anthology.org/2020. emnlp-main.550 . ", "page_idx": 11, "bbox": [107, 481.88836669921875, 504, 548.688232421875], "page_size": [612.0, 792.0]}
{"layout": 111, "type": "text", "text": "Daniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, and Hannaneh Hajishirzi. UNIFIEDQA: Crossing format boundaries with a single QA system. In  Findings of the Association for Computational Linguistics: EMNLP 2020 , pp. 1896–1907, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020. findings-emnlp.171. URL  https://a cl anthology.org/2020.findings-emnlp. 171 . ", "page_idx": 11, "bbox": [107, 557.8253173828125, 504, 624.6242065429688], "page_size": [612.0, 792.0]}
{"layout": 112, "type": "text", "text": "Omar Khattab and Matei Zaharia. ColBERT: Efficient and effective passage search via con- textual i zed late interaction over bert. In  Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval , pp. 39–48, 2020. URL https://arxiv.org/abs/2004.12832 . ", "page_idx": 11, "bbox": [107, 633.7612915039062, 504, 678.6422119140625], "page_size": [612.0, 792.0]}
{"layout": 113, "type": "text", "text": "Tom´ aˇ s Koˇ cisk\\` y, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, G´ abor Melis, and Edward Gre fens te tte. The Narrative QA Reading Comprehension Challenge.  Transactions of the Association for Computational Linguistics , 6:317–328, 2018. URL  https://arxiv. org/abs/1712.07040 . ", "page_idx": 11, "bbox": [107, 687.7293090820312, 504, 732.6612548828125], "page_size": [612.0, 792.0]}
{"layout": 114, "type": "text", "text": "Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K¨ uttler, Mike Lewis, Wen-tau Yih, Tim Rockt¨ aschel, et al. Retrieval-Augmented Gener- ation for Knowledge-Intensive NLP Tasks.  Advances in Neural Information Processing Systems , 33:9459–9474, 2020. URL  https://doi.org/10.48550/arXiv.2005.11401 . ", "page_idx": 12, "bbox": [107, 82.6185302734375, 504, 127.50041961669922], "page_size": [612.0, 792.0]}
{"layout": 115, "type": "text", "text": "Jerry Liu. LlamaIndex, 2022. URL  https://github.com/jerryjliu/llama index . ", "page_idx": 12, "bbox": [107, 134.87548828125, 493.30462646484375, 146.8804168701172], "page_size": [612.0, 792.0]}
{"layout": 116, "type": "text", "text": "Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the middle: How language models use long contexts. arXiv preprint arXiv:2307.03172 , 2023. URL  https://arxiv.org/abs/2307.03172 . ", "page_idx": 12, "bbox": [107, 154.2554931640625, 504, 188.1783905029297], "page_size": [612.0, 792.0]}
{"layout": 117, "type": "text", "text": "Ye Liu, Kazuma Hashimoto, Yingbo Zhou, Semih Yavuz, Caiming Xiong, and Philip Yu. Dense hierarchical retrieval for open-domain question answering. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (eds.),  Findings of the Association for Compu- tational Linguistics: EMNLP 2021 , pp. 188–200, Punta Cana, Dominican Republic, Novem- ber 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.findings-emnlp.19. URL  https://a cl anthology.org/2021.findings-emnlp.19 . ", "page_idx": 12, "bbox": [107, 195.552490234375, 504, 262.35235595703125], "page_size": [612.0, 792.0]}
{"layout": 118, "type": "text", "text": "Leland McInnes, John Healy, and James Melville. UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction, 2018. URL  https://arxiv.org/abs/1802. 03426 . arXiv preprint arXiv:1802.03426. ", "page_idx": 12, "bbox": [107, 269.7274169921875, 504, 303.64935302734375], "page_size": [612.0, 792.0]}
{"layout": 119, "type": "text", "text": "Sewon Min, Kenton Lee, Ming-Wei Chang, Kristina Toutanova, and Hannaneh Hajishirzi. Joint passage ranking for diverse multi-answer retrieval. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (eds.),  Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing , pp. 6997–7008, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021. emnlp-main.560. URL  https://a cl anthology.org/2021.emnlp-main.560 . ", "page_idx": 12, "bbox": [107, 311.0244140625, 504, 377.82427978515625], "page_size": [612.0, 792.0]}
{"layout": 120, "type": "text", "text": "Sewon Min, Weijia Shi, Mike Lewis, Xilun Chen, Wen-tau Yih, Hannaneh Hajishirzi, and Luke Z ett le moyer. Non parametric masked language modeling. In  Findings of the Association for Computational Linguistics: ACL 2023 , pp. 2097–2118, Toronto, Canada, July 2023. Associ- ation for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.132. URL  https: //a cl anthology.org/2023.findings-acl.132 . ", "page_idx": 12, "bbox": [107, 385.1993408203125, 504, 441.03924560546875], "page_size": [612.0, 792.0]}
{"layout": 121, "type": "text", "text": "Eric Mitchell, Charles Lin, Antoine Bosselut, Christopher D Manning, and Chelsea Finn. Memory-based model editing at scale. In  International Conference on Machine Learning , pp. 15817–15831. PMLR, 2022. URL  https://proceedings.mlr.press/v162/ mitchell 22 a/mitchell 22 a.pdf . ", "page_idx": 12, "bbox": [107, 448.414306640625, 504, 493.29620361328125], "page_size": [612.0, 792.0]}
{"layout": 122, "type": "text", "text": "Xiangyang Mou, Mo Yu, Bingsheng Yao, Chenghao Yang, Xiaoxiao Guo, Saloni Potdar, and Hui Su. Frustratingly hard evidence retrieval for QA over books. In  Proceedings of the First Joint Workshop on Narrative Understanding, Storylines, and Events , pp. 108–113, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.nuse-1.13. URL  https: //a cl anthology.org/2020.nuse-1.13 . ", "page_idx": 12, "bbox": [107, 500.6712646484375, 504, 556.5111694335938], "page_size": [612.0, 792.0]}
{"layout": 123, "type": "text", "text": "Inderjeet Nair, Aparna Garimella, Balaji Vasan Srinivasan, Natwar Modani, Niyati Chhaya, Srikr- ishna Karanam, and Sumit Shekhar. A neural CRF-based hierarchical approach for lin- ear text segmentation. In  Findings of the Association for Computational Linguistics: EACL 2023 , pp. 883–893, Dubrovnik, Croatia, May 2023. Association for Computational Linguis- tics. doi: 10.18653/v1/2023.findings-eacl.65. URL  https://a cl anthology.org/2023. findings-eacl.65 . ", "page_idx": 12, "bbox": [107, 563.88623046875, 504, 630.6862182617188], "page_size": [612.0, 792.0]}
{"layout": 124, "type": "text", "text": "Benjamin Newman, Luca Soldaini, Raymond Fok, Arman Cohan, and Kyle Lo. A controllable qa- based framework for de contextual iz ation.  arXiv preprint arXiv:2305.14772 , 2023. URL  https: //arxiv.org/pdf/2305.14772.pdf . ", "page_idx": 12, "bbox": [107, 638.0602416992188, 504, 671.9832153320312], "page_size": [612.0, 792.0]}
{"layout": 125, "type": "text", "text": "OpenAI. GPT-4 Technical Report.  ArXiv , abs/2303.08774, 2023. URL  https://arxiv.org/ abs/2303.08774 . ", "page_idx": 12, "bbox": [107, 679.3582763671875, 504, 702.3222045898438], "page_size": [612.0, 792.0]}
{"layout": 126, "type": "text", "text": "Richard Yuanzhe Pang, Alicia Parrish, Nitish Joshi, Nikita Nangia, Jason Phang, Angelica Chen, Vishakh Padmakumar, Johnny Ma, Jana Thompson, He He, and Samuel Bowman. QuALITY: ", "page_idx": 12, "bbox": [107, 709.697265625, 504, 732.6611938476562], "page_size": [612.0, 792.0]}
{"layout": 127, "type": "text", "text": "Question Answering with Long Input Texts, Yes! In  Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies , pp. 5336–5358, Seattle, United States, July 2022. Association for Computational Linguistics. URL  https://a cl anthology.org/2022.naacl-main.391 . ", "page_idx": 13, "bbox": [117, 82.6185302734375, 504, 127.50041961669922], "page_size": [612.0, 792.0]}
{"layout": 128, "type": "text", "text": "Fabio Petroni, Tim Rockt¨ aschel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H Miller, and Sebastian Riedel. Language models as knowledge bases?  arXiv preprint arXiv:1909.01066 , 2019. URL  https://arxiv.org/abs/1909.01066 . ", "page_idx": 13, "bbox": [107, 133.9124755859375, 504, 167.88438415527344], "page_size": [612.0, 792.0]}
{"layout": 129, "type": "text", "text": "Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models: Methods, Analysis & Insights from Training Gopher.  arXiv preprint arXiv:2112.11446 , 2021. URL  https://arxiv.org/abs/2112.11446 . ", "page_idx": 13, "bbox": [107, 174.346435546875, 504, 219.2283172607422], "page_size": [612.0, 792.0]}
{"layout": 130, "type": "text", "text": "Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton- Brown, and Yoav Shoham. In-context retrieval-augmented language models. arXiv preprint arXiv:2302.00083 , 2023. URL  https://arxiv.org/abs/2302.00083 . ", "page_idx": 13, "bbox": [107, 225.68939208984375, 504, 259.6123046875], "page_size": [612.0, 792.0]}
{"layout": 131, "type": "text", "text": "Nils Reimers and Iryna Gurevych. Sentence-BERT: Sentence embeddings using Siamese BERT- networks. In  Proceedings of the 2019 Conference on Empirical Methods in Natural Lan- guage Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) , pp. 3982–3992, Hong Kong, China, November 2019. Association for Com- putational Linguistics. doi: 10.18653/v1/D19-1410. URL  https://a cl anthology.org/ D19-1410 . ", "page_idx": 13, "bbox": [107, 266.0743408203125, 504, 332.87322998046875], "page_size": [612.0, 792.0]}
{"layout": 132, "type": "text", "text": "Adam Roberts, Colin Raffel, and Noam Shazeer. How Much Knowledge Can You Pack Into the Parameters of a Language Model? In  Proceedings of the 2020 Conference on Empir- ical Methods in Natural Language Processing (EMNLP) , pp. 5418–5426, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.437. URL https://a cl anthology.org/2020.emnlp-main.437 . ", "page_idx": 13, "bbox": [107, 339.3352966308594, 504, 395.17523193359375], "page_size": [612.0, 792.0]}
{"layout": 133, "type": "text", "text": "Stephen Robertson, Hugo Zaragoza, et al. The Probabilistic Relevance Framework: BM25 and Beyond.  Foundations and Trends in Information Retrieval , 3(4):333–389, 2009. URL  https: //doi.org/10.1561/1500000019 . ", "page_idx": 13, "bbox": [107, 401.6372985839844, 504, 435.56024169921875], "page_size": [612.0, 792.0]}
{"layout": 134, "type": "text", "text": "Stephen E Robertson, Steve Walker, Susan Jones, Micheline M Hancock-Beaulieu, Mike Gatford, et al. Okapi at TREC-3.  Nist Special Publication Sp , 109:109, 1995. URL  https://www. microsoft.com/en-us/research/publication/okapi-at-trec-3/ . ", "page_idx": 13, "bbox": [107, 442.02130126953125, 504, 475.9442138671875], "page_size": [612.0, 792.0]}
{"layout": 135, "type": "text", "text": "Devendra Singh Sachan, Mike Lewis, Dani Yogatama, Luke Z ett le moyer, Joelle Pineau, and Manzil Zaheer. Questions are all you need to train a dense passage retriever.  Transactions of the As- sociation for Computational Linguistics , 11:600–616, 2023. doi: 10.1162/tacl a 00564. URL https://a cl anthology.org/2023.tacl-1.35 . ", "page_idx": 13, "bbox": [107, 482.4062805175781, 504, 527.2885131835938], "page_size": [612.0, 792.0]}
{"layout": 136, "type": "text", "text": "Gideon Schwarz. Estimating the Dimension of a Model.  The annals of statistics , pp. 461–464, 1978. URL  https://project euclid.org/journals/annals-of-statistics/ volume-6/issue-2/Estimating-the-Dimension-of-a-Model/10.1214/ aos/1176344136.full . ", "page_idx": 13, "bbox": [107, 533.7495727539062, 504, 578.6315307617188], "page_size": [612.0, 792.0]}
{"layout": 137, "type": "text", "text": "Karen Sp¨ arck Jones. A Statistical Interpretation of Term Specificity and its Application in Re- trieval.  Journal of documentation , 28(1):11–21, 1972. URL  https://doi.org/10.1108/ eb026526 . ", "page_idx": 13, "bbox": [107, 585.0435791015625, 504, 619.0155029296875], "page_size": [612.0, 792.0]}
{"layout": 138, "type": "text", "text": "Simeng Sun, Kalpesh Krishna, Andrew Mattarella-Micke, and Mohit Iyyer. Do long-range language models actually use long-range context? In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (eds.),  Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing , pp. 807–822, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main. 62. URL  https://a cl anthology.org/2021.emnlp-main.62 . ", "page_idx": 13, "bbox": [107, 625.4775390625, 504, 692.2764892578125], "page_size": [612.0, 792.0]}
{"layout": 139, "type": "text", "text": "Zhiqing Sun, Xuezhi Wang, Yi Tay, Yiming Yang, and Denny Zhou. Recitation-augmented language models.  arXiv preprint arXiv:2210.01296 , 2022. URL  https://arxiv.org/abs/2210. 01296 . ", "page_idx": 13, "bbox": [107, 698.7385864257812, 504, 732.6614990234375], "page_size": [612.0, 792.0]}
{"layout": 140, "type": "text", "text": "Alon Talmor, Yanai Elazar, Yoav Goldberg, and Jonathan Berant. oLMpics– on what language model pre-training captures.  Transactions of the Association for Computational Linguistics , 8: 743–758, 2020. URL  https://arxiv.org/abs/1912.13283 . ", "page_idx": 14, "bbox": [108, 82.6185302734375, 504, 116.54143524169922], "page_size": [612.0, 792.0]}
{"layout": 141, "type": "text", "text": "Boxin Wang, Wei Ping, Peng Xu, Lawrence McAfee, Zihan Liu, Mohammad Shoeybi, Yi Dong, Oleksii Kuchaiev, Bo Li, Chaowei Xiao, et al. Shall we pretrain auto regressive language models with retrieval? a comprehensive study.  arXiv preprint arXiv:2304.06762 , 2023. URL  https: //arxiv.org/abs/2304.06762 . ", "page_idx": 14, "bbox": [108, 124.14349365234375, 504, 169.02537536621094], "page_size": [612.0, 792.0]}
{"layout": 142, "type": "text", "text": "Jeff Wu, Long Ouyang, Daniel M. Ziegler, Nisan Stiennon, Ryan Lowe, Jan Leike, and Paul Christiano. Recursively Summarizing Books with Human Feedback, 2021. URL  https: //arxiv.org/abs/2109.10862 . ", "page_idx": 14, "bbox": [108, 176.62744140625, 504, 210.5503387451172], "page_size": [612.0, 792.0]}
{"layout": 143, "type": "text", "text": "Adams Wei Yu, David Dohan, Minh-Thang Luong, Rui Zhao, Kai Chen, Mohammad Norouzi, and Quoc V. Le. QANet: Combining Local Convolution with Global Self-Attention for Read- ing Comprehension, 2018. URL  https://arxiv.org/abs/1804.09541 . arXiv preprint arXiv:1804.09541. ", "page_idx": 14, "bbox": [108, 218.15240478515625, 504, 263.0333251953125], "page_size": [612.0, 792.0]}
{"layout": 144, "type": "text", "text": "Wenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu, Mingxuan Ju, Soumya Sanyal, Chenguang Zhu, Michael Zeng, and Meng Jiang. Generate rather than retrieve: Large Language Models are strong context generators, 2022. URL  https://arxiv.org/abs/2209.10063 . ", "page_idx": 14, "bbox": [108, 270.6353759765625, 504, 304.55828857421875], "page_size": [612.0, 792.0]}
{"layout": 145, "type": "text", "text": "Shiyue Zhang, David Wan, and Mohit Bansal. Extractive is not faithful: An investigation of broad unfaithfulness problems in extractive sum mari z ation. In Anna Rogers, Jordan Boyd- Graber, and Naoaki Okazaki (eds.),  Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pp. 2153–2174, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.120. URL https://a cl anthology.org/2023.acl-long.120 . ", "page_idx": 14, "bbox": [108, 312.16033935546875, 504, 378.959228515625], "page_size": [612.0, 792.0]}
{"layout": 146, "type": "text", "text": "A S CALABILITY AND  C OM PUT AT IONAL  E FFICIENCY OF THE TREE-BUILDING PROCESS", "text_level": 1, "page_idx": 14, "bbox": [107, 400, 422, 426], "page_size": [612.0, 792.0]}
{"layout": 147, "type": "text", "text": "To assess the computational efficiency and cost-effectiveness of RAPTOR’s tree-building process, we conducted experiments on a consumer-grade laptop, specifically an Apple M1 Mac with 16GB of RAM. These experiments aimed to demonstrate the s cal ability and feasibility of RAPTOR on typical hardware. We varied the context length from 12,500 to 78,000 tokens and measured both the token expenditure and the time required to complete the tree-building process, from initial splitting and embedding to the construction of the final root node. ", "page_idx": 14, "bbox": [108, 439.30126953125, 504, 506.10015869140625], "page_size": [612.0, 792.0]}
{"layout": 148, "type": "image", "page_idx": 14, "img_path": "layout_images/2401.18059v1_12.jpg", "img_caption": "Figure 5: Token cost as a function of document length for QASPER, Narrative QA, and QuALITY. RAPTOR tree construction costs scale linearly with document length for each of the datasets. ", "bbox": [106, 517, 504, 679], "page_size": [612.0, 792.0], "ocr_text": "Prompt + Completion Tokens\n\n20000\n\n5000\n\nQasper Narrative QA Quality\n600000 B2000;\n2 500000 2 10000\n2 @\n§ 400000 5\n3 % 8000\nE €\n© 300000\ns s\n+ +\nPa £6000\n3 200000 4\n100000 4000\n°\n2000\n3 100000 200000 300000 400000 2000 3000 4000 5000 6000 7000 8000\n\n10000 15000 20000 25000 30000\nDocument Length (Tokens)\n\nDocument Length (Tokens)\n\nDocument Length (Tokens)\n", "vlm_text": "The image presents three line plots that show the token cost as a function of document length for three different datasets: QASPER, Narrative QA, and QuALITY. \n\n1. **QASPER** (left plot):\n   - The plot uses red color to represent data.\n   - The horizontal axis shows document length in tokens, ranging from 0 to 30,000.\n   - The vertical axis shows the total number of tokens for both the prompt and completion, ranging from 0 to 40,000.\n   - The graph illustrates a linear increase in token cost as document length increases.\n\n2. **Narrative QA** (middle plot):\n   - The plot uses blue color to represent data.\n   - The horizontal axis shows document length, ranging from 0 to 400,000 tokens.\n   - The vertical axis shows the total number of tokens for both the prompt and completion, ranging from 0 to 600,000.\n   - The graph depicts a linear relationship, with token cost steadily increasing with document length.\n\n3. **QuALITY** (right plot):\n   - The plot uses green color to represent data.\n   - The horizontal axis shows document length, ranging from 0 to 8,000 tokens.\n   - The vertical axis shows the total number of tokens for both the prompt and completion, ranging from 2,000 to 12,000.\n   - The graph demonstrates a generally linear increase in token cost with document length, though fluctuations are more noticeable as compared to the other datasets.\n\nThe caption of the image explains that RAPTOR tree construction costs scale linearly with document length across all three datasets, indicating that the increase in token usage is consistent as document length grows."}
{"layout": 149, "type": "text", "text": "Token Expenditure We empirically investigated the relationship between the initial document length and the total number of tokens expended during the tree-building process, which includes both the prompt and completion tokens. The document lengths varied significantly across the three datasets examined: QuALITY, QASPER, and Narrative QA. Figure  5  illustrates a clear linear corre- lation between the initial document length and the total token expenditure, emphasizing that RAP- TOR maintains a linear token scaling regardless of document complexity or length. ", "page_idx": 14, "bbox": [108, 698.3798828125, 504, 732.6614990234375], "page_size": [612.0, 792.0]}
{"layout": 150, "type": "text", "text": "", "page_idx": 15, "bbox": [108, 82.6185302734375, 504, 116.54143524169922], "page_size": [612.0, 792.0]}
{"layout": 151, "type": "image", "page_idx": 15, "img_path": "layout_images/2401.18059v1_13.jpg", "img_caption": "Figure 6: Build time as a function of document length for documents of up to 80,000 tokens. RAP- TOR tree construction time scales linearly with document length for each of the datasets. ", "bbox": [107, 146, 504, 338], "page_size": [612.0, 792.0], "ocr_text": "Time in seconds\n\n1500\n\n1000\n\n500\n\n", "vlm_text": "The image is a line graph showing the relationship between build time and document length in tokens for RAPTOR tree construction. The x-axis represents the length of the document in tokens, ranging from 0 to 80,000. The y-axis represents the time in seconds, ranging from 0 to 1500. The graph indicates that the build time scales linearly with the length of the document for each of the datasets, as stated in the caption. As the length of the document increases, the time taken to build also increases. The data points are plotted and connected with a line to show this linear relationship."}
{"layout": 152, "type": "text", "text": "Build Time We also empirically observed a consistent linear trend between the document length and the build time, as shown in Figure  6 . This suggests that RAPTOR scales linearly in terms of time, making it a viable solution for efficiently processing large corpora of varying lengths. ", "page_idx": 15, "bbox": [108, 359.5469055175781, 504, 393.8284912109375], "page_size": [612.0, 792.0]}
{"layout": 153, "type": "text", "text": "Conclusion Overall, our empirical results indicate that RAPTOR scales both in terms of tokens expended and build time. Even as the complexity and volume of the input text grow, the cost of constructing the tree scales predictably and linearly. This demonstrates that RAPTOR is computa- tionally efficient and well-suited for processing large and diverse corpora. ", "page_idx": 15, "bbox": [108, 406.45489501953125, 504, 451.6954345703125], "page_size": [612.0, 792.0]}
{"layout": 154, "type": "text", "text": "B A BLATION  S TUDY ON  C LUSTERING  M ECHANISM IN  RAPTOR ", "text_level": 1, "page_idx": 15, "bbox": [107, 468, 446, 481], "page_size": [612.0, 792.0]}
{"layout": 155, "type": "text", "text": "To assess the effectiveness of the clustering mechanism in our RAPTOR approach, we conducted an ablation study on the QuALITY dataset. This study compares RAPTOR’s performance with a balanced tree-style encoding and sum mari z ation of contiguous chunks, in contrast to our standard clustering method. ", "page_idx": 15, "bbox": [108, 494.3695068359375, 504, 539.2514038085938], "page_size": [612.0, 792.0]}
{"layout": 156, "type": "text", "text": "B.1 M ETHODOLOGY ", "text_level": 1, "page_idx": 15, "bbox": [107, 553, 203, 565], "page_size": [612.0, 792.0]}
{"layout": 157, "type": "text", "text": "Both configurations in this ablation study utilized SBERT embeddings and UnifiedQA to maintain consistency in retrieval. For RAPTOR, we employed our typical clustering and sum mari z ation process. In contrast, the alternative setup involved creating a balanced tree by recursively encoding and summarizing contiguous text chunks. We determined the window size for this setup based on the average cluster size observed in RAPTOR, which is approximately 6.7 nodes. Hence, we chose a window size of 7 nodes. The collapsed tree approach was applied for retrieval in both models. ", "page_idx": 15, "bbox": [108, 574.636474609375, 504, 641.4354248046875], "page_size": [612.0, 792.0]}
{"layout": 158, "type": "text", "text": "B.2 R ESULTS  & D ISCUSSION ", "text_level": 1, "page_idx": 15, "bbox": [107, 656, 239, 667], "page_size": [612.0, 792.0]}
{"layout": 159, "type": "text", "text": "The results of the ablation study are presented in table  9 . The results from this ablation study clearly indicate an improvement in accuracy when employing RAPTOR’s clustering mechanism over the recency-based tree approach. This finding substantiates our hypothesis that the clustering strategy in RAPTOR is more effective in capturing homogeneous content for sum mari z ation, thereby enhancing the overall retrieval performance. ", "page_idx": 15, "bbox": [108, 676.8204956054688, 504, 732.6614379882812], "page_size": [612.0, 792.0]}
{"layout": 160, "type": "text", "text": "Table 9: Ablation study results comparing RAPTOR with a recency-based tree approach ", "page_idx": 16, "bbox": [129.8769989013672, 80.32757568359375, 482.1246337890625, 92.33251190185547], "page_size": [612.0, 792.0]}
{"layout": 161, "type": "table", "page_idx": 16, "img_path": "layout_images/2401.18059v1_14.jpg", "bbox": [161, 101, 449, 147], "page_size": [612.0, 792.0], "ocr_text": "Configuration Accuracy\n\nRAPTOR + SBERT embeddings + UnifiedQA 56.6%\nRecency-based tree + SBERT embeddings + UnifiedQA 55.8%\n\n", "vlm_text": "The table lists two different configurations used for a particular task and their respective accuracy rates. The configurations and accuracies are as follows:\n\n1. Configuration: RAPTOR + SBERT embeddings + UnifiedQA\n   - Accuracy: 56.6%\n\n2. Configuration: Recency-based tree + SBERT embeddings + UnifiedQA\n   - Accuracy: 55.8% \n\nThe table compares the performance of these configurations in terms of accuracy, with the RAPTOR configuration achieving a slightly higher accuracy."}
{"layout": 162, "type": "text", "text": "C D ATASET  S TATISTICS AND  C OMPRESSION  R ATIOS ", "text_level": 1, "page_idx": 16, "bbox": [106, 166, 383, 178], "page_size": [612.0, 792.0]}
{"layout": 163, "type": "text", "text": "The average ratio of the summary length to the sum of child node lengths across all datasets is 0.28, indicating a  $72\\%$   compression rate. On average, the summary length is 131 tokens, and the average child node length is 86 tokens. Below are the detailed statistics for all three datasets: ", "page_idx": 16, "bbox": [107, 190.8095703125, 504, 224.7324676513672], "page_size": [612.0, 792.0]}
{"layout": 164, "type": "table", "page_idx": 16, "img_path": "layout_images/2401.18059v1_15.jpg", "table_caption": "Table 10: Statistics of Average Summary Length and Child Node Length Across Datasets ", "bbox": [126, 234, 485, 348], "page_size": [612.0, 792.0], "ocr_text": "Dataset Avg. Avg. Child Avg. # of Avg.\nSummary Node Text Child Nodes Compression\n\nLength Length Per Parent Ratio (%)\n(tokens) (tokens)\n\nAll Datasets 131 85.6 6.7 28\n\nQuALITY 124.4 87.9 5.7 .28\n\nNarrativeQA 129.7 85.5 6.8 27\n\n145.9 86.2 5.1 35\n\nQASPER\n", "vlm_text": "The table presents data from different datasets regarding their summarization characteristics. It includes the following columns:\n\n1. **Dataset**: Lists the datasets analyzed, including \"All Datasets,\" \"QuALITY,\" \"NarrativeQA,\" and \"QASPER.\"\n\n2. **Avg. Summary Length (tokens)**: Displays the average summary length in terms of tokens for each dataset. The average summary lengths are:\n   - All Datasets: 131 tokens\n   - QuALITY: 124.4 tokens\n   - NarrativeQA: 129.7 tokens\n   - QASPER: 145.9 tokens\n\n3. **Avg. Child Node Text Length (tokens)**: Shows the average text length in tokens for child nodes in each dataset. The average child node text lengths are:\n   - All Datasets: 85.6 tokens\n   - QuALITY: 87.9 tokens\n   - NarrativeQA: 85.5 tokens\n   - QASPER: 86.2 tokens\n\n4. **Avg. # of Child Nodes Per Parent**: Indicates the average number of child nodes per parent node in each dataset. The averages are:\n   - All Datasets: 6.7\n   - QuALITY: 5.7\n   - NarrativeQA: 6.8\n   - QASPER: 5.7\n\n5. **Avg. Compression Ratio (%)**: Represents the average compression ratio in percentage for each dataset. The ratios are:\n   - All Datasets: 28%\n   - QuALITY: 28%\n   - NarrativeQA: 27%\n   - QASPER: 35%"}
{"layout": 165, "type": "text", "text": "D S UM MARI Z ATION  P ROMPT ", "text_level": 1, "page_idx": 16, "bbox": [106, 369, 265, 382], "page_size": [612.0, 792.0]}
{"layout": 166, "type": "text", "text": "Table  11  shows the prompt used for sum mari z ation. ", "text_level": 1, "page_idx": 16, "bbox": [106, 395, 314, 406], "page_size": [612.0, 792.0]}
{"layout": 167, "type": "table", "page_idx": 16, "img_path": "layout_images/2401.18059v1_16.jpg", "table_caption": "Table 11: Prompt for Sum mari z ation ", "bbox": [137, 416, 473, 485], "page_size": [612.0, 792.0], "ocr_text": "Role Content\nsystem | You are a Summarizing Text Portal\nuser Write a summary of the following, including as many key details as\n\npossible: {context}:\n\n", "vlm_text": "The table has two columns: \"Role\" and \"Content.\" It has two rows:\n\n1. Under \"Role,\" it specifies \"system,\" and under \"Content,\" it states \"You are a Summarizing Text Portal.\"\n   \n2. Under \"Role,\" it specifies \"user,\" and under \"Content,\" it contains the instruction \"Write a summary of the following, including as many key details as possible: {context}.\""}
{"layout": 168, "type": "text", "text": "E H ALL UCI NATION  A NALYSIS ", "text_level": 1, "page_idx": 16, "bbox": [107, 506, 271, 519], "page_size": [612.0, 792.0]}
{"layout": 169, "type": "text", "text": "To assess the quality and accuracy of the sum mari zat ions within our RAPTOR model, we conducted an analysis focusing on hallucinations in the generated summaries. The summaries were generated by  gpt-3.5-turbo  and subsequently annotated to quantify the rates of hallucinations, to examine whether such inaccuracies propagate to parent nodes, and to evaluate their impact on question- answering (QA) tasks. ", "page_idx": 16, "bbox": [107, 531.0755615234375, 504, 586.91650390625], "page_size": [612.0, 792.0]}
{"layout": 170, "type": "text", "text": "E.1 M ETHODOLOGY ", "text_level": 1, "page_idx": 16, "bbox": [107, 599, 202, 611], "page_size": [612.0, 792.0]}
{"layout": 171, "type": "text", "text": "We randomly sampled 150 nodes across 40 stories and evaluated them for hallucinations. This sampling strategy provides a broad view of the model’s performance across different contexts. Each node was annotated by hand, and determined if it contained a hallucination. ", "page_idx": 16, "bbox": [107, 620.3865356445312, 504, 654.3095092773438], "page_size": [612.0, 792.0]}
{"layout": 172, "type": "text", "text": "E.2 F INDINGS ", "text_level": 1, "page_idx": 16, "bbox": [107, 667, 176, 678], "page_size": [612.0, 792.0]}
{"layout": 173, "type": "text", "text": "Out of the 150 nodes sampled,  $4\\%$   (6 nodes) contained some form of hallucination. Most commonly, these hallucinations originated from the model adding minor information possibly from its training data that was not present in the text being summarized, or from incorrectly extrapolating some information when creating the summary. ", "page_idx": 16, "bbox": [107, 687.779541015625, 504, 732.6614990234375], "page_size": [612.0, 792.0]}
{"layout": 174, "type": "text", "text": "Example: ", "text_level": 1, "page_idx": 17, "bbox": [107, 83, 149, 95], "page_size": [612.0, 792.0]}
{"layout": 175, "type": "text", "text": "Text of the child nodes: ", "text_level": 1, "page_idx": 17, "bbox": [107, 100, 200, 110], "page_size": [612.0, 792.0]}
{"layout": 176, "type": "text", "text": "”And you will come with me to my people? We may live here among them, and you will be a great warrior–oh, when Jor dies you may even be chief, for there is none so mighty as my warrior...”But your father will not permit it–Jor, my father, High Chief of the Galus, will not permit it, for like me you are cos-ata-lo. Oh, Co- Tan, if we but could!... Bradley noticed that she spoke in English–broken English like Co-Tan’s but equally appealing. ", "page_idx": 17, "bbox": [143, 120.41552734375, 468, 187.2153778076172], "page_size": [612.0, 792.0]}
{"layout": 177, "type": "text", "text": "Summary found in the parent of that node: ", "page_idx": 17, "bbox": [108, 196.0704345703125, 277.2644958496094, 207.9657745361328], "page_size": [612.0, 792.0]}
{"layout": 178, "type": "text", "text": "The protagonist, Bradley, is being asked by Co-Tan to stay with her people and become a great warrior, but he refuses and must return to his own country. Tom Billings of Santa Monica arrives and tells them he came to search for a man named Bowen J. Tyler, Jr. Ajor, Co-Tan’s sister, is excited about the possibility of going to Tom’s country to see strange and wonderful things... ", "page_idx": 17, "bbox": [143, 216.930419921875, 468, 272.77032470703125], "page_size": [612.0, 792.0]}
{"layout": 179, "type": "text", "text": "The hallucination here is that the summary states that Jr. Ajor and Co-Tan are sisters, but does not explicitly mention or imply this. ", "page_idx": 17, "bbox": [108, 281.6253662109375, 504, 304.58929443359375], "page_size": [612.0, 792.0]}
{"layout": 180, "type": "text", "text": "Upon reviewing all parent nodes, we found that hallucinations did not propagate to higher layers. Generally, the hallucinations were minor and did not alter the thematic interpretation of the text. ", "page_idx": 17, "bbox": [108, 309.5213623046875, 504, 332.48529052734375], "page_size": [612.0, 792.0]}
{"layout": 181, "type": "text", "text": "E.3 I MPACT ON  QA T ASKS ", "text_level": 1, "page_idx": 17, "bbox": [107, 346, 229, 356], "page_size": [612.0, 792.0]}
{"layout": 182, "type": "text", "text": "In our findings, hallucinations had no discernible impact on the performance of QA tasks. This sug- gests that hallucination is not a major concerns for the sum mari z ation component in our RAPTOR architecture. ", "page_idx": 17, "bbox": [108, 365.9813537597656, 504, 399.904296875], "page_size": [612.0, 792.0]}
{"layout": 183, "type": "text", "text": "F P SEUDOCODE FOR  R ETRIEVAL  M ETHODS ", "text_level": 1, "page_idx": 17, "bbox": [106, 416, 339, 429], "page_size": [612.0, 792.0]}
{"layout": 184, "type": "table", "page_idx": 17, "img_path": "layout_images/2401.18059v1_17.jpg", "bbox": [105, 441, 506, 605], "page_size": [612.0, 792.0], "ocr_text": "Algorithm 1 Tree Traversal Algorithm\n\nfunction TRAVERSETREE(tree, query, k)\nScurren < tree.layer(0]\nfor layer in range(tree.num_layers) do\ntop, < |]\nfor node in Scurrent dO\nscore + dot_product(query, node)\ntop_k.append((node, score) )\nend for\nStayer < Sorted(top_k)[:k].nodes\nScurrent oa Stayer\nend for\nreturn Sp US; US2U...US;\nend function\n", "vlm_text": "The table contains a pseudocode listing for an algorithm titled \"Algorithm 1: Tree Traversal Algorithm.\" This algorithm is designed to traverse a tree structure. Here is a brief explanation of the pseudocode:\n\n1. **Function Definition**: The function `TRAVERSETREE` takes three arguments: a `tree`, a `query`, and a parameter `k`.\n\n2. **Initialization**: \n   - `S_current` is initialized to the first layer of the tree using `tree.layer[0]`.\n\n3. **Loop through Tree Layers**: \n   - A `for` loop iterates over the number of layers in the tree using `range(tree.num_layers)`.\n\n4. **Processing Nodes in Current Layer**:\n   - An empty list `top_k` is initialized to keep track of the top-k nodes in each layer.\n   - Another loop iterates over each `node` in `S_current`, computing a `score` using the `dot_product` function with `query` and `node` as inputs.\n   - Each node along with its computed score is appended to the `top_k` list as a tuple `(node, score)`.\n\n5. **Select Top-k Nodes**:\n   - `S_layer` is assigned the top-k nodes from `sorted(top_k)[:k].nodes`, where `top_k` is sorted based on scores.\n\n6. **Update Current Layer**:\n   - `S_current` is updated to be the current set of selected top-k nodes, `S_layer`.\n\n7. **Return Statement**:\n   - The function returns the union of sets from all the layers traversed (`S_0 ∪ S_1 ∪ S_2 ... ∪ S_k`).\n\n8. **End of Function**: The function is concluded with an `end function` statement. \n\nThis algorithm likely performs a top-k selection of nodes at each layer of the tree according to some scoring system derived from the `dot_product` of nodes and a query, effectively filtering the tree traversed by prioritizing nodes with the highest scores."}
{"layout": 185, "type": "text", "text": "G Q UALITATIVE  A NALYSIS ", "text_level": 1, "page_idx": 17, "bbox": [107, 631, 255, 642], "page_size": [612.0, 792.0]}
{"layout": 186, "type": "text", "text": "To qualitatively examine RAPTOR’s retrieval process, we test it on thematic, multi-hop questions about a 1500-word version of the fairytale Cinderella. We compare the context retrieved by RAP- TOR with the context retrieved by Dense Passage Retrieval (DPR). Figure  4  in the main paper details the retrieval process within RAPTOR’s tree structure for two questions. The nodes that RAPTOR selects for each question are highlighted, while the leaf nodes that DPR selects for the same question are indicated with arrows. This comparison illustrates the advantage of RAPTOR’s tree structure. RAPTOR selects nodes from different layers depending on the level of granularity required by the ", "page_idx": 17, "bbox": [108, 654.902587890625, 504, 732.6614990234375], "page_size": [612.0, 792.0]}
{"layout": 187, "type": "table", "page_idx": 18, "img_path": "layout_images/2401.18059v1_18.jpg", "bbox": [108, 86, 507, 289], "page_size": [612.0, 792.0], "ocr_text": "Algorithm 2 Coiapsed tree Algorithm:\n\nfunction COLLAPSEDTREK(tree, query, k, max_tokens)\ntree + flatten(tree) > Flatten tree into 1D\ntop_nodes < |]\nfor node in tree do\ntop_nodes.append((node, dot_product(query, node))\nend for\ntop_nodes + sorted(top_nodes)\nresult < []\ntotal_tokens < 0\nfor node in top_nodes do\nif total_tokens + node.token_size < max-_tokens then\nresult.append(node)\nend if\ntotal_tokens < total_tokens + node.token_size\nend for\nreturn result\nend function\n\n", "vlm_text": "The table contains pseudocode for an algorithm labeled as \"Algorithm 2: Collapsed Tree Algorithm\". The function `COLLAPSEDTREE` is designed to process a data structure, presumably a tree, based on several parameters: `tree`, `query`, `k`, and `max_tokens`. Here's a breakdown of the algorithm:\n\n1. **Flatten the Tree**: The tree is first flattened into a one-dimensional structure using a function `flatten(tree)`.\n\n2. **Calculate Dot Product**: For each node in the flattened tree, a dot product between the `query` and the `node` is computed and stored in a list `top_nodes` along with the node itself.\n\n3. **Sorting**: The `top_nodes` list is sorted, although it is not specified what it is sorted by.\n\n4. **Initializing Result and Token Count**: The algorithm initializes an empty list `result` and sets `total_tokens` to zero.\n\n5. **Iterating Over Top Nodes**: It then iterates over each node in the sorted `top_nodes` list and checks if adding the current node's token size to `total_tokens` would exceed `max_tokens`.\n\n   - If not, the node is appended to the `result` list.\n   - The `total_tokens` is updated by adding the current node's token size.\n\n6. **Return Result**: The function finally returns the `result` list, which presumably contains a subset of nodes from the original tree that maximize some criteria while keeping within a token limit.\n\nThe algorithm essentially collapses a tree into a prioritized list of nodes based on their relevance to a query, constrained by a token limit."}
{"layout": 188, "type": "table", "page_idx": 18, "img_path": "layout_images/2401.18059v1_19.jpg", "table_caption": "Table 12: Relevant excerpts from text retrieved by RAPTOR and DPR for the questions on the fairytale Cinderella. ", "bbox": [117, 298, 495, 604], "page_size": [612.0, 792.0], "ocr_text": "Question: What is the central theme of the story?\n\nRAPTOR\n\nFairy Godmother helps Cinderella attend a ball by transforming her rags... Cinderella\nimpresses the Prince at the ball... she loses track of time and has to run home alone in\nthe darkness. The Prince is unable to find Cinderella and goes in search of her... She\nforgave her sisters, and treated them always very kindly, and the Prince had great cause\nto be glad that he had found the glass slipper.\n\nDPR\n\nTwo mice were turned into footmen; four grasshoppers into white horses. Next, the\nFairy touched Cinderella’s rags, and they became rich satin robes, trimmed with point\nlace. ... Alas! the clock had struck Eleven. She found no coach, only a pumpkin, and\nthe rat and mice ran quickly away when they saw her; while all her fine dress turned to\nrags, and she had to run home alone... They told her a beautiful Princess had been at\nthe ball, with whom the Prince was delighted. They did not know it was Cinderella.\n\nQuestion: How does Cinderella find a happy ending?\n\nRAPTOR\n\nThe story of Cinderella involves her godmother, a fairy, who transforms a pumpkin\ninto a grand coach with her wand and allows Cinderella to attend the ball. However,\nCinderella must return home before the clock strikes eleven or her dress will turn back\ninto rags... Cinderella impresses the Prince at the ball but leaves before he can find\nout who she is... The Prince searched for the owner of a lost glass slipper and found it\nbelonged to Cinderella. She forgave her sisters and the Prince was glad to have found\nher.\n\nDPR\n\nthe clock had struck Eleven... The Prince was very much surprised when he missed\nCinderella again, and leaving the ball, went in search of her... Fairy touched Cin-\nderella’s rags, and they became rich satin robes, trimmed with point lace... Her old\nshoes became a charming pair of glass slippers, which shone like diamonds. “Now go\nto the ball, my love,” she said, “‘and enjoy yourself. But remember, you must leave the\nroom before the clock strikes eleven. If you do not your dress will return to its original\ntags.”\n\n", "vlm_text": "The table compares RAPTOR and DPR responses to two questions about the story of Cinderella:\n\n1. **Question: What is the central theme of the story?**\n   - **RAPTOR**: Focuses on the Fairy Godmother helping Cinderella attend a ball, impressing the Prince, losing track of time, and the Prince searching for her. It highlights forgiveness and the finding of the glass slipper.\n   - **DPR**: Describes the magical transformation of animals and objects, Cinderella's attire change, and her need to leave by eleven. It mentions the Prince meeting a mysterious Princess at the ball, unaware it was Cinderella.\n\n2. **Question: How does Cinderella find a happy ending?**\n   - **RAPTOR**: Details the transformation magic, Cinderella impressing the Prince, her need to leave by eleven, and the Prince finding her through the glass slipper. It mentions her forgiveness.\n   - **DPR**: Focuses on the Prince searching for Cinderella, the Fairy transforming her rags back into a gown, and the creation of the glass slippers. It emphasizes the necessity to leave by eleven to maintain the magic."}
{"layout": 189, "type": "text", "text": "question at hand. Further, the information that would be retrieved by DPR is more often than not included in the context retrieved by RAPTOR, either directly as a leaf node or indirectly as part of a summary from a higher layer. ", "page_idx": 18, "bbox": [107, 627.007568359375, 504, 660.9304809570312], "page_size": [612.0, 792.0]}
{"layout": 190, "type": "text", "text": "”The first question we examine is “How does Cinderella find a happy ending?”, a multi-hop question best answered by synthesizing information from various text segments. To control for the language model’s potential familiarity with the Cinderella story, we instructed it to rely solely on the retrieved information for its answers. Table  13  shows the text retrieved by both RAPTOR and DPR for this question. RAPTOR’s context succinctly describes Cinderella’s journey to happiness, while DPR’s leaf nodes primarily focus on her initial transformation. The difference in retrieved information significantly impacts downstream tasks. When GPT-4 is provided with RAPTOR’s context, it gen- erates a detailed answer: “Cinderella finds a happy ending when the Prince searches for the owner of the lost glass slipper and discovers it belongs to Cinderella. They eventually marry, transform- ing Cinderella’s life for the better.” In contrast, using DPR’s context, GPT-4 states: “Based on the given context, it is not possible to determine how Cinderella finds a happy ending, as the text lacks information about the story’s conclusion.” ", "page_idx": 18, "bbox": [107, 665.861572265625, 504, 732.6614990234375], "page_size": [612.0, 792.0]}
{"layout": 191, "type": "text", "text": "", "page_idx": 19, "bbox": [108, 82.6185302734375, 504, 149.4183807373047], "page_size": [612.0, 792.0]}
{"layout": 192, "type": "text", "text": "The second question we examine is “What is the central theme of the story?”, a thematic question that requires holistic understanding of the entire text. The text retrieved by RAPTOR and DPR for this question is shown in Table  13 . The text retrieved by RAPTOR contains short descriptions of all the major parts of the story, whereas the text retrieved by DPR contains detailed descriptions of a narrow subset of the story. Again, the difference in retrieval mechanisms affects the performance of GPT-4 when answering the question. Given DPR’s context, it outputs “The central theme of the story is transformation and the power of inner beauty, as Cinderella, a kind and humble girl, is magically transformed into a beautiful princess, capturing the attention and admiration of the Prince and others at the ball.” This answer only takes into account the first portion of the story, up until Cinderella first meets the prince. In contrast, given RAPTOR’s context, GPT-4 outputs “The central theme of the story is transformation and overcoming adversity, as Cinderella, with the help of her Fairy Godmother, transforms from a mistreated and downtrodden girl into a beautiful and confident young woman who ultimately finds happiness and love with the Prince.” This is a more complete answer, demonstrating a comprehensive understanding of the story. ", "page_idx": 19, "bbox": [108, 154.34942626953125, 504, 308.8201904296875], "page_size": [612.0, 792.0]}
{"layout": 193, "type": "text", "text": "This qualitative analysis indicates that RAPTOR outperforms prior retrieval mechanisms because the information that it retrieves is more relevant and exhaustive, allowing for better performance on downstream tasks. ", "page_idx": 19, "bbox": [108, 313.75225830078125, 504, 347.6741943359375], "page_size": [612.0, 792.0]}
{"layout": 194, "type": "text", "text": "We also created a 2600-word story along with questions about its narrative and theme. An excerpt from the story is present below and the full PDF of this story is linked  here . For questions like “What is the central theme of the story?”, an upper-level node is retrieved which includes the sentence: “This story is about the power of human connection... inspiring and uplifting each other as they pursued their passions.” This summary, not explicitly present in the original text, almost directly answers the question. ", "page_idx": 19, "bbox": [108, 352.60626220703125, 504, 419.4051513671875], "page_size": [612.0, 792.0]}
{"layout": 195, "type": "text", "text": "Excerpt from ”The Eager Writer”: ", "text_level": 1, "page_idx": 19, "bbox": [107, 425, 256, 435], "page_size": [612.0, 792.0]}
{"layout": 196, "type": "text", "text": "”Ethan’s passion for writing had always been a part of him. As a child, he would often scribble stories and poems in his notebook, and as he grew older, his love for writing only intensified. His evenings were often spent in the dim light of his room, typing away at his laptop. He had recently taken a job as a content writer for an online marketing firm to pay the bills, but his heart still longed for the world of storytelling. However, like many aspiring writers, he struggled to find a foothold in the industry. He took a job as a content writer for an online marketing firm, but it was growing increasingly evident to him that this was not the path he wanted to pursue. It was during this time that he stumbled upon the Pathways app. The app offered a platform for people in similar professions to connect and share knowledge, and he saw it as an opportunity to finally connect with others who shared his passion for writing. Ethan saw an opportunity to meet others who shared his passion and could offer guidance and mentorship. He quickly signed up and was surprised by the number of writers he found on the platform, from well establish professionals to beginners just starting out in the business.” ", "page_idx": 19, "bbox": [143, 447.0442199707031, 468, 612.4740600585938], "page_size": [612.0, 792.0]}
{"layout": 197, "type": "text", "text": "H N ARRATIVE QA E VALUATION  S CRIPT ", "text_level": 1, "page_idx": 19, "bbox": [107, 628, 318, 642], "page_size": [612.0, 792.0]}
{"layout": 198, "type": "text", "text": "We made several modifications to AllenNLP’s evaluation script 3   to better fit our evaluation needs: ", "page_idx": 19, "bbox": [108, 651.3011474609375, 498.01446533203125, 666.9210815429688], "page_size": [612.0, 792.0]}
{"layout": 199, "type": "text", "text": "•  Added Smoothing:  Smoothing was incorporated to handle cases where BLEU score is zero, due to no n-gram matches occurring in the reference text. A BLEU score of zero skews the results, leading to an overly harsh evaluation for rare or novel phrases. By adding ", "page_idx": 19, "bbox": [135.3970489501953, 677.2655029296875, 504, 711.5470581054688], "page_size": [612.0, 792.0]}
{"layout": 200, "type": "text", "text": "a smoothing function, we prevent the BLEU scores from dropping to zero, providing a more fair evaluation. ", "page_idx": 20, "bbox": [143, 82.6185302734375, 504, 105.58245086669922], "page_size": [612.0, 792.0]}
{"layout": 201, "type": "text", "text": "•  Modified BLEU-4 Weighting:  The original script applied a weight of 1 to the highest order n-gram (4-gram) and 0 to the rest in its BLEU-4 calculation (i.e., weights=(0, 0, 0, 1)). This approach may overly focus on 4-gram matches while neglecting lower-order matches. To provide a more balanced evaluation, we evenly distributed the weight across all n-gram levels, changing the weights for the BLEU-4 calculation to (0.25, 0.25, 0.25, 0.25). •  Token iz ation before Mapping in METEOR Calculation:  The original script utilized a simple split and map method for METEOR calculation. We fixed this by first tokenizing the text and then mapping the tokens. This amendment improves the accuracy of the METEOR calculation by taking into account the correct linguistic boundaries of words. ", "page_idx": 20, "bbox": [135, 107.88286590576172, 504, 222.5803680419922], "page_size": [612.0, 792.0]}
{"layout": 202, "type": "table", "page_idx": 20, "img_path": "layout_images/2401.18059v1_20.jpg", "table_footnote": "Table 13: Relevant excerpts from text retrieved by RAPTOR and DPR for the questions on the fairytale Cinderella. ", "bbox": [116, 236, 494, 541], "page_size": [612.0, 792.0], "ocr_text": "Question: What is the central theme of the story?\n\nRAPTOR\n\nFairy Godmother helps Cinderella attend a ball by transforming her rags... Cinderella\nimpresses the Prince at the ball... she loses track of time and has to run home alone in\nthe darkness. The Prince is unable to find Cinderella and goes in search of her... She\nforgave her sisters, and treated them always very kindly, and the Prince had great cause\nto be glad that he had found the glass slipper.\n\nDPR\n\nTwo mice were turned into footmen; four grasshoppers into white horses. Next, the\nFairy touched Cinderella’s rags, and they became rich satin robes, trimmed with point\nlace. ... Alas! the clock had struck Eleven. She found no coach, only a pumpkin, and\nthe rat and mice ran quickly away when they saw her; while all her fine dress turned to\nrags, and she had to run home alone... They told her a beautiful Princess had been at\nthe ball, with whom the Prince was delighted. They did not know it was Cinderella.\n\nQuestion: How does Cinderella find a happy ending?\n\nRAPTOR\n\nThe story of Cinderella involves her godmother, a fairy, who transforms a pumpkin\ninto a grand coach with her wand and allows Cinderella to attend the ball. However,\nCinderella must return home before the clock strikes eleven or her dress will turn back\ninto rags... Cinderella impresses the Prince at the ball but leaves before he can find\nout who she is... The Prince searched for the owner of a lost glass slipper and found it\nbelonged to Cinderella. She forgave her sisters and the Prince was glad to have found\nher.\n\nDPR\n\nthe clock had struck Eleven... The Prince was very much surprised when he missed\nCinderella again, and leaving the ball, went in search of her... Fairy touched Cin-\nderella’s rags, and they became rich satin robes, trimmed with point lace... Her old\nshoes became a charming pair of glass slippers, which shone like diamonds. “Now go\nto the ball, my love,” she said, “and enjoy yourself. But remember, you must leave the\nroom before the clock strikes eleven. If you do not your dress will return to its original\ntags.”\n\n", "vlm_text": "The table contains responses from RAPTOR and DPR to two questions about the Cinderella story. \n\n1. **Question: What is the central theme of the story?**\n   - **RAPTOR:** Focuses on the transformation and the search for Cinderella by the Prince, emphasizing forgiveness and the glass slipper.\n   - **DPR:** Highlights the magical transformation and Cinderella's quick escape as her dress returns to rags, ending with the Prince's delight in finding the glass slipper's owner.\n\n2. **Question: How does Cinderella find a happy ending?**\n   - **RAPTOR:** Describes the fairy godmother's role and the Prince's search for Cinderella, leading to forgiveness and a reunion.\n   - **DPR:** Details the magical transformations and Cinderella's return to her original state, emphasizing the Prince's quest to find her after being charmed at the ball."}
{"layout": 203, "type": "text", "text": "I A NALYSIS OF  D IFFERENT  L AYERS ON  RAPTOR’ S  P ERFORMANCE ", "text_level": 1, "page_idx": 20, "bbox": [106, 562, 463, 575], "page_size": [612.0, 792.0]}
{"layout": 204, "type": "text", "text": "I.1 H OW DO DIFFERENT  L AYERS IMPACT PERFORMANCE  ? ", "page_idx": 20, "bbox": [108, 587.3755493164062, 361.576416015625, 599.3804931640625], "page_size": [612.0, 792.0]}
{"layout": 205, "type": "text", "text": "In this section, we present a detailed breakdown of RAPTOR’s retrieval performance when querying different layers of the hierarchical tree structure for various stories. These tables validate the utility of RAPTOR’s multi-layered structure for diverse query requirements. ", "page_idx": 20, "bbox": [108, 607.8985595703125, 504, 641.8214721679688], "page_size": [612.0, 792.0]}
{"layout": 206, "type": "text", "text": "Table 14: Performance of RAPTOR when querying different layers of the tree for Story 2. ", "page_idx": 20, "bbox": [126.33501434326172, 651.03857421875, 485.6659851074219, 663.0435180664062], "page_size": [612.0, 792.0]}
{"layout": 207, "type": "table", "page_idx": 20, "img_path": "layout_images/2401.18059v1_21.jpg", "bbox": [154, 672, 455, 725], "page_size": [612.0, 792.0], "ocr_text": "Layers Queried / Start Layer Layer 0 (Leaf Nodes) Layer1 Layer 2\n\n1 layer 58.8 47.1 41.1\n\n2 layers - 64.7 52.9\n3 layers - - 47.1\n\n", "vlm_text": "The table presents data on a multi-layered structure with three rows and four columns. The columns represent \"Layers Queried / Start Layer,\" \"Layer 0 (Leaf Nodes),\" \"Layer 1,\" and \"Layer 2.\" The rows are labeled as \"1 layer,\" \"2 layers,\" and \"3 layers,\" likely indicating the number of layers queried or starting layers in some hierarchy or system.\n\n- For \"1 layer,\" the values are 58.8 for Layer 0, 47.1 for Layer 1, and 41.1 for Layer 2.\n- For \"2 layers,\" no value is given for Layer 0, but Layer 1 has a value of 64.7, and Layer 2 is at 52.9.\n- For \"3 layers,\" no values are given for Layer 0 and Layer 1, but Layer 2 is at 47.1.\n\nThis table seems to depict some kind of measurement or metric across different layers of a structure, where Layer 0 is referred to as \"Leaf Nodes.\" The values could represent percentages, scores, or any other relevant metric, but the table does not provide further context on what these numbers specifically represent."}
{"layout": 208, "type": "image", "page_idx": 21, "img_path": "layout_images/2401.18059v1_22.jpg", "bbox": [149, 80, 448, 182], "page_size": [612.0, 792.0], "ocr_text": "°\n\nNarrativeQA\nee seer\nce os\nom DPR\n\nPercentage\n\nEt)\n\n&\n\n8\n\nQuality\n\nam\nlayer Layer Layer2 Layers Layer@\nLayer\n\ncee SoERT\nsem ones\nme Der\n\nTayer 2\n\nQasper\n\n", "vlm_text": "The image displays three bar charts comparing the performance of three different models (SBERT, BM25, and DPR) across various tasks: NarrativeQA, Quality, and Qasper. Each chart shows the percentage of some metric across different layers (Layer 0, Layer 1, etc.). The colors represent different models: light blue for SBERT, green for BM25, and red for DPR. The percentage values decrease as the layer number increases across all tasks."}
{"layout": 209, "type": "text", "text": "Figure 7: Histogram showing the percentage of nodes retrieved from different layers of the RAPTOR tree across three datasets (Narrative QA, Quality, and Qasper) using three retrievers (SBERT, BM25, and DPR). The data indicate that a substantial portion of the nodes contributing to the final retrieval comes from non-leaf layers, with a notable percentage from the first and second layers, highlighting the importance of RAPTOR’s hierarchical sum mari z ation in the retrieval process. ", "page_idx": 21, "bbox": [108, 193.62054443359375, 504, 249.46043395996094], "page_size": [612.0, 792.0]}
{"layout": 210, "type": "table", "page_idx": 21, "img_path": "layout_images/2401.18059v1_23.jpg", "table_caption": "Table 15: Performance of RAPTOR when querying different layers of the tree for Story 3. ", "bbox": [125, 260, 485, 334], "page_size": [612.0, 792.0], "ocr_text": "Layers Queried / Start Layer Layer 0 (Leaf Nodes) Layer1 Layer 2\n\n1 layer 66.6 61.1 61.1\n2 layers - 66.6 66.6\n3 layers - - 83.3\n\n", "vlm_text": "The table presents data relating to three different queries or layers with corresponding values across three different columns labeled as Layer 0 (Leaf Nodes), Layer 1, and Layer 2. Here's a breakdown of the table contents:\n\n1. The first column is labeled \"Layers Queried / Start Layer\" and lists the different scenarios of the layers queried:\n   - 1 layer\n   - 2 layers\n   - 3 layers\n\n2. The second column, \"Layer 0 (Leaf Nodes),\" provides values for each of the scenarios:\n   - For \"1 layer,\" the value is 66.6.\n   - There is no value (indicated by a dash) for \"2 layers\" and \"3 layers.\"\n\n3. The third column, \"Layer 1,\" presents values as follows:\n   - For \"1 layer,\" the value is 61.1.\n   - For \"2 layers,\" the value is 66.6.\n   - There is no value (indicated by a dash) for \"3 layers.\"\n\n4. The final column, \"Layer 2,\" shows:\n   - For \"1 layer,\" the value is 61.1.\n   - For \"2 layers,\" the value is 66.6.\n   - For \"3 layers,\" the value is 83.3, which is also bolded for emphasis.\n\nThe table seems to depict performance metrics or results specific to each layer query scenario across the three layers mentioned."}
{"layout": 211, "type": "table", "page_idx": 21, "img_path": "layout_images/2401.18059v1_24.jpg", "table_caption": "Table 16: Performance of RAPTOR when querying different layers of the tree for Story 4. ", "bbox": [125, 344, 486, 407], "page_size": [612.0, 792.0], "ocr_text": "Layers Queried / Start Layer Layer 0 (Leaf Nodes) Layer 1\n\n1 layer 94.7 84.2\n2 layers = 89.4\n\n", "vlm_text": "The table shows data related to layers queried or start layers across two main columns: \"Layer 0 (Leaf Nodes)\" and \"Layer 1\". It contains two rows that describe:\n\n1. For \"1 layer\" queried or started:\n   - The value in \"Layer 0 (Leaf Nodes)\" is 94.7.\n   - The value in \"Layer 1\" is 84.2.\n\n2. For \"2 layers\" queried or started:\n   - There is no value for \"Layer 0 (Leaf Nodes)\" (indicated by '-').\n   - The value in \"Layer 1\" is 89.4."}
{"layout": 212, "type": "table", "page_idx": 21, "img_path": "layout_images/2401.18059v1_25.jpg", "table_caption": "Table 17: Performance of RAPTOR when querying different layers of the tree for Story 5. ", "bbox": [125, 418, 486, 482], "page_size": [612.0, 792.0], "ocr_text": "Layers Queried / Start Layer Layer 0 (Leaf Nodes) Layer 1\n\n1 layer 57.9 47.3\n2 layers - 68.4\n", "vlm_text": "The table shows data for different layers queried or start layers, with respective values for \"Layer 0 (Leaf Nodes)\" and \"Layer 1\":\n\n- For \"1 layer\":\n  - Layer 0 (Leaf Nodes): 57.9\n  - Layer 1: 47.3\n  \n- For \"2 layers\":\n  - Layer 0 (Leaf Nodes): Not applicable or missing\n  - Layer 1: 68.4 (in bold)"}
{"layout": 213, "type": "text", "text": "I.2 W HICH  L AYERS DO  R ETRIEVED  N ODES COME FROM  ? ", "text_level": 1, "page_idx": 21, "bbox": [107, 502, 361, 514], "page_size": [612.0, 792.0]}
{"layout": 214, "type": "text", "text": "We further conduct an ablation study across all three datasets and across three different retrievers with RAPTOR with the collapsed tree retrieval to examine the layers from which the retrieved nodes originate. We observe that between  $18.5\\%$   to  $57\\%$   of the retrieved nodes come from non-leaf nodes. As illustrated in Figure  7 , the retrieval pattern across layers reveals the importance of RAPTOR’s multi-layered tree structure. Notably, a significant percentage of the nodes retrieved by RAPTOR using the DPR retriever for the Narrative QA dataset come from the first and second layers of the tree, as opposed to the leaf nodes. This pattern is consistent across the other datasets and retrievers, albeit with varying percentages. ", "page_idx": 21, "bbox": [108, 522.7105712890625, 504, 611.4285278320312], "page_size": [612.0, 792.0]}
{"layout": 215, "type": "text", "text": "Table 18: Percentage of nodes from non-leaf nodes across different datasets and retrievers ", "page_idx": 21, "bbox": [126.87401580810547, 621.20654296875, 485.12908935546875, 633.2114868164062], "page_size": [612.0, 792.0]}
{"layout": 216, "type": "table", "page_idx": 21, "img_path": "layout_images/2401.18059v1_26.jpg", "bbox": [209, 642, 400, 698], "page_size": [612.0, 792.0], "ocr_text": "Dataset DPR SBERT BM25\nNarrativeQA 57.36% 36.78% 34.96%\nQuality 32.28% 24.41% 32.36%\nQasper 22.93% 18.49% 22.76%\n\n", "vlm_text": "The table presents the performance of three different information retrieval models—DPR, SBERT, and BM25—across three datasets: NarrativeQA, Quality, and Qasper. The values in the table represent the performance percentages of each model on each dataset. Here is the detailed breakdown:\n\n- For the NarrativeQA dataset:\n  - DPR has a performance of 57.36%\n  - SBERT has a performance of 36.78%\n  - BM25 has a performance of 34.96%\n\n- For the Quality dataset:\n  - DPR has a performance of 32.28%\n  - SBERT has a performance of 24.41%\n  - BM25 has a performance of 32.36%\n\n- For the Qasper dataset:\n  - DPR has a performance of 22.93%\n  - SBERT has a performance of 18.49%\n  - BM25 has a performance of 22.76%\n\nOverall, DPR tends to have the highest performance across the datasets compared to SBERT and BM25."}
{"layout": 217, "type": "text", "text": "Table 19: Percentage of nodes from different layers with DPR as the retriever ", "page_idx": 22, "bbox": [151.76499938964844, 140.43658447265625, 460.2369384765625, 152.44151306152344], "page_size": [612.0, 792.0]}
{"layout": 218, "type": "table", "page_idx": 22, "img_path": "layout_images/2401.18059v1_27.jpg", "bbox": [210, 162, 401, 239], "page_size": [612.0, 792.0], "ocr_text": "Layer NarrativeQA Quality Qasper\n\n42.64% 67.71% 77.07%\n45.00% 29.43% 21.88%\n10.57% 2.85% 1.05%\n1.78% - -\n0.003% - -\n\nBRWNrRO\n", "vlm_text": "The table presents data related to three categories: NarrativeQA, Quality, and Qasper, across different layers (0 to 4). Here are the details:\n\n- **Layer 0:** \n  - NarrativeQA: 42.64%\n  - Quality: 67.71%\n  - Qasper: 77.07%\n\n- **Layer 1:** \n  - NarrativeQA: 45.00%\n  - Quality: 29.43%\n  - Qasper: 21.88%\n\n- **Layer 2:** \n  - NarrativeQA: 10.57%\n  - Quality: 2.85%\n  - Qasper: 1.05%\n\n- **Layer 3:** \n  - NarrativeQA: 1.78%\n  - Quality: Not provided\n  - Qasper: Not provided\n\n- **Layer 4:**\n  - NarrativeQA: 0.003%\n  - Quality: Not provided\n  - Qasper: Not provided\n\nSome data is missing for layers 3 and 4 in the Quality and Qasper columns."}
{"layout": 219, "type": "text", "text": "Table 20: Percentage of nodes from different layers with SBERT as the retriever ", "page_idx": 22, "bbox": [146.25100708007812, 366.3985595703125, 465.7515869140625, 378.40350341796875], "page_size": [612.0, 792.0]}
{"layout": 220, "type": "table", "page_idx": 22, "img_path": "layout_images/2401.18059v1_28.jpg", "bbox": [209, 387, 400, 455], "page_size": [612.0, 792.0], "ocr_text": "Layer NarrativeQA Quality Qasper\n0 63.22% 75.59% 81.51%\n1 31.51% 22.78% 17.84%\n2 4.85% 1.63% 0.65%\n3 0.42% - -\n\n", "vlm_text": "The table presents data across four layers (0 to 3) for three categories: NarrativeQA, Quality, and Qasper. \n\n- **Layer 0**:\n  - NarrativeQA: 63.22%\n  - Quality: 75.59%\n  - Qasper: 81.51%\n\n- **Layer 1**:\n  - NarrativeQA: 31.51%\n  - Quality: 22.78%\n  - Qasper: 17.84%\n\n- **Layer 2**:\n  - NarrativeQA: 4.85%\n  - Quality: 1.63%\n  - Qasper: 0.65%\n\n- **Layer 3**:\n  - NarrativeQA: 0.42%\n  - Quality: No data\n  - Qasper: No data"}
{"layout": 221, "type": "text", "text": "Table 21: Percentage of nodes from different layers with BM25 as the retriever ", "text_level": 1, "page_idx": 22, "bbox": [147, 582, 465, 593], "page_size": [612.0, 792.0]}
{"layout": 222, "type": "table", "page_idx": 22, "img_path": "layout_images/2401.18059v1_29.jpg", "bbox": [210, 603, 400, 670], "page_size": [612.0, 792.0], "ocr_text": "Layer NarrativeQA Quality Qasper\n0 65.04% 67.64% 77.24%\n1 28.79% 28.85% 21.57%\n2 5.36% 3.51% 1.19%\n3 0.81% - -\n\n", "vlm_text": "The table presents percentages for three datasets (\"NarrativeQA,\" \"Quality,\" and \"Qasper\") across different layers (0, 1, 2, and 3):\n\n- **Layer 0**:\n  - NarrativeQA: 65.04%\n  - Quality: 67.64%\n  - Qasper: 77.24%\n\n- **Layer 1**:\n  - NarrativeQA: 28.79%\n  - Quality: 28.85%\n  - Qasper: 21.57%\n\n- **Layer 2**:\n  - NarrativeQA: 5.36%\n  - Quality: 3.51%\n  - Qasper: 1.19%\n\n- **Layer 3**:\n  - NarrativeQA: 0.81%\n  - Quality: Not available\n  - Qasper: Not available"}
