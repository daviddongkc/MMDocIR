{"page": 0, "image_path": "doc_images/2210.02442v1_0.jpg", "ocr_text": "arXiv:2210.02442v1 [cs.CV] 5 Oct 2022\n\nMaking Your First Choice: To Address\nCold Start Problem in Vision Active Learning\n\nLiangyu Chen! Yutong Bai? Siyu Huang*> —Yongyi Lu’\nBihan Wen! Alan L. Yuille? Zongwei Zhou?*\n'Nanyang Technological University Johns Hopkins University ?Harvard University\n\nAbstract\n\nActive learning promises to improve annotation efficiency by iteratively selecting\nthe most important data to be annotated first. However, we uncover a striking\ncontradiction to this promise: active learning fails to select data as efficiently\nas random selection at the first few choices. We identify this as the cold start\nproblem in vision active learning, caused by a biased and outlier initial query. This\npaper seeks to address the cold start problem by exploiting the three advantages of\ncontrastive learning: (1) no annotation is required; (2) label diversity is ensured\nby pseudo-labels to mitigate bias; (3) typical data is determined by contrastive\nfeatures to reduce outliers. Experiments are conducted on CIFAR-10-LT and three\nmedical imaging datasets (i.e. Colon Pathology, Abdominal CT, and Blood Cell\nMicroscope). Our initial query not only significantly outperforms existing active\nquerying strategies but also surpasses random selection by a large margin. We\nforesee our solution to the cold start problem as a simple yet strong baseline to\nchoose the initial query for vision active learning.\n\nCode is available: https://github.com/c-liangyu/CS VAL\n\n1 Introduction\n\n“The secret of getting ahead is getting started.”\n— Mark Twain\n\nThe cold start problem was initially found in recommender systems [56, 39, 9, 23] when algorithms\nhad not gathered sufficient information about users with no purchase history. It also occurred in many\nother fields, such as natural language processing [55, 33] and computer vision [5, 11, 38] during the\nactive learning procedure!. Active learning promises to improve annotation efficiency by iteratively\nselecting the most important data to annotate. However, we uncover a striking contradiction to this\npromise: Active learning fails to select data as effectively as random selection at the first choice. We\nidentify this as the cold start problem in vision active learning and illustrate the problem using three\nmedical imaging applications (Figure la—c) as well as a natural imaging application (Figure 1d). Cold\nstart is a crucial topic [54, 30] because a performant initial query can lead to noticeably improved\nsubsequent cycle performance in the active learning procedure, evidenced in §3.3. There is a lack\nof studies that systematically illustrate the cold start problem, investigate its causes, and provide\npractical solutions to address it. To this end, we ask: What causes the cold start problem and how\ncan we Select the initial query when there is no labeled data available?\n\n“Corresponding author: Zongwei Zhou (zzhou82 @jh.edu)\n\n‘Active learning aims to select the most important data from the unlabeled dataset and query human experts\nto annotate new data. The newly annotated data is then added to improve the model. This process can be repeated\nuntil the model reaches a satisfactory performance level or the annotation budget is exhausted.\n\nPreprint. Under review.\n", "vlm_text": "Making Your First Choice: To Address Cold Start Problem in Vision Active Learning \nLiangyu Chen 1 Yutong Bai 2 Siyu Huang 3 Yongyi Lu 2 Bihan Wen 1 Alan L. Yuille 2 Zongwei Zhou ∗ 1 Nanyang Technological University 2 Johns Hopkins University 3 Harvard University \nAbstract \nActive learning promises to improve annotation efﬁciency by iterative ly selecting the most important data to be annotated ﬁrst. However, we uncover a striking contradiction to this promise: active learning fails to select data as efﬁciently as random selection at the ﬁrst few choices. We identify this as the cold start problem in vision active learning, caused by a biased and outlier initial query. This paper seeks to address the cold start problem by exploiting the three advantages of contrastive learning: (1) no annotation is required; (2) label diversity is ensured by pseudo-labels to mitigate bias; (3) typical data is determined by contrastive features to reduce outliers. Experiments are conducted on CIFAR-10-LT and three medical imaging datasets ( i.e . Colon Pathology, Abdominal CT, and Blood Cell Microscope). Our initial query not only sign i cant ly outperforms existing active querying strategies but also surpasses random selection by a large margin. We foresee our solution to the cold start problem as a simple yet strong baseline to choose the initial query for vision active learning. \nCode is available: https://github.com/c-liangyu/CSVAL \n1 Introduction \n“ The secret of getting ahead is getting started. \n— Mark Twain \nThe cold start problem was initially found in recommend er systems [ 56 ,  39 ,  9 ,  23 ] when algorithms had not gathered sufﬁcient information about users with no purchase history. It also occurred in many other ﬁelds, such as natural language processing [ 55 ,  33 ] and computer vision [ 5 ,  11 ,  38 ] during the active learning procedure 1 . Active learning promises to improve annotation efﬁciency by iterative ly selecting the most important data to annotate. However, we uncover a striking contradiction to this promise: Active learning fails to select data as effectively as random selection at the ﬁrst choice. We identify this as the cold start problem in vision active learning and illustrate the problem using three medical imaging applications (Figure 1a–c) as well as a natural imaging application (Figure 1d). Cold start is a crucial topic [ 54 ,  30 ] because a performant initial query can lead to noticeably improved subsequent cycle performance in the active learning procedure, evidenced in   $\\S3.3$  . There is a lack of studies that systematically illustrate the cold start problem, investigate its causes, and provide practical solutions to address it. To this end, we ask:  What causes the cold start problem and how can we select the initial query when there is no labeled data available? "}
{"page": 1, "image_path": "doc_images/2210.02442v1_1.jpg", "ocr_text": "Y BALD (Kirsch ef al., 2019) A Consistency (Gao et al., 2020) 4 Coreset (Sener et al., 2017) @ Random\n\n> Margin (Balcan et al., 2007) ll VAAL (Sinha et al., 2019) Entropy (Wang et al., 2014)\n1.0 Od 1.0 oo Pr\nLogg OME rrr ee\nog Ht pat og if! o\ne ort g 14\n3 +f i 5\nz |, { = A\n08 0.8) 4.\"\n10\" TO TO TO To° TO TO TOF\nNumber of images Number of images\n(a) PathMNIST (b) OrganAMNIST\n1.9 seem on 1.9 -\n. a? a t } i\ncee Paty ytine\ni era i 08 att }\nv vet tt y sf\nSi a i. Ht +\nar) tH og 1 set neni\n{\nOF TO: TO\" To\" TO TO TO\"\nNumber of images Number of images\n(c) BloodMNIST (d) CIFAR-10\n\nFigure 1: Cold start problem in vision active learning. Most existing active querying strategies\n(e.g. BALD, Consistency, etc.) are outperformed by random selection in selecting initial queries,\nsince random selection is i.i.d. to the entire dataset. However, some classes are not selected by active\nquerying strategies due to selection bias, so their results are not presented in the low budget regime.\n\nRandom selection is generally considered a baseline to start the active learning because the randomly\nsampled query is independent and identically distributed (i.i.d.) to the entire data distribution. As is\nknown, maintaining a similar distribution between training and test data is beneficial, particularly\nwhen using limited training data [25]. Therefore, a large body of existing work selects the initial query\nrandomly [10, 61, 55, 62, 18, 17, 42, 24, 22, 60], highlighting that active querying compromises\naccuracy and diversity compared to random sampling at the beginning of active learning [36, 63, 44,\n11, 20, 59]. Why? We attribute the causes of the cold start problem to the following two aspects:\n\n(i) Biased query: Active learning tends to select data that is biased to specific classes. Empirically,\nFigure 2 reveals that the class distribution in the selected query is highly unbalanced. These active\nquerying strategies (e.g. Entropy, Margin, VAAL, etc.) can barely outperform random sampling at\nthe beginning because some classes are simply not selected for training. It is because data of the\nminority classes occurs much less frequently than those of the majority classes. Moreover, datasets\nin practice are often highly unbalanced, particularly in medical images [32, 58]. This can escalate\nthe biased sampling. We hypothesize that the /abel diversity of a query is an important criterion to\ndetermine the importance of the annotation. To evaluate this hypothesis theoretically, we explore\nthe upper bound performance by enforcing a uniform distribution using ground truth (Table 1) To\nevaluate this hypothesis practically, we pursue the label diversity by exploiting the pseudo-labels\ngenerated by K-means clustering (Table 2). The label diversity can reduce the redundancy in the\nselection of majority classes, and increase the diversity by including data of minority classes.\n\n(ii) Outlier query: Many active querying strategies were proposed to select typical data and eliminate\noutliers, but they heavily rely on a trained classifier to produce predictions or features. For example,\nto calculate the value of Entropy, a trained classifier is required to predict logits of the data. However,\nthere is no such classifier at the start of active learning, at which point no labeled data is available\nfor training. To express informative features for reliable predictions, we consider contrastive\nlearning, which can be trained using unlabeled data only. Contrastive learning encourages models to\ndiscriminate between data augmented from the same image and data from different images [15, 13].\nSuch a learning process is called instance discrimination. We hypothesize that instance discrimination\ncan act as an alternative to select typical data and eliminate outliers. Specifically, the data that\nis hard to discriminate from others could be considered as typical data. With the help of Dataset\nMaps [48, 26]*, we evaluate this hypothesis and propose a novel active querying strategy that can\neffectively select typical data (hard-to-contrast data in our definition, see §2.2) and reduce outliers.\n\nIt is worthy noting that both [48] and [26] conducted a retrospective study, which analyzed existing active\nquerying strategies by using the ground truth. As a result, the values of confidence and variability in the Dataset\n", "vlm_text": "The image consists of four plots demonstrating the performance of various active learning query strategies in terms of AUC (Area Under the Curve) against the number of images used for training, across different datasets. \n\n1. **Top-left plot (a) PathMNIST**: The performance of different strategies on the PathMNIST dataset is shown. The random selection strategy, marked by grey circles, appears to outperform or match active learning strategies such as BALD, Consistency, Margin, VAAL, Coreset, and Entropy at lower budgets of images (fewer training images).\n\n2. **Top-right plot (b) OrganAMNIST**: The graph represents AUC versus the number of images for the OrganAMNIST dataset, showing a similar trend where random selection performs comparably or better than other strategies at lower images.\n\n3. **Bottom-left plot (c) BloodMNIST**: This plot relates to the BloodMNIST dataset and again shows random selection doing well initially compared to the other active learning strategies, which are represented by different red symbols.\n\n4. **Bottom-right plot (d) CIFAR-10**: For CIFAR-10, while random selection starts out strong, the active learning methods start to catch up or outperform as the number of images increases beyond the lower budget scenario.\n\nAll plots illustrate the cold start problem in active learning for vision tasks, emphasizing that random sampling can initially be more effective than certain active querying strategies because it is representative of the entire dataset. The legend and annotations specify the different active learning strategies being compared: BALD, Consistency, Margin, VAAL, Coreset, and Entropy. The caption discusses how many active strategies are outperformed by random selection initially due to selection bias, where some classes may not be adequately sampled by active learning strategies.\nRandom selection is generally considered a baseline to start the active learning because the randomly sampled query is independent and identically distributed (i.i.d.) to the entire data distribution. As is known, maintaining a similar distribution between training and test data is beneﬁcial, particularly when using limited training data [ 25 ]. Therefore, a large body of existing work selects the initial query randomly [ 10 ,  61 ,  55 ,  62 ,  18 ,  17 ,  42 ,  24 ,  22 ,  60 ], highlighting that active querying compromises accuracy and diversity compared to random sampling at the beginning of active learning [ 36 ,  63 ,  44 , 11, 20, 59]. Why? We attribute the causes of the cold start problem to the following two aspects: \n(i)  Biased query : Active learning tends to select data that is biased to speciﬁc classes. Empirically, Figure 2 reveals that the class distribution in the selected query is highly unbalanced. These active querying strategies ( e.g . Entropy, Margin, VAAL, etc.) can barely outperform random sampling at the beginning because some classes are simply not selected for training. It is because data of the minority classes occurs much less frequently than those of the majority classes. Moreover, datasets in practice are often highly unbalanced, particularly in medical images [ 32 ,  58 ]. This can escalate the biased sampling. We hypothesize that the  label diversity  of a query is an important criterion to determine the importance of the annotation. To evaluate this hypothesis theoretically, we explore the upper bound performance by enforcing a uniform distribution using ground truth (Table 1) To evaluate this hypothesis practically, we pursue the label diversity by exploiting the pseudo-labels generated by    $K$  -means clustering (Table 2). The label diversity can reduce the redundancy in the selection of majority classes, and increase the diversity by including data of minority classes. \n(ii)  Outlier query : Many active querying strategies were proposed to select typical data and eliminate outliers, but they heavily rely on a trained classiﬁer to produce predictions or features. For example, to calculate the value of Entropy, a trained classiﬁer is required to predict logits of the data. However, there is no such classiﬁer at the start of active learning, at which point no labeled data is available for training. To express informative features for reliable predictions, we consider contrastive learning, which can be trained using unlabeled data only. Contrastive learning encourages models to discriminate between data augmented from the same image and data from different images [ 15 ,  13 ]. Such a learning process is called instance discrimination. We hypothesize that instance discrimination can act as an alternative to select typical data and eliminate outliers. Spec i call y, the data that is hard to discriminate from others could be considered as typical data. With the help of Dataset Maps [ 48 ,  $26]^{2}$  , we evaluate this hypothesis and propose a novel active querying strategy that can effectively select  typical data  ( hard-to-contrast  data in our deﬁnition, see   $\\S2.2)$   and reduce outliers. "}
{"page": 2, "image_path": "doc_images/2210.02442v1_2.jpg", "ocr_text": "Systematic ablation experiments and qualitative visualizations in §3 confirm that (i) the level of label\ndiversity and (ii) the inclusion of typical data are two explicit criteria for determining the annotation\nimportance. Naturally, contrastive learning is expected to approximate these two criteria: pseudo-\nlabels in clustering implicitly enforce label diversity in the query; instance discrimination determines\ntypical data. Extensive results show that our initial query not only significantly outperforms existing\nactive querying strategies, but also surpasses random selection by a large margin on three medical\nimaging datasets (i.e. Colon Pathology, Abdominal CT, and Blood Cell Microscope) and two natural\nimaging datasets (i.e. CIFAR-10 and CIFAR-10-LT). Our active querying strategy eliminates the\nneed for manual annotation to ensure the label diversity within initial queries, and more importantly,\nstarts the active learning procedure with the typical data.\n\nTo the best of our knowledge, we are among the first to indicate and address the cold start problem in\nthe field of medical image analysis (and perhaps, computer vision), making three contributions: (1)\nillustrating the cold start problem in vision active learning, (2) investigating the underlying causes\nwith rigorous empirical analysis and visualization, and (3) determining effective initial queries for the\nactive learning procedure. Our solution to the cold start problem can be used as a strong yet simple\nbaseline to select the initial query for image classification and other vision tasks.\n\nRelated work. When the cold start problem was first observed in recommender systems, there were\nseveral solutions to remedy the insufficient information due to the lack of user history [63, 23]. In\nnatural language processing (NLP), Yuan et al. [55] were among the first to address the cold start\nproblem by pre-training models using self-supervision. They attributed the cold start problem to\nmodel instability and data scarcity. Vision active learning has shown higher performance than random\nselection [61, 47, 18, 2, 43, 34, 62], but there is limited study discussing how to select the initial query\nwhen facing the entire unlabeled dataset. A few studies somewhat indicated the existence of the cold\nstart problem: Lang et al. [30] explored the effectiveness of the /-center algorithm [16] to select the\ninitial queries. Similarly, Pourahmadi et al. [38] showed that a simple K’-means clustering algorithm\nworked fairly well at the beginning of active learning, as it was capable of covering diverse classes\nand selecting a similar number of data per class. Most recently, a series of studies [20, 54, 46, 37]\ncontinued to propose new strategies for selecting the initial query from the entire unlabeled data\nand highlighted that typical data (defined in varying ways) could significantly improve the learning\nefficiency of active learning at a low budget. In addition to the existing publications, our study\njustifies the two causes of the cold start problem, systematically presents the existence of the problem\nin six dominant strategies, and produces a comprehensive guideline of initial query selection.\n\n2 Method\n\nIn this section, we analyze in-depth the cause of cold start problem in two perspectives, biased query as\nthe inter-class query and outlier query as the intra-class factor. We provide a complementary method\nto select the initial query based on both criteria. §2.1 illustrates that label diversity is a favourable\nselection criterion, and discusses how we obtain label diversity via simple contrastive learning and\n-means algorithms. §2.2 describes an unsupervised method to sample atypical (hard-to-contrast)\nqueries from Dataset Maps.\n\n2.1 Inter-class Criterion: Enforcing Label Diversity to Mitigate Bias\n\nJX-means clustering. The selected query should cover data of diverse classes, and ideally, select\nsimilar number of data from each class. However, this requires the availability of ground truth, which\nare inaccessible according to the nature of active learning. Therefore, we exploit pseudo-labels\ngenerated by a simple A’-means clustering algorithm and select an equal number of data from each\ncluster to form the initial query to facilitate label diversity. Without knowledge about the exact\nnumber of ground-truth classes, over-clustering is suggested in recent works [51, 57] to increase\nperformances on the datasets with higher intra-class variance. Concretely, given 9, 11, 8 classes in\nthe ground truth, we set Av (the number of clusters) to 30 in our experiments.\n\nContrastive features. /-means clustering requires features of each data point. Li et al. [31]\nsuggested that for the purpose of clustering, contrastive methods (e.g. MoCo, SimCLR, BYOL) are\n\nMaps could not be computed under the practical active learning setting because the ground truth is a priori\nunknown. Our modified strategy, however, does not require the availability of ground truth (detailed in §2.2).\n", "vlm_text": "Systematic ablation experiments and qualitative visualization s in  $\\S3$   conﬁrm that (i) the level of label diversity and (ii) the inclusion of typical data are two explicit criteria for determining the annotation importance. Naturally, contrastive learning is expected to approximate these two criteria: pseudo- labels in clustering implicitly enforce label diversity in the query; instance discrimination determines typical data. Extensive results show that our initial query not only sign i cant ly outperforms existing active querying strategies, but also surpasses random selection by a large margin on three medical imaging datasets ( i.e . Colon Pathology, Abdominal CT, and Blood Cell Microscope) and two natural imaging datasets ( i.e . CIFAR-10 and CIFAR-10-LT). Our active querying strategy eliminates the need for manual annotation to ensure the label diversity within initial queries, and more importantly, starts the active learning procedure with the typical data. \nTo the best of our knowledge, we are among the ﬁrst to indicate and address the cold start problem in the ﬁeld of medical image analysis (and perhaps, computer vision), making three contributions: (1) illustrating the cold start problem in vision active learning, (2) investigating the underlying causes with rigorous empirical analysis and visualization, and (3) determining effective initial queries for the active learning procedure. Our solution to the cold start problem can be used as a strong yet simple baseline to select the initial query for image class i cation and other vision tasks. \nRelated work.  When the cold start problem was ﬁrst observed in recommend er systems, there were several solutions to remedy the in suf cie nt information due to the lack of user history [ 63 ,  23 ]. In natural language processing (NLP), Yuan  et al . [ 55 ] were among the ﬁrst to address the cold start problem by pre-training models using self-supervision. They attributed the cold start problem to model instability and data scarcity. Vision active learning has shown higher performance than random selection [ 61 ,  47 ,  18 ,  2 ,  43 ,  34 ,  62 ], but there is limited study discussing how to select the initial query when facing the entire unlabeled dataset. A few studies somewhat indicated the existence of the cold start problem: Lang  et al . [ 30 ] explored the effectiveness of the  $K$  -center algorithm [ 16 ] to select the initial queries. Similarly, Pourahmadi  et al . [ 38 ] showed that a simple  $K$  -means clustering algorithm worked fairly well at the beginning of active learning, as it was capable of covering diverse classes and selecting a similar number of data per class. Most recently, a series of studies [ 20 ,  54 ,  46 ,  37 ] continued to propose new strategies for selecting the initial query from the entire unlabeled data and highlighted that typical data (deﬁned in varying ways) could sign i cant ly improve the learning efﬁciency of active learning at a low budget. In addition to the existing publications, our study justiﬁes the two causes of the cold start problem, systematically presents the existence of the problem in six dominant strategies, and produces a comprehensive guideline of initial query selection. \n2 Method \nIn this section, we analyze in-depth the cause of cold start problem in two perspectives, biased query as the inter-class query and outlier query as the intra-class factor. We provide a complementary method to select the initial query based on both criteria.   $\\S2.1$   illustrates that label diversity is a favourable selection criterion, and discusses how we obtain label diversity via simple contrastive learning and  $K$  -means algorithms.   $\\S2.2$   describes an unsupervised method to sample atypical (hard-to-contrast) queries from Dataset Maps. \n2.1 Inter-class Criterion: Enforcing Label Diversity to Mitigate Bias \n $K$  -means clustering.  The selected query should cover data of diverse classes, and ideally, select similar number of data from each class. However, this requires the availability of ground truth, which are inaccessible according to the nature of active learning. Therefore, we exploit pseudo-labels generated by a simple    $K$  -means clustering algorithm and select an equal number of data from each cluster to form the initial query to facilitate label diversity. Without knowledge about the exact number of ground-truth classes, over-clustering is suggested in recent works [ 51 ,  57 ] to increase performances on the datasets with higher intra-class variance. Concretely, given 9, 11, 8 classes in the ground truth, we set    $K$   (the number of clusters) to 30 in our experiments. \nContrastive features.    $K$  -means clustering requires features of each data point. Li  et al . [ 31 ] suggested that for the purpose of clustering, contrastive methods ( e.g . MoCo, SimCLR, BYOL) are "}
{"page": 3, "image_path": "doc_images/2210.02442v1_3.jpg", "ocr_text": "Random Consistency VAAL Margin Entropy Coreset BALD Ours\n\napse == L = Ly — —\nbackground i i a if a a —\ndebris Se ee\nlymphocytes | i i —_— a EE ee\nmucus (i I Uy a —_ = —\nmuscle TT i i i a a a\nmucosa i 7 | Ly Ld _ |\nstoma Oe\nepithet as a __ | CC\nEntropy 3.154 3.116 2.800 2.858 2.852 3.006 3.094 3.122\n\nFigure 2: Label diversity of querying criteria. Random, the leftmost strategy, denotes the class\ndistribution of randomly queried samples, which can also reflect the approximate class distribution of\nthe entire dataset. As seen, even with a relatively larger initial query budget (40,498 images, 45%\nof the dataset), most active querying strategies are biased towards certain classes in the Path MNIST\ndataset. For example, VAAL prefers selecting data in the muscle class, but largely ignores data in the\nmucus and mucosa classes. On the contrary, our querying strategy selects more data from minority\nclasses (e.g., mucus and mucosa) while retaining the class distribution of major classes. Similar\nobservations in OrganAMNIST and BloodMNIST are shown in Appendix Figure 7. The higher the\nentropy is, the more balanced the class distribution is.\n\nmore suitable than generative methods (e.g. colorization, reconstruction) because the contrastive\nfeature matrix can be naturally regarded as cluster representations. Therefore, we use MoCo v2 [15]—\na popular self-supervised contrastive method—to extract image features.\n\nJx-means and MoCo v2 are certainly not the only choices for clustering and feature extraction. We\nemploy these two well-received methods for simplicity and efficacy in addressing the cold start\nproblem. Figure 2 shows our querying strategy can yield better label diversity than other six dominant\nactive querying strategies; similar observations are made in OrganAMNIST and BloodMNIST\n(Figure 7) as well as CIFAR-10 and CIFAR-10-LT (Figure 10).\n\n2.2 Intra-class Criterion: Querying Hard-to-Contrast Data to Avoid Outliers\n\nDataset map. Given K clusters generated from Criterion #1, we now determine which data points\nought to be selected from each cluster. Intuitively, a data point can better represent a cluster\ndistribution if it is harder to contrast itself with other data points in this cluster—we consider them\ntypical data. To find these typical data, we modify the original Dataset Map® by replacing the ground\ntruth term with a pseudo-label term. This modification is made because ground truths are unknown in\nthe active learning setting but pseudo-labels are readily accessible from Criterion #1. For a visual\ncomparison, Figure 3b and Figure 3c present the Data Maps based on ground truths and pseudo-labels,\nrespectively. Formally, the modified Data Map can be formulated as follows. Let D = {am }/_,\ndenote a dataset of MZ unlabeled images. Considering a minibatch of N images, for each image x,\nits two augmented views form a positive pair, denoted as #; and &;. The contrastive prediction task\non pairs of augmented images derived from the minibatch generate 2 images, in which a true label\ny;, for an anchor augmentation is associated with its counterpart of the positive pair. We treat the\nother 2(.V — 1) augmented images within a minibatch as negative pairs. We define the probability of\npositive pair in the instance discrimination task as:\n\nexp(sim(z;, 2;))/7\n\nret Unga exp(sim(z;, 2n))/7\"\n\nPi,g (ld)\n\n« 1\nPo (Ynltn) = 5[Pan—1,2n + Pan,2n—1); (2)\n\n2\n\nwhere sim(u, ) = u!v/||2|||| |] is the cosine similarity between wu and v; z2n—1 and Z2n, denote the\nprojection head output of a positive pair for the input a, in a batch; 1,2; € {0, 1} is an indicator\n\nDataset Map [12, 48] was proposed to analyze datasets by two measures: confidence and variability, defined\nas the mean and standard deviation of the model probability of ground truth along the learning trajectory.\n", "vlm_text": "This image is a bar graph depicting the label diversity of different querying strategies on the PathMNIST dataset. Each querying strategy (Random, Consistency, VAAL, Margin, Entropy, Coreset, BALD, and Ours) has a corresponding column showing the class distribution they select for various categories, such as adipose, background, debris, lymphocytes, mucus, muscle, mucosa, stroma, and epithelium.\n\nKey points from the image:\n\n- **Random** strategy shows a more uniform distribution across all classes.\n- **VAAL** has a preference for selecting data in the muscle class, ignoring mucus and mucosa.\n- The **Ours** strategy selects more from minority classes like mucus and mucosa while maintaining the distribution of major classes.\n- Each strategy's entropy is indicated at the bottom, with higher entropy reflecting a more balanced distribution. The values range, with Random having the highest entropy (3.154) and VAAL having the lowest (2.800).\nmore suitable than generative methods ( e.g . color iz ation, reconstruction) because the contrastive feature matrix can be naturally regarded as cluster representations. Therefore, we use MoCo v2 [ 15 ]— a popular self-supervised contrastive method—to extract image features. \n $K$  -means and MoCo v2 are certainly not the only choices for clustering and feature extraction. We employ these two well-received methods for simplicity and efﬁcacy in addressing the cold start problem. Figure 2 shows our querying strategy can yield better label diversity than other six dominant active querying strategies; similar observations are made in Organ AM NIST and BloodMNIST (Figure 7) as well as CIFAR-10 and CIFAR-10-LT (Figure 10). \n2.2 Intra-class Criterion: Querying Hard-to-Contrast Data to Avoid Outliers \nDataset map.  Given  $K$   clusters generated from Criterion #1, we now determine which data points ought to be selected from each cluster. Intuitively, a data point can better represent a cluster distribution if it is harder to contrast itself with other data points in this cluster—we consider them typical data. To ﬁnd these typical data, we modify the original Dataset   $\\mathrm{{\\bf~M}a p}^{3}$    by replacing the ground truth term with a pseudo-label term. This modi cation is made because ground truths are unknown in the active learning setting but pseudo-labels are readily accessible from Criterion #1. For a visual comparison, Figure 3b and Figure 3c present the Data Maps based on ground truths and pseudo-labels, respectively. Form y, the modiﬁed Data Map can be formulated a ollows. Let  $\\grave{\\mathcal{D}}=\\{\\pmb{x}_{m}\\}_{m=1}^{M}$  denote a dataset of  M  unlabeled images. Considering a minibatch of  N  images, for each image  ${\\pmb x}_{n}$  , its two augmented views form a positive pair, denoted as  $\\tilde{\\mathbf{x}}_{i}$   and  $\\tilde{\\pmb{x}}_{j}$  . The contrastive prediction task i on pairs of augmented images derived from the minibatch generate    $2N$   images, in which a true label  $y_{n}^{*}$    for an anchor augmentation is associated with its counterpart of the positive pair. We treat the other  $2(N-1)$   augmented images within a minibatch as negative pairs. We deﬁne the probability of positive pair in the instance discrimination task as: \n\n$$\np_{i,j}=\\frac{\\exp(\\sin(z_{i},z_{j}))/\\tau}{\\sum_{n=1}^{2N}\\mathbb{1}_{[n\\neq i]}\\exp(\\sin(z_{i},z_{n}))/\\tau},\n$$\n \n\n$$\np_{\\theta^{(e)}}(y_{n}^{*}|x_{n})=\\frac{1}{2}[p_{2n-1,2n}+p_{2n,2n-1}],\n$$\n \nwhere  $\\sin(\\pmb{u},)=\\pmb{u}^{\\top}\\pmb{v}/||\\pmb{u}||\\|\\pmb{v}\\|$  is the cosine similarit etween    $\\mathbfcal{U}$   and  $\\boldsymbol{v};z_{2n-1}$   and  $z_{2n}$   denote the projection head output of a positive pair for the input  ${\\pmb x}_{n}$   in a batch;    $\\mathbb{1}_{[n\\neq i]}\\in\\{0,1\\}$   is an indicator "}
{"page": 4, "image_path": "doc_images/2210.02442v1_4.jpg", "ocr_text": "© basophil © lymphocyte e ig\n© eosinophil © — monocyte platelet\n© erythroblast © neutrophil\n\n1.0\n\n—\n<2\nia tos\nOd...\n\nEasy to-learn 3\n\nSo ©\na i)\n,\nee\n\nconfidence\nS\n+\n\nw 5o. 4 es ‘\n. Me, . “J\nes $ rt 0.2 0.4 9.0 03 0.4 0.5\nome Hard-to- -_ variability variability Hard-to-contrast\n(a) Overall distribution (b) Data Map by ground truth (c) Data Map by pseudo-labels\n\nFigure 3: Active querying based on Dataset Maps. (a) Dataset overview. (b) Easy- and hard-to-\nlearn data can be selected from the maps based on ground truths [26]. This querying strategy has\ntwo limitations: it requires manual annotations and the data are stratified by classes in the 2D space,\nleading to a poor label diversity in the selected queries. (c) Easy- and hard-to-contrast data can be\nselected from the maps based on pseudo-labels. This querying strategy is label-free and the selected\nhard-to-contrast data represent the most common patterns in the entire dataset, as presented in (a).\nThese data are more suitable for training, and thus alleviate the cold start problem.\n\nfunction evaluating to 1 iff n 4 i and 7 denotes a temperature parameter. 0°) denotes the parameters\nat the end of the e\" epoch. We define confidence (ji) across E epochs as:\n\nE\n~ 1 *\nfm = FR Y_ Po (Yin ?m)- @)\n\ne=1\nThe confidence (fi,,,) is the Y-axis of the Dataset Maps (see Figure 3b-c).\n\nHard-to-contrast data. We consider the data with a low confidence value (Equation 3) as “hard-to-\ncontrast” because they are seldom predicted correctly in the instance discrimination task. Apparently,\nif the model cannot distinguish a data point with others, this data point is expected to carry typical\ncharacteristics that are shared across the dataset [40]. Visually, hard-to-contrast data gather in the\nbottom region of the Dataset Maps and “easy-to-contrast” data gather in the top region. As expected,\nhard-to-learn data are more typical, possessing the most common visual patterns as the entire dataset;\nwhereas easy-to-learn data appear like outliers [54, 26], which may not follow the majority data\ndistribution (examples in Figure 3a and Figure 3c). Additionally, we also plot the original Dataset\nMap [12, 48] in Figure 3b, which grouped data into hard-to-learn and easy-to-learn’. Although\nthe results in §3.2 show equally compelling performance achieved by both easy-to-learn [48] and\nhard-to-contrast data (ours), the latter do not require any manual annotation, and therefore are more\npractical and suitable for vision active learning.\n\nIn summary, to meet the both criteria, our proposed active querying strategy includes three steps:\n(i) extracting features by self-supervised contrastive learning, (ii) assigning clusters by /v-means\nalgorithm for label diversity, and (iii) selecting hard-to-contrast data from dataset maps.\n\n3. Experimental Results\n\nDatasets & metrics. Active querying strategies have a selection bias that is particularly harmful\nin long-tail distributions. Therefore, unlike most existing works [38, 54], which tested on highly\nbalanced annotated datasets, we deliberately examine our method and other baselines on long-\ntail datasets to simulate real-world scenarios. Three medical datasets of different modalities\n\n4Swayamdipta et al. [48] indicated that easy-to-learn data facilitated model training in the low budget regime\nbecause easier data reduced the confusion when the model approaching the rough decision boundary. In essence,\nthe advantage of easy-to-learn data in active learning aligned with the motivation of curriculum learning [6].\n", "vlm_text": "This image presents an analysis of a dataset related to blood cells, as indicated by the visual content and the caption text. The image is divided into three main parts:\n\n1. **(a) Overall distribution**: This section shows a visual overview of a dataset, comprising numerous small images of blood cells arranged in a grid. Each cell type is presumably represented by various colors in the second part of the image. The colors relate to different blood cell classes: basophil, eosinophil, erythroblast, lymphocyte, monocyte, neutrophil, and others like Ig and platelet.\n\n2. **(b) Data Map by ground truth**: This part presents a scatter plot categorized by confidence and variability. Each point is color-coded according to the type of blood cell it represents, corresponding to the legend at the top. The plots also highlight segments characterized as \"Easy-to-learn\" and \"Hard-to-learn\" with small cutout images of blood cells representing these categories.\n\n3. **(c) Data Map by pseudo-labels**: Similar to the section (b), this scatter plot visualizes confidence against variability, but here the categorization is based on pseudo-labels instead of ground truths. The plot distinguishes between \"Easy-to-contrast\" and \"Hard-to-contrast\" data points, which are indicated with accompanying sample images of blood cells.\n\nThe analysis utilizes an active querying approach to improve data selection for machine learning training, focusing on distinguishing between data that are easier or harder to learn or contrast. The use of pseudo-labels (as shown in section c) is noted to alleviate the cold start problem by identifying common patterns in the dataset.\nfunction evaluati to  1  iff  $n\\neq i$   and  $\\tau$   denotes a t rature p ameter.    $\\theta^{(e)}$    denotes the parameters at the end of the  $e^{\\mathrm{\\tilde{th}}}$    epoch. We deﬁne conﬁdence  $\\left(\\hat{\\mu}_{m}\\right)$   across  E  epochs as: \n\n$$\n\\hat{\\mu}_{m}=\\frac{1}{E}\\sum_{e=1}^{E}p_{\\theta^{(e)}}(y_{m}^{*}\\vert x_{m}).\n$$\n \nThe conﬁdence    $(\\hat{\\mu}_{m})$   is the Y-axis of the Dataset Maps (see Figure 3b-c). \nHard-to-contrast data.  We consider the data with a low conﬁdence value (Equation 3) as “hard-to- contrast” because they are seldom predicted correctly in the instance discrimination task. Apparently, if the model cannot distinguish a data point with others, this data point is expected to carry typical characteristics that are shared across the dataset [ 40 ]. Visually, hard-to-contrast data gather in the bottom region of the Dataset Maps and “easy-to-contrast” data gather in the top region. As expected, hard-to-learn data are more typical, possessing the most common visual patterns as the entire dataset; whereas easy-to-learn data appear like outliers [ 54 ,  26 ], which may not follow the majority data distribution (examples in Figure 3a and Figure 3c). Additionally, we also plot the original Dataset Map [ 12 ,  48 ] in Figure 3b, which grouped data into hard-to-learn and easy-to-learn 4 . Although the results in   $\\S3.2$   show equally compelling performance achieved by both easy-to-learn [ 48 ] and hard-to-contrast data (ours), the latter do not require any manual annotation, and therefore are more practical and suitable for vision active learning. \nIn summary, to meet the both criteria, our proposed active querying strategy includes three steps: (i) extracting features by self-supervised contrastive learning, (ii) assigning clusters by  $K$  -means algorithm for label diversity, and (iii) selecting hard-to-contrast data from dataset maps. \n3 Experimental Results \nDatasets   $\\&$   metrics.  Active querying strategies have a selection bias that is particularly harmful in long-tail distributions. Therefore, unlike most existing works [ 38 ,  54 ], which tested on highly balanced annotated datasets, we deliberately examine our method and other baselines on long- tail datasets to simulate real-world scenarios. Three medical datasets of different modalities Table 1:  Diversity is a signiﬁcant add-on to most querying strategies.  AUC scores of different querying strategies are compared on three medical imaging datasets. In either low budget ( i.e .   $0.5\\%$  or   $1\\%$   of MedMNIST datasets) or high budget ( i.e .   $10\\%$   or  $20\\%$   of CIFAR-10-LT) regimes, both random and active querying strategies beneﬁt from enforcing the label diversity of the selected data. The cells are highlighted in blue when adding diversity performs no worse than the original querying strategies. Coreset [ 41 ] works very well as its original form because this querying strategy has implicitly considered the label diversity (also veriﬁed in Table 2) by formulating a  $K$  -center problem, which selects    $K$   data points to represent the entire dataset. Some results are missing (marked as “-”) because the querying strategy fails to sample at least one data point for each class. Results of more sampling ratios are presented in Appendix Figures 6, 9. "}
{"page": 5, "image_path": "doc_images/2210.02442v1_5.jpg", "ocr_text": "Table 1: Diversity is a significant add-on to most querying strategies. AUC scores of different\nquerying strategies are compared on three medical imaging datasets. In either low budget (i.e. 0.5%\nor 1% of MedMNIST datasets) or high budget (i.e. 10% or 20% of CIFAR-10-LT) regimes, both\nrandom and active querying strategies benefit from enforcing the label diversity of the selected data.\n\nThe cells are highlighted in blue when adding diversity performs no worse than the original querying\nstrategies. Coreset [41] works very well as its original form because this querying strategy has\nimplicitly considered the label diversity (also verified in Table 2) by formulating a K’-center problem,\nwhich selects A data points to represent the entire dataset. Some results are missing (marked as “-””)\nbecause the querying strategy fails to sample at least one data point for each class. Results of more\nsampling ratios are presented in Appendix Figures 6, 9.\nPathMNIST OrganAMNIST BloodMNIST CIFAR-10-LT\n0.5% 1% 0.5% 1% 0.5% 1% 10% 20%\nUnif. (499) (899) (172) (345) (59) (119) (1420) (2841)\nRandom v 96.8+0.6 97.6£0.6 91.140.9 93.340.4 94.740.7 96.5+0.4 91.641.1 93.1+0.6\n96.4413 97.6£0.9 | 90.7411 93.140.7 | 93.2415 95.840.7 | 62.0+6.1 -\nConsistency 964401 97.9£0.1 | 923405  928£1.0 | 9290.9 95.9405 | SI4ELT 93.4402\n‘Onsistency 96.240.0  97.6£0.0 | 91.0+0.3  94.040.6 | 87.9+40.2 _ 95.540.5 | 67.1417.1 88.6403\nVAAL v 92.740.5 93.0£0.6 70.6£1.9 84.640.5 89.8413  93.4+0.9 | 92.640.2 93.7£0.4\nMarai v 979402 96.0£04 | 8IBEI2 858£14 | 897E1.9 94.7407 | 91.7E09 93.2402\nargin 91.0423 96.040.3 - 85.940.7 - - 81.9408 86.3403\nEnt 7 932416 952402 | 791L23 86.708 | 859E05 S18EI0 | 920E12 S19ETS\nntropy - 87.540.1 - - - - 65.6415.6 _ 86.4+0.2\nCoreset v 950422 948425 | 856L04 89.9E05 | 885406 4IETT | 91504 93.6402\norese! 95.640.7 _97.5£0.2 | 83.8+0.6 88.5404 | 87.3416 —94.041.2 | 65.9415.9  86.940.1\nBALD 7 958402  97.0L0.1 | 87.2403 89.2403 | 899E08 92.7207 | 928E01 90.8424\n92.042.3 95.3£1.0 - - 83.342.2 93.5413 | 64.9414.9 — 84.7+0.6\n\nTable 2: Class coverage of selected data. Compared with random selection (i.i.d. to entire data\ndistribution), most active querying strategies contain selection bias to specific classes, so the class\ncoverage in their selections might be poor, particularly using low budgets. As seen, using 0.002% or\neven smaller proportion of MedMNIST datasets, the class coverage of active querying strategies is\nmuch lower than random selection. By integrating K-means clustering with contrastive features, our\nquerying strategy is capable of covering 100% classes in most scenarios using low budgets (<0.002%\nof MedMNIST). We also found that our querying strategy covers the most of the classes in the\n\nCIFAR-10-LT dataset, which is designatedly more imbalanced.\n\nPathMNIST OrganAMNIST BloodMNIST CIFAR-10-LT\n\n0.00015% 0.00030% 0.001% 0.002% 0.001% 0.002% 0.2% 0.3%\n(13) (26) (34) (69) (11) (23) (24) (37)\n\nRandom 0.7940.11 0.95£0.07 | 0.91+0.08 — 0.9840.04 | 0.700.113 0.94£0.08 | 0.58£0.10  0.66-£0.12\nConsistency 0.78 0.88 0.82 0.91 0.75 0.88 0.50 0.70\nVAAL 0.11 O11 0.18 0.18 0.13 0.13 0.30 0.30\nMargin 0.67 0.78 0.73 0.82 0.63 0.75 0.60 0.70\nEntropy 0.33 0.33 0.45 0.73 0.63 0.63 0.40 0.70\nCoreset 0.66 0.78 0.91 1.00 0.63 0.88 0.60 0.70\nBALD 0.33 0.44 0.64 0.64 0.75 0.88 0.60 0.70\nOurs 0.78 1.00 1.00 1.00 1.00 1.00 0.70 0.80\n\nin MedMNIST [53] are used: PathMNIST (colorectal cancer tissue histopathological images),\nBloodMNIST (microscopic peripheral blood cell images), OrganAMNIST (axial view abdominal\nCT images of multiple organs). OrganAMNIST is augmented following Azizi et al. [3], while the\nothers following Chen et al. [15]. Area Under the ROC Curve (AUC) and Accuracy are used as the\nevaluation metrics. All results were based on at least three independent runs, and particularly, 100\nindependent runs for random selection. UMAP [35] is used to analyze feature clustering results.\n\nBaselines & implementations. We benchmark a total of seven querying strategies: (1) random\nselection, (2) Max-Entropy [52], (3) Margin [4], (4) Consistency [18], (5) BALD [28], (6) VAAL [45],\nand (7) Coreset [41]. For contrastive learning, we trained 200 epochs with MoCo v2, following its\ndefault hyperparameter settings. We set 7 to 0.05 in equation 2. To reproduce the large batch size and\niteration numbers in [13], we apply repeated augmentation [21, 49, 50] (detailed in Table 5). More\nbaseline and implementation details can be found in Appendix A.\n", "vlm_text": "\nThe table presents the results of various active learning strategies applied to different datasets with varying labeled data percentages. The datasets are PathMNIST, OrganAMNIST, BloodMNIST, and CIFAR-10-LT, each with different percentages of labeled data (e.g., 0.5%, 1%, 10%, and 20%). \n\nThe column \"Unif.\" indicates whether a uniform distribution was used in the selection strategy (✓ for yes, ✗ for no). Active learning strategies listed are: Random, Consistency, VAAL, Margin, Entropy, Coreset, and BALD.\n\nEach cell in the table provides the accuracy results (mean ± standard deviation) for each strategy on each dataset and labeled data percentage. Some cells are highlighted in blue, indicating either better performance or a unique characteristic/benchmark result according to the table's context. The numbers in parentheses next to the percentage of data indicate the number of samples used for training (e.g., 499 for 0.5% of PathMNIST).\n\nOverall, the table displays and compares the effectiveness of each strategy for different datasets and conditions, offering insights into their relative performance.\nTable 2:  Class coverage of selected data.  Compared with random selection (i.i.d. to entire data distribution), most active querying strategies contain selection bias to speciﬁc classes, so the class coverage in their selections might be poor, particularly using low budgets. As seen, using  $0.002\\%$   or even smaller proportion of MedMNIST datasets, the class coverage of active querying strategies is much lower than random selection. By integrating  $K$  -means clustering with contrastive features, our querying strategy is capable of covering   $100\\%$   classes in most scenarios using low budgets   $(\\leq\\!0.002\\%$  of MedMNIST). We also found that our querying strategy covers the most of the classes in the CIFAR-10-LT dataset, which is designated ly more imbalanced. \nThe table presents performance comparisons across different datasets and methods. It includes four datasets: PathMNIST, OrganAMNIST, BloodMNIST, and CIFAR-10-LT. Each dataset is associated with two sampling rates, represented as percentages with the exact number of samples in parentheses. The methods compared are grouped vertically as \"Random,\" \"Consistency,\" \"VAAL,\" \"Margin,\" \"Entropy,\" \"Coreset,\" \"BALD,\" and \"Ours.\"\n\nFor each combination of dataset and sampling rate, the table provides the results of these methods in terms of their performance metric, which seems to be accuracy or a similar measure. The \"Random\" method includes an average and standard deviation displayed in a \"mean ± standard deviation\" format.\n\n- For PathMNIST and the smallest sampling rate (0.00015%), \"Random\" performs at 0.79 ± 0.11, while \"Ours,\" \"Consistency,\" \"Margin,\" and \"Coreset\" all achieved 0.78.\n- At a sampling rate of 0.00030% for PathMNIST, \"Ours,\" \"Random,\" and several other methods reach a performance of 1.00.\n- In OrganAMNIST, \"Random\" scores 0.91 ± 0.08 and 0.98 ± 0.04 for the two sampling rates, with \"Ours\" reaching a perfect score of 1.00 in both cases.\n- BloodMNIST shows \"Random\" at 0.70 ± 0.13 and 0.94 ± 0.08, with \"Ours\" again achieving 1.00 for both sampling rates.\n- For CIFAR-10-LT, \"Random\" has performance metrics of 0.58 ± 0.10 and 0.66 ± 0.12. The \"Ours\" method performs at 0.70 and 0.80, respectively.\n\nThe \"Ours\" method consistently reaches 1.00 accuracy or the highest performance across all medical image datasets and performs better than or equal to other methods in the CIFAR-10-LT dataset.\nin MedMNIST [ 53 ] are used: PathMNIST (colorectal cancer tissue his to pathological images), BloodMNIST (microscopic peripheral blood cell images), Organ AM NIST (axial view abdominal CT images of multiple organs). Organ AM NIST is augmented following Azizi  et al . [ 3 ], while the others following Chen  et al . [ 15 ]. Area Under the ROC Curve (AUC) and Accuracy are used as the evaluation metrics. All results were based on at least three independent runs, and particularly, 100 independent runs for random selection. UMAP [35] is used to analyze feature clustering results. \nBaselines & implementations.  We benchmark a total of seven querying strategies: (1) random selection, (2) Max-Entropy [ 52 ], (3) Margin [ 4 ], (4) Consistency [ 18 ], (5) BALD [ 28 ], (6) VAAL [ 45 ], and (7) Coreset [ 41 ]. For contrastive learning, we trained 200 epochs with MoCo v2, following its default hyper parameter settings. We set  $\\tau$   to 0.05 in equation 2. To reproduce the large batch size and iteration numbers in [ 13 ], we apply repeated augmentation [ 21 ,  49 ,  50 ] (detailed in Table 5). More baseline and implementation details can be found in Appendix A. "}
{"page": 6, "image_path": "doc_images/2210.02442v1_6.jpg", "ocr_text": "BB Easy-totearn BB Hard-to-learn BB Easy-to-contrast BB Hard-to-contrast\n\n1.9; 1.9; 1.9;\n\n0.9} 0.9}\n\n0.8] 0.8]\ni) Vy\n\n3 3\nz z\n\n0.7] 0.7|\n\n0.6] 0.6]\n\n050.1% 2%\nllimages 23 images\n\n0.5!\n2481 images 3721 images\n\n(a) PathMNIST (b) OrganAMNIST (c) BloodMNIST () CIFAR-10-LT\n\n1 2%\n34images 69 images\n\n1 2%\n89images 179 images.\n\nFigure 4: Quantitative comparison of map-based querying strategies. Random selection (dot-\nlines) can be treated as a highly competitive baseline in cold start because it outperforms six popular\nactive querying strategies as shown in Figure 1. In comparison with random selection and three other\nquerying strategies, hard-to-contrast performs the best. Although easy-to-learn and hard-to-learn\nsometimes performs similarly to hard-to-contrast, their selection processes require ground truths [26],\nwhich are not available in the setting of active learning.\n\n3.1 Contrastive Features Enable Label Diversity to Mitigate Bias\n\nLabel coverage & diversity. Most active querying strategies have selection bias towards specific\nclasses, thus the class coverage in their selections might be poor (see Table 2), particularly at low\nbudgets. By simply enforcing label diversity to these querying strategies can significantly improve\nthe performance (see Table 1), which suggests that the label diversity is one of the causes that existing\nactive querying strategies perform poorer than random selection.\n\nOur proposed active querying strategy, however, is capable of covering 100% classes in most low\nbudget scenarios (<0.002% of full dataset) by integraing K-means clustering with contrastive\nfeatures.\n\n3.2 Pseudo-labels Query Hard-to-Contrast Data and Avoid Outliers\n\nHard-to-contrast data are practical for cold start problem. Figure 4 presents the quantitative\ncomparison of four map-based querying strategies, wherein easy- or hard-to-learn are selected by the\nmaps based on ground truths, easy- or hard-to-contrast are selected by the maps based on pseudo-\nlabels. Note that easy- or hard-to-learn are enforced with label diversity, due to their class-stratified\ndistributions in the projected 2D space (illustrated in Figure 3). Results suggest that selecting\neasy-to-learn or hard-to-contrast data contribute to the optimal models. In any case, easy- or hard-to-\nlearn data can not be selected without knowing ground truths, so these querying strategies are not\npractical for active learning procedure. Selecting hard-to-contrast, on the other hand, is a label-free\nstrategy and yields the highest performance amongst existing active querying strategies (reviewed\nin Figure 1). More importantly, hard-to-contrast querying strategy significantly outperforms random\nselection by 1.8% (94.14%+£1.0% vs. 92.27% 42.2%), 2.6% (84.35%+0.7% vs. 81.75%+2.1%),\nand 5.2% (88.51%+1.5% vs. 83.36%+3.5%) on PathMNIST, OrganAMNIST, and BloodMNIST,\nrespectively, by querying 0.1% of entire dataset. Similarly on CIFAR-10-LT, hard-to-contrast\nsignificantly outperforms random selection by 21.2% (87.35%+0.0% vs. 66.12%+0.9%) and 24.1%\n(90.59% +0.1% vs. 66.53%+0.5%) by querying 20% and 30% of entire dataset respectively. Note\nthat easy- or hard-to-learn are not enforced with label diversity, for a more informative comparison.\n\n3.3. On the Importance of Selecting Superior Initial Query\n\nA good start foresees improved active learning. We stress the importance of the cold start problem\nin vision active learning by conducting correlation analysis. Starting with 20 labeled images as the\ninitial query, the training set is increased by 10 more images in each active learning cycle. Figure 14a\npresents the performance along the active learning (each point in the curve accounts for 5 independent\ntrials). The initial query is selected by a total of 9 different strategies>, and subsequent queries are\n\n5Hard-to-learn is omitted because it falls behind other proposed methods by a large margin (Figure 4).\n", "vlm_text": "The image is a bar chart comparing map-based querying strategies across four datasets: PathMNIST, OrganAMNIST, BloodMNIST, and CIFAR-10-LT. It evaluates the performance using the AUC (Area Under the Curve) metric.\n\n1. **PathMNIST (89 and 179 images):**\n   - Strategies compared: Easy-to-learn, Hard-to-learn, Easy-to-contrast, Hard-to-contrast\n   - Hard-to-contrast performs best, followed by Easy-to-learn.\n\n2. **OrganAMNIST (34 and 69 images):**\n   - Hard-to-contrast shows the highest AUC, followed closely by Easy-to-learn and Easy-to-contrast.\n\n3. **BloodMNIST (11 and 23 images):**\n   - Hard-to-contrast is the top performer, with Easy-to-learn performing nearly as well.\n\n4. **CIFAR-10-LT (2481 and 3721 images):**\n   - Hard-to-contrast leads slightly, with Easy-to-contrast and Easy-to-learn also showing strong performance.\n\nOverall, the “hard-to-contrast” strategy tends to outperform others across different datasets. The caption suggests that while “easy-to-learn” and “hard-to-learn” strategies sometimes perform similarly to “hard-to-contrast,” they require ground truths, which are not always available in active learning scenarios.\n3.1 Contrastive Features Enable Label Diversity to Mitigate Bias \nLabel coverage   $\\&$   diversity.  Most active querying strategies have selection bias towards speciﬁc classes, thus the class coverage in their selections might be poor (see Table 2), particularly at low budgets. By simply enforcing label diversity to these querying strategies can sign i cant ly improve the performance (see Table 1), which suggests that the label diversity is one of the causes that existing active querying strategies perform poorer than random selection. \nOur proposed active querying strategy, however, is capable of covering   $100\\%$   classes in most low budget scenarios (  ${\\leq}0.002\\%$   of full dataset) by integraing    $K$  -means clustering with contrastive features. \n3.2 Pseudo-labels Query Hard-to-Contrast Data and Avoid Outliers \nHard-to-contrast data are practical for cold start problem.  Figure 4 presents the quantitative comparison of four map-based querying strategies, wherein easy- or hard-to-learn are selected by the maps based on ground truths, easy- or hard-to-contrast are selected by the maps based on pseudo- labels. Note that easy- or hard-to-learn are enforced with label diversity, due to their class-stratiﬁed distributions in the projected 2D space (illustrated in Figure 3). Results suggest that  selecting easy-to-learn or hard-to-contrast data contribute to the optimal models . In any case, easy- or hard-to- learn data can not be selected without knowing ground truths, so these querying strategies are not practical for active learning procedure. Selecting hard-to-contrast, on the other hand, is a label-free strategy and yields the highest performance amongst existing active querying strategies (reviewed in Figure 1). More importantly, hard-to-contrast querying strategy sign i cant ly outperforms random sele b  $1.8\\%$   $(94.14\\%{\\pm}1.0\\%$   $92.27\\%{\\pm}2.2\\%)$  ),   $2.6\\%$   (  $84.35\\%{\\pm}0.7\\%$   vs.   $81.75\\%\\pm2.1\\%)$  ), and 5.2% (88.51%  $88.51\\%{\\pm}1.5\\%$  ±  $83.36\\%{\\pm}3.5\\%)$  ± 3.5%) on PathMNIST, Organ AM NIST, and BloodMNIST, respectively, by querying 0.1% of entire dataset. Similarly on CIFAR-10-LT, hard-to-contrast perf lection by   $21.2\\%$   $87.35\\%{\\pm}0.0\\%$   vs.  $66.12\\%{\\pm}0.9\\%)$   and  $24.1\\%$   $(90.59\\%{\\pm}0.1\\%$  ± 0.1% vs. 66.53%  $66.53\\%{\\pm}0.5\\%)$  ± 0.5%) by querying 20% and 30% of entire dataset respectively. Note that easy- or hard-to-learn are not enforced with label diversity, for a more informative comparison. \n3.3 On the Importance of Selecting Superior Initial Query \nA good start foresees improved active learning.  We stress the importance of the cold start problem in vision active learning by conducting correlation analysis. Starting with 20 labeled images as the initial query, the training set is increased by 10 more images in each active learning cycle. Figure 14a presents the performance along the active learning (each point in the curve accounts for 5 independent trials). The initial query is selected by a total of 9 different strategies 5 , and subsequent queries are "}
{"page": 7, "image_path": "doc_images/2210.02442v1_7.jpg", "ocr_text": "Random Entropy Margin BALD Coreset\n\n100. 100. 1005 1005 1005\n0 0 904 904 904\ngal a 20 —— wl wo a ol\n8\n2 70 1 104 104 704\n-* Hard-to-Contrast\n60 60 604 604 07) ® Easy-to-Contrast\n50: u T T T 1 50: T T T T 1 50° u T T T 1 50: T T T T 1 50: = Easy-toLeam\n10 20 30 40 so Go 10 20 30 40 50 60 10 20 30 40 50 Go 10 20 30 40 50 60 10 20 30 40 50 6 6 Consistency\nai > Ent\n(a) Training from scratch ropy\n“© Margin\n1005 1005 100. 100. 100 = BALD\n-& VAAL\n904 904 0 90 90.\nx3 -* Coreset\nee | wf “Tos 80 of 80\n8\n2 704 704 1 10 10\n604 604 60 60. 60.\n50+ T T T T 1 50: T T T T 1 50: T T T T 1 50: T T T T 1 50: T T T T 7\n10 20 30 40 50 60 10 20 30 40 50 G0 10 2 30 40 50 60 10 20 30 40 50 60 10 20 30 40 50 60\n# of Labeled Images # of Labeled Images # of Labeled Images # of Labeled Images # of Labeled Images\n\n(b) Fine-tuning from self-supervised pre-training\n\nFigure 5: On the importance of selecting a superior initial query. Hard-to-contrast data (red lines)\noutperform other initial queries in every cycle of active learning on OrganaMNIST. We find that the\nperformance of the initial cycle (20 images) and the last cycle (50 images) are strongly correlated.\n\nselected by 5 different strategies. AUC,, denotes the AUC score achieved by the model that is trained\nby n labeled images. The Pearson correlation coefficient between AUC o (starting) and AUCso\n(ending) shows strong positive correlation (r = 0.79, 0.80, 0.91, 0.67, 0.92 for random selection,\nEntropy, Margin, BALD, and Coreset, respectively). This result is statistically significant (p <\n0.05). Hard-to-contrast data (our proposal) consistently outperforms the others on OrganAMNIST\n(Figure 5), BloodMNIST (Figure 13), and PathMNIST (Figure 14), and steadily improves the model\nperformances within the next active learning cycles.\n\nThe initial query is consequential regardless of model initialization. A pre-trained model can\nimprove the performance of each active learning cycle for both random and active selection [55], but\nthe cold start problem remains (evidenced in Figure 14b). This suggests that the model instability\nand data scarcity are two independent issues to be addressed for the cold start problem. Our “hard-to-\ncontrast” data selection criterion only exploits contrastive learning (an improved model), but also\ndetermines the typical data to be annotated first (a better query). As a result, when fine-tuning from\nMoCo v2, the Pearson correlation coefficient between AUC29 and AUCs remains high (r = 0.92,\n0.81, 0.70, 0.82, 0.85 for random selection, Entropy, Margin, BALD, and Coreset, respectively) and\nstatistically significant (p < 0.05).\n\n4 Conclusion\n\nThis paper systematically examines the causes of the cold start problem in vision active learning and\noffers a practical and effective solution to address this problem. Analytical results indicate that (1)\nthe level of label diversity and (2) the inclusion of hard-to-contrast data are two explicit criteria to\ndetermine the annotation importance. To this end, we devise a novel active querying strategy that can\nenforce label diversity and determine hard-to-contrast data. The results of three medical imaging and\ntwo natural imaging datasets show that our initial query not only significantly outperforms existing\nactive querying strategies but also surpasses random selection by a large margin. This finding is\nsignificant because it is the first few choices that define the efficacy and efficiency of the subsequent\nlearning procedure. We foresee our solution to the cold start problem as a simple, yet strong, baseline\nto sample the initial query for active learning in image classification.\n\nLimitation. This study provides an empirical benchmark of initial queries in active learning, while\nmore theoretical analyses can be provided. Yehuda et al. [54] also found that the choice of active\nlearning strategies depends on the initial query budget. A challenge is to articulate the quantity of\ndetermining active learning strategies, which we leave for future work.\n", "vlm_text": "The image consists of ten graphs that compare different methods in terms of Area Under the Curve (AUC) percentage with varying numbers of labeled images. It is divided into two rows:\n\n1. **Top Row** - Captioned as \"(a) Training from scratch,\" it includes graphs for different selection strategies such as Random, Entropy, Margin, BALD, and Coreset.\n\n2. **Bottom Row** - Captioned as \"(b) Fine-tuning from self-supervised pre-training,\" it shows the same strategies as the top row but differs in the pre-training approach.\n\nEach graph shows AUC (%) on the y-axis and the number of labeled images on the x-axis, comparing several methods like Hard-to-Contrast, Easy-to-Contrast, and others. Each method is represented by different line styles and symbols. The red line (Hard-to-Contrast) generally appears to outperform the other methods across the graphs.\nFigure 5:  On the importance of selecting a superior initial query.  Hard-to-contrast data (red lines) outperform other initial queries in every cycle of active learning on Organ aM NIST. We ﬁnd that the performance of the initial cycle (20 images) and the last cycle (50 images) are strongly correlated. \nselected by 5 different strategies.  $\\operatorname{succ}_{n}$   denotes the AUC score achieved by the model that is trained by    $n$   labeled images. The Pearson correlation coefﬁcient between   $\\mathrm{AUC_{20}}$   (starting) and   $\\mathrm{AUC_{50}}$  (ending) shows strong positive correlation   $r=0.79$  , 0.80, 0.91, 0.67, 0.92 for random selection, Entropy, Margin, BALD, and Coreset, respectively). This result is statistically signiﬁcant   $\\mathscr{p}<\n\n$  0.05). Hard-to-contrast data (our proposal) consistently outperforms the others on Organ AM NIST\n\n (Figure 5), BloodMNIST (Figure 13), and PathMNIST (Figure 14), and steadily improves the model performances within the next active learning cycles. \nThe initial query is consequential regardless of model initialization.  A pre-trained model can improve the performance of each active learning cycle for both random and active selection [ 55 ], but the cold start problem remains (evidenced in Figure 14b). This suggests that the model instability and data scarcity are two independent issues to be addressed for the cold start problem. Our “hard-to- contrast” data selection criterion only exploits contrastive learning (an improved model), but also determines the typical data to be annotated ﬁrst (a better query). As a result, when ﬁne-tuning from MoCo v2, the Pearson correlation coefﬁcient between  $\\mathrm{AUC_{20}}$   and   $\\mathrm{AUC_{50}}$   remains high (  $r=0.92$  , 0.81, 0.70, 0.82, 0.85 for random selection, Entropy, Margin, BALD, and Coreset, respectively) and statistically signiﬁcant   $(p<0.05)$  . \n4 Conclusion \nThis paper systematically examines the causes of the cold start problem in vision active learning and offers a practical and effective solution to address this problem. Analytical results indicate that (1) the level of label diversity and (2) the inclusion of hard-to-contrast data are two explicit criteria to determine the annotation importance. To this end, we devise a novel active querying strategy that can enforce label diversity and determine hard-to-contrast data. The results of three medical imaging and two natural imaging datasets show that our initial query not only sign i cant ly outperforms existing active querying strategies but also surpasses random selection by a large margin. This ﬁnding is signiﬁcant because it is the ﬁrst few choices that deﬁne the efﬁcacy and efﬁciency of the subsequent learning procedure. We foresee our solution to the cold start problem as a simple, yet strong, baseline to sample the initial query for active learning in image class i cation. \nLimitation.  This study provides an empirical benchmark of initial queries in active learning, while more theoretical analyses can be provided. Yehuda  et al . [ 54 ] also found that the choice of active learning strategies depends on the initial query budget. A challenge is to articulate the quantity of determining active learning strategies, which we leave for future work. "}
{"page": 8, "image_path": "doc_images/2210.02442v1_8.jpg", "ocr_text": "Potential societal impacts. Real-world data often exhibit long-tailed distributions, rather than the\nideal uniform distributions over each class. We improve active learning by enforcing label diversity\nand hard-to-contrast data. However, we only extensively test our strategies on academic datasets.\nIn many other real-world domains such as robotics and autonomous driving, the data may impose\nadditional constraints on annotation accessibility or learning dynamics, e.g., being fair or private. We\nfocus on standard accuracy and AUC as our evaluation metrics while ignoring other ethical issues in\nimbalanced data, especially in underrepresented minority classes.\n\nAcknowledgements\n\nThis work was supported by the Lustgarten Foundation for Pancreatic Cancer Research. The\nauthors want to thank Mingfei Gao for the discussion of initial query quantity and suggestions\non the implementation of consistency-based active learning framework. The authors also want to\nthank Guy Hacohen, Yuanhan Zhang, Akshay L. Chandra, Jingkang Yang, Hao Cheng, Rongkai\nZhang, and Junfei Xiao, for their feedback and constructive suggestions at several stages of the\nproject. Computational resources were provided by Machine Learning and Data Analytics Laboratory,\nNanyang Technological University. The authors thank the administrator Sung Kheng Yeo for his\ntechnical support.\n\nReferences\n\n1] Andrea Acevedo, Anna Merino, Santiago Alférez, Angel Molina, Laura Boldt, and José Rodellar. A\ndataset of microscopic peripheral blood cell images for development of automatic recognition systems.\nData in Brief, 30, 2020.\n\n2] Sharat Agarwal, Himanshu Arora, Saket Anand, and Chetan Arora. Contextual diversity for active learning.\nArXiv, abs/2008.05723, 2020.\n\n3] Shekoofeh Azizi, Basil Mustafa, Fiona Ryan, Zachary Beaver, Jan Freyberg, Jonathan Deaton, Aaron Loh,\nAlan Karthikesalingam, Simon Kornblith, Ting Chen, et al. Big self-supervised models advance medical\nimage classification. arXiv preprint arXiv:2101.05224, 2021.\n\n4] Maria-Florina Balcan, Andrei Broder, and Tong Zhang. Margin based active learning. In International\nConference on Computational Learning Theory, pages 35-50. Springer, 2007.\n\n5] Javad Zolfaghari Bengar, Joost van de Weijer, Bartlomiej Twardowski, and Bogdan Raducanu. Reducing\nlabel effort: Self-supervised meets active learning. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, pages 1631-1639, 2021.\n\n6] Yoshua Bengio, Jér6me Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In JCML\n’09, 2009.\n\n7) Maxim Berman, Hervé Jégou, Andrea Vedaldi, Iasonas Kokkinos, and Matthijs Douze. Multigrain: a\nunified image embedding for classes and instances. ArXiv, abs/1902.05509, 2019.\n\n8] Patrick Bilic, Patrick Ferdinand Christ, Eugene Vorontsov, Grzegorz Chlebus, Hao Chen, Qi Dou, Chi-Wing\nFu, Xiao Han, Pheng-Ann Heng, Jiirgen Hesser, et al. The liver tumor segmentation benchmark (its).\narXiv preprint arXiv: 1901.04056, 2019.\n\n9] Jestis Bobadilla, Fernando Ortega, Antonio Hernando, and Jestis Bernal. A collaborative filtering approach\nto mitigate the new user cold start problem. Knowledge-based systems, 26:225-238, 2012.\n\n0] Alexander Borisov, Eugene Tuv, and George Runger. Active batch learning with stochastic query by forest.\nIn JMLR: Workshop and Conference Proceedings (2010). Citeseer, 2010.\n\n1] Akshay L Chandra, Sai Vikas Desai, Chaitanya Devaguptapu, and Vineeth N Balasubramanian. On initial\npools for deep active learning. In NeurIPS 2020 Workshop on Pre-registration in Machine Learning, pages\n14-32. PMLR, 2021.\n\n2] Haw-Shiuan Chang, Erik Learned-Miller, and Andrew McCallum. Active bias: Training more accurate\nneural networks by emphasizing high variance samples. Advances in Neural Information Processing\nSystems, 30, 2017.\n\n3] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for\ncontrastive learning of visual representations. arXiv preprint arXiv:2002.05709, 2020.\n\n4] Xinlei Chen, Haogi Fan, Ross Girshick, and Kaiming He. Moco demo: Cifar-10. https:\n//colab.research. google. com/github/facebookresearch/moco/blob/colab-notebook/\ncolab/moco_cifar10_demo.ipynb. Accessed: 2022-05-26.\n", "vlm_text": "Potential societal impacts.  Real-world data often exhibit long-tailed distributions, rather than the ideal uniform distributions over each class. We improve active learning by enforcing label diversity and hard-to-contrast data. However, we only extensively test our strategies on academic datasets. In many other real-world domains such as robotics and autonomous driving, the data may impose additional constraints on annotation accessibility or learning dynamics, e.g., being fair or private. We focus on standard accuracy and AUC as our evaluation metrics while ignoring other ethical issues in imbalanced data, especially in underrepresented minority classes. \nAcknowledgements \nThis work was supported by the Lustgarten Foundation for Pancreatic Cancer Research. The authors want to thank Mingfei Gao for the discussion of initial query quantity and suggestions on the implementation of consistency-based active learning framework. The authors also want to thank Guy Hacohen, Yuanhan Zhang, Akshay L. Chandra, Jingkang Yang, Hao Cheng, Rongkai Zhang, and Junfei Xiao, for their feedback and constructive suggestions at several stages of the project. Computational resources were provided by Machine Learning and Data Analytics Laboratory, Nanyang Technological University. The authors thank the administrator Sung Kheng Yeo for his technical support.\n\n \nReferences \n[1]  Andrea Acevedo, Anna Merino, Santiago Alférez, Ángel Molina, Laura Boldú, and José Rodellar. A dataset of microscopic peripheral blood cell images for development of automatic recognition systems. Data in Brief , 30, 2020.\n\n [2]  Sharat Agarwal, Himanshu Arora, Saket Anand, and Chetan Arora. Contextual diversity for active learning. ArXiv , abs/2008.05723, 2020.\n\n [3]  Shekoofeh Azizi, Basil Mustafa, Fiona Ryan, Zachary Beaver, Jan Freyberg, Jonathan Deaton, Aaron Loh, Alan Kart hikes a lingam, Simon Kornblith, Ting Chen, et al. Big self-supervised models advance medical image class i cation.  arXiv preprint arXiv:2101.05224 , 2021.\n\n [4]  Maria-Florina Balcan, Andrei Broder, and Tong Zhang. Margin based active learning. In  International Conference on Computational Learning Theory , pages 35–50. Springer, 2007.\n\n [5]  Javad Zolfaghari Bengar, Joost van de Weijer, Bartlomiej Twardowski, and Bogdan Raducanu. Reducing label effort: Self-supervised meets active learning. In  Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 1631–1639, 2021.\n\n [6]  Yoshua Bengio, Jérôme Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In  ICML ’09 , 2009.\n\n [7]  Maxim Berman, Hervé Jégou, Andrea Vedaldi, Iasonas Kokkinos, and Matthijs Douze. Multigrain: a uniﬁed image embedding for classes and instances.  ArXiv , abs/1902.05509, 2019.\n\n [8]  Patrick Bilic, Patrick Ferdinand Christ, Eugene Vorontsov, Grzegorz Chlebus, Hao Chen, Qi Dou, Chi-Wing Fu, Xiao Han, Pheng-Ann Heng, Jürgen Hesser, et al. The liver tumor segmentation benchmark (lits). arXiv preprint arXiv:1901.04056 , 2019.\n\n [9]  Jesús Bobadilla, Fernando Ortega, Antonio Hernando, and Jesús Bernal. A collaborative ﬁltering approach to mitigate the new user cold start problem.  Knowledge-based systems , 26:225–238, 2012.\n\n [10]  Alexander Borisov, Eugene Tuv, and George Runger. Active batch learning with stochastic query by forest. In  JMLR: Workshop and Conference Proceedings (2010) . Citeseer, 2010.\n\n [11]  Akshay L Chandra, Sai Vikas Desai, Chaitanya Deva gupta pu, and Vineeth N Bala subramania n. On initial pools for deep active learning. In  NeurIPS 2020 Workshop on Pre-registration in Machine Learning , pages 14–32. PMLR, 2021.\n\n [12]  Haw-Shiuan Chang, Erik Learned-Miller, and Andrew McCallum. Active bias: Training more accurate neural networks by emphasizing high variance samples.  Advances in Neural Information Processing Systems , 30, 2017.\n\n [13]  Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations.  arXiv preprint arXiv:2002.05709 , 2020.\n\n [14]  Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Moco demo: Cifar-10. https: //colab.research.google.com/github/facebook research/moco/blob/colab-notebook/ colab/mo co ci far 10 demo.ipynb . Accessed: 2022-05-26. "}
{"page": 9, "image_path": "doc_images/2210.02442v1_9.jpg", "ocr_text": "20\n\n21\n\n22\n\n23\n\n24\n\n25\n\n26\n\n27\n\n28\n\n29\n30\n\n31\n\n32\n\n33\n\n34\n\n35\n\n36\n\nXinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum contrastive\nlearning. arXiv preprint arXiv:2003.04297, 2020.\n\nReza Zanjirani Farahani and Masoud Hekmatfar. Facility location: concepts, models, algorithms and case\nstudies. Springer Science & Business Media, 2009.\n\nYarin Gal, Riashat Islam, and Zoubin Ghahramani. Deep bayesian active learning with image data. In\nInternational Conference on Machine Learning, pages 1183-1192. PMLR, 2017.\n\nMingfei Gao, Zizhao Zhang, Guo Yu, Sercan 6 Ank, Larry S Davis, and Tomas Pfister. Consistency-based\n\nsemi-supervised active learning: Towards minimizing labeling cost. In European Conference on Computer\nVision, pages 510-526. Springer, 2020.\n\nPriya Goyal, Dhruv Mahajan, Abhinav Gupta, and Ishan Misra. Scaling and benchmarking self-supervised\nvisual representation learning. In Proceedings of the IEEE International Conference on Computer Vision,\npages 6391-6400, 2019.\n\nGuy Hacohen, Avihu Dekel, and Daphna Weinshall. Active learning on a budget: Opposite strategies suit\nhigh and low budgets. ArXiv, abs/2202.02794, 2022.\n\nElad Hoffer, Tal Ben-Nun, Itay Hubara, Niv Giladi, Torsten Hoefler, and Daniel Soudry. Augment your\nbatch: Improving generalization through instance repetition. 2020 IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), pages 8126-8135, 2020.\n\nAlex Holub, Pietro Perona, and Michael C Burl. Entropy-based active learning for object recognition. In\n2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops, pages\n1-8. IEEE, 2008.\n\nNeil Houlsby, José Miguel Hernandez-Lobato, and Zoubin Ghahramani. Cold-start active learning with\n\nrobust ordinal matrix factorization. In /nternational conference on machine learning, pages 766-774.\nPMLR, 2014.\n\nSiyu Huang, Tianyang Wang, Haoyi Xiong, Jun Huan, and Dejing Dou. Semi-supervised active learning\nwith temporal output discrepancy. In Proceedings of the IEEE/CVF International Conference on Computer\nVision, pages 3447-3456, 2021.\n\nShruti Jadon. Covid-19 detection from scarce chest x-ray image data using few-shot deep learning approach.\nIn Medical Imaging 2021: Imaging Informatics for Healthcare, Research, and Applications, volume 11601,\npage 116010X. International Society for Optics and Photonics, 2021.\n\nSiddharth Karamcheti, Ranjay Krishna, Li Fei-Fei, and Christopher D Manning. Mind your outliers!\n\ninvestigating the negative impact of outliers on active learning for visual question answering. arXiv preprint\narXiv:2107.02331, 2021.\n\nJakob Nikolas Kather, Johannes Krisam, Pornpimol Charoentong, Tom Luedde, Esther Herpel, Cleo-Aron\nWeis, Timo Gaiser, Alexander Marx, Nektarios A. Valous, Dyke Ferber, Lina Jansen, Constantino Carlos\nReyes-Aldasoro, Inka Zérnig, Dirk Jager, Hermann Brenner, Jenny Chang-Claude, Michael Hoffmeister,\nand Niels Halama. Predicting survival from colorectal cancer histology slides using deep learning: A\nretrospective multicenter study. PLoS Medicine, 16, 2019.\n\nAndreas Kirsch, Joost Van Amersfoort, and Yarin Gal. Batchbald: Efficient and diverse batch acquisition\nfor deep bayesian active learning. Advances in neural information processing systems, 32, 2019.\n\nAlex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.\nAdrian Lang, Christoph Mayer, and Radu Timofte. Best practices in pool-based active learning for image\nclassification. 2021.\n\nYunfan Li, Peng Hu, Zitao Liu, Dezhong Peng, Joey Tianyi Zhou, and Xi Peng. Contrastive clustering. In\n2021 AAAI Conference on Artificial Intelligence (AAAI), 2021.\n\nGeert Litjens, Thijs Kooi, Babak Ehteshami Bejnordi, Arnaud Arindra Adiyoso Setio, Francesco Ciompi,\nMohsen Ghafoorian, Jeroen Awm Van Der Laak, Bram Van Ginneken, and Clara I Sanchez. A survey on\ndeep learning in medical image analysis. Medical image analysis, 42:60-88, 2017.\n\nKaterina Margatina, Loic Barrault, and Nikolaos Aletras. Bayesian active learning with pretrained language\nmodels. arXiv preprint arXiv:2104.08320, 2021.\n\nChristoph Mayer and Radu Timofte. Adversarial sampling for active learning. In Proceedings of the\nIEEE/CVF Winter Conference on Applications of Computer Vision, pages 3071-3079, 2020.\n\nLeland McInnes, John Healy, and James Melville. Umap: Uniform manifold approximation and projection\nfor dimension reduction. arXiv preprint arXiv: 1802.03426, 2018.\n\nSudhanshu Mittal, Maxim Tatarchenko, Ozgiin Cicek, and Thomas Brox. Parting with illusions about deep\nactive learning. arXiv preprint arXiv: 1912.05361, 2019.\n\n10\n", "vlm_text": "[15]  Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum contrastive learning.  arXiv preprint arXiv:2003.04297 , 2020.\n\n [16]  Reza Zanjirani Farahani and Masoud Hekmatfar.  Facility location: concepts, models, algorithms and case studies . Springer Science & Business Media, 2009.\n\n [17]  Yarin Gal, Riashat Islam, and Zoubin Ghahramani. Deep bayesian active learning with image data. In International Conference on Machine Learning , pages 1183–1192. PMLR, 2017.\n\n [18]  Mingfei Gao, Zizhao Zhang, Guo Yu, Sercan Ö Arık, Larry S Davis, and Tomas Pﬁster. Consistency-based semi-supervised active learning: Towards minimizing labeling cost. In  European Conference on Computer Vision , pages 510–526. Springer, 2020.\n\n [19]  Priya Goyal, Dhruv Mahajan, Abhinav Gupta, and Ishan Misra. Scaling and benchmarking self-supervised visual representation learning. In  Proceedings of the IEEE International Conference on Computer Vision , pages 6391–6400, 2019.\n\n [20]  Guy Hacohen, Avihu Dekel, and Daphna Weinshall. Active learning on a budget: Opposite strategies suit high and low budgets.  ArXiv , abs/2202.02794, 2022.\n\n [21]  Elad Hoffer, Tal Ben-Nun, Itay Hubara, Niv Giladi, Torsten Hoeﬂer, and Daniel Soudry. Augment your batch: Improving generalization through instance repetition.  2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pages 8126–8135, 2020.\n\n [22] Alex Holub, Pietro Perona, and Michael C Burl. Entropy-based active learning for object recognition. In 2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops , pages 1–8. IEEE, 2008.\n\n [23]  Neil Houlsby, José Miguel Hernández-Lobato, and Zoubin Ghahramani. Cold-start active learning with robust ordinal matrix factorization. In  International conference on machine learning , pages 766–774. PMLR, 2014.\n\n [24]  Siyu Huang, Tianyang Wang, Haoyi Xiong, Jun Huan, and Dejing Dou. Semi-supervised active learning with temporal output discrepancy. In  Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 3447–3456, 2021.\n\n [25]  Shruti Jadon. Covid-19 detection from scarce chest x-ray image data using few-shot deep learning approach. In  Medical Imaging 2021: Imaging Informatics for Healthcare, Research, and Applications , volume 11601, page 116010X. International Society for Optics and Photonics, 2021.\n\n [26]  Siddharth Karamcheti, Ranjay Krishna, Li Fei-Fei, and Christopher D Manning. Mind your outliers! investigating the negative impact of outliers on active learning for visual question answering.  arXiv preprint arXiv:2107.02331 , 2021.\n\n [27]  Jakob Nikolas Kather, Johannes Krisam, Pornpimol Char oen tong, Tom Luedde, Esther Herpel, Cleo-Aron Weis, Timo Gaiser, Alexander Marx, Nektarios A. Valous, Dyke Ferber, Lina Jansen, Constantin o Carlos Reyes-Aldasoro, Inka Zörnig, Dirk Jäger, Hermann Brenner, Jenny Chang-Claude, Michael Hoff meister, and Niels Halama. Predicting survival from colorectal cancer histology slides using deep learning: A retrospective multi center study.  PLoS Medicine , 16, 2019.\n\n [28]  Andreas Kirsch, Joost Van Amersfoort, and Yarin Gal. Batchbald: Efﬁcient and diverse batch acquisition for deep bayesian active learning.  Advances in neural information processing systems , 32, 2019.\n\n [29] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.\n\n [30]  Adrian Lang, Christoph Mayer, and Radu Timofte. Best practices in pool-based active learning for image class i cation. 2021.\n\n [31]  Yunfan Li, Peng Hu, Zitao Liu, Dezhong Peng, Joey Tianyi Zhou, and Xi Peng. Contrastive clustering. In 2021 AAAI Conference on Artiﬁcial Intelligence (AAAI) , 2021.\n\n [32]  Geert Litjens, Thijs Kooi, Babak Ehteshami Bejnordi, Arnaud Arindra Adiyoso Setio, Francesco Ciompi, Mohsen Ghafoorian, Jeroen Awm Van Der Laak, Bram Van Ginneken, and Clara I Sánchez. A survey on deep learning in medical image analysis.  Medical image analysis , 42:60–88, 2017.\n\n [33]  Katerina Margatina, Loic Barrault, and Nikolaos Aletras. Bayesian active learning with pretrained language models.  arXiv preprint arXiv:2104.08320 , 2021.\n\n [34]  Christoph Mayer and Radu Timofte. Adversarial sampling for active learning. In  Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision , pages 3071–3079, 2020.\n\n [35]  Leland McInnes, John Healy, and James Melville. Umap: Uniform manifold approximation and projection for dimension reduction.  arXiv preprint arXiv:1802.03426 , 2018.\n\n [36]  Sudhanshu Mittal, Maxim Tatar chen ko, Özgün Çiçek, and Thomas Brox. Parting with illusions about deep active learning.  arXiv preprint arXiv:1912.05361 , 2019. "}
{"page": 10, "image_path": "doc_images/2210.02442v1_10.jpg", "ocr_text": "37\n\n38\n\n39\n\n50\n\n51\n\n52\n\n53\n\n54\n\n55\n\n56\n\n57\n\n58\n\n59\n\n60\n\nVishwesh Nath, Dong Yang, Holger R Roth, and Daguang Xu. Warm start active learning with proxy labels\nand selection via semi-supervised fine-tuning. In Jnternational Conference on Medical Image Computing\nand Computer-Assisted Intervention, pages 297-308. Springer, 2022.\n\nKossar Pourahmadi, Parsa Nooralinejad, and Hamed Pirsiavash. A simple baseline for low-budget active\nlearning. arXiv preprint arXiv:2110.12033, 2021.\n\nTian Qiu, Guang Chen, Zi-Ke Zhang, and Tao Zhou. An item-oriented recommendation algorithm on\ncold-start problem. EPL (Europhysics Letters), 95(5):58003, 2011.\n\nJoshua Robinson, Ching-Yao Chuang, Suvrit Sra, and Stefanie Jegelka. Contrastive learning with hard\nnegative samples. arXiv preprint arXiv:2010.04592, 2020.\n\nOzan Sener and Silvio Savarese. Active learning for convolutional neural networks: A core-set approach.\narXiv preprint arXiv: 1708.00489, 2017.\n\nBurr Settles. Active learning literature survey. 2009.\nChangjian Shui, Fan Zhou, Christian Gagné, and Boyu Wang. Deep active learning: Unified and principled\n\nmethod for query and training. In International Conference on Artificial Intelligence and Statistics, pages\n1308-1318. PMLR, 2020.\n\nOriane Siméoni, Mateusz Budnik, Yannis Avrithis, and Guillaume Gravier. Rethinking deep active learning:\nUsing unlabeled data at model training. In 2020 25th International Conference on Pattern Recognition\n(ICPR), pages 1220-1227. IEEE, 2021.\n\nSamarth Sinha, Sayna Ebrahimi, and Trevor Darrell. Variational adversarial active learning. In Proceedings\nof the IEEE/CVF International Conference on Computer Vision, pages 5972-5981, 2019.\n\nBen Sorscher, Robert Geirhos, Shashank Shekhar, Surya Ganguli, and Ari S Morcos. Beyond neural\nscaling laws: beating power law scaling via data pruning. arXiv preprint arXiv:2206.14486, 2022.\nJamshid Sourati, Ali Gholipour, Jennifer G Dy, Xavier Tomas-Fernandez, Sila Kurugol, and Simon K\nWarfield. Intelligent labeling based on fisher information for medical image segmentation using deep\nlearning. JEEE transactions on medical imaging, 38(11):2642-2653, 2019.\n\nSwabha Swayamdipta, Roy Schwartz, Nicholas Lourie, Yizhong Wang, Hannaneh Hajishirzi, Noah A\nSmith, and Yejin Choi. Dataset cartography: Mapping and diagnosing datasets with training dynamics.\narXiv preprint arXiv:2009.10795, 2020.\n\nZhan Tong, Yibing Song, Jue Wang, and Limin Wang. Videomae: Masked autoencoders are data-efficient\nlearners for self-supervised video pre-training. arXiv preprint arXiv:2203.12602, 2022.\n\nHugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv’e\nJegou. Training data-efficient image transformers & distillation through attention. In JCML, 2021.\n\nWouter Van Gansbeke, Simon Vandenhende, Stamatios Georgoulis, Marc Proesmans, and Luc Van Gool.\nScan: Learning to classify images without labels. In European Conference on Computer Vision, pages\n268-285. Springer, 2020.\n\nDan Wang and Yi Shang. A new active labeling method for deep learning. In 20/4 International joint\nconference on neural networks (IJCNN), pages 112-119. IEEE, 2014.\n\nJiancheng Yang, Rui Shi, Donglai Wei, Zequan Liu, Lin Zhao, Bilian Ke, Hanspeter Pfister, and Bingbing\nNi. Medmnist v2: A large-scale lightweight benchmark for 2d and 3d biomedical image classification.\narXiv preprint arXiv:2110.14795, 2021.\n\nOfer Yehuda, Avihu Dekel, Guy Hacohen, and Daphna Weinshall. Active learning through a covering lens.\narXiv preprint arXiv:2205.11320, 2022.\n\nMichelle Yuan, Hsuan-Tien Lin, and Jordan Boyd-Graber. Cold-start active learning through self-supervised\nlanguage modeling. arXiv preprint arXiv:2010.09535, 2020.\n\nZi-Ke Zhang, Chuang Liu, Yi-Cheng Zhang, and Tao Zhou. Solving the cold-start problem in recommender\nsystems with social tags. EPL (Europhysics Letters), 92(2):28002, 2010.\n\nEvgenii Zheltonozhskii, Chaim Baskin, Alex M Bronstein, and Avi Mendelson. Self-supervised learning\nfor large-scale unsupervised image clustering. arXiv preprint arXiv:2008.10312, 2020.\n\nS Kevin Zhou, Hayit Greenspan, Christos Davatzikos, James S Duncan, Bram van Ginneken, Anant\nMadabhushi, Jerry L Prince, Daniel Rueckert, and Ronald M Summers. A review of deep learning in\nmedical imaging: Imaging traits, technology trends, case studies with progress highlights, and future\npromises. Proceedings of the IEEE, 2021.\n\nZongwei Zhou. Towards Annotation-Efficient Deep Learning for Computer-Aided Diagnosis. PhD thesis,\nArizona State University, 2021.\n\nZongwei Zhou, Jae Shin, Ruibin Feng, R Todd Hurst, Christopher B Kendall, and Jianming Liang.\nIntegrating active learning and transfer learning for carotid intima-media thickness video interpretation.\nJournal of digital imaging, 32(2):290-299, 2019.\n\n11\n", "vlm_text": "[37]  Vishwesh Nath, Dong Yang, Holger R Roth, and Daguang Xu. Warm start active learning with proxy labels and selection via semi-supervised ﬁne-tuning. In  International Conference on Medical Image Computing and Computer-Assisted Intervention , pages 297–308. Springer, 2022.\n\n [38]  Kossar Pourahmadi, Parsa Noor a line j ad, and Hamed Pirsiavash. A simple baseline for low-budget active learning.  arXiv preprint arXiv:2110.12033 , 2021.\n\n [39]  Tian Qiu, Guang Chen, Zi-Ke Zhang, and Tao Zhou. An item-oriented recommendation algorithm on cold-start problem.  EPL (Euro physics Letters) , 95(5):58003, 2011.\n\n [40]  Joshua Robinson, Ching-Yao Chuang, Suvrit Sra, and Stefanie Jegelka. Contrastive learning with hard negative samples.  arXiv preprint arXiv:2010.04592 , 2020.\n\n [41]  Ozan Sener and Silvio Savarese. Active learning for convolutional neural networks: A core-set approach. arXiv preprint arXiv:1708.00489 , 2017.\n\n [42] Burr Settles. Active learning literature survey. 2009.\n\n [43]  Changjian Shui, Fan Zhou, Christian Gagné, and Boyu Wang. Deep active learning: Uniﬁed and principled method for query and training. In  International Conference on Artiﬁcial Intelligence and Statistics , pages 1308–1318. PMLR, 2020.\n\n [44]  Oriane Siméoni, Mateusz Budnik, Yannis Avrithis, and Guillaume Gravier. Rethinking deep active learning: Using unlabeled data at model training. In  2020 25th International Conference on Pattern Recognition (ICPR) , pages 1220–1227. IEEE, 2021.\n\n [45]  Samarth Sinha, Sayna Ebrahimi, and Trevor Darrell. Variation al adversarial active learning. In  Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 5972–5981, 2019.\n\n [46]  Ben Sorscher, Robert Geirhos, Shashank Shekhar, Surya Ganguli, and Ari S Morcos. Beyond neural scaling laws: beating power law scaling via data pruning.  arXiv preprint arXiv:2206.14486 , 2022.\n\n [47]  Jamshid Sourati, Ali Gholipour, Jennifer G Dy, Xavier Tomas-Fernandez, Sila Kurugol, and Simon K Warﬁeld. Intelligent labeling based on ﬁsher information for medical image segmentation using deep learning.  IEEE transactions on medical imaging , 38(11):2642–2653, 2019.\n\n [48]  Swabha S way am dip ta, Roy Schwartz, Nicholas Lourie, Yizhong Wang, Hannaneh Hajishirzi, Noah A Smith, and Yejin Choi. Dataset cartography: Mapping and diagnosing datasets with training dynamics. arXiv preprint arXiv:2009.10795 , 2020.\n\n [49]  Zhan Tong, Yibing Song, Jue Wang, and Limin Wang. Videomae: Masked auto encoders are data-efﬁcient learners for self-supervised video pre-training.  arXiv preprint arXiv:2203.12602 , 2022.\n\n [50]  Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre S a blay roll es, and Herv’e J’egou. Training data-efﬁcient image transformers & distillation through attention. In  ICML , 2021.\n\n [51]  Wouter Van Gansbeke, Simon Vanden he nde, Stamatios Georgoulis, Marc Proesmans, and Luc Van Gool. Scan: Learning to classify images without labels. In  European Conference on Computer Vision , pages 268–285. Springer, 2020.\n\n [52]  Dan Wang and Yi Shang. A new active labeling method for deep learning. In  2014 International joint conference on neural networks (IJCNN) , pages 112–119. IEEE, 2014.\n\n [53]  Jiancheng Yang, Rui Shi, Donglai Wei, Zequan Liu, Lin Zhao, Bilian Ke, Hanspeter Pﬁster, and Bingbing Ni. Medmnist v2: A large-scale lightweight benchmark for 2d and 3d biomedical image class i cation. arXiv preprint arXiv:2110.14795 , 2021.\n\n [54]  Ofer Yehuda, Avihu Dekel, Guy Hacohen, and Daphna Weinshall. Active learning through a covering lens. arXiv preprint arXiv:2205.11320 , 2022.\n\n [55]  Michelle Yuan, Hsuan-Tien Lin, and Jordan Boyd-Graber. Cold-start active learning through self-supervised language modeling.  arXiv preprint arXiv:2010.09535 , 2020.\n\n [56]  Zi-Ke Zhang, Chuang Liu, Yi-Cheng Zhang, and Tao Zhou. Solving the cold-start problem in recommend er systems with social tags.  EPL (Euro physics Letters) , 92(2):28002, 2010.\n\n [57] Evgenii Z helton oz hsk ii, Chaim Baskin, Alex M Bronstein, and Avi Mendelson. Self-supervised learning for large-scale unsupervised image clustering.  arXiv preprint arXiv:2008.10312 , 2020.\n\n [58]  S Kevin Zhou, Hayit Greenspan, Christos Davatzikos, James S Duncan, Bram van Ginneken, Anant Madabhushi, Jerry L Prince, Daniel Rueckert, and Ronald M Summers. A review of deep learning in medical imaging: Imaging traits, technology trends, case studies with progress highlights, and future promises.  Proceedings of the IEEE , 2021.\n\n [59]  Zongwei Zhou.  Towards Annotation-Efﬁcient Deep Learning for Computer-Aided Diagnosis . PhD thesis, Arizona State University, 2021.\n\n [60]  Zongwei Zhou, Jae Shin, Ruibin Feng, R Todd Hurst, Christopher B Kendall, and Jianming Liang. Integrating active learning and transfer learning for carotid intima-media thickness video interpretation. Journal of digital imaging , 32(2):290–299, 2019. "}
{"page": 11, "image_path": "doc_images/2210.02442v1_11.jpg", "ocr_text": "(61]\n\nZongwei Zhou, Jae Shin, Lei Zhang, Suryakanth Gurudu, Michael Gotway, and Jianming Liang. Fine-\ntuning convolutional neural networks for biomedical image analysis: actively and incrementally. In\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 7340-7349,\n2017.\n\nZongwei Zhou, Jae Y Shin, Suryakanth R Gurudu, Michael B Gotway, and Jianming Liang. Active,\ncontinual fine tuning of convolutional neural networks for reducing annotation efforts. Medical Image\nAnalysis, page 101997, 2021.\n\nYu Zhu, Jinghao Lin, Shibi He, Beidou Wang, Ziyu Guan, Haifeng Liu, and Deng Cai. Addressing the\nitem cold-start problem by attribute-driven active learning. JEEE Transactions on Knowledge and Data\nEngineering, 32(4):63 1-644, 2019.\n\n12\n", "vlm_text": "[61]  Zongwei Zhou, Jae Shin, Lei Zhang, Suryakanth Gurudu, Michael Gotway, and Jianming Liang. Fine- tuning convolutional neural networks for biomedical image analysis: actively and increment ally. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 7340–7349, 2017.\n\n [62]  Zongwei Zhou, Jae Y Shin, Suryakanth R Gurudu, Michael B Gotway, and Jianming Liang. Active, continual ﬁne tuning of convolutional neural networks for reducing annotation efforts.  Medical Image Analysis , page 101997, 2021.\n\n [63]  Yu Zhu, Jinghao Lin, Shibi He, Beidou Wang, Ziyu Guan, Haifeng Liu, and Deng Cai. Addressing the item cold-start problem by attribute-driven active learning.  IEEE Transactions on Knowledge and Data Engineering , 32(4):631–644, 2019. "}
{"page": 12, "image_path": "doc_images/2210.02442v1_12.jpg", "ocr_text": "A_ Implementation Configurations\n\nA.1_ Data Split\n\nPathMNIST with nine categories has 107,180 colorectal cancer tissue histopathological images\nextracted from Kather et al. [27], with 89,996/10,004/7,180 images for training/validation/testing.\nBloodMNIST contains 17,092 microscopic peripheral blood cell images extracted from Acevedo et\nal. [1] with eight categories, where 11,959/1,712/3,421 images for training/validation/testing.\nOrganAMNIST consists of the axial view abdominal CT images based on Bilic ef al. [8], with\n34,581/6,491/17,778 images of 11 categories for training/validation/testing. CIFAR-10-LT (p=100)\nconsists of a subset of CIFAR-10 [29], with 12,406/10,000 images for training/testing.\n\nA.2_ Training Recipe for Contrastive Learning\n\nPseudocode for Our Proposed Strategy. The algorithm 1 provides the pseudocode for our proposed\nhard-to-contrast initial query strategy, as elaborated in §2.\n\nAlgorithm 1: Active querying hard-to-contrast data\n\ninput:\nD = {x»}*_, {unlabeled dataset D contains M images}\nannotation budget B; the number of clusters A’; batch size N; the number of epochs F\nconstant 7; structure of encoder f, projection head g; augmentation T\n0), e € [1, E] {model parameters at epoch e during contrastive learning}\noutput:\nselected query Q\nQ=2\nfor epoche € {1,...,E}do\nfor sampled minibatch {z,, }\\_, do\nfor all n € {1,..., N} do\n\ndraw two augmentation functions t~T, t!~T\n\n# the first augmentation\nLn-1 = t(a&n)\nAon—1 = f (Len—1) # representation\nZ2n-1 = g(hon-1) # projection\n# the second augmentation\nBon = t!(Ln)\nhon = f (€2n) # representation\nZon = g(han) # projection\nend for\nfor alli € {1,...,2N} and j € {1,...,2N} do\nsig = &! 2; /(llzillllesll) # pairwise similarity\n\nexp(si,j)/T\n\nPig = Sy ines xP(rn)/7 # predicted probability of contrastive pre-text task\nend for\n1\nPoo (Ynltn) = 5 [P2n—1,2n + Pon2n-1]\nend for\nend for\n\nfor unlabeled images {z,,,}4_, do\njim =+4 ye Poe (y%, [atm)\nAssign 2,,, to one of the clusters computed by K-mean(h, Kx)\nend for\nfor all k € {1,...,K} do\nsort images in the cluster AK based on i in an ascending order\nquery labels for top B/K samples, yielding Qy\nQ= QUO:\nend for\nreturn QO\n\n13\n", "vlm_text": "A Implementation Con gu rations \nA.1 Data Split \nPathMNIST with nine categories has 107,180 colorectal cancer tissue his to pathological images extracted from Kather  et al . [ 27 ], with 89,996/10,004/7,180 images for training/validation/testing. BloodMNIST contains 17,092 microscopic peripheral blood cell images extracted from Acevedo  et al . [ 1 ] with eight categories, where 11,959/1,712/3,421 images for training/validation/testing. Organ AM NIST consists of the axial view abdominal CT images based on Bilic  et al . [ 8 ], with 34,581/6,491/17,778 images of 11 categories for training/validation/testing. CIFAR-10-LT (  $(\\rho{=}100)$  ) consists of a subset of CIFAR-10 [29], with 12,406/10,000 images for training/testing. \nA.2 Training Recipe for Contrastive Learning \nPseudocode for Our Proposed Strategy.  The algorithm 1 provides the pseudocode for our proposed hard-to-contrast initial query strategy, as elaborated in   $\\S2$  . \nAlgorithm 1:  Active querying hard-to-contrast data \ninput:  $\\mathbf{\\dot{\\mathcal{D}}}=\\{\\pmb{x}_{m}\\}_{m=1}^{M}$    { labeled dataset    $\\mathcal{D}$   contain  $M$   images} annotation budget  B ; the number of clusters  $K$  ; batch size  $N$  ; the number of epochs  $E$   $\\tau$  cture of encoder    $f$  , projection head    $g$  ; augmentation    $\\mathcal{T}$   $\\theta^{(e)},e\\in[1,E]$   ∈  {model parameters at epoch  $e$   during contrastive learning} output: ed query    $\\mathcal{Q}$   $\\boldsymbol{\\mathcal{Q}}=\\boldsymbol{\\mathcal{Q}}$  Q for  epoch    $e\\in\\{1,\\dots,E\\}$   do for  samp  $\\{\\pmb{x}_{n}\\}_{n=1}^{N}$    do for all  $n\\in\\{1,\\dots,N\\}$   ∈{ }  do draw two augmentation functions  $t\\!\\sim\\!\\tau$  ,    $t^{\\prime}\\!\\sim\\!\\tau$  # the ﬁrst augmentation  $\\begin{array}{r l}&{\\tilde{\\mathbf{x}}_{2n-1}=t(\\bar{\\mathbf{x}}_{n})}\\\\ &{\\mathbf{h}_{2n-1}=f(\\tilde{\\mathbf{x}}_{2n-1})}\\\\ &{z_{2n-1}=g(\\mathbf{h}_{2n-1})}\\end{array}$  # representation # projection # the second augmentation  $\\begin{array}{r}{\\tilde{\\mathbf{x}}_{2n}=t^{\\prime}(\\pmb{x}_{n})\\quad}\\\\ {\\pmb{h}_{2n}=f(\\tilde{\\pmb{x}}_{2n})\\quad}\\\\ {z_{2n}=g(\\pmb{h}_{2n})\\quad}\\end{array}$  # representation # projection end for for all    $i\\in\\{1,.\\,.\\,.\\,,2N\\}$   and    $j\\in\\{1,\\dots,2N\\}$   do  $s_{i,j}=z_{i}^{\\top}z_{j}/(\\|z_{i}\\|\\|z_{j}\\|)$  ∥ ∥∥ ∥ # pairwise similarity  $\\begin{array}{r}{p_{i,j}=\\frac{\\exp(s_{i,j})/\\tau}{\\sum_{n=1}^{2N}\\mathbb{1}_{[n\\neq i]}\\exp\\left(s_{i,n}\\right)/\\tau}}\\end{array}$  # predicted probability of contrastive pre-text task end for  $\\begin{array}{r}{p_{\\theta^{(e)}}(y_{n}^{*}|x_{n})=\\frac{1}{2}[p_{2n-1,2n}+p_{2n,2n-1}]}\\end{array}$  end for end for for  unlabeled images    $\\{\\pmb{x}_{m}\\}_{m=1}^{M}$    do  $\\begin{array}{r}{\\hat{\\mu}_{m}=\\frac{1}{E}\\sum_{e=1}^{E}p_{\\theta^{(e)}}\\bar{(y_{m}^{*}|x_{m})}}\\end{array}$  Assign  $\\pmb{x}_{m}$   to one of the clusters computed by  $K{\\mathrm{-mean}}(h,K)$  end for for all    $k\\in\\{1,\\ldots,K\\}$   do sort images in the cluster  $K$   based on  $\\hat{\\mu}$   in an ascending order query labels for top  $B/K$   samples, yielding  $Q_{k}$   ${\\bar{\\mathcal{Q}}}={\\dot{\\bar{\\mathcal{Q}}}}\\cup{\\mathcal{Q}}_{k}$  end for return    $\\mathcal{Q}$  "}
{"page": 13, "image_path": "doc_images/2210.02442v1_13.jpg", "ocr_text": "Table 3: Contrastive learning settings on MedMNIST and CIFAR-10-LT.\n\n(a) MedMNIST pre-training (b) CIFAR-10-LT pre-training\nconfig value config value\nbackbone ResNet-50 backbone ResNet-50\noptimizer SGD optimizer SGD\noptimizer momentum 0.9 optimizer momentum 0.9\nweight decay le-4 weight decay le-4\nbase learning rate! 0.03 base learning rate 0.03\nlearning rate schedule cosine decay learning rate schedule cosine decay\nwarmup epochs 5 warmup epochs 5\nepochs 200 epochs 800\nrepeated sampling [21] see Table 5 repeated sampling [21] none\naugmentation see Table 4 augmentation see Table 4\nbatch size 4096 batch size 512\nqueue length [15] 65536 queue length [15] 4096\nT (equation 1) 0.05 T (equation 1) 0.05\n\nIr = base_Irxbatchsize / 256 per the linear Ir scaling rule [19].\n\nTable 4: Data augmentations.\n\n(a) Augmentations for RGB images (b) Augmentations for OrganAMNIST\n\naugmentation augmentation\n\nhflip hflip\n\ncrop (0.08, 1] crop [0.08, 1]\n\ncolor jitter [0.4, 0.4, 0.4, 0.1], p=0.8 color jitter (0.4, 0.4, 0.4, 0.1], p=0.8\ngray scale rotation degrees=45\n\nGaussian blur | Omin=0.1, Omax=2.0, p=0.5\n\nPre-training Settings. Our settings mostly follow [15, 14]. Table 3a summarizes our contrastive\npre-training settings on MedMNIST, following [15]. Table 3a shows the corresponding pre-training\nsettings on CIFAR-10-LT, following the official MoCo demo on CIFAR-10 [14]. The contrastive\nlearning model is pre-trained on 2 NVIDIA RTX3090 GPUs with 24GB memory each. The\ntotal number of model parameters is 55.93 million, among which 27.97 million requires gradient\nbackpropagation.\n\nDataset Augmentation. We apply the same augmentation as in MoCo v2 [15] on all the images of\nRGB modalities to reproduce the optimal augmentation pipeline proposed by the authors, including\nPathMNIST, BloodMNIST, CIFAR-10-LT. Because OrganAMNIST is a grey scale CT image dataset,\nwe apply the augmentation in [3] designed for radiological images, replacing random gray scale and\nGaussian blur with random rotation. Table 4 shows the details of data augmentation.\n\nRepeated Augmentation. Our MoCo v2 pre-training is so fast in computation that data loading\nbecomes a new bottleneck that dominates running time in our setup. We perform repeated\naugmentation on MedMNIST datasets at the level of dataset, also to enlarge augmentation space\nand improve generalization. [21] proposed repeated augmentation in a growing batch mode to\nimprove generalization and convergence speed by reducing variances. This approach provokes a\nchallenge in computing resources. Recent works [21, 50, 7] proved that fixed batch mode also boosts\ngeneralization and optimization by increasing mutiplicity of augmentations as well as parameter\nupdates and decreasing the number of unique samples per batch, which holds the batch size fixed.\nBecause the original contrastive learning works [13, 15] were implemented on ImageNet dataset, we\nattempt to simulate the quantity of ImageNet per epoch to achieve optimal performances. The details\nare shown in Table 5.\n\nWe only applied repeated augmentation on MedMNIST, but not CIFAR-10-LT. This is because we\nfollow all the settings of the official CIFAR-10 demo [14] in which repeated augmentation is not\nemployed.\n\n14\n", "vlm_text": "The table shows data augmentation techniques along with their respective values:\n\n1. **hflip** - No specific value provided.\n2. **crop** - Range is \\([0.08, 1]\\).\n3. **color jitter** - Values are \\([0.4, 0.4, 0.4, 0.1]\\) with a probability \\(p=0.8\\).\n4. **gray scale** - No specific value provided.\n5. **Gaussian blur** - Values are \\(0.1, \\quad 0.2, \\quad p=0.5\\) (Note: Gaussian blur is partly cut off).\n\nEach row corresponds to a different augmentation method and its parameters.\nThe table lists various data augmentations along with their corresponding values:\n\n- **hflip**: No specific value provided.\n- **crop**: Range [0.08, 1].\n- **color jitter**: Values [0.4, 0.4, 0.4, 0.1], probability p = 0.8.\n- **rotation**: Degrees = 45.\nPre-training Settings.  Our settings mostly follow [ 15 ,  14 ]. Table 3a summarizes our contrastive pre-training settings on MedMNIST, following [ 15 ]. Table 3a shows the corresponding pre-training settings on CIFAR-10-LT, following the ofﬁcial MoCo demo on CIFAR-10 [ 14 ]. The contrastive learning model is pre-trained on 2 NVIDIA RTX3090 GPUs with 24GB memory each. The total number of model parameters is 55.93 million, among which 27.97 million requires gradient back propagation. \nDataset Augmentation.  We apply the same augmentation as in MoCo v2 [ 15 ] on all the images of RGB modalities to reproduce the optimal augmentation pipeline proposed by the authors, including PathMNIST, BloodMNIST, CIFAR-10-LT. Because Organ AM NIST is a grey scale CT image dataset, we apply the augmentation in [ 3 ] designed for radiological images, replacing random gray scale and Gaussian blur with random rotation. Table 4 shows the details of data augmentation. \nRepeated Augmentation.  Our MoCo v2 pre-training is so fast in computation that data loading becomes a new bottleneck that dominates running time in our setup. We perform repeated augmentation on MedMNIST datasets at the level of dataset, also to enlarge augmentation space and improve generalization. [ 21 ] proposed repeated augmentation in a growing batch mode to improve generalization and convergence speed by reducing variances. This approach provokes a challenge in computing resources. Recent works [ 21 ,  50 ,  7 ] proved that ﬁxed batch mode also boosts generalization and optimization by increasing muti pli city of augmentations as well as parameter updates and decreasing the number of unique samples per batch, which holds the batch size ﬁxed. Because the original contrastive learning works [ 13 ,  15 ] were implemented on ImageNet dataset, we attempt to simulate the quantity of ImageNet per epoch to achieve optimal performances. The details are shown in Table 5. \nWe only applied repeated augmentation on MedMNIST, but not CIFAR-10-LT. This is because we follow all the settings of the ofﬁcial CIFAR-10 demo [ 14 ] in which repeated augmentation is not employed. "}
{"page": 14, "image_path": "doc_images/2210.02442v1_14.jpg", "ocr_text": "Table 5: Repeated augmentation. For a faster model convergence, we apply repeated augmenta-\ntion [21, 49, 50] on MedMNIST by reproducing the large batch size and iteration numbers.\n\n# training repeated times # samples per epoch\nImageNet 1,281,167 1 1,281,167\nPathMNIST 89,996 14 1,259,944\nOrganAMNIST 34,581 37 1,279,497\nBloodMNIST 11,959 105 1,255,695\nCIFAR-10-LT(p=100) 12,406 1 12,406\n\nA.3 Training Recipe for MedMNIST and CIFAR-10\n\nBenchmark Settings. We evaluate the initial queries by the performance of model trained on the\nselected initial query, and present the results in Table 1, 7 and Figure 4. The benchmark experiments\nare performed on NVIDIA RTX 1080 GPUs, with the following settings in Table 6.\n\nCold Start Settings for Existing Active Querying Criteria. To compare the cold start performance\nof active querying criteria with random selection ( Figure 1), we trained a model with the test set and\napplied existing active querying criteria.\n\nTable 6: Benchmark settings. We apply the same settings for training MedMNIST, CIFAR-10, and\nCIFAR-10-LT.\n\nconfig value\nbackbone Inception-ResNet-v2\noptimizer SGD\nlearning rate 0.1\nlearning rate schedule reduce learning rate on plateau, factor=0.5, patience=8\nearly stopping patience 50\nmax epochs 10000\nflip, p=0.5\n: rotation, p=0.5, in 90, 180, or 270 degrees\n\naugmentation\n\nreverse color, p=0.1\n\nfade color, p=0.1, 80% random noises + 20% original image\n\nbatch size 128\n\n15\n", "vlm_text": "The table presents data on several datasets, including ImageNet, PathMNIST, OrganAMNIST, BloodMNIST, and CIFAR-10-LT (ρ=100). It contains three columns, each with specific details:\n\n1. **# training**: The number of training samples in each dataset.\n   - ImageNet: 1,281,167 samples\n   - PathMNIST: 89,996 samples\n   - OrganAMNIST: 34,581 samples\n   - BloodMNIST: 11,959 samples\n   - CIFAR-10-LT (ρ=100): 12,406 samples\n\n2. **repeated times**: The number of times the dataset is repeated to match a specific number of samples per epoch.\n   - ImageNet: 1\n   - PathMNIST: 14\n   - OrganAMNIST: 37\n   - BloodMNIST: 105\n   - CIFAR-10-LT (ρ=100): 1\n\n3. **# samples per epoch**: The total number of samples considered per epoch after applying the repetition.\n   - ImageNet: 1,281,167 samples\n   - PathMNIST: 1,259,944 samples\n   - OrganAMNIST: 1,279,497 samples\n   - BloodMNIST: 1,255,695 samples\n   - CIFAR-10-LT (ρ=100): 12,406 samples\n\nThe table is used to show how training samples are adjusted by repeating them to match a particular number of samples per epoch, providing a sense of uniformity across different datasets during training phases.\nA.3 Training Recipe for MedMNIST and CIFAR-10 \nBenchmark Settings.  We evaluate the initial queries by the performance of model trained on the selected initial query, and present the results in Table 1, 7 and Figure 4. The benchmark experiments are performed on NVIDIA RTX 1080 GPUs, with the following settings in Table 6. \nCold Start Settings for Existing Active Querying Criteria.  To compare the cold start performance of active querying criteria with random selection ( Figure 1), we trained a model with the test set and applied existing active querying criteria. \nThe table contains hyperparameters and configurations for a machine learning model:\n\n- **backbone**: Inception-ResNet-v2\n- **optimizer**: SGD\n- **learning rate**: 0.1\n- **learning rate schedule**: Reduce learning rate on plateau, factor=0.5, patience=8\n- **early stopping patience**: 50\n- **max epochs**: 10000\n- **augmentation**:\n  - Flip, p=0.5\n  - Rotation, p=0.5, in 90, 180, or 270 degrees\n  - Reverse color, p=0.1\n  - Fade color, p=0.1, 80% random noises + 20% original image\n- **batch size**: 128"}
{"page": 15, "image_path": "doc_images/2210.02442v1_15.jpg", "ocr_text": "B_ Additional Results on MedMNI\n\nB.1 Label Diversity is a Significant Add-on to Most Querying Strategies\n\nAs we present in Table 1, label diversity is an important underlying criterion in designing active\nquerying criteria. We plot the full results on all three MedMNIST datasets in Figure 6. Most existing\nactive querying strategies became more performant and robust in the presence of label diversity.\n\nBALD Consistency Coreset Margin VAAL Entropy\n(Kirsch ef al., 2017) (Gao et al., 2020) (Sener et al. 2017) (Balcan ef al., 2007) (Sinha ef al., 2019) (Wang et al., 2014)\n1.0) vs woe 1.0) . we 1.0; t _ 1.0 st 1.0) a Pr\n‘a ai” 4 : Ryd\nif ff iW await if\n0.9| 0.9 0.9 0.9 0.9| i Fy\n2 H\n. |\n0.8) 0.8) 0.8) 0.8) | 0.8)\nTo\" 10 To\" 10 TO” TO TO? TO TO? TO TO? TO\n# of images # of images # of images # of images # of images # of images\n(a) PathMNIST\n1.9 1.9 1.9 1.9 1.0) 1.0)\n7 we eal we Rad “o\nes # He 7? At\n' : ' Hi? i if\n0.9 rami 0.9} + 0.9) } 0.9) of 0.9 ‘ + 0.9} it\n2 t il i q\n? ' ft Ni }\n0.8| 08 08 0.8 0.8] 0.8} iH\n| i 4\nTo 10> 10 107 10> 10 10> 10 To7 10> 107 To10> 10\" “107-1010\n# of images # of images # of images # of images # of images # of images\n(b) OrganAMNIST\n1.9) 1.0 wen 10 worm 10 pr 10 1.0; een\n3 + we ye\né . a +\nA } f Py i\n0.9 og} + og 0.9 { 0.9 y 0.9\nfe) w Ht +\n2 \" {\n| t t\nog + 0.8] oa) | 03} + o.8| 0.8 |\nTO To\" TO TO 10: TO\" 10: we 49 TO\" TO TO\"\n# of images # of images # of images # of images # of images # of images\n\n(c) BloodMNIST\n\nFigure 6: [Extended from Table 1] Label diversity yields more performant and robust active\nquerying strategies. The experiments are conducted on three datasets in MedMNIST. The red and\ngray dots denote AUC scores of different active querying strategies with and without label diversity,\nrespectively. Most existing active querying strategies became more performant and robust in the\npresence of label diversity, e.g. BALD, Margin, VAAL, and Uncertainty in particular. Some gray dots\nare not plotted in the low budget regime because there are classes absent in the queries due to the\nselection bias.\n\n16\n", "vlm_text": "B Additional Results on MedMNIST \nB.1 Label Diversity is a Signiﬁcant Add-on to Most Querying Strategies \nAs we present in Table 1, label diversity is an important underlying criterion in designing active querying criteria. We plot the full results on all three MedMNIST datasets in Figure 6. Most existing active querying strategies became more performant and robust in the presence of label diversity. \nThe image shows a series of graphs demonstrating the impact of label diversity on the performance of different active querying strategies across three datasets within MedMNIST: PathMNIST, OrganMNIST, and BloodMNIST. Each graph plots the AUC (Area Under the Curve) against the number of images, comparing the performance of various active querying strategies with (red dots) and without (gray dots) label diversity.\n\nThe strategies analyzed include:\n\n1. BALD (Bayesian Active Learning by Disagreement)\n2. Consistency\n3. Coreset\n4. Margin\n5. VAAL (Variational Adversarial Active Learning)\n6. Entropy\n\nThe graphs indicate that most active querying strategies, such as BALD, Margin, VAAL, and Uncertainty, show improved performance and robustness in the presence of label diversity. The red dots generally lie above the gray dots, suggesting that incorporating label diversity results in higher AUC scores. Some gray dots are missing in low budget regimes due to selection bias causing class absence in queries."}
{"page": 16, "image_path": "doc_images/2210.02442v1_16.jpg", "ocr_text": "B.2_ Contrastive Features Enable Label Diversity to Mitigate Bias\n\nOur proposed active querying strategy is capable of covering the majority of classes in most low\nbudget scenarios by integrating K-means clustering and contrastive features, including the tail classes\n(e.g. femur-left, basophil). Compared to the existing active querying criteria, we achieve the best\nclass coverage of selected query among at all budgets presented in Table 2.\n\nRandom Consistency VAAL — Entropy Coreset BALD Ours\n\nbladder = | =\nfemur-left — —\nfemur-right = =\nheart —_\nkidney-left a —\nkidney-right — a\nliver __—_ —\nlung-left LL —\nlung-right a a\npancreas —— i\nspleen ———\n\n(a) OrganAMNIST\n\nRandom Consistency VAAL Margin Entropy Coreset BALD Ours\n\nbasophil = __| _ |\neosinophil — ! |\nerythroblast —_ a |_| —_ —_\nig | — ae\nlymphocyte LI a _ Ld\nmonocyte __ a ——\nneutrophil _ | _ |\nplatelet — | l\n\n(b) BloodMNIST\n\nFigure 7: [Continued from Figure 2] Our querying strategy yields better label diversity. Random\non the leftmost denotes the class distribution of randomly queried samples, which can also reflect the\napproximate class distribution of the entire dataset. As seen, even with a relatively larger initial query\nbudget (691 images, 2% of OrganAMNIST, and 2,391 images, 20% of BloodMNIST), most active\nquerying strategies are biased towards certain classes. For example in OrganAMNIST, VAAL prefers\nselecting data in the femur-right and platelet class, but largely ignores data in the lung, liver and\nmonocyte classes. On the contrary, our querying strategy not only selects more data from minority\nclasses (e.g., femur-left and basophil) while retaining the class distribution of major classes.\n\n17\n", "vlm_text": "B.2 Contrastive Features Enable Label Diversity to Mitigate Bias \nOur proposed active querying strategy is capable of covering the majority of classes in most low budget scenarios by integrating K-means clustering and contrastive features, including the tail classes ( e.g . femur-left, basophil). Compared to the existing active querying criteria, we achieve the best class coverage of selected query among at all budgets presented in Table 2. \nThe image contains two sets of bar charts comparing different methods based on their performance for two datasets: OrganAMNIST and BloodMNIST. \n\n- The first set (labeled \"a\") represents OrganAMNIST with categories such as bladder, femur-left, heart, etc.\n- The second set (labeled \"b\") represents BloodMNIST with categories like basophil, eosinophil, lymphocyte, etc.\n\nEach set compares several methods like Random, Consistency, VAAL, Margin, Entropy, Coreset, BALD, and Ours, showing quantitative results for each category.\nFigure 7: [Continued from Figure 2]  Our querying strategy yields better label diversity.  Random on the leftmost denotes the class distribution of randomly queried samples, which can also reﬂect the approximate class distribution of the entire dataset. As seen, even with a relatively larger initial query budget (691 images,  $2\\%$   of Organ AM NIST, and 2,391 images,   $20\\%$   of BloodMNIST), most active querying strategies are biased towards certain classes. For example in Organ AM NIST, VAAL prefers selecting data in the femur-right and platelet class, but largely ignores data in the lung, liver and monocyte classes. On the contrary, our querying strategy not only selects more data from minority classes (e.g., femur-left and basophil) while retaining the class distribution of major classes. "}
{"page": 17, "image_path": "doc_images/2210.02442v1_17.jpg", "ocr_text": "Easy-to-contrast\n\nHard-to-contrast\n\n(a) PathMNIST (b) OrganAMNIST (c) BloodMNIST\n\nFigure 8: Visualization of -means clustering and our active selection. UMAP [35] is used to\nvisualize the feature clustering. Colors indicate the ground truth. Contrastive features clustered by\nthe K-means algorithm present a fairly clear separation in the 2D space, which helps enforce the\nlabel diversity without the need of ground truth. The crosses denote the selected easy- (top) and\nhard-to-contrast (bottom) data. Overall, hard-to-contrast data have a greater spread within each cluster\nthan easy-to-contrast ones. In addition, we find that easy-to-contrast tends to select outlier classes\nthat do not belong to the majority class in a cluster (see red arrows). This behavior will invalidate the\npurpose of clustering and inevitably jeopardize the label diversity.\n\nSelected Query Visualization. To ease the analysis, we project the image features (extracted by a\ntrained MoCo v2 encoder) onto a 2D space by UMAP [35]. The assigned pseudo labels have large\noverlap with ground truths, suggesting that the features from MoCo v2 are quite discriminative for\neach class. Overall, Figure 8 shows that hard-to-contrast queries have a greater spread within each\ncluster than easy-to-contrast ones. Both strategies can cover 100% classes. Nevertheless, we notice\nthat easy-to-contrast selects local outliers in clusters: samples that do not belong to the majority class\nin a cluster. Such behavior will invalidate the purpose of clustering, which is to query uniformly by\nseparating classes. Additionally, it possibly exposes the risk of introducing out-of-distribution data to\nthe query, which undermines active learning [26].\n\n18\n", "vlm_text": "The image shows a visualization of $K$-means clustering applied to features from three datasets: PathMNIST, OrganAMNIST, and BloodMNIST. The visualization uses UMAP to project features into a 2D space, with different colors representing different ground truth classes. The top row represents \"easy-to-contrast\" data, while the bottom row represents \"hard-to-contrast\" data.\n\nCrosses mark data points selected either as easy-to-contrast or hard-to-contrast within each cluster. In the easy-to-contrast visualizations, red arrows point to outlier classes that lie within clusters dominated by different classes. Such selections are considered outliers and could negatively impact the intended label diversity in clustering.\n\nOverall, the hard-to-contrast data appear to be more widely spread within each cluster compared to the easy-to-contrast data. This suggests differences in data distribution and selection within these clustering contexts.\nSelected Query Visualization.  To ease the analysis, we project the image features (extracted by a trained MoCo v2 encoder) onto a 2D space by UMAP [ 35 ]. The assigned pseudo labels have large overlap with ground truths, suggesting that the features from MoCo v2 are quite disc rim i native for each class. Overall, Figure 8 shows that hard-to-contrast queries have a greater spread within each cluster than easy-to-contrast ones. Both strategies can cover   $100\\%$   classes. Nevertheless, we notice that easy-to-contrast selects  local outliers  in clusters: samples that do not belong to the majority class in a cluster. Such behavior will invalidate the purpose of clustering, which is to query uniformly by separating classes. Additionally, it possibly exposes the risk of introducing out-of-distribution data to the query, which undermines active learning [26]. "}
{"page": 18, "image_path": "doc_images/2210.02442v1_18.jpg", "ocr_text": "C_ Experiments on CIFAR-10 and CIFAR-10-LT\n\nC.1_ Label Diversity is a Significant Add-on to Most Querying Strategies\n\nAs illustrated in Table 7 and Figure 9, label diversity is an important underlying criterion in designing\nactive querying criteria on CIFAR-10-LT, an extremely imbalanced dataset. We compare the results\nof CIFAR-10-LT with MedMNIST datasets Figure 6. CIFAR-10-LT is more imbalanced than\nMedMNIST, and the performance gain and robustness improvement of label diversity CIFAR-10-LT\nis significantly larger than MedMNIST. Most of the active querying strategies fail to query all the\nclasses even at relatively larger initial query budgets.\n\nTable 7: Diversity is a significant add-on to most querying strategies. AUC scores of different\nquerying strategies are compared on CIFAR-10 and CIFAR-10-LT. In the low budget regime (e.g. 10%\nand 20% of the entire dataset), active querying strategies benefit from enforcing the label diversity of\nthe selected data. The cells are highlighted in blue when adding diversity performs no worse than the\noriginal querying strategies. Some results are missing (marked as “-”) because the querying strategy\nfails to sample at least one data point for each class. Results of more sampling ratios are presented in\n\nAppendix Figure 9.\n\nCIFAR-10-LT\n1% 5% 10% 20% 30% 40%\nUnif. (142) (710) (1420) (2841) (4261) (5682)\n\nConsistency v 78.0+1.2 90.0+0.1 91.4+1.1 93.4+0.2 93.2+0.2 94.6+0.2\n\n: : 67.1417.1 88.6+0.3 90.4+0.6 90.7+0.2\n\nv 80.9+1.0 90.3£0.5 92.6+0.2 93.7£0.4 93.9+0.8 94.5+0.2\n\nVAAL - - - - - T13£1.6\n\nMargin v 81.2+1.8 88.7+0.7 91.7+0.9 93.2+0.2 94.5+0.1 94.7+0.4\n\n: : 81.9+0.8 86.3+0.3 87.4+0.2 88.1+0.1\n\nEntropy v T8.A+1.4 89.6+0.5 92.0+1.2 91.9+1.3 94.0+0.6 94.0+0.7\n\n: 79.0+1.2 65.6+15.6 86.4+0.2 88.5+0.2 89.5+0.7\n\nCoreset v 80.8+1.0 89.7+1.3 91.5+0.4 93.6+0.2 93.4+0.7 94.8+0.1\n\n: : 65.9+15.9 86.9+0.1 88.2+0.1 90.3+0.2\n\nBALD v 83.340.6 90.8£0.3 92.8+0.1 90.82.4 94.0+0.8 94.7+0.4\n\n- 76.8+2.3 64.9+14.9 84.7+0.6 88.0+0.5 88.9+0.1\n\nC.2 Contrastive Features Enable Label Diversity to Mitigate Bias\n\nOur proposed active querying strategy is capable of covering the majority of classes in most low\n\nbudget scenarios by integrating K-means clustering and contrastive features, including the tail classes\n(horse, ship, and truck). Compared to the existing active querying criteria, we achieve the best class\ncoverage of selected query among at all budgets presented in Table 2. As depicted in Figure 9, our\nquerying strategy has a more similar distribution to the overall distribution of dataset and successfully\ncovers all the classes, with the highest proportion of minor classes (ship and truch) among random\nselection and all active querying methods.\n\n19\n", "vlm_text": "C Experiments on CIFAR-10 and CIFAR-10-LT \nC.1 Label Diversity is a Signiﬁcant Add-on to Most Querying Strategies \nAs illustrated in Table 7 and Figure 9, label diversity is an important underlying criterion in designing active querying criteria on CIFAR-10-LT, an extremely imbalanced dataset. We compare the results of CIFAR-10-LT with MedMNIST datasets Figure 6. CIFAR-10-LT is more imbalanced than MedMNIST, and the performance gain and robustness improvement of label diversity CIFAR-10-LT is sign i cant ly larger than MedMNIST. Most of the active querying strategies fail to query all the classes even at relatively larger initial query budgets. \nTable 7:  Diversity is a signiﬁcant add-on to most querying strategies.  AUC scores of different querying strategies are compared on CIFAR-10 and CIFAR-10-LT. In the low budget regime ( e.g .   $10\\%$  and  $20\\%$   of the entire dataset), active querying strategies beneﬁt from enforcing the label diversity of the selected data. The cells are highlighted in blue when adding diversity performs no worse than the original querying strategies. Some results are missing (marked as “-”) because the querying strategy fails to sample at least one data point for each class. Results of more sampling ratios are presented in Appendix Figure 9. \nThis table appears to show performance metrics (possibly accuracy or a similar measure) on the CIFAR-10-LT dataset for different methods and data percentages. Here's a summary of its structure:\n\n- The methods tested are Consistency, VAAL, Margin, Entropy, Coreset, and BALD.\n- Each method is tested with and without uniform data sampling (Unif.).\n- The dataset percentages range from 1% (142 instances) to 40% (5682 instances).\n- The performance results are shown with a mean ± standard deviation format for each percentage increase.\n\nThe background shading on some cells could suggest a highlight of results or particular conditions.\nC.2 Contrastive Features Enable Label Diversity to Mitigate Bias \nOur proposed active querying strategy is capable of covering the majority of classes in most low budget scenarios by integrating K-means clustering and contrastive features, including the tail classes (horse, ship, and truck). Compared to the existing active querying criteria, we achieve the best class coverage of selected query among at all budgets presented in Table 2. As depicted in Figure 9, our querying strategy has a more similar distribution to the overall distribution of dataset and successfully covers all the classes, with the highest proportion of minor classes (ship and truch) among random selection and all active querying methods. "}
{"page": 19, "image_path": "doc_images/2210.02442v1_19.jpg", "ocr_text": "BALD Consistency Coreset Margin VAAL Entropy\n(Kirsch ef al. 2017) (Gao et al., 2020) (Sener ef al., 2017) (Balcan et al., 2007) (Sinha et al., 2019) (Wang et al., 2014)\n1.9 eum 19 won 19 ware TO a 1.9 we ho Pod\ni at : Ryd\niy As # t\n: wn ;\n0.9| 0.9| i 0.9| 0.9| 0.9| 09 th,\nQ 4\n\"4\nof og 0.8} 0.8) 0.8) | 4\nTO’ TO\" TO’ TO\" TO’ TO\" TO’ TO\" TO’ TO\" TO’ TO\"\n# of images # of images # of images # of images # of images # of images\n(a) CIFAR-10\n1.0; 1.0; 1.0;\nod t\nwt “ foe\n' : '\n0.3} fo +s og} 0.8\nca ” ” io\nOo ” +\n2 . .\n<\n+\n0.6)* og! 0.6) 0.6) 0.6)\" 0.6)\"\n# of images # of images # of images # of images # of images # of images\n\nFigure 9: Diversity yields more performant and robust active querying strategies.\n\n(b) CIFAR-10-LT\n\nThe\n\nexperiments are conducted on CIFAR-10-LT. The red and gray dots denote AUC scores of different\nactive querying strategies with and without label diversity, respectively. Observations are consistent\nwith those in medical applications (see Figure 6): Most existing active querying strategies became\nmore performant and robust in the presence of label diversity.\n\n20\n", "vlm_text": "The image consists of two sets of charts, each containing five subplots. Each subplot represents the performance of a different method for a specific dataset. The datasets and methods are as follows:\n\n1. **Dataset: CIFAR-10**\n   - Methods: \n     - BALD (Kirsch et al., 2017)\n     - Consistency (Gao et al., 2020)\n     - Coreset (Sener et al., 2017)\n     - Margin (Balcan et al., 2007)\n     - VAAL (Sinha et al., 2019)\n     - Entropy (Wang et al., 2014)\n\n2. **Dataset: SVHN**\n   - Same methods as listed above.\n\nFor each method, the x-axis represents the number of images, while the y-axis represents the Area Under the Curve (AUC). Red circles and lines represent performance metrics along with error bars, which are likely indicating variability or confidence intervals in the AUC measurements.\n\nThese plots are examining how well each method performs in terms of AUC as the number of images in the dataset is increased, in a semi-logarithmic scale (logarithmic x-axis and linear y-axis).\nFigure 9: Diversity yields more performant and robust active querying strategies. The experiments are conducted on CIFAR-10-LT. The red and gray dots denote AUC scores of different active querying strategies with and without label diversity, respectively. Observations are consistent with those in medical applications (see Figure 6): Most existing active querying strategies became more performant and robust in the presence of label diversity. "}
{"page": 20, "image_path": "doc_images/2210.02442v1_20.jpg", "ocr_text": "Random Consistency VAAL Margin Entropy Coreset BALD\n\n:\n\nRandom Consistency VAAL Margin Entropy Coreset BALD Ours\n\nairplane\nautomobile\ncat\n\nbird\n\ndeer\n\ndog\n\nfrog\n\nhorse\n\nship\n\ntruck\n\na) CIFAR-10\n\nairplane\nautomobile\ncat\n\nbird\n\ndeer\n\ndog\n\nfrog\n\nhorse\n\nship\n\ntruck\n\naap\n~ |\n\n(b) CIFAR-10-LT\n\nFigure 10: Our querying strategy yields better label diversity. Random on the leftmost denotes\nthe class distribution of randomly queried samples, which can also reflect the approximate class\ndistribution of the entire dataset. As seen, even with a relatively larger initial query budget (5,000\nimages, 10% of CIFAR-10, and 1420 images, 10% of CIFAR-10-LT), most active querying strategies\nare biased towards certain classes. Our querying strategy, on the contrary, is capable of selecting\nmore data from the minority classes such as horse, ship, and truck.\n\n21\n", "vlm_text": "The image contains two bar charts comparing different methods across various categories. Each chart shows the performance of different strategies (Random, Consistency, VAAL, Margin, Entropy, Coreset, BALD, and Ours) related to the CIFAR-10 and CIFAR-10-LT datasets.\n\n- **Top Chart (a) CIFAR-10**: Displaying performance measures on categories like airplane, automobile, cat, etc.\n- **Bottom Chart (b) CIFAR-10-LT**: Showing performance on the same categories for a long-tail version of the dataset.\n\nEach method appears to have varying levels of performance across these categories.\nFigure 10:  Our querying strategy yields better label diversity.  Random on the leftmost denotes the class distribution of randomly queried samples, which can also reﬂect the approximate class distribution of the entire dataset. As seen, even with a relatively larger initial query budget (5,000 images,  $10\\%$   of CIFAR-10, and 1420 images,   $10\\%$   of CIFAR-10-LT), most active querying strategies are biased towards certain classes. Our querying strategy, on the contrary, is capable of selecting more data from the minority classes such as horse, ship, and truck. "}
{"page": 21, "image_path": "doc_images/2210.02442v1_21.jpg", "ocr_text": "© adipose © debris @ smooth muscle\n\n@ background © lymphocytes © colorectal adenocarcinoma epithelium\n© cancer-associated stroma @ mucus © normal colon mucosa\n1.0\n0.8\nbE\nQ\n; »\nZz 206 \"\n€ 3 Easy-to-contrast\na 50.4\nis}\n0.2 —\n0.2 04 ce 02 04\nHard-to-learn variability variability Hard-to-contrast\n(b) Data Map by ground truth (c) Data Map by pseudo-labels\n@ = bladder @ — kidney-left @ —_lung-right © heart\n© femur-left © kidney-right pancreas © lung-left\n© femur-right @ itiver @ = spleen\n1.0\nbE 0.8\nQ\nZ »\n= 20.6\nz 8\n3) E\ng 0.4\nfo) 8\n\np 0.2 0.2 0.4\nHard-to-learn variability variability Hard-to-contrast\n(d) Overall distribution (e) Data Map by ground truth (f) Data Map by pseudo-labels\n\nFigure 11: Active querying based on Dataset Maps. (a,d) PathMNIST and OrganAMNIST dataset\noverview. (b,e) Easy- and hard-to-learn data can be selected from the maps based on ground truths [26].\nThis querying strategy has two limitations: (1) requiring manual annotations and (2) data are stratified\nby classes in the 2D space, leading to a poor label diversity in the selected queries. (c,f) Easy- and\nhard-to-contrast data can be selected from the maps based on pseudo labels. This querying strategy is\nlabel-free and the selected “hard-to-contrast” data represent the most common patterns in the entire\ndataset. These data are more suitable for training and thus alleviate the cold start problem.\n\n22\n", "vlm_text": "The image appears to display an analysis of two medical imaging datasets, PathMNIST and OrganAMNIST, focusing on an active querying approach using Dataset Maps to select data for training machine learning models. Here's a breakdown of the components shown:\n\n1. **PathMNIST Overview**:\n   - (a) Overall distribution: A visual representation of the PathMNIST dataset, showing a grid of histology images that depict various tissue types.\n   - (b) Data Map by ground truth: Shows a scatter plot of the images based on their confidence and variability, where data are stratified by different classes (e.g., adipose, smooth muscle, etc.). Easy-to-learn and hard-to-learn samples are marked, requiring manual annotations.\n   - (c) Data Map by pseudo-labels: Similar scatter plot as in (b), but based on pseudo-labels instead of ground truth. Easy-to-contrast and hard-to-contrast samples are identified, which does not require manual labels and helps mitigate the cold start problem.\n\n2. **OrganAMNIST Overview**:\n   - (d) Overall distribution: Displays a grid of grayscale images from the OrganAMNIST dataset, representing different organ scans.\n   - (e) Data Map by ground truth: A scatter plot showing data categorized by various organs using ground truth. It highlights easy-to-learn and hard-to-learn samples, again requiring manual annotations.\n   - (f) Data Map by pseudo-labels: Shows a scatter plot similar to (e) but uses pseudo-labels for stratification. Easy-to-contrast and hard-to-contrast samples are highlighted to enhance training.\n\nOverall, the image depicts a strategy for selecting training data from medical imaging datasets by comparing methods that rely on ground truth versus those using pseudo-labels, highlighting their relative advantages and challenges."}
{"page": 22, "image_path": "doc_images/2210.02442v1_22.jpg", "ocr_text": "© airplane @ deer e ship © cat\n\n®@ automobile © dog truck\n© bird @ = frog horse\n1.0\n0.8\ne ~N\n£06\nic] Easy-to-contrast\ne\n50.4\n8\n\n0.2\n0.2 0.4 0.0 0.40 0.45 . —\nHard-to-learn variability variability Hard-to-contrast\n(a) Overall distribution (b) Data Map by ground truth (c) Data Map by pseudo-labels\n\nFigure 12: Active querying based on Dataset Maps. (a) CIFAR-10-LT dataset overview. (b) Easy-\nand hard-to-learn data can be selected from the maps based on ground truths [26]. This querying\nstrategy has two limitations: (1) requiring manual annotations and (2) data are stratified by classes in\nthe 2D space, leading to a poor label diversity in the selected queries. (c) Easy- and hard-to-contrast\ndata can be selected from the maps based on pseudo labels. This querying strategy is label-free and\nthe selected “hard-to-contrast” data represent the most common patterns in the entire dataset. These\ndata are more suitable for training and thus alleviate the cold start problem.\n\n23\n", "vlm_text": "This image illustrates different querying strategies based on Dataset Maps for the CIFAR-10-LT dataset. It consists of three parts:\n\n1. **(a) Overall distribution**: This section shows a collage of images from the CIFAR-10-LT dataset, representing a general overview of the data distribution, with various classes such as airplanes, automobiles, birds, cats, etc., indicated by different colored dots.\n\n2. **(b) Data Map by ground truth**: This plot represents data points in a 2D space defined by 'confidence' and 'variability'. The data points are colored according to their class. Easy-to-learn and hard-to-learn data points are highlighted. Easy-to-learn data are those with high confidence and low variability, while hard-to-learn data have low confidence and high variability. Four example images are shown for each category. This method requires manual annotations and may lead to poor label diversity in selected queries.\n\n3. **(c) Data Map by pseudo-labels**: Similar to (b), this plot is generated using pseudo labels instead of ground truth. The plot helps identify easy-to-contrast and hard-to-contrast data, representing the most common patterns in the dataset. Easy-to-contrast data are located towards higher confidence and lower variability, while hard-to-contrast data are in areas of medium confidence and higher variability. Four example images are provided for each category. This strategy is label-free and better suited for training, helping alleviate the cold start problem by offering a more diverse selection of data points based on the model's understanding."}
{"page": 23, "image_path": "doc_images/2210.02442v1_23.jpg", "ocr_text": "Random Entropy Margin BALD Coreset\n\n1005 100 100 1005 1005\n904 90. 90: 904 904\n= 004 80 80 so 20\n8\n2 704 10 70 104 104\n-* Hard-to-Contrast\n604 60 60 60 60\n“® Easy-to-Contrast\n$00 8 80 4 Easy-tocLearn\n10 20 30 40 50 6d 10 20 30 40 50 G0 19 20 30 40 50 Go 10 20 30 40 50 60 10 20 30 40 50 8 4. Consistency\n(a) Training from scratch > Entropy\n-© Margin\n1005 1005 100. 1005 100. = BALD\n“= VAAL\n904 ss 904 90: 904 90.\n- Coreset\n= 804 804 80 804 80.\ngS\n2 70+ 70+ 10 14 10\n604 60+ 60: 604 60\n50 50 T T T T 1 50: T T T T 1 50° T T T T 1 50:\n40 20 30 40 50 60 10 20 30 40 50 60 10 20 30 40 50 60 10 20 30 40 50 60 10 20 30 40 50 60\n# of Labeled Images # of Labeled Images # of Labeled Images # of Labeled Images # of Labeled Images\n\n(b) Fine-tuning from self-supervised pre-training\n\nFigure 13: Performance of each active learning querying strategies with different initial query\nstrategies on BloodMNIST. Hard-to-contrast initial query strategy (red lines) outperforms other\ninitial query strategies in every cycle of active learning. With each active learning querying strategy,\nthe performance of the initial cycle (20 labeled images) and the last cycle (50 labeled images) are\nstrongly correlated.\n\nRandom Entropy Margin BALD Coreset\n100: 100: 1005 1005 1004\n\n=\n8\n2 70 7 no\n\n“© Hard-to-Contrast\n60 604 604 604 a\n0 -® Easy-to-Contrast\n50: T T T 1 50: 50+ T T T 1 50: T T 1 50: ~~ Easy-to-Leam\n10 20 30 40 s0 60 10 20 30 40 50 60 10 20 30 40 50 Go 10 20 30 40 50 60 10 20 30 40 50 6 6 Consistency\n(a) Training from scratch > Entropy\n“© Margin\n1005 1005 100. 100. 1005 = BALD\n904 a 90. 90. 904 oe VAAL\nNs ~+—21— —__. > Coreset\n= 00 804 of  w 804\ns\n2 704 704 10 1 704\n60 oo 60 60 604\n‘50 T T T 1 50 T 1 50-4 T T T 1 50+ T T 1 50:\n0 20 30 40 50 6D 10 20 30 40 50 G0 10 20 30 40 50 60 10 2 20 40 50 60 10 20 30 40 50 60\n#of Labeled Images #of Labeled Images # of Labeled Images # of Labeled Images #of Labeled Images\n\n(b) Fine-tuning from self-supervised pre-training\n\nFigure 14: Performance of each active learning querying strategies with different initial query\nstrategies on PathMNIST. Hard-to-contrast initial query strategy (red lines) outperforms other\ninitial query strategies in every cycle of active learning. With each active learning querying strategy,\nthe performance of the initial cycle (20 labeled images) and the last cycle (50 labeled images) are\nstrongly correlated.\n\n2:\n\n&\n", "vlm_text": "The image is a set of graphs comparing the performance of different methods in terms of AUC (Area Under the Curve) percentage against the number of labeled images used in training. There are two main sections: \"Training from scratch\" (a) on the left and \"Fine-tuning from self-supervised pre-training\" (b) on the right. Each section contains multiple subplots for different selection strategies: Random, Entropy, Margin, BALD, and Coreset.\n\nFor each subplot:\n- The x-axis represents the number of labeled images, ranging from 10 to 60.\n- The y-axis represents the AUC percentage ranging from 50% to 100%.\n\nVarious lines in the graphs correspond to different selection strategies, such as:\n- Hard-to-Contrast (marked in red)\n- Easy-to-Contrast (marked in black squares)\n- Easy-to-Learn (marked in black triangles)\n- Consistency (marked in black diamonds)\n- Entropy (marked in black circles)\n- Margin (marked in black inverted triangles)\n- BALD (marked in black squares)\n- VAAL (marked in black diamonds)\n- Coreset (marked in black circles)\n\nThe lines indicate the relative performance of these strategies across different numbers of labeled data. The graph aims to demonstrate how various strategies perform with increasing labeled data, comparing the effectiveness of training from scratch to fine-tuning from self-supervised pre-trained models.\nFigure 13:  Performance of each active learning querying strategies with different initial query strategies on BloodMNIST.  Hard-to-contrast initial query strategy (red lines) outperforms other initial query strategies in every cycle of active learning. With each active learning querying strategy, the performance of the initial cycle (20 labeled images) and the last cycle (50 labeled images) are strongly correlated. \nThe image consists of a series of graphs comparing the performance of different strategies for active learning, as shown by AUC (Area Under the Curve) percentages, with respect to the number of labeled images used. The comparison is between two scenarios: (a) \"Training from scratch\" and (b) \"Fine-tuning from self-supervised pre-training.\"\n\nEach graph represents a different active learning strategy: Random, Entropy, Margin, BALD, and Coreset. Within each graph, different lines indicate performance of various approaches, such as Hard-to-Contrast, Easy-to-Contrast, Easy-to-Learn, Consistency, Entropy, Margin, BALD, VAAL, and Coreset.\n\n- The x-axis represents the number of labeled images (ranging from 10 to 60).\n- The y-axis represents the AUC percentage, ranging from 50% to 100%.\n\nThe most prominent line across all graphs, as indicated in red, is the \"Hard-to-Contrast\" method, which consistently achieves higher AUC percentages compared to other methods plotted in black. The image illustrates the effect of pre-training on active learning methods' performance across different quantities of labeled data.\nFigure 14:  Performance of each active learning querying strategies with different initial query strategies on PathMNIST.  Hard-to-contrast initial query strategy (red lines) outperforms other initial query strategies in every cycle of active learning. With each active learning querying strategy, the performance of the initial cycle (20 labeled images) and the last cycle (50 labeled images) are strongly correlated. "}
