{"page": 0, "image_path": "doc_images/P18-1013_0.jpg", "ocr_text": "A Unified Model for Extractive and Abstractive Summarization\nusing Inconsistency Loss\n\nWan-Ting Hsu!, Chieh-Kai Lin', Ming-Ying Lee!, Kerui Min’, Jing Tang’, Min Sun!\n! National Tsing Hua University, 7 Cheetah Mobile\n{hsuwanting, axk51013, masonyl03}@gapp.nthu.edu.tw,\n{minkerui, tangjing}@cmcm.com, sunmin@ee.nthu.edu.tw\n\nAbstract\n\nWe propose a unified model combining the\nstrength of extractive and abstractive sum-\nmarization. On the one hand, a simple\nextractive model can obtain sentence-level\nattention with high ROUGE scores but\nless readable. On the other hand, a more\ncomplicated abstractive model can obtain\nword-level dynamic attention to generate\na more readable paragraph. In our model,\nsentence-level attention is used to mod-\nulate the word-level attention such that\nwords in less attended sentences are less\nlikely to be generated. Moreover, a novel\ninconsistency loss function is introduced\nto penalize the inconsistency between two\nlevels of attentions. By end-to-end train-\ning our model with the inconsistency loss\nand original losses of extractive and ab-\nstractive models, we achieve state-of-the-\nart ROUGE scores while being the most\ninformative and readable summarization\non the CNN/Daily Mail dataset in a solid\nhuman evaluation.\n\n1 Introduction\n\nText summarization is the task of automatically\ncondensing a piece of text to a shorter version\nwhile maintaining the important points. The abil-\nity to condense text information can aid many ap-\nplications such as creating news digests, present-\ning search results, and generating reports. There\nare mainly two types of approaches: extractive\nand abstractive. Extractive approaches assemble\nsummaries directly from the source text typically\nselecting one whole sentence at a time. In con-\ntrast, abstractive approaches can generate novel\nwords and phrases not copied from the source text.\n\nOriginal Article: McDonald’s says...... The company says it expects the new\n‘Artisan Grilled Chicken’ to be in its more than 14,300 U.S. stores by the end\nof next week, in products including a new sandwich, as well as existing sand-\nwiches, wraps and salads. It says the biggest change is the removal of sodium\nphosphates, which it said was used to keep the chicken moist, in favor of\nvegetable starch. The new recipe also does not use maltodextrin, which Mc-\nDonald’s said is generally used as a sugar to increase browning oras a carrier\nfor seasoning. Jessica Foust, director of culinary innovation at McDonald's,\nsaid the changes were made because customers said they want ‘simple, clean\ningredients’ they are familiar with...... And Panera Bread has said it plans to\npurge artificial colors, flavors and preservatives from its food by 2016\n\nExtractive Approach: The company says it expects the new ‘Artisan\nGrilled Chicken’ to be in its more than 14,300 U.S. stores by the end of\nnext week, in products including a new sandwich, as well as existing sand-\nwiches, wraps and salads. It says the biggest change is the removal of sodium\nphosphates, which it said was used to keep the chicken moist, in favor of\nvegetable starch, The new recipe also does not use maltodextrin, which Mc-\nDonald’s said is generally used as a sugar to increase browning or as a carrier\nfor seasoning.\n\nAbstractive Approach: McDonald’s says it expects the new ‘Artisan Grilled\nChicken’ to be in its more than 14,300 U.S. stores by the end of next week.\nThe company says the changes were made because customers said they want\n‘simple, clean ingredients’ they are familiar with. McDonald’s said it plans\nto purge artificial colors, flavors and preservatives from its food by 2016.\n\nUnified Approach: McDonald’s says it expects the new ‘Artisan Grilled\nChicken’ to be in its more than 14,300 U.S. stores by the end of next week,\nin products including a new sandwich, as well as existing sandwiches, wraps\nand salads, It says the biggest change is the removal of sodium phosphates.\nThe new recipe also does not use maltodextrin, which McDonald’s said is\n\n132\n\ngenerally used as a sugar to increase browning or as a carrier for seasoning.\n\nFigure 1: Comparison of extractive, abstractive,\nand our unified summaries on a news article. The\nextractive model picks most important but inco-\nherent or not concise (see blue bold font) sen-\ntences. The abstractive summary is readable, con-\ncise but still Joses or mistakes some facts (see red\nitalics font). The final summary rewritten from\nfragments (see underline font) has the advantages\nfrom both extractive (importance) and abstractive\nadvantage (coherence (see green bold font)).\n\nHence, abstractive summaries can be more coher-\nent and concise than extractive summaries.\nExtractive approaches are typically simpler.\nThey output the probability of each sentence\nto be selected into the summary. Many ear-\nlier works on summarization (Cheng and Lapata,\n2016; Nallapati et al., 2016a, 2017; Narayan et al.,\n2017; Yasunaga et al., 2017) focus on extractive\nsummarization. Among them, Nallapati et al.\n\nProceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 132-141\nMelbourne, Australia, July 15 - 20, 2018. ©2018 Association for Computational Linguistics\n", "vlm_text": "A Uniﬁed Model for Extractive and Abstractive Summarization using Inconsistency Loss \nWan-Ting   $\\mathbf{H}\\mathbf{s}\\mathbf{u}^{1}$  , Chieh-Kai  $\\mathbf{L}\\mathbf{\\dot{n}}^{1}$  , Ming-Ying Lee 1 , Kerui  $\\mathbf{M}\\mathbf{\\ddot{n}}^{2}$  , Jing Tang 2 , Min Sun 1 1  National Tsing Hua University,  2  Cheetah Mobile \n{ hsuwanting, axk51013, masonyl03 } @gapp.nthu.edu.tw, minkerui, tangjing } @cmcm.com, sunmin@ee.nthu.edu.tw \nAbstract \nWe propose a uniﬁed model combining the strength of extractive and abstractive sum- marization. On the one hand, a simple extractive model can obtain sentence-level attention with high ROUGE scores but less readable. On the other hand, a more complicated abstractive model can obtain word-level dynamic attention to generate a more readable paragraph. In our model, sentence-level attention is used to mod- ulate the word-level attention such that words in less attended sentences are less likely to be generated. Moreover, a novel inconsistency loss function is introduced to penalize the inconsistency between two levels of attentions. By end-to-end train- ing our model with the inconsistency loss and original losses of extractive and ab- stractive models, we achieve state-of-the- art ROUGE scores while being the most informative and readable summarization on the CNN/Daily Mail dataset in a solid human evaluation. \n1 Introduction \nText summarization is the task of automatically condensing a piece of text to a shorter version while maintaining the important points. The abil- ity to condense text information can aid many ap- plications such as creating news digests, present- ing search results, and generating reports. There are mainly two types of approaches: extractive and abstractive. Extractive approaches assemble summaries directly from the source text typically selecting one whole sentence at a time. In con- trast, abstractive approaches can generate novel words and phrases not copied from the source text. \nThe table displays a comparison of different approaches to text summarization using an excerpt from an original article about McDonald's. \n\n1. **Original Article**: It discusses McDonald's release of the 'Artisan Grilled Chicken' across more than 14,300 U.S. stores by the end of the next week. It highlights changes such as the removal of sodium phosphates and maltodextrin from the recipe. It includes a quote from Jessica Foust about customer preferences and mentions Panera Bread's plans to remove artificial ingredients by 2016.\n\n2. **Extractive Approach**: This approach takes sentences directly from the original article, retaining specific details like the removal of sodium phosphates and maltodextrin, and includes these in a summarized form.\n\n3. **Abstractive Approach**: This approach creates a compact version by paraphrasing and rephrasing the article content. It maintains the core information about the Artisan Grilled Chicken release and the ingredient changes, although the mention of Panera Bread is altered.\n\n4. **Unified Approach**: This hybrid method combines elements of both extractive and abstractive techniques. It retains more direct quotes and structuring similar to the original article, while potentially integrating nuances or information concisely.\n\nThe table visually differentiates these approaches using distinct text formatting like colored and italicized words.\nFigure 1: Comparison of extractive, abstractive, and our uniﬁed summaries on a news article. The extractive model picks most important but  inco- herent or not concise  (see blue bold font) sen- tences. The abstractive summary is readable, con- cise but still  loses or mistakes some facts  (see red italics font). The ﬁnal summary rewritten from fragments (see underline font) has the advantages from both extractive (importance) and abstractive advantage ( coherence  (see green bold font)). \nHence, abstractive summaries can be more coher- ent and concise than extractive summaries. \nExtractive approaches are typically simpler. They output the probability of each sentence to be selected into the summary. Many ear- lier works on summarization ( Cheng and Lapata , 2016 ;  Nallapati et al. ,  2016a ,  2017 ;  Narayan et al. , 2017 ;  Yasunaga et al. ,  2017 ) focus on extractive summarization. Among them, Nallapati et al. "}
{"page": 1, "image_path": "doc_images/P18-1013_1.jpg", "ocr_text": "(2017) have achieved high ROUGE scores. On\nthe other hand, abstractive approaches (Nallapati\net al., 2016b; See et al., 2017; Paulus et al., 2017;\nFan et al., 2017; Liu et al., 2017) typically in-\nvolve sophisticated mechanism in order to para-\nphrase, generate unseen words in the source text,\nor even incorporate external knowledge. Neu-\nral networks (Nallapati et al., 2017; See et al.,\n2017) based on the attentional encoder-decoder\nmodel (Bahdanau et al., 2014) were able to gen-\nerate abstractive summaries with high ROUGE\nscores but suffer from inaccurately reproducing\nfactual details and an inability to deal with out-\nof-vocabulary (OOV) words. Recently, See et al.\n(2017) propose a pointer-generator model which\nhas the abilities to copy words from source text\nas well as generate unseen words. Despite recent\nprogress in abstractive summarization, extractive\napproaches (Nallapati et al., 2017; Yasunaga et al.,\n2017) and lead-3 baseline (.e., selecting the first\n3 sentences) still achieve strong performance in\nROUGE scores.\n\nWe propose to explicitly take advantage of the\nstrength of state-of-the-art extractive and abstrac-\ntive summarization and introduced the following\nunified model. Firstly, we treat the probabil-\nity output of each sentence from the extractive\nmodel (Nallapati et al., 2017) as sentence-level at-\ntention. Then, we modulate the word-level dy-\nnamic attention from the abstractive model (See\net al., 2017) with sentence-level attention such that\nwords in less attended sentences are less likely\nto be generated. In this way, extractive summa-\nrization mostly benefits abstractive summarization\nby mitigating spurious word-level attention. Sec-\nondly, we introduce a novel inconsistency loss\nfunction to encourage the consistency between\ntwo levels of attentions. The loss function can\nbe computed without additional human annota-\ntion and has shown to ensure our unified model\nto be mutually beneficial to both extractive and\nabstractive summarization. On CNN/Daily Mail\ndataset, our unified model achieves state-of-the-\nart ROUGE scores and outperforms a strong ex-\ntractive baseline (i.e., lead-3). Finally, to en-\nsure the quality of our unified model, we con-\nduct a solid human evaluation and confirm that our\nmethod significantly outperforms recent state-of-\nthe-art methods in informativity and readability.\n\nTo summarize, our contributions are twofold:\n\ne We propose a unified model combining\n\n133\n\nsentence-level and word-level attentions to\ntake advantage of both extractive and abstrac-\ntive summarization approaches.\n\nWe propose a novel inconsistency loss func-\ntion to ensure our unified model to be mutu-\nally beneficial to both extractive and abstrac-\ntive summarization. The unified model with\ninconsistency loss achieves the best ROUGE\nscores on CNN/Daily Mail dataset and out-\nperforms recent state-of-the-art methods in\ninformativity and readability on human eval-\nuation.\n\n2 Related Work\n\nText summarization has been widely studied in re-\ncent years. We first introduce the related works\nof neural-network-based extractive and abstrac-\ntive summarization. Finally, we introduce a few\nrelated works with hierarchical attention mecha-\nnism.\n\nExtractive summarization. Kagebiick et al.\n(2014) and Yin and Pei (2015) use neural networks\nto map sentences into vectors and select sentences\nbased on those vectors. Cheng and Lapata (2016),\nNallapati et al. (2016a) and Nallapati et al. (2017)\nuse recurrent neural networks to read the article\nand get the representations of the sentences and\narticle to select sentences. Narayan et al. (2017)\nutilize side information (i.e., image captions and\ntitles) to help the sentence classifier choose sen-\ntences. Yasunaga et al. (2017) combine recur-\nrent neural networks with graph convolutional net-\nworks to compute the salience (or importance) of\neach sentence. While some extractive summariza-\ntion methods obtain high ROUGE scores, they all\nsuffer from low readability.\n\nAbstractive summarization. Rush et al. (2015)\nfirst bring up the abstractive summarization task\nand use attention-based encoder to read the in-\nput text and generate the summary. Based on\nthem, Miao and Blunsom (2016) use a variational\nauto-encoder and Nallapati et al. (2016b) use a\nmore powerful sequence-to-sequence model. Be-\nsides, Nallapati et al. (2016b) create a new article-\nlevel summarization dataset called CNN/Daily\nMail by adapting DeepMind question-answering\ndataset (Hermann et al., 2015). Ranzato et al.\n(2015) change the traditional training method to\ndirectly optimize evaluation metrics (e.g., BLEU\nand ROUGE). Gu et al. (2016), See et al. (2017)\nand Paulus et al. (2017) combine pointer networks\n", "vlm_text": "( 2017 ) have achieved high ROUGE scores. On the other hand, abstractive approaches ( Nallapati et al. ,  2016b ;  See et al. ,  2017 ;  Paulus et al. ,  2017 ; Fan et al. ,  2017 ;  Liu et al. ,  2017 ) typically in- volve sophisticated mechanism in order to para- phrase, generate unseen words in the source text, or even incorporate external knowledge. Neu- ral networks ( Nallapati et al. ,  2017 ;  See et al. , 2017 ) based on the attentional encoder-decoder model ( Bahdanau et al. ,  2014 ) were able to gen- erate abstractive summaries with high ROUGE scores but suffer from inaccurately reproducing factual details and an inability to deal with out- of-vocabulary (OOV) words. Recently,  See et al. ( 2017 ) propose a pointer-generator model which has the abilities to copy words from source text as well as generate unseen words. Despite recent progress in abstractive summarization, extractive approaches ( Nallapati et al. ,  2017 ;  Yasunaga et al. , 2017 ) and lead-3 baseline (i.e., selecting the ﬁrst 3 sentences) still achieve strong performance in ROUGE scores. \nWe propose to explicitly take advantage of the strength of state-of-the-art extractive and abstrac- tive summarization and introduced the following uniﬁed model. Firstly, we treat the probabil- ity output of each sentence from the extractive model ( Nallapati et al. ,  2017 ) as sentence-level at- tention. Then, we modulate the word-level dy- namic attention from the abstractive model ( See et al. ,  2017 ) with sentence-level attention such that words in less attended sentences are less likely to be generated. In this way, extractive summa- rization mostly beneﬁts abstractive summarization by mitigating spurious word-level attention. Sec- ondly, we introduce a novel inconsistency loss function to encourage the consistency between two levels of attentions. The loss function can be computed without additional human annota- tion and has shown to ensure our uniﬁed model to be mutually beneﬁcial to both extractive and abstractive summarization. On CNN/Daily Mail dataset, our uniﬁed model achieves state-of-the- art ROUGE scores and outperforms a strong ex- tractive baseline (i.e., lead-3). Finally, to en- sure the quality of our uniﬁed model, we con- duct a solid human evaluation and conﬁrm that our method signiﬁcantly outperforms recent state-of- the-art methods in informativity and readability. \nTo summarize, our contributions are twofold:  We propose a uniﬁed model combining \n\nsentence-level and word-level attentions to take advantage of both extractive and abstrac- tive summarization approaches. \n•  We propose a novel inconsistency loss func- tion to ensure our uniﬁed model to be mutu- ally beneﬁcial to both extractive and abstrac- tive summarization. The uniﬁed model with inconsistency loss achieves the best ROUGE scores on CNN/Daily Mail dataset and out- performs recent state-of-the-art methods in informativity and readability on human eval- uation. \n2 Related Work \nText summarization has been widely studied in re- cent years. We ﬁrst introduce the related works of neural-network-based extractive and abstrac- tive summarization. Finally, we introduce a few related works with hierarchical attention mecha- nism. \nExtractive summarization. K˚ ageb¨ ack et al. ( 2014 ) and  Yin and Pei  ( 2015 ) use neural networks to map sentences into vectors and select sentences based on those vectors.  Cheng and Lapata  ( 2016 ), Nallapati et al.  ( 2016a ) and  Nallapati et al.  ( 2017 ) use recurrent neural networks to read the article and get the representations of the sentences and article to select sentences.  Narayan et al.  ( 2017 ) utilize side information (i.e., image captions and titles) to help the sentence classiﬁer choose sen- tences. Yasunaga et al.  ( 2017 ) combine recur- rent neural networks with graph convolutional net- works to compute the salience (or importance) of each sentence. While some extractive summariza- tion methods obtain high ROUGE scores, they all suffer from low readability. \nAbstractive summarization.  Rush et al.  ( 2015 ) ﬁrst bring up the abstractive summarization task and use attention-based encoder to read the in- put text and generate the summary. Based on them,  Miao and Blunsom  ( 2016 ) use a variational auto-encoder and  Nallapati et al.  ( 2016b ) use a more powerful sequence-to-sequence model. Be- sides,  Nallapati et al.  ( 2016b ) create a new article- level summarization dataset called CNN/Daily Mail by adapting DeepMind question-answering dataset ( Hermann et al. ,  2015 ). Ranzato et al. ( 2015 ) change the traditional training method to directly optimize evaluation metrics (e.g., BLEU and ROUGE).  Gu et al.  ( 2016 ),  See et al.  ( 2017 ) and  Paulus et al.  ( 2017 ) combine pointer networks "}
{"page": 2, "image_path": "doc_images/P18-1013_2.jpg", "ocr_text": "Sentence Attention (transparent bars) and Word Attention (solid bars)\n1 f\n\n10 ; |\n\nInconsistent\n\n|\n\nos\n\nSentence 2 |\n\nSentence 1 I\n\nSentence 3\n\nUpdated Word Attention\n\nMultiplying and\nRenormalizing\n\n=\n\nSentence and Word\nAttentions\n\nAttenuated\n\nSentence 1 Sentence 2 Sentence 3\n\nFigure 2: Our unified model combines the word-level and sentence-level attentions. Inconsistency occurs\n\nwhen word attention is high but sentence ai\n\n(Vinyals et al., 2015) into their models to deal\nwith out-of-vocabulary (OOV) words. Chen et al.\n(2016) and See et al. (2017) restrain their models\nfrom attending to the same word to decrease re-\npeated phrases in the generated summary. Paulus\net al. (2017) use policy gradient on summariza-\ntion and state out the fact that high ROUGE scores\nmight still lead to low human evaluation scores.\nFan et al. (2017) apply convolutional sequence-\nto-sequence model and design several new tasks\nfor summarization. Liu et al. (2017) achieve high\nreadability score on human evaluation using gen-\nerative adversarial networks.\n\nHierarchical attention. Attention mechanism\nwas first proposed by Bahdanau et al. (2014).\nYang et al. (2016) proposed a hierarchical atten-\ntion mechanism for document classification. We\nadopt the method of combining sentence-level and\nword-level attention in Nallapati et al. (2016b).\nHowever, their sentence attention is dynamic,\nwhich means it will be different for each generated\nword. Whereas our sentence attention is fixed for\nall generated words. Inspired by the high perfor-\nmance of extractive summarization, we propose to\nuse fixed sentence attention.\n\nOur model combines state-of-the-art extractive\nmodel (Nallapati et al., 2017) and abstractive\nmodel (See et al., 2017) by combining sentence-\nlevel attention from the former and word-level at-\ntention from the latter. Furthermore, we design an\ninconsistency loss to enhance the cooperation be-\ntween the extractive and abstractive models.\n\n3 Our Unified Model\n\nWe propose a unified model to combine the\nstrength of both state-of-the-art extractor (Nalla-\npati et al., 2017) and abstracter (See et al., 2017).\nBefore going into details of our model, we first de-\nfine the tasks of the extractor and abstracter.\n\nProblem definition. The input of both extrac-\n\n134\n\ntention is low (see red arrow).\n\ntor and abstracter is a sequence of words w =\n[w1, We, ..,Wm,--.], Where m is the word index.\nThe sequence of words also forms a sequence of\nsentences Ss = [51,89,...,8n,...], Where n is the\nsentence index. The m*” word is mapped into the\nn(m)\" sentence, where n(-) is the mapping func-\ntion. The output of the extractor is the sentence-\nlevel attention 8 = [(1, 62,...,8n,...], where Bn\nis the probability of the n” sentence been ex-\ntracted into the summary. On the other hand, our\nattention-based abstractor computes word-level at-\n\ntention at = [aj,as,...,a/,,...] dynamically\nwhile generating the ¢*” word in the summary.\n\nThe output of the abstracter is the summary text\ny=[y',y’,...y',...], where y’ is ¢” word in the\nsummary.\n\nIn the following, we introduce the mechanism\nto combine sentence-level and word-level atten-\ntions in Sec. 3.1. Next, we define the novel incon-\nsistency loss that ensures extractor and abstracter\nto be mutually beneficial in Sec. 3.2. We also give\nthe details of our extractor in Sec. 3.3 and our ab-\nstracter in Sec. 3.4. Finally, our training procedure\nis described in Sec. 3.5.\n\n3.1 Combining Attentions\n\nPieces of evidence (e.g., Vaswani et al. (2017))\nshow that attention mechanism is very important\nfor NLP tasks. Hence, we propose to explic-\nitly combine the sentence-level 3,, and word-level\nat, attentions by simple scalar multiplication and\nrenormalization. The updated word attention a,\nis '\nOm X Bim)\n\nYin On X Brim)\n\nThe multiplication ensures that only when both\nword-level at, and sentence-level 3, attentions\nare high, the updated word attention @/, can\nbe high. Since the sentence-level attention 6,\nfrom the extractor already achieves high ROUGE\n\nat __\nAm =\n\nqd)\n", "vlm_text": "The image illustrates a model's approach to handling word-level and sentence-level attention in text analysis. It is divided into two main parts. \n\nOn the left side, there are three separate bars each representing a sentence (Sentence 1, Sentence 2, and Sentence 3) with different colors indicating word attention within those sentences. The chart here highlights an inconsistency (marked with a red arrow labeled \"Inconsistent\") where a word in Sentence 3 has high word-level attention, but the overall sentence-level attention is low. This inconsistency is depicted by the height of the green bar in Sentence 3.\n\nOn the right side, after a process described as \"Multiplying and Renormalizing Sentence and Word Attentions,\" the same attention distribution is shown with adjusted values. Here, the previously inconsistent high attention in Sentence 3 is now reduced (marked as \"Attenuated\"), suggesting that the model has reconciled the attention levels to address the inconsistency. The overall attention levels across the sentences appear more balanced after this adjustment.\n( Vinyals et al. ,  2015 ) into their models to deal with out-of-vocabulary (OOV) words.  Chen et al. ( 2016 ) and  See et al.  ( 2017 ) restrain their models from attending to the same word to decrease re- peated phrases in the generated summary.  Paulus et al.  ( 2017 ) use policy gradient on summariza- tion and state out the fact that high ROUGE scores might still lead to low human evaluation scores. Fan et al.  ( 2017 ) apply convolutional sequence- to-sequence model and design several new tasks for summarization.  Liu et al.  ( 2017 ) achieve high readability score on human evaluation using gen- erative adversarial networks. \nHierarchical attention. Attention mechanism was ﬁrst proposed by  Bahdanau et al.  ( 2014 ). Yang et al.  ( 2016 ) proposed a hierarchical atten- tion mechanism for document classiﬁcation. We adopt the method of combining sentence-level and word-level attention in  Nallapati et al.  ( 2016b ). However, their sentence attention is dynamic, which means it will be different for each generated word. Whereas our sentence attention is ﬁxed for all generated words. Inspired by the high perfor- mance of extractive summarization, we propose to use ﬁxed sentence attention. \nOur model combines state-of-the-art extractive model ( Nallapati et al. ,  2017 ) and abstractive model ( See et al. ,  2017 ) by combining sentence- level attention from the former and word-level at- tention from the latter. Furthermore, we design an inconsistency loss to enhance the cooperation be- tween the extractive and abstractive models. \n3 Our Uniﬁed Model \nWe propose a uniﬁed model to combine the strength of both state-of-the-art extractor ( Nalla- pati et al. ,  2017 ) and abstracter ( See et al. ,  2017 ). Before going into details of our model, we ﬁrst de- ﬁne the tasks of the extractor and abstracter. \nProblem deﬁnition. The input of both extrac- tor and abstracter is a sequence of words    $\\textbf{w}=$   $[w_{1},w_{2},...,w_{m},...]$  , where    $m$   is the word index. The sequence of words also forms a sequence of sentences  $\\mathbf{s}\\;=\\;[s_{1},s_{2},...,s_{n},...].$  , where    $n$   is the sentence index. The    $m^{t h}$    word is mapped into the  $n(m)^{t h}$    sentence, where    $n(\\cdot)$   is the mapping func- tion. The output of the extractor is the sentence- level attention    $\\beta\\,=\\,[\\beta_{1},\\beta_{2},...,\\beta_{n},...]$  , where    $\\beta_{n}$  is the probability of the    $n^{t h}$    sentence been ex- tracted into the summary. On the other hand, our attention-based abstractor computes word-level at- tention    $\\alpha^{t}\\;\\;=\\;\\;\\left[\\alpha_{1}^{t},\\alpha_{2}^{t},...,\\alpha_{m}^{\\bar{t}},...\\right]$  \u0002 \u0003 dynamically while generating the    $t^{t h}$    word in the summary. The output of the abstracter is the summary text  $\\mathbf{y}=[y^{1},y^{2},...,y^{t},...]$  \u0002 \u0003 , where    $y^{t}$    is    $t^{t h}$    word in the summary. \n\nIn the following, we introduce the mechanism to combine sentence-level and word-level atten- tions in Sec.  3.1 . Next, we deﬁne the novel incon- sistency loss that ensures extractor and abstracter to be mutually beneﬁcial in Sec.  3.2 . We also give the details of our extractor in Sec.  3.3  and our ab- stracter in Sec.  3.4 . Finally, our training procedure is described in Sec.  3.5 . \n3.1 Combining Attentions \nPieces of evidence (e.g.,  Vaswani et al.  ( 2017 )) show that attention mechanism is very important for NLP tasks. Hence, we propose to explic- itly combine the sentence-level    $\\beta_{n}$   and word-level  $\\alpha_{m}^{t}$    attentions by simple scalar multiplication and renormalization. The updated word attention    $\\hat{\\alpha}_{m}^{t}$  is \n\n$$\n\\hat{\\alpha}_{m}^{t}=\\frac{\\alpha_{m}^{t}\\times\\beta_{n(m)}}{\\sum_{m}{\\alpha_{m}^{t}\\times\\beta_{n(m)}}}.\n$$\n \nThe multiplication ensures that only when both word-level    $\\alpha_{m}^{t}$    and sentence-level    $\\beta_{n}$    attentions are high, the updated word attention    $\\hat{\\alpha}_{m}^{t}$    can be high. Since the sentence-level attention    $\\beta_{n}$  from the extractor already achieves high ROUGE "}
{"page": 3, "image_path": "doc_images/P18-1013_3.jpg", "ocr_text": "Sentence-Level\nAttention\n\nSentence-level\nRNN\n\nGRU\n\nWord-level\nRNN\n\n9\n\nt of ft\nWe Ws ow\n\nW3 4 S 6 um ve\n\nFigure 3: Architecture of the extractor. We treat\nthe sigmoid output of each sentence as sentence-\nlevel attention € [0, 1].\n\nscores, Pn intuitively modulates the word-level at-\ntention a4, to mitigate spurious word-level atten-\ntion such that words in less attended sentences are\nless likely to be generated (see Fig. 2). As high-\nlighted in Sec. 3.4, the word-level attention a,\nsignificantly affects the decoding process of the\nabstracter. Hence, an updated word-level attention\nis our key to improve abstractive summarization.\n\n3.2. Inconsistency Loss\n\nInstead of only leveraging the complementary na-\nture between sentence-level and word-level atten-\ntions, we would like to encourage these two-levels\nof attentions to be mostly consistent to each other\nduring training as an intrinsic learning target for\nfree (i.e., without additional human annotation).\nExplicitly, we would like the sentence-level atten-\ntion to be high when the word-level attention is\nhigh. Hence, we design the following inconsis-\ntency loss,\n\n4) los Gj\n\nwhere XK is the set of top K attended words and\nT is the number of words in the summary. This\nimplicitly encourages the distribution of the word-\nlevel attentions to be sharp and sentence-level at-\ntention to be high. To avoid the degenerated so-\nlution for the distribution of word attention to be\none-hot and sentence attention to be high, we in-\nclude the original loss functions for training the\nextractor ( Ler in Sec. 3.3) and abstracter (Laps\nand Leoy in Sec. 3.4). Note that Eq. 1 is the only\npart that the extractor is interacting with the ab-\nstracter. Our proposed inconsistency loss facili-\ntates our end-to-end trained unified model to be\nmutually beneficial to both the extractor and ab-\nstracter.\n\na> Aan\n\n| eon\n\nx Bn(m))s\n\n135\n\n3.3. Extractor\n\nOur extractor is inspired by Nallapati et al. (2017).\nThe main difference is that our extractor does not\nneed to obtain the final summary. It mainly needs\nto obtain a short list of important sentences with\na high recall to further facilitate the abstractor.\nWe first introduce the network architecture and the\nloss function. Finally, we define our ground truth\nimportant sentences to encourage high recall.\nArchitecture. The model consists of a hierar-\nchical bidirectional GRU which extracts sentence\nrepresentations and a classification layer for pre-\ndicting the sentence-level attention {,, for each\nsentence (see Fig. 3).\nExtractor loss. The following sigmoid cross en-\ntropy loss is used,\n\nN\n\n1\nva (Gn log Bn +\nN n=1\n\nLert =\n\n(1 — gn) log(1 — Bn)),\n\n3\n\nwhere gn € {0, 1} is the ground-truth label for fie\nn“” sentence and N is the number of sentences.\nWhen gn = 1, it indicates that the n'® sentence\nshould be attended to facilitate abstractive summa-\nrization.\nGround-truth label. The goal of our extractor is\n0 extract sentences with high informativity, which\nmeans the extracted sentences should contain in-\nformation that is needed to generate an abstrac-\nive summary as much as possible. To obtain the\nground-truth labels g = {gn}n, first, we measure\nhe informativity of each sentence s,, in the arti-\ncle by computing the ROUGE-L recall score (Lin,\n2004) between the sentence s, and the reference\nabstractive summary ¥ = {g*}1. Second, we sort\nhe sentences by their informativity and select the\nsentence in the order of high to low informativity.\nWe add one sentence at a time if the new sentence\ncan increase the informativity of all the selected\nsentences. Finally, we obtain the ground-truth la-\nbels g and train our extractor by minimizing Eq. 3.\nNote that our method is different from Nallapati\net al. (2017) who aim to extract a final summary\nfor an article so they use ROUGE F-1 score to\nselect ground-truth sentences; while we focus on\nhigh informativity, hence, we use ROUGE recall\nscore to obtain as much information as possible\nwith respect to the reference summary y.\n\n<a\n\n3.4\n\nThe second part of our model is an abstracter\nthat reads the article; then, generate a summary\n\nAbstracter\n\n", "vlm_text": "The image depicts the architecture of an extractor model, which is specifically designed to handle text data. The architecture consists of two hierarchical layers: a word-level Recurrent Neural Network (RNN) and a sentence-level RNN, both implemented with Gated Recurrent Units (GRUs).\n\n- **Word-level RNN**: At the bottom layer, individual words (denoted as \\( w_1, w_2, \\ldots, w_9 \\)) are processed by GRU units. These words are fed into the word-level GRUs, which likely capture sequential dependencies and contextual relationships among the words.\n\n- **Sentence-level RNN**: The outputs from the word-level RNNs are aggregated and processed by higher-level GRUs at the sentence level. Each sentence is represented by its own GRU unit output. \n\n- **Sentence-Level Attention**: The architecture includes a mechanism to assign attention weights to each sentence. The sigmoid output of this model assigns attention scores to sentences, with values between 0 and 1, representing their importance in the context. In this image, the first sentence has an attention score of 0.9, the second is 0.2, and the third is 0.5.\n\nThis architecture is likely used for tasks like document summarization, information extraction, or other NLP tasks where understanding both word-level details and sentence-level context is important.\nscores,  $\\beta_{n}$   intuitively modulates the word-level at- tention    $\\alpha_{m}^{t}$    to mitigate spurious word-level atten- tion such that words in less attended sentences are less likely to be generated (see Fig.  2 ). As high- lighted in Sec.  3.4 , the word-level attention    $\\hat{\\alpha}_{m}^{t}$  signiﬁcantly affects the decoding process of the abstracter. Hence, an updated word-level attention is our key to improve abstractive summarization. \n3.2 Inconsistency Loss \nInstead of only leveraging the complementary na- ture between sentence-level and word-level atten- tions, we would like to encourage these two-levels of attentions to be mostly consistent to each other during training as an intrinsic learning target for free (i.e., without additional human annotation). Explicitly, we would like the sentence-level atten- tion to be high when the word-level attention is high. Hence, we design the following inconsis- tency loss, \n\n$$\nL_{i n c}=-\\frac{1}{T}\\sum_{t=1}^{T}\\log(\\frac{1}{|\\mathcal{K}|}\\sum_{m\\in\\mathcal{K}}\\alpha_{m}^{t}\\times\\beta_{n(m)}),\n$$\n \nhere    $\\mathcal{K}$   is the set of top  $\\mathbf{K}$   attended words and  $T$   is the number of words in the summary. This implicitly encourages the distribution of the word- level attentions to be sharp and sentence-level at- tention to be high. To avoid the degenerated so- lution for the distribution of word attention to be one-hot and sentence attention to be high, we in- clude the original loss functions for training the extractor (    $L_{e x t}$   in Sec.  3.3 ) and abstracter   $(L_{a b s}$  and    $L_{c o v}$   in Sec.  3.4 ). Note that Eq.  1  is the only part that the extractor is interacting with the ab- stracter. Our proposed inconsistency loss facili- tates our end-to-end trained uniﬁed model to be mutually beneﬁcial to both the extractor and ab- stracter. \n3.3 Extractor \nOur extractor is inspired by  Nallapati et al.  ( 2017 ). The main difference is that our extractor does not need to obtain the ﬁnal summary. It mainly needs to obtain a short list of important sentences with a high recall to further facilitate the abstractor. We ﬁrst introduce the network architecture and the loss function. Finally, we deﬁne our ground truth important sentences to encourage high recall. \nArchitecture. The model consists of a hierar- chical bidirectional GRU which extracts sentence representations and a classiﬁcation layer for pre- dicting the sentence-level attention    $\\beta_{n}$   for each sentence (see Fig.  3 ). \nExtractor loss.  The following sigmoid cross en- tropy loss is used, \n\n$$\nL_{e x t}=-\\frac{1}{N}\\sum_{n=1}^{N}(g_{n}\\log\\beta_{n}+(1-g_{n})\\log(1-\\beta_{n})),\n$$\n \nere  $g_{n}\\in\\{0,1\\}$   $n^{t h}$    sentence and  N  is the number of sentences. When    $g_{n}\\,=\\,1$  , it indicates that the    $n^{t h}$    sentence should be attended to facilitate abstractive summa- rization. \nGround-truth label.  The goal of our extractor is to extract sentences with high informativity, which means the extracted sentences should contain in- formation that is needed to generate an abstrac- tive summary as much as possible. To obtain the ground-truth labels    $\\mathbf{g}=\\{g_{n}\\}_{n}$  , ﬁrst, we measure the informativity of each sentence    $s_{n}$   in the arti- cle by computing the ROUGE-L recall score ( Lin , 2004 ) between the sentence    $s_{n}$   and the reference abstractive summary    $\\hat{\\bf y}=\\{\\hat{y}^{t}\\}_{t}$   { } . Second, we sort the sentences by their informativity and select the sentence in the order of high to low informativity. We add one sentence at a time if the new sentence can increase the informativity of all the selected sentences. Finally, we obtain the ground-truth la- bels  g  and train our extractor by minimizing Eq.  3 . Note that our method is different from  Nallapati et al.  ( 2017 ) who aim to extract a ﬁnal summary for an article so they use ROUGE F-1 score to select ground-truth sentences; while we focus on high informativity, hence, we use ROUGE recall score to obtain as much information as possible with respect to the reference summary    $\\hat{\\mathbf{y}}$  . \n3.4 Abstracter \nThe second part of our model is an abstracter that reads the article; then, generate a summary "}
{"page": 4, "image_path": "doc_images/P18-1013_4.jpg", "ocr_text": "Final Word Distribution P/#\"a!\n\nUpdated Word Attention d® ‘Word Distribution PP?”\n\nEncoder Hidden States (hf, .., hf} Decoder Hidden State hi\n\nContext Vector h*(a\")\n\nFigure 4: Decoding mechanism in the abstracter.\nIn the decoder step t, our updated word at-\ntention & is used to generate context vector\nh*(&*). Hence, it updates the final word distri-\nbution Pfim!,\n\nword-by-word. We use the pointer-generator net-\nwork proposed by See et al. (2017) and combine\nit with the extractor by combining sentence-level\nand word-level attentions (Sec. 3.1).\nPointer-generator network. The pointer-\ngenerator network (See et al., 2017) is a specially\ndesigned sequence-to-sequence attentional model\nthat can generate the summary by copying words\nin the article or generating words from a fixed vo-\ncabulary at the same time. The model contains\na bidirectional LSTM which serves as an encoder\nto encode the input words w and a unidirectional\nLSTM which serves as a decoder to generate the\nsummary y. For details of the network architec-\nture, please refer to See et al. (2017). In the fol-\nlowing, we describe how the updated word atten-\ntion &' affects the decoding process.\n\nNotations. We first define some notations. hy,\nis the encoder hidden state for the m*” word. h?\nis the decoder hidden state in step t. h*(&!) =\nyv at, x hf, is the context vector which is\na function of the updated word attention &.\nprec (h* (&*)) is the probability distribution over\nthe fixed vocabulary before applying the copying\nmechanism.\n\npreeeh (n*(&')) (4)\n= softmax(W2(Wi[h?, h*(&')] + b1) + be),\n\nwhere Wi, Wo, b; and be are learnable parame-\nters, Proce = {procab where Pv (h*(&!))\nis the probability of word w being decoded.\nps\" (h*(&)) € [0,1] is the generating proba-\nbility (see Eq.8 in See et al. (2017)) and 1 —\nps” (h* (&!)) is the copying probability.\n\nFinal word distribution. Pjf'\"“'(a*) is the final\nprobability of word w being decoded (i.e., y! =\nw). It is related to the updated word attention &!\nas follows (see Fig. 4),\n\n136\n\nphin(at) = pren(n' (a!) Puem(h\"(@)) (5)\n\n(1—prr(ne(a'))) SD at,\n\nM:Wm=Ww\nNote that Pfr’ = {pfim@’ | is the probability\ndistribution over the fixed vocabulary and out-of-\nvocabulary (OOV) words. Hence, OOV words can\nbe decoded. Most importantly, it is clear from\nEq. 5 that Pf\" (&) is a function of the updated\nword attention a. Finally, we train the abstracter\nto minimize the negative log-likelihood:\n\n+\n\nT\n1 i nN\nFans = — 7 D108 Phat), 6)\nt=\n\nwhere #j' is the ¢’” token in the reference abstrac-\ntive summary.\n\nCoverage mechanism. We also apply cover-\nage mechanism (See et al., 2017) to prevent the\nabstracter from repeatedly attending to the same\nplace. In each decoder step t, we calculate the\ncoverage vector c! tal) ” which indicates\nso far how much attention has been paid to every\ninput word. The coverage vector c! will be used to\ncalculate word attention & (see Eq.11 in See et al.\n(2017)). Moreover, coverage loss Leoy is calcu-\nlated to directly penalize the repetition in updated\nword attention &*:\n\nT M\n\n1 pn\nLeov = a S- S min(a,,c!,) -\n\nt=1m=1\n\n(7)\n\nThe objective function for training the abstracter\nwith coverage mechanism is the weighted sum of\nnegative log-likelihood and coverage loss.\n\n3.5 Training Procedure\n\nWe first pre-train the extractor by minimizing Lezt\nin Eq. 3 and the abstracter by minimizing Lap,\nand Loy in Eq. 6 and Eq. 7, respectively. When\npre-training, the abstracter takes ground-truth ex-\ntracted sentences (i.e., sentences with g, = 1) as\ninput. To combine the extractor and abstracter,\nwe proposed two training settings : (1) two-stages\ntraining and (2) end-to-end training.\n\nTwo-stages training. In this setting, we view the\nsentence-level attention 3 from the pre-trained ex-\ntractor as hard attention. The extractor becomes\na classifier to select sentences with high attention\n(ie., Bn > threshold). We simply combine the\nextractor and abstracter by feeding the extracted\nsentences to the abstracter. Note that we finetune\nthe abstracter since the input text becomes extrac-\ntive summary which is obtained from the extractor.\n\n", "vlm_text": "This image is a schematic diagram illustrating a sequence-to-sequence model featuring a pointer-generator network, commonly used in natural language processing tasks like text summarization. The diagram shows the combination of word distributions and attention mechanisms to generate a final word distribution:\n\n1. **Encoder Hidden States (\\(h^e_1, ..., h^e_M\\))**: These are outputs from the encoder, representing the input sequence in a higher-dimensional space.\n\n2. **Updated Word Attention (\\(\\alpha^t\\))**: This is obtained by attending over the encoder hidden states, which allows the model to focus on relevant parts of the input sequence.\n\n3. **Context Vector (\\(h^*(\\alpha^t)\\))**: Derived from the updated word attention, it encapsulates the contextual information to be fed into the decoder.\n\n4. **Decoder Hidden State (\\(h^d_t\\))**: The state at each step of the decoder helps in generating the target sequence.\n\n5. **Word Distribution (\\(p_{vocab}\\))**: This distribution is generated based on the vocabulary, representing the likelihood of each word being the output at a given decoder step.\n\n6. **Probability \\(p_{gen}\\)**: This is a scalar value that decides how much to rely on generating a word from the vocabulary versus copying words from the input sequence.\n\n7. **Final Word Distribution (\\(p^{final}\\))**: This combines the vocabulary distribution and the attention distribution, weighted by \\(p_{gen}\\) and \\(1-p_{gen}\\), respectively, to produce an output that can either generate a new word or point to a word in the source text.\n\nThe diagram reflects the combined approach of traditional sequence-to-sequence models with pointer-generator techniques to handle out-of-vocabulary words by copying from the source text when generating.\nFigure 4: Decoding mechanism in the abstracter. In the decoder step    $t$  , our updated word at- tention    $\\hat{\\alpha}^{t}$    is used to generate context vector  $h^{\\ast}(\\hat{\\mathbf{\\alpha}}^{t})$  . Hence, it updates the ﬁnal word distri- bution  $\\mathbf{P}^{f i n a l}$  . \nword-by-word. We use the pointer-generator net- work proposed by  See et al.  ( 2017 ) and combine it with the extractor by combining sentence-level and word-level attentions (Sec.  3.1 ). \nPointer-generator network. The pointer- generator network ( See et al. ,  2017 ) is a specially designed sequence-to-sequence attentional model that can generate the summary by copying words in the article or generating words from a ﬁxed vo- cabulary at the same time. The model contains a bidirectional LSTM which serves as an encoder to encode the input words  w  and a unidirectional LSTM which serves as a decoder to generate the summary  y . For details of the network architec- ture, please refer to  See et al.  ( 2017 ). In the fol- lowing, we describe how the updated word atten- tion  $\\hat{\\alpha}^{t}$    affects the decoding process. \nNotations.  We ﬁrst deﬁne some notations.    $h_{m}^{e}$  is the encoder hidden state for the  $m^{t h}$    word.    $h_{t}^{d}$  is the decoder hidden state in step    $t$  .    $h^{*}(\\hat{\\alpha}^{t})\\;=\\;$   $\\textstyle\\sum_{m}^{M}{\\hat{\\alpha}}_{m}^{t}\\ \\times\\ h_{m}^{e}$    ×   is the context vector which is a function of the updated word attention    $\\hat{\\alpha}^{t}$  .  $\\mathbf{P}^{v o c a b}(h^{*}(\\hat{\\alpha}^{t}))$   is the probability distribution over the ﬁxed vocabulary before applying the copying mechanism. \n\n$$\n\\begin{array}{r l r}{\\lefteqn{\\mathbf{P}^{v o c a b}(h^{*}(\\hat{\\pmb{\\alpha}}^{t}))}}&{{}}&{(4}\\\\ &{{}}&{=\\mathrm{softmax}(W_{2}(W_{1}[h_{t}^{d},h^{*}(\\hat{\\pmb{\\alpha}}^{t})]+b_{1})+b_{2}),}\\end{array}\n$$\n \nwhere    $W_{1},\\;W_{2},\\;b_{1}$   and    $b_{2}$   are learnable parame- ters.    $\\mathbf{P}^{v o c a b}=\\{P_{w}^{v o c a b}\\}_{w}$  }  where    $P_{w}^{v o c a b}(h^{*}(\\hat{\\alpha}^{t}))$  is the probability of word    $w$   being decoded.  $p^{g e n}(h^{*}(\\hat{\\alpha}^{t}))\\ \\in\\ [0,1]$   ∈  is the generating proba- bility (see Eq.8 in  See et al.  ( 2017 )) and    $1\\:-\\:$   $p^{g e n}(h^{*}(\\hat{\\alpha}^{t}))$   is the copying probability. \nFinal word distribution.    $P_{w}^{f i n a l}(\\hat{\\alpha}^{t})$   is the ﬁnal probability of word    $w$   being decoded (i.e.,    $y^{t}\\,=$   $w_{c}$  ). It is related to the updated word attention  $\\hat{\\alpha}^{t}$  as follows (see Fig.  4 ), \n\n$$\n\\begin{array}{r c l}{{P_{w}^{f i n a l}(\\hat{\\alpha}^{t})}}&{{=}}&{{p^{g e n}(h^{*}(\\hat{\\alpha}^{t}))P_{w}^{v o c a b}(h^{*}(\\hat{\\alpha}^{t}))\\left(5\\right)}}\\\\ {{}}&{{+}}&{{(1-p^{g e n}(h^{*}(\\hat{\\alpha}^{t})))\\displaystyle\\sum_{m:w_{m}=w}\\hat{\\alpha}_{m}^{t}.}}\\end{array}\n$$\n \nNote that    $\\mathbf{P}^{f i n a l}\\,=\\,\\{P_{w}^{f i n a l}\\}_{w}$  }  is the probability distribution over the ﬁxed vocabulary and out-of- vocabulary (OOV) words. Hence, OOV words can be decoded. Most importantly, it is clear from Eq.  5  that    $P_{w}^{f i n a l}(\\hat{\\alpha}^{t})$   is a function of the updated word attention  $\\hat{\\alpha}^{t}$  . Finally, we train the abstracter to minimize the negative log-likelihood: \n\n$$\nL_{a b s}=-\\frac{1}{T}\\sum_{t=1}^{T}\\log P_{\\hat{y}^{t}}^{f i n a l}(\\hat{\\alpha}^{t})\\;,\n$$\n \nwhere    $\\hat{y}^{t}$    is the    $t^{t h}$    token in the reference abstrac- tive summary. \nCoverage mechanism. We also apply cover- age mechanism ( See et al. ,  2017 ) to prevent the abstracter from repeatedly attending to the same place. In each decoder step    $t$  , we calculate the coverage vector    $\\textstyle\\mathbf{c}^{t}\\;=\\;\\sum_{t^{\\prime}=0}^{t-1}\\hat{\\alpha}^{t^{\\prime}}$    which indicates so far how much attention has been paid to every input word. The coverage vector    $\\mathbf{c}^{t}$    will be used to calculate word attention  $\\hat{\\alpha}^{t}$    (see Eq.11 in  See et al. ( 2017 )). Moreover, coverage loss    $L_{c o v}$   is calcu- lated to directly penalize the repetition in updated word attention  $\\hat{\\alpha}^{t}$  : \n\n$$\nL_{c o v}=\\frac{1}{T}\\sum_{t=1}^{T}\\sum_{m=1}^{M}\\operatorname*{min}(\\hat{\\alpha}_{m}^{t},\\mathbf{c}_{m}^{t})\\;.\n$$\n \nThe objective function for training the abstracter with coverage mechanism is the weighted sum of negative log-likelihood and coverage loss. \n3.5 Training Procedure \nWe ﬁrst pre-train the extractor by minimizing    $L_{e x t}$  in Eq.  3  and the abstracter by minimizing    $L_{a b s}$  and    $L_{c o v}$   in Eq.  6  and Eq.  7 , respectively. When pre-training, the abstracter takes ground-truth ex- tracted sentences (i.e., sentences with    $g_{n}=1$  ) as input. To combine the extractor and abstracter, we proposed two training settings : (1) two-stages training and (2) end-to-end training. \nTwo-stages training.  In this setting, we view the sentence-level attention    $\\beta$   from the pre-trained ex- tractor as hard attention. The extractor becomes a classiﬁer to select sentences with high attention (i.e.,    $\\beta_{n}\\ >$   threshold ). We simply combine the extractor and abstracter by feeding the extracted sentences to the abstracter. Note that we ﬁnetune the abstracter since the input text becomes extrac- tive summary which is obtained from the extractor. "}
{"page": 5, "image_path": "doc_images/P18-1013_5.jpg", "ocr_text": "End-to-end training. For end-to-end training, the\nsentence-level attention ( is soft attention and will\nbe combined with the word-level attention a! as\ndescribed in Sec. 3.1. We end-to-end train the\nextractor and abstracter by minimizing four loss\nfunctions: Legt, Labs, Lcov, aS Well as Dine in\nEq. 2. The final loss is as below:\n\nLe2e = Mi Leat t A2Labs t A3Lcov t AaLine,\n(8)\nwhere Aj, Ag, A3, A4 are hyper-parameters. In our\nexperiment, we give Lez a bigger weight (e.g.,\nAi = 5) when end-to-end training with L;,,- since\nwe found that L;,,- is relatively large such that the\n\nextractor tends to ignore Lezt.\n\n4 Experiments\n\nWe introduce the dataset and implementation de-\ntails of our method evaluated in our experiments.\n\n4.1 Dataset\n\nWe evaluate our models on the CNN/Daily Mail\ndataset (Hermann et al., 2015; Nallapati et al.,\n2016b; See et al., 2017) which contains news sto-\nries in CNN and Daily Mail websites. Each ar-\nticle in this dataset is paired with one human-\nwritten multi-sentence summary. This dataset has\ntwo versions: anonymized and non-anonymized.\nThe former contains the news stories with all the\nnamed entities replaced by special tokens (e.g.,\n@entity2); while the latter contains the raw text\nof each news story. We follow See et al. (2017)\nand obtain the non-anonymized version of this\ndataset which has 287,113 training pairs, 13,368\nvalidation pairs and 11,490 test pairs.\n\n4.2 Implementation Details\n\nWe train our extractor and abstracter with 128-\ndimension word embeddings and set the vocabu-\nlary size to 50k for both source and target text. We\nfollow Nallapati et al. (2017) and See et al. (2017)\nand set the hidden dimension to 200 and 256 for\nthe extractor and abstracter, respectively. We use\nAdagrad optimizer (Duchi et al., 2011) and apply\nearly stopping based on the validation set. In the\ntesting phase, we limit the length of the summary\nto 120.\n\nPre-training. We use learning rate 0.15 when pre-\ntraining the extractor and abstracter. For the ex-\ntractor, we limit both the maximum number of\nsentences per article and the maximum number\nof tokens per sentence to 50 and train the model\n\n137\n\nfor 27k iterations with the batch size of 64. For\nthe abstracter, it takes ground-truth extracted sen-\ntences (i.e., sentences with g,, = 1) as input. We\nlimit the length of the source text to 400 and the\nlength of the summary to 100 and use the batch\nsize of 16. We train the abstracter without cov-\nerage mechanism for 88k iterations and continue\ntraining for 1k iterations with coverage mecha-\nnism (Laps : Leoy = 1: 1).\n\nTwo-stages training. The abstracter takes ex-\ntracted sentences with 6, > 0.5, where @ is ob-\ntained from the pre-trained extractor, as input dur-\ning two-stages training. We finetune the abstracter\nfor 10k iterations.\n\nEnd-to-end training. During end-to-end training,\nwe will minimize four loss functions (Eq. 8) with\nAy = 5 and Ay = A3 = Aq = 1. We set K to\n3 for computing L;,-. Due to the limitation of the\nmemory, we reduce the batch size to 8 and thus use\na smaller learning rate 0.01 for stability. The ab-\nstracter here reads the whole article. Hence, we in-\ncrease the maximum length of source text to 600.\nWe end-to-end train the model for 50k iterations.\n\n5 Results\n\nOur unified model not only generates an abstrac-\ntive summary but also extracts the important sen-\ntences in an article. Our goal is that both of the\ntwo types of outputs can help people to read and\nunderstand an article faster. Hence, in this sec-\ntion, we evaluate the results of our extractor in\nSec. 5.1 and unified model in Sec. 5.2. Further-\nmore, in Sec. 5.3, we perform human evaluation\nand show that our model can provide a better ab-\nstractive summary than other baselines.\n\n5.1 Results of Extracted Sentences\n\nTo evaluate whether our extractor obtains enough\ninformation for the abstracter, we use full-length\nROUGE recall scores! between the extracted sen-\ntences and reference abstractive summary. High\nROUGE recall scores can be obtained if the\nextracted sentences include more words or se-\nquences overlapping with the reference abstrac-\ntive summary. For each article, we select sen-\ntences with the sentence probabilities 6 greater\nthan 0.5. We show the results of the ground-truth\nsentence labels (Sec. 3.3) and our models on the\n\n‘All our ROUGE scores are reported by the official\nROUGE script. We use the py rouge package.\nhttps://pypi.org/project/pyrouge/0.1.3/\n", "vlm_text": "End-to-end training.  For end-to-end training, the sentence-level attention  $\\beta$   is soft attention and will be combined with the word-level attention    $\\alpha^{t}$    as described in Sec.  3.1 . We end-to-end train the extractor and abstracter by minimizing four loss functions:    $L_{e x t},\\ L_{a b s},\\ L_{c o v}$  , as well as    $L_{i n c}$   in Eq.  2 . The ﬁnal loss is as below: \n\n$$\nL_{e2e}=\\lambda_{1}L_{e x t}+\\lambda_{2}L_{a b s}+\\lambda_{3}L_{c o v}+\\lambda_{4}L_{i n c},\n$$\n \nwhere  $\\lambda_{1},\\,\\lambda_{2},\\,\\lambda_{3},\\,\\lambda_{4}$   are hyper-parameters. In our experiment, we give    $L_{e x t}$   a bigger weight (e.g.,  $\\lambda_{1}=5.$  ) when end-to-end training with    $L_{i n c}$   since we found that    $L_{i n c}$   is relatively large such that the extractor tends to ignore    $L_{e x t}$  . \n4 Experiments \nWe introduce the dataset and implementation de- tails of our method evaluated in our experiments. \n4.1 Dataset \nWe evaluate our models on the CNN/Daily Mail dataset ( Hermann et al. ,  2015 ;  Nallapati et al. , 2016b ;  See et al. ,  2017 ) which contains news sto- ries in CNN and Daily Mail websites. Each ar- ticle in this dataset is paired with one human- written multi-sentence summary. This dataset has two versions:  anonymized  and  non-anonymized . The former contains the news stories with all the named entities replaced by special tokens (e.g., @entity2 ); while the latter contains the raw text of each news story. We follow  See et al.  ( 2017 ) and obtain the  non-anonymized  version of this dataset which has 287,113 training pairs, 13,368 validation pairs and 11,490 test pairs. \n4.2 Implementation Details \nWe train our extractor and abstracter with 128- dimension word embeddings and set the vocabu- lary size to 50k for both source and target text. We follow  Nallapati et al.  ( 2017 ) and  See et al.  ( 2017 ) and set the hidden dimension to 200 and 256 for the extractor and abstracter, respectively. We use Adagrad optimizer ( Duchi et al. ,  2011 ) and apply early stopping based on the validation set. In the testing phase, we limit the length of the summary to 120. \nPre-training.  We use learning rate 0.15 when pre- training the extractor and abstracter. For the ex- tractor, we limit both the maximum number of sentences per article and the maximum number of tokens per sentence to 50 and train the model for   $27\\mathrm{k}$   iterations with the batch size of 64. For the abstracter, it takes ground-truth extracted sen- tences (i.e., sentences with    $g_{n}=1)$  ) as input. We limit the length of the source text to 400 and the length of the summary to 100 and use the batch size of 16. We train the abstracter without cov- erage mechanism for   $88\\mathbf{k}$   iterations and continue training for 1k iterations with coverage mecha- nism   $(L_{a b s}:L_{c o v}=1:1)$  ). \n\nTwo-stages training. The abstracter takes ex- tracted sentences with    $\\beta_{n}\\,>\\,0.5$  , where    $\\beta$   is ob- tained from the pre-trained extractor, as input dur- ing two-stages training. We ﬁnetune the abstracter for 10k iterations. \nEnd-to-end training.  During end-to-end training, we will minimize four loss functions (Eq.  8 ) with  $\\lambda_{1}\\,=\\,5$   and    $\\lambda_{2}\\,=\\,\\lambda_{3}\\,=\\,\\lambda_{4}\\,=\\,1$  . We set K to 3 for computing  $L_{i n c}$  . Due to the limitation of the memory, we reduce the batch size to 8 and thus use a smaller learning rate 0.01 for stability. The ab- stracter here reads the whole article. Hence, we in- crease the maximum length of source text to 600. We end-to-end train the model for  $50\\mathrm{k}$   iterations. \n5 Results \nOur uniﬁed model not only generates an abstrac- tive summary but also extracts the important sen- tences in an article. Our goal is that both of the two types of outputs can help people to read and understand an article faster. Hence, in this sec- tion, we evaluate the results of our extractor in Sec.  5.1  and uniﬁed model in Sec.  5.2 . Further- more, in Sec.  5.3 , we perform human evaluation and show that our model can provide a better ab- stractive summary than other baselines. \n5.1 Results of Extracted Sentences \nTo evaluate whether our extractor obtains enough information for the abstracter, we use full-length ROUGE recall scores 1   between the extracted sen- tences and reference abstractive summary. High ROUGE recall scores can be obtained if the extracted sentences include more words or se- quences overlapping with the reference abstrac- tive summary. For each article, we select sen- tences with the sentence probabilities    $\\beta$   greater than  0 . 5 . We show the results of the ground-truth sentence labels (Sec.  3.3 ) and our models on the "}
{"page": 6, "image_path": "doc_images/P18-1013_6.jpg", "ocr_text": "Method ROUGE-1 | ROUGE-2 | ROUGE-L\n\npre-trained 73.50 35.55 68.57\n\nend2end w/o inconsistency loss 72.97 35.11 67.99\n\nend2end w/ inconsistency loss 78.40 39.45 73.83\n\nground-truth labels 89.23 49.36 85.46\nTable 1: ROUGE recall scores of the extracted sentences. pre-trained indicates the extractor trained on\nthe ground-truth labels. end2end indicates the extractor after end-to-end training with the abstracter. Note\n\n+£0.33.\n\nthat ground-truth labels show the upper-bound performance since the reference summary to calculate\nROUGE-recall is abstractive. All our ROUGE scores have a 95% confidence interval with at most\nMethod ROUGE-1 | ROUGE-2 | ROUGE-L\nHierAttn (Nallapati et al., 2016b)* 32.75 12.21 29.01\nDeepRL (Paulus et al., 2017)* 39.87 15.82 36.90\npointer-generator (See et al., 2017) 39.53 17.28 36.38\nGAN (Liu et al., 2017) 39.92 17.65 36.71\ntwo-stage (ours) 39.97 17.43 36.34\nend2end w/o inconsistency loss (ours) 40.19 17.67 36.68\nend2end w/ inconsistency loss (ours) 40.68 17.97 37.13\nlead-3 (See et al., 2017) 40.34 17.70 36.57\n\nTable 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our\ntwo-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our\nmodel trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores\nhave a 95% confidence interval with at most +0.24. ‘*’ indicates the model is trained and evaluated on\nthe anonymized dataset and thus is not strictly comparable with ours.\n\ntest set of the CNN/Daily Mail dataset in Table\n1. Note that the ground-truth extracted sentences\ncan’t get ROUGE recall scores of 100 because ref-\nerence summary is abstractive and may contain\nsome words and sequences that are not in the arti-\ncle. Our extractor performs the best when end-to-\nend trained with inconsistency loss.\n\n5.2. Results of Abstractive Summarization\n\nWe use full-length ROUGE-1, ROUGE-2 and\nROUGE-L F-1 scores to evaluate the generated\nsummaries. We compare our models (two-stage\nand end-to-end) with state-of-the-art abstractive\nsummarization models (Nallapati et al., 2016b;\nPaulus et al., 2017; See et al., 2017; Liu et al.,\n2017) and a strong lead-3 baseline which directly\nuses the first three article sentences as the sum-\nmary. Due to the writing style of news articles,\nthe most important information is often written\nat the beginning of an article which makes lead-\n3 a strong baseline. The results of ROUGE F-1\nscores are shown in Table 2. We prove that with\nhelp of the extractor, our unified model can outper-\nform pointer-generator (the third row in Table 2)\n\neven with two-stages training (the fifth row in Ta-\nble 2). After end-to-end training without incon-\nsistency loss, our method already achieves better\nROUGE scores by cooperating with each other.\nMoreover, our model end-to-end trained with in-\nconsistency loss achieves state-of-the-art ROUGE\nscores and exceeds lead-3 baseline.\n\nIn order to quantify the effect of inconsistency\nloss, we design a metric — inconsistency rate Ring\n— to measure the inconsistency for each generated\nsummary. For each decoder step t, if the word with\nmaximum attention belongs to a sentence with low\nattention (i-¢., 8p (argmax(a*)) < mean(@)), we de-\nfine this step as an inconsistent step tinc. The in-\nconsistency rate Rin is then defined as the per-\ncentage of the inconsistent steps in the summary.\n\nCount (tine)\na ,\n\nwhere T is the length of the summary. The av-\nerage inconsistency rates on test set are shown in\nTable 4. Our inconsistency loss significantly de-\ncrease Rine from about 20% to 4%. An example\nof inconsistency improvement is shown in Fig. 5.\n\nRine = (9)\n\n138\n", "vlm_text": "The table presents a comparison of different methods' performance based on ROUGE scores, which are commonly used to evaluate the quality of machine-generated summaries compared to human-written summaries. The table has four columns:\n\n1. **Method**: Lists the different approaches or models being evaluated. There are four methods: \n   - \"pre-trained\"\n   - \"end2end w/o inconsistency loss\"\n   - \"end2end w/ inconsistency loss\"\n   - \"ground-truth labels\"\n\n2. **ROUGE-1**: This column presents the ROUGE-1 scores for each method. The scores are:\n   - \"pre-trained\": 73.50\n   - \"end2end w/o inconsistency loss\": 72.97\n   - \"end2end w/ inconsistency loss\": 78.40 (highlighted as bold, indicating perhaps the best among the compared methods for this metric)\n   - \"ground-truth labels\": 89.23\n\n3. **ROUGE-2**: This column shows the ROUGE-2 scores for each method. The scores are:\n   - \"pre-trained\": 35.55\n   - \"end2end w/o inconsistency loss\": 35.11\n   - \"end2end w/ inconsistency loss\": 39.45 (highlighted as bold)\n   - \"ground-truth labels\": 49.36\n\n4. **ROUGE-L**: This column displays the ROUGE-L scores for each method. The scores are:\n   - \"pre-trained\": 68.57\n   - \"end2end w/o inconsistency loss\": 67.99\n   - \"end2end w/ inconsistency loss\": 73.83 (highlighted as bold)\n   - \"ground-truth labels\": 85.46\n\nOverall, the \"end2end w/ inconsistency loss\" method appears to perform better than the other model-based approaches in terms of ROUGE-1, ROUGE-2, and ROUGE-L metrics. The \"ground-truth labels\" represent the ideal scenario, showing the highest scores across all metrics.\nTable 1: ROUGE recall scores of the extracted sentences.  pre-trained  indicates the extractor trained on the ground-truth labels.  end2end  indicates the extractor after end-to-end training with the abstracter. Note that  ground-truth labels  show the upper-bound performance since the reference summary to calculate ROUGE-recall is abstractive. All our ROUGE scores have a   $95\\%$   conﬁdence interval with at most  $\\pm0.33$  . \nThe table presents the results of different methods for text summarization using ROUGE metrics. The methods are listed in the first column and include:\n\n1. HierAttn (Nallapati et al., 2016b)\n2. DeepRL (Paulus et al., 2017)\n3. pointer-generator (See et al., 2017)\n4. GAN (Liu et al., 2017)\n5. two-stage (ours)\n6. end2end w/o inconsistency loss (ours)\n7. end2end w/ inconsistency loss (ours)\n8. lead-3 (See et al., 2017)\n\nThe subsequent columns report the performance scores in terms of ROUGE-1, ROUGE-2, and ROUGE-L metrics. The highest scores in each category are bolded. According to the table:\n\n- The \"end2end w/ inconsistency loss (ours)\" method achieves the highest scores: 40.68 for ROUGE-1, 17.97 for ROUGE-2, and 37.13 for ROUGE-L.\n- Other methods have varying levels of performance, with \"DeepRL\" and \"pointer-generator\" also achieving relatively high ROUGE scores.\nTable 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores have a  $95\\%$   conﬁdence interval with at most    $\\pm0.24$  . ‘ ∗ ’ indicates the model is trained and evaluated on the anonymized dataset and thus is not strictly comparable with ours. \ntest set of the CNN/Daily Mail dataset in Table 1 . Note that the ground-truth extracted sentences can’t get ROUGE recall scores of 100 because ref- erence summary is abstractive and may contain some words and sequences that are not in the arti- cle. Our extractor performs the best when end-to- end trained with inconsistency loss. \n5.2 Results of Abstractive Summarization \nWe use full-length ROUGE-1, ROUGE-2 and ROUGE-L F-1 scores to evaluate the generated summaries. We compare our models (two-stage and end-to-end) with state-of-the-art abstractive summarization models ( Nallapati et al. ,  2016b ; Paulus et al. ,  2017 ;  See et al. ,  2017 ;  Liu et al. , 2017 ) and a strong lead-3 baseline which directly uses the ﬁrst three article sentences as the sum- mary. Due to the writing style of news articles, the most important information is often written at the beginning of an article which makes lead- 3 a strong baseline. The results of ROUGE F-1 scores are shown in Table  2 . We prove that with help of the extractor, our uniﬁed model can outper- form pointer-generator (the third row in Table  2 ) \neven with two-stages training (the ﬁfth row in Ta- ble  2 ). After end-to-end training without incon- sistency loss, our method already achieves better ROUGE scores by cooperating with each other. Moreover, our model end-to-end trained with in- consistency loss achieves state-of-the-art ROUGE scores and exceeds lead-3 baseline. \nIn order to quantify the effect of inconsistency loss, we design a metric – inconsistency rate    $R_{i n c}$  – to measure the inconsistency for each generated summary. For each decoder step  $t$  , if the word with maximum attention belongs to a sentence with low attention (i.e.,    $\\beta_{n(\\operatorname{argmax}(\\alpha^{t}))}<\\operatorname{mean}(\\beta))$  , we de- ﬁne this step as an inconsistent step    $t_{i n c}$  . The in- consistency rate    $R_{i n c}$   is then deﬁned as the per- centage of the inconsistent steps in the summary. \n\n$$\nR_{i n c}=\\frac{\\mathrm{count}(t_{i n c})}{T},\n$$\n \nwhere    $T$   is the length of the summary. The av- erage inconsistency rates on test set are shown in Table  4 . Our inconsistency loss signiﬁcantly de- crease    $R_{i n c}$   from about    $20\\%$   to    $4\\%$  . An example of inconsistency improvement is shown in Fig.  5 . "}
{"page": 7, "image_path": "doc_images/P18-1013_7.jpg", "ocr_text": "Method informativity | conciseness | readability\nDeepRL (Paulus et al., 2017) 3.23 2.97 2.85\npointer-generator (See et al., 2017) 3.18 3.36 3.47\nGAN (Liu et al., 2017) 3.22 3.52 3.51\nOurs 3.58 3.40 3.70\nreference 3.43 3.61 3.62\n\nTable 3: Comparing human evaluation results with state-of-the-art methods.\n\nMethod avg. Rine\nw/o incon. loss 0.198\nw/ incon. loss 0.042\n\nTable 4: Inconsistency rate of our end-to-end\ntrained model with and without inconsistency loss.\n\nWithout inconsistency loss:\nIf that was a tornado, it was one monster of one. Luckily, so far it looks\nlike no one was hurt. With tornadoes touching down near Dallas on Sun-\nday, Ryan Shepard snapped a photo of a black cloud formation reach-\ning down to the ground. He said it was a tornado. It wouldn’t be an\nexaggeration to say it looked half a mile wide. More like a mile, said\nJamie Moore, head of emergency management in Johnson County, Texas.\nIt could have been one the National Weather Service warned about in a\ntweet as severe thunderstorms drenched the area, causing street flooding.\n\n()\n\nWith inconsistency loss:\nIf that was a tornado, it was one monster of one. Luckily, so far it looks\nlike no one was hurt. With tornadoes touching down near Dallas on\nSunday, Ryan Shepard snapped a photo of a black cloud formation\nreaching down to the ground. He said it was a tornado. It wouldn't be\nan exaggeration to say it looked half a mile wide. More like a mile, said\nJamie Moore, head of emergency management in Johnson County, Texas.\nIt could have been one the National Weather Service warned about in\na tweet as severe thunderstorms drenched the area, causing street flood-\ning. |.)\n\nFigure 5: Visualizing the consistency between\nsentence and word attentions on the original ar-\nticle. We highlight word (bold font) and sentence\n(underline font) attentions. We compare our meth-\nods trained with and without inconsistency loss.\nInconsistent fragments (see red bold font) occur\nwhen trained without the inconsistency loss.\n\n5.3. Human Evaluation\n\nWe perform human evaluation on Amazon Me-\nchanical Turk (MTurk)* to evaluate the informa-\ntivity, conciseness and readability of the sum-\nWe compare our best model (end2end\nwith inconsistency loss) with pointer-generator\n(See et al., 2017), generative adversarial network\n(Liu et al., 2017) and deep reinforcement model\n(Paulus et al., 2017). For these three models, we\nuse the test set outputs provided by the authors’.\n\nmaries.\n\n*https://www.mturk.com/\n\nSnttps://github.com/abisee/\npointer-generator and https://likicode.com\nfor the first two. For DeepRL, we asked through email.\n\n139\n\nWe randomly pick 100 examples in the test set.\nAll generated summaries are re-capitalized and\nde-tokenized. Since Paulus et al. (2017) trained\ntheir model on anonymized data, we also recover\nthe anonymized entities and numbers of their out-\nputs.\n\nWe show the article and 6 summaries (reference\nsummary, 4 generated summaries and a random\nsummary) to each human evaluator. The random\nsummary is a reference summary randomly picked\nfrom other articles and is used as a trap. We show\nthe instructions of three different aspects as: (1)\nInformativity: how well does the summary cap-\nture the important parts of the article? (2) Con-\nciseness: is the summary clear enough to explain\neverything without being redundant? (3) Read-\nability: how well-written (fluent and grammatical)\nthe summary is? The user interface of our human\nevaluation is shown in the supplementary material.\n\nWe ask the human evaluator to evaluate each\nsummary by scoring the three aspects with 1 to\n5 score (higher the better). We reject all the eval-\nuations that score the informativity of the random\nsummary as 3, 4 and 5. By using this trap mech-\nanism, we can ensure a much better quality of\nour human evaluation. For each example, we first\nask 5 human evaluators to evaluate. However, for\nthose articles that are too long, which are always\nskipped by the evaluators, it is hard to collect 5\nreliable evaluations. Hence, we collect at least 3\nevaluations for every example. For each summary,\nwe average the scores over different human evalu-\nators.\n\nThe results are shown in Table 3. The reference\nsummaries get the best score on conciseness since\nthe recent abstractive models tend to copy sen-\ntences from the input articles. However, our model\nlearns well to select important information and\nform complete sentences so we even get slightly\nbetter scores on informativity and readability than\nthe reference summaries. We show a typical ex-\nample of our model comparing with other state-of-\n", "vlm_text": "The table displays a comparison of methods based on three metrics: informativity, conciseness, and readability. The methods listed are:\n\n- **DeepRL (Paulus et al., 2017)**\n  - Informativity: 3.23\n  - Conciseness: 2.97\n  - Readability: 2.85\n\n- **Pointer-generator (See et al., 2017)**\n  - Informativity: 3.18\n  - Conciseness: 3.36\n  - Readability: 3.47\n\n- **GAN (Liu et al., 2017)**\n  - Informativity: 3.22\n  - Conciseness: 3.52\n  - Readability: 3.51\n\n- **Ours**\n  - Informativity: 3.58 (bolded)\n  - Conciseness: 3.40\n  - Readability: 3.70 (bolded)\n\n- **Reference**\n  - Informativity: 3.43\n  - Conciseness: 3.61 (bolded)\n  - Readability: 3.62\n\nThe bold numbers highlight the highest scores within each metric.\nThe table presents a comparison of two methods based on their average inconsistency loss, denoted as \"avg. \\( R_{inc} \\).\" It includes the following data:\n\n1. Method: \"w/o incon. loss\" – has an average inconsistency loss (\\( R_{inc} \\)) of 0.198.\n2. Method: \"w/ incon. loss\" – has an average inconsistency loss (\\( R_{inc} \\)) of 0.042. \n\nThis table seems to showcase the effect of employing inconsistency loss in a method, likely indicating a reduction in inconsistency when it is utilized.\nTable 4: Inconsistency rate of our end-to-end trained model with and without inconsistency loss. \nThe image contains two text sections comparing passages \"Without inconsistency loss\" and \"With inconsistency loss,\" likely intended to demonstrate the impact of a certain technique or method in text generation or summarization. Both sections provide a similar narrative describing a photographic account of a tornado or a black cloud formation near Dallas, Texas, with some differences highlighted in red to indicate variations between the two versions. Key elements include mentions of Ryan Shepard taking a photo, the National Weather Service, and weather events causing street flooding.\nFigure 5: Visualizing the consistency between sentence and word attentions on the original ar- ticle. We highlight word (bold font) and sentence (underline font) attentions. We compare our meth- ods trained with and without inconsistency loss. Inconsistent fragments (see red bold font) occur when trained without the inconsistency loss. \n5.3 Human Evaluation \nWe perform human evaluation on Amazon Me- chanical Turk (MTurk) 2   to evaluate the informa- tivity, conciseness and readability of the sum- maries. We compare our best model (end2end with inconsistency loss) with pointer-generator\n\n ( See et al. ,  2017 ), generative adversarial network\n\n ( Liu et al. ,  2017 ) and deep reinforcement model\n\n ( Paulus et al. ,  2017 ). For these three models, we use the test set outputs provided by the authors 3 . \nWe randomly pick 100 examples in the test set. All generated summaries are re-capitalized and de-tokenized. Since  Paulus et al.  ( 2017 ) trained their model on anonymized data, we also recover the anonymized entities and numbers of their out- puts. \nWe show the article and 6 summaries (reference summary, 4 generated summaries and a random summary) to each human evaluator. The random summary is a reference summary randomly picked from other articles and is used as a trap. We show the instructions of three different aspects as: (1) Informativity: how well does the summary cap- ture the important parts of the article? (2) Con- ciseness: is the summary clear enough to explain everything without being redundant? (3) Read- ability: how well-written (ﬂuent and grammatical) the summary is? The user interface of our human evaluation is shown in the supplementary material. \nWe ask the human evaluator to evaluate each summary by scoring the three aspects with 1 to 5 score (higher the better). We reject all the eval- uations that score the informativity of the random summary as 3, 4 and 5. By using this trap mech- anism, we can ensure a much better quality of our human evaluation. For each example, we ﬁrst ask 5 human evaluators to evaluate. However, for those articles that are too long, which are always skipped by the evaluators, it is hard to collect 5 reliable evaluations. Hence, we collect at least 3 evaluations for every example. For each summary, we average the scores over different human evalu- ators. \nThe results are shown in Table  3 . The reference summaries get the best score on conciseness since the recent abstractive models tend to copy sen- tences from the input articles. However, our model learns well to select important information and form complete sentences so we even get slightly better scores on informativity and readability than the reference summaries. We show a typical ex- ample of our model comparing with other state-of- Figure 6: Typical Comparison. Our model attended at the most important information (blue bold font) matching well with the reference summary; while other state-of-the-art methods generate repeated or less important information (red italic font). "}
{"page": 8, "image_path": "doc_images/P18-1013_8.jpg", "ocr_text": "Original article (truncated):\n\nA chameleon balances carefully on a branch, waiting calmly for its prey... except that if you look closely, you will see that this picture is not all that it seems.\nFor the ‘creature’ poised to pounce is not a colourful species of lizard but something altogether more human. Featuring two carefully painted female models,\nit is a clever piece of sculpture designed to create an amazing illusion. It is the work of Italian artist Johannes Stoetter. Scroll down for video. Can you see us?\nItalian artist Johannes Stoetter has painted two naked women to look like a chameleon. The 37-year-old has previously transformed his models into\nfrogs and parrots but this may be his most intricate and impressive piece to date. Stoctter daubed water-based body paint on the naked models to create\nthe multicoloured effect, then intertwined them to form the shape of a chameleon. To complete the deception, the models rested on a bench painted to\nmatch their skin and held the green branch in the air beneath them. Stoetter can take weeks to plan one of his pieces and hours to paint it. Speaking about The\nChameleon, he said: ‘I worked about four days to design the motif bigger and paint it with colours. The body painting took me about six hours with the help\nof an assistant. I covered the hair with natural clay to make the heads look bald.’ Camouflage job: A few finishing touches are applied to the two naked models\nto complete the transformation. ‘There are different difficulties on different levels as in every work, but I think that my passion and love to my work is so big,\nthat I figure out a way to deal with difficulties, My main inspirations are nature, my personal life-philosophy, every-day-life and people themselves.’ However,\nthe finished result existed only briefly before the models were able to get up and wash the paint off with just a video and some photographs to record it. (...)\n\nReference summary:\nJohannes Stoetter’s artwork features two carefully painted female models. The 37-year-old has previously transformed models into frogs and parrots. Daubed\nwater-based body paint on naked models to create the effect. Completing the deception, models rested on bench painted to match skin.\n\nDeepRL:\n\nItalian artist Johannes Stoetter has painted female models to look like a chameleon. The 37-year-old has previously transformed his models into frogs and\nparrots but this may be the work of Italian artist. He has painted nude models and it is a clever piece of sculpture designed to create an amazing illusion. It is\nwork of artist Johannes Stoetter.\n\nGAN:\nItalian artist Johannes Stoetter has painted two naked women to look like a chameleon. The 37-year-old has previously transformed his models into frogs and\nparrots but this may be his most intricate and impressive piece to date.\n\nPointer-generator:\nItalian artist Johannes Stoetter has painted two naked women to look like a chameleon. It is the work of Italian artist Johannes Stoetter. Stoetter daubed\nwater-based body paint on the naked models to create the multicoloured effect, then intertwined them to form the shape of a chameleon.\n\nOur unified model (with inconsistency loss):\nJohannes Stoetter has painted two naked women to look like a chameleon, The 37-year-old has previously transformed his models into frogs and parrots.\nStoetter daubed water-based body paint on the naked models to create the multicoloured effect, then intertwined them to form the shape of a chameleon.\n\nFigure 6: Typical Comparison. Our model attended at the most important information (blue bold font)\nmatching well with the reference summary; while other state-of-the-art methods generate repeated or\nless important information (red italic font).\n\nthe-art methods in Fig. 6. More examples (5 using References\n\nCNN/Daily Mail news articles and 3 using non- Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\n\nnews articles as inputs) are provided in the supple- gio. 2014. Neural machine translation by jointly\n\nmentary material. learning to align and translate. In Proceedings of the\n2015 International Conference on Learning Repre-\nsentations.\n\n6 Conclusion\nQian Chen, Xiaodan Zhu, Zhenhua Ling, Si Wei, and\n\nWe propose a unified model combining the Hui Jiang. 2016. Distraction-based neural networks\nstrength of extractive and abstractive summariza- for modeling documents. In Proceedings of the\ntion. Most importantly, a novel inconsistency loss Twenty-Fifth International Joint Conference on Ar-\n\nfunction is introduced to penalize the inconsis- tificial Intelligence (IJCAF 16).\n\ntency between two levels of attentions. The in- Jianpeng Cheng and Mirella Lapata. 2016. Neural\n\nconsistency loss enables extractive and abstrac- summarization by extracting sentences and words.\n\ntive summarization to be mutually beneficial. By In Proceedings of the 54th Annual Meeting of the\n_. . Association for Computational Linguistics (Volume\n\nend-to-end training of our model, we achieve the 1: Long Papers), volume 1, pages 484-494.\n\nbest ROUGE-recall and ROUGE while being the\n\nmost informative and readable summarization on Adaptive subgradient methods for online learning\n\nthe CNN/ Daily Mail dataset in a solid human eval- and stochastic optimization. Journal of Machine\n\nuation. Learning Research, 12(Jul):2121-2159.\n\nJohn Duchi, Elad Hazan, and Yoram Singer. 2011.\n\nAngela Fan, David Grangier, and Michael Auli. 2017.\nControllable abstractive summarization. arXiv\npreprint arXiv:1711.05217.\n\nAcknowledgments\n\nWe thank the support from Cheetah Mobile, Na-\ntional Taiwan University, and MOST 107-2634-F- _—_ Jiatao Gu, Zhengdong Lu, Hang Li, and Victor OK\n\n007-007, 106-3114-E-007-004, 107-2633-E-002- Li. 2016. incorporating copying mechanism mr\n\n: : sequence-to-sequence learning. In Proceedings oj\n001. We thank Yun-Zhu Song for assistance with the 54th Annual Meeting of the Association for Com-\nuseful survey and experiment on the task of ab- putational Linguistics (Volume I: Long Papers),\nstractive summarization. volume 1, pages 1631-1640.\n\n140\n", "vlm_text": "\nthe-art methods in Fig.  6 . More examples (5 using CNN/Daily Mail news articles and 3 using non- news articles as inputs) are provided in the supple- mentary material. \n6 Conclusion \nWe propose a uniﬁed model combining the strength of extractive and abstractive summariza- tion. Most importantly, a novel inconsistency loss function is introduced to penalize the inconsis- tency between two levels of attentions. The in- consistency loss enables extractive and abstrac- tive summarization to be mutually beneﬁcial. By end-to-end training of our model, we achieve the best ROUGE-recall and ROUGE while being the most informative and readable summarization on the CNN/Daily Mail dataset in a solid human eval- uation. \nAcknowledgments \nWe thank the support from Cheetah Mobile, Na- tional Taiwan University, and MOST 107-2634-F- 007-007, 106-3114-E-007-004, 107-2633-E-002- 001. We thank Yun-Zhu Song for assistance with useful survey and experiment on the task of ab- stractive summarization. \nReferences \nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben- gio. 2014. Neural machine translation by jointly learning to align and translate. In  Proceedings of the 2015 International Conference on Learning Repre- sentations . \nQian Chen, Xiaodan Zhu, Zhenhua Ling, Si Wei, and Hui Jiang. 2016. Distraction-based neural networks for modeling documents. In  Proceedings of the Twenty-Fifth International Joint Conference on Ar- tiﬁcial Intelligence (IJCAI-16) . \nJianpeng Cheng and Mirella Lapata. 2016. Neural summarization by extracting sentences and words. In  Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , volume 1, pages 484–494. \nJohn Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research , 12(Jul):2121–2159. \nAngela Fan, David Grangier, and Michael Auli. 2017. Controllable abstractive summarization. arXiv preprint arXiv:1711.05217 . \nJiatao Gu, Zhengdong Lu, Hang Li, and Victor OK Li. 2016. Incorporating copying mechanism in sequence-to-sequence learning. In  Proceedings of the 54th Annual Meeting of the Association for Com- putational Linguistics (Volume 1: Long Papers) , volume 1, pages 1631–1640. "}
{"page": 9, "image_path": "doc_images/P18-1013_9.jpg", "ocr_text": "Karl Moritz Hermann, Tomas Kocisky, Edward\nGrefenstette, Lasse Espeholt, Will Kay, Mustafa Su-\nleyman, and Phil Blunsom. 2015. Teaching ma-\nchines to read and comprehend. In Advances in Neu-\nral Information Processing Systems, pages 1693-\n1701.\n\nMikael Kagebiick, Olof Mogren, Nina Tahmasebi, and\nDevdatt Dubhashi. 2014. Extractive summariza-\ntion using continuous vector space models. In Pro-\nceedings of the 2nd Workshop on Continuous Vector\nSpace Models and their Compositionality (CVSC),\npages 31-39.\n\nChin-Yew Lin. 2004. Rouge: A package for auto-\nmatic evaluation of summaries. Text Summarization\nBranches Out.\n\nLinging Liu, Yao Lu, Min Yang, Qiang Qu, Jia Zhu,\nand Hongyan Li. 2017. Generative adversarial net-\nwork for abstractive text summarization. In Proced-\ndings of the 2018 Association for the Advancement\nof Artificial Intelligence.\n\nYishu Miao and Phil Blunsom. 2016. Language as a\nlatent variable: Discrete generative models for sen-\ntence compression. In Proceedings of the 2016 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 319-328.\n\nRamesh Nallapati, Feifei Zhai, and Bowen Zhou. 2017.\nSummarunner: A recurrent neural network based se-\nquence model for extractive summarization of doc-\numents. In Proceddings of the 2017 Association\nfor the Advancement of Artificial Intelligence, pages\n3075-3081.\n\nRamesh Nallapati, Bowen Zhou, and Mingbo Ma.\n2016a. Classify or select: Neural architectures for\nextractive document summarization. arXiv preprint\narXiv:1611.04244.\n\nRamesh Nallapati, Bowen Zhou, Cicero dos San-\ntos, Caglar Gulcehre, and Bing Xiang. 2016b.\nAbstractive text summarization using sequence-to-\nsequence mns and beyond. In Proceedings of The\n20th SIGNLL Conference on Computational Natu-\nral Language Learning, pages 280-290.\n\nShashi Narayan, Nikos Papasarantopoulos, Mirella La-\npata, and Shay B Cohen. 2017. Neural extrac-\ntive summarization with side information. arXiv\npreprint arXiv: 1704.04530.\n\nRomain Paulus, Caiming Xiong, and Richard Socher.\n2017. A deep reinforced model for abstractive sum-\nmarization. In Proceedings of the 2018 Interna-\ntional Conference on Learning Representations.\n\nMarc’ Aurelio Ranzato, Sumit Chopra, Michael Auli,\nand Wojciech Zaremba. 2015. Sequence level train-\ning with recurrent neural networks. arXiv preprint\narXiv:1511.06732.\n\n141\n\nAlexander M Rush, Sumit Chopra, and Jason Weston.\n2015. A neural attention model for abstractive sen-\ntence summarization. In Proceedings of the 2015\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 379-389.\n\nAbigail See, Peter J Liu, and Christopher D Manning.\n2017. Get to the point: Summarization with pointer-\ngenerator networks. In Proceedings of the 55th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), volume 1,\npages 1073-1083.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, pages 6000-6010.\n\nOriol Vinyals, Meire Fortunato, and Navdeep Jaitly.\n2015. Pointer networks. In Advances in Neural In-\nformation Processing Systems, pages 2692-2700.\n\nZichao Yang, Diyi Yang, Chris Dyer, Xiaodong He,\nAlex Smola, and Eduard Hovy. 2016. Hierarchi-\ncal attention networks for document classification.\nIn Proceedings of the 2016 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 1480-1489.\n\nMichihiro Yasunaga, Rui Zhang, Kshitijh Meelu,\nAyush Pareek, Krishnan Srinivasan, and Dragomir\nRadev. 2017. Graph-based neural multi-document\nsummarization. In Proceedings of the 21st Confer-\nence on Computational Natural Language Learning\n(CoNLL 2017), pages 452-462.\n\nWenpeng Yin and Yulong Pei. 2015. Optimizing sen-\ntence modeling and selection for document summa-\ntization. In Proceedings of the 24th International\nJoint Conference on Artificial Intelligence, pages\n1383-1389. AAAI Press.\n", "vlm_text": "Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Su- leyman, and Phil Blunsom. 2015. Teaching ma- chines to read and comprehend. In  Advances in Neu- ral Information Processing Systems , pages 1693– 1701. Mikael K˚ ageb¨ ack, Olof Mogren, Nina Tahmasebi, and Devdatt Dubhashi. 2014. Extractive summariza- tion using continuous vector space models. In  Pro- ceedings of the 2nd Workshop on Continuous Vector Space Models and their Compositional it y (CVSC) , pages 31–39. Chin-Yew Lin. 2004. Rouge: A package for auto- matic evaluation of summaries.  Text Summarization Branches Out . Linqing Liu, Yao Lu, Min Yang, Qiang Qu, Jia Zhu, and Hongyan Li. 2017. Generative adversarial net- work for abstractive text summarization. In  Proced- dings of the 2018 Association for the Advancement of Artiﬁcial Intelligence . Yishu Miao and Phil Blunsom. 2016. Language as a latent variable: Discrete generative models for sen- tence compression. In  Proceedings of the 2016 Con- ference on Empirical Methods in Natural Language Processing , pages 319–328. Ramesh Nallapati, Feifei Zhai, and Bowen Zhou. 2017. Summarunner: A recurrent neural network based se- quence model for extractive summarization of doc- uments. In  Proceddings of the 2017 Association for the Advancement of Artiﬁcial Intelligence , pages 3075–3081. Ramesh Nallapati, Bowen Zhou, and Mingbo Ma. 2016a. Classify or select: Neural architectures for extractive document summarization.  arXiv preprint arXiv:1611.04244 . Ramesh Nallapati, Bowen Zhou, Cicero dos San- tos, Caglar Gulcehre, and Bing Xiang. 2016b. Abstractive text summarization using sequence-to- sequence rnns and beyond. In  Proceedings of The 20th SIGNLL Conference on Computational Natu- ral Language Learning , pages 280–290. Shashi Narayan, Nikos Papasarantopoulos, Mirella La- pata, and Shay B Cohen. 2017. Neural extrac- tive summarization with side information. arXiv preprint arXiv:1704.04530 . Romain Paulus, Caiming Xiong, and Richard Socher. 2017. A deep reinforced model for abstractive sum- marization. In  Proceedings of the 2018 Interna- tional Conference on Learning Representations . Marc’Aurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba. 2015. Sequence level train- ing with recurrent neural networks.  arXiv preprint arXiv:1511.06732 . \nAlexander M Rush, Sumit Chopra, and Jason Weston. 2015. A neural attention model for abstractive sen- tence summarization. In  Proceedings of the 2015 Conference on Empirical Methods in Natural Lan- guage Processing , pages 379–389. Abigail See, Peter J Liu, and Christopher D Manning. 2017. Get to the point: Summarization with pointer- generator networks. In  Proceedings of the 55th An- nual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , volume 1, pages 1073–1083. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In  Advances in Neural Information Pro- cessing Systems , pages 6000–6010. Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. 2015. Pointer networks. In  Advances in Neural In- formation Processing Systems , pages 2692–2700. Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He, Alex Smola, and Eduard Hovy. 2016. Hierarchi- cal attention networks for document classiﬁcation. In  Proceedings of the 2016 Conference of the North American Chapter of the Association for Computa- tional Linguistics: Human Language Technologies , pages 1480–1489. Michihiro Yasunaga, Rui Zhang, Kshitijh Meelu, Ayush Pareek, Krishnan Srinivasan, and Dragomir Radev. 2017. Graph-based neural multi-document summarization. In  Proceedings of the 21st Confer- ence on Computational Natural Language Learning (CoNLL 2017) , pages 452–462. Wenpeng Yin and Yulong Pei. 2015. Optimizing sen- tence modeling and selection for document summa- rization. In  Proceedings of the 24th International Joint Conference on Artiﬁcial Intelligence , pages 1383–1389. AAAI Press. "}
