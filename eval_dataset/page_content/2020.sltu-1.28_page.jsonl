{"page": 0, "image_path": "doc_images/2020.sltu-1.28_0.jpg", "ocr_text": "Proceedings of the Ist Joint SLTU and CCURL Workshop (SLTU-CCURL 2020), pages 202-210\nLanguage Resources and Evaluation Conference (LREC 2020), Marseille, 11-16 May 2020\n© European Language Resources Association (ELRA), licensed under CC-B Y-NC\n\nCorpus Creation for Sentiment Analysis in Code-Mixed Tamil-English Text\n\nBharathi Raja Chakravarthi!,Vigneshwaran Muralidaran?,\nRuba Priyadharshini’, John P. McCrae!\n‘Insight SFI Research Centre for Data Analytics, Data Science Institute,\nNational University of Ireland Galway, {bharathi.raja, john.mcecrae} @insight-centre.org\nSchool of English, Communication and Philosophy, Cardiff University, muralidaranV @ cardiff.ac.uk\n3Saraswathi Narayanan College, Madurai, India, rubapriyadharshini.a@ gmail.com\n\nAbstract\n\nUnderstanding the sentiment of a comment from a video or an image is an essential task in many applications. Sentiment analysis\nof a text can be useful for various decision-making processes. One such application is to analyse the popular sentiments of videos\non social media based on viewer comments. However, comments from social media do not follow strict rules of grammar, and they\ncontain mixing of more than one language, often written in non-native scripts. Non-availability of annotated code-mixed data for a\nlow-resourced language like Tamil also adds difficulty to this problem. To overcome this, we created a gold standard Tamil-English\ncode-switched, sentiment-annotated corpus containing 15,744 comment posts from YouTube. In this paper, we describe the process of\ncreating the corpus and assigning polarities. We present inter-annotator agreement and show the results of sentiment analysis trained on\n\nthis corpus as a benchmark.\n\nKeywords: code mixed, Tamil, sentiment, corpus, dataset\n\n1. Introduction\n\nSentiment analysis has become important in social media\nresearch (Yang and Eisenstein, 2017). Until recently these\napplications were created for high-resourced languages\nwhich analysed monolingual utterances. But social media\nin multilingual communities contains more code-mixed\ntext (Barman et al., 2014; Chanda et al., 2016; Pratapa et\nal., 2018a; Winata et al., 2019a). Our study focuses on\nsentiment analysis in Tamil, which has little annotated data\nfor code-mixed scenarios (Phani et al., 2016; Jose et al.,\n2020). Features based on the lexical properties such as a\ndictionary of words and parts of speech tagging have less\nperformance compared to the supervised learning (Kannan\net al., 2016) approaches using annotated data. However, an\nannotated corpus developed for monolingual data cannot\ndeal with code-mixed usage and therefore it fails to yield\ngood results (AlGhamdi et al., 2016; Aguilar et al., 2018)\ndue to mixture of languages at different levels of linguistic\nanalysis.\n\nCode-mixing is common among speakers in a bilingual\nspeech community. As English is seen as the language\nof prestige and education, the influence of lexicon, con-\nnectives and phrases from English language is common\nin spoken Tamil. It is largely observed in educated\nspeakers although not completely absent amongst less\neducated and uneducated speakers (Krishnasamy, 2015).\nDue to their pervasiveness of English online, code-mixed\nTamil-English (Tanglish) sentences are often typed in\nRoman script (Suryawanshi et al., 2020a; Suryawanshi et\nal., 2020b).\n\nWe present TamilMixSentiment ', a dataset of YouTube\nvideo comments in Tanglish. TamilMixSentiment was de-\n\nveloped with guidelines following the work of Mohammad\n\n‘https://github.com/bharathichezhiyan/TamilMixSentiment\n\n(2016) and without annotating the word level language\ntag. The instructions enabled light and speedy annotation\nwhile maintaining consistency. The overall inter-annotator\nagreement in terms of Kripendorffs’s a (Krippendorff,\n1970) stands at 0.6. In total, 15,744 comments were\nannotated; this makes the largest general domain sentiment\ndataset for this relatively low-resource language with\ncode-mixing phenomenon.\n\nWe observed all the three types of code-mixed sentences -\n- Inter-Sentential switch, Intra-Sentential switch and Tag\nswitching. Most comments were written in Roman script\nwith either Tamil grammar with English lexicon or English\ngrammar with Tamil lexicon. Some comments were written\nin Tamil script with English expressions in between. The\nfollowing examples illustrate the point.\n\ne Intha padam vantha piragu yellarum Thala ya\nkondaduvanga. - After the movie release, everybody\nwill celebrate the hero. Tamil words written in Roman\nscript with no English switch.\n\ne Trailer late ah parthavanga like podunga. - Those\nwho watched the trailer late, please like it. Tag switch-\ning with English words.\n\ne Omg .. use head phones. Enna bgm da saami.. -\n- OMG! Use your headphones. Good Lord, What a\nbackground score! Inter-sentential switch\n\ne I think sivakarthickku hero getup set aagala. - /\nthink the hero role does not suit Sivakarthick. Intra-\nsentential switch between clauses.\n\nIn this work we present our dataset, annotation scheme\nand investigate the properties and statistics of the dataset\nand information about the annotators. We also present\nbaseline classification results on the new dataset with ten\n\n202\n", "vlm_text": "Corpus Creation for Sentiment Analysis in Code-Mixed Tamil-English Text \nBharathi Raja Chakravarthi 1 ,Vigneshwaran Muralidaran 2 , Ruba Priyadharshini 3 , John P. McCrae 1 \n1 Insight SFI Research Centre for Data Analytics, Data Science Institute, National University of Ireland Galway,  { bharathi.raja, john.mccrae } @insight-ce re.org 2 School of English, Communication and Philosophy, Cardiff University, muralidaranV@cardiff.ac.uk 3 Saraswathi Narayanan College, Madurai, India, rub a pri yad harsh in i.a  $@$  gmail.com \nAbstract \nUnderstanding the sentiment of a comment from a video or an image is an essential task in many applications. Sentiment analysis of a text can be useful for various decision-making processes. One such application is to analyse the popular sentiments of videos on social media based on viewer comments. However, comments from social media do not follow strict rules of grammar, and they contain mixing of more than one language, often written in non-native scripts. Non-availability of annotated code-mixed data for a low-resourced language like Tamil also adds difﬁculty to this problem. To overcome this, we created a gold standard Tamil-English code-switched, sentiment-annotated corpus containing 15,744 comment posts from YouTube. In this paper, we describe the process of creating the corpus and assigning polarities. We present inter-annotator agreement and show the results of sentiment analysis trained on this corpus as a benchmark. \nKeywords:  code mixed, Tamil, sentiment, corpus, dataset \n1. Introduction \nSentiment analysis has become important in social media research (Yang and Eisenstein, 2017). Until recently these applications were created for high-resourced languages which analysed monolingual utterances. But social media in multilingual communities contains more code-mixed text (Barman et al., 2014; Chanda et al., 2016; Pratapa et al., 2018a; Winata et al., 2019a). Our study focuses on sentiment analysis in Tamil, which has little annotated data for code-mixed scenarios (Phani et al., 2016; Jose et al., 2020). Features based on the lexical properties such as a dictionary of words and parts of speech tagging have less performance compared to the supervised learning (Kannan et al., 2016) approaches using annotated data. However, an annotated corpus developed for monolingual data cannot deal with code-mixed usage and therefore it fails to yield good results (AlGhamdi et al., 2016; Aguilar et al., 2018) due to mixture of languages at different levels of linguistic analysis. \nCode-mixing is common among speakers in a bilingual speech community. As English is seen as the language of prestige and education, the inﬂuence of lexicon, con- nectives and phrases from English language is common in spoken Tamil. It is largely observed in educated speakers although not completely absent amongst less educated and uneducated speakers (Krishnasamy, 2015). Due to their pervasiveness of English online, code-mixed Tamil-English (Tanglish) sentences are often typed in Roman script (Suryawanshi et al., 2020a; Suryawanshi et al., 2020b). \nWe present Tamil Mix Sentiment   1 , a dataset of YouTube video comments in Tanglish. Tamil Mix Sentiment was de- veloped with guidelines following the work of Mohammad (2016) and without annotating the word level language tag. The instructions enabled light and speedy annotation while maintaining consistency. The overall inter-annotator agreement in terms of Kripendorffs’s    $\\alpha$   (Krippendorff, 1970) stands at 0.6. In total, 15,744 comments were annotated; this makes the largest general domain sentiment dataset for this relatively low-resource language with code-mixing phenomenon. \n\nWe observed all the three types of code-mixed sentences - - Inter-Sentential switch, Intra-Sentential switch and Tag switching. Most comments were written in Roman script with either Tamil grammar with English lexicon or English grammar with Tamil lexicon. Some comments were written in Tamil script with English expressions in between. The following examples illustrate the point. \n•  Intha padam vantha piragu yellarum Thala ya kondaduvanga.  -  After the movie release, everybody will celebrate the hero.  Tamil words written in Roman script with no English switch. •  Trailer late  ah parthavanga  like  podunga.  -  Those who watched the trailer late, please like it.  Tag switch- ing with English words. •  Omg .. use head phones.  Enna  bgm  da saami ..  - -  OMG! Use your headphones. Good Lord, What a background score!  Inter-sentential switch •  I think  sivakarthickku  hero getup set  aagala.  -  I think the hero role does not suit Sivakarthick.  Intra- sentential switch between clauses. \nIn this work we present our dataset, annotation scheme and investigate the properties and statistics of the dataset and information about the annotators. We also present baseline classiﬁcation results on the new dataset with ten models to establish a baseline for future comparisons. The best results were achieved with models that use logistic regression and random forest. "}
{"page": 1, "image_path": "doc_images/2020.sltu-1.28_1.jpg", "ocr_text": "models to establish a baseline for future comparisons. The\nbest results were achieved with models that use logistic\nregression and random forest.\n\nThe contribution of this paper is two-fold:\n\n1. We present the first gold standard code-mixed Tamil-\nEnglish dataset annotated for sentiment analysis.\n\n2. We provide an experimental analysis of logistic re-\ngression, naive Bayes, decision tree, random forest,\nSVM, dynamic meta-embedding, contextualized dy-\nnamic meta-embedding, 1DConv-LSTM and BERT\non our code-mixed data for sentiment classification.\n\n2. Related Work\n\nRecently, there has been a considerable amount of work and\neffort to collect resources for code-switched text. However,\ncode-switched datasets and lexicons for sentiment analy-\nsis are still limited in number, size and availability. For\nmonolingual analysis, there exist various corpora for En-\nglish (Hu and Liu, 2004; Wiebe et al., 2005; Jiang et al.,\n2019), Russian (Rogers et al., 2018), German (Cieliebak\net al., 2017), Norwegian (Mehlum et al., 2019) and Indian\nlanguages (Agrawal et al., 2018; Rani et al., 2020).\n\nWhen it comes to code-mixing, an English-Hindi corpus\nwas created by (Sitaram et al., 2015; Joshi et al., 2016; Pa-\ntra et al., 2018), an English-Spanish corpus was introduced\nby (Solorio et al., 2014; Vilares et al., 2015; Vilares et al.,\n2016), and a Chinese-English one (Lee and Wang, 2015)\nwas collected from Weibo.com and English-Bengali data\nwere released by Patra et al. (Patra et al., 2018).\n\nTamil is a Dravidian language spoken by Tamil people in\nIndia, Sri Lanka and by the Tamil diaspora around the\nworld, with official recognition in India, Sri Lanka and\nSingapore (Chakravarthi et al., 2018; Chakravarthi et al.,\n2019a; Chakravarthi et al., 2019b; Chakravarthi et al.,\n2019c). Several research activities on sentiment analysis\nin Tamil (Padmamala and Prema, 2017) and other Indian\nlanguages (Ranjan et al., 2016; Das and Bandyopadhyay,\n2010; A.R. et al., 2012; Phani et al., 2016; Prasad et al.,\n2016; Priyadharshini et al., 2020; Chakravarthi et al., 2020)\nare happening because the sheer number of native speakers\nare a potential market for commercial NLP applications.\nHowever, sentiment analysis on Tamil-English code-mixed\ndata (Patra et al., 2018) is under-developed and data tare not\nreadily available for research.\n\nUntil recently, word-level annotations were used for\nresearch in code-mixed corpora. Almost all the previous\nsystems proposed were based on data annotated at the\nword-level. This is not only time-consuming but also\nexpensive to create. However, neural networks and meta-\nembeddings (Kiela et al., 2018) have shown great promise\nin code-switched research without the need for word-level\nannotation. In particular, work by Winata et al. (2019a)\nlearns to utilise information from pre-trained embeddings\nwithout explicit word-level language tags. A recent\nwork by Winata et al. (2019b) utilised the subword-level\ninformation from closely related languages to improve the\nperformance on the code-mixed text.\n\nAs there was no previous dataset available for Tamil-\nEnglish (Tanglish) sentiment annotation, we create a sen-\ntiment dataset for Tanglish with voluntary annotators. We\nalso show the baseline results with a few models explained\nin Section 5.\n\nPositive state: There is an explicit or implicit clue in the text suggesting that the\nspeaker is ina positive state, ie., happy, admiring, relaxed, forgiving, etc.\nCpfoap 2 amjayflona: UPA Laif yssLyFonoren 2. cmpaiflanandS igi\na pAUIGEsApnG craupBH)S Gare _unsCaun wepepsraGan snénpEdt\nApAdlénpon. a.50: WADA, ITLL, Sof), ocrahlssa epsdus e camiaysscr\n\n© Understand\n\nO no\n\nNegative state: There is an explicit or implicit clue in the text suggesting that the\nspeaker is in a negative state, ie., sad, angry, anxious, violent, etc. «isla\nLamjayflena: UPA. aij Pjverpusmen 2 amrjayplenaruISG,\naeQPUGADN} craé11g,)6 Gareficiic9_wnsCaun wepgpsioTEGaun snenpsdt\nAphBenpen. a.50: Conaib, Camu, upspib, exenoid apsSuenar.*\n\n© Understand\n\nO no\n\nBoth positive and negative, or mixed, feelings: There is an explicit or implicit clue\nin the text suggesting that the speaker is experiencing both positive and\nnegative feeling, Example: Comparing two movies Gain oBGid sass s@eaL\nsamjaflera: UAC eu somauuiten 2 anjaflorouidisD erie\napAUGsApn} créup)e GarefiU_wnsCaun wepgpsioraGan snéngEdt\nApHAcnpan. a.sn: QyarG Soop wsoe GiiIG UpAOB, *\n\n© Understand\n\nO no\n\nNeutral state: There is no explicit or implicit indicator of the speaker’s emotional\nstate: Examples are asking for like or subscription or questions about release\ndate or movie dialog etc. BOP\nGavotte wins Geum weopqpaions Geum 5.\nmad sag sicrdlents Asis Qandrdlés Got_Lgy, Utd Garaflagid C56)\nBaumd Cato, Panpiuuevsend upPlu uplayscr. *\n\n2 Cuban\n\nO Understand\n\nO no\n(a) Example |\n\nChoose the best sentiment *\n\nThala fans hu sera gt. vachu siylasm. By Viay fan\nO Positive\n\nO Negative\n\nO Mined feeiings\n\n© unknown state\n\nO ottami\n\nChoose the best sentiment *\n\nEpdi da Kujay fons autoke vangucinga\nO Positive\n\nO Negative\n\nO Mined feeiings\n\n© unknown state\n\nO ottami\n\n(b) Example 2\n\nFigure 1: Examples of Google Form.\n\n203\n", "vlm_text": "\nThe contribution of this paper is two-fold: \n1. We present the ﬁrst gold standard code-mixed Tamil- \n2. We provide an experimental analysis of logistic re- gression, naive Bayes, decision tree, random forest, SVM, dynamic meta-embedding, contextualized dy- namic meta-embedding, 1DConv-LSTM and BERT on our code-mixed data for sentiment classiﬁcation. \n2. Related Work \nRecently, there has been a considerable amount of work and effort to collect resources for code-switched text. However, code-switched datasets and lexicons for sentiment analy- sis are still limited in number, size and availability. For monolingual analysis, there exist various corpora for En- glish (Hu and Liu, 2004; Wiebe et al., 2005; Jiang et al., 2019), Russian (Rogers et al., 2018), German (Cieliebak et al., 2017), Norwegian (Mæhlum et al., 2019) and Indian languages (Agrawal et al., 2018; Rani et al., 2020). When it comes to code-mixing, an English-Hindi corpus was created by (Sitaram et al., 2015; Joshi et al., 2016; Pa- tra et al., 2018), an English-Spanish corpus was introduced by (Solorio et al., 2014; Vilares et al., 2015; Vilares et al., 2016), and a Chinese-English one (Lee and Wang, 2015) was collected from Weibo.com and English-Bengali data were released by Patra et al. (Patra et al., 2018). Tamil is a Dravidian language spoken by Tamil people in India, Sri Lanka and by the Tamil diaspora around the world, with ofﬁcial recognition in India, Sri Lanka and Singapore (Chakravarthi et al., 2018; Chakravarthi et al., 2019a; Chakravarthi et al., 2019b; Chakravarthi et al., 2019c). Several research activities on sentiment analysis in Tamil (Padmamala and Prema, 2017) and other Indian languages (Ranjan et al., 2016; Das and Bandyopadhyay, 2010; A.R. et al., 2012; Phani et al., 2016; Prasad et al., 2016; Priyadharshini et al., 2020; Chakravarthi et al., 2020) are happening because the sheer number of native speakers are a potential market for commercial NLP applications. However, sentiment analysis on Tamil-English code-mixed data (Patra et al., 2018) is under-developed and data tare not readily available for research. \nUntil recently, word-level annotations were used for research in code-mixed corpora. Almost all the previous systems proposed were based on data annotated at the word-level. This is not only time-consuming but also expensive to create. However, neural networks and meta- embeddings (Kiela et al., 2018) have shown great promise in code-switched research without the need for word-level annotation. In particular, work by Winata et al. (2019a) learns to utilise information from pre-trained embeddings without explicit word-level language tags. A recent work by Winata et al. (2019b) utilised the subword-level information from closely related languages to improve the performance on the code-mixed text. \nAs there was no previous dataset available for Tamil- English (Tanglish) sentiment annotation, we create a sen- timent dataset for Tanglish with voluntary annotators. We also show the baseline results with a few models explained in Section 5. \nThe image displays a section from a form or survey where participants are asked to evaluate statements and indicate their level of understanding regarding different emotional states based on textual clues. The statements are divided into four categories:\n\n1. **Positive State**: This involves identifying explicit or implicit clues in the text suggesting the speaker is in a positive emotional state (e.g., happy, admiring, relaxed). Below the description, there are radio button options for \"Understand\" or \"No.\"\n\n2. **Negative State**: This involves identifying explicit or implicit clues in the text suggesting the speaker is in a negative emotional state (e.g., sad, angry, anxious). Radio button options for \"Understand\" or \"No\" are provided.\n\n3. **Both Positive and Negative, or Mixed, Feelings**: This involves identifying clues indicating that the speaker has both positive and negative emotions simultaneously. Participants can choose \"Understand\" or \"No.\"\n\n4. **Neutral State**: This involves identifying scenarios where there are no explicit or implicit indicators of the speaker's emotional state, such as asking questions about subscriptions or movie release dates. Options for \"Understand\" or \"No\" are given.\n\nEach category includes text both in English and Tamil, explaining the context and examples of emotional states. Participants are expected to select the appropriate response based on their understanding.\nThe image shows a form with two sections, each asking the viewer to choose the best sentiment for a given statement. The options for sentiment are: Positive, Negative, Mixed feelings, Unknown state, and Not-Tamil. The statements seem to be comments related to fans of Thala (Ajith Kumar) and Vijay, who are prominent Tamil actors. The statements appear in Tamil with some Roman script. The first example translates roughly to: \"Thala fans got a great gift... will do something... By Vijay fan.\" The second one translates to: \"How do the Vijay fans receive auto likes?\""}
{"page": 2, "image_path": "doc_images/2020.sltu-1.28_2.jpg", "ocr_text": "3. Corpus Creation and Annotation\n\nOur goal was to create a code-mixed dataset for Tamil to\nensure that enough data are available for research purposes.\nWe used the YouTube Comment Scraper tool? and collected\n184,573 sentences for Tamil from YouTube comments. We\ncollected the comments from the trailers of a movies re-\nleased in 2019. Many of the them contained sentences\nthat were either entirely written in English or code-mixed\nTamil-English or fully written in Tamil. So we filtered out\na non-code-mixed corpus based on language identification\nat comment level using the langdetect library >. Thus if\nthe comment is written fully in Tamil or English, we dis-\ncarded that comment since monolingual resources are avail-\nable for these languages. We also identified if the sentences\nwere written in other languages such as Hindi, Malayalam,\nUrdu, Telugu, and Kannada. We preprocessed the com-\nments by removing the emoticons and applying a sentence\nlength filter. We want to create a code-mixed corpus of\nreasonable size with sentences that have fairly defined sen-\ntiments which will be useful for future research. Thus our\nfilter removed sentences with less than five words and more\nthan 15 words after cleaning the data. In the end we got\n15,744 Tanglish sentences.\n\n3.1. Annotation Setup\n\nFor annotation, we adopted the approach taken by Moham-\nmad (2016), and a minimum of three annotators annotated\neach sentence in the dataset according to the following\nschema shown in the Figure 1. We added new category\nOther language: If the sentence is written in some other\nlanguage other than Tamil or English. Examples for this\nare the comments written in other Indian languages using\nthe Roman script. The annotation guidelines are given in\nEnglish and Tamil.\n\nAs we have collected data from YouTube we anonymized to\nkeep the privacy of the users who commented on it. As the\nvoluntary annotators’ personal information were collected\nto know about the them, this gives rise to both ethical, pri-\nvacy and legal concerns. Therefore, the annotators were\ninformed in the beginning that their data is being recorded\nand they can choose to withdraw from the process at any\nstage of annotation. The annotators should actively agree\nto being recorded. We created Google Forms in which we\ncollected the annotators’ email addresses which we used\nto ensure that an annotator was allowed to label a given\nsentence only once. We collected the information on gen-\nder, education and medium of instruction in school to know\nthe diversity of annotators. Each Google form has been\nset to contain a maximum of 100 sentences. Example of\nthe Google form is given in the Figure 1. The annotators\nhave to agree that they understood the scheme; otherwise,\nthey cannot proceed further. Three steps complete the an-\nnotation setup. First, each sentence was annotated by two\npeople. In the second step, the data were collected if both\nof them agreed. In the case of conflict, a third person an-\nnotated the sentence. In the third step, if all the three of\n\n*https://github.com/philbot9/youtube-comment-scraper\n3https://pypi.org/project/langdetect/\n\nthem did not agree, then two more annotators annotated the\nsentences.\n\nGender Male 9\nFemale 2\nHigher Education Undegraduate | 2\nGraduate 2\nPostgraduate | 7\nMedium of Schooling | English 6\nTamil 5\nTotal 11\n\nTable 1: Annotators\n\n3.2. Annotators\n\nTo control the quality of annotation, we removed the an-\nnotator who did not annotate well in the first form. For\nexample, if the annotators showed unreasonable delay in\nresponding or if they labelled all sentences with the same\nsentiment or if more than fifty annotations in a form were\nwrong, we removed those contributions. Eleven volun-\nteers were involved in the process. All of them were na-\ntive speakers of Tamil with diversity in gender, educational\nlevel and medium of instruction in their school education.\nTable 1 shows information about the annotators. The vol-\nunteers were instructed to fill up the Google form, and 100\nsentences were sent to them. If an annotator offers to vol-\nunteer more, the next Google form is sent to them with an-\nother set of 100 sentences and in this way each volunteer\nchooses to annotate as many sentences from the corpus as\nthey want. We send the forms to an equal number of male\nand female annotators. However, from Table 1, we can see\nthat only two female annotators volunteered to contribute.\n\n3.3. Corpus Statistics\n\nCorpus statistics is given in the Table 2. The distribution\nof released data is shown in Table 3. The entire dataset of\n15,744 sentences was randomly shuffled and split into three\nparts as follows: 11,335 sentences were used for training,\n1,260 sentences form the validation set and 3,149 sentences\nwere used for testing. The machine learning models were\napplied to this subset of data rather than k-fold cross valida-\ntion. The only other code-mixed dataset of reasonable size\nthat we could find was an earlier work by Remmiya Devi et\nal. (2016) on code-mix entity extraction for Hindi-English\nand Tamil-English tweets, released as a part of the shared\ntask in FIRE 2016. The dataset consisted of 3,200 Tanglish\ntweets used for training and 1,376 tweets for testing.\n\n3.4. Inter Annotator Agreement\n\nWe used Krippendorff’s alpha (a) (Krippendorff, 1970)\nto measure inter-annotator agreement because of the nature\nof our annotation setup. This is a robust statistical measure\nthat accounts for incomplete data and, therefore, does not\nrequire every annotator to annotate every sentence. It is also\na measure that takes into account the degree of disagree-\nment between the predicted classes, which is crucial in our\nannotation scheme. For instance, if the annotators disagree\n\n204\n", "vlm_text": "3. Corpus Creation and Annotation \nOur goal was to create a code-mixed dataset for Tamil to ensure that enough data are available for research purposes. We used the  YouTube Comment Scraper tool 2   and collected 184,573 sentences for Tamil from YouTube comments. We collected the comments from the trailers of a movies re- leased in 2019. Many of the them contained sentences that were either entirely written in English or code-mixed Tamil-English or fully written in Tamil. So we ﬁltered out a non-code-mixed corpus based on language identiﬁcation at comment level using the  langdetect library   3 . Thus if the comment is written fully in Tamil or English, we dis- carded that comment since monolingual resources are avail- able for these languages. We also identiﬁed if the sentences were written in other languages such as Hindi, Malayalam, Urdu, Telugu, and Kannada. We preprocessed the com- ments by removing the emoticons and applying a sentence length ﬁlter. We want to create a code-mixed corpus of reasonable size with sentences that have fairly deﬁned sen- timents which will be useful for future research. Thus our ﬁlter removed sentences with less than ﬁve words and more than 15 words after cleaning the data. In the end we got 15,744 Tanglish sentences. \n3.1. Annotation Setup \nFor annotation, we adopted the approach taken by Moham- mad (2016), and a minimum of three annotators annotated each sentence in the dataset according to the following schema shown in the Figure 1. We added new category Other language:  If the sentence is written in some other language other than Tamil or English. Examples for this are the comments written in other Indian languages using the Roman script. The annotation guidelines are given in English and Tamil. \nAs we have collected data from YouTube we anonymized to keep the privacy of the users who commented on it. As the voluntary annotators’ personal information were collected to know about the them, this gives rise to both ethical, pri- vacy and legal concerns. Therefore, the annotators were informed in the beginning that their data is being recorded and they can choose to withdraw from the process at any stage of annotation. The annotators should actively agree to being recorded. We created Google Forms in which we collected the annotators’ email addresses which we used to ensure that an annotator was allowed to label a given sentence only once. We collected the information on gen- der, education and medium of instruction in school to know the diversity of annotators. Each Google form has been set to contain a maximum of 100 sentences. Example of the Google form is given in the Figure 1. The annotators have to agree that they understood the scheme; otherwise, they cannot proceed further. Three steps complete the an- notation setup. First, each sentence was annotated by two people. In the second step, the data were collected if both of them agreed. In the case of conﬂict, a third person an- notated the sentence. In the third step, if all the three of them did not agree, then two more annotators annotated the sentences. \n\nThe table presents data under three main categories: Gender, Higher Education, and Medium of Schooling. Each category lists specific attributes with their corresponding counts.\n\n1. **Gender**:\n   - Male: 9\n   - Female: 2\n\n2. **Higher Education**:\n   - Undergraduate: 2\n   - Graduate: 2\n   - Postgraduate: 7\n\n3. **Medium of Schooling**:\n   - English: 6\n   - Tamil: 5\n\n4. **Total**: \n   - The total number for each category is 11.\n3.2. Annotators \nTo control the quality of annotation, we removed the an- notator who did not annotate well in the ﬁrst form. For example, if the annotators showed unreasonable delay in responding or if they labelled all sentences with the same sentiment or if more than ﬁfty annotations in a form were wrong, we removed those contributions. Eleven volun- teers were involved in the process. All of them were na- tive speakers of Tamil with diversity in gender, educational level and medium of instruction in their school education. Table 1 shows information about the annotators. The vol- unteers were instructed to ﬁll up the Google form, and 100 sentences were sent to them. If an annotator offers to vol- unteer more, the next Google form is sent to them with an- other set of 100 sentences and in this way each volunteer chooses to annotate as many sentences from the corpus as they want. We send the forms to an equal number of male and female annotators. However, from Table 1, we can see that only two female annotators volunteered to contribute. \n3.3. Corpus Statistics \nCorpus statistics is given in the Table 2. The distribution of released data is shown in Table 3. The entire dataset of 15,744 sentences was randomly shufﬂed and split into three parts as follows: 11,335 sentences were used for training, 1,260 sentences form the validation set and 3,149 sentences were used for testing. The machine learning models were applied to this subset of data rather than k-fold cross valida- tion. The only other code-mixed dataset of reasonable size that we could ﬁnd was an earlier work by Remmiya Devi et al. (2016) on code-mix entity extraction for Hindi-English and Tamil-English tweets, released as a part of the shared task in FIRE 2016. The dataset consisted of 3,200 Tanglish tweets used for training and 1,376 tweets for testing. \n3.4. Inter Annotator Agreement \nWe used  Krippendorff’s alpha    $(\\alpha)$   (Krippendorff, 1970) to measure inter-annotator agreement because of the nature of our annotation setup. This is a robust statistical measure that accounts for incomplete data and, therefore, does not require every annotator to annotate every sentence. It is also a measure that takes into account the degree of disagree- ment between the predicted classes, which is crucial in our annotation scheme. For instance, if the annotators disagree "}
{"page": 3, "image_path": "doc_images/2020.sltu-1.28_3.jpg", "ocr_text": "Language pair Tamil-English\nNumber of Tokens 169,833\nVocabulary Size 30,898\nNumber of Posts 15,744\nNumber of Sentences 17,926\nAverage number of Tokens per post 10\nAverage number of sentences per post 1\n\nTable 2: Corpus statistic of and Tamil-English\n\nClass Tamil-English\nPositive 10,559\nNegative 2,037\nMixed feelings 1,801\nNeutral 850\nOther language 497\nTotal 15,744\n\nTable 3: Data Distribution\n\nbetween Positive and Negative class, this disagreement is\nmore serious than when they disagree between Mixed feel-\nings and Neutral. a can handle such disagreements. a is\ndefined as:\n\nDo\na=1-— 1\nD, qd)\nD, is the observed disagreement between sentiment la-\nbels by the annotators and D, is the disagreement expected\nwhen the coding of sentiments can be attributed to chance\nrather than due to the inherent property of the sentiment\n\nitself. 1\nD, = n » » ck metric Sek (2)\n\n1 :\nD. = nin 1) » » Ne » Nk metric Sek (3)\n\nHere 0-4 Ne nz and n refer to the frequencies of values\nin coincidence matrices and metric refers to any metric\nor level of measurement such as nominal, ordinal, inter-\nval, ratio and others. Krippendorff’s alpha applies to all\nthese metrics. We used nominal and interval metric to cal-\nculate annotator agreement. The range of a is between 0\nand 1,1 > a > 0. When a is | there is perfect agreement\nbetween annotators and when 0 the agreement is entirely\ndue to chance. Our annotation produced an agreement of\n0.6585 using nominal metric and 0.6799 using interval met-\nric.\n\n4. Difficult Examples\n\nIn this section we talk about some examples that were dif-\nficult to annotate.\n\n1. Enakku iru mugan trailer gnabagam than varuthu\n- All it reminds me of is the trailer of the movie Irumu-\ngan. Not sure whether the speaker enjoyed Irumugan\ntrailer or disliked it or simply observed the similarities\nbetween the two trailers.\n\n2. Rajini ah vida akshay mass ah irukane - Akshay\nlooks more amazing than Rajini. Difficult to decide\nif it is a disappointment that the villain looks better\nthan the hero or a positive appreciation for the villain\nactor.\n\n3. Ada dei nama sambatha da dei - J wonder, Is this\nour sampath? Hey!. Conflict between neutral and pos-\nitive.\n\n4. Lokesh kanagaraj movie naalae.... English\nRap....Song vandurum - /f it is a movie of Lokesh\nkanagaraj, it always has an English rap song. Am-\nbiguous sentiment.\n\nAccording to the instructions, questions about music direc-\ntor, movie release date and remarks about when the speaker\nis watching the video should be treated as neutral. However\nthe above examples show that some comments about the ac-\ntors and movies can be ambiguously interpreted as neutral\nor positive or negative. We found annotator disagreements\nin such sentences.\n\n5. Benchmark Systems\n\nIn order to provide a simple baseline, we applied vari-\nous machine learning algorithms for determining the senti-\nments of YouTube posts in code-mixed Tamil-English lan-\nguage.\n\n5.1. Experimental Settings\n\n5.1.1. Logistic Regression (LR):\n\nWe evaluate the Logistic Regression model with L2 regular-\nization. The input features are the Term Frequency Inverse\nDocument Frequency (TF-IDF) values of up to 3 grams.\n\n5.1.2. Support Vector Machine (SVM):\n\nWe evaluate the SVM model with L2 regularization. The\nfeatures are the same as in LR. The purpose of SVM classi-\nfication algorithm is to define optimal hyperplane in N di-\nmensional space to separate the data points from each other.\n\n5.1.3. K-Nearest Neighbour (K-NN):\nWe use KNN for classification with 3,4,5,and 9 neighbours\nby applying uniform weights.\n\n5.1.4. Decision Tree (DT):\n\nDecision trees have been previously used in NLP tasks for\nclassification. In decision tree, the prediction is done by\nsplitting the root training set into subsets as nodes, and each\nnode contains output of the decision, label or condition. Af-\nter sequentially choosing alternative decisions, each node\n\n205\n", "vlm_text": "The table presents data related to a Tamil-English language pair, detailing various linguistic characteristics:\n\n1. The total number of tokens is 169,833.\n2. The vocabulary size (i.e., the number of unique words) is 30,898.\n3. There are 15,744 posts.\n4. The total number of sentences is 17,926.\n5. The average number of tokens per post is 10.\n6. The average number of sentences per post is 1.\nThe table provides a distribution of categories for a Tamil-English dataset. It consists of the following:\n\n- Positive: 10,559 entries\n- Negative: 2,037 entries\n- Mixed feelings: 1,801 entries\n- Neutral: 850 entries\n- Other language: 497 entries\n\nOverall, the table totals 15,744 entries across these categories.\nbetween  Positive  and  Negative  class, this disagreement is more serious than when they disagree between  Mixed feel- ings  and  Neutral .    $\\alpha$   can handle such disagreements.    $\\alpha$   is deﬁned as: \n\n$$\n\\alpha=1-\\frac{D_{o}}{D_{e}}\n$$\n \n $D_{o}$   is the observed disagreement between sentiment la- bels by the annotators and    $D_{e}$   is the disagreement expected when the coding of sentiments can be attributed to chance rather than due to the inherent property of the sentiment itself. \n\n$$\nD_{o}=\\frac{1}{n}\\sum_{c}\\sum_{k}o_{c k\\;m e t r i c}\\;\\delta_{c k}^{2}\n$$\n \n\n$$\nD_{e}=\\frac{1}{n(n-1)}\\sum_{c}\\sum_{k}n_{c}\\cdot\\,n_{k\\;m e t r i c}\\,\\delta_{c k}^{2}\n$$\n \nHere  $o_{c k}\\ n_{c}\\ n_{k}$  and  $n$   refer to the frequencies of values in coincidence matrices and  metric  refers to any metric or level of measurement such as nominal, ordinal, inter- val, ratio and others. Krippendorff’s alpha applies to all these metrics. We used nominal and interval metric to cal- culate annotator agreement. The range of  $\\alpha$   is between 0 and 1,  $1\\geq\\alpha\\geq0$  . When    $\\alpha$   is 1 there is perfect agreement between annotators and when 0 the agreement is entirely due to chance. Our annotation produced an agreement of 0.6585 using nominal metric and 0.6799 using interval met- ric. \n4. Difﬁcult Examples \nIn this section we talk about some examples that were dif- ﬁcult to annotate. \n1.  Enakku iru mugan  trailer  gnabagam than varuthu -  All it reminds me of is the trailer of the movie Irumu- gan . Not sure whether the speaker enjoyed Irumugan trailer or disliked it or simply observed the similarities between the two trailers. \n2.  Rajini ah vida akshay  mass  ah irukane  -  Akshay looks more amazing than Rajini . Difﬁcult to decide if it is a disappointment that the villain looks better than the hero or a positive appreciation for the villain actor. 3.  Ada dei nama sambatha da dei  -  I wonder, Is this our sampath? Hey!.  Conﬂict between neutral and pos- itive. 4.  Lokesh kanagaraj movie naalae.... English Rap....Song  vandurum  -  If it is a movie of Lokesh kanagaraj, it always has an English rap song . Am- biguous sentiment. \nAccording to the instructions, questions about music direc- tor, movie release date and remarks about when the speaker is watching the video should be treated as neutral. However the above examples show that some comments about the ac- tors and movies can be ambiguously interpreted as neutral or positive or negative. We found annotator disagreements in such sentences. \n5. Benchmark Systems \nIn order to provide a simple baseline, we applied vari- ous machine learning algorithms for determining the senti- ments of YouTube posts in code-mixed Tamil-English lan- guage. \n5.1. Experimental Settings \n5.1.1. Logistic Regression (LR): \nWe evaluate the Logistic Regression model with L2 regular- ization. The input features are the Term Frequency Inverse Document Frequency (TF-IDF) values of up to 3 grams. \n5.1.2. Support Vector Machine (SVM): \nWe evaluate the SVM model with L2 regularization. The features are the same as in LR. The purpose of SVM classi- ﬁcation algorithm is to deﬁne optimal hyperplane in N di- mensional space to separate the data points from each other. \n5.1.3. K-Nearest Neighbour (K-NN): \nWe use KNN for classiﬁcation with 3,4,5,and 9 neighbours by applying uniform weights. \n5.1.4. Decision Tree (DT): \nDecision trees have been previously used in NLP tasks for classiﬁcation. In decision tree, the prediction is done by splitting the root training set into subsets as nodes, and each node contains output of the decision, label or condition. Af- ter sequentially choosing alternative decisions, each node "}
{"page": 4, "image_path": "doc_images/2020.sltu-1.28_4.jpg", "ocr_text": "Classifier Positive | Negative | Neutral | Mixed | Other language | Micro Avg | Macro Avg | Weighted Avg\nKNN 0.70 0.23 0.35 0.16 0.06 0.45 0.30 0.53\nDecision Tree 0.71 0.30 0.24) 0.17 0.60 0.61 0.40 0.56\nRandom Forest 0.69 0.51 0.80 | 0.41 0.68 0.68 0.62 0.63\nLogistic Regression 0.68 0.56 0.61 0.36 0.76 0.68 0.59 0.62\nNaive Bayes 0.66 0.62 0.00} 0.40 0.69 0.66 0.48 0.59\nSVM 0.66 0.00 0.00 | 0.00 0.00 0.66 0.13 0.43\n1DConv-LSTM 0.71 0.30 0.00) 0.14 0.67 0.63 0.36 0.54\nDME 0.68 0.34 0.31 0.29 0.71 0.67 0.46 0.57\nCDME 0.67 0.56 0.56 | 0.20 0.68 0.67 0.53 0.59\nBERT Multilingual 0.67 0.00 0.00 | 0.00 0.64 0.67 0.26 0.46\nTable 4: Precision\nClassifier Positive | Negative | Neutral | Mixed | Other language | Micro Avg | Macro Avg | Weighted Avg\nKNN 0.63 0.04 0.10] 0.02 0.61 0.45 0.28 0.45\nDecision Tree 0.83 0.21 0.13 0.12 0.54 0.61 0.36 0.61\nRandom Forest 0.98 0.18 0.09 0.04 0.55 0.68 0.32 0.68\nLogistic Regression 0.98 0.13 0.06 0.01 0.32 0.68 0.30 0.68\nNaive Bayes 1.00 0.01 0.00} 0.01 0.18 0.66 0.24 0.67\nSVM 1.00 0.00 0.00 | 0.00 0.00 0.66 0.20 0.66\n1DConv-LSTM 0.91 0.11 0.00} 0.10 0.28 0.63 0.28 0.63\nDME 0.99 0.03 0.02 | 0.01 0.49 0.67 0.31 0.57\nCDME 0.99 0.01 0.03 0.00 0.52 0.67 0.31 0.67\nBERT Multilingual 0.99 0.00 0.00 | 0.00 0.58 0.67 0.31 0.46\nTable 5: Recall\nClassifier Positive | Negative | Neutral | Mixed | Other language | Micro Avg | Macro Avg | Weighted Avg\nKNN 0.66 0.06 0.15 0.04 0.10 0.45 0.29 0.50\nDecision Tree 0.77 0.24 0.17 0.14 0.54 0.61 0.38 0.58\nRandom Forest 0.81 0.18 0.09 0.04 0.55 0.68 0.42 0.65\nLogistic Regression 0.81 0.21 0.12 0.03 0.45 0.68 0.40 0.64\nNaive Bayes 0.80 0.02 0.00} 0.01 0.29 0.66 0.32 0.63\nSVM 0.79 0.00 0.00 | 0.00 0.00 0.66 0.16 0.52\n1DConv-LSTM 0.80 0.16 0.00) 0.12 0.39 0.63 0.31 0.58\nDME 0.80 0.05 0.04 | 0.01 0.58 0.67 0.37 0.57\nCDME 0.80 0.02 0.05 0.01 0.59 0.67 0.39 0.63\nBERT Multilingual 0.80 0.00 0.00 | 0.00 0.61 0.67 0.28 0.46\n\nTable 6: F-score\n\nrecursively is split again and finally the classifier defines\nsome rules to predict the result. We used it to classify the\nsentiments for baseline. Maximum depth was 800 and min-\nimum sample splits were 5 for DT. The criterion were Gini\nand entropy.\n\n5.1.5. Random Forest (RF):\nIn random forest, the classifier randomly generates trees\n\nwithout defining rules. We evaluate the RF model with\nsame features as in DT.\n\n5.1.6. Multinominal Naive Bayes (MNB):\n\nNaive-Bayes classifier is a probabilistic model, which is de-\nrived from Bayes Theorem that finds the probability of hy-\npothesis activity to the given evidence activity. We evaluate\nthe MNB model with our data using a=1 with TF-IDF vec-\ntors.\n\n5.1.7. 1DConv-LSTM:\n\nThe model we evaluated consists of Embedding layer,\nDropout, 1DConv with activation ReLU, Max-pooling and\nLSTM. The embeddings are randomly initialized.\n\n5.1.8. BERT-Multilingual:\n\nDevlin et al. (2019) introduced a language representation\nmodel which is Bidirectional Encoder Representation from\nTransforms. It is designed to pre-train from unlabelled text\nand can be fine-tuned by adding last layer. BERT has been\nused for many text classification tasks (Tayyar Madabushi\net al., 2019; Ma et al., 2019; Cohan et al., 2019). We ex-\nplore classification of a code-mixed data into their corre-\nsponding sentiment categories.\n\n5.1.9. DME and CDME:\n\nWe also implemented the Dynamic Meta Embedding (Kiela\net al., 2018) to evaluate our model. As a first step, we used\nWord2Vec and FastText to train from our dataset since dy-\n\n206\n", "vlm_text": "The table presents performance metrics for different classifiers used in a text classification task. Each row represents a classifier, while the columns provide performance scores across various sentiment categories and average metrics. Here's a breakdown of the contents:\n\n- **Classifiers**: Includes KNN, Decision Tree, Random Forest, Logistic Regression, Naive Bayes, SVM, 1DConv-LSTM, DME, CDME, and BERT Multilingual.\n  \n- **Sentiment Categories**: For each classifier, scores are listed for the following sentiment categories:\n  - **Positive**\n  - **Negative**\n  - **Neutral**\n  - **Mixed**\n  - **Other language**\n\n- **Average Metrics**: The performance is further averaged across classes with:\n  - **Micro Avg** (Micro Average)\n  - **Macro Avg** (Macro Average)\n  - **Weighted Avg** (Weighted Average)\n\nEach value in the table represents a specific performance measure (likely a measure like precision, recall, or F1-score) for the respective sentiment category or average metric for the corresponding classifier.\nThis table compares different classifiers based on their performance in various sentiment categories, as well as their averages across these categories. The columns are categorized as follows:\n\n1. **Classifier**: Lists the names of different classifiers that were evaluated.\n2. **Positive, Negative, Neutral, Mixed, Other language**: These columns display numerical values indicating the performance of each classifier in identifying each sentiment category or language classification.\n3. **Micro Avg**: Shows the micro-average performance score for each classifier across all categories.\n4. **Macro Avg**: Displays the macro-average performance score, which is the average performance across all categories without considering class imbalance.\n5. **Weighted Avg**: Provides the weighted average performance score, taking into account the number of instances in each class.\n\nEach value in the table represents a performance metric (such as precision, recall, or F1-score) for the respective classifier and sentiment category. The table does not provide specific details about the metrics or the dataset used.\nThe table presents performance metrics for several classifiers used to evaluate text data, possibly in a context related to natural language processing or sentiment analysis. The classifiers listed are:\n\n1. KNN (K-Nearest Neighbors)\n2. Decision Tree\n3. Random Forest\n4. Logistic Regression\n5. Naive Bayes\n6. SVM (Support Vector Machine)\n7. 1DConv-LSTM (1D Convolutional Long Short-Term Memory)\n8. DME (a model not explicitly defined here)\n9. CDME (a model not explicitly defined here)\n10. BERT Multilingual\n\nFor each classifier, the table shows performance scores across different categories:\n\n- Positive\n- Negative\n- Neutral\n- Mixed\n- Other language\n\nAdditionally, the table provides aggregate performance metrics:\n\n- Micro Average (Micro Avg)\n- Macro Average (Macro Avg)\n- Weighted Average (Weighted Avg)\n\nThese scores appear to be typical performance metrics, possibly precision, recall, or F1-score, though the exact metric isn't specified from the table. The values range from 0 to 1, indicating the effectiveness of each classifier in handling the respective category or overall performance measures.\nrecursively is split again and ﬁnally the classiﬁer deﬁnes some rules to predict the result. We used it to classify the sentiments for baseline. Maximum depth was 800 and min- imum sample splits were 5 for DT. The criterion were Gini and entropy. \n5.1.5. Random Forest (RF): \nIn random forest, the classiﬁer randomly generates trees without deﬁning rules. We evaluate the RF model with same features as in DT. \n5.1.6. Multinominal Naive Bayes (MNB): \nNaive-Bayes classiﬁer is a probabilistic model, which is de- rived from Bayes Theorem that ﬁnds the probability of hy- pothesis activity to the given evidence activity. We evaluate the MNB model with our data using  $\\alpha{=}1$   with TF-IDF vec- tors. \n5.1.7. 1DConv-LSTM: \nThe model we evaluated consists of Embedding layer, Dropout, 1DConv with activation ReLU, Max-pooling and LSTM. The embeddings are randomly initialized. \n5.1.8. BERT-Multilingual: \nDevlin et al. (2019) introduced a language representation model which is Bidirectional Encoder Representation from Transforms. It is designed to pre-train from unlabelled text and can be ﬁne-tuned by adding last layer. BERT has been used for many text classiﬁcation tasks (Tayyar Madabushi et al., 2019; Ma et al., 2019; Cohan et al., 2019). We ex- plore classiﬁcation of a code-mixed data into their corre- sponding sentiment categories. \n5.1.9. DME and CDME: \nWe also implemented the Dynamic Meta Embedding (Kiela et al., 2018) to evaluate our model. As a ﬁrst step, we used Word2Vec and FastText to train from our dataset since dy- namic meta-embedding is an effective method for the su- pervised learning of embedding ensembles. "}
{"page": 5, "image_path": "doc_images/2020.sltu-1.28_5.jpg", "ocr_text": "namic meta-embedding is an effective method for the su-\npervised learning of embedding ensembles.\n\n5.2. Experiment Results and Discussion\n\nThe experimental results of the sentiment classification task\nusing different methods are shown in terms of precision in\nTable 4, recall in Table 5, and F-score in Table 6. We used\nsklearn * for evaluation. The micro-average is calculated\nby aggregating the contributions of all classes to compute\nthe average metric. In a multi-class classification setup,\nmicro-average is preferable if there are class imbalances.\nFor instance in our data, we have many more examples\nof positive classes than other classes. A macro-average\nwill compute the metrics (precision, recall, F-score)\nindependently for each class and then take the average.\nThus this metric treats all classes equally and it does not\ntake imbalance into account. A weighted average takes the\nmetrics from each class just like macro but the contribution\nof each class to the average is weighted by the number of\nexamples available for it. For our test, positive is 2,075,\nnegative is 424, neutral is 173, mixed feelings are 377, and\nnon-Tamil is 100.\n\nAs shown in the tables, all the classification algorithms\nperform poorly on the code-mixed dataset. Logistic\nregression, random forest classifiers and decision trees\nwere the ones that fared comparatively better across all\nsentiment classes. Surprisingly, the classification result by\nthe SVM model has much worse diversity than the other\nmethods. Applying deep learning methods also does not\nlead to higher scores on the three automatic metrics. We\nthink this stems from the characteristics of the dataset.\nThe classification scores for different sentiment classes\nappear to be in line with the distribution of sentiments in\nthe dataset.\n\nThe dataset is not a balanced distribution. Table 3 shows\nthat out of total 15,744 sentences 67% belong to Positive\nclass while the other sentiment classes share 13%, 5%\nand 3% respectively. The precision, recall and F-measure\nscores are higher for the Positive class while the scores\nfor Neutral and Mixed feeling classes were disastrous.\nApart from their low distribution in the dataset, these two\nclasses are difficult to annotate for even human annotators\nas discussed in Section 4. In comparison, the Negative\nand Other language classes were better. We suspect this\nis due to more explicit clues for negative and non-Tamil\nwords and due to relatively higher distribution of negative\ncomments in the data.\n\nSince we collected the post from movie trailers, we got\nmore positive sentiment than others as the people who\nwatch trailers are more likely to be interested in movies\nand this skews the overall distribution. However, as the\ncode-mixing phenomenon is not incorporated in the ear-\nlier models, this resource could be taken as a starting point\nfor further research. There is significant room for improve-\nment in code-mixed research with our dataset. In our ex-\nperiments, we only utilized the machine learning methods,\n\n*https://scikit-learn.org/\n\nbut more information such as linguistic information or hier-\narchical meta-embedding can be utilized. This dataset can\nbe used to create a multilingual embedding for code-mixed\ndata (Pratapa et al., 2018b).\n\n6. Conclusion\n\nWe presented, to the best of our knowledge, the most sub-\nstantial corpus for under-resourced code-mixed Tanglish\nwith annotations for sentiment polarity. We achieved a\nhigh inter-annotator agreement in terms of Krippendorff a\nfrom voluntary annotators on contributions collected using\nGoogle form. We created baselines with gold standard an-\nnotated data and presented our results for each class in Pre-\ncision, Recall, and F-Score. We expect this resource will\nenable the researchers to address new and exciting prob-\nlems in code-mixed research.\n\n7. Acknowledgments\n\nThis publication has emanated from research supported\nin part by a research grant from Science Founda-\ntion Ireland (SFI) under Grant Number SFI/12/RC/2289\n(Insight), SFI/12/RC/2289_P2 (Insight_2), co-funded by\nthe European Regional Development Fund as_ well\nas by the EU H2020 programme under grant agree-\nments 731015 (ELEXIS-European Lexical Infrastruc-\nture), 825182 (Prét-a-LLOD), and Irish Research Council\ngrant IRCLA/2017/129 (CARDAMOM-Comparative Deep\nModels of Language for Minority and Historical Lan-\nguages).\n\n8. Bibliographical References\n\nAgrawal, R., Chenthil Kumar, V., Muralidharan, V., and\nSharma, D. (2018). No more beating about the bush :\nA step towards idiom handling for Indian language NLP.\nIn Proceedings of the Eleventh International Conference\non Language Resources and Evaluation (LREC 2018),\nMiyazaki, Japan, May. European Language Resources\nAssociation (ELRA).\n\nGustavo Aguilar, et al., editors. (2018). Proceedings of the\nThird Workshop on Computational Approaches to Lin-\nguistic Code-Switching, Melbourne, Australia, July. As-\nsociation for Computational Linguistics.\n\nAlGhamdi, F., Molina, G., Diab, M., Solorio, T., Hawwari,\nA., Soto, V., and Hirschberg, J. (2016). Part of speech\ntagging for code switched data. In Proceedings of\nthe Second Workshop on Computational Approaches to\nCode Switching, pages 98-107, Austin, Texas, Novem-\nber. Association for Computational Linguistics.\n\nAR., B., Joshi, A., and Bhattacharyya, P. (2012). Cross-\nlingual sentiment analysis for Indian languages using\nlinked WordNets. In Proceedings of COLING 2012:\nPosters, pages 73-82, Mumbai, India, December. The\nCOLING 2012 Organizing Committee.\n\nBarman, U., Das, A., Wagner, J., and Foster, J. (2014).\nCode mixing: A challenge for language identification\nin the language of social media. In Proceedings of the\nFirst Workshop on Computational Approaches to Code\nSwitching, pages 13-23, Doha, Qatar, October. Associa-\ntion for Computational Linguistics.\n\n207\n", "vlm_text": "\n5.2. Experiment Results and Discussion \nThe experimental results of the sentiment classiﬁcation task using different methods are shown in terms of precision in Table 4, recall in Table 5, and F-score in Table 6. We used sklearn   4   for evaluation. The micro-average is calculated by aggregating the contributions of all classes to compute the average metric. In a multi-class classiﬁcation setup, micro-average is preferable if there are class imbalances. For instance in our data, we have many more examples of positive classes than other classes. A macro-average will compute the metrics (precision, recall, F-score) independently for each class and then take the average. Thus this metric treats all classes equally and it does not take imbalance into account. A weighted average takes the metrics from each class just like macro but the contribution of each class to the average is weighted by the number of examples available for it. For our test, positive is 2,075, negative is 424, neutral is 173, mixed feelings are 377, and non-Tamil is 100. \nAs shown in the tables, all the classiﬁcation algorithms perform poorly on the code-mixed dataset. Logistic regression, random forest classiﬁers and decision trees were the ones that fared comparatively better across all sentiment classes. Surprisingly, the classiﬁcation result by the SVM model has much worse diversity than the other methods. Applying deep learning methods also does not lead to higher scores on the three automatic metrics. We think this stems from the characteristics of the dataset. The classiﬁcation scores for different sentiment classes appear to be in line with the distribution of sentiments in the dataset. \nThe dataset is not a balanced distribution. Table 3 shows that out of total 15,744 sentences   $67\\%$   belong to  Positive class while the other sentiment classes share  $13\\%$  ,   $5\\%$  and   $3\\%$   respectively. The precision, recall and F-measure scores are higher for the  Positive  class while the scores for  Neutral  and  Mixed feeling  classes were disastrous. Apart from their low distribution in the dataset, these two classes are difﬁcult to annotate for even human annotators as discussed in Section 4. In comparison, the  Negative and  Other language  classes were better. We suspect this is due to more explicit clues for negative and non-Tamil words and due to relatively higher distribution of negative comments in the data. \nSince we collected the post from movie trailers, we got more positive sentiment than others as the people who watch trailers are more likely to be interested in movies and this skews the overall distribution. However, as the code-mixing phenomenon is not incorporated in the ear- lier models, this resource could be taken as a starting point for further research. There is signiﬁcant room for improve- ment in code-mixed research with our dataset. In our ex- periments, we only utilized the machine learning methods, but more information such as linguistic information or hier- archical meta-embedding can be utilized. This dataset can be used to create a multilingual embedding for code-mixed data (Pratapa et al., 2018b). \n\n6. Conclusion \nWe presented, to the best of our knowledge, the most sub- stantial corpus for under-resourced code-mixed Tanglish with annotations for sentiment polarity. We achieved a high inter-annotator agreement in terms of Krippendorff    $\\alpha$  from voluntary annotators on contributions collected using Google form. We created baselines with gold standard an- notated data and presented our results for each class in Pre- cision, Recall, and F-Score. We expect this resource will enable the researchers to address new and exciting prob- lems in code-mixed research. \n7. Acknowledgments \nThis publication has emanated from research supported in part by a research grant from Science Founda- tion Ireland (SFI) under Grant Number SFI/12/RC/2289 (Insight), SFI/12/RC/2289 P2 (Insight 2), co-funded by the European Regional Development Fund as well as by the EU H2020 programme under grant agree- ments 731015 (ELEXIS-European Lexical Infrastruc- ture), 825182 (Prˆ et-\\` a-LLOD), and Irish Research Council grant IRCLA/2017/129 (CARDAMOM-Comparative Deep Models of Language for Minority and Historical Lan- guages). \n8. Bibliographical References \nAgrawal, R., Chenthil Kumar, V., Muralidharan, V., and Sharma, D. (2018). No more beating about the bush : A step towards idiom handling for Indian language NLP. In  Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018) , Miyazaki, Japan, May. European Language Resources Association (ELRA). Gustavo Aguilar, et al., editors. (2018).  Proceedings of the Third Workshop on Computational Approaches to Lin- guistic Code-Switching , Melbourne, Australia, July. As- sociation for Computational Linguistics. AlGhamdi, F., Molina, G., Diab, M., Solorio, T., Hawwari, A., Soto, V., and Hirschberg, J. (2016). Part of speech tagging for code switched data. In  Proceedings of the Second Workshop on Computational Approaches to Code Switching , pages 98–107, Austin, Texas, Novem- ber. Association for Computational Linguistics. A.R., B., Joshi, A., and Bhattacharyya, P. (2012). Cross- lingual sentiment analysis for Indian languages using linked WordNets. In  Proceedings of COLING 2012: Posters , pages 73–82, Mumbai, India, December. The COLING 2012 Organizing Committee. Barman, U., Das, A., Wagner, J., and Foster, J. (2014). Code mixing: A challenge for language identiﬁcation in the language of social media. In  Proceedings of the First Workshop on Computational Approaches to Code Switching , pages 13–23, Doha, Qatar, October. Associa- tion for Computational Linguistics. "}
{"page": 6, "image_path": "doc_images/2020.sltu-1.28_6.jpg", "ocr_text": "Chakravarthi, B. R., Arcan, M., and McCrae, J. P. (2018).\nImproving wordnets for under-resourced languages us-\ning machine translation. In Proceedings of the 9th\nGlobal WordNet Conference (GWC 2018), page 78.\n\nChakravarthi, B. R., Arcan, M., and McCrae, J. P. (2019a).\nComparison of different orthographies for machine\ntranslation of under-resourced dravidian languages. In\n2nd Conference on Language, Data and Knowledge\n(LDK 2019). Schloss Dagstuhl-Leibniz-Zentrum fuer In-\nformatik.\n\nChakravarthi, B. R., Arcan, M., and McCrae, J. P. (2019b).\nWordNet gloss translation for under-resourced languages\nusing multilingual neural machine translation. In Pro-\nceedings of the Second Workshop on Multilingualism at\nthe Intersection of Knowledge Bases and Machine Trans-\nlation, pages 1-7, Dublin, Ireland, 19 August. European\nAssociation for Machine Translation.\n\nChakravarthi, B. R., Priyadharshini, R., Stearns, B., Jaya-\npal, A., S, S., Arcan, M., Zarrouk, M., and McCrae, J. P.\n(2019c). Multilingual multimodal machine translation\nfor Dravidian languages utilizing phonetic transcription.\nIn Proceedings of the 2nd Workshop on Technologies for\nMT of Low Resource Languages, pages 56-63, Dublin,\nIreland, 20 August. European Association for Machine\nTranslation.\n\nChakravarthi, B. R., Jose, N., Suryawanshi, S., Sherly,\nE., and McCrae, J. P. (2020). A sentiment analy-\nsis dataset for code-mixed Malayalam-English. In Pro-\nceedings of the Ist Joint Workshop of SLTU (Spoken\nLanguage Technologies for Under-resourced languages)\nand CCURL (Collaboration and Computing for Under-\nResourced Languages) (SLTU-CCURL 2020), Marseille,\nFrance, May. European Language Resources Association\n(ELRA).\n\nChanda, A., Das, D., and Mazumdar, C. (2016). Unrav-\neling the English-Bengali code-mixing phenomenon. In\nProceedings of the Second Workshop on Computational\nApproaches to Code Switching, pages 80-89, Austin,\nTexas, November. Association for Computational Lin-\nguistics.\n\nCieliebak, M., Deriu, J. M., Egger, D., and Uzdilli, F.\n(2017). A Twitter corpus and benchmark resources for\nGerman sentiment analysis. In Proceedings of the Fifth\nInternational Workshop on Natural Language Process-\ning for Social Media, pages 45-51, Valencia, Spain,\nApril. Association for Computational Linguistics.\n\nCohan, A., Beltagy, I., King, D., Dalvi, B., and Weld, D.\n(2019). Pretrained language models for sequential sen-\ntence classification. In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference on\nNatural Language Processing (EMNLP-IJCNLP), pages\n3693-3699, Hong Kong, China, November. Association\nfor Computational Linguistics.\n\nDas, A. and Bandyopadhyay, S. (2010). SentiWordNet for\nIndian languages. In Proceedings of the Eighth Work-\nshop on Asian Language Resouces, pages 56-63, Bei-\njing, China, August. Coling 2010 Organizing Commit-\ntee.\n\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K.\n\n(2019). BERT: Pre-training of deep bidirectional trans-\nformers for language understanding. In Proceedings of\nthe 2019 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Human\nLanguage Technologies, Volume I (Long and Short Pa-\npers), pages 4171-4186, Minneapolis, Minnesota, June.\nAssociation for Computational Linguistics.\n\nHu, M. and Liu, B. (2004). Mining and summarizing\ncustomer reviews. In Proceedings of the Tenth ACM\nSIGKDD International Conference on Knowledge Dis-\ncovery and Data Mining, KDD ’04, page 168-177, New\nYork, NY, USA. Association for Computing Machinery.\n\nJiang, Q., Chen, L., Xu, R., Ao, X., and Yang, M. (2019).\nA challenge dataset and effective models for aspect-\nbased sentiment analysis. In Proceedings of the 2019\nConference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Conference on\nNatural Language Processing (EMNLP-IJCNLP), pages\n6279-6284, Hong Kong, China, November. Association\nfor Computational Linguistics.\n\nJose, N., Chakravarthi, B. R., Suryawanshi, S., Sherly, E.,\nand McCrae, J. P. (2020). A survey of current datasets\nfor code-switching research. In 2020 6th International\nConference on Advanced Computing & Communication\nSystems (ICACCS).\n\nJoshi, A., Prabhu, A., Shrivastava, M., and Varma, V.\n(2016). Towards sub-word level compositions for senti-\nment analysis of Hindi-English code mixed text. In Pro-\nceedings of COLING 2016, the 26th International Con-\nference on Computational Linguistics: Technical Papers,\npages 2482-2491, Osaka, Japan, December. The COL-\nING 2016 Organizing Committee.\n\nKannan, A., Mohanty, G., and Mamidi, R. (2016). To-\nwards building a SentiWordNet for Tamil. In Proceed-\nings of the 13th International Conference on Natural\nLanguage Processing, pages 30-35, Varanasi, India, De-\ncember. NLP Association of India.\n\nKiela, D., Wang, C., and Cho, K. (2018). Dynamic meta-\nembeddings for improved sentence representations. In\nProceedings of the 2018 Conference on Empirical Meth-\nods in Natural Language Processing, pages 1466-1477,\nBrussels, Belgium, October-November. Association for\nComputational Linguistics.\n\nKrippendorff, K. (1970). Estimating the reliability, sys-\ntematic error and random error of interval data. Educa-\ntional and Psychological Measurement, 30(1):61-70.\n\nKrishnasamy, K. (2015). Code mixing among Tamil-\nEnglish bilingual children. International Journal of So-\ncial Science and Humanity, 5(9):788.\n\nLee, S. and Wang, Z. (2015). Emotion in code-switching\ntexts: Corpus construction and analysis. In Proceedings\nof the Eighth SIGHAN Workshop on Chinese Language\nProcessing, pages 91-99, Beijing, China, July. Associa-\ntion for Computational Linguistics.\n\nMa, X., Xu, P., Wang, Z., Nallapati, R., and Xiang,\nB. (2019). Domain adaptation with BERT-based do-\nmain classification and data selection. In Proceedings\nof the 2nd Workshop on Deep Learning Approaches for\nLow-Resource NLP (DeepLo 2019), pages 76-83, Hong\n\n208\n", "vlm_text": "Chakravarthi, B. R., Arcan, M., and McCrae, J. P. (2018). Improving wordnets for under-resourced languages us- ing machine translation. In  Proceedings of the 9th Global WordNet Conference (GWC 2018) , page 78. Chakravarthi, B. R., Arcan, M., and McCrae, J. P. (2019a). Comparison of different orthographies for machine translation of under-resourced dravidian languages. In 2nd Conference on Language, Data and Knowledge (LDK 2019) . Schloss Dagstuhl-Leibniz-Zentrum fuer In- formatik. Chakravarthi, B. R., Arcan, M., and McCrae, J. P. (2019b). WordNet gloss translation for under-resourced languages using multilingual neural machine translation. In  Pro- ceedings of the Second Workshop on Multilingualism at the Intersection of Knowledge Bases and Machine Trans- lation , pages 1–7, Dublin, Ireland, 19 August. European Association for Machine Translation. Chakravarthi, B. R., Priyadharshini, R., Stearns, B., Jaya- pal, A., S, S., Arcan, M., Zarrouk, M., and McCrae, J. P. (2019c). Multilingual multimodal machine translation for Dravidian languages utilizing phonetic transcription. In  Proceedings of the 2nd Workshop on Technologies for MT of Low Resource Languages , pages 56–63, Dublin, Ireland, 20 August. European Association for Machine Translation. Chakravarthi, B. R., Jose, N., Suryawanshi, S., Sherly, E., and McCrae, J. P. (2020). A sentiment analy- sis dataset for code-mixed Malayalam-English. In  Pro- ceedings of the 1st Joint Workshop of SLTU (Spoken Language Technologies for Under-resourced languages) and CCURL (Collaboration and Computing for Under- Resourced Languages) (SLTU-CCURL 2020) , Marseille, France, May. European Language Resources Association (ELRA). Chanda, A., Das, D., and Mazumdar, C. (2016). Unrav- eling the English-Bengali code-mixing phenomenon. In Proceedings of the Second Workshop on Computational Approaches to Code Switching , pages 80–89, Austin, Texas, November. Association for Computational Lin- guistics. Cieliebak, M., Deriu, J. M., Egger, D., and Uzdilli, F. (2017). A Twitter corpus and benchmark resources for German sentiment analysis. In  Proceedings of the Fifth International Workshop on Natural Language Process- ing for Social Media , pages 45–51, Valencia, Spain, April. Association for Computational Linguistics. Cohan, A., Beltagy, I., King, D., Dalvi, B., and Weld, D. (2019). Pretrained language models for sequential sen- tence classiﬁcation. In  Proceedings of the 2019 Confer- ence on Empirical Methods in Natural Language Pro- cessing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) , pages 3693–3699, Hong Kong, China, November. Association for Computational Linguistics. Das, A. and Bandyopadhyay, S. (2010). SentiWordNet for Indian languages. In  Proceedings of the Eighth Work- shop on Asian Language Resouces , pages 56–63, Bei- jing, China, August. Coling 2010 Organizing Commit- tee. Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. \n(2019). BERT: Pre-training of deep bidirectional trans- formers for language understanding. In  Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Pa- pers) , pages 4171–4186, Minneapolis, Minnesota, June. Association for Computational Linguistics. Hu, M. and Liu, B. (2004). Mining and summarizing customer reviews. In  Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Dis- covery and Data Mining , KDD  $^{'}04$  , page 168–177, New York, NY, USA. Association for Computing Machinery. Jiang, Q., Chen, L., Xu, R., Ao, X., and Yang, M. (2019). A challenge dataset and effective models for aspect- based sentiment analysis. In  Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) , pages 6279–6284, Hong Kong, China, November. Association for Computational Linguistics. Jose, N., Chakravarthi, B. R., Suryawanshi, S., Sherly, E., and McCrae, J. P. (2020). A survey of current datasets for code-switching research. In  2020 6th International Conference on Advanced Computing & Communication Systems (ICACCS) . Joshi, A., Prabhu, A., Shrivastava, M., and Varma, V. (2016). Towards sub-word level compositions for senti- ment analysis of Hindi-English code mixed text. In  Pro- ceedings of COLING 2016, the 26th International Con- ference on Computational Linguistics: Technical Papers , pages 2482–2491, Osaka, Japan, December. The COL- ING 2016 Organizing Committee. Kannan, A., Mohanty, G., and Mamidi, R. (2016). To- wards building a SentiWordNet for Tamil. In  Proceed- ings of the 13th International Conference on Natural Language Processing , pages 30–35, Varanasi, India, De- cember. NLP Association of India. Kiela, D., Wang, C., and Cho, K. (2018). Dynamic meta- embeddings for improved sentence representations. In Proceedings of the 2018 Conference on Empirical Meth- ods in Natural Language Processing , pages 1466–1477, Brussels, Belgium, October-November. Association for Computational Linguistics. Krippendorff, K. (1970). Estimating the reliability, sys- tematic error and random error of interval data.  Educa- tional and Psychological Measurement , 30(1):61–70. Krishnasamy, K. (2015). Code mixing among Tamil- English bilingual children.  International Journal of So- cial Science and Humanity , 5(9):788. Lee, S. and Wang, Z. (2015). Emotion in code-switching texts: Corpus construction and analysis. In  Proceedings of the Eighth SIGHAN Workshop on Chinese Language Processing , pages 91–99, Beijing, China, July. Associa- tion for Computational Linguistics. Ma, X., Xu, P., Wang, Z., Nallapati, R., and Xiang, B. (2019). Domain adaptation with BERT-based do- main classiﬁcation and data selection. In  Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP (DeepLo 2019) , pages 76–83, Hong "}
{"page": 7, "image_path": "doc_images/2020.sltu-1.28_7.jpg", "ocr_text": "Kong, China, November. Association for Computational\nLinguistics.\n\nMehlum, P., Barnes, J., Ovrelid, L., and Velldal, E. (2019).\nAnnotating evaluative sentences for sentiment analy-\nsis: a dataset for Norwegian. In Proceedings of the\n22nd Nordic Conference on Computational Linguistics,\npages 121-130, Turku, Finland, September—October.\nLinképing University Electronic Press.\n\nMohammad, S. (2016). A practical guide to sentiment an-\nnotation: Challenges and solutions. In Proceedings of\nthe 7th Workshop on Computational Approaches to Sub-\njectivity, Sentiment and Social Media Analysis, pages\n174-179, San Diego, California, June. Association for\nComputational Linguistics.\n\nPadmamala, R. and Prema, V. (2017). Sentiment analysis\nof online Tamil contents using recursive neural network\nmodels approach for Tamil language. In 20/7 IEEE In-\nternational Conference on Smart Technologies and Man-\nagement for Computing, Communication, Controls, En-\nergy and Materials (ICSTM), pages 28-31, Aug.\n\nPatra, B. G., Das, D., and Das, A. (2018). Sentiment anal-\nysis of code-mixed indian languages: An overview of\nsail_code-mixed shared task @ icon-2017. arXiv preprint\narXiv: 1803.06745.\n\nPhani, S., Lahiri, S., and Biswas, A. (2016). Sentiment\nanalysis of Tweets in three Indian languages. In Pro-\nceedings of the 6th Workshop on South and Southeast\nAsian Natural Language Processing (WSSANLP2016),\npages 93-102, Osaka, Japan, December. The COLING\n2016 Organizing Committee.\n\nPrasad, S. S., Kumar, J., Prabhakar, D. K., and Tripathi, S.\n(2016). Sentiment mining: An approach for Bengali and\nTamil tweets. In 20/6 Ninth International Conference\non Contemporary Computing (IC3), pages 1-4, Aug.\n\nPratapa, A., Bhat, G., Choudhury, M., Sitaram, S., Danda-\npat, S., and Bali, K. (2018a). Language modeling for\ncode-mixing: The role of linguistic theory based syn-\nthetic data. In Proceedings of the 56th Annual Meeting of\nthe Association for Computational Linguistics (Volume\n1: Long Papers), pages 1543-1553, Melbourne, Aus-\ntralia, July. Association for Computational Linguistics.\n\nPratapa, A., Choudhury, M., and Sitaram, S. (2018b).\nWord embeddings for code-mixed language processing.\nIn Proceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing, pages 3067-\n3072, Brussels, Belgium, October-November. Associa-\ntion for Computational Linguistics.\n\nPriyadharshini, R., Chakravarthi, B. R., Vegupatti, M., and\nMcCrae, J. P. (2020). Named entity recognition for\ncode-mixed Indian corpus using meta embedding. In\n2020 6th International Conference on Advanced Com-\nputing & Communication Systems (ICACCS).\n\nRani, P., Suryawanshi, S., Goswami, K., Chakravarthi,\nB.R., Fransen, T., and McCrae, J. P. (2020). A compar-\native study of different state-of-the-art hate speech de-\ntection methods for Hindi-English code-mixed data. In\nProceedings of the Second Workshop on Trolling, Ag-\ngression and Cyberbullying, Marseille, France, May. Eu-\nropean Language Resources Association (ELRA).\n\nRanjan, P., Raja, B., Priyadharshini, R., and Balabantaray,\nR. C. (2016). A comparative study on code-mixed data\nof Indian social media vs formal text. In 2016 2nd Inter-\nnational Conference on Contemporary Computing and\nInformatics (IC31), pages 608-611.\n\nRemmiya Devi, G., Veena, P., Anand Kumar, M., and So-\nman, K. (2016). Amrita-cen@ fire 2016: Code-mix\nentity extraction for Hindi-English and Tamil-English\ntweets. In CEUR workshop proceedings, volume 1737,\npages 304-308.\n\nRogers, A., Romanov, A., Rumshisky, A., Volkova, S.,\nGronas, M., and Gribov, A. (2018). RuSentiment: An\nenriched sentiment analysis dataset for social media in\nRussian. In Proceedings of the 27th International Con-\nference on Computational Linguistics, pages 755-763,\nSanta Fe, New Mexico, USA, August. Association for\nComputational Linguistics.\n\nSitaram, D., Murthy, S., Ray, D., Sharma, D., and Dhar,\nK. (2015). Sentiment analysis of mixed language em-\nploying hindi-english code switching. In 20/5 Interna-\ntional Conference on Machine Learning and Cybernetics\n(ICMLC), volume 1, pages 271-276, July.\n\nSolorio, T., Blair, E., Maharjan, S., Bethard, S., Diab, M.,\nGhoneim, M., Hawwari, A., AlGhamdi, F., Hirschberg,\nJ., Chang, A., and Fung, P. (2014). Overview for the first\nshared task on language identification in code-switched\ndata. In Proceedings of the First Workshop on Compu-\ntational Approaches to Code Switching, pages 62-72,\nDoha, Qatar, October. Association for Computational\nLinguistics.\n\nSuryawanshi, S., Chakravarthi, B. R., Arcan, M., and\nBuitelaar, P. (2020a). Multimodal meme dataset (Multi-\nOFF) for identifying offensive content in image and text.\nIn Proceedings of the Second Workshop on Trolling, Ag-\ngression and Cyberbullying, Marseille, France, May. Eu-\nropean Language Resources Association (ELRA).\n\nSuryawanshi, S., Chakravarthi, B. R., Verma, P., Arcan, M.,\nMcCrae, J. P., and Buitelaar, P. (2020b). A dataset for\ntroll classification of Tamil memes. In Proceedings of\nthe 5th Workshop on Indian Language Data Resource\nand Evaluation (WILDRE-5), Marseille, France, May.\nEuropean Language Resources Association (ELRA).\n\nTayyar Madabushi, H., Kochkina, E., and Castelle, M.\n(2019). Cost-sensitive BERT for generalisable sentence\nclassification on imbalanced data. In Proceedings of the\nSecond Workshop on Natural Language Processing for\nInternet Freedom: Censorship, Disinformation, and Pro-\npaganda, pages 125-134, Hong Kong, China, Novem-\nber. Association for Computational Linguistics.\n\nVilares, D., Alonso, M. A., and Gdémez-Rodriguez, C.\n(2015). Sentiment analysis on monolingual, multilingual\nand code-switching Twitter corpora. In Proceedings of\nthe 6th Workshop on Computational Approaches to Sub-\njectivity, Sentiment and Social Media Analysis, pages 2—\n8, Lisboa, Portugal, September. Association for Compu-\ntational Linguistics.\n\nVilares, D., Alonso, M. A., and Gomez-Rodriguez, C.\n(2016). En-es-cs: An English-Spanish code-switching\ntwitter corpus for multilingual sentiment analysis. In\nNicoletta Calzolari (Conference Chair), et al., edi-\n\n209\n", "vlm_text": "Kong, China, November. Association for Computational Linguistics. Mæhlum, P., Barnes, J., Øvrelid, L., and Velldal, E. (2019). Annotating evaluative sentences for sentiment analy- sis: a dataset for Norwegian. In  Proceedings of the 22nd Nordic Conference on Computational Linguistics , pages 121–130, Turku, Finland, September–October. Link¨ oping University Electronic Press. Mohammad, S. (2016). A practical guide to sentiment an- notation: Challenges and solutions. In  Proceedings of the 7th Workshop on Computational Approaches to Sub- jectivity, Sentiment and Social Media Analysis , pages 174–179, San Diego, California, June. Association for Computational Linguistics. Padmamala, R. and Prema, V. (2017). Sentiment analysis of online Tamil contents using recursive neural network models approach for Tamil language. In  2017 IEEE In- ternational Conference on Smart Technologies and Man- agement for Computing, Communication, Controls, En- ergy and Materials (ICSTM) , pages 28–31, Aug. Patra, B. G., Das, D., and Das, A. (2018). Sentiment anal- ysis of code-mixed indian languages: An overview of sail code-mixed shared task  $@$   icon-2017.  arXiv preprint arXiv:1803.06745 . Phani, S., Lahiri, S., and Biswas, A. (2016). Sentiment analysis of Tweets in three Indian languages. In  Pro- ceedings of the 6th Workshop on South and Southeast Asian Natural Language Processing (WSSANLP2016) , pages 93–102, Osaka, Japan, December. The COLING 2016 Organizing Committee. Prasad, S. S., Kumar, J., Prabhakar, D. K., and Tripathi, S. (2016). Sentiment mining: An approach for Bengali and Tamil tweets. In  2016 Ninth International Conference on Contemporary Computing (IC3) , pages 1–4, Aug. Pratapa, A., Bhat, G., Choudhury, M., Sitaram, S., Danda- pat, S., and Bali, K. (2018a). Language modeling for code-mixing: The role of linguistic theory based syn- thetic data. In  Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 1543–1553, Melbourne, Aus- tralia, July. Association for Computational Linguistics. Pratapa, A., Choudhury, M., and Sitaram, S. (2018b). Word embeddings for code-mixed language processing. In  Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing , pages 3067– 3072, Brussels, Belgium, October-November. Associa- tion for Computational Linguistics. Priyadharshini, R., Chakravarthi, B. R., Vegupatti, M., and McCrae, J. P. (2020). Named entity recognition for code-mixed Indian corpus using meta embedding. In 2020 6th International Conference on Advanced Com- puting & Communication Systems (ICACCS) . Rani, P., Suryawanshi, S., Goswami, K., Chakravarthi, B. R., Fransen, T., and McCrae, J. P. (2020). A compar- ative study of different state-of-the-art hate speech de- tection methods for Hindi-English code-mixed data. In Proceedings of the Second Workshop on Trolling, Ag- gression and Cyberbullying , Marseille, France, May. Eu- ropean Language Resources Association (ELRA). \nRanjan, P., Raja, B., Priyadharshini, R., and Balabantaray, R. C. (2016). A comparative study on code-mixed data of Indian social media vs formal text. In  2016 2nd Inter- national Conference on Contemporary Computing and Informatics (IC3I) , pages 608–611. Remmiya Devi, G., Veena, P., Anand Kumar, M., and So- man, K. (2016). Amrita-cen  $@$   ﬁre 2016: Code-mix entity extraction for Hindi-English and Tamil-English tweets. In  CEUR workshop proceedings , volume 1737, pages 304–308. Rogers, A., Romanov, A., Rumshisky, A., Volkova, S., Gronas, M., and Gribov, A. (2018). RuSentiment: An enriched sentiment analysis dataset for social media in Russian. In  Proceedings of the 27th International Con- ference on Computational Linguistics , pages 755–763, Santa Fe, New Mexico, USA, August. Association for Computational Linguistics. Sitaram, D., Murthy, S., Ray, D., Sharma, D., and Dhar, K. (2015). Sentiment analysis of mixed language em- ploying hindi-english code switching. In  2015 Interna- tional Conference on Machine Learning and Cybernetics (ICMLC) , volume 1, pages 271–276, July. Solorio, T., Blair, E., Maharjan, S., Bethard, S., Diab, M., Ghoneim, M., Hawwari, A., AlGhamdi, F., Hirschberg, J., Chang, A., and Fung, P. (2014). Overview for the ﬁrst shared task on language identiﬁcation in code-switched data. In  Proceedings of the First Workshop on Compu- tational Approaches to Code Switching , pages 62–72, Doha, Qatar, October. Association for Computational Linguistics. Suryawanshi, S., Chakravarthi, B. R., Arcan, M., and Buitelaar, P. (2020a). Multimodal meme dataset (Multi- OFF) for identifying offensive content in image and text. In  Proceedings of the Second Workshop on Trolling, Ag- gression and Cyberbullying , Marseille, France, May. Eu- ropean Language Resources Association (ELRA). Suryawanshi, S., Chakravarthi, B. R., Verma, P., Arcan, M., McCrae, J. P., and Buitelaar, P. (2020b). A dataset for troll classiﬁcation of Tamil memes. In  Proceedings of the 5th Workshop on Indian Language Data Resource and Evaluation (WILDRE-5) , Marseille, France, May. European Language Resources Association (ELRA). Tayyar Madabushi, H., Kochkina, E., and Castelle, M. (2019). Cost-sensitive BERT for generalisable sentence classiﬁcation on imbalanced data. In  Proceedings of the Second Workshop on Natural Language Processing for Internet Freedom: Censorship, Disinformation, and Pro- paganda , pages 125–134, Hong Kong, China, Novem- ber. Association for Computational Linguistics. Vilares, D., Alonso, M. A., and G´ omez-Rodr´ ıguez, C. (2015). Sentiment analysis on monolingual, multilingual and code-switching Twitter corpora. In  Proceedings of the 6th Workshop on Computational Approaches to Sub- jectivity, Sentiment and Social Media Analysis , pages 2– 8, Lisboa, Portugal, September. Association for Compu- tational Linguistics. Vilares, D., Alonso, M. A., and G´ omez-Rodr´ ıguez, C. (2016). En-es-cs: An English-Spanish code-switching twitter corpus for multilingual sentiment analysis. In Nicoletta Calzolari (Conference Chair), et al., edi- "}
{"page": 8, "image_path": "doc_images/2020.sltu-1.28_8.jpg", "ocr_text": "tors, Proceedings of the Tenth International Conference\non Language Resources and Evaluation (LREC 2016),\nParis, France, may. European Language Resources As-\nsociation (ELRA).\n\nWiebe, J., Wilson, T., and Cardie, C. (2005). Annotating\nexpressions of opinions and emotions in language. Lan-\nguage Resources and Evaluation, 39(2):165—210, May.\n\nWinata, G. I., Lin, Z., and Fung, P. (2019a). Learning mul-\ntilingual meta-embeddings for code-switching named\nentity recognition. In Proceedings of the 4th Workshop\non Representation Learning for NLP (RepL4NLP-2019),\npages 181-186, Florence, Italy, August. Association for\nComputational Linguistics.\n\nWinata, G. L, Lin, Z., Shin, J., Liu, Z., and Fung,\nP. (2019b). Hierarchical meta-embeddings for code-\nswitching named entity recognition. In Proceedings\nof the 2019 Conference on Empirical Methods in\nNatural Language Processing and the 9th Interna-\ntional Joint Conference on Natural Language Process-\ning (EMNLP-IJCNLP), pages 3532-3538, Hong Kong,\nChina, November. Association for Computational Lin-\nguistics.\n\nYang, Y. and Eisenstein, J. (2017). Overcoming lan-\nguage variation in sentiment analysis with social atten-\ntion. Transactions of the Association for Computational\nLinguistics, 5:295-307.\n\n210\n", "vlm_text": "tors,  Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC 2016) , Paris, France, may. European Language Resources As- sociation (ELRA). Wiebe, J., Wilson, T., and Cardie, C. (2005). Annotating expressions of opinions and emotions in language.  Lan- guage Resources and Evaluation , 39(2):165–210, May. Winata, G. I., Lin, Z., and Fung, P. (2019a). Learning mul- tilingual meta-embeddings for code-switching named entity recognition. In  Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019) , pages 181–186, Florence, Italy, August. Association for Computational Linguistics. Winata, G. I., Lin, Z., Shin, J., Liu, Z., and Fung, P. (2019b). Hierarchical meta-embeddings for code- switching named entity recognition. In  Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th Interna- tional Joint Conference on Natural Language Process- ing (EMNLP-IJCNLP) , pages 3532–3538, Hong Kong, China, November. Association for Computational Lin- guistics. Yang, Y. and Eisenstein, J. (2017). Overcoming lan- guage variation in sentiment analysis with social atten- tion.  Transactions of the Association for Computational Linguistics , 5:295–307. "}
