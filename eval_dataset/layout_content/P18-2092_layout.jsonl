{"layout": 0, "type": "text", "text": "Exploiting Document Knowledge for Aspect-level Sentiment Classiﬁcation ", "text_level": 1, "page_idx": 0, "bbox": [172, 66, 424, 101], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 1, "type": "text", "text": "Ruidan   $\\mathbf{H}\\mathbf{e}^{\\dag\\ddag}$  , Wee Sun Lee † , Hwee Tou  $\\mathbf{N}\\mathbf{g}^{\\dagger}$  ,  and  Daniel Dahlmeier ‡ † Department of Computer Science, National University of Singapore ‡ SAP Innovation Center Singapore ", "page_idx": 0, "bbox": [127.58200073242188, 111, 472.4498596191406, 154.94039916992188], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 2, "type": "text", "text": "† { ruidanhe,leews,nght } @comp.nus.edu.sg ‡ d.dahlmeier@sap.com ", "page_idx": 0, "bbox": [166.6279296875, 155.09225463867188, 433.9039306640625, 182.96942138671875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 3, "type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0, "bbox": [158, 205, 204, 218], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 4, "type": "text", "text": "Attention-based long short-term memory (LSTM) networks have proven to be use- ful in aspect-level sentiment classiﬁca- tion. However, due to the difﬁculties in annotating aspect-level data, existing public datasets for this task are all rela- tively small, which largely limits the ef- fectiveness of those neural models. In this paper, we explore two approaches that transfer knowledge from document- level data, which is much less expensive to obtain, to improve the performance of aspect-level sentiment classiﬁcation. We demonstrate the effectiveness of our ap- proaches on 4 public datasets from Se- mEval 2014, 2015, and 2016, and we show that attention-based LSTM beneﬁts from document-level knowledge in multi- ple ways. ", "page_idx": 0, "bbox": [89, 224.96693420410156, 273, 481.9975280761719], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 5, "type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0, "bbox": [71, 488, 156, 501], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 6, "type": "text", "text": "Given a sentence and an opinion target (also called an aspect term) occurring in the sentence, aspect- level sentiment classiﬁcation aims to determine the sentiment polarity in the sentence towards the opinion target. An opinion target or target for short refers to a word or a phrase describing an aspect of an entity. For example, in the sentence “ This little place has a cute interior decor but the prices are quite expensive ”, the targets are  interior decor  and prices , and they are associated with positive and negative sentiment respectively. ", "page_idx": 0, "bbox": [71, 509.00006103515625, 290, 657.637451171875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 7, "type": "text", "text": "A sentence may contain multiple sentiment- target pairs, thus one challenge is to separate different opinion contexts for different targets. For this purpose, state-of-the-art neural meth- ods ( Wang et al. ,  2016 ;  Liu and Zhang ,  2017 ;  Chen et al. ,  2017 ) adopt attention-based LSTM net- works, where the LSTM aims to capture sequen- tial patterns and the attention mechanism aims to emphasize target-speciﬁc contexts for encod- ing sentence representations. Typically, LSTMs only show their potential when trained on large datasets. However, aspect-level training data re- quires the annotation of all opinion targets in a sentence, which is costly to obtain in practice. As such, existing public aspect-level datasets are all relatively small. Insufﬁcient training data limits the effectiveness of neural models. ", "page_idx": 0, "bbox": [71, 658.041015625, 290, 766.031494140625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 8, "type": "text", "text": "", "page_idx": 0, "bbox": [307, 205.4190216064453, 525, 326.9585266113281], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 9, "type": "text", "text": "Despite the lack of aspect-level labeled data, enormous document-level labeled data are eas- ily accessible online such as Amazon reviews. These reviews contain substantial linguistic pat- terns and come with sentiment labels naturally. In this paper, we hypothesize that aspect-level sentiment classiﬁcation can be improved by em- ploying knowledge gained from document-level sentiment classiﬁcation. Speciﬁcally, we ex- plore two transfer methods to incorporate this sort of knowledge – pretraining and multi-task learning. In our experiments, we ﬁnd that both methods are helpful and combining them achieves signiﬁcant improvements over attention- based LSTM models trained only on aspect-level data. We also illustrate by examples that ad- ditional knowledge from document-level data is beneﬁcial in multiple ways. Our source code can be obtained from  https://github.com/ ruidan/Aspect-level-sentiment . ", "page_idx": 0, "bbox": [307, 327.57403564453125, 525, 598.1536254882812], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 10, "type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 0, "bbox": [306, 608, 397, 622], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 11, "type": "text", "text": "Various neural models ( Dong et al. ,  2014 ;  Nguyen and Shirai ,  2015 ;  Vo and Zhang ,  2015 ;  Tang et al. , 2016a , b ;  Wang et al. ,  2016 ;  Zhang et al. ,  2016 ; Liu and Zhang ,  2017 ;  Chen et al. ,  2017 ) have been proposed for aspect-level sentiment classiﬁcation. The main idea behind these works is to develop neural architectures that are able to learn continu- ous features and capture the intricate relation be- tween a target and context words. However, to sufﬁciently train these models, substantial aspect- level annotated data is required, which is expen- sive to obtain in practice. ", "page_idx": 0, "bbox": [307, 630.9431762695312, 525, 766.0316162109375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 12, "type": "text", "text": "", "page_idx": 1, "bbox": [71, 63.68701934814453, 291, 90.38247680664062], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 13, "type": "text", "text": "We explore both pretraining and multi-task learning for transferring knowledge from docu- ment level to aspect level. Both methods are widely studied in the literature. Pretraining is a common technique used in computer vision where low-level neural layers can be usefully transferred to different tasks ( Krizhevsky and Sutskever , 2012 ;  Zeiler and Fergus ,  2014 ). In natural lan- guage processing (NLP), some efforts have been initiated on pretraining LSTMs ( Dai and Le ,  2015 ; Zoph et al. ,  2016 ;  Ramachandran et al. ,  2017 ) for sequence-to-sequence models in both super- vised and unsupervised settings, where promising results have been obtained. On the other hand, multi-task learning simultaneously trains on sam- ples in multiple tasks with a combined objec- tive ( Collobert and Weston ,  2008 ;  Luong et al. , 2015a ;  Liu et al. ,  2016 ), which has improved model generalization ability in certain cases. In the work of Mou et al. ( 2016 ), the authors investi- gated the transferability of neural models in NLP applications with extensive experiments, showing that transferability largely depends on the seman- tic relatedness of the source and target tasks. For our problem, we hypothesize that aspect-level sen- timent classiﬁcation can be improved by employ- ing knowledge gained from document-level senti- ment classiﬁcation, as these two tasks are highly related semantically. ", "page_idx": 1, "bbox": [71, 91.24298858642578, 291, 483.7666320800781], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 14, "type": "text", "text": "3 Models ", "text_level": 1, "page_idx": 1, "bbox": [71, 495, 128, 508], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 15, "type": "text", "text": "3.1 Attention-based LSTM ", "text_level": 1, "page_idx": 1, "bbox": [71, 518, 205, 530], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 16, "type": "text", "text": "We ﬁrst describe a conventional implementation of an attention-based LSTM model for this task. We use it as a baseline model and extend it with pretraining and multi-task learning approaches for incorporating document-level knowledge. ", "page_idx": 1, "bbox": [71, 536.4021606445312, 291, 603.74462890625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 17, "type": "text", "text": "The inputs are a sentence    $s=(w_{1},w_{2},...,w_{n})$  consisting of    $n$   words, and an opinion target    $x=$   $(x_{1},x_{2},...,x_{m})$   occurring in the sentence consist- ing of a subsequence of    $m$   words from    $s$  . Each word is associated with a continuous word embed- ding  $\\mathbf{e}_{w}$   ( Mikolov et al. ,  2013 ) from an embedding mat x    $\\mathbf{E}\\in\\mathbb{R}^{V\\times d}$  , where  $V$   is the vocabulary size and  d  is the embedding dimension. ", "page_idx": 1, "bbox": [71, 604, 291, 712.5955810546875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 18, "type": "text", "text": "LSTM is used to capture sequential informa- tion, and outputs a sequence of hidden vectors: ", "page_idx": 1, "bbox": [71, 713.4561767578125, 291, 740.150634765625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 19, "type": "equation", "text": "\n$$\n[\\mathbf{h}_{1},...,\\mathbf{h}_{n}]=\\mathrm{{LSTM}}([\\mathbf{e}_{w_{1}},...,\\mathbf{e}_{w_{n}}],\\theta_{l s t m})\n$$\n ", "text_format": "latex", "page_idx": 1, "bbox": [79, 751, 270, 767], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 20, "type": "text", "text": "An attention layer assigns a weight    $\\alpha_{i}$   to each word in the sentence. The ﬁnal target-speciﬁc rep- resentation of the sentence    $s$   is then given by: ", "page_idx": 1, "bbox": [306, 63.68720245361328, 526, 103.93167114257812], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 21, "type": "equation", "text": "\n$$\n\\mathbf{z}=\\sum_{i=1}^{n}\\alpha_{i}\\mathbf{h}_{i}\n$$\n ", "text_format": "latex", "page_idx": 1, "bbox": [385, 110, 447, 146], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 22, "type": "text", "text": "And    $\\alpha_{i}$   is computed as follows: ", "page_idx": 1, "bbox": [306, 153.67420959472656, 444.9341735839844, 168.45565795898438], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 23, "type": "equation", "text": "\n$$\n\\begin{array}{r l}&{\\boldsymbol{\\alpha}_{i}=\\frac{\\exp(\\boldsymbol{\\beta}_{i})}{\\sum_{j=1}^{n}\\exp(\\boldsymbol{\\beta}_{j})}}\\\\ &{\\boldsymbol{\\beta}_{i}=f_{s c o r e}(\\mathbf{h}_{i},\\mathbf{t})=t a n h(\\mathbf{h}_{i}^{T}\\mathbf{W}_{a}\\mathbf{t})}\\\\ &{\\mathbf{t}=\\cfrac{1}{m}\\sum_{i=1}^{m}\\mathbf{e}_{x_{i}}}\\end{array}\n$$\n ", "text_format": "latex", "page_idx": 1, "bbox": [334, 173, 498, 260], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 24, "type": "text", "text": "where    $\\mathbf{t}$   is the target representation computed as the averaged word embedding of the target.    $f_{s}$  score is a content-based function that captures the se- mantic association between a word and the target, for which we adopt the formulation used in ( Lu- ong et al. ,  2015b ;  He et al. ,  2017 ) with parameter matrix  $\\mathbf{W}_{a}\\in\\mathbb{R}^{d\\times d}$  . ", "page_idx": 1, "bbox": [306, 266.367919921875, 526, 360.8084411621094], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 25, "type": "text", "text": "The sentence representation    $\\mathbf{z}$   is fed into an out- put layer to predict the probability distribution    $\\mathbf{p}$  over sentiment labels on the target: ", "page_idx": 1, "bbox": [306, 361.21197509765625, 526, 401.4564514160156], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 26, "type": "equation", "text": "\n$$\n\\mathbf{p}=s o f t m a x(\\mathbf{W}_{o}\\mathbf{z}+\\mathbf{b}_{o})\n$$\n ", "text_format": "latex", "page_idx": 1, "bbox": [353, 410, 478, 425], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 27, "type": "text", "text": "We refer to this baseline model as LSTM+ATT. It is trained via cross entropy minimization: ", "page_idx": 1, "bbox": [306, 434.1659851074219, 526, 460.8604736328125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 28, "type": "equation", "text": "\n$$\nJ=-\\sum_{i\\in D}\\log\\mathbf{p}_{i}(c_{i})\n$$\n ", "text_format": "latex", "page_idx": 1, "bbox": [368, 467, 464, 497], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 29, "type": "text", "text": "where  $D$   denotes the overall training corpus,  $c_{i}$   de- notes the true label for sample    $i$  , and    $\\mathbf{p}_{i}(c_{i})$   de- notes the probability of the true label. ", "page_idx": 1, "bbox": [306, 504.593994140625, 526, 544.8384399414062], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 30, "type": "text", "text": "3.2 Transfer Approaches ", "text_level": 1, "page_idx": 1, "bbox": [306, 554, 431, 567], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 31, "type": "text", "text": "LSTM  $^+$  ATT is used as our aspect-level model with parameter set  $\\begin{array}{r l r}{\\theta_{a s p e c t}\\ }&{{}}&{=}\\end{array}$   $\\{\\mathbf{E},\\theta_{l s t m},\\mathbf{W}_{a},\\mathbf{W}_{o},\\mathbf{b}_{o}\\}$  . We also build a standard LSTM-based classiﬁer based on document-level training examples. This network is the same as the   $\\mathrm{LSTM+ATT}$   apart from the lack of the attention layer. The training ob- jective is also cross entropy minimization as shown in equation ( 7 ) and the parameter set is  $\\theta_{d o c}=\\{\\mathbf{E}^{\\prime},\\theta_{l s t m}^{\\prime},\\mathbf{W}_{o}^{\\prime},\\mathbf{b}_{o}^{\\prime}\\}$  } . ", "page_idx": 1, "bbox": [306, 571.2949829101562, 526, 713.7907104492188], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 32, "type": "text", "text": "Pretraining  (PRET): In this setting, we ﬁrst train on document-level examples. The last hidden vec- tor from the LSTM outputs is used as the doc- ument representation. We initialize the relevant ", "page_idx": 1, "bbox": [306, 711.8452758789062, 526, 766.0314331054688], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 33, "type": "table", "page_idx": 2, "img_path": "layout_images/P18-2092_0.jpg", "table_caption": "Table 1: Dataset description. ", "bbox": [106, 62, 258, 170], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "Dataset\n\nPos Neg Neu\n\nDI Restaurantl4-Train 2164 807 637\nRestaurant 14-Test 728 196 196\n\nD2 Laptop 14-Train 994 870 464\nLaptop14-Test 341 128 169\n\nD3 Restaurant!5-Train 1178 382 S50\nRestaurant 15-Test 43932835\n\nD4 Restaurantl6-Train 1620 709 88\nRestaurant 16-Test 597 19038\n\n", "vlm_text": "The table contains information about datasets labeled D1 to D4, indicating the number of positive (Pos), negative (Neg), and neutral (Neu) entries for both training and test sets. Here’s the breakdown:\n\n- **D1:**\n  - Restaurant14-Train: 2164 Pos, 807 Neg, 637 Neu\n  - Restaurant14-Test: 728 Pos, 196 Neg, 196 Neu\n\n- **D2:**\n  - Laptop14-Train: 994 Pos, 870 Neg, 464 Neu\n  - Laptop14-Test: 341 Pos, 128 Neg, 169 Neu\n\n- **D3:**\n  - Restaurant15-Train: 1178 Pos, 382 Neg, 50 Neu\n  - Restaurant15-Test: 439 Pos, 328 Neg, 35 Neu\n\n- **D4:**\n  - Restaurant16-Train: 1620 Pos, 709 Neg, 88 Neu\n  - Restaurant16-Test: 597 Pos, 190 Neg, 38 Neu"}
{"layout": 34, "type": "text", "text": "parameters    $\\mathbf{E},\\theta_{l s t m},\\mathbf{W}_{o},\\mathbf{b}_{o}$   of  $\\mathrm{LSTM+ATT}$   with the pretrained weights, and train it on aspect-level examples to ﬁne tune those weights and learn    ${\\bf W}_{a}$  which is randomly initialized. ", "page_idx": 2, "bbox": [71, 190.62904357910156, 290, 244.42257690429688], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 35, "type": "text", "text": "Multi-task Learning  (MULT): This approach si- multaneously trains two tasks – document-level and aspect-level classiﬁcation. In this setting, the embedding layer   $\\mathbf{\\Psi}(\\mathbf{E})$   and the LSTM layer   $(\\theta_{l s t m})$  are shared by both tasks, and a document is rep- resented as the mean vector over LSTM outputs. The other parameters are task-speciﬁc. The over- all loss function is then given by: ", "page_idx": 2, "bbox": [71, 249.4023895263672, 290, 357.7846374511719], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 36, "type": "equation", "text": "\n$$\nL=J+\\lambda U\n$$\n ", "text_format": "latex", "page_idx": 2, "bbox": [150, 365, 211, 378], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 37, "type": "text", "text": "where    $U$   is the loss function of document-level classiﬁcation.    $\\lambda\\in(0,1)$   is a hyperparameter that controls the weight of  U . ", "page_idx": 2, "bbox": [71, 387.6041564941406, 290, 427.84765625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 38, "type": "text", "text": "Combined  (PRET  $^+$  MULT): In this setting, we ﬁrst perform PRET on document-level exam- ples. We use the pretrained weights for parame- ter initialization for both aspect-level model and document-level model, and then perform MULT as discussed above. ", "page_idx": 2, "bbox": [71, 432.82745361328125, 290, 514.1116943359375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 39, "type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 2, "bbox": [72, 523, 155, 537], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 40, "type": "text", "text": "4.1 Datasets and Experimental Settings ", "text_level": 1, "page_idx": 2, "bbox": [71, 545, 263, 557], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 41, "type": "text", "text": "We run experiments on four benchmark aspect- level datasets, taken from SemEval 2014 ( Pontiki et al. ,  2014 ), SemEval 2015 ( Pontiki et al. ,  2015 ), and SemEval 2016 ( Pontiki et al. ,  2016 ). Fol- lowing previous work ( Tang et al. ,  2016b ;  Wang et al. ,  2016 ), we remove samples with  conﬂicting polarities in all datasets 1 . Statistics of the resulting datasets are presented in Table  1 . ", "page_idx": 2, "bbox": [71, 562.1282348632812, 290, 670.11767578125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 42, "type": "text", "text": "We derived two document-level datasets from Yelp2014 ( Tang et al. ,  2015 ) and the Amazon Electronics dataset ( McAuley et al. ,  2015 ) respec- tively. The original reviews were rated on a 5- point scale. We consider 3-class classiﬁcation and thus label reviews with rating  $<3,>3$  , and  $=3$   as negative, positive, and neutral respectively. Each sampled dataset contains   $30\\mathbf{k}$   instances with ex- actly balanced class labels. We pair up an aspect- level dataset and a document-level dataset when they are from a similar domain – the Yelp dataset is used by D1, D3, and D4 for PRET and MULT, and the Electronics dataset is only used by D2. ", "page_idx": 2, "bbox": [71, 670.521240234375, 290, 737.8636474609375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 43, "type": "text", "text": "", "page_idx": 2, "bbox": [306, 63.68701934814453, 525, 171.67752075195312], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 44, "type": "text", "text": "In all experiments, 300-dimension GloVe vec- tors ( Pennington et al. ,  2014 ) are used to initialize  $\\mathbf{E}$   and    $\\mathbf{E}^{\\prime}$    when pretraining is not conducted for weight initialization. These vectors are also used for initializing    $\\mathbf{E}^{\\prime}$    in the pretraining phase. Val- ues for hyperparameters are obtained from experi- ments on development sets. We randomly sample  $20\\%$   of the original training data from the aspect- level dataset as the development set and only use the remaining   $80\\%$   for training. For all experi- ments, the dimension of LSTM hidden vectors is set to 300,    $\\lambda$   is set to 0.1, and we use dropout with probability 0.5 on sentence/document representa- tions before the output layer. We use RMSProp as the optimizer with the decay rate set to 0.9 and the base learning rate set to 0.001. The mini-batch size is set to 32. ", "page_idx": 2, "bbox": [306, 173.5630645751953, 525, 403.4956359863281], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 45, "type": "text", "text": "4.2 Model Comparison ", "text_level": 1, "page_idx": 2, "bbox": [306, 418, 422, 430], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 46, "type": "text", "text": "Table  2  shows the results of LSTM, LSTM  $+$  ATT, PRET, MULT, PRET  $^+$  MULT, and four representa- tive prior works ( Tang et al. ,  2016a , b ;  Wang et al. , 2016 ;  Chen et al. ,  2017 ). Signiﬁcance tests are conducted for testing the robustness of methods under random parameter initialization. Both accu- racy and macro-F1 are used for evaluation as label distribution is unbalanced. The reported numbers are obtained as the average value over 5 runs with random initialization for each method. ", "page_idx": 2, "bbox": [306, 438.2901611328125, 525, 573.377685546875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 47, "type": "text", "text": "We observe that PRET is very helpful, and con- sistently gives a   $1{-}3\\%$   increase in accuracy over LSTM  $+$  ATT across all datasets. The improve- ments in macro-F1 scores are even more, espe- cially on D3 and D4 where the labels are ex- tremely unbalanced. MULT gives similar perfor- mance as   $\\mathrm{LSTM+ATT}$   on D1 and D2, but im- provements can be clearly observed for D3 and D4. The combination (PRET  $^+$  MULT) overall yields better results. ", "page_idx": 2, "bbox": [306, 575.2642211914062, 525, 710.3526611328125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 48, "type": "text", "text": "There are two main reasons why the improve- ments of macro-F1 scores are more signiﬁcant on D3 and D4 than on D1: (1) D1 has much more neutral examples in the training set. A classiﬁer ", "page_idx": 2, "bbox": [306, 712.2382202148438, 525, 766.0316772460938], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 49, "type": "table", "page_idx": 3, "img_path": "layout_images/P18-2092_1.jpg", "table_caption": "Table 2: Average accuracies and Macro-F1 scores over 5 runs with random initialization. The best results are in bold.   ∗ indicates that PRET  $^+$  MULT is signiﬁcantly better than Tang et al. ( 2016a ), Wang et al. ( 2016 ), Tang et al. ( 2016b ), Chen et al. ( 2017 ), LSTM, and LSTM+ATT with    $p\\,<\\,0.05$   according to one-tailed unpaired t-test. ", "bbox": [70, 62, 527, 228], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "Methods DI D2 D3 D4\nAcc. — Macro-Fl_— Acc. _ Macro-Fl Acc. Macro-F1 Acc. Macro-Fl\n\nTang et al. (2016a) 75.37 64.51 68.25 65.96 76.39 58.70 82.16 54.21\nWang et al. (2016) 78.60 67.02 68.88 63.93 78.48 62.84 83.77 6171\nTang etal. (2016b) —- 76.87 66.40 68.91 62.79 771.89 59.52 83.04 57.91\nChen et al. (2017) 78.48 68.54 72.08 68.43 79.98 60.57 83.88 62.14\nLSTM 75.23 64.21 66.79 64.02 75.28 54.10 81.95 58.11\nLSTM+ATT 76.83 66.48 68.07 64.82 77.38 60.52 82.73 59.12\nOurs: PRET 78.28 68.55 71.32 68.53 80.67 68.31 84.87 70.73\nOurs: MULT 7141 66.68 68.65 64.57 81.05 65.69 83.27 64.56\nOurs: PRET+MULT 79.11 69.73* TAS 67.46 81.30* 68.74* 85.58\" 69.76\"\n", "vlm_text": "The table presents performance metrics for various methods evaluated across four datasets: D1, D2, D3, and D4. For each dataset, accuracy (Acc.) and Macro-F1 scores are reported. The methods compared include:\n\n1. Tang et al. (2016a)\n2. Wang et al. (2016)\n3. Tang et al. (2016b)\n4. Chen et al. (2017)\n5. LSTM\n6. LSTM+ATT\n7. Ours: PRET\n8. Ours: MULT\n9. Ours: PRET+MULT\n\nFor each method and dataset, both accuracy and Macro-F1 scores are provided, with some scores marked with an asterisk (*) to possibly denote they are the best results in that particular column or highlight them for significance. The best results in terms of accuracy and Macro-F1 for each dataset are highlighted in bold. The \"Ours: PRET+MULT\" method frequently outperforms others, particularly in Macro-F1 scores across all datasets, as indicated by the asterisks."}
{"layout": 50, "type": "table", "page_idx": 3, "img_path": "layout_images/P18-2092_2.jpg", "table_caption": "Table 3: PRET with different transferred layers. Averaged results over 5 runs are reported. ", "bbox": [100, 240, 497, 339], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "Settings DI D2 D3 D4\nAcc. — Macro-Fl Acc. — Macro-Fl_— Acc. | Macro-Fl Acc. — Macro-Fl\n\nLSTM only 78.09 67.85 71.04 66.80 78.95 65.30 83.85 67.11\nEmbeddings only TIA 67.19 69.12 65.06 80.13 67.04 84.12 70.11\nOutput layer only 76.88 66.81 69.63 66.07 78.30 64.49 82.55 62.83\nWithout LSTM T1AS 67.25 69.82 66.63 80.27 68.02 84.80 70.27\nWithout embeddings 77.97 67.96 70.59 67.16 79.08 65.56 83.94 68.79\nWithout output layer 78.36 68.06 71.10 67.87 80.82 67.68 84.71 70.48\n\n", "vlm_text": "The table presents the performance of different model settings on four datasets (D1, D2, D3, D4) in terms of accuracy (Acc.) and macro-F1 score (Macro-F1). The settings evaluated are:\n\n1. **LSTM only**: This setting uses only the LSTM component of the model.\n   - D1: 78.09% Acc., 67.85 Macro-F1\n   - D2: 71.04% Acc., 66.80 Macro-F1\n   - D3: 78.95% Acc., 65.30 Macro-F1\n   - D4: 83.85% Acc., 67.11 Macro-F1\n\n2. **Embeddings only**: This setting uses only the embeddings component.\n   - D1: 77.12% Acc., 67.19 Macro-F1\n   - D2: 69.12% Acc., 65.06 Macro-F1\n   - D3: 80.13% Acc., 67.04 Macro-F1\n   - D4: 84.12% Acc., 70.11 Macro-F1\n\n3. **Output layer only**: This setting uses only the output layer component.\n   - D1: 76.88% Acc., 66.81 Macro-F1\n   - D2: 69.63% Acc., 66.07 Macro-F1\n   - D3: 78.30% Acc., 64.49 Macro-F1\n   - D4: 82.55% Acc., 62.83 Macro-F1\n\n4. **Without LSTM**: This setting includes all components except the LSTM.\n   - D1: 77.45% Acc., 67.25 Macro-F1\n   - D2: 69.82% Acc., 66.63 Macro-F1\n   - D3: 80.27% Acc., 68.02 Macro-F1\n   - D4: 84.80% Acc., 70.27 Macro-F1\n\n5. **Without embeddings**: This setting includes all components except the embeddings.\n   - D1: 77.97% Acc., 67.96 Macro-F1\n   - D2: 70.59% Acc., 67.16 Macro-F1\n   - D3: 79.08% Acc., 65.56 Macro-F1\n   - D4: 83.94% Acc., 68.79 Macro-F1\n\n6. **Without output layer**: This setting includes all components except the output layer.\n   - D1: 78.36% Acc., 68.06 Macro-F1\n   - D2: 71.10% Acc., 67.87 Macro-F1\n   - D3: 80."}
{"layout": 51, "type": "text", "text": "without any external knowledge might still be able to learn some neutral-related features on D1 but it is very hard to learn from D3 and D4. (2) The numbers of neutral examples in the test sets of D3 and D4 are very small. Thus, the precision and recall on neutral class will be largely affected by even a small prediction difference (e.g., with 5 more neutral examples correctly identiﬁed, recall is increased by more than   $10\\%$   on both datasets). As a result, the macro-F1 scores on D3 and D4 are affected more. ", "page_idx": 3, "bbox": [72, 361.7740173339844, 290, 510.41156005859375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 52, "type": "text", "text": "4.3 Ablation Tests ", "text_level": 1, "page_idx": 3, "bbox": [72, 528, 163, 540], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 53, "type": "text", "text": "Table  2  indicates that a large percentage of the per- formance gain comes from PRET. To better un- derstand the transfer effects of different layers – embedding layer   $\\mathbf{\\Psi}(\\mathbf{E})$  , LSTM layer   $(\\theta_{l s t m})$  , and output layer   $({\\bf W}_{o},{\\bf b}_{o})-\\mathrm{we}$   conduct ablation tests on PRET with different layers transfered from the document-level model to the aspect-level model. Results are presented in Table  3 . “LSTM only” denotes the setting where only the LSTM layer is transferred, and “Without LSTM” denotes the set- ting where only the embedding and output layers are transferred (excluding the LSTM layer). The key observations are: (1) Transfer is helpful in all settings. Improvements over LSTM  $^+$  ATT are observed even when only one layer is transferred. (2) Overall, transfers of the LSTM and embedding layer are more useful than the output layer. This is what we expect, since the output layer is nor- mally more task-speciﬁc. (3) Transfer of the em- bedding layer is more helpful on D3 and D4. One possible explanation is that the label distribution is extremely unbalanced on these two datasets. Sen- timent information is not adequately captured by GloVe word embeddings. Therefore, with a small number of training examples in the negative and neutral classes, the embeddings trained by aspect- level classiﬁcation still do not effectively capture the true semantics of the relevant opinion words. Transfer of the embedding layer can greatly help in this case. ", "page_idx": 3, "bbox": [72, 549.6480712890625, 290, 766.031494140625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 54, "type": "text", "text": "", "page_idx": 3, "bbox": [307, 361.7740478515625, 525, 551.0595703125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 55, "type": "text", "text": "4.4 Analysis ", "text_level": 1, "page_idx": 3, "bbox": [307, 567, 371, 581], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 56, "type": "text", "text": "To show that aspect-level classiﬁcation indeed beneﬁts from document-level knowledge, we conduct experiments to vary the percentage of document-level training examples from 0.0 to 1.0 for PRET  $+$  MULT. The changes of accuracies and macro-F1 scores on the four datasets are shown in Figure  1 . The improvements on accuracies with increasing number of document examples are sta- ble across all datasets. For macro-F1 scores, the improvements on D1 and D2 are stable. We ob- serve sharp increases in the macro-F1 scores of D3 and D4 when changing the percentage from 0 to 0.4. This may be related to their extremely ", "page_idx": 3, "bbox": [307, 590.2951049804688, 525, 766.031494140625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 57, "type": "image", "page_idx": 4, "img_path": "layout_images/P18-2092_3.jpg", "img_caption": "Figure 1: Results of PRET  $^+$  MULT vs. percentage of document-level training data. ", "bbox": [72, 65, 291, 318], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "Accuracy (%)\n\nMacro-F1 (%)\n\n85 i\n\n80)\n\n75\n\n° ee eee\n=\n\nBot eo os os |\n\neg Lee\n\n0 02 04 06 og 1\n\nPercentage of document-level training examples\n\n70\n\n‘\na\nw\na\nw\n\n0 02 0.4 0.6 08 1\n\nPercentage of document-level training examples\n", "vlm_text": "The image contains two line graphs that illustrate the performance of a method called \"PRET $^+$ MULT\" based on the percentage of document-level training examples. The top graph shows accuracy percentages, while the bottom graph displays Macro-F1 percentages. Both graphs plot the metrics as a function of the percentage of document-level training examples.\n\nThe top graph indicates that as the percentage of document-level training examples increases, the accuracy of all four models (D1, D2, D3, D4) tends to improve. Model D4 consistently achieves the highest accuracy across all percentages.\n\nThe bottom graph reflects a similar trend for Macro-F1 scores, where increasing the percentage of document-level training examples generally leads to better performance across all models. Again, Model D4 typically achieves the best Macro-F1 scores.\n\nEach model (D1, D2, D3, D4) is represented using a different color and marker (D1: blue squares, D2: orange circles, D3: gray diamonds, D4: red crosses)."}
{"layout": 58, "type": "text", "text": "unbalanced label distribution. In such cases, with the knowledge gained from a small number of bal- anced document-level examples, aspect-level pre- dictions on neutral examples can be signiﬁcantly improved. ", "page_idx": 4, "bbox": [72, 342.0699768066406, 290, 409.4124755859375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 59, "type": "text", "text": "To better understand in which conditions the proposed method is helpful, we analyze a sub- set of test examples that are correctly classi- ﬁed by PRET  $^+$  MULT but are misclassiﬁed by LSTM  $\\cdot+$  ATT. We ﬁnd that the beneﬁts brought by document-level knowledge are typically shown in four ways. ", "page_idx": 4, "bbox": [72, 410.53900146484375, 290, 504.9804992675781], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 60, "type": "text", "text": "First of all, to our surprise, LSTM+ATT made obvious mistakes on some instances with common opinion words. Below are two examples where the target is enclosed in [] with its true sentiment indi- cated in the subscript: ", "page_idx": 4, "bbox": [72, 506.10699462890625, 290, 573.449462890625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 61, "type": "text", "text": "1.  “I was highly disappointed in the [food] neg .” 2.  “This particular location certainly uses sub- standard [meats] neg .” ", "page_idx": 4, "bbox": [72, 574.5760498046875, 290, 616.1066284179688], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 62, "type": "text", "text": "In the above examples, LSTM  $^+$  ATT does attend to the right opinion words, but makes the wrong predictions. One possible reason is that the word embeddings from GloVe without PRET do not effectively capture sentiment information, while the aspect-level training samples are not sufﬁcient to capture that for certain words. PRET  $^+$  MULT eliminates this kind of errors. ", "page_idx": 4, "bbox": [72, 616.6700439453125, 290, 724.6604614257812], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 63, "type": "text", "text": "Another ﬁnding is that our method helps to better capture domain-speciﬁc opinion words due to additional knowledge from documents that are ", "page_idx": 4, "bbox": [72, 725.7870483398438, 290, 766.031494140625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 64, "type": "text", "text": "1.  “The smaller [size] pos  was a bonus because of space restrictions.” 2.  “The [price  $\\jmath_{p o s}$   is 200 dollars down.” LSTM  $^+$  ATT attends on  smaller  correctly for the ﬁrst example but makes the wrong prediction as smaller  can be negative in many cases. It does not even capture  down  in the second example. Thirdly, we ﬁnd that LSTM  $^+$  ATT made a num- ber of errors on sentences with negation words: 1.  I have experienced no problems, [works] pos as anticipated. 2.  [Service] neg  not the friendliest to our party! LSTMs typically only show their potential on ", "page_idx": 4, "bbox": [307, 77.23609161376953, 525, 252.97262573242188], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 65, "type": "text", "text": "large datasets. Without sufﬁcient training exam- ples, it may not be able to effectively capture various sequential patterns. Pretraining the net- work on larger document-level corpus addresses this problem. ", "page_idx": 4, "bbox": [307, 253.3761749267578, 525, 320.7186584472656], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 66, "type": "text", "text": "Lastly, PRET  $^+$  MULT makes fewer errors on recognizing neutral instances. This can also be ob- served from the macro-F1 scores in Table  2 . The lack of training examples makes the prediction of neutral instances very difﬁcult for all previous methods. Knowledge from document-level exam- ples with balanced labels compensates for this dis- advantage. ", "page_idx": 4, "bbox": [307, 321.1221923828125, 525, 429.1117248535156], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 67, "type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 4, "bbox": [307, 439, 382, 451], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 68, "type": "text", "text": "The effectiveness of existing aspect-level neural models is limited due to the difﬁculties in obtain- ing training data in practice. Our work is the ﬁrst attempt to incorporate knowledge from document- level corpus for training aspect-level sentiment classiﬁers. We have demonstrated the effective- ness of our proposed approaches and analyzed the major beneﬁts brought by the knowledge transfer. The proposed approaches can be potentially in- tegrated with other aspect-level neural models to further boost their performance. ", "page_idx": 4, "bbox": [307, 458.9392395019531, 525, 607.5767211914062], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 69, "type": "text", "text": "References ", "text_level": 1, "page_idx": 4, "bbox": [307, 630, 363, 642], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 70, "type": "text", "text": "Peng Chen, Zhongqian Sun, Lidong Bing, and Wei Yang. 2017. Recurrent attention network on mem- ory for aspect sentiment analysis. In  Conference on Empirical Methods in Natural Language Processing (EMNLP 2017) . Ronan Collobert and Jason Weston. 2008. A uniﬁed architecture for natural language processing: Deep neural networks with multitask learning. In  Inter- national Conference on Machine Learning (ICML 2008) . ", "page_idx": 4, "bbox": [307, 647.8948364257812, 525, 765.7658081054688], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 71, "type": "text", "text": "Andrew M. Dai and Quoc V. Le. 2015. Semi- supervised sequence learning. In  Neural Informa- tion Processing Systems (NIPS 2015) . Li Dong, Furu Wei, Chuanqi Tan, Duyu Tang, Ming Zhou, and Ke Xu. 2014. Adaptive recursive neural network for target-dependent Twitter sentiment clas- siﬁcation. In  Annual Meeting of the Association for Computational Linguistics (ACL 2014) . Ruidan He, Wee Sun Lee, Hwee Tou  $\\mathrm{Mg}$  , and Daniel Dahlmeier. 2017. An unsupervised neural attention model for aspect extraction. In  Annual Meeting of the Association for Computational Linguistics (ACL 2017) . Alex Krizhevsky and Ilya Sutskever. 2012. Imagenet classiﬁcation with deep convolutional neural net- works. In  Neural Information Processing Systems (NIPS 2012) . Jiangming Liu and Yue Zhang. 2017. Attention model- ing for target sentiment. In  Conference of the Euro- pean Chapter of the Association for Computational Linguistics (EACL 2017) . Yang Liu, Sujian Li, Xiaodong Zhang, and Zhifang Sui. 2016. Implicit discourse relation classiﬁcation via multi-task neural networks. In  AAAI Conference on Artiﬁcial Intelligence (AAAI 2016) . Minh-Tang Luong, Hieu Pham, and Christopher D. Manning. 2015b. Effective approaches to attention- based neural machine translation. In  Annual Meet- ing of the Association for Computational Linguistics (ACL 2015) . Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. 2015a. Multi-task se- quence to sequence learning. In  International Con- ference on Learning Representation (ICLR 2015) . Julian J. McAuley, Christopher Targett, Qinfeng Shi, and Anton van den Hengel. 2015. Image-based rec- ommendations on styles and substitutes. In  The 38th International ACM SIGIR Conference on Research and Development in Information Retrieval . Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor- rado, and Jeffrey Dean. 2013. Distributed repre- sentations of words and phrases and their composi- tionality. In  Neural Information Processing Systems (NIPS 2013) . Lili Mou, Zhao Meng, Rui Yan, Ge Li, Yan Xu, Lu Zhang, and Zhi Jin. 2016. How transferable are neural networks in NLP applications? In  Con- ference on Empirical Methods in Natural Language Processing (EMNLP 2016) . Thien Hai Nguyen and Kiyoaki Shirai. 2015. PhraseRNN: Phrase recursive neural network for aspect-based sentiment analysis. In  Conference on Empirical Methods in Natural Language Processing (EMNLP 2015) . ", "page_idx": 5, "bbox": [71, 64.56158447265625, 290, 765.7650756835938], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 72, "type": "text", "text": "Jeffrey Pennington, Richard Socher, and Christo- pher D Manning. 2014. GloVe: Global vectors for word representation. In  Conference on Empirical Methods in Natural Language Processing (EMNLP 2014) . Maria Pontiki, Dimitrios Galanis, Haris Papageorgiou, Suresh Manandhar, and Ion Androutsopoulos. 2015. SemEval-2015 task 12: Aspect based sentiment analysis. In  International Workshop on Semantic Evaluation (SemEval 2015) . Maria Pontiki, Dimitrios Galanis, John Pavlopou- los, Haris Papageorgiou, Ion Androutsopoulos, and Suresh Manandhar. 2014. SemEval-2014 task 4: Aspect based sentiment analysis. In  International Workshop on Semantic Evaluation (SemEval 2014) . Maria Pontiki, Dimitris Galanis, Haris Papageor- giou, Ion Androutsopoulos, Suresh Manandhar, Mo- hammed AL-Smadi, Mahmoud Al-Ayyoub, Yanyan Zhao, Bing Qin, Orph´ ee De Clercq, Veronique Hoste, Marianna Apidianaki, Xavier Tannier, Na- talia Loukachevitch, Evgeniy Kotelnikov, N´ uria Bel, Salud Maria Jim´ enez-Zafra, and G¨ uls ¸en Eryi˘ git. 2016. SemEval-2016 task 5: Aspect based senti- ment analysis. In  International Workshop on Se- mantic Evaluation (SemEval 2016) . Prajit Ramachandran, Peter J. Liu, and Quoc V. Le. 2017. Unsupervised pretraining for sequence to sequence learning. In  Conference on Empirical Methods in Natural Language Processing (EMNLP 2017) . Duyu Tang, Bing Qin, Xiaocheng Feng, and Ting Liu. 2016a. Effective LSTMs for target-dependent senti- ment classiﬁcation. In  International Conference on Computational Linguistics (COLING 2016) . Duyu Tang, Bing Qin, and Ting Liu. 2015. Learn- ing semantic representation of users and products for document level sentiment classiﬁcation. In  Annual Meeting of the Association for Computational Lin- guistics (ACL 2015) . Duyu Tang, Bing Qin, and Ting Liu. 2016b. Aspect level sentiment classiﬁcation with deep memory net- work. In  Conference on Empirical Methods in Nat- ural Language Processing (EMNLP 2016) . Duy-Tin Vo and Yue Zhang. 2015. Target-dependent Twitter sentiment classiﬁcation with rich automatic features. In  International Joint Conference on Arti- ﬁcial Intelligence (IJCAI 2015) . Yequan Wang, Minlie Huang, Li Zhao, and Xiaoyan Zhu. 2016. Attention-based LSTM for aspect-level sentiment classiﬁcation. In  Conference on Em- pirical Methods in Natural Language Processing (EMNLP 2016) . Matthew D. Zeiler and Rob Fergus. 2014. Visualizing and understanding convolutional networks. In  Euro- pean Conference on Computer Vision (ECCV 2014) . ", "page_idx": 5, "bbox": [307, 64.5611572265625, 525, 765.7636108398438], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 73, "type": "text", "text": "Meishan Zhang, Yue Zhang, and Duy-Tin Vo. 2016. Gated neural networks for targeted sentiment anal- ysis. In  AAAI Conference on Artiﬁcial Intelligence (AAAI 2016) . Barret Zoph, Deniz Yuret, Jonathan May, and Kevin Knight. 2016. Transfer learning for low-resource neural machine translation. In  Conference on Em- pirical Methods in Natural Language Processing (EMNLP 2016) . ", "page_idx": 6, "bbox": [72, 64.56158447265625, 290, 173.2043914794922], "page_size": [595.2760009765625, 841.8900146484375]}
