{"layout": 0, "type": "text", "text": "Disentangled Representation Learning for Non-Parallel Text Style Transfer ", "text_level": 1, "page_idx": 0, "bbox": [167, 68, 429, 101], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 1, "type": "text", "text": "Vineet John, Lili Mou, Hareesh Bahuleyan, Olga Vechtomova University of Waterloo ", "page_idx": 0, "bbox": [143, 123.88201904296875, 458, 152.66641235351562], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 2, "type": "text", "text": "{ vineet.john,hpallika,ovechtom } @uwaterloo.ca doublepower.mou@gmail.com ", "page_idx": 0, "bbox": [143, 153.55836486816406, 458, 180.03541564941406], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 3, "type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0, "bbox": [159, 223, 204, 236], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 4, "type": "text", "text": "This paper tackles the problem of disentan- gling the latent representations of style and content in language models. We propose a simple yet effective approach, which incorpo- rates auxiliary multi-task and adversarial ob- jectives, for style prediction and bag-of-words prediction, respectively. We show, both qual- itatively and quantitatively, that the style and content are indeed disentangled in the latent space. This disentangled latent representation learning can be applied to style transfer on non-parallel corpora. We achieve high perfor- mance in terms of transfer accuracy, content preservation, and language ﬂuency, in compar- ison to various previous approaches. ", "page_idx": 0, "bbox": [89, 249.57855224609375, 273, 428.9564514160156], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 5, "type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0, "bbox": [72, 442, 155, 456], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 6, "type": "text", "text": "The neural network has been a successful learning machine during the past decade due to its highly expressive modeling capability, which is a conse- quence of multiple layers of non-linear transfor- mations of input features. Such transformations, however, make intermediate features “latent,” in the sense that they do not have explicit meaning and are not interpretable. Therefore, neural net- works are usually treated as black-box machinery. ", "page_idx": 0, "bbox": [71, 466.1209411621094, 290, 587.659423828125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 7, "type": "text", "text": "Disentangling the latent space of neural net- works has become an increasingly important re- search topic. In the image domain, for example, Chen et al.  ( 2016 ) use adversarial and information maximization objectives to produce interpretable latent representations that can be tweaked to ad- just writing style for handwritten digits, as well as lighting and orientation for face models. However, this problem is less explored in natural language processing. ", "page_idx": 0, "bbox": [71, 588.8109741210938, 290, 723.8994140625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 8, "type": "text", "text": "In this paper, we address the problem of dis- entangling the latent space of neural networks for text generation. Our model is built on an autoen- coder that encodes a sentence to the latent space (vector representation) by learning to reconstruct the sentence itself. We would like the latent space to be disentangled with respect to different fea- tures, namely,  style  and  content  in our task. ", "page_idx": 0, "bbox": [307, 223.4199981689453, 525, 331.4095153808594], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 9, "type": "text", "text": "To accomplish this, we propose a simple yet ef- fective approach that combines multi-task and ad- versarial objectives. We artiﬁcially divide the la- tent representation into two parts: the style space and content space, where we consider the senti- ment of a sentence as its style. We design a sys- tematic set of auxiliary losses, enforcing the sepa- ration of style and content latent spaces. In partic- ular, the multi-task loss operates on a latent space to ensure that the space does contain the infor- mation we wish to encode. The adversarial loss, on the contrary, minimizes the predictability of in- formation that should not be contained in a given latent space. In early work, researchers typically work with the style space ( Shen et al. ,  2017 ;  Fu et al. ,  2018 ), but simply ignore the content space, as it is hard to formalize what “content” actually refers to. Cycle consistency of back-translation deﬁnes content implicitly ( Xu et al. ,  2018 ), but requires reinforcement learning over the discrete sentence space, which could be extremely difﬁcult to train. ", "page_idx": 0, "bbox": [307, 332.16204833984375, 525, 629.840576171875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 10, "type": "text", "text": "In our paper, we propose to approximate the content information by bag-of-words (BoW) fea- tures, where we focus on style-neutral, non- stopwords. Along with traditional style-oriented auxiliary losses, our BoW multi-task loss and BoW adversarial loss enable better disentangle- ment of the style and content spaces. ", "page_idx": 0, "bbox": [307, 630.5941162109375, 525, 725.0345458984375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 11, "type": "text", "text": "The learned disentangled latent space can be di- rectly used for text style transfer, which aims to transform a given sentence to a new sentence with the same content but a different style. We follow the setting where the model is trained on a non- parallel but style-labeled corpus ( Hu et al. ,  2017 ; Shen et al. ,  2017 ); thus, we call it  non-parallel text style transfer . With our disentangled latent space, we simply use the autoencoder to encode the con- tent vector of a sentence, but ignore its encoded style vector. We then infer from the training data an empirical embedding of the style that we would like to transfer to. The encoded content vector and the empirically-inferred style vector are concate- nated and fed to the decoder. This grafting tech- nique enables us to obtain a new sentence similar in content to the input sentence, but with a differ- ent style. ", "page_idx": 0, "bbox": [307, 725.787109375, 525, 766.0315551757812], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 12, "type": "text", "text": "", "page_idx": 1, "bbox": [72, 63.68701934814453, 290, 266.5215759277344], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 13, "type": "text", "text": "We conducted experiments on two benchmark datasets. Both qualitative and quantitative results show that the style and content spaces are indeed disentangled well. In the style-transfer evaluation, we achieve high performance in style-transfer ac- curacy, content preservation, as well as language ﬂuency, compared with previous results. Ablation tests also show that all our auxiliary losses can be combined well, each playing its own role in disen- tangling the latent space. ", "page_idx": 1, "bbox": [72, 267.787109375, 290, 402.8756408691406], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 14, "type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1, "bbox": [71, 416, 162, 429], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 15, "type": "text", "text": "Disentangling neural networks’ latent space has been explored in computer vision in recent years, and researchers have successfully disentangled the features (such as rotation and color) of im- ages ( Chen et al. ,  2016 ;  Higgins et al. ,  2017 ). In these approaches, the disentanglement is purely unsupervised, as no style labels are needed. Un- fortunately, we have not observed disentangled features by applying these approaches in text rep- resentations, and thus we require style labels in our approach. ", "page_idx": 1, "bbox": [72, 439.5301818847656, 290, 588.167724609375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 16, "type": "text", "text": "Style-transfer has also been explored in com- puter vision. For example,  Gatys et al.  ( 2016 ) show that the artistic style of an image can be cap- tured well by certain statistics. ", "page_idx": 1, "bbox": [72, 589.4332275390625, 290, 643.2266845703125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 17, "type": "text", "text": "In NLP, the deﬁnition of “style” itself is vague, and as a convenient starting point, researchers of- ten treat sentiment as a salient style attribute.  Hu et al.  ( 2017 ) propose to control the sentiment by using discriminators to reconstruct sentiment and content from generated sentences. However, there is no evidence that the latent space would be disen- tangled by simply reconstructing a sentence.  Shen et al.  ( 2017 ) use a pair of adversarial discrimina- tors to align the recurrent hidden decoder states of original and style-transferred sentences, for a given style. Fu et al.  ( 2018 ) propose two ap- proaches: training style-speciﬁc embeddings and training separate style-speciﬁc decoders. Their style embeddings are similar to an earlier study by study by  Ficler and Goldberg  ( 2017 ). Their multi- decoder approach is used by  Nogueira dos Santos et al.  ( 2018 ), and is extended to private-shared net- works for styled generation ( Zhang et al. ,  2018 ). Zhao et al.  ( 2018 ) also extend the multi-decoder approach and use a Wasserstein-distance penalty to align content representations of sentences with different styles. Tsvetkov et al.  ( 2018 ) use a machine-translation preprocessing step to strip au- thor style from documents, and then use a multi- decoder model to convert the result into a sentence with a speciﬁc style. ", "page_idx": 1, "bbox": [72, 644.4922485351562, 290, 766.0316772460938], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 18, "type": "text", "text": "", "page_idx": 1, "bbox": [307, 63.68726348876953, 525, 307.1688537597656], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 19, "type": "text", "text": "Recently, cycle consistency of back-translation is applied to ensure content preservation ( Xu et al. , 2018 ;  Logeswaran et al. ,  2018 ). These methods re- quire reinforcement learning and are usually difﬁ- cult to train. ", "page_idx": 1, "bbox": [307, 310.75439453125, 525, 378.0959167480469], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 20, "type": "text", "text": "Li et al.  ( 2018 ) propose a hybrid retrieval and generation method that transfers the style by re- trieving and incrementally editing a sentence sim- ilar to the source sentence. ", "page_idx": 1, "bbox": [307, 381.6814270019531, 525, 435.47393798828125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 21, "type": "text", "text": "Rao and Tetreault  ( 2018 ) treat the formality of writing as a style, and create a parallel corpus for style transfer with sequence-to-sequence models. This is beyond the scope of our paper, as we focus on non-parallel text style transfer. ", "page_idx": 1, "bbox": [307, 439.0584716796875, 525, 506.4009704589844], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 22, "type": "text", "text": "Style transfer generation is also related to non- parallel machine translation, where researchers apply similar techniques of adversarial alignment, back translation, etc. ( Lample et al. ,  2018a , b ;  Con- neau et al. ,  2018 ). ", "page_idx": 1, "bbox": [307, 509.98553466796875, 525, 577.3280029296875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 23, "type": "text", "text": "Our paper differs from previous work in that we accomplish style transfer with a disentangled la- tent space, for which we propose a systematic set of auxiliary losses. ", "page_idx": 1, "bbox": [307, 580.9125366210938, 525, 634.7059326171875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 24, "type": "text", "text": "3 Approach ", "text_level": 1, "page_idx": 1, "bbox": [307, 656, 377, 670], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 25, "type": "text", "text": "Figure  1  shows the overall framework of our ap- proach. We will ﬁrst present an autoencoder as our base model. Then we design the auxiliary losses for style and content disentanglement. Finally, we introduce our approach to style-transfer text gen- eration. ", "page_idx": 1, "bbox": [307, 685.1405029296875, 525, 766.031982421875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 26, "type": "image", "page_idx": 2, "img_path": "layout_images/P19-1041_0.jpg", "img_caption": "Figure 1: Overview of our approach. ", "bbox": [81, 61, 279, 227], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "(a) Training Phase Jmut(s) Sais(c)\n\nVy\n\nthe s the\nproduct GRU) __\"y)_—__»-|GRU product } Ja,\nis great RNN c RNN is great\n\nf X\n\nImui(c) Iais(s)\n\n(b) Inference Phase 3\nXN\nes\nthe S| 1S. the\nbook {GRU [-»joRU book\n\nis good |RNN ic“ Ie) RNIN\n\nx*\n\nis boring\n\n", "vlm_text": "The image depicts a schematic of a machine learning approach involving GRU (Gated Recurrent Unit) RNNs (Recurrent Neural Networks), which is divided into two phases: Training and Inference.\n\n**(a) Training Phase**:\n- The left part of the image shows a GRU RNN processing a phrase, \"the product is great\", and generating two elements, `s` (content) and `c` (context).\n- The elements `s` and `c` are processed and fed into another GRU RNN to reconstruct the input phrase \"the product is great\".\n- The loss functions `J_rec`, `J_mul(s)`, `J_mul(c)`, `J_dis(s)`, and `J_dis(c)` are depicted with red arrows to denote the optimization objectives. These likely represent different objectives involved in training, such as reconstruction (`J_rec`) and disentangling or regularizing (`J_mul` and `J_dis`).\n\n**(b) Inference Phase**:\n- The bottom part of the image illustrates the inference process, where a GRU RNN processes a new input, \"the book is good\", to produce `s*`, `c*`, and then a synthesized vector `ŝ`.\n- This synthesized vector `ŝ` is subsequently fed into another GRU RNN to produce the output, \"the book is boring\", implying some form of content manipulation or transformation based on the learned representations during training.\n\nTogether, these phases represent a system designed to generate, modify, or reconstruct text sequences using learned content (`s`) and context (`c`) representations, with the potential application in tasks like text generation, style transfer, or context-aware modifications in natural language processing (NLP)."}
{"layout": 27, "type": "text", "text": "3.1 Autoencoder ", "text_level": 1, "page_idx": 2, "bbox": [71, 254, 157, 266], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 28, "type": "text", "text": "An autoencoder encodes an input to a latent vector space, from which it reconstructs the input itself. ", "page_idx": 2, "bbox": [71, 275.958984375, 290, 302.6534729003906], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 29, "type": "text", "text": "Let  ${\\bf x_{\\alpha}}={\\bf\\alpha}(x_{1},x_{2},\\cdot\\cdot\\cdot x_{n})$   be an input sequence with  $n$   words. Our encoder uses a recurrent neural network (RNN) with gated recurrent units (GRUs, Cho et al. ,  2014 ); it reads  x  word-by-word, and performs a linear transformation of the ﬁnal hid- den state to obtain a hidden vector representation  $h$  . ", "page_idx": 2, "bbox": [71, 305, 290, 400.0804748535156], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 30, "type": "text", "text": "Then, a decoder RNN generates a sentence word-by-word, which ideally should be    $_\\mathrm{x}$   itself. Suppose at a time step    $t$   the decoder RNN predicts the word  $x_{t}$   with probability  $p(x_{t}|h,x_{1}\\cdot\\cdot\\cdot x_{t-1})$  , the autoencoder is trained with a sequence- aggregated cross-entropy loss, given by ", "page_idx": 2, "bbox": [71, 403.0660095214844, 290, 483.95751953125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 31, "type": "equation", "text": "\n$$\nJ_{\\mathrm{AE}}({\\pmb\\theta}_{\\mathrm{E}},{\\pmb\\theta}_{\\mathrm{D}})=-\\sum_{t=1}^{n}\\log p(x_{t}|{\\pmb h},x_{1}\\cdot\\cdot\\cdot x_{t-1})\n$$\n ", "text_format": "latex", "page_idx": 2, "bbox": [79, 500, 282, 536], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 32, "type": "text", "text": "where    $\\theta_{\\mathrm{E}}$   and    $\\theta_{\\mathrm{D}}$   are the parameters of the en- coder and decoder, respectively. For brevity, we only present the loss for a single data point (i.e., a sentence) throughout the paper. Total loss sums over all data points, and is implemented with mini- batches. Both the encoder and decoder are deter- ministic functions in the this model ( Rumelhart et al. ,  1986 ), and thus, we call it a  deterministic autoencoder (DAE).", "page_idx": 2, "bbox": [71, 547, 290, 668.6044921875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 33, "type": "text", "text": "Variational Autoencoder. Alternatively, we may use a variational autoencoder (VAE,  Kingma and Welling ,  2013 ), which imposes a probabilistic distribution on the latent vector. The decoder re- constructs data based on the sampled latent vector from its posterior, and the Kullback–Leibler (KL, 1951 ) divergence is penalized for regularization. ", "page_idx": 2, "bbox": [71, 671.1983032226562, 290, 766.0314331054688], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 34, "type": "text", "text": "Formally, the VAE loss is ", "page_idx": 2, "bbox": [318, 63.68701934814453, 429.4795227050781, 76.83248901367188], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 35, "type": "equation", "text": "\n$$\n\\begin{array}{r}{J_{\\mathrm{AE}}(\\pmb{\\theta}_{\\mathrm{E}},\\pmb{\\theta}_{\\mathrm{D}})=-\\,\\mathbb{E}_{q_{E}(\\pmb{h}|\\mathrm{x})}[\\log p(\\mathrm{x}|\\pmb{h})]\\quad\\quad}\\\\ {+\\,\\lambda_{\\mathrm{k}!}\\,\\mathrm{KL}(q_{E}(\\pmb{h}|\\mathrm{x})\\|p(\\pmb{h}))}\\end{array}\n$$\n ", "text_format": "latex", "page_idx": 2, "bbox": [317, 87, 502, 122], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 36, "type": "text", "text": "where    $\\lambda_{\\mathrm{bl}}$   is the hyperparameter balancing the reconstruction loss and the KL term.  $p(h)$  is the prior, typically the standard normal\n\n  $\\mathcal{N}(\\mathbf{0},\\operatorname{I})$  .  $q_{E}(h|\\mathrm{x})$   is the posterior in the form\n\n  ${\\mathcal{N}}(\\pmb{\\mu},\\mathrm{diag}\\,\\pmb{\\sigma}^{2})$  , where    $\\pmb{\\mu}$   and    $\\pmb{\\sigma}$   are predicted by the encoder. ", "page_idx": 2, "bbox": [306, 132.80104064941406, 527, 213.69253540039062], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 37, "type": "text", "text": "Compared with DAE, the reconstruction of VAE is based on the samples of the posterior, which populates encoded representations into a neighbourhood close to its prior and thus smooths the latent space.  Bowman et al.  ( 2016 ) show that VAE enables more ﬂuent sentence generation from a latent space than DAE. ", "page_idx": 2, "bbox": [306, 214.4990997314453, 527, 308.9396057128906], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 38, "type": "text", "text": "The autoencoding loss serves as our primary training objective for sentence generation. For dis- entangled representation learning, we hope that    $h$  can be separated into two spaces    $\\mathbf{\\nabla}_{\\mathbf{S}}$   and    $_c$  , repre- senting style and content, respectively, i.e.,    $h\\ =$   $[s;c]$  , where    $[\\cdot;\\cdot]$   denotes concatenation. This is accomplished by a systematic design of auxiliary losses described below, and shown in Figure  1 a. ", "page_idx": 2, "bbox": [306, 309.74615478515625, 527, 417.7356872558594], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 39, "type": "text", "text": "3.2 Style-Oriented Losses ", "text_level": 1, "page_idx": 2, "bbox": [306, 428, 434, 441], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 40, "type": "text", "text": "We ﬁrst design auxiliary losses that ensure the style information is contained in the style space    $\\mathbf{\\nabla}_{s}$  . This involves (1) a multi-task loss that ensures    $\\mathbf{\\nabla}_{s}$   is discriminative for the style, and (2) an adversarial loss that ensures  $_c$   is not. ", "page_idx": 2, "bbox": [306, 446.6462097167969, 527, 513.9887084960938], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 41, "type": "text", "text": "Multi-Task Loss for Style.  In the dataset, each sentence is labeled with its style, particularly, bi- nary sentiment of positive or negative, following most previous work ( Hu et al. ,  2017 ;  Shen et al. , 2017 ;  Fu et al. ,  2018 ;  Zhao et al. ,  2018 ). ", "page_idx": 2, "bbox": [306, 514.4025268554688, 527, 582.1376953125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 42, "type": "text", "text": "We build a two-way softmax classiﬁer (equiva- lent to logistic regression) on the style space    $\\mathbf{\\nabla}_{s}$   to predict the style label, given by ", "page_idx": 2, "bbox": [306, 582.9432373046875, 527, 623.1876831054688], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 43, "type": "equation", "text": "\n$$\n\\pmb{y}_{s}=\\mathrm{softmax}(W_{\\mathrm{mul(s)}}\\pmb{s}+\\pmb{b}_{\\mathrm{mul(s)}})\n$$\n ", "text_format": "latex", "page_idx": 2, "bbox": [339, 634, 492, 650], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 44, "type": "text", "text": "where    $\\theta_{\\mathrm{multi(s)}}\\;=\\;\\left[W_{\\mathrm{multi(s)}};b_{\\mathrm{multi(s)}}\\right]$   are the param- eters of the style classiﬁer in the setting of multi- task learning, and  $\\pmb{y}_{s}$   is the output of softmax layer. ", "page_idx": 2, "bbox": [306, 661, 527, 703.3536987304688], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 45, "type": "text", "text": "The classiﬁer is trained with cross-entropy loss against the ground-truth distribution  $t_{s}(\\cdot)$   by ", "page_idx": 2, "bbox": [306, 702.5242309570312, 527, 729.2186889648438], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 46, "type": "equation", "text": "\n$$\nJ_{\\mathrm{mul(s)}}(\\pmb{\\theta}_{\\mathrm{E}};\\pmb{\\theta}_{\\mathrm{mul(s)}})=-\\sum_{l\\in\\mathrm{labels}}t_{s}(l)\\log y_{s}(l)\n$$\n ", "text_format": "latex", "page_idx": 2, "bbox": [311, 738, 508, 769], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 47, "type": "text", "text": "In fact, we train the style classiﬁer at the same time as the autoencoding loss. Thus, this could be viewed as  multi-task  learning, incentivizing the entire model to not only decode the sentence, but also predict its sentiment from the style vector    $\\mathbf{\\nabla}_{\\mathbf{S}}$  . We denote it by “mul(s).” The idea of multi-task training is not new and has been used in previous work for sentence representation learning ( Jernite et al. ,  2017 ) and sentiment analysis ( Balikas et al. , 2017 ), among others. ", "page_idx": 3, "bbox": [71, 63.68701934814453, 290, 198.77554321289062], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 48, "type": "text", "text": "Adversarial Loss for Style. The multi-task loss only ensures that the style space contains style information. However, the content space might also contain style information, which is undesir- able for disentanglement. ", "page_idx": 3, "bbox": [71, 199.56736755371094, 290, 267.3016052246094], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 49, "type": "text", "text": "We thus apply an adversarial loss to discour- age the content space containing style information. We ﬁrst train a separate classiﬁer, called an  adver- sary , that deliberately discriminates the style label based on the content vector    $_c$  . Then, the encoder is trained to encode a content space from which its adversary cannot predict the style. ", "page_idx": 3, "bbox": [71, 268.48614501953125, 290, 362.9266662597656], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 50, "type": "text", "text": "Concretely, the adversarial discriminator and its training objective have a similar form as Eqns. ( 3 ) and ( 4 ), but with different input and parameters, given by ", "page_idx": 3, "bbox": [71, 364.1101989746094, 290, 417.9036865234375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 51, "type": "equation", "text": "\n$$\n\\begin{array}{r}{\\pmb{y}_{s}=\\mathrm{softmax}(\\pmb{W}_{\\mathrm{dis(s)}}\\pmb{c}+\\pmb{b}_{\\mathrm{dis(s)}})}\\\\ {J_{\\mathrm{dis(s)}}(\\pmb{\\theta}_{\\mathrm{dis(s)}})=-\\sum_{l\\in\\mathrm{labels}}t_{c}(l)\\log y_{s}(l)}\\end{array}\n$$\n ", "text_format": "latex", "page_idx": 3, "bbox": [78, 429, 271, 470], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 52, "type": "text", "text": "where  $\\theta_{\\mathrm{{dis(s)}}}=[W_{\\mathrm{{dis(s)}}};b_{\\mathrm{{dis(s)}}}]$   are the parameters of the adversary. ", "page_idx": 3, "bbox": [71, 480, 290, 507.28570556640625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 53, "type": "text", "text": "It should be emphasized that, when we train the adversary, the gradient is not propagated back to the autoencoder, i.e., the vector    $^c$   is treated as shal- low features. Therefore, we view    $J_{\\mathrm{{dis(s)}}}$   as a func- tion of    $\\theta_{\\mathrm{{dis(s)}}}$   only, whereas    $J_{\\mathrm{multi}(\\mathrm{s})}$   is a function of both    $\\theta_{\\mathrm{E}}$   and    $\\theta_{\\mathrm{multi}(\\mathrm{s})}$  . ", "page_idx": 3, "bbox": [71, 508.46923828125, 290, 590.9976806640625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 54, "type": "text", "text": "Having trained an adversary, we would like the autoencoder to be tuned in such an  ad hoc  fashion that    $_c$   is not discriminative for style. In existing literature, there could be different approaches, for example, maximizing the adversary’s loss ( Shen et al. ,  2017 ;  Zhao et al. ,  2018 ) or penalizing the entropy of the adversary’s prediction (  $\\mathrm{^{Ru}}$   et al. , 2018 ). In our work, we adopt the latter, as it can be easily extended to multi-category classiﬁ- cation, used in Subsection  3.3 . Formally, the style- oriented adversarial objective is to maximize ", "page_idx": 3, "bbox": [71, 590.5452270507812, 290, 739.1826782226562], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 55, "type": "equation", "text": "\n$$\nJ_{\\mathrm{adv}(\\mathrm{s})}(\\pmb{\\theta}_{\\mathrm{E}})=\\mathcal{H}(\\pmb{y}_{s}|\\pmb{c};\\pmb{\\theta}_{\\mathrm{dis}(\\mathrm{s})})\n$$\n ", "text_format": "latex", "page_idx": 3, "bbox": [115, 752, 246, 767], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 56, "type": "text", "text": "where    $\\pmb{y}_{s}$   is the predicted distribution over the style labels and    $\\begin{array}{r}{\\mathcal{H}(\\pmb{p})\\ =\\ -\\sum_{i\\in\\mathrm{labels}}p_{i}\\log p_{i}}\\end{array}$  ∈  is the entropy of the adversary. Here,    $J_{\\mathrm{adv(s)}}$   is max- imized with respect to the encoder    $\\theta_{\\mathrm{E}}$   and we ﬁx\n\n  $\\theta_{\\mathrm{{dis(s)}}}$  . The objective attains maximum value when\n\n  $\\pmb{y}_{s}$   is uniform. ", "page_idx": 3, "bbox": [307, 63.68726348876953, 525, 146.21572875976562], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 57, "type": "text", "text": "While adversarial loss has been explored in pre- vious style-transfer studies ( Shen et al. ,  2017 ;  Fu et al. ,  2018 ), it has not been combined with the multi-task loss. As shown in our experiments, a simple combination of these two losses is promis- ingly effective, achieving better style transfer per- formance than a variety of previous methods. ", "page_idx": 3, "bbox": [307, 145.7842559814453, 525, 240.22476196289062], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 58, "type": "text", "text": "3.3 Content-Oriented Losses ", "text_level": 1, "page_idx": 3, "bbox": [307, 253, 448, 265], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 59, "type": "text", "text": "The above style-oriented losses only regularize style information, but they do not impose any con- straint on how the content information should be encoded. ", "page_idx": 3, "bbox": [307, 271.3062744140625, 525, 325.0997619628906], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 60, "type": "text", "text": "In practice, the style space is usually smaller than content space. But it is unrealistic to expect that the content would not ﬂow into the style space simply because of its limited capacity. Therefore, we need to design content-oriented losses to reg- ularize the content information. In most previous work, however, the treatment of content is miss- ing ( Hu et al. ,  2017 ;  Fu et al. ,  2018 ). ", "page_idx": 3, "bbox": [307, 326.3042907714844, 525, 434.2947998046875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 61, "type": "text", "text": "Inspired by the above combination of multi-task and adversarial losses, we apply the same idea to the content space. However, it is usually hard to deﬁne what “content” actually refers to. ", "page_idx": 3, "bbox": [307, 435.49932861328125, 525, 489.2918395996094], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 62, "type": "text", "text": "To this end, we propose to approximate the con- tent information by bag-of-words (BoW) features. The BoW feature of a sentence is a vector, each element indicating the probability of a word’s oc- currence. For a sentence  x  with    $N$   words, the word  $w_{*}$  ’s BoW probability is    $\\begin{array}{r}{t_{c}(w_{*})=\\frac{\\sum_{i=1}^{N}\\mathbb{I}\\{w_{i}=w_{*}\\}}{N}}\\end{array}$  P , where  $\\mathbb{I}\\{\\cdot\\}$   is an indicator function. Here, we only consider content words, excluding stopwords and sentiment words ( Hu and Liu ,  2004 ),   since we focus on “content” information. It should be men- tioned that the removal of stopwords and senti- ment words is not essential, but results in better performance. We analyze the effect of using dif- ferent vocabularies in Appendix  B . ", "page_idx": 3, "bbox": [307, 490.49737548828125, 525, 682.2394409179688], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 63, "type": "text", "text": "Multi-Task Loss for Content.  Similar to the style-oriented loss, the multi-task loss for content, denoted as “mul(c),” ensures that the content space  $_c$   contains content information, i.e., BoW features. We introduce a softmax classiﬁer over the BoW vocabulary ", "page_idx": 3, "bbox": [307, 683.0513305664062, 525, 723.6874389648438], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 64, "type": "text", "text": "", "page_idx": 4, "bbox": [71, 63.68701934814453, 290, 103.93148803710938], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 65, "type": "equation", "text": "\n$$\n\\pmb{y}_{c}=\\mathrm{softmax}(W_{\\mathrm{mul(c)}}\\pmb{c}+\\pmb{b}_{\\mathrm{mul(c)}})\n$$\n ", "text_format": "latex", "page_idx": 4, "bbox": [104, 110, 258, 125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 66, "type": "text", "text": "where  $\\theta_{\\mathrm{mul(c)}}{=}[W_{\\mathrm{mul(c)}};b_{\\mathrm{mul(c)}}]$   are the classiﬁer’s parameters;  $\\pmb{y}_{c}$   is the predicted BoW distribution. ", "page_idx": 4, "bbox": [71, 132, 290, 160.98342895507812], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 67, "type": "text", "text": "The training objective is a cross-entropy loss against the ground-truth distribution    $t_{c}(\\cdot)$  : ", "page_idx": 4, "bbox": [71, 159.75099182128906, 290, 186.44546508789062], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 68, "type": "equation", "text": "\n$$\nJ_{\\mathrm{mul(c)}}(\\pmb{\\theta}_{\\mathrm{E}};\\pmb{\\theta}_{\\mathrm{mul(c)}})=-\\sum_{w\\in\\mathrm{volab}}t_{c}(w)\\log y_{c}(w)\n$$\n ", "text_format": "latex", "page_idx": 4, "bbox": [79, 191, 282, 222], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 69, "type": "text", "text": "where the optimization is performed with both en- coder parameters    $\\pmb{\\theta}_{\\mathrm{E}}$   and the multi-task classiﬁer  $\\theta_{\\mathrm{multi(c)}}$  . Notice that, although the target distribu- tion is not one-hot for BoW, the cross-entropy loss in Eqn. ( 9 ) has the same form as ( 4 ). ", "page_idx": 4, "bbox": [71, 232.69496154785156, 290, 300.0374450683594], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 70, "type": "text", "text": "It is also interesting that, at ﬁrst glance, the multi-task loss for content appears to be redun- dant to the autoencoding loss, when in fact, it is not. The autoencoding loss only requires that the model could reconstruct the sentence based on the combined content and style spaces, but does not ensure their separation. The multi-task loss fo- cuses on content words and is applied to the con- tent space only. ", "page_idx": 4, "bbox": [71, 300.44097900390625, 290, 421.9804992675781], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 71, "type": "text", "text": "Adversarial Loss for Content.  To ensure that the style space does not contain content informa- tion, we design our ﬁnal auxiliary loss, the BoW adversarial loss for content, denoted as “adv(c).” ", "page_idx": 4, "bbox": [71, 421.9913024902344, 290, 476.1775207519531], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 72, "type": "text", "text": "We build a content adversary, a softmax classi- ﬁer on the style space predicting BoW features ", "page_idx": 4, "bbox": [71, 476.5810546875, 290, 503.2755432128906], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 73, "type": "equation", "text": "\n$$\n\\begin{array}{r l}&{\\pmb{y}_{c}=\\mathrm{softmax}(W_{\\mathrm{dis(c)}}^{\\top}\\pmb{s}+\\pmb{b}_{\\mathrm{dis(c)}})}\\\\ &{\\quad J_{\\mathrm{dis(c)}}(\\pmb{\\theta}_{\\mathrm{dis(c)}})=-\\displaystyle\\sum_{w\\in\\mathrm{vacab}}t_{c}(w)\\log{y_{c}(w)}}\\end{array}\n$$\n ", "text_format": "latex", "page_idx": 4, "bbox": [82, 507, 263, 556], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 74, "type": "text", "text": "where  $\\theta_{\\mathrm{dis(c)}}=[W_{\\mathrm{dis(c)}};b_{\\mathrm{dis(c)}}]$   are the classiﬁer’s parameters for BoW prediction. ", "page_idx": 4, "bbox": [71, 561, 290, 588.6725463867188], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 75, "type": "text", "text": "The adversarial loss for the model is to maxi- mize the entropy of the discriminator ", "page_idx": 4, "bbox": [71, 589.0761108398438, 290, 615.7705078125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 76, "type": "equation", "text": "\n$$\nJ_{\\mathrm{adv}(\\mathrm{c})}(\\pmb{\\theta}_{\\mathrm{E}})=\\mathcal{H}(\\pmb{y}_{c}|\\pmb{s};\\pmb{\\theta}_{\\mathrm{dis}(\\mathrm{c})})\n$$\n ", "text_format": "latex", "page_idx": 4, "bbox": [115, 622, 246, 637], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 77, "type": "text", "text": "Again,    $J_{\\mathrm{{dis(c)}}}$   is trained with respect to the dis- criminator’s parameters    $\\theta_{\\mathrm{{dis(c)}}}$  , whereas    $J_{\\mathrm{adv}(\\mathrm{c})}$   is trained with respect to    $\\theta_{\\mathrm{E}}$  , similar to the adversar- ial loss for style. ", "page_idx": 4, "bbox": [71, 644.4921264648438, 290, 698.2855224609375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 78, "type": "text", "text": "Our BoW-based, content-oriented losses are novel in the style-transfer literature. While they do not directly work with “style,” they regularize the content information, so that the style and con- tent can be better disentangled. ", "page_idx": 4, "bbox": [71, 698.6890869140625, 290, 766.031494140625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 79, "type": "text", "text": "1  foreach  mini-batch  do ", "page_idx": 4, "bbox": [307, 64.80839538574219, 416.9330139160156, 79.00112915039062], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 80, "type": "text", "text": "2 minimize    $J_{\\mathrm{{dis(s)}}}\\big(\\pmb{\\theta}_{\\mathrm{{dis(s)}}}\\big)$   w.r.t.    $\\theta_{\\mathrm{{dis(s)}}}$  ; 3 minimize    $J_{\\mathrm{{dis(c)}}}(\\pmb{\\theta}_{\\mathrm{{dis(c)}}})$   w.r.t.    $\\theta_{\\mathrm{{dis(c)}}}$  ; 4 minimize    $\\boldsymbol{J}_{\\mathrm{over}}$   w.r.t.    $\\theta_{\\mathrm{E}},\\theta_{\\mathrm{D}},\\theta_{\\mathrm{mul(s)}},\\theta_{\\mathrm{mul(c)}}.$  ; 5  end ", "page_idx": 4, "bbox": [307, 78, 519.6907958984375, 134.19503784179688], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 81, "type": "text", "text": "3.4 Training Process ", "text_level": 1, "page_idx": 4, "bbox": [307, 169, 411, 182], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 82, "type": "text", "text": "The overall loss    $J_{\\mathrm{ovr}}$   for our model comprises sev- eral terms: the autoencoder’s reconstruction ob- jective, the multi-task and adversarial objectives, for style and content, respectively, given by ", "page_idx": 4, "bbox": [307, 186, 525, 240.03457641601562], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 83, "type": "equation", "text": "\n$$\n\\begin{array}{r l r}&{J_{\\mathrm{ov}}=J_{\\mathrm{AE}}(\\pmb{\\theta}_{\\mathrm{E}},\\pmb{\\theta}_{\\mathrm{D}})}&{(13)}\\\\ &{}&{+\\lambda_{\\mathrm{mul(s)}}J_{\\mathrm{mul(s)}}\\big(\\pmb{\\theta}_{\\mathrm{E}},\\pmb{\\theta}_{\\mathrm{mul(s)}}\\big)-\\lambda_{\\mathrm{adv(s)}}J_{\\mathrm{adv(s)}}\\big(\\pmb{\\theta}_{\\mathrm{E}}\\big)}\\\\ &{}&{+\\lambda_{\\mathrm{mul(c)}}J_{\\mathrm{mul(c)}}\\big(\\pmb{\\theta}_{\\mathrm{E}},\\pmb{\\theta}_{\\mathrm{mul(c)}}\\big)-\\lambda_{\\mathrm{adv(c)}}J_{\\mathrm{adv(c)}}\\big(\\pmb{\\theta}_{\\mathrm{E}}\\big)}\\end{array}\n$$\n ", "text_format": "latex", "page_idx": 4, "bbox": [307, 248, 523, 299], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 84, "type": "text", "text": "where  $\\lambda\\mathbf{s}$   are the hyperparameters that balance the autoencoding loss and these auxiliary losses. ", "page_idx": 4, "bbox": [307, 307.65509033203125, 525, 334.3495788574219], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 85, "type": "text", "text": "To put it all together, the model training in- volves an alternation of optimizing the adversaries by    $J_{\\mathrm{{dis(s)}}}$   and    $J_{\\mathrm{{dis(c)}}}$  , and the model itself by    $\\boldsymbol{J}_{\\mathrm{over}}$  , shown in Algorithm  1 . ", "page_idx": 4, "bbox": [307, 334.75408935546875, 525, 388.5466003417969], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 86, "type": "text", "text": "3.5 Generating Style-Transferred Sentences ", "text_level": 1, "page_idx": 4, "bbox": [307, 399, 519, 410], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 87, "type": "text", "text": "A direct application of our disentangled latent space is style-transfer sentence generation, i.e., we can synthesize a sentence with generally the same meaning but a different style in the inference stage. ", "page_idx": 4, "bbox": [307, 415.1541442871094, 525, 468.9476318359375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 88, "type": "text", "text": "Let  $\\mathrm{{x}^{*}}$  be an input sentence with    $s^{*}$  and    $c^{*}$  be- ing the encoded style and content vectors, respec- tively. If we would like to transfer its content to a different style, we compute an empirical estimate of the target style’s vector  $\\hat{\\pmb{s}}$   of the training set, us- ing ", "page_idx": 4, "bbox": [307, 469.3511657714844, 525, 550.24267578125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 89, "type": "equation", "text": "\n$$\n\\hat{\\pmb{s}}=\\frac{\\sum_{i\\in\\mathrm{target\\;cycle}}s_{i}}{\\#\\;\\mathrm{target\\;cycle}}\n$$\n ", "text_format": "latex", "page_idx": 4, "bbox": [356, 547.25, 476, 578], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 90, "type": "text", "text": "The inferred target style  $\\hat{\\pmb{s}}$   is concatenated with the encoded content    $c^{*}$  for decoding style-transferred sentences, as shown in Figure  1 b. ", "page_idx": 4, "bbox": [307, 582.5549926757812, 525, 622.7984619140625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 91, "type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 4, "bbox": [307, 633, 391, 646], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 92, "type": "text", "text": "4.1 Datasets ", "text_level": 1, "page_idx": 4, "bbox": [307, 655, 372, 666], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 93, "type": "text", "text": "We conducted experiments on two datasets, Yelp and Amazon reviews. Both comprise sentences la- beled by binary sentiment (positive or negative). They are used to train latent space disentangle- ment as well as to evaluate sentiment transfer. ", "page_idx": 4, "bbox": [307, 671.5910034179688, 525, 738.9324340820312], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 94, "type": "text", "text": "Yelp Service Reviews.  We used the Yelp re- view dataset, following previous work ( Shen et al. , 2017 ;  Zhao et al. ,  2018 ). It contains 444101, 63483, and 126670 labeled reviews for train, vali- dation, and test, respectively. We set the maximum length of a sentence to 15 words and the vocabu- lary size to  ${\\sim}9200$  , following  Shen et al.  ( 2017 ). ", "page_idx": 4, "bbox": [307, 738.9442749023438, 525, 766.0314331054688], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 95, "type": "text", "text": "", "page_idx": 5, "bbox": [72, 63.68701934814453, 290, 131.02951049804688], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 96, "type": "text", "text": "Amazon Product Reviews.  We further eval- uate our model with an Amazon review dataset, following some other previous papers ( Fu et al. , 2018 ).   It contains 555142, 2000, and 2000 la- beled reviews for train, validation, and test, re- spectively. The maximum length of a sentence is set to 20 words and the vocabulary size is    ${\\sim}58\\mathrm{k\\Omega}$  , as in  Fu et al.  ( 2018 ). ", "page_idx": 5, "bbox": [72, 131.26133728027344, 290, 239.64358520507812], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 97, "type": "text", "text": "4.2 Experimental Settings ", "text_level": 1, "page_idx": 5, "bbox": [71, 250, 200, 262], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 98, "type": "text", "text": "Our RNN has a hidden state of 256 dimensions, linearly transformed to a style space of 8 dimen- sions and a content space of 128 dimensions. They were chosen empirically, and we found them ro- bust to model performance. For the decoder, we fed the latent vector    $\\pmb{h}=[\\pmb{s},\\pmb{c}]$   to the hidden state at each step. ", "page_idx": 5, "bbox": [72, 267.56109619140625, 290, 362.0016174316406], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 99, "type": "text", "text": "We used the Adam optimizer ( Kingma and Ba , 2014 ) for the autoencoder and the RMSProp op- timizer ( Tieleman and Hinton ,  2012 ) for the dis- criminators, following stability tricks in adversar- ial training ( Arjovsky et al. ,  2017 ). Each optimizer has an initial learning rate of    $10^{-3}$  . Our model is trained for 20 epochs, by which time it has con- verged. The word embedding layer was initial- ized by word2vec ( Mikolov et al. ,  2013 ) trained on respective training sets. Both the autoencoder and the discriminators are trained once per mini- batch with    $\\lambda_{\\mathrm{multi(s)}}=10$  ,    $\\lambda_{\\mathrm{mul(c)}}=3$  ,    $\\lambda_{\\mathrm{adv(s)}}=1$  , and    $\\lambda_{\\mathrm{adv(c)}}\\,=\\,0.03$  . These hyperparameters were tuned by a log-scale grid search within two orders of magnitude around the default value  1 ; we chose the values yielding the best validation results. ", "page_idx": 5, "bbox": [72, 362.6251525878906, 290, 579.0086669921875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 100, "type": "text", "text": "For the VAE model, the KL penalty is weighted by    $\\lambda_{\\mathrm{k l(s)}}$   and    $\\lambda_{\\mathrm{kl(c)}}$   for style and content, respec- tively. We set both to  0 . 03 , tuned by the same method of log-scale grid search. During training, we also used the  sigmoid  KL annealing schedule, following  Bahuleyan et al.  ( 2018 ). ", "page_idx": 5, "bbox": [72, 579.6332397460938, 290, 660.524658203125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 101, "type": "text", "text": "4.3 Exp. I: Disentangling Latent Space ", "text_level": 1, "page_idx": 5, "bbox": [72, 671, 264, 684], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 102, "type": "text", "text": "First, we analyze how the style (sentiment) and content of the latent space are disentangled. We ", "page_idx": 5, "bbox": [72, 688.4422607421875, 290, 715.13671875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 103, "type": "table", "page_idx": 5, "img_path": "layout_images/P19-1041_1.jpg", "bbox": [319, 62, 516, 126], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "Yelp Amazon\nLatent Suace DAE | VAE || DAE | VAE\nNone (majority guess) 0.60 0.51\nContent space (c) 0.66 | 0.70 0.67 | 0.69\nStyle space (s) 0.97 | 0.97 0.82 | 0.81\nComplete space ([s;c]) || 0.97 | 0.97 || 0.82 | 0.81\n\n", "vlm_text": "The table presents the performance of models on two datasets, Yelp and Amazon, using different types of latent spaces. It compares the performance of Denoising Autoencoder (DAE) and Variational Autoencoder (VAE) models based on their ability to utilize various latent spaces. The performance metrics likely represent accuracy or a related metric.\n\nHere's a breakdown of the table:\n\n- **None (majority guess):**\n  - Yelp: 0.60\n  - Amazon: 0.51\n  - This represents the baseline performance using majority class guessing without leveraging latent space.\n\n- **Content Space (c):**\n  - Yelp: \n    - DAE: 0.66\n    - VAE: 0.70\n  - Amazon: \n    - DAE: 0.67\n    - VAE: 0.69\n  - This evaluates the models' performance when only the content space is utilized.\n\n- **Style Space (s):**\n  - Yelp: \n    - DAE: 0.97\n    - VAE: 0.97\n  - Amazon:\n    - DAE: 0.82\n    - VAE: 0.81\n  - This evaluates the models' performance when only the style space is utilized.\n\n- **Complete Space ([s; c]):**\n  - Yelp:\n    - DAE: 0.97\n    - VAE: 0.97\n  - Amazon:\n    - DAE: 0.82\n    - VAE: 0.81\n  - This assesses the models' performance when both content and style spaces are used.\n\nThe results indicate that utilizing style space (and complete space, which includes style space) yields the highest performance on both datasets, particularly for the Yelp dataset."}
{"layout": 104, "type": "image", "page_idx": 5, "img_path": "layout_images/P19-1041_2.jpg", "img_caption": "Figure 2: t-SNE plots of the disentangled style and con- tent spaces on Yelp (with all auxiliary losses). ", "bbox": [306, 158, 526, 307], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "Style Space Content Space\n(a) DAE b F\n\n(b) VAE\n\n", "vlm_text": "The image shows t-SNE plots that visualize the disentangled style and content spaces for Yelp data. There are two rows corresponding to different models: DAE (Denoising Autoencoder) and VAE (Variational Autoencoder). Each row contains two plots, one for style space and one for content space.\n\n- In the style space plots (left column), data points are represented using crosses and are color-coded as either 'neg' (negative, in blue) or 'pos' (positive, in red). For DAE, the points are separated into distinct clusters for each class, while in VAE, the points are also noticeably clustered but less distinctly separated compared to DAE.\n\n- In the content space plots (right column), both models appear to have overlapping clusters of 'neg' and 'pos' points, suggesting less separation in this space. The content space is more uniformly distributed than the style space in both models.\n\nOverall, the figure is demonstrating the ability of DAE and VAE to separate the data into different style and content representations, as visualized by t-SNE plots."}
{"layout": 105, "type": "text", "text": "train separate logistic regression sentiment clas- siﬁers on different latent spaces, and report their classiﬁcation accuracy in Table  1 . ", "page_idx": 5, "bbox": [307, 329.8000183105469, 525, 370.04449462890625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 106, "type": "text", "text": "We see the 128-dimensional content vector    $_c$   is not particularly discriminative for style. Its accu- racy is slightly better than majority guess. How- ever, the 8-dimensional style vector  s , despite its low dimensionality, achieves substantially higher style classiﬁcation accuracy. When combining content and style vectors, we observe no further improvement. These results verify the effective- ness of our disentangling approach, as the style space contains style information, whereas the con- tent space does not. ", "page_idx": 5, "bbox": [307, 370.90301513671875, 525, 519.54052734375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 107, "type": "text", "text": "We show t-SNE plots ( van der Maaten and Hin- ton ,  2008 ) for both DAE and VAE in Figure  2 . As seen, sentences with different styles are no- ticeably separated in a clean manner in the style space (LHS), but are indistinguishable in the con- tent space (RHS). It is also evident that the latent space learned by VAE is considerably smoother and more continuous than the one learned by DAE. ", "page_idx": 5, "bbox": [307, 520.3991088867188, 525, 628.3895263671875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 108, "type": "text", "text": "4.4 Exp. II: Non-Parallel Text Style Transfer ", "text_level": 1, "page_idx": 5, "bbox": [307, 640, 523, 652], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 109, "type": "text", "text": "In this experiment, we apply the disentangled la- tent space to sentiment-transfer text generation. ", "page_idx": 5, "bbox": [307, 657.5861206054688, 525, 684.280517578125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 110, "type": "text", "text": "Metrics.  We evaluate competing models based on (1) style transfer accuracy, (2) content preser- vation, and (3) quality of generated language. The evaluation of sentence generation has proven to be difﬁcult in contemporary literature, so we adopt a few automatic metrics and use human judgment as ", "page_idx": 5, "bbox": [307, 684.7473754882812, 525, 766.0315551757812], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 111, "type": "text", "text": "Style-Transfer Accuracy  ( STA ): We follow most previous work ( Hu et al. ,  2017 ;  Shen et al. ,  2017 ; Fu et al. ,  2018 ) and train a separate convolutional neural network (CNN) to predict the sentiment of a sentence ( Kim ,  2014 ), which is then used to ap- proximate the style transfer accuracy. In other words, we report the CNN classiﬁer’s accuracy on the style-transferred sentences, considering the target style to be the ground-truth. While the style classiﬁer itself may not be perfect, it achieves a reasonable sentiment accuracy on the validation sets  $(97\\%$  for Yelp;  $82\\%$  for Amazon).Thus,it provides a quantitative way of evaluating the strength of style transfer. ", "page_idx": 6, "bbox": [71, 78.60728454589844, 290, 268.2845764160156], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 112, "type": "text", "text": "Cosine Similarity  ( CS ): We followed  Fu et al. ( 2018 ) and computed the cosine measure be- tween source and generated sentence embeddings, which are the concatenation of  min ,  max , and mean  of word embeddings (sentiment words re- moved). This provides a rough estimation of con- tent preservation. ", "page_idx": 6, "bbox": [71, 270.0583801269531, 290, 364.8916320800781], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 113, "type": "text", "text": "Word Overlap  ( WO ): We ﬁnd that cosine simi- larity, although correlated to human judgment, is not a sensitive measure. Instead, we propose a simple and effective measure that counts the un- igram word overlap rate of the original sentence  x and the style-transferred sentence  y , computed by  $\\frac{c o u n t(\\mathbf{x}\\cap\\mathbf{y})}{c o u n t(\\mathbf{x}\\cup\\mathbf{y})}$  . Here, we exclude both stopwords and ∪ sentiment words. ", "page_idx": 6, "bbox": [71, 366.6664123535156, 290, 476.20947265625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 114, "type": "text", "text": "Perplexity  ( PPL ): We use a trigram Kneser– Ney (KN,  Kneser and Ney ,  1995 ) language model as a quantitative and automated metric to evaluate the ﬂuency of a sentence. It estimates the empiri- cal distribution of trigrams in a corpus, and com- putes the perplexity of a test sentence. We trained the language model on the respective datasets, and report PPL on the generated sentences. A smaller PPL indicates more ﬂuent sentences. ", "page_idx": 6, "bbox": [71, 477.9832763671875, 290, 599.9144897460938], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 115, "type": "text", "text": "Geometric Mean  ( GM ): We use the geometric mean of STA, WO, and 1/PPL—reﬂecting trans- fer strength, content preservation, and ﬂuency, respectively—to obtain an aggregated score con- sidering all aspects. Notice that a smaller PPL is desired; thus, we use 1/PPL when computing GM. Also, cosine similarity (CS) is not included, because it is insensitive yet repetitive with word overlap (WO). Here, we adopt the geometric mean so that the scale of each metric does not inﬂuence the judgment. ", "page_idx": 6, "bbox": [71, 601.6893920898438, 290, 750.719482421875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 116, "type": "text", "text": "Manual Evaluation:  Despite the above auto- matic metrics, we also conduct human evaluations to further conﬁrm the performance of our model. This was done on the Yelp dataset only, due to the amount of manual effort involved. We asked 6 human annotators to rate each sentence on a 1–5 Likert scale ( Stent et al. ,  2005 ) in terms of transfer strength ( TS ), content preservation ( CP ), and language quality ( LQ ). This evaluation was conducted in a strictly blind fashion: samples ob- tained from all evaluated models were randomly shufﬂed, so that the annotator was unaware of which model generated a particular sentence. The inter-rater agreement—as measured by Krippen- dorff’s alpha ( Klaus ,  2004 ) for our Likert scale ratings—is 0.74, 0.68, and 0.72 for these three as- pects, respectively. According to  Klaus  ( 2004 ), this is an acceptable inter-rater agreement. We also computed the geometric mean ( GM ) to ob- tain an aggregated score. ", "page_idx": 6, "bbox": [82, 752.8860473632812, 290, 766.031494140625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 117, "type": "text", "text": "", "page_idx": 6, "bbox": [307, 63.68708038330078, 525, 320.7186584472656], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 118, "type": "text", "text": "Overall performance. We compare our ap- proach with previous state-of-the-art work in Ta- ble  2 . For competing methods, we quote re- sults from existing papers whenever possible. In some studies, the authors have released their style- transferred sentences, and we tested them with our metrics. A caveat is that this may involve a different data split, providing a rough (but unbi- ased) comparison. For others, we re-evaluated the model using publicly available code. We sought comparison with  Hu et al.  ( 2017 ), but unfortu- nately could not ﬁnd publicly available code. In- stead we sought performance comparisons of their model in subsequent work, and found that, accord- ing to the human evaluation in  Shen et al.  ( 2017 ), Hu et al.  ( 2017 ) is comparable but slightly worse than  Shen et al.  ( 2017 ). The latter is compared with our model in terms of both automatic metrics and human evaluation. ", "page_idx": 6, "bbox": [307, 323.6644592285156, 525, 581.0877685546875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 119, "type": "text", "text": "We see in Table  2  a clear trade-off between style transfer and content preservation, as they are con- tradictory goals. Especially, a few models have a transfer accuracy lower than  $50\\%$  . They are shown in gray, and not the focus of the comparison, be- cause the system cannot achieve the goal of style transfer most of the time. ", "page_idx": 6, "bbox": [307, 584.4263305664062, 525, 678.8667602539062], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 120, "type": "text", "text": "Our method achieves high style-transfer accu- racy (STA) in both experiments. On the Yelp dataset, it outperforms previous methods by more than   $7\\%$  , whereas on Amazon, VAE is   $1\\%$   lower than  Tsvetkov et al.  ( 2018 ), ranking second. ", "page_idx": 6, "bbox": [307, 682.2052612304688, 525, 749.5477294921875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 121, "type": "text", "text": "Our approach achieves high content preserva- ", "page_idx": 6, "bbox": [318, 752.8862915039062, 525, 766.03173828125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 122, "type": "table", "page_idx": 7, "img_path": "layout_images/P19-1041_3.jpg", "bbox": [95, 62, 503, 169], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "Model Yelp Dataset Amazon Dataset\n\nSTAT | CSt | wot | PPL! | GMt || STAT] CS\" | WOT | PPL? | GMT\nStyle-Embedding (Fu et al., 2018) 0.18 | 0.96 | 0.67 124 | 0.10 0.401 | 0.937 ] 0.36 32 0.17\nCross-Alignment (Shen et al., 2017) 0.787 | 0.89 | 0.21 95) 0.12 0.61 0.89 0.02 202 0.04\nMulti-Decoder (Zhao et al., 2018) 0.827 | 0.88 | 0.27 85 0.14 0.55 0.93 0.17 15 0.11\nDel-Ret-Gen (Li et al., 2018) 0.86 | 0.94 | 0.52 70 0.19 0.43 0.98 0.80 65 0.17\nBackTranslate (Tsvetkov et al., 2018) 0.85 | 0.83 | 0.08 206 0.07 0.83 0.82 0.02 115 0.05\nCycle-RL (Xu et al., 2018) * 0.80 | 0.92 | 0.43 470 0.09 0.72 0.91 0.22 332 0.08\nOurs (DAE) 0.88 | 0.92 | 0.55 52: 0.21 0.72 0.92 0.35 73 0.15\nOurs (VAE) 0.93 | 0.90 | 0.47 32 0.24 0.82 0.90 | 0.20 63 0.14\n\n", "vlm_text": "The table compares various models based on their performance on the Yelp and Amazon Datasets. It lists models with metrics including STA, CS, WO, PPL, and GM, with some differences between the two datasets:\n\n- **Models**: Various models like Style-Embedding, Cross-Alignment, Multi-Decoder, Del-Ret-Gen, BackTranslate, Cycle-RL, and two variants of the author's model (DAE and VAE).\n\n- **Metrics**:\n  - **STA↑**: Style Transfer Accuracy (higher is better)\n  - **CS↑**: Content Similarity (higher is better)\n  - **WO↑**: Word Overlap (higher is better)\n  - **PPL↓**: Perplexity (lower is better)\n  - **GM↑**: Geometric Mean (higher is better)\n\nThe metrics are presented for both the Yelp and Amazon datasets, being highlighted for certain models based on performance. Bold values indicate the best performance in the respective columns."}
{"layout": 123, "type": "text", "text": "Table 2: Performance of text style transfer.  STA:  Style transfer accuracy.  CS:  Cosine similarity.  WO:  Word overlap rate.  PPL:  Perplexity.  GM:  Geometric mean. The larger ↑ (or lower ↓ ), the better.   † Quoted from previous papers (with the same data split). ‡ Involving custom data splits, providing a rough (but unbiased) comparison. Others are based on our replication, and we use published code whenever possible. We achieve 0.809 and 0.835 transfer accuracy on the Yelp dataset, close to the results in  Shen et al.  ( 2017 ) and  Zhao et al.  ( 2018 ), respectively, showing that our replication is fair. Gray numbers show that a method fails to transfer style most of the time. ", "page_idx": 7, "bbox": [72, 178.5099334716797, 525, 250.6495819091797], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 124, "type": "table", "page_idx": 7, "img_path": "layout_images/P19-1041_4.jpg", "table_caption": "Table 3: Manual evaluation on the Yelp dataset. ", "bbox": [85, 269, 276, 355], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "Model TS | CP | LQ || GM\nFu et al. (2018) 1.67 | 3.84 | 3.66 || 2.86\nShen et al. (2017) || 3.63 | 3.07 | 3.08 |} 3.25\nZhao et al. (2018) |} 3.55 | 3.09 | 3.77 || 3.46\nOurs (DAE) 3.67 | 3.64 | 4.19 || 3.83\nOurs (VAE) 4.32 | 3.73 | 4.48 || 4.16\n\n", "vlm_text": "The table presents the performance of different models evaluated based on four criteria: TS, CP, LQ, and GM. Here's a breakdown of the table content:\n\n1. **Model Names:**\n   - Fu et al. (2018)\n   - Shen et al. (2017)\n   - Zhao et al. (2018)\n   - Ours (DAE)\n   - Ours (VAE)\n\n2. **Performance Metrics:**\n   - TS\n   - CP\n   - LQ\n   - GM\n\n3. **Scores for Each Model:**\n   - **Fu et al. (2018):**\n     - TS: 1.67\n     - CP: 3.84\n     - LQ: 3.66\n     - GM: 2.86\n   - **Shen et al. (2017):**\n     - TS: 3.63\n     - CP: 3.07\n     - LQ: 3.08\n     - GM: 3.25\n   - **Zhao et al. (2018):**\n     - TS: 3.55\n     - CP: 3.09\n     - LQ: 3.77\n     - GM: 3.46\n   - **Ours (DAE):**\n     - TS: 3.67\n     - CP: 3.64\n     - LQ: 4.19\n     - GM: 3.83\n   - **Ours (VAE):**\n     - TS: 4.32\n     - CP: 3.73\n     - LQ: 4.48\n     - GM: 4.16\n\nBased on the scores, the model \"Ours (VAE)\" achieved the highest scores across all metrics compared to the other models."}
{"layout": 125, "type": "text", "text": "tion as well. Among all the methods that can achieve more than   $50\\%$   transfer accuracy, DAE has the highest word overlap (WO) on Yelp; VAE is also high, although slightly lower than  Li et al. ( 2018 ). On Amazon, the phenomenon is similar. DAE is the best; VAE is  $2\\%$   lower in WO (al- though   $10\\%$   better in transfer accuracy), compared with  $\\mathrm{Xu}$   et al.  ( 2018 ). ", "page_idx": 7, "bbox": [72, 377.98199462890625, 290, 485.9715270996094], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 126, "type": "text", "text": "For language ﬂuency, VAE yields the best PPL in both datasets. It is also noted that, the cycle reinforcement learning (Cycle-RL) approach does not generate ﬂuent sentences ( Xu et al. ,  2018 ). They have unusually high PPL scores, but after reading the samples provided by the authors (via personal email correspondence) we are assured that the sentences obtained by Cycle-RL are less ﬂuent. ", "page_idx": 7, "bbox": [72, 486.863037109375, 290, 608.4014892578125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 127, "type": "text", "text": "When we consider all the above aspects, our approach (either DAE or VAE) has the highest geometric meaning (GM), showing that we have achieved good balance on transfer strength, con- tent preservation, as well as language ﬂuency. ", "page_idx": 7, "bbox": [72, 609.2930908203125, 290, 676.634521484375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 128, "type": "text", "text": "Table  3  presents the results of human evalua- tion on selected methods. Again, we see that the style embedding model ( Fu et al. ,  2018 ) is ineffec- tive as it has a very low transfer strength, and that our method outperforms other baselines in all as- ", "page_idx": 7, "bbox": [72, 677.5260620117188, 290, 744.8685302734375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 129, "type": "table", "page_idx": 7, "img_path": "layout_images/P19-1041_5.jpg", "bbox": [319, 270, 515, 335], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "Objectives STA | CS | WO | PPL || GM\nJane 0.11 [ 0.94 | 0.47 [ 40 [| 0.11\nTaz, Touts) 0.77 | 0.91 | 033 | 41 |] 0.18\nTas, Jadvis) 0.78 | 0.89 | 0.23 | 35 || 0.17\nTats Imus Jaavey [| 0.91 | 0.87 | 0.17 | 23 |f 0.19\nJats Smuisy» Jean» |! 9,93 | 0.90 | 0.47) 32 |) 0.24\n\nJTmui(c)s\n\n", "vlm_text": "The table presents several performance metrics under different objectives for a given task or experiment. Here's a breakdown of the content in the table:\n\n- **Column Headers:**\n  - **Objectives:** Lists the different combinations of objectives used in the experiments.\n  - **STA:** Provides scores related to this specific measure or evaluation criterion.\n  - **CS:** Provides scores for this evaluation criterion.\n  - **WO:** Shows scores for this specific evaluation measure.\n  - **PPL:** Represents Perplexity scores, typically used to evaluate language models.\n  - **GM:** Represents some cumulative or overall metric, possibly a geometric mean.\n\n- **Objectives and their corresponding scores:**\n  1. **\\( J_{AE} \\):**  \n     - STA: 0.11\n     - CS: 0.94 (in bold, indicating a notable or best score)\n     - WO: 0.47\n     - PPL: 40\n     - GM: 0.11\n\n  2. **\\( J_{AE}, J_{mul(s)} \\):**  \n     - STA: 0.77\n     - CS: 0.91\n     - WO: 0.33\n     - PPL: 41\n     - GM: 0.18\n\n  3. **\\( J_{AE}, J_{adv(s)} \\):**  \n     - STA: 0.78\n     - CS: 0.89\n     - WO: 0.23\n     - PPL: 35\n     - GM: 0.17\n\n  4. **\\( J_{AE}, J_{mul(s)}, J_{adv(s)} \\):**  \n     - STA: 0.91\n     - CS: 0.87\n     - WO: 0.17\n     - PPL: 23 (in bold, indicating a notable or best score)\n     - GM: 0.19\n\n  5. **\\( J_{AE}, J_{mul(s)}, J_{adv(s)}, J_{mul(c)}, J_{adv(c)} \\):**  \n     - STA: 0.93 (in bold, indicating a notable or best score)\n     - CS: 0.90\n     - WO: 0.47 (in bold, indicating a notable or best score)\n     - PPL: 32\n     - GM: 0.24 (in bold, indicating a notable or best score)\n\nThe bolded values might indicate the best performance across respective columns, providing insight into which objective combinations are most effective based on each metric."}
{"layout": 130, "type": "text", "text": "Table 4: Ablation tests on Yelp. In all variants, we fol- low the same protocol of style transfer by substituting an empirical estimate of the target style vector. ", "page_idx": 7, "bbox": [307, 344.4485778808594, 525, 380.364501953125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 131, "type": "text", "text": "pects. The results are consistent with Table  2 . This also implies that the automatic metrics we used are reasonable, and could be extrapolated to different models; it also shows consistent evidence of the effectiveness of our approach. ", "page_idx": 7, "bbox": [307, 408.05596923828125, 525, 475.3984680175781], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 132, "type": "text", "text": "Ablation Test. We conducted ablation tests on the Yelp dataset, and show results in Table  4 . With    $J_{\\mathrm{AE}}$   only, we cannot achieve reasonable style transfer accuracy by substituting an empirically estimated style vector of the target style. This is because the style and content spaces would not be disentangled spontaneously with the autoencoding loss alone. With either  ${\\cal J}_{\\mathrm{multi}}(\\mathrm{s})$   or    $J_{\\mathrm{adv(s)}}$  , the model achieves reasonable transfer accuracy and cosine similarity. Combining them together improves the transfer accuracy to  $90\\%$  , outperforming previous methods by a margin of  $5\\%$   (Table  2 ). This shows that the multi-task loss and the adversarial loss work in different ways. Our insight of combining the two auxiliary losses is a simple yet effective way of disentangling latent space. ", "page_idx": 7, "bbox": [307, 478.4592590332031, 525, 695.2354736328125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 133, "type": "text", "text": "On the other hand,    ${\\cal J}_{\\mathrm{multi}(\\mathrm{s})}$   and    $J_{\\mathrm{adv(s)}}$   only reg- ularize the style information, leading to gradual drop of content preserving scores. Then, we use another insight of introducing content-oriented auxiliary losses,    ${\\cal J}_{\\mathrm{mul(c)}}$   and    $J_{\\mathrm{adv}(\\mathrm{c})}$  , based on BoW features, which regularize the content information in the same way as style. By incorporating all these auxiliary losses, we achieve high transfer ac- curacy, high content preservation, as well as high language ﬂuency. ", "page_idx": 7, "bbox": [307, 698.6890258789062, 525, 767.70947265625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 134, "type": "text", "text": "", "page_idx": 8, "bbox": [71, 63.68701934814453, 290, 131.02951049804688], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 135, "type": "text", "text": "5 Conclusion and Future Work ", "text_level": 1, "page_idx": 8, "bbox": [70, 143, 240, 156], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 136, "type": "text", "text": "In this paper, we propose an effective approach for disentangling style and content latent spaces. We systematically combine multi-task and adver- sarial objectives to separate content and style from each other, where we also propose to approximate content information with bag-of-words features of style-neutral, non-stopword vocabulary. ", "page_idx": 8, "bbox": [71, 165.09205627441406, 290, 259.5325622558594], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 137, "type": "text", "text": "Both qualitative and quantitative experiments show that the latent space is indeed separated into style and content parts. The disentangled space can be directly applied to text style-transfer tasks. Our method achieves high style-transfer strength, high content-preservation scores, as well as high language ﬂuency, compared with previous work. ", "page_idx": 8, "bbox": [71, 260.361083984375, 290, 354.8025817871094], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 138, "type": "text", "text": "Our approach can be naturally extended to non- categorical styles, because our style feature is en- coded from the input sentence. Non-categorical styles cannot be easily handled by ﬁxed style embeddings or style-speciﬁc decoders ( Fu et al. , 2018 ).  Bao et al.  ( 2019 ) have successfully shown that the syntax and semantics of a sentence can be disentangled from each other. ", "page_idx": 8, "bbox": [71, 355.631103515625, 290, 463.6216125488281], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 139, "type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 8, "bbox": [72, 476, 166, 488], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 140, "type": "text", "text": "We thank all reviewers for insightful comments. This work was supported in part by the NSERC grant RGPIN-261439-2013 and an Amazon Re- search Award. We would also like to acknowl- edge NVIDIA Corporation for the donated Titan Xp GPU. ", "page_idx": 8, "bbox": [71, 497.6831359863281, 290, 578.5745849609375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 141, "type": "text", "text": "References ", "text_level": 1, "page_idx": 8, "bbox": [71, 604, 128, 615], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 142, "type": "text", "text": "2017.  Wasserstein generative adversarial networks . In  ICML , pages 214–223. Hareesh Bahuleyan, Lili Mou, Olga Vechtomova, and Pascal Poupart. 2018. Variational attention for sequence-to-sequence models . In  COLING , pages 1672–1682. Georgios Balikas, Simon Moura, and Massih-Reza Amini. 2017. Multitask learning for ﬁne-grained twitter sentiment analysis . In  SIGIR , pages 1005– 1008. ", "page_idx": 8, "bbox": [71, 633.792724609375, 290, 765.7656860351562], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 143, "type": "text", "text": "Yu Bao, Hao Zhou, Shujian Huang, Lei Li, Lili Mou, Olga Vechtomova, XIN-YU DAI, and Jiajun CHEN. 2019. Generating sentences from disentangled syn- tactic and semantic spaces. In  ACL . Samuel R. Bowman, Luke Vilnis, Oriol Vinyals, An- drew M. Dai, Rafal Jozefowicz, and Samy Ben- gio. 2016.  Generating sentences from a continuous space . In  CoNLL , pages 10–21. Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. 2016.  Infogan: Interpretable representation learning by information maximizing generative adversarial nets . In  NIPS , pages 2172–2180. Kyunghyun Cho, Bart van Merrienboer, Caglar Gul- cehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using RNN encoder-decoder for statistical machine translation . In  EMNLP , pages 1724–1734. Alexis Conneau, Guillaume Lample, Marc’Aurelio Ranzato, Ludovic Denoyer, and Herv´ e J´ egou. 2018. Word translation without parallel data . In  ICLR . Jessica Ficler and Yoav Goldberg. 2017.  Controlling linguistic style aspects in neural language genera- tion . In  Proc. Workshop on Stylistic Variation , pages 94–104. Zhenxin Fu, Xiaoye Tan, Nanyun Peng, Dongyan Zhao, and Rui Yan. 2018.  Style transfer in text: Ex- ploration and evaluation . In  AAAI , pages 663–670. Leon A. Gatys, Alexander S. Ecker, and Matthias Bethge. 2016.  Image style transfer using convolu- tional neural networks . In  CVPR , pages 2414–2423. Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed, and Alexander Lerchner. 2017. Beta- VAE: Learning basic visual concepts with a con- strained variational framework . In  ICLR . Minqing Hu and Bing Liu. 2004.  Mining and summa- rizing customer reviews . In  KDD , pages 168–177. Zhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan Salakhutdinov, and Eric P. Xing. 2017.  Toward con- trolled generation of text . In  ICML , pages 1587– 1596. Yacine Jernite, Samuel R. Bowman, and David Son- tag. 2017.  Discourse-based objectives for fast un- supervised sentence representation learning .  arXiv , abs/1705.00557. Yoon Kim. 2014.  Convolutional neural networks for sentence classiﬁcation . In  EMNLP , pages 1746– 1751. Diederik P. Kingma and Jimmy Ba. 2014.  Adam: A method for stochastic optimization .  arXiv prerpint arXiv:1412.6980.", "page_idx": 8, "bbox": [307, 64.561767578125, 525, 765.765380859375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 144, "type": "text", "text": "Diederik P. Kingma and Max Welling. 2013. Auto- encoding variational Bayes . arXiv preprint arXiv:1312.6114 . Krippendorff Klaus. 2004.  Content Analysis: An Intro- duction to Its Methodology . Sage Publications. Reinhard Kneser and Hermann Ney. 1995.  Improved backing-off for m-gram language modeling . In ICASSP , pages 181–184. Solomon Kullback and Richard A Leibler. 1951.  On information and sufﬁciency .  The Annals of Mathe- matical Statistics, 22(1):79–86.Guillaume Lample, Alexis Conneau, Ludovic De- noyer, and Marc’Aurelio Ranzato. 2018a. Unsu- pervised machine translation using monolingual cor- pora only . In  ICLR . Guillaume Lample, Myle Ott, Alexis Conneau, Lu- dovic Denoyer, and Marc’Aurelio Ranzato. 2018b. Phrase-based & neural unsupervised machine trans- lation . In  EMNLP , pages 5039–5049. Juncen Li, Robin Jia, He He, and Percy Liang. 2018. Delete, retrieve, generate: A simple approach to sen- timent and style transfer . In  NAACL-HLT , pages 1865–1874. Lajanugen Logeswaran, Honglak Lee, and Samy Ben- gio. 2018.  Content preserving text generation with attribute controls . In  NIPS , pages 5108–5118. Laurens van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-SNE .  JMLR , 9:2579–2605. Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Corrado, and Jeffrey Dean. 2013.  Distributed repre- sentations of words and phrases and their composi- tionality . In  NIPS , pages 3111–3119. Sudha Rao and Joel R. Tetreault. 2018. Dear sir or madam, may I introduce the GYAFC dataset: Cor- pus, benchmarks and metrics for formality style transfer . In  NAACL-HLT , pages 129–140. D. E. Rumelhart, G. E. Hinton, and R. J. Williams. 1986. Learning internal representations by error propagation . In  Parallel Distributed Processing: Explorations in the Microstructure of Cognition , volume 1, pages 318–362.Cicero Nogueira dos Santos, Igor Melnyk, and Inkit Padhi. 2018.  Fighting offensive language on social media with unsupervised text style transfer . In  ACL (Short Papers) , pages 189–194. Tianxiao Shen, Tao Lei, Regina Barzilay, and Tommi S. Jaakkola. 2017.  Style transfer from non-parallel text by cross-alignment . In  NIPS , pages 6833–6844. Amanda Stent, Matthew Marge, and Mohit Singhai. 2005.  Evaluating evaluation methods for generation in the presence of variation . In  CICLing , pages 341– 351. ", "page_idx": 9, "bbox": [71, 64.56158447265625, 290, 765.7651977539062], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 145, "type": "text", "text": "Tijmen Tieleman and Geoffrey Hinton. 2012. Lecture 6.5-rmsprop: Divide the gradient by a running av- erage of its recent magnitude.  COURSERA: Neural Networks for Machine Learning . Yulia Tsvetkov, Alan W. Black, Ruslan Salakhutdi- nov, and Shrimai Prabhumoye. 2018.  Style transfer through back-translation . In  ACL , pages 866–876. Jingjing Xu, Xu Sun, Qi Zeng, Xiaodong Zhang, Xu- ancheng Ren, Houfeng Wang, and Wenjie Li. 2018. Unpaired sentiment-to-sentiment translation: A cy- cled reinforcement learning approach . In  ACL , pages 979–988. Ye Zhang, Nan Ding, and Radu Soricut. 2018. SHAPED: Shared-private encoder-decoder for text style adaptation . In  NAACL-HLT , pages 1528–1538. Junbo Jake Zhao, Yoon Kim, Kelly Zhang, Alexan- der M. Rush, and Yann LeCun. 2018.  Adversarially regularized autoencoders . In  ICML , pages 5897– 5906. ", "page_idx": 9, "bbox": [307, 64.561279296875, 525, 309.69195556640625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 146, "type": "text", "text": "A Qualitative Examples ", "text_level": 1, "page_idx": 9, "bbox": [306, 321, 440, 335], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 147, "type": "text", "text": "Table  5  provides several examples of our style- transfer model. Results show that we can success- fully transfer the sentiment while preserving the content of a sentence. ", "page_idx": 9, "bbox": [307, 341.63543701171875, 525, 395.4289245605469], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 148, "type": "text", "text": "B Effect of the BoW Vocabulary ", "text_level": 1, "page_idx": 9, "bbox": [306, 405, 481, 420], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 149, "type": "text", "text": "Table  6  demonstrates the effect of choosing dif- ferent BoW vocabulary for the auxiliary content losses. As seen, we are able to achieve reasonable performance with any of these vocabularies, but using a vocabulary that excludes sentiment words and stopwords performs the best. ", "page_idx": 9, "bbox": [307, 426.9614562988281, 525, 507.85296630859375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 150, "type": "table", "page_idx": 10, "img_path": "layout_images/P19-1041_6.jpg", "table_caption": "Table 5: Examples of style transferred sentence generation. ", "bbox": [72, 121, 525, 471], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "Original (Positive)\n\nDAE Transferred (Negative)\n\nVAE Transferred (Negative)\n\nthe food is excellent and the\n\nthe food was a bit bad but the\n\nthe food was bland and i am not\n\nservice is exceptional staff was exceptional thrilled with this\n\nthe waitresses are friendly and || the guys are rude and helpful the waitresses are rude and are\nhelpful lazy\n\nthe restaurant itself is romantic || the restaurant itself is awkward | the restaurant itself was dirty\nand quiet and quite crowded\n\ngreat deal horrible deal no deal\n\nboth times i have eaten the\nlunch buffet and it was out-\nstanding\n\ntheir burgers were decent but\nthe eggs were not the consis-\ntency\n\nboth times i have eaten here the\nfood was mediocre at best\n\nOriginal (Negative)\n\nDAE Transferred (Positive)\n\nVAE Transferred (Positive)\n\nthe desserts were very bland\n\nthe desserts were very good\n\nthe desserts were very good\n\nit was a bed of lettuce and\nspinach with italian\nmeats and cheeses\n\nsome\n\nit was a beautiful setting and\njust had a large variety of ger-\nman flavors\n\nit was a huge assortment of fla-\nvors and italian food\n\nthe people behind the counter\nwere not friendly whatsoever\n\nthe best selection behind the\nregister and service presenta-\ntion\n\nthe people behind the counter is\nfriendly caring\n\nthe interior is old and generally\nfalling apart\n\nthe decor is old and now per-\nfectly\n\nthe interior is old and noble\n\nthey are clueless\n\nthey are stoked\n\nthey are genuinely profession-\nals\n\n", "vlm_text": "The table presents transformations of restaurant reviews using two different methods: DAE Transferred and VAE Transferred. It's divided into two sections:\n\n1. **Original (Positive)**: Positive reviews and their transformations to negative sentiment.\n   - DAE Transferred (Negative)\n   - VAE Transferred (Negative)\n\n2. **Original (Negative)**: Negative reviews and their transformations to positive sentiment.\n   - DAE Transferred (Positive)\n   - VAE Transferred (Positive)\n\nEach row compares the original sentiment with how it's changed by the two transfer methods. For instance, a positive review like \"the food is excellent and the service is exceptional\" gets shifted to neutral or negative tones by these methods."}
{"layout": 151, "type": "table", "page_idx": 10, "img_path": "layout_images/P19-1041_7.jpg", "table_caption": "Table 6: Analysis of the BoW vocabulary. ", "bbox": [88, 604, 508, 699], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "BoW Vocabulary STA | CS | WO | PPL || GM\nFull corpus vocabulary 0.822 | 0.896 | 0.344 | 30 || 0.21\nVocabulary without sentiment words 0.872 | 0.901 | 0.359 | 30 || 0.22\nVocabulary without stopwords 0.836 | 0.894 | 0.429 | 33 0.22\nVocabulary without stopwords and sentiment words || 0.934 | 0.904 | 0.473 | 32 || 0.24\n\n", "vlm_text": "The table presents data related to different configurations of Bag of Words (BoW) vocabulary and their corresponding metrics. Here's a breakdown of the table content:\n\n- Column Headers:\n  - **BoW Vocabulary**: Lists the different vocabulary configurations.\n  - **STA**: Numeric metric value for each vocabulary configuration.\n  - **CS**: Another numeric metric value for each configuration.\n  - **WO**: Another numeric metric value for each configuration.\n  - **PPL**: This metric value is presented in a bold font for some configurations.\n  - **GM**: Another metric value for each configuration.\n\n- Rows:\n  - **Full corpus vocabulary**: \n    - STA: 0.822\n    - CS: 0.896\n    - WO: 0.344\n    - PPL: **30**\n    - GM: 0.21\n\n  - **Vocabulary without sentiment words**: \n    - STA: 0.872\n    - CS: 0.901\n    - WO: 0.359\n    - PPL: **30**\n    - GM: 0.22\n\n  - **Vocabulary without stopwords**: \n    - STA: 0.836\n    - CS: 0.894\n    - WO: 0.429\n    - PPL: 33\n    - GM: 0.22\n\n  - **Vocabulary without stopwords and sentiment words**: \n    - STA: **0.934**\n    - CS: **0.904**\n    - WO: **0.473**\n    - PPL: 32\n    - GM: **0.24**\n\nThe table suggests that different BoW vocabulary configurations have different effects on the metrics STA, CS, WO, PPL, and GM. Notably, removing both stopwords and sentiment words seems to yield the best results for STA, CS, WO, and GM metrics."}
