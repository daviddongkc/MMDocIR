{"page": 0, "image_path": "doc_images/P19-1561_0.jpg", "ocr_text": "Combating Adversarial Misspellings with Robust Word Recognition\n\nDanish Pruthi\n\nBhuwan Dhingra\n\nZachary C. Lipton\n\nCarnegie Mellon University\nPittsburgh, USA\n{ddanish, bdhingra}@cs.cmu.edu, zlipton@cmu.edu\n\nAbstract\n\nTo combat adversarial spelling mistakes, we\npropose placing a word recognition model in\nfront of the downstream classifier. Our word\nrecognition models build upon the RNN semi-\ncharacter architecture, introducing several new\nbackoff strategies for handling rare and un-\nseen words. Trained to recognize words cor-\nrupted by random adds, drops, swaps, and\nkeyboard mistakes, our method achieves 32%\nrelative (and 3.3% absolute) error reduction\nover the vanilla semi-character model. No-\ntably, our pipeline confers robustness on the\ndownstream classifier, outperforming both ad-\nversarial training and off-the-shelf spell check-\ners. Against a BERT model fine-tuned for sen-\ntiment analysis, a single adversarially-chosen\ncharacter attack lowers accuracy from 90.3%\nto 45.8%. Our defense restores accuracy to\n75%!. Surprisingly, better word recognition\ndoes not always entail greater robustness. Our\nanalysis reveals that robustness also depends\nupon a quantity that we denote the sensitivity.\n\n1 Introduction\n\nDespite the rapid progress of deep learning tech-\nniques on diverse supervised learning tasks, these\nmodels remain brittle to subtle shifts in the data\ndistribution. Even when the permissible changes\nare confined to barely-perceptible perturbations,\ntraining robust models remains an open challenge.\nFollowing the discovery that imperceptible attacks\ncould cause image recognition models to misclas-\nsify examples (Szegedy et al., 2013), a veritable\nsub-field has emerged in which authors iteratively\npropose attacks and countermeasures.\n\nFor all the interest in adversarial computer vi-\nsion, these attacks are rarely encountered out-\nside of academic research. However, adversarial\n\n‘All code for our defenses, attacks, and baselines is\n\navailable at https://github.com/danishpruthi/\nAdversarial-Misspellings\n\nAlteration Movie Review Label\nOriginal A triumph, relentless and beautiful +\nnema in its downbeat darkness\nSwa A triumph, relentless and beuatiful _\nP in its downbeat darkness\nD A triumph, relentless and beautiful\nTOP in its dwnbeat darkness ~\nA triumph, relentless and beautiful\n+ Defense oe : +\nin its downbeat darkness\nA triumph, relentless and beautiful\n+ Defense we .\nin its downbeat darkness\nTable 1: Adversarial spelling mistakes inducing senti-\nment misclassification and word-recognition defenses.\n\nmisspellings constitute a longstanding real-world\nproblem. Spammers continually bombard email\nservers, subtly misspelling words in efforts to\nevade spam detection while preserving the emails’\nintended meaning (Lee and Ng, 2005; Fumera\net al., 2006). As another example, programmatic\ncensorship on the Internet has spurred communi-\nties to adopt similar methods to communicate sur-\nreptitiously (Bitso et al., 2013).\n\nIn this paper, we focus on adversarially-chosen\nspelling mistakes in the context of text classifica-\ntion, addressing the following attack types: drop-\nping, adding, and swapping internal characters\nwithin words. These perturbations are inspired by\npsycholinguistic studies (Rawlinson, 1976; Matt\nDavis, 2003) which demonstrated that humans can\ncomprehend text altered by jumbling internal char-\nacters, provided that the first and last characters of\neach word remain unperturbed.\n\nFirst, in experiments addressing both BiLSTM\nand fine-tuned BERT models, comprising four\ndifferent input formats: word-only, char-only,\nword+char, and word-piece (Wu et al., 2016), we\ndemonstrate that an adversary can degrade a clas-\nsifier’s performance to that achieved by random\nguessing. This requires altering just two charac-\n\n5582\n\nProceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5582-5591\nFlorence, Italy, July 28 - August 2, 2019. ©2019 Association for Computational Linguistics\n", "vlm_text": "Combating Adversarial Misspellings with Robust Word Recognition \nDanish Pruthi Bhuwan Dhingra Zachary C. Lipton Carnegie Mellon University Pittsburgh, USA { ddanish, bdhingra } @cs.cmu.edu ,  zlipton@cmu.edu \nAbstract \nTo combat adversarial spelling mistakes, we propose placing a word recognition model in front of the downstream classiﬁer. Our word recognition models build upon the RNN semi- character architecture, introducing several new backoff  strategies for handling rare and un- seen words. Trained to recognize words cor- rupted by random adds, drops, swaps, and keyboard mistakes, our method achieves    $32\\%$  relative (and    $3.3\\%$   absolute) error reduction over the vanilla semi-character model. No- tably, our pipeline confers robustness on the downstream classiﬁer, outperforming both ad- versarial training and off-the-shelf spell check- ers. Against a BERT model ﬁne-tuned for sen- timent analysis, a single adversarially-chosen character attack lowers accuracy from    $90.3\\%$  to    $45.8\\%$  . Our defense restores accuracy to  $75\\%^{1}$  . Surprisingly, better word recognition does not always entail greater robustness. Our analysis reveals that robustness also depends upon a quantity that we denote the  sensitivity . \n1 Introduction \nDespite the rapid progress of deep learning tech- niques on diverse supervised learning tasks, these models remain brittle to subtle shifts in the data distribution. Even when the permissible changes are conﬁned to barely-perceptible perturbations, training robust models remains an open challenge. Following the discovery that imperceptible attacks could cause image recognition models to misclas- sify examples ( Szegedy et al. ,  2013 ), a veritable sub-ﬁeld has emerged in which authors iteratively propose attacks and countermeasures. \nFor all the interest in adversarial computer vi- sion, these attacks are rarely encountered out- side of academic research. However, adversarial \nThe table displays different versions of a movie review under the category \"Alteration\" with corresponding \"Label\" assigned to each one. Here's a breakdown of the table:\n\n1. **Original:**\n   - Movie Review: \"A triumph, relentless and beautiful in its downbeat darkness\"\n   - Label: +\n\n2. **Swap:**\n   - Movie Review: \"A triumph, relentless and beautiful in its downbeat darkness\" (the word \"beautiful\" is in red)\n   - Label: −\n\n3. **Drop:**\n   - Movie Review: \"A triumph, relentless and beautiful in its dwnbeat darkness\" (the word \"downbeat\" is misspelled as \"dwnbeat\")\n   - Label: −\n\n4. **+ Defense:**\n   - Movie Review: \"A triumph, relentless and beautiful in its downbeat darkness\" (the word \"beautiful\" is in blue)\n   - Label: +\n\n5. **+ Defense:**\n   - Movie Review: \"A triumph, relentless and beautiful in its downbeat darkness\" (the word \"downbeat\" is in blue)\n   - Label: +\n\nThus, the table explores how different text alterations in the movie review affect its label, with the \"+ Defense\" entries suggesting a corrected or enhanced version that maintains the positive label despite potential textual manipulations.\nmisspellings constitute a  longstanding real-world problem . Spammers continually bombard email servers, subtly misspelling words in efforts to evade spam detection while preserving the emails’ intended meaning ( Lee and Ng ,  2005 ;  Fumera et al. ,  2006 ). As another example, programmatic censorship on the Internet has spurred communi- ties to adopt similar methods to communicate sur- reptitiously ( Bitso et al. ,  2013 ). \nIn this paper, we focus on adversarially-chosen spelling mistakes in the context of text classiﬁca- tion, addressing the following attack types: drop- ping, adding, and swapping internal characters within words. These perturbations are inspired by psycho linguistic studies ( Rawlinson ,  1976 ;  Matt Davis ,  2003 ) which demonstrated that humans can comprehend text altered by jumbling internal char- acters, provided that the ﬁrst and last characters of each word remain unperturbed. \nFirst, in experiments addressing both BiLSTM and ﬁne-tuned BERT models, comprising four different input formats: word-only, char-only, word+char, and word-piece ( Wu et al. ,  2016 ), we demonstrate that an adversary can degrade a clas- siﬁer’s performance to that achieved by random guessing.  This requires altering just two charac- ters per sentence . Such modiﬁcations might ﬂip words either to a different word in the vocabu- lary or, more often, to the out-of-vocabulary to- ken  UNK  . Consequently, adversarial edits can de- grade a word-level model by transforming the in- formative words to  UNK  . Intuitively, one might suspect that word-piece and character-level mod- els would be less susceptible to spelling attacks as they can make use of the residual word con- text. However, our experiments demonstrate that character and word-piece models are in fact  more vulnerable.  We show that this is due to the ad- versary’s effective capacity for ﬁner grained ma- nipulations on these models. While against a word-level model, the adversary is mostly lim- ited to  UNK  -ing words, against a word-piece or character-level model, each character-level add, drop, or swap produces a distinct input, providing the adversary with a greater set of options. "}
{"page": 1, "image_path": "doc_images/P19-1561_1.jpg", "ocr_text": "ters per sentence. Such modifications might flip\nwords either to a different word in the vocabu-\nlary or, more often, to the out-of-vocabulary to-\nken UNK . Consequently, adversarial edits can de-\ngrade a word-level model by transforming the in-\nformative words to UNK . Intuitively, one might\nsuspect that word-piece and character-level mod-\nels would be less susceptible to spelling attacks\nas they can make use of the residual word con-\ntext. However, our experiments demonstrate that\ncharacter and word-piece models are in fact more\nvulnerable. We show that this is due to the ad-\nversary’s effective capacity for finer grained ma-\nnipulations on these models. While against a\nword-level model, the adversary is mostly lim-\nited to UNK -ing words, against a word-piece or\ncharacter-level model, each character-level add,\ndrop, or swap produces a distinct input, providing\nthe adversary with a greater set of options.\n\nSecond, we evaluate first-line techniques in-\ncluding data augmentation and adversarial train-\ning, demonstrating that they offer only marginal\nbenefits here, e.g., a BERT model achieving 90.3\naccuracy on a sentiment classification task, is\ndegraded to 64.1 by an adversarially-chosen 1-\ncharacter swap in the sentence, which can only be\nrestored to 69.2 by adversarial training.\n\nThird (our primary contribution), we propose\na task-agnostic defense, attaching a word recog-\nnition model that predicts each word in a sen-\ntence given a full sequence of (possibly mispelled)\ninputs. The word recognition model’s outputs\ncomprise the input to a downstream classification\nmodel. Our word recognition models build upon\nthe RNN-based semi-character word recognition\nmodel due to Sakaguchi et al. (2017). While our\nword recognizers are trained on domain-specific\ntext from the task at hand, they often predict UNK\nat test time, owing to the small domain-specific\nvocabulary. To handle unobserved and rare words,\nwe propose several backoff strategies including\nfalling back on a generic word recognizer trained\non a larger corpus. Incorporating our defenses,\nBERT models subject to 1-character attacks are\nrestored to 88.3, 81.1, 78.0 accuracy for swap,\ndrop, add attacks respectively, as compared to\n69.2, 63.6, and 50.0 for adversarial training\n\nFourth, we offer a detailed qualitative analysis,\ndemonstrating that a low word error rate alone is\ninsufficient for a word recognizer to confer robust-\nness on the downstream task. Additionally, we\n\nfind that it is important that the recognition model\nsupply few degrees of freedom to an attacker. We\nprovide a metric to quantify this notion of sensi-\ntivity in word recognition models and study its re-\nlation to robustness empirically. Models with low\nsensitivity and word error rate are most robust.\n\n2 Related Work\n\nSeveral papers address adversarial attacks on NLP\nsystems. Changes to text, whether word- or\ncharacter-level, are all perceptible, raising some\nquestions about what should rightly be considered\nan adversarial example (Ebrahimi et al., 2018b;\nBelinkov and Bisk, 2018). Jia and Liang (2017)\naddress the reading comprehension task, show-\ning that by appending distractor sentences to the\nend of stories from the SQuAD dataset (Rajpurkar\net al., 2016), they could cause models to output in-\ncorrect answers. Inspired by this work, Glockner\net al. (2018) demonstrate an attack that breaks en-\ntailment systems by replacing a single word with\neither a synonym or its hypernym. Recently, Zhao\net al. (2018) investigated the problem of producing\nnatural-seeming adversarial examples, noting tha\nadversarial examples in NLP are often ungram-\nmatical (Li et al., 2016).\n\nIn related work on character-level attacks,\nEbrahimi et al. (2018b,a) explored gradient-based\nmethods to generate string edits to fool classifica-\ntion and translation systems, respectively. While\ntheir focus is on efficient methods for generat-\ning adversaries, ours is on improving the wors\ncase adversarial performance. Similarly, Belinkov\nand Bisk (2018) studied how synthetic and natu-\nral noise affects character-level machine transla-\ntion. They considered structure invariant represen-\ntations and adversarial training as defenses against\nsuch noise. Here, we show that an auxiliary word\nrecognition model, which can be trained on unla-\nbeled data, provides a strong defense.\n\nSpelling correction (Kukich, 1992) is often\nviewed as a sub-task of grammatical error correc-\nion (Ng et al., 2014; Schmaltz et al., 2016). Clas-\nsic methods rely on a source language model and a\nnoisy channel model to find the most likely correc-\nion for a given word (Mays et al., 1991; Brill and\nMoore, 2000). Recently, neural techniques have\nbeen applied to the task (Sakaguchi et al., 2017;\nLi et al., 2018), which model the context and or-\nhography of the input together. Our work extends\nhe ScRNN model of Sakaguchi et al. (2017).\n\n5583\n", "vlm_text": "\nSecond, we evaluate ﬁrst-line techniques in- cluding data augmentation and adversarial train- ing, demonstrating that they offer only marginal beneﬁts here, e.g., a BERT model achieving  90 . 3 accuracy on a sentiment classiﬁcation task, is degraded to  64 . 1  by an adversarially-chosen  1 - character swap in the sentence, which can only be restored to  69 . 2  by adversarial training. \nThird (our primary contribution), we propose a task-agnostic defense, attaching a word recog- nition model that predicts each word in a sen- tence given a full sequence of (possibly mispelled) inputs. The word recognition model’s outputs comprise the input to a downstream classiﬁcation model. Our word recognition models build upon the RNN-based semi-character word recognition model due to  Sakaguchi et al.  ( 2017 ). While our word recognizers are trained on domain-speciﬁc text from the task at hand, they often predict  UNK at test time, owing to the small domain-speciﬁc vocabulary. To handle unobserved and rare words, we propose several  backoff  strategies including falling back on a generic word recognizer trained on a larger corpus. Incorporating our defenses, BERT models subject to 1-character attacks are restored to  88 . 3 ,  81 . 1 ,  78 . 0  accuracy for swap, drop, add attacks respectively, as compared to 69 . 2 ,  63 . 6 , and  50 . 0  for adversarial training \nFourth, we offer a detailed qualitative analysis, demonstrating that a low word error rate alone is insufﬁcient for a word recognizer to confer robust- ness on the downstream task. Additionally, we ﬁnd that it is important that the recognition model supply few degrees of freedom to an attacker. We provide a metric to quantify this notion of  sensi- tivity  in word recognition models and study its re- lation to robustness empirically. Models with low sensitivity  and  word error rate are most robust. \n\n2 Related Work \nSeveral papers address adversarial attacks on NLP systems. Changes to text, whether word- or character-level, are all perceptible, raising some questions about what should rightly be considered an adversarial example ( Ebrahimi et al. ,  2018b ; Belinkov and Bisk ,  2018 ).  Jia and Liang  ( 2017 ) address the reading comprehension task, show- ing that by appending  distractor sentences  to the end of stories from the SQuAD dataset ( Rajpurkar et al. ,  2016 ), they could cause models to output in- correct answers. Inspired by this work,  Glockner et al.  ( 2018 ) demonstrate an attack that breaks en- tailment systems by replacing a single word with either a synonym or its hypernym. Recently,  Zhao et al.  ( 2018 ) investigated the problem of producing natural-seeming adversarial examples, noting that adversarial examples in NLP are often ungram- matical ( Li et al. ,  2016 ). \nIn related work on character-level attacks, Ebrahimi et al.  ( 2018b , a ) explored gradient-based methods to generate string edits to fool classiﬁca- tion and translation systems, respectively. While their focus is on efﬁcient methods for generat- ing adversaries, ours is on improving the worst case adversarial performance. Similarly,  Belinkov and Bisk  ( 2018 ) studied how synthetic and natu- ral noise affects character-level machine transla- tion. They considered structure invariant represen- tations and adversarial training as defenses against such noise. Here, we show that an auxiliary word recognition model, which can be trained on unla- beled data, provides a strong defense. \nSpelling correction ( Kukich ,  1992 ) is often viewed as a sub-task of grammatical error correc- tion (  $\\mathrm{Mg}$   et al. ,  2014 ;  Schmaltz et al. ,  2016 ). Clas- sic methods rely on a source language model and a noisy channel model to ﬁnd the most likely correc- tion for a given word ( Mays et al. ,  1991 ;  Brill and Moore ,  2000 ). Recently, neural techniques have been applied to the task ( Sakaguchi et al. ,  2017 ; Li et al. ,  2018 ), which model the context and or- thography of the input together. Our work extends the ScRNN model of  Sakaguchi et al.  ( 2017 ). "}
{"page": 2, "image_path": "doc_images/P19-1561_2.jpg", "ocr_text": "3 Robust Word Recognition\n\nTo tackle character-level adversarial attacks, we\nintroduce a simple two-stage solution, placing a\nword recognition model (W) before the down-\nstream classifier (C’). Under this scheme, all inputs\nare classified by the composed model C'o W. This\nmodular approach, with W and C trained sepa-\nrately, offers several benefits: (i) we can deploy the\nsame word recognition model for multiple down-\nstream classification tasks/models; and (ii) we can\ntrain the word recognition model with larger unla-\nbeled corpora.\n\nAgainst adversarial mistakes, two important\nfactors govern the robustness of this combined\nmodel: W’s accuracy in recognizing misspelled\nwords and W’s sensitivity to adversarial perturba-\ntions on the same input. We discuss these aspects\nin detail below.\n\n3.1 ScRNN with Backoff\n\nWe now describe semi-character RNNs for word\nrecognition, explain their limitations, and suggest\ntechniques to improve them.\n\nScRNN Model Inspired by the psycholinguis-\ntic studies (Matt Davis, 2003; Rawlinson, 1976),\nSakaguchi et al. (2017) proposed a semi-character\nbased RNN (ScRNN) that processes a sentence\nof words with misspelled characters, predict-\ning the correct words at each step. Let s =\n{w, w2,...,Wn} denote the input sentence, a se-\nquence of constituent words w;. Each input word\n(w;) is represented by concatenating (i) a one hot\nvector of the first character (wj1); (ii) a one hot\nrepresentation of the last character (wi, where\nI is the length of word w,); and (iii) a bag of\ncharacters representation of the internal characters\n(Sj wij). ScRNN treats the first and the last\ncharacters individually, and is agnostic to the or-\ndering of the internal characters. Each word, rep-\nresented accordingly, is then fed into a BiLSTM\ncell. At each sequence step, the training target is\nthe correct corresponding word (output dimension\nequal to vocabulary size), and the model is opti-\nmized with cross-entropy loss.\n\nBackoff Variations While Sakaguchi et al.\n(2017) demonstrate strong word recognition per-\nformance, a drawback of their evaluation setup is\nthat they only attack and evaluate on the subset\nof words that are a part of their training vocabu-\nlary. In such a setting, the word recognition per-\n\nformance is unreasonably dependant on the cho-\nsen vocabulary size. In principle, one can design\nmodels to predict (correctly) only a few chosen\nwords, and ignore the remaining majority and still\nreach 100% accuracy. For the adversarial setting,\nrare and unseen words in the wild are particularly\ncritical, as they provide opportunities for the at-\ntackers. A reliable word-recognizer should handle\nthese cases gracefully. Below, we explore different\nways to back off when the SCRNN predicts UNK\n(a frequent outcome for rare and unseen words):\n\ne Pass-through: word-recognizer passes on\nthe (possibly misspelled) word as is.\n\ne Backoff to neutral word: Alternatively,\nnoting that passing UNK -predicted words\nthrough unchanged exposes the downstream\nmodel to potentially corrupted text, we con-\nsider backing off to a neutral word like\n\na’, which has a similar distribution across\nclasses.\n\ne Backoff to background model: We also\nconsider falling back upon a more generic\nword recognition model trained upon a larger,\nless-specialized corpus whenever the fore-\nground word recognition model predicts\nUNK *. Figure | depicts this scenario pic-\ntorially.\n\nEmpirically, we find that the background model\n(by itself) is less accurate, because of the large\nnumber of words it is trained to predict. Thus,\nit is best to train a precise foreground model on\nan in-domain corpus and focus on frequent words,\nand then to resort to a general-purpose background\nmodel for rare and unobserved words. Next, we\ndelineate our second consideration for building ro-\nbust word-recognizers.\n\n3.2. Model Sensitivity\n\nIn computer vision, an important factor determin-\ning the success of an adversary is the norm con-\nstraint on the perturbations allowed to an image\n(|x — x’||oo < ©). Higher values of € lead to\na higher chance of mis-classification for at least\none x’. Defense methods such as quantization (Xu\net al., 2017) and thermometer encoding (Buck-\nman et al., 2018) try to reduce the space of pertur-\nbations available to the adversary by making the\nmodel invariant to small changes in the input.\n\n?Potentially the background model could be trained with\nfull vocabulary so that it never predicts UNK\n\n5584\n", "vlm_text": "3 Robust Word Recognition \nTo tackle character-level adversarial attacks, we introduce a simple two-stage solution, placing a word recognition model   $(W)$   before the down- stream classiﬁer   $(C)$  . Under this scheme, all inputs are classiﬁed by the comp ed mo l  $C\\circ W$  . This modular approach, with  W  and  C  trained sepa- rately, offers several beneﬁts: (i) we can deploy the same word recognition model for multiple down- stream classiﬁcation tasks/models; and (ii) we can train the word recognition model with larger unla- beled corpora. \nAgainst adversarial mistakes, two important factors govern the robustness of this combined model:    $W$  ’s  accuracy  in recognizing misspelled words and    $W$  ’s  sensitivity  to adversarial perturba- tions on the same input. We discuss these aspects in detail below. \n3.1 ScRNN with Backoff \nWe now describe semi-character RNNs for word recognition, explain their limitations, and suggest techniques to improve them. \nScRNN Model Inspired by the psycholinguis- tic studies ( Matt Davis ,  2003 ;  Rawlinson ,  1976 ), Sakaguchi et al.  ( 2017 ) proposed a semi-character based RNN (ScRNN) that processes a sentence of words with misspelled characters, predict- ing the correct words at each step. Let    $s\\_=$   $\\{w_{1},w_{2},.\\,.\\,.\\,,w_{n}\\}$   denote the input sentence, a se- quence of constituent words    $w_{i}$  . Each input word  $(w_{i})$   is represented by concatenating (i) a one hot vector of the ﬁrst character   $(\\mathbf{w_{i1}})$  ; (ii) a one hot representation of the last character (  $\\mathbf{\\dot{w}i l}$  , where  $l$   is the length of word    $w_{i}$  ); and (iii) a bag of characters representation of the internal characters  $\\begin{array}{r l}{(\\sum_{j=2}^{l-1}\\mathbf{w_{ij}})}\\end{array}$  . ScRNN treats the ﬁrst and the last characters individually, and is agnostic to the or- dering of the internal characters. Each word, rep- resented accordingly, is then fed into a BiLSTM cell. At each sequence step, the training target is the correct corresponding word (output dimension equal to vocabulary size), and the model is opti- mized with cross-entropy loss. \nBackoff Variations While  Sakaguchi et al. ( 2017 ) demonstrate strong word recognition per- formance, a drawback of their evaluation setup is that they only attack and evaluate on the subset of words that are a part of their training vocabu- lary. In such a setting, the word recognition per- formance is unreasonably dependant on the cho- sen vocabulary size. In principle, one can design models to predict (correctly) only a few chosen words, and ignore the remaining majority and still reach  $100\\%$   accuracy.  For the adversarial setting, rare and unseen words in the wild are particularly critical, as they provide opportunities for the at- tackers.  A reliable word-recognizer should handle these cases gracefully. Below, we explore different ways to  back off  when the ScRNN predicts  UNK (a frequent outcome for rare and unseen words): \n\n•  Pass-through : word-recognizer passes on the (possibly misspelled) word as is. \n•  Backoff to neutral word : Alternatively, noting that passing UNK  -predicted words through unchanged exposes the downstream model to potentially corrupted text, we con- sider backing off to a neutral word like ‘a’, which has a similar distribution across classes. \n•  Backoff to background model : We also consider falling back upon a more generic word recognition model trained upon a larger, less-specialized corpus whenever the fore- ground word recognition model predicts UNK   2 . Figure  1  depicts this scenario pic- torially. \nEmpirically, we ﬁnd that the background model (by itself) is less accurate, because of the large number of words it is trained to predict. Thus, it is best to train a precise foreground model on an in-domain corpus and focus on frequent words, and then to resort to a general-purpose background model for rare and unobserved words. Next, we delineate our second consideration for building ro- bust word-recognizers. \n3.2 Model Sensitivity \nIn computer vision, an important factor determin- ing the success of an adversary is the norm con- straint on the perturbations allowed to an image  $(||\\mathbf{x}\\,-\\,\\mathbf{x}^{\\prime}||_{\\infty}\\ <\\ \\epsilon)$  . Higher values of    $\\epsilon$   lead to a higher chance of mis-classiﬁcation for at least one  $\\mathbf{x}^{\\prime}$  . Defense methods such as quantization (  $\\mathrm{Xu}$  et al. ,  2017 ) and thermometer encoding ( Buck- man et al. ,  2018 ) try to reduce the space of pertur- bations available to the adversary by making the model invariant to small changes in the input. "}
{"page": 3, "image_path": "doc_images/P19-1561_3.jpg", "ocr_text": "tender yet\n\ntleend| r ylelt !aaciernt| g\n\n1 |) ||\n\nY\n— —\n\nY\n\ntender yet\n\nForeground Model\n\nlacreating and darkly funny fable\n\nflabll e\n\nBackground Model t\n\nSemi-character\nRepresentation\n\n|\n\nlacerating and darkly funny fable\n\nFigure 1: A schematic sketch of our proposed word recognition system, consisting of a foreground and a back-\nground model. We train the foreground model on the smaller, domain-specific dataset, and the background model\non a larger dataset (e.g., the IMDB movie corpus). We train both models to reconstruct the correct word from\nthe orthography and context of the individual words, using synthetically corrupted inputs during training. Subse-\nquently, we invoke the background model whenever the foreground model predicts UNK .\n\nIn NLP, we often get such invariance for free,\ne.g., for a word-level model, most of the pertur-\nbations produced by our character-level adversary\nlead to an UNK at its input. If the model is robust\nto the presence of these UNK tokens, there is little\nroom for an adversary to manipulate it. Character-\nlevel models, on the other hand, despite their supe-\nrior performance in many tasks, do not enjoy such\ninvariance. This characteristic invariance could be\nexploited by an attacker. Thus, to limit the number\nof different inputs to the classifier, we wish to re-\nduce the number of distinct word recognition out-\nputs that an attacker can induce, not just the num-\nber of words on which the model is “fooled”. We\ndenote this property of a model as its sensitivity.\n\nWe can quantify this notion for a word recogni-\ntion system W as the expected number of unique\noutputs it assigns to a set of adversarial pertur-\nbations. Given a sentence s from the set of sen-\ntences S, let A(s) = 51/, 59’,\nset of n perturbations to it under attack type A,\nand let V be the function that maps strings to an\ninput representation for the downstream classifier.\nFor a word level model, V would transform sen-\ntences to a sequence of word ids, mapping OOV\nwords to the same UNK ID. Whereas, for a char\n(or word+char, word-piece) model, V would map\ninputs to a sequence of character IDs. Formally,\nsensitivity is defined as\n\nHu(V oW(s1'),...,V0W(sn’))\n\nn\n\nA\nSty 8\n\nd)\n\nwhere V o W(s;) returns the input representation\n(of the downstream classifier) for the output string\nproduced by the word-recognizer W using s; and\n#£.,(-) counts the number of unique arguments.\nIntuitively, we expect a high value of Siey to\nlead to a lower robustness of the downstream clas-\nsifier, since the adversary has more degrees of\nfreedom to attack the classifier. Thus, when using\nword recognition as a defense, it is prudent to de-\nsign a low sensitivity system with a low error rate.\nHowever, as we will demonstrate, there is often a\ntrade-off between sensitivity and error rate.\n\n3.3 Synthesizing Adversarial Attacks\n\nSuppose we are given a classifier CC : S > Y\nwhich maps natural language sentences s € S to\na label from a predefined set y € Y. An adversary\nfor this classifier is a function A which maps a sen-\ntence s to its perturbed versions {s/,s,...,s/,}\nsuch that each s/ is close to s under some notion\nof distance between sentences. We define the ro-\nbustness of classifier C' to the adversary A as:\n\nin 1[C(s’\nJeity ME®)\n\nRoa = Es =yl], @\n\nwhere y represents the ground truth label for s. In\npractice, a real-world adversary may only be able\nto query the classifier a few times, hence Ro,4\nrepresents the worst-case adversarial performance\nof C. Methods for generating adversarial exam-\nples, such as HotFlip (Ebrahimi et al., 2018b), fo-\ncus on efficient algorithms for searching the min\n\n5585\n", "vlm_text": "\nThe image illustrates a diagram representing a hybrid model combining two components: a \"Background Model\" and a \"Foreground Model\". \n\n- **Background Model**: Shown in gray with a sequence of nodes labeled \\( h_1, h_2, ..., h_n \\).\n- **Foreground Model**: Shown in green with corresponding nodes labeled the same as the background model, indicating a parallel process.\n- **Input Structure**: Each node in the models receives input from blue boxes containing semi-character representations, such as “t |eend| r”.\n- **UNK Node**: Located in the foreground model's flow, suggesting it handles unknown inputs.\n- **Directional Arrows**: Indicate information flow; both models seem to process sequences in a left-to-right manner with some feedback loops within the foreground model.\n\nThe caption mentions \"Semi-character Representation\", indication that the diagram deals with text processing or language modeling.\nFigure 1: A schematic sketch of our proposed word recognition system, consisting of a  foreground  and a  back- ground  model. We train the foreground model on the smaller, domain-speciﬁc dataset, and the background model on a larger dataset (e.g., the IMDB movie corpus). We train both models to reconstruct the correct word from the orthography and context of the individual words, using synthetically corrupted inputs during training. Subse- quently, we invoke the background model whenever the foreground model predicts  UNK  . \nIn NLP, we often get such invariance for free, e.g., for a word-level model, most of the pertur- bations produced by our character-level adversary lead to an  UNK  at its input. If the model is robust to the presence of these  UNK  tokens, there is little room for an adversary to manipulate it. Character- level models, on the other hand, despite their supe- rior performance in many tasks, do not enjoy such invariance. This characteristic invariance could be exploited by an attacker. Thus, to limit the number of different inputs to the classiﬁer, we wish to re- duce the number of distinct word recognition out- puts that an attacker can induce, not just the num- ber of words on which the model is “fooled”. We denote this property of a model as its  sensitivity . \nWe can quantify this notion for a word recogni- tion system    $W$   as the expected number of unique outputs it assigns to a set of adversarial pertur- bations. Given a sentence    $s$   from the set of sen- tences  $s$  , let    $A(s)\\,=\\,s_{1}{}^{\\prime},s_{2}{}^{\\prime},.\\,.\\,.\\,,s_{n}{}^{\\prime}$    denote  e set of    $n$   perturbations to it under attack type  A , and let    $V$   be the function that maps strings to an input representation for the downstream classiﬁer. For a word level model,  $V$   would transform sen- tences to a sequence of word ids, mapping OOV words to the same  UNK  ID. Whereas, for a char (or word  $^+$  char, word-piece) model,    $V$   would map inputs to a sequence of character IDs. Formally, sensitivity is deﬁned as \n\n$$\nS_{W,V}^{A}=\\mathbb{E}_{s}\\left[\\frac{\\#u(V\\circ W(s_{1}^{\\prime}),.\\,.\\,,V\\circ W(s_{n}^{\\prime}))}{n}\\right],\n$$\n \nwhere    $V\\circ W(s_{i})$   returns the input representation (of the downstream classiﬁer) for the output string produced by the word-recognizer    $W$   using    $s_{i}$   and #  $_u(\\cdot)$   counts the number of unique arguments. \nIntuitively, we expect a high value of    $S_{W,V}^{A}$    to lead to a lower robustness of the downstream clas- siﬁer, since the adversary has more degrees of freedom to attack the classiﬁer. Thus, when using word recognition as a defense, it is prudent to de- sign a low sensitivity system with a low error rate. However, as we will demonstrate, there is often a trade-off between sensitivity and error rate. \n3.3 Synthesizing Adversarial Attacks \nSuppose we are given a classiﬁer    $C\\,:\\,\\mathcal{S}\\,\\rightarrow\\,\\mathcal{Y}$  which maps natural language sentences  s  $s\\in\\mathcal S$   ∈S  to a label from a predeﬁned set    $y\\in\\mathcal{Y}$  . An adversary for this classiﬁer is a function  A  which maps a sen- tence    $s$   to its perturbed versions    $\\{s_{1}^{\\prime},s_{2}^{\\prime},\\ldots,s_{n}^{\\prime}\\}$  } such that each  $s_{i}^{\\prime}$    is close to    $s$   under some notion of distance between sentences. We deﬁne the ro- bustness of classiﬁer    $C$   to the adversary    $A$   as: \n\n$$\nR_{C,A}=\\mathbb{E}_{s}\\left[\\operatorname*{min}_{{s^{\\prime}}\\in A(s)}\\mathbb{1}[C({s^{\\prime}})=y]\\right],\n$$\n \nwhere  $y$   represents the ground truth label for    $s$  . In practice, a real-world adversary may only be able to query the classiﬁer a few times, hence    $R_{C,A}$  represents the  worst-case  adversarial performance of    $C$  . Methods for generating adversarial exam- ples, such as HotFlip ( Ebrahimi et al. ,  2018b ), fo- cus on efﬁcient algorithms for searching the  min above. Improving    $R_{C,A}$   would imply better ro- bustness against all these methods. "}
{"page": 4, "image_path": "doc_images/P19-1561_4.jpg", "ocr_text": "above. Improving Rc,4 would imply better ro-\nbustness against all these methods.\n\nAllowed Perturbations (A(s)) We explore ad-\nversaries which perturb sentences with four types\nof character-level edits: (1) Swap: swapping two\nadjacent internal characters of a word. (2) Drop:\nremoving an internal character of a word. (3) Key-\nboard: substituting an internal character with ad-\njacent characters of QWERTY keyboard (4) Add:\ninserting a new character internally in a word. In\nline with the psycholinguistic studies (Matt Davis,\n2003; Rawlinson, 1976), to ensure that the pertur-\nbations do not affect human ability to comprehend\nthe sentence, we only allow the adversary to edit\nthe internal characters of a word, and not edit stop-\nwords or words shorter than 4 characters.\n\nAttack Strategy For /-character attacks, we try\nall possible perturbations listed above until we\nfind an adversary that flips the model prediction.\nFor 2-character attacks, we greedily fix the edit\nwhich had the least confidence among 1-character\nattacks, and then try all the allowed perturbations\non the remaining words. Higher order attacks can\nbe performed in a similar manner. The greedy\nstrategy reduces the computation required to ob-\ntain higher order attacks?, but also means that the\nrobustness score is an upper bound on the true ro-\nbustness of the classifier.\n\n4 Experiments and Results\n\nIn this section, we first discuss our experiments on\nthe word recognition systems.\n\n4.1 Word Error Correction\n\nData: We evaluate the spell correctors from §3 on\nmovie reviews from the Stanford Sentiment Tree-\nbank (SST) (Socher et al., 2013). The SST dataset\nconsists of 8544 movie reviews, with a vocabu-\nlary of over 16K words. As a background cor-\npus, we use the IMDB movie reviews (Maas et al.,\n2011), which contain 54K movie reviews, and a\nvocabulary of over 78K words. The two datasets\ndo not share any reviews in common. The spell-\ncorrection models are evaluated on their ability\nto correct misspellings. The test setting consists\nof reviews where each word (with length > 4,\nbarring stopwords) is attacked by one of the at-\ntack types (from swap, add, drop and keyboard at-\n\n*Its complexity is O(1), instead of OI\") where 1 is the\nsentence length and m is the order.\n\ntacks). In the all attack setting, we mix all attacks\nby randomly choosing one for each word. This\nmost closely resembles a real world attack setting.\n\nExperimental Setup In addition to our word\nrecognition models, we also compare to After\nThe Deadline (ATD), an open-source spell cor-\nrector*. We found ATD to be the best freely-\navailable corrector>. We refer the reader to Sak-\naguchi et al. (2017) for comparisons of SCRNN to\nother anonymized commercial spell checkers.\n\nFor the SCRNN model, we use a single-layer Bi-\nLSTM with a hidden dimension size of 50. The\ninput representation consists of 198 dimensions,\nwhich is thrice the number of unique characters\n(66) in the vocabulary. We cap the vocabulary\nsize to 10K words, whereas we use the entire vo-\ncabulary of 78470 words when we backoff to the\nbackground model. For training these networks,\nwe corrupt the movie reviews according to all at-\ntack types, i.e., applying one of the 4 attack types\nto each word, and trying to reconstruct the original\nwords via cross entropy loss.\n\nWord Recognition\nSpell-Corrector Swap Drop Add Key All\nATD 72 126 133 69 11.2\n\nScRNN (78K) 6.3 10.2 8.7 9.8 8.7\nScRNN (10K) w/ Backoff Variants\n\nPass-Through 8.5 10.5 10.7 11.2 10.2\nNeutral 8.7 10.9 108 114 10.6\nBackground 5.4 8.1 6.4 7.6 6.9\n\nTable 2: Word Error Rates (WER) of SCRNN with each\nbackoff strategy, plus ATD and an ScRNN trained only\non the background corpus (78K vocabulary) The error\nrates include 5.25% OOV words.\n\nResults We calculate the word error rates\n(WER) of each of the models for different at-\ntacks and present our findings in Table 2. Note\nthat ATD incorrectly predicts 11.2 words for ev-\nery 100 words (in the ‘all’ setting), whereas, all of\nthe backoff variations of the ScRNN reconstruct\nbetter. The most accurate variant involves backing\noff to the background model, resulting in a low er-\nror rate of 6.9%, leading to the best performance\non word recognition. This is a 32% relative error\n\n*https ://www.afterthedeadline.com/\n\n5We compared ATD with Hunspell (http:\n//nunspell.github.io/), which is used in Linux\napplications. ATD was significantly more robust owing to\ntaking context into account while correcting.\n\n5586\n", "vlm_text": "\nAllowed Perturbations    $\\left(A(s)\\right)$  We explore ad- versaries which perturb sentences with four types of character-level edits: (1) Swap: swapping two adjacent internal characters of a word. (2) Drop: removing an internal character of a word. (3) Key- board: substituting an internal character with ad- jacent characters of QWERTY keyboard (4) Add: inserting a new character internally in a word. In line with the psycho linguistic studies ( Matt Davis , 2003 ;  Rawlinson ,  1976 ), to ensure that the pertur- bations do not affect human ability to comprehend the sentence, we only allow the adversary to edit the internal characters of a word, and not edit stop- words or words shorter than  4  characters. \nAttack Strategy For    $I$  -character  attacks, we try all possible perturbations listed above until we ﬁnd an adversary that ﬂips the model prediction. For  2-character  attacks, we greedily ﬁx the edit which had the least conﬁdence among 1-character attacks, and then try all the allowed perturbations on the remaining words. Higher order attacks can be performed in a similar manner. The greedy strategy reduces the computation required to ob- tain higher order attacks 3 , but also means that the robustness score is an upper bound on the true ro- bustness of the classiﬁer. \n4 Experiments and Results \nIn this section, we ﬁrst discuss our experiments on the word recognition systems. \n4.1 Word Error Correction \nData : We evaluate the spell correctors from  § 3  on movie reviews from the Stanford Sentiment Tree- bank (SST) ( Socher et al. ,  2013 ). The SST dataset consists of  8544  movie reviews, with a vocabu- lary of over 16K words. As a background cor- pus, we use the IMDB movie reviews ( Maas et al. , 2011 ), which contain  54 K movie reviews, and a vocabulary of over 78K words. The two datasets do not share any reviews in common. The spell- correction models are evaluated on their ability to correct misspellings. The test setting consists of reviews where each word (with length    $\\geq\\ 4$  , barring stopwords) is attacked by one of the at- tack types (from swap, add, drop and keyboard at- tacks). In the  all  attack setting, we mix all attacks by randomly choosing one for each word. This most closely resembles a real world attack setting. \n\nExperimental Setup In addition to our word recognition models, we also compare to After The Deadline (ATD), an open-source spell cor- rector 4 . We found ATD to be the best freely- available corrector 5 . We refer the reader to  Sak- aguchi et al.  ( 2017 ) for comparisons of ScRNN to other anonymized commercial spell checkers. \nFor the ScRNN model, we use a single-layer Bi- LSTM with a hidden dimension size of  50 . The input representation consists of  198  dimensions, which is thrice the number of unique characters ( 66 ) in the vocabulary. We cap the vocabulary size to  10 K words, whereas we use the entire vo- cabulary of  78470  words when we backoff to the background model. For training these networks, we corrupt the movie reviews according to all at- tack types, i.e., applying one of the  4  attack types to each word, and trying to reconstruct the original words via cross entropy loss. \nThe table presents a comparison of the effectiveness of different spell-correction methods across various types of spelling errors. There are three spell-correctors listed: ATD, ScRNN with a dataset of 78K words, and ScRNN with a dataset of 10K words using three backoff variants (Pass-Through, Neutral, and Background). The types of spelling errors considered in the table are Swap, Drop, Add, Key, and All. The numbers in the table represent some form of metric or score—likely error rates or percentages. Lower numbers would typically indicate better performance in correcting that type of spelling error. For instance, the Background variant of ScRNN (10K) has the lowest scores across all types of errors compared to the other methods listed.\nResults We calculate the word error rates (WER) of each of the models for different at- tacks and present our ﬁndings in Table  2 . Note that ATD incorrectly predicts  11 . 2  words for ev- ery  100  words (in the ‘all’ setting), whereas, all of the backoff variations of the ScRNN reconstruct better. The most accurate variant involves backing off to the background model, resulting in a low er- ror rate of  $6.9\\%$  , leading to the best performance on word recognition. This is a    $32\\%$   relative error reduction compared to the vanilla ScRNN model with a pass-through backoff strategy. We can at- tribute the improved performance to the fact that there are    $5.25\\%$   words in the test corpus that are unseen in the training corpus, and are thus only recoverable by backing off to a larger corpus. No- tably, only training on the larger background cor- pus does worse, at  $8.7\\%$  , since the distribution of word frequencies is different in the background corpus compared to the foreground corpus. "}
{"page": 5, "image_path": "doc_images/P19-1561_5.jpg", "ocr_text": "reduction compared to the vanilla SCRNN model\nwith a pass-through backoff strategy. We can at-\ntribute the improved performance to the fact that\nthere are 5.25% words in the test corpus that are\nunseen in the training corpus, and are thus only\nrecoverable by backing off to a larger corpus. No-\ntably, only training on the larger background cor-\npus does worse, at 8.7%, since the distribution of\nword frequencies is different in the background\ncorpus compared to the foreground corpus.\n\n4.2 Robustness to adversarial attacks\n\nWe use sentiment analysis and paraphrase detec-\ntion as downstream tasks, as for these two tasks,\n1-2 character edits do not change the output labels.\n\nExperimental Setup For sentiment classifica-\ntion, we systematically study the effect of\ncharacter-level adversarial attacks on two architec-\ntures and four different input formats. The first\narchitecture encodes the input sentence into a se-\nquence of embeddings, which are then sequen-\ntially processed by a BiLSTM. The first and last\nstates of the BiLSTM are then used by the soft-\nmax layer to predict the sentiment of the input. We\nconsider three input formats for this architecture:\n(1) Word-only: where the input words are encoded\nusing a lookup table; (2) Char-only: where the\ninput words are encoded using a separate single-\nlayered BiLSTM over their characters; and (3)\nWord+Char: where the input words are encoded\nusing a concatenation of (1) and (2) °.\n\nThe second architecture uses the fine-tuned\nBERT model (Devlin et al., 2018), with an input\nformat of word-piece tokenization. This model\nhas recently set a new state-of-the-art on sev-\neral NLP benchmarks, including the sentiment\nanalysis task we consider here. All models\nare trained and evaluated on the binary version\nof the sentence-level Stanford Sentiment Tree-\nbank (Socher et al., 2013) dataset with only pos-\nitive and negative reviews.\n\nWe also consider the task of paraphrase detec-\ntion. Here too, we make use of the fine-tuned\nBERT (Devlin et al., 2018), which is trained and\nevaluated on the Microsoft Research Paraphrase\nCorpus (MRPC) (Dolan and Brockett, 2005).\n\nImplementation details: The embedding dimension size\nfor the word, char and word+char models are 64, 32 and\n64 + 32 respectively, with 64, 64 and 128 set as the hidden\ndimension sizes for the three models.\n\nBaseline defense strategies Two common\nmethods for dealing with adversarial examples\ninclude: (1) data augmentation (DA) (Krizhevsky\net al., 2012); and (2) adversarial training (Adv)\n(Goodfellow et al., 2014). In DA, the trained\nmodel is fine-tuned after augmenting the training\nset with an equal number of examples randomly\nattacked with a l-character edit. In Adv, the\ntrained model is fine-tuned with additional adver-\nsarial examples (selected at random) that produce\nincorrect predictions from the current-state classi-\nfier. The process is repeated iteratively, generating\nand adding newer adversarial examples from the\nupdated classifier model, until the adversarial\naccuracy on dev set stops improving.\n\nResults In Table 3, we examine the robustness\nof the sentiment models under each attack and de-\nfense method. In the absence of any attack or\ndefense, BERT (a word-piece model) performs\nthe best (90.3%7) followed by word+char mod-\nels (80.5%), word-only models (79.2%) and then\nchar-only models (70.3%). However, even single-\ncharacter attacks (chosen adversarially) can be\ncatastrophic, resulting in a significantly degraded\nperformance of 46%, 57%, 59% and 33%, respec-\ntively under the ‘all’ setting.\n\nIntuitively, one might suppose that word-piece\nand character-level models would be more robust\nto such attacks given they can make use of the\nremaining context. However, we find that they\nare the more susceptible. To see why, note that\nthe word ‘beautiful’ can only be altered in a few\nways for word-only models, either leading to an\nUNK or an existing vocabulary word, whereas,\nword-piece and character-only models treat each\nunique character combination differently. This\nprovides more variations that an attacker can ex-\nploit. Following similar reasoning, add and key\nattacks pose a greater threat than swap and drop\nattacks. The robustness of different models can be\nordered as word-only > word+char > char-only ~\nword-piece, and the efficacy of different attacks as\nadd > key > drop > swap.\n\nNext, we scrutinize the effectiveness of defense\nmethods when faced against adversarially chosen\nattacks. Clearly from table 3, DA and Adv are not\n\n7The reported accuracy on SST-B by BERT in Glue\nBenchmarks is slightly higher as it is trained and evalu-\nated on phrase-level sentiment prediction task which has\nmore training examples compared to the sentence-level task\nwe consider. We use the official source code at https:\n//github.com/google-research/bert\n\n5587\n", "vlm_text": "\n4.2 Robustness to adversarial attacks \nWe use sentiment analysis and paraphrase detec- tion as downstream tasks, as for these two tasks, 1 - 2  character edits do not change the output labels. \nExperimental Setup For sentiment classiﬁca- tion, we systematically study the effect of character-level adversarial attacks on two architec- tures and four different input formats. The ﬁrst architecture encodes the input sentence into a se- quence of embeddings, which are then sequen- tially processed by a BiLSTM. The ﬁrst and last states of the BiLSTM are then used by the soft- max layer to predict the sentiment of the input. We consider three input formats for this architecture: (1) Word-only: where the input words are encoded using a lookup table; (2) Char-only: where the input words are encoded using a separate single- layered BiLSTM over their characters; and (3) Word + Char: where the input words are encoded using a concatenation of (1) and (2)   6 . \nThe second architecture uses the ﬁne-tuned BERT model ( Devlin et al. ,  2018 ), with an input format of word-piece tokenization. This model has recently set a new state-of-the-art on sev- eral NLP benchmarks, including the sentiment analysis task we consider here. All models are trained and evaluated on the binary version of the sentence-level Stanford Sentiment Tree- bank ( Socher et al. ,  2013 ) dataset with only pos- itive and negative reviews. \nWe also consider the task of paraphrase detec- tion. Here too, we make use of the ﬁne-tuned BERT ( Devlin et al. ,  2018 ), which is trained and evaluated on the Microsoft Research Paraphrase Corpus (MRPC) ( Dolan and Brockett ,  2005 ). \nBaseline defense strategies Two common methods for dealing with adversarial examples include: (1) data augmentation ( DA ) ( Krizhevsky et al. ,  2012 ); and (2) adversarial training ( Adv ) ( Goodfellow et al. ,  2014 ). In  DA , the trained model is ﬁne-tuned after augmenting the training set with an equal number of examples randomly attacked with a 1-character edit. In  Adv , the trained model is ﬁne-tuned with additional adver- sarial examples (selected at random) that produce incorrect predictions from the current-state classi- ﬁer. The process is repeated iteratively, generating and adding newer adversarial examples from the updated classiﬁer model, until the adversarial accuracy on dev set stops improving. \nResults In Table  3 , we examine the robustness of the sentiment models under each attack and de- fense method. In the absence of any attack or defense, BERT (a word-piece model) performs the best   $(90.3\\%^{7})$  ) followed by word+char mod- els   $(80.5\\%)$  , word-only models   $(79.2\\%)$   and then char-only models   $(70.3\\%)$  ). However, even single- character attacks (chosen adversarially) can be catastrophic, resulting in a signiﬁcantly degraded performance of    $46\\%$  ,    $57\\%$  ,  $59\\%$   and    $33\\%$  , respec- tively under the ‘all’ setting. \nIntuitively, one might suppose that word-piece and character-level models would be more robust to such attacks given they can make use of the remaining context. However, we ﬁnd that they are the more susceptible. To see why, note that the word ‘beautiful’ can only be altered in a few ways for word-only models, either leading to an UNK  or an existing vocabulary word, whereas, word-piece and character-only models treat each unique character combination differently. This provides more variations that an attacker can ex- ploit. Following similar reasoning,  add  and  key attacks pose a greater threat than  swap  and  drop attacks. The robustness of different models can be ordered as word-only    $>$   word+char  $>$   char-only    $\\sim$  word-piece, and the efﬁcacy of different attacks as add  $>\\mathsf{k e y}>\\mathsf{d r o p}>\\mathsf{s w a p.}$  \nNext, we scrutinize the effectiveness of defense methods when faced against adversarially chosen attacks. Clearly from table  3 , DA and Adv are not "}
{"page": 6, "image_path": "doc_images/P19-1561_6.jpg", "ocr_text": "Sentiment Analysis (1-char attack/2-char attack)\n\nModel No attack Swap Drop Add Key All\nWord-Level Models\nBiLSTM 79.2 (64.3/53.6)  (63.7/52.7)  (60.0/43.2)  (60.2/42.4) — (58.6/40.2)\nBiLSTM + ATD 79.3 (76.2/75.3)  (66.5/59.9) — (55.6/47.5) — (62.6/57.6) —_ (55.8/37.0)\nBiLSTM + Pass-through 79.3 (78.6/78.5)  (69.1/65.3)  (65.0/59.2) — (69.6/65.6) — (63.2/52.4)\nBiLSTM + Background 78.8 (78.9/78.4)  (69.6/66.8) — (62.6/56.4) — (68.2/62.2) —_ (59.6/49.0)\nBiLSTM + Neutral 80.1 (80.1/79.9)  (72.4/70.2) — (67.2/61.2) —(69.0/64.6) — (63.2/54.0)\nChar-Level Models\nBiLSTM 70.3 (53.6/42.9)  (48.8/37.1)  (33.8/14.8) — (40.8/22.0) — (32.6/14.0)\nBiLSTM + ATD 71.0 (66.6/65.2)  (58.0/53.0) (54.6/44.4)  (61.6/57.5) — (46.5/35.4)\nBiLSTM + Pass-through 70.3 (65.8/62.9)  (58.3/54.2) (54.0/44.2)  (58.8/52.4) — (51.6/39.8)\nBiLSTM + Background 70.1 (70.3/69.8)  (60.4/57.7) — (57.4/52.6) — (58.8/54.2) — (53.6/47.2)\nBiLSTM + Neutral 70.7 (70.7/70.7)  (62.1/60.5) — (57.8/53.6)  (61.4/58.0) — (55.2/48.4)\nWord+Char Models\nBiLSTM 80.5 (63.9/52.3)  (62.8/50.8) — (57.8/39.8)  (58.4/40.8) — (56.6/35.6)\nBiLSTM + ATD 80.8 (78.0/77.3)  (67.7/60.9) — (55.6/50.5) — (68.7/64.6) — (48.5/37.4)\nBiLSTM + Pass-through 80.1 (79.0/78.7)  (69.5/65.7) — (64.0/59.0) — (66.0/62.0) — (61.5/56.5)\nBiLSTM + Background 79.5 (79.6/79.0)  (69.7/66.7) — (62.0/57.0) — (65.0/56.5) —_ (59.4/49.8)\nBiLSTM + Neutral 79.5 (79.5/79.4) — (71.2/68.8) — (65.0/59.0) — (65.5/61.5) — (61.5/55.5)\nWord-piece Models\nBERT 90.3 (64.1/47.4)  (59.2/39.9)  (46.2/26.4)  (54.3/34.9) — (45.8/24.6)\nBERT + DA 90.2 (68.3/50.6)  (62.7/39.9) — (43.6/17.0) — (57.7/32.4) —(41.0/15.8)\nBERT + Adv 89.6 (69.2/52.9)  (63.6/40.5) — (50.0/22.0) — (60.1/36.6) — (47.0/20.2)\nBERT + ATD 89.0 (84.5/84.5)  (73.0/64.0) (77.0/69.5) —_ (80.0/75.0) —_ (67.0/55.0)\nBERT + Pass-through 89.8 (85.5/83.9)  (78.9/75.0)  (70.4/64.4) — (75.3/70.3) — (68.0/58.5)\nBERT + Background 89.3 (89.1/89.1) — (79.3/76.5)  (76.5/71.0) — (77.5/74.4) _—_(73.0/67.5)\nBERT + Neutral 88.3 (88.3/88.3)  (81.1/79.5) — (78.0/74.0) —(78.8/76.8) —_ (75.0/68.0)\n\nTable 3: Accuracy of various classification models, with and without defenses, under adversarial attacks. Even\n1-character attacks significantly degrade classifier performance. Our defenses confer robustness, recovering over\n76% of the original accuracy, under the ‘all’ setting for all four model classes.\n\neffective in this case. We observed that despite a\nlow training error, these models were not able to\ngeneralize to attacks on newer words at test time.\nATD spell corrector is the most effective on key-\nboard attacks, but performs poorly on other attack\ntypes, particularly the add attack strategy.\n\nThe ScRNN model with pass-through backoff\noffers better protection, bringing back the adver-\nsarial accuracy within 5% range for the swap at-\ntack. It is also effective under other attack classes,\nand can mitigate the adversarial effect in word-\npiece models by 21%, character-only models by\n19%, and in word, and word+char models by over\n4.5% . This suggests that the direct training signal\nof word error correction is more effective than the\nindirect signal of sentiment classification available\nto DA and Adv for model robustness.\n\nWe observe additional gains by using back-\nground models as a backoff alternative, because of\nits lower word error rate (WER), especially, under\n\nthe swap and drop attacks. However, these gains\ndo not consistently translate in all other settings,\nas lower WER is necessary but not sufficient. Be-\nsides lower error rate, we find that a solid defense\nshould furnish the attacker the fewest options to\nattack, i.e. it should have a low sensitivity. As\nwe shall see in section § 4.3, the backoff neutral\nvariation has the lowest sensitivity due to mapping\nUNK predictions to a fixed neutral word. Thus, it\nresults in the highest robustness on most of the at-\ntack types for all four model classes.\n\nModel No Attack _ All attacks\n1-char 2-char\nBERT 89.0 60.0 31.0\nBERT + ATD 89.9 75.8 61.6\nBERT + Pass-through 89.0 84.5 81.5\nBERT + Neutral 84.0 82.5 82.5\n\nTable 4: Accuracy of BERT, with and without defenses,\non MRPC when attacked under the ‘all’ attack setting.\n\n5588\n", "vlm_text": "This table presents the performance (likely accuracy or another metric) of different models under various types of text perturbations or attacks. The models are categorized into four groups: Word-Level, Char-Level, Word+Char, and Word-piece Models. Each of these groups contains entries for different variations of the model architecture like BiLSTM or BERT with enhancements such as ATD, Pass-through, Background, and Neutral.\n\nThe table columns include:\n\n- **No attack**: The performance without any manipulations.\n- **Swap, Drop, Add, Key**: Performance under different types of perturbations.\n- **All**: Averages or overall performance across all types of attacks.\n\nValues in parentheses represent two numbers, possibly different metrics or measurements under the same condition. Numbers in bold seem to indicate the best performance for each perturbation type within a sub-category.\neffective in this case. We observed that despite a low training error, these models were not able to generalize to attacks on newer words at test time. ATD spell corrector is the most effective on key- board attacks, but performs poorly on other attack types, particularly the add attack strategy. \nThe ScRNN model with pass-through backoff offers better protection, bringing back the adver- sarial accuracy within  $5\\%$   range for the swap at- tack. It is also effective under other attack classes, and can mitigate the adversarial effect in word- piece models by    $21\\%$  , character-only models by  $19\\%$  , and in word, and word  $^+$  char models by over  $4.5\\%$   . This suggests that the direct training signal of word error correction is more effective than the indirect signal of sentiment classiﬁcation available to DA and Adv for model robustness. \nWe observe additional gains by using back- ground models as a backoff alternative, because of its lower word error rate (WER), especially, under the swap and drop attacks. However, these gains do not consistently translate in all other settings, as lower WER is necessary but not sufﬁcient. Be- sides lower error rate, we ﬁnd that a solid defense should furnish the attacker the fewest options to attack, i.e. it should have a low sensitivity. As we shall see in section    $\\S~4.3$  , the backoff neutral variation has the lowest sensitivity due to mapping UNK  predictions to a ﬁxed neutral word. Thus, it results in the highest robustness on most of the at- tack types for all four model classes. \n\nThe table presents a comparison of different models' performance under scenarios with and without attacks. These models are evaluated using some kind of performance or accuracy metric, as demonstrated by the numerical values in the table. Here’s an outline of the table structure and data:\n\n- **Columns**:\n  - The first column specifies the model variants.\n  - The second column, labeled \"No Attack,\" shows the performance metric for each model when there is no attack.\n  - The subsequent columns, labeled under \"All attacks,\" show performance for two types of attacks: \"1-char\" and \"2-char.\"\n\n- **Rows**:\n  - The first row describes the standard BERT model's performance:\n    - Without attacks: 89.0\n    - With 1-char attacks: 60.0\n    - With 2-char attacks: 31.0\n  \n  - The second row refers to \"BERT + ATD\":\n    - Without attacks: 89.9\n    - With 1-char attacks: 75.8\n    - With 2-char attacks: 61.6\n\n  - The third row describes \"BERT + Pass-through\":\n    - Without attacks: 89.0\n    - With 1-char attacks: 84.5 (Bold)\n    - With 2-char attacks: 81.5\n\n  - The last row describes \"BERT + Neutral\":\n    - Without attacks: 84.0\n    - With 1-char attacks: 82.5\n    - With 2-char attacks: 82.5 (Bold)\n\nFrom this table, we can infer that:\n- The BERT model's performance significantly drops under attack conditions.\n- The \"BERT + Pass-through\" and \"BERT + Neutral\" models maintain relatively higher performance under the 1-char and 2-char attacks compared to standard BERT.\n- \"BERT + Pass-through\" has the highest improvement in performance with 1-char attacks than other models, as indicated by the bold value of 84.5.\n- \"BERT + Neutral\" has the highest performance under 2-char attack, indicated by the bold value of 82.5.\nTable 4: Accuracy of BERT, with and without defenses, on MRPC when attacked under the ‘all’ attack setting. "}
{"page": 7, "image_path": "doc_images/P19-1561_7.jpg", "ocr_text": "Sensitivity Analysis\n\nBackoff Swap Drop Add Key All\n\nClosed Vocabulary Models (word-only)\n\nPass-Through 17.6 19.7 0.8 73 11.3\nBackground 19.5 22.3 1.1 9.5 13.1\nNeutral 17.5 19.7 0.8 7.2 11.3\n\nOpen Vocab. Models (char/word+char/word-piece)\n\nPass-Through 39.6 35.3 19.2 26.9 30.3\nBackground 20.7 25.1 1.3 11.6 14.7\nNeutral 17.5 19.7 0.8 7.2 11.3\n\nTable 5: Sensitivity values for word recognizers. Neu-\ntral backoff shows lowest sensitivity.\n\nTable 4 shows the accuracy of BERT on 200 ex-\namples from the dev set of the MRPC paraphrase\ndetection task under various attack and defense\nsettings. We re-trained the SCRNN model vari-\nants on the MRPC training set for these experi-\nments. Again, we find that simple 1-2 character\nattacks can bring down the accuracy of BERT sig-\nnificantly (89% to 31%). Word recognition mod-\nels can provide an effective defense, with both our\npass-through and neutral variants recovering most\nof the accuracy. While the neutral backoff model\nis effective on 2-char attacks, it hurts performance\nin the no attack setting, since it incorrectly mod-\nifies certain correctly spelled entity names. Since\nthe two variants are already effective, we did not\ntrain a background model for this task.\n\n4.3 Understanding Model Sensitivity\n\nExperimental setup To study model sensitiv-\nity, for each sentence, we perturb one randomly-\nchosen word and replace it with all possible per-\nturbations under a given attack type. The resulting\nset of perturbed sentences is then fed to the word\nrecognizer (whose sensitivity is to be estimated).\nAs described in equation 1, we count the number\nof unique predictions from the output sentences.\nTwo corrections are considered unique if they are\nmapped differently by the downstream classifier.\n\nResults The neutral backoff variant has the low-\nest sensitivity (Table 5). This is expected, as it\nreturns a fixed neutral word whenever the SCRNN\npredicts an UNK , therefore reducing the number\nof unique outputs it predicts. Open vocabulary\n(ie. char-only, word+char, word-piece) down-\nstream classifiers consider every unique combi-\nnation of characters differently, whereas word-\nonly classifiers internally treat all out of vocab-\nulary (OOV) words alike. Hence, for char-only,\n\n12\n11\n\n11} 63.2 e\nes 2 wo) 222 .\n10 51.6\nG 9 Eo\n= =\n\n@ Pass-through\n\n7| @ Background 59-6,\n@ Neutral 7 e\n6 53.6\n12 13 10 20 30\n\nSensitivity Sensitivity\n\nFigure 2: Effect of sensitivity and word error rate on\nrobustness (depicted by the bubble sizes) in word-only\nmodels (left) and char-only models (right).\n\nword+char, and word-piece models, the pass-\nthrough version is more sensitive than the back-\nground variant, as it passes words as is (and each\ncombination is considered uniquely). However,\nfor word-only models, pass-through is less sen-\nsitive as all the OOV character combinations are\nrendered identical.\n\nIdeally, a preferred defense is one with low sen-\nsitivity and word error rate. In practice, however,\nwe see that a low error rate often comes at the cost\nof sensitivity. We visualize this trade-off in Fig-\nure 2, where we plot WER and sensitivity on the\ntwo axes, and depict the robustness when using\ndifferent backoff variants. Generally, sensitivity is\nthe more dominant factor out of the two, as the er-\nror rates of the considered variants are reasonably\nlow.\n\nHuman Intelligibility We verify if the senti-\nment (of the reviews) is preserved with char-level\nattacks. In a human study with 50 attacked (and\nsubsequently misclassified), and 50 unchanged re-\nviews, it was noted that 48 and 49, respectively,\npreserved the sentiment.\n\n5 Conclusion\n\nAs character and word-piece inputs become com-\nmonplace in modern NLP pipelines, it is worth\nhighlighting the vulnerability they add. We\nshow that minimally-doctored attacks can bring\ndown accuracy of classifiers to random guess-\ning. We recommend word recognition as a safe-\nguard against this and build upon RNN-based\nsemi-character word recognizers. We discover that\nwhen used as a defense mechanism, the most ac-\ncurate word recognition models are not always the\nmost robust against adversarial attacks. Addition-\nally, we highlight the need to control the sensitiv-\nity of these models to achieve high robustness.\n\n5589\n", "vlm_text": "The table presents data comparing different models with respect to various criteria. It is divided into two main sections: Closed Vocabulary Models (word-only) and Open Vocabulary Models (char/word+char/word-piece). \n\nFor each section, the models are further divided into three types: Pass-Through, Background, and Neutral. The table measures their performance across five tasks: Swap, Drop, Add, Key, and All.\n\n- **Closed Vocabulary Models (word-only):**\n  - **Pass-Through:** Swap (17.6), Drop (19.7), Add (0.8), Key (7.3), All (11.3)\n  - **Background:** Swap (19.5), Drop (22.3), Add (1.1), Key (9.5), All (13.1)\n  - **Neutral:** Swap (17.5), Drop (19.7), Add (0.8), Key (7.2), All (11.3)\n\n- **Open Vocabulary Models (char/word+char/word-piece):**\n  - **Pass-Through:** Swap (39.6), Drop (35.3), Add (19.2), Key (26.9), All (30.3)\n  - **Background:** Swap (20.7), Drop (25.1), Add (1.3), Key (11.6), All (14.7)\n  - **Neutral:** Swap (17.5), Drop (19.7), Add (0.8), Key (7.2), All (11.3)\nTable  4  shows the accuracy of BERT on  $200\\,\\mathrm{ex}\\cdot$  - amples from the dev set of the MRPC paraphrase detection task under various attack and defense settings. We re-trained the ScRNN model vari- ants on the MRPC training set for these experi- ments. Again, we ﬁnd that simple  1 - 2  character attacks can bring down the accuracy of BERT sig- niﬁcantly   $(89\\%$   to    $31\\%$  ). Word recognition mod- els can provide an effective defense, with both our pass-through and neutral variants recovering most of the accuracy. While the neutral backoff model is effective on  2 -char attacks, it hurts performance in the  no attack  setting, since it incorrectly mod- iﬁes certain correctly spelled entity names. Since the two variants are already effective, we did not train a background model for this task. \n4.3 Understanding Model Sensitivity \nExperimental setup To study model sensitiv- ity, for each sentence, we perturb one randomly- chosen word and replace it with all possible per- turbations under a given attack type. The resulting set of perturbed sentences is then fed to the word recognizer (whose sensitivity is to be estimated). As described in equation  1 , we count the number of unique predictions from the output sentences. Two corrections are considered unique if they are mapped differently by the downstream classiﬁer. \nResults The neutral backoff variant has the low- est sensitivity (Table  5 ). This is expected, as it returns a ﬁxed neutral word whenever the ScRNN predicts an  UNK  , therefore reducing the number of unique outputs it predicts. Open vocabulary (i.e. char-only, word+char, word-piece) down- stream classiﬁers consider every unique combi- nation of characters differently, whereas word- only classiﬁers internally treat all out of vocab- ulary (OOV) words alike. Hence, for char-only, \nThe image consists of two scatter plots, each representing the relationship between sensitivity and word error rate (WER) for two types of models: word-only models (left plot) and char-only models (right plot). The bubble sizes in the plots depict robustness. \n\nThe left plot shows:\n- Three data points, represented by bubbles of different colors: blue (Pass-through), orange (Background), and green (Neutral).\n- Sensitivity ranges from about 11.4 to 12.6, and WER ranges from 9.5 to 11.5.\n- Two overlapping green and blue bubbles correspond to WER of 11 and sensitivity of approximately 12, with robustness value 63.2.\n- An orange bubble represents a WER of around 10.5 and a sensitivity of around 12.7, with robustness value 59.6.\n\nThe right plot shows:\n- Three data points, again represented by differently colored bubbles: blue, orange, and green.\n- Sensitivity ranges from about 10 to 30, and WER from 6.5 to 11.\n- A large green bubble corresponds to a WER of 11 and sensitivity of around 12, with robustness value 55.2.\n- An orange bubble represents a WER of 7 and sensitivity of about 10, with robustness value 53.6.\n- A blue bubble shows a WER of approximately 10 and a sensitivity of approximately 30, with robustness value 51.6.\n\nThe legend explains the colors of the bubbles: blue for Pass-through, orange for Background, and green for Neutral.\nword+char, and word-piece models, the pass- through version is more sensitive than the back- ground variant, as it passes words as is (and each combination is considered uniquely). However, for word-only models, pass-through is less sen- sitive as all the OOV character combinations are rendered identical. \nIdeally, a preferred defense is one with low sen- sitivity and word error rate. In practice, however, we see that a low error rate often comes at the cost of sensitivity. We visualize this trade-off in Fig- ure  2 , where we plot WER and sensitivity on the two axes, and depict the robustness when using different backoff variants. Generally, sensitivity is the more dominant factor out of the two, as the er- ror rates of the considered variants are reasonably low. \nHuman Intelligibility We verify if the senti- ment (of the reviews) is preserved with char-level attacks. In a human study with  50  attacked (and subsequently misclassiﬁed), and 50 unchanged re- views, it was noted that  48  and  49 , respectively, preserved the sentiment. \n5 Conclusion \nAs character and word-piece inputs become com- monplace in modern NLP pipelines, it is worth highlighting the vulnerability they add. We show that minimally-doctored attacks can bring down accuracy of classiﬁers to random guess- ing. We recommend word recognition as a safe- guard against this and build upon RNN-based semi-character word recognizers. We discover that when used as a defense mechanism, the most ac- curate word recognition models are not always the most robust against adversarial attacks. Addition- ally, we highlight the need to control the sensitiv- ity of these models to achieve high robustness. "}
{"page": 8, "image_path": "doc_images/P19-1561_8.jpg", "ocr_text": "6 Acknowledgements\n\nThe authors are grateful to Graham Neubig, Ed-\nuard Hovy, Paul Michel, Mansi Gupta, and An-\ntonios Anastasopoulos for suggestions and feed-\nback.\n\nReferences\n\nYonatan Belinkov and Yonatan Bisk. 2018. Synthetic\nand natural noise both break neural machine transla-\ntion. In International Conference on Learning Rep-\nresentations (ICLR).\n\nConstance Bitso, Ina Fourie, and Theo JD Bothma.\n2013. Trends in transition from classical cen-\nsorship to internet censorship: selected country\noverviews. Innovation: journal of appropriate\nlibrarianship and information work in Southern\n\nAfrica, 2013(46):166-191.\n\nEric Brill and Robert C. Moore. 2000. An improved\nerror model for noisy channel spelling correction.\nIn Proceedings of the 38th Annual Meeting on As-\nsociation for Computational Linguistics, ACL ’00,\npages 286-293, Stroudsburg, PA, USA. Association\nfor Computational Linguistics.\n\nJacob Buckman, Aurko Roy, Colin Raffel, and Ian\nGoodfellow. 2018. Thermometer encoding: One hot\nway to resist adversarial examples. International\nConference on Learning Representations (ICLR).\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv: 1810.04805.\n\nWilliam B Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nIn Proceedings of the Third International Workshop\non Paraphrasing (IWP2005).\n\nJavid Ebrahimi, Daniel Lowd, and Dejing Dou. 2018a.\nOn adversarial examples for character-level neural\nmachine translation. In Jnternational Conference on\nComputational Linguistics (COLING).\n\nJavid Ebrahimi, Anyi Rao, Daniel Lowd, and Dejing\nDou. 2018b. Hotflip: White-box adversarial exam-\nples for nlp. In Association for Computational Lin-\nguistics (ACL).\n\nGiorgio Fumera, Ignazio Pillai, and Fabio Roli. 2006.\nSpam filtering based on the analysis of text infor-\nmation embedded into images. Journal of Machine\nLearning Research (JMLR).\n\nMax Glockner, Vered Shwartz, and Yoav Goldberg.\n2018. Breaking nli systems with sentences that re-\nquire simple lexical inferences. In Association for\nComputational Linguistics (ACL).\n\nJan J Goodfellow, Jonathon Shlens, and Christian\nSzegedy. 2014. Explaining and harnessing adversar-\nial examples. In International Conference on Learn-\ning Representations (ICLR).\n\nRobin Jia and Percy Liang. 2017. Adversarial exam-\nples for evaluating reading comprehension systems.\nEmpirical Methods in Natural Language Processing\n(EMNLP).\n\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E Hin-\nton. 2012. Imagenet classification with deep con-\nvolutional neural networks. In Advances in neural\ninformation processing systems (NIPS).\n\nKaren Kukich. 1992. Techniques for automatically\ncorrecting words in text. Acm Computing Surveys\n(CSUR), 24(4):377-439.\n\nHonglak Lee and Andrew Y Ng. 2005. Spam deobfus-\ncation using a hidden markov model. In CEAS.\n\nHao Li, Yang Wang, Xinyu Liu, Zhichao Sheng, and\nSi Wei. 2018. Spelling error correction using a\nnested rmn model and pseudo training data. arXiv\npreprint arXiv: 1811.00238.\n\nJiwei Li, Will Monroe, and Dan Jurafsky. 2016. Un-\nderstanding neural networks through representation\nerasure. arXiv preprint arXiv: 1612.08220.\n\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham,\nDan Huang, Andrew Y. Ng, and Christopher Potts.\n2011. Learning word vectors for sentiment analysis.\nIn Association for Computational Linguistics (ACL).\n\nMatt Davis. 2003. Psycholinguistic  evi-\ndence on scrambled letters in reading.\nhttps://www.mrc-cbu.cam.ac.uk/\npeople/matt.davis/cmabridge/.\n\nEric Mays, Fred J. Damerau, and Robert L. Mercer.\n1991. Context based spelling correction. Informa-\ntion Processing & Management, 27(5):517 — 522.\n\nHwee Tou Ng, Siew Mei Wu, Ted Briscoe, Christian\nHadiwinoto, Raymond Hendy Susanto, and Christo-\npher Bryant. 2014. The conll-2014 shared task on\ngrammatical error correction. In Proceedings of the\nEighteenth Conference on Computational Natural\nLanguage Learning: Shared Task, pages 1-14.\n\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQUAD: 100,000+ questions for\nmachine comprehension of text. In Empirical Meth-\nods in Natural Language Processing (EMNLP).\n\nGraham Ernest Rawlinson. 1976. The significance of\nletter position in word recognition. Ph.D. thesis,\nUniversity of Nottingham.\n\nKeisuke Sakaguchi, Kevin Duh, Matt Post, and Ben-\njamin Van Durme. 2017. Robsut wrod reocginiton\nvia semi-character recurrent neural network. In As-\nsociation for the Advancement of Artificial Intelli-\ngence (AAAI).\n\n5590\n", "vlm_text": "6 Acknowledgements \nThe authors are grateful to Graham Neubig, Ed- uard Hovy, Paul Michel, Mansi Gupta, and An- tonios Anastasopoulos for suggestions and feed- back. \nReferences \nYonatan Belinkov and Yonatan Bisk. 2018. Synthetic and natural noise both break neural machine transla- tion. In  International Conference on Learning Rep- \nresentations (ICLR) . Constance Bitso, Ina Fourie, and Theo JD Bothma. 2013. Trends in transition from classical cen- sorship to internet censorship: selected country overviews. Innovation: journal of appropriate librarianship and information work in Southern Africa , 2013(46):166–191. Eric Brill and Robert C. Moore. 2000.  An improved error model for noisy channel spelling correction . In  Proceedings of the 38th Annual Meeting on As- sociation for Computational Linguistics , ACL   $^{'}00$  , pages 286–293, Stroudsburg, PA, USA. Association for Computational Linguistics. Jacob Buckman, Aurko Roy, Colin Raffel, and Ian Goodfellow. 2018. Thermometer encoding: One hot way to resist adversarial examples. International Conference on Learning Representations (ICLR) . Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understand- ing.  arXiv preprint arXiv:1810.04805 . William B Dolan and Chris Brockett. 2005. Automati- cally constructing a corpus of sentential paraphrases. In  Proceedings of the Third International Workshop on Paraphrasing (IWP2005) . Javid Ebrahimi, Daniel Lowd, and Dejing Dou. 2018a. On adversarial examples for character-level neural machine translation. In  International Conference on Computational Linguistics (COLING) . Javid Ebrahimi, Anyi Rao, Daniel Lowd, and Dejing Dou. 2018b. Hotﬂip: White-box adversarial exam- ples for nlp. In  Association for Computational Lin- guistics (ACL) . Giorgio Fumera, Ignazio Pillai, and Fabio Roli. 2006. Spam ﬁltering based on the analysis of text infor- mation embedded into images.  Journal of Machine Learning Research (JMLR) . Max Glockner, Vered Shwartz, and Yoav Goldberg. 2018. Breaking nli systems with sentences that re- quire simple lexical inferences. In  Association for Computational Linguistics (ACL) . \nIan J Goodfellow, Jonathon Shlens, and Christian Szegedy. 2014. Explaining and harnessing adversar- ial examples. In  International Conference on Learn- ing Representations (ICLR) . Robin Jia and Percy Liang. 2017. Adversarial exam- ples for evaluating reading comprehension systems. Empirical Methods in Natural Language Processing (EMNLP) . Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hin- ton. 2012. Imagenet classiﬁcation with deep con- volutional neural networks. In  Advances in neural information processing systems (NIPS) . Karen Kukich. 1992. Techniques for automatically correcting words in text.  Acm Computing Surveys (CSUR) , 24(4):377–439. Honglak Lee and Andrew Y Ng. 2005. Spam deobfus- cation using a hidden markov model. In  CEAS . Hao Li, Yang Wang, Xinyu Liu, Zhichao Sheng, and Si Wei. 2018. Spelling error correction using a nested rnn model and pseudo training data.  arXiv preprint arXiv:1811.00238 . Jiwei Li, Will Monroe, and Dan Jurafsky. 2016. Un- derstanding neural networks through representation erasure.  arXiv preprint arXiv:1612.08220 . Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. 2011.  Learning word vectors for sentiment analysis . In  Association for Computational Linguistics (ACL) . Matt Davis. 2003. Psycho linguistic evi- dence on scrambled letters in reading. https://www.mrc-cbu.cam.ac.uk/ people/matt.davis/cmabridge/ . Eric Mays, Fred J. Damerau, and Robert L. Mercer. 1991.  Context based spelling correction .  Informa- tion Processing & Management , 27(5):517 – 522. Hwee Tou Ng, Siew Mei Wu, Ted Briscoe, Christian Hadiwinoto, Raymond Hendy Susanto, and Christo- pher Bryant. 2014. The conll-2014 shared task on grammatical error correction. In  Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task , pages 1–14. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD:   $100{,}000{+}$   questions for machine comprehension of text. In  Empirical Meth- ods in Natural Language Processing (EMNLP) . Graham Ernest Rawlinson. 1976.  The signiﬁcance of letter position in word recognition . Ph.D. thesis, University of Nottingham. Keisuke Sakaguchi, Kevin Duh, Matt Post, and Ben- jamin Van Durme. 2017. Robsut wrod reocginiton via semi-character recurrent neural network. In  As- sociation for the Advancement of Artiﬁcial Intelli- gence (AAAI) . "}
{"page": 9, "image_path": "doc_images/P19-1561_9.jpg", "ocr_text": "Allen Schmaltz, Yoon Kim, Alexander M. Rush, and\nStuart Shieber. 2016. Sentence-level grammatical\nerror identification as sequence-to-sequence correc-\ntion. In Proceedings of the 11th Workshop on In-\nnovative Use of NLP for Building Educational Ap-\nplications, pages 242-251, San Diego, CA. Associ-\nation for Computational Linguistics.\n\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D Manning, Andrew Ng, and\nChristopher Potts. 2013. Recursive deep models\nfor semantic compositionality over a sentiment tree-\nbank. In Empirical Methods in Natural Language\nProcessing (EMNLP).\n\nChristian Szegedy, Wojciech Zaremba, Ilya Sutskever,\nJoan Bruna, Dumitru Erhan, Ian Goodfellow, and\nRob Fergus. 2013. Intriguing properties of neural\nnetworks. arXiv preprint arXiv: 1312.6199.\n\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V\nLe, Mohammad Norouzi, Wolfgang Macherey,\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus\nMacherey, et al. 2016. Google’s neural ma-\nchine translation system: Bridging the gap between\nhuman and machine translation. arXiv preprint\narXiv:1609.08144.\n\nWeilin Xu, David Evans, and Yanjun Qi. 2017. Feature\nsqueezing: Detecting adversarial examples in deep\nneural networks. arXiv preprint arXiv:1704.01155.\n\nZhengli Zhao, Dheeru Dua, and Sameer Singh. 2018.\nGenerating natural adversarial examples. In Jnter-\nnational Conference on Learning Representations\n(ICLR).\n\n5591\n", "vlm_text": "Allen Schmaltz, Yoon Kim, Alexander M. Rush, and Stuart Shieber. 2016. Sentence-level grammatical error identiﬁcation as sequence-to-sequence correc- tion . In  Proceedings of the 11th Workshop on In- novative Use of NLP for Building Educational Ap- plications , pages 242–251, San Diego, CA. Associ- ation for Computational Linguistics. \nRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositional it y over a sentiment tree- bank. In  Empirical Methods in Natural Language Processing (EMNLP) . \nChristian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. 2013. Intriguing properties of neural networks.  arXiv preprint arXiv:1312.6199 . \nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. 2016. Google’s neural ma- chine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144 . \nWeilin Xu, David Evans, and Yanjun Qi. 2017. Feature squeezing: Detecting adversarial examples in deep neural networks.  arXiv preprint arXiv:1704.01155 . \nZhengli Zhao, Dheeru Dua, and Sameer Singh. 2018. Generating natural adversarial examples. In  Inter- national Conference on Learning Representations (ICLR) . "}
