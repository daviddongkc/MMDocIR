{"page": 0, "image_path": "doc_images/P18-1009_0.jpg", "ocr_text": "Ultra-Fine Entity Typing\n\nEunsol Choi‘ Omer Levy?\n\nYejin Choi'#\n\nLuke Zettlemoyer'\n\niPaul G. Allen School of Computer Science & Engineering, University of Washington\n# Allen Institute for Artificial Intelligence, Seattle WA\n{eunsol, omerlevy, yejin, 1sz}@cs.washington.edu\n\nAbstract\n\nWe introduce a new entity typing task:\ngiven a sentence with an entity mention,\nthe goal is to predict a set of free-form\nphrases (e.g. skyscraper, songwriter, or\ncriminal) that describe appropriate types\nfor the target entity. This formulation al-\nlows us to use a new type of distant super-\nvision at large scale: head words, which\nindicate the type of the noun phrases they\nappear in. We show that these ultra-fine\ntypes can be crowd-sourced, and intro-\nduce new evaluation sets that are much\nmore diverse and fine-grained than exist-\ning benchmarks. We present a model that\ncan predict open types, and is trained using\na multitask objective that pools our new\nhead-word supervision with prior supervi-\nsion from entity linking. Experimental re-\nsults demonstrate that our model is effec-\ntive in predicting entity types at varying\ngranularity; it achieves state of the art per-\nformance on an existing fine-grained en-\ntity typing benchmark, and sets baselines\nfor our newly-introduced datasets.!\n\n1 Introduction\n\nEntities can often be described by very fine\ngrained types. Consider the sentences “Bill robbed\nJohn. He was arrested.” The noun phrases “John,”\n“Bill” and “he” have very specific types that\ncan be inferred from the text. This includes the\nfacts that “Bill” and “he” are both likely “crimi-\nnal” due to the “robbing” and “arresting,” while\n“John” is more likely a “victim” because he was\n“robbed.” Such fine-grained types (victim, crimi-\nnal) are important for context-sensitive tasks such\n\n‘Our data and model can be downloaded from:\nhttp://nlp.cs.washington.edu/entity_type\n\n87\n\nSentence with Target Entity Entity Types\n\nevent, festival, rit-\nual, custom, cere-\nmony, party, cele-\nbration\n\nperson, accused,\nsuspect, defendant\nevent, plan, mis-\nsion, action\n\nDuring the Inca Empire, {the Inti\nRaymi} was the most important\nof four ceremonies celebrated in\nCusco.\n\n{They} have been asked to appear\nin court to face the charge.\n\nBan praised Rwanda’s commit-\nment to the UN and its role in\n{peacemaking operations}.\n\nTable 1: Examples of entity mentions and their an-\nnotated types, as annotated in our dataset. The en-\ntity mentions are bold faced and in the curly brack-\nets. The bold blue types do not appear in existing\nfine-grained type ontologies.\n\nas coreference resolution and question answering\n(e.g. “Who was the victim?”). Inferring such types\nfor each mention (John, he) is not possible given\ncurrent typing models that only predict relatively\ncoarse types and only consider named entities.\n\nTo address this challenge, we present a new\ntask: given a sentence with a target entity men-\ntion, predict free-form noun phrases that describe\nappropriate types for the role the target entity plays\nin the sentence. Table 1 shows three examples that\nexhibit a rich variety of types at different granular-\nities. Our task effectively subsumes existing fine-\ngrained named entity typing formulations due to\nthe use of a very large type vocabulary and the fact\nthat we predict types for all noun phrases, includ-\ning named entities, nominals, and pronouns.\n\nIncorporating fine-grained entity types has im-\nproved entity-focused downstream tasks, such as\nrelation extraction (Yaghoobzadeh et al., 2017a),\nquestion answering (Yavuz et al., 2016), query\nanalysis (Balog and Neumayer, 2012), and coref-\nerence resolution (Durrett and Klein, 2014). These\nsystems used a relatively coarse type ontology.\nHowever, manually designing the ontology is a\nchallenging task, and it is difficult to cover all pos-\n\nProceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 87-96\nMelbourne, Australia, July 15 - 20, 2018. ©2018 Association for Computational Linguistics\n", "vlm_text": "Ultra-Fine Entity Typing \nEunsol Choi † Omer Levy † Yejin Choi † ♯ Luke Zettlemoyer † † Paul G. Allen School of Computer Science & Engineering, University of Washington ♯ Allen Institute for Artiﬁcial Intelligence, Seattle WA { eunsol,omerlevy,yejin,lsz } @cs.washington.edu \nAbstract \nWe introduce a new entity typing task: given a sentence with an entity mention, the goal is to predict a set of free-form phrases (e.g. skyscraper, songwriter, or criminal) that describe appropriate types for the target entity. This formulation al- lows us to use a new type of distant super- vision at large scale: head words, which indicate the type of the noun phrases they appear in. We show that these ultra-ﬁne types can be crowd-sourced, and intro- duce new evaluation sets that are much more diverse and ﬁne-grained than exist- ing benchmarks. We present a model that can predict open types, and is trained using a multitask objective that pools our new head-word supervision with prior supervi- sion from entity linking. Experimental re- sults demonstrate that our model is effec- tive in predicting entity types at varying granularity; it achieves state of the art per- formance on an existing ﬁne-grained en- tity typing benchmark, and sets baselines for our newly-introduced datasets. \n1 Introduction \nEntities can often be described by very ﬁne grained types. Consider the sentences “Bill robbed John. He was arrested.” The noun phrases “John,” “Bill,” and “he” have very speciﬁc types that can be inferred from the text. This includes the facts that “Bill” and “he” are both likely “crimi- nal” due to the “robbing” and “arresting,” while\n\n “John” is more likely a “victim” because he was\n\n “robbed.” Such ﬁne-grained types (victim, crimi- nal) are important for context-sensitive tasks such \nThe table consists of two columns: \"Sentence with Target Entity\" and \"Entity Types.\"\n\n1. **Sentence with Target Entity**:\n   - The first sentence is: \"During the Inca Empire, {the Inti Raymi} was the most important of four ceremonies celebrated in Cusco.\"\n   - The second sentence is: \"{They} have been asked to appear in court to face the charge.\"\n   - The third sentence is: \"Ban praised Rwanda’s commitment to the UN and its role in {peacemaking operations}.\"\n\n2. **Entity Types**:\n   - For the target entity \"the Inti Raymi\" in the first sentence, the entity types are: event, festival, ritual, custom, ceremony, party, celebration.\n   - For the target entity \"They\" in the second sentence, the entity types are: person, accused, suspect, defendant.\n   - For the target entity \"peacemaking operations\" in the third sentence, the entity types are: event, plan, mission, action.\nTable 1: Examples of entity mentions and their an- notated types, as annotated in our dataset. The en- tity mentions are bold faced and in the curly brack- ets. The bold blue types do not appear in existing ﬁne-grained type ontologies. \nas coreference resolution and question answering (e.g. “Who was the victim?”). Inferring such types for each mention (John, he) is not possible given current typing models that only predict relatively coarse types and only consider named entities. \nTo address this challenge, we present a new task: given a sentence with a target entity men- tion, predict free-form noun phrases that describe appropriate types for the role the target entity plays in the sentence. Table  1  shows three examples that exhibit a rich variety of types at different granular- ities. Our task effectively subsumes existing ﬁne- grained named entity typing formulations due to the use of a very large type vocabulary and the fact that we predict types for all noun phrases, includ- ing named entities, nominals, and pronouns. \nIncorporating ﬁne-grained entity types has im- proved entity-focused downstream tasks, such as relation extraction ( Yaghoobzadeh et al. ,  2017a ), question answering ( Yavuz et al. ,  2016 ), query analysis ( Balog and Neumayer ,  2012 ), and coref- erence resolution ( Durrett and Klein ,  2014 ). These systems used a relatively coarse type ontology. However, manually designing the ontology is a challenging task, and it is difﬁcult to cover all pos- "}
{"page": 1, "image_path": "doc_images/P18-1009_1.jpg", "ocr_text": "a) Our Dataset\n\nb) OntoNotes\n\nc) FIGER\n\nFigure 1: A visualization of all the labels that cover 90% of the data, where a bubble’s size is proportional\nto the label’s frequency. Our dataset is much more diverse and fine grained when compared to existing\ndatasets (OntoNotes and FIGER), in which the top 5 types cover 70-80% of the data.\n\nsible concepts even within a limited domain. This\ncan be seen empirically in existing datasets, where\nthe label distribution of fine-grained entity typing\ndatasets is heavily skewed toward coarse-grained\ntypes. For instance, annotators of the OntoNotes\ndataset (Gillick et al., 2014) marked about half of\nthe mentions as “other,” because they could not\nfind a suitable type in their ontology (see Figure 1\nfor a visualization and Section 2.2 for details).\n\nOur more open, ultra-fine vocabulary, where\ntypes are free-form noun phrases, alleviates the\nneed for hand-crafted ontologies, thereby greatly\nincreasing overall type coverage. To better un-\nderstand entity types in an unrestricted setting,\nwe crowdsource a new dataset of 6,000 examples.\nCompared to previous fine-grained entity typing\ndatasets, the label distribution in our data is sub-\nstantially more diverse and fine-grained. Annota-\ntors easily generate a wide range of types and can\ndetermine with 85% agreement if a type generated\nby another annotator is appropriate. Our evalu-\nation data has over 2,500 unique types, posing a\nchallenging learning problem.\n\nWhile our types are harder to predict, they also\nallow for a new form of contextual distant super-\nvision. We observe that text often contains cues\nthat explicitly match a mention to its type, in the\nform of the mention’s head word. For example,\n“the incumbent chairman of the African Union”\nis a type of “chairman.” This signal comple-\nments the supervision derived from linking entities\nto knowledge bases, which is context-oblivious.\nFor example, “Clint Eastwood” can be described\n\n88\n\nwith dozens of types, but context-sensitive typing\nwould prefer “director” instead of “mayor” for the\nsentence “Clint Eastwood won ‘Best Director’ for\nMillion Dollar Baby.”\n\nWe combine head-word supervision, which pro-\nvides ultra-fine type labels, with traditional sig-\nnals from entity linking. Although the problem is\nmore challenging at finer granularity, we find that\nmixing fine and coarse-grained supervision helps\nsignificantly, and that our proposed model with\na multitask objective exceeds the performance of\nexisting entity typing models. Lastly, we show\nthat head-word supervision can be used for previ-\nous formulations of entity typing, setting the new\nstate-of-the-art performance on an existing fine-\ngrained NER benchmark.\n\n2 Task and Data\n\nGiven a sentence and an entity mention e within\nit, the task is to predict a set of natural-language\nphrases T that describe the type of e. The selec-\ntion of T’ is context sensitive; for example, in “Bill\nGates has donated billions to eradicate malaria,’\nBill Gates should be typed as “philanthropist” and\nnot “inventor.” This distinction is important for\ncontext-sensitive tasks such as coreference resolu-\ntion and question answering (e.g. “Which philan-\nthropist is trying to prevent malaria?”).\n\nWe annotate a dataset of about 6,000 mentions\nvia crowdsourcing (Section 2.1), and demonstrate\nthat using an large type vocabulary substantially\nincreases annotation coverage and diversity over\nexisting approaches (Section 2.2).\n", "vlm_text": "The image is a visual comparison of three datasets, demonstrating the diversity and granularity of labeled data types within each dataset. The caption describes this comparison, highlighting how these datasets cover different proportions of specific label types.\n\n1. **(a) Our Dataset**: This visualization shows a large central bubble labeled \"Person,\" surrounded by many smaller bubbles with labels such as \"leader,\" \"object,\" \"event,\" \"organization,\" and more. The abundance of different smaller bubbles indicates a diverse and fine-grained dataset, where the label \"Person\" covers a significant yet relatively smaller percentage of the dataset compared to the others.\n\n2. **(b) OntoNotes**: This shows a large bubble labeled \"Other\" taking up a significant portion of the space, with smaller bubbles labeled \"/company,\" \"/location,\" \"/person,\" \"/organization,\" \"/legal,\" and \"/country.\" This indicates that OntoNotes has broader, less specific categories, with \"Other\" being the most frequent category in the dataset.\n\n3. **(c) FIGER**: Here, the bubble labeled \"Person\" is the largest among various labeled categories such as \"/organization,\" \"/location,\" \"/event,\" and others. FIGER, similar to OntoNotes, has a few large categories, with \"Person\" being the most frequent label.\n\nOverall, the image illustrates that \"Our Dataset\" is more diverse and fine-grained than the other two, as indicated by the many different smaller bubbles representing a variety of specific labels. In contrast, OntoNotes and FIGER have fewer categories, with a significant portion of their data concentrated in a handful of broad labels.\nsible concepts even within a limited domain. This can be seen empirically in existing datasets, where the label distribution of ﬁne-grained entity typing datasets is heavily skewed toward coarse-grained types. For instance, annotators of the OntoNotes dataset ( Gillick et al. ,  2014 ) marked about half of the mentions as “other,” because they could not ﬁnd a suitable type in their ontology (see Figure  1 for a visualization and Section  2.2  for details). \nOur more open, ultra-ﬁne vocabulary, where types are free-form noun phrases, alleviates the need for hand-crafted ontologies, thereby greatly increasing overall type coverage. To better un- derstand entity types in an unrestricted setting, we crowdsource a new dataset of 6,000 examples. Compared to previous ﬁne-grained entity typing datasets, the label distribution in our data is sub- stantially more  diverse  and  ﬁne-grained . Annota- tors easily generate a wide range of types and can determine with   $85\\%$   agreement if a type generated by another annotator is appropriate. Our evalu- ation data has over 2,500 unique types, posing a challenging learning problem. \nWhile our types are harder to predict, they also allow for a new form of contextual distant super- vision. We observe that text often contains cues that explicitly match a mention to its type, in the form of the mention’s head word. For example, “the incumbent chairman of the African Union” is a type of “chairman.” This signal comple- ments the supervision derived from linking entities to knowledge bases, which is context-oblivious. For example, “Clint Eastwood” can be described with dozens of types, but context-sensitive typing would prefer “director” instead of “mayor” for the sentence “Clint Eastwood won ‘Best Director’ for Million Dollar Baby.” \n\nWe combine head-word supervision, which pro- vides ultra-ﬁne type labels, with traditional sig- nals from entity linking. Although the problem is more challenging at ﬁner granularity, we ﬁnd that mixing ﬁne and coarse-grained supervision helps signiﬁcantly, and that our proposed model with a multitask objective exceeds the performance of existing entity typing models. Lastly, we show that head-word supervision can be used for previ- ous formulations of entity typing, setting the new state-of-the-art performance on an existing ﬁne- grained NER benchmark. \n2 Task and Data \nGiven a sentence and an entity mention    $e$   within it, the task is to predict a set of natural-language phrases    $T$   that describe the type of    $e$  . The selec- tion of  $T$  is context sensitive; for example, in “Bill Gates has donated billions to eradicate malaria,” Bill Gates should be typed as “philanthropist” and not “inventor.” This distinction is important for context-sensitive tasks such as coreference resolu- tion and question answering (e.g. “Which philan- thropist is trying to prevent malaria?”). \nWe annotate a dataset of about 6,000 mentions via crowdsourcing (Section  2.1 ), and demonstrate that using an large type vocabulary substantially increases annotation coverage and diversity over existing approaches (Section  2.2 ). "}
{"page": 2, "image_path": "doc_images/P18-1009_2.jpg", "ocr_text": "2.1 Crowdsourcing Entity Types\n\nTo capture multiple domains, we sample sentences\nfrom Gigaword (Parker et al., 2011), OntoNotes\n(Hovy et al., 2006), and web articles (Singh et al.,\n2012). We select entity mentions by taking max-\nimal noun phrases from a constituency parser\n(Manning et al., 2014) and mentions from a coref-\nerence resolution system (Lee et al., 2017).\n\nWe provide the sentence and the target entity\nmention to five crowd workers on Mechanical\nTurk, and ask them to annotate the entity’s type.\nTo encourage annotators to generate fine-grained\ntypes, we require at least one general type (e.g.\nperson, organization, location) and two specific\ntypes (e.g. doctor, fish, religious institute), from\na type vocabulary of about 10K frequent noun\nphrases. We use WordNet (Miller, 1995) to ex-\npand these types automatically by generating all\ntheir synonyms and hypernyms based on the most\ncommon sense, and ask five different annotators to\nvalidate the generated types. Each pair of annota-\ntors agreed on 85% of the binary validation deci-\nsions (i.e. whether a type is suitable or not) and\n0.47 in Fleiss’s «. To further improve consistency,\nthe final type set contained only types selected by\nat least 3/5 annotators. Further crowdsourcing de-\ntails are available in the supplementary material.\n\nOur collection process focuses on precision.\nThus, the final set is diverse but not comprehen-\nsive, making evaluation non-trivial (see Section 5).\n\n2.2 Data Analysis\n\nWe collected about 6,000 examples. For analysis,\n\nwe classified each type into three disjoint bins:\n\ne 9 general types: person, location, object, orga-\nnization, place, entity, object, time, event\n\ne 121 fine-grained types, mapped to fine-grained\nentity labels from prior work (Ling and Weld,\n2012; Gillick et al., 2014) (e.g. film, athlete)\n\ne 10,201 ultra-fine types, encompassing every\nother label in the type space (e.g. detective, law-\nsuit, temple, weapon, composer)\n\nOn average, each example has 5 labels: 0.9 gen-\n\neral, 0.6 fine-grained, and 3.9 ultra-fine types.\n\nAmong the 10,000 ultra-fine types, 2,300 unique\n\ntypes were actually found in the 6,000 crowd-\n\nsourced examples. Nevertheless, our distant su-\npervision data (Section 3) provides positive train-\ning examples for every type in the entire vocabu-\nlary, and our model (Section 4) can and does pre-\ndict from a 10K type vocabulary. For example,\n\n89\n\n— Our Dataset\n--- FIGER\n* OntoNotes\n\nCumulative Proportion of Covered labels\n\nT T T T\n\nNumber of Labels\n\nFigure 2: The label distribution across different\nevaluation datasets. In existing datasets, the top\n4 or 7 labels cover over 80% of the labels. In ours,\nhe top 50 labels cover less than 50% of the data.\n\nhe model correctly predicts “television network”\nand “archipelago” for some mentions, even though\nhat type never appears in the 6,000 crowdsourced\nexamples.\n\nImproving Type Coverage We observe that\nprior fine-grained entity typing datasets are heav-\nily focused on coarse-grained types. To quan-\nify our observation, we calculate the distribu-\nion of types in FIGER (Ling and Weld, 2012),\nOntoNotes (Gillick et al., 2014), and our data.\nFor examples with multiple types (|Z| > 1), we\ncounted each type 1/|T' times.\n\nFigure 2 shows the percentage of labels covered\nby the top N labels in each dataset. In previous\nenitity typing datasets, the distribution of labels\nis highly skewed towards the top few labels. To\ncover 80% of the examples, FIGER requires only\nthe top 7 types, while OntoNotes needs only 4; our\ndataset requires 429 different types.\n\nFigure | takes a deeper look by visualizing the\ntypes that cover 90% of the data, demonstrating\nthe diversity of our dataset. It is also striking that\nmore than half of the examples in OntoNotes are\nclassified as “other,” perhaps because of the limi-\ntation of its predefined ontology.\n\nImproving Mention Coverage Existing\ndatasets focus mostly on named entity mentions,\nwith the exception of OntoNotes, which contained\nnominal expressions. This has implications on\nthe transferability of FIGER/OntoNotes-based\nmodels to tasks such as coreference resolution,\nwhich need to analyze all types of entity mentions\n(pronouns, nominal expressions, and named entity\n", "vlm_text": "2.1 Crowdsourcing Entity Types \nTo capture multiple domains, we sample sentences from Gigaword ( Parker et al. ,  2011 ), OntoNotes ( Hovy et al. ,  2006 ), and web articles ( Singh et al. , 2012 ). We select entity mentions by taking max- imal noun phrases from a constituency parser ( Manning et al. ,  2014 ) and mentions from a coref- erence resolution system ( Lee et al. ,  2017 ). \nWe provide the sentence and the target entity mention to ﬁve crowd workers on Mechanical Turk, and ask them to annotate the entity’s type. To encourage annotators to generate ﬁne-grained types, we require at least one general type (e.g. person, organization, location) and two speciﬁc types (e.g. doctor, ﬁsh, religious institute), from a type vocabulary of about 10K frequent noun phrases. We use WordNet ( Miller ,  1995 ) to ex- pand these types automatically by generating all their synonyms and hypernyms based on the most common sense, and ask ﬁve different annotators to validate the generated types. Each pair of annota- tors agreed on   $85\\%$   of the binary validation deci- sions (i.e. whether a type is suitable or not) and 0.47 in Fleiss’s    $\\kappa$  . To further improve consistency, the ﬁnal type set contained only types selected by at least  $3/5$   annotators. Further crowdsourcing de- tails are available in the supplementary material. \nOur collection process focuses on precision. Thus, the ﬁnal set is diverse but not comprehen- sive, making evaluation non-trivial (see Section  5 ). \n2.2 Data Analysis \nWe collected about 6,000 examples. For analysis, we classiﬁed each type into three disjoint bins:\n\n \n•  9  general  types: person, location, object, orga- nization, place, entity, object, time, event\n\n •  121  ﬁne-grained  types, mapped to ﬁne-grained entity labels from prior work ( Ling and Weld , 2012 ;  Gillick et al. ,  2014 ) (e.g. ﬁlm, athlete)\n\n •  10,201  ultra-ﬁne  types, encompassing every other label in the type space (e.g. detective, law- suit, temple, weapon, composer) \nOn average, each example has 5 labels: 0.9 gen- eral, 0.6 ﬁne-grained, and 3.9 ultra-ﬁne types. Among the 10,000 ultra-ﬁne types, 2,300 unique types were actually found in the 6,000 crowd- sourced examples. Nevertheless, our distant su- pervision data (Section  3 ) provides positive train- ing examples for every type in the entire vocabu- lary, and our model (Section  4 ) can and does pre- dict from a 10K type vocabulary. For example, \nThe image is a line graph showing the cumulative proportion of covered labels as the number of labels increases, across three different evaluation datasets: \"Our Dataset,\" \"FIGER,\" and \"OntoNotes.\" The x-axis represents the number of labels, ranging from 0 to 50, while the y-axis represents the cumulative proportion of covered labels, ranging from 0.0 to 1.0.\n\nThe graph has three lines representing three datasets:\n1. A blue solid line for \"Our Dataset,\" which shows a gradual increase in the cumulative proportion of covered labels as the number of labels increases. Even with 50 labels, it covers less than 50% of the total data.\n2. A red dashed line for \"FIGER,\" which shows a steep increase initially, reaching over 80% coverage with fewer than 10 labels, and then plateauing.\n3. A green dotted line for \"OntoNotes,\" which also shows a rapid increase and reaches over 80% coverage with fewer than 7 labels, and then levels off.\n\nThe caption clarifies that the existing datasets (FIGER and OntoNotes) have a high concentration of coverage within the top few labels, whereas their dataset achieves broader coverage with a more diverse spread across 50 labels.\nthe model correctly predicts “television network” and “archipelago” for some mentions, even though that type never appears in the 6,000 crowdsourced examples. \nImproving Type Coverage We observe that prior ﬁne-grained entity typing datasets are heav- ily focused on coarse-grained types. To quan- tify our observation, we calculate the distribu- tion of types in FIGER ( Ling and Weld ,  2012 ), OntoNotes ( Gillick et al. ,  2014 ), and our data. For examples with multiple types   $(|T|\\,>\\,1)$  , we counted each type    $1/|T|$   times. \nFigure  2  shows the percentage of labels covered by the top    $N$   labels in each dataset. In previous enitity typing datasets, the distribution of labels is highly skewed towards the top few labels. To cover   $80\\%$   of the examples, FIGER requires only the top 7 types, while OntoNotes needs only 4; our dataset requires 429 different types. \nFigure  1  takes a deeper look by visualizing the types that cover   $90\\%$   of the data, demonstrating the diversity of our dataset. It is also striking that more than half of the examples in OntoNotes are classiﬁed as “other,” perhaps because of the limi- tation of its predeﬁned ontology. \nImproving Mention Coverage Existing datasets focus mostly on named entity mentions, with the exception of OntoNotes, which contained nominal expressions. This has implications on the transferability of FIGER/OntoNotes-based models to tasks such as coreference resolution, which need to analyze all types of entity mentions (pronouns, nominal expressions, and named entity "}
{"page": 3, "image_path": "doc_images/P18-1009_3.jpg", "ocr_text": "Source Example Sentence Labels Size Prec.\n. . Western powers that brokered the proposed deal in Vi- | power\nHead Words enna are likely to balk, said Valerie Lincy, a researcher\nwith the Wisconsin Project. 20M | 80.4%\nAlexis Kaniaris, CEO of the organizing company Eu- | radio, station, —ra-\nropartners, explained, speaking in a radio program in na- | dio_station\ntional radio station NET.\nEntity Linking | Toyota recalled more than 8 million vehicles globally over | manufacturer 2.7™M | 77.7%\n+ Definitions sticky pedals that can become entrapped in floor mats.\nEntity Linking | Iced Earth’s musical style is influenced by many traditional | person, 2.5M | 77.6%\n+KB heavy metal groups such as Black Sabbath. author, mu:\nTable 2: Distant supervision examples and statistics. We extracted the headword and Wikipedia def-\n\ninition supervision from Gigaword and Wikilink corpora. KB-based supervision is mapped from prior\n\nwork, which used Wikipedia and news corpora.\n\nmentions). Our new dataset provides a well-\nrounded benchmark with roughly 40% pronouns,\n38% nominal expressions, and 22% named entity\nmentions. The case of pronouns is particularly\ninteresting, since the mention itself provides little\ninformation.\n\n3 Distant Supervision\n\nTraining data for fine-grained NER systems is\ntypically obtained by linking entity mentions and\ndrawing their types from knowledge bases (KBs).\nThis approach has two limitations: recall can suf-\nfer due to KB incompleteness (West et al., 2014),\nand precision can suffer when the selected types\ndo not fit the context (Ritter et al., 2011). We al-\nleviate the recall problem by mining entity men-\ntions that were linked to Wikipedia in HTML,\nand extract relevant types from their encyclope-\ndic definitions (Section 3.1). To address the pre-\ncision issue (context-insensitive labeling), we pro-\npose a new source of distant supervision: auto-\nmatically extracted nominal head words from raw\ntext (Section 3.2). Using head words as a form\nof distant supervision provides fine-grained infor-\nmation about named entities and nominal men-\ntions. While a KB may link “the 44th president\nof the United States” to many types such as author,\nlawyer, and professor, head words provide only the\ntype “president”, which is relevant in the context.\n\nWe experiment with the new distant supervi-\nsion sources as well as the traditional KB super-\nvision. Table 2 shows examples and statistics for\neach source of supervision. We annotate 100 ex-\namples from each source to estimate the noise and\nusefulness in each signal (precision in Table 2).\n\n90\n\n3.1 Entity Linking\n\nFor KB supervision, we leveraged training data\nfrom prior work (Ling and Weld, 2012; Gillick\net al., 2014) by manually mapping their ontology\nto our 10,000 noun type vocabulary, which cov-\ners 130 of our labels (general and fine-grained).”\nSection 6 defines this mapping in more detail.\n\nTo improve both entity and type coverage of KB\nsupervision, we use definitions from Wikipedia.\nWe follow Shnarch et al. () who observed that the\nfirst sentence of a Wikipedia article often states\nhe entity’s type via an “is a” relation; for exam-\nple, “Roger Federer is a Swiss professional tennis\nplayer.” Since we are using a large type vocabu-\nlary, we can now mine this typing information.*\nWe extracted descriptions for 3.1M entities which\ncontain 4,600 unique type labels such as ““compe-\nition,” “movement,” and “village.”\n\nWe bypass the challenge of automatically link-\ning entities to Wikipedia by exploiting existing hy-\nperlinks in web pages (Singh et al., 2012), fol-\nlowing prior work (Ling and Weld, 2012; Yosef\net al., 2012). Since our heuristic extraction of\nypes from the definition sentence is somewhat\nnoisy, we use a more conservative entity linking\npolicy* that yields a signal with similar overall ac-\ncuracy to KB-linked data.\n\nData from:\nshimaokasonse/NFGEC\n\n3We extract types by applying a dependency parser (Man-\nning et al., 2014) to the definition sentence, and taking nouns\nthat are dependents of a copular edge or connected to nouns\nlinked to copulars via appositive or conjunctive edges.\n\nhttps://github.com/\n\nOnly link if the mention contains the Wikipedia entity’s\nname and the entity’s name contains the mention’s head.\n", "vlm_text": "The table provides information on data sources and their characteristics. It is divided into five columns: \"Source,\" \"Example Sentence,\" \"Labels,\" \"Size,\" and \"Prec. (Precision).\"\n\n1. **Source:** \n   - \"Head Words\"\n   - \"Entity Linking + Definitions\"\n   - \"Entity Linking + KB\"\n\n2. **Example Sentence:**\n   - For \"Head Words,\" examples include discussions about Western powers in Vienna and a Greek radio program.\n   - For \"Entity Linking + Definitions,\" the example sentence talks about Toyota recalling vehicles.\n   - For \"Entity Linking + KB,\" the example describes Iced Earth’s musical influence from Black Sabbath.\n\n3. **Labels:** \n   - \"Head Words\" has labels such as \"power\" and \"radio, station, radio_station.\"\n   - \"Entity Linking + Definitions\" includes the label \"manufacturer.\"\n   - \"Entity Linking + KB\" has labels like \"person, artist, actor, author, musician.\"\n\n4. **Size:** \n   - \"Head Words\" is 20 million.\n   - \"Entity Linking + Definitions\" is 2.7 million.\n   - \"Entity Linking + KB\" is 2.5 million.\n\n5. **Prec. (Precision):**\n   - \"Head Words\" has a precision of 80.4%.\n   - \"Entity Linking + Definitions\" has a precision of 77.7%.\n   - \"Entity Linking + KB\" has a precision of 77.6%.\nmentions). Our new dataset provides a well- rounded benchmark with roughly   $40\\%$   pronouns,  $38\\%$   nominal expressions, and  $22\\%$   named entity mentions. The case of pronouns is particularly interesting, since the mention itself provides little information. \n3 Distant Supervision \nTraining data for ﬁne-grained NER systems is typically obtained by linking entity mentions and drawing their types from knowledge bases (KBs). This approach has two limitations: recall can suf- fer due to KB incompleteness ( West et al. ,  2014 ), and precision can suffer when the selected types do not ﬁt the context ( Ritter et al. ,  2011 ). We al- leviate the recall problem by mining entity men- tions that were linked to Wikipedia in HTML, and extract relevant types from their encyclope- dic deﬁnitions (Section  3.1 ). To address the pre- cision issue (context-insensitive labeling), we pro- pose a new source of distant supervision: auto- matically extracted nominal head words from raw text (Section  3.2 ). Using head words as a form of distant supervision provides ﬁne-grained infor- mation about named entities and nominal men- tions. While a KB may link “the 44th president of the United States” to many types such as author, lawyer, and professor, head words provide only the type “president”, which is relevant in the context. \nWe experiment with the new distant supervi- sion sources as well as the traditional KB super- vision. Table  2  shows examples and statistics for each source of supervision. We annotate 100 ex- amples from each source to estimate the noise and usefulness in each signal (precision in Table 2). \n3.1 Entity Linking \nFor KB supervision, we leveraged training data from prior work ( Ling and Weld ,  2012 ;  Gillick et al. ,  2014 ) by manually mapping their ontology to our 10,000 noun type vocabulary, which cov- ers 130 of our labels (general and ﬁne-grained). Section 6 deﬁnes this mapping in more detail. \nTo improve both entity and type coverage of KB supervision, we use deﬁnitions from Wikipedia. We follow Shnarch et al. () who observed that the ﬁrst sentence of a Wikipedia article often states the entity’s type via an “is a” relation; for exam- ple, “Roger Federer is a Swiss professional tennis player.” Since we are using a large type vocabu- lary, we can now mine this typing information. We extracted descriptions for 3.1M entities which contain 4,600 unique type labels such as “compe- tition,” “movement,” and “village.” \nWe bypass the challenge of automatically link- ing entities to Wikipedia by exploiting existing hy- perlinks in web pages ( Singh et al. ,  2012 ), fol- lowing prior work ( Ling and Weld ,  2012 ;  Yosef et al. ,  2012 ). Since our heuristic extraction of types from the deﬁnition sentence is somewhat noisy, we use a more conservative entity linking policy 4   that yields a signal with similar overall ac- curacy to KB-linked data. "}
{"page": 4, "image_path": "doc_images/P18-1009_4.jpg", "ocr_text": "3.2. Contextualized Supervision\n\nMany nominal entity mentions include detailed\ntype information within the mention itself. For\nexample, when describing Titan V as “the newly-\nreleased graphics card”, the head words and\nphrases of this mention (“graphics card” and\n“card’”) provide a somewhat noisy, but very easy\nto gather, context-sensitive type signal.\n\nWe extract nominal head words with a depen-\ndency parser (Manning et al., 2014) from the Gi-\ngaword corpus as well as the Wikilink dataset.\nTo support multiword expressions, we included\nnouns that appear next to the head if they form a\nphrase in our type vocabulary. Finally, we lower-\ncase all words and convert plural to singular.\n\nOur analysis reveals that this signal has a com-\nparable accuracy to the types extracted from en-\ntity linking (around 80%). Many errors are from\nthe parser, and some errors stem from idioms and\ntransparent heads (e.g. “parts of capital” labeled as\n“part”). While the headword is given as an input\nto the model, with heavy regularization and multi-\ntasking with other supervision sources, this super-\nvision helps encode the context.\n\n4 Model\n\nWe design a model for predicting sets of types\ngiven a mention in context. The architec-\nture resembles the recent neural AttentiveNER\nmodel (Shimaoka et al., 2017), while improving\nthe sentence and mention representations, and in-\ntroducing a new multitask objective to handle mul-\ntiple sources of supervision. The hyperparameter\nsettings are listed in the supplementary material.\n\nContext Representation Given a_ sentence\n%1,...,%p, We represent each token x; using a\npre-trained word embedding w;. We concate-\nnate an additional location embedding J; which\nindicates whether x; is before, inside, or after\nthe mention. We then use [{2;;1;] as an input to a\nbidirectional LSTM, producing a contextualized\nrepresentation h,; for each token; this is different\nfrom the architecture of Shimaoka et al. 2017,\nwho used two separate bidirectional LSTMs on\neach side of the mention. Finally, we represent the\ncontext c as a weighted sum of the contextualized\ntoken representations using MLP-based attention:\n\na; = SoftMax;(va - relu(Wahi))\n\nWhere W, and vq are the parameters of the atten-\ntion mechanism’s MLP, which allows interaction\n\n91\n\nbetween the forward and backward directions of\nthe LSTM before computing the weight factors.\n\nMention Representation We represent the\nmention m as the concatenation of two items:\n(a) a character-based representation produced\nby a CNN on the entire mention span, and (b) a\nweighted sum of the pre-trained word embeddings\nin the mention span computed by attention,\nsimilar to the mention representation in a recent\ncoreference resolution model (Lee et al., 2017).\nThe final representation is the concatenation of the\ncontext and mention representations: r = [c; m].\n\nLabel Prediction We learn a type label embed-\nding matrix W; € R”*¢ where n is the number of\nlabels in the prediction space and d is the dimen-\nsion of r. This matrix can be seen as a combination\nof three sub matrices, Wgeneral; Wine, Wuttra,\neach of which contains the representations of the\ngeneral, fine, and ultra-fine types respectively. We\npredict each type’s probability via the sigmoid of\nits inner product with r: y = o(W;r). We predict\nevery type t for which y, > 0.5, or arg max y; if\nhere is no such type.\n\nMultitask Objective The distant supervision\nsources provide partial supervision for ultra-fine\nypes; KBs often provide more general types,\nwhile head words usually provide only ultra-fine\nypes, without their generalizations. In other\nwords, the absence of a type at a different level\nof abstraction does not imply a negative signal;\ne.g. when the head word is “inventor”, the model\nshould not be discouraged to predict “person”.\n\nPrior work used a customized hinge loss (Ab-\nhishek et al., 2017) or max margin loss (Ren et al.,\n2016a) to improve robustness to noisy or incom-\nplete supervision. We propose a multitask objec-\ntive that reflects the characteristic of our training\ndataset. Instead of updating all labels for each ex-\nample, we divide labels into three bins (general,\nfine, and ultra-fine), and update labels only in bin\ncontaining at least one positive label. Specifically,\nthe training objective is to minimize J where t is\nthe target vector at each granularity:\n\nTait = Igeneral . general (t)\nT Taine . Lfine (t)\nTr Fruttra . Luttra(t)\n\nWhere Icategory(t) is an indicator function that\nchecks if ¢ contains a type in the category, and\n", "vlm_text": "3.2 Contextualized Supervision \nMany nominal entity mentions include detailed type information within the mention itself. For example, when describing Titan  $\\mathrm{v}$   as “the newly- released graphics card”, the head words and phrases of this mention (“graphics card” and “card”) provide a somewhat noisy, but very easy to gather, context-sensitive type signal. \nWe extract nominal head words with a depen- dency parser ( Manning et al. ,  2014 ) from the Gi- gaword corpus as well as the Wikilink dataset. To support multiword expressions, we included nouns that appear next to the head if they form a phrase in our type vocabulary. Finally, we lower- case all words and convert plural to singular. \nOur analysis reveals that this signal has a com- parable accuracy to the types extracted from en- tity linking (around   $80\\%$  ). Many errors are from the parser, and some errors stem from idioms and transparent heads (e.g. “parts of capital” labeled as “part”). While the headword is given as an input to the model, with heavy regularization and multi- tasking with other supervision sources, this super- vision helps encode the context. \n4 Model \nWe design a model for predicting sets of types given a mention in context. The architec- ture resembles the recent neural AttentiveNER model ( Shimaoka et al. ,  2017 ), while improving the sentence and mention representations, and in- troducing a new multitask objective to handle mul- tiple sources of supervision. The hyperparameter settings are listed in the supplementary material. \nContext Representation Given a sentence  $x_{1},\\ldots,x_{n}$  , we represent each token    $x_{i}$   using a pre-trained word embedding    $w_{i}$  . We concate- nate an additional location embedding    $l_{i}$   which indicates whether    $x_{i}$   is before, inside, or after the mention. We then use    $[x_{i};l_{i}]$   as an input to a bidirectional LSTM, producing a contextualized representation    $h_{i}$   for each token; this is different from the architecture of Shimaoka et al. 2017 , who used two separate bidirectional LSTMs on each side of the mention. Finally, we represent the context  $c$   as a weighted sum of the contextualized token representations using MLP-based attention: \n\n$$\na_{i}=\\mathrm{SoftMax}_{i}(v_{a}\\cdot\\mathrm{relu}(W_{a}h_{i}))\n$$\n \nWhere    $W_{a}$   and  $v_{a}$   are the parameters of the atten- tion mechanism’s MLP, which allows interaction between the forward and backward directions of the LSTM before computing the weight factors. \n\nMention Representation We represent the mention    $m$   as the concatenation of two items: (a) a character-based representation produced by a CNN on the entire mention span, and (b) a weighted sum of the pre-trained word embeddings in the mention span computed by attention, similar to the mention representation in a recent coreference resolution model ( Lee et al. ,  2017 ). The ﬁnal representation is the concatenation of the context and mention representations:    $r=[c;m]$  . \nLabel Prediction We learn a type label embed- ding matrix    $W_{t}\\in\\mathbb{R}^{n\\times d}$    where  $n$   i the number of labels in the prediction space and  d  is the dimen- sion of    $r$  . This matrix can be seen as a combination of three sub matrices,    $W_{g e n e r a l},W_{f i n e},W_{u l t r a},$  each of which contains the representations of the general, ﬁne, and ultra-ﬁne types respectively. We predict each type’s probability via the sigmoid of its inner product with  r :    $y=\\sigma(W_{t}r)$  . We predict every type    $t$   for which    $y_{t}\\,>\\,0.5$  , or  arg max    $y_{t}$   if there is no such type. \nMultitask Objective The distant supervision sources provide partial supervision for ultra-ﬁne types; KBs often provide more general types, while head words usually provide only ultra-ﬁne types, without their generalizations. In other words, the absence of a type at a different level of abstraction does not imply a negative signal; e.g. when the head word is “inventor”, the model should not be discouraged to predict “person”. \nPrior work used a customized hinge loss ( Ab- hishek et al. ,  2017 ) or max margin loss ( Ren et al. , 2016a ) to improve robustness to noisy or incom- plete supervision. We propose a multitask objec- tive that reﬂects the characteristic of our training dataset. Instead of updating all labels for each ex- ample, we divide labels into three bins (general, ﬁne, and ultra-ﬁne), and update labels only in bin containing at least one positive label. Speciﬁcally, the training objective is to minimize    $J$   where    $t$   is the target vector at each granularity: \n\n$$\n\\begin{array}{r l}&{J_{\\mathrm{all}}=J_{\\mathrm{general}}\\cdot\\mathbb{1}_{\\mathrm{general}}(t)}\\\\ &{~~~~+\\;J_{\\mathrm{fine}}\\cdot\\mathbb{1}_{\\mathrm{fine}}(t)}\\\\ &{~~~~+\\;J_{\\mathrm{intra}}\\cdot\\mathbb{1}_{\\mathrm{intra}}(t)}\\end{array}\n$$\n \nWhere    $\\mathbb{1}_{\\mathrm{cageogy}}(t)$   is an indicator function that checks if    $t$   contains a type in the category, and "}
{"page": 5, "image_path": "doc_images/P18-1009_5.jpg", "ocr_text": "Dev Test\nModel | MRRP RFI | MRRP RFI\nAttentiveNER | 0.221 537 15.0 23.5 | 0.223. 542 15.2 23.7\nOurModel | 0.229 48.1 232 313 | 0.234 47.1 242 32.0\n\nTable 3: Performance of our model and AttentiveNER (Shimaoka et al., 2017) on the new entity typing\n\nbenchmark, using same training data. We show results for both development and\n\nest sets.\n\n. Total General (1918) Fine (1289) Ultra-Fine (7594)\nTrain Data | wirR PROFIL | P oR. FI | P oR. FI P oR. FI\nAll 0.229 48.1 23.2 313] 60.3 61.6 61.0 | 404 384 394 | 428 88 146\n—Crowd | 0.173 40.1 148 216] 537 45.6 49.3] 208 185 196] 544 46 84\n— Head 0.220 50.3 19.6 28.2 | 588 628 60.7 | 444 298 35.6 | 462 4.7 85\n-EL 0.225 48.4 22.3 306 | 62.2 60.1 612] 40.3 261 31.7] 414 99 160\n\nTable 4: Results on the development set for different type granularity and for dif\n\nerent supervision data\n\nwith our model. In each row, we remove a single source of supervision. Entity linking (EL) includes\nsupervision from both KB and Wikipedia definitions. The numbers in the first row are example counts\n\nfor each type granularity.\n\nIeategory is the category-specific logistic regression\nobjective:\n\nJ=—- ti - log(y;) + (1 — tj) -log(1 — yx)\n\n5 Evaluation\n\nExperiment Setup The crowdsourced dataset\n(Section 2.1) was randomly split into train, devel-\nopment, and test sets, each with about 2,000 ex-\namples. We use this relatively small manually-\nannotated training set (Crowd in Table 4) along-\nside the two distant supervision sources: entity\nlinking (KB and Wikipedia definitions) and head\nwords. To combine supervision sources of differ-\nent magnitudes (2K crowdsourced data, 4.7M en-\ntity linking data, and 20M head words), we sample\na batch of equal size from each source at each it-\neration. We reimplement the recent AttentiveNER\nmodel (Shimaoka et al., 2017) for reference.>\n\nWe report macro-averaged precision, recall, and\nFI, and the average mean reciprocal rank (MRR).\n\nResults Table 3 shows the performance of\nour model and our reimplementation of Atten-\ntiveNER. Our model, which uses a multitask ob-\njective to learn finer types without punishing more\ngeneral types, shows recall gains at the cost of\ndrop in precision. The MRR score shows that our\n\n5We use the AttentiveNER model with no engineered fea-\ntures or hierarchical label encoding (as a hierarchy is not clear\nin our label setting) and let it predict from the same label\nspace, training with the same supervision data.\n\n92\n\nmodel is slightly better than the baseline at ranking\ncorrect types above incorrect ones.\n\nTable 4 shows the performance breakdown for\ndifferent type granularity and different supervi-\nsion. Overall, as seen in previous work on fine-\ngrained NER literature (Gillick et al., 2014; Ren\net al., 2016a), finer labels were more challenging\nto predict than coarse grained labels, and this is-\nsue is exacerbated when dealing with ultra-fine\ntypes. All sources of supervision appear to be\nuseful, with crowdsourced examples making the\nbiggest impact. Head word supervision is par-\nticularly helpful for predicting ultra-fine labels,\nwhile entity linking improves fine label prediction.\nThe low general type performance is partially be-\ncause of nominal/pronoun mentions (e.g. “it”),\nand because of the large type inventory (some-\nimes “location” and “place” are annotated inter-\nchangeably).\n\nAnalysis We manually analyzed 50 examples\nrom the development set, four of which we\npresent in Table 5. Overall, the model was able to\ngenerate accurate general types and a diverse set of\nype labels. Despite our efforts to annotate a com-\nprehensive type set, the gold labels still miss many\npotentially correct labels (example (a): “man” is\nreasonable but counted as incorrect). This makes\nhe precision estimates lower than the actual per-\normance level, with about half the precision er-\nrors belonging to this category. Real precision\nerrors include predicting co-hyponyms (example\n(b): “accident” instead of “‘attack”), and types that\n\n", "vlm_text": "The table compares the performance of two models, \"AttentiveNER\" and \"Our Model,\" across development (Dev) and test datasets. The performance metrics include Mean Reciprocal Rank (MRR), Precision (P), Recall (R), and F1-score (F1). For each dataset (Dev and Test), the table provides the MRR score followed by the precision, recall, and F1-score. \n\n- For the Dev dataset:\n  - AttentiveNER: MRR is 0.221, Precision is 53.7, Recall is 15.0, and F1-score is 23.5.\n  - Our Model: MRR is 0.229, Precision is 48.1, Recall is 23.2, and F1-score is 31.3.\n\n- For the Test dataset:\n  - AttentiveNER: MRR is 0.223, Precision is 54.2, Recall is 15.2, and F1-score is 23.7.\n  - Our Model: MRR is 0.234, Precision is 47.1, Recall is 24.2, and F1-score is 32.0.\n\n\"Our Model\" demonstrates slightly better MRR scores on both Dev and Test datasets compared to \"AttentiveNER,\" and significantly superior Recall and F1 scores, while \"AttentiveNER\" has higher Precision scores.\nThe table presents performance metrics across different datasets. It is structured to display various evaluation metrics for models trained on different portions of the dataset. Here's the breakdown:\n\n- **Columns:**\n  - The first column lists different training datasets or conditions: \"All\", \"– Crowd\", \"– Head\", and \"– EL\".\n  - The next set of columns (Total, General, Fine, Ultra-Fine) provides the evaluation metrics for each dataset category:\n    - **MRR (Mean Reciprocal Rank)** is shown in a standalone column.\n    - **Total, General, Fine, and Ultra-Fine** each include three sub-columns: Precision (P), Recall (R), and F1-score (F1). The numbers in parentheses next to General, Fine, and Ultra-Fine indicate the number of instances in each category.\n\n- **Rows:**\n  - **All**: Metrics when the model is trained on the entire dataset.\n  - **– Crowd**: Metrics when the crowd-sourced data is excluded from the training.\n  - **– Head**: Metrics when the head portion of the dataset is excluded.\n  - **– EL**: Metrics when entity linking data is excluded.\n\n**Notable Figures:**\n- The highest values in each column seem to be emphasized in bold.\n- The table provides detailed metrics on the impact of excluding parts of the data on the training performance, testing varied aspects of fine-grained and ultra-fine grained categorization tasks. \n\nThis type of table is commonly used in research papers to detail how different data compositions affect model performance.\nTable 4: Results on the development set for different type granularity and for different supervision data with our model. In each row, we remove a single source of supervision. Entity linking (EL) includes supervision from both KB and Wikipedia deﬁnitions. The numbers in the ﬁrst row are example counts for each type granularity. \n $J_{\\mathrm{theory}}$   is the category-speciﬁc logistic regression objective: \n\n$$\nJ=-\\sum_{i}t_{i}\\cdot\\log(y_{i})+(1-t_{i})\\cdot\\log(1-y_{i})\n$$\n \n5 Evaluation \nExperiment Setup The crowdsourced dataset (Section  2.1 ) was randomly split into train, devel- opment, and test sets, each with about 2,000 ex- amples. We use this relatively small manually- annotated training set ( Crowd  in Table  4 ) along- side the two distant supervision sources: entity linking (KB and Wikipedia deﬁnitions) and head words. To combine supervision sources of differ- ent magnitudes (2K crowdsourced data, 4.7M en- tity linking data, and 20M head words), we sample a batch of equal size from each source at each it- eration. We reimplement the recent AttentiveNER model ( Shimaoka et al. ,  2017 ) for reference. \nWe report macro-averaged precision, recall, and F1, and the average mean reciprocal rank (MRR). \nResults Table  3  shows the performance of our model and our re implementation of Atten- tiveNER. Our model, which uses a multitask ob- jective to learn ﬁner types without punishing more general types, shows recall gains at the cost of drop in precision. The MRR score shows that our model is slightly better than the baseline at ranking correct types above incorrect ones. \n\nTable  4  shows the performance breakdown for different type granularity and different supervi- sion. Overall, as seen in previous work on ﬁne- grained NER literature ( Gillick et al. ,  2014 ;  Ren et al. ,  2016a ), ﬁner labels were more challenging to predict than coarse grained labels, and this is- sue is exacerbated when dealing with ultra-ﬁne types. All sources of supervision appear to be useful, with crowdsourced examples making the biggest impact. Head word supervision is par- ticularly helpful for predicting ultra-ﬁne labels, while entity linking improves ﬁne label prediction. The low general type performance is partially be- cause of nominal/pronoun mentions (e.g. “it”), and because of the large type inventory (some- times “location” and “place” are annotated inter- changeably). \nAnalysis We manually analyzed 50 examples from the development set, four of which we present in Table  5 . Overall, the model was able to generate accurate general types and a diverse set of type labels. Despite our efforts to annotate a com- prehensive type set, the gold labels still miss many potentially correct labels (example (a): “man” is reasonable but counted as incorrect). This makes the precision estimates lower than the actual per- formance level, with about half the precision er- rors belonging to this category. Real precision errors include predicting co-hyponyms (example (b): “accident” instead of “attack”), and types that "}
{"page": 6, "image_path": "doc_images/P18-1009_6.jpg", "ocr_text": "Example Bruguera said {he} had problems with his left leg and had grown tired early during the match .\n(a) | Annotation | person, athlete, player, adult, male, contestant\nPrediction | person, athlete, player, adult, male, contestant, defendant, man\nExample {The explosions} occurred on the night of October 7 , against the Hilton Taba and campsites used by\nIsraelis in Ras al-Shitan.\n(b) | Annotation | event calamity, attack, disaster\nPrediction | event, accident\nExample Similarly , Enterprise was considered for refit to replace Challenger after {the latter} was destroyed ,\nbut Endeavour was built from structural spares instead .\n(c) | Annotation | object, spacecraft, rocket, thing, vehicle, shuttle\nPrediction | event\nContext “ There is a wealth of good news in this report , and I ’m particularly encouraged by the progress {we}\nare making against AIDS , ” HHS Secretary Donna Shalala said in a statement.\n(d) | Annotation | government, group, organization, hospital,administration, socialist\nPrediction | government, group, person\nTable 5: Example and predictions from our best model on the development set. Entity mentions are\nmarked with curly brackets, the correct predictions are boldfaced, and the missing labels are italicized\nand written in red.\n\nmay be true, but are not supported by the context.\n\nWe found that the model often abstained from\npredicting any fine-grained types. Especially in\nchallenging cases as in example (c), the model\npredicts only general types, explaining the low re-\ncall numbers (28% of examples belong to this cat-\negory). Even when the model generated correct\nfine-grained types as in example (d), the recall was\noften fairly low since it did not generate a com-\nplete set of related fine-grained labels.\n\nEstimating the performance of a model in an in-\ncomplete label setting and expanding label cover-\nage are interesting areas for future work. Our task\nalso poses a potential modeling challenge; some-\ntimes, the model predicts two incongruous types\n(e.g. “location” and “person’”), which points to-\nwards modeling the task as a joint set prediction\ntask, rather than predicting labels individually. We\nprovide sample outputs on the project website.\n\n6 Improving Existing Fine-Grained NER\nwith Better Distant Supervision\n\nWe show that our model and distant supervision\ncan improve performance on an existing fine-\ngrained NER task. We chose the widely-used\nOntoNotes (Gillick et al., 2014) dataset which in-\ncludes nominal and named entity mentions.°\n\n®While we were inspired by FIGER (Ling and Weld,\n2012), the dataset presents technical difficulties. The test set\nhas only 600 examples, and the development set was labeled\nwith distant supervision, not manual annotation. We there-\nfore focus our evaluation on OntoNotes.\n\n93\n\nAugmenting the Training Data The original\nOntoNotes training set (ONTO in Tables 6 and 7)\nis extracted by linking entities to a KB. We supple-\nment this dataset with our two new sources of dis-\ntant supervision: Wikipedia definition sentences\n(WIKI) and head word supervision (HEAD) (see\nSection 3). To convert the label space, we manu-\nally map a single noun from our natural-language\nvocabulary to each formal-language type in the\nOntoNotes ontology. 77% of OntoNote’s types\ndirectly correspond to suitable noun labels (e.g.\n“doctor” to “/person/doctor”), whereas the other\ncases were mapped with minimal manual effort\n(e.g. “musician” to “person/artist/music’”, “politi-\ncian” to “/person/political_figure”). We then ex-\npand these labels according to the ontology to in-\nclude their hypernyms (‘/person/political_figure”\nwill also generate ‘“/person’”). Lastly, we create\nnegative examples by assigning the “/other” label\nto examples that are not mapped to the ontology.\nThe augmented dataset contains 2.5M/0.6M new\npositive/negative examples, of which 0.9M/0.1M\nare from Wikipedia definition sentences and\n1.6M/0.5M from head words.\n\nExperiment Setup We compare performance to\nother published results and to our reimplemen-\ntation of AttentiveNER (Shimaoka et al., 2017).\nWe also compare models trained with different\nsources of supervision. For this dataset, we did not\nuse our multitask objective (Section 4), since ex-\npanding types to include their ontological hyper-\nnyms largely eliminates the partial supervision as-\n", "vlm_text": "The table presents a comparison between human-generated annotations and predicted annotations for certain examples or contexts, as indicated in the left column. Each row represents a distinct example (labeled (a) to (d)) with associated annotations and predictions.\n\nIn more detail:\n- Column for Example/Context: This column contains a sentence or fragment with a highlighted word or phrase surrounded by curly braces (e.g., {he}, {The explosions}, {the latter}, {we}).\n- Annotation: This row lists the human-generated categories or labels for the highlighted words (e.g., \"person, athlete, player, adult, male, contestant\" for example (a)).\n- Prediction: This row lists the labels generated by a model for the highlighted words (e.g., \"person, athlete, player, adult, male, contestant, defendant, man\" for example (a)).\n\nThe table allows for the comparison of human annotations with model predictions, with correctly predicted terms typically colored in blue and incorrectly predicted or unmatched terms shown in red. Discrepancies and overlaps between annotations and predictions can be observed across different examples.\nmay be true, but are not supported by the context. \nWe found that the model often abstained from predicting any ﬁne-grained types. Especially in challenging cases as in example (c), the model predicts only general types, explaining the low re- call numbers (  $28\\%$   of examples belong to this cat- egory). Even when the model generated correct ﬁne-grained types as in example (d), the recall was often fairly low since it did not generate a com- plete set of related ﬁne-grained labels. \nEstimating the performance of a model in an in- complete label setting and expanding label cover- age are interesting areas for future work. Our task also poses a potential modeling challenge; some- times, the model predicts two incongruous types (e.g. “location” and “person”), which points to- wards modeling the task as a joint set prediction task, rather than predicting labels individually. We provide sample outputs on the project website. \n6 Improving Existing Fine-Grained NER with Better Distant Supervision \nWe show that our model and distant supervision can improve performance on an existing ﬁne- grained NER task. We chose the widely-used OntoNotes ( Gillick et al. ,  2014 ) dataset which in- cludes nominal and named entity mentions. \nAugmenting the Training Data The original OntoNotes training set (O NTO  in Tables  6  and  7 ) is extracted by linking entities to a KB. We supple- ment this dataset with our two new sources of dis- tant supervision: Wikipedia deﬁnition sentences (W IKI ) and head word supervision (H EAD ) (see Section  3 ). To convert the label space, we manu- ally map a single noun from our natural-language vocabulary to each formal-language type in the OntoNotes ontology.  $77\\%$   of OntoNote’s types directly correspond to suitable noun labels (e.g. “doctor” to “/person/doctor”), whereas the other cases were mapped with minimal manual effort (e.g. “musician” to “person/artist/music”, “politi- cian” to “/person/political ﬁgure”). We then ex- pand these labels according to the ontology to in- clude their hypernyms (“/person/political ﬁgure” will also generate “/person”). Lastly, we create negative examples by assigning the “/other” label to examples that are not mapped to the ontology. The augmented dataset contains   $2.5\\mathbf{M}/0.6\\mathbf{M}$   new positive/negative examples, of which   $0.9\\mathbf{M}/0.1\\mathbf{M}$  are from Wikipedia deﬁnition sentences and  $1.6\\mathbf{M}/0.5\\mathbf{M}$   from head words. \nExperiment Setup We compare performance to other published results and to our reimplemen- tation of AttentiveNER ( Shimaoka et al. ,  2017 ). We also compare models trained with different sources of supervision. For this dataset, we did not use our multitask objective (Section  4 ), since ex- panding types to include their ontological hyper- nyms largely eliminates the partial supervision as- "}
{"page": 7, "image_path": "doc_images/P18-1009_7.jpg", "ocr_text": "Ace. Ma-Fl Mi-F1\nAttentiveNER++ 51.7 70.9 64.9\nAFET (Ren et al., 2016a) 55.1 7A 64.7\nLNR (Ren et al., 2016b) 57.2 71S 66.1\nOurs (ONTO+WIKI+HEAD) 59.5 76.8 71.8\n\nTable 6: Results on the OntoNotes fine-grained\nentity typing test set. The first two models (At-\ntentiveNER++ and AFET) use only KB-based su-\npervision. LNR uses a filtered version of the KB-\nbased training set. Our model uses all our distant\nsupervision sources.\n\nTraining Data Performance\nModel ONTO WIKI HEAD | Acc. MaFl  MiFl\nAttn. v 46.5 63.3 58.3\nNER v v v | 53.7 72.8 68.0\nv 417 64.2 59.5\nv 48.5 67.6 63.6\nOurs v v | 57.9 73.0 66.9\nv v | 60.1 75.0 68.7\nv v v | 61.6 77.3 71.8\n\nTable 7: Ablation study on the OntoNotes fine-\ngrained entity typing development. The second\nrow isolates dataset improvements, while the third\nrow isolates the model.\n\nsumption. Following prior work, we report macro-\nand micro-averaged F1 score, as well as accuracy\n(exact set match).\n\nResults Table 6 shows the overall performance\non the test set. Our combination of model and\ntraining data shows a clear improvement from\nprior work, setting a new state-of-the art result.”\nIn Table 7, we show an ablation study. Our new\nsupervision sources improve the performance of\nboth the AttentiveNER model and our own. We\nobserve that every supervision source improves\nperformance in its own right. Particularly, the\nnaturally-occurring head-word supervision seems\nto be the prime source of improvement, increasing\nperformance by about 10% across all metrics.\n\nPredicting Miscellaneous Types While analyz-\ning the data, we observed that over half of the men-\ntions in OntoNotes’ development set were anno-\ntated only with the miscellaneous type (“/other”).\nFor both models in our evaluation, detecting the\nmiscellaneous category is substantially easier than\n\n7We did not compare to a system from (Yogatama et al.,\n\n2015), which reports slightly higher test number (72.98 micro\nF1) as they used a different, unreleased test set.\n\n94\n\nproducing real types (94% F1 vs. 58% FI with\nour best model). We provide further details of this\nanalysis in the supplementary material.\n\n7 Related Work\n\nFine-grained NER has received growing atten-\ntion, and is used in many applications (Gupta\net al., 2017; Ren et al., 2017; Yaghoobzadeh et al.,\n2017b; Raiman and Raiman, 2018). Researchers\nstudied typing in varied contexts, including men-\ntions in specific sentences (as we consider) (Ling\nand Weld, 2012; Gillick et al., 2014; Yogatama\net al., 2015; Dong et al., 2015; Schutze et al.,\n2017), corpus-level prediction (Yaghoobzadeh and\nSchiitze, 2016), and lexicon level (given only a\nnoun phrase with no context) (Yao et al., 2013).\n\nRecent work introduced fine-grained type on-\ntologies (Rabinovich and Klein, 2017; Murty\net al., 2017; Corro et al., 2015), defined using\nWikipedia categories (100), Freebase types (1K)\nand WordNet senses (16K). However, they focus\non named entities, and data has been challeng-\ning to gather, often approximating gold annota-\ntions with distant supervision. In contrast, (1) our\nontology contains any frequent noun phrases that\ndepicts a type, (2) our task goes beyond named\nentities, covering every noun phrase (even pro-\nnouns), and (3) we provide crowdsourced annota-\ntions which provide context-sensitive, fine grained\ntype labels.\n\nContextualized fine-grained entity typing is re-\nlated to selectional preference (Resnik, 1996; Pan-\ntel et al., 2007; Zapirain et al., 2013; de Cruys,\n2014), where the goal is to induce semantic gen-\neralizations on the type of arguments a predicate\nprefers. Rather than focusing on predicates, we\ncondition on the entire sentence to deduce the ar-\nguments’ types, which allows us to capture more\nnuanced types. For example, not every type that\nfits “He played the violin in his room” is also\nsuitable for “He played the violin in the Carnegie\nHall”. Entity typing here can be connected to ar-\ngument finding in semantic role labeling.\n\nTo deal with noisy distant supervision for\nKB population and entity typing, researchers\nused multi-instance multi-label learning (Sur-\ndeanu et al., 2012; Yaghoobzadeh et al., 2017b) or\ncustom losses (Abhishek et al., 2017; Ren et al.,\n2016a). Our multitask objective handles noisy su-\npervision by pooling different distant supervision\nsources across different levels of granularity.\n", "vlm_text": "The table presents comparative performance metrics for different Named Entity Recognition (NER) models. It includes the following columns:\n\n1. **Model**: Lists the models being compared. The table includes the following models:\n   - AttentiveNER++\n   - AFET (Ren et al., 2016a)\n   - LNR (Ren et al., 2016b)\n   - Ours (ONTO+WIKI+HEAD)\n\n2. **Acc.**: Refers to accuracy, which measures the proportion of correct predictions made by the model. The values for each model are as follows:\n   - AttentiveNER++: 51.7\n   - AFET: 55.1\n   - LNR: 57.2\n   - Ours: 59.5\n\n3. **Ma-F1**: Refers to the macro F1 score, which is the harmonic mean of precision and recall, calculated across multiple classes and averaged without taking class imbalance into account. The values are:\n   - AttentiveNER++: 70.9\n   - AFET: 71.1\n   - LNR: 71.5\n   - Ours: 76.8\n\n4. **Mi-F1**: Refers to the micro F1 score, which is calculated by considering the total true positives, false negatives, and false positives of all classes. This metric takes class imbalance into account. The values are:\n   - AttentiveNER++: 64.9\n   - AFET: 64.7\n   - LNR: 66.1\n   - Ours: 71.8\n\n\"Ours (ONTO+WIKI+HEAD)\" appears to be the proposed model or method in the context, which achieves the highest values in all three metrics.\nTable 6: Results on the OntoNotes ﬁne-grained entity typing test set. The ﬁrst two models (At- tentive  $\\tt N E R++$   and AFET) use only KB-based su- pervision. LNR uses a ﬁltered version of the KB- based training set. Our model uses all our distant supervision sources. \nThe table presents a comparison of performance metrics for different models and training data combinations on some task. Specifically, it compares the performance of an \"Attn. NER\" model to a model labeled \"Ours.\"\n\n1. **Models**:\n   - \"Attn. NER\" \n   - \"Ours\"\n\n2. **Training Data Types**:\n   - ONTO\n   - WIKI\n   - HEAD\n\n3. **Performance Metrics**:\n   - Accuracy (Acc.)\n   - Macro-averaged F1-score (MaF1)\n   - Micro-averaged F1-score (MiF1)\n\n4. **Attn. NER Performance**:\n   - Training on ONTO: Acc. 46.5, MaF1 63.3, MiF1 58.3\n   - Training on ONTO, WIKI, HEAD: Acc. 53.7, MaF1 72.8, MiF1 68.0\n\n5. **Our Model's Performance**:\n   - Training on ONTO: Acc. 41.7, MaF1 64.2, MiF1 59.5\n   - Training on WIKI: Acc. 48.5, MaF1 67.6, MiF1 63.6\n   - Training on HEAD: Acc. 57.9, MaF1 73.0, MiF1 66.9\n   - Training on ONTO, WIKI: Acc. 60.1, MaF1 75.0, MiF1 68.7\n   - Training on ONTO, WIKI, HEAD: Acc. 61.6, MaF1 77.3, MiF1 71.8\n\nOverall, the table indicates that for both models, training with a combination of ONTO, WIKI, and HEAD data yields the best performance across all three metrics, with \"Ours\" achieving the highest scores.\nTable 7: Ablation study on the OntoNotes ﬁne- grained entity typing development. The second row isolates dataset improvements, while the third row isolates the model. \nsumption. Following prior work, we report macro- and micro-averaged F1 score, as well as accuracy (exact set match). \nResults Table  6  shows the overall performance on the test set. Our combination of model and training data shows a clear improvement from prior work, setting a new state-of-the art result. \nIn Table  7 , we show an ablation study. Our new supervision sources improve the performance of both the AttentiveNER model and our own. We observe that every supervision source improves performance in its own right. Particularly, the naturally-occurring head-word supervision seems to be the prime source of improvement, increasing performance by about   $10\\%$   across all metrics. \nPredicting Miscellaneous Types While analyz- ing the data, we observed that over half of the men- tions in OntoNotes’ development set were anno- tated only with the miscellaneous type (“/other”). For both models in our evaluation, detecting the miscellaneous category is substantially easier than producing real types (  $94\\%$   F1 vs.   $58\\%$   F1 with our best model). We provide further details of this analysis in the supplementary material. \n\n7 Related Work \nFine-grained NER has received growing atten- tion, and is used in many applications ( Gupta et al. ,  2017 ;  Ren et al. ,  2017 ;  Yaghoobzadeh et al. , 2017b ;  Raiman and Raiman ,  2018 ). Researchers studied typing in varied contexts, including men- tions in speciﬁc sentences (as we consider) ( Ling and Weld ,  2012 ;  Gillick et al. ,  2014 ;  Yogatama et al. ,  2015 ;  Dong et al. ,  2015 ;  Schutze et al. , 2017 ), corpus-level prediction ( Yaghoobzadeh and Sch¨ utze ,  2016 ), and lexicon level (given only a noun phrase with no context) ( Yao et al. ,  2013 ). \nRecent work introduced ﬁne-grained type on- tologies ( Rabinovich and Klein ,  2017 ;  Murty et al. ,  2017 ;  Corro et al. ,  2015 ), deﬁned using Wikipedia categories (100), Freebase types (1K) and WordNet senses (16K). However, they focus on named entities, and data has been challeng- ing to gather, often approximating gold annota- tions with distant supervision. In contrast, (1) our ontology contains any frequent noun phrases that depicts a type, (2) our task goes beyond named entities, covering every noun phrase (even pro- nouns), and (3) we provide crowdsourced annota- tions which provide context-sensitive, ﬁne grained type labels. \nContextualized ﬁne-grained entity typing is re- lated to selectional preference ( Resnik ,  1996 ;  Pan- tel et al. ,  2007 ;  Zapirain et al. ,  2013 ;  de Cruys , 2014 ), where the goal is to induce semantic gen- eralizations on the type of arguments a predicate prefers. Rather than focusing on predicates, we condition on the entire sentence to deduce the ar- guments’ types, which allows us to capture more nuanced types. For example, not every type that ﬁts “ He  played the violin in his room” is also suitable for “ He  played the violin in the Carnegie Hall”. Entity typing here can be connected to ar- gument ﬁnding in semantic role labeling. \nTo deal with noisy distant supervision for KB population and entity typing, researchers used multi-instance multi-label learning ( Sur- deanu et al. ,  2012 ;  Yaghoobzadeh et al. ,  2017b ) or custom losses ( Abhishek et al. ,  2017 ;  Ren et al. , 2016a ). Our multitask objective handles noisy su- pervision by pooling different distant supervision sources across different levels of granularity. "}
{"page": 8, "image_path": "doc_images/P18-1009_8.jpg", "ocr_text": "8 Conclusion\n\nUsing virtually unrestricted types allows us to ex-\npand the standard KB-based training methodol-\nogy with typing information from Wikipedia defi-\nnitions and naturally-occurring head-word super-\nvision. These new forms of distant supervision\nboost performance on our new dataset as well as\non an existing fine-grained entity typing bench-\nmark. These results set the first performance lev-\nels for our evaluation dataset, and suggest that the\ndata will support significant future work.\n\nAcknowledgement\n\nThe research was supported in part the ARO\n(W911NF-16-1-0121) the NSF (IIS- 1252835, IIS-\n1562364), and an Allen Distinguished Investigator\nAward. We would like to thank the reviewers for\nconstructive feedback. Also thanks to Yotam Es-\nhel and Noam Cohen for providing the Wikilink\ndataset. Special thanks to the members of UW\nNLP for helpful discussions and feedback.\n\nReferences\n\nAbhishek, Ashish Anand, and Amit Awekar. 2017.\nFine-grained entity type classification by jointly\nlearning representations and label embeddings. In\nProceedings of European Chapter of Association for\nComputational Linguistics.\n\nKrisztian Balog and Robert Neumayer. 2012. Hier-\narchical target type identification for entity-oriented\nqueries. In Proceedings of the Conference on Infor-\nmation and Knowledge Management.\n\nLuciano Del Corro, Abdalghani Abujabal, Rainer\nGemulla, and Gerhard Weikum. 2015.  Finet:\nContext-aware fine-grained named entity typing. In\nProceedings of the conference on Empirical Meth-\nods in Natural Language Processing.\n\nTim Van de Cruys. 2014. A neural network approach to\nselectional preference acquisition. In Proceedings\nof Empirical Methods in Natural Language Process-\ning.\n\nLi Dong, Furu Wei, Hong Sun, Ming Zhou, and Ke Xu.\n2015. A hybrid neural model for type classification\nof entity mentions. In Proceedings of International\nJoint Conference on Artificial Intelligence.\n\nGreg Durrett and Dan Klein. 2014. A joint model for\nentity analysis: Coreference, typing, and linking. In\nTransactions of the Association for Computational\nLinguistics.\n\n95\n\nDaniel Gillick, Nevena Lazic, Kuzman Ganchev, Jesse\nKirchner, and David Huynh. 2014. — Context-\ndependent fine-grained entity type tagging. CoRR,\nabs/1412.1820.\n\nNitish Gupta, Sameer Singh, and Dan Roth. 2017. En-\ntity linking via joint encoding of types, descriptions,\nand context. In Proceedings of the Conference on\nEmpirical Methods in Natural Language Process-\ning, pages 2671-2680.\n\nEduard Hovy, Mitchell Marcus, Martha Palmer, Lance\nRamshaw, and Ralph Weischedel. 2006. Ontonotes:\nthe 90% solution. In Proceedings of the human lan-\nguage technology conference of the North American\nChapter of the Association for Computational Lin-\nguistics, Companion Volume: Short Papers, pages\n57-60. Association for Computational Linguistics.\n\nKenton Lee, Luheng He, Mike Lewis, and Luke Zettle-\nmoyer. 2017. End-to-end neural coreference resolu-\ntion. In Proceedings of the Conference on Empirical\nMethods in Natural Language Processing.\n\nXiao Ling and Daniel S Weld. 2012. Fine-grained en-\ntity recognition. In Proceedings of Association for\nthe Advancement of Artificial Intelligence. Citeseer.\n\nChristopher D. Manning, Mihai Surdeanu, John Bauer,\nJenny Finkel, Steven J. Bethard, and David Mc-\nClosky. 2014. The Stanford CoreNLP natural lan-\nguage processing toolkit. In Association for Compu-\ntational Linguistics (ACL) System Demonstrations,\npages 55-60.\n\nGeorge A Miller. 1995. Wordnet: a lexical database for\nenglish. Communications of the ACM, 38(11):39-\n4l.\n\nShikhar Murty, Patrick Verga, Luke Vilnis, and Andrew\nMcCallum. 2017. Finer grained entity typing with\ntypenet. In AKBC Workshop.\n\nPatrick Pantel, Rahul Bhagat, Bonaventura Coppola,\nTimothy Chklovski, and Eduard H. Hovy. 2007. Isp:\nLearning inferential selectional preferences. In Pro-\nceedings of North American Chapter of the Associ-\nation for Computational Linguistics.\n\nRobert Parker, David Graff, David Kong, Ke Chen, and\nKazuaki Maeda. 2011. English gigaword fifth edi-\ntion (Ide2011t07). In Linguistic Data Consortium.\n\nMaxim Rabinovich and Dan Klein. 2017. Fine-grained\nentity typing with high-multiplicity assignments. In\nProceedings of Association for Computational Lin-\nguistics (ACL).\n\nJonathan Raiman and Olivier Raiman. 2018. Deep-\ntype: Multilingual entity linking by neural type sys-\ntem evolution. In Association for the Advancement\nof Artificial Intelligence.\n\nXiang Ren, Wenqi He, Meng Qu, Lifu Huang, Heng\nJi, and Jiawei Han. 2016a. Afet: Automatic fine-\ngrained entity typing by hierarchical partial-label\n", "vlm_text": "8 Conclusion \nUsing virtually unrestricted types allows us to ex- pand the standard KB-based training methodol- ogy with typing information from Wikipedia deﬁ- nitions and naturally-occurring head-word super- vision. These new forms of distant supervision boost performance on our new dataset as well as on an existing ﬁne-grained entity typing bench- mark. These results set the ﬁrst performance lev- els for our evaluation dataset, and suggest that the data will support signiﬁcant future work. \nAcknowledgement \nThe research was supported in part the ARO (W911NF-16-1-0121) the NSF (IIS-1252835, IIS- 1562364), and an Allen Distinguished Investigator Award. We would like to thank the reviewers for constructive feedback. Also thanks to Yotam Es- hel and Noam Cohen for providing the Wikilink dataset. Special thanks to the members of UW NLP for helpful discussions and feedback. \nReferences \nAbhishek, Ashish Anand, and Amit Awekar. 2017. Fine-grained entity type classiﬁcation by jointly learning representations and label embeddings. In Proceedings of European Chapter of Association for Computational Linguistics . Krisztian Balog and Robert Neumayer. 2012. Hier- archical target type identiﬁcation for entity-oriented queries. In  Proceedings of the Conference on Infor- mation and Knowledge Management . Luciano Del Corro, Abdalghani Abujabal, Rainer Gemulla, and Gerhard Weikum. 2015. Finet: Context-aware ﬁne-grained named entity typing. In Proceedings of the conference on Empirical Meth- ods in Natural Language Processing . Tim Van de Cruys. 2014. A neural network approach to selectional preference acquisition. In  Proceedings of Empirical Methods in Natural Language Process- ing . Li Dong, Furu Wei, Hong Sun, Ming Zhou, and Ke Xu. 2015. A hybrid neural model for type classiﬁcation of entity mentions. In  Proceedings of International Joint Conference on Artiﬁcial Intelligence . Greg Durrett and Dan Klein. 2014. A joint model for entity analysis: Coreference, typing, and linking. In Transactions of the Association for Computational Linguistics . \nDaniel Gillick, Nevena Lazic, Kuzman Ganchev, Jesse Kirchner, and David Huynh. 2014. Context- dependent ﬁne-grained entity type tagging.  CoRR , abs/1412.1820. \nNitish Gupta, Sameer Singh, and Dan Roth. 2017. En- tity linking via joint encoding of types, descriptions, and context. In  Proceedings of the Conference on Empirical Methods in Natural Language Process- ing , pages 2671–2680. \nEduard Hovy, Mitchell Marcus, Martha Palmer, Lance Ramshaw, and Ralph Weischedel. 2006. Ontonotes: the  $90\\%$   solution. In  Proceedings of the human lan- guage technology conference of the North American Chapter of the Association for Computational Lin- guistics, Companion Volume: Short Papers , pages 57–60. Association for Computational Linguistics. \nKenton Lee, Luheng He, Mike Lewis, and Luke Zettle- moyer. 2017. End-to-end neural coreference resolu- tion. In  Proceedings of the Conference on Empirical Methods in Natural Language Processing . \nXiao Ling and Daniel S Weld. 2012. Fine-grained en- tity recognition. In  Proceedings of Association for the Advancement of Artiﬁcial Intelligence . Citeseer. \nChristopher D. Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven J. Bethard, and David Mc- Closky. 2014.  The Stanford CoreNLP natural lan- guage processing toolkit . In  Association for Compu- tational Linguistics (ACL) System Demonstrations , pages 55–60. \nGeorge A Miller. 1995. Wordnet: a lexical database for english.  Communications of the ACM , 38(11):39– 41. \nShikhar Murty, Patrick Verga, Luke Vilnis, and Andrew McCallum. 2017. Finer grained entity typing with typenet. In  AKBC Workshop . \nPatrick Pantel, Rahul Bhagat, Bonaventura Coppola, Timothy Chklovski, and Eduard H. Hovy. 2007. Isp: Learning inferential selectional preferences. In  Pro- ceedings of North American Chapter of the Associ- ation for Computational Linguistics . \nRobert Parker, David Graff, David Kong, Ke Chen, and Kazuaki Maeda. 2011. English gigaword ﬁfth edi- tion (ldc2011t07). In  Linguistic Data Consortium . \nMaxim Rabinovich and Dan Klein. 2017. Fine-grained entity typing with high-multiplicity assignments. In Proceedings of Association for Computational Lin- guistics (ACL) . \nXiang Ren, Wenqi He, Meng Qu, Lifu Huang, Heng Ji, and Jiawei Han. 2016a. Afet: Automatic ﬁne- grained entity typing by hierarchical partial-label "}
{"page": 9, "image_path": "doc_images/P18-1009_9.jpg", "ocr_text": "embedding. In Proceedings Empirical Methods in\nNatural Language Processing.\n\nXiang Ren, Wenqi He, Meng Qu, Clare R. Voss, Heng\nJi, and Jiawei Han. 2016b. Label noise reduction in\nentity typing by heterogeneous partial-label embed-\nding. In Proceedings of Knowledge Discovery and\nData Mining.\n\nXiang Ren, Zeqiu Wu, Wenqi He, Meng Qu, Clare R.\nVoss, Heng Ji, Tarek F. Abdelzaher, and Jiawei Han.\n2017. Cotype: Joint extraction of typed entities and\nrelations with knowledge bases. In Proceedings of\nWorld Wide Web Conference.\n\nPhilip Resnik. 1996. Selectional constraints: an\ninformation-theoretic model and its computational\nrealization. Cognition, 61 1-2:127-59.\n\nAlan Ritter, Sam Clark, Oren Etzioni, et al. 2011.\nNamed entity recognition in tweets: an experimental\nstudy. In Proceedings of the Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n1524-1534. Association for Computational Linguis-\ntics.\n\nHinrich Schutze, Ulli Waltinger, and Sanjeev Karn.\n2017. End-to-end trainable attentive decoder for hi-\nerarchical entity classification. In Proceedings of\nEuropean Chapter of Association for Computational\nLinguistics.\n\nSonse Shimaoka, Pontus Stenetorp, Kentaro Inui, and\nSebastian Riedel. 2017. An attentive neural archi-\ntecture for fine-grained entity type classification. In\nProceedings of the European Chapter of Association\nfor Computational Linguistics (ACL).\n\nEyal Shnarch, Libby Barak, and Ido Dagan. Extract-\ning lexical reference rules from wikipedia. In Pro-\nceedings of the Joint Conference of the 47th Annual\nMeeting of the ACL and the 4th International Joint\nConference on Natural Language Processing of the\nAFNLP.\n\nSameer Singh, Amarnag Subramanya, Fernando\nPereira, and Andrew McCallum. 2012. Wik-\nilinks: A large-scale cross-document coreference\ncorpus labeled via links to Wikipedia. Techni-\ncal Report UM-CS-2012-015, University of Mas-\nsachusetts, Amherst.\n\nMihai Surdeanu, Julie Tibshirani, Ramesh Nallap-\nati, and Christopher D. Manning. 2012. Multi-\ninstance multi-label learning for relation extraction.\nIn EMNLP-CONLL.\n\nRobert West, Evgeniy Gabrilovich, Kevin Murphy,\nShaohua Sun, Rahul Gupta, and Dekang Lin. 2014.\nKnowledge base completion via search-based ques-\ntion answering. In Proceedings of World Wide Web\nConference.\n\nYadollah Yaghoobzadeh, Heike Adel, and Hinrich\nSchiitze. 2017a. Noise mitigation for neural entity\ntyping and relation extraction. In Proceedings of the\n\n96\n\nConference of the European Chapter of the Associa-\ntion for Computational Linguistics, abs/1612.07495.\n\nYadollah Yaghoobzadeh, Heike Adel, and Hinrich\nSchiitze. 2017b. Noise mitigation for neural entity\ntyping and relation extraction. In Proceedings of\nEuropean Chapter of Association for Computational\nLinguistics.\n\nYadollah Yaghoobzadeh and Hinrich Schiitze. 2016.\nCorpus-level fine-grained entity typing using con-\ntextual information. Proceedings of the Conference\non Empirical Methods in Natural Language Pro-\ncessing.\n\nLimin Yao, Sebastian Riedel, and Andrew McCallum.\n2013. Universal schema for entity type prediction.\nIn Automatic KnowledgeBase Construction Work-\nshop at the Conference on Information and Knowl-\nedge Management.\n\nSemih Yavuz, Izzeddin Gur, Yu Su, Mudhakar Srivatsa,\nand Xifeng Yan. 2016. Improving semantic parsing\nvia answer type inference. In Proceedings of Empir-\nical Methods in Natural Language Processing.\n\nDani Yogatama, Daniel Gillick, and Nevena Lazic.\n2015. Embedding methods for fine grained entity\ntype classification. In Proceedings of Association\nfor Computational Linguistics (ACL).\n\nM Amir Yosef, Sandro Bauer, Johannes Hoffart, Marc\nSpaniol, and Gerhard Weikum. 2012. Hyena: Hier-\narchical type classification for entity names. In Pro-\nceedings of the International Conference on Compu-\ntational Linguistics.\n\nBefiat Zapirain, Eneko Agirre, Lluis Marquez i Villo-\ndre, and Mihai Surdeanu. 2013. Selectional pref-\nerences for semantic role classification. Computa-\ntional Linguistics, 39:631-663.\n", "vlm_text": "embedding. In  Proceedings Empirical Methods in Natural Language Processing . \nXiang Ren, Wenqi He, Meng Qu, Clare R. Voss, Heng Ji, and Jiawei Han. 2016b. Label noise reduction in entity typing by heterogeneous partial-label embed- ding. In  Proceedings of Knowledge Discovery and Data Mining . Xiang Ren, Zeqiu Wu, Wenqi He, Meng Qu, Clare R. Voss, Heng Ji, Tarek F. Abdelzaher, and Jiawei Han. 2017. Cotype: Joint extraction of typed entities and relations with knowledge bases. In  Proceedings of World Wide Web Conference . Philip Resnik. 1996. Selectional constraints: an information-theoretic model and its computational realization.  Cognition , 61 1-2:127–59. Alan Ritter, Sam Clark, Oren Etzioni, et al. 2011. Named entity recognition in tweets: an experimental study. In  Proceedings of the Conference on Empiri- cal Methods in Natural Language Processing , pages 1524–1534. Association for Computational Linguis- tics. Hinrich Schutze, Ulli Waltinger, and Sanjeev Karn. 2017. End-to-end trainable attentive decoder for hi- erarchical entity classiﬁcation. In  Proceedings of European Chapter of Association for Computational Linguistics . Sonse Shimaoka, Pontus Stenetorp, Kentaro Inui, and Sebastian Riedel. 2017. An attentive neural archi- tecture for ﬁne-grained entity type classiﬁcation. In Proceedings of the European Chapter of Association for Computational Linguistics (ACL) . Eyal Shnarch, Libby Barak, and Ido Dagan. Extract- ing lexical reference rules from wikipedia. In  Pro- ceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP . Sameer Singh, Amarnag Subramanya, Fernando Pereira, and Andrew McCallum. 2012. Wik- ilinks: A large-scale cross-document coreference corpus labeled via links to Wikipedia. Techni- cal Report UM-CS-2012-015, University of Mas- sachusetts, Amherst. Mihai Surdeanu, Julie Tibshirani, Ramesh Nallap- ati, and Christopher D. Manning. 2012. Multi- instance multi-label learning for relation extraction. In  EMNLP-CoNLL . Robert West, Evgeniy Gabrilovich, Kevin Murphy, Shaohua Sun, Rahul Gupta, and Dekang Lin. 2014. Knowledge base completion via search-based ques- tion answering . In  Proceedings of World Wide Web Conference . Yadollah Yaghoobzadeh, Heike Adel, and Hinrich Sch¨ utze. 2017a. Noise mitigation for neural entity typing and relation extraction.  In Proceedings of the \nConference of the European Chapter of the Associa- tion for Computational Linguistics , abs/1612.07495. \nYadollah Yaghoobzadeh, Heike Adel, and Hinrich Sch¨ utze. 2017b. Noise mitigation for neural entity typing and relation extraction. In  Proceedings of European Chapter of Association for Computational Linguistics . Yadollah Yaghoobzadeh and Hinrich Sch¨ utze. 2016. Corpus-level ﬁne-grained entity typing using con- textual information.  Proceedings of the Conference on Empirical Methods in Natural Language Pro- cessing . Limin Yao, Sebastian Riedel, and Andrew McCallum. 2013. Universal schema for entity type prediction. In  Automatic KnowledgeBase Construction Work- shop at the Conference on Information and Knowl- edge Management . Semih Yavuz, Izzeddin Gur, Yu Su, Mudhakar Srivatsa, and Xifeng Yan. 2016. Improving semantic parsing via answer type inference. In  Proceedings of Empir- ical Methods in Natural Language Processing . Dani Yogatama, Daniel Gillick, and Nevena Lazic. 2015. Embedding methods for ﬁne grained entity type classiﬁcation. In  Proceedings of Association for Computational Linguistics (ACL) . M Amir Yosef, Sandro Bauer, Johannes Hoffart, Marc Spaniol, and Gerhard Weikum. 2012. Hyena: Hier- archical type classiﬁcation for entity names. In  Pro- ceedings of the International Conference on Compu- tational Linguistics . Be˜ nat Zapirain, Eneko Agirre, Llu´ ıs M\\` arquez i Villo- dre, and Mihai Surdeanu. 2013. Selectional pref- erences for semantic role classiﬁcation.  Computa- tional Linguistics , 39:631–663. "}
