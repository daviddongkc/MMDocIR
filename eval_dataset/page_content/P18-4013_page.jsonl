{"page": 0, "image_path": "doc_images/P18-4013_0.jpg", "ocr_text": "NCRF++: An Open-source Neural Sequence Labeling Toolkit\n\nJie Yang and Yue Zhang\nSingapore University of Technology and Design\njie_yang@mymail.sutd.edu.sg\nyue_zhang@sutd.edu.sg\n\nAbstract\n\nThis paper describes NCRF++, a toolkit\nfor neural sequence labeling. NCRF++\nis designed for quick implementation of\ndifferent neural sequence labeling models\nwith a CRF inference layer. It provides\nusers with an inference for building the\ncustom model structure through configu-\nration file with flexible neural feature de-\nsign and utilization. Built on PyTorch!,\nthe core operations are calculated in batch,\nmaking the toolkit efficient with the accel-\neration of GPU. It also includes the imple-\nmentations of most state-of-the-art neural\nsequence labeling models such as LSTM-\nCRF, facilitating reproducing and refine-\nment on those methods.\n\n1 Introduction\n\nSequence labeling is one of the most fundamental\nNLP models, which is used for many tasks such as\nnamed entity recognition (NER), chunking, word\nsegmentation and part-of-speech (POS) tagging.\nIt has been traditionally investigated using statis-\ntical approaches (Lafferty et al., 2001; Ratinov\nand Roth, 2009), where conditional random fields\n(CRF) (Lafferty et al., 2001) has been proven as\nan effective framework, by taking discrete features\nas the representation of input sequence (Sha and\nPereira, 2003; Keerthi and Sundararajan, 2007).\nWith the advances of deep learning, neural se-\nquence labeling models have achieved state-of-\nthe-art for many tasks (Ling et al., 2015; Ma\nand Hovy, 2016; Peters et al., 2017). Features\nare extracted automatically through network struc-\ntures including long short-term memory (LSTM)\n(Hochreiter and Schmidhuber, 1997) and convolu-\ntion neural network (CNN) (LeCun et al., 1989),\n\n'http://pytorch.org/\n\n##NetworkConfiguration##\n\nuse_crf=True\n\nword_seg_feature=LSTM\n\nword_seg_layer=1\n\nchar_seq-feature=CNN\n\nfeature=[POS] emb_dir=None emb_size=10\nfeature=[Cap] emb_dir=% (cap_emb_dir)\n##Hyperparameters##\n\nFigure 1: Configuration file segment\n\nwith distributed word representations. Similar to\ndiscrete models, a CRF layer is used in many\nstate-of-the-art neural sequence labeling models\nfor capturing label dependencies (Collobert et al.,\n2011; Lample et al., 2016; Peters et al., 2017).\n\nThere exist several open-source statistical CRF\nsequence labeling toolkits, such as CRF++2, CRF-\nSuite (Okazaki, 2007) and FlexCRFs (Phan et al.,\n2004), which provide users with flexible means of\nfeature extraction, various training settings and de-\ncoding formats, facilitating quick implementation\nand extension on state-of-the-art models. On the\nother hand, there is limited choice for neural se-\nquence labeling toolkits. Although many authors\nreleased their code along with their sequence la-\nbeling papers (Lample et al., 2016; Ma and Hovy,\n2016; Liu et al., 2018), the implementations are\nmostly focused on specific model structures and\nspecific tasks. Modifying or extending can need\nenormous coding.\n\nIn this paper, we present Neural CRF++\n(NCRF++)°, a neural sequence labeling toolkit\nbased on PyTorch, which is designed for solv-\ning general sequence labeling tasks with effective\nand efficient neural models. It can be regarded as\nthe neural version of CRF++, with both take the\nCoNLL data format as input and can add hand-\n\n*https://taku910.github.io/crfpp/\n\n3Code is available at https://github.com/\njiesutd/NCRFpp.\n\nProceedings of the 56th Annual Meeting of the Association for Computational Linguistics-System Demonstrations, pages 74-79\nMelbourne, Australia, July 15 - 20, 2018. ©2018 Association for Computational Linguistics\n", "vlm_text": "NCRF++: An Open-source Neural Sequence Labeling Toolkit \nJie Yang  and  Yue Zhang Singapore University of Technology and Design jie yang@mymail.sutd.edu.sg yue zhang@sutd.edu.sg \nAbstract \nThis paper describes   $\\mathrm{NCRF++}$  , a toolkit for neural sequence labeling.  $\\mathrm{NCRF++}$  is designed for quick implementation of different neural sequence labeling models with a CRF inference layer. It provides users with an inference for building the custom model structure through conﬁgu- ration ﬁle with ﬂexible neural feature de- sign and utilization. Built on PyTorch 1 , the core operations are calculated in batch, making the toolkit efﬁcient with the accel- eration of GPU. It also includes the imple- mentations of most state-of-the-art neural sequence labeling models such as LSTM- CRF, facilitating reproducing and reﬁne- ment on those methods. \n1 Introduction \nSequence labeling is one of the most fundamental NLP models, which is used for many tasks such as named entity recognition (NER), chunking, word segmentation and part-of-speech (POS) tagging. It has been traditionally investigated using statis- tical approaches ( Lafferty et al. ,  2001 ;  Ratinov and Roth ,  2009 ), where conditional random ﬁelds (CRF) ( Lafferty et al. ,  2001 ) has been proven as an effective framework, by taking discrete features as the representation of input sequence ( Sha and Pereira ,  2003 ;  Keerthi and Sundararajan ,  2007 ). \nWith the advances of deep learning, neural se- quence labeling models have achieved state-of- the-art for many tasks ( Ling et al. ,  2015 ;  Ma and Hovy ,  2016 ;  Peters et al. ,  2017 ). Features are extracted automatically through network struc- tures including long short-term memory (LSTM) ( Hochreiter and Schmidhuber ,  1997 ) and convolu- tion neural network (CNN) ( LeCun et al. ,  1989 ), \n##Network Configuration## use crf  $\\leftrightharpoons$  True word seq feature  $=$  LSTM word seq layer  $\\cdot{=}1$  char seq feature  $\\mathfrak{z}\\!=\\!\\mathrm{CNN}\\!\\mathrm{N}$  feature  $=$  [POS] emb dir  $=$  None emb size  $\\scriptstyle{:=10}$  feature  $=$  [Cap] emb dir  $\\mathbf{\\Sigma}=\\frac{\\circ}{\\circ}$  (cap emb dir) ##Hyperparameters## ... \nFigure 1: Conﬁguration ﬁle segment \nwith distributed word representations. Similar to discrete models, a CRF layer is used in many state-of-the-art neural sequence labeling models for capturing label dependencies ( Collobert et al. , 2011 ;  Lample et al. ,  2016 ;  Peters et al. ,  2017 ). \nThere exist several open-source statistical CRF sequence labeling toolkits, such as  $\\mathrm{CRF++}^{2}$  , CRF- Suite ( Okazaki ,  2007 ) and FlexCRFs ( Phan et al. , 2004 ), which provide users with ﬂexible means of feature extraction, various training settings and de- coding formats, facilitating quick implementation and extension on state-of-the-art models. On the other hand, there is limited choice for neural se- quence labeling toolkits. Although many authors released their code along with their sequence la- beling papers ( Lample et al. ,  2016 ;  Ma and Hovy , 2016 ;  Liu et al. ,  2018 ), the implementations are mostly focused on speciﬁc model structures and speciﬁc tasks. Modifying or extending can need enormous coding. \nIn this paper, we present Neural   $\\mathrm{CRF++}$   $(\\mathrm{NCRF}++)^{3}$  , a neural sequence labeling toolkit based on PyTorch, which is designed for solv- ing general sequence labeling tasks with effective and efﬁcient neural models. It can be regarded as the neural version of   $\\mathrm{CRF++}$  , with both take the CoNLL data format as input and can add hand- "}
{"page": 1, "image_path": "doc_images/P18-4013_1.jpg", "ocr_text": "Char\nSequence\nLayer\n\nWord\nSequence\nLayer\n\nInference\nLayer\n\nInference Layer: Softmax or CRF\n\nFigure 2: NCRF++ for sentence “I love Bruce Lee”. Green, red, yellow and blue circles represent\ncharacter embeddings, word embeddings, character sequence representations and word sequence repre-\nsentations, respectively. The grey circles represent the embeddings of sparse features.\n\ncrafted features to CRF framework conveniently.\nWe take the layerwise implementation, which in-\ncludes character sequence layer, word sequence\nlayer and inference layer. NCRF++ is:\n\ne Fully configurable: users can design their\nneural models only through a configuration file\nwithout any code work. Figure | shows a seg-\nment of the configuration file. It builds a LSTM-\nCRF framework with CNN to encode character\nsequence (the same structure as Ma and Hovy\n(2016)), plus POS and Cap features, within 10\nlines. This demonstrates the convenience of de-\nsigning neural models using NCRF++.\n\ne Flexible with features: human-defined fea-\ntures have been proved useful in neural se-\nquence labeling (Collobert et al., 2011; Chiu and\nNichols, 2016). Similar to the statistical toolkits,\nNCRF++ supports user-defined features but using\ndistributed representations through lookup tables,\nwhich can be initialized randomly or from exter-\nnal pretrained embeddings (embedding directory:\nemb_dir in Figure 1). In addition, NCRF++ in-\ntegrates several state-of-the-art automatic feature\nextractors, such as CNN and LSTM for character\nsequences, leading easy reproduction of many re-\ncent work (Lample et al., 2016; Chiu and Nichols,\n2016; Ma and Hovy, 2016).\n\ne Effective and efficient: we reimplement sev-\neral state-of-the-art neural models (Lample et al.,\n2016; Ma and Hovy, 2016) using NCRF++. Ex-\nperiments show models built in NCRF++ give\ncomparable performance with reported results in\nthe literature. Besides, NCRF++ is implemented\n\n75\n\nusing batch calculation, which can be acceler-\nated using GPU. Our experiments demonstrate\nthat NCRF++ as an effective and efficient toolkit.\ne Function enriched: NCRF++ extends the\nViterbi algorithm (Viterbi, 1967) to enable decod-\ning n best sequence labels with their probabilities.\nTaking NER, Chunking and POS tagging as typ-\nical examples, we investigate the performance of\nmodels built in NCRF++, the influence of\ndefined and automatic features, the performance\nof nbest decoding and the running speed with the\nbatch size. Detail results are shown in Section 3.\n\nnuman-\n\n2 NCRF++ Architecture\n\nThe framework of NCRF++ is shown in Figure 2.\nNCRF++ is designed with three layers: a character\nsequence layer; a word sequence layer and infer-\nence layer. For each input word sequence, words\nare represented with word embeddings. The char-\nacter sequence layer can be used to automatically\nextract word level features by encoding the char-\nacter sequence within the word. Arbitrary hand-\ncrafted features such as capitalization [Cap],\nPOS tag [POS], prefixes [Pre] and suffixes\n[Suf] are also supported by NCRF++. Word\nrepresentations are the concatenation of word em-\nbeddings (red circles), character sequence encod-\ning hidden vector (yellow circles) and handcrafted\nneural features (grey circles). Then the word se-\nquence layer takes the word representations as in-\nput and extracts the sentence level features, which\nare fed into the inference layer to assign a label\nto each word. When building the network, users\n\n", "vlm_text": "The image is a diagram representing the structure of the $\\mathrm{NCRF++}$ model applied to the sentence “I love Bruce Lee.” It consists of three layers:\n\n1. **Char Sequence Layer**: \n   - Green circles represent character embeddings for each character in the words of the sentence.\n   - These embeddings are processed through an RNN/CNN.\n\n2. **Word Sequence Layer**: \n   - The output from the Char Sequence Layer, along with additional features represented by grey circles, contributes to word embeddings (represented by red and yellow circles).\n   - These are combined and processed to form word sequence representations.\n\n3. **Inference Layer**:\n   - Blue circles represent the word sequence representations processed through the RNN/CNN.\n   - The final output is determined using either a Softmax or CRF (Conditional Random Field) function.\ncrafted features to CRF framework conveniently. We take the layerwise implementation, which in- cludes character sequence layer, word sequence layer and inference layer.   $\\mathrm{NCRF++}$   is: \n•  Fully conﬁgurable : users can design their neural models only through a conﬁguration ﬁle without any code work. Figure  1  shows a seg- ment of the conﬁguration ﬁle. It builds a LSTM- CRF framework with CNN to encode character sequence (the same structure as  Ma and Hovy ( 2016 )), plus  POS  and  Cap  features, within 10 lines. This demonstrates the convenience of de- signing neural models using  $\\mathrm{NCRF++}$  . \n•  Flexible with features : human-deﬁned fea- tures have been proved useful in neural se- quence labeling ( Collobert et al. ,  2011 ;  Chiu and Nichols ,  2016 ). Similar to the statistical toolkits,  $\\mathrm{NCRF++}$   supports user-deﬁned features but using distributed representations through lookup tables, which can be initialized randomly or from exter- nal pretrained embeddings (embedding directory: emb dir  in Figure  1 ). In addition,  $\\mathrm{NCRF++}$   in- tegrates several state-of-the-art automatic feature extractors, such as CNN and LSTM for character sequences, leading easy reproduction of many re- cent work ( Lample et al. ,  2016 ;  Chiu and Nichols , 2016 ;  Ma and Hovy ,  2016 ). \n•  Effective and efﬁcient : we reimplement sev- eral state-of-the-art neural models ( Lample et al. , 2016 ;  Ma and Hovy ,  2016 ) using  $\\mathrm{NCRF++}$  . Ex- periments show models built in   $\\mathrm{NCRF++}$   give comparable performance with reported results in the literature. Besides,   $\\mathrm{NCRF++}$   is implemented using batch calculation, which can be acceler- ated using GPU. Our experiments demonstrate that   $\\mathrm{NCRF++}$   as an effective and efﬁcient toolkit. •  Function enriched :  $\\mathrm{NCRF++}$   extends the Viterbi algorithm ( Viterbi ,  1967 ) to enable decod- ing  $n$   best sequence labels with their probabilities. Taking NER, Chunking and POS tagging as typ- ical examples, we investigate the performance of models built in   $\\mathrm{NCRF++}$  , the inﬂuence of human- deﬁned and automatic features, the performance of  nbest  decoding and the running speed with the batch size. Detail results are shown in Section  3 . \n\n2  $\\mathbf{NCHF}++$   Architecture \nThe framework of  $\\mathrm{NCRF++}$   is shown in Figure  2 .  $\\mathrm{NCRF++}$   is designed with three layers: a character sequence layer; a word sequence layer and infer- ence layer. For each input word sequence, words are represented with word embeddings. The char- acter sequence layer can be used to automatically extract word level features by encoding the char- acter sequence within the word. Arbitrary hand- crafted features such as capitalization  [Cap] , POS tag  [POS] , preﬁxes  [Pre]  and sufﬁxes [Suf]  are also supported by   $\\mathrm{NCRF++}$  . Word representations are the concatenation of word em- beddings (red circles), character sequence encod- ing hidden vector (yellow circles) and handcrafted neural features (grey circles). Then the word se- quence layer takes the word representations as in- put and extracts the sentence level features, which are fed into the inference layer to assign a label to each word. When building the network, users only need to edit the conﬁguration ﬁle to conﬁg- ure the model structure, training settings and hy- perparameters. We use layer-wised encapsulation in our implementation. Users can extend  $\\mathrm{NCRF++}$  by deﬁning their own structure in any layer and in- tegrate it into  $\\mathrm{NCRF++}$   easily. "}
{"page": 2, "image_path": "doc_images/P18-4013_2.jpg", "ocr_text": "only need to edit the configuration file to config-\nure the model structure, training settings and hy-\nperparameters. We use layer-wised encapsulation\nin our implementation. Users can extend NCRF++\nby defining their own structure in any layer and in-\ntegrate it into NCRF++ easily.\n\n2.1 Layer Units\n2.1.1 Character Sequence Layer\n\nThe character sequence layer integrates several\ntypical neural encoders for character sequence in-\nformation, such as RNN and CNN. It is easy to se-\nlect our existing encoder through the configuration\nfile (by setting char_seq_feature in Figure\n1). Characters are represented by character em-\nbeddings (green circles in Figure 2), which serve\nas the input of character sequence layer.\n\ne Character RNN and its variants Gated Re-\ncurrent Unit (GRU) and LSTM are supported by\nNCRF++. The character sequence layer uses\nbidirectional RNN to capture the left-to-right and\nright-to-left sequence information, and concate-\nnates the final hidden states of two RNNs as the\nencoder of the input character sequence.\n\ne Character CNN takes a sliding window to cap-\nture local features, and then uses a max-pooling for\naggregated encoding of the character sequence.\n\n2.1.2 Word Sequence Layer\n\nSimilar to the character sequence layer, NCRF++\nsupports both RNN and CNN as the word se-\nquence feature extractor. The selection can be con-\nfigurated through word_seq_feature in Fig-\nure 1. The input of the word sequence layer is a\nword representation, which may include word em-\nbeddings, character sequence representations and\nhandcrafted neural features (the combination de-\npends on the configuration file). The word se-\nquence layer can be stacked, building a deeper fea-\nture extractor.\n\ne Word RNN together with GRU and LSTM are\navailable in NCRF++, which are popular struc-\ntures in the recent literature (Huang et al., 2015;\nLample et al., 2016; Ma and Hovy, 2016; Yang\net al., 2017). Bidirectional RNNs are supported\nto capture the left and right contexted information\nof each word. The hidden vectors for both direc-\ntions on each word are concatenated to represent\nthe corresponding word.\n\ne Word CNN utilizes the same sliding window as\ncharacter CNN, while a nonlinear function (Glo-\nrot et al., 2011) is attached with the extracted fea-\n\n76\n\ntures. Batch normalization (Ioffe and Szegedy,\n2015) and dropout (Srivastava et al., 2014) are also\nsupported to follow the features.\n\n2.1.3 Inference Layer\n\nThe inference layer takes the extracted word se-\nquence representations as features and assigns la-\nbels to the word sequence. NCRF++ supports both\nsoftmax and CRF as the output layer. A linear\nlayer firstly maps the input sequence representa-\ntions to label vocabulary size scores, which are\nused to either model the label probabilities of each\nword through simple softmax or calculate the label\nscore of the whole sequence.\ne Softmax maps the label scores into a probabil-\nity space. Due to the support of parallel decod-\ning, softmax is much more efficient than CRF and\nworks well on some sequence labeling tasks (Ling\net al., 2015). In the training process, various loss\nfunctions such as negative likelihood loss, cross\nentropy loss are supported.\n\ne CRF captures label dependencies by adding\ntransition scores between neighboring labels.\nNCRF++ supports CRF trained with the sentence-\nlevel maximum log-likelihood loss. During the\ndecoding process, the Viterbi algorithm is used to\nsearch the label sequence with the highest proba-\nbility. In addition, NCRF++ extends the decoding\nalgorithm with the support of nbest output.\n\n2.2 User Interface\n\nNCRF++ provides users with abundant network\nconfiguration interfaces, including the network\nstructure, input and output directory setting, train-\ning settings and hyperparameters. By editing a\nconfiguration file, users can build most state-of-\nthe-art neural sequence labeling models. On the\nother hand, all the layers above are designed as\n“plug-in” modules, where user-defined layer can\nbe integrated seamlessly.\n\n2.2.1 Configuration\n\ne Networks can be configurated in the three\nlayers as described in Section 2.1. It con-\ntrols the choice of neural structures in character\nand word levels with char_seg_feature and\nword_seq_feature, respectively. The infer-\nence layer is set by use_crf. It also defines the\nusage of handcrafted features and their properties\nin feature.\n\ne VO is the input and output file directory\nconfiguration. It includes training_dir,\n", "vlm_text": "\n2.1 Layer Units \n2.1.1 Character Sequence Layer \nThe character sequence layer integrates several typical neural encoders for character sequence in- formation, such as RNN and CNN. It is easy to se- lect our existing encoder through the conﬁguration ﬁle (by setting  char seq feature  in Figure 1 ). Characters are represented by character em- beddings (green circles in Figure  2 ), which serve as the input of character sequence layer. \n•  Character RNN  and its variants Gated Re- current Unit (GRU) and LSTM are supported by  $\\mathrm{NCRF++}$  . The character sequence layer uses bidirectional RNN to capture the left-to-right and right-to-left sequence information, and concate- nates the ﬁnal hidden states of two RNNs as the encoder of the input character sequence. \n•  Character CNN  takes a sliding window to cap- ture local features, and then uses a  max-pooling  for aggregated encoding of the character sequence. \n2.1.2 Word Sequence Layer \nSimilar to the character sequence layer,   $\\mathrm{NCRF++}$  supports both RNN and CNN as the word se- quence feature extractor. The selection can be con- ﬁgurated through  word seq feature  in Fig- ure  1 . The input of the word sequence layer is a word representation, which may include word em- beddings, character sequence representations and handcrafted neural features (the combination de- pends on the conﬁguration ﬁle). The word se- quence layer can be stacked, building a deeper fea- ture extractor. \n•  Word RNN  together with GRU and LSTM are available in   $\\mathrm{NCRF++}$  , which are popular struc- tures in the recent literature ( Huang et al. ,  2015 ; Lample et al. ,  2016 ;  Ma and Hovy ,  2016 ;  Yang et al. ,  2017 ). Bidirectional RNNs are supported to capture the left and right contexted information of each word. The hidden vectors for both direc- tions on each word are concatenated to represent the corresponding word. \n•  Word CNN  utilizes the same sliding window as character CNN, while a nonlinear function ( Glo- rot et al. ,  2011 ) is attached with the extracted fea- tures. Batch normalization ( Ioffe and Szegedy , 2015 ) and dropout ( Srivastava et al. ,  2014 ) are also supported to follow the features. \n\n2.1.3 Inference Layer \nThe inference layer takes the extracted word se- quence representations as features and assigns la- bels to the word sequence.   $\\mathrm{NCRF++}$   supports both softmax and CRF as the output layer. A linear layer ﬁrstly maps the input sequence representa- tions to label vocabulary size scores, which are used to either model the label probabilities of each word through simple softmax or calculate the label score of the whole sequence. \n•  Softmax  maps the label scores into a probabil- ity space. Due to the support of parallel decod- ing, softmax is much more efﬁcient than CRF and works well on some sequence labeling tasks ( Ling et al. ,  2015 ). In the training process, various loss functions such as negative likelihood loss, cross entropy loss are supported. \n•  CRF  captures label dependencies by adding transition scores between neighboring labels.  $\\mathrm{NCRF++}$   supports CRF trained with the sentence- level maximum log-likelihood loss. During the decoding process, the Viterbi algorithm is used to search the label sequence with the highest proba- bility. In addition,  $\\mathrm{NCRF++}$   extends the decoding algorithm with the support of  nbest  output. \n2.2 User Interface \n $\\mathrm{NCRF++}$   provides users with abundant network conﬁguration interfaces, including the network structure, input and output directory setting, train- ing settings and hyperparameters. By editing a conﬁguration ﬁle, users can build most state-of- the-art neural sequence labeling models. On the other hand, all the layers above are designed as “plug-in” modules, where user-deﬁned layer can be integrated seamlessly. \n2.2.1 Conﬁguration \n•  Networks  can be conﬁgurated in the three layers as described in Section  2.1 . It con- trols the choice of neural structures in character and word levels with  char seq feature  and word seq feature , respectively. The infer- ence layer is set by  use crf . It also deﬁnes the usage of handcrafted features and their properties in  feature . \n•  I/O  is the input and output ﬁle directory conﬁguration. It includes  training dir, "}
{"page": 3, "image_path": "doc_images/P18-4013_3.jpg", "ocr_text": "Table 1: Results on three benchmarks.\n\ndev-dir, test_dir, raw-dir, pretrained\ncharacter or word embedding (char_emb_dim\nor word_emb_dim), and decode file directory\ndecode-dir).\n\nTraining the\n(Loss_function), optimizer (optimizer)*\nshuffle training instances train_shuffle and\nverage batch loss ave_batch-_loss.\nHyperparameter includes most of the param-\ners in the networks and training such as learn-\ning rate (lr) and its decay (1r_decay), hidden\nlayer size of word and character (hidden_dim\nand char_hidden_dim), nbest size (nbest),\nbatch size (batch_size), dropout (dropout),\netc. Note that the embedding size of each hand-\ncrafted feature is configured in the networks con-\nfiguration (Eeature=[POS] emb_dir=None\nemb_size=10 in Figure 1).\n\ne includes loss function\n\na\ne\ne\n\n2.2.2 Extension\n\nUsers can write their own custom modules on all\nthree layers, and user-defined layers can be inte-\ngrated into the system easily. For example, if a\nuser wants to define a custom character sequence\nlayer with a specific neural structure, he/she only\nneeds to implement the part between input char-\nacter sequence indexes to sequence representa-\ntions. All the other networks structures can be\nused and controlled through the configuration file.\nA README file is given on this.\n\n3 Evaluation\n\n3.1 Settings\n\nTo evaluate the performance of our toolkit, we\nconduct the experiments on several datasets. For\nNER task, CoNLL 2003 data (Tjong Kim Sang\n\n‘Currently NCRF++ supports five — optimizers:\nSGD/AdaGrad/AdaDelta/RMSProp/Adam.\n\n77\n\nModels NER chunking | POS Features P R F\nFl-value | Fl-value | Acc Baseline WLSTM+CREF | 80.44 | 87.88 | 89.15\n\nNochar+WCNN+CRF 88.90 94.23 96.99 +POS 90.61 | 89.28 | 89.94\n\nCLSTM+WCNN+CRF 90.70 94.76 97.38 Human Feature +Cap 90.74 | 90.43 | 90.58\n\nCCNN+WCNN+CRF 90.43 94.77 97.33 +POS+Cap | 90.92 | 90.27 | 90.59\n\nNochar+WLSTM+CRF | 89.45 94.49 97.20 Auto Feature +CLSTM = | 91.22 | 91.17 | 91.20\n\nCLSTM+WLSTM+CRF | 91.20 _ | 95.00 97.49 +CCNN | 91.66 | 91.04 | 91.35\n\nCCNN+WLSTM+CRF 91.35 95.06 97.46\n\nLample et al. (2016) 90.94 - 97.51 Table 2: Results using different features.\n\nMa and Hovy (2016) 91.21 - 97.55\n\nYang et al. (2017) 91.20 94.66 97.55\n\nPeters et al. (2017) 90.87 95.00 - and De Meulder, 2003) with the standard split\n\nis used. For the chunking task, we perform ex-\nperiments on CoNLL 2000 shared task (Tjong\nKim Sang and Buchholz, 2000), data split is fol-\nlowing Reimers and Gurevych (2017). For POS\ntagging, we use the same data and split with Ma\nand Hovy (2016). We test different combinations\nof character representations and word sequence\nrepresentations on these three benchmarks. Hy-\nperparameters are mostly following Ma and Hovy\n(2016) and almost keep the same in all these exper-\niments>. Standard SGD with a decaying learning\nrate is used as the optimizer.\n\n3.2 Results\n\nTable 1 shows the results of six CRF-based mod-\nels with different character sequence and word\nsequence representations on three benchmarks.\nState-of-the-art results are also listed. In this table,\n“Nochar” suggests a model without character se-\nquence information. ““CLSTM” and ““CCNN” rep-\nresent models using LSTM and CNN to encode\ncharacter sequence, respectively. Similarly, “WL-\nSTM” and “WCNN” indicate that the model uses\nLSTM and CNN to represent word sequence, re-\nspectively.\n\nAs shown in Table 1, “WCNN” based mod-\nels consistently underperform the “WLSTM”\nbased models, showing the advantages of LSTM\non capturing global features. Character in-\nformation can improve model performance sig-\nnificantly, while using LSTM or CNN give\nsimilar improvement. Most of state-of-the-art\nmodels utilize the framework of word LSTM-\nCRF with character LSTM or CNN features\n(correspond to “CLSTM+WLSTM+CRPF” and\n“CCNN+WLSTM+CRF” of our models) (Lample\net al., 2016; Ma and Hovy, 2016; Yang et al., 2017;\nPeters et al., 2017). Our implementations can\nachieve comparable results, with better NER and\n\n>We use a smaller learning rate (0.005) on CNN based\nword sequence representation.\n", "vlm_text": "The table presents the performance of various models across three different tasks: Named Entity Recognition (NER), Chunking, and Part-of-Speech (POS) tagging. Each model is evaluated using either the F1-value (for NER and Chunking) or Accuracy (for POS) as the metric.\n\n1. **Models**:\n    - The models are combinations of different components indicated by their names:\n        - Nochar+WCNN+CRF\n        - CLSTM+WCNN+CRF\n        - CCNN+WCNN+CRF\n        - Nochar+WLSTM+CRF\n        - CLSTM+WLSTM+CRF\n        - CCNN+WLSTM+CRF\n\n2. **NER (Named Entity Recognition)**:\n    - Performance is reported as the F1-value.\n    - Best performance in NER is 91.35, achieved by the model: CCNN+WLSTM+CRF.\n\n3. **Chunking**:\n    - Performance is reported as the F1-value.\n    - Best performance in Chunking is 95.06, achieved by the model: CCNN+WLSTM+CRF.\n\n4. **POS (Part-of-Speech) tagging**:\n    - Performance is reported as Accuracy.\n    - Highest accuracy is 97.55, achieved by Ma and Hovy (2016) and Yang et al. (2017).\n\n5. **Additional references**:\n    - Lample et al. (2016), Ma and Hovy (2016), Yang et al. (2017), and Peters et al. (2017) are additional models cited for comparison.\n    - Note that some values for these references are missing or not applicable (e.g., Chunking F1-value is not provided for Lample et al. and Peters et al.).\n\nOverall, the table primarily compares the efficiency of different model architectures in processing NER, Chunking, and POS tasks, highlighting significant achievers in each category.\ndev dir, test dir, raw dir , pretrained character or word embedding ( char emb dim or  word emb dim ), and decode ﬁle directory\n\n ( decode dir ).\n\n \n• ( loss function Training includes ), optimizer ( the optimizer loss function ) 4 shufﬂe training instances  train shuffle  and average batch loss  ave batch loss . \n•  Hyperparameter  includes most of the param- eters in the networks and training such as learn- ing rate   $(\\mathtt{L}\\mathtt{r})$   and its decay ( lr decay ), hidden layer size of word and character ( hidden dim and  char hidden dim ),  nbest  size ( nbest ), batch size ( batch size ), dropout ( dropout ), etc. Note that the embedding size of each hand- crafted feature is conﬁgured in the networks con- ﬁguration ( feature  $=$  [POS] emb dir  $=$  None emb si  $z\\!\\in\\!=\\!10$   in Figure  1 ). \n2.2.2 Extension \nUsers can write their own custom modules on all three layers, and user-deﬁned layers can be inte- grated into the system easily. For example, if a user wants to deﬁne a custom character sequence layer with a speciﬁc neural structure, he/she only needs to implement the part between input char- acter sequence indexes to sequence representa- tions. All the other networks structures can be used and controlled through the conﬁguration ﬁle. A  README  ﬁle is given on this. \n3 Evaluation \n3.1 Settings \nTo evaluate the performance of our toolkit, we conduct the experiments on several datasets. For NER task, CoNLL 2003 data ( Tjong Kim Sang \nThe table presents a comparison of different features and their impact on model performance, measured by precision (P), recall (R), and F1 score (F). Below are the details:\n\n1. **Baseline:**\n   - **Features:** WLSTM+CRF\n   - **Performance:**\n     - Precision (P): 80.44\n     - Recall (R): 87.88\n     - F1 Score (F): 89.15\n\n2. **Human Feature:**\n   - **Features:**\n     - +POS\n       - Precision (P): 90.61\n       - Recall (R): 89.28\n       - F1 Score (F): 89.94\n     - +Cap\n       - Precision (P): 90.74\n       - Recall (R): 90.43\n       - F1 Score (F): 90.58\n     - +POS+Cap\n       - Precision (P): 90.92\n       - Recall (R): 90.27\n       - F1 Score (F): 90.59\n\n3. **Auto Feature:**\n   - **Features:**\n     - +CLSTM\n       - Precision (P): 91.22\n       - Recall (R): 91.17\n       - F1 Score (F): 91.20\n     - +CCNN\n       - Precision (P): 91.66\n       - Recall (R): 91.04\n       - F1 Score (F): 91.35\n\nThe table indicates that the use of automatic features (Auto Feature) such as CLSTM and CCNN results in better precision, recall, and F1 score compared to the baseline and human-engineered features.\nand De Meulder ,  2003 ) with the standard split is used. For the chunking task, we perform ex- periments on CoNLL 2000 shared task ( Tjong Kim Sang and Buchholz ,  2000 ), data split is fol- lowing  Reimers and Gurevych  ( 2017 ). For POS tagging, we use the same data and split with  Ma and Hovy  ( 2016 ). We test different combinations of character representations and word sequence representations on these three benchmarks. Hy- perparameters are mostly following  Ma and Hovy ( 2016 ) and almost keep the same in all these exper- iments 5 . Standard SGD with a decaying learning rate is used as the optimizer. \n3.2 Results \nTable  1  shows the results of six CRF-based mod- els with different character sequence and word sequence representations on three benchmarks. State-of-the-art results are also listed. In this table, “Nochar” suggests a model without character se- quence information. “CLSTM” and “CCNN” rep- resent models using LSTM and CNN to encode character sequence, respectively. Similarly, “WL- STM” and “WCNN” indicate that the model uses LSTM and CNN to represent word sequence, re- spectively. \nAs shown in Table  1 , “WCNN” based mod- els consistently underperform the “WLSTM” based models, showing the advantages of LSTM on capturing global features. Character in- formation can improve model performance sig- niﬁcantly, while using LSTM or CNN give similar improvement. Most of state-of-the-art models utilize the framework of word LSTM- CRF with character LSTM or CNN features\n\n (correspond to “CLSTM  $+$  WLSTM  $+$  CRF” and\n\n “CCNN  $^+$  WLSTM  $^+$  CRF” of our models) ( Lample et al. ,  2016 ;  Ma and Hovy ,  2016 ;  Yang et al. ,  2017 ; Peters et al. ,  2017 ). Our implementations can achieve comparable results, with better NER and "}
{"page": 4, "image_path": "doc_images/P18-4013_4.jpg", "ocr_text": "1.00\n\nI\na\neo\n0.98 v\nsere\nee *\nS 0.96 aa\n3 -\n° «\n2 ’\n6 0.94 is\n/\n/\n/\n0.92 / y= Token Accuracy\n4 =a+ Entity F1-value\no 12 3 4 5 6 7 8 9 10 11\nN best\n\nFigure 3: Oracle performance with nbest.\n\nchunking performances and slightly lower POS\ntagging accuracy. Note that we use almost the\nsame hyperparameters across all the experiments\nto achieve the results, which demonstrates the ro-\nbustness of our implementation. The full experi-\nmental results and analysis are published in Yang\net al. (2018).\n\n3.3. Influence of Features\n\nWe also investigate the influence of different fea-\ntures on system performance. Table 2 shows the\nresults on the NER task. POS tag and capital in-\ndicator are two common features on NER tasks\n(Collobert et al., 2011; Huang et al., 2015; Strubell\net al., 2017). In our implementation, each POS\ntag or capital indicator feature is mapped as 10-\ndimension feature embeddings through randomly\ninitialized feature lookup table °. The feature em-\nbeddings are concatenated with the word embed-\ndings as the representation of the corresponding\nword. Results show that both human features\n[POS] and [Cap] can contribute the NER sys-\ntem, this is consistent with previous observations\n(Collobert et al., 2011; Chiu and Nichols, 2016).\nBy utilizing LSTM or CNN to encode character\nsequence automatically, the system can achieve\nbetter performance on NER task.\n\n3.4. N best Decoding\n\nWe investigate nbest Viterbi decoding\non NER dataset through the best model\n“CCNN+WLSTM+CRF”. Figure 3 shows\n\nthe oracle entity Fl-values and token accuracies\nwith different nbest sizes. The oracle Fl-value\n\n®feature=[POS] emb_dir=None emb_size=10\nfeature=[Cap] emb_dir=None emb_size=10\n\n78\n\nME Decoding Speed\nME Training Speed\n\nul\n\n8 10 15 20 30 50 100200\nBatch Size\n\n2000\n\nHb\nuw\nSo\n3\n\n1000\n\nSpeed (sent/s)\n\nuw\nSo\nSo\n\n°\n\nFigure 4: Speed with batch size.\n\nrises significantly with the increasement of nbest\nsize, reaching 97.47% at n = 10 from the baseline\nof 91.35%. The token level accuracy increases\nfrom 98.00% to 99.39% in J0-best. Results show\nthat the nbest outputs include the gold entities and\nlabels in a large coverage, which greatly enlarges\nthe performance of successor tasks.\n\n3.5 Speed with Batch Size\n\nAs NCRF++ is implemented on batched calcula-\ntion, it can be greatly accelerated through paral-\nlel computing through GPU. We test the system\nspeeds on both training and decoding process on\nNER dataset using a Nvidia GTX 1080 GPU. As\nshown in Figure 4, both the training and the decod-\ning speed can be significantly accelerated through\na large batch size. The decoding speed reaches sat-\nuration at batch size 100, while the training speed\nkeeps growing. The decoding speed and training\nspeed of NCRF++ are over 2000 sentences/second\nand 1000 sentences/second, respectively, demon-\nstrating the efficiency of our implementation.\n\n4 Conclusion\n\nWe presented NCRF++, an open-source neural\nsequence labeling toolkit, which has a CRF ar-\nchitecture with configurable neural representation\nlayers. Users can design custom neural models\nthrough the configuration file. NCRF++ supports\nflexible feature utilization, including handcrafted\nfeatures and automatically extracted features. It\ncan also generate nbest label sequences rather than\nthe best one. We conduct a series of experiments\nand the results show models built on NCRF++\ncan achieve state-of-the-art results with an effi-\ncient running speed.\n", "vlm_text": "The image is a line graph depicting \"Oracle performance with nbest.\" It shows the relationship between \"N best\" on the x-axis and \"Oracle scores\" on the y-axis. There are two lines representing different metrics: \n\n1. The blue line (with inverted triangle markers) represents \"Token Accuracy.\" This line starts near an Oracle score of 0.98 when N is 1 and gradually increases, maintaining a high level, and appears to slightly rise as N increases from 1 to 11.\n\n2. The red line (with triangle markers) represents \"Entity F1-value.\" This line starts around an Oracle score of 0.92 at N of 1 and rises more sharply compared to the blue line as N increases, eventually approaching about 0.97 as the N value reaches 11.\n\nThe graph includes a legend in the bottom right corner, labeling the two lines as \"Token Accuracy\" and \"Entity F1-value.\" The overall trend indicates that both Oracle scores improve as the N best increases, with Token Accuracy generally being higher than Entity F1-value at corresponding points.\nchunking performances and slightly lower POS tagging accuracy. Note that we use almost the same hyperparameters across all the experiments to achieve the results, which demonstrates the ro- bustness of our implementation. The full experi- mental results and analysis are published in  Yang et al.  ( 2018 ). \n3.3 Inﬂuence of Features \nWe also investigate the inﬂuence of different fea- tures on system performance. Table  2  shows the results on the NER task. POS tag and capital in- dicator are two common features on NER tasks ( Collobert et al. ,  2011 ;  Huang et al. ,  2015 ;  Strubell et al. ,  2017 ). In our implementation, each POS tag or capital indicator feature is mapped as 10- dimension feature embeddings through randomly initialized feature lookup table   6 . The feature em- beddings are concatenated with the word embed- dings as the representation of the corresponding word. Results show that both human features [POS]  and  [Cap]  can contribute the NER sys- tem, this is consistent with previous observations ( Collobert et al. ,  2011 ;  Chiu and Nichols ,  2016 ). By utilizing LSTM or CNN to encode character sequence automatically, the system can achieve better performance on NER task. \n3.4 N best Decoding \nWe investigate nbest Viterbi decoding on NER dataset through the best model  $\\scriptstyle\\mathrm{\\\"CCNN+WLSTM+CRF^{\\prime}}$  . Figure 3 shows the oracle entity F1-values and token accuracies with different  nbest  sizes. The oracle F1-value \nThe image is a bar chart titled \"Speed with batch size,\" which illustrates the relationship between batch size and speed in terms of sentences per second (sent/s). The x-axis represents the batch size, ranging from 1 to 200, while the y-axis represents the speed in sent/s, ranging from 0 to 2000.\n\nThere are two types of speed data represented in the chart:\n\n1. **Decoding Speed** - Shown in blue bars. It increases with the batch size, displaying a consistently higher speed compared to training speed for all batch sizes.\n\n2. **Training Speed** - Shown in red bars. It also increases with batch size but at a slower rate compared to decoding speed.\n\nAs the batch size increases, both decoding and training speeds increase, but decoding speed is significantly faster than training speed at all batch sizes, as indicated by the blue bars being taller than the red bars for each batch size.\nrises signiﬁcantly with the increasement of  nbest size, reaching  $97.47\\%$   at    $n=10$   from the baseline of   $91.35\\%$  . The token level accuracy increases from  $98.00\\%$   to   $99.39\\%$   in    $I O$  -best . Results show that the  nbest  outputs include the gold entities and labels in a large coverage, which greatly enlarges the performance of successor tasks. \n3.5 Speed with Batch Size \nAs  $\\mathrm{NCRF++}$   is implemented on batched calcula- tion, it can be greatly accelerated through paral- lel computing through GPU. We test the system speeds on both training and decoding process on NER dataset using a Nvidia GTX 1080 GPU. As shown in Figure  4 , both the training and the decod- ing speed can be signiﬁcantly accelerated through a large batch size. The decoding speed reaches sat- uration at batch size 100, while the training speed keeps growing. The decoding speed and training speed of   $\\mathrm{NCRF++}$   are over 2000 sentences/second and 1000 sentences/second, respectively, demon- strating the efﬁciency of our implementation. \n4 Conclusion \nWe presented   $\\mathrm{NCRF++}$  , an open-source neural sequence labeling toolkit, which has a CRF ar- chitecture with conﬁgurable neural representation layers. Users can design custom neural models through the conﬁguration ﬁle.   $\\mathrm{NCRF++}$   supports ﬂexible feature utilization, including handcrafted features and automatically extracted features. It can also generate  nbest  label sequences rather than the best one. We conduct a series of experiments and the results show models built on   $\\mathrm{NCRF++}$  can achieve state-of-the-art results with an efﬁ- cient running speed. "}
{"page": 5, "image_path": "doc_images/P18-4013_5.jpg", "ocr_text": "References\n\nJason Chiu and Eric Nichols. 2016. Named\nentity recognition with bidirectional LSTM-\nCNNs. Transactions of the Association\nfor Computational Linguistics 4:357-370.\nhttps://transacl.org/ojs/index.php/tacl/article/view/792.\n\nRonan Collobert, Jason Weston, Léon Bottou, Michael\nKarlen, Koray Kavukcuoglu, and Pavel Kuksa.\n2011. Natural language processing (almost) from\nscratch. Journal of Machine Learning Research\n12(Aug):2493-2537.\n\nXavier Glorot, Antoine Bordes, and Yoshua Bengio.\n2011. Deep sparse rectifier neural networks. In Jn-\nternational Conference on Artificial Intelligence and\nStatistics. pages 315-323.\n\n1997.\n\nNeural computation\n\nSepp Hochreiter and Jiirgen Schmidhuber.\nLong short-term memory.\n9(8):1735-1780.\n\nZhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidi-\nrectional LSTM-CRF models for sequence tagging.\narXiv preprint arXiv:1508.01991 .\n\nSergey loffe and Christian Szegedy. 2015. Batch nor-\nmalization: Accelerating deep network training by\nreducing internal covariate shift. In International\nConference on Machine Learning. pages 448-456.\n\nS Sathiya Keerthi and Sellamanickam Sundararajan.\n2007. Crf versus svm-struct for sequence labeling.\nYahoo Research Technical Report .\n\nJohn Lafferty, Andrew McCallum, and Fernando CN\nPereira. 2001. Conditional random fields: Prob-\nabilistic models for segmenting and labeling se-\nquence data. In International Conference on Ma-\nchine Learning. volume 1, pages 282-289.\n\nGuillaume Lample, Miguel Ballesteros, Sandeep Sub-\nramanian, Kazuya Kawakami, and Chris Dyer. 2016.\nNeural architectures for named entity recognition.\nIn NAACL-HLT. pages 260-270.\n\nYann LeCun, Bernhard Boser, John S Denker, Don-\nnie Henderson, Richard E Howard, Wayne Hubbard,\nand Lawrence D Jackel. 1989. Backpropagation ap-\nplied to handwritten zip code recognition. Neural\ncomputation 1(4):541-551.\n\nWang Ling, Chris Dyer, Alan W Black, Isabel Tran-\ncoso, Ramon Fermandez, Silvio Amir, Luis Marujo,\nand Tiago Luis. 2015. Finding function in form:\nCompositional character models for open vocabu-\nlary word representation. In EMNLP. pages 1520-—\n1530.\n\nLiyuan Liu, Jingbo Shang, Frank Xu, Xiang Ren, Huan\nGui, Jian Peng, and Jiawei Han. 2018. Empower\nsequence labeling with task-aware neural language\nmodel. In AAAI.\n\n79\n\nXuezhe Ma and Eduard Hovy. 2016. End-to-end se-\nquence labeling via Bi-directional LSTM-CNNs-\nCRF. In ACL. volume 1, pages 1064-1074.\n\nNaoaki Okazaki. 2007. Crfsuite: a fast implementation\nof conditional random fields (crfs) .\n\nMatthew Peters, Waleed Ammar, Chandra Bhagavat-\nula, and Russell Power. 2017. Semi-supervised se-\nquence tagging with bidirectional language models.\nIn ACL. volume 1, pages 1756-1765.\n\nXuan-Hieu Phan, Le-Minh Nguyen, and Cam-Tu\nNguyen. 2004. Flexcrfs: Flexible conditional ran-\ndom fields.\n\nLev Ratinov and Dan Roth. 2009. Design challenges\nand misconceptions in named entity recognition. In\nCoNLL. pages 147-155.\n\nNils Reimers and Iryna Gurevych. 2017. Reporting\nscore distributions makes a difference: Performance\nstudy of Istm-networks for sequence tagging. In\nEMNLP. pages 338-348.\n\nFei Sha and Fernando Pereira. 2003. Shallow pars-\ning with conditional random fields. In NAACL-HLT.\npages 134-141.\n\nNitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky,\nIlya Sutskever, and Ruslan Salakhutdinov. 2014.\nDropout: a simple way to prevent neural networks\nfrom overfitting. Journal of Machine Learning Re-\nsearch 15(1):1929-1958.\n\nEmma Strubell, Patrick Verga, David Belanger, and\nAndrew McCallum. 2017. Fast and accurate entity\nrecognition with iterated dilated convolutions. In\nEMNLP. pages 2670-2680.\n\nErik F Tjong Kim Sang and Sabine Buchholz. 2000.\nIntroduction to the conll-2000 shared task: Chunk-\ning. In Proceedings of the 2nd workshop on Learn-\ning language in logic and the 4th conference on\n\nComputational natural language learning-Volume\n7. pages 127-132.\n\nErik F Tjong Kim Sang and Fien De Meulder.\n2003. Introduction to the conll-2003 shared task:\nLanguage-independent named entity recognition. In\nHLT-NAACL. pages 142-147.\n\nAndrew Viterbi. 1967. Error bounds for convolutional\ncodes and an asymptotically optimum decoding al-\ngorithm. JEEE transactions on Information Theory\n13(2):260-269.\n\nJie Yang, Shuailong Liang, and Yue Zhang. 2018. De-\nsign challenges and misconceptions in neural se-\nquence labeling. In COLING.\n\nZhilin Yang, Ruslan Salakhutdinov, and William W\nCohen. 2017. Transfer learning for sequence tag-\nging with hierarchical recurrent networks. In Jnter-\nnational Conference on Learning Representations.\n", "vlm_text": "References \nJason Chiu and Eric Nichols. 2016. Named entity recognition with bidirectional LSTM- CNNs . Transactions of the Association for Computational Linguistics 4:357–370. https://transacl.org/ojs/index.php/tacl/article/view/792 Ronan Collobert, Jason Weston, L´ eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. Journal of Machine Learning Research 12(Aug):2493–2537. Xavier Glorot, Antoine Bordes, and Yoshua Bengio. 2011. Deep sparse rectiﬁer neural networks. In  In- ternational Conference on Artiﬁcial Intelligence and Statistics . pages 315–323. Sepp Hochreiter and J¨ urgen Schmidhuber. 1997. Long short-term memory. Neural computation 9(8):1735–1780. Zhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidi- rectional LSTM-CRF models for sequence tagging. arXiv preprint arXiv:1508.01991  . Sergey Ioffe and Christian Szegedy. 2015. Batch nor- malization: Accelerating deep network training by reducing internal covariate shift. In  International Conference on Machine Learning . pages 448–456. S Sathiya Keerthi and Sellamanickam Sundararajan. 2007. Crf versus svm-struct for sequence labeling. Yahoo Research Technical Report  . John Lafferty, Andrew McCallum, and Fernando CN Pereira. 2001. Conditional random ﬁelds: Prob- abilistic models for segmenting and labeling se- quence data. In  International Conference on Ma- chine Learning . volume 1, pages 282–289. Guillaume Lample, Miguel Ballesteros, Sandeep Sub- ramanian, Kazuya Kawakami, and Chris Dyer. 2016. Neural architectures for named entity recognition. In  NAACL-HLT . pages 260–270. Yann LeCun, Bernhard Boser, John S Denker, Don- nie Henderson, Richard E Howard, Wayne Hubbard, and Lawrence D Jackel. 1989. Backpropagation ap- plied to handwritten zip code recognition.  Neural computation  1(4):541–551. Wang Ling, Chris Dyer, Alan W Black, Isabel Tran- coso, Ramon Fermandez, Silvio Amir, Luis Marujo, and Tiago Luis. 2015. Finding function in form: Compositional character models for open vocabu- lary word representation. In  EMNLP . pages 1520– 1530. Liyuan Liu, Jingbo Shang, Frank Xu, Xiang Ren, Huan Gui, Jian Peng, and Jiawei Han. 2018. Empower sequence labeling with task-aware neural language model. In  AAAI . \nXuezhe Ma and Eduard Hovy. 2016. End-to-end se- quence labeling via Bi-directional LSTM-CNNs- CRF. In  ACL . volume 1, pages 1064–1074. Naoaki Okazaki. 2007. Crfsuite: a fast implementation of conditional random ﬁelds (crfs) . Matthew Peters, Waleed Ammar, Chandra Bhagavat- ula, and Russell Power. 2017. Semi-supervised se- quence tagging with bidirectional language models. In  ACL . volume 1, pages 1756–1765. Xuan-Hieu Phan, Le-Minh Nguyen, and Cam-Tu Nguyen. 2004. Flexcrfs: Flexible conditional ran- dom ﬁelds. Lev Ratinov and Dan Roth. 2009. Design challenges and misconceptions in named entity recognition. In CoNLL . pages 147–155. Nils Reimers and Iryna Gurevych. 2017. Reporting score distributions makes a difference: Performance study of lstm-networks for sequence tagging. In EMNLP . pages 338–348. Fei Sha and Fernando Pereira. 2003. Shallow pars- ing with conditional random ﬁelds. In  NAACL-HLT . pages 134–141. Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: a simple way to prevent neural networks from overﬁtting.  Journal of Machine Learning Re- search  15(1):1929–1958. Emma Strubell, Patrick Verga, David Belanger, and Andrew McCallum. 2017. Fast and accurate entity recognition with iterated dilated convolutions. In EMNLP . pages 2670–2680. Erik F Tjong Kim Sang and Sabine Buchholz. 2000. Introduction to the conll-2000 shared task: Chunk- ing. In  Proceedings of the 2nd workshop on Learn- ing language in logic and the 4th conference on Computational natural language learning-Volume 7 . pages 127–132. Erik F Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the conll-2003 shared task: Language-independent named entity recognition. In HLT-NAACL . pages 142–147. Andrew Viterbi. 1967. Error bounds for convolutional codes and an asymptotically optimum decoding al- gorithm.  IEEE transactions on Information Theory 13(2):260–269. Jie Yang, Shuailong Liang, and Yue Zhang. 2018. De- sign challenges and misconceptions in neural se- quence labeling. In  COLING . Zhilin Yang, Ruslan Salakhutdinov, and William W Cohen. 2017. Transfer learning for sequence tag- ging with hierarchical recurrent networks. In  Inter- national Conference on Learning Representations . "}
