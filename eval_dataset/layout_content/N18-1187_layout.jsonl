{"layout": 0, "type": "text", "text": "Dialogue Learning with Human Teaching and Feedback in End-to-End Trainable Task-Oriented Dialogue Systems ", "text_level": 1, "page_idx": 0, "bbox": [80, 68, 518, 101], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 1, "type": "text", "text": "Bing Liu 1 , Gokhan T¨ ur 2 , Dilek Hakkani-T¨ ur 2 , Pararth Shah 2 , Larry Heck 3 † 1 Carnegie Mellon University, Pittsburgh, PA, USA ", "page_idx": 0, "bbox": [107.97498321533203, 114.51702880859375, 496.3208312988281, 143.36642456054688], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 2, "type": "text", "text": "2 Google Research, Mountain View, CA,USA  3 Samsung Research, Mountain View, CA, USA liubing@cmu.edu ,  { dilekh,pararth } @google.com , { gokhan.tur,larry.heck } @ieee.org ", "page_idx": 0, "bbox": [77.18092346191406, 142.90841674804688, 523.3554077148438, 193.3270263671875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 3, "type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0, "bbox": [159, 223, 204, 236], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 4, "type": "text", "text": "In this work, we present a hybrid learn- ing method for training task-oriented dialogue systems through online user interactions. Pop- ular methods for learning task-oriented dia- logues include applying reinforcement learn- ing with user feedback on supervised pre- training models. Efﬁciency of such learning method may suffer from the mismatch of di- alogue state distribution between ofﬂine train- ing and online interactive learning stages. To address this challenge, we propose a hybrid imitation and reinforcement learning method, with which a dialogue agent can effectively learn from its interaction with users by learn- ing from human teaching and feedback. We design a neural network based task-oriented dialogue agent that can be optimized end-to- end with the proposed learning method. Ex- perimental results show that our end-to-end dialogue agent can learn effectively from the mistake it makes via imitation learning from user teaching. Applying reinforcement learn- ing with user feedback after the imitation learning stage further improves the agent’s ca- pability in successfully completing a task. ", "page_idx": 0, "bbox": [89, 245.36859130859375, 273, 544.2974243164062], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 5, "type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0, "bbox": [72, 553, 155, 567], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 6, "type": "text", "text": "Task-oriented dialogue systems assist users to complete tasks in speciﬁc domains by understand- ing user’s request and aggregate useful informa- tion from external resources within several dia- logue turns. Conventional task-oriented dialogue systems have a complex pipeline ( Rudnicky et al. , 1999 ;  Raux et al. ,  2005 ;  Young et al. ,  2013 ) con- sisting of independently developed and modularly connected components for natural language un- derstanding (NLU) ( Mesnil et al. ,  2015 ;  Liu and Lane ,  2016 ;  Hakkani-T¨ ur et al. ,  2016 ), dialogue state tracking (DST) ( Henderson et al. ,  2014c ; ", "page_idx": 0, "bbox": [72, 575.2478637695312, 290, 737.4342651367188], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 7, "type": "text", "text": "Mrkˇ si´ c et al. ,  2016 ), and dialogue policy learn- ing ( Gasic and Young ,  2014 ;  Shah et al. ,  2016 ;  Su et al. ,  2016 ,  2017 ). These system components are usually trained independently, and their optimiza- tion targets may not fully align with the overall system evaluation criteria (e.g. task success rate and user satisfaction). Moreover, errors made in the upper stream modules of the pipeline propa- gate to downstream components and get ampliﬁed, making it hard to track the source of errors. ", "page_idx": 0, "bbox": [307, 223.36402893066406, 525, 358.5065612792969], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 8, "type": "text", "text": "To address these limitations with the con- ventional task-oriented dialogue systems, re- cent efforts have been made in designing end- to-end learning solutions with neural network based methods. Both supervised learning (SL) based ( Wen et al. ,  2017 ;  Bordes and Weston , 2017 ;  Liu and Lane ,  2017a ) and deep reinforce- ment learning (RL) based systems ( Zhao and Es- kenazi ,  2016 ;  Li et al. ,  2017 ;  Peng et al. ,  2017 ) have been studied in the literature. Comparing to chit-chat dialogue models that are usually trained ofﬂine using single-turn context-response pairs, task-oriented dialogue model involves reasoning and planning over multiple dialogue turns. This makes it especially important for a system to be able to learn from users in an interactive manner. Comparing to SL models, systems trained with RL by receiving feedback during users interac- tions showed improved model robustness against diverse dialogue scenarios ( Williams and Zweig , 2016 ;  Liu and Lane ,  2017b ). ", "page_idx": 0, "bbox": [307, 359.43408203125, 525, 643.5635986328125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 9, "type": "text", "text": "A critical step in learning RL based task- oriented dialogue models is dialogue policy learn- ing. Training dialogue policy online from scratch typically requires a large number of interactive learning sessions before an agent can reach a satis- factory performance level. Recent works ( Hender- son et al. ,  2008 ;  Williams et al. ,  2017 ;  Liu et al. , 2017 ) explored pre-training the dialogue model using human-human or human-machine dialogue corpora before performing interactive learning with RL to address this concern. A potential draw- back with such pre-training approach is that the model may suffer from the mismatch of dialogue state distributions between supervised training and interactive learning stages. While interacting with users, the agent’s response at each turn has a di- rect inﬂuence on the distribution of dialogue state that the agent will operate on in the upcoming di- alogue turns. If the agent makes a small mistake and reaches an unfamiliar state, it may not know how to recover from it and get back to a normal dialogue trajectory. This is because such recovery situation may be rare for good human agents and thus are not well covered in the supervised train- ing corpus. This will result in compounding er- rors in a dialogue which may lead to failure of a task. RL exploration might ﬁnally help to ﬁnd cor- responding actions to recover from a bad state, but the search process can be very inefﬁcient. ", "page_idx": 0, "bbox": [307, 644.4911499023438, 525, 766.0305786132812], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 10, "type": "text", "text": "", "page_idx": 1, "bbox": [72, 63.68604278564453, 290, 334.2666320800781], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 11, "type": "text", "text": "To ameliorate the effect of dialogue state distri- bution mismatch between ofﬂine training and RL interactive learning, we propose a hybrid imitation and reinforcement learning method. We ﬁrst let the agent to interact with users using its own pol- icy learned from supervised pre-training. When an agent makes a mistake, we ask users to correct the mistake by demonstrating the agent the right ac- tions to take at each turn. This user corrected dia- logue sample, which is guided by the agent’s own policy, is then added to the existing training cor- pus. We ﬁne-tune the dialogue policy with this di- alogue sample aggregation ( Ross et al. ,  2011 ) and continue such user teaching process for a number of cycles. Since asking for user teaching at each dialogue turn is costly, we want to reduce this user teaching cycles as much as possible and continue the learning process with RL by collecting simple forms of user feedback (e.g. a binary feedback, positive or negative) only at the end of a dialogue. Our main contributions in this work are: ", "page_idx": 1, "bbox": [72, 334.670166015625, 290, 618.7997436523438], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 12, "type": "text", "text": "•  We design a neural network based task- oriented dialogue system which can be op- timized end-to-end for natural language un- derstanding, dialogue state tracking, and dia- logue policy learning. ", "page_idx": 1, "bbox": [82, 623.7453002929688, 290, 691.0877075195312], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 13, "type": "text", "text": "•  We propose a hybrid imitation and reinforce- ment learning method for end-to-end model training in addressing the challenge with dia- logue state distribution mismatch between of- ﬂine training and interactive learning. ", "page_idx": 1, "bbox": [82, 698.6882934570312, 290, 766.0307006835938], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 14, "type": "text", "text": "The remainder of the paper is organized as fol- lows. In section 2, we discuss related work in building end-to-end task-oriented dialogue sys- tems. In section 3, we describe the proposed model and learning method in detail. In Section 4, we describe the experiment setup and discuss the results. Section 5 gives the conclusions. ", "page_idx": 1, "bbox": [307, 63.68628692626953, 525, 158.12680053710938], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 15, "type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1, "bbox": [307, 171, 397, 185], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 16, "type": "text", "text": "Popular approaches in learning task-oriented dialogue include modeling the task as a par- tially observable Markov Decision Process (POMDP) ( Young et al. ,  2013 ). RL can be applied in the POMDP framework to learn dialogue policy online by interacting with users ( Gaˇ si´ et al. ,  2013 ). The dialogue state and system action space have to be carefully designed in order to make the policy learning tractable ( Young et al. , 2013 ), which limits the model’s usage to restricted domains. ", "page_idx": 1, "bbox": [307, 195.4273223876953, 525, 344.0638732910156], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 17, "type": "text", "text": "Recent efforts have been made in designing end-to-end solutions for task-oriented dialogues, inspired by the success of encoder-decoder based neural network models in non-task-oriented con- versational systems ( Serban et al. ,  2015 ;  Li et al. , 2016 ). Wen et al. ( Wen et al. ,  2017 ) designed an end-to-end trainable neural dialogue model with modularly connected system components. This system is a supervised learning model which is evaluated on ﬁxed dialogue corpora. It is un- known how well the model performance gener- alizes to unseen dialogue state during user inter- actions. Our system is trained by a combina- tion of supervised and deep RL methods, as it is shown that RL may effectively improve dialogue success rate by exploring a large dialogue action space ( Henderson et al. ,  2008 ;  Li et al. ,  2017 ). ", "page_idx": 1, "bbox": [307, 345.4383850097656, 525, 575.3709716796875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 18, "type": "text", "text": "Bordes and Weston ( 2017 ) proposed a task- oriented dialogue model using end-to-end memory networks. In the same line of research, people ex- plored using query-regression networks ( Seo et al. , 2016 ), gated memory networks ( Liu and Perez , 2017 ), and copy-augmented networks ( Eric and Manning ,  2017 ) to learn the dialogue state. These systems directly select a ﬁnal response from a list of response candidates conditioning on the dia- logue history without doing slot ﬁlling or user goal tracking. Our model, on the other hand, explic- itly tracks user’s goal for effective integration with knowledge bases (KBs). Robust dialogue state tracking has been shown ( Jurˇ c´ ıˇ cek et al. ,  2012 ) to be critical in improving dialogue success in task completion. ", "page_idx": 1, "bbox": [307, 576.7454833984375, 525, 766.0299072265625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 19, "type": "text", "text": "", "page_idx": 2, "bbox": [72, 63.68604278564453, 290, 90.38150024414062], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 20, "type": "text", "text": "Dhingra et al. ( 2017 ) proposed an end-to-end RL dialogue agent for information access. Their model focuses on bringing different i ability to the KB query operation by introducing a “soft” re- trieval process in selecting the KB entries. Such soft-KB lookup is prone to entity updates and ad- ditions in the KB, which is common in real world information systems. In our model, we use sym- bolic queries and leave the selection of KB enti- ties to external services (e.g. a recommender sys- tem), as entity ranking in real world systems can be made with much richer features (e.g. user pro- ﬁles, location and time context, etc.). Quality of the generated symbolic query is directly related to the belief tracking performance. In our pro- posed end-to-end system, belief tracking can be optimized together with other system components (e.g. language understanding and policy) during interactive learning with users. ", "page_idx": 2, "bbox": [72, 91.75501251220703, 290, 348.7865905761719], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 21, "type": "text", "text": "Williams et al. ( 2017 ) proposed a hybrid code network for task-oriented dialogue that can be trained with supervised and reinforcement learn- ing. They show that RL performed with a super- vised pre-training model using labeled dialogues improves learning speed dramatically. They did not discuss the potential issue of dialogue state distribution mismatch between supervised pre- training and RL interactive learning, which is ad- dressed in our dialogue learning framework. ", "page_idx": 2, "bbox": [72, 350.1601257324219, 290, 485.2486572265625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 22, "type": "text", "text": "3 Proposed Method ", "text_level": 1, "page_idx": 2, "bbox": [71, 499, 181, 512], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 23, "type": "text", "text": "Figure  1  shows the overall system architecture of the proposed end-to-end task-oriented dialogue model. We use a hierarchical LSTM neural net- work to encode a dialogue with a sequence of turns. User input to the system in natural lan- guage format is encoded to a continuous vector via a bidirectional LSTM utterance encoder. This user utterance encoding, together with the encoding of the previous system action, serves as the input to a dialogue-level LSTM. State of this dialogue-level LSTM maintains a continuous representation of the dialogue state. Based on this state, the model generates a probability distribution over candidate values for each of the tracked goal slots. A query command can then be formulated with the state tracking outputs and issued to a knowledge base to retrieve requested information. Finally, the system produces a dialogue action, which is conditioned on information from the dialogue state, the esti- mated user’s goal, and the encoding of the query results . This dialogue action, together with the user goal tracking results and the query results, is used to generate the ﬁnal natural language system response via a natural language generator (NLG). We describe each core model component in detail in the following sections. ", "page_idx": 2, "bbox": [72, 522.5481567382812, 290, 766.0306396484375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 24, "type": "text", "text": "", "page_idx": 2, "bbox": [306, 63.68622589111328, 526, 171.67672729492188], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 25, "type": "text", "text": "3.1 Utterance Encoding ", "text_level": 1, "page_idx": 2, "bbox": [307, 180, 426, 193], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 26, "type": "text", "text": "We use a bidirectional LSTM to encode the user utterance to a continuous representation. We refer to this LSTM as the utterance-level LSTM. The user utterance vector is generated by concatenat- ing the last forward and backward LSTM states. Let    $\\mathbf{U}_{k}=\\left(w_{1},w_{2},...,w_{T_{k}}\\right)$   be the user utterance at turn    $k$   with    $T_{k}$   words. These words are ﬁrstly mapped to an embedding space, and further serve as the step inputs to the bidirectional LSTM. Let →  and    $\\hat{h}_{t}$   represent the forward and backward LSTM state outputs at time step    $t$  . The user ut- terance vector    $U_{k}$   is produced by:    $U_{k}=[\\overrightarrow{h_{T_{k}}},\\overleftarrow{h_{1}}]$    , where  $\\overrightarrow{h_{T_{k}}}$   and  $\\overleftarrow{h_{1}}$   are the last states in the forward and backward LSTMs. ", "page_idx": 2, "bbox": [306, 198.25123596191406, 526, 392.42877197265625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 27, "type": "text", "text": "3.2 Dialogue State Tracking ", "text_level": 1, "page_idx": 2, "bbox": [306, 402, 446, 414], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 28, "type": "text", "text": "Dialogue state tracking, or belief tracking, main- tains the state of a conversation, such as user’s goals, by accumulating evidence along the se- quence of dialogue turns. Our model maintains the dialogue state in a continuous form in the dialogue-level LSTM   $(\\mathrm{LSTM_{D}})$  ) state    $s_{k}$  .    $s_{k}$   is up- dated after the model processes each dialogue turn by taking in the encoding of user utterance    $U_{k}$   and the encoding of the previous turn system output  $A_{k-1}$  . This dialogue state serves as the input to the dialogue state tracker. The tracker updates its es- timation of the user’s goal represented by a list of slot-value pairs. A probability distribution    $P(l_{k}^{m})$  is maintained over candidate values for each goal slot type    $m\\in M$  : ", "page_idx": 2, "bbox": [306, 419.0032958984375, 526, 621.8378295898438], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 29, "type": "equation", "text": "\n$$\n\\begin{array}{r l}&{s_{k}=\\mathrm{{LSTM}_{D}}(s_{k-1},\\ [U_{k},\\ A_{k-1}])}\\\\ &{P(l_{k}^{m}\\mid\\mathbf{U}_{\\le k},\\ \\mathbf{A}_{<k})=\\mathrm{{Slat}D i s t}_{m}(s_{k})}\\end{array}\n$$\n ", "text_format": "latex", "page_idx": 2, "bbox": [330, 630, 502, 664], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 30, "type": "text", "text": "where    $\\mathrm{StotDiss}_{m}$   is a single hidden layer MLP with  softmax  activation over slot type    $m\\in M$  . ", "page_idx": 2, "bbox": [306, 672, 526, 699.2117919921875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 31, "type": "text", "text": "3.3 KB Operation ", "text_level": 1, "page_idx": 2, "bbox": [307, 708, 399, 722], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 32, "type": "text", "text": "The dialogue state tracking outputs are used to form an API call command to retrieve information from a knowledge base. The API call command is ", "page_idx": 2, "bbox": [306, 725.786376953125, 526, 766.02978515625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 33, "type": "image", "page_idx": 3, "img_path": "layout_images/N18-1187_0.jpg", "img_caption": "Figure 1: Proposed end-to-end task-oriented dialogue system architecture. ", "bbox": [91, 64, 505, 244], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "System dialogue act\n\nembedding at tur\n\nmikes Tracking\n\nUser: Movie for the day\nafter tomorrow, please\n\nBi-LSTM\nUtterance Encoder\n\nLSTM\nDialogue State\n\nUser utterance Lo\n\nencoding at turn k\n\nSystem dialogue act\nat turn k\n\nSystem: Ok, what\ntime do you prefer?\n\nNatural Language\nGenerator\n\nDialogue State |\n\n| date=Th\n\n'time=none\n\n+— request(time)\n\nPolicy\nNetwork\n\nwursday\n\n4 Knowledge\nBase\n\nQuery results\nencoding\n", "vlm_text": "The image depicts a proposed architecture for an end-to-end task-oriented dialogue system. Here's a breakdown of the components and flow within the system:\n\n1. **User Input**: The dialogue starts with a user input, shown as \"User: Movie for the day after tomorrow, please\".\n\n2. **Bi-LSTM Utterance Encoder**: The user input is processed through a bidirectional Long Short-Term Memory (Bi-LSTM) encoder to generate an encoding of the user's utterance at turn k.\n\n3. **System Dialogue Act Embedding**: The system includes an embedding of the dialogue act performed by the system at the previous turn (k-1).\n\n4. **LSTM Dialogue State**: This component processes the user utterance encoding along with the previous system dialogue act embedding to update the dialogue state.\n\n5. **Dialogue State Tracking**: The updated dialogue state is tracked, identifying slots or variables like \"date\" which is set to \"Thursday\", and \"time\" which is set to \"none\".\n\n6. **Knowledge Base**: The tracked dialogue state can query an external knowledge base to retrieve relevant information, resulting in a query result encoding.\n\n7. **Policy Network**: Based on the dialogue state and query results, the system uses a policy network to determine the next system dialogue act at turn k, which in this case is \"request(time)\".\n\n8. **Natural Language Generator**: This takes the determined system dialogue act and generates a natural language response, shown as \"System: Ok, what time do you prefer?\".\n\nThe system is designed to process user requests and generate relevant responses in a task-oriented manner, utilizing components like encoders, a policy network, and a natural language generator to manage dialogue turns."}
{"layout": 34, "type": "text", "text": "produced by replacing the tokens in a query com- mand template with the best hypothesis for each goal slot from the dialogue state tracking output. Alternatively, an n-best list of API calls can be generated with the most probable candidate values for the tracked goal slots. In interfacing with KBs, instead of using a soft KB lookup as in ( Dhingra et al. ,  2017 ), our model sends symbolic queries to the KB and leaves the ranking of the KB entities to an external recommender system. Entity rank- ing in real world systems can be made with much richer features (e.g. user proﬁles, local context, etc.) in the back-end system other than just fol- lowing entity posterior probabilities conditioning on a user utterance. Hence ranking of the KB en- tities is not a part of our proposed neural dialogue model. In this work, we assume that the model re- ceives a ranked list of KB entities according to the issued query and other available sources, such as user models. ", "page_idx": 3, "bbox": [72, 263.64105224609375, 290, 534.2216186523438], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 35, "type": "text", "text": "Once the KB query results are returned, we save the retrieved entities to a queue and encode the re- sult summary to a vector. Rather then encoding the real KB entity values as in ( Bordes and Weston , 2017 ;  Eric and Manning ,  2017 ), we only encode a summary of the query results (i.e. item availabil- ity and number of matched items). This encoding serves as a part of the input to the policy network. ", "page_idx": 3, "bbox": [72, 534.9751586914062, 290, 642.964599609375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 36, "type": "text", "text": "3.4 Dialogue Policy ", "text_level": 1, "page_idx": 3, "bbox": [71, 654, 168, 666], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 37, "type": "text", "text": "A dialogue policy selects the next system action in response to the user’s input based on the cur- rent dialogue state. We use a deep neural network to model the dialogue policy. There are three in- puts to the policy network, (1) the dialogue-level LSTM state    $s_{k}$  , (2) the log probabilities of candi- date values from the belief tracker  $v_{k}$  , and (3) the ", "page_idx": 3, "bbox": [72, 671.5891723632812, 290, 766.0305786132812], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 38, "type": "image", "page_idx": 3, "img_path": "layout_images/N18-1187_1.jpg", "img_caption": "Figure 2: Dialogue state and policy network. ", "bbox": [318, 265, 512, 408], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "System action\nat turn k\n\nPolicy Network\n\nQuery results\nencoding\n\nLSTM Dialogue State, Sk\n\n", "vlm_text": "The image depicts a high-level architecture of a dialogue state and policy network used in a dialogue system. The key components of the diagram are:\n\n1. **LSTM Dialogue State (`s_k`)**: At the bottom of the diagram, an LSTM (Long Short-Term Memory) network is used to manage and update the dialogue state, denoted as `s_k`.\n\n2. **Query Results Encoding (`E_k`)**: This component encodes the results of queries, which may be used to inform the policy network. It's shown on the right and feeds into the policy network.\n\n3. **Slot Value Logits (`v_k`)**: This component generates logits for slot values, which are inputs into the policy network. It forms a connection between the LSTM dialogue state and the policy network.\n\n4. **Policy Network**: Consists of a series of processing layers that take inputs from both the slot value logits and the LSTM dialogue state, aiming to produce a suitable system action at a given turn, labeled as `a_k`.\n\n5. **System Action at Turn (`a_k`)**: The output of the policy network is the action taken by the system at a specific turn in the dialogue, which is shown at the top of the diagram.\n\nOverall, the design represents a framework for selecting system actions in a dialogue based on the current state and relevant inputs."}
{"layout": 39, "type": "text", "text": "encoding of the query results summary    $E_{k}$  . The policy network emits a system action in the form of a dialogue act conditioning on these inputs: ", "page_idx": 3, "bbox": [307, 426, 525, 466.4225158691406], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 40, "type": "equation", "text": "\n$$\nP(a_{k}\\mid U_{\\leq k},\\;A_{<k},\\;E_{\\leq k})=\\mathrm{PoisyNet}(s_{k},v_{k},E_{k})\n$$\n ", "text_format": "latex", "page_idx": 3, "bbox": [305, 474, 532, 490], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 41, "type": "text", "text": "where    $v_{k}$   represents the concatenated log probabil- ities of candidate values for each goal slot,    $E_{k}$   is the encoding of query results, and  PolicyNet  is a single hidden layer MLP with  softmax  activation function over all system actions. ", "page_idx": 3, "bbox": [307, 510.1650390625, 525, 577.5075073242188], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 42, "type": "text", "text": "The emitted system action is ﬁnally used to pro- duce a system response in natural language format by combining the state tracker outputs and the re- trieved KB entities. We use a template based NLG in this work. The delexicalised tokens in the NLG template are replaced by the values from either the estimated user goal values or the KB entities, de- pending on the emitted system action. ", "page_idx": 3, "bbox": [307, 577.9110717773438, 525, 685.9005126953125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 43, "type": "text", "text": "3.5 Supervised Pre-training ", "text_level": 1, "page_idx": 3, "bbox": [306, 695, 444, 708], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 44, "type": "text", "text": "By connecting all the system components, we have an end-to-end model for task-oriented dialogue. Each system component is a neural network that takes in underlying system component’s outputs in a continuous form that is fully differentiable, and the entire system (utterance encoding, dia- logue state tracking, and policy network) can be trained end-to-end. ", "page_idx": 3, "bbox": [307, 712.2369995117188, 525, 766.0304565429688], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 45, "type": "text", "text": "", "page_idx": 4, "bbox": [71, 63.68604278564453, 291, 117.47952270507812], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 46, "type": "text", "text": "We ﬁrst train the system in a supervised man- ner by ﬁtting task-oriented dialogue samples. The model predicts the true user goal slot values and the next system action at each turn of a dia- logue. We optimize the model parameter set  $\\theta$   by minimizing a linear interpolation of cross-entropy losses for dialogue state tracking and system ac- tion prediction: ", "page_idx": 4, "bbox": [71, 117.94202423095703, 291, 225.93252563476562], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 47, "type": "equation", "text": "\n$$\n\\begin{array}{r l r}{\\lefteqn{\\operatorname*{min}_{\\theta}\\sum_{k=1}^{K}-\\Big[\\sum_{m=1}^{M}\\lambda_{l^{m}}\\log P(l_{k}^{m*}|\\mathbf{U}_{\\le k},\\mathbf{A}_{<k},\\mathbf{E}_{<k};\\theta)}}\\\\ &{}&{\\quad\\quad+\\lambda_{a}\\log P(a_{k}^{*}|\\mathbf{U}_{\\le k},\\mathbf{A}_{<k},\\mathbf{E}_{\\le k};\\theta)\\,\\Big]\\quad}\\end{array}\n$$\n ", "text_format": "latex", "page_idx": 4, "bbox": [69, 233, 298, 296], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 48, "type": "text", "text": "where    $\\lambda\\mathbf{s}$   are the linear interpolation weights for the cost of each system output.    $l_{k}^{m*}$  is the ground th labe  for the tracked user goal slot type    $m\\in$   $M$   at the  k th turn, and  $a_{k}^{*}$    is the true system action in the corpus. ", "page_idx": 4, "bbox": [71, 319.7740478515625, 291, 387.11651611328125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 49, "type": "text", "text": "3.6 Imitation Learning with Human Teaching ", "text_level": 1, "page_idx": 4, "bbox": [71, 396, 248, 423], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 50, "type": "text", "text": "Once obtaining a supervised training dialogue agent, we further let the agent to learn interactively from users by conducting task-oriented dialogues. Supervised learning succeeds when training and test data distributions match. During the agent’s interaction with users, any mistake made by the agent or any deviation in the user’s behavior may lead to a different dialogue state distribution than the one that the supervised learning agent saw dur- ing ofﬂine training. A small mistake made by the agent due to this covariate shift ( Ross and Bagnell , 2010 ;  Ross et al. ,  2011 ) may lead to compound- ing errors which ﬁnally lead to failure of a task. To address this issue, we propose a dialogue imi- tation learning method which allows the dialogue agent to learn from human teaching. We let the supervised training agent to interact with users us- ing its learned dialogue policy  $\\pi_{\\boldsymbol{\\theta}}(a|s)$  . With this, we collect additional dialogue samples that are guided by the agent’s own policy, rather than by the expert policy as those in the supervised train- ing corpora. When the agent make mistakes, we ask users to correct the mistakes and demonstrate the expected actions and predictions for the agent to make. Such user teaching precisely addresses Algorithm 1  Dialogue Learning with Human Teaching and Feedback ", "page_idx": 4, "bbox": [71, 427.70404052734375, 291, 766.030517578125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 51, "type": "text", "text": "", "page_idx": 4, "bbox": [307, 62.64030075073242, 526, 89.72750854492188], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 52, "type": "text", "text": "1:  Train model end-to-end on dialogue samples  $D$   with MLE and obt  $\\pi_{\\theta}(a|s)\\vartriangleright{\\bf e q}\\,4$  2:  for  learning iteration  k  $k=1:K$   do 3: Run    $\\pi_{\\theta}(a|s)$   with user to collect new dialogue samples    $D_{\\pi}$  4: Ask user to correct the mistakes in the tracked user’s goal for each dialogue turn in    $D_{\\pi}$  5: Add the newly labeled dialogue samples to the existing corpora:    $D\\leftarrow D\\cup D_{\\pi}$  6: Train model end-to-end on    $D$   and obtain an updated policy    $\\pi_{\\boldsymbol{\\theta}}(a|s)$   $\\triangleright\\mathrm{eq}\\,4$  7:  end for 8:  for  learning iteration    $k=1:N$   do 9: Run    $\\pi_{\\boldsymbol{\\theta}}(a|s)$   with user for a new dialogue 10: Collect user feedback as reward    $r$  11: Update model end-to-end and obtain an updated policy  $\\pi_{\\theta}(a|s)$  ▷ eq 5 ", "page_idx": 4, "bbox": [307, 90.76801300048828, 526, 334.2497253417969], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 53, "type": "text", "text": "12:  end for ", "text_level": 1, "page_idx": 4, "bbox": [309, 338.25, 360, 347], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 54, "type": "text", "text": "the limitations of the currently learned dialogue model, as these newly collected dialogue samples are driven by the agent’s own policy. Speciﬁcally, in this study we let an expert user to correct the mistake made by the agent in tracking the user’s goal at the end of each dialogue turn. This new batch of annotated dialogues are then added to the existing training corpus. We start the next round of supervised model training on this aggregated corpus to obtain an updated dialogue policy, and continue this dialogue imitation learning cycles. ", "page_idx": 4, "bbox": [307, 371.34503173828125, 526, 519.9825439453125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 55, "type": "text", "text": "3.7 Reinforcement Learning with Human Feedback ", "text_level": 1, "page_idx": 4, "bbox": [307, 531, 508, 557], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 56, "type": "text", "text": "Learning from human teaching can be costly, as it requires expert users to provide corrections at each dialogue turn. We want to minimize the num- ber of such imitation dialogue learning cycles and continue to improve the agent via a form of super- vision signal that is easier to obtain. After the imi- tation learning stage, we further optimize the neu- ral dialogue system with RL by letting the agent to interact with users and learn from user feed- back. Different from the turn-level corrections in the imitation dialogue learning stage, the feedback is only collected at the end of a dialogue. A pos- itive reward is collected for successful tasks, and a zero reward is collected for failed tasks. A step penalty is applied to each dialogue turn to encour- age the agent to complete the task in fewer steps. In this work, we only use task-completion as the metric in designing the dialogue reward. One can extend it by introducing additional factors to the reward functions, such as naturalness of interac- tions or costs associated with KB queries. ", "page_idx": 4, "bbox": [307, 563.1961059570312, 526, 766.030517578125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 57, "type": "text", "text": "", "page_idx": 5, "bbox": [71, 63.68604278564453, 290, 144.57754516601562], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 58, "type": "text", "text": "To encourage the agent to explore the dialogue action space, we let the agent to follow a softmax policy during RL training by sampling system ac- tions from the policy network outputs. We apply REINFORCE algorithm ( Williams ,  1992 ) in op- timizing the network parameters. The objective function can be written as    $J_{k}(\\theta)\\;=\\;\\mathbb{E}_{\\theta}\\left[R_{k}\\right]\\;=$   $\\mathbb{E}_{\\theta}\\left[\\sum_{t=0}^{K-k}\\gamma^{t}r_{k+t}\\right]$  hP i , with  $\\gamma\\in[0,1)$   being the dis- count factor. With likelihood ratio gradient esti- mator, the gradient of the objective function can be derived as: ", "page_idx": 5, "bbox": [71, 145.05006408691406, 290, 298.3854675292969], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 59, "type": "equation", "text": "\n$$\n\\begin{array}{r l}&{\\nabla_{\\theta}J_{k}(\\theta)=\\nabla_{\\theta}\\mathbb{E}_{\\theta}\\left[R_{k}\\right]}\\\\ &{\\qquad\\qquad=\\displaystyle\\sum_{a_{k}}\\pi_{\\theta}(a_{k}|s_{k})\\nabla_{\\theta}\\log\\pi_{\\theta}(a_{k}|s_{k})R_{k}}\\\\ &{\\qquad\\qquad=\\mathbb{E}_{\\theta}\\left[\\nabla_{\\theta}\\log\\pi_{\\theta}(a_{k}|s_{k})R_{k}\\right]}\\end{array}\n$$\n ", "text_format": "latex", "page_idx": 5, "bbox": [77, 304, 284, 371], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 60, "type": "text", "text": "This last expression above gives us an unbiased gradient estimator. ", "page_idx": 5, "bbox": [71, 393.26605224609375, 290, 419.9615173339844], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 61, "type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5, "bbox": [71, 430, 155, 444], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 62, "type": "text", "text": "4.1 Datasets ", "text_level": 1, "page_idx": 5, "bbox": [71, 452, 136, 464], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 63, "type": "text", "text": "We evaluate the proposed method on DSTC2 ( Henderson et al. ,  2014a ) dataset in restaurant search domain and an internally collected dialogue corpus 1   in movie booking domain. The movie booking dialogue corpus has an average number of 8.4 turns per dialogue. Its training set has 100K di- alogues, and the development set and test set each has 10K dialogues. ", "page_idx": 5, "bbox": [71, 469.51605224609375, 290, 577.5065307617188], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 64, "type": "text", "text": "The movie booking dialogue corpus is gener- ated ( Shah et al. ,  2018 ) using a ﬁnite state ma- chine based dialogue agent and an agenda based user simulator ( Schatzmann et al. ,  2007 ) with nat- ural language utterances rewritten by real users. The user simulator can be conﬁgured with differ- ent personalities, showing various levels of ran- domness and cooperativeness. This user simula- tor is also used to interact with our end-to-end training agent during imitation and reinforcement learning stages. We randomly select a user proﬁle when conducting each dialogue simulation. Dur- ing model evaluation, we use an extended set of natural language surface forms over the ones used during training time to evaluate the generalization capability of the proposed end-to-end model in handling diverse natural language inputs. ", "page_idx": 5, "bbox": [71, 577.9790649414062, 290, 726.615478515625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 65, "type": "text", "text": "", "page_idx": 5, "bbox": [307, 63.68604278564453, 525, 144.57754516601562], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 66, "type": "text", "text": "4.2 Training Settings ", "text_level": 1, "page_idx": 5, "bbox": [307, 154, 412, 167], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 67, "type": "text", "text": "The size of the dialogue-level and utterance-level LSTM state is set as 200 and 150 respectively. Word embedding size is 300. Embedding size for system action and slot values is set as 32. Hidden layer size of the policy network is set as 100. We use Adam optimization method ( Kingma and Ba , 2014 ) with initial learning rate of 1e-3. Dropout rate of 0.5 is applied during supervised training to prevent the model from over-ﬁtting. ", "page_idx": 5, "bbox": [307, 171.17506408691406, 525, 292.7135925292969], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 68, "type": "text", "text": "In imitation learning, we perform mini-batch model update after collecting every 25 dialogues. System actions are sampled from the learned pol- icy to encourage exploration. The system action is deﬁned with the act and slot types from a dia- logue act ( Henderson et al. ,  2013 ). For example, the dialogue act “  $\\ \\cdot c o n f i r m(d a t e=m o n d a y)\"$   is mapped to a system action “ confirm date ” and a candidate value “ monday ” for slot type “ date ”. The slot types and values are from the dialogue state tracking output. ", "page_idx": 5, "bbox": [307, 293.11712646484375, 525, 441.7545471191406], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 69, "type": "text", "text": "In RL optimization, we update the model with every mini-batch of 25 samples. Dialogue is con- sidered successful based on two conditions: (1) the goal slot values estimated from dialogue state tracking fully match to the user’s true goal values, and (2) the system is able to conﬁrm with the user the tracked goal values and offer an entity which is ﬁnally accepted by the user. Maximum allowed number of dialogue turn is set as 15. A positive reward of   $+15.0$   is given at the end of a success- ful dialogue, and a zero reward is given to a failed case. We apply a step penalty of -1.0 for each turn to encourage shorter dialogue for task completion. ", "page_idx": 5, "bbox": [307, 442.1590576171875, 525, 617.8945922851562], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 70, "type": "text", "text": "4.3 Supervised Learning Results ", "text_level": 1, "page_idx": 5, "bbox": [306, 628, 466, 640], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 71, "type": "text", "text": "Table  4.3  and Table  4.3  show the supervised learn- ing model performance on DSTC2 and the movie booking corpus. Evaluation is made on DST accu- racy. For the evaluation on DSTC2 corpus, we use the live ASR transcriptions as the user input utter- ances. Our proposed model achieves near state-of- the-art dialogue state tracking results on DSTC2 corpus, on both individual slot tracking and joint slot tracking, comparing to the recent published results using RNN ( Henderson et al. ,  2014b ) and neural belief tracker (NBT) ( Mrkˇ si´ c et al. ,  2016 ). In the movie booking domain, our model also achieves promising performance on both individ- ual slot tracking and joint slot tracking accuracy. Instead of using ASR hypothesis as model input as in DSTC2, here we use text based input which has much lower noise level in the evaluation of the movie booking tasks. This partially explains the higher DST accuracy in the movie booking do- main comparing to DSTC2. ", "page_idx": 5, "bbox": [307, 644.4911499023438, 525, 766.0305786132812], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 72, "type": "text", "text": "", "page_idx": 6, "bbox": [72, 63.68604278564453, 290, 212.32357788085938], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 73, "type": "table", "page_idx": 6, "img_path": "layout_images/N18-1187_2.jpg", "table_caption": "Table 1: Dialogue state tracking results on DSTC2 ", "bbox": [72, 220, 295, 312], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "Model Area Food Price Joint\n\nRNN 92 86 86 69\nRNN+sem. dict 92 86 92 71\nNBT 90 84 94 72\n\nOur SL model 90 84 92 72\n\n", "vlm_text": "The table presents the performance of different models (RNN, RNN+sem. dict, NBT, and Our SL model) across four categories: Area, Food, Price, and Joint. The values in the table appear to be percentages representing the accuracy or performance score of each model in the respective category.\n\n- RNN achieves scores of 92 for Area, 86 for Food, 86 for Price, and 69 for Joint.\n- RNN with a semantic dictionary (RNN+sem. dict) scores 92 for Area, 86 for Food, 92 for Price, and 71 for Joint.\n- NBT scores 90 for Area, 84 for Food, 94 for Price, and 72 for Joint.\n- Our SL model scores 90 for Area, 84 for Food, 92 for Price, and 72 for Joint. \n\nThese performance results highlight variances in each model's effectiveness, particularly notable in the Joint category, where scores range from 69 to 72."}
{"layout": 74, "type": "table", "page_idx": 6, "img_path": "layout_images/N18-1187_3.jpg", "table_footnote": "Table 2: DST results on movie booking dataset ", "bbox": [86, 332, 276, 445], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "Goal slot Accuracy\nNum of Tickets 98.22\nMovie 91.86\nTheater Name ‘97.33\nDate 99.31\nTime 97.71\nJoint 84.57\n", "vlm_text": "The table provides the accuracy rates for recognizing different goal slots in a task or application related to booking or scheduling. Here is the breakdown:\n\n- Num of Tickets: 98.22% accuracy\n- Movie: 91.86% accuracy\n- Theater Name: 97.33% accuracy\n- Date: 99.31% accuracy\n- Time: 97.71% accuracy\n- Joint accuracy (overall or combined accuracy for recognizing all slots together): 84.57% \n\nThis suggests the model or system performs very well in individual slot recognition, particularly for Date, but has a lower accuracy when considering all slots simultaneously."}
{"layout": 75, "type": "text", "text": "4.4 Imitation and RL Results ", "text_level": 1, "page_idx": 6, "bbox": [71, 465, 216, 477], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 76, "type": "text", "text": "Evaluations of interactive learning with imitation and reinforcement learning are made on metrics of (1) task success rate, (2) dialogue turn size, and (3) DST accuracy. Figures  3 ,  4 , and  5  show the learning curves for the three evaluation metrics. In addition, we compare model performance on task success rate using two different RL training settings, the end-to-end training and the policy- only training, to show the advantages of perform- ing end-to-end system optimization with RL. ", "page_idx": 6, "bbox": [72, 481.9010009765625, 290, 616.989501953125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 77, "type": "text", "text": "Task Success Rate As shown in the learning curves in Figure  3 , the SL model performs poorly. This might largely due to the compounding er- rors caused by the mismatch of dialogue state dis- tribution between ofﬂine training and interactive learning. We use an extended set of user NLG templates during interactive evaluation. Many of the test NLG templates are not seen by the super- vised training agent. Any mistake made by the agent in understanding the user’s request may lead to compounding errors in the following dialogue ", "page_idx": 6, "bbox": [72, 617.0003051757812, 290, 766.0304565429688], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 78, "type": "image", "page_idx": 6, "img_path": "layout_images/N18-1187_4.jpg", "img_caption": "Figure 3: Interactive learning curves on task success rate. ", "bbox": [306, 64, 527, 245], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "Task Success Rate\n\nTask Success Rate over Time (smoothed)\n\n» SL Baseline\n—@ SL+RL\n\nHD KK DEKE DEREK Gar SL +IL500 +. RL\n—k- SL +1L 1000 + RL\n\n0 2000 4000 6000 8000 10000\nInteractive Dialogue Learning Sessions\n", "vlm_text": "The image is a line graph depicting the task success rate over time across different interactive dialogue learning sessions. The title of the graph is \"Task Success Rate over Time (smoothed).\" The x-axis represents interactive dialogue learning sessions ranging from 0 to 10,000, and the y-axis represents the task success rate ranging from 0.3 to 0.7.\n\nThere are four different learning methods plotted on the graph:\n\n1. **SL Baseline**: Represented by light blue 'x' marks, this line remains constant at a task success rate of around 0.3.\n2. **SL + RL**: Represented by red pentagon-shaped points, this line increases steadily and reaches a task success rate of slightly above 0.5 after 10,000 sessions.\n3. **SL + IL 500 + RL**: Represented by yellow triangle-shaped points, this line starts similarly to the SL + RL line but increases more sharply, reaching a success rate between 0.55 and 0.6.\n4. **SL + IL 1000 + RL**: Represented by blue star-shaped points, this line quickly achieves high success rates, stabilizing around 0.65.\n\nThe graph includes dashed vertical lines and circles highlighting the points where the task success rates for \"SL + IL 500 + RL\" and \"SL + IL 1000 + RL\" initially surpass that of \"SL + RL.\" An inset legend helps differentiate between the lines' representations."}
{"layout": 79, "type": "text", "text": "turns, which cause ﬁnal task failure. The red curve  $\\left(\\mathrm{SL}\\ +\\ \\mathrm{RL}\\right)$   shows the performance of the model that has RL applied on the supervised pre-training model. We can see that interactive learning with RL using a weak form of supervision from user feedback continuously improves the task success rate with the growing number of user interactions. We further conduct experiments in learning dia- logue model from scratch using only RL (i.e. with- out supervised pre-training), and the task success rate remains at a very low level after 10K dialogue simulations. We believe that it is because the di- alogue state space is too complex for the agent to learn from scratch, as it has to learn a good NLU model in combination with a good policy to complete the task. The yellow curve   $(\\tt S L\\mathrm{~\\pm~}+\\mathrm{~\\pm~}\\mathrm{LL}$   $500\\ \\mathrm{~+~}\\ \\mathrm{RL}$  ) shows the performance of the model that has 500 episodes of imitation learning over the SL model and continues with RL optimization. It is clear from the results that applying imitation learning on supervised training model efﬁciently improves task success rate. RL optimization af- ter imitation learning increases the task success rate further. The blue curve   $\\left(\\mathrm{SL}\\ \\ +\\ \\ \\mathbb{L}\\ \\ 10\\,0\\,0\\ \\ +}$  RL ) shows the performance of the model that has 1000 episodes of imitation learning over the SL model and continues with RL. Similarly, it shows hints that imitation learning may effectively adapt the supervised training model to the dialogue state distribution during user interactions. ", "page_idx": 6, "bbox": [306, 264.59002685546875, 526, 670.66162109375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 80, "type": "text", "text": "Average Dialogue Turn Size Figure  4  shows the curves for the average turn size of successful dialogues. We observe decreasing number of dia- logue turns in completing a task along the grow- ing number of interactive learning sessions. This shows that the dialogue agent learns better strate- gies in successfully completing the task with fewer ", "page_idx": 6, "bbox": [306, 671.1974487304688, 526, 766.0305786132812], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 81, "type": "image", "page_idx": 7, "img_path": "layout_images/N18-1187_5.jpg", "img_caption": "Figure 4: Interactive learning curves on average dia- logue turn size. ", "bbox": [71, 64, 291, 246], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "Average Turn Size\na be Pol 90\nuw fo} uw ro}\n\na\n°o\n\nAverage Turn Size over Time (smoothed)\n\nLo DEDEDE DEDEDE DE HEHE DED\n\nSL Baseline\n—@ SL+RL\nhe SL+IL500 + RL\n—ke SL+1L 1000 + RL\n0 2000 4000 6000 8000 10000\n\nInteractive Dialogue Learning Sessions\n", "vlm_text": "The image is a graph showing interactive learning curves related to dialogue systems, specifically depicting how the average dialogue turn size changes over time, across various training sessions. The x-axis represents the number of interactive dialogue learning sessions (up to 10,000), and the y-axis represents the average turn size (ranging from 6.0 to 9.0).\n\nThere are four different lines on the graph, each representing a different method or combination of methods for training dialogue systems:\n1. The light blue 'x' markers (SL Baseline): Representing the supervised learning baseline.\n2. The red diamond markers (SL + RL): Representing a combination of supervised learning with reinforcement learning.\n3. The yellow triangle markers (SL + IL 500 + RL): Representing a combination of supervised learning, interactive learning (500 sessions), and reinforcement learning.\n4. The blue star markers (SL + IL 1000 + RL): Representing a combination of supervised learning, interactive learning (1000 sessions), and reinforcement learning.\n\nThe graph demonstrates how each method impacts the dialogue turn size over the course of the learning sessions. The average turn size starts above 8.0 for most methods and trends downward with the progression of learning sessions, with the SL + RL line showing the most significant decrease."}
{"layout": 82, "type": "text", "text": "number of dialogue turns. The red curve with RL applied directly after supervised pre-training model gives the lowest average number of turns at the end of the interactive learning cycles, com- paring to models with imitation dialogue learn- ing. This seems to be contrary to our observa- tion in Figure  3  that imitation learning with hu- man teaching helps in achieving higher task suc- cess rate. By looking into the generated dialogues, we ﬁnd that the  SL   $+$   RL  model can handle easy tasks well but fails to complete more challenging tasks. Such easy tasks typically can be handled with fewer number of turns, which result in the low average turn size for the    $S\\mathbb{L}\\;\\;+\\;\\;\\mathbb{R}\\mathbb{L}$   model. On the other hand, the imitation plus RL models attempt to learn better strategies to handle those more challenging tasks, resulting in higher task success rates and also slightly increased dialogue length comparing to  $S\\mathbb{L}\\;\\;+\\;\\;\\mathbb{R}\\mathbb{L}$   model. ", "page_idx": 7, "bbox": [72, 274.51702880859375, 290, 531.548583984375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 83, "type": "text", "text": "Dialogue State Tracking Accuracy Similar to the results on task success rate, we see that im- itation learning with human teaching quickly im- proves dialogue state tracking accuracy in just a few hundred interactive learning sessions. The joint slots tracking accuracy in the evaluation of SL model using ﬁxed corpus is   $84.57\\%$   as in Table 4.3 . The accuracy drops to  $50.51\\%$   in the interac- tive evaluation with the introduction of new NLG templates. Imitation learning with human teach- ing effectively adapts the neural dialogue model to the new user input and dialogue state distributions, improving the DST accuracy to  $67.47\\%$   after only 500 imitation dialogue learning sessions. Another encouraging observation is that RL on top of SL model and IL model not only improves task suc- cess rate by optimizing dialogue policy, but also ", "page_idx": 7, "bbox": [72, 535.7054443359375, 290, 766.0305786132812], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 84, "type": "image", "page_idx": 7, "img_path": "layout_images/N18-1187_6.jpg", "img_caption": "Figure 5: Interactive learning curves on dialogue state tracking accuracy. ", "bbox": [306, 64, 527, 242], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "eo oe ee ge\nul a fon) el. ~ foe]\nan oOo aM oS uM oso\n\nAverage DST Accuracy\n\n=\nu\nfo}\n\nSL Baseline\n—@ SL+RL\n“de SL+ IL 500 + RL\n\nKHER EA mofo SEF IL 1000 + RL\n\n0\n\n2000 4000 6000 8000 10000\nInteractive Dialogue Learning Sessions\n", "vlm_text": "The image is a line graph depicting the interactive learning curves of dialogue state tracking (DST) accuracy over time. The x-axis represents the number of interactive dialogue learning sessions, ranging from 0 to 10,000. The y-axis shows the average DST accuracy, ranging from 0.50 to 0.80. \n\nThere are four different lines on the graph, each representing a different approach:\n\n1. **SL Baseline** (cyan x's): This line maintains a constant accuracy of about 0.50 throughout the sessions. It is a baseline using supervised learning only.\n\n2. **SL + RL** (red hexagons): This line starts at about 0.50 and shows a gradual increase in accuracy, plateauing around 0.65. It represents a combination of supervised learning and reinforcement learning.\n\n3. **SL + IL 500 + RL** (yellow triangles): This line also starts at about 0.50 but rises more quickly to around 0.72, where it plateaus. It indicates the use of supervised learning, 500 interactive learning steps, and reinforcement learning.\n\n4. **SL + IL 1000 + RL** (blue stars): This line starts similarly, rising steeply to about 0.75, where it plateaus. It represents supervised learning, 1000 interactive learning steps, and reinforcement learning.\n\nThe graph highlights two specific points with large circles: one at around 200 for the blue stars line and another around 2500 for the yellow triangles line. These circles likely highlight significant improvement points or benchmarks of interest in terms of accuracy and interactivity. The chart also mentions that the accuracy data is smoothed over time."}
{"layout": 85, "type": "image", "page_idx": 7, "img_path": "layout_images/N18-1187_7.jpg", "img_caption": "Figure 6: Interactive learning curves on task success rate with different RL training settings. ", "bbox": [306, 257, 527, 441], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "0.7\n\n0.6\n\n0.5\n\nTask Success Rate\n\n0.4\n\n0.3\n\nTask Success Rate over Time (smoothed)\n\na talial\n\nKe RIK\n\nattteet®\n\n~~ae@e\nSL Baseline\n\nwe\n« @ @- SL + policy-only RL\neo? ~@- SL + end-to-end RL\nMMMM MEMEO ee SL + IL 1000 + policy-only RL\n—k- SL + IL 1000 + end-to-end RL\n0 2000 4000 6000 8000 10000\nInteractive Dialogue Learning Sessions\n\ne\nef\n", "vlm_text": "The image is a line graph depicting the interactive learning curves for task success rate over time with different reinforcement learning (RL) training settings. The x-axis represents the number of interactive dialogue learning sessions, ranging from 0 to 10,000. The y-axis shows the task success rate, ranging from 0.3 to 0.7.\n\nThe graph includes five different lines, each representing different RL training settings:\n1. **SL Baseline** - represented by a line with x-marks, shows a relatively flat success rate around 0.3.\n2. **SL + policy-only RL** - represented by a line with pentagons, starts near 0.4 and gradually increases to about 0.55.\n3. **SL + end-to-end RL** - represented by a line with squares, follows a similar upward trend as the policy-only RL but starts slightly higher and reaches around 0.58.\n4. **SL + IL 1000 + policy-only RL** - represented by a line with hexagons, starts higher than the standard policy-only RL and increases to around 0.6.\n5. **SL + IL 1000 + end-to-end RL** - represented by a line with stars, starts the highest and reaches above 0.65, demonstrating the most significant improvement in task success rate among the settings.\n\nThe legend in the image clarifies the representation of each training setting with specific markers for easy identification. The line graph visually demonstrates the varying success rates achieved by each method over time, with the interactive learning session sizes on the x-axis contributing to the performance escalation in task success rate on the y-axis."}
{"layout": 86, "type": "text", "text": "further improves dialogue state tracking perfor- mance. This shows the beneﬁts of performing end- to-end optimization of the neural dialogue model with RL during interactive learning. ", "page_idx": 7, "bbox": [307, 466.072021484375, 525, 519.865478515625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 87, "type": "text", "text": "End-to-End RL Optimization  To further show the beneﬁt of performing end-to-end optimization of dialogue agent, we compare models with two different RL training settings, the end-to-end train- ing and the policy-only training. End-to-end RL training is what we applied in previous evaluation sections, in which the gradient propagates from system action output layer all the way back to the natural language user input layer. Policy-only training refers to only updating the policy network parameters during interactive learning with RL, with all the other underlying system parameters ﬁxed. The evaluation results are shown in Fig- ure  6 . From these learning curves, we see clear advantage of performing end-to-end model update in achieving higher dialogue task success rate dur- ing interactive learning comparing to only updat- ing the policy network. ", "page_idx": 7, "bbox": [307, 522.1553344726562, 525, 766.0304565429688], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 88, "type": "text", "text": "4.5 Human User Evaluations ", "text_level": 1, "page_idx": 8, "bbox": [71, 64, 214, 76], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 89, "type": "text", "text": "We further evaluate the proposed method with human judges recruited via Amazon Mechanical Turk. Each judge is asked to read a dialogue be- tween our model and user simulator and rate each system turn on a scale of 1 (frustrating) to 5 (opti- mal way to help the user). Each turn is rated by 3 different judges. We collect and rate 100 dialogues for each of the three models: (i) SL model, (ii) SL model followed by 1000 episodes of IL, (iii) SL and IL followed by RL. Table  3  lists the mean and standard deviation of human scores overall sys- tem turns. Performing interactive learning with imitation and reinforcement learning clearly im- proves the quality of the model according to hu- man judges. ", "page_idx": 8, "bbox": [72, 82.30805206298828, 290, 285.1426086425781], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 90, "type": "table", "page_idx": 8, "img_path": "layout_images/N18-1187_8.jpg", "bbox": [94, 295, 267, 354], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "Model Score\n\nSL 3.987 + 0.086\nSL + IL 1000 4.378 + 0.082\nSL + IL 1000+ RL | 4.603 + 0.067\n\n", "vlm_text": "The table presents a comparison of different models based on their scores. It consists of two columns: \"Model\" and \"Score.\" \n\n- The \"Model\" column lists the models being compared. There are three models:\n  1. SL\n  2. SL + IL 1000\n  3. SL + IL 1000 + RL\n\n- The \"Score\" column lists the scores achieved by each model along with a margin of error or uncertainty. The scores are as follows:\n  1. SL: 3.987 ± 0.086\n  2. SL + IL 1000: 4.378 ± 0.082\n  3. SL + IL 1000 + RL: 4.603 ± 0.067\n\nThese scores likely represent some form of performance measure for each model, where a higher score indicates better performance. The numbers following the ± symbol represent the standard deviation or margin of error for each score."}
{"layout": 91, "type": "text", "text": "Table 3: Human evaluation results. Mean and standard deviation of crowd worker scores (between 1 to 5). ", "page_idx": 8, "bbox": [72, 361.4875793457031, 290, 385.447509765625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 92, "type": "text", "text": "5 Conclusions ", "text_level": 1, "page_idx": 8, "bbox": [71, 405, 152, 418], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 93, "type": "text", "text": "In this work, we focus on training task-oriented dialogue systems through user interactions, where the agent improves through communicating with users and learning from the mistake it makes. We propose a hybrid learning approach for such sys- tems using end-to-end trainable neural network model. We present a hybrid imitation and rein- forcement learning method, where we ﬁrstly train a dialogue agent in a supervised manner by learn- ing from dialogue corpora, and continuously to improve it by learning from user teaching and feedback with imitation and reinforcement learn- ing. We evaluate the proposed learning method with both ofﬂine evaluation on ﬁxed dialogue cor- pora and interactive evaluation with users. Exper- imental results show that the proposed neural dia- logue agent can effectively learn from user teach- ing and improve task success rate with imitation learning. Applying reinforcement learning with user feedback after imitation learning with user teaching improves the model performance further, not only on the dialogue policy but also on the dialogue state tracking in the end-to-end training framework. ", "page_idx": 8, "bbox": [72, 427.7040100097656, 290, 752.4815063476562], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 94, "type": "text", "text": "References ", "text_level": 1, "page_idx": 8, "bbox": [307, 64, 363, 76], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 95, "type": "text", "text": "Antoine Bordes and Jason Weston. 2017. Learning end-to-end goal-oriented dialog. In  International Conference on Learning Representations . ", "page_idx": 8, "bbox": [307, 82.11260986328125, 525, 116.03453826904297], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 96, "type": "text", "text": "Bhuwan Dhingra, Lihong Li, Xiujun Li, Jianfeng Gao, Yun-Nung Chen, Faisal Ahmed, and Li Deng. 2017. Towards end-to-end reinforcement learning of dia- logue agents for information access. In  ACL . ", "page_idx": 8, "bbox": [307, 123.66162109375, 525, 168.5435028076172], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 97, "type": "text", "text": "Mihail Eric and Christopher D Manning. 2017. A copy-augmented sequence-to-sequence architecture gives good performance on task-oriented dialogue. In  EACL . ", "page_idx": 8, "bbox": [307, 176.1695556640625, 525, 221.0514373779297], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 98, "type": "text", "text": "Milica Gaˇ si´ c, Catherine Breslin, Matthew Henderson, Dongho Kim, Martin Szummer, Blaise Thomson, Pirros Tsiakoulis, and Steve Young. 2013. On- line policy optimisation of bayesian spoken dialogue systems via human interaction. In  ICASSP . ", "page_idx": 8, "bbox": [307, 228.62750244140625, 525, 284.51837158203125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 99, "type": "text", "text": "Milica Gasic and Steve Young. 2014. Gaussian pro- cesses for pomdp-based dialogue manager optimiza- tion. IEEE/ACM Transactions on Audio, Speech, and Language Processing  . ", "page_idx": 8, "bbox": [307, 292.1444091796875, 525, 337.02630615234375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 100, "type": "text", "text": "Dilek Hakkani-T¨ ur, G¨ okhan T¨ ur, Asli Celikyilmaz, Yun-Nung Chen, Jianfeng Gao, Li Deng, and Ye- Yi Wang. 2016. Multi-domain joint semantic frame parsing using bi-directional rnn-lstm. In  Inter- speech . ", "page_idx": 8, "bbox": [307, 344.6033630371094, 525, 400.4932861328125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 101, "type": "text", "text": "James Henderson, Oliver Lemon, and Kallirroi Georgila. 2008. Hybrid reinforcement/supervised learning of dialogue policies from ﬁxed data sets. Computational Linguistics  . ", "page_idx": 8, "bbox": [307, 408.1193542480469, 525, 453.00128173828125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 102, "type": "text", "text": "Matthew Henderson, Blaise Thomson, and Jason Williams. 2013. Dialog state tracking challenge 2 & 3.  http://camdial.org/˜mh521/dstc/ . ", "page_idx": 8, "bbox": [307, 460.6283264160156, 525, 496.0219421386719], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 103, "type": "text", "text": "Matthew Henderson, Blaise Thomson, and Jason Williams. 2014a. The second dialog state tracking challenge. In  SIGDIAL . ", "page_idx": 8, "bbox": [307, 502.1773376464844, 525, 536.1002807617188], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 104, "type": "text", "text": "Matthew Henderson, Blaise Thomson, and Steve Young. 2014b. Robust dialog state tracking using delexicalised recurrent neural networks and unsu- pervised gate. In  IEEE SLT . ", "page_idx": 8, "bbox": [307, 543.726318359375, 525, 588.6082763671875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 105, "type": "text", "text": "Matthew Henderson, Blaise Thomson, and Steve Young. 2014c. Word-based dialog state tracking with recurrent neural networks. In  SIGDIAL . ", "page_idx": 8, "bbox": [307, 596.2343139648438, 525, 630.1572875976562], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 106, "type": "text", "text": "Filip Jurˇ c´ ıˇ cek, Blaise Thomson, and Steve Young. 2012. Reinforcement learning for parameter esti- mation in statistical spoken dialogue systems.  Com- puter Speech & Language  26(3):168–192. ", "page_idx": 8, "bbox": [307, 637.7343139648438, 525, 682.665283203125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 107, "type": "text", "text": "Diederik Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. In  International Conference on Learning Representations . ", "page_idx": 8, "bbox": [307, 690.2923583984375, 525, 724.2142944335938], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 108, "type": "text", "text": "Jiwei Li, Michel Galley, Chris Brockett, Georgios P Spithourakis, Jianfeng Gao, and Bill Dolan. 2016. A persona-based neural conversation model. In  ACL . ", "page_idx": 8, "bbox": [307, 731.84130859375, 525, 765.7642822265625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 109, "type": "text", "text": "Xuijun Li, Yun-Nung Chen, Lihong Li, and Jianfeng Gao. 2017. End-to-end task-completion neural dia- logue systems.  arXiv preprint arXiv:1703.01008  . Bing Liu and Ian Lane. 2016. Joint online spoken lan- guage understanding and language modeling with recurrent neural networks. In  SIGDIAL . Bing Liu and Ian Lane. 2017a. An end-to-end trainable neural network model with belief tracking for task- oriented dialog. In  Interspeech . Bing Liu and Ian Lane. 2017b. Iterative policy learning in end-to-end trainable task-oriented neural dialog models. In  Proceedings of IEEE ASRU . Bing Liu, Gokhan Tur, Dilek Hakkani-Tur, Pararth Shah, and Larry Heck. 2017. End-to-end optimiza- tion of task-oriented dialogue model with deep rein- forcement learning. In  NIPS Workshop on Conver- sational AI . Fei Liu and Julien Perez. 2017. Gated end-to-end memory networks. In  EACL . Gr´ egoire Mesnil, Yann Dauphin, Kaisheng Yao, Yoshua Bengio, Li Deng, Dilek Hakkani-Tur, Xi- aodong He, Larry Heck, Gokhan Tur, Dong Yu, et al. 2015. Using recurrent neural networks for slot ﬁll- ing in spoken language understanding.  IEEE/ACM Transactions on Audio, Speech and Language Pro- cessing (TASLP)  . Nikola Mrkˇ si´ c, Diarmuid O S´ eaghdha, Tsung-Hsien Wen, Blaise Thomson, and Steve Young. 2016. Neural belief tracker: Data-driven dialogue state tracking.  arXiv preprint arXiv:1606.03777  . Baolin Peng, Xiujun Li, Lihong Li, Jianfeng Gao, Asli Celikyilmaz, Sungjin Lee, and Kam-Fai Wong. 2017. Composite task-completion dialogue policy learning via hierarchical deep reinforcement learn- ing. In  Proceedings of EMNLP . Antoine Raux, Brian Langner, Dan Bohus, Alan W Black, and Maxine Eskenazi. 2005. Lets go pub- lic! taking a spoken dialog system to the real world. In  Interspeech . St´ ephane Ross and Drew Bagnell. 2010. Efﬁcient re- ductions for imitation learning. In  Proceedings of the thirteenth international conference on artiﬁcial intelligence and statistics . pages 661–668. St´ ephane Ross, Geoffrey J Gordon, and Drew Bagnell. 2011. A reduction of imitation learning and struc- tured prediction to no-regret online learning. In  In- ternational Conference on Artiﬁcial Intelligence and Statistics . pages 627–635. Alexander I Rudnicky, Eric H Thayer, Paul C Constan- tinides, Chris Tchou, R Shern, Kevin A Lenzo, Wei Xu, and Alice Oh. 1999. Creating natural dialogs in the carnegie mellon communicator system. In  Eu- rospeech . ", "page_idx": 9, "bbox": [72, 64.56060791015625, 290, 765.76416015625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 110, "type": "text", "text": "Jost Schatzmann, Blaise Thomson, Karl Weilhammer, Hui Ye, and Steve Young. 2007. Agenda-based user simulation for bootstrapping a pomdp dialogue sys- tem. In  NAACL-HLT . Minjoon Seo, Ali Farhadi, and Hannaneh Hajishirzi. 2016. Query-regression networks for machine com- prehension.  arXiv preprint arXiv:1606.04582  . Iulian V Serban, Alessandro Sordoni, Yoshua Bengio, Aaron Courville, and Joelle Pineau. 2015. Build- ing end-to-end dialogue systems using generative hi- erarchical neural network models. arXiv preprint arXiv:1507.04808  . Pararth Shah, Dilek Hakkani-T¨ ur, Liu Bing, and Gokhan T¨ ur. 2018. Bootstrapping a neural conver- sational agent with dialogue self-play, crowdsourc- ing and on-line reinforcement learning. In  NAACL- HLT . Pararth Shah, Dilek Hakkani-T¨ ur, and Larry Heck. 2016. Interactive reinforcement learning for task- oriented dialogue management. In  NIPS 2016 Deep Learning for Action and Interaction Workshop . Pei-Hao Su, Pawel Budzianowski, Stefan Ultes, Mil- ica Gasic, and Steve Young. 2017. Sample-efﬁcient actor-critic reinforcement learning with supervised data for dialogue management. In  SIGDIAL . Pei-Hao Su, Milica Gasic, Nikola Mrksic, Lina Rojas- Barahona, Stefan Ultes, David Vandyke, Tsung- Hsien Wen, and Steve Young. 2016. On-line active reward learning for policy optimisation in spoken di- alogue systems. In  ACL . Tsung-Hsien Wen, David Vandyke, Nikola Mrkˇ si´ c, Milica Gaˇ si´ c, Lina M. Rojas-Barahona, Pei-Hao Su, Stefan Ultes, and Steve Young. 2017. A network- based end-to-end trainable task-oriented dialogue system. In  EACL . Jason D Williams, Kavosh Asadi, and Geoffrey Zweig. 2017. Hybrid code networks: practical and efﬁcient end-to-end dialog control with supervised and rein- forcement learning. In  ACL . Jason D Williams and Geoffrey Zweig. 2016. End- to-end lstm-based dialog control optimized with su- pervised and reinforcement learning.  arXiv preprint arXiv:1606.01269  . Ronald J Williams. 1992. Simple statistical gradient- following algorithms for connectionist reinforce- ment learning.  Machine learning  . Steve Young, Milica Gaˇ si´ c, Blaise Thomson, and Ja- son D Williams. 2013. Pomdp-based statistical spo- ken dialog systems: A review.  Proceedings of the IEEE  101(5):1160–1179. Tiancheng Zhao and Maxine Eskenazi. 2016. Towards end-to-end learning for dialog state tracking and management using deep reinforcement learning. In SIGDIAL . ", "page_idx": 9, "bbox": [307, 64.56024169921875, 525, 757.3668823242188], "page_size": [595.2760009765625, 841.8900146484375]}
