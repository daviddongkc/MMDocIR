{"page": 0, "image_path": "doc_images/2310.07609v1_0.jpg", "ocr_text": "arXiv:2310.07609v1 [cs.CL] 11 Oct 2023\n\nQACHECK: A Demonstration System\nfor Question-Guided Multi-Hop Fact-Checking\n\nLiangming Pan!? Xinyuan Lu?\n\nMin-Yen Kan® Preslav Nakov!\n\n'MBZUAI University of California, Santa Barbara * National University of Singapore\n\nliangmingpan@ucsb. edu\nkanmy@comp.nus. edu. sg\n\nAbstract\n\nFact-checking real-world claims often requires\ncomplex, multi-step reasoning due to the ab-\nsence of direct evidence to support or refute\nthem. However, existing fact-checking sys-\ntems often lack transparency in their decision-\nmaking, making it challenging for users to com-\nprehend their reasoning process. To address\nthis, we propose the Question-guided Multi-\nhop Fact-Checking (QACHECK) system, which\nguides the model’s reasoning process by ask-\ning a series of questions critical for verifying\na claim. QACHECK has five key modules: a\nclaim verifier, a question generator, a question-\nanswering module, a QA validator, and a rea-\nsoner. Users can input a claim into QACHECK,\nwhich then predicts its veracity and provides\na comprehensive report detailing its reasoning\nprocess, guided by a sequence of (question,\nanswer) pairs. QACHECK! also provides the\nsource of evidence supporting each question,\nfostering a transparent, explainable, and user-\nfriendly fact-checking process.\n\n1 Introduction\n\nIn our age characterized by large amounts of both\ntrue and false information, fact-checking is not\nonly crucial for counteracting misinformation but\nalso plays a vital role in fostering trust in AI sys-\ntems. However, the process of validating real-world\nclaims is rarely straightforward. Unlike the simplic-\nity of supporting or refuting a claim with a single\npiece of direct evidence, real-world claims often\nresemble multi-layered puzzles that require com-\nplex and multi-step reasoning to solve (Jiang et al.,\n2020; Nguyen et al., 2020; Aly and Vlachos, 2022;\nChen et al., 2022; Pan et al., 2023).\n\nAs an example, to verify the claim “Sunlight can\nreach the deepest part of the Black Sea.”, it may be\nchallenging to find direct evidence on the web that\n\n'QACHECK is public available at https: //github.com/\n\nXinyuanLu@@/QACheck. A recorded video is at https: //www.\nyoutube. com/watch?v=ju8kxS1dM64\n\nluxinyuan@u.nus.edu\n\npreslav.nakov@mbzuai.ac.ae\n\nClaim: Sunlight can travel to the deepest part of the Black Sea.\n\n[ech KJ\nQ1: What is the greatest depth ermrmmnmnsnnnnns se\nof the Black Sea? -\n221m\nA1: Black sea has a maximum\ndepth of 2,212 meters.\n[ech KJ\n\nC2) Q2: How far can sunlight\npenetrate water?\n\nA2: Sunlight does not penetrate\nwater below 1,000 meters.\n\nse; 2,212 is greater than 1,000. Therefore, the claim is €3]7\\539\n\nFigure 1: An example of question-guided reasoning for\nfact-checking complex real-world claims.\n\nrefutes or supports this claim. Instead, a human\nfact-checker needs to decompose the claim, gather\nmultiple pieces of evidence, and perform step-by-\nstep reasoning (Pan et al., 2023). This reasoning\nprocess can be formulated as question-guided rea-\nsoning, where the verification of the claim is guided\nby asking and answering a series of relevant ques-\ntions, as shown in Figure 1. In this example, we se-\nquentially raise two questions: “What is the great-\nest depth of the Black Sea?” and “How far can\nsunlight penetrate water?”. After independently\nanswering these two questions by gathering rele-\nvant information from the Web, we can assert that\nthe initial claim is false with simple reasoning.\nWhile several models (Liu et al., 2020; Zhong\net al., 2020; Aly and Vlachos, 2022) have been\nproposed to facilitate multi-step reasoning in fact-\nchecking, they generally lack transparency in their\nreasoning processes. These models simply take a\nclaim as input, then output a veracity label without\nan explicit explanation. Recent attempts, such as\nQuin+ (Samarinas et al., 2021) and WhatThe Wiki-\nFact (Chernyavskiy et al., 2021), have aimed to de-\nvelop more explainable fact-checking systems, by\n", "vlm_text": "QAC HECK : A Demonstration System for Question-Guided Multi-Hop Fact-Checking \nLiangming   $\\mathbf{Pan}^{1,2}$    Xinyuan  $\\mathbf{L}\\mathbf{u}^{3}$  Min-Yen Kan 3 Preslav Nakov 1 1 MBZUAI 2 University of California, Santa Barbara 3  National University of Singapore \nliang ming pan@ucsb.edu luxinyuan@u.nus.edu kanmy@comp.nus.edu.sg preslav.nakov@mbzuai.ac.ae \nAbstract \nFact-checking real-world claims often requires complex, multi-step reasoning due to the ab- sence of direct evidence to support or refute them. However, existing fact-checking sys- tems often lack transparency in their decision- making, making it challenging for users to com- prehend their reasoning process. To address this, we propose the  Question-guided Multi- hop Fact-Checking  (QAC HECK ) system, which guides the model’s reasoning process by ask- ing a series of questions critical for verifying a claim. QAC HECK  has five key modules: a claim verifier, a question generator, a question- answering module, a QA validator, and a rea- soner. Users can input a claim into QAC HECK , which then predicts its veracity and provides a comprehensive report detailing its reasoning process, guided by a sequence of (question, answer) pairs. QAC HECK 1   also provides the source of evidence supporting each question, fostering a transparent, explain able, and user- friendly fact-checking process. \n1 Introduction \nIn our age characterized by large amounts of both true and false information, fact-checking is not only crucial for counteracting misinformation but also plays a vital role in fostering trust in AI sys- tems. However, the process of validating real-world claims is rarely straightforward. Unlike the simplic- ity of supporting or refuting a claim with a single piece of direct evidence, real-world claims often resemble multi-layered puzzles that require com- plex and multi-step reasoning to solve ( Jiang et al. , 2020 ;  Nguyen et al. ,  2020 ;  Aly and Vlachos ,  2022 ; Chen et al. ,  2022 ;  Pan et al. ,  2023 ). \nAs an example, to verify the claim  “Sunlight can reach the deepest part of the Black Sea.” , it may be challenging to find direct evidence on the web that \nThe image illustrates a process for fact-checking the claim: \"Sunlight can travel to the deepest part of the Black Sea.\"\n\n1. **Claim**: Sunlight can travel to the deepest part of the Black Sea.\n\n2. **Question 1 (Q1)**: What is the greatest depth of the Black Sea?\n   - **Answer 1 (A1)**: The Black Sea has a maximum depth of 2,212 meters.\n   - A search result image shows the depth as 2,212 meters.\n\n3. **Question 2 (Q2)**: How far can sunlight penetrate water?\n   - **Answer 2 (A2)**: Sunlight does not penetrate water below 1,000 meters.\n   - A search result image indicates sunlight penetrates up to 1,000 meters.\n\n4. **Conclusion**: Since 2,212 meters is greater than 1,000 meters, the claim is marked as false.\nrefutes or supports this claim. Instead, a human fact-checker needs to decompose the claim, gather multiple pieces of evidence, and perform step-by- step reasoning ( Pan et al. ,  2023 ). This reasoning process can be formulated as  question-guided rea- soning , where the verification of the claim is guided by asking and answering a series of relevant ques- tions, as shown in Figure  1 . In this example, we se- quentially raise two questions:  “What is the great- est depth of the Black Sea?”  and  “How far can sunlight penetrate water?” . After independently answering these two questions by gathering rele- vant information from the Web, we can assert that the initial claim is  false  with simple reasoning. \nWhile several models ( Liu et al. ,  2020 ;  Zhong et al. ,  2020 ;  Aly and Vlachos ,  2022 ) have been proposed to facilitate multi-step reasoning in fact- checking, they generally lack transparency in their reasoning processes. These models simply take a claim as input, then output a veracity label without an explicit explanation. Recent attempts, such as Quin+ (Samarinas et al., 2021) and What The Wiki-Fact  ( Cher nya v ski y et al. ,  2021 ), have aimed to de- velop more explain able fact-checking systems, by searching and visualizing the supporting evidence for a given claim. However, these systems primar- ily validate a claim from a  single  document, and do not provide a detailed, step-by-step visualization of the reasoning process as shown in Figure  1 . "}
{"page": 1, "image_path": "doc_images/2310.07609v1_1.jpg", "ocr_text": "searching and visualizing the supporting evidence\nfor a given claim. However, these systems primar-\nily validate a claim from a single document, and do\nnot provide a detailed, step-by-step visualization\nof the reasoning process as shown in Figure 1.\n\nWe introduce the Question-guided Multi-hop\nFact-Checking (QACHECK) system, which ad-\ndresses the aforementioned issues by generating\nmulti-step explanations via question-guided rea-\nsoning. To facilitate an explainable reasoning pro-\ncess, QACHECK manages the reasoning process by\nguiding the model to self-generate a series of ques-\ntions vital for claim verification. Our system, as\ndepicted in Figure 2, is composed of five modules:\n1) a claim verifier that assesses whether sufficient\ninformation has been gathered to verify the claim,\n2) a question generator to generate the next rele-\nvant question, 3) a question-answering module to\nanswer the raised question, 4) a QA validator to\nevaluate the usefulness of the generated (Q, A) pair,\nand 5) a reasoner to output the final veracity label\nbased on all collected contexts.\n\nQACHECK offers enough adaptability, allowing\nusers to customize the design of each module by\nintegrating with different models. For example,\nwe provide three alternative implementations for\nthe QA component: the retriever-reader model,\nthe FLAN-T5 model, and the GPT3-based reciter—\nreader model. Furthermore, we offer a user-friendly\ninterface for users to fact-check any input claim\nand visualize its detailed question-guided reason-\ning process. The screenshot of our user interface is\nshown in Figure 4. We will discuss the implementa-\ntion details of the system modules in Section 3 and\nsome evaluation results in Section 4. Finally, we\npresent the details of the user interface in Section 5.\nand conclude and discuss future work in Section 6.\n\n2 Related Work\n\nFact-Checking Systems. The recent surge in\nautomated fact-checking research aims to miti-\ngate the spread of misinformation. Various fact-\nchecking systems, for example, TANBIH? (Zhang\net al., 2019), PRTA? (Martino et al., 2020),\nand WHATTHEWIKIFACT* (Chernyavskiy et al.,\n2021) predominantly originating from Wikipedia\nand claims within political or scientific domains,\nhave facilitated this endeavor. However, the major-\n\nhttps: //www. tanbih. org/about\n5https: //propaganda.qcri.org/\n‘https: //www. tanbih. org/whatthewikifact\n\nClaim\n77\" (Dowehavesufficient ~\n\nt contexts to verify the claim?) \\\n\n'\n\nH Claim Verifi Relevant H\n\na\n\naim Veririer Context !\n\n' 1\n1\n\n1\n\n! (What is the next question to ask?) '\n1\n\n1\n\n! Question Generator ®\n\n' 1\n\n| (Is the QA-pair !\n\n! (What is the answer to the question?) correct and useful?) 1\n1\n\n1\n\n1 q 1\n\n\\ QA Model aA Validator a\n\nre Reasoner\n\nWikipedia Corpus Label\n\nFigure 2: The architecture of our QACHECK system.\n\nity of these systems limit the validation or refuta-\ntion of a claim to a single document, indicating\na gap in systems for multi-step reasoning (Pan\net al., 2023). The system most similar to ours\nis Quin+ (Samarinas et al., 2021), which demon-\nstrates evidence retrieval in a single step. In con-\ntrast, our QACHECK shows a question-led multi-\nstep reasoning process with explanations and re-\ntrieved evidence for each reasoning step. In sum-\nmary, our system 1) supports fact-checking real-\nworld claims that require multi-step reasoning, and\n2) enhances transparency and helps users have a\nclear understanding of the reasoning process.\n\nExplanation Generation. Simply predicting a\nveracity label to the claim is not persuasive, and can\neven enhance mistaken beliefs (Guo et al., 2022).\nHence, it is necessary for automated fact-checking\nmethods to provide explanations to support model\npredictions. Traditional approaches have utilized\nattention weights, logic, or summary generation\nto provide post-hoc explanations for model pre-\ndictions (Lu and Li, 2020; Ahmadi et al., 2019;\nKotonya and Toni, 2020; Jolly et al., 2022; Xing\net al., 2022). In contrast, our approach employs\nquestion—answer pair based explanations, offering\nmore human-like and natural explanations.\n\n3 System Architecture\n\nFigure 2 shows the general architecture of our sys-\ntem, comprised of five principal modules: a Claim\n", "vlm_text": "\nWe introduce the  Question-guided Multi-hop Fact-Checking  (QAC HECK ) system, which ad- dresses the aforementioned issues by generating multi-step explanations via question-guided rea- soning. To facilitate an explain able reasoning pro- cess, QAC HECK  manages the reasoning process by guiding the model to self-generate a series of ques- tions vital for claim verification. Our system, as depicted in Figure  2 , is composed of five modules: 1) a  claim verifier  that assesses whether sufficient information has been gathered to verify the claim, 2) a  question generator  to generate the next rele- vant question, 3) a  question-answering  module to answer the raised question, 4) a    $Q A$   validator  to evaluate the usefulness of the generated (Q, A) pair, and 5) a  reasoner  to output the final veracity label based on all collected contexts. \nQAC HECK  offers enough adaptability, allowing users to customize the design of each module by integrating with different models. For example, we provide three alternative implementations for the QA component: the retriever–reader model, the FLAN-T5 model, and the GPT3-based reciter– reader model. Furthermore, we offer a user-friendly interface for users to fact-check any input claim and visualize its detailed question-guided reason- ing process. The screenshot of our user interface is shown in Figure  4 . We will discuss the implementa- tion details of the system modules in Section  3  and some evaluation results in Section  4 . Finally, we present the details of the user interface in Section  5 . and conclude and discuss future work in Section  6 . \n2 Related Work \nFact-Checking Systems. The recent surge in automated fact-checking research aims to miti- gate the spread of misinformation. Various fact- checking systems, for example, T ANBIH 2   ( Zhang et al. ,  2019 ), PRTA 3   ( Martino et al. ,  2020 ), and W HAT T HE W IKI F ACT 4   ( Cher nya v ski y et al. , 2021 ) predominantly originating from Wikipedia and claims within political or scientific domains, have facilitated this endeavor. However, the major- \nThe image is a flowchart illustrating the architecture of the QAC HECK system. Here's a breakdown of the process:\n\n1. **Claim**: The starting point of the process.\n2. **Claim Verifier**: Assesses whether there are sufficient contexts to verify the claim and interacts with the \"Relevant Context\".\n3. **Question Generator**: Determines the next question to ask based on the verification process.\n4. **QA Model**: Answers the generated question.\n5. **Validator**: Checks if the QA pair (Question and Answer) is correct and useful.\n6. **Reasoner**: Uses information from the Wikipedia Corpus and QA to arrive at a conclusion.\n7. **Label**: The final outcome or classification of the claim.\n\nThe system relies on elements like the Wikipedia Corpus to provide context and evidence, and loops through its components to ensure accurate verification.\nity of these systems limit the validation or refuta- tion of a claim to a single document, indicating a gap in systems for multi-step reasoning ( Pan et al. ,  2023 ). The system most similar to ours is    $Q u i n+$   ( Samarinas et al. ,  2021 ), which demon- strates evidence retrieval in a single step. In con- trast, our QAC HECK  shows a question-led multi- step reasoning process with explanations and re- trieved evidence for each reasoning step. In sum- mary, our system 1) supports fact-checking real- world claims that require multi-step reasoning, and 2) enhances transparency and helps users have a clear understanding of the reasoning process. \nExplanation Generation. Simply predicting a veracity label to the claim is not persuasive, and can even enhance mistaken beliefs ( Guo et al. ,  2022 ). Hence, it is necessary for automated fact-checking methods to provide explanations to support model predictions. Traditional approaches have utilized attention weights, logic, or summary generation to provide post-hoc explanations for model pre- dictions ( Lu and Li ,  2020 ;  Ahmadi et al. ,  2019 ; Kotonya and Toni ,  2020 ;  Jolly et al. ,  2022 ;  Xing et al. ,  2022 ). In contrast, our approach employs question–answer  pair based explanations, offering more human-like and natural explanations. \n3 System Architecture \nFigure  2  shows the general architecture of our sys- tem, comprised of five principal modules: a Claim Verifier    $\\mathcal{D}$  , a Que ion Generat    $\\mathcal{Q}$  , a Question- nswering Model  A , a Validator  V , and a R R . We first initialize an empty context  C  ${\\mathcal{C}}\\,=\\,\\emptyset$   ∅ . Upon the receipt of a new input claim    $c$  , the sys- tem first utilizes the  claim verifier  to determine the sufficiency of the existing context to validate the claim,  i.e. ,    $\\mathcal{D}(c,\\mathcal{C})\\,\\rightarrow\\,\\{\\mathsf{T r u e},\\mathsf{F a l s e}\\}$  . If the output is  False , the  question generator  learns to generate the next question that is necessary for ver- ifying the claim,  i.e. ,    $\\mathcal{Q}(c,\\mathcal{C})\\to q$  . The  question- answering  model is then applied to answer the question and provide the supported evidence,  i.e. ,  $\\mathcal{A}(q)\\rightarrow a,e$  , where  $a$   is the predicted answer, and  $e$   is the retrieved evidence that supports the an- swer. Afterward, the  validator  is used to validate the usefulness of the newly-generated (Q, A) pair based on the existing context and the claim,  i.e. ,  $\\mathcal{V}(c,\\{q,a\\},\\mathcal{C})\\,\\rightarrow\\,\\{\\mathsf{T r u e},\\mathsf{F a l s e}\\}$  . If the output is  True , the    $(q,a)$   pair is added into the context  $C$  . Otherwise, the question generator is asked to generate another question. We repeat this process of calling    ${\\mathcal{D}}\\,\\rightarrow\\,{\\mathcal{Q}}\\,\\rightarrow\\,{\\mathcal{A}}\\,\\rightarrow\\,{\\mathcal{V}}$   until the claim verifier returns a  True  indicating that the current context    $C$   contains sufficient information to ver- ify the claim    $c$  . In this case, the  reasoner  module is called to utilize the stored relevant context to justify the veracity of the claim and outputs the fi- nal label,  i.e. ,    $\\mathcal{R}(c,\\mathcal{C})\\rightarrow\\{\\mathsf{S u p p o r t e d},\\mathsf{R e f u t e d}\\}.$  . The subsequent sections provide a comprehensive description of the five key modules in QAC HECK . "}
{"page": 2, "image_path": "doc_images/2310.07609v1_2.jpg", "ocr_text": "Verifier D, a Question Generator Q, a Question-\nAnswering Model A, a Validator V, and a Reasoner\nR. We first initialize an empty context C = 9).\nUpon the receipt of a new input claim c, the sys-\ntem first utilizes the claim verifier to determine\nthe sufficiency of the existing context to validate\nthe claim, i.e., D(c,C) + {True, False}. If the\noutput is False, the question generator learns to\ngenerate the next question that is necessary for ver-\nifying the claim, i.e., Q(c,C) + q. The question-\nanswering model is then applied to answer the\nquestion and provide the supported evidence, i.e.,\nA(q) — a, e, where a is the predicted answer, and\ne is the retrieved evidence that supports the an-\nswer. Afterward, the validator is used to validate\nthe usefulness of the newly-generated (Q, A) pair\nbased on the existing context and the claim, i.e.,\nV(c, {q,a},C) — {True, False}. If the output\nis True, the (q,a) pair is added into the context\nC. Otherwise, the question generator is asked to\ngenerate another question. We repeat this process\nof calling D > Q > A -— YV until the claim\nverifier returns a True indicating that the current\ncontext C' contains sufficient information to ver-\nify the claim c. In this case, the reasoner module\nis called to utilize the stored relevant context to\njustify the veracity of the claim and outputs the fi-\nnal label, i.c., R(c,C) —> {Supported, Refuted}.\nThe subsequent sections provide a comprehensive\ndescription of the five key modules in QACHECK.\n\n3.1 Claim Verifier\n\nThe claim verifier is a central component of\nQACHECK, with the specific role of determining\nif the current context information is sufficient to\nverify the claim. This module is to ensure that the\nsystem can efficiently complete the claim verifica-\ntion process without redundant reasoning. We build\nthe claim verifier based on InstructGPT (Ouyang\net al., 2022), utilizing its powerful in-context learn-\ning ability. Recent large language models such\nas InstructGPT (Ouyang et al., 2022) and GPT-\n4 (OpenAL, 2023) have demonstrated strong few-\nshot generalization ability via in-context learning,\nin which the model can efficiently learn a task when\nprompted with the instruction of the task together\nwith a small number of demonstrations. We take ad-\nvantage of InstructGPT’s in-context learning abil-\nity to implement the claim verifier. We prompt\nInstructGPT with ten distinct in-context examples\nas detailed in Appendix A.1, where each example\n\nconsists of a claim and relevant question—answer\npairs. We then prompt the model with the claim,\nthe context, and the following instruction:\n\nClaim = (@2))\n\nWe already know the following:\nCan we know whether the claim is\ntrue or false now? Yes or no?\n\nIf the response is ‘no’, we proceed to the question\ngenerator module. Conversely, if the response is\n‘yes’, the process jumps to call the reasoner module.\n\n3.2 Question Generator\n\nThe question generator module is called when the\ninitial claim lacks the necessary context for veri-\nfication. This module aims to generate the next\nrelevant question needed for verifying the claim.\nSimilar to the claim verifier, we also leverage In-\nstructGPT for in-context learning. We use slightly\ndifferent prompts for generating the initial question\nand the follow-up questions. The detailed prompts\nare in Appendix A.2. For the initial question gen-\neration, the instruction is:\n\nClaim = (@2i))\n\nTo verify the above claim, we can\nfirst ask a simple question:\n\nFor follow-up questions, the instruction is:\n\nClaim = (@5i))\n\nWe already know the following:\nTo verify the claim, what is the\nnext question we need to know the\nanswer to?\n\n3.3, Question Answering Model\n\nAfter generating a question, the Question Answer-\ning (QA) module retrieves corresponding evidence\nand provides an answer as the output. The system’s\nreliability largely depends on the accuracy of the\nQA module’s responses. Understanding the need\nfor different QA methods in various fact-checking\nscenarios, we introduce three different implemen-\ntations for the QA module, as shown in Figure 3.\n\nRetriever-Reader. We first integrate the well-\nknown retriever-reader framework, a prevalent\nQA paradigm originally introduced by Chen et al.\n(2017). In this framework, a retriever first re-\n\ntrieves relevant documents from a large evidence\n", "vlm_text": "\n3.1 Claim Verifier \nThe claim verifier is a central component of QAC HECK , with the specific role of determining if the current context information is sufficient to verify the claim. This module is to ensure that the system can efficiently complete the claim verifica- tion process without redundant reasoning. We build the claim verifier based on Instruct GP T ( Ouyang et al. ,  2022 ), utilizing its powerful  in-context learn- ing  ability. Recent large language models such as Instruct GP T ( Ouyang et al. ,  2022 ) and GPT- 4 ( OpenAI ,  2023 ) have demonstrated strong few- shot generalization ability via  in-context learning , in which the model can efficiently learn a task when prompted with the instruction of the task together with a small number of demonstrations. We take ad- vantage of Instruct GP T’s in-context learning abil- ity to implement the claim verifier. We prompt Instruct GP T with ten distinct in-context examples as detailed in Appendix  A.1 , where each example consists of a claim and relevant question–answer pairs. We then prompt the model with the claim, the context, and the following instruction: \n\nThe image contains text that discusses a process of evaluating a claim. It includes three main sections:\n\n1. \"Claim = CLAIM\" - indicates that a specific claim is being evaluated, although no concrete claim is provided in the image.\n\n2. \"We already know the following: CONTEXT\" - suggests that there is some pre-existing context or information related to the claim, which will be considered in the evaluation, but the actual context is not shown in the image.\n\n3. \"Can we know whether the claim is true or false now? Yes or no?\" - poses a question about the ability to determine the truthfulness of the claim given the context, asking for a binary answer (yes or no).\nIf the response is  ‘no’ , we proceed to the question generator module. Conversely, if the response is ‘yes’ , the process jumps to call the reasoner module. \n3.2 Question Generator \nThe question generator module is called when the initial claim lacks the necessary context for veri- fication. This module aims to generate the next relevant question needed for verifying the claim. Similar to the claim verifier, we also leverage In- structGPT for in-context learning. We use slightly different prompts for generating the initial question and the follow-up questions. The detailed prompts are in Appendix  A.2 . For the  initial  question gen- eration, the instruction is: \nClaim   $=$   CLAIM To verify the above claim, we can first ask a simple question: \nFor  follow-up  questions, the instruction is: \nThe image contains text related to verifying a claim. It presents a structure for analysis that includes:\n\n1. **Claim** labeled as \"CLAIM\"\n2. A statement, \"We already know the following:\" followed by \"CONTEXT\"\n3. A prompt asking, \"To verify the claim, what is the next question we need to know the answer to?\" \n\nThis layout suggests a focus on evaluating and analyzing claims through context and further questioning.\n3.3 Question Answering Model \nAfter generating a question, the Question Answer- ing (QA) module retrieves corresponding evidence and provides an answer as the output. The system’s reliability largely depends on the accuracy of the QA module’s responses. Understanding the need for different QA methods in various fact-checking scenarios, we introduce three different implemen- tations for the QA module, as shown in Figure  3 . \nRetriever–Reader. We first integrate the well- known  retriever–reader  framework, a prevalent QA paradigm originally introduced by  Chen et al. ( 2017 ). In this framework, a  retriever  first re- trieves relevant documents from a large evidence "}
{"page": 3, "image_path": "doc_images/2310.07609v1_3.jpg", "ocr_text": "(a) Retriever-Reader\nQuestion\n\nWikipedia |\n% <Retrieved Evidence>\n\nms — Q: <Question>\n\nRetriever The answer is:\n\n(b) FLAN-T5 Q: <Question>\n\n. The answer is:\nQuestion\n\nRetrieve a Wikipedia\n—— article relevant to\nthis question.\n\nInstructGPT\n\na\n\nFigure 3: Illustrations of the three different implementa-\ntions of the Question Answering module in QACHECK.\n\ncorpus, and then a reader predicts an answer con-\nditioned on the retrieved documents. For the ev-\nidence corpus, we use the Wikipedia dump pro-\nvided by the Knowledge-Intensive Language Tasks\n(KILT) benchmark (Petroni et al., 2021), in which\nthe Wikipedia articles have been pre-processed\nand separated into paragraphs. For the retriever,\nwe apply the widely-used sparse retrieval based\non BM25 (Robertson and Zaragoza, 2009), imple-\nmented with the Pyserini toolkit (Lin et al., 2021).\nFor the reader, we use the RoBERTa-large (Liu\net al., 2019) model fine-tuned on the SQUAD\ndataset (Rajpurkar et al., 2016), using the imple-\nmentation from PrimeQA> (Sil et al., 2023).\n\nFLAN-TS5S. While effective, the retriever-reader\nframework is constrained by its reliance on the ev-\nidence corpus. In scenarios where a user’s claim\nis outside the scope of Wikipedia, the system\nmight fail to produce a credible response. To en-\nhance flexibility, we also incorporate the FLAN-T5\nmodel (Chung et al., 2022), a Seq2Seq model pre-\ntrained on more than 1.8K tasks with instruction\n\nShttps: //github.com/primeqa/primeqa\n\ntuning. It directly takes the question as input and\nthen generates the answer and the evidence, based\non the model’s parametric knowledge.\n\nGPT Reciter—Reader. Recent studies (Sun et al.,\n2023; Yu et al., 2023) have demonstrated the\ngreat potential of the GPT series, such as Instruct-\nGPT (Ouyang et al., 2022) and GPT-4 (OpenAI,\n2023), to function as robust knowledge reposito-\nries. The knowledge can be retrieved by properly\nprompting the model. Drawing from this insight,\nwe introduce the GPT Reciter—Reader approach.\nGiven a question, we prompt the InstructGPT to\n“recite” the knowledge stored within it, and Instruct-\nGPT responds with relevant evidence. The evi-\ndence is then fed into a reader model to produce\nthe corresponding answer. While this method, like\nFLAN-TS, does not rely on a specific corpus, it\nstands out by using InstructGPT. This offers a\nmore dependable parametric knowledge base than\nFLAN-TS.\n\nThe above three methods provide a flexible and\nrobust QA module, allowing for switching between\nthe methods as required, depending on the claim\nbeing verified and the available contextual informa-\ntion. In the following, we use GPT Reciter—Reader\nas the default implementation for our QA module.\n\n3.4 QA Validator\n\nThe validator module ensures the usefulness of the\nnewly-generated QA pairs. For a QA pair to be\nvalid, it must satisfy two criteria: 1) it brings addi-\ntional information to the current context C, and 2) it\nis useful for verifying the original claim. We again\nimplement the validator by prompting InstructGPT\nwith a suite of ten demonstrations shown in Ap-\npendix A.3. The instruction is as follows:\n\nClaim = (Ey)\n\nWe already know the following:\n\nNow we further know:\n\nDoes the QA pair have additional\nknowledge useful for verifying\nthe claim?\n\nThe validator acts as a safeguard against the system\nproducing redundant or irrelevant QA pairs. Upon\nvalidation of a QA pair, it is added to the current\ncontext C. Subsequently, the system initiates an-\nother cycle of calling the claim verifier, question\ngenerator, question answering, and validation.\n", "vlm_text": "The image depicts three different implementations of a Question Answering (QA) module in a system referred to as QAC HECK. Here's a breakdown:\n\n1. **(a) Retriever–Reader:**\n   - The process begins with a question input.\n   - A retriever searches Wikipedia for relevant evidence.\n   - The retrieved evidence and the question are passed to a reader.\n   - The reader provides the answer.\n\n2. **(b) FLAN-T5:**\n   - The question is directly inputted into the FLAN-T5 model.\n   - The model generates an answer.\n\n3. **(c) GPT Reciter–Reader:**\n   - The question is inputted into InstructGPT.\n   - InstructGPT retrieves a relevant Wikipedia article.\n   - The retrieved information is sent to a reader.\n   - The reader outputs the answer.\n\nEach implementation depicts a different approach to answering questions using various methods and technologies.\ncorpus, and then a  reader  predicts an answer con- ditioned on the retrieved documents. For the ev- idence corpus, we use the Wikipedia dump pro- vided by the Knowledge-Intensive Language Tasks (KILT) benchmark ( Petroni et al. ,  2021 ), in which the Wikipedia articles have been pre-processed and separated into paragraphs. For the retriever, we apply the widely-used sparse retrieval based on BM25 ( Robertson and Zaragoza ,  2009 ), imple- mented with the Pyserini toolkit ( Lin et al. ,  2021 ). For the reader, we use the  RoBERTa-large  ( Liu et al. ,  2019 ) model fine-tuned on the SQuAD dataset ( Rajpurkar et al. ,  2016 ), using the imple- mentation from  Prime  $Q A^{5}$    ( Sil et al. ,  2023 ). \nFLAN-T5. While effective, the retriever–reader framework is constrained by its reliance on the ev- idence corpus. In scenarios where a user’s claim is outside the scope of Wikipedia, the system might fail to produce a credible response. To en- hance flexibility, we also incorporate the  FLAN-T5 model ( Chung et al. ,  2022 ), a Seq2Seq model pre- trained on more than 1.8K tasks with instruction tuning. It directly takes the question as input and then generates the answer and the evidence, based on the model’s parametric knowledge. \n\nGPT Reciter–Reader. Recent studies ( Sun et al. , 2023 ;  Yu et al. ,  2023 ) have demonstrated the great potential of the GPT series, such as Instruct- GPT ( Ouyang et al. ,  2022 ) and GPT-4 ( OpenAI , 2023 ), to function as robust knowledge reposito- ries. The knowledge can be retrieved by properly prompting the model. Drawing from this insight, we introduce the  GPT Reciter–Reader  approach. Given a question, we prompt the Instruct GP T to “recite”  the knowledge stored within it, and Instruct- GPT responds with relevant evidence. The evi- dence is then fed into a  reader  model to produce the corresponding answer. While this method, like FLAN-T5, does not rely on a specific corpus, it stands out by using Instruct GP T. This offers a more dependable parametric knowledge base than FLAN-T5. \nThe above three methods provide a flexible and robust QA module, allowing for switching between the methods as required, depending on the claim being verified and the available contextual informa- tion. In the following, we use GPT Reciter–Reader as the default implementation for our QA module. \n3.4 QA Validator \nThe validator module ensures the usefulness of the newly-generated QA pairs. For a QA pair to be valid, it must satisfy two criteria: 1) it brings addi- tional information to the current context    $\\mathcal{C}$  , and 2) it is useful for verifying the original claim. We again implement the validator by prompting Instruct GP T with a suite of ten demonstrations shown in Ap- pendix  A.3 . The instruction is as follows: \nThe image contains text related to verifying a claim. It includes highlighted terms such as \"CLAIM,\" \"CONTEXT,\" and \"NEW QA PAIR,\" and asks whether the QA pair provides additional knowledge useful for verifying the claim.\nThe validator acts as a safeguard against the system producing redundant or irrelevant QA pairs. Upon validation of a QA pair, it is added to the current context    $\\mathcal{C}$  . Subsequently, the system initiates an- other cycle of calling the claim verifier, question generator, question answering, and validation. "}
{"page": 4, "image_path": "doc_images/2310.07609v1_4.jpg", "ocr_text": "QACheck: Question-Guided Multi-hop Fact-Checking Demo\ndesigned y\n\nInstructions: Select a claim or just enter your own claim otherwise, and then check the model's output.\n\nQA Mode!\n\nGPT Reciter-Reader\n\n© Please select a claim.\n\nUlrich Walter's employer is headquartered in Cologne.\n\n1. Select or input a custom claim\n\n2. Submit to fact-check the input claim\n\nInput Claim:\n\nLars Onsager won the Nobel prize when he was 30 years old.\n\nQuestion Answering Decomposition:\n\nPP Reasoning depth: 0\n\n@ Predicted Answer: 1968\n\n> Reasoning depth: 1\n\n@© Generated Question: Which year was Lars Onsager born?\n\n@ Predicted Answer: 1903\n\n4. The final prediction result with rationale\n\nPrediction with rationale:\n\nwon the Nob in 1968. Lars Ons s born in 190\n\n3. Visualize the question-answering guided reasoning process\n\n@ Generated Question: In which year did Lars Onsager win the Nobel prize?\n\nThe Nobel Prize in Chemistry 1968 was awarded to Lars\nOnsager for the discovery of the reciprocal relations\nb his name, which are fundamental for the\nthermodynamics of irreversible processes.\n\nr (27 November 1903-5 October 1976) was a\nAmerican theoretical physicist and physical\nchemist.\n\nNobel prize. Th\n\nFigure 4: The screenshot of the QACHECK user interface showing its key annotated functions. First, users have the\noption to select a claim or manually input a claim that requires verification. Second, users can start the verification\nprocess by clicking the Submit button. Third, the system shows a step-by-step question-answering guided reasoning\nprocess. Each step includes the reasoning depth, the generated question, relevant retrieved evidence, and the\ncorresponding predicted answer. Finally, it presents the final prediction /abel with the supporting rationale.\n\n3.5 Reasoner\n\nThe reasoner is called when the claim verifier deter-\nmines that the context C is sufficient to verify the\nclaim or the system hits the maximum allowed iter-\nations, set to 5 by default. The reasoner is a special\nquestion-answering model which takes the context\nC and the claim ¢ as inputs and then answers the\nquestion “Ts the claim true or false?”. The model\nis also requested to output the rationale with the\nprediction. We provide two different implementa-\n\ntions for the reasoner: 1) the end-to-end QA model\nbased on FLAN-TS, and 2) the InstructGPT model\nwith the prompts given in Appendix A.4.\n\n4 Performance Evaluation\n\nTo evaluate the performance of our QACHECK,\nwe use two fact-checking datasets that contain\ncomplex claims requiring multi-step reasoning:\nHOVER (Jiang et al., 2020) and FEVEROUS (Aly\net al., 2021), following the same experimental set-\n", "vlm_text": "The image shows a user interface of a fact-checking demo called \"QACheck: Question-Guided Multi-hop Fact-Checking Demo.\" It's annotated to highlight its key functions:\n\n1. Users can either select a predefined claim or input a custom claim that they want to verify.\n2. To begin the verification process, users click the \"Submit\" button.\n3. The system then visualizes a step-by-step reasoning process for answering the query. This includes:\n   - Reasoning depth\n   - Generated question\n   - Relevant retrieved evidence\n   - Predicted answer\n4. Finally, it displays the prediction result with supporting rationale. In the example, the claim about Lars Onsager winning the Nobel prize at 30 is checked and found to be false, with an explanation provided.\n3.5 Reasoner \nThe reasoner is called when the claim verifier deter- mines that the context  $\\mathcal{C}$   is sufficient to verify the claim or the system hits the maximum allowed iter- ations, set to 5 by default. The reasoner is a special question-answering model which takes the context  $\\mathcal{C}$   and the claim    $c$   as inputs and then answers the question  “Is the claim true or false?” . The model is also requested to output the rationale with the prediction. We provide two different implementa- tions for the reasoner: 1) the end-to-end QA model based on FLAN-T5, and 2) the Instruct GP T model with the prompts given in Appendix  A.4 . \n\n4 Performance Evaluation \nTo evaluate the performance of our QAC HECK , we use two fact-checking datasets that contain complex claims requiring multi-step reasoning: HOVER ( Jiang et al. ,  2020 ) and FEVEROUS ( Aly et al. ,  2021 ), following the same experimental set- "}
{"page": 5, "image_path": "doc_images/2310.07609v1_5.jpg", "ocr_text": "Model HOVER FEVEROUS\n2-hop 3-hop 4-hop\n\nInstructGPT\n\n- Direct | 56.51 51.75 49.68 60.13\n\n- CoT 57.20 53.66 51.83 61.05\nCodex 55.57 53.42 45.59 57.85\nFLAN-T5 48.27 52.11 51.13 55.16\nProgramFC 54.27 54.18 52.88 59.66\nQACheck 55.67 54.67 52.35 59.47\n\nTable 1: Evaluation of F1 scores for different models.\nThe bold text shows the best results for each setting.\n\ntings used in Pan et al. (2023). HOVER con-\ntains 1,126 two-hop claims, 1,835 three-hop claims,\nand 1,039 four-hop claims, while FEVEROUS has\n2,962 multi-hop claims. We compare our method\nwith the baselines of directly applying InstructGPT\nwith two different prompting methods: (i) direct\nprompting with the claim, and (ii) CoT (Wei et al.,\n2022) or chain-of-thought prompting with few-\nshot demonstrations of reasoning explanations. We\nalso compare with ProgramFC (Pan et al., 2023),\nFLAN-T5 (Chung et al., 2022), and Codex (Chen\net al., 2021). We use the reported results for the\nbaseline models from Pan et al. (2023).\n\nThe evaluation results are shown in Table 1. Our\nQACHECK system achieves a macro-F1 score of\n55.67, 54.67, and 52.35 on HOVER two-hop, three-\nhop, and four-hop claims, respectively. It achieves\na 59.47 F1 score on FEVEROUS. These scores are\nbetter than directly using InstructGPT, Codex, or\nFLAN-TS. They are also on par with the systems\nthat apply claim decomposition strategies, i.e., CoT,\nand ProgramFC. The results demonstrate the effec-\ntiveness of our QACHECK system. Especially, the\nQACHECK has better improvement over the end-\nto-end models on claims with high reasoning depth.\nThis indicates that decomposing a complex claim\ninto simpler steps with question-guided reasoning\ncan facilitate more accurate reasoning.\n\n5 User Interface\n\nWe create a demo system based on Flask°® for ver-\nifying open-domain claims with QACHECK, as\nshown in Figure 4. The QACHECK demo is de-\nsigned to be intuitive and user-friendly, enabling\nusers to input any claim or select from a list of\npre-defined claims (top half of Figure 4).\n\nhttps: //flask.palletsprojects.com/en/2.3.x/\n\nUpon selecting or inputting a claim, the user\ncan start the fact-checking process by clicking the\n“Submit” button. The bottom half of Figure 4 shows\na snapshot of QACHECK’s output for the input\nclaim “Lars Onsager won the Nobel prize when\nhe was 30 years old”. The system visualizes the\ndetailed question-guided reasoning process. For\neach reasoning step, the system shows the index\nof the reasoning step, the generated question, and\nthe predicted answer to the question. The retrieved\nevidence to support the answer is shown on the\nright for each step. The system then shows the final\nveracity prediction for the original claim accom-\npanied by a comprehensive rationale in the “Pre-\ndiction with rationale” section. This step-by-step\nillustration not only enhances the understanding of\nour system’s fact-checking process but also offers\ntransparency to its functioning.\n\nQACHECK also allows users to change the un-\nderlying question—answering model. As shown\nat the top of Figure 4, users can select between\nthe three different QA models introduced in Sec-\ntion 3.3, depending on their specific requirements\nor preferences. Our demo system will be open-\nsourced under the Apache-2.0 license.\n\n6 Conclusion and Future Works\n\nThis paper presents the QACHECK system, a novel\napproach designed for verifying real-world com-\nplex claims. QACHECK conducts the reasoning\nprocess with the guidance of asking and answer-\ning a series of questions and answers. Specifically,\nQACHECK iteratively generates contextually rel-\nevant questions, retrieves and validates answers,\njudges the sufficiency of the context information,\nand finally, reasons out the claim’s truth value\nbased on the accumulated knowledge. QACHECK\nleverages a wide range of techniques, such as in-\ncontext learning, document retrieval, and question-\nanswering, to ensure a precise, transparent, explain-\nable, and user-friendly fact-checking process.\n\nIn the future, we plan to enhance QACHECK\n1) by integrating additional knowledge bases to\nfurther improve the breadth and depth of informa-\ntion accessible to the system (Feng et al., 2023;\nKim et al., 2023), and 2) by incorporating a multi-\nmodal interface to support image (Chakraborty\net al., 2023), table (Chen et al., 2020; Lu et al.,\n2023), and chart-based fact-checking (Akhtar et al.,\n2023), which can broaden the system’s utility in\nprocessing and analyzing different forms of data.\n\n", "vlm_text": "The table presents the performance of different AI models on two datasets: HOVER and FEVEROUS. The metrics provided are likely accuracy or F1 scores, measured in percentage, over various tasks:\n\n1. **HOVER Task Types**:\n   - The tasks are divided into 2-hop, 3-hop, and 4-hop questions. These terms likely refer to the complexity or steps involved in reasoning required to answer the questions correctly.\n\n2. **Models and Their Performance**:\n   - **InstructGPT Direct**:\n     - 2-hop: 56.51\n     - 3-hop: 51.75\n     - 4-hop: 49.68\n     - FEVEROUS: 60.13\n   - **InstructGPT CoT** (Chain of Thought):\n     - 2-hop: 57.20\n     - 3-hop: 53.66\n     - 4-hop: 51.83\n     - FEVEROUS: 61.05\n   - **Codex**:\n     - 2-hop: 55.57\n     - 3-hop: 53.42\n     - 4-hop: 45.59\n     - FEVEROUS: 57.85\n   - **FLAN-T5**:\n     - 2-hop: 48.27\n     - 3-hop: 52.11\n     - 4-hop: 51.13\n     - FEVEROUS: 55.16\n   - **ProgramFC**:\n     - 2-hop: 54.27\n     - 3-hop: 54.18\n     - 4-hop: 52.88\n     - FEVEROUS: 59.66\n\n3. **QAcheck Model**:\n   - For HOVER:\n     - 2-hop: 55.67\n     - 3-hop: 54.67\n     - 4-hop: 52.35\n   - FEVEROUS: 59.47\n\nOverall, InstructGPT CoT appears to perform the best among the models listed, with the highest scores for the 2-hop and 3-hop HOVER tasks and the FEVEROUS dataset.\ntings used in  Pan et al.  ( 2023 ). HOVER con- tains 1,126 two-hop claims, 1,835 three-hop claims, and 1,039 four-hop claims, while FEVEROUS has 2,962 multi-hop claims. We compare our method with the baselines of directly applying Instruct GP T with two different prompting methods: ( i )  direct prompting with the claim, and   $(i i)$   CoT  ( Wei et al. , 2022 ) or chain-of-thought prompting with few- shot demonstrations of reasoning explanations. We also compare with  ProgramFC  ( Pan et al. ,  2023 ), FLAN-T5  ( Chung et al. ,  2022 ), and  Codex  ( Chen et al. ,  2021 ). We use the reported results for the baseline models from  Pan et al.  ( 2023 ). \nThe evaluation results are shown in Table  1 . Our QAC HECK  system achieves a macro-F1 score of 55.67, 54.67, and 52.35 on HOVER two-hop, three- hop, and four-hop claims, respectively. It achieves a 59.47 F1 score on FEVEROUS. These scores are better than directly using Instruct GP T, Codex, or FLAN-T5. They are also on par with the systems that apply claim decomposition strategies,  i.e. ,  CoT , and  ProgramFC . The results demonstrate the effec- tiveness of our QAC HECK  system. Especially, the QAC HECK  has better improvement over the end- to-end models on claims with high reasoning depth. This indicates that decomposing a complex claim into simpler steps with question-guided reasoning can facilitate more accurate reasoning. \n5 User Interface \nWe create a demo system based on Flask 6   for ver- ifying open-domain claims with QAC HECK , as shown in Figure  4 . The QAC HECK  demo is de- signed to be intuitive and user-friendly, enabling users to input any claim or select from a list of pre-defined claims (top half of Figure  4 ). \nUpon selecting or inputting a claim, the user can start the fact-checking process by clicking the “Submit”  button. The bottom half of Figure  4  shows a snapshot of QAC HECK ’s output for the input claim  “Lars Onsager won the Nobel prize when he was 30 years old” . The system visualizes the detailed question-guided reasoning process. For each reasoning step, the system shows the index of the reasoning step, the generated question, and the predicted answer to the question. The retrieved evidence to support the answer is shown on the right for each step. The system then shows the final veracity prediction for the original claim accom- panied by a comprehensive rationale in the  “Pre- diction with rationale”  section. This step-by-step illustration not only enhances the understanding of our system’s fact-checking process but also offers transparency to its functioning. \nQAC HECK  also allows users to change the un- derlying question–answering model. As shown at the top of Figure  4 , users can select between the three different QA models introduced in Sec- tion  3.3 , depending on their specific requirements or preferences. Our demo system will be open- sourced under the Apache-2.0 license. \n6 Conclusion and Future Works \nThis paper presents the QAC HECK  system, a novel approach designed for verifying real-world com- plex claims. QAC HECK  conducts the reasoning process with the guidance of asking and answer- ing a series of questions and answers. Specifically, QAC HECK  iterative ly generates con textually rel- evant questions, retrieves and validates answers, judges the sufficiency of the context information, and finally, reasons out the claim’s truth value based on the accumulated knowledge. QAC HECK leverages a wide range of techniques, such as in- context learning, document retrieval, and question- answering, to ensure a precise, transparent, explain- able, and user-friendly fact-checking process. \nIn the future, we plan to enhance QAC HECK 1) by integrating additional knowledge bases to further improve the breadth and depth of informa- tion accessible to the system ( Feng et al. ,  2023 ; Kim et al. ,  2023 ), and 2) by incorporating a multi- modal interface to support image ( Chakra bor ty et al. ,  2023 ), table ( Chen et al. ,  2020 ;  Lu et al. , 2023 ), and chart-based fact-checking ( Akhtar et al. , 2023 ), which can broaden the system’s utility in processing and analyzing different forms of data. "}
{"page": 6, "image_path": "doc_images/2310.07609v1_6.jpg", "ocr_text": "Limitations\n\nWe identify two main limitations of QACHECK.\nFirst, several modules of our QACHECK currently\nutilize external API-based large language models,\nsuch as InstructGPT. This reliance on external APIs\ntends to prolong the response time of our system.\nAs a remedy, we are considering the integration\nof open-source, locally-run large language models\nlike LLaMA (Touvron et al., 2023). Secondly, the\ncurrent scope of our QACHECK is confined to eval-\nuating True/False claims. Recognizing the signifi-\ncance of also addressing Not Enough Information\nclaims, we plan to devise strategies to incorporate\nthese in upcoming versions of the system.\n\nEthics Statement\n\nThe use of large language models requires a signifi-\ncant amount of energy for computation for training,\nwhich contributes to global warming. Our work\nperforms few-shot in-context learning instead of\ntraining models from scratch, so the energy foot-\nprint of our work is less. The large language model\n(InstructGPT) whose API we use for inference con-\nsumes significant energy.\n\nAcknowledgement\n\nThis project is supported by the Ministry of Edu-\ncation, Singapore, under its MOE AcRF TIER 3\nGrant (MOE-MOET32022-0001). The computa-\ntional work for this article was partially performed\non resources of the National Supercomputing Cen-\ntre, Singapore.\n\nReferences\n\nNaser Ahmadi, Joohyung Lee, Paolo Papotti, and Mo-\nhammed Saeed. 2019. Explainable fact checking\nwith probabilistic answer set programming. In Pro-\nceedings of the 2019 Truth and Trust Online Confer-\nence (TTO).\n\nMubashara Akhtar, Oana Cocarascu, and Elena Simperl.\n2023. Reading and reasoning over chart images for\nevidence-based automated fact-checking. In Find-\nings of the Association for Computational Linguistics\n(EACL), pages 399-414.\n\nRami Aly, Zhijiang Guo, Michael Sejr Schlichtkrull,\nJames Thorne, Andreas Vlachos, Christos\nChristodoulopoulos, Oana Cocarascu, and Arpit\nMittal. 2021. FEVEROUS: Fact Extraction and\nVERification Over Unstructured and Structured in-\nformation. In Proceedings of the Neural Information\nProcessing Systems (NeurIPS) Track on Datasets\nand Benchmarks.\n\nRami Aly and Andreas Vlachos. 2022. Natural logic-\nguided autoregressive multi-hop document retrieval\nfor fact verification. In Proceedings of the 2022 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 6123-6135.\n\nMegha Chakraborty, Khusbu Pahwa, Anku Rani,\nAdarsh Mahor, Aditya Pakala, Arghya Sarkar,\nHarshit Dave, Ishan Paul, Janvita Reddy, Preethi Gu-\nrumurthy, Ritvik G, Samahriti Mukherjee, Shreyas\nChatterjee, Kinjal Sensharma, Dwip Dalal, Suryavar-\ndan S, Shreyash Mishra, Parth Patwa, Aman Chadha,\nAmit P. Sheth, and Amitava Das. 2023. FAC-\nTIFY3M: A benchmark for multimodal fact ver-\nification with explainability through 5w question-\nanswering. CoRR, abs/2306.05523.\n\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine\nBordes. 2017. Reading wikipedia to answer open-\ndomain questions. In Proceedings of the 55th Annual\nMeeting of the Association for Computational Lin-\nguistics (ACL), pages 1870-1879.\n\nJifan Chen, Aniruddh Sriram, Eunsol Choi, and Greg\nDurrett. 2022. Generating literal and implied sub-\nquestions to fact-check complex claims. In Proceed-\nings of the 2022 Conference on Empirical Methods\nin Natural Language Processing (EMNLP), pages\n3495-3516.\n\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan,\nHenrique Ponde de Oliveira Pinto, Jared Kaplan,\nHarrison Edwards, Yuri Burda, Nicholas Joseph,\nGreg Brockman, Alex Ray, Raul Puri, Gretchen\nKrueger, Michael Petrov, Heidy Khlaaf, Girish Sas-\ntry, Pamela Mishkin, Brooke Chan, Scott Gray,\nNick Ryder, Mikhail Pavlov, Alethea Power, Lukasz\nKaiser, Mohammad Bavarian, Clemens Winter,\nPhilippe Tillet, Felipe Petroski Such, Dave Cum-\nmings, Matthias Plappert, Fotios Chantzis, Eliza-\nbeth Barnes, Ariel Herbert-Voss, William Hebgen\nGuss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie\nTang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,\nWilliam Saunders, Christopher Hesse, Andrew N.\nCarr, Jan Leike, Joshua Achiam, Vedant Misra, Evan\nMorikawa, Alec Radford, Matthew Knight, Miles\nBrundage, Mira Murati, Katie Mayer, Peter Welinder,\nBob McGrew, Dario Amodei, Sam McCandlish, Ilya\nSutskever, and Wojciech Zaremba. 2021. Evaluat-\ning large language models trained on code. ArXiv\npreprint, abs/2107.03374.\n\nWenhu Chen, Hongmin Wang, Jianshu Chen, Yunkai\nZhang, Hong Wang, Shiyang Li, Xiyou Zhou, and\nWilliam Yang Wang. 2020. Tabfact: A large-scale\ndataset for table-based fact verification. In Proceed-\nings of 8th International Conference on Learning\nRepresentations (ICLR).\n\nAnton Chernyavskiy, Dmitry Ilvovsky, and Preslav\nNakov. 2021. Whatthewikifact: Fact-checking\nclaims against wikipedia. In Proceedings of the 30th\nACM International Conference on Information and\nKnowledge Management (CIKM), pages 4690-4695.\n", "vlm_text": "Limitations \nWe identify two main limitations of QAC HECK . First, several modules of our QAC HECK  currently utilize external API-based large language models, such as Instruct GP T. This reliance on external APIs tends to prolong the response time of our system. As a remedy, we are considering the integration of open-source, locally-run large language models like LLaMA ( Touvron et al. ,  2023 ). Secondly, the current scope of our QAC HECK  is confined to eval- uating  True/False  claims. Recognizing the signifi- cance of also addressing  Not Enough Information claims, we plan to devise strategies to incorporate these in upcoming versions of the system. \nEthics Statement \nThe use of large language models requires a signifi- cant amount of energy for computation for training, which contributes to global warming. Our work performs few-shot in-context learning instead of training models from scratch, so the energy foot- print of our work is less. The large language model (Instruct GP T) whose API we use for inference con- sumes significant energy. \nAcknowledgement \nThis project is supported by the Ministry of Edu- cation, Singapore, under its MOE AcRF TIER 3 Grant (MOE-MOET32022-0001). The computa- tional work for this article was partially performed on resources of the National Super computing Cen- tre, Singapore. \nReferences \nNaser Ahmadi, Joohyung Lee, Paolo Papotti, and Mo- hammed Saeed. 2019.  Explain able fact checking with probabilistic answer set programming . In  Pro- ceedings of the 2019 Truth and Trust Online Confer- ence (TTO) . \nMubashara Akhtar, Oana Cocarascu, and Elena Simperl. 2023.  Reading and reasoning over chart images for evidence-based automated fact-checking . In  Find- ings of the Association for Computational Linguistics (EACL) , pages 399–414. \nRami Aly, Zhijiang Guo, Michael Sejr Sch licht kru ll, James Thorne, Andreas Vlachos, Christos Christo dou lo poul os, Oana Cocarascu, and Arpit Mittal. 2021. FEVEROUS: Fact Extraction and VERification Over Unstructured and Structured in- formation . In  Proceedings of the Neural Information Processing Systems (NeurIPS) Track on Datasets and Benchmarks . \nRami Aly and Andreas Vlachos. 2022.  Natural logic- guided auto regressive multi-hop document retrieval for fact verification . In  Proceedings of the 2022 Con- ference on Empirical Methods in Natural Language Processing (EMNLP) , pages 6123–6135. \nMegha Chakra bor ty, Khusbu Pahwa, Anku Rani, Adarsh Mahor, Aditya Pakala, Arghya Sarkar, Harshit Dave, Ishan Paul, Janvita Reddy, Preethi Gu- rumurthy, Ritvik G, Samahriti Mukherjee, Shreyas Chatterjee, Kinjal Sensharma, Dwip Dalal, Suryavar- dan S, Shreyash Mishra, Parth Patwa, Aman Chadha, Amit P. Sheth, and Amitava Das. 2023. FAC- TIFY3M: A benchmark for multimodal fact ver- ification with explain ability through 5w question- answering .  CoRR , abs/2306.05523. \nDanqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. 2017.  Reading wikipedia to answer open- domain questions . In  Proceedings of the 55th Annual Meeting of the Association for Computational Lin- guistics (ACL) , pages 1870–1879. \nJifan Chen, Aniruddh Sriram, Eunsol Choi, and Greg Durrett. 2022.  Generating literal and implied sub- questions to fact-check complex claims . In  Proceed- ings of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pages 3495–3516. \nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sas- try, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cum- mings, Matthias Plappert, Fotios Chantzis, Eliza- beth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021.  Evaluat- ing large language models trained on code .  ArXiv preprint , abs/2107.03374. \nWenhu Chen, Hongmin Wang, Jianshu Chen, Yunkai Zhang, Hong Wang, Shiyang Li, Xiyou Zhou, and William Yang Wang. 2020.  Tabfact: A large-scale dataset for table-based fact verification . In  Proceed- ings of 8th International Conference on Learning Representations (ICLR) . \nAnton Cher nya v ski y, Dmitry Ilvovsky, and Preslav Nakov. 2021. What the wiki fact: Fact-checking claims against wikipedia . In  Proceedings of the 30th ACM International Conference on Information and Knowledge Management (CIKM) , pages 4690–4695. "}
{"page": 7, "image_path": "doc_images/2310.07609v1_7.jpg", "ocr_text": "Hyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang,\nMostafa Dehghani, Siddhartha Brahma, Albert Web-\nson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suz-\ngun, Xinyun Chen, Aakanksha Chowdhery, Sharan\nNarang, Gaurav Mishra, Adams Yu, Vincent Y. Zhao,\nYanping Huang, Andrew M. Dai, Hongkun Yu, Slav\nPetrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam\nRoberts, Denny Zhou, Quoc V. Le, and Jason Wei.\n2022. Scaling instruction-finetuned language models.\nCoRR, abs/2210.11416.\n\nShangbin Feng, Vidhisha Balachandran, Yuyang Bai,\nand Yulia Tsvetkov. 2023. Factkb: Generalizable fac-\ntuality evaluation using language models enhanced\nwith factual knowledge. CoRR, abs/2305.08281.\n\nZhijiang Guo, Michael Sejr Schlichtkrull, and Andreas\nVlachos. 2022. A survey on automated fact-checking.\nTransactions of the Association for Computational\nLinguistics (TACL), 10:178-206.\n\nYichen Jiang, Shikha Bordia, Zheng Zhong, Charles\nDognin, Maneesh Singh, and Mohit Bansal. 2020.\nHoVer: A dataset for many-hop fact extraction and\nclaim verification. In Findings of the Association for\nComputational Linguistics (EMNLP), pages 3441-\n3460.\n\nShailza Jolly, Pepa Atanasova, and Isabelle Augenstein.\n2022. Generating fluent fact checking explanations\nwith unsupervised post-editing. Information, 13:500.\n\nJiho Kim, Sungjin Park, Yeonsu Kwon, Yohan Jo, James\nThorne, and Edward Choi. 2023. Factkg: Fact veri-\nfication via reasoning on knowledge graphs. In Pro-\nceedings of the 61st Annual Meeting of the Associ-\nation for Computational Linguistics (ACL), pages\n16190-16206.\n\nNeema Kotonya and Francesca Toni. 2020. Explainable\nautomated fact-checking for public health claims. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 7740-7754.\n\nJimmy Lin, Xueguang Ma, Sheng-Chieh Lin, Jheng-\nHong Yang, Ronak Pradeep, and Rodrigo Frassetto\nNogueira. 2021. Pyserini: A python toolkit for re-\nproducible information retrieval research with sparse\nand dense representations. In Proceedings of Inter-\nnational ACM SIGIR Conference on Research and\nDevelopment in Information Retrieval (SIGIR), pages\n2356-2362.\n\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized BERT pretraining\napproach. CoRR, abs/1907.11692.\n\nZhenghao Liu, Chenyan Xiong, Maosong Sun, and\nZhiyuan Liu. 2020. Fine-grained fact verification\nwith kernel graph attention network. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics (ACL), pages 7342-7351.\n\nXinyuan Lu, Liangming Pan, Qian Liu, Preslav Nakov,\nand Min-Yen Kan. 2023. SCITAB: A challeng-\ning benchmark for compositional reasoning and\nclaim verification on scientific tables. CoRR,\nabs/2305.13186.\n\nYi-Ju Lu and Cheng-Te Li. 2020. GCAN: graph-aware\nco-attention networks for explainable fake news de-\ntection on social media. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational\nLinguistics (ACL), pages 505-514.\n\nGiovanni Da San Martino, Shaden Shaar, Yifan Zhang,\nSeunghak Yu, Alberto Barrén-Cedeifio, and Preslav\nNakov. 2020. Prta: A system to support the analysis\nof propaganda techniques in the news. In Proceed-\nings of the 58th Annual Meeting of the Association for\nComputational Linguistics: System Demonstrations\n(ACL), pages 287-293.\n\nVan-Hoang Nguyen, Kazunari Sugiyama, Preslav\nNakov, and Min-Yen Kan. 2020. FANG: leveraging\nsocial context for fake news detection using graph\nrepresentation. In Proceedings of the 29th ACM Inter-\nnational Conference on Information and Knowledge\nManagement (CIKM), pages 1165-1174.\n\nOpenAI. 2023.\nabs/2303.08774.\n\nGPT-4 technical report. CoRR,\n\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll L. Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray,\nJohn Schulman, Jacob Hilton, Fraser Kelton, Luke\nMiller, Maddie Simens, Amanda Askell, Peter Welin-\nder, Paul F. Christiano, Jan Leike, and Ryan Lowe.\n2022. Training language models to follow instruc-\ntions with human feedback. In Proceedings of the\n36th Conference on Neural Information Processing\nSystems (NeurIPS).\n\nLiangming Pan, Xiaobao Wu, Xinyuan Lu, Anh Tuan\nLuu, William Yang Wang, Min-Yen Kan, and Preslav\nNakov. 2023. Fact-checking complex claims with\nprogram-guided reasoning. In Proceedings of the\n61st Annual Meeting of the Association for Computa-\ntional Linguistics (ACL), pages 6981-7004.\n\nFabio Petroni, Aleksandra Piktus, Angela Fan, Patrick\nS. H. Lewis, Majid Yazdani, Nicola De Cao, James\nThorne, Yacine Jernite, Vladimir Karpukhin, Jean\nMaillard, Vassilis Plachouras, Tim Rocktischel, and\nSebastian Riedel. 2021. KILT: a benchmark for\nknowledge intensive language tasks. In Proceed-\nings of the 2021 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies (NAACL-\nHLT), pages 2523-2544.\n\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. Squad: 100, 000+ questions\nfor machine comprehension of text. In Proceedings\nof the 2016 Conference on Empirical Methods in\nNatural Language Processing (EMNLP), pages 2383-\n2392.\n", "vlm_text": "Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Web- son, Shixiang Shane Gu, Zhuyun Dai, Mirac Suz- gun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Y. Zhao, Yanping Huang, Andrew M. Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. 2022.  Scaling instruction-finetuned language models . CoRR , abs/2210.11416. \nShangbin Feng, Vidhisha Bala chandra n, Yuyang Bai, and Yulia Tsvetkov. 2023.  Factkb: General iz able fac- tuality evaluation using language models enhanced with factual knowledge .  CoRR , abs/2305.08281. \nZhijiang Guo, Michael Sejr Sch licht kru ll, and Andreas Vlachos. 2022.  A survey on automated fact-checking . Transactions of the Association for Computational Linguistics (TACL) , 10:178–206. \nYichen Jiang, Shikha Bordia, Zheng Zhong, Charles Dognin, Maneesh Singh, and Mohit Bansal. 2020. HoVer: A dataset for many-hop fact extraction and claim verification . In  Findings of the Association for Computational Linguistics (EMNLP) , pages 3441– 3460. \nShailza Jolly, Pepa Atanasova, and Isabelle Augenstein. 2022.  Generating fluent fact checking explanations with unsupervised post-editing .  Information , 13:500. \nJiho Kim, Sungjin Park, Yeonsu Kwon, Yohan Jo, James Thorne, and Edward Choi. 2023.  Factkg: Fact veri- fication via reasoning on knowledge graphs . In  Pro- ceedings of the 61st Annual Meeting of the Associ- ation for Computational Linguistics (ACL) , pages 16190–16206. \nNeema Kotonya and Francesca Toni. 2020.  Explain able automated fact-checking for public health claims . In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pages 7740–7754. \nJimmy Lin, Xueguang Ma, Sheng-Chieh Lin, Jheng- Hong Yang, Ronak Pradeep, and Rodrigo Frassetto Nogueira. 2021.  Pyserini: A python toolkit for re- producible information retrieval research with sparse and dense representations . In  Proceedings of Inter- national ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR) , pages 2356–2362.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man- dar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Z ett le moyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized BERT pre training approach .  CoRR , abs/1907.11692. \nZhenghao Liu, Chenyan Xiong, Maosong Sun, and Zhiyuan Liu. 2020.  Fine-grained fact verification with kernel graph attention network . In  Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL) , pages 7342–7351. \nXinyuan Lu, Liangming Pan, Qian Liu, Preslav Nakov, and Min-Yen Kan. 2023. SCITAB: A challeng- ing benchmark for compositional reasoning and claim verification on scientific tables . CoRR , abs/2305.13186. \nYi-Ju Lu and Cheng-Te Li. 2020.  GCAN: graph-aware co-attention networks for explain able fake news de- tection on social media . In  Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL) , pages 505–514. \nGiovanni Da San Martino, Shaden Shaar, Yifan Zhang, Seunghak Yu, Alberto Barrón-Cedeño, and Preslav Nakov. 2020.  Prta: A system to support the analysis of propaganda techniques in the news . In  Proceed- ings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations (ACL) , pages 287–293. \nVan-Hoang Nguyen, Kazunari Sugiyama, Preslav Nakov, and Min-Yen Kan. 2020.  FANG: leveraging social context for fake news detection using graph representation . In  Proceedings of the 29th ACM Inter- national Conference on Information and Knowledge Management (CIKM) , pages 1165–1174. \nOpenAI. 2023. GPT-4 technical report . CoRR , abs/2303.08774. \nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welin- der, Paul F. Christiano, Jan Leike, and Ryan Lowe. 2022.  Training language models to follow instruc- tions with human feedback . In  Proceedings of the 36th Conference on Neural Information Processing Systems (NeurIPS) . \nLiangming Pan, Xiaobao Wu, Xinyuan Lu, Anh Tuan Luu, William Yang Wang, Min-Yen Kan, and Preslav Nakov. 2023.  Fact-checking complex claims with program-guided reasoning . In  Proceedings of the 61st Annual Meeting of the Association for Computa- tional Linguistics (ACL) , pages 6981–7004. \nFabio Petroni, Aleksandra Piktus, Angela Fan, Patrick S. H. Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard, Vassilis Plachouras, Tim Rock t s chel, and Sebastian Riedel. 2021. KILT: a benchmark for knowledge intensive language tasks . In  Proceed- ings of the 2021 Conference of the North American Chapter of the Association for Computational Lin- guistics: Human Language Technologies (NAACL- HLT) , pages 2523–2544. \nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,   ${000+}$   questions for machine comprehension of text . In  Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pages 2383– 2392. "}
{"page": 8, "image_path": "doc_images/2310.07609v1_8.jpg", "ocr_text": "Stephen E. Robertson and Hugo Zaragoza. 2009. The\nprobabilistic relevance framework: BM25 and be-\nyond. Journal of Foundations and Trends in Informa-\ntion Retrieval, 3(4):333-389.\n\nChris Samarinas, Wynne Hsu, and Mong-Li Lee. 2021.\nImproving evidence retrieval for automated explain-\nable fact-checking. In Proceedings of the 2021 Con-\nference of the North American Chapter of the As-\nsociation for Computational Linguistics: Human\nLanguage Technologies: System Demonstrations,\n(NAACL-HLT), pages 84-91.\n\nAvi Sil, Jaydeep Sen, Bhavani Iyer, Martin Franz, Kshi-\ntij Fadnis, Mihaela Bornea, Sara Rosenthal, J. Scott\nMcCarley, Rong Zhang, Vishwajeet Kumar, Yulong\nLi, Md. Arafat Sultan, Riyaz Bhat, Jiirgen Brof,\nRadu Florian, and Salim Roukos. 2023. Primeqa:\nThe prime repository for state-of-the-art multilingual\nquestion answering research and development. In\nProceedings of the 61st Annual Meeting of the Associ-\nation for Computational Linguistics: System Demon-\nstrations (ACL), pages 51-62.\n\nZhigqing Sun, Xuezhi Wang, Yi Tay, Yiming Yang, and\nDenny Zhou. 2023. Recitation-augmented language\nmodels. In Proceedings of the 11th International\nConference on Learning Representations (ICLR).\n\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Roziére, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurélien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023. Llama: Open\nand efficient foundation language models. CoRR,\nabs/2302.13971.\n\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed H. Chi, Quoc Le, and Denny Zhou. 2022.\nChain of thought prompting elicits reasoning in large\nlanguage models. CoRR, abs/2201.11903.\n\nRui Xing, Shraey Bhatia, Timothy Baldwin, and\nJey Han Lau. 2022. Automatic explanation genera-\ntion for climate science claims. In Proceedings of the\nThe 20th Annual Workshop of the Australasian Lan-\nguage Technology Association (ALTA), pages 122-\n129.\n\nWenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu,\nMingxuan Ju, Soumya Sanyal, Chenguang Zhu,\nMichael Zeng, and Meng Jiang. 2023. Generate\nrather than retrieve: Large language models are\nstrong context generators. In Proceedings of the 11th\nInternational Conference on Learning Representa-\ntions (ICLR).\n\nYifan Zhang, Giovanni Da San Martino, Alberto Barrén-\nCedefio, Salvatore Romeo, Jisun An, Haewoon Kwak,\nTodor Staykovski, Israa Jaradat, Georgi Karadzhov,\nRamy Baly, Kareem Darwish, James R. Glass, and\nPreslav Nakov. 2019. Tanbih: Get to know what you\nare reading. In Proceedings of the 2019 Conference\non Empirical Methods in Natural Language Process-\ning and the 9th International Joint Conference on\n\nNatural Language Processing: System Demonstra-\ntions (EMNLP-IJCNLP), pages 223-228.\n\nWanjun Zhong, Jingjing Xu, Duyu Tang, Zenan Xu,\nNan Duan, Ming Zhou, Jiahai Wang, and Jian Yin.\n2020. Reasoning over semantic-level graph for fact\nchecking. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics\n(ACL), pages 6170-6180.\n", "vlm_text": "Stephen E. Robertson and Hugo Zaragoza. 2009.  The probabilistic relevance framework: BM25 and be- yond .  Journal of Foundations and Trends in Informa- tion Retrieval , 3(4):333–389. \nChris Samarinas, Wynne Hsu, and Mong-Li Lee. 2021. Improving evidence retrieval for automated explain- able fact-checking . In  Proceedings of the 2021 Con- ference of the North American Chapter of the As- sociation for Computational Linguistics: Human Language Technologies: System Demonstrations, (NAACL-HLT) , pages 84–91. \nNatural Language Processing: System Demonstra- tions (EMNLP-IJCNLP) , pages 223–228. \nWanjun Zhong, Jingjing Xu, Duyu Tang, Zenan Xu, Nan Duan, Ming Zhou, Jiahai Wang, and Jian Yin. 2020.  Reasoning over semantic-level graph for fact checking . In  Proceedings of the 58th Annual Meet- ing of the Association for Computational Linguistics (ACL) , pages 6170–6180. \nAvi Sil, Jaydeep Sen, Bhavani Iyer, Martin Franz, Kshi- tij Fadnis, Mihaela Bornea, Sara Rosenthal, J. Scott McCarley, Rong Zhang, Vishwajeet Kumar, Yulong Li, Md. Arafat Sultan, Riyaz Bhat, Jürgen Broß, Radu Florian, and Salim Roukos. 2023.  Primeqa: The prime repository for state-of-the-art multilingual question answering research and development . In Proceedings of the 61st Annual Meeting of the Associ- ation for Computational Linguistics: System Demon- strations (ACL) , pages 51–62. \nZhiqing Sun, Xuezhi Wang, Yi Tay, Yiming Yang, and Denny Zhou. 2023.  Recitation-augmented language models . In  Proceedings of the 11th International Conference on Learning Representations (ICLR) . \nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurélien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023.  Llama: Open and efficient foundation language models .  CoRR , abs/2302.13971. \nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed H. Chi, Quoc Le, and Denny Zhou. 2022. Chain of thought prompting elicits reasoning in large language models .  CoRR , abs/2201.11903. \nRui Xing, Shraey Bhatia, Timothy Baldwin, and Jey Han Lau. 2022.  Automatic explanation genera- tion for climate science claims . In  Proceedings of the The 20th Annual Workshop of the Australasian Lan- guage Technology Association (ALTA) , pages 122– 129. \nWenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu, Mingxuan Ju, Soumya Sanyal, Chenguang Zhu, Michael Zeng, and Meng Jiang. 2023. Generate rather than retrieve: Large language models are strong context generators . In  Proceedings of the 11th International Conference on Learning Representa- tions (ICLR) . \nYifan Zhang, Giovanni Da San Martino, Alberto Barrón- Cedeño, Salvatore Romeo, Jisun An, Haewoon Kwak, Todor Staykovski, Israa Jaradat, Georgi Karadzhov, Ramy Baly, Kareem Darwish, James R. Glass, and Preslav Nakov. 2019.  Tanbih: Get to know what you are reading . In  Proceedings of the 2019 Conference on Empirical Methods in Natural Language Process- ing and the 9th International Joint Conference on "}
{"page": 9, "image_path": "doc_images/2310.07609v1_9.jpg", "ocr_text": "A Prompts\n\nA.1 Prompts for Claim Verifier\n\nClaim = Superdrag and Collective Soul are\nboth rock bands.\n\nWe already know the following:\n\nQuestion 1 = Is Superdrag a rock band?\nAnswer 1 = Yes\n\nCan we know whether the claim is\n\ntrue or false now? Yes or no?\n\nPrediction = No, we cannot know.\n\nClaim = Superdrag and Collective Soul are\nboth rock bands.\nWe already know the following:\n\nQuestion 1 = Is Superdrag a rock band?\n\nAnswer 1 = Yes\n\nQuestion 2 = Is Collective Soul a rock band?\nAnswer 2 = Yes\n\nCan we know whether the claim is\ntrue or false now? Yes or no?\nPrediction = Yes, we can know.\n\n<1 demonstrations in total>\nClaim = [[CLAIM]]\n\nClaim = CLAIM\n\nWe already know the following:\nCLQA_CONTEXTS]]\n\nCan we know whether the claim is\ntrue or false now? Yes or no?\nPrediction =\n\nQuestion = Is Collective Soul a rock band?\nAnswer = Yes\n\nDoes the QA pair have additional\nknowledge useful for verifying the claim?\nThe answer: Yes\n\n<10 demonstrations in total>\n\nClaim = [[CLAIM]]\n\nWe already know the following:\n[CLQA_CONTEXTS]]\n\nNow we further know:\n\nCLINEW_QA_PAIR]]\n\nDoes the QA pair have additional\nknowledge useful for verifying the claim?\nThe answer:\n\nA.4_ Prompts for Reasoner\n\nA.2 Prompts for Question Generation\n\nPrompts for the initial question generation\n\nClaim = Superdrag and Collective Soul are\nboth rock bands.\n\nTo verify the above claim, we can\n\nfirst ask a simple question:\n\nQuestion = Is Superdrag a rock band?\n\n<1 demonstrations in total>\nClaim = [ECLAIM]]\n\nTo verify the above claim, we can\nfirst ask a simple question:\nQuestion =\n\nContexts:\nQl: When Lars Onsager won the Nobel Prize?\nAl: 1968\n\nQ2: When was Lars Onsager born?\n\nA2: 1903\n\nClaim = Lars Onsager won the Nobel Prize\nwhen he was 3@ years old.\n\nIs this claim true or false?\n\nAnswer :\nLars Onsager won the Nobel Prize in 1968.\nLars Onsager was born in 1903.\n\nTherefore, the final answer is: False.\n\n<10 demonstrations in total>\nContexts:\n\nCLCONTEXTS]]\n\nClaim = [[CLAIM]]\n\nIs this claim true or false?\nAnswer :\n\nTherefore, the final answer is\n\nPrompts for the follow-up question generation\n\nClaim = Superdrag and Collective Soul are\nboth rock bands.\n\nWe already know the following:\n\nQuestion 1 = Is Superdrag a rock band?\nAnswer 1 = Yes\n\nTo verify the claim, what is the\n\nnext question we need to know the\n\nanswer to?\n\nQuestion 2 = Is Collective Soul a rock band?\n\n<1 demonstrations in total>\nClaim = [ECLAIM]]\n\nWe already know the following:\nCEQA_CONTEXTSJJ\n\nTo verify the claim, what is the\nnext question we need to know the\nanswer to?\n\nQuestion [[Q_INDEX]] =\n\nA.3 Prompts for Validator\n\nClaim = Superdrag and Collective Soul are\nboth rock bands.\n\nWe already know the following:\n\nQuestion = Is Superdrag a rock band?\nAnswer = Yes\n\nNow we further know:\n\n", "vlm_text": "A Prompts \nA.1 Prompts for Claim Verifier \nClaim  $=$   Superdrag and Collective Soul are both rock bands. We already know the following: Question   $\\begin{array}{r l}{1}&{{}=}\\end{array}$   Is Superdrag a rock band? Answer   $\\begin{array}{r l}{1}&{{}=}\\end{array}$   Yes Can we know whether the claim is true or false now? Yes or no? Prediction    $=\\textsf{N o}$   , we cannot know. Claim  $=$   Superdrag and Collective Soul are both rock bands. We already know the following: Question   $\\begin{array}{r l}{1}&{{}=}\\end{array}$   Is Superdrag a rock band? Answer   $\\begin{array}{r l}{1}&{{}=}\\end{array}$   Yes Question   $\\begin{array}{r l}{^2}&{{}=}\\end{array}$  = Is Collective Soul a rock band? Answer   $\\begin{array}{r l}{^2}&{{}=}\\end{array}$  = Yes Can we know whether the claim is true or false now? Yes or no? Prediction    $=$   Yes , we can know.\n\n  $<\\!1\\,\\emptyset$   demonstrations in total >\n\n -------- Claim  $=$   [[CLAIM]] Claim  $=$   CLAIM We already know the following: [[ QA CONTEXTS ]] Can we know whether the claim is true or false now? Yes or no? Prediction    $=$  \nA.2 Prompts for Question Generation Prompts for the initial question generation \nClaim  $=$   Superdrag and Collective Soul are both rock bands. To verify the above claim , we can first ask a simple question: Question    $=$   Is Superdrag a rock band?\n\n  $<\\!1\\,\\emptyset$   demonstrations in total >\n\n -------- Claim  $=$   [[CLAIM]] To verify the above claim , we can first ask a simple question: Question  $=$  \nQuestion  $=$   Is Collective Soul a rock band? Answer  $=$   Yes Does the QA pair have additional knowledge useful for verifying the claim? The answer: Yes\n\n  $<\\!1\\,\\emptyset$   demonstrations in total >\n\n -------- Claim    $=$   [[ CLAIM ]] We already know the following: [[ QA CONTEXTS ]] Now we further know: [[ NEW QA PAIR ]] Does the QA pair have additional knowledge useful for verifying the claim? The answer: \nA.4 Prompts for Reasoner \nContexts: Q1: When Lars Onsager won the Nobel Prize? A1: 1968 Q2: When was Lars Onsager born? A2: 1903 Claim  $=$   Lars Onsager won the Nobel Prize when he was 30 years old. Is this claim true or false? Answer : Lars Onsager won the Nobel Prize in 1968. Lars Onsager was born in 1903. Therefore , the final answer is: False.\n\n  $<\\!1\\,\\emptyset$   demonstrations in total >\n\n -------- Contexts: [[ CONTEXTS ]] Claim    $=$   [[ CLAIM ]] Is this claim true or false? Answer : Therefore , the final answer is \nPrompts for the follow-up question generation \nClaim  $=$   Superdrag and Collective Soul are both rock bands. We already know the following: Question   $\\begin{array}{r l}{1}&{{}=}\\end{array}$   Is Superdrag a rock band? Answer   $\\begin{array}{r l}{1}&{{}=}\\end{array}$   Yes To verify the claim , what is the next question we need to know the answer to? Question   $^{2\\ }=$   Is Collective Soul a rock band?\n\n  $<\\!1\\,\\emptyset$   demonstrations in total >\n\n -------- Claim  $=$   [[CLAIM]] We already know the following: [[ QA CONTEXTS ]] To verify the claim , what is the next question we need to know the answer to? Question  [[ Q_INDEX ]] = \nA.3 Prompts for Validator \nClaim  $=$   Superdrag and Collective Soul are both rock bands. We already know the following: Question    $=$   Is Superdrag a rock band? Answer  $=$   Yes Now we further know: "}
