{"page": 0, "image_path": "doc_images/2305.13186v3_0.jpg", "ocr_text": "arXiv:2305.13186v3 [cs.CL] 23 Oct 2023\n\nScITAB: A Challenging Benchmark for Compositional Reasoning\nand Claim Verification on Scientific Tables\n\nXinyuan Lu*!? Liangming Pan** Qian Liu‘\n\nPreslav Nakov®\n\nMin-Yen Kan?\n\nISEP Program, NUS Graduate School ? National University of Singapore\n3University of California, Santa Barbara “Sea AILab ®MBZUAI\n\nluxinyuan@u.nus.edu\n\nliangmingpan@ucsb. edu\n\nliuqian@sea.com\n\npreslav.nakov@mbzuai.ac.ae kanmy@comp.nus.edu.sg\n\nAbstract\n\nCurrent scientific fact-checking benchmarks ex-\nhibit several shortcomings, such as biases aris-\ning from crowd-sourced claims and an over-\nreliance on text-based evidence. We present\nScITAB, a challenging evaluation dataset con-\nsisting of 1.2K expert-verified scientific claims\nthat 1) originate from authentic scientific publi-\ncations and 2) require compositional reason-\ning for verification. The claims are paired\nwith evidence-containing scientific tables an-\nnotated with labels. Through extensive evalu-\nations, we demonstrate that SCITAB poses a\nsignificant challenge to state-of-the-art models,\nincluding table-based pretraining models and\nlarge language models. All models except GPT-\n4 achieved performance barely above random\nguessing. Popular prompting techniques, such\nas Chain-of-Thought, do not achieve much\nperformance gains on SCITAB. Our analy-\nsis uncovers several unique challenges posed\nby ScITAB, including table grounding, claim\nambiguity, and compositional reasoning. Our\ncodes and data are publicly available at https:\n//github.com/XinyuanLu@@/SciTab.\n\n1 Introduction\n\nScientific fact-checking is a crucial process that in-\nvolves validating the accuracy of scientific claims\nby cross-referencing them with established scien-\ntific literature, research, or data (Guo et al., 2022).\nThis process is crucial for preserving the integrity\nof scientific information, preventing the spread of\nmisinformation, and fostering public trust in re-\nsearch findings. However, the sheer volume of\nscientific data and claims can be overwhelming for\nmanual fact-checking, making automated scientific\nfact-checking an imperative research area of NLP.\n\nScientific fact-checking has advanced signifi-\ncantly with benchmarks including Sci-Fact (Wad-\nden et al., 2020), Sci-Fact Open (Wadden et al.,\n2022), and COVID-Fact (Saakyan et al., 2021).\n\n“Equal Contribution.\n\nHowever, these datasets still exhibit several limi-\ntations. First, the claims are crowd-sourced rather\nthan collected from real scientific papers. This\nleads to problems such as bias in human annotation,\na lack of diversity, and shallow claims that do not\nreflect the complexity of scientific reasoning. For\nexample, most claims in Sci-Fact can be validated\nby a single sentence in a paper’s abstract, which\noversimplifies the scientific discourse. Second, the\nclaims in the existing benchmarks are solely vali-\ndated against text-based evidence, primarily paper\nabstracts. However, in many scientific processes,\nclaims are intrinsically tied to quantitative experi-\nmental data, commonly presented in tables and fig-\nures. This disparity highlights a significant gap be-\ntween the existing benchmarks and real-world sci-\nentific fact-checking needs. To bridge these gaps,\na dataset that 1) compiles real-world claims from\nscientific papers, and 2) includes original scientific\ndata such as tables and figures, is needed.\n\nIn this paper, we propose a novel dataset SC-\nITAB, which fulfills these stated criteria. It contains\n1,225 challenging scientific claims, each demand-\ning compositional reasoning for verification using\nscientific tables. Our data is derived from the Sci-\nGen dataset (Moosavi et al., 2021), a resource that\nincludes scientific tables and claims crawled from\narXiv.org. We first manually filter out the check-\nworthy scientific claims from the raw data. Follow-\ning this, we employ a strategy of human—model\ncollaboration, as depicted in Figure 2, to generate\nclaims that are either contradicted or unverifiable\nbased on the table’s content. Figure 1 shows a\nclaim from SCITAB and the corresponding reason-\ning process to verify it. Compared with existing\nbenchmarks, SCITAB is closer to real-world scien-\ntific fact-checking in terms of more realistic claims\nand table-based evidence. Through data analysis,\nwe further show that the claims in SCITAB neces-\nsitate a more comprehensive and nuanced set of\nreasoning skills for verification, e.g., numerical rea-\n", "vlm_text": "S CI T AB : A Challenging Benchmark for Compositional Reasoning and Claim Verification on Scientific Tables \nXinyuan  $\\mathbf{L}\\mathbf{u}^{*1,2}$    Liangming Pan ∗ 3 Qian Liu 4 Preslav Nakov 5 Min-Yen Kan 2 \n1 ISEP Program, NUS Graduate School 2  National University of Singapore 3 University of California, Santa Barbara 4 Sea AI Lab 5 MBZUAI \nluxinyuan@u.nus.edu liang ming pan@ucsb.edu liuqian@sea.com preslav.nakov@mbzuai.ac.ae kanmy@comp.nus.edu.sg \nAbstract \nCurrent scientific fact-checking benchmarks ex- hibit several shortcomings, such as biases aris- ing from crowd-sourced claims and an over- reliance on text-based evidence. We present S CI T AB , a challenging evaluation dataset con- sisting of   $1.2\\mathrm{K}$   expert-verified scientific claims that 1) originate from authentic scientific publi- cations and 2) require compositional reason- ing for verification. The claims are paired with evidence-containing scientific tables an- notated with labels. Through extensive evalu- ations, we demonstrate that S CI T AB  poses a significant challenge to state-of-the-art models, including table-based pre training models and large language models. All models except GPT- 4 achieved performance barely above random guessing. Popular prompting techniques, such as Chain-of-Thought, do not achieve much performance gains on S CI T AB . Our analy- sis uncovers several unique challenges posed by S CI T AB , including table grounding, claim ambiguity, and compositional reasoning. Our codes and data are publicly available at  https: //github.com/X in yuan Lu 00/SciTab . \n1 Introduction \nScientific fact-checking is a crucial process that in- volves validating the accuracy of scientific claims by cross-referencing them with established scien- tific literature, research, or data ( Guo et al. ,  2022 ). This process is crucial for preserving the integrity of scientific information, preventing the spread of misinformation, and fostering public trust in re- search findings. However, the sheer volume of scientific data and claims can be overwhelming for manual fact-checking, making automated scientific fact-checking an imperative research area of NLP. Scientific fact-checking has advanced signifi- cantly with benchmarks including Sci-Fact ( Wad- den et al. ,  2020 ), Sci-Fact Open ( Wadden et al. , However, these datasets still exhibit several limi- tations. First, the claims are  crowd-sourced  rather than collected from real scientific papers. This leads to problems such as bias in human annotation, a lack of diversity, and shallow claims that do not reflect the complexity of scientific reasoning. For example, most claims in Sci-Fact can be validated by a single sentence in a paper’s abstract, which oversimplifies the scientific discourse. Second, the claims in the existing benchmarks are solely vali- dated against  text-based evidence , primarily paper abstracts. However, in many scientific processes, claims are intrinsically tied to quantitative experi- mental data, commonly presented in tables and fig- ures. This disparity highlights a significant gap be- tween the existing benchmarks and real-world sci- entific fact-checking needs. To bridge these gaps, a dataset that 1) compiles real-world claims from scientific papers, and 2) includes original scientific data such as tables and figures, is needed. \n\nIn this paper, we propose a novel dataset S C - I T AB , which fulfills these stated criteria. It contains 1,225 challenging scientific claims, each demand- ing compositional reasoning for verification using scientific tables. Our data is derived from the Sci- Gen dataset ( Moosavi et al. ,  2021 ), a resource that includes scientific tables and claims crawled from arXiv.org. We first manually filter out the check- worthy scientific claims from the raw data. Follow- ing this, we employ a strategy of human–model collaboration, as depicted in Figure  2 , to generate claims that are either contradicted or un verifiable based on the table’s content. Figure  1  shows a claim from S CI T AB  and the corresponding reason- ing process to verify it. Compared with existing benchmarks, S CI T AB  is closer to real-world scien- tific fact-checking in terms of more realistic claims and table-based evidence. Through data analysis, we further show that the claims in S CI T AB  neces- sitate a more comprehensive and nuanced set of reasoning skills for verification,  e.g.,  numerical rea- "}
{"page": 1, "image_path": "doc_images/2305.13186v3_1.jpg", "ocr_text": "Paper: When Choosing Plausible Alternatives, Clever Hans can be Clever\nPaper ID: 1911.00225v1\n\n55.3% 9.40%\n\nwas 55% 61.8% 11.0%\nto 82% 40.2% 16.4%\nthe 85% 38.8% 17.0%\na 106% 57.5% 21.2%\n\nTable 2: Applicability (App.), Productivity (Prod.) and Coverage (Cov.) of the various\nwords in the alternatives of the COPA dev set.\n\nSupported Claim\n\nA's productivity of 57.5% expresses that it appears in 7.5% more\n\noften than expected by random chance.\n\nRefuted Claim\n\nA's productivity of 57.5% expresses that it appears in 9.5% more\n\noften than expected by random chance.\n\nNot Enough Info Claim\n\nThe low performance of “to” can be explained by the fact that it\nis responsible for only 4.6% of the inference in the training set.\n\nin 47%\n\nA's productivity of 57.5% expresses that it appears in 7.5%\nmore often than expected by random chance.\n\nClosed-domain knowledge: eae\n\n|\n\nProductivity corresponds to Prod. column |\n\nQ Simple lookup\n\nA’s productivity is 57.5%\n\nThe number of random chance is 50%\n—_—-\n\nThe subtraction result between 57.5% and 50% is 7.5%\n\nFact checker : Supported (V)\n\nes Reasoning Graph\n\nFigure 1: An example of our SCITAB dataset (left) and its corresponding reasoning graph (right). Each data entry\ncontains paper name, paper id, table, one claim, and its corresponding label (Supported, Refuted, Not Enough Info).\n\nsoning and commonsense knowledge, etc.\n\nWe employ SCITAB as a diagnostic dataset for\nbenchmarking the zero-shot and in-context learn-\ning performance for a wide range of state-of-the-\nart models, including table-based pretraining mod-\nels, encoder—decoder models, open source lan-\nguage models, and API-based language models.\nWe observe that all models, with the exception\nof GPT-4, can only achieve marginally superior\nF, scores than random guessing, which under-\nscores the challenging nature of SCITAB. Addi-\ntionally, established prompting methods like Chain-\nof-Thought (Wei et al., 2022) and Program-of-\nThought (Chen et al., 2022) which typically en-\nhance performance across most reasoning tasks,\ndo not bring performance gain on SCITAB. Our\nerror analysis sheds light on several unique chal-\nlenges in SCITAB that may lead to this, such as\ntable grounding, dealing with ambiguous claims,\nand compositional reasoning. We make our dataset\nfully accessible to the research community.\n\n2 The SCITAB Dataset\n\nWe adopt a human—model collaboration strategy\nto construct SCITAB, as shown in Figure 2. We\ndescribe the steps involved in data preparation (Sec-\ntion 2.1), automatic claim generation (Section 2.2),\nand manual claim verification (Section 2.3).\n\n2.1 Data Preparation\n\nWe use the publicly available SciGen (Moosavi\net al., 2021) dataset as our primary data source.\n\nThe dataset was created by crawling computer sci-\nence papers from arXiv. The tables and the texts\nexplaining the tables are extracted from the papers\n0 create (table, description) pairs for the task of\ndata-to-text generation. From all the table descrip-\nions of SciGen, we first filter the check-worthy\nscientific claims following the criteria established\nby Lee et al. (2009) for academic writing'. We\n‘ocus on the descriptions that serve the purpose\nof “highlighting and commenting on key data”,\ni.e., describing research findings based on the data\npresented in scientific tables. Given the task’s ob-\njective nature and to save the cost of human labor,\nwe hire a graduate student majoring in computer\nscience to manually select scientific claims based\non the aforementioned criteria using the user inter-\nface in Appendix A.2. This decision was based on\na pilot annotation which showed that a well-trained\nannotator can achieve over 95% accuracy in filter-\ning scientific claims. To safeguard the quality, we\ninclude an option to mark the claim as “Discard-It’s\nnot a claim, or it’s an incomplete, or not grammat-\nically correct sentence.” during the subsequent\nclaim verification process. Using this approach, we\nfiltered out 872 real-world scientific claims from\n1,301 table descriptions in the SciGen dataset.\n\n2.2 Automatic Claim Generation\n\nFalse Claims. A fact-checking dataset requires\nboth true and false claims. However, acquir-\ning false claims that naturally occur within well-\n\n‘Detailed criteria are given in Appendix A.1\n", "vlm_text": "The image illustrates an example from a dataset called S CI T AB, along with a reasoning graph. The left side includes a table from a paper titled \"When Choosing Plausible Alternatives, Clever Hans can be Clever\" with Paper ID: 1911.00225v1. The table shows data on Applicability (App.), Productivity (Prod.), and Coverage (Cov.) of certain words. \n\nThere's a claim about \"A’s productivity of 57.5% expressing that it appears in 7.5% more often than expected by random chance.\" The claim is supported because the reasoning graph verifies that productivity corresponds to the Prod. column. Using commonsense and closed-domain knowledge, the graph establishes the productivity as 57.5% and random chance as 50%. Subtraction confirms the claim, leading to the conclusion that the fact checker supports the claim as valid. \n\nThe image presents different interpretations: one claim is supported, another refuted, and one marked as not having enough info.\nsoning and commonsense knowledge,  etc . \nWe employ S CI T AB  as a diagnostic dataset for benchmarking the zero-shot and in-context learn- ing performance for a wide range of state-of-the- art models, including table-based pre training mod- els, encoder–decoder models, open source lan- guage models, and API-based language models. We observe that all models, with the exception of GPT-4, can only achieve marginally superior  $F_{1}$   scores than random guessing, which under- scores the challenging nature of S CI T AB . Addi- tionally, established prompting methods like Chain- of-Thought ( Wei et al. ,  2022 ) and Program-of- Thought ( Chen et al. ,  2022 ) which typically en- hance performance across most reasoning tasks, do not bring performance gain on S CI T AB . Our error analysis sheds light on several unique chal- lenges in S CI T AB  that may lead to this, such as table grounding, dealing with ambiguous claims, and compositional reasoning. We make our dataset fully accessible to the research community. \n2 The S CI T AB  Dataset \nWe adopt a  human–model collaboration  strategy to construct S CI T AB , as shown in Figure  2 . We describe the steps involved in data preparation (Sec- tion  2.1 ), automatic claim generation (Section  2.2 ), and manual claim verification (Section  2.3 ). \n2.1 Data Preparation \nWe use the publicly available SciGen ( Moosavi et al. ,  2021 ) dataset as our primary data source. \nThe dataset was created by crawling computer sci- ence papers from arXiv. The tables and the texts explaining the tables are extracted from the papers to create (table, description) pairs for the task of data-to-text generation. From all the table descrip- tions of SciGen, we first filter the check-worthy scientific claims following the criteria established by  Lee et al.  ( 2009 ) for academic writing 1 . We focus on the descriptions that serve the purpose of “highlighting and commenting on key data”, i.e. , describing research findings based on the data presented in scientific tables. Given the task’s ob- jective nature and to save the cost of human labor, we hire a graduate student majoring in computer science to manually select scientific claims based on the aforementioned criteria using the user inter- face in Appendix  A.2 . This decision was based on a pilot annotation which showed that a well-trained annotator can achieve over   $95\\%$   accuracy in filter- ing scientific claims. To safeguard the quality, we include an option to mark the claim as “Discard-It’s not a claim, or it’s an incomplete, or not grammat- ically correct sentence.” during the subsequent claim verification process. Using this approach, we filtered out 872 real-world scientific claims from 1,301 table descriptions in the SciGen dataset. \n2.2 Automatic Claim Generation \nFalse Claims. A fact-checking dataset requires both true and false claims. However, acquir- ing false claims that naturally occur within well- "}
{"page": 2, "image_path": "doc_images/2305.13186v3_2.jpg", "ocr_text": "@ Automatic Claim Generation @) Manual Claim Verification\n\n_InsuctepT\n\nata\n\nHuman verifier\n\nFree- = claim ——————— Not aan Info claim C”\n\nES\n\nma extraction\n\na\n\nTexts\n\nSciGen dataset\n\n@ Data Preparation\n\nClaim with opposite meaning C’\n\nOriginal claim C\n\nata\n\nHuman verifier Supported claim C\n\nRefuted claim C’\n\n| InstructGPT\n\nFigure 2: The human-model collaboration construction process of SCITAB, which contains three steps: 1) data\npreparation (including data preprocessing and claim extraction) 2) automatic claim generation (including refuted\nand Not Enough Info claim generation) and 3) manual claim verification.\n\nverified scientific publications is a challenging\ntask. Following SciFact (Wadden et al., 2020) and\nCOVID-Fact (Saakyan et al., 2021), we seek to\ncreate false claims by generating counter-claims of\nthe original true claims. Unlike previous works\nthat purely rely on crowd-workers to compose\ncounter-claims — a process that is costly and prone\nto annotation artifacts — we leverage the strong\ninstruction-following capabilities of large language\nmodels (LLMs) to assist humans in generating can-\ndidate counter-claims. Specifically, we prompt\nInstructGPT (Ouyang et al., 2022) with the orig-\ninal claim and the instruction: Please modify the\noriginal claims to convey the opposite meaning\nwith minimum edits. To foster a varied set of gen-\nerated claims, we include five diverse in-context\nexamples and employ a high decoding tempera-\nture setting of 0.7. By mandating minimal edits,\nwe ensure that the counter-claims remain lexically\nclose to the original claims, which is crucial in\npreventing fact-checking models from relying on\nsuperficial lexical patterns for verification.\n\nUnverifiable Claims. To construct a more chal-\nlenging dataset, we also integrate claims that are\nunverifiable with the table information (labeled as\nNot Enough Info, NEI). We leverage InstructGPT\nto generate candidate NEI claims by prompting the\nmodel with the original table and the instruction:\nPlease generate 5 relevant scientific claims based\non the information in the table. This process yields\na diverse set of free-formed claims that enrich the\ndiversity of SCITAB. However, as LLMs tend to\ngenerate content that might not always be grounded\nin the provided data, many of the generated claims\nturn out to be relevant but unverifiable with respect\n\nto the table. We adopt manual verification (elabo-\nrated in Section 2.3) to select them as NEI claims.\n\n2.3. Manual Claim Verification\n\nWe subsequently employ a human verification pro-\ncess for two purposes: first, to verify the quality of\nthe 872 false claims and 900 NEI claims that were\ngenerated by InstructGPT; second, to critically re-\nview the 872 real-world scientific claims obtained\nin Section 2.1. This task involves selecting claims\nthat can be verified exclusively based on the infor-\nmation presented in the table, without the need for\nadditional context from the associated paper.\n\nFor each pair of the true claim c and its corre-\nsponding generated counter-claim c’, we ask the\nannotator to choose one of the following three op-\ntions: (A) c is not exclusively supported by the\ntable, (B) c is exclusively supported by the table,\nbut c’ is not refuted by the table, and (C) c is not\nexclusively supported by the table, and c’ is not\nrefuted by the table. For each candidate NEI claim,\nwe ask the annotator to judge whether it is unverifi-\nable with respect to the table.\n\nAnnotator Recruitment. Given that our data\nsource is from computer science papers, we recruit\nuniversity students majoring in computer science\nwith basic math and programming backgrounds\nfor annotation. We ask each annotator to fill in\na questionnaire, including their age, department,\nmaximum workload per week, etc. After that, we\nprovide a training session to ensure they understand\nthe task and can use the annotation interfaces (Ap-\npendix B.2 and B.3). We also give them three\nsamples to test their understanding. We recruit\ntwelve annotators that passed the training session.\n", "vlm_text": "The image illustrates the human-model collaboration process for constructing S CI T AB, consisting of three steps:\n\n1. **Data Preparation:**\n   - Involves data preprocessing and claim extraction from the SciGen dataset, organizing data into tables and texts.\n\n2. **Automatic Claim Generation:**\n   - Utilizes InstructGPT to generate claims, including creating free-formed claims and claims with opposite meanings for refuted and \"Not Enough Info\" scenarios.\n\n3. **Manual Claim Verification:**\n   - Human verifiers assess the claims, resulting in categorization as \"Supported,\" \"Refuted,\" or \"Not Enough Info.\"\n\nEach step in the process is connected visually, showing the flow from data preparation to claim verification.\nverified scientific publications is a challenging task. Following SciFact ( Wadden et al. ,  2020 ) and COVID-Fact ( Saakyan et al. ,  2021 ), we seek to create false claims by generating counter-claims of the original true claims. Unlike previous works that purely rely on crowd-workers to compose counter-claims — a process that is costly and prone to annotation artifacts — we leverage the strong instruction-following capabilities of large language models (LLMs) to assist humans in generating can- didate counter-claims. Specifically, we prompt Instruct GP T  ( Ouyang et al. ,  2022 ) with the orig- inal claim and the instruction:  Please modify the original claims to convey the opposite meaning with minimum edits . To foster a varied set of gen- erated claims, we include five diverse in-context examples and employ a high decoding tempera- ture setting of  0 . 7 . By mandating minimal edits, we ensure that the counter-claims remain lexically close to the original claims, which is crucial in preventing fact-checking models from relying on superficial lexical patterns for verification. \nUn verifiable Claims. To construct a more chal- lenging dataset, we also integrate claims that are un verifiable  with the table information (labeled as Not Enough Info,  NEI ). We leverage  Instruct GP T to generate candidate NEI claims by prompting the model with the original table and the instruction: Please generate 5 relevant scientific claims based on the information in the table . This process yields a diverse set of free-formed claims that enrich the diversity of S CI T AB . However, as LLMs tend to generate content that might not always be grounded in the provided data, many of the generated claims turn out to be  relevant but un verifiable  with respect to the table. We adopt manual verification (elabo- rated in Section  2.3 ) to select them as NEI claims. \n\n2.3 Manual Claim Verification \nWe subsequently employ a human verification pro- cess for two purposes: first, to verify the quality of the 872 false claims and 900 NEI claims that were generated by  Instruct GP T ; second, to critically re- view the 872 real-world scientific claims obtained in Section  2.1 . This task involves selecting claims that can be verified exclusively based on the infor- mation presented in the table, without the need for additional context from the associated paper. \nFor each pair of the true claim    $c$   and its corre- sponding generated counter-claim    $c^{\\prime}$  , we ask the annotator to choose one of the following three op- tions: (A)    $c$   is not exclusively supported by the table, (B)  $c$   is exclusively supported by the table, but    $c^{\\prime}$    is not refuted by the table, and (C)    $c$   is not exclusively supported by the table, and    $c^{\\prime}$    is not refuted by the table. For each candidate NEI claim, we ask the annotator to judge whether it is unverifi- able with respect to the table. \nAnnotator Recruitment. Given that our data source is from computer science papers, we recruit university students majoring in computer science with basic math and programming backgrounds for annotation. We ask each annotator to fill in a questionnaire, including their age, department, maximum workload per week, etc. After that, we provide a training session to ensure they understand the task and can use the annotation interfaces (Ap- pendix  B.2  and  B.3 ). We also give them three samples to test their understanding. We recruit twelve annotators that passed the training session. "}
{"page": 3, "image_path": "doc_images/2305.13186v3_3.jpg", "ocr_text": "Statistics TabFact FEVEROUS SEM-TAB-FACTS SCITAB\nDomain Wiki Tables = Wiki Tables Scientific Articles Scientific Articles\nAnnotator AMT AMT AMT Experts\nMax. Reasoning Hops 7 2 1 11\n\nSupported 54% 56% 58% 37%\nVeracity Refuted 46% 39% 38% 34%\nNEI _ 5% 4% 29%\nTotal # of Claims 117,854 87,026 5,715 1,225\nAvg. claims per table 7AL 0.07 5.27 6.16\n\nTable 1: Comparison of SCITAB to three recent table fact verification datasets: TabFact (Chen et al., 2020),\nFEVEROUS (Aly et al., 2021), and SEM-TAB-FACTS (Wang et al., 2021). The table presents statistics related\nto the domain, annotator (AMT represents Amazon Mechanical Turk), maximum reasoning hops, veracity labels\npercentage of each dataset, the total number of claims, and average claims per table.\n\nIn compliance with ethical guidelines, we ensure\nfair compensation for the annotators. Each claim\nannotation is reimbursed at a rate of 0.37 USD,\nresulting in an hourly wage of 11.2 USD?.\n\nQuality Control and Annotator Agreement. To\nensure the quality of the annotation, we apply strict\nquality control procedures following the guide-\nlines outlined in the Dataset Statement (Bender and\nFriedman, 2018). We assign two different anno-\ntators to perform a two-round annotation for each\nclaim, while two authors review and resolve any\nidentified errors or issues. To measure the inter-\nannotator agreement, we use Cohen’s Kappa (Co-\nhen, 1960). Our inter-annotator agreement is 0.630\nfor the false claim verification task (872 claims\nin total) and 0.719 for the NEI claim verification\ntask (900 claims in total). Both values indicate\nsubstantial agreement among the annotators.\n\n3 Data Analysis\n\nTable 1 shows the statistics of our SCITAB dataset\nand the comparison with three existing table fact-\nchecking datasets: TabFact (Chen et al., 2020),\nFEVEROUS (Aly et al., 2021), and SEM-TAB-\nFACTS (Wang et al., 2021). Compared with these\ndatasets, SCITAB is 1) annotated by domain ex-\nperts rather than crowd-sourced workers, 2) con-\ntains more challenging claims that require up to\n11 reasoning steps for verification, and 3) has a\nmore balanced distribution of veracity labels and\na higher percentage of NEI claims. We conduct a\nmore in-depth analysis of SCITAB as follows.\n\n3.1 Reasoning Analysis\n\nReasoning Types. To study the nature of reason-\ning involved in fact-checking claims in SCITAB,\n\nThe payment is fair and aligned with the guideline for\ndataset creation (Bender and Friedman, 2018).\n\nwe adapt the set of table-based reasoning cate-\ngories from INFOTABS (Gupta et al., 2020) to\ndefine 14 atomic reasoning types, as shown in Ta-\nble 2. Among them, “‘closed-domain knowledge”\nand “open-domain knowledge” are specially de-\nsigned for SCITAB. Closed-domain knowledge\nrefers to obtaining background information from\nthe table caption or title, e.g., knowing that “Prod.”\nrefers to “Productivity” from the table caption in\nFigure 1. Open-domain knowledge refers to com-\nmonsense knowledge not presented in the table,\ne.g., the relationship between precision and recall.\nGiven the designed reasoning types, we manually\nanalyze 100 samples in SCITAB, by annotating the\ngraph of reasoning steps for verifying each claim.\nWe identify 476 atomic reasoning steps from the\n100 analyzed samples and show the proportion for\neach reasoning type in Table 2. We observe that SC-\nITAB has a multifaceted complex range of reason-\ning types and a high proportion of claims requiring\ndifferent types of domain knowledge.\n\nReasoning Depth. We further measure the rea-\nsoning depth (the number of required reasoning\nsteps) for each claim and show the reasoning depth\ndistribution in Figure 3. We find that the analyzed\nclaims have an average depth of 4.76 and a max-\nimum depth of 11. Moreover, 86% of the claims\nrequiring 3 or more reasoning steps, which demon-\nstrates the complexity of reasoning in SCITAB.\n\n=\n\nReasoning Graph. We showcase the reasoning\ngraph for the example in Figure | on the right side\nof the figure. Verifying this claim requires vari-\nous types of reasoning including: 1) background\nknowledge from the table caption: “productivity”\ncorresponds to the “Prod.” column in the table; 2)\ncommonsense knowledge: “random chance” means\n50% accuracy; 3) simple lookup: “A’s productiv-\nity” refers to the cell located at the last row and\n", "vlm_text": "The table shows statistics comparing four datasets: TabFact, FEVEROUS, SEM-TAB-FACTS, and SciTab. \n\n- **Domains**: TabFact and FEVEROUS use Wiki Tables, while SEM-TAB-FACTS and SciTab use Scientific Articles.\n- **Annotators**: AMT (Amazon Mechanical Turk) is used for TabFact, FEVEROUS, and SEM-TAB-FACTS, while SciTab uses Experts.\n- **Max. Reasoning Hops**: TabFact has 7, FEVEROUS has 2, SEM-TAB-FACTS has 1, and SciTab has 11.\n- **Veracity**:\n  - TabFact: 54% Supported, 46% Refuted\n  - FEVEROUS: 56% Supported, 39% Refuted, 5% NEI (Not Enough Information)\n  - SEM-TAB-FACTS: 58% Supported, 38% Refuted, 4% NEI\n  - SciTab: 37% Supported, 34% Refuted, 29% NEI\n- **Total # of Claims**: TabFact has 117,854, FEVEROUS has 87,026, SEM-TAB-FACTS has 5,715, and SciTab has 1,225.\n- **Avg. Claims per Table**: TabFact has 7.11, FEVEROUS has 0.07, SEM-TAB-FACTS has 5.27, and SciTab has 6.16.\nIn compliance with ethical guidelines, we ensure fair compensation for the annotators. Each claim annotation is reimbursed at a rate of 0.37 USD, resulting in an hourly wage of   $11.2\\,\\mathrm{USD}^{2}$  . \nQuality Control and Annotator Agreement. To ensure the quality of the annotation, we apply strict quality control procedures following the guide- lines outlined in the Dataset Statement ( Bender and Friedman ,  2018 ). We assign two different anno- tators to perform a two-round annotation for each claim, while two authors review and resolve any identified errors or issues. To measure the inter- annotator agreement, we use Cohen’s Kappa ( Co- hen ,  1960 ). Our inter-annotator agreement is 0.630 for the false claim verification task (872 claims in total) and 0.719 for the NEI claim verification task (900 claims in total). Both values indicate substantial agreement among the annotators. \n3 Data Analysis \nTable  1  shows the statistics of our S CI T AB  dataset and the comparison with three existing table fact- checking datasets: TabFact ( Chen et al. ,  2020 ), FEVEROUS ( Aly et al. ,  2021 ), and SEM-TAB- FACTS ( Wang et al. ,  2021 ). Compared with these datasets, S CI T AB  is 1) annotated by domain ex- perts rather than crowd-sourced workers, 2) con- tains more challenging claims that require up to 11 reasoning steps for verification, and 3) has a more balanced distribution of veracity labels and a higher percentage of NEI claims. We conduct a more in-depth analysis of S CI T AB  as follows. \n3.1 Reasoning Analysis \nReasoning Types. To study the nature of reason- ing involved in fact-checking claims in S CI T AB , we adapt the set of table-based reasoning cate- gories from INFOTABS ( Gupta et al. ,  2020 ) to define 14 atomic reasoning types, as shown in Ta- ble  2 . Among them, “closed-domain knowledge” and “open-domain knowledge” are specially de- signed for S CI T AB . Closed-domain knowledge refers to obtaining background information from the table caption or title,  e.g. , knowing that “Prod.” refers to “Productivity” from the table caption in Figure  1 . Open-domain knowledge refers to com- monsense knowledge not presented in the table, e.g. , the relationship between precision and recall. Given the designed reasoning types, we manually analyze 100 samples in S CI T AB , by annotating the graph of reasoning steps for verifying each claim. We identify 476 atomic reasoning steps from the 100 analyzed samples and show the proportion for each reasoning type in Table  2 . We observe that S C - I T AB  has a multifaceted complex range of reason- ing types and a high proportion of claims requiring different types of domain knowledge. \n\nReasoning Depth. We further measure the  rea- soning depth  (the number of required reasoning steps) for each claim and show the reasoning depth distribution in Figure  3 . We find that the analyzed claims have an average depth of 4.76 and a max- imum depth of 11. Moreover,   $86\\%$   of the claims requiring 3 or more reasoning steps, which demon- strates the complexity of reasoning in S CI T AB . \nReasoning Graph. We showcase the reasoning graph for the example in Figure  1  on the right side of the figure. Verifying this claim requires vari- ous types of reasoning including: 1)  background knowledge from the table caption : “productivity” corresponds to the “Prod.” column in the table; 2) commonsense knowledge : “random chance” means  $50\\%$   accuracy; 3)  simple lookup : “A’s productiv- ity” refers to the cell located at the last row and "}
{"page": 4, "image_path": "doc_images/2305.13186v3_4.jpg", "ocr_text": "Function Names Descriptions Prop. (%)\nSimple lookup Retrieve the value for a specific cell. 20.6\nComparison Compare two numbers. 19.5\nClosed-domain knowledge _ Extract information from context sentences in the table caption or article. 12.1\nOpen-domain knowledge Extract additional information required by domain experts. 5.3\nCommonsense knowledge _ Extract commonsense knowledge necessary for claim verification. 5.3\nSubtract Perform subtraction of two numbers. 5.3\nDivide Perform division of two numbers. 5.3\nRank Determine the rank of a set of numbers. 5.3\nDifferent / Same Determine if two numbers are different or the same. 5.3\nAdd Calculate the sum of two numbers. 4.0\nMax / Min Retrieve the maximum or minimum number from a set of numbers. 3.1\nCol / Rowname Retrieve the column or row name from the table. 3.1\nTrend same/different Determine the trend for two columns or rows, whether they are the same or different. 2.9\nSet check Verify if a value belongs to a set of numbers. 2.9\nTable 2: The function names, descriptions, and their proportions in our SCITAB dataset.\n\n25 a greater diversity in refuted claims compared to\nS20 18 20 Sci-Fact. Besides common error types such as “in-\nz 1s 15 15 correct calculation results” (41.7%), there are also\n3 unique types of errors that are more reflective of the\n3 10 é& 7. complexities in real-world scientific claims. For\non 5 39 i example, 33.33% of the refuted claims contain “in-\n\n= correct approximation words”, and 10.0% are cases\n\n123 45 67 8 9 1011\nReasoning Steps\n\nFigure 3: The distribution histogram of reasoning steps\nin our SCITAB dataset. The x-axis is the reasoning steps\nin each claim, and the y-axis is the frequency for each\nreasoning step. The shallow claims (with 1-2 reasoning\nsteps) are highlighted in red, while the deep claims (with\n3+ reasoning steps) are highlighted in blue.\n\nthe “Prod.” column; and 4) numerical reasoning:\nthe difference between 57.5% and 50% is 7.5%.\nThis case study provides further insights into the\ncomplexity and variety of reasoning involved in\nScITAB, revealing the difficulty of the dataset.\n\n3.2. Refuted and NEI Claims Analysis\n\nOne potential risk of model-generated claims is\nthat they may lack diversity and exhibit the same\npattern. For example, in the Sci-Fact (Wadden\net al., 2020) dataset where the refuted claims are\ngenerated by flapping the meaning of the original\ntrue claims, we found that out of 100 randomly\nsampled refuted claims, 85 simply negated the orig-\ninal claim by adding negation words such as “not”\n(more details in Appendix C). To evaluate the diver-\nsity of claims for our SCITAB dataset, we randomly\nselect 60 refuted claims and then manually anno-\ntate their reasons for refutation. Results are shown\nin Table 3 (top half). We find that SCITAB exhibits\n\nwhere “the claim is partially right”, consistent with\nthe fact that ambiguity and half-truths are common\nphenomena in scientific discourse. Additional ex-\namples of refuted claims are in Appendix E.\n\nThe NEI claims (bottom half; Table 3) also ex-\nhibit diverse reasoning patterns. The two most\ncommon features for unverifiable claims are insuf-\nficient evidence in the table and the lack of back-\nground knowledge. The lack of closed-domain\nknowledge is another reason for NEI, where ad-\nditional information in the paper is necessary to\nverify the claim. Other reasons include the use of\nvague pronouns (e.g., “it”, “this”) brings ambiguity\nto the claim. These distinct refuted and NEI reason-\ning types highlight the unique features of SCITAB,\nmaking it a more comprehensive and realistic rep-\nresentation of the challenges faced in real-world\nscientific fact-checking.\n\n4 Experiment\n\nWe formally define the task of scientific table-based\nfact-checking as follows. A scientific table T con-\nsists of a table caption P and the table content\n({Tijli < Rr,j < Cr} with Rr rows and Cr\ncolumns, where T;,; is the content in the (é, j)th\ncell. Given a claim C describing a fact to be ver-\nified against the table 7, a table fact-checking\nmodel F predicts a label Y to verify whether C\nis supported, refuted, or can not be verified by the\n", "vlm_text": "The table lists functions related to data analysis tasks, along with descriptions and the proportion (%) of each function's usage. Here's a breakdown:\n\n- **Simple lookup (20.6%)**: Retrieve the value for a specific cell.\n- **Comparison (19.5%)**: Compare two numbers.\n- **Closed-domain knowledge (12.1%)**: Extract information from context sentences in the table caption or article.\n- **Open-domain knowledge (5.3%)**: Extract additional information required by domain experts.\n- **Commonsense knowledge (5.3%)**: Extract commonsense knowledge necessary for claim verification.\n- **Subtract (5.3%)**: Perform subtraction of two numbers.\n- **Divide (5.3%)**: Perform division of two numbers.\n- **Rank (5.3%)**: Determine the rank of a set of numbers.\n- **Different / Same (5.3%)**: Determine if two numbers are different or the same.\n- **Add (4.0%)**: Calculate the sum of two numbers.\n- **Max / Min (3.1%)**: Retrieve the maximum or minimum number from a set of numbers.\n- **Col / Rowname (3.1%)**: Retrieve the column or row name from the table.\n- **Trend same/different (2.9%)**: Determine the trend for two columns or rows, whether they are the same or different.\n- **Set check (2.9%)**: Verify if a value belongs to a set of numbers.\nThe image is a histogram depicting the distribution of reasoning steps in a dataset known as SCI TAB. \n\n- The x-axis represents the number of reasoning steps involved in each claim, ranging from 1 to 11.\n- The y-axis indicates the frequency of each reasoning step as a percentage.\n\nThe histogram bars are color-coded:\n- Red bars indicate \"shallow\" claims, which involve 1–2 reasoning steps. Specifically, 6% of claims involve 1 reasoning step, and 8% involve 2 reasoning steps.\n- Blue bars represent \"deep\" claims, which have 3 or more reasoning steps. The distribution is as follows: 15% for 3 steps, 18% for 4 steps, 20% for 5 steps, 15% for 6 steps, 7% for 7 steps, 5% for 8 steps, 3% for 9 steps, 2% for 10 steps, and 1% for 11 steps.\nthe “Prod.” column; and 4)  numerical reasoning : the difference between   $57.5\\%$   and   $50\\%$   is   $7.5\\%$  . This case study provides further insights into the complexity and variety of reasoning involved in S CI T AB , revealing the difficulty of the dataset. \n3.2 Refuted and NEI Claims Analysis \nOne potential risk of model-generated claims is that they may lack diversity and exhibit the same pattern. For example, in the Sci-Fact ( Wadden et al. ,  2020 ) dataset where the refuted claims are generated by flapping the meaning of the original true claims, we found that out of 100 randomly sampled refuted claims, 85 simply negated the orig- inal claim by adding negation words such as “not” (more details in Appendix  C ). To evaluate the diver- sity of claims for our S CI T AB  dataset, we randomly select 60 refuted claims and then manually anno- tate their reasons for refutation. Results are shown in Table  3  (top half). We find that S CI T AB  exhibits a greater diversity in refuted claims compared to Sci-Fact. Besides common error types such as “in- correct calculation results”   $(41.7\\%)$  , there are also unique types of errors that are more reflective of the complexities in real-world scientific claims. For example,   $33.33\\%$   of the refuted claims contain “in- correct approximation words”, and   $10.0\\%$   are cases where “the claim is partially right”, consistent with the fact that ambiguity and half-truths are common phenomena in scientific discourse. Additional ex- amples of refuted claims are in Appendix  E . \n\nThe NEI claims (bottom half; Table  3 ) also ex- hibit diverse reasoning patterns. The two most common features for un verifiable claims are insuf- ficient evidence in the table and the lack of back- ground knowledge. The lack of closed-domain knowledge is another reason for NEI, where ad- ditional information in the paper is necessary to verify the claim. Other reasons include the use of vague pronouns ( e.g. , “it”, “this”) brings ambiguity to the claim. These distinct refuted and NEI reason- ing types highlight the unique features of S CI T AB , making it a more comprehensive and realistic rep- resent ation of the challenges faced in real-world scientific fact-checking. \n4 Experiment \nWe formally define the task of scientific table-based fact-checking as follows.  scientific table    $\\mathcal{T}$  con- sists of a table caption  P  and the table content  $(\\{T_{i,j}|i\\,\\leq\\,R_{T},j\\,\\leq\\,C_{T}\\}$   with    $R_{T}$   rows a  $C_{T}$  columns, where  $T_{i,j}$   is the content in the  (  $(i,j)$  th cell. Given a claim    $C$   describing a fact to be ver- ified a inst the table    $\\mathcal{T}$  , a  table fact-checkin model  F  predicts a label    $y$   to verify whether  C is supported, refuted, or can not be verified by the "}
{"page": 5, "image_path": "doc_images/2305.13186v3_5.jpg", "ocr_text": "Refuted Reasons Prop. (%)\nThe calculation result is wrong. 41.7\nThe approximation word is wrong. 33.3\nThe claim is partially right. 10.0\nThe values in the claim do not match. 8.3\nThe operation type is wrong. 6.7\nNEI Reasons Prop. (%)\nThe claim does not have enough matching evidence. 33.3\nThe claim lacks open-domain knowledge. 25.0\nThe claim lacks closed-domain knowledge. 15.0\nThe claim refers to another table. 11.7\nThe claim contains vague pronouns. 8.3\nThe claim omits specific information. 6.7\n\nTable 3: Refuted and NEI reasons and their estimated\nproportions (Prop.) in SCITAB.\n\ninformation in T.\n\nConsidering the real-world situation that large-\nscale training data is either not available or expen-\nsive to collect, we focus on the zero-shot/in-context\nevaluation where the model can only access ze-\nro/few in-domain data from SCITAB. To this end,\nwe randomly hold out 5 tables with 25 claims as\nmodel-accessible data and use the rest of the data\nas the unseen test set. This also prevents the model\nfrom learning spurious features that lead to over-\nestimated performance (Schuster et al., 2019).\n\n4.1 Models\n\nWe conduct a comprehensive evaluation of SCITAB\nfor various models, including table-based pretrain-\ning models, encoder—decoder models, open source\nLLMs, and closed source LLMs. We also study the\nhuman performance to analyze the upper bounds\non SCITAB.\n\nTable-based LLMs. These are pre-trained trans-\nformer models fine-tuned on tabular data. We\nchoose three different models: 1) TAPAS (Herzig\net al., 2020), a BERT-based model fine-tuned on\nmillions of tables from English Wikipedia and cor-\nresponding texts, 2) TAPEX (Liu et al., 2022b), a\nmodel that fine-tunes BART (Lewis et al., 2020) on\na large-scale synthetic dataset generated by synthe-\nsizing executable SQL queries and their execution\noutputs, and 3) TAPEX-Zero (Liu et al., 2023b), an\nenlarged version of TAPEX. For TAPAS and TAPEX,\nwe use their fine-tuned version on TabFact (Chen\net al., 2020) for table fact-checking.\n\nEncoder—Decoder LLMs. We also use en-\ncoder—decoder models where both the input and\noutput are sequences of tokens. To adapt the model\nto take the table as input, we flatten the table as\na sequence following Chen et al. (2020). The in-\n\nput is then formulated as [T; P; C; Q], where T is\nthe linearized table, and @ is a question template\n“Based on the information in the table, is the above\nclaim true? A) True B) False C) Unknown?”. We\nchoose FLAN-T5 (Chung et al., 2022), an improved\nTS model (Raffel et al., 2020) pre-trained on more\nthan 1.8K tasks with instruction tuning, which has\nachieved strong zero-shot/in-context performance\non other fact-checking benchmarks.\n\nOpen Source LLMs. We also evaluate the per-\nformance of state-of-the-art open source LLMs,\nincluding 1) LLaMA (Touvron et al., 2023), the first\nopen-source model by Meta AI; 2) Alpaca (Taori\net al., 2023), an instruction-following language\nmodel fine-tuned on LLaMA; and 3) Vicuna (Chi-\nang et al., 2023), the arguably best-performed open-\nsource LLMs that claimed to achieve 90% quality\ncompared to OpenAI ChatGPT. We use the same\ninput format as in the encoder-decoder model.\n\nClosed Source LLMs. These are closed-source\nLLMs that require API calls for inference, includ-\ning InstructGPT (text-davinci-003) (Ouyang\net al., 2022) and GPT-4 (OpenAI, 2023). We\nevaluate the setting that directly predicts the la-\nbel and the Chain-of-Thought (CoT) (Wei et al.,\n2022) setting, which generates explanations be-\nfore predicting the final label. We also include the\nProgram-of-Thoughts (PoT) (Chen et al., 2022)\nmodel that has shown strong ability in solving com-\nplex numerical reasoning tasks. It first parses the\nreasoning steps as Python programs and then exe-\ncutes them on a Python interpreter to derive accu-\nrate answers. Since most claims in SCITAB also re-\nquire numerical reasoning, we want to test whether\nprogram-guided reasoning can be extended to table-\nbased fact-checking.\n\nHuman Performance. To examine how humans\nperform on our SCITAB dataset, we hired an anno-\ntator from our candidate annotators pool, following\nthe same training procedure as other annotators.\nIn the case of 2-class classification, we randomly\nselected 40 samples: 20 each for supported and\nrefuted claims. For 3-class classification, we ran-\ndomly selected 60 random samples, ensuring an\neven distribution of 20 samples across the three la-\nbel categories (supported, refuted, and not enough\ninformation). The annotator took approximately\n1.5 hours for the 2-class fact-checking task and 2\nhours for the 3-class setting. We report the Macro-\nFI scores at the bottom of Table 4.\n", "vlm_text": "The table contains two sections titled \"Refuted Reasons\" and \"NEI Reasons,\" each listing reasons along with their proportional percentages:\n\n**Refuted Reasons:**\n- The calculation result is wrong: 41.7%\n- The approximation word is wrong: 33.3%\n- The claim is partially right: 10.0%\n- The values in the claim do not match: 8.3%\n- The operation type is wrong: 6.7%\n\n**NEI Reasons:**\n- The claim does not have enough matching evidence: 33.3%\n- The claim lacks open-domain knowledge: 25.0%\n- The claim lacks closed-domain knowledge: 15.0%\n- The claim refers to another table: 11.7%\n- The claim contains vague pronouns: 8.3%\n- The claim omits specific information: 6.7%\ninformation in  $\\mathcal{T}$  \nConsidering the real-world situation that large- scale training data is either not available or expen- sive to collect, we focus on the  zero-shot/in-context evaluation where the model can only access ze- ro/few in-domain data from S CI T AB . To this end, we randomly hold out 5 tables with 25 claims as model-accessible data and use the rest of the data as the unseen test set. This also prevents the model from learning spurious features that lead to over- estimated performance ( Schuster et al. ,  2019 ). \n4.1 Models \nWe conduct a comprehensive evaluation of S CI T AB for various models, including table-based pretrain- ing models, encoder–decoder models, open source LLMs, and closed source LLMs. We also study the human performance to analyze the upper bounds on S CI T AB . \nTable-based LLMs. These are pre-trained trans- former models fine-tuned on tabular data. We choose three different models: 1)  TAPAS  ( Herzig et al. ,  2020 ), a BERT-based model fine-tuned on millions of tables from English Wikipedia and cor- responding texts, 2)  TAPEX  ( Liu et al. ,  2022b ), a model that fine-tunes BART ( Lewis et al. ,  2020 ) on a large-scale synthetic dataset generated by synthe- sizing executable SQL queries and their execution outputs, and 3)  TAPEX-Zero  ( Liu et al. ,  2023b ), an enlarged version of  TAPEX . For  TAPAS  and  TAPEX , we use their fine-tuned version on TabFact ( Chen et al. ,  2020 ) for table fact-checking. \nEncoder–Decoder LLMs. We also use en- coder–decoder models where both the input and output are sequences of tokens. To adapt the model to take the table as input, we flatten the table as a sequence following  Chen et al.  ( 2020 ). The in- put is then formulated as    $[\\Tilde{T};P;C;Q]$  , where  $\\tilde{T}$   is the linearized table, and    $Q$   is a question template “Based on the information in the table, is the above claim true? A) True B) False C) Unknown?”. We choose  FLAN-T5  ( Chung et al. ,  2022 ), an improved T5 model ( Raffel et al. ,  2020 ) pre-trained on more than 1.8K tasks with instruction tuning, which has achieved strong zero-shot/in-context performance on other fact-checking benchmarks. \n\nOpen Source LLMs. We also evaluate the per- formance of state-of-the-art open source LLMs, including 1)  LLaMA  ( Touvron et al. ,  2023 ), the first open-source model by Meta AI; 2)  Alpaca  ( Taori et al. ,  2023 ), an instruction-following language model fine-tuned on LLaMA; and 3)  Vicuna  ( Chi- ang et al. ,  2023 ), the arguably best-performed open- source LLMs that claimed to achieve  $90\\%$   quality compared to OpenAI  ChatGPT . We use the same input format as in the encoder-decoder model. \nClosed Source LLMs. These are closed-source LLMs that require API calls for inference, includ- ing  Instruct GP T (text-davinci-003)  ( Ouyang et al. ,  2022 ) and  GPT-4  ( OpenAI ,  2023 ). We evaluate the setting that directly predicts the la- bel and the  Chain-of-Thought  (CoT) ( Wei et al. , 2022 ) setting, which generates explanations be- fore predicting the final label. We also include the Program-of-Thoughts  (PoT) ( Chen et al. ,  2022 ) model that has shown strong ability in solving com- plex numerical reasoning tasks. It first parses the reasoning steps as Python programs and then exe- cutes them on a Python interpreter to derive accu- rate answers. Since most claims in S CI T AB  also re- quire numerical reasoning, we want to test whether program-guided reasoning can be extended to table- based fact-checking. \nHuman Performance. To examine how humans perform on our S CI T AB  dataset, we hired an anno- tator from our candidate annotators pool, following the same training procedure as other annotators. In the case of 2-class classification, we randomly selected 40 samples: 20 each for supported and refuted claims. For 3-class classification, we ran- domly selected 60 random samples, ensuring an even distribution of 20 samples across the three la- bel categories (supported, refuted, and not enough information). The annotator took approximately 1.5 hours for the 2-class fact-checking task and 2 hours for the 3-class setting. We report the Macro- F1 scores at the bottom of Table  4 . "}
{"page": 6, "image_path": "doc_images/2305.13186v3_6.jpg", "ocr_text": "Models # of Para. Zero-shot In-Context\n2-class 3-class 2-class 3-class\nTAPAS-large (Tabfact) (Herzig et al., 2020) 340M 50.30 — — —_\nI. Table-based TAPEX-large (Tabfact) (Liuetal., 2022b) 400M 56.06 — — —_\nLLMs TAPEX-Zero-large (Liu et al., 2023b) 780M 48.28 29.72 4244 23.47\nTAPEX-Zero-XL (Liu et al., 2023b) 3B 49.77 34.30 42.12 25.62\nFlan-T5-base (Chung et al., 2022) 250M 47.38 26.56 44.82 24.09\nII. Encoder-Decoder Flan-T5-large (Chung et al., 2022) 780M _ —51.58 32.55 49.62 27.30\nLLMs FLan-T5-XL (Chung et al., 2022) 3B «(52.41 38.05 48.05 29.21\nFlan-T5-XXL (Chung et al., 2022) 11B) 59.60 34.91 60.48 34.04\nAlpaca-7B (Taori et al., 2023) 7B 37.22 27.59 40.46 =. 28.95\nIII. Open source Vicuna-7B (Chiang et al., 2023) 7B 63.62 32.47 50.35 34.26\nLLMs Vicuna-1 3B (Chiang et al., 2023) 13B 41.82 29.63 55.11 35.16\nLLaMA-7B (Touvron et al., 2023) TB 49.05 32.26 45.24 27.17\nLLaMA-13B (Touvron et al., 2023) 13B 53.97 37.18 44.39 32.66\nInstructGPT (Ouyang et al., 2022) 175B 68.44 4141 68.10 41.58\nTV. Close source InstructGPT+CoT (Ouyang et al., 2022) 175B _ _ 68.46 42.60\nLLMs PoT (Chen et al., 2022) 175B — — 63.79 —\nGPT-4 (OpenAl, 2023) — 78.22 6480 77.98 63.21\nGPT-4+CoT (OpenAl, 2023) _ —_ —_ 76.85 62.77\nHuman _— —_— —_— 92.40 84.73\n\nTable 4: Macro-F’, of baselines on SCITAB for different settings. The # of Para. indicates the number of parameters\nin the models. The TAPAS and TAPEX models are fine-tuned on the TabFact dataset, while others perform zero-shot\nlearning. The bold text indicates the best performance among | to III, while the underlined text indicates the overall\n\nbest performance among all the models.\n\n4.2 Main Results\n\nWe evaluate all models under both zero-shot and\nin-context settings. In the zero-shot setting, the\nmodel does not have access to any in-domain data.\nIn the in-context setting, we provide three hold-\nout examples as demonstrations. We report two\nsets of results: the 2-class case, where examples\nlabeled as NEI are excluded (since some models\ncannot process NEI claims), and the 3-class case\nincluding all three labels. The results are shown in\nTable 4. We have five major observations.\n\n1. In general, all open source LLMs, including\nencoder—decoder models and decoder-only models,\n\ndo not achieve very promising results on SCITAB\nand they still have a large gap from human per-\n\nformance. The best result is 63.62 for the 2-class\nsetting (Vicuna-7B and 38.05 for the 3-class set-\nting (FLAN-T5-XL). Both results are only moder-\nately better (+13.62 and +4.72) than random guess-\ning. In contrast, a well-trained human annotator\ncan achieve 92.46 and 84.73 F1 scores in the 2-\nclass and 3-class settings, respectively. This re-\nveals the challenging nature of SCITAB and its\npotential to be the future benchmark for scientific\nfact-checking.\n\n2. Counter-intuitively, table-based LLMs do not\noutperform models pre-trained on pure texts, for\nexample, FLAN-T5. This discrepancy may be at-\ntributed to the dissimilarity between the distribution\nof tables in scientific literature and publicly avail-\nable table corpus. For example, scientific tables\ncommonly include both row and column headers,\nwhereas most tables in Wikipedia lack row headers.\nMeanwhile, the claims in our dataset are usually\nmuch longer than those in previous works, raising\nchallenges to table-based LLMs.\n\n3. The results in the 3-class setting are notably\npoorer than those in the 2-class setting. This dis-\ncrepancy reveals the challenges that most models\nface when confronted with the NEI class. One\nplausible explanation could be the inherent diffi-\nculty in distinguishing between ‘refuted’ and ‘NEI’\nclaims — a task that even trained human annotators\nstruggle with, as noted by Jiang et al. (2020). Our\nforthcoming error analysis will further demonstrate\nthat the inclusion of the NEI class tends to dimin-\nish the models’ confidence, causing a shift in their\npredictions from ‘supported/refuted’ to ‘NEI’.\n\n4. Interestingly, the provision of in-context ex-\namples does not result in improved performance for\n\n", "vlm_text": "The table compares the performance of various large language models (LLMs) across different categories on a classification task. It includes four main types of LLMs: Table-based LLMs, Encoder-Decoder LLMs, Open Source LLMs, and Close Source LLMs. Human performance is also listed for comparison.\n\nKey elements include:\n\n- **Models**: The name and source of each model.\n- **# of Para.**: The number of parameters in each model (e.g., 340M, 7B, 175B).\n- **Zero-shot**: Performance in a scenario where the model hasn't been trained specifically on the task. Scores for 2-class and 3-class classification are presented.\n- **In-Context**: Performance when models are given some context or examples regarding the task. Scores for 2-class and 3-class classification are presented.\n\nOverall, the table shows how models perform with different numbers of classes and in zero-shot versus in-context scenarios. The human performance is shown to be notably higher than any model.\n4.2 Main Results \nWe evaluate all models under both  zero-shot  and in-context  settings. In the zero-shot setting, the model does not have access to any in-domain data. In the in-context setting, we provide three hold- out examples as demonstrations. We report two sets of results: the  2-class  case, where examples labeled as NEI are excluded (since some models cannot process NEI claims), and the  3-class  case including all three labels. The results are shown in Table  4 . We have five major observations. \n1. In general, all open source LLMs, including encoder–decoder models and decoder-only models, do not achieve very promising results on SCITAB and they still have a large gap from human per- formance. The best result is 63.62 for the 2-class setting ( Vicuna-7B  and 38.05 for the 3-class set- ting ( FLAN-T5-XL ). Both results are only moder- ately better   $(+13.62\\$   and  $+4.72$  ) than random guess- ing. In contrast, a well-trained human annotator can achieve 92.46 and 84.73 F1 scores in the 2- class and 3-class settings, respectively. This re- veals the challenging nature of S CI T AB  and its potential to be the future benchmark for scientific fact-checking. \n2. Counter-intuitively, table-based LLMs do not outperform models pre-trained on pure texts, for example,  FLAN-T5 . This discrepancy may be at- tributed to the dissimilarity between the distribution of tables in scientific literature and publicly avail- able table corpus. For example, scientific tables commonly include both row and column headers, whereas most tables in Wikipedia lack row headers. Meanwhile, the claims in our dataset are usually much longer than those in previous works, raising challenges to table-based LLMs. \n3. The results in the 3-class setting are notably poorer than those in the 2-class setting. This dis- crepancy reveals the challenges that most models face when confronted with the NEI class. One plausible explanation could be the inherent diffi- culty in distinguishing between ‘refuted’ and ‘NEI’ claims — a task that even trained human annotators struggle with, as noted by  Jiang et al.  ( 2020 ). Our forthcoming error analysis will further demonstrate that the inclusion of the NEI class tends to dimin- ish the models’ confidence, causing a shift in their predictions from ‘supported/refuted’ to ‘NEI’. "}
{"page": 7, "image_path": "doc_images/2305.13186v3_7.jpg", "ocr_text": "the majority of models. This observation is some-\nwhat expected for open source LLMs as they have\nnot been reported to possess in-context learning ca-\npabilities. Nonetheless, it is surprising to find that\neven with chain-of-thought prompting, in-context\ndemonstrations do not yield positive effects for In-\nstructGPT and GPT-4. Our error analysis on the\nPoT offers some insight into this phenomenon and\nwill be discussed in the next section.\n\n5. Closed source LLMs perform better than open\nsource LLMs, with GPT-4 achieving 78.22 macro-\nF; for the 2-class setting and 64.80 for the 3-class\nsetting. This aligns with the assertion that GPT-4\nhas a strong ability to perform complex reason-\ning (OpenAI, 2023) and we show that this ability\ncan generalize to tabular data as well. However, the\nblack-box nature of OpenAI models restricts our\nfurther analysis of its behavior.\n\n4.3. Error Analysis\n\nInstructGPT and GPT-4. We show the confu-\nsion matrices for InstructGPT and GPT-4 under the\nzero-shot 3-class setting in Figure 4. We find that\nboth models have difficulty in accurately predicting\nthe NEI class. InstructGPT displays a pattern of\n“less confident’, frequently classifying supported\nand refuted claims as ‘NEI’. In contrast, GPT-4 ex-\nhibits overconfidence, incorrectly categorizing NEI\nclaims as either supported or refuted. This corrob-\norates our earlier observation that distinguishing\nwhether a claim is verifiable is one of the key chal-\nlenges for SCITAB.\n\nFurther, we also examine individual error in-\nstances, with typical examples provided in Fig-\nures 11 and 12 of Appendix F. The majority of\n‘supported’ claims that were incorrectly classified\nas ‘refuted’ (Case 6) involve numerical reasoning\nor comparison. Conversely, when ‘refuted’ claims\nare inaccurately predicted as ‘supported’ (Case 3),\nwe find that LLMs often overlook claims contain-\ning negation, indicating a lack of deep compre-\nhension. For cases where ‘supported’ or ‘refuted’\nclaims are erroneously predicted as ‘NEI’ (Cases 1\nand 2), such claims typically demand extensive rea-\nsoning and a deep understanding of the research\nfindings. Interestingly, when faced with these com-\nplex cases, the model tends to default to the safer\nchoice of ‘uncertain’ (NEI).\n\nPoT. Unexpectedly, incorporating a Python in-\nterpreter does not confer any advantage on our\ndataset (as shown in Table 4), despite its positive\n\nInstructGPT Label Distribution Percentage (%)\n\nSupported} 9.1 1s Supported\n3\n\nRefuted| 4.6 54 15 3 Refuted\n8\n\nnei] 28 17 5 NEL\n\nSupported Refuted NEI\nPrediction Label\n\nGPT-4 Label Distribution Percentage (%)\n\nGold Label\n\nSupported Refuted NEI\nPrediction Label\n\nFigure 4: Confusion matrix for InstructGPT (left) and\nGPT-4 (right) in the zero-shot 3-class classification task.\n\nError Type Estimated Proportion (%)\nI. Grounding errors 50\nIL. Ambiguity errors 22\nIII. Calculation errors 20\nIV. Program errors 8\n\nTable 5: The error types and their estimated proportions\nfor incorrectly-predicted samples in PoT.\n\nimpacts on other numerical reasoning tasks. In or-\nder to understand this, we randomly selected 50\nclaims wherein the PoT incorrectly predicted the\nfinal veracity labels and evaluated the quality of\nthe generated Python programs. We divide the er-\nrors into four categories, as assessed by human\nannotators: (i) Grounding errors, where the pro-\ngram incorrectly associates data with the respective\ncells in the table; (ii) Ambiguity errors, where the\nclaim contains ambiguous expressions that the pro-\ngram fails to represent; (iii) Calculation errors,\nwhere incorrect floating point arithmetic calcula-\ntion in Python lead to inaccurate results and (iv)\nProgram errors, which encompass mistakes such\nas incorrect or missing arguments/variables, and\nerroneous operations. We present the error analysis\nin Table 5, and examples of program errors can be\nfound in Figure 13 and Figure 14 in Appendix G.\nCompared to other datasets, categories (i) and (ii)\npresent unique challenges in our dataset. Cate-\ngory (i) underlines the difficulty in accurately ref-\nerencing the specific cells to which a claim refers.\nCategory (ii), on the other hand, emphasizes the\ndifficulties posed by the ambiguous nature of scien-\ntific claims, such as “A is significantly better than\nB”, to program-based methods. This connection\nfurther emphasizes the contribution of our work\nin addressing the mismatches between reasoning\ntypes and the occurrence of grounding errors.\n\n5 Related Work\n\nScientific Fact-Checking Datasets. Existing\ndatasets for scientific fact-checking are summa-\n", "vlm_text": "the majority of models. This observation is some- what expected for open source LLMs as they have not been reported to possess in-context learning ca- pabilities. Nonetheless, it is surprising to find that even with chain-of-thought prompting, in-context demonstrations do not yield positive effects for In- structGPT and GPT-4. Our error analysis on the PoT offers some insight into this phenomenon and will be discussed in the next section. \n5. Closed source LLMs perform better than open source LLMs, with GPT-4 achieving 78.22 macro-  $F_{1}$   for the 2-class setting and 64.80 for the 3-class setting. This aligns with the assertion that GPT-4 has a strong ability to perform complex reason- ing ( OpenAI ,  2023 ) and we show that this ability can generalize to tabular data as well. However, the black-box nature of OpenAI models restricts our further analysis of its behavior. \n4.3 Error Analysis \nInstruct GP T and GPT-4. We show the confu- sion matrices for Instruct GP T and GPT-4 under the zero-shot 3-class setting in Figure  4 . We find that both models have difficulty in accurately predicting the NEI class. Instruct GP T displays a pattern of “less confident”, frequently classifying supported and refuted claims as ‘NEI’. In contrast, GPT-4 ex- hibits over confidence, incorrectly categorizing NEI claims as either supported or refuted. This corrob- orates our earlier observation that distinguishing whether a claim is  verifiable  is one of the key chal- lenges for S CI T AB . \nFurther, we also examine individual error in- stances, with typical examples provided in Fig- ures  11  and  12  of Appendix  F . The majority of ‘supported’ claims that were incorrectly classified as ‘refuted’ (Case 6) involve numerical reasoning or comparison. Conversely, when ‘refuted’ claims are inaccurately predicted as ‘supported’ (Case 3), we find that LLMs often overlook claims contain- ing negation, indicating a lack of deep compre- hension. For cases where ‘supported’ or ‘refuted’ claims are erroneously predicted as ‘NEI’ (Cases 1 and 2), such claims typically demand extensive rea- soning and a deep understanding of the research findings. Interestingly, when faced with these com- plex cases, the model tends to default to the safer choice of ‘uncertain’ (NEI). \nPoT. Unexpectedly, incorporating a Python in- terpreter does not confer any advantage on our dataset (as shown in Table  4 ), despite its positive \nThe image shows two confusion matrices side by side comparing the performance of InstructGPT (left) and GPT-4 (right) on a zero-shot 3-class classification task. Each matrix illustrates the percentage distribution across prediction labels—Supported, Refuted, NEI (Not Enough Information)—versus the actual gold labels.\n\n- **InstructGPT Confusion Matrix:**\n  - Supported: Predicted as Supported (9.1%), Refuted (1.5%), NEI (26.8%)\n  - Refuted: Predicted as Supported (4.6%), Refuted (5.4%), NEI (23.6%)\n  - NEI: Predicted as Supported (2.8%), Refuted (1.7%), NEI (24.6%)\n\n- **GPT-4 Confusion Matrix:**\n  - Supported: Predicted as Supported (32.1%), Refuted (4.7%), NEI (0.4%)\n  - Refuted: Predicted as Supported (8.3%), Refuted (25.2%), NEI (0.1%)\n  - NEI: Predicted as Supported (10.3%), Refuted (8.5%), NEI (10.4%)\n\nThe color intensity represents the percentage values, with darker shades indicating higher percentages.\nThe table lists types of errors and their estimated proportions in percentages:\n\n- Grounding errors: 50%\n- Ambiguity errors: 22%\n- Calculation errors: 20%\n- Program errors: 8%\nTable 5: The error types and their estimated proportions for incorrectly-predicted samples in PoT. \nimpacts on other numerical reasoning tasks. In or- der to understand this, we randomly selected 50 claims wherein the PoT incorrectly predicted the final veracity labels and evaluated the quality of the generated Python programs. We divide the er- rors into four categories, as assessed by human annotators: ( i )  Grounding errors , where the pro- gram incorrectly associates data with the respective cells in the table; ( ii )  Ambiguity errors , where the claim contains ambiguous expressions that the pro- gram fails to represent; ( iii )  Calculation errors , where incorrect floating point arithmetic calcula- tion in Python lead to inaccurate results and   $(i\\nu)$  Program errors , which encompass mistakes such as incorrect or missing arguments/variables, and erroneous operations. We present the error analysis in Table  5 , and examples of program errors can be found in Figure  13  and Figure  14  in Appendix  G . Compared to other datasets, categories   $(i)$   and   $(i i)$  present unique challenges in our dataset. Cate- gory   $(i)$   underlines the difficulty in accurately ref- erencing the specific cells to which a claim refers. Category   $(i i)$  , on the other hand, emphasizes the difficulties posed by the ambiguous nature of scien- tific claims, such as “A is significantly better than  $\\mathbf{B}^{\\ast}$  , to program-based methods. This connection further emphasizes the contribution of our work in addressing the mismatches between reasoning types and the occurrence of grounding errors. \n5 Related Work \nScientific Fact-Checking Datasets. Existing datasets for scientific fact-checking are summa- rized in a recent survey from  Vladika and Matthes ( 2023 ). These datasets differ in: 1)  domain : bi- ology ( Wadden et al. ,  2020 ;  Akhtar et al. ,  2022 ), COVID-19 ( Saakyan et al. ,  2021 ;  Sarrouti et al. , 2021 ;  Mohr et al. ,  2022 ;  Wang et al. ,  2023 ), and cli- mate ( Diggelmann et al. ,  2020 ), 2)  claim creation : crowd-sourced claims  v.s.  natural claims, and 3) evidence source : Wikipedia articles ( Diggelmann et al. ,  2020 ) or research papers ( Wadden et al. , 2020 ,  2022 ;  Sarrouti et al. ,  2021 ). However, most of these datasets rely on text evidence to verify claims. SEM-TAB-FACTS ( Wang et al. ,  2021 ) is the only existing dataset based on scientific tables, but it is limited to simple, crowd-sourced claims. To bridge this gap, we construct S CI T AB  which contains complex claims from authentic scientific papers with table-based evidence. "}
{"page": 8, "image_path": "doc_images/2305.13186v3_8.jpg", "ocr_text": "rized in a recent survey from Vladika and Matthes\n(2023). These datasets differ in: 1) domain: bi-\nology (Wadden et al., 2020; Akhtar et al., 2022),\nCOVID-19 (Saakyan et al., 2021; Sarrouti et al.,\n2021; Mohr et al., 2022; Wang et al., 2023), and cli-\nmate (Diggelmann et al., 2020), 2) claim creation:\ncrowd-sourced claims v.s. natural claims, and 3)\nevidence source: Wikipedia articles (Diggelmann\net al., 2020) or research papers (Wadden et al.,\n2020, 2022; Sarrouti et al., 2021). However, most\nof these datasets rely on text evidence to verify\nclaims. SEM-TAB-FACTS (Wang et al., 2021) is\nthe only existing dataset based on scientific tables,\nbut it is limited to simple, crowd-sourced claims.\nTo bridge this gap, we construct SCITAB which\ncontains complex claims from authentic scientific\npapers with table-based evidence.\n\nTable-based Reasoning. Table-based reasoning\nrequires reasoning over both free-form natural lan-\nguage queries and (semi-)structured tables. Early\nworks either rely on executable languages (e.g.,\nSQL and SPARQL) to access the tabular data (Yin\net al., 2016; Yu et al., 2018) or employ graph neural\nnetworks to capture logical structure in statements,\ne.g., LogicFactChecker (Zhong et al., 2020) and\nProgVGAT (Yang et al., 2020). However, these ap-\nproaches often struggle with generalization, as they\nare tightly bound to specific table formats and lan-\nguage patterns. To address this, we have seen a shift\ntoward table pre-training, with the advent of Table-\nBERT (Chen et al., 2020), TAPAS (Herzig et al.,\n2020), SaMoE (Zhou et al., 2022), PASTA (Gu\net al., 2022), and DATER (Ye et al., 2023). These\nmethods encode sentence-table pairs using lan-\nguage models and transform table-based reason-\ning into question-answering or natural language\ninference. In our work, we focus on evaluating pre-\ntraining-based methods on SCITAB because they\nnot only demonstrate superior performance but also\noffer the benefits of few-shot learning.\n\n6 Conclusion and Future Work\n\nWe present SCITAB, a novel dataset for scien-\ntific fact-checking that addresses the limitations\nof existing benchmarks. By incorporating real-\nworld scientific claims and their corresponding\nevidence in the form of tables, SCITAB offers a\nmore comprehensive and fine-grained representa-\ntion of scientific reasoning. The challenging nature\nof SCITAB is evident from the performance of the\nstate-of-the-art, highlighting the need for further\n\nresearch. For example, we believe that addressing\nthe challenges posed by ambiguous claims repre-\nsents a crucial direction for research in scientific\nfact-checking (Glockner et al., 2023; Liu et al.,\n2023a). One potential approach is to enhance the\ndisambiguation of ambiguous claims by leverag-\ning contextual information or external knowledge\nsources. Additionally, studying the compositional-\nity in table-based reasoning is an interesting direc-\ntion. Consider the work of Self-Ask (Press et al.,\n2022), which proposed the “compositionality gap”\nmetric to measure the capability of LLMs in com-\npositional reasoning. Such evaluations can be en-\nriched by annotating SCITAB with ground-truth\nreasoning depths and structured reasoning graphs.\nBeyond this, another direction worth exploring is\nequipping the LLMs with external tools to further\nimprove the model. For example, the use of GPT-\n4 plugins, Program-guided Fact-Checking (Pan\net al., 2023) or adopting approaches from other tool-\naugmented LLMs like Toolformer (Schick et al.,\n2023) and Chameleon (Lu et al., 2023).\n\nEthics Statement\n\nWe have received approval from the Institutional\nReview Board (IRB)* for our data collection. The\nIRB reviewed our experimental design and research\nprocedures to ensure that they do not pose more\nthan minimal risks to research participants. We\ntake steps to protect research participants’ privacy\nand the confidentiality of their data. The review\nprocess took two months to complete.\n\nLimitations\n\nFirstly, the method and dataset are primarily de-\nsigned for languages with limited morphology,\nsuch as English. Secondly, our SCITAB dataset\nis specifically focused on fact-checking scientific\nclaims based on tables, which represents only\none aspect of scientific fact-checking. Further re-\nsearch can explore the integration of other forms\nof evidence, including textual evidence and fig-\nure evidence, to enhance the fact-checking process.\nThirdly, our SCITAB dataset is primarily focused\non numerical reasoning types, as it is derived from\nthe SciGen dataset, which also emphasizes numeri-\ncal reasoning. It would be beneficial for future stud-\nies to incorporate a wider range of reasoning types\nto provide a more comprehensive fact-checking\n\nShttps: //www.nus.edu.sg/research/irb. The NUS-\nIRB Reference Code is NUS-IRB-2022-599\n", "vlm_text": "\nTable-based Reasoning. Table-based reasoning requires reasoning over both free-form natural lan- guage queries and (semi-)structured tables. Early works either rely on executable languages ( e.g. , SQL and SPARQL) to access the tabular data ( Yin et al. ,  2016 ;  Yu et al. ,  2018 ) or employ graph neural networks to capture logical structure in statements, e.g. , Logic Fact Checker ( Zhong et al. ,  2020 ) and ProgVGAT ( Yang et al. ,  2020 ). However, these ap- proaches often struggle with generalization, as they are tightly bound to specific table formats and lan- guage patterns. To address this, we have seen a shift toward table pre-training, with the advent of Table- BERT ( Chen et al. ,  2020 ), TAPAS ( Herzig et al. , 2020 ), SaMoE ( Zhou et al. ,  2022 ), PASTA ( Gu et al. ,  2022 ), and DATER ( Ye et al. ,  2023 ). These methods encode sentence-table pairs using lan- guage models and transform table-based reason- ing into question-answering or natural language inference. In our work, we focus on evaluating pre- training-based methods on S CI T AB  because they not only demonstrate superior performance but also offer the benefits of few-shot learning. \n6 Conclusion and Future Work \nWe present S CI T AB , a novel dataset for scien- tific fact-checking that addresses the limitations of existing benchmarks. By incorporating real- world scientific claims and their corresponding evidence in the form of tables, S CI T AB  offers a more comprehensive and fine-grained representa- tion of scientific reasoning. The challenging nature of S CI T AB  is evident from the performance of the state-of-the-art, highlighting the need for further research. For example, we believe that addressing the challenges posed by ambiguous claims repre- sents a crucial direction for research in scientific fact-checking ( Glockner et al. ,  2023 ;  Liu et al. , 2023a ). One potential approach is to enhance the disambiguation of ambiguous claims by leverag- ing contextual information or external knowledge sources. Additionally, studying the compositional- ity in table-based reasoning is an interesting direc- tion. Consider the work of Self-Ask ( Press et al. , 2022 ), which proposed the “compositional it y gap” metric to measure the capability of LLMs in com- positional reasoning. Such evaluations can be en- riched by annotating S CI T AB  with ground-truth reasoning depths and structured reasoning graphs. Beyond this, another direction worth exploring is equipping the LLMs with external tools to further improve the model. For example, the use of GPT- 4 plugins, Program-guided Fact-Checking ( Pan et al. ,  2023 ) or adopting approaches from other tool- augmented LLMs like Toolformer ( Schick et al. , 2023 ) and Chameleon ( Lu et al. ,  2023 ). \n\nEthics Statement \nWe have received approval from the Institutional Review Board (IRB) 3   for our data collection. The IRB reviewed our experimental design and research procedures to ensure that they do not pose more than minimal risks to research participants. We take steps to protect research participants’ privacy and the confidentiality of their data. The review process took two months to complete. \nLimitations \nFirstly, the method and dataset are primarily de- signed for languages with limited morphology, such as English. Secondly, our S CI T AB  dataset is specifically focused on fact-checking scientific claims based on tables, which represents only one aspect of scientific fact-checking. Further re- search can explore the integration of other forms of evidence, including textual evidence and fig- ure evidence, to enhance the fact-checking process. Thirdly, our S CI T AB  dataset is primarily focused on numerical reasoning types, as it is derived from the SciGen dataset, which also emphasizes numeri- cal reasoning. It would be beneficial for future stud- ies to incorporate a wider range of reasoning types to provide a more comprehensive fact-checking framework. Lastly, it would be valuable to ex- plore additional annotation types, such as reason- ing graphs, to further enrich the depth of analysis and capture more intricate relationships within the claims and evidence. "}
{"page": 9, "image_path": "doc_images/2305.13186v3_9.jpg", "ocr_text": "framework. Lastly, it would be valuable to ex-\nplore additional annotation types, such as reason-\ning graphs, to further enrich the depth of analysis\nand capture more intricate relationships within the\nclaims and evidence.\n\nAcknowledgements\n\nThis research is supported by the Ministry of Ed-\nucation, Singapore, under its MOE AcRF Tier 3\nGrant (MOE-MOET32022-0001). The computa-\ntional work for this article was partially performed\non resources of the National Supercomputing Cen-\ntre, Singapore (https: //www.nscc.sg).\n\nReferences\n\nMubashara Akhtar, Oana Cocarascu, and Elena Simperl.\n2022. Pubhealthtab: A public health table-based\ndataset for evidence-based fact checking. In Findings\nof the 2022 Annual Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics (NAACL), pages 1-16.\n\nRami Aly, Zhijiang Guo, Michael Sejr Schlichtkrull,\nJames Thorne, Andreas Vlachos, Christos\nChristodoulopoulos, Oana Cocarascu, and Arpit\nMittal. 2021. .FEVEROUS: fact extraction and\nverification over unstructured and structured infor-\nmation. In Proceedings of the Neural Information\nProcessing Systems (NeurIPS) Track on Datasets\nand Benchmarks.\n\nEmily M. Bender and Batya Friedman. 2018. Data\nstatements for natural language processing: Toward\nmitigating system bias and enabling better science.\nTransactions of the Association for Computational\nLinguistics (TACL), 6:587-604.\n\nWenhu Chen, Xueguang Ma, Xinyi Wang, and\nWilliam W. Cohen. 2022. Program of thoughts\nprompting: Disentangling computation from rea-\nsoning for numerical reasoning tasks. CoRR,\nabs/2211.12588.\n\nWenhu Chen, Hongmin Wang, Jianshu Chen, Yunkai\nZhang, Hong Wang, Shiyang Li, Xiyou Zhou, and\nWilliam Yang Wang. 2020. Tabfact: A large-scale\ndataset for table-based fact verification. In Proceed-\nings of the 8th International Conference on Learning\nRepresentations (ICLR).\n\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion\nStoica, and Eric P. Xing. 2023. Vicuna: An open-\nsource chatbot impressing gpt-4 with 90%* chatgpt\nquality.\n\nHyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang,\n\nMostafa Dehghani, Siddhartha Brahma, Albert Web-\nson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suz-\ngun, Xinyun Chen, Aakanksha Chowdhery, Sharan\nNarang, Gaurav Mishra, Adams Yu, Vincent Y. Zhao,\nYanping Huang, Andrew M. Dai, Hongkun Yu, Slav\nPetrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam\nRoberts, Denny Zhou, Quoc V. Le, and Jason Wei.\n2022. Scaling instruction-finetuned language models.\nCoRR, abs/2210.11416.\n\nJacob Cohen. 1960. A coefficient of agreement for\nnominal scales. Educational and Psychological Mea-\nsurement, 20:37 — 46.\n\nThomas Diggelmann, Jordan L. Boyd-Graber, Jan-\nnis Bulian, Massimiliano Ciaramita, and Markus\nLeippold. 2020. CLIMATE-FEVER: A dataset for\nverification of real-world climate claims. CoRR,\nabs/2012.00614.\n\nMax Glockner, Ieva Staliiinaité, James Thorne, Gisela\nVallejo, Andreas Vlachos, and Iryna Gurevych. 2023.\nAmbifc: Fact-checking ambiguous claims with evi-\ndence. CoRR, abs/2104.00640.\n\nZihui Gu, Ju Fan, Nan Tang, Preslav Nakov, Xiao-\nman Zhao, and Xiaoyong Du. 2022. PASTA: table-\noperations aware fact verification via sentence-table\ncloze pre-training. In Proceedings of the 2022 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 4971-4983.\n\nZhijiang Guo, Michael Sejr Schlichtkrull, and Andreas\nVlachos. 2022. A survey on automated fact-checking.\nTransactions of the Association for Computational\nLinguistics (TACL), 10:178-206.\n\nVivek Gupta, Maitrey Mehta, Pegah Nokhiz, and Vivek\nSrikumar. 2020. INFOTABS: inference on tables\nas semi-structured data. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational\nLinguistics (ACL), pages 2309-2324.\n\nJonathan Herzig, Pawel Krzysztof Nowak, Thomas\nMiller, Francesco Piccinno, and Julian Martin Eisen-\nschlos. 2020. Tapas: Weakly supervised table parsing\nvia pre-training. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics (ACL), pages 4320-4333.\n\nYichen Jiang, Shikha Bordia, Zheng Zhong, Charles\nDognin, Maneesh Kumar Singh, and Mohit Bansal.\n2020. Hover: A dataset for many-hop fact extraction\nand claim verification. In Findings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), volume EMNLP 2020, pages\n3441-3460.\n\nW.Y. Lee, L. Ho, and M.E.T. Ng. 2009. Research Writ-\ning: A Workbook for Graduate Students. Prentice\nHall.\n\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020.\nBART: denoising sequence-to-sequence pre-training\n", "vlm_text": "\nAcknowledgements \nThis research is supported by the Ministry of Ed- ucation, Singapore, under its MOE AcRF Tier 3 Grant (MOE-MOET32022-0001). The computa- tional work for this article was partially performed on resources of the National Super computing Cen- tre, Singapore ( https://www.nscc.sg ). \nReferences \nMubashara Akhtar, Oana Cocarascu, and Elena Simperl. 2022.  Pub health tab: A public health table-based dataset for evidence-based fact checking . In  Findings of the 2022 Annual Conference of the North Amer- ican Chapter of the Association for Computational Linguistics (NAACL) , pages 1–16. \nRami Aly, Zhijiang Guo, Michael Sejr Sch licht kru ll, James Thorne, Andreas Vlachos, Christos Christo dou lo poul os, Oana Cocarascu, and Arpit Mittal. 2021. FEVEROUS: fact extraction and verification over unstructured and structured infor- mation . In  Proceedings of the Neural Information Processing Systems (NeurIPS) Track on Datasets and Benchmarks . \nEmily M. Bender and Batya Friedman. 2018.  Data statements for natural language processing: Toward mitigating system bias and enabling better science . Transactions of the Association for Computational Linguistics (TACL) , 6:587–604. \nWenhu Chen, Xueguang Ma, Xinyi Wang, and William W. Cohen. 2022. Program of thoughts prompting: Disentangling computation from rea- soning for numerical reasoning tasks . CoRR , abs/2211.12588. \nWenhu Chen, Hongmin Wang, Jianshu Chen, Yunkai Zhang, Hong Wang, Shiyang Li, Xiyou Zhou, and William Yang Wang. 2020.  Tabfact: A large-scale dataset for table-based fact verification . In  Proceed- ings of the 8th International Conference on Learning Representations (ICLR) . \nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. 2023.  Vicuna: An open- source chatbot impressing gpt-4 with  $90\\%^{*}$   chatgpt quality . \nHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, \nMostafa Dehghani, Siddhartha Brahma, Albert Web- son, Shixiang Shane Gu, Zhuyun Dai, Mirac Suz- gun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Y. Zhao, Yanping Huang, Andrew M. Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. 2022.  Scaling instruction-finetuned language models . CoRR , abs/2210.11416. \nJacob Cohen. 1960.  A coefficient of agreement for nominal scales .  Educational and Psychological Mea- surement , 20:37 – 46. Thomas Diggelmann, Jordan L. Boyd-Graber, Jan- nis Bulian, Massimiliano Ciaramita, and Markus Leippold. 2020.  CLIMATE-FEVER: A dataset for verification of real-world climate claims . CoRR , abs/2012.00614. Max Glockner, Ieva Stali unait e, James Thorne, Gisela Vallejo, Andreas Vlachos, and Iryna Gurevych. 2023. Ambifc: Fact-checking ambiguous claims with evi- dence .  CoRR , abs/2104.00640. Zihui Gu, Ju Fan, Nan Tang, Preslav Nakov, Xiao- man Zhao, and Xiaoyong Du. 2022.  PASTA: table- operations aware fact verification via sentence-table cloze pre-training . In  Proceedings of the 2022 Con- ference on Empirical Methods in Natural Language Processing (EMNLP) , pages 4971–4983. Zhijiang Guo, Michael Sejr Sch licht kru ll, and Andreas Vlachos. 2022.  A survey on automated fact-checking . Transactions of the Association for Computational Linguistics (TACL) , 10:178–206. Vivek Gupta, Maitrey Mehta, Pegah Nokhiz, and Vivek Srikumar. 2020.  INFOTABS: inference on tables as semi-structured data . In  Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL) , pages 2309–2324. Jonathan Herzig, Pawel Krzysztof Nowak, Thomas Müller, Francesco Piccinno, and Julian Martin Eisen- schlos. 2020.  Tapas: Weakly supervised table parsing via pre-training . In  Proceedings of the 58th Annual Meeting of the Association for Computational Lin- guistics (ACL) , pages 4320–4333. Yichen Jiang, Shikha Bordia, Zheng Zhong, Charles Dognin, Maneesh Kumar Singh, and Mohit Bansal. 2020.  Hover: A dataset for many-hop fact extraction and claim verification . In  Findings of the 2020 Con- ference on Empirical Methods in Natural Language Processing (EMNLP) , volume EMNLP 2020, pages 3441–3460. W.Y. Lee, L. Ho, and M.E.T. Ng. 2009.  Research Writ- ing: A Workbook for Graduate Students . Prentice Hall. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghaz vi nine j ad, Abdel rahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Z ett le moyer. 2020. BART: denoising sequence-to-sequence pre-training "}
{"page": 10, "image_path": "doc_images/2305.13186v3_10.jpg", "ocr_text": "for natural language generation, translation, and com-\nprehension. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics\n(ACL), pages 7871-7880.\n\nAlisa Liu, Swabha Swayamdipta, Noah A. Smith, and\nYejin Choi. 2022a. WANLI: worker and AI collabo-\nration for natural language inference dataset creation.\nIn Findings of the 2022 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 6826-6847.\n\nAlisa Liu, Zhaofeng Wu, Julian Michael, Alane Suhr,\nPeter West, Alexander Koller, Swabha Swayamdipta,\nNoah A. Smith, and Yejin Choi. 2023a. We’re afraid\nlanguage models aren’t modeling ambiguity. CoRR,\nabs/2304.14399.\n\nQian Liu, Bei Chen, Jiaqi Guo, Morteza Ziyadi, Zeqi\nLin, Weizhu Chen, and Jian-Guang Lou. 2022b.\nTAPEX: table pre-training via learning a neural SQL\nexecutor. In Proceedings of the 10th International\nConference on Learning Representations (ICLR).\n\nQian Liu, Fan Zhou, Zhengbao Jiang, Longxu Dou, and\nMin Lin. 2023b. From zero to hero: Examining the\npower of symbolic tasks in instruction tuning. CoRR,\nabs/2304.07995.\n\nPan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-\nWei Chang, Ying Nian Wu, Song-Chun Zhu, and Jian-\nfeng Gao. 2023. Chameleon: Plug-and-play compo-\nsitional reasoning with large language models. CoRR,\nabs/2304.09842.\n\nIsabelle Mohr, Amelie Wiihrl, and Roman Klinger.\n2022. Covert: A corpus of fact-checked biomedical\nCOVID-19 tweets. In Proceedings of the 13th Lan-\nguage Resources and Evaluation Conference (LREC),\npages 244-257.\n\nNafise Sadat Moosavi, Andreas Riicklé, Dan Roth,\nand Iryna Gurevych. 2021. Scigen: a dataset for\nreasoning-aware text generation from scientific ta-\nbles. In Proceedings of the Neural Information Pro-\ncessing Systems (NeurlPS) Track on Datasets and\nBenchmarks.\n\nOpenAI. 2023.\nabs/2303.08774.\n\nGPT-4 technical report. CoRR,\n\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll L. Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray,\nJohn Schulman, Jacob Hilton, Fraser Kelton, Luke\nMiller, Maddie Simens, Amanda Askell, Peter Welin-\nder, Paul F. Christiano, Jan Leike, and Ryan Lowe.\n2022. Training language models to follow instruc-\ntions with human feedback. In Proceedings of the\nAnnual Conference on Neural Information Process-\ning Systems (NeurIPS).\n\nLiangming Pan, Xiaobao Wu, Xinyuan Lu, Anh Tuan\nLuu, William Yang Wang, Min-Yen Kan, and Preslav\nNakov. 2023. Fact-checking complex claims with\nprogram-guided reasoning. In Proceedings of the\n\n61st Annual Meeting of the Association for Computa-\ntional Linguistics (ACL), pages 6981-7004.\n\nOfir Press, Muru Zhang, Sewon Min, Ludwig Schmidt,\nNoah A. Smith, and Mike Lewis. 2022. Measuring\nand narrowing the compositionality gap in language\nmodels. CoRR, abs/2210.03350.\n\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring the\nlimits of transfer learning with a unified text-to-text\ntransformer. Journal of Machine Learning Research\n(JMLR), 21:140:1-140:67.\n\nArkadiy Saakyan, Tuhin Chakrabarty, and Smaranda\nMuresan. 2021. Covid-fact: Fact extraction and veri-\nfication of real-world claims on COVID-19 pandemic.\nIn Proceedings of the 59th Annual Meeting of the As-\nsociation for Computational Linguistics (ACL), pages\n2116-2129.\n\nMourad Sarrouti, Asma Ben Abacha, Yassine Mrabet,\nand Dina Demner-Fushman. 2021. Evidence-based\nfact-checking of health-related claims. In Findings\nof the 2021 Conference on Empirical Methods in\nNatural Language Processing (EMNLP), pages 3499—\n3512.\n\nTimo Schick, Jane Dwivedi-Yu, Roberto Dessi, Roberta\nRaileanu, Maria Lomeli, Luke Zettlemoyer, Nicola\nCancedda, and Thomas Scialom. 2023. Toolformer:\nLanguage models can teach themselves to use tools.\nCoRR, abs/2302.04761.\n\nTal Schuster, Darsh J. Shah, Yun Jie Serene Yeo,\nDaniel Filizzola, Enrico Santus, and Regina Barzilay.\n2019. Towards debiasing fact verification models. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 3417-3423.\n\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B. Hashimoto. 2023. Stanford alpaca:\nAn instruction-following llama model. https://\ngithub. com/tatsu-lab/stanford_alpaca.\n\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Roziére, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023. Llama: Open\nand efficient foundation language models. CoRR,\nabs/2302.13971.\n\nJuraj Vladika and Florian Matthes. 2023. Scientific fact-\nchecking: A survey of resources and approaches. In\n\nFindings of the 61st Association for Computational\nLinguistics (ACL), pages 6215-6230.\n\nDavid Wadden, Shanchuan Lin, Kyle Lo, Lucy Lu\nWang, Madeleine van Zuylen, Arman Cohan, and\nHannaneh Hajishirzi. 2020. Fact or fiction: Verifying\nscientific claims. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 7534-7550.\n", "vlm_text": "prehension . In  Proceedings of the 58th Annual Meet- ing of the Association for Computational Linguistics (ACL) , pages 7871–7880. Alisa Liu, Swabha S way am dip ta, Noah A. Smith, and Yejin Choi. 2022a.  WANLI: worker and AI collabo- ration for natural language inference dataset creation . In  Findings of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pages 6826–6847. Alisa Liu, Zhaofeng Wu, Julian Michael, Alane Suhr, Peter West, Alexander Koller, Swabha S way am dip ta, Noah A. Smith, and Yejin Choi. 2023a.  We’re afraid language models aren’t modeling ambiguity .  CoRR , abs/2304.14399. Qian Liu, Bei Chen, Jiaqi Guo, Morteza Ziyadi, Zeqi Lin, Weizhu Chen, and Jian-Guang Lou. 2022b. TAPEX: table pre-training via learning a neural SQL executor . In  Proceedings of the 10th International Conference on Learning Representations (ICLR) . Qian Liu, Fan Zhou, Zhengbao Jiang, Longxu Dou, and Min Lin. 2023b.  From zero to hero: Examining the power of symbolic tasks in instruction tuning .  CoRR , abs/2304.07995. Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai- Wei Chang, Ying Nian Wu, Song-Chun Zhu, and Jian- feng Gao. 2023.  Chameleon: Plug-and-play compo- sitional reasoning with large language models .  CoRR , abs/2304.09842. Isabelle Mohr, Amelie Wührl, and Roman Klinger. 2022.  Covert: A corpus of fact-checked biomedical COVID-19 tweets . In  Proceedings of the 13th Lan- guage Resources and Evaluation Conference (LREC) , pages 244–257. Nafise Sadat Moosavi, Andreas Rücklé, Dan Roth, and Iryna Gurevych. 2021.  Scigen: a dataset for reasoning-aware text generation from scientific ta- bles . In  Proceedings of the Neural Information Pro- cessing Systems (NeurIPS) Track on Datasets and Benchmarks . OpenAI. 2023. GPT-4 technical report . CoRR , abs/2303.08774. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welin- der, Paul F. Christiano, Jan Leike, and Ryan Lowe. 2022.  Training language models to follow instruc- tions with human feedback . In  Proceedings of the Annual Conference on Neural Information Process- ing Systems (NeurIPS) . Liangming Pan, Xiaobao Wu, Xinyuan Lu, Anh Tuan Luu, William Yang Wang, Min-Yen Kan, and Preslav Nakov. 2023.  Fact-checking complex claims with program-guided reasoning . In  Proceedings of the \n61st Annual Meeting of the Association for Computa- tional Linguistics (ACL) , pages 6981–7004. Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A. Smith, and Mike Lewis. 2022.  Measuring and narrowing the compositional it y gap in language models .  CoRR , abs/2210.03350. Colin Raffel, Noam Shazeer, Adam Roberts, Kather- ine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020.  Exploring the limits of transfer learning with a unified text-to-text transformer .  Journal of Machine Learning Research (JMLR) , 21:140:1–140:67. Arkadiy Saakyan, Tuhin Chakra barty, and Smaranda Muresan. 2021. Covid-fact: Fact extraction and veri- fication of real-world claims on COVID-19 pandemic. In  Proceedings of the 59th Annual Meeting of the As- sociation for Computational Linguistics (ACL) , pages 2116–2129. Mourad Sarrouti, Asma Ben Abacha, Yassine Mrabet, and Dina Demner-Fushman. 2021.  Evidence-based fact-checking of health-related claims . In  Findings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pages 3499– 3512. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Z ett le moyer, Nicola Cancedda, and Thomas Scialom. 2023.  Toolformer: Language models can teach themselves to use tools . CoRR , abs/2302.04761. Tal Schuster, Darsh J. Shah, Yun Jie Serene Yeo, Daniel Filizzola, Enrico Santus, and Regina Barzilay. 2019.  Towards debiasing fact verification models . In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pages 3417–3423. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford alpaca: An instruction-following llama model. https:// github.com/tatsu-lab/stanford alpaca . Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023.  Llama: Open and efficient foundation language models .  CoRR , abs/2302.13971. Juraj Vladika and Florian Matthes. 2023.  Scientific fact- checking: A survey of resources and approaches . In Findings of the 61st Association for Computational Linguistics (ACL) , pages 6215–6230. David Wadden, Shanchuan Lin, Kyle Lo, Lucy Lu Wang, Madeleine van Zuylen, Arman Cohan, and Hannaneh Hajishirzi. 2020.  Fact or fiction: Verifying scientific claims . In  Proceedings of the 2020 Con- ference on Empirical Methods in Natural Language Processing (EMNLP) , pages 7534–7550. "}
{"page": 11, "image_path": "doc_images/2305.13186v3_11.jpg", "ocr_text": "David Wadden, Kyle Lo, Bailey Kuehl, Arman Cohan,\nIz Beltagy, Lucy Lu Wang, and Hannaneh Hajishirzi.\n2022. Scifact-open: Towards open-domain scientific\nclaim verification. In Findings of the 2022 Confer-\nence on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 4719-4734.\n\nGengyu Wang, Kate Harwood, Lawrence Chillrud,\nAmith Ananthram, Melanie Subbiah, and Kathleen R.\nMcKeown. 2023. Check-covid: Fact-checking\nCOVID-19 news claims with scientific evidence. In\nFindings of the 61st Association for Computational\nLinguistics (ACL), pages 14114-14127.\n\nNancy Xin Ru Wang, Diwakar Mahajan, Marina\nDanilevsky, and Sara Rosenthal. 2021. Semeval-2021\ntask 9: Fact verification and evidence finding for tabu-\nlar data in scientific documents (SEM-TAB-FACTS).\nIn Proceedings of the 15th International Workshop\non Semantic Evaluation (SemEval@ACL/IJCNLP),\npages 317-326.\n\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed H. Chi, Quoc Le, and Denny Zhou. 2022.\nChain of thought prompting elicits reasoning in large\nlanguage models. CoRR, abs/2201.11903.\n\nXiaoyu Yang, Feng Nie, Yufei Feng, Quan Liu, Zhigang\nChen, and Xiaodan Zhu. 2020. Program enhanced\nfact verification with verbalization and graph atten-\ntion network. In Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Process-\ning (EMNLP), pages 7810-7825.\n\nYunhu Ye, Binyuan Hui, Min Yang, Binhua Li, Fei\nHuang, and Yongbin Li. 2023. Large language mod-\nels are versatile decomposers: Decomposing evi-\ndence and questions for table-based reasoning. In\nProceedings of the 46th International ACM Confer-\nence on Research and Development in Information\nRetrieval (SIGIR), pages 174-184.\n\nPengcheng Yin, Zhengdong Lu, Hang Li, and Ben Kao.\n2016. Neural enquirer: Learning to query tables in\nnatural language. In Proceedings of the 25th Inter-\nnational Joint Conference on Artificial Intelligence\n(IJCAI), pages 2308-2314.\n\nTao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga,\nDongxu Wang, Zifan Li, James Ma, Irene Li,\nQingning Yao, Shanelle Roman, Zilin Zhang, and\nDragomir R. Radev. 2018. Spider: A large-scale\nhuman-labeled dataset for complex and cross-domain\nsemantic parsing and text-to-sql task. In Proceedings\nof the 2018 Conference on Empirical Methods in\nNatural Language Processing (EMNLP), pages 391 1—\n3921.\n\nWanjun Zhong, Duyu Tang, Zhangyin Feng, Nan Duan,\nMing Zhou, Ming Gong, Linjun Shou, Daxin Jiang,\nJiahai Wang, and Jian Yin. 2020. Logicalfactchecker:\nLeveraging logical operations for fact checking with\ngraph module network. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational\nLinguistics (ACL), pages 6053-6065.\n\nYuxuan Zhou, Xien Liu, Kaiyin Zhou, and Ji Wu. 2022.\nTable-based fact verification with self-adaptive mix-\nture of experts. In Findings of the 60th Associa-\ntion for Computational Linguistics (ACL), pages 139-\n149.\n", "vlm_text": "David Wadden, Kyle Lo, Bailey Kuehl, Arman Cohan, Iz Beltagy, Lucy Lu Wang, and Hannaneh Hajishirzi. 2022.  Scifact-open: Towards open-domain scientific claim verification . In  Findings of the 2022 Confer- ence on Empirical Methods in Natural Language Processing (EMNLP) , pages 4719–4734. \nYuxuan Zhou, Xien Liu, Kaiyin Zhou, and Ji Wu. 2022. Table-based fact verification with self-adaptive mix- ture of experts . In  Findings of the 60th Associa- tion for Computational Linguistics (ACL) , pages 139– 149. \nGengyu Wang, Kate Harwood, Lawrence Chillrud, Amith Ananthram, Melanie Subbiah, and Kathleen R. McKeown. 2023. Check-covid: Fact-checking COVID-19 news claims with scientific evidence . In Findings of the 61st Association for Computational Linguistics (ACL) , pages 14114–14127. \nNancy Xin Ru Wang, Diwakar Mahajan, Marina Danilevsky, and Sara Rosenthal. 2021.  Semeval-2021 task 9: Fact verification and evidence finding for tabu- lar data in scientific documents (SEM-TAB-FACTS) . In  Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval@ACL/IJCNLP) , pages 317–326. \nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed H. Chi, Quoc Le, and Denny Zhou. 2022. Chain of thought prompting elicits reasoning in large language models .  CoRR , abs/2201.11903. \nXiaoyu Yang, Feng Nie, Yufei Feng, Quan Liu, Zhigang Chen, and Xiaodan Zhu. 2020.  Program enhanced fact verification with verb aliz ation and graph atten- tion network . In  Proceedings of the 2020 Conference on Empirical Methods in Natural Language Process- ing (EMNLP) , pages 7810–7825. \nYunhu Ye, Binyuan Hui, Min Yang, Binhua Li, Fei Huang, and Yongbin Li. 2023.  Large language mod- els are versatile decomposers: Decomposing evi- dence and questions for table-based reasoning . In Proceedings of the 46th International ACM Confer- ence on Research and Development in Information Retrieval (SIGIR) , pages 174–184. \nPengcheng Yin, Zhengdong Lu, Hang Li, and Ben Kao. 2016.  Neural enquirer: Learning to query tables in natural language . In  Proceedings of the 25th Inter- national Joint Conference on Artificial Intelligence (IJCAI) , pages 2308–2314. \nTao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingning Yao, Shanelle Roman, Zilin Zhang, and Dragomir R. Radev. 2018.  Spider: A large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-sql task . In  Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pages 3911– 3921. \nWanjun Zhong, Duyu Tang, Zhangyin Feng, Nan Duan, Ming Zhou, Ming Gong, Linjun Shou, Daxin Jiang, Jiahai Wang, and Jian Yin. 2020.  Logical fact checker: Leveraging logical operations for fact checking with graph module network . In  Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL) , pages 6053–6065. "}
{"page": 12, "image_path": "doc_images/2305.13186v3_12.jpg", "ocr_text": "A Claim Extraction Procedure\n\nA.1 Claim Definition\n\nIn academic writing (Lee et al., 2009), the accompa-\nnying text for data, presented as tables and figures),\ntypically includes three fundamental elements as\noutlined below. These elements encompass the def-\ninition of claims, which involve highlighting key\ndata (KD) and commenting on key data (COM)\nthat emphasizes and comments on the key data.\n\nLocation of results (LOC). Statements that lo-\ncate where the figure/table is found, e.g., Figure 7\ndisplays the mean percentile scores.\n\nHighlighting of key data (KD). Statements that\nhighlight the important data, e.g.,(1) Highest or\nlowest values (2) Overall trend or pattern in the\ndata (3) Points that do not seem to fit the pattern\nor trend, etc. (4) Results which provide answers to\nyour research questions\n\nCommenting on key data (COM). Statements\nthat interpret the data. There are three types of\ncomments: (1) Generalization (deductions and im-\nplications drawn from the results), e.g., “This in-\ndicates that ..”’ (2) Comparison of results with\nthose from prior studies, e.g., “Different from ...”\n(3) Explanation or speculation (possible reasons\nor cause-effect relationships for the results), e.g.,\n“The possible reason is that ...”\n\nA.2 Claim Extraction Interface\n\nFigure 5 shows the user interface for the claim\nextraction task.\n\nB_ Manual Claim Verification Procedure\n\nB.1 Annotator Training Process\n\nOur annotator selection and training process is sys-\ntematic and thorough to ensure the highest quality\nannotations. We initiate the process by advertising\non our university’s platform. Interested candidates\nare then required to complete a registration form.\nFrom these responses, the authors identify suitable\nannotators based on set criteria. Once shortlisted,\nthe potential annotators are invited for a training\nsession, which can be conducted either in-person\nor via Zoom, lasting approximately one hour. This\nsession is divided into three parts. Firstly, the au-\nthors provide a comprehensive overview of the task\ndefinition, ensuring clarity on what is expected.\nSimilar to WANLI (Liu et al., 2022a), during our\n\ntraining sessions*, commonsense interpretations\n\nand a minimum amount of logical inference are\nacceptable. Next, a demonstration is given on how\nto navigate and utilize the annotation interface ef-\nfectively. Following this, a series of trial tests are\nreleased to the annotators. This is to verify their\nunderstanding and capability in the task. Last, we\nspecify the deadline for completing annotations,\noutline how we check the quality of their work,\nbrief them on a post-annotation survey, and explain\nthe reimbursement procedure. A Q&A session is\nalso incorporated to address any uncertainties or\nconcerns. After receiving their reimbursement, the\nannotators signed an agreement sheet to ensure its\nreceipt.\n\nB.2. NEI Claim Verification Interface\n\nFigure 6 shows the user interface for the NEI claim\nverification task.\n\nB.3 Refuted Claim Verification Interface\n\nFigure 7 shows the user interface for the refuted\nclaim verification task.\n\nB.4 Annotation Post-Survey\n\nFigure 8 shows the examples of post-annotation\nsurvey questions and the answers of annotators.\n\nC_ Analysis of Refuted Reasons in the\nSci-Fact dataset\n\nTable 6 provides an analysis of the reasons for re-\nfuted claims in the Sci-Fact dataset, along with\ntheir estimated proportions. A random sample of\n100 refuted claims was selected, and the results\nindicate that 85% of claims were simply negated\nusing terms like “not” or paraphrased based on the\nevidence sentences. Additionally, 6% of the refuted\nclaims were attributed to incorrect calculation re-\nsults, while 6% were identified as having wrong\ncommonsense knowledge. A smaller proportion of\nrefuted claims (3%) were found to have incorrect\nopen-domain knowledge.\n\nD_ Discussions on Human-Machine\nCollaboration\n\nOur final data creation pipeline undergoes repeti-\ntive testing and revision until it reaches its current\n\n4 All the related materials including the advertisement, a\nsample of the registration form and the agreement sheet are\navailable at https: //github.com/XinyuanLuQQ/SciTab.\n", "vlm_text": "A Claim Extraction Procedure \nA.1 Claim Definition \nIn academic writing ( Lee et al. ,  2009 ), the accompa- nying text for data, presented as tables and figures), typically includes three fundamental elements as outlined below. These elements encompass the def- inition of claims, which involve highlighting key data (KD) and commenting on key data (COM) that emphasizes and comments on the key data. \nLocation of results (LOC). Statements that lo- cate where the figure/table is found,  e.g.,  Figure 7 displays the mean percentile scores. \nHighlighting of key data (KD). Statements that highlight the important data,  e.g., (1) Highest or lowest values (2) Overall trend or pattern in the data (3) Points that do not seem to fit the pattern or trend, etc. (4) Results which provide answers to your research questions \nCommenting on key data (COM). Statements that interpret the data. There are three types of comments: (1) Generalization (deductions and im- plications drawn from the results),  e.g.,  “This in- dicates that ...” (2) Comparison of results with those from prior studies,  e.g.,  “Different from ...” (3) Explanation or speculation (possible reasons or cause-effect relationships for the results),  e.g., “The possible reason is that ...” \nA.2 Claim Extraction Interface \nFigure  5  shows the user interface for the claim extraction task. \nB Manual Claim Verification Procedure \nB.1 Annotator Training Process \nOur annotator selection and training process is sys- tematic and thorough to ensure the highest quality annotations. We initiate the process by advertising on our university’s platform. Interested candidates are then required to complete a registration form. From these responses, the authors identify suitable annotators based on set criteria. Once shortlisted, the potential annotators are invited for a training session, which can be conducted either in-person or via Zoom, lasting approximately one hour. This session is divided into three parts. Firstly, the au- thors provide a comprehensive overview of the task definition, ensuring clarity on what is expected. Similar to WANLI ( Liu et al. ,  2022a ), during our training sessions 4 , commonsense interpretations and a minimum amount of logical inference are acceptable. Next, a demonstration is given on how to navigate and utilize the annotation interface ef- fectively. Following this, a series of trial tests are released to the annotators. This is to verify their understanding and capability in the task. Last, we specify the deadline for completing annotations, outline how we check the quality of their work, brief them on a post-annotation survey, and explain the reimbursement procedure. A Q&A session is also incorporated to address any uncertainties or concerns. After receiving their reimbursement, the annotators signed an agreement sheet to ensure its receipt. \n\nB.2 NEI Claim Verification Interface \nFigure  6  shows the user interface for the NEI claim verification task. \nB.3 Refuted Claim Verification Interface \nFigure  7  shows the user interface for the refuted claim verification task. \nB.4 Annotation Post-Survey \nFigure  8  shows the examples of post-annotation survey questions and the answers of annotators. \nC Analysis of Refuted Reasons in the Sci-Fact dataset \nTable  6  provides an analysis of the reasons for re- futed claims in the Sci-Fact dataset, along with their estimated proportions. A random sample of 100 refuted claims was selected, and the results indicate that   $85\\%$   of claims were simply negated using terms like “not” or paraphrased based on the evidence sentences. Additionally,  $6\\%$   of the refuted claims were attributed to incorrect calculation re- sults, while  $6\\%$   were identified as having wrong commonsense knowledge. A smaller proportion of refuted claims   $(3\\%)$   were found to have incorrect open-domain knowledge. \nD Discussions on Human-Machine Collaboration \nOur final data creation pipeline undergoes repeti- tive testing and revision until it reaches its current "}
{"page": 13, "image_path": "doc_images/2305.13186v3_13.jpg", "ocr_text": "Refuted Reasons Prop. (%)\nNegation (+not) and paraphrasing. 85\nThe calculation result is wrong. 6\nThe commonsense knowledge is wrong. 6\nThe open-domain knowledge is wrong. 3\n\nTable 6: The refuted reasons and their estimated propor-\ntions (Prop.) in the Sci-Fact dataset.\n\nform. In our pilot annotation, we found that man-\nual verification played the most essential role in the\nvalidation of claims marked as “Not Enough Infor-\nmation(NEI)”. Initially, we planned to rely solely\non LLMs for generating NEI claims. Our criteria\nfor the NEI claim is that “the claim should be flu-\nent, logical, and relevant to the table. However,\nthe claim cannot be verified as true or false solely\nbased on the information in the table.” However,\nafter a careful examination of the LLM output, we\nfound that LLM tends to generate claims that are\neither not logical or irrelevant to the table content.\nTherefore, human efforts are required to further\nselect NEI claims that meet our criteria. Out of\n900 initial NEI claims generated by LLMs, man-\nual verification narrowed them down to only 355\nclaims, taking up 40% of the original count. While\nit may not have served as crucial a role as filtering\nNEI claims, human verification also safeguarded\nthe data quality in other annotation processes. For\nexample, among the “supported” claims originally\nappearing in the scientific paper, human validation\nstill identified 10 cases that were actually not sup-\nported (e.g., wrong number matching.)\n\nE_ Case Study for Refuted Claims\n\nFigure 9 and Figure 10 show five examples of re-\nfuted cases. Below, we provide explanations for\neach of these error cases.\n\nCase A The calculation result is wrong. It pro-\nduces incorrect calculation results. The accurate\nresult should be 27.9-21.7 = 6.2.\n\nCase B The approximation word is wrong. It\ngenerates incorrect approximation words, as 19.4\nis not significantly lower compared to 23.3.\n\nCase C The claim is partially right. The claim\nis generally correct, with the exception of the\nBShift column which does not fulfill the claim.\n\nCase D The values in the claim do not match.\nThe value in the claim does not align with the cor-\n\nresponding value in the table. The correct value\nshould be 27.9.\n\nCase E The operation type is wrong. It applies\nthe incorrect operation type. For instance, in the\ncase of GCN+RC+LA (9), it is not accurate to claim\nthat it is better than DCGCN1 because 22.9 > 22.0\nand 53.0 > 52.6.\n\nF_ Error Cases for InstructGPT\n\nFigure 11 and Figure 12 show six error examples of\nInstructGPT in the zero-shot setting when applied\nto our SCITAB dataset.\n\nError Type 1: Supported predicted as NEI.\nThis error type indicates a discrepancy between the\ngold label, which is Supported, and the predicted\nlabel, which is NEI.\n\nError Type 2: Refuted predicted as NEI. This\nerror type indicates a discrepancy between the gold\nlabel, which is Refuted, and the predicted label,\nwhich is NEI.\n\nError Type 3: Refuted predicted as Supported.\nThis error type indicates a discrepancy between\nthe gold label, which is Refuted, and the predicted\nlabel, which is Supported.\n\nError Type 4: NEI predicted as Supported.\nThis error type indicates a discrepancy between\nthe gold label, which is NEI, and the predicted\nlabel, which is Supported.\n\nError Type 5: NEI predicted as Refuted. This\nerror type indicates a discrepancy between the gold\nlabel, which is NEI, and the predicted label, which\nis Refuted.\n\nError Type 6: Supported predicted as Refuted.\nThis error type indicates a discrepancy between the\ngold label, which is Supported, and the predicted\nlabel, which is Refuted.\n\nG_ Error Cases for Program-of-Thoughts\n\nFigure 13 and Figure 14 show five error examples\nof Program-of-Thoughts when applied to our Sc-\nITAB dataset. Below, we provide explanations for\neach of the error cases.\n\nError Case 1. It exhibits incorrect entity linking\n(Grounding error) and incorrect operation (Pro-\ngram error). The codes “winograd_baseline\n= 73.06” and “winocoref_baseline = 88.48”\nshould be “I11iCons_winograd = 53.26” and\n", "vlm_text": "The table lists different reasons for refuting information along with their respective proportions (in percentage). \n\n- Negation (+not) and paraphrasing: 85%\n- The calculation result is wrong: 6%\n- The commonsense knowledge is wrong: 6%\n- The open-domain knowledge is wrong: 3%\nform. In our pilot annotation, we found that man- ual verification played the most essential role in the validation of claims marked as “Not Enough Infor- mation(NEI)”. Initially, we planned to rely solely on LLMs for generating NEI claims. Our criteria for the NEI claim is that “the claim should be flu- ent, logical, and relevant to the table. However, the claim cannot be verified as true or false solely based on the information in the table.” However, after a careful examination of the LLM output, we found that LLM tends to generate claims that are either not logical or irrelevant to the table content. Therefore, human efforts are required to further select NEI claims that meet our criteria. Out of 900 initial NEI claims generated by LLMs, man- ual verification narrowed them down to only 355 claims, taking up  $40\\%$   of the original count. While it may not have served as crucial a role as filtering NEI claims, human verification also safeguarded the data quality in other annotation processes. For example, among the “supported” claims originally appearing in the scientific paper, human validation still identified 10 cases that were actually not sup- ported ( e.g ., wrong number matching.) \nE Case Study for Refuted Claims \nFigure  9  and Figure  10  show five examples of re- futed cases. Below, we provide explanations for each of these error cases. \nCase  A  The calculation result is wrong. It pro- duces incorrect calculation results. The accurate result should be    $27.9{-}21.7\\:=\\:6\\,.\\,2.$  . \nCase  $\\pmb{B}$   The approximation word is wrong. It generates incorrect approximation words, as 19.4 is not significantly lower compared to 23.3. \nCase    $c$   The claim is partially right. The claim is generally correct, with the exception of the BShift column  which does not fulfill the claim. \nCase  $\\pmb{D}$   The values in the claim do not match. The value in the claim does not align with the cor- responding value in the table. The correct value should be  27.9 . \n\nCase  $\\pmb{{\\cal E}}$   The operation type is wrong. It applies the incorrect operation type. For instance, in the case of    $\\mathsf{G C N}\\!\\!+\\!\\mathsf{R C}\\!+\\!\\mathsf{L A}$   (9) , it is not accurate to claim that it is better than  DCGCN1  because    $22.9\\:>\\:22.\\emptyset$  and   $53\\,.\\,\\emptyset\\ >\\ 52\\,.\\,6$  . \nF Error Cases for Instruct GP T \nFigure  11  and Figure  12  show six error examples of Instruct GP T in the zero-shot setting when applied to our S CI T AB  dataset. \nError Type 1: Supported predicted as NEI. This error type indicates a discrepancy between the gold label, which is Supported, and the predicted label, which is NEI. \nError Type 2: Refuted predicted as NEI. This error type indicates a discrepancy between the gold label, which is Refuted, and the predicted label, which is NEI. \nError Type 3: Refuted predicted as Supported. This error type indicates a discrepancy between the gold label, which is Refuted, and the predicted label, which is Supported. \nError Type 4: NEI predicted as Supported. This error type indicates a discrepancy between the gold label, which is NEI, and the predicted label, which is Supported. \nError Type 5: NEI predicted as Refuted. This error type indicates a discrepancy between the gold label, which is NEI, and the predicted label, which is Refuted. \nError Type 6: Supported predicted as Refuted. This error type indicates a discrepancy between the gold label, which is Supported, and the predicted label, which is Refuted. \nG Error Cases for Program-of-Thoughts \nFigure  13  and Figure  14  show five error examples of Program-of-Thoughts when applied to our S C - I T AB  dataset. Below, we provide explanations for each of the error cases. \nError Case 1. It exhibits incorrect entity linking ( Grounding error ) and incorrect operation   $(P r o-$  gram error ). The codes “ wino grad baseline  $=\\;73.\\theta6^{\\ast}$   and “ wino core f baseline  $=\\ 88.48\"$  should be “ Ill iCons wino grad   $=\\ 53.26^{\\circ}$   and "}
{"page": 14, "image_path": "doc_images/2305.13186v3_14.jpg", "ocr_text": "1D: 2-2\nPaper ID: 1805.11461v1\n\nPaper Name: Syntactic Dependency Representations in Neural Relation Classification\n\nRepresentation Hyper Hyper parameters Hyper Hyper\nparameters Num. Feature parameters parameters L2\nFilter size maps Activation func. Reg.\nCoNLLOS 4s 1000 Softplus Laset01 1.13¢-03\nSB 45 806 Sigmoid 8.13e-02\nUDvI3 5 716 Softplus 1.66e+00\n\nHyper\nparameters\nLearning rate\n\n1.79e-03\n\n9.63E-04\n\nHyper\nparameters\n\nDropout Prob.\n\n087\n\nFlfavg.inS- Fi. (avg. in 5-\nfold) with fold) with\ndefault values optimal yalues\n\nB34 7449\n72.83 75.05\n68.93 69.57\n\nCaption: Table 2: Hyper parameter optimization results for each model with different representation. The max pooling strategy consistently performs better in all mode!\n\nvariations.\n\nSentence to annotate:\n\nThe results furthermore show that the sdps based on the Stanford Basic (SB) representation provide the best performance,\n\nfollowed by the CoNLLO8 representation.\n> Description\n\n© Background\n@© Claim\n\nProgress: [fj\n\nFigure 5: The user interface for the claim extraction task.\n\n1D: 0-0-0\nPaper ID: 1911.00225v1\n\nPaper: When Choosing Plausible Alternatives, Clever Hans can be Clever\n\n© Table\nModel Training data\nBERT-large-FT B-COPA\nBERT-large-FT B-COPA (50%)\nBERT-large-FT COPA\nRoBERTa-large-FT B-COPA\nRoBERTa-large-FT B-COPA (50%)\nRoBERTa-large-FT COPA\n\nOverall\n\n745 (#0.7)\n\n74.3 (42.2)\n\n765 (2.7)\n\n89.0 (+ 0.3)\n\n86.1 (2.2)\n\n87.7 (+09)\n\nEasy\n\n74,7 (404)\n\n76.8 (+ 1.9)\n\n83.9 (+44)\n\n88.9 (+ 2.1)\n\n87.4 (+ 1.1)\n\n91.6 (+ 1.1)\n\nHard\n\n74.4 (£0,9)\n\n72.8 (3.1)\n\n71.9 (42.5)\n\n89.0 (+ 0.8)\n\n85.4 (+ 2.9)\n\n85.3 (+ 2.0)\n\nCaption: Table 5: Results of fine-tuned models on Balanced COPA. Easy: instances with superficial cues, Hard: instances without superficial cues.\n\n© Claim\n\nFor both BERT and RoBERTa, more data leads to more performance improvements, although the growth rate on ROBERTa is slightly faster,\n\nFigure 6: The user interface for the NEI claim verification task.\n\n“T11iCons_winocoref = 74.32” respectively. Ad-\nditionally, the “>” operation should be changed to\n\n“ye\n>=\".\n\nError Case 2. It exhibits incomplete entity link-\ning (Grounding error). The program should also\n\nparse other baseline results, such as ‘SFEGAN_WER\n= 14.9”.\n\nError Case 3.\n\nIt fails to generate a correct pro-\ngram (Program error). The variables and logical\nfunctions in the programs are incorrect. For in-\n", "vlm_text": "\nThe table presents a comparison of different representations and their corresponding hyperparameters, along with the F1 scores in 5-fold validation.\n\n- **Columns**:\n  - Representation\n  - Hyperparameters: Filter size, Num. Feature maps, Activation function, L2 Regularization, Learning rate, Dropout Probability\n  - F1 score (average in 5-fold) with default values\n  - F1 score (average in 5-fold) with optimal values\n\n- **Rows**:\n  1. **CoNLL08**\n     - Filter size: 4-5\n     - Num. Feature maps: 1000\n     - Activation function: Softplus\n     - L2 Regularization: 1.15e+01\n     - Learning rate: 1.13e-03\n     - Dropout Probability: 1\n     - F1 with default: 73.34\n     - F1 with optimal: 74.49\n\n  2. **SB**\n     - Filter size: 4-5\n     - Num. Feature maps: 806\n     - Activation function: Sigmoid\n     - L2 Regularization: 8.13e-02\n     - Learning rate: 1.79e-03\n     - Dropout Probability: 0.87\n     - F1 with default: 72.83\n     - F1 with optimal: 75.05\n\n  3. **UD v1.3**\n     - Filter size: 5\n     - Num. Feature maps: 716\n     - Activation function: Softplus\n     - L2 Regularization: 1.66e+00\n     - Learning rate: 9.63e-04\n     - Dropout Probability: 1\n     - F1 with default: 68.93\n     - F1 with optimal: 69.57\n\n\nThe image shows a user interface for a claim extraction task. It includes radio button options labeled \"Description,\" \"Background,\" and \"Claim,\" with \"Claim\" selected. Below these options is a green button labeled \"Annotate.\" At the bottom, there's a progress bar with a label \"Progress:\" and a partial blue fill indicating progress.\n\n\nThe table displays the results of fine-tuned models on Balanced COPA. It includes comparisons of BERT-large-FT and RoBERTa-large-FT models using different training data:\n\n1. **Model**: Names of the models (BERT-large-FT and RoBERTa-large-FT).\n2. **Training Data**: Different training datasets used (B-COPA, B-COPA (50%), and COPA).\n3. **Overall**: Overall performance scores, with standard deviation in parentheses.\n4. **Easy**: Performance scores on easy instances (with superficial cues), plus standard deviation.\n5. **Hard**: Performance scores on hard instances (without superficial cues), with standard deviation.\n\nThe caption notes that \"Easy\" instances have superficial cues, while \"Hard\" instances do not. The table suggests that more data leads to improved performance, with a slightly faster growth rate for RoBERTa.\n“ Ill iCons wino core f  $\\dot{{\\,}}=74\\,.\\,32{}^{\\circ}$   respectively. Ad- ditionally, the “ > ” operation should be changed to  $\\left.>=\\right.^{\\ast}$  . \nError Case 2. It exhibits incomplete entity link- ing ( Grounding error ). The program should also parse other baseline results, such as ‘ SFEGAN_WER  $=\\ 14.9^{\\circ}$  . \n\nError Case 3. It fails to generate a correct pro- gram ( Program error ). The variables and logical functions in the programs are incorrect. For in- "}
{"page": 15, "image_path": "doc_images/2305.13186v3_15.jpg", "ocr_text": "1D: train-O-1\n\nPaper ID: 1911.00225v1\n\nPaper: When Choosing Plausible Alternatives, Clever Hans can be Clever\n\nB Table\nModel Training data Overall Easy Hard\nBERT-large-FT B-COPA 745 (£07) 74.1 (£04) 144 (£09)\nBERT-large-FT B-COPA (50%) 743 (£22) 768 (+ 1.9) 728 (3.1)\nBERT-large-FT COPA 765 (£27) 83.9 (44.4) 119 (£25)\nRoBERTa-large-FT B-COPA 89.0 (+0.3) 88.9 (+21) 89.0 (+08)\nRoBERTa-large-FT B-COPA (50%) 86.1 (#22) 874 (41.1) 854 (42.9)\nRoBERTa-large-FT COPA 87.7 (£09) 91.6 (£11) 85.3 (£20)\n\nCaption: Table 5: Results of fine-tuned models on Balanced COPA. Easy: instances with superficial cues, Hard: instances without superficial cues.\n\n© Original Claim\n\nThe smaller performance gap between Easy and Hard subsets indicates that training on BCOPA encourages BERT and RoBERTa to rely less on\nsuperficial cues.\n\nQ Generated Refuted Claim\n\nThe larger performance gap between Easy and Hard subsets indicates that training on BCOPA encourages BERT and RoBERTa to rely more on\nsuperficial cues.\n\nFigure 7: The user interface for the refuted claim verification task\n\nstance, ““G2S_GAT_BLEU_LDC2015E86” should be\n“G2S_GIN_BLEU_LDC2015E86”. The logical func-\ntion “and” should be replaced with “or”.\n\nError Case 4. It fails to generate a precise pro-\ngram for the approximation word “comparable”\n(Ambiguity error). Currently, the program defines\n“comparable” as “larger than”, which is not accurate\nenough.\n\nError Case 5. It generates the correct program,\nbut the calculation result is inaccurate due to incor-\nrect float digits in the Python code (Calculation er-\nror). For instance, Python may output ’1.9499999”,\nwhich is not equal to *1.95’.\n", "vlm_text": "\n\n\nThe image contains a table showing the performance of fine-tuned models on Balanced COPA. It compares the results for models BERT-large-FT and RoBERTa-large-FT, using different training datasets. The performance is presented in terms of overall score, as well as scores on \"Easy\" and \"Hard\" instances. Below the table, there's a section titled \"Original Claim\" which discusses the performance gap between the Easy and Hard subsets, suggesting that training on BCOPA might encourage the models to rely less on superficial cues. The \"Generated Refuted Claim\" presents a reverse interpretation, indicating reliance on superficial cues.\nstance, “ G 2 S GAT BLEU L DC 2015 E 86 ” should be “ G 2 S G IN BLEU L DC 2015 E 86 ”. The logical func- tion “ and ” should be replaced with “ or ”. \nError Case 4. It fails to generate a precise pro- gram for the approximation word “comparable”\n\n ( Ambiguity error ). Currently, the program defines\n\n “comparable” as “larger than”, which is not accurate enough. \nError Case 5. It generates the correct program, but the calculation result is inaccurate due to incor- rect float digits in the Python code ( Calculation er- ror ). For instance, Python may output ’1.9499999’, which is not equal to   ${}^{'}1.95'$  . "}
{"page": 16, "image_path": "doc_images/2305.13186v3_16.jpg", "ocr_text": "Annotation Post Survey\n\nAnnotator 1:\n\nIs the task demonstration clear to you?\n\nYes, clear.\n\nWhat do you think is the difficulty of this task? (1-10 points, 10 points is the most\ndifficult)\n\n5-6.\n\nWhich part is the most difficult for you? Why?\n\nJudgment, understanding the way of original author think.\n\nDo you think the annotation batch is appropriate? What is the maximum batch\namount for you in a week?\n\nYes. 2 batches in a week during the examination. 4 during vacation.\n\nCould you provide some advice on how to improve the annotation platform?\nLooping for multiple operations.\n\nAnnotator 2:\n\nIs the task demonstration clear to you?\n\nYes.\n\nWhat do you think is the difficulty of this task? (1-10 points, 10 points is the most\ndifficult)\n\n6\n\nWhich part is the most difficult for you? Why?\n\nTable understanding; different parameters in the attributes.\n\nDo you think the annotation batch is appropriate? What is the maximum batch\namount for you in a week?\n\nOk. 2-3 batches\n\nWould you like to attend this session again as a 2-week participation?\n\nok.\n\nCould you provide some advice on how to improve the annotation platform?\n\n| preferred to write down the annotations on the platform.\n\nAnnotator 3:\n\nIs the task demonstration clear to you?\n\nYes, clear. the difficulty is different between demo and real annotation.\n\nWhat do you think is the difficulty of this task? (1-10 points, 10 points is the most\ndifficult)\n\n7\n\nWhich part is the most difficult for you? Why?\n\nTable understanding-vocabulary.\n\nDo you think the sample amount is appropriate? What is the maximum batch amount\nfor you in a week (1 batch contains 20 samples)?\n\n10-15 samples for an hour. 50 samples a week.\n\nWould you like to attend this session again as a 2-week participation?\nMaybe not. But 15 samples offline for a week is ok.\n\nCould you provide some advice on how to improve the annotation platform?\n| think the current platform is fine for me.\n\nFigure 8: The examples of post-annotation survey questions and the answers of annotators.\n", "vlm_text": "Annotation Post Survey  \nAnnotator 1:  \n•   Is the task demonstration clear to you?  Yes,  clear.  \n\n •   What do you think is the difficulty of this task?  (1-10 points, 10 points is the most  difficult)  5-6.  \n\n •   Which part is the most difficult for you? Why?  Judgment, understanding the way of original author think.  \n\n •   Do you think the annotation batch is appropriate? What is the maximum batch  amount for you in a week?   Yes. 2 batches in a week during the examination. 4 during vacation.  \n\n •   Could you provide some advice on how to improve the annotation platform?    Looping for multiple operations.   \n\n \nAnnotator 2:   \n•   Is the task demonstration clear to you?  Yes.  \n\n •   What do you think is the difficulty of this task?  (1-10 points, 10 points is the most  difficult)  6  \n\n •   Which part is the most difficult for you? Why?  Table understanding; different parameters in the attributes.  \n\n •   Do you think the annotation batch is appropriate? What is the maximum batch  amount for you in a week?   Ok. 2-3 batches  \n\n •   Would you like to attend this session again as a 2-week participation?   ok.  \n\n •   Could you provide some advice on how to improve the annotation platform?   I preferred to write down the annotations on the platform.  \n\n \nAnnotator 3:   \n•   Is the task demonstration clear to you?  Yes, clear. the difficulty is different between demo and real annotation. \n\n •   What do you think is the difficulty of this task?  (1-10 points, 10 points is the most  difficult)  7 \n\n •   Which part is the most difficult for you? Why?  Table understanding-vocabulary.  \n\n •   Do you think the sample amount is appropriate? What is the maximum batch amount  for you in a week (1 batch contains 20 samples)?   10-15  samples for an hour. 50 samples a week.    \n\n •   Would you like to attend this session again as a 2-week participation?    Maybe not. But 15 samples offline for a week is ok.     \n\n •   Could you provide some advice on how to improve the annotation platform?    I think the current platform is fine for me.  \nFigure 8: The examples of post-annotation survey questions and the answers of annotators. "}
{"page": 17, "image_path": "doc_images/2305.13186v3_17.jpg", "ocr_text": "H Prompts\nH.1 Zero-shot Prompts\n\nTable: <input_table>\nClaim: <input_claim>\n\nBased on the information in the Table, is the above claim true?\nA) the claim is true.\n\nB) the claim is false.\n\nC) it is impossible to tell.\n\nH.2 Few-shot Prompts\n\nRead the following table and then answer a question.\n\nCaption: Table 5: Results of fine-tuned models on Balanced COPA. Easy: instances with superficial cues,\nHard: instances without superficial cues.\n\nTa\n|| Model | Training data | Overall | Easy | Hard ||\n\n|| BERT-large-FT | B-COPA | 74.5 (40.7) | 74.7 (40.4) | 74.4 (40.9) II\n\n|| BERT-large-FT | B-COPA (50%) | 74.3 (42.2) | 76.8 (£1.9) | 72.8 (43.1) II\ni\n\ni\n\ni\n\nif\n\nBERT-large-FT | COPA | 76.5 (+2.7) | 83.9 (44.4) | 71.9 (42.5) II\nRoBERTa-large-FT | B-COPA | 89.0 (+0.3) | 88.9 (+2.1) | 89.@ (+0.8) ||\nRoBERTa-large-FT | B-COPA (50%) | 86.1 (+2.2) | 87.4 (1.1) | 85.4 (+2.9) II\nRoBERTa-large-FT | COPA | 87.7 (+0.9) | 91.6 (+1.1) | 85.3 (+2.0) |]\n\nClaim: RoBERTa-large outperforms BERT-large when fine-tuned on full and balanced COPA.\nQuestion: Is the above claim true or false? Please directly give the answer.\n\nAnswer:\n\nThe claim is true.\n\nCaption: Table 5: Results of fine-tuned models on Balanced COPA. Easy: instances with superficial cues,\nHard: instances without superficial cues.\n\nModel | Training data | Overall | Easy | Hard ||\nBERT-large-FT | B-COPA | 74.5 (40.7) | 74.7 (40.4) | 74.4 (40.9) II\n\nTal\nI\nI\n|| BERT-large-FT | B-COPA (50%) | 74.3 (42.2) | 76.8 (41.9) | 72.8 (43.1) II\nI\nI\nI\nI\n\nBERT-large-FT | COPA | 76.5 (+2.7) | 83.9 (44.4) | 71.9 (+2.5) II\nRoBERTa-large-FT | B-COPA | 89.0 (+0.3) | 88.9 (+2.1) | 89.@ (+0.8) ||\nRoBERTa-large-FT | B-COPA (50%) | 86.1 (+2.2) | 87.4 (+1.1) | 85.4 (+2.9) II\nRoBERTa-large-FT | COPA | 87.7 (+0.9) | 91.6 (+1.1) | 85.3 (+2.0) |]\n\nClaim: The difference between RoBERTa-large-FT and BERT-large-FT is 3.8 points on B-COPA,\nwhich is significantly smaller than the difference in COPA.\n\nQuestion: Is the above claim true or false? Please directly give the answer.\n\nAnswer:\nThe claim is false.\n\nCaption: Table 4: The ablation study on the WoZ2.0 dataset with the joint goal accuracy on the test set.\n\nFor~*- Hierachical-Attn'', we remove the residual connections between the attention modules in the CMR\ndecoders and all the attention memory access are based on the output from the LSTM.\nFor~*- MLP'', we further replace the MLP with a single linear layer with the non-linear activation.\n\nTable:\n\n|| Model | Joint Acc. ||\n\n|| COMER | 88.64% ||\n\n|| - Hierachical-Attn | 86.69% ||\nI] - MLP | 83.24% ||\n\nClaim: [CONTINUE] The effectiveness of our hierarchical attention design is proved by an accuracy drop\nof 1.95% after removing residual connections and the hierarchical stack of our attention modules.\n\nQuestion: Is the above claim true or false? Please directly give the answer.\n\nAnswer:\nThe claim is true.\n\nCaption: Table 4: Scores for different training objectives on the linguistic probing tasks.\n\nTable:\n\n|| Method | Depth| BShift| SubjNum | Tense | CoordInv | Length | ObjNum | TopConst | SOMO | WC |]\nI] CMow-c | 36.2 | 66.0 | 81.1 | 78.7 | 61.7 | 83.9 | 79.1 | 73.6 | 50.4 | 66.8 ||\n\nI] CMOW-R | 35.1 | 70.8 | 82.0 | 80.2 | 61.8 | 82.8 | 79.7 | 74.2 | 50.7 | 72.9 ||\n\nI] CBOW-c | 34.3 | 50.5 | 79.8 | 79.9 | 53.0 | 75.9 | 79.8 | 72.9 | 48.6 | 89.0 ||\n\n|| CBOW-R | 33.0 | 49.6 | 79.3 | 78.4 | 53.6 | 74.5 | 78.6 | 72.0 | 49.6 | 89.5 ||\n\n", "vlm_text": "H Prompts \nH.1 Zero-shot Prompts \nTable :  <input table> \nClaim :  <input claim> \nBased on the information in the Table , is the above claim true? A) the claim is true. B) the claim is false. C) it is impossible to tell. \nH.2 Few-shot Prompts \nRead the following table and then answer a question. \nCaption : Table 5: Results of fine -tuned models on Balanced COPA. Easy: instances with superficial cues , Hard: instances without superficial cues.\n\n \nTable \n|| Model | Training data | Overall | Easy | Hard ||\n\n || BERT -large -FT | B-COPA | 74.5 ( ± 0.7) | 74.7 ( ± 0.4) | 74.4 ( ± 0.9) ||\n\n || BERT -large -FT | B-COPA (50%) | 74.3 ( ± 2.2) | 76.8 ( ± 1.9) | 72.8 ( ± 3.1) ||\n\n || BERT -large -FT | COPA | 76.5 ( ± 2.7) | 83.9 ( ± 4.4) | 71.9 ( ± 2.5) ||\n\n || RoBERTa -large -FT | B-COPA | 89.0 ( ± 0.3) | 88.9 ( ± 2.1) | 89.0 ( ± 0.8) ||\n\n || RoBERTa -large -FT | B-COPA (50%) | 86.1 ( ± 2.2) | 87.4 ( ± 1.1) | 85.4 ( ± 2.9) ||\n\n || RoBERTa -large -FT | COPA | 87.7 ( ± 0.9) | 91.6 ( ± 1.1) | 85.3 ( ± 2.0) || \nClaim : RoBERTa -large outperforms BERT -large when fine -tuned on full and balanced COPA. \nQuestion : Is the above claim true or false? Please directly give the answer. \nAnswer \nThe claim is true. \nCaption : Table 5: Results of fine -tuned models on Balanced COPA. Easy: instances with superficial cues , Hard: instances without superficial cues.\n\n \nTable \n|| Model | Training data | Overall | Easy | Hard ||\n\n || BERT -large -FT | B-COPA | 74.5 ( ± 0.7) | 74.7 ( ± 0.4) | 74.4 ( ± 0.9) ||\n\n || BERT -large -FT | B-COPA (50%) | 74.3 ( ± 2.2) | 76.8 ( ± 1.9) | 72.8 ( ± 3.1) ||\n\n || BERT -large -FT | COPA | 76.5 ( ± 2.7) | 83.9 ( ± 4.4) | 71.9 ( ± 2.5) ||\n\n || RoBERTa -large -FT | B-COPA | 89.0 ( ± 0.3) | 88.9 ( ± 2.1) | 89.0 ( ± 0.8) ||\n\n || RoBERTa -large -FT | B-COPA (50%) | 86.1 ( ± 2.2) | 87.4 ( ± 1.1) | 85.4 ( ± 2.9) ||\n\n || RoBERTa -large -FT | COPA | 87.7 ( ± 0.9) | 91.6 ( ± 1.1) | 85.3 ( ± 2.0) || \nClaim : The difference between RoBERTa -large -FT and BERT -large -FT is 3.8 points on B-COPA , which is significantly smaller than the difference in COPA. \nQuestion : Is the above claim true or false? Please directly give the answer. \nAnswer \nThe claim is false. \nCaption : Table 4: The ablation study on the WoZ2.0 dataset with the joint goal accuracy on the test set. For  \\`\\` - Hier a chic al -Attn'', we remove the residual connections between the attention modules in the CMR decoders and all the attention memory access are based on the output from the LSTM. For  \\`\\` - MLP'', we further replace the MLP with a single linear layer with the non -linear activation.\n\n \nTable : \n|| Model | Joint Acc. ||\n\n || COMER | 88.64% ||\n\n || - Hierachical -Attn | 86.69% ||\n\n || - MLP | 83.24% || \nClaim : [CONTINUE] The effectiveness of our hierarchical attention design is proved by an accuracy drop of 1.95% after removing residual connections and the hierarchical stack of our attention modules. \nQuestion : Is the above claim true or false? Please directly give the answer. \nAnswer \nThe claim is true. \nCaption : Table 4: Scores for different training objectives on the linguistic probing tasks.\n\n \nTable \n|| Method | Depth| BShift| SubjNum | Tense | CoordInv | Length | ObjNum | TopConst | SOMO | WC ||\n\n || CMOW -C | 36.2 | 66.0 | 81.1 | 78.7 | 61.7 | 83.9 | 79.1 | 73.6 | 50.4 | 66.8 ||\n\n || CMOW -R | 35.1 | 70.8 | 82.0 | 80.2 | 61.8 | 82.8 | 79.7 | 74.2 | 50.7 | 72.9 ||\n\n || CBOW -C | 34.3 | 50.5 | 79.8 | 79.9 | 53.0 | 75.9 | 79.8 | 72.9 | 48.6 | 89.0 ||\n\n || CBOW -R | 33.0 | 49.6 | 79.3 | 78.4 | 53.6 | 74.5 | 78.6 | 72.0 | 49.6 | 89.5 || "}
{"page": 18, "image_path": "doc_images/2305.13186v3_18.jpg", "ocr_text": "Claim: While CMOW-R and CMOW-C perform comparably on most probing tasks,\nCMOW-C yields 5 points higher scores on WordContent and BigramShift.\n\nQuestion: Is the above claim true or false? Please directly give the answer.\n\nAnswer:\nThe claim is false.\n\n(--+ more in-context examples here -- -)\n\nCaption: <input_caption>\n\nTable: <input_table>\n\nClaim: <input_claim>\n\nQuestion: Is the above claim true or false? Please directly give the answer.\n\nAnswer:\n\nH.3 Chain-of-Thought Prompts\n\nRead the following table and then answer a question.\n\nCaption: Table 5: Results of fine-tuned models on Balanced COPA. Easy: instances with superficial cues,\nHard: instances without superficial cues.\n\nModel | Training data | Overall | Easy | Hard ||\nBERT-large-FT | B-COPA | 74.5 (40.7) | 74.7 (40.4) | 74.4 (40.9) II\n\nTal\nI\nI\n|| BERT-large-FT | B-COPA (50%) | 74.3 (42.2) | 76.8 (41.9) | 72.8 (43.1) II\nI\nI\nI\nI\n\nBERT-large-FT | COPA | 76.5 (+2.7) | 83.9 (44.4) | 71.9 (+2.5) II\nRoBERTa-large-FT | B-COPA | 89.0 (+0.3) | 88.9 (+2.1) | 89.@ (+0.8) ||\nRoBERTa-large-FT | B-COPA (50%) | 86.1 (+2.2) | 87.4 (1.1) | 85.4 (+2.9) II\nRoBERTa-large-FT | COPA | 87.7 (+0.9) | 91.6 (+1.1) | 85.3 (+2.0) |]\n\nClaim: RoBERTa-large outperforms BERT-large when fine-tuned on full and balanced COPA.\nQuestion: Is the above claim true or false? Let us think step by step.\n\nReasoning:\nWe need to compare the performance of RoBERTa-large and BERT-large on full and balanced COPA.\n\nFor full COPA, RoBERTa-large has an overall performance of 87.7 (table[6,2]),\n\nwhile BERT-large has an overall performance of 76.5 (table[3,2]).\n\nTherefore, RoBERTa-large outperforms BERT-large on full COPA.\n\nFor balanced COPA, RoBERTa-large has an overall performance of 89.0 (table[4,2]),\n\nwhile BERT-large has an overall performance of 74.5 (table[1,2]).\n\nTherefore, RoBERTa-large outperforms BERT-large on balanced COPA.\n\nFinally, since RoBERTa-large outperforms BERT-large on both full and balanced COPA, the Claim is true.\n\nAnswer:\n\nTherefore, the claim is true.\n\nCaption: Table 5: Results of fine-tuned models on Balanced COPA. Easy: instances with superficial cues,\nHard: instances without superficial cues.\n\nModel | Training data | Overall | Easy | Hard ||\nBERT-large-FT | B-COPA | 74.5 (40.7) | 74.7 (40.4) | 74.4 (40.9) II\n\nTal\nI\nI\n|| BERT-large-FT | B-COPA (50%) | 74.3 (42.2) | 76.8 (41.9) | 72.8 (43.1) II\nI\nI\nI\nI\n\nBERT-large-FT | COPA | 76.5 (+2.7) | 83.9 (44.4) | 71.9 (42.5) II\nRoBERTa-large-FT | B-COPA | 89.0 (+0.3) | 88.9 (+2.1) | 89.@ (+0.8) ||\nRoBERTa-large-FT | B-COPA (50%) | 86.1 (+2.2) | 87.4 (1.1) | 85.4 (+2.9) II\nRoBERTa-large-FT | COPA | 87.7 (+0.9) | 91.6 (+1.1) | 85.3 (+2.0) |]\n\nClaim: The difference between RoBERTa-large-FT and BERT-large-FT is 3.8 points on B-COPA,\nwhich is significantly smaller than the difference in COPA.\n\nQuestion: Is the above claim true or false? Let us think step by step.\n\nReasoning:\nTo verify this claim, we need to compare the performance difference between RoBERTa-large-FT\nand BERT-large-FT on B-COPA and COPA.\n\nThe difference between RoBERTa-large-FT and BERT-large-FT on B-COPA is 89.0 - 74.5 = 14.5\n(table[4,2] - table[1,2]).\n\nThe difference between RoBERTa-large-FT and BERT-large-FT on COPA is 87.7 - 76.5 = 11.2\n(table([6,2] - table[3,2]).\n\nThe difference between RoBERTa-large-FT and BERT-large-FT on B-COPA is significantly larger\nthan the difference on COPA. Therefore, the Claim is false.\n\nAnswer:\nTherefore, the claim is false.\n\nCaption: Table 4: The ablation study on the WoZ2.0 dataset with the joint goal accuracy on the test set.\n\n", "vlm_text": "Claim : While CMOW -R and CMOW -C perform comparably on most probing tasks , CMOW -C yields 5 points higher scores on Word Content and Big ram Shift. \nQuestion : Is the above claim true or false? Please directly give the answer. \nAnswer \nThe claim is false. (  more in-context examples here \nCaption :  <input caption> \nTable :  <input table> \nClaim :  <input claim> \nQuestion : Is the above claim true or false? Please directly give the answer. \nAnswer : \nH.3 Chain-of-Thought Prompts \nRead the following table and then answer a question. \nCaption : Table 5: Results of fine -tuned models on Balanced COPA. Easy: instances with superficial cues , Hard: instances without superficial cues.\n\n \nTable \n|| Model | Training data | Overall | Easy | Hard ||\n\n || BERT -large -FT | B-COPA | 74.5 ( ± 0.7) | 74.7 ( ± 0.4) | 74.4 ( ± 0.9) ||\n\n || BERT -large -FT | B-COPA (50%) | 74.3 ( ± 2.2) | 76.8 ( ± 1.9) | 72.8 ( ± 3.1) ||\n\n || BERT -large -FT | COPA | 76.5 ( ± 2.7) | 83.9 ( ± 4.4) | 71.9 ( ± 2.5) ||\n\n || RoBERTa -large -FT | B-COPA | 89.0 ( ± 0.3) | 88.9 ( ± 2.1) | 89.0 ( ± 0.8) ||\n\n || RoBERTa -large -FT | B-COPA (50%) | 86.1 ( ± 2.2) | 87.4 ( ± 1.1) | 85.4 ( ± 2.9) ||\n\n || RoBERTa -large -FT | COPA | 87.7 ( 0.9) | 91.6 ( 1.1) | 85.3 ( 2.0) || \nClaim : RoBERTa -large outperforms BERT -large when fine -tuned on full and balanced COPA. \nQuestion : Is the above claim true or false? Let us think step by step. \nReasoning : \nWe need to compare the performance of RoBERTa -large and BERT -large on full and balanced COPA. For full COPA , RoBERTa -large has an overall performance of 87.7 (table [6,2]), while BERT -large has an overall performance of 76.5 (table [3 ,2]). Therefore , RoBERTa -large outperforms BERT -large on full COPA. For balanced COPA , RoBERTa -large has an overall performance of 89.0 (table [4,2]), while BERT -large has an overall performance of 74.5 (table [1 ,2]). Therefore , RoBERTa -large outperforms BERT -large on balanced COPA. \nFinally , since RoBERTa -large outperforms BERT -large on both full and balanced COPA , the Claim is true. \nAnswer : \nTherefore , the claim is true. \nCaption : Table 5: Results of fine -tuned models on Balanced COPA. Easy: instances with superficial cues , Hard: instances without superficial cues.\n\n \nTable : \n|| Model | Training data | Overall | Easy | Hard ||\n\n || BERT -large -FT | B-COPA | 74.5 ( ± 0.7) | 74.7 ( ± 0.4) | 74.4 ( ± 0.9) ||\n\n || BERT -large -FT | B-COPA (50%) | 74.3 ( ± 2.2) | 76.8 ( ± 1.9) | 72.8 ( ± 3.1) ||\n\n || BERT -large -FT | COPA | 76.5 ( ± 2.7) | 83.9 ( ± 4.4) | 71.9 ( ± 2.5) ||\n\n || RoBERTa -large -FT | B-COPA | 89.0 ( ± 0.3) | 88.9 ( ± 2.1) | 89.0 ( ± 0.8) ||\n\n || RoBERTa -large -FT | B-COPA (50%) | 86.1 ( ± 2.2) | 87.4 ( ± 1.1) | 85.4 ( ± 2.9) ||\n\n || RoBERTa -large -FT | COPA | 87.7 ( 0.9) | 91.6 ( 1.1) | 85.3 ( 2.0) || \nClaim : The difference between RoBERTa -large -FT and BERT -large -FT is 3.8 points on B-COPA , which is significantly smaller than the difference in COPA. \nQuestion : Is the above claim true or false? Let us think step by step. \nReasoning : \nTo verify this claim , we need to compare the performance difference between RoBERTa -large -FT and BERT -large -FT on B-COPA and COPA. The difference between RoBERTa -large -FT and BERT -large -FT on B-COPA is   $89\\cdot\\theta\\;\\;-\\;\\;74\\;.5\\;\\;=\\;\\;1\\,4\\;.\\;5$  (table [4,2] - table [1 ,2]). The difference between RoBERTa -large -FT and BERT -large -FT on COPA is   $87\\cdot7\\;\\textrm{\\--}\\;76\\,.\\,5\\;\\textrm{=}\\;\\;1\\,1\\,.\\,2$  (table [6,2] - table [3 ,2]). The difference between RoBERTa -large -FT and BERT -large -FT on B-COPA is significantly larger than the difference on COPA. Therefore , the Claim is false. \nAnswer : \nTherefore , the claim is false. \nCaption : Table 4: The ablation study on the WoZ2.0 dataset with the joint goal accuracy on the test set. "}
{"page": 19, "image_path": "doc_images/2305.13186v3_19.jpg", "ocr_text": "For~*- Hierachical-Attn'', we remove the residual connections between the attention modules in the CMR\ndecoders and all the attention memory access are based on the output from the LSTM.\n\nFor\"*- MLP'', we further replace the MLP with a single linear layer with the non-linear activation.\nTable:\n\n|| Model | Joint Acc. ||\n|| COMER | 88.64% ||\n\n|| - Hierachical-Attn | 86.69% ||\n\nI] - MLP | 83.24% |]\n\nClaim: [CONTINUE] The effectiveness of our hierarchical attention design is proved by an accuracy drop\nof 1.95% after removing residual connections and the hierarchical stack of our attention modules.\n\nQuestion: Is the above claim true or false? Let us think step by step.\n\nReasoning:\nTo verify this claim, we need to compare the performance of COMER with\n\nand without hierarchical attention design.\n\nThe performance of COMER with hierarchical attention design is 88.64% (table[2,2]).\nThe performance of COMER without hierarchical attention design is 86.69% (table[3,2]).\nTherefore, the performance of COMER with hierarchical attention design is higher than\nthe performance of COMER without hierarchical attention design by 88.64 - 86.69 = 1.95.\nTherefore, the Claim is true.\n\nAnswer:\nTherefore, the claim is true.\n\nCaption: Table 4: Scores for different training objectives on the linguistic probing tasks.\n\nTable\n\n|| Method | Depth| BShift| SubjNum | Tense | CoordInv | Length | ObjNum | TopConst | SOMO | WC |]\nI] CMow-c | 36.2 | 66.0 | 81.1 | 78.7 | 61.7 | 83.9 | 79.1 | 73.6 | 50.4 | 66.8 ||\n\nI] CMOW-R | 35.1 | 70.8 | 82.0 | 80.2 | 61.8 | 82.8 | 79.7 | 74.2 | 50.7 | 72.9 ||\n\nI] CBOW-c | 34.3 | 50.5 | 79.8 | 79.9 | 53.0 | 75.9 | 79.8 | 72.9 | 48.6 | 89.0 ||\n\nI] CBOW-R | 33.0 | 49.6 | 79.3 | 78.4 | 53.6 | 74.5 | 78.6 | 72.0 | 49.6 | 89.5 ||\n\nClaim: While CMOW-R and CMOW-C perform comparably on most probing tasks,\nCMOW-C yields 5 points higher scores on WordContent and BigramShift.\n\nQuestion: Is the above claim true or false? Let us think step by step.\n\nReasoning:\nTo verify this claim, we need to compare the performance of CMOW-R\nand CMOW-C on WordContent and BigramShift.\n\nThe performance of CMOW-R on WordContent is 72.9 (table[2,10]).\nThe performance of CMOW-C on WordContent is 66.8 (table[1,10]).\nThe performance of CMOW-R on BigramShift is 70.8 (table[2,2]).\n\nThe performance of CMOW-C on BigramShift is 66.0 (table[1,2]).\nTherefore, the performance of CMOW-C is higher than\n\nthe performance of CMOW-R on WordContent by 66.8 - 72.9 = -6.1.\nThe performance of CMOW-C is lower than\nthe performance of CMOW-R on BigramShift by 66.0 - 70.8 = -4.8.\n\nTherefore, the Claim is false.\n\nAnswer:\nTherefore, the claim is false.\n\n(--+ more in-context examples here -- -)\n\nCaption: <input_caption>\n\nTable: <input_table>\n\nClaim: <input_claim>\n\nQuestion: Is the above claim true or false? Let us think step by step.\nReasoning:\n\nAnswer:\n\nH.4_ Program-of-Thoughts Prompts\n\nRead the following table and then write Python code to answer a question:\n(please call the function equal(a, b) to check whether a and b are equal)\n\nTable 4: The ablation study on the WoZ2.0 dataset with the joint goal accuracy on the test set.\nHierachical-Attn'', we remove the residual connections between the attention modules in the CMR\ndecoders and all the attention memory access are based on the output from the LSTM.\n\nFor~*- MLP'', we further replace the MLP with a single linear layer with the non-linear activation.\n\nTable:\n|| Model | Joint Acc. ||\n\n|| COMER | 88.64% ||\n\n|| - Hierachical-Attn | 86.69% ||\n\n", "vlm_text": "For  \\`\\` - Hier a chic al -Attn'', we remove the residual connections between the attention modules in the CMR decoders and all the attention memory access are based on the output from the LSTM. \nTable :\n\n \n|| Model | Joint Acc. ||\n\n || COMER | 88.64% ||\n\n || - Hierachical -Attn | 86.69% ||\n\n || - MLP | 83.24% || \nClaim : [CONTINUE] The effectiveness of our hierarchical attention design is proved by an accuracy drop of 1.95% after removing residual connections and the hierarchical stack of our attention modules. \nQuestion : Is the above claim true or false? Let us think step by step. \nReasoning \nTo verify this claim , we need to compare the performance of COMER with and without hierarchical attention design. The performance of COMER with hierarchical attention design is 88.64% (table [2 ,2]). The performance of COMER without hierarchical attention design is 86.69% (table [3 ,2]). Therefore , the performance of COMER with hierarchical attention design is higher than the performance of COMER without hierarchical attention design by   $8\\,8\\;.\\;6\\,4\\;\\;-\\;\\;8\\,6\\;.\\;6\\,9\\;\\;=\\;\\;1\\;.\\;95\\;.$  Therefore , the Claim is true. \nAnswer : Therefore , the claim is true. \nCaption : Table 4: Scores for different training objectives on the linguistic probing tasks.\n\n \nTable : \n|| Method | Depth| BShift| SubjNum | Tense | CoordInv | Length | ObjNum | TopConst | SOMO | WC ||\n\n || CMOW -C | 36.2 | 66.0 | 81.1 | 78.7 | 61.7 | 83.9 | 79.1 | 73.6 | 50.4 | 66.8 ||\n\n || CMOW -R | 35.1 | 70.8 | 82.0 | 80.2 | 61.8 | 82.8 | 79.7 | 74.2 | 50.7 | 72.9 ||\n\n || CBOW -C | 34.3 | 50.5 | 79.8 | 79.9 | 53.0 | 75.9 | 79.8 | 72.9 | 48.6 | 89.0 ||\n\n || CBOW -R | 33.0 | 49.6 | 79.3 | 78.4 | 53.6 | 74.5 | 78.6 | 72.0 | 49.6 | 89.5 || \nClaim : While CMOW -R and CMOW -C perform comparably on most probing tasks , CMOW -C yields 5 points higher scores on Word Content and Big ram Shift. \nQuestion : Is the above claim true or false? Let us think step by step. \nReasoning : \nTo verify this claim , we need to compare the performance of CMOW -R and CMOW -C on Word Content and Big ram Shift. The performance of CMOW -R on Word Content is 72.9 (table [2 ,10]). The performance of CMOW -C on Word Content is 66.8 (table [1 ,10]). The performance of CMOW -R on Big ram Shift is 70.8 (table [2 ,2]). The performance of CMOW -C on Big ram Shift is 66.0 (table [1 ,2]). Therefore , the performance of CMOW -C is higher than the performance of CMOW -R on Word Content by   $\\begin{array}{l l l l l l}{{66\\;.\\;8}}&{{\\;-\\;}}&{{72\\;.\\;9}}&{{=}}&{{-\\,6\\;.\\;1}}\\end{array}$  . The performance of CMOW -C is lower than the performance of CMOW -R on Big ram Shift by   $\\begin{array}{r c l}{66\\;.\\;\\emptyset}&{{}{\\mathrm{~-~}}}&{7\\emptyset\\;.\\;8}&{{}{\\mathrm{~=~}}}&{{\\mathrm{~-~}}4\\;.\\;8}\\end{array}$  . Therefore , the Claim is false. \nAnswer \nTherefore , the claim is false. \n(  more in-context examples here ) \nTable :  <input table> \nClaim :  <input claim> \nQuestion : Is the above claim true or false? Let us think step by step. \nReasoning \nAnswer \nH.4 Program-of-Thoughts Prompts \nRead the following table and then write Python code to answer a question: (please call the function equal(a, b) to check whether a and b are equal) \nCaption : Table 4: The ablation study on the WoZ2.0 dataset with the joint goal accuracy on the test set. For  \\`\\` - Hier a chic al -Attn'', we remove the residual connections between the attention modules in the CMR decoders and all the attention memory access are based on the output from the LSTM. For  \\`\\` - MLP'', we further replace the MLP with a single linear layer with the non -linear activation.\n\n \nTable \n|| Model | Joint Acc. ||\n\n || COMER | 88.64% ||\n\n || - Hierachical -Attn | 86.69% || "}
{"page": 20, "image_path": "doc_images/2305.13186v3_20.jpg", "ocr_text": "I] - MLP | 83.24% ||\n\nClaim: [CONTINUE] The effectiveness of our hierarchical attention design is proved by\nan accuracy drop of 1.95% after removing residual connections\nand the hierarchical stack of our attention modules.\n\nQuestion: Based on the information in the table, is the above claim true or false?\n\n# Python Code, return ans\n\nCOMER_acc = 88.64\n\nCOMER_acc_no_residual = 86.69\n\naccuracy_drop = COMER_acc - COMER_acc_no_residual\nans = equal(accuracy_drop, 1.95)\n\nRead the following table and then write Python code to answer a question:\n(please call the function equal(a, b) to check whether a and b are equal)\n\nCaption: Table 3: Ablation study of capsule net and word-level attention on Wikidata dataset.\n\nTable:\n|| Recall | 0.1 | @.2 | 0.3 | AUC ||\n\n|| -Word-ATT | 0.648 | 0.515 | 0.395 | 0.389 ||\n\n|| -Capsule | 0.635 | 0.507 | 0.413 | 0.386 ||\n\n|| Our Model | 0.650 | 0.519 | 0.422 | 0.405 ||\n\nClaim: According to the table, the drop of precision demonstrates\nthat the word-level attention is quite useful.\n\nQuestion: Based on the information in the table, is the above claim true or false?\n\n# Python Code, return ans\nour_model_recalls = [@.650, 0.519, @.422, 0.405]\nwithout_word_att_recalls = [0.648, @.515, 0.395, 0.389]\nans = True\nfor i in range(4):\nif our_model_recalls[i] < without_word_att_recalls[i]:\n\nans = False\n\nbreak\nRead the following table and then write Python code to answer a question:\n(please call the function equal(a, b) to check whether a and b are equal)\n\nCaption: Table 4: Scores for different training objectives on the linguistic probing tasks.\n\nTable\n\n|| Method | Depth| BShift| SubjNum | Tense | CoordInv | Length | ObjNum | TopConst | SOMO | WC |]\nI] CMow-c | 36.2 | 66.0 | 81.1 | 78.7 | 61.7 | 83.9 | 79.1 | 73.6 | 50.4 | 66.8 ||\n\nI] CMOW-R | 35.1 | 70.8 | 82.0 | 80.2 | 61.8 | 82.8 | 79.7 | 74.2 | 50.7 | 72.9 ||\n\nI] CBOW-c | 34.3 | 50.5 | 79.8 | 79.9 | 53.0 | 75.9 | 79.8 | 72.9 | 48.6 | 89.0 ||\n\nI] CBOW-R | 33.0 | 49.6 | 79.3 | 78.4 | 53.6 | 74.5 | 78.6 | 72.0 | 49.6 | 89.5 ||\n\nClaim: While CMOW-R and CMOW-C perform comparably on most probing tasks,\nCMOW-C yields 5 points higher scores on WordContent and BigramShift.\n\nQuestion: Based on the information in the table, is the above claim true or false?\n# Python Code, return ans\n\nCMOW_C_score_on_WC = 66.8\n\nCMOW_C_score_on_BShift = 66.0\n\nCMOW_R_score_on_WC = 72.9\n\nCMOW_R_score_on_BShift = 70.8\n\nans = equal(CMOW_C_score_on_WC - CMOW_R_score_on_WC, 5)\n\nand equal (CMOW_C_score_on_BShift - CMOW_R_score_on_BShift, 5)\n\n(--+ more in-context examples here -- -)\n\nRead the following table and then write Python code to answer a question:\n(please call the function equal(a, b) to check whether a and b are equal)\n\nCaption: <input_caption>\n\nTable: <input_table>\n\nClaim: <input_claim>\n\nQuestion: Based on the information in the table, is the above claim true or false?\n\n# Python Code, return ans\n\n", "vlm_text": "|| - MLP | 83.24% || \nClaim : [CONTINUE] The effectiveness of our hierarchical attention design is proved by an accuracy drop of   $1\\,.\\,95\\%$   after removing residual connections and the hierarchical stack of our attention modules. \nQuestion : Based on the information in the table , is the above claim true or false? \n# Python Code , return ans COMER_acc  $=\\phantom{-}88.64$  COMER acc no residual  $=\\phantom{-}86.69$  accuracy drop  $=$   COMER_acc - COMER acc no residual ans  $=$   equal(accuracy drop , 1.95) \nRead the following table and then write Python code to answer a question: (please call the function equal(a, b) to check whether a and b are equal) \nCaption : Table 3: Ablation study of capsule net and word -level attention on Wikidata dataset. \nTable :\n\n \n|| Recall | 0.1 | 0.2 | 0.3 | AUC ||\n\n || -Word -ATT | 0.648 | 0.515 | 0.395 | 0.389 ||\n\n || -Capsule | 0.635 | 0.507 | 0.413 | 0.386 ||\n\n || Our Model | 0.650 | 0.519 | 0.422 | 0.405 || \nClaim : According to the table , the drop of precision demonstrates that the word -level attention is quite useful. \nQuestion : Based on the information in the table , is the above claim true or false? \n# Python Code , return ans our model recalls  $=$  [0.650 , 0.519, 0.422, 0.405]without word at t recalls  $=$   [0.648 , 0.515, 0.395 , 0.389] ans  $=$   True for i in range (4): if our model recalls[i]   $<$   without word at t recalls[i]: ans   $=$   False break \nRead the following table and then write Python code to answer a question: (please call the function equal(a, b) to check whether a and b are equal) \nCaption : Table 4: Scores for different training objectives on the linguistic probing tasks.\n\n \nTable \n|| Method | Depth| BShift| SubjNum | Tense | CoordInv | Length | ObjNum | TopConst | SOMO | WC ||\n\n || CMOW -C | 36.2 | 66.0 | 81.1 | 78.7 | 61.7 | 83.9 | 79.1 | 73.6 | 50.4 | 66.8 ||\n\n || CMOW -R | 35.1 | 70.8 | 82.0 | 80.2 | 61.8 | 82.8 | 79.7 | 74.2 | 50.7 | 72.9 ||\n\n || CBOW -C | 34.3 | 50.5 | 79.8 | 79.9 | 53.0 | 75.9 | 79.8 | 72.9 | 48.6 | 89.0 ||\n\n || CBOW -R | 33.0 | 49.6 | 79.3 | 78.4 | 53.6 | 74.5 | 78.6 | 72.0 | 49.6 | 89.5 || \nClaim : While CMOW -R and CMOW -C perform comparably on most probing tasks , CMOW -C yields 5 points higher scores on Word Content and Big ram Shift. \nQuestion : Based on the information in the table , is the above claim true or false? \n# Python Code , return ans C MOW C score on WC = 66.8 C MOW C score on B Shift  $=\\phantom{-}66.0$  C MOW R score on WC  $=\\quad72\\cdot9$  C MOW R score on B Shift   $=\\quad7\\,\\varnothing\\;.\\;8$  ans  $=$   equal(C MOW C score on WC - C MOW R score on WC , 5) \n( · · ·  more in-context examples here  · · · ) \nRead the following table and then write Python code to answer a question: (please call the function equal(a, b) to check whether a and b are equal) \nCaption :  <input caption> \nTable :  <input table> \nClaim :  <input claim> \nQuestion : Based on the information in the table , is the above claim true or false? \n# Python Code , return ans "}
{"page": 21, "image_path": "doc_images/2305.13186v3_21.jpg", "ocr_text": "A. The calculation result is wrong.\n\nModel T #P BC\nSeq2SeqB (Beck et al., 2018) S 284M 21.7 49.1\nGGNN2Seq (Beck et al., 2018) S 28.3M 23.3 50.4\nSeq2SeqB (Beck et al., 2018) E 142M 26.6 52.5\nGGNN2Segq (Beck et al., 2018) E 141M 27.5 53.5\n\nS 19.1M 27.9 57.3\n\nDCGCN (ours) E 92.5M 30.4 59.6\n\nTable 2: Main results on AMR17. GCNSEQ (Damonte\nand Cohen, 2019) achieves 24.5 BLEU points. #P\nshows the model size in terms of parameters; “S” and\n“E” denote single and ensemble models, respectively.\n\nClaim: For example, the single DCGCN model gains 5.9 more BLEU points than the single\nmodels of Seq2SeqB on AMR17.\n\nGCN Bc _GCN Bc\n\n+RC (2) 16.8 48.1 +RC+LA (2) 18.3 47.9\n+RC (4) 18.4 49.6 +RC+LA (4) 18.0 51.1\n+RC (6) 19.9 49.7 +RC+LA (6) 21.3 50.8\n+RC (9) 21.1 50.5 +RC+LA (9) 22.0 52.6\n\n+RC (10) 20.7 50.7 +RC+LA (10) 21.2 52.9\nDCGCNI1 (9) 22.9 53.0 DCGCN3 (27) 24.8 54.7\nDCGCN2 (18) 24.2 54.4 DCGCN4 (36) 25.5 55.4\n\nTable 6: Comparisons with baselines. +RC denotes\nGCNs with residual connections. +RC+LA refers to\nGCNs with both residual connections and layer aggre-\ngations. DCGCNi represents our model with i blocks,\ncontaining i x (m + m) layers. The number of layers\nfor each model is shown in parenthesis.\n\n| Claim: However, when we set the number to 18, GCN+RC+LA achieves a BLEU score of 19.4,\nt_ which is significantly worse than the BLEU score obtained by DCGCN2 (23.3).\n\nC. The claim is partially right.\nTable 4: Scores for different training objectives on the linguistic probing tasks.\n\nMethod Depth BShift SubjNum Tense CoordInv Length ObjNum ‘TopConst SOMO WC\n\nCMOW-C 36.2 66.0 81.1 78.7 61.7 83.9 79.1 73.6 504 66.8\nCBOW-C 43 50.5 798 79.9 53.0 75.9 79.8 729 48.6 89.0\nCBOW-R 33.0 49.6 793 78.4 53.6 74.5 78.6 72.0 49.6 89.5\n\nClaim: While CMOW-R and CMOW-C perform comparably on most probing tasks, CMOW-C\nyields 5 points lower scores on WordContent and BigramShift.\n\n1\n1\n1\n\\\n\\\n\\\n'\ni\n1\n\\\nCMOW-R 35.1 8 82.0 80.2 61.8 82.8 79.7 74.2 50.7 72.9 '\n1\n1\nf\n\\\n\\\n'\n1\n1\n'\nBShift column doesn't satisfy the claim. t\n\nFigure 9: The refuted claims cases A to C. Case A represents the calculation result is wrong. Case B represents the\napproximation word is wrong. Case C represents the claim is partially right.\n", "vlm_text": "Claim:  For example, the single DCGCN model gains  5.9  more BLEU points than the single  models of Seq2SeqB on AMR17. \nB. The approximation word is wrong. \nClaim:  However, when we set the number to 18,   $\\mathsf{G C N+R C+L A}$   achieves a BLEU score of 19.4,  which is  significantly  worse than the BLEU score obtained by DCGCN2 (23.3). \nC. The claim is partially right. \nThe table presents results for four methods (CMOW-C, CMOW-R, CBOW-C, CBOW-R) evaluated across several metrics. Here's a breakdown:\n\n### Methods\n- CMOW-C\n- CMOW-R\n- CBOW-C\n- CBOW-R\n\n### Metrics\n1. **Depth**\n   - CMOW-C: 36.2\n   - CMOW-R: 35.1\n   - CBOW-C: 34.3\n   - CBOW-R: 33.0\n\n2. **BShift**\n   - CMOW-C: 66.0\n   - CMOW-R: 70.8\n   - CBOW-C: 50.5\n   - CBOW-R: 49.6\n\n3. **SubjNum**\n   - CMOW-C: 81.1\n   - CMOW-R: 82.0\n   - CBOW-C: 79.8\n   - CBOW-R: 79.3\n\n4. **Tense**\n   - CMOW-C: 78.7\n   - CMOW-R: 80.2\n   - CBOW-C: 79.9\n   - CBOW-R: 78.4\n\n5. **CoordInv**\n   - CMOW-C: 61.7\n   - CMOW-R: 61.8\n   - CBOW-C: 53.0\n   - CBOW-R: 53.6\n\n6. **Length**\n   - CMOW-C: 83.9\n   - CMOW-R: 82.8\n   - CBOW-C: 75.9\n   - CBOW-R: 74.5\n\n7. **ObjNum**\n   - CMOW-C: 79.1\n   - CMOW-R: 79.7\n   - CBOW-C: 79.8\n   - CBOW-R: 78.6\n\n8. **TopConst**\n   - CMOW-C: 73.6\n   - CMOW-R: 74.2\n   - CBOW-C: 72.9\n   - CBOW-R: 72.0\n\n9. **SOMO**\n   - CMOW-C: 50.4\n   - CMOW-R: 50.7\n   - CBOW-C: 48.6\n   - CBOW-R: 49.6\n\n10. **WC**\n    - CMOW-C: 66.8\n    - CMOW-R: 72.9\n    - CBOW-C: 89.0\n    - CBOW-R: 89.5\n\nThe highest value for each metric is bolded in the original table.\nClaim:  While CMOW-R and CMOW-C perform comparably on most probing tasks, CMOW-C  yields 5 points lower scores on Word Content and Big ram Shift. BShift column doesn't satisfy the claim. \nFigure 9: The refuted claims cases  $A$   to    $C$  . Case  $A$   represents the calculation result is wrong. Case    $B$   represents the approximation word is wrong. Case  $C$   represents the claim is partially right. "}
{"page": 22, "image_path": "doc_images/2305.13186v3_22.jpg", "ocr_text": "D. The values in the claim do not match.\n\n1\n\\\n\n' Model T #P B Cc\n| Seq2SeqB (Beck et al., 2018) S 28,4M 21.7 49.1\n' GGNN2Seq (Beck et al., 2018) S 28.3M 23.3 50.4\n' Seq2SeqB (Beck et al., 2018) E 142M 26.6 52.5\n' GGNN2Seq (Beck et al., 2018) E 141M 27.5 53.5\nH S$ 19.1M 27.9 57.3\n1\n\n' DCGCN (ours) E 92.5M 30.4 59.6\n1\n\nTable 2: Main results on AMR17. GCNSEQ (Damonte\n! and Cohen, 2019) achieves 24.5 BLEU points. #P\n\\\n\n\\\n\n'\n\n'\n\n'\n\n\\\n\n1\n\nshows the model size in terms of parameters; “S” and\n“E” denote single and ensemble models, respectively.\n\nClaim: Our single model achieves 27.6 BLEU points, which is the new state-of-the-art\nresult for single models.\n\nE. The operation type is wrong.\n\nGCN Bc _ GCN Bc\n\n+RC (2) 16.8 48.1 +RC+LA (2) 18.3 47.9\n+RC (4) 18.4 49.6 +RC+LA (4) 18.0 51.1\n+RC (6) 19.9 49.7 +RC+LA (6) 21.3 50.8\n\n1\n1\n\n1\n\n1\n\n1\n\n1\n\n1\n\n1\n\n1\n\n'\n\n' +RC (9) 21.1 50.5 +RC+LA (9) 22.0 52.6\nt +RC (10) 20.7 50.7 +RC+LA (10) 21.2 52.9\n' DCGCNI1 (9) 22.9 53.0 DCGCN3 (27) 24.8 54.7\nH DCGCN2 (18) 24.2 54.4 DCGCN4 (36) 25.5 55.4\n1\n\n1\n\n1\n\n1\n\n1\n\n1\n\n1\n\n1\n\n1\n\n1\n\n1\n\n1\n\n1\n\n1\n\n1\n\n1\n\nTable 6: Comparisons with baselines. +RC denotes\nGCNs with residual connections. +RC+LA refers to\nGCNs with both residual connections and layer aggre-\ngations. DCGCNi represents our model with i blocks,\ncontaining i x (nm + m) layers. The number of layers\nfor each model is shown in parenthesis.\n\nClaim: GCN+RC+LA (9) is better than DCGCN1.\n\nFigure 10: The refuted claims cases D and E. Case D represents the values in the claim do not match. Case E\nrepresents the operation type is wrong.\n", "vlm_text": "The image contains two sections labeled D and E, each highlighting discrepancies in claims:\n\n**D. The values in the claim do not match.**\n- A table lists various model names, parameters (#P), and BLEU scores (B):\n  - Seq2SeqB (single/ensemble)\n  - GCN2Seq (single/ensemble)\n  - DCGCN (ours) with a BLEU score of 27.9 for the single model.\n- The claim states that their single model achieves 27.6 BLEU points, suggesting a mismatch with the table value (27.9).\n\n**E. The operation type is wrong.**\n- A table lists various GCN models with BLEU scores (B) and additional operations like +RC and +RC+LA.\n- The claim suggests GCN+RC+LA (9) is better than DCGCN1.\n- The table shows GCN+RC+LA (9) with a BLEU score of 21.2, which is lower than DCGCN1’s 22.9.\n\nThese sections critique inaccuracies or misrepresentations in the claims based on the presented data.\nFigure 10: The refuted claims cases  $D$   and  $E$  . Case    $D$   represents the values in the claim do not match. Case    $E$  represents the operation type is wrong. "}
{"page": 23, "image_path": "doc_images/2305.13186v3_23.jpg", "ocr_text": "Error case 1 (43.9%) : Gold Label: Supported Prediction Label: NEI\nTable 3: Precision, recall and F-measure for methods using the top 1,000 words with the highest number of contexts.\n| Lang Corpus | Patt DSim SLQS TF DF  DocSub HClust\n\nEuroparl | 0.1173 0.0366 0.0503 0.0548 0.0443 0.0761\nTed Talks | 0.1125 0.0301 _ 0.0382 0.0441 0.0710 _ 0.0664\n\nEuroparl | Sser 0.3330 0.5257 0.6109 0.5984 0.7311 0.5676\n\nTed Talks | 0.5387 0.2907 0.5300 0.6117 0.6159 0.6533 0.5656\nEuroparl\nau Ted Talks\nPT Europarl | 0.0111 0.3554 0.5795 0.6727 0.5184 0.0053 0.0012\nTed Talks | 0.0004 0.3142 0.5484 0.6877 0.5515 0.4706 0.0011\nEN Europarl | 0.0591 0.0671 0.0922 0.1015 0.1003 0.0044 0.0033\nF Ted Talks | 0.0035 0.0564 0.0713 0.0791 0.0822, 0.1121 (0.0037\nPT Europarl | 0.0217 0.3438 0.5513 0.6403 0.5555 0.0105 0.0024\nTed Talks | 0.0008 0.3020 0.5390 0.6475 0.5819 0.5471 0.0022\n\nClaim: Analyzing Table 3, we can observe that all values of precision using the Portuguese\ncorpora have higher scores when compared with the English corpora.\n\nError case 2 (38.7%): Gold Label: Refuted Prediction Label: NEI\n\nTable 3: Precision, recall and F-measure for methods using the top 1,000 words with the highest number of contexts.\n| Lang Corpus | Patt DSim SLQS TF DF  DocSub HClust\n\n0.1173 0.0366 0.0503 0.0548 0.0443 0.0761\n0.1125 0.0301 0.0382 0.0441 0.0710 _ 0.0664\n\nEuroparl\nTed Talks\n\nP\nPT Europarl | 0.5163 0.3330 0.5257 0.6109 0.5984 0.7311 0.5676\nTed Talks | 0.5387 0.2907 0.5300 0.6117 0.6159 0.6533 0.5656\n\nEN Europarl | 0.0396 0.3999 0.5499 0.6045 0.5887 0.0023\n\nR Ted Talks | 0.0018 0.4442 0.5377_0.5657__0.6077__—(0.2666\nPT Europarl | 0.0111 0.3554 0.5795 0.6727 0.5184 0.0053 0.0012\nTed Talks | 0.0004 0.3142 0.5484 0.6877 0.5515 0.4706 0.0011\nEN Europarl | 0.0591 0.0671 0.0922 0.1015 0.1003 0.0044 0.0033\nF Ted Talks | 0.0035 0.0564 0.0713 0.0791 0.0822, 0.1121 (0.0037\nPT Europarl | 0.0217 0.3438 0.5513 0.6403 0.5555 0.0105 0.0024\nTed Talks | 0.0008 0.3020 0.5390 0.6475 0.5819 0.5471 0.0022\n\nClaim: Analyzing Table 3, we can observe that all values of precision using the English\ncorpora have higher scores when compared with the Portuguese corpora.\n\nError case 3 (7.6%) : Gold Label: Refuted Prediction Label: Supported\n\nClaim: With the coverage mechanism, the result drops by 1.7/2.4 points for B/C scores.\n\n1\n' 1\n' Model Bc t\n1 DCGCN4 25.5 55.4 |\n| Encoder Modules 1\n1 -Linear Combination 23.7 53.2 '\nt -Global Node 24.2 54.6 '\n' -Direction Aggregation 24.6 54.6 1\n1 -Graph Attention 24.9 54.7 |\n' -Global Node&Linear Combination 22.9 52.4 1\n1 Decoder Modules !\nt -Coverage Mechanism 23.8 53.0 '\nI Table 9: Ablation study for modules used in the graph |\n' encoder and the LSTM decoder '\n;\n' i\n\nFigure 11: Error Cases 1-3 for InstructGPT in the zero-shot setting.\n", "vlm_text": "Error case 1  (43.9%)  : Gold Label:  Supported   Prediction Label:   NEI \n\nThe table displays precision, recall, and F1 scores for different methods applied to two corpora (\"Europarl\" and \"Ted Talks\") in two languages (EN, PT). The methods listed are Patt, DSim, SLQS, TF, DF, DocSub, and HClust. Each method's performance is quantified with numerical values. Key observations include:\n\n- For precision (\\( \\mathcal{P} \\)), the DocSub method has the highest score for the \"Europarl\" corpus in PT (0.7311).\n- For recall (\\( \\mathcal{R} \\)), the TF method exhibits the highest score for the \"Ted Talks\" corpus in PT (0.6877).\n- For F1 scores (\\( \\mathcal{F} \\)), again, the highest value appears for the TF method for the \"Ted Talks\" corpus in PT (0.6403). \n\nValues are formatted for precision, recall, and F1 respectively, with bold numbers indicating the highest in each category within the same language-corpus pair.\nClaim : Analyzing Table 3, we can observe that all values of precision using the Portuguese  corpora have higher scores when compared with the English corpora. \nError case 2  (38.7%):  Gold Label:  Refuted  Prediction Label:  NEI \n\nThe table shows scores for various metrics across different languages (EN for English, PT for Portuguese) and corpora (Europarl, Ted Talks). \n\n- The table is divided into three sections labeled \\(\\mathcal{P}\\), \\(\\mathcal{R}\\), and \\(\\mathcal{F}\\).\n- Each section contains metrics: Patt, DSim, SLQS, TF, DF, DocSub, and HClust.\n- Bold numbers indicate highlighted or significant scores.\nClaim : Analyzing Table 3, we can observe that all values of precision using the English  corpora have higher scores when compared with the Portuguese corpora. \nError case 3  (7.6%) :  Gold Label:  Refuted  Prediction Label:   Supported \nThe table presents results from an ablation study for modules in a graph encoder and an LSTM decoder. It consists of configurations alongside their performance scores in columns B and C.\n\n- The overall model \"DCGCN4\" scores 25.5 in column B and 55.4 in column C.\n- Encoder Modules and their scores:\n  - Linear Combination: 23.7 (B), 53.2 (C)\n  - Global Node: 24.2 (B), 54.6 (C)\n  - Direction Aggregation: 24.6 (B), 54.6 (C)\n  - Graph Attention: 24.9 (B), 54.7 (C)\n  - Global Node & Linear Combination: 22.9 (B), 52.4 (C)\n- Decoder Modules and their scores:\n  - Coverage Mechanism: 23.8 (B), 53.0 (C)\n\nThe table is labeled as Table 9.\nClaim:  With the coverage mechanism, the result drops by 1.7/2.4 points for B/C scores. "}
{"page": 24, "image_path": "doc_images/2305.13186v3_24.jpg", "ocr_text": "Error case 4 (4.6%) : Gold Label: NEI Prediction Label: Supported\n\nin-domain out-of-domain\nSQuAD QA-SRL\n\ni\n1\n1\n1\n1\n1\n'\n1\nt EM Fl EM Fl\n| 31.76 75.37 | 10.99 50.10\n| +coverage 32.67 76.83 | 10.63 50.89\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\nMQAN\n\nBIDAF (ELMO) | 70.43 79.76 | 28.35 49.98\n+coverage 71.07 80.15 | 30.58 52.43\n\nTable 3: Impact of using coverage for improving generalization across the datasets of similar tasks. Both models\nare trained on the SQuAD training data.\n\nClaim: This is particularly true for the BIDAF model.\n\nError case 5 (2.8%) : Gold Label: NEI Prediction Label: Refuted\n\nTABLE V\nWORD INTRUSION TEST RESULTS: CORRECT ANSWERS OUT OF 300\nQUESTIONS\nGloVe Imparted\nMean/Std 85/6.9 212/244\n\nClaim: These results show that the questionnaire takers had an average accuracy of\n98.2% in answering word intrusion questions for words associated with meanings\n\ni\n1\n\n1\n\n1\n\n1\n\n1\n\n1\n\n1\n\n1\n\n1\n\n1\n\n1 es\n| Participants Ll to 5 = SOV88/82/78/97 =. 2212/1 70/207/229/242\n1\n\n1\n\n1\n\n1\n\n1\n\n1\n\nt\n\n1\n\n| imparted by standard word embeddings\n\n1\n\nError case 6 (2.4%) : Gold Label: Supported Prediction Label: Refuted\n\nGCN Bc _GCN BC\n\n+RC (2) 16.8 48.1 +RC+LA (2) 18.3 47.9\n+RC (4) 18.4 49.6 +RC+LA (4) 18.0 51.1\n+RC (6) 19.9 49.7 +RC+LA (6) 21.3 50.8\n+RC (9) 21.1 50.5 +RC+LA (9) 22.0 52.6\n\n+RC (10) 20.7 50.7 +RC+LA (10) 21.2 52.9\nDCGCN1 (9) 22.9 53.0 DCGCN3 (27) 24.8 54.7\nDCGCN2 (18) 24.2 54.4 DCGCN4 (36) 25.5 55.4\n\nTable 6: Comparisons with baselines. +RC denotes\nGCNs with residual connections. +RC+LA refers to\nGCNs with both residual connections and layer aggre-\ngations. DCGCNi represents our model with i blocks,\ncontaining i x (m + m) layers. The number of layers\nfor each model is shown in parenthesis.\n\nClaim: For example, GCN+RC+LA (10) achieves a BLEU score of 21.2, which is worse\nthan GCN+RC+LA (9).\n\nFigure 12: Error Cases 4-6 for InstructGPT in the zero-shot setting.\n\n", "vlm_text": "Error case 4  (4.6%) :  Gold Label:  NEI   Prediction Label:   Supported \nThe table presents evaluation results for two models, MQAN and BIDAF (ELMO), on in-domain (SQuAD) and out-of-domain (QA-SRL) datasets. It shows Exact Match (EM) and F1 scores for both models:\n\n- **MQAN (SQuAD):**\n  - EM: 31.76\n  - F1: 75.37\n\n- **MQAN + coverage (SQuAD):**\n  - EM: 32.67\n  - F1: 76.83\n\n- **BIDAF (ELMO) (SQuAD):**\n  - EM: 70.43\n  - F1: 79.76\n\n- **BIDAF (ELMO) + coverage (SQuAD):**\n  - EM: 71.07\n  - F1: 80.15\n\n- **MQAN (QA-SRL):**\n  - EM: 10.99\n  - F1: 50.10\n\n- **MQAN + coverage (QA-SRL):**\n  - EM: 10.63\n  - F1: 50.89\n\n- **BIDAF (ELMO) (QA-SRL):**\n  - EM: 28.35\n  - F1: 49.98\n\n- **BIDAF (ELMO) + coverage (QA-SRL):**\n  - EM: 30.58\n  - F1: 52.43\n\nThe \"+coverage\" denotes an enhancement applied to these models, resulting in generally improved scores.\n\nClaim:  This is particularly true for the BIDAF model. \nError case 5  (2.8%) :  Gold Label:  NEI  Prediction Label:   Refuted \n\nThe table presents data under two columns: \"GloVe\" and \"Imparted.\" \n\nFor \"Participants 1 to 5\":\n- GloVe: 80, 88, 82, 78, 97\n- Imparted: 212, 170, 207, 229, 242\n\nFor \"Mean/Std\" (Mean and Standard Deviation):\n- GloVe: Mean = 85, Std = 6.9\n- Imparted: Mean = 212, Std = 24.4\nClaim:  These results show that the questionnaire takers had an average accuracy of   $98.2\\%$   in answering word intrusion questions for words associated with meanings  imparted by standard word embeddings \nError case 6  (2.4%) :  Gold Label:  Supported  Prediction Label:   Refuted \nThe table presents a comparison of different GCN (Graph Convolutional Network) models with baseline results. Here's a breakdown of what's in the table:\n\n- **Models Compared:**\n  - **+RC:** GCNs with residual connections.\n  - **+RC+LA:** GCNs with both residual connections and layer aggregations.\n  - **DCGCNi:** A model with \\(i\\) blocks, each containing \\(i \\times (n + m)\\) layers.\n\n- **Columns:**\n  - The table has four main columns: two sections titled \"GCN\" with their sub-columns labeled \"B\" and \"C\".\n\n- **Rows:**\n  - Different configurations of GCN models are compared with the number of layers indicated in parentheses, such as (2), (4), (6), (9), (10), (9), (27), (18), and (36).\n\n- **Values:**\n  - Each model configuration has two values associated with it in columns B and C, representing some performance metrics.\n\n- **Best Performance:**\n  - The model configuration with +RC (9) shows superior performance in B (21.1) and C (50.5) without layer aggregation.\n  - The DCGCN4 (36) configuration shows the best performance overall with B (25.5) and C (55.4).\n\nThe table also includes a footnote that explains the meanings of various terms and specifies that the number of layers for each model is shown in parentheses.\nClaim:  For example, GCN+RC+LA (10) achieves a BLEU score of 21.2, which is worse  than  $\\mathsf{G C N}\\!+\\!\\mathsf{R C}\\!+\\!\\mathsf{L A}$   (9). "}
{"page": 25, "image_path": "doc_images/2305.13186v3_25.jpg", "ocr_text": "Error case 1\n\nDataset Metric Illinois | IlliCons | Rahman and Ng (2012) | KnowFeat | KnowCons | KnowComb\nWinograd Precision | 51.48 53.26 73.05 T7181 74.93 7641\nWinoCoref | AntePre 68.37 74.32 = 88.48 88.95 89.32\n\nTable 7: Performance results on Winograd and WinoCoref datasets. All our three systems are trained on\n\nWinoCoref, and we evaluate the predictions on both datasets. Our systems improve over the baselines by\n\nover than 20% on Winograd and over 15% on WinoCoref.\nClaim: The best performing system is KnowComb. It improves by over 20% over a state-of-art\ngeneral coreference system on Winograd. On the WinoCoref dataset, it improves by 15%.\ngold: supports, prediction: refutes,\nprograms: [\"winograd_baseline = 73.05\\n winograd_knowcomb = 76.41\\n winocoref_baseline\n= 88.48\\nwinocoref_knowcomb = 89.32\\nans = (winograd_knowcomb - winograd_baseline >\n20) and (winocoref_knowcomb - winocoref_baseline > 15)\"\n\ngold: refutes,\n\ni 1\ni 1\ni 1\n' TABLET prediction: supports, '\n, WERs (%) AND DCE OF DIFFERENT SPEECH ENHANCEMENT METHODS . I\n1 ON LIBRISPEECH + DEMAND TEST SET programs: [ |\n! AAS_wAC_1_wAD_0_WER = 15.6\\n !\nao od _| WER (7) | DCE AAS WAC_1_wAD_105_WER = 14.4\\n\nt Wiener filter 19.5 0.722 AAS_wAC_1_wAD_0_DCE = 0.330\\n\n' Minimizing DCE 15.8 | 0.269 AAS_wAC_1_wAD_105_DCE = 0.303\\n '\n' FSEGAN 14.9 0.291 ans = (AAS_wAC_1_wAD_105_WER < {\n' AAS (wac = 1,wap =0) 15.6 0.330 AAS_wAC_1_wAD_0_WER) '\n! AAS wac =1, wap =10°) | 14.4 _| 0.303 and (AAS_wAC_1_wAD_105_DCE< '\n' Clean speech 57 0.0 AAS_wAC_1_wAD_0_DCE) '\nt _ 7 7 i)\n\n1\n\\ 1\n\nError case 3 programs : [\n\"G2S_GIN_BLEU_LDC2015E86 = 22.93\\n\nModel BLEU METEOR I =\n. LDC2015E86 G2S_GAT_BLEU_LDC2015E86 = 23.42\\n\nG2S_GGNN_BLEU_LDC2015E86 = 24.32\\n\nKonstas et al. (2017) 22.00 - _ — — a\nSong et al. (2018) 23.28 30.10 G2S_GIN_METEOR_LDC2015E86 = 29.72\\n\n\nG2S_GAT_METEOR_LDC2015E86 = 29.87\\n\n\nCao et al. (2019) 23.50 - _\n\nDamonteetal2019) 2440-2349 ©«—«=«G2S_GGNN_METEOR_LDC2015E86 = 20.591n\n\nGuo etal. (2019) 25.70 . gate “on\nG2S_GAT_BLEU_LDC2017T10 = 26.72\\n\n\nS28 22.55 £0.17 29.90£031 G95” GGNN_BLEU_LDC2017T10 = 27.87\\n\n\n628\"GrN 22.93 £0.20 29.72 £0.09 Gs\" GIN_METEOR_LDC2017T10 = 32.62\\n\n\nG2S-GAT 23.42 £0.16 29.87+0.14 Gog GAT _METEOR_LDC2017T10 = 32.52\\n\n\nG2S-GGNN 24.32 £0.16 30.53 +030 G25 GGNN_METEOR_LDC2017T10 = 33.21\\n\n\nLDC2017T10 ans = (G2S_GAT_BLEU_LDC2015E86 >\n\nBack et al. (2018) 23.30 -\n\nSong et al. (2018) 24.86 31.56 and (G2S_GGNN_BLEU_LDC2015E86 >\n\nDamonte et al.(2019) 24.54 24.07 G2S_GAT_BLEU_LDC2015E86)\n\nCao et al. (2019) 26.80 - and (G2S_GAT_METEOR_LDC2015E86 >\n\nGuo etal. (2019) _ 27.60 - G2S_GIN_METEOR_LDC2015E86)\n\n$28 22.73 £0.18 30.15+£0.14 and (G2S_GGNN_METEOR_LDC2015E86 >\n\nG2S-GIN 26.90 +0.19 32.62+0.04 G2S GAT _METEOR_LDC2015E86)\n\nG2S-GAT 26.72 +0.20 32.52+0.02 and (G2S_GAT_BLEU_LDC2017T10 >\n\nG2S-GGNN 27.87 +0.15 33.21+0.15 G2S_GIN_BLEU_LDC2017T10)\n\nand (G2S_GGNN_BLEU_LDC2017T10 >\nTable 2: BLEU and METEOR scores on the test set of G2S_GAT_BLEU_LDC2017T10)\n\nLDC2015E86 and LDC2017T 10 datasets. and (G2S_GAT_METEOR_LDC2017T10 >\n\nG2S_GIN_METEOR_LDC2017T10) and\n(G2S_GGNN_METEOR_LDC2017T10 > G\"\n\nclaim: This suggests that graph encoders based on gating mechanisms are very effective in\ntext generation models.\n\ni\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\ni\n1\n1\n\nG2S_GIN_BLEU_LDC2015E86) {\n1\n\n1\n\ni\n\ni\n\n1\n\n1\n\n1\n\n1\n\n1\n\n1\n\ni\n\n1\n\n1\n\n1\n\n1\n\n1\n\ngold: supports, prediction: refutes\n\nFigure 13: Error Cases 1-3 for Program-of-Thoughts. Error Case | exhibits incorrect entity linking (Grounding\nerror) and incorrect operation (Program error). Error Case 2 exhibits incomplete entity linking (Grounding error).\nError Case 3 exhibits Program error since it fails to generate a correct program.\n", "vlm_text": "Error case 1 \nThe table presents performance results on the Winograd and WinoCoref datasets. It shows Precision, AntePre, and overall scores for three systems, with the highest results highlighted:\n\n- **Winograd:**\n  - Baseline: 73.05\n  - KnowComb: 76.41\n\n- **WinoCoref:**\n  - Baseline: 88.48\n  - KnowComb: 89.32\n\nBoth systems show improvements with KnowComb, but not by the amounts claimed in the accompanying caption text.\nError case 2 \n $(\\%)$   $^+$  gold : refutes, \n\nprograms : [ \nThis table presents the results of various speech enhancement methods, showing the Word Error Rate (WER) and the Distortion-Constrained Enhancement (DCE) values:\n\n- **No enhancement**: WER of 17.3%, DCE of 0.828.\n- **Wiener filter**: WER of 19.5%, DCE of 0.722.\n- **Minimizing DCE**: WER of 15.8%, DCE of **0.269**.\n- **FSEGAN**: WER of 14.9%, DCE of 0.291.\n- **AAS (\\(w_{AC} = 1, w_{AD} = 0\\))**: WER of 15.6%, DCE of 0.330.\n- **AAS (\\(w_{AC} = 1, w_{AD} = 10^5\\))**: WER of 14.4%, DCE of 0.303.\n- **Clean speech**: WER of 5.7%, DCE of 0.0. \n\nThe bold value indicates the lowest DCE for that row.\nThe table contains BLEU and METEOR scores for different models evaluated on two datasets: LDC2015E86 and LDC2017T10.\n\n### LDC2015E86\n- **Konstas et al. (2017):** BLEU 22.00\n- **Song et al. (2018):** BLEU 23.28, METEOR 30.10\n- **Cao et al. (2019):** BLEU 23.50\n- **Damonte et al. (2019):** BLEU 24.40, METEOR 23.60\n- **Guo et al. (2019):** BLEU 25.70\n\n**Model Comparisons:**\n- **S2S:** BLEU 22.55 ± 0.17, METEOR 29.90 ± 0.31\n- **G2S-GIN:** BLEU 22.93 ± 0.09, METEOR 29.72 ± 0.09\n- **G2S-GAT:** BLEU 23.42 ± 0.16, METEOR 29.87 ± 0.10\n- **G2S-GGNN:** BLEU 24.32 ± 0.16, METEOR 30.53 ± 0.30\n\n### LDC2017T10\n- **Back et al. (2018):** BLEU 23.30\n- **Song et al. (2018):** BLEU 24.86, METEOR 31.56\n- **Damonte et al. (2019):** BLEU 24.54, METEOR 24.07\n- **Cao et al. (2019):** BLEU 26.80\n- **Guo et al. (2019):** BLEU 27.60\n\n**Model Comparisons:**\n- **S2S:** BLEU 22.73 ± 0.18, METEOR 30.15 ± 0.14\n- **G2S-GIN:** BLEU 26.90 ± 0.19, METEOR 32.62 ± 0.14\n- **G2S-GAT:** BLEU 26.72 ± 0.32, METEOR 32.52 ± 0.25\n- **G2S-GGNN:** BLEU 27.87 ± 0.15, METEOR 33.21 ± 0.15\n\n### Summary\nThe table suggests that graph encoders based on gating mechanisms (G2S-GGNN) are very effective in text generation models, achieving higher scores compared to other evaluated models. The results are given with their respective standard deviations."}
{"page": 26, "image_path": "doc_images/2305.13186v3_26.jpg", "ocr_text": "Error case 4\n\nROUGE-1 ROUGE-2 evel\n\nSystem R(%)_P(%) F(%) R(%)_ P(%) F(%) R(%)_P(%) F (%)\n\nILP 24.5 41.1 29.3+0.5 79 15.0 9.9+0.5 13.6 22.6 15.6+0.4\nSum-Basic 28.4 444 33.1+0.5 8.5 15.6 10.440.4 14.7 22.9  16.740.5\nKL-Sum 39.5 34.6  35.5+0.5 13.0 12.7 12.3+0.5 15.2 21.1 16.340.5\nLexRank 42.1 39.5  38.7+0.5 14.7 15.3 14.2+0.5 143 21.5 16.0+0.5\nMEAD 45.5 36.5  38.5+0.5 17.9 14.9 15.4+0.5 278 29.2 26.8+0.5\nSVM 19.0 48.8 24.7408 1S 21.1 10.0+0.5 32.7 34.3 31.4+0.4\nLogReg 26.9 34.5  28.740.6 6.4 9.9 7340.4 12.2 14.9 12.740.5\nLogReg” 28.0 34.8 29.4+0.6 6.9 10.4 7.8+0.4 12.1 14.5 12.5+0.5\nHAN 31.0 42.8 — 33.740.7 11.2 17.8 12.740.5 26.9 34.1 32.4+0.5\nHAN+pretrainT 32.2 424  34.4+0.7 11.5 17.5 12.9+0.5 29.6 35.8  32.2+0.5\nHAN+pretrainU 32.1 42.1 33.8+0.7 11.6 17.6 _12.9+0.5 30.1 35.6 32.340.5\nHAN” 38.1 40.5 37.8+0.5 14.0 17.1 14.7+0.5 32.5 34.4  33.4+0.5\nHAN+pretrainU” By) 404  37.6+0.5 13.6 16.9 14.4+0.5 33.9 33.8  33.8+0.5\n\nTable 1: Results of thread summarization. ‘HAN’ models are our proposed approaches adapted from the hierarchical attention networks (Yang\netal. 2016). The models can be pretrained using unlabeled threads from TripAdvisor (*T’) and Ubuntuforum (*U’). r indicates a redundancy\nremoval step is applied. We report the variance of F-scores across all threads (‘-”). A redundancy removal step improves recall scores (shown\nin gray) of the HAN models and boosts performance.\n\nclaim: Pretraining the HAN models, although intuitively promising, yields only comparable\nresults with those without.\n\ngold: supports, prediction: refutes\n\nPrograms:\n\n[\"HAN_F_score = 37.8\\n\n\nHAN_pretrainT_F_score = 37.6\\n\n\nHAN_pretrainU_F_score = 37.6\\n\n\nans = (HAN_F_score - HAN_pretrainT_F_score == 0.2) and\n\ni\n'\n'\n'\n'\n'\n'\n'\n'\n'\ni\ni\ni\ni\ni\ni\ni\ni\ni\ni\ni\ni\ni\n'\ni\n\nHAN+pretrainT” || 37.9 404 37.6405] 135 168 144405 | 325 344 33.410.5 !\ni\ni\ni\ni\ni\ni\ni\ni\ni\ni\ni\ni\ni\ni\ni\ni\ni\ni\ni\ni\ni\ni\ni\n1\n\n- i\n\n(HAN_F_score - HAN_pretrainU_F_score !\n\nporn nnn nn nn 45\n\nError case 5\nClaim: The effectiveness of our hierarchical attention\n\nModel Joint Ace. design is proved by an accuracy drop of 1.95% after\nCOMER 88.64% removing residual connections and the hierarchical stack\n-Hierachical-Attn 86.69% of our attention modules.\",\n\n-MLP 83.24% gold: supports,\n\nprediction: refutes,\nTable 4: The ablation study on the WoZ2.0 dataset programs: [*\n\nwith the joint goal accuracy on the test set. For “- -\nHierachical-Attn”, we remove the residual connections COMER_acc = 88.64\\n\n\nbetween the attention modules in the CMR decoders COMER_acc_no_residual = 86.69\\n\nand all the attention memory access are based on the accuracy _drop = COMER_acc -\n\noutput from the LSTM. For “- MLP”, we further re- i,\nplace the MLP with a single linear layer with the non- COMER_acc_no_residuafin\n\nlinear activation. ans = (accuracy_drop == 1.95)\"]\n\nFigure 14: Error Cases 4 and 5 for Program-of-Thoughts. Error Case 4 exhibits Ambiguity error since it fails to\ngenerate a precise program for the approximation word “comparable”. Error Case 5 exhibits Calculation error since\nit generates the correct program, but the calculation result is inaccurate due to incorrect float digits in the Python\ncode.\n", "vlm_text": "The table presents the results of thread summarization using different models, evaluated using ROUGE-1, ROUGE-2, and Sentence-Level metrics. The systems compared include ILP, Sum-Basic, KL-Sum, LexRank, MEAD, SVM, LogReg, LogReg* (with redundancy removal), and several variations of HAN (Hierarchical Attention Networks).\n\nThe table specifically highlights results for HAN models with and without pretraining using unlabeled threads from TripAdvisor (T) and UbuntuForum (U). Redundancy removal is indicated by an asterisk.\n\nKey observations:\n- Pretrained models (HAN+pretrainT, HAN+pretrainU) are compared against non-pretrained and redundancy-removed models.\n- F-scores (shown in gray) reveal that redundancy removal generally improves performance.\n- The claim suggests that pretraining yields similar results to those without pretraining, supported by comparisons of F-scores among different HAN variations.\nThe table presents ablation study results on the WoZ2.0 dataset showing the joint goal accuracy:\n\n- **COMER:** 88.64%\n- **- Hierarchical-Attn:** 86.69%\n- **- MLP:** 83.24%\n\nBelow the table, there's an explanation that removing residual connections and hierarchical stack components of attention modules results in accuracy changes. The claim is about the hierarchical attention design's effectiveness, related to a 1.95% accuracy drop, which the prediction refutes.\nFigure 14: Error Cases 4 and 5 for Program-of-Thoughts. Error Case 4 exhibits  Ambiguity error  since it fails to generate a precise program for the approximation word “comparable”. Error Case 5 exhibits  Calculation error  since it generates the correct program, but the calculation result is inaccurate due to incorrect float digits in the Python code. "}
