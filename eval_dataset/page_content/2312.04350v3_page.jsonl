{"page": 0, "image_path": "doc_images/2312.04350v3_0.jpg", "ocr_text": "arXiv:2312.04350v3 [cs.CL] 17 Jan 2024\n\nCLADDER: Assessing Causal Reasoning in\nLanguage Models\n\nZhijing Jin'”; Yuen Chen!; Felix Leeb’-; Luigi Gresele!’;\nOjasv Kamal’, Zhiheng Lyu‘, Kevin Blin”, Fernando Gonzalez”, Max Kleiman-Weiner*,\nMrinmaya Sachan’, Bernhard Schélkopf!\n\n'MPI for Intelligent Systems, Tiibingen ETH Ziirich 71IT Kharagpur\n4University of Hong Kong *University of Washington\n\njinzhi@ethz.ch chenyuen0103@berkeley.edu\nfleeb@tue.mpg.de luigi.gresele@tue.mpg.de\n\nAbstract\n\nThe ability to perform causal reasoning is widely considered a core feature of in-\ntelligence. In this work, we investigate whether large language models (LLMs) can\ncoherently reason about causality. Much of the existing work in natural language\nprocessing (NLP) focuses on evaluating commonsense causal reasoning in LLMs,\nthus failing to assess whether a model can perform causal inference in accordance\nwith a set of well-defined formal rules. To address this, we propose a new NLP\ntask, causal inference in natural language, inspired by the “causal inference engine’\npostulated by Judea Pearl et al. We compose a large dataset, CLADDER, with\n10K samples: based on a collection of causal graphs and queries (associational,\ninterventional, and counterfactual), we obtain symbolic questions and ground-truth\nanswers, through an oracle causal inference engine. These are then translated into\nnatural language. We evaluate multiple LLMs on our dataset, and we introduce\nand evaluate a bespoke chain-of-thought prompting strategy, CAUSALCOT. We\nshow that our task is highly challenging for LLMs, and we conduct an in-depth\nanalysis to gain deeper insights into the causal reasoning abilities of LLMs.!\n\n1 Introduction\n\nOnce we really understand the logic behind causal thinking, we could emulate it\non modern computers and create an “artificial scientist”.\n\n— Pearl and Mackenzie [2018]\n\nCausal reasoning is believed to be one of the hallmarks of human intelligence [29, 68]. The ability to\ndraw causal inferences from available information is crucial for scientific understanding and rational\ndecision-making: for example, knowing whether smoking causes cancer might enable consumers\nto make a more informed decision [17, 18]; assessing the causal effect of a vaccine is essential for\neffective policy-making during a pandemic [14, 44, 72, 97]; and understanding the interplay behind\nfamily background, education and income helps devise effective education policies [10, 11, 30, 73].\n\nOur opening quote therefore mirrors the aspirations of many scientists in artificial intelligence\nand causal inference: to construct a machine capable of performing sound causal reasoning, and\nable to answer causal questions at scale and with ease. Recent advances in large language models\n(LLMs) have brought about a paradigm shift in natural language processing (NLP) and artificial\nintelligence [7, 15, 39, 56, 76, 103, inter alia]. These transformative developments raise the question\nof whether these machines are already capable of causal reasoning: Do LLMs understand causality?\n\n“Main contributors.\n‘Our data is open-sourced at https : //huggingface.co/datasets/causalNLP/cladder, and our code\ncan be found at https: //github.com/causalNLP/cladder.\n\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).\n", "vlm_text": "CL ADDER : Assessing Causal Reasoning in Language Models \nZhijing Jin , Yuen Chen , Felix Leeb , Luigi Gresele ∗ Ojasv Kamal 3 , Zhiheng Lyu 4 , Kevin Blin 2 , Fernando Gonzalez 2 , Max Kleiman-Weiner 5 , Mrinmaya Sachan 2 , Bernhard Schölkopf 1 \n1MPI for Intelligent Systems, Tübingen2ETH Zürich3IIT Kharagpur4 University of Hong Kong 5 University of Washington jinzhi@ethz.ch chen yuen 0103@berkeley.edu fleeb@tue.mpg.de luigi.gresele@tue.mpg.de \nAbstract \nThe ability to perform causal reasoning is widely considered a core feature of in- telligence. In this work, we investigate whether large language models (LLMs) can coherently reason about causality. Much of the existing work in natural language processing (NLP) focuses on evaluating  commonsense  causal reasoning in LLMs, thus failing to assess whether a model can perform causal inference in accordance with a set of well-defined  formal rules . To address this, we propose a new NLP task,  causal inference in natural language , inspired by the  “causal inference engine” postulated by Judea Pearl et al. We compose a large dataset, CL ADDER , with 10K samples: based on a collection of causal graphs and queries (association al, interventional, and counter factual), we obtain symbolic questions and ground-truth answers, through an oracle causal inference engine. These are then translated into natural language. We evaluate multiple LLMs on our dataset, and we introduce and evaluate a bespoke chain-of-thought prompting strategy, C AUSAL C O T. We show that our task is highly challenging for LLMs, and we conduct an in-depth analysis to gain deeper insights into the causal reasoning abilities of LLMs. \n1 Introduction \nOnce we really understand the logic behind causal thinking, we could emulate it on modern computers and create an “artificial scientist”. \n—  Pearl and Mackenzie  [ 2018 ] \nCausal reasoning is believed to be one of the hallmarks of human intelligence [ 29 ,  68 ]. The ability to draw causal inferences from available information is crucial for scientific understanding and rational decision-making: for example, knowing whether smoking causes cancer might enable consumers to make a more informed decision [ 17 ,  18 ]; assessing the causal effect of a vaccine is essential for effective policy-making during a pandemic [ 14 ,  44 ,  72 ,  97 ]; and understanding the interplay behind family background, education and income helps devise effective education policies [ 10 ,  11 ,  30 ,  73 ]. \nOur opening quote therefore mirrors the aspirations of many scientists in artificial intelligence and causal inference: to construct a machine capable of performing sound causal reasoning, and able to answer causal questions at scale and with ease. Recent advances in large language models (LLMs) have brought about a paradigm shift in natural language processing (NLP) and artificial intelligence [ 7 ,  15 ,  39 ,  56 ,  76 ,  103 ,  inter alia ]. These transformative developments raise the question of whether these machines are already capable of causal reasoning:  Do LLMs understand causality? "}
{"page": 1, "image_path": "doc_images/2312.04350v3_1.jpg", "ocr_text": "Question: Imagine a self-contained, hypothetical world with only the following conditions, and without any unmentioned factors or\ncausal relationships:\n\nPhysical vulnerability has a direct effect on the likelihood of fatality and vaccination decision. Vaccination has a direct effect on\nthe fatality rate.\n\nIn the entire population, 50% of the people are vulnerable to a certain disease.\n\nFor vulnerable and vaccinated people, the fatality rate is 4%. For vulnerable and unvaccinated people, the fatality rate is 7%.\nFor strong and vaccinated people, the fatality rate is 1%. For strong and unvaccinated people, the fatality rate is 5.8%.\nOverall, the fatality rate for vaccinated people is 5%, while the fatality rate for unvaccinated people is 4.5%.\n\nDoes getting vaccinated increase the likelihood of death?\n\nCLadder\n\nGround-Truth Answer: No\n\nCorrect steps to lead to the ground-truth answer:\n\n1) Parse the causal graph: Confounding\nSubskill: Causal Relation Extraction\n\n5) Derive the estimand using causal inference:\n\nELY | do(X=1)] - E[Y|do(x = 0)] Subskill: Formal Causal Inference\n\n= \\sum_{Z=v} P(Z=z)\"[P(Y=1|Z=z,X=1) - P(Y=1|Z=z, X=0)]\n\n2) Classify the query type: Average Treatment Effect\nSubskill: Causal Question Classification\n\n0)}\n0))\n\n= P(Z=0)\"[P(Y=1|Z=0,X=1) - P(Y:\n+ P(Z=1)\"[P(Y=1|Z=1,X=1) - P(Y:\n\n3) Formulate the query to its symbolic form:\nEIY | do(x=1)] - E[Y|do(x = 0)] ‘Subskill: Formalization\n\n6) Solve for the estimand by plugging in the relevant data in Step 4:\n.5*(0.01 - 0.058)+0.5*(0.04-0.07)\n.039 Subskill: Arithmetics\n\nFigure 1: Example question in our CLADDER dataset featuring an instance of Simpson’s paradox [63]. We\ngenerate the following (symbolic) triple: (i) the causal query; (ii) the ground-truth answer, derived through a\ncausal inference engine [66]; and (iii) a step-by-step explanation. We then verbalize these questions by turning\nthem into stories, inspired by examples from the causality literature, which can be expressed in natural language.\n\nMany previous works addressed the above question by focusing on commonsense causality [34, 100,\n101], inspired by the literature that explores LLMs as knowledge bases [40, 70, 83] (we refer to this\nline of work as causality as knowledge). This involves assessing the alignment between commonsense\nknowledge about causal relationships in humans and LLMs. This line of work generally does not focus\non evaluating how well models are capable of causal reasoning. For example, it may be difficult\nto rule out the possibility that LLMs perform potentially unreliable amortized causal inference,\nanswering causal questions by a simple repetition of verbal patterns present in the texts composing\ntheir training data:*> in other words, LLMs may just be “causal parrots” [100].\n\nIn this work, we introduce a way to test the formal causal reasoning in LLMs. To this end, we\nintroduce the CLADDER dataset. The specificity of CLADDER is that causal questions posed in\nnatural language are grounded in symbolic questions and ground truth answers: the latter are derived\nthrough an oracle causal inference engine (CI engine) [66], which abides by the rules of the causal\ninference approach described by Pearl [61], based on graphical models and structural causal models\n(SCMs) [23, 59, 61, 69, 88]. We compose more than 10,000 causal questions that cover a variety of\ncausal queries across the three rungs of the Ladder of Causation [3, 66]|—i.e., associational (Rung 1),\ninterventional (Rung 2), and counterfactual (Rung 3). We consider several causal graphs, giving rise\nto scenarios which require different causal inference abilities. Additionally, we generate ground-truth\nexplanations with step-by-step reasoning for more in-depth analysis of LLM behavior. Our symbolic\nquestions and answers are then verbalized, by turning them into stories which can be expressed in\nnatural language. To probe whether LLMs employ amortized causal inference, we construct stories\nwith commonsensical, as well as anti-commonsensical and with nonsensical causal relations: in these\nlatter cases, amortized causal inference is expected to fail, whereas formal causal reasoning would\nstill yield the correct answer. An example question from CLADDER is shown in Figure |.\n\nExploiting CLADDER, we also introduce a method to elicit sound causal reasoning in LLMs\nand help them solve challenging causality questions. Specifically, we develop CAUSALCOT, a\nchain-of-thought prompting strategy [96] inspired by the CI engine, which prompts the LLM to\nextract the causal graph, causal query, and available “data” (e.g., conditional or interventional do-\nprobabilities [24]) from the question, formalize them precisely, and perform correct causal inferences.\n\nwhich may itself contain instances of fallacious causal reasoning.\n>The extent to which this would imply an inaptitude of LLMs for causal reasoning has been questioned [38].\n", "vlm_text": "The image presents a hypothetical scenario involving vaccination, physical vulnerability, and fatality rates. It poses the question: \"Does getting vaccinated increase the likelihood of death?\" The ground-truth answer is \"No.\"\n\nThe image outlines the steps to reach this conclusion:\n\n1. **Parse the causal graph**: Identify confounding relationships.  \n   - Subskill: Causal Relation Extraction\n\n2. **Classify the query type**: Identify it as an Average Treatment Effect.\n   - Subskill: Causal Question Classification\n\n3. **Formulate the query in symbolic form**: E[Y | do(X=1)] - E[Y|do(X=0)].\n   - Subskill: Formalization\n\n4. **Collect available data**: Provides various probabilities related to vulnerability and vaccination.\n\n5. **Derive the estimand using causal inference**: Calculate using causal relationships.\n   - Subskill: Formal Causal Inference\n\n6. **Solve for the estimand**: Plug in available data and perform calculations.\n   - Subskill: Arithmetics\n\nThe final calculation shows a negative effect size, leading to the conclusion that getting vaccinated does not increase the likelihood of death.\nFigure 1: Example question in our CL ADDER  dataset featuring an instance of  Simpson’s paradox  [ 63 ]. We generate the following (symbolic) triple: (i) the causal query; (ii) the ground-truth answer, derived through a causal inference engine  [ 66 ]; and (iii) a step-by-step explanation. We then  verbalize  these questions by turning them into stories, inspired by examples from the causality literature, which can be expressed in natural language. \nMany previous works addressed the above question by focusing on  commonsense  causality [ 34 ,  100 , 101 ], inspired by the literature that explores LLMs as  knowledge bases  [ 40 ,  70 ,  83 ] (we refer to this line of work as  causality as knowledge ). This involves assessing the alignment between commonsense knowledge about causal relationships in humans and LLMs. This line of work generally does not focus on evaluating how well models are capable of  causal reasoning . For example, it may be difficult to rule out the possibility that LLMs perform potentially unreliable  amortized causal inference , answering causal questions by a simple repetition of verbal patterns present in the texts composing their training data: 2 ,   in other words, LLMs may just be  “causal parrots”  [ 100 ]. \nIn this work, we introduce a way to test the  formal causal reasoning in LLMs . To this end, we introduce the CL ADDER  dataset. The specificity of CL ADDER  is that causal questions posed in natural language are  grounded in symbolic questions and ground truth answers : the latter are derived through an oracle  causal inference engine (CI engine)  [ 66 ], which abides by the rules of the causal inference approach described by Pearl  [ 61 ] , based on graphical models and structural causal models (SCMs) [ 23 ,  59 ,  61 ,  69 ,  88 ]. We compose more than 10,000 causal questions that cover a variety of causal queries across the three rungs of the  Ladder of Causation  [ 3 ,  66 ]—i.e.,  association al (Rung 1) , interventional (Rung 2) , and  counter factual (Rung 3) . We consider several causal graphs, giving rise to scenarios which require different causal inference abilities. Additionally, we generate ground-truth explanations with step-by-step reasoning for more in-depth analysis of LLM behavior. Our symbolic questions and answers are then  verbalized , by turning them into stories which can be expressed in natural language. To probe whether LLMs employ amortized causal inference, we construct stories with common sens ical, as well as anti-common sens ical and with nonsensical causal relations: in these latter cases, amortized causal inference is expected to fail, whereas formal causal reasoning would still yield the correct answer. An example question from CL ADDER  is shown in Figure  1 . \nExploiting CL ADDER , we also introduce a method to elicit sound causal reasoning in LLMs and help them solve challenging causality questions. Specifically, we develop  C AUSAL C O T , a chain-of-thought prompting strategy [ 96 ] inspired by the CI engine, which prompts the LLM to extract the causal graph, causal query, and available “data” (e.g., conditional or interventional  do - probabilities [ 24 ]) from the question, formalize them precisely, and perform correct causal inferences. "}
{"page": 2, "image_path": "doc_images/2312.04350v3_2.jpg", "ocr_text": "Our experiments indicate that CAUSALCOT achieves an accuracy of 70.40%, which substantially\nimproves the performance of vanilla GPT-4 by 8.37 points on CLADDER.\n\nWe summarize the main contributions of our work:\n\n1. In contrast to most other works on causality in LLMs, focusing on commonsense causal\nknowledge, our goal is to assess the LLMs’ ability to perform formal causal reasoning\n(briefly reviewed in Section 2).\n\n2. We introduce CLADDER (Section 3), a dataset containing more than 10K causal questions,\nspanning all three rungs of the ladder of causation, several causal graphs, and various stories\nfor verbalization.\n\n3. We develop CAUSALCOT (Section 4), a chain-of-thought prompting strategy to elicit\nformal causal reasoning in LLMs, inspired by the causal inference engine.\n\n4. We perform extensive experiments on eight LLMs (Section 5), analyze fine-grained errors\nto showcase the limitations of LLMs in formal causal reasoning, and suggest directions\nfor future research.\n\n2 Preliminaries on Causal Inference\n\nOur dataset design takes inspiration from the Causal Inference Engine as postulated by Pearl and\nMackenzie [66], see also [59]. We begin with a brief overview of the causality framework by Pearl\net al. [67]. This framework was largely developed within the field of artificial intelligence, and there-\nfore puts particular emphasis on algorithmic aspects of causal reasoning (e.g., [62])—which makes\nit particularly suited for our work, where we want to algorithmically generate ground truth answers\nto causal queries, without having to appeal to common sense to assess the correctness of an answer.\n\n2.1 The Ladder of Causation\n\nThe Ladder of Causation, introduced by Pearl and Mackenzie [66], is a proposed taxonomy, and\nhierarchy, of causal inference tasks [3]. It consists of three distinct rungs.\n\nRung 1 (“seeing’’). This describes statistical associations (“How often do I take an aspirin when\nI have a headache?”). Rung 1 deals with statistical dependences among random variables, and\ninvolves probabilistic reasoning about joint and conditional distributions, P(X = x,Y = y) and\nP(Y = y|X =), which can be formalised through Bayesian Networks [12, 58] representing a set\nof variables and their conditional dependencies via a directed acyclic graph (DAG).\n\nRung 2 (“doing”). This enables us to formalize the concept of actively intervening in the world, and\nmodifying it toward some end (“Jf J take an aspirin now, will my headache subside?”). Interventions\ncan be formalized using the do-operator [24] and Causal Bayesian Networks [67] to represent, for\nexample, the distribution over Y when intervening on X to set its value to x as P(Y = y|do(X = 2)).\n\nRung 3 (“imagining”’). This rung deals with counterfactual reasoning, i.e., reasoning about alter-\nnative scenarios in which the world could have been different, possibly even contradicting the factual\nstate (“Would my headache have subsided, if I had taken an aspirin?” ). Counterfactual probabilities\ncan be written as P(Y, = y), representing the probability that “Y would be y, had X been x”. Reason-\ning about Rung 3 quantities requires the introduction of Structural Causal Models (SCMs) [67]. SCMs\nare especially powerful as they enable any quantity in Rungs 1, 2, and 3 to be formulated precisely [3].\n\n2.2 Causal Inference\n\nIdentification. Causal inference is especially difficult since we typically only have measurements\nfrom /ower rungs, but want to reason about higher ones. A crucial question is then under what\nconditions are such inferences possible, i.e., what assumptions and measurements are required\nto unambiguously answer a causal query of interest: this is the question of identification. As\nargued in [3], “it is generically impossible to draw higher-layer inferences using only lower-layer\ninformation”. One may be able to draw inferences at a higher layer given a combination of partial\nknowledge of the underlying SCM, in the form of a causal graph, and data at lower layers. The\ngraphical structure therefore plays a crucial role in bridging the rungs of the Ladder of Causation, and\nmany prior works have been dedicated to exploiting properties of the graph to transform higher-rung\nqueries into expressions which can be estimated based on lower-rung quantities [36, 64, 84].\n\n4We refer to [3, 65] for a comprehensive introduction. See also Appendix C for further details.\n", "vlm_text": "Our experiments indicate that C AUSAL C O T achieves an accuracy of   $70.40\\%$  , which substantially improves the performance of vanilla GPT-4 by 8.37 points on CL ADDER . \nWe summarize the  main contributions  of our work: \n1.  In contrast to most other works on causality in LLMs, focusing on  commonsense causal knowledge , our goal is to assess the LLMs’ ability to perform  formal causal reasoning (briefly reviewed in Section  2 ). 2.  We introduce CL ADDER  (Section  3 ), a dataset containing more than 10K causal questions, spanning all three rungs of the ladder of causation, several causal graphs, and various stories for verb aliz ation. 3.  We develop C AUSAL C O T (Section  4 ), a chain-of-thought prompting strategy to elicit formal causal reasoning in LLMs, inspired by the  causal inference engine . 4.  We perform extensive experiments on eight LLMs (Section  5 ), analyze fine-grained errors to showcase the limitations of LLMs in formal causal reasoning, and suggest directions for future research. \n2 Preliminaries on Causal Inference \nOur dataset design takes inspiration from the  Causal Inference Engine  as postulated by Pearl and Mackenzie  [ 66 ] , see also [ 59 ]. We begin with a brief overview of the causality framework by Pearl et al.  [ 67 ] .   This framework was largely developed within the field of artificial intelligence, and there- fore puts particular emphasis on  algorithmic  aspects of causal reasoning (e.g., [ 62 ])—which makes it particularly suited for our work, where we want to algorithmic ally generate ground truth answers to causal queries, without having to appeal to common sense to assess the correctness of an answer. \n2.1 The Ladder of Causation \nThe  Ladder of Causation , introduced by Pearl and Mackenzie  [ 66 ] , is a proposed taxonomy, and hierarchy, of causal inference tasks [ 3 ]. It consists of three distinct rungs. \nRung 1 ( “seeing” ). This describes statistical associations ( “How often do I take an aspirin when I have a headache?” ). Rung 1 deals with statistical dependence s among random variables, and involves probabilistic reasoning about joint and conditional distributions,    $P(X=x,Y=y)$   and  $P(Y=y|X=x)$  , which can be formalised through  Bayesian Networks  [ 12 ,  58 ] representing a set of variables and their conditional dependencies via a directed acyclic graph (DAG). \nRung 2 ( “doing” ). This enables us to formalize the concept of actively intervening in the world, and modifying it toward some end ( “If I take an aspirin now, will my headache subside?” ). Interventions can be formalized using the  do-operator  [ 24 ] and  Causal Bayesian Networks  [ 67 ] to represent, for example, the distribution over  $Y$   when intervening on    $X$   to set its value to  $x$   as  $P(Y=y|\\mathrm{\\bar{do}}(X=x))$  . \nRung 3 ( “imagining” ). This rung deals with counter factual reasoning, i.e., reasoning about alter- native scenarios in which the world could have been different, possibly even contradicting the factual state ( “Would my headache have subsided, if I had taken an aspirin?” ). Counter factual probabilities can be written as  $P(Y_{x}=y)$  , representing the probability that “  $\\because$  would be  $y$  , had    $X$   been  $x\"$  . Reason- ing about Rung 3 quantities requires the introduction of  Structural Causal Models (SCMs)  [ 67 ]. SCMs are especially powerful as they enable any quantity in Rungs 1, 2, and 3 to be formulated precisely [ 3 ]. \n2.2 Causal Inference \nIdentification. Causal inference is especially difficult since we typically only have measurements from  lower  rungs, but want to reason about  higher  ones. A crucial question is then under what conditions are such inferences possible, i.e., what assumptions and measurements are required to unambiguously answer a causal query of interest: this is the question of  identification . As argued in [ 3 ],  “it is generically impossible to draw higher-layer inferences using only lower-layer information” . One may be able to draw inferences at a higher layer given a combination of partial knowledge of the underlying SCM, in the form of a causal graph, and data at lower layers. The graphical structure therefore plays a crucial role in bridging the rungs of the Ladder of Causation, and many prior works have been dedicated to exploiting properties of the graph to transform higher-rung queries into expressions which can be estimated based on lower-rung quantities [ 36 ,  64 ,  84 ]. "}
{"page": 3, "image_path": "doc_images/2312.04350v3_3.jpg", "ocr_text": "Formal Part of the Question Generation\n\nSample a causal graph Sample a query type Generate\nis grap) ple a query typ available data\nv v\nCommon causal graphs with treatment-effect (X-Y) pairs Rung 1: Association Observational:\n¢ Marginal prob. Conditional prob. Pu.) = a\nConfounding: es Diamond: OSS ay Rung 2: Intervention Interventional:\n© ATE « Valid adjustment set EL...ldo(...)] = ..\nMediation: «GOD Chain: YO) Rung 3: Counterfactuals\n¢ Counterfactual prob. ATT\neNDE eNIE\nCollision: Fork: ‘Yap to the estimand\nATE = E[Y| do(X=1)] - E[Y |do(X=0)]\n\nApply do-calculus given the causal graph\n“ Generate the data s.t.\n=Tp22P(2= 2) (E(Y X= 1,Z=2)- the estimand is|identifiable\n\nE(Y|X=0,Z=z)}\n\nv\n\nNatural Language Part of the Question Generation\n\nSample a degree of alignment Sample a story for\nwith common sense variable name instantiation\n\nLevels of Empirical Alignment Stories (Variable Name instantiations)\n\nFor Commonsensical Confounding Graphs:\n© Story 1: X=vaccine, Z=vulnerability, Y=fatality rate\n# Story 2: X=drug, Z=gender, Y=recovery\n© Story 3: X=treatment, Z=age, Y=recovery\n\n: example option to choose\n\nNX\n\nVerbalize the\nentire question\n Commonsensical\n\n¢ Anti-commonsensical\nNonsensical\n\nFigure 2: The data-generating process of the CLADDER dataset. The upper part of the figure describes the\nformal part of the question generation, which samples inputs for the CI Engine and derives a ground truth answer.\nThe bottom part describes the natural language part of the question generation—i.e., its verbalization, based on\nmultiple stories and different degrees of alignment with commonsense knowledge.\n\nCausal Inference Engine. An overarching objective of this research is the construction of a Causal\nInference Engine (CI Engine) (37, 59, 66], which takes as input a query, a graph, and some available\ndata (typically from lower rungs than the query); and outputs whether a solution exists, and, if\nso, an equivalent expression of the query which is estimable from the available data. While some\nprevious works refer to the CI engine in the context of Rung 2 queries, where it corresponds to the\ndo-calculus [36, 84], here we refer to it in a more general sense, encompassing all three rungs.\n\n3 Composing the CLADDER Dataset\n\nTask Formulation. Like in the example of Figure 1, our dataset D := {(q;, ai, e:)}x, consists\nof N triples, each containing a question q;, binary answer a; € {Yes, No}, and an explanation e;.\nOur main task is to test the accuracy of the prediction function f : q +> a, i.e., a LLM which maps\na natural language causal question to an answer. Apart from directly evaluating the answer, we also\ncompose the ground-truth explanations e to evaluate the reasoning steps of LLMs.\n\nDesign Principles. In the composition of our dataset, we adhere to the following design principles.\nFirst, we ensure broad coverage of all rungs of the ladder of causation. Second, we avoid settings that\ninvolve continuous variables and use binary variables instead: this is partly due to the large availability\nof identifiability results for binary and categorical variables, and partly because queries involving\nbinary variables lend themselves to more natural-sounding verbalization. Moreover, since LLMs strug-\ngle with calculation-heavy tasks [32, 91], and we are chiefly interested in causal reasoning abilities,\nwe focus on graphs with few (three to four) variables, in various common configurations, to produce\nquestions which are identifiable from the outset. Lastly, we carefully design a rich set of templates\nto translate the abstract formulas into grammatically correct and natural-sounding, fluent prompts.\n\nOverall Pipeline. The generation pipeline for CLADDER, depicted in Figure 2, consists of two parts:\n\n1. In the Formal Part (which we illustrate in Section 3.1), we specify all the required inputs (query,\nmodel, data) and the ground truth answer generated by the CI Engine.\n", "vlm_text": "The image shows a flowchart describing the data-generating process for the CL ADDER dataset, divided into two main parts:\n\n1. **Formal Part of the Question Generation:**\n   - **Sample a causal graph:** Options include common graphs such as confounding, mediation, collision, diamond, chain, and fork.\n   - **Sample a query type:** This can be association, intervention, or counterfactuals, with estimands like Average Treatment Effect (ATE).\n   - **Generate available data:** Includes observational and interventional data, applying do-calculus for identifiability.\n\n2. **Natural Language Part of the Question Generation:**\n   - **Sample a degree of alignment with commonsense knowledge:** Includes commonsensical (e.g., smoking causes cancer), anti-commonsensical, and nonsensical.\n   - **Sample a story for variable name instantiation:** Example stories align with causal graphs, such as \"X=vaccine, Z=vulnerability, Y=fatality rate\" for a commonsensical confounding graph.\n  \nThe process results in the verbalization of the entire question by aligning formal and natural language aspects.\nCausal Inference Engine. An over arching objective of this research is the construction of a  Causal Inference Engine (CI Engine)  [ 37 ,  59 ,  66 ], which takes as input a query, a graph, and some available data (typically from lower rungs than the query); and outputs whether a solution exists, and, if so, an equivalent expression of the query which is estimable from the available data. While some previous works refer to the CI engine in the context of Rung 2 queries, where it corresponds to the do -calculus [ 36 ,  84 ], here we refer to it in a more general sense, encompassing all three rungs. \n3 Composing the CL ADDER  Dataset \nTask Formulation. Like in the examp of Figure  1 , our  $\\mathcal{D}:=\\{(\\pmb{q}_{i},\\pmb{a}_{i},\\pmb{e}_{i})\\}_{i=1}^{N}$    consists of    $N$   triples, each containing a question  $\\mathbf{\\nabla}q_{i}$  , binary answer  $a_{i}\\in\\{\\mathbf{Yes,No}\\}$    } , and an explanation  $e_{i}$  . Our main task is to test the accuracy of the prediction function  $f:q\\mapsto a$   7→ , i.e., a LLM which maps a natural language causal question to an answer. Apart from directly evaluating the answer, we also compose the ground-truth explanations    $e$   to evaluate the reasoning steps of LLMs. \nDesign Principles. In the composition of our dataset, we adhere to the following design principles. First, we ensure broad coverage of all rungs of the ladder of causation. Second, we avoid settings that involve continuous variables and use binary variables instead: this is partly due to the large availability of ident if i ability results for binary and categorical variables, and partly because queries involving binary variables lend themselves to more natural-sounding verb aliz ation. Moreover, since LLMs strug- gle with calculation-heavy tasks [ 32 ,  91 ], and we are chiefly interested in causal reasoning abilities, we focus on graphs with few (three to four) variables, in various common configurations, to produce questions which are identifiable from the outset. Lastly, we carefully design a rich set of templates to translate the abstract formulas into grammatically correct and natural-sounding, fluent prompts. \nOverall Pipeline. The generation pipeline for CL ADDER , depicted in Figure  2 , consists of two parts: \n1.  In the  Formal Part  (which we illustrate in Section  3.1 ), we specify all the required inputs (query, model, data) and the ground truth answer generated by the CI Engine. "}
{"page": 4, "image_path": "doc_images/2312.04350v3_4.jpg", "ocr_text": "2. In the Natural Language Part (in Section 3.2), we verbalize the formal queries and specification of\nthe causal model and data by associating them to a story or narrative, using a rich set of templates.\n\n3.1. Formal Part of the Question Formulation\n\nThe first step of our data generating process is to construct a set of inputs to the CI Engine such\nthat by design there exists a well-defined ground truth answer: i.e., we construct triples of causal\nqueries, graphs, and data such that the query can be unambiguously answered based on the available\ndata (ensuring identifiability by construction).> The ground truth causal models, which specify all\nquantities which are considered measurable in our questions, are causal Bayesian networks (CBNs),\nwhere each causal mechanism (i.e., conditional probability of a variable given its parents in the\nfactorization according to the causal graph G) corresponds to a Bernoulli distribution. We compile a\nselection of graphs G based on examples drawn from multiple sources from the literature [66, 67,\n69, 88], where suitable graph structures are used to illustrate toy problems in causal inference. The\ncomplete list of structures we consider can be found in Appendix A.3; the complete list of sources\nin Appendix A.1.\n\nSelecting Query Types. We again draw from the causal inference literature to collect common\nquery types in each rung. As illustrated in the “Sample a query type” box in Figure 2, for Rung 1, we\ncan ask about probability distributions such as marginal probabilities and conditional probabilities.\nFor Rung 2 questions, we can enquire average treatment effects (ATE) (“how will Y change if X\nchanges from x to x'?”), or what constitutes a valid adjustment set that can block all backdoor\nspurious correlations between X and Y. Lastly, for Rung 3, we include counterfactuals (“what\nwould happen to Y had X been x’ instead of x?”), average treatment effect on the treated (ATT)\n(“for the subpopulation whose X changed from x to x', how does their Y change on average?”’),\nnatural direct effect (NDE) (“what is the direct effect of X in Y, but not through the mediator?”),\nand natural indirect effect (NIE) (“what is the effect from X to Y through the mediator?”).\n\nApplying the Causal Inference Engine for the Ground-truth answer. By construction, the causal\nprocesses we define encapsulates all necessary information to make the causal quantities of the query\ntypes identifiable. This allows us to apply the rules of causal inference to obtain an estimand for each\ncausal graph and query type, and evaluate the estimand to get a ground truth answer. The Rung 2\nqueries simplify to Rung | terms using the rules of do-calculus [59], and, for the Rung 3 queries, we\napply methods of counterfactual causal inference [67] (with details in Appendix C.3). The estimand\nalso specifies exactly which terms are necessary to include in the prompt as “available data” in order\nto ensure that enough information is provided to answer the question correctly (i.e., for identifiability),\nprovided the correct causal reasoning is applied. Our entire code base of the data generation process\ncan be found at our GitHub repository, https: //github.com/causalNLP/cladder.\n\n3.2. Natural Language Part of the Question Formulation\n\nWhile Section 3.1 describes a way to generate the ground-truth causal model, query and answers, com-\nputed through a causal inference engine, real-world causal reasoning problems are expressed in natural\nlanguage rather than symbolic expressions. The next part of the data generation pipeline therefore\nfocuses on the verbalization of all these components with a plausible narrative in natural language.\n\nGenerating the Stories. For each causal graph, we collect a set of two to five stories which consist\nof a list of variable names for each node in the graph. The stories are primarily selected from examples\nin commonly cited causal inference books and papers (see Appendix A.1), which ensures that the\nstories and corresponding causal graph structures adhere to empirical common sense (e.g., the drug-\ngender-recovery example of Pearl and Mackenzie [66]). However, it is very likely that at least some\nof the stories appear in the training data of many LLMs. Therefore, we also generate various anti-\ncommon sense and nonsensical variants of the stories, meant to isolate the effects of memorization.\nFor the anti-commonsensical stories, we randomly do one of the actions: (1) replace the effect\nvariable Y with an unusual attribute, that would not be an effect variable in any of the stories (e.g.,\n“ear shape”); or (2) create an irrelevant treatment variable X that does not play a causal role in any of\nour commonsensical stories, such as “playing card games” (see Appendix A.7). For the nonsensical\nvariants, we invent artificial words as variable names such as “zory” and “qixy” (see Appendix A.6). .\n\n5We use the term “data” to denote numerical values of conditional or do-probabilities, and not as collections\nof data samples. This is in line with how the term is used in other descriptions of the CI Engine [37, 66].\n", "vlm_text": "2.  In the  Natural Language Part  (in Section  3.2 ), we verbalize the formal queries and specification of the causal model and data by associating them to a story or narrative, using a rich set of templates. \n3.1 Formal Part of the Question Formulation \nThe first step of our data generating process is to construct a set of inputs to the CI Engine such that  by design  there exists a well-defined ground truth answer: i.e., we construct triples of causal queries, graphs, and data such that the query can be unambiguously answered based on the available data (ensuring  ident if i ability  by construction).   The ground truth causal models, which specify all quantities which are considered measurable in our questions, are causal Bayesian networks (CBNs), where each causal mechanism (i.e., conditional probability of a variable given its parents in the factorization according to the causal graph  $G$  ) corresponds to a Bernoulli distribution. We compile a selection of graphs  $G$   based on examples drawn from multiple sources from the literature [ 66 ,  67 , 69 ,  88 ], where suitable graph structures are used to illustrate toy problems in causal inference. The complete list of structures we consider can be found in Appendix  A.3 ; the complete list of sources in Appendix  A.1 . \nSelecting Query Types. We again draw from the causal inference literature to collect common query types in each rung. As illustrated in the  “Sample a query type”  box in Figure  2 , for Rung 1, we can ask about probability distributions such as marginal probabilities and conditional probabilities. For Rung 2 questions, we can enquire  average treatment effects (ATE)  ( “how will    $Y$   change if    $X$  changes from    $x$   to  $x^{\\prime}\\!\\!\\stackrel{_{.}}{?}$  ), or what constitutes a valid adjustment set that can block all backdoor spurious correlations between  $X$   and  $Y$  . Lastly, for Rung 3, we include  counter factual s  ( “what would happen to  $Y$   had    $X$   been    $x^{\\prime}$    instead of    $x?\")$  ),  average treatment effect on the treated (ATT) ( “for the sub population whose    $X$   changed from    $x$   to    $x^{\\prime}$  , how does their  $Y$   change on average?” ), natural direct effect (NDE)  ( “what is the direct effect of  $X$   in  $Y$  , but not through the mediator?” ), and  natural indirect effect (NIE)  ( “what is the effect from    $X$   to  $Y$  through the mediator?” ). \nApplying the Causal Inference Engine for the Ground-truth answer. By construction, the causal processes we define encapsulates all necessary information to make the causal quantities of the query types identifiable. This allows us to apply the rules of causal inference to obtain an estimand for each causal graph and query type, and evaluate the estimand to get a ground truth answer. The Rung 2 queries simplify to Rung 1 terms using the rules of  $d o$  -calculus [ 59 ], and, for the Rung 3 queries, we apply methods of counter factual causal inference [ 67 ] (with details in Appendix  C.3 ). The estimand also specifies exactly which terms are necessary to include in the prompt as  “available data”  in order to ensure that enough information is provided to answer the question correctly (i.e., for ident if i ability), provided the correct causal reasoning is applied. Our entire code base of the data generation process can be found at our GitHub repository,  https://github.com/causalNLP/cladder . \n3.2 Natural Language Part of the Question Formulation \nWhile Section  3.1  describes a way to generate the ground-truth causal model, query and answers, com- puted through a causal inference engine, real-world causal reasoning problems are expressed in natural language rather than symbolic expressions. The next part of the data generation pipeline therefore focuses on the verb aliz ation of all these components with a plausible narrative in natural language. \nGenerating the Stories. For each causal graph, we collect a set of two to five  stories  which consist of a list of variable names for each node in the graph. The stories are primarily selected from examples in commonly cited causal inference books and papers (see Appendix  A.1 ), which ensures that the stories and corresponding causal graph structures adhere to empirical common sense (e.g., the drug- gender-recovery example of Pearl and Mackenzie  [ 66 ] ). However, it is very likely that at least some of the stories appear in the training data of many LLMs. Therefore, we also generate various  anti- common sense  and  nonsensical  variants of the stories, meant to isolate the effects of memorization. For the anti-common sens ical stories, we randomly do one of the actions: (1) replace the effect variable  $Y$   with an unusual attribute, that would not be an effect variable in any of the stories (e.g., “ear shape”); or (2) create an irrelevant treatment variable    $X$   that does not play a causal role in any of our common sens ical stories, such as “playing card games” (see Appendix  A.7 ). For the nonsensical variants, we invent artificial words as variable names such as “zory” and “qixy” (see Appendix  A.6 ). . "}
{"page": 5, "image_path": "doc_images/2312.04350v3_5.jpg", "ocr_text": "Verbalizing the Prompts. The verbalization procedure applies the mapping of symbolic variables\nto semantic concepts to form a plausible narrative for the underlying causal process and then translates\nthe symbolic expressions from the underlying causal process to natural language using carefully\ndesigned templates.\n\nSpecifically, we use several different grammatical forms for each semantic concept t in the story to\nmake the resulting prompt sound natural and grammatically correct. We first have the overall variable\nname Voverail (€) (e.g., the recovery status), and, then, for each binary value i € {0, 1}, we compose\nits noun Upoun(t = #) (e.g., recovery), verb (e.g., to recover), sentence Usent (£ = 7) (e.g., the patients\nrecover), noun with attributive clause vat: (¢ = 2) (e.g., patients who recover), and third conditional\nUcona(t = 7) (e.g., if the patient had recovered).\n\nUsing these elements, we first verbalize the causal graph by iterating through each node and its\noutgoing edges, using the template “t has a direct effect on CH(t).”, where CH(-) denotes the set of\ndirect effects (children) of a variable. Then, for the available data d, we verbalize each conditional\nprobability by “For vattr(tm = i), the probability of Unoun(tn = 1) is p.”, and each marginal\nprobability by “The overall probability of vattr(¢ = 1) is p.” Note that our distributions are Bernoulli,\nso it is adequate to just introduce the parameter p, which is the likelihood of ¢ = 1. For example, we\ngenerate sentences such as “The overall probability of recovery is 60%.” and “For patients who have\nsmall kidney stones, the probability of recovery is 70%.” Finally, for the query q, we instantiate each\nquery type in our dataset following our question templates in Appendix A.5 such that the questions\ncan always be answered with “yes” or “no”.\n\nGenerating the Explanations. Apart from the question-answer pairs, we also generate the step-by-\nstep explanations. Our goal is to provide all intermediate reasoning steps a student of causal inference\nwould use to answer the questions, so that each necessary subskill necessary for causal inference\ncan be evaluated individually. We identify the following six subskills: ® causal graph extraction;\n® correct query type interpretation; ® symbolic formalization of the query; ® semantic parsing to\ncompile the available data; © estimand derivation; and © arithmetic calculation to solve the estimand,\nas in the colored boxes in Figure 1. Our explanation e verbalizes all the elements ®-© as sequential\nsteps using our template in Appendix A.8.\n\n3.3 Dataset Statistics\n\nOur data-generating procedure has the potential to algorithmically generate a vast large number of\nquestions. In practice, we pick a dataset size that is large enough to be representative, and at the same\ntime not too large to be problematic given the expensive inference costs of LLMs. We therefore set\nour dataset size to be 10K, and report the statistics in Table 1.\n\nThe dataset roughly balance across the query types, graph structures, stories, and ground truth answers\n(as seen in Figure 3). Note that some causal queries are only compatible with a subset of the graphs,\nthereby resulting in a slightly lower representation of those queries (such as the NDE and NIE). More\ndetails on our design choices can be found in Appendix A.4.\n\nTotal Rung! Rung2  Rung3\nSize\n# Samples 10,112 | 3,160 3,160 3,792\nQuestion 4 NIE art\n# Sentences/Sample 6.01 5.88 5.37 6.65 . &\n# Words/Sample 80.9 73.43 76.95 90.42 é corner\n# Nodes/Graph 3.52 3.5 3.5 3.54 &\n# Edges/Graph 3.38 3.3 3.3 3.5 ° Rung 1\nAnswer Rung 2 &\nPositive Class (%) 50 50 50 50 %, =\nExplanations %o, are x\n# Sentences/Sample 9.11 9.1 8.1 9.96\n# Words/Sample 47.95 49.87 32.8 58.97\n\nFigure 3: Distributions of\nTable 1: Statistics of our CLADDER dataset v1.5. query types in our 10K data.\n\n3.4 Data Quality Check\n\nOur dataset is generated through an algorithmic procedure, which has the following potential benefits:\nformal correctness; zero human annotation cost; and, most importantly, controllability—e.g., for\n", "vlm_text": "Verbalizing the Prompts. The verb aliz ation procedure applies the mapping of symbolic variables to semantic concepts to form a plausible narrative for the underlying causal process and then translates the symbolic expressions from the underlying causal process to natural language using carefully designed templates. \nSpecifically, we use several different grammatical forms for each semantic concept    $t$   in the story to make the resulting prompt sound natural and grammatically correct. We first have the overall variable name    $v_{\\mathrm{overall}}(t)$  , the recovery status), and, then, for each binary   $i\\in\\{0,1\\}$  , we compose its noun  $v_{\\mathrm{moon}}(t=i)$   (e.g., recovery), verb (e.g., to recover), sentence  $v_{\\mathrm{sent}}({\\pmb t}=i)$   (e.g., the patients recover), noun with attributive clause    $v_{\\mathrm{{att}}}(\\mathbfit{t}=\\boldsymbol{i})$   (e.g., patients who recover), and third conditional  $v_{\\mathrm{cond}}(\\mathbf{\\dot{t}}=\\dot{\\iota})$   (e.g., if the patient had recovered). \nUsing these elements, we first verbalize the causal graph by iterating through each node and its outgoing edges, using the template   $^{**}$   has a direct effect on  $\\mathbf{CH}(t){}\"$     $\\mathbf{CH}(\\cdot)$   denotes the set of direct effects (children) of a variable. Then, for the available data  d , we verbalize each conditional probability by “For    $v_{\\mathrm{{att}}}(t_{m}\\;=\\;i)$  , the probability of    $v_{\\mathrm{ Ḋ n Ḍ }}(t_{n}\\,=\\,1)$   is    $p$  .”, and each marginal probability by “The overall probability of  $v_{\\mathrm{{att}}}(t=1)$   is  $p$  .” Note that our distributions are Bernoulli, so it is adequate to just introduce the parameter  $p$  , which is the likelihood of    $t=1$  . For example, we generate sentences such as “The overall probability of recovery is  $60\\%$  .” and “For patients who have small kidney stones, the probability of recovery is   $70\\%$  .” Finally, for the query  $\\pmb q$  , we instantiate each query type in our dataset following our question templates in Appendix  A.5  such that the questions can always be answered with “yes” or “no”. \nGenerating the Explanations. Apart from the question-answer pairs, we also generate the step-by- step explanations. Our goal is to provide all intermediate reasoning steps a student of causal inference would use to answer the questions, so that each necessary subskill necessary for causal inference can be evaluated individually. We identify the following six subskills:    $\\textcircled{1}$  causal graph extraction;  $\\circledast$  correct query type interpretation;    $\\textcircled{3}$  symbolic formalization of the query;    $\\clubsuit$  semantic parsing to compile the available data;  $\\mathfrak{G}$  estimand derivation; and    $\\mathfrak{G}$  arithmetic calculation to solve the estimand, as in the colored boxes in Figure  1 . Our explanation  $e$   verbalizes all the elements  $^{\\textregistered}$  as sequential steps using our template in Appendix  A.8 . \n3.3 Dataset Statistics \nOur data-generating procedure has the potential to algorithmic ally generate a vast large number of questions. In practice, we pick a dataset size that is large enough to be representative, and at the same time not too large to be problematic given the expensive inference costs of LLMs. We therefore set our dataset size to be 10K, and report the statistics in Table  1 . \nThe dataset roughly balance across the query types, graph structures, stories, and ground truth answers (as seen in Figure  3 ). Note that some causal queries are only compatible with a subset of the graphs, thereby resulting in a slightly lower representation of those queries (such as the NDE and NIE). More details on our design choices can be found in Appendix  A.4 . \nThe table provides a breakdown of a dataset across three categories labeled as \"Rung 1,\" \"Rung 2,\" and \"Rung 3,\" along with an overall \"Total\" category.\n\nHere are the contents:\n\n1. **Size**\n   - Total number of samples in the dataset: 10,112\n   - Number of samples in each rung:\n     - Rung 1: 3,160\n     - Rung 2: 3,160\n     - Rung 3: 3,792\n\n2. **Question**\n   - Average number of sentences per sample:\n     - Total: 6.01\n     - Rung 1: 5.88\n     - Rung 2: 5.37\n     - Rung 3: 6.65\n   - Average number of words per sample:\n     - Total: 80.9\n     - Rung 1: 73.43\n     - Rung 2: 76.95\n     - Rung 3: 90.42\n   - Average number of nodes per graph:\n     - Total: 3.52\n     - Rung 1: 3.5\n     - Rung 2: 3.5\n     - Rung 3: 3.54\n   - Average number of edges per graph:\n     - Total: 3.38\n     - Rung 1: 3.3\n     - Rung 2: 3.3\n     - Rung 3: 3.5\n\n3. **Answer**\n   - Percentage of positive class samples: 50% across all rungs and total\n\n4. **Explanations**\n   - Average number of sentences per sample:\n     - Total: 9.11\n     - Rung 1: 9.1\n     - Rung 2: 8.1\n     - Rung 3: 9.96\n   - Average number of words per sample:\n     - Total: 47.95\n     - Rung 1: 49.87\n     - Rung 2: 32.8\n     - Rung 3: 58.97\n\nThe table provides statistical measures related to the structure and characteristics of the samples, questions, answers, and explanations within the dataset, distributed across different rungs.\nThe image is a circular chart with three concentric segments, each labeled as \"Rung 1\", \"Rung 2\", and \"Rung 3\". These segments appear to represent different categories or types of queries. Each rung is divided into sections labeled as follows:\n\n- Rung 1: \n  - \"Cond. Prob.\" (Conditional Probability)\n  - \"Marg. Prob.\" (Marginal Probability)\n\n- Rung 2: \n  - \"ATE\" (Average Treatment Effect)\n  - \"Adjust. Set\" \n\n- Rung 3:\n  - \"NIE\" (Natural Indirect Effect)\n  - \"NDE\" (Natural Direct Effect)\n  - \"ATT\" (Average Treatment effect on the Treated)\n  - \"Counterf.\" (Counterfactual)\n\nThe colors transition from blue in Rung 1 to shades of orange/red in Rungs 2 and 3, suggesting a progression or hierarchy among the query types. The chart is labeled as \"Figure 3: Distributions of query types in our 10K data.\" This could suggest that the figure categorizes and visualizes the distribution of different query types that are part of a dataset consisting of 10,000 entries.\n3.4 Data Quality Check \nOur dataset is generated through an algorithmic procedure, which has the following potential benefits: formal correctness; zero human annotation cost; and, most importantly, control l ability—e.g., for the question distribution, as well as for making it more unlikely that the data was previously seen by the model. However, since the dataset is different from common NLP datasets collected from human natural language writing, we also need to perform additional data quality checks. We therefore checked for a list of non-formal, natural language properties: grammatical it y; human readability; naturalness/perplexity; and how well humans perform on this task. "}
{"page": 6, "image_path": "doc_images/2312.04350v3_6.jpg", "ocr_text": "the question distribution, as well as for making it more unlikely that the data was previously seen\nby the model. However, since the dataset is different from common NLP datasets collected from\nhuman natural language writing, we also need to perform additional data quality checks. We therefore\nchecked for a list of non-formal, natural language properties: grammaticality; human readability;\nnaturalness/perplexity; and how well humans perform on this task.\n\nFor grammaticality, we ran a grammatical error check on our dataset using the LanguageTool\npackage [51], and got on average 1.26 grammatical errors per 100 words (i.e., 98.74% correctness),\nwhich shows that most of the language in our dataset follows English grammar. For human readability,\nwe checked how comprehensible the questions are to students who have taken causality courses. We\nselected a random subset of 50 questions from the dataset, and let a graduate student annotator go\nthrough the questions to judge whether they could understand them or not: 96% of the questions were\ndeemed readable. Next, for the naturalness/perplexity score, we used the open-sourced GPT-2 model\nand obtained a perplexity score of 21.17 on our dataset, which is substantially lower (i.e., closer to\nthe distribution of natural human-written text) than the one of MATH [32], a commonly used dataset\nof maths questions. Lastly, we conducted a sanity check where one expert evaluator tried to solve a\nrandom sample of 50 questions from the dataset, and we recorded an accuracy of 82% on this task.\n\n4 Our CAUSALCOT Model\n\n——E——————\n\nOur Causal Chain-of-Thought (CausalCoT) Model:\nGuidance: Address the question by following the steps below:\n==» Extract the causal graph.\n\nPreparation Step 2| Determine the query type. Average Treatment Effect (ATE)\nPhase [step 3| Formalize the query. E[Y\\do(X=1)] - E[Y\\do(X = 0)]\n\n[step 4] Gather all relevant data.\n\n+-\nDeduce the estimand using causal inference: Given all the information above, deduce the estimand\nusing skills such as do-calculus, counterfactual prediction, and the basics of probabilities.\n\nSolution 1 = Yz=7 P(Z=z) [P(¥=1|Z=z,X=1)-P(Y=1|Z=z, X:\n\n‘0)] (Apply backdoor adjustment formula)\n\nPhase\nCalculate the estimand: Insert the relevant data in Step 4 into the estimand, perform basic arithmetic\ncalculations, and derive the final answer.\n\n} ATED) = w= -0.021<0\n\nBased on all the reasoning above, output one word to answer the initial question with just \"Yes\" or \"No\".\n\nFinal answer: No\n\nFigure 4: Illustration of our CAUSALCOT prompting strategy, which designs a chain of subquestions inspired by\nthe idea of a CI engine [66].\n\nIn order to guide LLMs in correctly answering the questions in CLADDER, we draw inspiration\nfrom the ideal functioning of the CI engine [66], which breaks down a causal reasoning problem\ninto multiple symbolically-grounded, simpler steps. We develop CAUSALCOT, a multi-step causal\nchain-of-thought prompt in Figure 4, which combines formal causal reasoning skills with the idea of\nchain-of-thought prompting [96] and the use of scratch pads for solving more complicated problems\nrequiring a long list of steps [55] for LLMs.\n\nWe base our prompt design on the multi-step reasoning process of causal inference as shown in\nFigure 4, first starting with four preparation steps: ® identifying the causal graph structure; @\ndetermining the causal query type;° ® formulating the query symbolically precisely; and ® extracting\nrelevant data from the prompt. Then, given all the information collected in the preparation stage, we\nintroduce the formal solution: © correctly deducing the estimand using causal inference techniques;\nand finally © evaluating the estimand to answer the question. This set of steps require both natural\nlanguage understanding to parse the question (as in most steps in the preparation phase), as well as\nformal causal reasoning to derive the correct estimand (as in the solution phase).\n\n°This step amounts to a multi-class classification problem, where each class is a different causal query.\n", "vlm_text": "\nFor grammatical it y, we ran a grammatical error check on our dataset using the Language Tool package [ 51 ], and got on average 1.26 grammatical errors per 100 words (i.e.,   $98.74\\%$   correctness), which shows that most of the language in our dataset follows English grammar. For human readability, we checked how comprehensible the questions are to students who have taken causality courses. We selected a random subset of 50 questions from the dataset, and let a graduate student annotator go through the questions to judge whether they could understand them or not:  $96\\%$   of the questions were deemed readable. Next, for the naturalness/perplexity score, we used the open-sourced GPT-2 model and obtained a perplexity score of 21.17 on our dataset, which is substantially lower (i.e., closer to the distribution of natural human-written text) than the one of MATH [ 32 ], a commonly used dataset of maths questions. Lastly, we conducted a sanity check where one expert evaluator tried to solve a random sample of 50 questions from the dataset, and we recorded an accuracy of   $82\\%$   on this task. \nThe image displays a visual representation of the Causal Chain-of-Thought (CausalCoT) Model, which is a methodological framework for answering a causal inference question. The model is divided into two main phases: the Preparation Phase and the Solution Phase, followed by delivering a final answer.\n\n1. **Preparation Phase**: \n   - **Step 1**: Extract the causal graph.\n   - **Step 2**: Determine the query type.\n   - **Step 3**: Formalize the query.\n   - **Step 4**: Gather all relevant data.\n\n2. **Solution Phase**:\n   - **Step 5**: Deduce the estimand using causal inference techniques like do-calculus, counterfactual prediction, and the basics of probabilities.\n   - **Step 6**: Calculate the estimand by inserting relevant data into the estimand formula, performing calculations, and deriving the final answer.\n\n3. **Final Question**: Using the reasoning, provide a final answer to the initial question with either \"Yes\" or \"No\". \n\nThe specific question addressed in this model is the Average Treatment Effect (ATE), expressed as \\(E[Y|do(X=1)] - E[Y|do(X=0)]\\), with a small detailed causal graph involving variables X, Y, and Z. The image also includes notation and calculations, showing the final result for ATE as -0.021, which leads to the final answer being \"No\".\nIn order to guide LLMs in correctly answering the questions in CL ADDER , we draw inspiration from the ideal functioning of the CI engine [ 66 ], which breaks down a causal reasoning problem into multiple symbolically-grounded, simpler steps. We develop C AUSAL C O T, a multi-step causal chain-of-thought prompt in Figure  4 , which combines formal causal reasoning skills with the idea of chain-of-thought prompting [ 96 ] and the use of scratch pads for solving more complicated problems requiring a long list of steps [ 55 ] for LLMs. \nWe base our prompt design on the multi-step reasoning process of causal inference as shown in Figure  4 , first starting with four preparation steps:    $\\textcircled{1}$  identifying the causal graph structure;    $\\circledast$  determining the causal query type; 6  $\\textcircled{3}$  formulating the query symbolically precisely; and    $\\clubsuit$  extracting relevant data from the prompt. Then, given all the information collected in the preparation stage, we introduce the formal solution:    $\\mathfrak{G}$  correctly deducing the estimand using causal inference techniques; and finally    $\\mathfrak{G}$  evaluating the estimand to answer the question. This set of steps require both  natural language understanding  to parse the question (as in most steps in the preparation phase), as well as formal causal reasoning  to derive the correct estimand (as in the solution phase). "}
{"page": 7, "image_path": "doc_images/2312.04350v3_7.jpg", "ocr_text": "We build our CAUSALCOT prompting strategy using GPT-4 [56], a recent autoregressive LLM that\nachieves state-of-the-art performance on many tasks. This latest model builds upon the previous series\nof general pretrained models (GPT) [7, 76] and adds reinforcement learning with human feedback,\nor instruction-tuning [1, 57, 104], to align the model responses to free-form questions with human\npreferences. It has achieved human-competitive performance over a list of tasks [8, 43, 54, 56, 105],\namong which the more formal tasks unseen in the training data still remain elusive [42, 78, 91].\n\nGiven a causal question q, we provide the LLM a list of instructions £ := (s,..., 86) consisting\nof the detailed descriptions of the six steps s1,..., 86 in Figure 4. As the model fiim : $i > Ti\nautoregressively produces responses 71,--- ,7¢ sequentially corresponding to the six steps, we\nconcatenate all the above before asking the final question “Based on all the reasoning above, output\none word to answer the initial question with just ‘Yes’ or ‘No’.” See the complete prompt in\nAppendix B.1. In the end, we obtain the binary answer a € {Yes, No} as the final result.\n\nCompared with the standard strategy of directly prompting the LLMs a question, we impose an\ninductive bias upon LLMs by using the causal inference framework, thus incorporating some of\nthe powerful, principled insights of the causal inference community for NLP tasks. In this way, we\nenhance the strong natural language ability of LLMs with formal causal reasoning skills.\n\n5 Testing LLMs with CLADDER\n\n5.1 Experimental Setup\n\nOur empirical investigation focuses on some of the most recent language models. We include the latest\nGPT-4 [56] with 1T parameters by the time we conduct the experiments (i.e., gpt-4-1106-preview),\nthe previous ChatGPT (i.e., GPT-3.5) with 175B parameters, and then a series of earlier models with\ninstruction-tuning on the 175B GPT-3 (text-davinci-001, -002, and -003) [57]. As baselines, we also\ninclude the non-instruction-tuned GPT-3 (davinci). We use the OpenAI API with temperature 0 when\nquerying these models. We also include open-source, more efficient models like LLaMa [93] and its\ninstruction-tuned version Alpaca [92], both with the same number of parameters, 6.7B.\n\n5.2. Main Results\n\n. a Acc. by Rung Acc. by Commonsense Alignment\n\nOverall Acc. 1 2 3 Comm. Nonsens. Anti-C.\nRandom 49.27 50.28 48.40 49.12 | 49.01 49.69 49.12\nLLaMa 44.03 48.23 29.46 52.66 | 45.14 44.22 42.67\nAlpaca 44.66 52.03 29.53 51.13 | 44.86 44.40 44.77\nGPT-3 Non-Instr. (davinci) 49.92 50.00 49.75 50.00 | 49.06 49.97 50.72\nGPT-3 Instr. (text-davinci-001) 51.40 51.30 52.63 50.47 | 54.31 50.13 50.05\nGPT-3 Instr. (text-davinci-002) 53.15 50.85 56.96 51.90 | 55.33 52.47 51.81\nGPT-3 Instr. (text-davinci-003) 56.26 51.11 62.97 54.96 | 56.83 54.79 57.49\nGPT-3.5 52.18 51.80 54.78 50.32 | 54.09 50.68 52.09\nGPT-4 62.03 63.01 62.82 60.55 | 62.27 63.09 60.47\n+ CAUSALCOT 70.40 83.35 67.47 62.05 | 69.25 71.58 70.12\n\nTable 2: Performance of all models on our CLADDER dataset v1.5. We report the overall accuracy (Acc.), and\nalso fine-grained accuracy by rung, and by degree of commonsense alignment, from commonsensical (Comm.),\nnonsensical (Nonsens.), to anti-commonsensical (Anti-C.).\n\nWe compare the performance of all models in Table 2. First, we can see that the causal reasoning task\nin CLADDER is in general very challenging for all models. Models such as the earlier, non-instruction-\ntuned GPT-3, and both LLaMa and Alpaca are around random performance. With instruction-tuning,\nmodels start to show some improvement. And amongst all, our CAUSALCOT achieves the highest\nperformance of 70.40%, which is substantially better than the vanilla GPT-4 by 8.37 points. Moreover,\nCAUSALCOT also achieve the best performance across all three rungs of causal questions, with a\nmonotonically decreasing performance as the rungs get higher, i.e., the questions get more difficult.\nSee Appendix D for experiments on our earlier dataset v1.0.\n\n5.3 Isolating the Effect of Data Contamination\n\nA well-known problem with evaluating LLMs on question-answering tasks is the data contamination\nproblem, i.e., that LLMs perform well on a test set because the test set is (unintentionally) contained\npartially or even entirely in the training data [7, 56]. We address this problem by creating not only the\ncommonsensical subset of our dataset, but also anti-commonsensical and nonsensical, both of which,\n", "vlm_text": "We build our C AUSAL C O T prompting strategy using GPT-4 [ 56 ], a recent auto regressive LLM that achieves state-of-the-art performance on many tasks. This latest model builds upon the previous series of general pretrained models (GPT) [ 7 ,  76 ] and adds reinforcement learning with human feedback, or instruction-tuning [ 1 ,  57 ,  104 ], to align the model responses to free-form questions with human preferences. It has achieved human-competitive performance over a list of tasks [ 8 ,  43 ,  54 ,  56 ,  105 ], among which the more formal tasks unseen in the training data still remain elusive [ 42 ,  78 ,  91 ]. \nGiven a causal question  $\\pmb q$  , we provide the LLM a list of instructions    $\\ell:=(\\pmb{s}_{1},.\\,.\\,.\\,,\\pmb{s}_{6})$   consisting of the detailed descriptions of the six  $s_{1},\\ldots,s_{6}$   in Figure  4 . As the model    $f_{\\mathrm{LLM}}:s_{i}\\mapsto\\pmb{r}_{i}$  auto regressive ly produces responses  $r_{1},\\cdot\\cdot\\cdot\\ ,r_{6}$   · · ·  sequentially corresponding to the six steps, we concatenate all the above before asking the final question “Based on all the reasoning above, output one word to answer the initial question with just ‘Yes’ or ‘No’.” See the complete prompt in Appendix  B.1 . In the end, we obtain the binary answer  $a\\in\\{\\mathrm{Yes,No}\\}$   as the final result. \nCompared with the standard strategy of directly prompting the LLMs a question, we impose an inductive bias  upon LLMs by using the causal inference framework, thus incorporating some of the powerful, principled insights of the causal inference community for NLP tasks. In this way, we enhance the strong natural language ability of LLMs with formal causal reasoning skills. \n5 Testing LLMs with CL ADDER \n5.1 Experimental Setup \nOur empirical investigation focuses on some of the most recent language models. We include the latest GPT-4 [ 56 ] with 1T parameters by the time we conduct the experiments (i.e., gpt-4-1106-preview), the previous ChatGPT (i.e., GPT-3.5) with 175B parameters, and then a series of earlier models with instruction-tuning on the 175B GPT-3 (text-davinci-001, -002, and -003) [ 57 ]. As baselines, we also include the non-instruction-tuned GPT-3 (davinci). We use the OpenAI API with temperature 0 when querying these models. We also include open-source, more efficient models like LLaMa [ 93 ] and its instruction-tuned version Alpaca [ 92 ], both with the same number of parameters, 6.7B. \nThe table presents accuracy metrics for various models evaluated in different categories:\n\n1. **Overall Acc. (Accuracy):**\n   - Random: 49.27\n   - LLama: 44.03\n   - Alpaca: 44.66\n   - GPT-3 Non-Instr. (davinci): 49.92\n   - GPT-3 Instr. (text-davinci-001): 51.40\n   - GPT-3 Instr. (text-davinci-002): 53.15\n   - GPT-3 Instr. (text-davinci-003): 56.26\n   - GPT-3.5: 52.18\n   - GPT-4: 62.03\n   - + CAUSALCoT: 70.40\n\n2. **Acc. by Rung:**\n   - Models are evaluated over three rungs with the following accuracies:\n     - Rung 1: Highest achieved by +CAUSALCoT at 83.35\n     - Rung 2: Highest achieved by +CAUSALCoT at 67.47\n     - Rung 3: Highest achieved by +CAUSALCoT at 62.05\n\n3. **Acc. by Commonsense Alignment:**\n   - Models are tested on three categories:\n     - Comm.: Highest with GPT-4 at 62.27\n     - Nonsens.: Highest with +CAUSALCoT at 71.58\n     - Anti-C.: Highest with +CAUSALCoT at 70.12\n\nThe overall trend shows increasing accuracy through different versions of the GPT models, with the best performance achieved by GPT-4 and further improved with +CAUSALCoT.\nWe compare the performance of all models in Table  2 . First, we can see that the causal reasoning task in CL ADDER  is in general very challenging for all models. Models such as the earlier, non-instruction- tuned GPT-3, and both LLaMa and Alpaca are around random performance. With instruction-tuning, models start to show some improvement. And amongst all, our C AUSAL C O T achieves the highest performance of  $70.40\\%$  , which is substantially better than the vanilla GPT-4 by 8.37 points. Moreover, C AUSAL C O T also achieve the best performance across all three rungs of causal questions, with a monotonically decreasing performance as the rungs get higher, i.e., the questions get more difficult. See Appendix  D  for experiments on our earlier dataset v1.0. \n5.3 Isolating the Effect of Data Contamination \nA well-known problem with evaluating LLMs on question-answering tasks is the data contamination problem, i.e., that LLMs perform well on a test set because the test set is (unintentionally) contained partially or even entirely in the training data [ 7 ,  56 ]. We address this problem by creating not only the common sens ical subset of our dataset, but also anti-common sens ical and nonsensical, both of which, by construction, are very likely not in the training data of LLMs. From the accuracy by commonsense alignment degree in Table  2 , we can see the original GPT-4 model performs the worst on the anti- common sens ical subset (1.8 points lower than that on the common sens ical subset). However, our C AUSAL C O T enhances the reasoning ability across all levels, with substantial improvement on anti-common sens ical data by 9.65 points, highlighting the strength of C AUSAL C O T on unseen data. "}
{"page": 8, "image_path": "doc_images/2312.04350v3_8.jpg", "ocr_text": "by construction, are very likely not in the training data of LLMs. From the accuracy by commonsense\nalignment degree in Table 2, we can see the original GPT-4 model performs the worst on the anti-\ncommonsensical subset (1.8 points lower than that on the commonsensical subset). However, our\nCAUSALCOT enhances the reasoning ability across all levels, with substantial improvement on\nanti-commonsensical data by 9.65 points, highlighting the strength of CAUSALCOT on unseen data.\n\n5.4 Error Analysis by Subquestions\n\nStep © Step @ Step@&® Step@ Step ©\nNode Edge Dist. ({) Overall FI Rung 1 Rung 2 Rung 3 Estimand FI Arithmetic\n99.34 97.01 1.69 50.65 69.99 59.14 42.12 53 47.53 99\n\nTable 3: Performance for each step in CAUSALCOT. For Step ®, we report the F1 score of node prediction, edge\nprediction, and also the graph edit distance (Dist.) with the true graph. See more details in Appendix E.1.\n\nWe conduct a fine-grained error analysis by looking into the performance of different steps of\nCAUSALCOT in Table 3.’ We can see that the model is good at Step ® to extract causal graph\nG, achieving high F1 scores for predicting both the nodes and the edges correctly, although not\nperfect, still leaving a graph edit distance of 1.69 between the ground truth causal graph and the\nmodel-identified graph. The other steps are more challenging for the model. Among those, Steps\n®, ® and © require careful and correct application of causal inference, where the model struggles.\nThis reveals a notable weakness of current LLMs to perform formal causal reasoning, which is an\nimportant direction for future work on improving and enhancing LLMs. To better understand the\nreasoning abilities of LLMs, we also perform an extensive analysis taking the entire reasoning chain\nof our CAUSALCOT and the ground-truth explanations, to produce 20 fine-grained scores about\nthe multi-step reasoning quality using the ROSCOE framework [25], and show detailed results in\nAppendix E.2.\n\n5.5 Effect of In-Context Learning Marg. [i LL.\nAs an additional analysis, we look into the effect of in-context learn- cont I,\ning (ICL) by providing an example solution before asking the ques- Count _| -0\ntion. The interesting question to us is whether models can generalize AIT | --10\nacross different query types. Namely, we keep our CAUSALCOT ype a a --20\nframework, and prepend a reasoning example of query type i, and NIE ia\nthen calculate how much improvement it can bring when models © Ok SR KM\nanswer new questions of query type j. In Figure 5, we can see that WFP PP LT\n\nconditional probability and NIE are the questions that benefit the _ ;\nmost from ICL, and showing examples of marginal probability and Figure € a Heatnnap showing the\nATT are among the most helpful to all questions in general. ‘ow helptul each query type 1s to\n\nsolving subsequent query types.\n6 Related Work\n\nSkill evaluation for LLMs. Our work may be seen as part of the literature aimed at evaluating the\nperformance of current LLMs [7, 15, 56, 76, 103, inter alia], focusing on understanding their strengths\nand weaknesses. Various studies into the capabilities of LLMs [8, 39, 56, 74] change people’s\nperception of domains such as education [2, 80], medicine [54, 87], law [43], and computational!\nsocial science [105]. However, most work evaluates new models on existing datasets from previously-\ncurated large-scale benchmarks [89, 94, 95], or human exams [41, 43, 56] which is becoming\nincreasingly unreliable due to training set contamination.\n\nCausality-related skills for NLP. With the increasing attention on LLMs and causality [100, 101],\nwe review several formulations of causality-related skills for NLP, which we summarize into (1)\ncausality as knowledge, (2) causality as language comprehension, and (3) causality as reasoning. In\nthe causality-as-knowledge line of work, many existing studies investigate how well NLP models\nunderstand commonsense causality, such as the cause and effect of an agent’s action [81], motivation\nand emotional reaction in a social context [82], correspondence of a set of steps with a high-leve\ngoal [102], development of a story given a different beginning [75], and how in general LLMs serve\nas a knowledge base of causality [100]. Concurrent work [45] focuses on evaluating LLMs on\nvarious causality related tasks by leveraging the conceptual knowledge accrued from the training\n\n7We experienced some rate-limiting in the fine-grained analysis of LLMs that are only accessible through a\nweb API. As a result, we occasionally had to evaluate on a subset of 2K random samples.\n", "vlm_text": "\nThe table contains data related to different steps in a process, which seem to be numbered sequentially. Here's a breakdown of the data presented:\n\n- **Step ①**: \n  - **Node**: 99.34\n  - **Edge**: 97.01\n  - **Dist. (↓)**: 1.69\n\n- **Step ②**:\n  - **Overall F1**: 50.65\n  - **Rung 1**: 69.99\n  - **Rung 2**: 59.14\n  - **Rung 3**: 42.12\n\n- **Step ③ & ⑤**:\n  - **Estimand**: 53\n\n- **Step ④**:\n  - **F1**: 47.53\n\n- **Step ⑥**:\n  - **Arithmetic**: 99\n\nThe table seems to track various metrics (like F1 scores, node and edge percentages, distances) across multiple steps, potentially indicative of different stages or evaluations in a process.\nWe conduct a fine-grained error analysis by looking into the performance of different steps of C AUSAL C O T in Table  3 .   We can see that the model is good at Step    $\\textcircled{1}$  to extract causal graph  $\\mathcal{G}$  , achieving high F1 scores for predicting both the nodes and the edges correctly, although not perfect, still leaving a graph edit distance of 1.69 between the ground truth causal graph and the model-identified graph. The other steps are more challenging for the model. Among those, Steps  $\\circledast$  ,  $\\textcircled{3}$  and  $\\mathfrak{G}$  require careful and correct application of causal inference, where the model struggles. This reveals a notable weakness of current LLMs to perform formal causal reasoning, which is an important direction for future work on improving and enhancing LLMs. To better understand the reasoning abilities of LLMs, we also perform an extensive analysis taking the entire reasoning chain of our C AUSAL C O T and the ground-truth explanations, to produce 20 fine-grained scores about the multi-step reasoning quality using the ROSCOE framework [ 25 ], and show detailed results in Appendix  E.2 . \n5.5 Effect of In-Context Learning \nAs an additional analysis, we look into the effect of in-context learn- ing (ICL) by providing an example solution before asking the ques- tion. The interesting question to us is whether models can generalize across different query types. Namely, we keep our C AUSAL C O T framework, and prepend a reasoning example of query type    $i$  , and then calculate how much improvement it can bring when models answer new questions of query type  $j$  . In Figure  5 , we can see that conditional probability and NIE are the questions that benefit the most from ICL, and showing examples of marginal probability and ATT are among the most helpful to all questions in general. \nThe image is a heatmap that visualizes how helpful each query type is in solving subsequent query types. The query types listed are \"Marg.\" (Marginal), \"Cond.\" (Conditional), \"ATE\" (Average Treatment Effect), \"Count.\", \"ATT\" (Average Treatment Effect on the Treated), \"NDE\" (Natural Direct Effect), and \"NIE\" (Natural Indirect Effect). The cells in the heatmap are colored in shades of blue, with a corresponding color bar indicating the numerical range from negative values to positive values. Darker shades typically represent higher values, either positively or negatively, indicating the level of helpfulness between the query types in solving subsequent queries.\n6 Related Work \nSkill evaluation for LLMs. Our work may be seen as part of the literature aimed at evaluating the performance of current LLMs [ 7 ,  15 ,  56 ,  76 ,  103 ,  inter alia ], focusing on understanding their strengths and weaknesses. Various studies into the capabilities of LLMs [ 8 ,  39 ,  56 ,  74 ] change people’s perception of domains such as education [ 2 ,  80 ], medicine [ 54 ,  87 ], law [ 43 ], and computational social science [ 105 ]. However, most work evaluates new models on existing datasets from previously- curated large-scale benchmarks [ 89 ,  94 ,  95 ], or human exams [ 41 ,  43 ,  56 ] which is becoming increasingly unreliable due to training set contamination. \nCausality-related skills for NLP. With the increasing attention on LLMs and causality [ 100 ,  101 ], we review several formulations of causality-related skills for NLP, which we summarize into (1) causality as knowledge, (2) causality as language comprehension, and (3) causality as reasoning. In the  causality-as-knowledge  line of work, many existing studies investigate how well NLP models understand commonsense causality, such as the cause and effect of an agent’s action [ 81 ], motivation and emotional reaction in a social context [ 82 ], correspondence of a set of steps with a high-level goal [ 102 ], development of a story given a different beginning [ 75 ], and how in general LLMs serve as a knowledge base of causality [ 100 ]. Concurrent work [ 45 ] focuses on evaluating LLMs on various causality related tasks by leveraging the conceptual knowledge accrued from the training data, rather than formal causal inference, except for their causal sufficiency analysis which is close to our counter factual questions. Importantly, most work in this line does not define explicit causal graphs, making it difficult to quantitatively define the ground-truth causal relationships in a principled way. The  causality-as-language-comprehension  line of work stems from traditional linguistic studies on causal connectives and causal language usage [ 9 ,  90 ,  99 ], to the recent causal relation extraction [ 4 ,  33 ,  98 ] to identify cause-effect pairs as a subtask of information extraction from text. "}
{"page": 9, "image_path": "doc_images/2312.04350v3_9.jpg", "ocr_text": "data, rather than formal causal inference, except for their causal sufficiency analysis which is close\nto our counterfactual questions. Importantly, most work in this line does not define explicit causal\ngraphs, making it difficult to quantitatively define the ground-truth causal relationships in a principled\nway. The causality-as-language-comprehension line of work stems from traditional linguistic studies\non causal connectives and causal language usage [9, 90, 99], to the recent causal relation extraction\n[4, 33, 98] to identify cause-effect pairs as a subtask of information extraction from text.\n\nFinally, for causality as formal reasoning, our CLADDER work formulates the task of causal infer-\nence for NLP, and our other work, CORR2CAUSE [42], addresses the causal discovery problem to\ninfer causation from correlation. Together, they cover the two major branches of causal reasoning\ninvestigated in existing technical literature on causality. See a comprehensive comparison of literature\nin Appendix F.\n\n7 Discussion of Limitations and Future Work\n\nA Natural Language “Mini Turing Test” for Causality. Pearl and Mackenzie [66] describe an\nideal “mini-Turing test” to assess understanding of causal inference, and argue that if a machine can\nanswer all possible questions correctly, then it “understands” causality. According to the authors, this\nis because there are no possible shortcuts when you consider all possible combinations of queries,\ngraphs and data in this ideal test: due to their combinatorial explosion, the machine can only answer\nall questions right if it correctly applies causal reasoning. From this point of view, our work constitutes\na first step towards a mini-Turing test formulated in natural language. However, we cover only some\nof the commonly studied causal queries spanning all three rungs. Future work may extend this to\nfurther queries, such as, e.g., path-specific effects other than NDE and NIE [52], thereby increasing\nthe number of potential questions and moving closer to the ideal test.\n\nLLMs and Causal Reasoning. _ It has been claimed that LLMs understand causality well (e.g., [45]\nreport high performance, such as 97% and 92%). In contrast, our work suggests that LLMs may\nstill be far from reasoning reliably about causality (reaching only 60+% on CLADDER). As argued\nin Section 1, we believe that investigating this aspect may be of particular importance, since causal\ninference is crucial in many policy-relevant scenarios, where reliable AI systems could assist decision-\nmaking: from epidemiology [22, 79] to economics [10, 37] to fairness [47, 71]. Testing the abilities of\nthese systems in semi-realistic scenarios is therefore crucial, motivating some of the design choices in\nour dataset: e.g., the example in Figure | was inspired by similar questions which arose in the context\nof the COVID-19 pandemic, where incorrect causal reasoning resulted in a fallacy where vaccinations\nwere considered to be harmful instead of beneficial [20, 49]. Further work may be dedicated to making\nthe questions and verbalizations even closer to realistic instances of causal inference problems.\n\nA CI Engine Plug-in for LLMs. An interesting direction for future research could be to provide\nthe LLM access to an actual implementation of the CI engine. For example, Davis and Aaronson [13]\ntested the improvement of math abilities in LLMs augmented with plug-ins (i.e., external modules\nthat extend the model’s capabilities by adding specific functionality or customizing its behaviour for\nparticular tasks, like a calculator), suggesting that they significantly enhance the model’s ability to\nsolve these problems. However, even with plug-ins, there are still often “interface” failures: that\nis, “/the LLM] often has trouble formulating problems in a way that elicits useful answers from the\nplug-ins”. We hypothesise that something similar would happen for causal inference: even once\nsuitable plug-ins are built, the language-to-tool interface may still be a non-trivial research question.\n\n8 Conclusion\n\nWe proposed formal causal reasoning as a new task to evaluate LLMs, and created the CLADDER\nbenchmark, covering several aspects of causal inference across all rungs of the ladder of causation\nand verbalizations involving semi-realistic scenarios. To address the task, we proposed a prompting\nstrategy, CAUSALCOT, inspired by the principles of formal causal inference, which introduces\nmultistep chain-of-thought reasoning for causal questions. Extensive experiments indicate that this\ndataset is highly challenging, thus offering a principled tool to gain a better understanding of the\nreasoning abilities of LLMs and to develop better models for causal reasoning in natural language.\n\nAcknowledgment\n\nWe thank Sergio Hernan Garrido Mejia for pointing us to Python implementations of the causal\ninference engine. We thank Armen Aghajanyan for the idea of testing psychological bias in LLMs,\n\n10\n", "vlm_text": "\nFinally, for  causality as formal reasoning , our CL ADDER  work formulates the task of causal infer- ence for NLP, and our other work, C ORR 2C AUSE  [ 42 ], addresses the causal discovery problem to infer causation from correlation. Together, they cover the two major branches of causal reasoning investigated in existing technical literature on causality. See a comprehensive comparison of literature in Appendix  F . \n7 Discussion of Limitations and Future Work \nA Natural Language  “Mini Turing Test”  for Causality. Pearl and Mackenzie  [ 66 ]  describe an ideal  “mini-Turing test”  to assess understanding of causal inference, and argue that if a machine can answer all possible questions correctly, then it “understands” causality. According to the authors, this is because there are no possible shortcuts when you consider all possible combinations of queries, graphs and data in this ideal test: due to their combinatorial explosion, the machine can only answer all questions right if it correctly applies causal reasoning. From this point of view, our work constitutes a  first step towards a mini-Turing test formulated in natural language . However, we cover only some of the commonly studied causal queries spanning all three rungs. Future work may extend this to further queries, such as, e.g., path-specific effects other than NDE and NIE [ 52 ], thereby increasing the number of potential questions and moving closer to the ideal test. \nLLMs and Causal Reasoning. It has been claimed that LLMs understand causality well (e.g., [ 45 ] report high performance, such as  $97\\%$   and  $92\\%$  ). In contrast, our work suggests that LLMs may still be far from reasoning reliably about causality (reaching only  $60+\\%$   on CL ADDER ). As argued in Section  1 , we believe that investigating this aspect may be of particular importance, since causal inference is crucial in many policy-relevant scenarios, where reliable AI systems could assist decision- making: from epidemiology [ 22 ,  79 ] to economics [ 10 ,  37 ] to fairness [ 47 ,  71 ]. Testing the abilities of these systems in semi-realistic scenarios is therefore crucial, motivating some of the design choices in our dataset: e.g., the example in Figure  1  was inspired by similar questions which arose in the context of the COVID-19 pandemic, where incorrect causal reasoning resulted in a fallacy where vaccinations were considered to be harmful instead of beneficial [ 20 ,  49 ]. Further work may be dedicated to making the questions and verb aliz at ions even closer to realistic instances of causal inference problems. \nA CI Engine Plug-in for LLMs. An interesting direction for future research could be to provide the LLM access to an actual implementation of the CI engine. For example, Davis and Aaronson  [ 13 ] tested the improvement of math abilities in LLMs augmented with plug-ins (i.e., external modules that extend the model’s capabilities by adding specific functionality or customizing its behaviour for particular tasks, like a calculator), suggesting that they significantly enhance the model’s ability to solve these problems. However, even with plug-ins, there are still often  “interface”  failures: that is,  “[the LLM] often has trouble formulating problems in a way that elicits useful answers from the plug-ins” . We hypothesis e that something similar would happen for causal inference: even once suitable plug-ins are built, the language-to-tool interface may still be a non-trivial research question. \n8 Conclusion \nWe proposed formal causal reasoning as a new task to evaluate LLMs, and created the CL ADDER benchmark, covering several aspects of causal inference across all rungs of the ladder of causation and verb aliz at ions involving semi-realistic scenarios. To address the task, we proposed a prompting strategy, C AUSAL C O T, inspired by the principles of formal causal inference, which introduces multistep chain-of-thought reasoning for causal questions. Extensive experiments indicate that this dataset is highly challenging, thus offering a principled tool to gain a better understanding of the reasoning abilities of LLMs and to develop better models for causal reasoning in natural language. \nAcknowledgment \nWe thank Sergio Hernan Garrido Mejia for pointing us to Python implementations of the causal inference engine. We thank Armen Aghajanyan for the idea of testing psychological bias in LLMs, which partly contributes to the idea of exposing causal bias in LLMs. We thank András Strausz for various timely coding help, especially for our Simpson’s paradox case. "}
{"page": 10, "image_path": "doc_images/2312.04350v3_10.jpg", "ocr_text": "which partly contributes to the idea of exposing causal bias in LLMs. We thank Andras Strausz for\nvarious timely coding help, especially for our Simpson’s paradox case.\n\nThe material presented in this manuscript is partly based upon works supported by the German\nFederal Ministry of Education and Research (BMBF): Tiibingen AI Center, FKZ: 011S18039B; by\nthe Machine Learning Cluster of Excellence, EXC number 2064/1 — Project number 390727645;\nthe Swiss National Science Foundation (Project No. 197155); a Responsible AI grant by the\nHaslerstiftung; an ETH Grant (ETH-19 21-1); and by the John Templeton Foundation (grant #61156).\nZhijing Jin is supported by PhD fellowships from the Future of Life Institute and Open Philanthropy,\nas well as the travel support from ELISE (GA no 951847) for the ELLIS program. Felix Leeb is\nsupported by the International Max Planck Research School for Intelligent Systems (IMPRS-IS).\nLuigi Gresele is supported by the VideoPredict project, FKZ: 011821088.\n\nAuthor Contributions\n\nThe conceptualization and design of this project was led by Zhijing, Luigi and Felix, and supervised\nby Mrinmaya on the NLP part, and Bernhard on the causality part. Max provided timely insights\nfrom cognitive science on different types of causal tasks and on the project design. In the exploration\nstage, Ojasv did substantial work on discovering causal fallacies in news and on Twitter, which, while\nnot included in the current systematic way of generating causal inference questions, was a significant\ncontribution in the course of the project and in comparing various task formulations.\n\nAs for the operationalization and programming, the dataset composition was mainly led by Yuen\nand Felix, together with daily discussions with Zhijing, and weekly discussions with Luigi. Zhiheng\nsupported an important function of generating the backdoor adjustment set for a given causal graph\nwith the treatment and effect variables. The experiments are mainly conducted by Zhijing and\nFernando, with Kevin finishing the evaluation results using the ROSCOE package.\n\nReferences\n\n[1] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma,\nDawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath,\nJackson Kernion, Tom Conerly, Sheer El Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny\nHernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine\nOlsson, Dario Amodei, Tom B. Brown, Jack Clark, Sam McCandlish, Chris Olah, Benjamin\nMann, and Jared Kaplan. 2022. Training a helpful and harmless assistant with reinforcement\nlearning from human feedback. CoRR, abs/2204.05862. [Cited on page 8.]\n\n2] David Baidoo-Anu and Leticia Owusu Ansah. 2023. Education in the era of generative artificial\nintelligence (AI): Understanding the potential benefits of ChatGPT in promoting teaching and\nlearning. Available at SSRN 4337484. [Cited on page 9.]\n\n3] Elias Bareinboim, Juan D Correa, Duligur Ibeling, and Thomas Icard. 2022. On Pearl’s\nhierarchy and the foundations of causal inference. In Probabilistic and causal inference: the\nworks of judea pearl, pages 507-556. [Cited on pages 2 and 3.]\n\n4] Steven Bethard, William Corvey, Sara Klingenstein, and James H. Martin. 2008. Building a\ncorpus of temporal-causal structure. In Proceedings of the Sixth International Conference on\nLanguage Resources and Evaluation (LREC’08), Marrakech, Morocco. European Language\nResources Association (ELRA). [Cited on page 10.]\n\n5] Chandra Bhagavatula, Ronan Le Bras, Chaitanya Malaviya, Keisuke Sakaguchi, Ari Holtzman,\nHannah Rashkin, Doug Downey, Wen-tau Yih, and Yejin Choi. 2020. Abductive commonsense\nreasoning. In 8th International Conference on Learning Representations, ICLR 2020, Addis\nAbaba, Ethiopia, April 26-30, 2020. OpenReview.net. [Cited on page 27.]\n\n6] Stephan Bongers, Patrick Forré, Jonas Peters, and Joris M Mooij. 2021. Foundations of\nstructural causal models with cycles and latent variables. The Annals of Statistics, 49(5):2885—\n2915. [Cited on page 23.]\n\n7| Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla\nDhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agar-\nwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh,\n\n11\n", "vlm_text": "\nThe material presented in this manuscript is partly based upon works supported by the German Federal Ministry of Education and Research (BMBF): Tübingen AI Center, FKZ: 01IS18039B; by the Machine Learning Cluster of Excellence, EXC number 2064/1 – Project number 390727645; the Swiss National Science Foundation (Project No. 197155); a Responsible AI grant by the Has ler stiftung; an ETH Grant (ETH-19 21-1); and by the John Templeton Foundation (grant #61156). Zhijing Jin is supported by PhD fellowships from the Future of Life Institute and Open Philanthropy, as well as the travel support from ELISE (GA no 951847) for the ELLIS program. Felix Leeb is supported by the International Max Planck Research School for Intelligent Systems (IMPRS-IS). Luigi Gresele is supported by the Video Predict project, FKZ: 01IS21088. \nAuthor Contributions \nThe conceptualization and design of this project was led by Zhijing, Luigi and Felix, and supervised by Mrinmaya on the NLP part, and Bernhard on the causality part. Max provided timely insights from cognitive science on different types of causal tasks and on the project design. In the exploration stage, Ojasv did substantial work on discovering causal fallacies in news and on Twitter, which, while not included in the current systematic way of generating causal inference questions, was a significant contribution in the course of the project and in comparing various task formulations. \nAs for the op e rationalization and programming, the dataset composition was mainly led by Yuen and Felix, together with daily discussions with Zhijing, and weekly discussions with Luigi. Zhiheng supported an important function of generating the backdoor adjustment set for a given causal graph with the treatment and effect variables. The experiments are mainly conducted by Zhijing and Fernando, with Kevin finishing the evaluation results using the ROSCOE package. \nReferences \n[1]  Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom B. Brown, Jack Clark, Sam McCandlish, Chris Olah, Benjamin Mann, and Jared Kaplan. 2022. Training a helpful and harmless assistant with reinforcement learning from human feedback.  CoRR , abs/2204.05862. [Cited on page  8 .] [2]  David Baidoo-Anu and Leticia Owusu Ansah. 2023. Education in the era of generative artificial intelligence (AI): Understanding the potential benefits of ChatGPT in promoting teaching and learning.  Available at SSRN 4337484 . [Cited on page  9 .] [3]  Elias Bareinboim, Juan D Correa, Duligur Ibeling, and Thomas Icard. 2022. On Pearl’s hierarchy and the foundations of causal inference. In  Probabilistic and causal inference: the works of judea pearl , pages 507–556. [Cited on pages  2  and  3 .] [4]  Steven Bethard, William Corvey, Sara K lingen stein, and James H. Martin. 2008. Building a corpus of temporal-causal structure. In  Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC’08) , Marrakech, Morocco. European Language Resources Association (ELRA). [Cited on page  10 .] [5]  Chandra Bhaga va tula, Ronan Le Bras, Chaitanya Malaviya, Keisuke Sakaguchi, Ari Holtzman, Hannah Rashkin, Doug Downey, Wen-tau Yih, and Yejin Choi. 2020. Abductive commonsense reasoning. In  8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020 . OpenReview.net. [Cited on page  27 .] [6]  Stephan Bongers, Patrick Forré, Jonas Peters, and Joris M Mooij. 2021. Foundations of structural causal models with cycles and latent variables.  The Annals of Statistics , 49(5):2885– 2915. [Cited on page  23 .] [7]  Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neel a kant an, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agar- wal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, "}
{"page": 11, "image_path": "doc_images/2312.04350v3_11.jpg", "ocr_text": "[15]\n\n[20\n\nDaniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler,\nMateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCan-\ndlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot\nlearners. In Advances in Neural Information Processing Systems 33: Annual Conference on\nNeural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.\n[Cited on pages 1, 8, and 9.]\n\nSébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece\nKamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott M. Lundberg, Harsha Nori, Hamid Palangi,\nMarco Tilio Ribeiro, and Yi Zhang. 2023. Sparks of artificial general intelligence: Early\nexperiments with GPT-4. CoRR, abs/2303.12712. [Cited on pages 8 and 9.]\n\nAngela Cao, Gregor Williamson, and Jinho D. Choi. 2022. A cognitive approach to annotating\ncausal constructions in a cross-genre corpus. In Proceedings of the 16th Linguistic Annota-\ntion Workshop (LAW-XVI) within LREC2022, pages 151-159, Marseille, France. European\nLanguage Resources Association. [Cited on page 10.]\n\nDavid Card. 1999. The causal effect of education on earnings. Handbook of labor economics,\n3:1801-1863. [Cited on pages | and 10.]\n\nRaj Chetty, John N Friedman, Nathaniel Hilger, Emmanuel Saez, Diane Whitmore Schanzen-\nbach, and Danny Yagan. 2011. How does your kindergarten classroom affect your earnings?\nEvidence from project star. The Quarterly journal of economics, 126(4):1593-1660. [Cited on\npage 1.]\n\nRobert G Cowell, Philip Dawid, Steffen L Lauritzen, and David J Spiegelhalter. 2007. Proba-\nbilistic networks and expert systems: Exact computational methods for Bayesian networks.\nSpringer Science & Business Media. [Cited on page 3.]\n\nEmest Davis and Scott Aaronson. 2023. Testing GPT-4 with Wolfram Alpha and Code\nInterpreter plug-ins on math and science problems. arXiv preprint arXiv:2308.05713. [Cited\non page 10.]\n\nGaston De Serres, France Markowski, Eveline Toth, Monique Landry, Danielle Auger, Marléne\nMercier, Philippe Bélanger, Bruno Turmel, Horacio Arruda, Nicole Boulianne, et al. 2013.\nLargest measles epidemic in North America in a decade—Quebec, Canada, 2011: Contribution\nof susceptibility, serendipity, and superspreading events. The Journal of infectious diseases,\n207(6):990-998. [Cited on page 1|.]\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-\ntraining of deep bidirectional transformers for language understanding. In Proceedings of\nthe 2019 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-\n4186, Minneapolis, Minnesota. Association for Computational Linguistics. [Cited on pages |\nand 9.]\n\nQuang Do, Yee Seng Chan, and Dan Roth. 2011. Minimally supervised event causality\nidentification. In Proceedings of the 2011 Conference on Empirical Methods in Natural Lan-\nguage Processing, pages 294-303, Edinburgh, Scotland, UK. Association for Computational\nLinguistics. [Cited on page 27.]\n\nRichard Doll and A Bradford Hill. 1950. Smoking and carcinoma of the lung. British medical\njournal, 2(4682):739. [Cited on page 1.]\n\nRichard Doll and A Bradford Hill. 1954. The mortality of doctors in relation to their smoking\nhabits. British medical journal, 1(4877):1451. [Cited on page 1.]\n\nJesse Dunietz, Lori Levin, and Jaime Carbonell. 2017. The BECauSE corpus 2.0: Annotating\ncausality and overlapping relations. In Proceedings of the 11th Linguistic Annotation Workshop,\npages 95-104, Valencia, Spain. Association for Computational Linguistics. [Cited on page 27.]\n\nJordan Ellenberg. 2021. Coronavirus vaccines work. But this statistical illusion makes people\nthink they don’t. The Washington Post. [Cited on page 10.]\n\n12\n", "vlm_text": "Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCan- dlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In  Advances in Neural Information Processing Systems 33: Annual Conference on \n[Cited on pages  1 ,  8 , and  9 .]\n\n [8]  Sébastien Bubeck, Varun Chandra sekar an, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott M. Lundberg, Harsha Nori, Hamid Palangi, Marco Túlio Ribeiro, and Yi Zhang. 2023. Sparks of artificial general intelligence: Early experiments with GPT-4.  CoRR , abs/2303.12712. [Cited on pages  8  and  9 .]\n\n [9]  Angela Cao, Gregor Williamson, and Jinho D. Choi. 2022. A cognitive approach to annotating causal constructions in a cross-genre corpus. In  Proceedings of the 16th Linguistic Annota- tion Workshop (LAW-XVI) within LREC2022 , pages 151–159, Marseille, France. European Language Resources Association. [Cited on page  10 .]\n\n [10]  David Card. 1999. The causal effect of education on earnings.  Handbook of labor economics , 3:1801–1863. [Cited on pages  1  and  10 .]\n\n [11]  Raj Chetty, John N Friedman, Nathaniel Hilger, Emmanuel Saez, Diane Whitmore Schanzen- bach, and Danny Yagan. 2011. How does your kindergarten classroom affect your earnings? Evidence from project star.  The Quarterly journal of economics , 126(4):1593–1660. [Cited on page  1 .]\n\n [12]  Robert G Cowell, Philip Dawid, Steffen L Lauritzen, and David J Spiegel halter. 2007.  Proba- bilistic networks and expert systems: Exact computational methods for Bayesian networks . Springer Science & Business Media. [Cited on page  3 .]\n\n [13]  Ernest Davis and Scott Aaronson. 2023. Testing GPT-4 with Wolfram Alpha and Code Interpreter plug-ins on math and science problems.  arXiv preprint arXiv:2308.05713 . [Cited on page  10 .]\n\n [14]  Gaston De Serres, France Markowski, Eveline Toth, Monique Landry, Danielle Auger, Marlène Mercier, Philippe Bélanger, Bruno Turmel, Horacio Arruda, Nicole Boulianne, et al. 2013. Largest measles epidemic in North America in a decade—Quebec, Canada, 2011: Contribution of susceptibility, serendipity, and super spreading events.  The Journal of infectious diseases , 207(6):990–998. [Cited on page  1 .]\n\n [15]  Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre- training of deep bidirectional transformers for language understanding. In  Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) , pages 4171– 4186, Minneapolis, Minnesota. Association for Computational Linguistics. [Cited on pages  1 and  9 .]\n\n [16]  Quang Do, Yee Seng Chan, and Dan Roth. 2011. Minimally supervised event causality identification. In  Proceedings of the 2011 Conference on Empirical Methods in Natural Lan- guage Processing , pages 294–303, Edinburgh, Scotland, UK. Association for Computational Linguistics. [Cited on page  27 .]\n\n [17]  Richard Doll and A Bradford Hill. 1950. Smoking and carcinoma of the lung.  British medical journal , 2(4682):739. [Cited on page  1 .]\n\n [18]  Richard Doll and A Bradford Hill. 1954. The mortality of doctors in relation to their smoking habits.  British medical journal , 1(4877):1451. [Cited on page  1 .]\n\n [19]  Jesse Dunietz, Lori Levin, and Jaime Carbonell. 2017. The BECauSE corpus 2.0: Annotating causality and overlapping relations. In  Proceedings of the 11th Linguistic Annotation Workshop , pages 95–104, Valencia, Spain. Association for Computational Linguistics. [Cited on page  27 .]\n\n [20]  Jordan Ellenberg. 2021. Coronavirus vaccines work. But this statistical illusion makes people think they don’t.  The Washington Post . [Cited on page  10 .] "}
{"page": 12, "image_path": "doc_images/2312.04350v3_12.jpg", "ocr_text": "[21\n\n[22\n\n[23\n\n[24\n\n[25\n\n[26\n\n[27\n\n[28\n\n[29\n\n[30\n\n[31\n\n[32]\n\n[33]\n\n[34]\n\n[35]\n\nJérg Frohberg and Frank Binder. 2022. CRASS: A novel data set and benchmark to test\ncounterfactual reasoning of large language models. In Proceedings of the Thirteenth Lan-\nguage Resources and Evaluation Conference, pages 2126-2140, Marseille, France. European\nLanguage Resources Association. [Cited on page 27.]\n\nThomas A Glass, Steven N Goodman, Miguel A Hernan, and Jonathan M Samet. 2013. Causal\ninference in public health. Annual review of public health, 34:61-75. [Cited on page 10.]\n\nMadelyn Glymour, Judea Pearl, and Nicholas P Jewell. 2016. Causal inference in statistics: A\nprimer. John Wiley and Sons. [Cited on pages 2 and 19.]\n\nMoisés Goldszmidt and Judea Pearl. 1992. Rank-based systems: A simple approach to belief\nrevision, belief update, and reasoning about evidence and actions. KR, 92:661-672. [Cited on\npages 2 and 3.]\n\nOlga Golovneva, Moya Chen, Spencer Poff, Martin Corredor, Luke Zettlemoyer, Maryam\nFazel-Zarandi, and Asli Celikyilmaz. 2022. Roscoe: A suite of metrics for scoring step-by-step\nreasoning. [Cited on pages 9 and 25.]\n\nAndrew Gordon, Zornitsa Kozareva, and Melissa Roemmele. 2012. SemEval-2012 task\n7: Choice of plausible alternatives: An evaluation of commonsense causal reasoning. In\n*SEM 2012: The First Joint Conference on Lexical and Computational Semantics — Volume\n1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of\nthe Sixth International Workshop on Semantic Evaluation (SemEval 2012), pages 394-398,\nMontréal, Canada. Association for Computational Linguistics. [Cited on page 27.]\n\nJoseph Y. Halpern and Judea Pearl. 2005. Causes and explanations: A structural-model\napproach. part i: Causes. The British Journal for the Philosophy of Science, 56(4):843-887.\n[Cited on page 19.]\n\nJoseph Y Halpern and Judea Pearl. 2005. Causes and explanations: A structural-model\napproach. part ii: Explanations. The British journal for the philosophy of science. [Cited on\npage 19.]\n\nYuval Noah Harari. 2014. Sapiens: A brief history of humankind. Random House. [Cited on\npage 1.]\n\nJames J Heckman, Lance J Lochner, and Petra E Todd. 2006. Earnings functions, rates of\nreturn and treatment effects: The mincer equation and beyond. Handbook of the Economics of\nEducation, 1:307-458. [Cited on page 1.]\n\nTris Hendrickx, Su Nam Kim, Zornitsa Kozareva, Preslav Nakov, Diarmuid 0 Séaghdha,\nSebastian Padé, Marco Pennacchiotti, Lorenza Romano, and Stan Szpakowicz. 2010. SemEval-\n2010 task 8: Multi-way classification of semantic relations between pairs of nominals. In\nProceedings of the 5th International Workshop on Semantic Evaluation, pages 33-38, Uppsala,\nSweden. Association for Computational Linguistics. [Cited on page 27.]\n\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn\nSong, and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the MATH\ndataset. In Proceedings of the Neural Information Processing Systems Track on Datasets and\nBenchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual. [Cited on\npages 4 and 7.]\n\nChristopher Hidey and Kathy McKeown. 2016. Identifying causal relations using parallel\nWikipedia articles. In Proceedings of the 54th Annual Meeting of the Association for Computa-\ntional Linguistics (Volume 1: Long Papers), pages 1424-1433, Berlin, Germany. Association\nfor Computational Linguistics. [Cited on page 10.]\n\nMatthew Ho, Aditya Sharma, Justin Chang, Michael Saxon, Sharon Levy, Yujie Lu, and\nWilliam Yang Wang. 2022. Wikiwhy: Answering and explaining cause-and-effect questions.\narXiv preprint arXiv:2210.12152. [Cited on page 2.]\n\nMark Hopkins and Judea Pearl. 2007. Causality and counterfactuals in the situation calculus.\nJournal of Logic and Computation, 17(5):939-953. [Cited on page 19.]\n\n13\n", "vlm_text": "[21]  Jörg Frohberg and Frank Binder. 2022. CRASS: A novel data set and benchmark to test counter factual reasoning of large language models. In  Proceedings of the Thirteenth Lan- guage Resources and Evaluation Conference , pages 2126–2140, Marseille, France. European Language Resources Association. [Cited on page  27 .]\n\n [22]  Thomas A Glass, Steven N Goodman, Miguel A Hernán, and Jonathan M Samet. 2013. Causal inference in public health.  Annual review of public health , 34:61–75. [Cited on page  10 .]\n\n [23]  Madelyn Glymour, Judea Pearl, and Nicholas P Jewell. 2016.  Causal inference in statistics: A primer . John Wiley and Sons. [Cited on pages  2  and  19 .]\n\n [24]  Moisés Goldszmidt and Judea Pearl. 1992. Rank-based systems: A simple approach to belief revision, belief update, and reasoning about evidence and actions.  KR , 92:661–672. [Cited on pages  2  and  3 .]\n\n [25]  Olga Golovneva, Moya Chen, Spencer Poff, Martin Corredor, Luke Z ett le moyer, Maryam Fazel-Zarandi, and Asli Cel i kyi l maz. 2022. Roscoe: A suite of metrics for scoring step-by-step reasoning. [Cited on pages  9  and  25 .]\n\n [26]  Andrew Gordon, Zornitsa Kozareva, and Melissa Roemmele. 2012. SemEval-2012 task 7: Choice of plausible alternatives: An evaluation of commonsense causal reasoning. In \\*SEM 2012: The First Joint Conference on Lexical and Computational Semantics – Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval 2012) , pages 394–398, Montréal, Canada. Association for Computational Linguistics. [Cited on page  27 .]\n\n [27]  Joseph Y. Halpern and Judea Pearl. 2005. Causes and explanations: A structural-model approach. part i: Causes.  The British Journal for the Philosophy of Science , 56(4):843–887. [Cited on page  19 .]\n\n [28]  Joseph Y Halpern and Judea Pearl. 2005. Causes and explanations: A structural-model approach. part ii: Explanations.  The British journal for the philosophy of science . [Cited on page  19 .]\n\n [29]  Yuval Noah Harari. 2014.  Sapiens: A brief history of humankind . Random House. [Cited on page  1 .]\n\n [30]  James J Heckman, Lance J Lochner, and Petra E Todd. 2006. Earnings functions, rates of return and treatment effects: The mincer equation and beyond.  Handbook of the Economics of Education , 1:307–458. [Cited on page  1 .]\n\n [31]  Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva, Preslav Nakov, Diarmuid Ó Séaghdha, Sebastian Padó, Marco Penna c chi ott i, Lorenza Romano, and Stan Szpakowicz. 2010. SemEval- 2010 task 8: Multi-way classification of semantic relations between pairs of nominals. In Proceedings of the 5th International Workshop on Semantic Evaluation , pages 33–38, Uppsala, Sweden. Association for Computational Linguistics. [Cited on page  27 .]\n\n [32]  Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the MATH dataset. In  Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual . [Cited on pages  4  and  7 .]\n\n [33]  Christopher Hidey and Kathy McKeown. 2016. Identifying causal relations using parallel Wikipedia articles. In  Proceedings of the 54th Annual Meeting of the Association for Computa- tional Linguistics (Volume 1: Long Papers) , pages 1424–1433, Berlin, Germany. Association for Computational Linguistics. [Cited on page  10 .]\n\n [34]  Matthew Ho, Aditya Sharma, Justin Chang, Michael Saxon, Sharon Levy, Yujie Lu, and William Yang Wang. 2022. Wikiwhy: Answering and explaining cause-and-effect questions. arXiv preprint arXiv:2210.12152 . [Cited on page  2 .]\n\n [35]  Mark Hopkins and Judea Pearl. 2007. Causality and counter factual s in the situation calculus. Journal of Logic and Computation , 17(5):939–953. [Cited on page  19 .] "}
{"page": 13, "image_path": "doc_images/2312.04350v3_13.jpg", "ocr_text": "[36]\n\n[37]\n\n[38]\n[39]\n\n[40]\n\n[41]\n\n[42\n\n[43\n\n[44\n\n[45\n\n[46\n\n[47\n\n[48\n\n[49\n\n[50\n\n[51\n\nYimin Huang and Marco Valtorta. 2006. Pearl’s calculus of intervention is complete. In\nProceedings of the Twenty-Second Conference on Uncertainty in Artificial Intelligence, pages\n217-224. [Cited on pages 3 and 4.]\n\nPaul Hiinermund and Elias Bareinboim. 2019. Causal inference and data fusion in econometrics.\nCoRR, abs/1912.09104. [Cited on pages 4, 5, and 10.]\n\nFerenc Huszar. 2023. We may be surprised again: Why i take Ilms seriously. [Cited on page 2.]\n\nOana Ignat, Zhijing Jin, Artem Abzaliev, Laura Biester, Santiago Castro, Naihao Deng, Xinyi\nGao, Aylin Gunal, Jacky He, Ashkan Kazemi, Muhammad Khalifa, Namho Koh, Andrew Lee,\nSiyang Liu, Do June Min, Shinka Mori, Joan Nwatu, Verénica Pérez-Rosas, Siqi Shen, Zekun\nWang, Winston Wu, and Rada Mihalcea. 2023. A PhD student’s perspective on research in\nNLP in the era of very large language models. CoRR, abs/2305.12544. [Cited on pages |\nand 9.]\n\nZhengbao Jiang, Frank F. Xu, Jun Araki, and Graham Neubig. 2020. How can we know\nwhat language models know? Transactions of the Association for Computational Linguistics,\n8:423-438. [Cited on page 2.]\n\nZhijing Jin, Abhinav Lalwani, Tejas Vaidhya, Xiaoyu Shen, Yiwen Ding, Zhiheng Lyu,\nMrinmaya Sachan, Rada Mihalcea, and Bernhard Schélkopf. 2022. Logical fallacy detection.\nIn Findings of the Association for Computational Linguistics: EMNLP 2022, pages 7180-7198,\nAbu Dhabi, United Arab Emirates. Association for Computational Linguistics. [Cited on\npage 9.]\n\nZhijing Jin, Jiarui Liu, Zhiheng Lyu, Spencer Poff, Mrinmaya Sachan, Rada Mihalcea, Mona T.\nDiab, and Bernhard Schélkopf. 2023. Can large language models infer causation from\ncorrelation? CoRR, abs/2306.05836. [Cited on pages 8, 10, and 27.]\n\nDaniel Martin Katz, Michael James Bommarito, Shang Gao, and Pablo Arredondo. 2023.\nGpt-4 passes the bar exam. Available at SSRN 4389233. [Cited on pages 8 and 9.]\n\nArmin Kekié, Jonas Dehning, Luigi Gresele, Julius von Kiigelgen, Viola Priesemann, and\nBernhard Schélkopf. 2023. Evaluating vaccine allocation strategies using simulation-assisted\ncausal modeling. Patterns. [Cited on page 1.]\n\nEmre Kiciman, Robert Ness, Amit Sharma, and Chenhao Tan. 2023. Causal reasoning and\nlarge language models: Opening a new frontier for causality. arXiv preprint arXiv:2305.00050.\n[Cited on pages 9 and 10.]\n\nYash Kumar Lal, Nathanael Chambers, Raymond Mooney, and Niranjan Balasubramanian.\n2021. TellMeWhy: A dataset for answering why-questions in narratives. In Findings of\nthe Association for Computational Linguistics: ACL-IJCNLP 2021, pages 596-610, Online.\nAssociation for Computational Linguistics. [Cited on page 27.]\n\nJoshua R Loftus, Chris Russell, Matt J Kusner, and Ricardo Silva. 2018. Causal reasoning for\nalgorithmic fairness. arXiv preprint arXiv:1805.05859. [Cited on page 10.]\n\nParamita Mirza, Rachele Sprugnoli, Sara Tonelli, and Manuela Speranza. 2014. Annotating\ncausality in the TempEval-3 corpus. In Proceedings of the EACL 2014 Workshop on Computa-\ntional Approaches to Causality in Language (CAtoCL), pages 10-19, Gothenburg, Sweden.\nAssociation for Computational Linguistics. [Cited on page 27.]\n\nJeffrey Morris. 2021. Israeli data: How can efficacy vs. severe disease be strong when 60% of\nhospitalized are vaccinated? Accessed: 27th of October 2023. [Cited on page 10.]\n\nNasrin Mostafazadeh, Alyson Grealish, Nathanael Chambers, James Allen, and Lucy Van-\nderwende. 2016. CaTeRS: Causal and temporal relation scheme for semantic annotation of\nevent structures. In Proceedings of the Fourth Workshop on Events, pages 51-61, San Diego,\nCalifornia. Association for Computational Linguistics. [Cited on page 27.]\n\nDaniel Naber et al. 2003. A rule-based style and grammar checker. [Cited on page 7.]\n\n14\n", "vlm_text": "[36]  Yimin Huang and Marco Valtorta. 2006. Pearl’s calculus of intervention is complete. In Proceedings of the Twenty-Second Conference on Uncertainty in Artificial Intelligence , pages 217–224. [Cited on pages  3  and  4 .]\n\n [37]  Paul Hünermund and Elias Bareinboim. 2019. Causal inference and data fusion in econometrics. CoRR , abs/1912.09104. [Cited on pages  4 ,  5 , and  10 .]\n\n [38]  Ferenc Huszár. 2023. We may be surprised again: Why i take llms seriously. [Cited on page  2 .]\n\n [39]  Oana Ignat, Zhijing Jin, Artem Abzaliev, Laura Biester, Santiago Castro, Naihao Deng, Xinyi Gao, Aylin Gunal, Jacky He, Ashkan Kazemi, Muhammad Khalifa, Namho Koh, Andrew Lee, Siyang Liu, Do June Min, Shinka Mori, Joan Nwatu, Verónica Pérez-Rosas, Siqi Shen, Zekun Wang, Winston Wu, and Rada Mihalcea. 2023. A PhD student’s perspective on research in NLP in the era of very large language models.  CoRR , abs/2305.12544. [Cited on pages  1 and  9 .]\n\n [40]  Zhengbao Jiang, Frank F. Xu, Jun Araki, and Graham Neubig. 2020. How can we know what language models know?  Transactions of the Association for Computational Linguistics , 8:423–438. [Cited on page  2 .]\n\n [41]  Zhijing Jin, Abhinav Lalwani, Tejas Vaidhya, Xiaoyu Shen, Yiwen Ding, Zhiheng Lyu, Mrinmaya Sachan, Rada Mihalcea, and Bernhard Schölkopf. 2022. Logical fallacy detection. In  Findings of the Association for Computational Linguistics: EMNLP 2022 , pages 7180–7198, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. [Cited on page  9 .]\n\n [42]  Zhijing Jin, Jiarui Liu, Zhiheng Lyu, Spencer Poff, Mrinmaya Sachan, Rada Mihalcea, Mona T. Diab, and Bernhard Schölkopf. 2023. Can large language models infer causation from correlation?  CoRR , abs/2306.05836. [Cited on pages  8 ,  10 , and  27 .]\n\n [43]  Daniel Martin Katz, Michael James Bommarito, Shang Gao, and Pablo Arredondo. 2023. Gpt-4 passes the bar exam.  Available at SSRN 4389233 . [Cited on pages  8  and  9 .]\n\n [44]  Armin Keki´ c, Jonas Dehning, Luigi Gresele, Julius von Kügelgen, Viola Priesemann, and Bernhard Schölkopf. 2023. Evaluating vaccine allocation strategies using simulation-assisted causal modeling.  Patterns . [Cited on page  1 .]\n\n [45]  Emre Kıcıman, Robert Ness, Amit Sharma, and Chenhao Tan. 2023. Causal reasoning and large language models: Opening a new frontier for causality.  arXiv preprint arXiv:2305.00050 . [Cited on pages  9  and  10 .]\n\n [46]  Yash Kumar Lal, Nathanael Chambers, Raymond Mooney, and Niranjan Bala subramania n. 2021. TellMeWhy: A dataset for answering why-questions in narratives. In  Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021 , pages 596–610, Online. Association for Computational Linguistics. [Cited on page  27 .]\n\n [47]  Joshua R Loftus, Chris Russell, Matt J Kusner, and Ricardo Silva. 2018. Causal reasoning for algorithmic fairness.  arXiv preprint arXiv:1805.05859 . [Cited on page  10 .]\n\n [48]  Paramita Mirza, Rachele Sprugnoli, Sara Tonelli, and Manuela Speranza. 2014. Annotating causality in the TempEval-3 corpus. In  Proceedings of the EACL 2014 Workshop on Computa- tional Approaches to Causality in Language (CAtoCL) , pages 10–19, Gothenburg, Sweden. Association for Computational Linguistics. [Cited on page  27 .]\n\n [49]  Jeffrey Morris. 2021. Israeli data: How can efficacy vs. severe disease be strong when  $60\\%$   of hospitalized are vaccinated? Accessed: 27th of October 2023. [Cited on page  10 .]\n\n [50]  Nasrin Most af azad eh, Alyson Grealish, Nathanael Chambers, James Allen, and Lucy Van- derwende. 2016. CaTeRS: Causal and temporal relation scheme for semantic annotation of event structures. In  Proceedings of the Fourth Workshop on Events , pages 51–61, San Diego, California. Association for Computational Linguistics. [Cited on page  27 .]\n\n [51] Daniel Naber et al. 2003. A rule-based style and grammar checker. [Cited on page  7 .] "}
{"page": 14, "image_path": "doc_images/2312.04350v3_14.jpg", "ocr_text": "[52] Razieh Nabi and Ilya Shpitser. 2018. Fair inference on outcomes. In Proceedings of the\n\n[53\n[54\n\n[55\n\n[56\n\n[57\n\n[58\n\n[59\n\n[60\n\n[61\n\n[62\n\n[63\n\n[64\n\n[65\n\n[66\n\n[67\n\n[68\n\n[69\n\nThirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative\nApplications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational\nAdvances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7,\n2018, pages 1931-1940. AAAI Press. [Cited on page 10.]\n\nBrady Neal. 2020. Introduction to causal inference. [Cited on page 19.]\n\nHarsha Nori, Nicholas King, Scott Mayer McKinney, Dean Carignan, and Eric Horvitz. 2023.\nCapabilities of GPT-4 on medical challenge problems. CoRR, abs/2303.13375. [Cited on\npages 8 and 9.]\n\nMaxwell I. Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin,\nDavid Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton,\nand Augustus Odena. 2021. Show your work: Scratchpads for intermediate computation with\nlanguage models. CoRR, abs/2112.00114. [Cited on page 7.]\n\nOpenAI. 2023. GPT-4 technical report. CoRR, abs/2303.08774. [Cited on pages 1, 8, and 9.]\n\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin,\nChong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton,\nFraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano,\nJan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human\nfeedback. CoRR, abs/2203.02155. [Cited on page 8.]\n\nJudea Pearl. 1988. Probabilistic reasoning in intelligent systems: Networks of plausible\ninference. Morgan Kaufmann. [Cited on page 3.]\n\nJudea Pearl. 1995. Causal diagrams for empirical research. Biometrika, 82(4):669-688. [Cited\non pages 2, 3, 4, 5, and 24.]\n\nJudea Pearl. 2009. Causal inference in statistics: An overview. Statistics Surveys, 3(none):96 —\n146. [Cited on page 19.]\n\nJudea Pearl. 2009. Causality: Models, reasoning and inference (2nd ed.). Cambridge\nUniversity Press. [Cited on pages 2 and 23.]\n\nJudea Pearl. 2011. The algorithmization of counterfactuals. Annals of Mathematics and\nArtificial Intelligence, 61:29-39. [Cited on page 3.]\n\nJudea Pearl. 2022. Comment: understanding simpson’s paradox. In Probabilistic and causal\ninference: The works of judea Pearl, pages 399-412. [Cited on page 2.]\n\nJudea Pearl and Elias Bareinboim. 2022. External validity: From do-calculus to transportability\nacross populations. In Probabilistic and causal inference: The works of Judea Pearl, pages\n451-482. [Cited on page 3.]\n\nJudea Pearl, Madelyn Glymour, and Nicholas P Jewell. 2016. Causal inference in statistics: A\nprimer. John Wiley & Sons. [Cited on page 3.]\n\nJudea Pearl and Dana Mackenzie. 2018. The book of why: The new science of cause and effect.\nBasic books. [Cited on pages 1, 2, 3, 4, 5, 7, 10, and 19.]\n\nJudea Pearl et al. 2000. Causality: Models, reasoning and inference. Cambridge University\nPress. [Cited on pages 3, 5, 19, and 24.]\n\nDerek C Penn and Daniel J Povinelli. 2007. Causal cognition in human and nonhuman animals:\nA comparative, critical review. Annu. Rev. Psychol., 58:97—-118. [Cited on page 1|.]\n\nJonas Peters, Dominik Janzing, and Bernhard Schélkopf. 2017. Elements of causal inference:\nFoundations and learning algorithms. The MIT Press. [Cited on pages 2, 5, and 19.]\n\n15\n", "vlm_text": "[52]  Razieh Nabi and Ilya Shpitser. 2018. Fair inference on outcomes. In  Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018 , pages 1931–1940. AAAI Press. [Cited on page  10 .]\n\n \n[53] Brady Neal. 2020. Introduction to causal inference. [Cited on page  19 .]\n\n \n[54]  Harsha Nori, Nicholas King, Scott Mayer McKinney, Dean Carignan, and Eric Horvitz. 2023. Capabilities of GPT-4 on medical challenge problems.  CoRR , abs/2303.13375. [Cited on pages  8  and  9 .]\n\n \n[55]  Maxwell I. Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Micha lewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, \nlanguage models.  CoRR , abs/2112.00114. [Cited on page  7 .]\n\n [56]  OpenAI. 2023. GPT-4 technical report.  CoRR , abs/2303.08774. [Cited on pages  1 ,  8 , and  9 .]\n\n [57]  Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback.  CoRR , abs/2203.02155. [Cited on page  8 .]\n\n [58]  Judea Pearl. 1988.  Probabilistic reasoning in intelligent systems: Networks of plausible inference . Morgan Kaufmann. [Cited on page  3 .]\n\n [59]  Judea Pearl. 1995. Causal diagrams for empirical research.  Biometrika , 82(4):669–688. [Cited on pages  2 ,  3 ,  4 ,  5 , and  24 .]\n\n [60]  Judea Pearl. 2009. Causal inference in statistics: An overview.  Statistics Surveys , 3(none):96 – 146. [Cited on page  19 .]\n\n [61]  Judea Pearl. 2009. Causality: Models, reasoning and inference (2nd ed.) . Cambridge University Press. [Cited on pages  2  and  23 .]\n\n [62]  Judea Pearl. 2011. The algorithm iz ation of counter factual s.  Annals of Mathematics and Artificial Intelligence , 61:29–39. [Cited on page  3 .]\n\n [63] Judea Pearl. 2022. Comment: understanding simpson’s paradox. In  Probabilistic and causal inference: The works of judea Pearl , pages 399–412. [Cited on page  2 .]\n\n [64]  Judea Pearl and Elias Bareinboim. 2022. External validity: From do-calculus to transport ability across populations. In  Probabilistic and causal inference: The works of Judea Pearl , pages 451–482. [Cited on page  3 .]\n\n [65]  Judea Pearl, Madelyn Glymour, and Nicholas P Jewell. 2016.  Causal inference in statistics: A primer . John Wiley & Sons. [Cited on page  3 .]\n\n [66]  Judea Pearl and Dana Mackenzie. 2018.  The book of why: The new science of cause and effect . Basic books. [Cited on pages  1 ,  2 ,  3 ,  4 ,  5 ,  7 ,  10 , and  19 .]\n\n [67]  Judea Pearl et al. 2000.  Causality: Models, reasoning and inference . Cambridge University Press. [Cited on pages  3 ,  5 ,  19 , and  24 .]\n\n [68]  Derek C Penn and Daniel J Povinelli. 2007. Causal cognition in human and nonhuman animals: A comparative, critical review.  Annu. Rev. Psychol. , 58:97–118. [Cited on page  1 .]\n\n [69]  Jonas Peters, Dominik Janzing, and Bernhard Schölkopf. 2017.  Elements of causal inference: Foundations and learning algorithms . The MIT Press. [Cited on pages  2 ,  5 , and  19 .] "}
{"page": 15, "image_path": "doc_images/2312.04350v3_15.jpg", "ocr_text": "[70]\n\n[71\n\n[72\n\n[73\n\n[74\n\n[75\n\n[76]\n\n(77]\n\n[78\n\n[79\n\n[80\n\n[81\n\n[82]\n\n[83]\n\nFabio Petroni, Tim Rocktischel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang\nWu, and Alexander Miller. 2019. Language models as knowledge bases? In Proceedings\nof the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th\nInternational Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages\n2463-2473, Hong Kong, China. Association for Computational Linguistics. [Cited on page 2.]\n\nDrago Plecko and Elias Bareinboim. 2022. Causal fairness analysis. arXiv preprint\narXiv:2207.11385. [Cited on page 10.]\n\nStanley A Plotkin. 2005. Vaccines: Past, present and future. Nature medicine, 11(Suppl\n4):S5-S11. [Cited on page 1.]\n\nGeorge Psacharopoulos and Harry Anthony Patrinos. 2004. Returns to investment in education:\nA further update. Education economics, 12(2):111—134. [Cited on page 1|.]\n\nChengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro Yasunaga, and Diyi\nYang. 2023. Is chatgpt a general-purpose natural language processing task solver? CoRR,\nabs/2302.06476. [Cited on page 9.]\n\nLianhui Qin, Antoine Bosselut, Ari Holtzman, Chandra Bhagavatula, Elizabeth Clark, and\nYejin Choi. 2019. Counterfactual story reasoning and generation. In Proceedings of the 2019\nConference on Empirical Methods in Natural Language Processing and the 9th International\nJoint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 5043-5053,\nHong Kong, China. Association for Computational Linguistics. [Cited on pages 9 and 27.]\n\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019.\nLanguage models are unsupervised multitask learners. OpenAI Blog, 1(8). [Cited on pages 1,\n8, and 9.]\n\nHannah Rashkin, Maarten Sap, Emily Allaway, Noah A. Smith, and Yejin Choi. 2018.\nEvent2Mind: Commonsense inference on events, intents, and reactions. In Proceedings\nof the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long\nPapers), pages 463-473, Melbourne, Australia. Association for Computational Linguistics.\n[Cited on page 27.]\n\nYasaman Razeghi, Robert L Logan IV, Matt Gardner, and Sameer Singh. 2022. Impact of\npretraining term frequencies on few-shot numerical reasoning. In Findings of the Association\nfor Computational Linguistics: EMNLP 2022, pages 840-854, Abu Dhabi, United Arab\nEmirates. Association for Computational Linguistics. [Cited on page 8.]\n\nKenneth J Rothman and Sander Greenland. 2005. Causation and causal inference in epidemi-\nology. American journal of public health, 95(S1):S144-S 150. [Cited on page 10.]\n\nJiirgen Rudolph, Samson Tan, and Shannon Tan. 2023. Chatgpt: Bullshit spewer or the end of\ntraditional assessments in higher education? Journal of Applied Learning and Teaching, 6(1).\n[Cited on page 9.]\n\nMaarten Sap, Ronan Le Bras, Emily Allaway, Chandra Bhagavatula, Nicholas Lourie, Hannah\nRashkin, Brendan Roof, Noah A. Smith, and Yejin Choi. 2019. ATOMIC: an atlas of machine\ncommonsense for if-then reasoning. In The Thirty-Third AAAI Conference on Artificial\nIntelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence\nConference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial\nIntelligence, EAAI 2019, Honolulu, Hawaii, USA, January 27 - February 1, 2019, pages\n3027-3035. AAAI Press. [Cited on pages 9 and 27.]\n\nMaarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, and Yejin Choi. 2019. Social\niqa: Commonsense reasoning about social interactions. In EMNLP 2019. [Cited on pages 9\nand 27.]\n\nTaylor Shin, Yasaman Razeghi, Robert L. Logan IV, Eric Wallace, and Sameer Singh. 2020.\nAutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated\nPrompts. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 4222-4235, Online. Association for Computational Linguistics.\n[Cited on page 2.]\n\n16\n", "vlm_text": "[70]  Fabio Petroni, Tim Rock t s chel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. 2019. Language models as knowledge bases? In  Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) , pages 2463–2473, Hong Kong, China. Association for Computational Linguistics. [Cited on page  2 .]\n\n [71]  Drago Plecko and Elias Bareinboim. 2022. Causal fairness analysis. arXiv preprint arXiv:2207.11385 . [Cited on page  10 .]\n\n [72]  Stanley A Plotkin. 2005. Vaccines: Past, present and future.  Nature medicine , 11(Suppl 4):S5–S11. [Cited on page  1 .]\n\n [73]  George P sachar o poul os and Harry Anthony Patrinos. 2004. Returns to investment in education: A further update.  Education economics , 12(2):111–134. [Cited on page  1 .]\n\n [74]  Chengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro Yasunaga, and Diyi Yang. 2023. Is chatgpt a general-purpose natural language processing task solver?  CoRR , abs/2302.06476. [Cited on page  9 .]\n\n [75]  Lianhui Qin, Antoine Bosselut, Ari Holtzman, Chandra Bhaga va tula, Elizabeth Clark, and Yejin Choi. 2019. Counter factual story reasoning and generation. In  Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) , pages 5043–5053, Hong Kong, China. Association for Computational Linguistics. [Cited on pages  9  and  27 .]\n\n [76]  Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners.  OpenAI Blog , 1(8). [Cited on pages  1 , 8 , and  9 .]\n\n [77]  Hannah Rashkin, Maarten Sap, Emily Allaway, Noah A. Smith, and Yejin Choi. 2018. Event2Mind: Commonsense inference on events, intents, and reactions. In  Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 463–473, Melbourne, Australia. Association for Computational Linguistics. [Cited on page  27 .]\n\n [78]  Yasaman Razeghi, Robert L Logan IV, Matt Gardner, and Sameer Singh. 2022. Impact of pre training term frequencies on few-shot numerical reasoning. In  Findings of the Association for Computational Linguistics: EMNLP 2022 , pages 840–854, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. [Cited on page  8 .]\n\n [79]  Kenneth J Rothman and Sander Greenland. 2005. Causation and causal inference in epidemi- ology.  American journal of public health , 95(S1):S144–S150. [Cited on page  10 .]\n\n [80]  Jürgen Rudolph, Samson Tan, and Shannon Tan. 2023. Chatgpt: Bullshit spewer or the end of traditional assessments in higher education?  Journal of Applied Learning and Teaching , 6(1). [Cited on page  9 .]\n\n [81]  Maarten Sap, Ronan Le Bras, Emily Allaway, Chandra Bhaga va tula, Nicholas Lourie, Hannah Rashkin, Brendan Roof, Noah A. Smith, and Yejin Choi. 2019. ATOMIC: an atlas of machine commonsense for if-then reasoning. In  The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019, Honolulu, Hawaii, USA, January 27 - February 1, 2019 , pages 3027–3035. AAAI Press. [Cited on pages  9  and  27 .]\n\n [82]  Maarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, and Yejin Choi. 2019. Social iqa: Commonsense reasoning about social interactions. In  EMNLP 2019 . [Cited on pages  9 and  27 .]\n\n [83]  Taylor Shin, Yasaman Razeghi, Robert L. Logan IV, Eric Wallace, and Sameer Singh. 2020. AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts. In  Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pages 4222–4235, Online. Association for Computational Linguistics. [Cited on page  2 .] "}
{"page": 16, "image_path": "doc_images/2312.04350v3_16.jpg", "ocr_text": "[84]\n\n[85]\n\n[86]\n\n[87]\n\n[88]\n\n[89]\n\n[90]\n\n[91]\n\n[92]\n\n[93]\n\n[94]\n\nIlya Shpitser and Judea Pearl. 2006. Identification of conditional interventional distributions.\nIn 22nd Conference on Uncertainty in Artificial Intelligence, UAI 2006, pages 437-444. [Cited\non pages 3 and 4.]\n\nIlya Shpitser and Judea Pearl. 2006. Identification of joint interventional distributions in recur-\nsive semi-markovian causal models. In Proceedings, The Twenty-First National Conference\non Artificial Intelligence and the Eighteenth Innovative Applications of Artificial Intelligence\nConference, July 16-20, 2006, Boston, Massachusetts, USA, pages 1219-1226. AAAI Press.\n[Cited on page 24.]\n\nShikhar Singh, Nuan Wen, Yu Hou, Pegah Alipoormolabashi, Te-lin Wu, Xuezhe Ma, and\nNanyun Peng. 2021. COM2SENSE: A commonsense reasoning benchmark with complemen-\ntary sentences. In Findings of the Association for Computational Linguistics: ACL-IJCNLP\n2021, pages 883-898, Online. Association for Computational Linguistics. [Cited on page 27.]\n\nKaran Singhal, Shekoofeh Azizi, Tao Tu, S. Sara Mahdavi, Jason Wei, Hyung Won Chung,\nNathan Scales, Ajay Kumar Tanwani, Heather Cole-Lewis, Stephen Pfohl, Perry Payne,\nMartin Seneviratne, Paul Gamble, Chris Kelly, Nathaneal Schirli, Aakanksha Chowdhery,\nPhilip Andrew Mansfield, Blaise Agiiera y Arcas, Dale R. Webster, Gregory S. Corrado,\nYossi Matias, Katherine Chou, Juraj Gottweis, Nenad Tomasev, Yun Liu, Alvin Rajkomar,\nJoelle K. Barral, Christopher Semturs, Alan Karthikesalingam, and Vivek Natarajan. 2022.\nLarge language models encode clinical knowledge. CoRR, abs/2212.13138. [Cited on page 9.]\n\nPeter Spirtes, Clark Glymour, and Richard Scheines. 2000. Causation, Prediction, and Search,\nSecond Edition. Adaptive computation and machine learning. MIT Press. [Cited on pages 2\nand 5.]\n\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid,\nAdam Fisch, Adam R. Brown, Adam Santoro, Aditya Gupta, Adria Garriga-Alonso, Ag-\nnieszka Kluska, Aitor Lewkowycz, Akshat Agarwal, Alethea Power, Alex Ray, Alex Warstadt,\nAlexander W. Kocurek, Ali Safaya, Ali Tazarv, Alice Xiang, Alicia Parrish, Allen Nie, Aman\nHussain, Amanda Askell, Amanda Dsouza, Ameet Rahane, Anantharaman S. Iyer, Anders\nAndreassen, Andrea Santilli, Andreas Stuhlmiiller, Andrew M. Dai, Andrew La, Andrew K.\nLampinen, Andy Zou, Angela Jiang, Angelica Chen, Anh Vuong, Animesh Gupta, Anna\nGottardi, Antonio Norelli, Anu Venkatesh, Arash Gholamidavoodi, Arfa Tabassum, Arul\nMenezes, Arun Kirubarajan, Asher Mullokandov, Ashish Sabharwal, Austin Herrick, Avia\nEfrat, Aykut Erdem, Ayla Karakas, and et al. 2022. Beyond the imitation game: Quantifying\nand extrapolating the capabilities of language models. CoRR, abs/2206.04615. [Cited on\npage 9.]\n\nManfred Stede. 2008. Connective-based local coherence analysis: A lexicon for recognizing\ncausal relationships. In Semantics in Text Processing. STEP 2008 Conference Proceedings,\npages 221-237. College Publications. [Cited on page 10.]\n\nAlessandro Stolfo, Zhijing Jin, Kumar Shridhar, Bernhard Schélkopf, and Mrinmaya Sachan.\n2023. A causal framework to quantify the robustness of mathematical reasoning with language\nmodels. In Proceedings of the 61st Annual Meeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), Toronto, Canada. Association for Computational Linguistics.\n[Cited on pages 4 and 8.]\n\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin,\nPercy Liang, and Tatsunori B. Hashimoto. 2023. Stanford alpaca: An instruction-following\nllama model. https: //github.com/tatsu-lab/stanford_alpaca. [Cited on page 8.]\n\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-\nthée Lacroix, Baptiste Roziére, Naman Goyal, Eric Hambro, Faisal Azhar, Aurélien Rodriguez,\nArmand Joulin, Edouard Grave, and Guillaume Lample. 2023. Llama: Open and efficient\nfoundation language models. CoRR, abs/2302.13971. [Cited on page 8.]\n\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill,\nOmer Levy, and Samuel R. Bowman. 2019. Superglue: A stickier benchmark for general-\npurpose language understanding systems. In Advances in Neural Information Processing\n\n17\n", "vlm_text": "[84]  Ilya Shpitser and Judea Pearl. 2006. Identification of conditional interventional distributions. In  22nd Conference on Uncertainty in Artificial Intelligence, UAI 2006 , pages 437–444. [Cited on pages  3  and  4 .]\n\n \n[85]  Ilya Shpitser and Judea Pearl. 2006. Identification of joint interventional distributions in recur- sive semi-markovian causal models. In  Proceedings, The Twenty-First National Conference on Artificial Intelligence and the Eighteenth Innovative Applications of Artificial Intelligence Conference, July 16-20, 2006, Boston, Massachusetts, USA , pages 1219–1226. AAAI Press. [Cited on page  24 .]\n\n \n[86]  Shikhar Singh, Nuan Wen, Yu Hou, Pegah Ali poor mola bash i, Te-lin Wu, Xuezhe Ma, and Nanyun Peng. 2021. COM2SENSE: A commonsense reasoning benchmark with complemen- tary sentences. In  Findings of the Association for Computational Linguistics: ACL-IJCNLP\n\n \n[87]  Karan Singhal, Shekoofeh Azizi, Tao Tu, S. Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Kumar Tanwani, Heather Cole-Lewis, Stephen Pfohl, Perry Payne, Martin Sen evi rat ne, Paul Gamble, Chris Kelly, Nathaneal Schärli, Aakanksha Chowdhery, Philip Andrew Mansfield, Blaise Agüera y Arcas, Dale R. Webster, Gregory S. Corrado, Yossi Matias, Katherine Chou, Juraj Gottweis, Nenad Tomasev, Yun Liu, Alvin Rajkomar, Joelle K. Barral, Christopher Semturs, Alan Kart hikes a lingam, and Vivek Natarajan. 2022. Large language models encode clinical knowledge.  CoRR , abs/2212.13138. [Cited on page  9 .]\n\n [88]  Peter Spirtes, Clark Glymour, and Richard Scheines. 2000.  Causation, Prediction, and Search, Second Edition . Adaptive computation and machine learning. MIT Press. [Cited on pages  2 and  5 .]\n\n [89]  Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R. Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, Ag- nieszka Kluska, Aitor Lewkowycz, Akshat Agarwal, Alethea Power, Alex Ray, Alex Warstadt, Alexander W. Kocurek, Ali Safaya, Ali Tazarv, Alice Xiang, Alicia Parrish, Allen Nie, Aman Hussain, Amanda Askell, Amanda Dsouza, Ameet Rahane, Anantha raman S. Iyer, Anders Andreassen, Andrea Santilli, Andreas Stu hl m ller, Andrew M. Dai, Andrew La, Andrew K. Lampinen, Andy Zou, Angela Jiang, Angelica Chen, Anh Vuong, Animesh Gupta, Anna Gottardi, Antonio Norelli, Anu Venkatesh, Arash Ghola mid a vo odi, Arfa Tabassum, Arul Menezes, Arun Ki rub a rajan, Asher Mull ok and ov, Ashish Sabharwal, Austin Herrick, Avia Efrat, Aykut Erdem, Ayla Karakas, and et al. 2022. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models.  CoRR , abs/2206.04615. [Cited on page  9 .]\n\n [90]  Manfred Stede. 2008. Connective-based local coherence analysis: A lexicon for recognizing causal relationships. In  Semantics in Text Processing. STEP 2008 Conference Proceedings , pages 221–237. College Publications. [Cited on page  10 .]\n\n [91]  Alessandro Stolfo, Zhijing Jin, Kumar Shridhar, Bernhard Schölkopf, and Mrinmaya Sachan. 2023. A causal framework to quantify the robustness of mathematical reasoning with language models. In  Proceedings of the 61st Annual Meeting of the Association for Computational Lin- guistics (Volume 1: Long Papers) , Toronto, Canada. Association for Computational Linguistics. [Cited on pages  4  and  8 .]\n\n [92]  Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford alpaca: An instruction-following llama model.  https://github.com/tatsu-lab/stanford alpaca . [Cited on page  8 .]\n\n [93]  Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo- thée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurélien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023. Llama: Open and efficient foundation language models.  CoRR , abs/2302.13971. [Cited on page  8 .]\n\n [94]  Alex Wang, Yada Pr uk s a chat kun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. 2019. Superglue: A stickier benchmark for general- Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada , pages 3261–3275. [Cited on page  9 .]\n\n "}
{"page": 17, "image_path": "doc_images/2312.04350v3_17.jpg", "ocr_text": "[95]\n\n[96\n\n[97\n\n[98\n\n[99\n\n[104]\n\n[105]\n\nSystems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS\n2019, December 8-14, 2019, Vancouver, BC, Canada, pages 3261-3275. [Cited on page 9.]\n\nYizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei,\nAtharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap,\nEshaan Pathak, Giannis Karamanolakis, Haizhi Gary Lai, Ishan Purohit, Ishani Mondal,\nJacob Anderson, Kirby Kuznia, Krima Doshi, Kuntal Kumar Pal, Maitreya Patel, Mehrad\nMoradshahi, Mihir Parmar, Mirali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit\nVerma, Ravsehaj Singh Puri, Rushang Karia, Savan Doshi, Shailaja Keyur Sampat, Siddhartha\nMishra, Sujan Reddy A, Sumanta Patro, Tanay Dixit, and Xudong Shen. 2022. Super-\nnaturalinstructions: Generalization via declarative instructions on 1600+ NLP tasks. In\nProceedings of the 2022 Conference on Empirical Methods in Natural Language Processing,\nEMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pages 5085-5109.\nAssociation for Computational Linguistics. [Cited on page 9.]\n\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi,\nQuoc V. Le, and Denny Zhou. 2022. Chain of thought prompting elicits reasoning in large\nlanguage models. In Advances in Neural Information Processing Systems. [Cited on pages 2\nand 7.]\n\nCynthia G Whitney, Fangjun Zhou, James Singleton, and Anne Schuchat. 2014. Benefits\nfrom immunization during the vaccines for children program era—united states, 1994-2013.\nMorbidity and Mortality Weekly Report, 63(16):352. [Cited on page 1.]\n\nJinghang Xu, Wanli Zuo, Shining Liang, and Xianglin Zuo. 2020. A review of dataset and\nlabeling methods for causality extraction. In Proceedings of the 28th International Conference\non Computational Linguistics, pages 1519-1531, Barcelona, Spain (Online). International\nCommittee on Computational Linguistics. [Cited on page 10.]\n\nBei Yu, Yingya Li, and Jun Wang. 2019. Detecting causal language use in science find-\nings. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 4664-4674, Hong Kong, China. Association for Computational\nLinguistics. [Cited on page 10.]\n\nMatej Zeéevi¢, Moritz Willig, Devendra Singh Dhami, and Kristian Kersting. 2023. Causal\nparrots: Large language models may talk causality but are not causal. Transactions on Machine\nLearning Research. [Cited on pages 2 and 9.]\n\nCheng Zhang, Stefan Bauer, Paul Bennett, Jiangfeng Gao, Wenbo Gong, Agrin Hilmkil, Joel\nJennings, Chao Ma, Tom Minka, Nick Pawlowski, et al. 2023. Understanding causality with\nlarge language models: Feasibility and opportunities. arXiv preprint arXiv:2304.05524. [Cited\non pages 2 and 9.]\n\nLi Zhang, Qing Lyu, and Chris Callison-Burch. 2020. Reasoning about goals, steps, and\ntemporal ordering with WikiHow. In Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP), pages 4630-4639, Online. Association\nfor Computational Linguistics. [Cited on pages 9 and 27.]\n\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen,\nChristopher Dewan, Mona T. Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott,\nSam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang,\nand Luke Zettlemoyer. 2022. OPT: open pre-trained transformer language models. CoRR,\nabs/2205.01068. [Cited on pages | and 9.]\n\nDaniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford, Dario Amodei,\nPaul F. Christiano, and Geoffrey Irving. 2019. Fine-tuning language models from human\npreferences. CoRR, abs/1909.08593. [Cited on page 8.]\n\nCaleb Ziems, William Held, Omar Shaikh, Jiaao Chen, Zhehao Zhang, and Diyi Yang. 2023.\nCan large language models transform computational social science? [Cited on pages 8 and 9.]\n\n18\n", "vlm_text": "\n[95]  Yizhong Wang, Swaroop Mishra, Pegah Ali poor mola bash i, Yeganeh Kordi, Amirreza Mirzaei, Atharva Naik, Arjun Ashok, Arut Selvan Dhana sekar an, Anjana Arunkumar, David Stap, Eshaan Pathak, Giannis Karam a no lak is, Haizhi Gary Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson, Kirby Kuznia, Krima Doshi, Kuntal Kumar Pal, Maitreya Patel, Mehrad Moradshahi, Mihir Parmar, Mirali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh Puri, Rushang Karia, Savan Doshi, Shailaja Keyur Sampat, Siddhartha Mishra, Sujan Reddy A, Sumanta Patro, Tanay Dixit, and Xudong Shen. 2022. Super- natural instructions: Generalization via declarative instructions on   $1600+$   NLP tasks. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022 , pages 5085–5109. Association for Computational Linguistics. [Cited on page  9 .]\n\n [96]  Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. 2022. Chain of thought prompting elicits reasoning in large language models. In  Advances in Neural Information Processing Systems . [Cited on pages  2 and  7 .]\n\n [97]  Cynthia G Whitney, Fangjun Zhou, James Singleton, and Anne Schuchat. 2014. Benefits from immunization during the vaccines for children program era—united states, 1994–2013. Morbidity and Mortality Weekly Report , 63(16):352. [Cited on page  1 .]\n\n [98]  Jinghang Xu, Wanli Zuo, Shining Liang, and Xianglin Zuo. 2020. A review of dataset and labeling methods for causality extraction. In  Proceedings of the 28th International Conference on Computational Linguistics , pages 1519–1531, Barcelona, Spain (Online). International Committee on Computational Linguistics. [Cited on page  10 .]\n\n [99]  Bei Yu, Yingya Li, and Jun Wang. 2019. Detecting causal language use in science find- ings. In  Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) , pages 4664–4674, Hong Kong, China. Association for Computational Linguistics. [Cited on page  10 .]\n\n [100]  Matej Zeˇ cevi´ c, Moritz Willig, Devendra Singh Dhami, and Kristian Kersting. 2023. Causal parrots: Large language models may talk causality but are not causal.  Transactions on Machine Learning Research . [Cited on pages  2  and  9 .]\n\n [101] Cheng Zhang, Stefan Bauer, Paul Bennett, Jiangfeng Gao, Wenbo Gong, Agrin Hilmkil, Joel Jennings, Chao Ma, Tom Minka, Nick Pawlowski, et al. 2023. Understanding causality with large language models: Feasibility and opportunities.  arXiv preprint arXiv:2304.05524 . [Cited on pages  2  and  9 .]\n\n [102]  Li Zhang, Qing Lyu, and Chris Callison-Burch. 2020. Reasoning about goals, steps, and temporal ordering with WikiHow. In  Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pages 4630–4639, Online. Association for Computational Linguistics. [Cited on pages  9  and  27 .]\n\n [103]  Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona T. Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Z ett le moyer. 2022. OPT: open pre-trained transformer language models.  CoRR , abs/2205.01068. [Cited on pages  1  and  9 .]\n\n [104]  Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul F. Christiano, and Geoffrey Irving. 2019. Fine-tuning language models from human preferences.  CoRR , abs/1909.08593. [Cited on page  8 .]\n\n [105]  Caleb Ziems, William Held, Omar Shaikh, Jiaao Chen, Zhehao Zhang, and Diyi Yang. 2023. Can large language models transform computational social science? [Cited on pages  8  and  9 .] "}
{"page": 18, "image_path": "doc_images/2312.04350v3_18.jpg", "ocr_text": "A. Supplementary for Dataset Generation\n\nA.1 List of References for Causal Inference\n\nWhen collecting the causal graphs, query types, and commonsensical stories for our dataset, we took\nour examples from the following books (sorted by year):\n\n1. Causality [67]\n\n2. Causal inference in statistics: A Primer [23]\n3. Elements of Causal Inference [69]\n\n4. The Book of Why [66]\n\n5. Introduction to Causal Inference [53]\n\nAnd the following papers:\n\n1. Causes and Explanations: A Structural-Model Approach. Part I: Causes [27]\n\n2. Causes and Explanations: A Structural-Model Approach. Part I: Explanations [28]\n3. Causality and Counterfactuals in the Situation Calculus [35]\n\n4. Causal inference in statistics: An overview [60\n\nCausal Graph - Treatment-\nAlias CGTE Alias Effect Pair\n\n—\nConfounding Confounding oxy\nMediation SO Sy\n\nimmorality Fork oo\nCollision S30\n©\n\nDiamond Diamond aoe\nDiamondCut osG4e\n\nChain Chain\n% 70>\nIV Vv Y\nZ\nArrowhead ‘Arrowhead Se\n\nFrontdoor Frontdoor Cxry\n\nFigure 6: List of all ten causal graphs with treatment-effect pairs (CGTEs). We omit CGTEs that trivially\nresemble existing ones.\n\nA.2 Formulation of the Query Types\n\nHere, we introduce all the query types included in our dataset.\n\nRung-1 Queries: Marginal and Conditional Probabilities. For marginal probabilities, we ask\nquestions about the overall distribution of a variable. For conditional probabilities, we ask whether\nconditioning on one variable increases or decreases the likelihood of another variable. For the\nexplaining away questions, we condition on a collider node and ask how that affects the correlation\nbetween the two parents.\n\n19\n", "vlm_text": "A Supplementary for Dataset Generation \nA.1 List of References for Causal Inference \nWhen collecting the causal graphs, query types, and common sens ical stories for our dataset, we took our examples from the following books (sorted by year): \n1. Causality [ 67 ] 2. Causal inference in statistics: A Primer [ 23 ] 3. Elements of Causal Inference [ 69 ] 4. The Book of Why [ 66 ] 5. Introduction to Causal Inference [ 53 ] \nAnd the following papers: \n1. Causes and Explanations: A Structural-Model Approach. Part I: Causes [ 27 ] 2. Causes and Explanations: A Structural-Model Approach. Part II: Explanations [ 28 ] 3. Causality and Counter factual s in the Situation Calculus [ 35 ] 4. Causal inference in statistics: An overview [ 60 ] \nThe image is a table comparing different causal graph structures with their aliases and treatment-effect pairs. It includes the following:\n\n1. **Confounding**\n   - CGTE Alias: Confounding, Mediation\n   - Treatment-Effect Pair: Two graphs showing different causal paths between variables X and Y.\n\n2. **Immorality**\n   - CGTE Alias: Fork, Collision\n   - Treatment-Effect Pair: Two graphs showing different formations with variable X at a junction affecting Y and another variable.\n\n3. **Diamond**\n   - CGTE Alias: Diamond, DiamondCut\n   - Treatment-Effect Pair: Two graphs shaped like diamonds or split paths with variables X and Y connected through intermediaries.\n\n4. **Chain**\n   - CGTE Alias: Chain\n   - Treatment-Effect Pair: A linear path from variable X to Y through another variable.\n\n5. **IV**\n   - CGTE Alias: IV\n   - Treatment-Effect Pair: An instrumental variable Z affects X and Y.\n\n6. **Arrowhead**\n   - CGTE Alias: Arrowhead\n   - Treatment-Effect Pair: A branching graph with variable X affecting Y and another intermediary variable.\n\n7. **Frontdoor**\n   - CGTE Alias: Frontdoor\n   - Treatment-Effect Pair: A graph showing both direct and indirect paths from X to Y through Z.\n\nEach section provides a visual representation of the causal relationships among variables, highlighting different structural forms.\nFigure 6: List of all ten causal graphs with treatment-effect pairs (CGTEs). We omit CGTEs that trivially resemble existing ones. \nA.2 Formulation of the Query Types \nHere, we introduce all the query types included in our dataset. \nRung-1 Queries: Marginal and Conditional Probabilities. For marginal probabilities, we ask questions about the overall distribution of a variable. For conditional probabilities, we ask whether conditioning on one variable increases or decreases the likelihood of another variable. For the explaining away questions, we condition on a collider node and ask how that affects the correlation between the two parents. "}
{"page": 19, "image_path": "doc_images/2312.04350v3_19.jpg", "ocr_text": "Rung-2 Queries: ATE and Adjustment Set. For ATE questions, we ask whether the treatment\n(X = 1) increases or decreases the likelihood of the effect variable Y = y. For adjustment set\nquestions, we ask whether a set of variables should be adjusted for when estimating the causal\neffect between treatment and effect. By adjusting, we aim to blocked the non-causal paths from the\ntreatments to effect, and hence eliminate spurious correlation. For example, to query whether the set\ngender is an adjustment set for the effect of a treatment on recovery, we ask \"To estimate the effect\nof the treatment on recovery, should we directly look at how the treatment correlates with recovery,\nor should we look at gender-specific correlation?\" In the collider bias questions, similarly to the\nexplaining away questions, we condition on a collider variable and ask about how an intervention on\none of the parents (treatment X ) affects the other parent (outcome Y ). However since by construction\nX and Y do not have common causes, the answer to this question is always “no\n\nRung-3 Queries: Counterfactual Probability, ATT, NDE, and NIE. For counterfactual probabil-\nity, we ask about what would have been the likelihood of Y = y, if the treatment variable X had been\nx, given sufficient evidence e such that the query is identifiable. For ATT, we ask how the likelihood\nof Y = y would change for those who received treatment (X = 1) if there had been no treatment\n(X = 0). For NDE, we ask whether the X = 1 directly increases or decreases the likelihood of the\nY = y, not through any mediators. For NIE, we ask whether the treatment (setting X = 1) increases\nor decreases the likelihood of Y = y through mediators, not directly.\n\nA.3 Collection of Causal Graphs\nWe include all the ten causal graphs with treatment-effect pairs (CGTEs) in Figure 6.\n\nNote that one causal graph can have several different CGTEs, such as the confounding structure,\nwhich has three CGTEs: confounding, mediation, and collision in the triangle form. To generate all\nthe causal graphs and CGTEs here, we iterate all commonly used ones within four nodes in the CI\nbooks, and omit CGTEs whose solution by CI methods trivially resembles existing ones.\n\nA4 Data Coverage\n\nStarting from the full set of 12 distinct causal graphs and 10 query types, there are a few combinations\nthat must be omitted as the ground truth answer would be trivial or ill-defined. For example, in the\n“Immorality” graph, the treatment “X” and outcome “Y” are by construction statistically independent,\nso there correlation is necessarily 0. Similarly, there are several graphs where certain causal queries\nare ill-defined or don’t make sense to ask. Specifically:\n\n1. For the Natural Direct Effect, we only include questions on the “IV”, “Arrowhead”, “Con-\nfounding”, “Mediation” and “DiamondCut” graphs.\n\n2. For the Natural Indirect Effect, we only include questions on the “Mediation”, “Frontdoor”,\n“Arrowhead”, “Diamond” and “Chain” graphs.\n\n3. For the Collider Bias and Explaining Away effect, we only include questions on the “Colli-\nsion” graph.\n\n4. For the Average Treatment Effect, we include questions on all graphs except “Collision”.\n\n5. For the (deterministic) Counterfactuals, we include questions on all graphs except “Colli-\nsion”.\n\n6. For the Average Treatment Effect on the Treated (ATT), we include questions on all graphs\nexcept “Collision” and “IV”.\n\nThe “balanced” benchmark (main benchmark in v1.5), containing 10,112 questions split between all\nstories, graphs, query types, and commonsensicalness, is balanced such that there are roughly the\nsame number of questions for each distinct story-graph-query combination (ranging from 50-100\nper combination) across the different variants: commonsense, anticommonsense, and nonsense.\nFurthermore, we balance the distribution of correct answers so that there are the same number of\n“yes’’s and “no”’s.\n\nThe “aggregate” variant (main benchmark in v1.0) contains 10,560 questions and is primarily\nbalanced across all stories. However since the number of stories for each variant (commonsense,\nanticommonsense, and nonsense) varies significantly, the results in an unbalanced benchmark in\nterms of sensicalness.\n\n20\n", "vlm_text": "Rung-2 Queries: ATE and Adjustment Set. For ATE questions, we ask whether the treatment  $X=1)$  ) increases or decreases the likelihood of the effect variable  $Y\\,=\\,y$  . For adjustment set questions, we ask whether a set of variables should be adjusted for when estimating the causal effect between treatment and effect. By adjusting, we aim to blocked the non-causal paths from the treatments to effect, and hence eliminate spurious correlation. For example, to query whether the set gender is an adjustment set for the effect of a treatment on recovery, we ask    $^{\\prime\\prime}T o$   estimate the effect of the treatment on recovery, should we directly look at how the treatment correlates with recovery, or should we look at gender-specific correlation?\"  In the collider bias questions, similarly to the explaining away questions, we condition on a collider variable and ask about how an intervention on one of the parents (treatment    $X$  ) affects the other parent (outcome    $Y$  ). However since by construction  $X$   and  $Y$   do not have common causes, the answer to this question is always “no”. \nRung-3 Queries: Counter factual Probability, ATT, NDE, and NIE. For counter factual probabil- ity, we ask about what would have been the likelihood of    $Y=y$  , if the treatment variable  $X$   had been  $x$  , given sufficient evidence  $e$   such that the query is identifiable. For ATT, we ask how the likelihood of    $Y=y$   would change for those who received treatment   $X=1$  ) if there had been no treatment (  $X=0$  ). For NDE, we ask whether the    $X=1$   directly increases or decreases the likelihood of the  $Y=y$  , not through any mediators. For NIE, we ask whether the treatment (setting    $X=1$  ) increases or decreases the likelihood of  $Y=y$   through mediators, not directly. \nA.3 Collection of Causal Graphs \nWe include all the ten causal graphs with treatment-effect pairs (CGTEs) in Figure  6 . \nNote that one causal graph can have several different CGTEs, such as the confounding structure, which has three CGTEs: confounding, mediation, and collision in the triangle form. To generate all the causal graphs and CGTEs here, we iterate all commonly used ones within four nodes in the CI books, and omit CGTEs whose solution by CI methods trivially resembles existing ones. \nA.4 Data Coverage \nStarting from the full set of 12 distinct causal graphs and 10 query types, there are a few combinations that must be omitted as the ground truth answer would be trivial or ill-defined. For example, in the “Immorality” graph, the treatment “X” and outcome “Y” are by construction statistically independent, so there correlation is necessarily 0. Similarly, there are several graphs where certain causal queries are ill-defined or don’t make sense to ask. Specifically: \n1.  For the Natural Direct Effect, we only include questions on the “IV”, “Arrowhead”, “Con- founding”, “Mediation” and “DiamondCut” graphs. 2.  For the Natural Indirect Effect, we only include questions on the “Mediation”, “Frontdoor”, “Arrowhead”, “Diamond” and “Chain” graphs. 3.  For the Collider Bias and Explaining Away effect, we only include questions on the “Colli- sion” graph. 4. For the Average Treatment Effect, we include questions on all graphs except “Collision”. 5.  For the (deterministic) Counter factual s, we include questions on all graphs except “Colli- sion”. 6.  For the Average Treatment Effect on the Treated (ATT), we include questions on all graphs except “Collision” and “IV”. \nThe “balanced” benchmark (main benchmark in v1.5), containing 10,112 questions split between all stories, graphs, query types, and common sens ical ness, is balanced such that there are roughly the same number of questions for each distinct story-graph-query combination (ranging from 50-100 per combination) across the different variants: commonsense, anti commonsense, and nonsense. Furthermore, we balance the distribution of correct answers so that there are the same number of “yes”s and “no”s. \nThe “aggregate” variant (main benchmark in v1.0) contains 10,560 questions and is primarily balanced across all stories. However since the number of stories for each variant (commonsense, anti commonsense, and nonsense) varies significantly, the results in an unbalanced benchmark in terms of sens ical ness. "}
{"page": 20, "image_path": "doc_images/2312.04350v3_20.jpg", "ocr_text": "A.5 Query Form and Text Templates\nWe provide in Table 4 the text templates we use for each query type.\n\nQuery Type Symbolic Expres- Natural Language Question Template\nsion\nRung 1: Association\nMarg. Prob. P(Y) Is the overall likelihood of {vpoun(X = 1)} greater than\nchance?\nCond. Prob. P(Y|X) Is the chance of {vnoun(Y = 1)} larger when observing\n\n{Unoun(X = 1)}?\n\nRung 2: Intervention\n\nATE E[Y |do(X = 1)]|— Will {vnoun(X = 1)} increase the chance of {vnoun(Y = 1)}?\nE[Y |do(X = 0)]\n\nAdjust. Set If S opens a back- To understand how {voverai(X)} affects {voveran(Y = 1)},\ndoor path should we look directly at how {voverai(X)} correlates with\n\n{voverati (Y )} in general, or this correlation case by case accord-\ning to {voveran(S)}?\n\nRung 3: Counterfactuals\n\nCounterf. Prob. -P(Yz = y) Can we infer that {usent(Y = 1)} had it been that {vcona(X =\n1)} instead of X=0?\n\nATT E[¥i — Yo|X = 1) For {vattr(X = 1)}, would it be more likely to see {unoun(Y =\n1)} {vcona(X = 0)}?\n\nNDE E[¥i,m@p — Y1,mo] If we disregard the mediation effect through {voverai(Y = 1)},\nwould {vnoun(X = 1)} still positively affect {unoun(Y = 1)}?\n\nNIE E[Yo,m, — Yo,mp} Does {voveran(X)} affect {voveran(Y)} through\n\n{Voverali (Other Vars) }?\nTable 4: Example natural language templates for each query type.\n\nA.6 Nonsensical Stories\n\nTo come up with a collection of nonsensical variable names, we use GPT-4 to generate some\nmeaningless words. Specifically, we use the prompt: “Create 100 non-existent words that are\nshort, i.e., within 5-characters.”, with temperature=0 with the OpenAI interface. The collection of\nnonsensical words we later use as variable names are as follows: ziblo, truq, fyze, glimx, jorv, wexi,\nsnov, yupt, kraz, qixy, vubr, chiz, pliv, moxa, fygo, rukz, tasp, xevo, jyke, wibl, zorf, quzy, nyrp,\ngwex, smez, vytz, hupx, cwoj, lirf, ovka, pexu, yigz, twaz, kwox, zuph, fraq, jyxo, swoy, uvzi, nekl,\ngyZzp, rixq, vwem, xyfu, blyz, qwip, zeku, tijv, yomx, hwaz, czix, plof, muvy, fyqo, rujz, tasb, xevi,\njyka, wibm, zorx, quzw, nyro, gwet, smeu, vyta, hupz, cwoi, lirg, ovki, pexy, yigw, twac, kwoz, zupj,\nfraq, jyxi, swoq, uvzo, nekm, gyzl, rixw, vwen, xyfo, blyx, qwiu, zeky, tijw, yomz, hwax, czir, ploz,\nmuvgq, fyqi, rujx, tasn, xevu, jyko, wibp, zory, and quzt.\n\nA.7 Anti-Commonsensical Stories\nFor the anti-commonsensical stories, we randomly do one of the actions:\n\n1. Replace the effect variable Y with an attribute that would not be an effect variable in any of\nthe stories. Such replacement variables include: “lip thickness”, “earthquakes”, “lactose\nintolerance”, “rainfall”, “is allergic to peanuts”, “brown eyes”, “curly hair”, “black hair”,\n“foot size”, “freckles”\n\n2. Create an irrelevant treatment variable X that does not play a causal role in any of our\n\n3966.\n\ncommonsensical stories. Such as: “can swim”, “is religious”, “has a brother”, “has visited\nEngland”, “likes spicy food”, “is vegetarian”, “speaks english”, “drinks coffee”, “plays card\n\n99 Ke,\n\ngames”, “listens to jazz’, “solar eclipse’, “has a sister”, “full moon”\n\nTo transform a commonsensical story into an anti-commonsensical story, we apply one of these\nreplacements sampled uniformly, resulting in stories such as:\n\n¢ Ability to swim has a direct effect on studying habit and exam score. Studying habit has a\ndirect effect on exam score.\n\n* Gender has a direct effect on department competitiveness and peanut allergy. Department\ncompetitiveness has a direct effect on peanut allergy.\n\n21\n", "vlm_text": "A.5 Query Form and Text Templates \nThe table provides an overview of different query types, their symbolic expressions, and corresponding natural language question templates across three rungs of causal reasoning: Association, Intervention, and Counterfactuals.\n\n1. **Rung 1: Association**\n   - **Query Type:** \n     - Marginal Probability\n     - Conditional Probability\n   - **Symbolic Expression:** \n     - \\( P(Y) \\)\n     - \\( P(Y|X) \\)\n   - **Natural Language Question Template:** \n     - \"Is the overall likelihood of \\({v_\\text{noun}(X = 1)}\\) greater than chance?\"\n     - \"Is the chance of \\({v_\\text{noun}(Y = 1)}\\) larger when observing \\({v_\\text{noun}(X = 1)}\\)?\"\n\n2. **Rung 2: Intervention**\n   - **Query Type:** \n     - Average Treatment Effect (ATE)\n     - Adjustment Set\n   - **Symbolic Expression:** \n     - \\( \\mathbb{E}[Y|do(X = 1)] - \\mathbb{E}[Y|do(X = 0)] \\)\n     - If \\( S \\) opens a back-door path\n   - **Natural Language Question Template:** \n     - \"Will \\({v_\\text{noun}(X = 1)}\\) increase the chance of \\({v_\\text{noun}(Y = 1)}\\)?\"\n     - \"To understand how \\({v_\\text{overall}(X)}\\) affects \\({v_\\text{overall}(Y = 1)}\\), should we look directly at how \\({v_\\text{overall}(X)}\\) correlates with \\({v_\\text{overall}(Y)}\\) in general, or this correlation case by case according to \\({v_\\text{overall}(S)}\\)?\"\n\n3. **Rung 3: Counterfactuals**\n   - **Query Type:** \n     - Counterfactual Probability\n     - Average Treatment Effect on the Treated (ATT)\n     - Natural Direct Effect (NDE)\n     - Natural Indirect Effect (NIE)\n   - **Symbolic Expression:** \n     - \\( P(Y_x = y) \\)\n     - \\( \\mathbb{E}[Y_1 - Y_0|X = 1] \\)\n     - \\( \\mathbb{E}[Y_{1, M_0} - Y_{1, M_0}] \\)\n     - \\( \\mathbb{E}[Y_{0, M_1} - Y_{0, M_0}] \\)\n   - **Natural Language Question Template:**\n     - \"Can we infer that \\({v_\\text{sent}(Y = 1)}\\) had it been that \\({v_\\text{cond}(X = 1)}\\)\nA.6 Nonsensical Stories \nTo come up with a collection of nonsensical variable names, we use GPT-4 to generate some meaningless words. Specifically, we use the prompt: “Create 100 non-existent words that are short, i.e., within 5-characters.”, with temperature  $\\mathord{:=}0$   with the OpenAI interface. The collection of nonsensical words we later use as variable names are as follows: ziblo, truq, fyze, glimx, jorv, wexi, snov, yupt, kraz, qixy, vubr, chiz, pliv, moxa, fygo, rukz, tasp, xevo, jyke, wibl, zorf, quzy, nyrp, gwex, smez, vytz, hupx, cwoj, lirf, ovka, pexu, yigz, twaz, kwox, zuph, fraq, jyxo, swoy, uvzi, nekl, gyzp, rixq, vwem, xyfu, blyz, qwip, zeku, tijv, yomx, hwaz, czix, plof, muvy, fyqo, rujz, tasb, xevi, jyka, wibm, zorx, quzw, nyro, gwet, smeu, vyta, hupz, cwoi, lirg, ovki, pexy, yigw, twac, kwoz, zupj, fraq, jyxi, swoq, uvzo, nekm, gyzl, rixw, vwen, xyfo, blyx, qwiu, zeky, tijw, yomz, hwax, czir, ploz, muvq, fyqi, rujx, tasn, xevu, jyko, wibp, zory, and quzt. \nA.7 Anti-Common sens ical Stories \nFor the anti-common sens ical stories, we randomly do one of the actions: \n1.  Replace the effect variable  $Y$   with an attribute that would not be an effect variable in any of the stories. Such replacement variables include: “lip thickness”, “earthquakes”, “lactose intolerance”, “rainfall”, “is allergic to peanuts”, “brown eyes”, “curly hair”, “black hair”, “foot size”, “freckles” 2.  Create an irrelevant treatment variable    $X$   that does not play a causal role in any of our common sens ical stories. Such as: “can swim”, “is religious”, “has a brother”, “has visited England”, “likes spicy food”, “is vegetarian”, “speaks english”, “drinks coffee”, “plays card games”, “listens to jazz”, “solar eclipse”, “has a sister”, “full moon” \nTo transform a common sens ical story into an anti-common sens ical story, we apply one of these replacements sampled uniformly, resulting in stories such as: \n•  Ability to swim has a direct effect on studying habit and exam score. Studying habit has a direct effect on exam score. •  Gender has a direct effect on department competitiveness and peanut allergy. Department competitiveness has a direct effect on peanut allergy. "}
{"page": 21, "image_path": "doc_images/2312.04350v3_21.jpg", "ocr_text": "¢ Liking spicy food has a direct effect on relationship status. Appearance has a direct effect\non relationship status.\n\n¢ Playing card games has a direct effect on diabetes and lifespan. Smoking has a direct effect\non diabetes and lifespan. Diabetes has a direct effect on lifespan. Smoking is unobserved.\n\nFor a full list of the replacements and how the replacements are made, check out the code.\n\nA.8_ Explanation Template\nStep ® Extract the causal graph: The causal graph expressed in the context is: \"G\".\n\nStep @ Identify the query type: The query type of the above question is \"query_type\".\n\nStep ® Formulate the query to its symbolic form: The formal form of the query is\n\"symbolic_expression\".\n\nStep ® Collect all the available data: The available data are: \"d\".\n\nStep © Derive the estimand: Based on the graph structure and causal query, the question\ncan be simplified into estimand \"est\".\n\nStep © Solve for the estimand: Plug in the available data \"d\" into \"est\".\nest (d)\n® float(a)\n\nSince the estimate for the estimand is float(a), the overall answer to the question is bool(a).\n\nB_ Experimental Details\n\nB.1 CAUSALCOT Prompt\nQ: [question from the dataset]\n\nGuidance: Address the question by following the steps below:\n\nStep 1) Extract the causal graph: Identify the causal graph that depicts the relationships in the scenario.\nThe diagram should simply consist of edges denoted in \"varl -> var2\" format, separated by commas.\nStep 2) Determine the query type: Identify the type of query implied by the main question. Choices\n\nwow win\n\ninclude \"marginal probability\", \"conditional probability\", “explaining away effect\", \"backdoor ad-\njustment set\", \"average treatment effect\", \"collider bias\", \"normal counterfactual question\", \"average\ntreatment effect on treated\", \"natural direct effect\" or \"natural indirect effect\". Your answer should\n\nonly be a term from the list above, enclosed in quotation marks.\n\nStep 3) Formalize the query: Translate the query into its formal mathematical expression based on its\ntype, utilizing the \"do(-)\" notation or counterfactual notations as needed.\n\nStep 4) Gather all relevant data: Extract all the available data. Your answer should contain nothing\nbut marginal probabilities and conditional probabilities in the form \"P(...)=...\" or \"P(...1...=...\", each\nprobability being separated by a semicolon. Stick to the previously mentioned denotations for the\nvariables.\n\nStep 5) Deduce the estimand using causal inference: Given all the information above, deduce the\nestimand using skills such as do-calculus, counterfactual prediction, and the basics of probabilities.\nAnswer step by step.\n\nStep 6) Calculate the estimand: Insert the relevant data in Step 4 into the estimand, perform basic\narithmetic calculations, and derive the final answer. There is an identifiable answer. Answer step by\nstep.\n\nA: [LLM previous response]\n\nQ: Based on all the reasoning above, output one word to answer the initial question with just \"Yes\" or\n\"No\".\n\n22\n", "vlm_text": "•  Liking spicy food has a direct effect on relationship status. Appearance has a direct effect on relationship status. •  Playing card games has a direct effect on diabetes and lifespan. Smoking has a direct effect on diabetes and lifespan. Diabetes has a direct effect on lifespan. Smoking is unobserved. \nFor a full list of the replacements and how the replacements are made, check out the code. \nA.8 Explanation Template \nStep    $\\textcircled{\\scriptsize{1}}$  Extract the causal graph: The causal graph expressed in the context is: \"  $\\mathcal{G}^{\\prime}$  \". \nStep    $\\circledast$  Identify the query type: The query type of the above question is \" query _ type \". \nStep    $\\textcircled{3}$  Formulate the query to its symbolic form: The formal form of the query is \" symbolic _ expression \". \nStep    $\\clubsuit$  Collect all the available data: The available data are:   $\"d\"$  \nStep    $\\mathfrak{G}$  Derive the estimand: Based on the graph structure and causal query, the question can be simplified into estimand \" est \". \nStep    $\\mathfrak{G}$  Solve for the estimand: Plug in the available data \"  $\"d\"$   into \" est \".\n\n  $\\mathrm{est}(\\pmb{d})\n\n$   $\\approx\\mathrm{{floor}}(a)$  \nSince the estimate for the estimand is  $\\operatorname{flat}(a)$  , the overall answer to the question is    $\\operatorname{b o o l}(a)$  . \nB Experimental Details \nB.1 C AUSAL C O T Prompt \nQ:  [question from the dataset] \nGuidance: Address the question by following the steps below: \nStep 1) Extract the causal graph: Identify the causal graph that depicts the relationships in the scenario. The diagram should simply consist of edges denoted in \"var1  $->$   var2\" format, separated by commas. \nStep 2) Determine the query type: Identify the type of query implied by the main question. Choices include \"marginal probability\", \"conditional probability\", \"explaining away effect\", \"backdoor ad- justment set\", \"average treatment effect\", \"collider bias\", \"normal counter factual question\", \"average treatment effect on treated\", \"natural direct effect\" or \"natural indirect effect\". Your answer should only be a term from the list above, enclosed in quotation marks. \nStep 3) Formalize the query: Translate the query into its formal mathematical expression based on its type, utilizing the \"do(·)\" notation or counter factual notations as needed. \nStep 4) Gather all relevant data: Extract all the available data. Your answer should contain nothing but marginal probabilities and conditional probabilities in the form  $\"\\mathrm{P}(\\ldots){=}{\\ldots}\"$   or   $\"\\mathrm{P}(...|...){=}{...}\"$  , each probability being separated by a semicolon. Stick to the previously mentioned denotations for the variables. \nStep 5) Deduce the estimand using causal inference: Given all the information above, deduce the estimand using skills such as do-calculus, counter factual prediction, and the basics of probabilities. Answer step by step. \nStep 6) Calculate the estimand: Insert the relevant data in Step 4 into the estimand, perform basic arithmetic calculations, and derive the final answer. There is an identifiable answer. Answer step by step. \nA:  [LLM previous response] \nQ: Based on all the reasoning above, output one word to answer the initial question with just \"Yes\" or \"No\". "}
{"page": 22, "image_path": "doc_images/2312.04350v3_22.jpg", "ocr_text": "A: [LLM final answer]\n\nC_ Additional Technical Background for Preliminaries\n\nC.1 Graphical Models\n\nWe adopt the causal inference framework described in [61]. A causal graph G := (V, E) consists\nof a set of k vertices V : {Vj,...,V,} and directed edges E := {e;;}, where the existence of\neach e;; means that there is a direct causation from V; to V;, also denoted as V; + Vj. We also\nintroduce some notations to describe the relative positions among the nodes. Following a standard\nassumption in causality (but see, e.g., [6]), we will assume that G is a direct acyclic graph (DAG),\nwhere we denote the parents of a node V; as PA(V;) := {Vj|ei; € E}. We denote descendants\nDE(V,) := {Vj|Vj > --- > Vi € E} of anode V; as all the nodes that have at least one direct\npath leading to a node. We call a node V;, as a confounder (i.e., common cause) of the other two\nnodes V; and V; if e;;,e4; € E; a collider (i.e., common effect) if e;;,, ej, € FE; and a mediator if\nCiks kj © E.\n\nAmong all the variables in V, we use X and Y to denote two special variables, the treatment and\neffect, respectively.\nC.2 Illustration of the Three Rungs of the Causal Ladder\n\nIn Figure 7, we illustrate the difference among the three rungs by enumerating what actions are\nperformed on the variables other than target variables X and Y.\n\nRung 1. Association Rung 2. Intervention Rung 3. Counterfactuals\n\nThe correlation of X and Y, i.e., P(Y[X), flows\nthrough all undirected paths: do(X) Force X to bé the\n\n(2) Direct causation path Direct intevention on X counterfaciyal value x. 1 \\\ncuts of alits parents (K) / ‘ el\nOSG O : ;\nH \\To completely isolate the effect of X, we\n\nthrough . >\nthe confounder Z Average over all the non-descendants SON Hook at the counterfactual PCY | X=x)\n\nof X to get PCY | do(X))\n\n”’ Namely, we infer all non-descendants\n\n__e-’” of Xas if X were still the original value x\n\nFigure 7: The Causal Ladder consists of three rungs: association, intervention and counterfactuals. We color in\nblue the treatment X and effect Y, as well as the actions on X. We color in orange words about how to get the\nestimand, and we use the orange circle to include all the non-descendants of X.\n\nC.3 Causal Inference Methods\nWe introduce do-calculus which can downgrade the Rung-2 queries to Rung-1 quantities when it is\napplicable, and counterfactual predictions which downgrade the Rung-3 queries.\n\nC.3.1 Do-Calculus\nDo-Operator as a Notation As mentioned in Rung 2, the do-operator is a convenient notation to\nrepresent an intervention on a variable. For example, do(X = 2) sets the value of variable X to x.\n\nThree Inference Rules for Climbing the Ladder Do-calculus is a set of rules that allows us to\nanswer higher-rung questions using lower-rung quantities, such as probability distributions of Rung 1.\nGiven a causal graphical model with and four disjoint sets of variables X,Y, Z, and W, and a joint\nprobability distribution that is Markov and faithful to the graph, do-calculus contains the following\nthree rules:\n\nRule 1 (Insertion/deletion of observations):\nP(Y|do(X), Z,W) = P(Y|do(X),W), (1)\n\nif Y and Z are d-separated by X U W in G*, the graph obtained from G by removing all arrows\npointing into variables in _X .\n\nRule 2 (Action/observation exchange):\n\nP(Y|do(X),do(Z), W) = P(Y|do(X), Z,W), (2)\n\n23\n", "vlm_text": "A:  [LLM final answer] \nC Additional Technical Background for Preliminaries \nC.1 Graphical Models \nWe adopt the causal inference framework described in [ 61 ]. A causal graph  $G:=(V,E)$   consists of a  of    $k$   vertices    $V\\,:\\,\\{V_{1},.\\,.\\,.\\,,V_{k}\\}$   and directe edg  $\\pmb{{\\cal E}}\\,:=\\,\\{\\bar{e}_{i j}\\}$  , whe istence of each  $e_{i j}$   means that there is a direct causation from  $V_{i}$   to  $V_{j}$  , also denoted as  $V_{i}\\,\\rightarrow\\,V_{j}$   → . We also introduce some notations to describe the relative positions among the nodes. Following a standard assumption in causality (but see, e.g., [ 6 , we  $\\mathcal{G}$  rect acyclic graph (DAG),  $V_{i}$   as  $\\mathbf{PA}(V_{i}):=\\{V_{j}|e_{i j}\\in E\\}$   { |  ∈ } . We denote  descendants  $\\mathbf{DE}(V_{i}):=\\{V_{j}|V_{j}\\ {\\overset{.}{\\to}}\\ \\cdot\\cdot\\ {\\overset{.}{\\to}}\\ V_{i}\\in E\\}$   { |  →· · · →  ∈ }  a node  $V_{i}$   as all the nodes that have at least one direct path leading to a node. We call a node  $V_{k}$   as a  confounder  (i.e., common cause) of the other two  $V_{i}$  d    $V_{j}$   if    $e_{k i},e_{k j}\\in\\pmb{E}$  ; a  collider  (i.e., common effect) if    $e_{i k},e_{j k}\\in E$  ; and a  mediator  if  $e_{i k},e_{k j}\\in E$   ∈ . \nAmong all the variables in    $V$  , we use    $X$   and    $Y$   to denote two special variables, the treatment and effect, respectively. \nC.2 Illustration of the Three Rungs of the Causal Ladder \nIn Figure  7 , we illustrate the difference among the three rungs by enumerating what actions are performed on the variables other than target variables    $X$   and  $Y$  . \nThe image presents a diagram of the Causal Ladder, which has three rungs: Association, Intervention, and Counterfactuals.\n\n1. **Rung 1: Association**\n   - Shows the correlation of variables \\(X\\) and \\(Y\\), denoting \\(\\text{P}(Y|X)\\).\n   - Includes direct causation and a backdoor path through a confounder \\(Z\\).\n\n2. **Rung 2: Intervention**\n   - Illustrates doing an intervention on \\(X\\) (denoted as \\(\\text{do}(X)\\)), cutting off its parents.\n   - Emphasizes averaging over all non-descendants of \\(X\\) to derive \\(\\text{P}(Y|\\text{do}(X))\\).\n\n3. **Rung 3: Counterfactuals**\n   - Involves forcing \\(X\\) to take a counterfactual value \\(x'\\).\n   - Highlights inferring all non-descendants of \\(X\\) with the counterfactual condition \\(\\text{P}_{Y_x}(x)\\).\n\nThe treatments and actions on \\(X\\) are highlighted in blue, whereas the methods for obtaining estimands and the non-descendants of \\(X\\) are represented in orange.\nC.3 Causal Inference Methods \nWe introduce do-calculus which can downgrade the Rung-2 queries to Rung-1 quantities when it is applicable, and counter factual predictions which downgrade the Rung-3 queries. \nC.3.1 Do-Calculus \nDo-Operator as a Notation As mentioned in Rung 2, the  do -operator is a convenient notation to represent an intervention on a variable. For example,    $\\operatorname{do}(X=x)$   sets the value of variable  $X$   to  $x$  . \nThree Inference Rules for Climbing the Ladder Do-calculus is a set of rules that allows us to answer higher-rung questions using lower-rung quantities, such as probability distributions of Rung 1. Given a causal graphical model with and four disjoint sets of variables    $X,Y,Z$  , and  $W$  , and a joint probability distribution that is Markov and faithful to the graph, do-calculus contains the following three rules: \nRule  $I$   (Insertion/deletion of observations): \n\n$$\nP(Y|\\operatorname{do}(X),Z,W)=P(Y|\\operatorname{do}(X),W)\\;,\n$$\n \nif    $Y$   and  $Z$   are   $\\mathbf{d}\\cdot$  -separat  by    $X\\cup W$   in    $G^{*}$  , the graph obtained from    $\\mathcal{G}$   by removing all arrows pointing into variables in  X . \nRule 2 (Action/observation exchange): \n\n$$\nP(Y|\\operatorname{do}(X),\\operatorname{do}(Z),W)=P(Y|\\operatorname{do}(X),Z,W)\\;,\n$$\n "}
{"page": 23, "image_path": "doc_images/2312.04350v3_23.jpg", "ocr_text": "if Y and Z are d-separated by X U W in Gt, the graph obtained from G by removing all arrows\npointing into variables in _X and all arrows pointing out of variables in Z.\n\nRule 3 (Insertion/deletion of actions):\nP(Y|do(X),do(Z),W) = P(Y|do(X),W) , (3)\nif Y and Z are d-separated by X U W in G?, the graph obtained from G by first removing all arrows\n\npointing into variables in X (thus creating G*) and then removing all arrows pointing into variables\nin Z that are not ancestors of any variable in W in G*.\n\nThese rules are sound and complete [85]. Namely, iff we have all the terms on the right hand side,\nthen the causal term on the left hand side is identifiable.\n\nExample Application of Do-Calculus Taking the example in Figure 2, g; maps the query type\nATE to its symbolic expression E[Y | do(X = 1)] — E[Y| do(X = 0)].\nNext, go further simplifies the estimand given the confounding graph, as in the flow chart in the\nmiddle of Figure 2:\n\nATE := E[Y|do(X = 1)] — E[Y|do(X = 0)] (4)\n\nSO P(Z = 2)[EW|X = 1,7 = 2) EY |X =0,2 = 2), (5)\n\nwhich which resolves all the do(-) terms to probability terms. This example shows the famous\nbackdoor adjustment in do-calculus [59].\n\nC.3.2 Three Steps for Counterfactual Prediction\n\nGiven a SCM M, distribution on the exogenous variables P(u), and evidence e from the model\n(M, P(u)), the probability of the counterfactual \"if X had been « then Y would have been y, given\nwe observed e,” denoted P(Y, = ye), can be evaluated using the following three steps [67]:\n\nAbduction: Update the probability distribution P(w) by the evidence e to obtain P(ule)\n\nAction: Modify M by the action do(X = x), i.e. replace X with X = x in the structural equations,\nto obtain the modified SCM M,,\n\nPrediction: Use the modified model (M,, P(ule)), to compute the probability of Y = y.\n\nD_ Previous Results on CLADDER v1.0\nD.1 Dataset Statistics for v1.0\n\nTotal Rung! Rung2  Rung3\nSize\nonesunPtes 10,560 | 3,288 3,288 3,984 counterr, NDE\n# Sentences/Sample 6.85 6.00 7.00 7.25 NIE ATT\n# Words/Sample 94.47 76.41 96.84 103.42\n# Nodes/Graph 3.54 3.54 3.55 3.54 3 iz\n# Edges/Graph 3.44 3.41 3.43 3.46 e Rung2 “st See\nAnswer %\nPositive Class (%) 50 50 50 50 . ATE\nExplanations Marg, Prop,\n#Sentences/Sample = 13.11 12.04 13.76 13.83 .\n# Words/Sample 146.82 | 141.88 147.88 151.30 Figure 8: Distributions of\nquery types in our dataset\nTable 5: Statistics of our CLADDER data v1.0. v1.0.\n\nOur data-generating procedure has the potential to algorithmically generate very large amounts of\nquestions. In practice, we pick a dataset size that is large enough to be representative, and at the same\ntime not too large to be problematic given the expensive inference costs of LLMs. We therefore set\nour dataset size to be 10K. We report the statistics of our dataset in Table 5.\n\nThe dataset roughly balanced across the query types, graph structures, stories, and ground-truth\nanswers (as seen in Figure 8). Note that there are some slight adjustments such as more samples for\nATE because it allows us to test various techniques, including backdoor and front door adjustments.\nMore details on our design choices can be found in Appendix A.4.\n\n24\n", "vlm_text": "if    $Y$   and  $Z$   are   $\\mathbf{d}$  -separat  by    $X\\cup W$   in    $G^{\\dagger}$  , the graph obtained fro  $\\mathcal{G}$   by removing all arrows pointing into variables in  X  and all arrows pointing out of variables in  Z . \nRule 3 (Insertion/deletion of actions): \n\n$$\nP(Y|\\operatorname{do}(X),\\operatorname{do}(Z),W)=P(Y|\\operatorname{do}(X),W)\\;,\n$$\n \nif  $Y$   and  $Z$   are d-separate y  $X\\cup W$   in    $G^{\\ddagger}$  the graph obtained from    $\\mathcal{G}$   by first removing all arrows pointing into variables in  X  (thus creating  $G^{*}$  ) and then removing all arrows pointing into variables in    $Z$   that are not ancestors of any variable in    $W$   in  $G^{*}$  . \nThese rules are sound and complete [ 85 ]. Namely, iff we have all the terms on the right hand side, then the causal term on the left hand side is identifiable. \nExample Application of Do-Calculus Taking the example in Figure  2 ,  $g_{1}$   maps the query type ATE to its symbolic expression  $\\mathbb{E}[Y|\\operatorname{do}(X=1)]-\\mathbb{E}[Y|\\operatorname{do}(X=0)]$  . \nNext,    $g_{2}$   further simplifies the estimand given the confounding graph, as in the flow chart in the middle of Figure  2 : \n\n$$\n\\begin{array}{l}{\\mathrm{ATE}:=\\mathbb{E}[Y|\\operatorname{do}(X=1)]-\\mathbb{E}[Y|\\operatorname{do}(X=0)]}\\\\ {\\qquad=\\displaystyle\\sum_{z}P(Z=z)[\\mathbb{E}(Y|X=1,Z=z)-\\mathbb{E}(Y|X=0,Z=z)]\\:,}\\end{array}\n$$\n \nwhich which resolves all the  $\\mathrm{{do}(\\cdot)}$   terms to probability terms. This example shows the famous backdoor adjustment in do-calculus [ 59 ]. \nC.3.2 Three Steps for Counter factual Prediction \nGiven a SCM    $M$  , distribution on the exogenous variables  $P(u)$  , and evidence    $e$   from the model  $\\langle M,P(u)\\rangle$  , t e probabili nter factual \"if    $X$   had been  $x$   then    $Y$  would have been y, given we observed  e ,” denoted  $P(Y_{x}=y|e)$  | , can be evaluated using the following three steps [ 67 ]: \nAbduction:  Update the probability distribution  $P(u)$   by the evidence    $e$   to obtain    $P(u|e)$  \nAction:  Modify    $M$   by the action    $d o(X=x)$  , i.e. replace  $X$   with    $X=x$   in the structural equations, to obtain the modified SCM    $M_{x}$  \nPrediction:  Use the modified model    $\\langle M_{x},P(u|e)\\rangle$  , to compute the probability of    $Y=y$  . \nD Previous Results on CL ADDER  v1.0 \nD.1 Dataset Statistics for v1.0 \nThe table provides a breakdown of data characteristics across different categories labeled as \"Total,\" \"Rung 1,\" \"Rung 2,\" and \"Rung 3.\"\n\n1. **Size**:\n   - Total samples: 10,560\n   - Rung 1: 3,288 samples\n   - Rung 2: 3,288 samples\n   - Rung 3: 3,984 samples\n\n2. **Question**:\n   - Average sentences per sample: \n     - Total: 6.85\n     - Rung 1: 6.00\n     - Rung 2: 7.00\n     - Rung 3: 7.25\n   - Average words per sample: \n     - Total: 94.47\n     - Rung 1: 76.41\n     - Rung 2: 96.84\n     - Rung 3: 103.42\n   - Average nodes per graph:\n     - Total: 3.54\n     - Rung 1: 3.54\n     - Rung 2: 3.55\n     - Rung 3: 3.54\n   - Average edges per graph:\n     - Total: 3.44\n     - Rung 1: 3.41\n     - Rung 2: 3.43\n     - Rung 3: 3.46\n\n3. **Answer**:\n   - Positive class percentage is consistently 50 across all categories.\n\n4. **Explanations**:\n   - Average sentences per sample:\n     - Total: 13.11\n     - Rung 1: 12.04\n     - Rung 2: 13.76\n     - Rung 3: 13.83\n   - Average words per sample:\n     - Total: 146.82\n     - Rung 1: 141.88\n     - Rung 2: 147.88\n     - Rung 3: 151.30\nThe image is a circular chart that illustrates the distribution of different query types in a dataset labeled as \"v1.0\". The chart is divided into three main sections or \"rungs\", each containing different types of queries.\n\n- **Rung 1** (Green Section): Represents probabilistic queries, including \"Cond. Prob.\" (conditional probability) and \"Marg. Prob.\" (marginal probability).\n- **Rung 2** (Blue Section): Contains causal inference queries, specifically \"ATE\" (Average Treatment Effect) and \"Adjust. Set\".\n- **Rung 3** (Orange Section): Encompasses more complex causal reasoning queries such as \"Counterf.\" (Counterfactual), \"NDE\" (Natural Direct Effect), \"NIE\" (Natural Indirect Effect), and \"ATT\" (Average Treatment effect on the Treated).\n\nThe chart is visually structured to show progression or hierarchy among these query types, with Rung 1 likely representing more basic types of queries, progressing to more complex queries in Rung 3.\nOur data-generating procedure has the potential to algorithmic ally generate very large amounts of questions. In practice, we pick a dataset size that is large enough to be representative, and at the same time not too large to be problematic given the expensive inference costs of LLMs. We therefore set our dataset size to be 10K. We report the statistics of our dataset in Table  5 . \nThe dataset roughly balanced across the query types, graph structures, stories, and ground-truth answers (as seen in Figure  8 ). Note that there are some slight adjustments such as more samples for ATE because it allows us to test various techniques, including backdoor and front door adjustments. More details on our design choices can be found in Appendix  A.4 . "}
{"page": 24, "image_path": "doc_images/2312.04350v3_24.jpg", "ocr_text": "D.2. Main Results on v1.0\n\nAcc. by Run; Acc. by Empirical Alignment\nOverall Ace. 1 3 . 3 Antec, Nonsens. Comm.\nRandom 49.27 50.28 48.40 49.12 | 49.69 49.01 49.12\nLLaMa 45.22 63.33 31.10 41.45 | 45.31 45.21 45.12\nAlpaca 45.54 63.33 31.57 41.91 | 45.94 45.21 45.49\nGPT-3 Non-Instr. (davinci) 47.42 63.88 32.99 44.89 | 47.0 48.28 46.97\nGPT-3 Instr. (text-davinci-001) 57.07 63.95 63.63 48.04 | 59.12 57.81 54.28\nGPT-3 Instr. (text-davinci-002) 56.24 46.03 69.55 55.04 | 54.75 59.65 54.31\nGPT-3 Instr. (text-davinci-003) 62.69 58.0 80.83 54.52 | 63.93 62.09 62.05\nGPT-3.5 (queried in May 2023) 61.71 65.12 69.9 54.11 | 65.43 55.15 64.55\nGPT-4 (queried in May 2023) 64.28 53.94 81.87 63.11 | 65.75 60.87 66.21\n+ CAUSALCOT 66.64 61.67 86.13 58.23 | 69.32 63.02 67.60\n\nTable 6: Performance of all models on our CLADDER dataset v1.0. We report the overall accuracy (Acc.), and\nalso fine-grained accuracy by rung and by empirical alignment.\n\nWe compare the performance of all models in Table 6. First, we can see that the causal reasoning\ntask in CLADDER is in general very challenging for all models. And models such as the earlier,\nnon-instruction-tuned GPT-3 and both LLaMa and Alpaca are no better than random performance.\nWith instruction-tuning, models start to show some improvement. And amongst all, our CAUSALCOT\nachieves the highest performance of 66.64%, which is 2.36 points better than vanilla GPT-4.\n\nMoreover, from the accuracy by empirical alignment level in Table 6, we can see that the original\nGPT-4 model performs the best on commonsensical data, but 5.34 points worse on nonsensical\ndata. However, our CAUSALCOT enhances the reasoning ability across all levels, with substantial\nimprovement on anti-commonsensical data and nonsensical data, indicating that CAUSALCOT is\nparticularly beneficial on unseen data.\n\nD.3 Ablation Study on v1.0\n\nWe conduct an ablation study for our multi-step CAUSALCOT. We ablate. §=——————_\neach of the four subquestions, and observe in Table 7 that classifying the CAUSALCOT 66.64\nquery type and formalizing it has the most effect on the model’s perfor- w/o Step ® 64.54\nmance, which might be because that they are the crucial formalization w/o Step @ 63.74\nstep in order to do the causal inference correctly. Meanwhile, removing w/o Step ® 63.43\nSteps ® and ®, which are mostly about parsing the prompt correctly, _ w/o Step ® 64.47\nhave the least impact on performance. Table 7; Ablation study.\n\nE More Experiments\n\nE.1 Details of Our Error Analysis\n\nFor Step 2 about the query type prediction, we report the overall F1 classification score, and also\nFI by rungs. For the rest of the steps, we manually annotate the correctness of 100 samples of\nCAUSALCOT. We report the correctness of est by accuracy, and the correctness of the predicted set\nof available data by taking the Fl with the ground-truth d. For Step 5, we report the accuracy of\nwhether the model simplifies the estimand correctly to est’ using causal inference, and also arithmetic\ncorrectness (Arith.).\n\nE.2, ROSCOE Evaluation\n\nWe employed the ROSCOE suite of evaluation metrics on step-by-step text reasoning, as introduced\nby [25], to automate the evaluation of the outputs from CAUSALCOT on 2,000 randomly sampled\nquestions from our dataset. Differing from conventional metrics, ROSCOE is specifically designed\nto scrutinize the quality of large language model outputs, focusing on aspects such as semantic\nconsistency, logicality, informativeness, fluency, and factuality, all evaluated within the context of\nstep-by-step reasoning, rather than solely the final response. This allows for a more objective and\ncomprehensive assessment of a model’s output, greatly aiding in the verification of its interpretability.\nThe results of this evaluation can be found in Table 8 and Figure 9. We consider the model’s\nperformance as unsatisfying if it falls out of the top quantile, namely receiving a score s € [0, 1]\nsmaller than 0.25 when the score should be minimized, or greater than 0.75 when it should be\nmaximized.\n\n25\n", "vlm_text": "The table presents accuracy results for various models evaluated across different dimensions. Here's a breakdown:\n\n1. **Overall Acc.** (Overall Accuracy):\n   - Random: 49.27\n   - LLama: 45.22\n   - Alpaca: 45.54\n   - GPT-3 Non-Instr. (davinci): 47.42\n   - GPT-3 Instr. (text-davinci-001): 57.07\n   - GPT-3 Instr. (text-davinci-002): 56.24\n   - GPT-3 Instr. (text-davinci-003): 62.69\n   - GPT-3.5 (queried in May 2023): 61.71\n   - GPT-4 (queried in May 2023): 64.28\n   - + CausalCoT: 66.64\n\n2. **Acc. by Rung** (Accuracy by Rung Levels 1, 2, 3):\n   - Random: 50.28, 48.40, 49.12 respectively\n   - LLama: 63.33, 31.10, 41.45 respectively\n   - Alpaca: 63.33, 31.57, 41.91 respectively\n   - GPT-3 Non-Instr.: 63.88, 32.99, 44.89 respectively\n   - GPT-3 Instr. (text-davinci-001): 63.95, 63.63, 48.04 respectively\n   - GPT-3 Instr. (text-davinci-002): 46.03, 69.55, 55.04 respectively\n   - GPT-3 Instr. (text-davinci-003): 58.0, 80.83, 54.52 respectively\n   - GPT-3.5: 65.12, 69.9, 54.11 respectively\n   - GPT-4: 53.94, 81.87, 63.11 respectively\n   - + CausalCoT: 61.67, 86.13, 58.23 respectively\n\n3. **Acc. by Empirical Alignment** (Anti-C., Nonsens., Comm.):\n   - Random: 49.69, 49.01, 49.12 respectively\n   - LLama: 45.31, 45.21, 45.12 respectively\n   - Alpaca: 45.94, 45.21, 45.49 respectively\n   - GPT-3 Non-Instr.: 47.0, 48.28, 46.97 respectively\n   - GPT-3 Instr. (text-davinci-001): 59.12, 57.81, 54.28 respectively\nWe compare the performance of all models in Table  6 . First, we can see that the causal reasoning task in CL ADDER  is in general very challenging for all models. And models such as the earlier, non-instruction-tuned GPT-3 and both LLaMa and Alpaca are no better than random performance. With instruction-tuning, models start to show some improvement. And amongst all, our C AUSAL C O T achieves the highest performance of   $66.64\\%$  , which is 2.36 points better than vanilla GPT-4. \nMoreover, from the accuracy by empirical alignment level in Table  6 , we can see that the original GPT-4 model performs the best on common sens ical data, but 5.34 points worse on nonsensical data. However, our C AUSAL C O T enhances the reasoning ability across all levels, with substantial improvement on anti-common sens ical data and nonsensical data, indicating that C AUSAL C O T is particularly beneficial on unseen data. \nD.3 Ablation Study on v1.0 \nWe conduct an ablation study for our multi-step C AUSAL C O T. We ablate each of the four sub questions, and observe in Table  7  that classifying the query type and formalizing it has the most effect on the model’s perfor- mance, which might be because that they are the crucial formalization step in order to do the causal inference correctly. Meanwhile, removing Steps    $\\textcircled{\\scriptsize{1}}$  and    $\\clubsuit$  , which are mostly about parsing the prompt correctly, have the least impact on performance. \nThe table presents accuracy (Acc.) values for a method called CAUSALCoT and variations of it that omit specific steps. Here are the details:\n\n- **CAUSALCoT**: 66.64%\n- **w/o Step ①**: 64.54%\n- **w/o Step ②**: 63.74%\n- **w/o Step ③**: 63.43%\n- **w/o Step ④**: 64.47% \n\nIt shows the impact on accuracy when each step is removed.\nTable 7: Ablation study. \nE More Experiments \nE.1 Details of Our Error Analysis \nFor Step 2 about the query type prediction, we report the overall F1 classification score, and also F1 by rungs. For the rest of the steps, we manually annotate the correctness of 100 samples of C AUSAL C O T. We report the correctness of  est  by accuracy, and the correctness of the predicted set of available data by taking the F1 with the ground-truth    $^{d}$  . For Step 5, we report the accuracy of whether the model simplifies the estimand correctly to  $\\mathrm{est}^{\\prime}$    using causal inference, and also arithmetic correctness (Arith.). \nE.2 ROSCOE Evaluation \nWe employed the ROSCOE suite of evaluation metrics on step-by-step text reasoning, as introduced by [ 25 ], to automate the evaluation of the outputs from C AUSAL C O T on 2,000 randomly sampled questions from our dataset. Differing from conventional metrics, ROSCOE is specifically designed to scrutinize the quality of large language model outputs, focusing on aspects such as semantic consistency, logicality, informative ness, fluency, and factuality, all evaluated within the context of step-by-step reasoning, rather than solely the final response. This allows for a more objective and comprehensive assessment of a model’s output, greatly aiding in the verification of its interpret ability. The results of this evaluation can be found in Table  8  and Figure  9 . We consider the model’s performance as unsatisfying if it falls out of the top quantile, namely receiving a score    $s\\in[0,1]$  smaller than 0.25 when the score should be minimized, or greater than 0.75 when it should be maximized. "}
{"page": 25, "image_path": "doc_images/2312.04350v3_25.jpg", "ocr_text": "We can see in the plot that the good-performing aspects are faithfulness to the original question,\nreasoning alignment with the ground truth, and absence of external hallucinations, which are consis-\ntently within the top quantile. This suggests that the model carries out accurate reasoning within the\nconstraints of the fictitious world introduced in each question.\n\nHowever, there are some performance dips in redundancy, perplexity chain, and missing step metrics.\nThe first two could potentially be attributed to complex elements such as graph notation, while the\nrelatively lower “missing step” score warrants further investigation. Despite these observations, this\nanalysis largely aligns with our qualitative understanding of the models’ good response ability in\nanswering causal questions in our dataset.\n\nMean St Min 25% 50% 75% Max\n\nFaithfulness 0.89 0.02 0.83 0.88 0.89 0.90 0.93\nInformativeness Step 0.88 0.0 0.83 0.87 0.88 0.89 0.92\nInformativeness Chain 0.88 0.03 0.76 0.87 0.89 0.90 0.96\nFaithfulness Word 0.95 0.0 0.92 0.94 0.95 0.96 0.97\nRepetition Word 0.02 0.02 -0.00 0.00 0.02 0.04 0.05\nRepetition Step 0.02 0.01 -0.00 0.00 0.01 0.03 0.06\nReasoning Alignment 0.92 0.0 0.86 0.9 0.92 0.93 0.95\nExternal Hallucination 0.97 0.02 0.84 0.96 0.97 0.98 0.99\nRedundancy 0.80 0.05 0.56 0.77 0.80 0.83 0.92\nCommon Sense Error 0.95 0.0 0.86 0.94 0.95 0.96 0.98\nMissing Step 0.78 0.03 0.58 0.76 0.78 0.80 0.88\n\nSemantic Coverage Step 0.99 0.0 0.95 0.98 0.99 0.99 1.00\nSemantic Coverage Chain 0.98 0.0 0.93 0.98 0.98 0.99 0.99\nDiscourse Representation 0.06 0.13 0.00 0.0 0.01 0.05 0.67\nCoherence Step Vs Step 0.14 0.27 0.00 0.00 0.01 0.07 0.94\n\nPerplexity Step 0.02 0.0 0.00 0.02 0.02 0.03 0.07\nPerplexity Chain 0.17 0.07 0.05 0.1 0.17 0.23 0.42\nPerplexity Step Max 0.00 0.00 0.00 0.00 0.00 0.01 0.02\nGrammar Step 0.93 0.04 0.77 0.90 0.93 0.96 0.99\nGrammar Step Max 0.53 0.35 0.02 0.12 0.65 0.85 0.99\n\nTable 8: Statistics of ROSCOE scores evaluated on answers from CAUSALCOT on 2,000 randomly sampled\nquestions from our dataset.\n\nF Comparison with Existing Causality-Related Datasets\n\nWe show in Table 9 the distinction of our work from all existing causality-related datasets that address\neither the causality-as-knowledge task, or the causality-as-language-comprehension task.\n\n26\n", "vlm_text": "We can see in the plot that the good-performing aspects are faithfulness to the original question, reasoning alignment with the ground truth, and absence of external hallucinations, which are consis- tently within the top quantile. This suggests that the model carries out accurate reasoning within the constraints of the fictitious world introduced in each question. \nHowever, there are some performance dips in redundancy, perplexity chain, and missing step metrics. The first two could potentially be attributed to complex elements such as graph notation, while the relatively lower “missing step” score warrants further investigation. Despite these observations, this analysis largely aligns with our qualitative understanding of the models’ good response ability in answering causal questions in our dataset. \nThe table contains statistics for various metrics related to some form of analysis, possibly in the context of language processing, machine learning, or data analysis judging by the metric names. For each metric, the table provides the following statistical measurements:\n\n1. **Mean**: The average score of the metric.\n2. **Std (Standard Deviation)**: Indicates the variability or dispersion of the scores.\n3. **Min**: The minimum score observed for the metric.\n4. **25%**: The first quartile, indicating that 25% of the scores fall below this value.\n5. **50%**: The median value (second quartile), which is the middle point of the data set.\n6. **75%**: The third quartile, showing that 75% of the scores are below this value.\n7. **Max**: The maximum score observed for the metric.\n\nThe metrics listed include:\n\n- Faithfulness\n- Informativeness (Step and Chain)\n- Faithfulness Word\n- Repetition (Word and Step)\n- Reasoning Alignment\n- External Hallucination\n- Redundancy\n- Common Sense Error\n- Missing Step\n- Semantic Coverage (Step and Chain)\n- Discourse Representation\n- Coherence Step Vs Step\n- Perplexity (Step and Chain)\n- Perplexity Step Max\n- Grammar\n- Grammar Step Max\n\nThese metrics appear to evaluate aspects such as faithfulness, informativeness, repetition, reasoning, hallucinations, redundancy, errors, coverage, discourse representation, coherence, perplexity, and grammar. Each metric is quantified by the statistical measures provided, illustrating their performance or extent across a particular dataset or model.\nF Comparison with Existing Causality-Related Datasets \nWe show in Table  9  the distinction of our work from all existing causality-related datasets that address either the causality-as-knowledge task, or the causality-as-language-comprehension task. "}
{"page": 26, "image_path": "doc_images/2312.04350v3_26.jpg", "ocr_text": "Question Types Skill Types\nFormalization\nAssoc. Interv. Counterf. Cl Method of Causal Causal RI\nQueries\n\nE Qualitative\nReasoning\n\nDatasets for Causality as Knowledge (Commonsense Causality)\nCOPA [2012]\nEvent2Mind [2018]\nATOMIC [2019]\nSocialIQA [2019]\nTimeTravel [2019]\nGoal-Step [2020]\nAbductive (ART) [2020]\nCom2Sense [2021]\nCRASS [2022]\nDatasets for Causality as Lang\nSemEval2021 Task8 [2010]\nEventCausality [2011]\nCausal-TimeBank [2014]\nCaTeRS [2016]\nBECauSE [2017]\nTellMeWhy [2021]\nDatasets for Formal Causal Reasoning\nCorr2Cause [42\n\nx x\nCLADDER (Ours) ov ov ov v\n\nTable 9: Comparison of our dataset and existing causal or reasoning datasets. The aim of our dataset is to test\nthe pure reasoning ability of LLMs on causal questions. For each dataset, we first identify whether its question\ntypes cover the three rungs: association (Assoc.), intervention (Interv.), and counterfactuals (Counterf.). We also\ncheck what skill types the dataset tests: the application of causal inference methods (CI Method), formalization\nof causal queries, causal relation extraction from the given text (Causal RE), and qualitative reasoning.\n\nx*NANANNANANS\n\\% O&K OK OOO\n%K O&K KK OK OK OK OKO\n%K O&K KK OK OK OK OKO\n\nage Comprehension (Causal Relation Extraction)\n\nee ee ee ee ee ee,\nNNN NNR OK OKO OO\n\nN\\ [os sO\n\nx\nx\nx\nx\nx\nx\nx\n\nXx WOK KKK KK OK OK OK OK OK OK OKO\n\nQA px we KO\nQON fe ee eK\n\n27\n", "vlm_text": "The table categorizes datasets based on types of causality analysis and the skills they test. It is divided into three sections:\n\n1. **Datasets for Causality as Knowledge (Commonsense Causality):**\n   - Lists datasets like COPA, ATOMIC, etc.\n   - Evaluates them on question types (Association, Intervention, Counterfactual) and skill types (CI Method, Formalization of Causal Queries, Causal Relation Extraction, Qualitative Reasoning).\n\n2. **Datasets for Causality as Language Comprehension (Causal Relation Extraction):**\n   - Includes datasets like SemEval2021 Task8, EventCausality, etc.\n   - Similar evaluation criteria as above.\n\n3. **Datasets for Formal Causal Reasoning:**\n   - Includes Corr2Cause and CLADDER.\n   - Assessed on the same criteria.\n\nSymbols used:\n- ✔️: Indicates the presence of a specific feature in the dataset.\n- ❌: Indicates the absence of a specific feature in the dataset."}
{"page": 27, "image_path": "doc_images/2312.04350v3_27.jpg", "ocr_text": "faithfulness informativeness step informativeness chain\n\n‘so 00 350\n300\n+00\n300 250\n0 200\nmo 200 150\n100\n00\n100 so\n° o °\noo 02 «08 ~~ ~SCBSSCSO oo 02 +08 08 08 10 oo 02 «08 + +06 08 10\nfaithfulness ww repetition word sooo repetition step\n00 0\n700\nsoo ‘00\n0\n500 coo wo\n400 40\n400\n300 0\n200 200 00\n100 100\n° ° °\n00 2 ry vl CY a oo of 06 8\nreasoning alignment external hallucination redundancy\n00\n00 vs\nhad 0 150\n500\noo ws\n400 100\n00\n300 5\n200 70 0\nwo 200 a |\n° ° °\noo 2 oe 08 oo 02 +08 06 08 10 ot 02 os 08)\n‘common sense error missing step semantic coverage step\n00 00 ‘00\n700 oso 700\n«00 00\n0\n500 500\n400 150 400\n300 00 300\n200 200\n100 %° wo\n° o °\noo 02S os 10 oo 02 08 06 08 10 oo 0?”~«OSSCSCHSSC«SCO\nsemantic coverage chain discourse representation coherence step vs step\n1200\n00\n1000 1009\n00 00 800\nwoo eo\n400\n400 400\n200\n200 200\noo |= 02~SCO«SSsCiCHSC«iHS*«i oo 8 02~C«OSSCtHSC«BS*«i oo) 02~SC«SSsCHSC«iHCS*«a\nperplexity step perplexity chain perplexity step max\nas 1600\n0 so 1400\n1200\nfoo} 2s\n1000\n100 wo\n400 1s\neo\n200 0 400\nFo 200\n° ° °\noo 2 oe os a oo 86 02)SCtCiSSSCHSC«CHS*«s oo 02 08 06 08 10\nrammar ste yammar step max\nx00 9 P g P\n\n400\nws 0\n150 00\nws 20\nwo 200\n150\n100\n\noh Ba\no 8\n\n00 0204 0608 10 oo 8602S 06 0810\n\nFigure 9: ROSCOE scores of answers from CAUSALCOT on 2,000 randomly sampled questions from our\ndataset.\n\n28\n", "vlm_text": "The image contains multiple histograms displaying ROSCOE scores for different evaluation criteria related to answers from Causal CoT on a dataset of 2,000 randomly sampled questions. Each histogram represents a different metric:\n\n1. Faithfulness\n2. Informativeness Step\n3. Informativeness Chain\n4. Faithfulness WW\n5. Repetition Word\n6. Repetition Step\n7. Reasoning Alignment\n8. External Hallucination\n9. Redundancy\n10. Common Sense Error\n11. Missing Step\n12. Semantic Coverage Step\n13. Semantic Coverage Chain\n14. Discourse Representation\n15. Coherence Step vs Step\n16. Perplexity Step\n17. Perplexity Chain\n18. Perplexity Step Max\n19. Grammar Step\n20. Grammar Step Max\n\nEach plot has a similar x-axis ranging from 0 to 1, representing the score, while the y-axis reflects the frequency/count of answers achieving a particular score."}
