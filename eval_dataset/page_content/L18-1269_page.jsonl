{"page": 0, "image_path": "doc_images/L18-1269_0.jpg", "ocr_text": "SentEval: An Evaluation Toolkit for Universal Sentence Representations\n\nAlexis Conneau* and Douwe Kiela\nFacebook Artificial Intelligence Research\n{aconneau,dkiela} @ fb.com\n\nAbstract\n\nWe introduce SentEval, a toolkit for evaluating the quality of universal sentence representations. SentEval encompass\n\nsa variety of\n\ntasks, including binary and multi-class classification, natural language inference and sentence similarity. The set of tasks was selected\nbased on what appears to be the community consensus regarding the appropriate evaluations for universal sentence representations.\nThe toolkit comes with scripts to download and preprocess datasets, and an easy interface to evaluate sentence encoders. The aim is to\nprovide a fairer, less cumbersome and more centralized way for evaluating sentence representations.\n\nKeywords: representation learning, evaluation\n\n1. Introduction\n\nFollowing the recent word embedding upheaval, one of\nNLP’s next challenges has become the hunt for univer-\nsal general-purpose sentence representations. What distin-\nguishes these representations, or embeddings, is that they\nare not necessarily trained to perform well on one specific\ntask. Rather, their value lies in their transferability, i.e.,\ntheir ability to capture information that can be of use in any\nkind of system or pipeline, on a variety of tasks.\n\nWord embeddings are particularly useful in cases where\nthere is limited training data, leading to sparsity and poor\nvocabulary coverage, which in turn lead to poor generaliza-\ntion capabilities. Similarly, sentence embeddings (which\nare often built on top of word embeddings) can be used\nto further increase generalization capabilities, composing\nunseen combinations of words and encoding grammatical\nconstructions that are not present in the task-specific train-\ning data. Hence, high-quality universal sentence represen-\ntations are highly desirable for a variety of downstream\nNLP tasks.\n\nThe evaluation of general-purpose word and sentence em-\nbeddings has been problematic (Chiu et al., 2016; Faruqui\net al., 2016), leading to much discussion about the best way\nto go about it'. On the one hand, people have measured\nperformance on intrinsic evaluations, e.g. of human judg-\nments of word or sentence similarity ratings (Agirre et al.,\n2012; Hill et al., 2016b) or of word associations (Vulié et\nal., 2017). On the other hand, it has been argued that the\nfocus should be on downstream tasks where these repre-\nsentations would actually be applied (Ettinger et al., 2016;\nNayak et al., 2016). In the case of sentence representa-\ntions, there is a wide variety of evaluations available, many\nfrom before the “embedding era’, that can be used to as-\nsess representational quality on that particular task. Over\nthe years, something of a consensus has been established,\nmostly based on the evaluations in seminal papers such as\nSkipThought (Kiros et al., 2015), concerning what evalu-\nations to use. Recent works in which various alternative\nsentence encoders are compared use a similar set of tasks\n\n“LIUM, Université Le Mans\n'See also recent workshops on evaluating representations for\nNLP, e.g. RepEval: https://repeval2017.github.io/\n\n(Hill et al., 2016a; Conneau et al., 2017).\n\nImplementing pipelines for this large set of evaluations,\neach with its own peculiarities, is cumbersome and in-\nduces unnecessary wheel reinventions. Another well-\nknown problem with the current status quo, where every-\none uses their own evaluation pipeline, is that different pre-\nprocessing schemes, evaluation architectures and hyperpa-\nrameters are used. The datasets are often small, meaning\nthat minor differences in evaluation setup may lead to very\ndifferent outcomes, which implies that results reported in\npapers are not always fully comparable.\n\nIn order to overcome these issues, we introduce SentEval?:\na toolkit that makes it easy to evaluate universal sentence\nrepresentation encoders on a large set of evaluation tasks\nthat has been established by community consensus.\n\n2. Aims\n\nThe aim of SentEval is to make research on universal sen-\ntence representations fairer, less cumbersome and more\ncentralized. To achieve this goal, SentEval encompasses\nthe following:\n\n* one central set of evaluations, based on what appears\nto be community consensus;\n\n* one common evaluation pipeline with fixed standard\nhyperparameters, apart from those tuned on validation\nsets, in order to avoid discrepancies in reported results;\nand\n\n* easy access for anyone, meaning: a straightforward\ninterface in Python, and scripts necessary to download\nand preprocess the relevant datasets.\n\nIn addition, we provide examples of models, such as a sim-\nple bag-of-words model. These could potentially also be\nused to extrinsically evaluate the quality of word embed-\ndings in NLP tasks.\n\n3. Evaluations\n\nOur aim is to obtain general-purpose sentence embeddings\nthat capture generic information, which should be useful\n\n*https://github.com/facebookresearch/SentEval\n\n1699\n", "vlm_text": "SentEval: An Evaluation Toolkit for Universal Sentence Representations \nAlexis Conneau ∗ and Douwe Kiela Facebook Artiﬁcial Intelligence Research { aconneau,dkiela }  $@$  fb.com \nAbstract \nWe introduce SentEval, a toolkit for evaluating the quality of universal sentence representations. SentEval encompasses a variety of tasks, including binary and multi-class classiﬁcation, natural language inference and sentence similarity. The set of tasks was selected based on what appears to be the community consensus regarding the appropriate evaluations for universal sentence representations. The toolkit comes with scripts to download and preprocess datasets, and an easy interface to evaluate sentence encoders. The aim is to provide a fairer, less cumbersome and more centralized way for evaluating sentence representations. \nKeywords:  representation learning, evaluation \n1. Introduction \nFollowing the recent word embedding upheaval, one of NLP’s next challenges has become the hunt for univer- sal general-purpose sentence representations. What distin- guishes these representations, or embeddings, is that they are not necessarily trained to perform well on one speciﬁc task. Rather, their value lies in their transferability, i.e., their ability to capture information that can be of use in any kind of system or pipeline, on a variety of tasks. \nWord embeddings are particularly useful in cases where there is limited training data, leading to sparsity and poor vocabulary coverage, which in turn lead to poor generaliza- tion capabilities. Similarly, sentence embeddings (which are often built on top of word embeddings) can be used to further increase generalization capabilities, composing unseen combinations of words and encoding grammatical constructions that are not present in the task-speciﬁc train- ing data. Hence, high-quality universal sentence represen- tations are highly desirable for a variety of downstream NLP tasks. \nThe evaluation of general-purpose word and sentence em- beddings has been problematic (Chiu et al., 2016; Faruqui et al., 2016), leading to much discussion about the best way to go about it 1 . On the one hand, people have measured performance on intrinsic evaluations, e.g. of human judg- ments of word or sentence similarity ratings (Agirre et al., 2012; Hill et al., 2016b) or of word associations (Vuli´ c et al., 2017). On the other hand, it has been argued that the focus should be on downstream tasks where these repre- sentations would actually be applied (Ettinger et al., 2016; Nayak et al., 2016). In the case of sentence representa- tions, there is a wide variety of evaluations available, many from before the “embedding era”, that can be used to as- sess representational quality on that particular task. Over the years, something of a consensus has been established, mostly based on the evaluations in seminal papers such as SkipThought (Kiros et al., 2015), concerning what evalu- ations to use. Recent works in which various alternative sentence encoders are compared use a similar set of tasks (Hill et al., 2016a; Conneau et al., 2017). \n\nImplementing pipelines for this large set of evaluations, each with its own peculiarities, is cumbersome and in- duces unnecessary wheel reinventions. Another well- known problem with the current status quo, where every- one uses their own evaluation pipeline, is that different pre- processing schemes, evaluation architectures and hyperpa- rameters are used. The datasets are often small, meaning that minor differences in evaluation setup may lead to very different outcomes, which implies that results reported in papers are not always fully comparable. \nIn order to overcome these issues, we introduce SentEval 2 : a toolkit that makes it easy to evaluate universal sentence representation encoders on a large set of evaluation tasks that has been established by community consensus. \n2. Aims \nThe aim of SentEval is to make research on universal sen- tence representations fairer, less cumbersome and more centralized. To achieve this goal, SentEval encompasses the following: \n• one central set of evaluations, based on what appears to be community consensus; • one common evaluation pipeline with ﬁxed standard hyperparameters, apart from those tuned on validation sets, in order to avoid discrepancies in reported results; and • easy access for anyone, meaning: a straightforward interface in Python, and scripts necessary to download and preprocess the relevant datasets. \nIn addition, we provide examples of models, such as a sim- ple bag-of-words model. These could potentially also be used to extrinsically evaluate the quality of word embed- dings in NLP tasks. \n3. Evaluations \nOur aim is to obtain general-purpose sentence embeddings that capture generic information, which should be useful "}
{"page": 1, "image_path": "doc_images/L18-1269_1.jpg", "ocr_text": "name N | task C | examples label(s)\nMR 11k | sentiment (movies) 2 | “Too slow for a younger crowd , too shallow for an older one.” neg\nCR 4k | product reviews 2 | “We tried it out christmas night and it worked great .” pos\nSUBJ 10k | subjectivity/objectivity | 2 | “A movie that doesn’t aim too high , but doesn’t need to.” subj\nMPQA | 11k | opinion polarity 2 | “don’t want”; “would like to tell’; neg, pos\nTREC 6k | question-type 6 | “What are the twin cities ?” LOC:city\nSST-2 70k | sentiment (movies) 2 | “Audrey Tautou has a knack for picking roles that magnify her [..]” pos\nSST-5 12k | sentiment (movies) 5 | “nothing about this movie works.” 0\nTable 1: Classification tasks. C is the number of classes and N is the number of samples.\nname N | task | output | premise hypothesis label\nSNLI 560k | NLI 3 “A small girl wearing a pink jacket | “The carousel is moving.” entailment\nis riding on a carousel.”\nSICK-E 10k | NLI 3 “A man is sitting on a chair and rub- | “There is no man sitting on a chair | contradiction\nbing his eyes” and rubbing his eyes”\nSICK-R 10k | STS (0, 5] “A man is singing a song and playing | “A man is opening a package that 1.6\nthe guitar” contains headphones”\nSTS14 4.5k | STS (0, 5] “Liquid ammonia leak kills 15 in | “Liquid ammonia leak kills at least 46\nShanghai” 15 in Shanghai”\nMRPC 5.7k | PD 2 “The procedure is generally per- | “The technique is used during paraphrase\nformed in the second or third | the second and, occasionally, third\ntrimester.” trimester of pregnancy.”\nCOCO 565k | ICR sim “A group of people on some horses rank\nriding through the beach.”\n\nTable 2: Natural Language Inference and Semantic Similarity tasks. NLI labels are contradiction, neutral and entail-\nment. STS labels are scores between 0 and 5. PD=paraphrase detection, ICR=image-caption retrieval.\n\nfor a broad set of tasks. To evaluate the quality of these\nrepresentations, we use them as features in various transfer\ntasks.\n\nBinary and multi-class classification We use a set of\nbinary classification tasks (see Table 1) that covers var-\nious types of sentence classification, including sentiment\nanalysis (MR and both binary and fine-grained SST) (Pang\nand Lee, 2005; Socher et al., 2013), question-type (TREC)\n(Voorhees and Tice, 2000), product reviews (CR) (Hu and\nLiu, 2004), subjectivity/objectivity (SUBJ) (Pang and Lee,\n2004) and opinion polarity (MPQA) (Wiebe et al., 2005).\nWe generate sentence vectors and classifier on top, either in\nthe form of a Logistic Regression or an MLP. For MR, CR,\nSUBJ and MPQA, we use nested 10-fold cross-validation,\nfor TREC cross-validation and for SST standard validation.\n\nEntailment and semantic relatedness We also include\nthe SICK dataset (Marelli et al., 2014) for entailment\n(SICK-E), and semantic relatedness datasets including\nSICK-R and the STS Benchmark dataset (Cer et al., 2017).\nFor semantic relatedness, which consists of predicting a se-\nmantic score between 0 and 5 from two input sentences, we\nfollow the approach of Tai et al. (2015a) and learn to pre-\ndict the probability distribution of relatedness scores. Sen-\ntEval reports Pearson and Spearman correlation. In addi-\ntion, we include the SNLI dataset (Bowman et al., 2015),\na collection of 570k human-written English supporting the\ntask of natural language inference (NLI), also known as rec-\n\n3Antonio Rivera - CC BY 2.0 - flickr\n\nognizing textual entailment (RTE) which consists of pre-\ndicting whether two input sentences are entailed, neutral\nor contradictory. SNLI was specifically designed to serve\nas a benchmark for evaluating text representation learning\nmethods.\n\nSemantic Textual Similarity While semantic related-\nness requires training a model on top of the sentence em-\nbeddings, we also evaluate embeddings on the unsuper-\nvised SemEval tasks. These datasets include pairs of sen-\ntences taken from news articles, forum discussions, news\nconversations, headlines, image and video descriptions la-\nbeled with a similarity score between 0 and 5. The goal is\nto evaluate how the cosine distance between two sentences\ncorrelate with a human-labeled similarity score through\nPearson and Spearman correlations. We include STS tasks\nfrom 2012 (Agirre et al., 2012), 2013+ (Agirre et al., 2013),\n2014 (Agirre et al., 2014), 2015 (Agirre et al., 2015) and\n2016 (Agirre et al., 2016). Each of these tasks includes\nseveral subtasks. SentEval reports both the average and the\nweighted average (by number of samples in each subtask)\nof the Pearson and Spearman correlations.\n\nParaphrase detection The Microsoft Research Para-\nphrase Corpus (MRPC) (Dolan et al., 2004) is composed\nof pairs of sentences which have been extracted from\nnews sources on the Web. Sentence pairs have been\nhuman-annotated according to whether they capture a para-\nphrase/semantic equivalence relationship. We use the same\n\n4Due to License issues, we do not include the SMT subtask.\n\n1700\n", "vlm_text": "This table provides a summary of various text classification tasks and datasets. Here is a breakdown of the columns in the table:\n\n- **name**: The name or abbreviation of the dataset.\n- **N**: The number of samples in the dataset, indicated in thousands (e.g., \"11k\" means 11,000 samples).\n- **task**: The type of task or application the dataset is used for. Examples include sentiment analysis, product reviews, subjectivity/objectivity classification, opinion polarity, and question type.\n- **C**: The number of classes or labels associated with the task.\n- **examples**: Sample sentences or phrases that are representative of the data found in each dataset.\n- **label(s)**: The labels or categories that the examples fall into, such as \"neg\" (negative), \"pos\" (positive), \"subj\" (subjective), or specific classification categories like \"LOC:city\" (location: city). \n\nThis table showcases different natural language processing tasks along with examples, the number of classes, and possible labels for the data within those tasks.\nThe table lists several datasets that are commonly used for tasks involving natural language processing (NLP) and image-captioning. Here's a breakdown of what each column represents:\n\n- **name**: The name of the dataset.\n  - SNLI\n  - SICK-E\n  - SICK-R\n  - STS14\n  - MRPC\n  - COCO \n\n- **N**: The size of the dataset in number of examples, with 'k' representing thousands.\n  - SNLI: 560k\n  - SICK-E: 10k\n  - SICK-R: 10k\n  - STS14: 4.5k\n  - MRPC: 5.7k\n  - COCO: 565k\n\n- **task**: The type of task for which the dataset is used.\n  - SNLI: NLI (Natural Language Inference)\n  - SICK-E: NLI\n  - SICK-R: STS (Semantic Textual Similarity)\n  - STS14: STS\n  - MRPC: PD (Paraphrase Detection)\n  - COCO: ICR (Image-Caption Retrieval)\n\n- **output**: The kind of output associated with the dataset.\n  - SNLI: 3 (3 possible outcomes—entailment, contradiction, neutral)\n  - SICK-E: 3\n  - SICK-R: [0, 5] (a score range assessing similarity)\n  - STS14: [0, 5]\n  - MRPC: 2 (binary outcome—paraphrase or not)\n  - COCO: sim (similarity score)\n\n- **premise**: An example of a premise from the dataset. This can be a sentence or an image.\n  - Varied examples provided.\n\n- **hypothesis**: An example of a hypothesis related to the premise, which also varies by dataset.\n  - Varied examples provided.\n\n- **label**: The expected label for the pair of premise and hypothesis, indicating the relationship between them or their similarity.\n  - SNLI: entailment\n  - SICK-E: contradiction\n  - SICK-R: 1.6\n  - STS14: 4.6\n  - MRPC: paraphrase\n  - COCO: rank (this usually relates to the relevance or rank for retrieval tasks)\n\nIn summary, each row details a different dataset, highlighting its size, the task it is meant to solve, the typical output it generates, and examples of data entries, along with labels that signify the relationships or similarity between the entries.\nfor a broad set of tasks. To evaluate the quality of these representations, we use them as features in various transfer tasks. \nBinary and multi-class classiﬁcation We use a set of binary classiﬁcation tasks (see Table 1) that covers var- ious types of sentence classiﬁcation, including sentiment analysis (MR and both binary and ﬁne-grained SST) (Pang and Lee, 2005; Socher et al., 2013), question-type (TREC) (Voorhees and Tice, 2000), product reviews (CR) (Hu and Liu, 2004), subjectivity/objectivity (SUBJ) (Pang and Lee, 2004) and opinion polarity (MPQA) (Wiebe et al., 2005). We generate sentence vectors and classiﬁer on top, either in the form of a Logistic Regression or an MLP. For MR, CR, SUBJ and MPQA, we use nested 10-fold cross-validation, for TREC cross-validation and for SST standard validation. \nEntailment and semantic relatedness We also include the SICK dataset (Marelli et al., 2014) for entailment (SICK-E), and semantic relatedness datasets including SICK-R and the STS Benchmark dataset (Cer et al., 2017). For semantic relatedness, which consists of predicting a se- mantic score between 0 and 5 from two input sentences, we follow the approach of Tai et al. (2015a) and learn to pre- dict the probability distribution of relatedness scores. Sen- tEval reports Pearson and Spearman correlation. In addi- tion, we include the SNLI dataset (Bowman et al., 2015), a collection of 570k human-written English supporting the task of natural language inference (NLI), also known as rec- ognizing textual entailment (RTE) which consists of pre- dicting whether two input sentences are entailed, neutral or contradictory. SNLI was speciﬁcally designed to serve as a benchmark for evaluating text representation learning methods. \n\nSemantic Textual Similarity While semantic related- ness requires training a model on top of the sentence em- beddings, we also evaluate embeddings on the unsuper- vised SemEval tasks. These datasets include pairs of sen- tences taken from news articles, forum discussions, news conversations, headlines, image and video descriptions la- beled with a similarity score between 0 and 5. The goal is to evaluate how the cosine distance between two sentences correlate with a human-labeled similarity score through Pearson and Spearman correlations. We include STS tasks from 2012 (Agirre et al., 2012),  $2013^{4}$    (Agirre et al., 2013), 2014 (Agirre et al., 2014), 2015 (Agirre et al., 2015) and 2016 (Agirre et al., 2016). Each of these tasks includes several subtasks. SentEval reports both the average and the weighted average (by number of samples in each subtask) of the Pearson and Spearman correlations. \nParaphrase detection The Microsoft Research Para- phrase Corpus (MRPC) (Dolan et al., 2004) is composed of pairs of sentences which have been extracted from news sources on the Web. Sentence pairs have been human-annotated according to whether they capture a para- phrase/semantic equivalence relationship. We use the same approach as with SICK-E, except that our classiﬁer has only 2 classes, i.e., the aim is to predict whether the sentences are paraphrases or not. "}
{"page": 2, "image_path": "doc_images/L18-1269_2.jpg", "ocr_text": "approach as with SICK-E, except that our classifier has only\n2 classes, i.e., the aim is to predict whether the sentences are\nparaphrases or not.\n\nCaption-Image retrieval The caption-image retrieval\ntask evaluates joint image and language feature models (Lin\net al., 2014). The goal is either to rank a large collection\nof images by their relevance with respect to a given query\ncaption (Image Retrieval), or ranking captions by their rel-\nevance for a given query image (Caption Retrieval). The\nCOCO dataset provides a training set of 113k images with 5\ncaptions each. The objective consists of learning a caption-\nimage compatibility score Leir(a, y) from a set of aligned\nimage-caption pairs as training data. We use a pairwise\nranking-loss Leir(a, y):\n\nS > max(0,a — s(Vy, Ux) + s(Vy, Urp)) +\n\ny k\n\nS > max(0,a — s(Ux, Vy) + s(Ux,Vyx)),\nx2 kt\n\nwhere (x,y) consists of an image y with one of its asso-\nciated captions x, (yx)x and (yx), are negative examples\nof the ranking loss, a is the margin and s corresponds to\nthe cosine similarity. U and V are learned linear trans-\nformations that project the caption x and the image y to\nthe same embedding space. We measure Recall@K, with\nK € {1,5, 10}, ie., the percentage of images/captions for\nwhich the corresponding caption/image is one of the first\nK retrieved; and median rank. We use the same splits\nas Karpathy and Fei-Fei (2015), i.e., we use 113k images\n(each containing 5 captions) for training, 5k images for\nvalidation and 5k images for test. For evaluation, we split\nthe 5k images in 5 random sets of 1k images on which we\ncompute the mean R@Q1, R@5, R@10 and median (Med r)\nover the 5 splits. We include 2048-dimensional pretrained\nResNet-101 (He et al., 2016) features for all images.\n\n4. Usage and Requirements\n\nOur evaluations comprise two different types: ones where\nwe need to learn on top of the provided sentence represen-\ntations (e.g. classification/regression) and ones where we\nsimply take the cosine similarity between the two represen-\ntations, as in the STS tasks. In the binary and multi-class\nclassification tasks, we fit either a Logistic Regression clas-\nsifier or an MLP with one hidden layer on top of the sen-\ntence representations. For the natural language inference\ntasks, where we are given two sentences u and v, we pro-\nvide the classifier with the input (u, v, |w—v|, u* v). To fit\nthe Pytorch models, we use Adam (Kingma and Ba, 2014),\nwith a batch size 64. We tune the L2 penalty of the classifier\nwith grid-search on the validation set. When using Sent-\nEval, two functions should be implemented by the user:\n\n* prepare (params, dataset): sees the whole\ndataset and applies any necessary preprocessing, such\nas constructing a lookup table of word embeddings\n(this function is optional); and\n\n* batcher (params, batch): given a batch of in-\nput sentences, returns an array of the sentence embed-\ndings for the respective inputs.\n\nThe main bat cher function allows the user to encode text\nsentences using any Python framework. For example, the\nbatcher function might be a wrapper around a model writ-\nten in Pytorch, TensorFlow, Theano, DyNet, or any other\nframework>. To illustrate the use, here is an example of\nwhat an evaluation script looks like, having defined the pre-\npare and batcher functions:\n\nimport senteval\nse = senteval.engine.SE(\n\nparams, batcher, prepare)\ntransfer_tasks = ['MR', 'CR']\nresults = se.eval(transfer_tasks)\n\nParameters Both functions make use of a params ob-\nject, which contains the settings of the network and the\nevaluation. SentEval has several parameters that influence\nthe evaluation procedure. These include the following:\n\n* task_path (str, required): path to the data.\n* seed (int): random seed for reproducibility.\n\n* batch_size (int): size of minibatch of text sen-\ntences provided to batcher (sentences are sorted by\nlength).\n\n¢ kfold (int): k in the kfold-validation (default: 10).\nThe default config is:\n\nparams = {'task_path':\n\"usepytorch!:\n\n\"kfold': 10}\n\nPATH_TO_DATA,\nTrue,\n\nWe also give the user the ability to customize the classifier\nused for the classification tasks.\n\nClassifier To be comparable to the results published in\nthe literature, users should use the following parameters for\nLogistic Regression:\n\nparams['classifier'] =\n{'nhid': 0, 'optim': 'adam',\n\"batch_size': 64, 'tenacity': 5,\n\"epoch_size': 4}\n\nThe parameters of the classifier include:\n\n¢ nhid (int): number of hidden units of the MLP; if\nnhid> 0, a Multi-Layer Perceptron with one hidden\nlayer and a Sigmoid nonlinearity is used.\n\n* optim (str): classifier optimizer (default: adam).\n\n* batch_size (int): batch size for training the classi-\nfier (default: 64).\n\n* tenacity (int): stopping criterion; maximum num-\nber of times the validation error does not decrease.\n\n* epoch_size (int): number of passes through the\ntraining set for one epoch.\n\n5Or any other programming language, as long as the vectors\ncan be passed to, or loaded from, code written in Python.\n\n1701\n", "vlm_text": "\nCaption-Image retrieval The caption-image retrieval task evaluates joint image and language feature models (Lin et al., 2014). The goal is either to rank a large collection of images by their relevance with respect to a given query caption (Image Retrieval), or ranking captions by their rel- evance for a given query image (Caption Retrieval). The COCO dataset provides a training set of   $113\\mathrm{k}$   images with 5 captions each. The objective consists of learning a caption- image compatibility score    $\\mathcal{L}_{\\mathrm{{circ}}}(x,y)$   from a set of aligned image-caption pairs as training data. We use a pairwise ranking-loss    $\\mathcal{L}_{\\mathrm{{circ}}}(x,y)$  : \n\n$$\n\\sum_{y}\\sum_{k}\\operatorname*{max}(0,\\alpha-s(V y,U x)+s(V y,U x_{k}))+\n$$\n \n\n$$\n\\sum_{x}\\sum_{k^{\\prime}}\\operatorname*{max}(0,\\alpha-s(U x,V y)+s(U x,V y_{k^{\\prime}})),\n$$\n \nwhere    $(x,y)$   consists of an image    $y$   with one of its asso- ciated captions  $x$  ,    $(y_{k})_{k}$   and    $(y_{k^{\\prime}})_{k^{\\prime}}$   are negative examples of the ranking loss,    $\\alpha$   is the margin and    $s$   corresponds to the cosine similarity.    $U$   and    $V$   are learned linear trans- formations that project the caption    $x$   and the image    $y$   to the same embedding space. We measure Recall  $@\\mathbf{K}$  , with  $\\mathbf{K}\\in\\{1,5,10\\}$  , i.e., the percentage of images/captions for which the corresponding caption/image is one of the ﬁrst  $\\mathbf{K}$   retrieved; and median rank. We use the same splits as Karpathy and Fei-Fei (2015), i.e., we use   $113\\mathrm{k}$   images (each containing 5 captions) for training,   $5\\mathbf{k}$   images for validation and   $5\\mathrm{k}$   images for test. For evaluation, we split the  $5\\mathrm{k}$   images in 5 random sets of 1k images on which we compute the mean    $R@1,R@5,R@10$   and median (Med r) over the 5 splits. We include 2048-dimensional pretrained ResNet-101 (He et al., 2016) features for all images. \n4. Usage and Requirements \nOur evaluations comprise two different types: ones where we need to learn on top of the provided sentence represen- tations (e.g. classiﬁcation/regression) and ones where we simply take the cosine similarity between the two represen- tations, as in the STS tasks. In the binary and multi-class classiﬁcation tasks, we ﬁt either a Logistic Regression clas- siﬁer or an MLP with one hidden layer on top of the sen- tence representations. For the natural language inference tasks, where we are given two sentences    $u$   and    $v$  , we pro- vide the classiﬁer with the input    $\\langle u,v,|u-v|,u\\ast v\\rangle$  . To ﬁt the Pytorch models, we use Adam (Kingma and Ba, 2014), with a batch size 64. We tune the L2 penalty of the classiﬁer with grid-search on the validation set. When using Sent- Eval, two functions should be implemented by the user: \n•  prepare(params, dataset) : sees the whole dataset and applies any necessary preprocessing, such as constructing a lookup table of word embeddings (this function is optional); and \n•  batcher(params, batch)  $:$   given a batch of in- put sentences, returns an array of the sentence embed- dings for the respective inputs. \nThe main  batcher  function allows the user to encode text sentences using any Python framework. For example, the batcher function might be a wrapper around a model writ- ten in Pytorch, TensorFlow, Theano, DyNet, or any other framework 5 . To illustrate the use, here is an example of what an evaluation script looks like, having deﬁned the pre- pare and batcher functions: \nimport senteval  $\\begin{array}{r l}{S\\ominus}&{{}=}\\end{array}$   senteval.engine.SE( params, batcher, prepare) transfer_tasks   $=$   ['MR', 'CR'] results   $=$   se.eval(transfer_tasks) \nParameters Both functions make use of a  params  ob- ject, which contains the settings of the network and the evaluation. SentEval has several parameters that inﬂuence the evaluation procedure. These include the following: \n•  task path  (str, required): path to the data. •  seed  (int): random seed for reproducibility. •  batch size  (int): size of minibatch of text sen- tences provided to batcher (sentences are sorted by length). •  kfold  (int): k in the kfold-validation (default: 10). \nThe default conﬁg is: \nWe also give the user the ability to customize the classiﬁer used for the classiﬁcation tasks. \nClassiﬁer To be comparable to the results published in the literature, users should use the following parameters for Logistic Regression: \nThe parameters of the classiﬁer include: \n•  nhid  (int): number of hidden units of the MLP; if nhid  $>\\,0$  , a Multi-Layer Perceptron with one hidden layer and a Sigmoid nonlinearity is used. •  optim  (str): classiﬁer optimizer (default: adam). •  batch size  (int): batch size for training the classi- ﬁer (default: 64). •  tenacity  (int): stopping criterion; maximum num- ber of times the validation error does not decrease. •  epoch size  (int): number of passes through the training set for one epoch. \n $^{5}\\mathrm{{Pr}}$   any other programming language, as long as the vectors can be passed to, or loaded from, code written in Python. "}
{"page": 3, "image_path": "doc_images/L18-1269_3.jpg", "ocr_text": "Model MR CR SUBJ MPQA_ SST-2 SST-5 TREC MRPC SICK-E\nRepresentation learning (transfer)\n\nGloVe LogReg | 774 78.7 91.2 87.7 80.3 44.7 83.0 72.7/81.0 78.5\n\nGloVe MLP 71.7 79.9 92.2 88.7 82.3 45.4 85.2 73.0/80.9 79.0\n\nfastText LogReg| 78.2 80.2 91.8 88.0 82.3 45.1 83.4 74.4/82.4 78.9\n\nfastText MLP 78.0 814 92.9 88.5 84.0 45.1 85.6 74.4/82.3 80.2\n\nSkipThought 794 83.1 93.7 89.3 82.9 - 88.4 72.4/81.6 79.5\n\nInferSent 81.1 86.3 92.4 90.2 84.6 46.3 88.2 76.2/83.1 86.3\nSupervised methods directly trained for each task (no transfer)\n\nSOTA 83.17 86.3! 95.5! 93.3! 89.52 52.47 96.1? 80.4/85.93 84.54\n\nTable 3: Transfer test results for various baseline methods. We include supervised results trained directly on each task (no\ntransfer). Results ! correspond to AdaSent (Zhao et al., 2015), to BLSTM-2DCNN (Zhou et al., 2016), ? to TF-KLD (Ji\nand Eisenstein, 2013) and ‘ to Illinois-LH system (Lai and Hockenmaier, 2014).\n\n* dropout (float): dropout rate in the case of MLP.\n\nFor use cases where there are multiple calls to SentEval,\ne.g when evaluating the sentence encoder at every epoch of\ntraining, we propose the following prototyping set of pa-\nrameters, which will lead to slightly worse results but will\nmake the evaluation significantly faster:\n\nparams['classifier'] =\n\n{'nhid': 0, 'optim': 'rmsprop',\n\"batch_size': 128, 'tenacity': 3,\n\"epoch_size': 2}\n\nYou may also pass additional parameters to the params\nobject in order which will further be accessible from the\nprepare and batcher functions (e.g a pretrained model).\n\nDatasets In order to obtain the data and preprocess\nit so that it can be fed into SentEval, we provide the\nget_-transfer_data.bash script in the data directory.\nThe script fetches the different datasets from their known\nlocations, unpacks them and preprocesses them. We to-\nkenize each of the datasets with the MOSES tokenizer\n(Koehn et al., 2007) and convert all files to UTF-8 encod-\ning. Once this script has been executed, the task_path pa-\nrameter can be set to indicate the path of the data directory.\n\nRequirements SentEval is written in Python. In order\nto run the evaluations, the user will need to install numpy,\nscipy and recent versions of pytorch and scikit-learn. In\norder to facilitate research where no GPUs are available,\nwe offer for the evaluations to be run on CPU (using scikit-\nlearn) where possible. For the bigger datasets, where more\ncomplicated models are often required, for instance STS\nBenchmark, SNLI, SICK-R and the image-caption retrieval\ntasks, we recommend pytorch models on a single GPU.\n\n5. Baselines\n\nSeveral baseline models are evaluated in Table 3:\n\n* Continuous bag-of-words embeddings (average of\nword vectors). We consider the most commonly used\npretrained word vectors available, namely the fastText\n(Mikolov et al., 2017) and the GloVe (Pennington et\nal., 2014) vectors trained on CommonCrawl.\n\n¢ SkipThought vectors (Ba et al., 2016)\n\n¢ InferSent vectors (Conneau et al., 2017)\n\nIn addition to these methods, we include the results of cur-\nrent state-of-the-art methods for which both the encoder\nand the classifier are trained on each task (no transfer). For\nGloVe and fastText bag-of-words representations, we re-\nport the results for Logistic Regression and Multi-Layer\nPerceptron (MLP). For the MLP classifier, we tune the\ndropout rate and the number of hidden units in addition\nto the L2 regularization. We do not observe any improve-\nment over Logistic Regression for methods that already\nhave a large embedding size (4096 for Infersent and 4800\nfor SkipThought). On most transfer tasks, supervised meth-\nods that are trained directly on each task still outperform\ntransfer methods. Our hope is that SentEval will help the\ncommunity build sentence representations with better gen-\neralization power that can outperform both the transfer and\nthe supervised methods.\n\n6. Conclusion\n\nUniversal sentence representations are a hot topic in NLP\nresearch. Making use of a generic sentence encoder allows\nmodels to generalize and transfer better, even when trained\non relatively small datasets, which makes them highly de-\nsirable for downstream NLP tasks.\n\nWe introduced SentEval as a fair, straightforward and cen-\ntralized toolkit for evaluating sentence representations. We\nhave aimed to make evaluation as easy as possible: sen-\ntence encoders can be evaluated by implementing a simple\nPython interface, and we provide a script to download the\nnecessary evaluation datasets. In future work, we plan to\nenrich SentEval with additional tasks as the consensus on\nthe best evaluation for sentence embeddings evolves. In\nparticular, tasks that probe for specific linguistic properties\nof the sentence embeddings (Shi et al., 2016; Adi et al.,\n2017) are interesting directions towards understanding how\nthe encoder understands language. We hope that our toolkit\nwill be used by the community in order to ensure that fully\ncomparable results are published in research papers.\n\n1702\n", "vlm_text": "The table presents the performance of various models on several NLP benchmark datasets. The table is divided into two main sections: \"Representation learning (transfer)\" and \"Supervised methods directly trained for each task (no transfer).\" \n\n**Columns and the tasks they represent:**\n- **MR, CR, SUBJ, MPQA**: These are datasets for various NLP tasks, such as sentiment analysis and subjectivity/objectivity classification.\n- **SST-2, SST-5**: These are sentiment analysis tasks from the Stanford Sentiment Treebank, with SST-2 being binary and SST-5 being fine-grained.\n- **TREC**: A task for question classification.\n- **MRPC**: The Microsoft Research Paraphrase Corpus for evaluating paraphrase identification, with results shown as accuracy/f1-score.\n- **SICK-E**: The Semantic Textual Similarity task focused on entailment.\n\n**Rows for \"Representation learning (transfer)\":**\n1. **GloVe LogReg**: Uses GloVe embeddings with logistic regression.\n2. **GloVe MLP**: Uses GloVe embeddings with a multilayer perceptron.\n3. **fastText LogReg**: Uses fastText embeddings with logistic regression.\n4. **fastText MLP**: Uses fastText embeddings with a multilayer perceptron.\n5. **SkipThought**: A model based on Skip-Thought vectors.\n6. **InferSent**: A sentence embedding model that shows the best transfer learning results in this section.\n\n**Row for \"Supervised methods directly trained for each task (no transfer)\"**:\n- **SOTA**: State-of-the-art models trained specifically for each dataset, which provide a benchmark for performance without transfer learning.\n\nThe numbers in the cells represent specific performance metrics (likely accuracy or F1 scores) for each model on the respective tasks. The superscripts (e.g., ¹, ², ³, ⁴) may refer to footnotes not visible here, possibly indicating references or specific configurations. Generally, supervised models that are trained directly on each task outperform models using transfer learning on those tasks, although InferSent performs strongly across various tasks in the representation learning section.\n•  dropout  (ﬂoat): dropout rate in the case of MLP. \nFor use cases where there are multiple calls to SentEval, e.g when evaluating the sentence encoder at every epoch of training, we propose the following prototyping set of pa- rameters, which will lead to slightly worse results but will make the evaluation signiﬁcantly faster: \nparams['classifier']   $=$  {'nhid': 0, 'optim': 'rmsprop', 'batch_size': 128, 'tenacity': 3, 'epoch_size': 2} \nYou may also pass additional parameters to the  params object in order which will further be accessible from the prepare and batcher functions (e.g a pretrained model). \nDatasets In order to obtain the data and preprocess it so that it can be fed into SentEval, we provide the get transfer data.bash  script in the data directory. The script fetches the different datasets from their known locations, unpacks them and preprocesses them. We to- kenize each of the datasets with the MOSES tokenizer (Koehn et al., 2007) and convert all ﬁles to UTF-8 encod- ing. Once this script has been executed, the task path pa- rameter can be set to indicate the path of the data directory. \nRequirements SentEval is written in Python. In order to run the evaluations, the user will need to install numpy, scipy and recent versions of pytorch and scikit-learn. In order to facilitate research where no GPUs are available, we offer for the evaluations to be run on CPU (using scikit- learn) where possible. For the bigger datasets, where more complicated models are often required, for instance STS Benchmark, SNLI, SICK-R and the image-caption retrieval tasks, we recommend pytorch models on a single GPU. \n5. Baselines \nSeveral baseline models are evaluated in Table 3: \n• Continuous bag-of-words embeddings (average of word vectors). We consider the most commonly used pretrained word vectors available, namely the fastText (Mikolov et al., 2017) and the GloVe (Pennington et al., 2014) vectors trained on CommonCrawl. \n• SkipThought vectors (Ba et al., 2016) \n• InferSent vectors (Conneau et al., 2017) \nIn addition to these methods, we include the results of cur- rent state-of-the-art methods for which both the encoder and the classiﬁer are trained on each task (no transfer). For GloVe and fastText bag-of-words representations, we re- port the results for Logistic Regression and Multi-Layer Perceptron (MLP). For the MLP classiﬁer, we tune the dropout rate and the number of hidden units in addition to the L2 regularization. We do not observe any improve- ment over Logistic Regression for methods that already have a large embedding size (4096 for Infersent and 4800 for SkipThought). On most transfer tasks, supervised meth- ods that are trained directly on each task still outperform transfer methods. Our hope is that SentEval will help the community build sentence representations with better gen- eralization power that can outperform both the transfer and the supervised methods. \n6. Conclusion \nUniversal sentence representations are a hot topic in NLP research. Making use of a generic sentence encoder allows models to generalize and transfer better, even when trained on relatively small datasets, which makes them highly de- sirable for downstream NLP tasks. \nWe introduced SentEval as a fair, straightforward and cen- tralized toolkit for evaluating sentence representations. We have aimed to make evaluation as easy as possible: sen- tence encoders can be evaluated by implementing a simple Python interface, and we provide a script to download the necessary evaluation datasets. In future work, we plan to enrich SentEval with additional tasks as the consensus on the best evaluation for sentence embeddings evolves. In particular, tasks that probe for speciﬁc linguistic properties of the sentence embeddings (Shi et al., 2016; Adi et al., 2017) are interesting directions towards understanding how the encoder understands language. We hope that our toolkit will be used by the community in order to ensure that fully comparable results are published in research papers. "}
{"page": 4, "image_path": "doc_images/L18-1269_4.jpg", "ocr_text": "Model\n\nSST’?12. SST’13_ SST’14_ _SST’15 SST’16 SICK-R_ SST-B\n\nRepresentation learning (transfer)\n\nGloVe BoW 52.1 49.6 54.6 56.1 51.4 79.9 64.7\n\nfastText BoW 58.3 57.9 64.9 67.6 64.3 82.0 70.2\n\nSkipThought-LN| 30.8 24.8 31.4 31.0 - 85.8 72.1\n\nInferSent 59.2 58.9 69.6 71.3 71.5 88.3 75.6\n\nChar-phrase 66.1 57.2 TAT 76.1 - - -\nSupervised methods directly trained for each task (no transfer)\n\nPP-Proj 60.0! 56.8! 71.3! 74.8! - 86.87 -\n\nTable 4: Evaluation of sentence representations on the semantic textual similarity benchmarks. Numbers reported are\nPearson correlations x100. We use the average of Pearson correlations for STS’12 to STS’16 which are composed of\nseveral subtasks. Charagram-phrase numbers were taken from (Wieting et al., 2016). Results ! correspond to PP-Proj\n(Wieting et al., 2015) and ? from Tree-LSTM (Tai et al., 2015b).\n\n7. Bibliographical References\n\nAdi, Y., Kermany, E., Belinkov, Y., Lavi, O., and Goldberg,\nY. (2017). Fine-grained analysis of sentence embed-\ndings using auxiliary prediction tasks. In Proceedings\nof ICLR Conference Track, Toulon, France. Published\nonline: https://openreview.net/group?id=\nICLR.cc/2017/conference.\n\nAgirre, E., Diab, M., Cer, D., and Gonzalez-Agirre, A.\n(2012). Semeval-2012 task 6: A pilot on semantic tex-\ntual similarity. In Proceedings of Semeval-2012, pages\n385-393.\n\nAgirre, E., Cer, D., Diab, M., Gonzalez-agirre, A., and\nGuo, W. (2013). sem 2013 shared task: Semantic tex-\ntual similarity, including a pilot on typed-similarity. In\nIn *SEM 2013: The Second Joint Conference on Lexical\nand Computational Semantics. Association for Compu-\ntational Linguistics.\n\nAgirre, E., Baneab, C., Cardiec, C., Cerd, D., Diabe, M.,\nGonzalez-Agirre, A., Guof, W., Mihalceab, R., Rigaua,\nG., and Wiebeg, J. (2014). Semeval-2014 task 10:\nMultilingual semantic textual similarity. SemEval 2014,\npage 81.\n\nAgirre, E., Banea, C., Cardie, C., Cer, D. M., Diab, M. T.,\nGonzalez-Agirre, A., Guo, W., Lopez-Gazpio, I., Mar-\nitxalar, M., Mihalcea, R., et al. (2015). Semeval-2015\ntask 2: Semantic textual similarity, english, spanish and\npilot on interpretability. In SemEval@ NAACL-HLT,\npages 252-263.\n\nAgirre, E., Baneab, C., Cerd, D., Diabe, M., Gonzalez-\nAgirre, A., Mihalceab, R., Rigaua, G., Wiebef, J., and\nDonostia, B. C. (2016). Semeval-2016 task 1: Semantic\ntextual similarity, monolingual and cross-lingual evalua-\ntion. Proceedings of SemEval, pages 497-511.\n\nBa, J. L., Kiros, J. R., and Hinton, G. E. (2016). Layer nor-\nmalization. Advances in neural information processing\nsystems (NIPS).\n\nBowman, S. R., Angeli, G., Potts, C., and Manning, C. D.\n(2015). A large annotated corpus for learning natural\nlanguage inference. In Proceedings of EMNLP.\n\nCer, D., Diab, M., Agirre, E., Lopez-Gazpio, L., and Spe-\ncia, L. (2017). Semeval-2017 task 1: Semantic textual\nsimilarity-multilingual and cross-lingual focused evalua-\n\ntion. arXiv preprint arXiv: 1708.00055.\n\nChiu, B., Korhonen, A., and Pyysalo, S. (2016). Intrinsic\nevaluation of word vectors fails to predict extrinsic per-\nformance. In First Workshop on Evaluating Vector Space\nRepresentations for NLP (RepEval).\n\nConneau, A., Kiela, D., Schwenk, H., Barrault, L., and\nBordes, A. (2017). Supervised learning of universal\nsentence representations from natural language inference\ndata. In Proceedings of EMNLP, Copenhagen, Denmark.\n\nDolan, B., Quirk, C., and Brockett, C. (2004). Unsuper-\nvised construction of large paraphrase corpora: Exploit-\ning massively parallel news sources. In Proceedings of\nACL, page 350.\n\nEttinger, A., Elgohary, A., and Resnik, P. (2016). Probing\nfor semantic evidence of composition by means of sim-\nple classification tasks. In First Workshop on Evaluating\nVector Space Representations for NLP (RepEval), page\n134.\n\nFaruqui, M., Tsvetkov, Y., Rastogi, P., and Dyer, C. (2016).\nProblems with evaluation of word embeddings using\nword similarity tasks. arXiv preprint arXiv: 1605.02276.\n\nHe, K., Zhang, X., Ren, S., and Sun, J. (2016). Deep resid-\nual learning for image recognition. In Proceedings of\nCVPR.\n\nHill, F, Cho, K., and Korhonen, A. (2016a). Learning\ndistributed representations of sentences from unlabelled\ndata. In Proceedings of NAACL.\n\nHill, F, Reichart, R., and Korhonen, A. (2016b). Simlex-\n999: Evaluating semantic models with (genuine) similar-\nity estimation. Computational Linguistics.\n\nHu, M. and Liu, B. (2004). Mining and summarizing cus-\ntomer reviews. In Proceedings of SIGKDD, pages 168—\n177.\n\nJi, Y. and Eisenstein, J. (2013). Discriminative improve-\nments to distributional sentence similarity. In Proceed-\nings of the 2013 Conference on Empirical Methods in\nNatural Language Processing (EMNLP).\n\nKarpathy, A. and Fei-Fei, L. (2015). Deep visual-semantic\nalignments for generating image descriptions. In Pro-\nceedings of CVPR, pages 3128-3137.\n\nKingma, D. P. and Ba, J. (2014). Adam: A method\nfor stochastic optimization. In Proceedings of the 3rd\n\n1703\n", "vlm_text": "The table presents the performance of different models on various datasets concerning representation learning (transfer). The datasets are denoted by column headers: SST'12, SST'13, SST'14, SST'15, SST'16, SICK-R, and SST-B. The models compared in these columns are listed in the first column: GloVe BoW, fastText BoW, SkipThought-LN, InferSent, and Char-phrase. The numbers in the table represent performance metrics (likely accuracy or another evaluation metric) for each model on each dataset. Here's a brief overview of the results:\n\n- GloVe BoW: Performance ranges from 49.6 on SST'13 to 79.9 on SICK-R.\n- fastText BoW: Performance ranges from 57.9 on SST'13 to 82.0 on SICK-R.\n- SkipThought-LN: Performance ranges from 24.8 on SST'13 to 85.8 on SICK-R. It has missing data for SST'16.\n- InferSent: Performance ranges from 58.9 on SST'13 to 88.3 on SICK-R.\n- Char-phrase: Performance ranges from 57.2 on SST'13 to 76.1 on SST'15. It has missing data for SST'16, SICK-R, and SST-B.\n\nOverall, InferSent performs best on most datasets, particularly on SICK-R, while SkipThought-LN performs the worst on SST'13.\nTable 4: Evaluation of sentence representations on the semantic textual similarity benchmarks. Numbers reported are Pearson correlations  $_{x100}$  . We use the average of Pearson correlations for STS’12 to STS’16 which are composed of several subtasks. Charagram-phrase numbers were taken from (Wieting et al., 2016). Results   1   correspond to PP-Proj (Wieting et al., 2015) and   2   from Tree-LSTM (Tai et al., 2015b). \n7. Bibliographical References \nAdi, Y., Kermany, E., Belinkov, Y., Lavi, O., and Goldberg, Y. (2017). Fine-grained analysis of sentence embed- dings using auxiliary prediction tasks. In  Proceedings of ICLR Conference Track , Toulon, France. Published online:  https://openreview.net/group?id  $=$  ICLR.cc/2017/conference . Agirre, E., Diab, M., Cer, D., and Gonzalez-Agirre, A. (2012). Semeval-2012 task 6: A pilot on semantic tex- tual similarity. In  Proceedings of Semeval-2012 , pages 385–393. Agirre, E., Cer, D., Diab, M., Gonzalez-agirre, A., and Guo, W. (2013). sem 2013 shared task: Semantic tex- tual similarity, including a pilot on typed-similarity. In In \\*SEM 2013: The Second Joint Conference on Lexical and Computational Semantics. Association for Compu- tational Linguistics . Agirre, E., Baneab, C., Cardiec, C., Cerd, D., Diabe, M., Gonzalez-Agirre, A., Guof, W., Mihalceab, R., Rigaua, G., and Wiebeg, J. (2014). Semeval-2014 task 10: Multilingual semantic textual similarity.  SemEval 2014 , page 81. Agirre, E., Banea, C., Cardie, C., Cer, D. M., Diab, M. T., Gonzalez-Agirre, A., Guo, W., Lopez-Gazpio, I., Mar- itxalar, M., Mihalcea, R., et al. (2015). Semeval-2015 task 2: Semantic textual similarity, english, spanish and pilot on interpret ability. In  SemEval@ NAACL-HLT , pages 252–263. Agirre, E., Baneab, C., Cerd, D., Diabe, M., Gonzalez- Agirre, A., Mihalceab, R., Rigaua, G., Wiebef, J., and Donostia, B. C. (2016). Semeval-2016 task 1: Semantic textual similarity, monolingual and cross-lingual evalua- tion.  Proceedings of SemEval , pages 497–511. Ba, J. L., Kiros, J. R., and Hinton, G. E. (2016). Layer nor- malization.  Advances in neural information processing systems (NIPS) . Bowman, S. R., Angeli, G., Potts, C., and Manning, C. D. (2015). A large annotated corpus for learning natural language inference. In  Proceedings of EMNLP . Cer, D., Diab, M., Agirre, E., Lopez-Gazpio, I., and Spe- cia, L. (2017). Semeval-2017 task 1: Semantic textual similarity-multilingual and cross-lingual focused evalua- \n\nChiu, B., Korhonen, A., and Pyysalo, S. (2016). Intrinsic evaluation of word vectors fails to predict extrinsic per- formance. In  First Workshop on Evaluating Vector Space Representations for NLP (RepEval) . Conneau, A., Kiela, D., Schwenk, H., Barrault, L., and Bordes, A. (2017). Supervised learning of universal sentence representations from natural language inference data. In  Proceedings of EMNLP , Copenhagen, Denmark. Dolan, B., Quirk, C., and Brockett, C. (2004). Unsuper- vised construction of large paraphrase corpora: Exploit- ing massively parallel news sources. In  Proceedings of ACL , page 350. Ettinger, A., Elgohary, A., and Resnik, P. (2016). Probing for semantic evidence of composition by means of sim- ple classiﬁcation tasks. In  First Workshop on Evaluating Vector Space Representations for NLP (RepEval) , page 134. Faruqui, M., Tsvetkov, Y., Rastogi, P., and Dyer, C. (2016). Problems with evaluation of word embeddings using word similarity tasks.  arXiv preprint arXiv:1605.02276 . He, K., Zhang, X., Ren, S., and Sun, J. (2016). Deep resid- ual learning for image recognition. In  Proceedings of CVPR . Hill, F., Cho, K., and Korhonen, A. (2016a). Learning distributed representations of sentences from unlabelled data. In  Proceedings of NAACL . Hill, F., Reichart, R., and Korhonen, A. (2016b). Simlex- 999: Evaluating semantic models with (genuine) similar- ity estimation.  Computational Linguistics . Hu, M. and Liu, B. (2004). Mining and summarizing cus- tomer reviews. In  Proceedings of SIGKDD , pages 168– 177. Ji, Y. and Eisenstein, J. (2013). Discriminative improve- ments to distributional sentence similarity. In  Proceed- ings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP) . Karpathy, A. and Fei-Fei, L. (2015). Deep visual-semantic alignments for generating image descriptions. In  Pro- ceedings of CVPR , pages 3128–3137. Kingma, D. P. and Ba, J. (2014). Adam: A method for stochastic optimization. In  Proceedings of the 3rd "}
{"page": 5, "image_path": "doc_images/L18-1269_5.jpg", "ocr_text": "International Conference on Learning Representations\n(ICLR).\n\nKiros, R., Zhu, Y., Salakhutdinov, R. R., Zemel, R., Urta-\nsun, R., Torralba, A., and Fidler, S. (2015). Skip-thought\nvectors. In Advances in neural information processing\nsystems, pages 3294-3302.\n\nKoehn, P., Hoang, H., Birch, A., Callison-Burch, C., Fed-\nerico, M., Bertoldi, N., Cowan, B., Shen, W., Moran, C.,\nZens, R., Dyer, C., Bojar, O., Constantin, A., and Herbst,\nE. (2007). Moses: Open source toolkit for statistical\nmachine translation. In Proceedings of the 45th Annual\nMeeting of the ACL on Interactive Poster and Demon-\nstration Sessions, ACL ’07, pages 177-180, Strouds-\nburg, PA, USA. Association for Computational Linguis-\ntics.\n\nLai, A. and Hockenmaier, J. (2014). Illinois-lh: A deno-\ntational and distributional approach to semantics. Proc.\nSemEval, 2:5.\n\nLin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P.,\nRamanan, D., Dollar, P., and Zitnick, C. L. (2014).\nMicrosoft coco: Common objects in context. In Euro-\npean Conference on Computer Vision, pages 740-755.\nSpringer International Publishing.\n\nMarelli, M., Menini, S., Baroni, M., Bentivogli, L.,\nBernardi, R., and Zamparelli, R. (2014). A SICK cure\nfor the evaluation of compositional distributional seman-\ntic models. In Proceedings of LREC.\n\nMikolov, T., Grave, E., Bojanowski, P., Puhrsch, C., and\nJoulin, A. (2017). Advances in pre-training distributed\nword representations.\n\nNayak, N., Angeli, G., and Manning, C. D. (2016). Eval-\nuating word embeddings using a representative suite of\npractical tasks. In First Workshop on Evaluating Vector\nSpace Representations for NLP (RepEval), page 19.\n\nPang, B. and Lee, L. (2004). A sentimental educa-\ntion: Sentiment analysis using subjectivity summariza-\ntion based on minimum cuts. In Proceedings of ACL,\npage 271.\n\nPang, B. and Lee, L. (2005). Seeing stars: Exploiting class\nrelationships for sentiment categorization with respect to\nrating scales. In Proceedings of ACL, pages 115-124.\n\nPennington, J., Socher, R., and Manning, C. D. (2014).\nGlove: Global vectors for word representation. In Pro-\nceedings of the 2014 Conference on Empirical Methods\nin Natural Language Processing (EMNLP), volume 14,\npages 1532-1543.\n\nShi, X., Padhi, L, and Knight, K. (2016). Does string-\nbased neural MT learn source syntax? In Proceedings\nof EMNLP, pages 1526-1534, Austin, Texas.\n\nSocher, R., Perelygin, A., Wu, J. Y., Chuang, J., Manning,\nC. D., Ng, A. Y., Potts, C., et al. (2013). Recursive\ndeep models for semantic compositionality over a senti-\nment treebank. In Proceedings of EMNLP, pages 1631—\n1642.\n\nTai, K. S., Socher, R., and Manning, C. D. (2015a).\nImproved semantic representations from tree-structured\nlong short-term memory networks. Proceedings of ACL.\n\nTai, K. S., Socher, R., and Manning, C. D. (2015b).\nImproved semantic representations from tree-structured\n\nlong short-term memory networks. Proceedings of the\n53rd Annual Meeting of the Association for Computa-\ntional Linguistics (ACL).\n\nVoorhees, E. M. and Tice, D. M. (2000). Building a ques-\ntion answering test collection. In Proceedings of the\n23rd annual international ACM SIGIR conference on Re-\nsearch and development in information retrieval, pages\n200-207. ACM.\n\nVulié, I., Kiela, D., and Korhonen, A. (2017). Evalua-\ntion by association: A systematic study of quantitative\nword association evaluation. In Proceedings of EACL,\nvolume 1, pages 163-175.\n\nWiebe, J., Wilson, T., and Cardie, C. (2005). Annotating\nexpressions of opinions and emotions in language. Lan-\nguage resources and evaluation, 39(2):165-210.\n\nWieting, J., Bansal, M., Gimpel, K., and Livescu, K.\n(2015). Towards universal paraphrastic sentence embed-\ndings. Proceedings of the 4th International Conference\non Learning Representations (ICLR).\n\nWieting, J., Bansal, M., Gimpel, K., and Livescu, K.\n(2016). Charagram: Embedding words and sentences\nvia character n-grams. Proceedings of the 2016 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing (EMNLP).\n\nZhao, H., Lu, Z., and Poupart, P. (2015). Self-adaptive hi-\nerarchical sentence model. In Proceedings of the 24th\nInternational Conference on Artificial Intelligence, IJ-\nCAI’15, pages 4069-4076. AAAI Press.\n\nZhou, P., Qi, Z., Zheng, S., Xu, J., Bao, H., and Xu, B.\n(2016). Text classification improved by integrating bidi-\nrectional Istm with two-dimensional max pooling. Pro-\nceedings of COLING 2016, the 26th International Con-\nference on Computational Linguistics.\n\n1704\n", "vlm_text": "International Conference on Learning Representations (ICLR) . Kiros, R., Zhu, Y., Salakhutdinov, R. R., Zemel, R., Urta- sun, R., Torralba, A., and Fidler, S. (2015). Skip-thought vectors. In  Advances in neural information processing systems , pages 3294–3302. Koehn, P., Hoang, H., Birch, A., Callison-Burch, C., Fed- erico, M., Bertoldi, N., Cowan, B., Shen, W., Moran, C., Zens, R., Dyer, C., Bojar, O., Constantin, A., and Herbst, E. (2007). Moses: Open source toolkit for statistical machine translation. In  Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demon- stration Sessions , ACL   $^{'}07$  , pages 177–180, Strouds- burg, PA, USA. Association for Computational Linguis- tics. Lai, A. and Hockenmaier, J. (2014). Illinois-lh: A deno- tational and distributional approach to semantics.  Proc. SemEval , 2:5. Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll´ ar, P., and Zitnick, C. L. (2014). Microsoft coco: Common objects in context. In  Euro- pean Conference on Computer Vision , pages 740–755. Springer International Publishing. Marelli, M., Menini, S., Baroni, M., Bentivogli, L., Bernardi, R., and Zamparelli, R. (2014). A SICK cure for the evaluation of compositional distributional seman- tic models. In  Proceedings of LREC . Mikolov, T., Grave, E., Bojanowski, P., Puhrsch, C., and Joulin, A. (2017). Advances in pre-training distributed word representations. Nayak, N., Angeli, G., and Manning, C. D. (2016). Eval- uating word embeddings using a representative suite of practical tasks. In  First Workshop on Evaluating Vector Space Representations for NLP (RepEval) , page 19. Pang, B. and Lee, L. (2004). A sentimental educa- tion: Sentiment analysis using subjectivity summariza- tion based on minimum cuts. In  Proceedings of ACL , page 271. Pang, B. and Lee, L. (2005). Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. In  Proceedings of ACL , pages 115–124. Pennington, J., Socher, R., and Manning, C. D. (2014). Glove: Global vectors for word representation. In  Pro- ceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) , volume 14, pages 1532–1543. Shi, X., Padhi, I., and Knight, K. (2016). Does string- based neural MT learn source syntax? In  Proceedings of EMNLP , pages 1526–1534, Austin, Texas. Socher, R., Perelygin, A., Wu, J. Y., Chuang, J., Manning, C. D., Ng, A. Y., Potts, C., et al. (2013). Recursive deep models for semantic compositional it y over a senti- ment treebank. In  Proceedings of EMNLP , pages 1631— 1642. Tai, K. S., Socher, R., and Manning, C. D. (2015a). Improved semantic representations from tree-structured long short-term memory networks.  Proceedings of ACL . Tai, K. S., Socher, R., and Manning, C. D. (2015b). Improved semantic representations from tree-structured \nlong short-term memory networks.  Proceedings of the 53rd Annual Meeting of the Association for Computa- tional Linguistics (ACL) . Voorhees, E. M. and Tice, D. M. (2000). Building a ques- tion answering test collection. In  Proceedings of the 23rd annual international ACM SIGIR conference on Re- search and development in information retrieval , pages 200–207. ACM. Vuli´ c, I., Kiela, D., and Korhonen, A. (2017). Evalua- tion by association: A systematic study of quantitative word association evaluation. In  Proceedings of EACL , volume 1, pages 163–175. Wiebe, J., Wilson, T., and Cardie, C. (2005). Annotating expressions of opinions and emotions in language.  Lan- guage resources and evaluation , 39(2):165–210. Wieting, J., Bansal, M., Gimpel, K., and Livescu, K. (2015). Towards universal paraphrastic sentence embed- dings.  Proceedings of the 4th International Conference on Learning Representations (ICLR) . Wieting, J., Bansal, M., Gimpel, K., and Livescu, K. (2016). Charagram: Embedding words and sentences via character n-grams.  Proceedings of the 2016 Confer- ence on Empirical Methods in Natural Language Pro- cessing (EMNLP) . Zhao, H., Lu, Z., and Poupart, P. (2015). Self-adaptive hi- erarchical sentence model. In  Proceedings of the 24th International Conference on Artiﬁcial Intelligence , IJ- CAI’15, pages 4069–4076. AAAI Press. Zhou, P., Qi, Z., Zheng, S., Xu, J., Bao, H., and Xu, B. (2016). Text classiﬁcation improved by integrating bidi- rectional lstm with two-dimensional max pooling.  Pro- ceedings of COLING 2016, the 26th International Con- ference on Computational Linguistics . "}
