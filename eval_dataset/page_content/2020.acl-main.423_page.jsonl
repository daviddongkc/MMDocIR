{"page": 0, "image_path": "doc_images/2020.acl-main.423_0.jpg", "ocr_text": "SenseBERT: Driving Some Sense into BERT\n\nYoav Levine Barak Lenz\nShai Shalev-Shwartz\n\nOr Dagan OriRam Dan Padnos_ Or Sharir\nAmnon Shashua Yoav Shoham\n\nAI21 Labs, Tel Aviv, Israel\n\n{yoavl,barakl,ord,orir,.\n\nAbstract\n\nThe ability to learn from large unlabeled cor-\npora has allowed neural language models to\nadvance the frontier in natural language under-\nstanding. However, existing self-supervision\ntechniques operate at the word form level,\nwhich serves as a surrogate for the underly-\ning semantic content. This paper proposes a\nmethod to employ weak-supervision directly\nat the word sense level. Our model, named\nSenseBERT, is pre-trained to predict not only\nthe masked words but also their WordNet su-\npersenses. Accordingly, we attain a lexical-\nsemantic level language model, without the use\nof human annotation. SenseBERT achieves sig-\nnificantly improved lexical understanding, as\nwe demonstrate by experimenting on SemEval\nWord Sense Disambiguation, and by attaining\na state of the art result on the “Word in Context’\ntask.\n\n1 Introduction\n\nNeural language models have recently undergone\na qualitative leap forward, pushing the state of the\nart on various NLP tasks. Together with advances\nin network architecture (Vaswani et al., 2017), the\nuse of self-supervision has proven to be central\nto these achievements, as it allows the network to\nlearn from massive amounts of unannotated text.\nThe self-supervision strategy employed in BERT\n(Devlin et al., 2019) involves masking some of the\nwords in an input sentence, and then training the\nmodel to predict them given their context. Other\nproposed approaches for self-supervised objectives,\nincluding unidirectional (Radford et al., 2019), per-\nmutational (Yang et al., 2019), or word insertion-\nbased (Chan et al., 2019) methods, operate simi-\nlarly, over words. However, since a given word\nform can possess multiple meanings (e.g., the word\n‘bass’ can refer to a fish, a guitar, a type of singer,\netc.), the word itself is merely a surrogate of its\n\n.-}@ai2l.com\n\nactual meaning in a given context, referred to as its\nsense. Indeed, the word-form level is viewed as a\nsurface level which often introduces challenging\nambiguity (Navigli, 2009).\n\nIn this paper, we bring forth a novel method-\nology for applying weak-supervision directly on\nthe level of a word’s meaning. By infusing word-\nsense information into BERT’s pre-training sig-\nnal, we explicitely expose the model to lexical\nsemantics when learning from a large unanno-\ntated corpus. We call the resultant sense-informed\nmodel SenseBERT. Specifically, we add a masked-\nword sense prediction task as an auxiliary task in\nBERT’s pre-training. Thereby, jointly with the stan-\ndard word-form level language model, we train a\nsemantic-level language model that predicts the\nmissing word’s meaning. Our method does not\nrequire sense-annotated data; self-supervised learn-\ning from unannotated text is facilitated by using\nWordNet (Miller, 1998), an expert constructed in-\nventory of word senses, as weak supervision.\n\nWe focus on a coarse-grained variant of a word’s\nsense, referred to as its WordNet supersense, in\norder to mitigate an identified brittleness of fine-\ngrained word-sense systems, caused by arbitrary\nsense granularity, blurriness, and general subjec-\ntiveness (Kilgarriff, 1997; Schneider, 2014). Word-\nNet lexicographers organize all word senses into 45\nsupersense categories, 26 of which are for nouns,\n15 for verbs, 3 for adjectives and 1 for adverbs (see\nfull supersense table in the supplementary materi-\nals). Disambiguating a word’s supersense has been\nwidely studied as a fundamental lexical categoriza-\ntion task (Ciaramita and Johnson, 2003; Basile,\n2012; Schneider and Smith, 2015).\n\nWe employ the masked word’s allowed super-\nsenses list from WordNet as a set of possible labels\nfor the sense prediction task. The labeling of words\nwith a single supersense (e.g., ‘sword’ has only the\nsupersense noun.artifact) is straightforward: We\n\n4656\nProceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4656-4667\n\nJuly 5 - 10, 2020. ©2020 Association for Computational Linguistics\n\n", "vlm_text": "SenseBERT: Driving Some Sense into BERT \nYoav Levine Barak Lenz Or Dagan Ori Ram Dan Padnos Or Sharir Shai Shalev-Shwartz Amnon Shashua Yoav Shoham \nAI21 Labs, Tel Aviv, Israel \nyoavl,barakl,ord,orir,... } @ai21.com \nAbstract \nThe ability to learn from large unlabeled cor- pora has allowed neural language models to advance the frontier in natural language under- standing. However, existing self-supervision techniques operate at the word  form  level, which serves as a surrogate for the underly- ing semantic content. This paper proposes a method to employ weak-supervision directly at the word  sense  level. Our model, named SenseBERT, is pre-trained to predict not only the masked words but also their WordNet su- persenses. Accordingly, we attain a lexical- semantic level language model, without the use of human annotation. SenseBERT achieves sig- niﬁcantly improved lexical understanding, as we demonstrate by experimenting on SemEval Word Sense Disambiguation, and by attaining a state of the art result on the ‘Word in Context’ task. \n1 Introduction \nNeural language models have recently undergone a qualitative leap forward, pushing the state of the art on various NLP tasks. Together with advances in network architecture ( Vaswani et al. ,  2017 ), the use of self-supervision has proven to be central to these achievements, as it allows the network to learn from massive amounts of unannotated text. \nThe self-supervision strategy employed in BERT ( Devlin et al. ,  2019 ) involves masking some of the words in an input sentence, and then training the model to predict them given their context. Other proposed approaches for self-supervised objectives, including unidirectional ( Radford et al. ,  2019 ), per- mutational ( Yang et al. ,  2019 ), or word insertion- based ( Chan et al. ,  2019 ) methods, operate simi- larly, over words. However, since a given word form can possess multiple meanings ( e.g. , the word ‘bass’ can refer to a ﬁsh, a guitar, a type of singer, etc. ), the word itself is merely a surrogate of its actual meaning in a given context, referred to as its sense . Indeed, the word-form level is viewed as a surface level which often introduces challenging ambiguity ( Navigli ,  2009 ). \n\nIn this paper, we bring forth a novel method- ology for applying weak-supervision directly on the level of a word’s meaning. By infusing word- sense information into BERT’s pre-training sig- nal, we explicitely expose the model to lexical semantics when learning from a large unanno- tated corpus. We call the resultant sense-informed model  SenseBERT . Speciﬁcally, we add a masked- word sense prediction task as an auxiliary task in BERT’s pre-training. Thereby, jointly with the stan- dard word-form level language model, we train a semantic-level language model  that predicts the missing word’s meaning. Our method does not require sense-annotated data; self-supervised learn- ing from unannotated text is facilitated by using WordNet ( Miller ,  1998 ), an expert constructed in- ventory of word senses, as weak supervision. \nWe focus on a coarse-grained variant of a word’s sense, referred to as its WordNet  supersense , in order to mitigate an identiﬁed brittleness of ﬁne- grained word-sense systems, caused by arbitrary sense granularity, blurriness, and general subjec- tiveness ( Kilgarriff ,  1997 ;  Schneider ,  2014 ). Word- Net lexicographers organize all word senses into  45 supersense categories,  26  of which are for nouns, 15  for verbs,  3  for adjectives and  1  for adverbs (see full supersense table in the supplementary materi- als). Disambiguating a word’s supersense has been widely studied as a fundamental lexical categoriza- tion task ( Ciaramita and Johnson ,  2003 ;  Basile , 2012 ;  Schneider and Smith ,  2015 ). \nWe employ the masked word’s allowed super- senses list from WordNet as a set of possible labels for the sense prediction task. The labeling of words with a single supersense ( e.g. , ‘sword’ has only the supersense noun.artifact) is straightforward: We train the network to predict this supersense given the masked word’s context. As for words with mul- tiple supersenses ( e.g. , ‘bass’ can be: noun.food, noun.animal, noun.artifact, noun.person,  etc. ), we train the model to predict any of these senses, lead- ing to a simple yet effective soft-labeling scheme. "}
{"page": 1, "image_path": "doc_images/2020.acl-main.423_1.jpg", "ocr_text": "train the network to predict this supersense given\nthe masked word’s context. As for words with mul-\ntiple supersenses (e.g., ‘bass’ can be: noun.food,\nnoun.animal, noun.artifact, noun.person, efc.), we\ntrain the model to predict any of these senses, lead-\ning to a simple yet effective soft-labeling scheme.\n\nWe show that SenseBERT gasp outscores both\nBERTogase and BERT, arce by a large margin on\na supersense variant of the SemEval Word Sense\nDisambiguation (WSD) data set standardized in Ra-\nganato et al. (2017). Notably, SenseBERT re-\nceives competitive results on this task without fune-\ntuning, i.e., when training a linear classifier over\nthe pretrained embeddings, which serves as a tes-\ntament for its self-acquisition of lexical semantics.\nFurthermore, we show that SenseBERT gasp sur-\npasses BERT; arce in the Word in Context (WiC)\ntask (Pilehvar and Camacho-Collados, 2019) from\nthe SuperGLUE benchmark (Wang et al., 2019),\nwhich directly depends on word-supersense aware-\nness. A single SenseBERT,arce model achieves\nstate of the art performance on WiC with a score of\n72.14, improving the score of BERT, arce by 2.5\npoints.\n\n2 Related Work\n\nNeural network based word embeddings first ap-\npeared as a static mapping (non-contextualized),\nwhere every word is represented by a constant pre-\ntrained embedding (Mikolov et al., 2013; Penning-\nton et al., 2014). Such embeddings were shown\nto contain some amount of word-sense informa-\ntion (Iacobacci et al., 2016; Yuan et al., 2016;\nArora et al., 2018; Le et al., 2018). Addition-\nally, sense embeddings computed for each word\nsense in the word-sense inventory (e.g. WordNet)\nhave been employed, relying on hypernymity re-\nlations (Rothe and Schiitze, 2015) or the gloss for\neach sense (Chen et al., 2014). These approaches\nrely on static word embeddings and require a large\namount of annotated data per word sense.\n\nThe introduction of contextualized word embed-\ndings (Peters et al., 2018), for which a given word’s\nembedding is context-dependent rather than pre-\ncomputed, has brought forth a promising prospect\nfor sense-aware word embeddings. Indeed, visual-\nizations in Reif et al. (2019) show that sense sen-\nsitive clusters form in BERT’s word embedding\nspace. Nevertheless, we identify a clear gap in\nthis abilty. We show that a vanilla BERT model\ntrained with the current word-level self-supervision,\n\nburdened with the implicit task of disambiguat-\ning word meanings, often fails to grasp lexical\nsemantics, exhibiting high supersense misclassi-\nfication rates. Our suggested weakly-supervised\nword-sense signal allows SenseBERT to signifi-\ncantly bridge this gap.\n\nMoreover, SenseBERT exhibits an improvement\nin lexical semantics ability (reflected by the Word\nin Context task score) even when compared to mod-\nels with WordNet infused linguistic knowledge.\nSpecifically we compare to Peters et al. (2019)\nwho re-contextualize word embeddings via a word-\nto-entity attention mechanism (where entities are\nWordNet lemmas and synsets), and to Loureiro and\nJorge (2019) which construct sense embeddings\nfrom BERT’s word embeddings and use the Word-\nNet graph to enhance coverage (see quantitative\ncomparison in table 3).\n\n3 Incorporating Word-Supersense\nInformation in Pre-training\n\nIn this section, we present our proposed method for\nintegrating word sense-information within Sense-\nBERT’s pre-training. We start by describing the\nvanilla BERT architecture in subsection 3.1. We\nconceptually divide it into an internal transformer\nencoder and an external mapping W which trans-\nlates the observed vocabulary space into and out of\nthe transformer encoder space [see illustration in\nfigure I(a)].\n\nIn the subsequent subsections, we frame our con-\ntribution to the vanilla BERT architecture as an ad-\ndition of a parallel external mapping to the words\nsupersenses space, denoted S [see illustration in fig-\nure 1(b)]. Specifically, in section 3.2 we describe\nthe loss function used for learning S in parallel to\nW, effectively implementing word-form and word-\nsense multi-task learning in the pre-training stage.\nThen, in section 3.3 we describe our methodology\nfor adding supersense information in S to the initial\nTransformer embedding, in parallel to word-level\ninformation added by W. In section 3.4 we ad-\ndress the issue of supersense prediction for out-of-\nvocabulary words, and in section 3.5 we describe\nour modification of BERT’s masking strategy, pri-\noritizing single-supersensed words which carry a\nclearer semantic signal.\n\n3.1 Background\n\nThe input to BERT is a sequence of words {x) €\n{0,1}? }N | where 15% of the words are re-\n\n4657\n", "vlm_text": "\nWe show that SenseBERT BASE  outscores both  $\\mathrm{BERT_{BSE}}$   and BERT LARGE  by a large margin on a supersense variant of the SemEval Word Sense Disambiguation (WSD) data set standardized in  Ra- ganato et al.  ( 2017 ). Notably, SenseBERT re- ceives competitive results on this task without fune- tuning,  i.e. , when training a linear classiﬁer over the pretrained embeddings, which serves as a tes- tament for its self-acquisition of lexical semantics. Furthermore, we show that SenseBERT BASE  sur- passes BERT LARGE  in the Word in Context (WiC) task ( Pilehvar and Camacho-Collados ,  2019 ) from the SuperGLUE benchmark ( Wang et al. ,  2019 ), which directly depends on word-supersense aware- ness. A single SenseBERT LARGE  model achieves state of the art performance on WiC with a score of 72 . 14 , improving the score of BERT LARGE  by  2 . 5 points. \n2 Related Work \nNeural network based word embeddings ﬁrst ap- peared as a static mapping (non-contextualized), where every word is represented by a constant pre- trained embedding ( Mikolov et al. ,  2013 ;  Penning- ton et al. ,  2014 ). Such embeddings were shown to contain some amount of word-sense informa- tion ( Iacobacci et al. ,  2016 ;  Yuan et al. ,  2016 ; Arora et al. ,  2018 ;  Le et al. ,  2018 ). Addition- ally, sense embeddings computed for each word sense in the word-sense inventory (e.g. WordNet) have been employed, relying on hypernymity re- lations ( Rothe and Sch utze ,  2015 ) or the gloss for each sense ( Chen et al. ,  2014 ). These approaches rely on static word embeddings and require a large amount of annotated data per word sense. \nThe introduction of contextualized word embed- dings ( Peters et al. ,  2018 ), for which a given word’s embedding is context-dependent rather than pre- computed, has brought forth a promising prospect for sense-aware word embeddings. Indeed, visual- izations in  Reif et al.  ( 2019 ) show that sense sen- sitive clusters form in BERT’s word embedding space. Nevertheless, we identify a clear gap in this abilty. We show that a vanilla BERT model trained with the current word-level self-supervision, burdened with the implicit task of disambiguat- ing word meanings, often fails to grasp lexical semantics, exhibiting high supersense misclassi- ﬁcation rates. Our suggested weakly-supervised word-sense signal allows SenseBERT to signiﬁ- cantly bridge this gap. \n\nMoreover, SenseBERT exhibits an improvement in lexical semantics ability (reﬂected by the Word in Context task score) even when compared to mod- els with WordNet infused linguistic knowledge. Speciﬁcally we compare to Peters et al. (2019)who re-contextualize word embeddings via a word- to-entity attention mechanism (where entities are WordNet lemmas and synsets), and to  Loureiro and Jorge  ( 2019 ) which construct sense embeddings from BERT’s word embeddings and use the Word- Net graph to enhance coverage (see quantitative comparison in table  3 ). \n3 Incorporating Word-Supersense Information in Pre-training \nIn this section, we present our proposed method for integrating word sense-information within Sense- BERT’s pre-training. We start by describing the vanilla BERT architecture in subsection  3.1 . We conceptually divide it into an internal transformer encoder and an external mapping  $W$  which trans- lates the observed vocabulary space into and out of the transformer encoder space [see illustration in ﬁgure  1(a) ]. \nIn the subsequent subsections, we frame our con- tribution to the vanilla BERT architecture as an ad- dition of a parallel external mapping to the words supersenses space, denoted    $S$   [see illustration in ﬁg- ure  1(b) ]. Speciﬁcally, in section  3.2  we describe the loss function used for learning    $S$   in parallel to  $W$  , effectively implementing word-form and word- sense multi-task learning in the pre-training stage. Then, in section  3.3  we describe our methodology for adding supersense information in  $S$   to the initial Transformer embedding, in parallel to word-level information added by    $W$  . In section  3.4  we ad- dress the issue of supersense prediction for out-of- vocabulary words, and in section  3.5  we describe our modiﬁcation of BERT’s masking strategy, pri- oritizing single-supersensed words which carry a clearer semantic signal. \n3.1 Background \nThe input to BERT is a ence of words    $\\{x^{(j)}\\in$   $\\{0,1\\}^{\\bar{D}_{W}}\\}_{j=1}^{N}$    where  15%  of the words are re- "}
{"page": 2, "image_path": "doc_images/2020.acl-main.423_2.jpg", "ocr_text": "gl) Wr po\n\nl-—>\n\n: ; Transformer oa words\n(a) BERT [MASK] WW We? + po > | encoder wy\nAl) We) Dp (N)\naD) We \\SMeY) pl”\n: wo : : : 1 —> Wr yrores\nTransformer\n(b) SenseBERT [MASK] Wa) +|SMa9\\+ p? | > | encoder senses\n'T SENSES\n: f- : : , Ey\nal ‘N) Wa \\SMa® Pp (N)\n\nFigure 1: SenseBERT includes a masked-word supersense prediction task, pre-trained jointly with BERT’s original\nmasked-word prediction task (Devlin et al., 2019) (see section 3.2). As in the original BERT, the mapping from the\nTransformer dimension to the external dimension is the same both at input and at output (W for words and S' for\nsupersenses), where M/ denotes a fixed mapping between word-forms and their allowed WordNet supersenses (see\nsection 3.3). The vectors p\\) denote positional embeddings. For clarity, we omit a reference to a sentence-level\nNext Sentence Prediction task trained jointly with the above.\n\nplaced by a [MASK] token (see treatment of sub-\nword tokanization in section 3.4). Here N is the\ninput sentence length, Dy is the word vocabulary\nsize, and x9) is a 1-hot vector corresponding to\nthe j\" input word. For every masked word, the\noutput of the pretraining task is a word-score vec-\ntor ys € Rw containing the per-word score.\nBERT’s architecture can be decomposed to (1) an\ninternal Transformer encoder architecture (Vaswani\net al., 2017) wrapped by (2) an external mapping\nto the word vocabulary space, denoted by W.!\n\nThe Transformer encoder operates over a se-\nquence of word embeddings we € R¢, where\ndis the Transformer encoder’s hidden dimension.\nThese are passed through multiple attention-based\nTransformer layers, producing a new sequence\nof contextualized embeddings at each layer. The\nTransformer encoder output is the final sequence\nof contextualized word embeddings oat eR.\n\nThe external mapping W ¢ R¢%*?W is effec-\ntively a translation between the external word vo-\ncabulary dimension and the internal Transformer\ndimension. Original words in the input sentence\nare translated into the Transformer block by apply-\ning this mapping (and adding positional encoding\nvectors p\\) € R®):\n\nyp) = We +p (1)\n\ninput\n\n'For clarity, we omit a description of the Next Sentence\nPrediction task which we employ as in Devlin et al. (2019).\n\nThe word-score vector for a masked word at po-\nsition j is extracted from the Transformer en-\ncoder output by applying the transpose: y¥°\"4s =\nWr) ar [see illustration in figure 1(a)]. The\nuse of the same matrix W as the mapping in and\nout of the transformer encoder space is referred to\nas weight tying (Inan et al., 2017; Press and Wolf,\n2017).\n\nGiven a masked word in position 7, BERT’s\noriginal masked-word prediction pre-training task\nis to have the softmax of the word-score vector\nyrds = Wren get as close as possible to a\n1-hot vector corresponding to the masked word.\nThis is done by minimizing the cross-entropy loss\nbetween the softmax of the word-score vector and\n\na 1-hot vector corresponding to the masked word:\nLim = — log p(w context), (2)\n\nwhere w is the masked word, the context is com-\nposed of the rest of the input sequence, and the\nprobability is computed by:\n\nexp (ynores)\n\n= oor? 3\nSwen wesy &\n\np(w|context) =\n\nwhere y“°\"S denotes the w'\" entry of the word-\n\nscore vector.\n\n4658\n", "vlm_text": "The image contrasts two models: BERT and SenseBERT. \n\n(a) BERT:\n- In the BERT model, a sequence of inputs labeled \\(x^{(1)}\\) to \\(x^{(N)}\\) is processed.\n- A masked token [MASK] is present in the input.\n- These inputs are combined with word embeddings \\(W\\) (highlighted in red) and position embeddings \\(p\\).\n- The sum of word embeddings \\( Wx^{(j)} \\) and position embeddings \\( p^{(j)} \\) is processed by a Transformer encoder.\n- The output of the Transformer encoder is then used to predict the masked token \\(y^{words}\\) using the transposed word embeddings \\(W^T\\).\n\n(b) SenseBERT:\n- Similar to BERT, the sequence of inputs includes a masked token.\n- The inputs are combined with both word embeddings \\(W\\) and sense embeddings \\(S\\) (highlighted in blue), along with position embeddings \\(p\\).\n- The aggregation \\(Wx^{(j)} + SMx^{(j)} + p^{(j)}\\) is passed through a Transformer encoder.\n- The output of the Transformer encoder in SenseBERT simultaneously produces predictions for word labels \\(y^{words}\\) and sense labels \\(y^{senses}\\) using the transposed matrices \\(W^T\\) and \\(S^T\\), respectively.\n\nOverall, the diagram illustrates how SenseBERT extends BERT by incorporating additional semantic sense information to improve the model's understanding.\nFigure 1: SenseBERT includes a masked-word supersense prediction task, pre-trained jointly with BERT’s original masked-word prediction task ( Devlin et al. ,  2019 ) (see section  3.2 ). As in the original BERT, the mapping from the Transformer dimension to the external dimension is the same both at input and at output (  $W$   for words and    $S$   for supersenses), where  $M$   denotes a ﬁxed mapping between word-forms and their allowed WordNet supersenses (see section  3.3 ). The vectors    $p^{(j)}$    denote positional embeddings. For clarity, we omit a reference to a sentence-level Next Sentence Prediction task trained jointly with the above. \nplaced by a [MASK] token (see treatment of sub- word tokanization in section  3.4 ). Here    $N$   is the input sentence length,    $D_{W}$   is the word vocabulary size, and    $x^{(j)}$    is a 1-hot vector corresponding to the    $j^{\\mathrm{th}}$    input word. For every masked word, the output of the pretraining task is a word-score vec- tor    $y^{\\mathrm{models}}\\in\\mathbb{R}^{D_{W}}$    containing the per-word score. BERT’s architecture can be decomposed to  (1)  an internal Transformer encoder architecture ( Vaswani et al. ,  2017 ) wrapped by  (2)  an external mapping to the word vocabulary space, denoted by  $W$  . 1 \nThe Transformer encoder operates over a se- quence of word embeddings    $\\hat{v_{\\mathrm{input}}^{(j)}}\\,\\in\\,\\mathbb{R}^{d}$    , where  $d$   is the Transformer encoder’s hidden dimension. These are passed through multiple attention-based Transformer layers, producing a new sequence of contextualized embeddings at each layer. The Transformer encoder output is the ﬁnal sequence of contextualized word embeddings  $v_{\\mathrm{output}}^{(j)}\\in\\mathbb{R}^{d}$  ∈ . \nThe external mapping  $W\\,\\,\\in\\,\\mathbb{R}^{d\\times D_{W}}$   ∈   is effec- tively a translation between the external word vo- cabulary dimension and the internal Transformer dimension. Original words in the input sentence are translated into the Transformer block by apply- ing this mapping (and adding positional encoding vectors  $\\boldsymbol{p}^{(j)^{-}\\bar{\\mathbf{\\alpha}}}\\in\\bar{\\mathbb{R}}^{d}$  ): \n\n$$\nv_{\\mathrm{input}}^{(j)}=W x^{(j)}+p^{(j)}\n$$\n \nThe word-score vector for a masked word at po- sition    $j$   is extracted from the Transformer en- coder output by applying the transpose:    $y^{\\mathrm{words}}=$   $\\bar{W^{\\top}}v_{\\mathrm{output}}^{(j)}$    [see illustration in ﬁgure  1(a) ]. The use of the same matrix    $W$   as the mapping in and out of the transformer encoder space is referred to as  weight tying  ( Inan et al. ,  2017 ;  Press and Wolf , 2017 ). \nGiven a masked word in position    $j$  , BERT’s original masked-word prediction pre-training task is to have the softmax of the word-score vector  $y^{\\mathrm{words}}\\,=\\,W^{\\top}v_{\\mathrm{output}}^{(j)}$    get as close as possible to a 1-hot vector corresponding to the masked word. This is done by minimizing the cross-entropy loss between the softmax of the word-score vector and a 1-hot vector corresponding to the masked word: \n\n$$\n\\mathcal{L}_{\\mathrm{LM}}=-\\log p(w|\\mathrm{convex}),\n$$\n \nwhere    $w$   is the masked word, the context is com- posed of the rest of the input sequence, and the probability is computed by: \n\n$$\np(w|{c o n t e x t})=\\frac{\\exp\\left(y_{w}^{\\mathrm{worlds}}\\right)}{\\sum_{w^{\\prime}}\\exp\\left(y_{w^{\\prime}}^{\\mathrm{worlds}}\\right)},\n$$\n \nwhere    $y_{w}^{\\mathrm{worlds}}$  denotes the    $w^{\\mathrm{th}}$    entry of the word- score vector. "}
{"page": 3, "image_path": "doc_images/2020.acl-main.423_3.jpg", "ocr_text": "3.2. Weakly-Supervised Supersense\nPrediction Task\n\nJointly with the above procedure for training the\nword-level language model of SenseBERT, we\ntrain the model to predict the supersense of every\nmasked word, thereby training a semantic-level lan-\nguage model. This is done by adding a parallel ex-\nternal mapping to the words supersenses space, de-\nnoted S € R¢*s [see illustration in figure 1(b)],\nwhere Ds = 45 is the size of supersenses vocabu-\nlary. Ideally, the objective is to have the softmax of\nthe sense-score vector y®\"8°5 € RPS := § TU\nget as close as possible to a 1-hot vector correspond-\ning to the word’s supersense in the given context.\n\nFor each word w in our vocabulary, we employ\nthe WordNet word-sense inventory for constructing\nA(w), the set of its “allowed” supersenses. Specifi-\ncally, we apply a WordNet Lemmatizer on w, ex-\ntract the different synsets that are mapped to the\nlemmatized word in WordNet, and define A(w) as\nthe union of supersenses coupled to each of these\nsynsets. As exceptions, we set A(w) = © for\nthe following: (i) short words (up to 3 characters),\nsince they are often treated as abbreviations, (ii)\nstop words, as WordNet does not contain their main\nsynset (e.g. ‘he’ is either the element helium or the\nhebrew language according to WordNet), and (iii)\ntokens that represent part-of-word (see section 3.4\nfor further discussion on these tokens).\n\nGiven the above construction, we employ a com-\nbination of two loss terms for the supersense-level\nlanguage model. The following allowed-senses\nterm maximizes the probability that the predicted\nsense is in the set of allowed supersenses of the\nmasked word w:\n\nLillowed — _ log p(s € A(w)|context)\n= —log S- p(s|context), (4)\nscA(w)\n\nwhere the probability for a supersense s is given\nby:\nSey XPV) |\nThe soft-labeling scheme given above, which\ntreats all the allowed supersenses of the masked\nword equally, introduces noise to the supersense la-\nbels. We expect that encountering many contexts in\na sufficiently large corpus will reinforce the correct\nlabels whereas the signal of incorrect labels will\ndiminish. To illustrate this, consider the following\nexamples for the food context:\n\np(s|context) =\n\n(5)\n\n1. “This bass is delicious”\n(supersenses: noun.food, noun.artifact, etc.)\n\n2. “This chocolate is delicious”\n(supersenses: noun.food, noun.attribute, etc.)\n\n3. “This pickle is delicious”\n(supersenses: noun.food, noun.state, efc.)\n\nMasking the marked word in each of the examples\nresults in three identical input sequences, each with\na different sets of labels. The ground truth label,\nnoun.food, appears in all cases, so that its probabil-\nity in contexts indicating food is increased whereas\nthe signals supporting other labels cancel out.\n\nWhile C2l!ewed pushes the network in the right\ndirection, minimizing this loss could result in the\nnetwork becoming overconfident in predicting a\nstrict subset of the allowed senses for a given word,\ni.e., a collapse of the prediction distribution. This\nis especially acute in the early stages of the training\nprocedure, when the network could converge to the\nnoisy signal of the soft-labeling scheme.\n\nTo mitigate this issue, the following regulariza-\ntion term is added to the loss, which encourages\na uniform prediction distribution over the allowed\nsupersenses:\n\nLom=- >. + ioep(s|context), (6\n\nSIM = Ate) g p(s|context), (6)\nseA(w)\n\ni.e., a cross-entropy loss with a uniform distribution\nover the allowed supersenses.\n\nOverall, jointly with the regular word level lan-\nguage model trained with the loss in eq. 2, we train\nthe semantic level language model with a combined\nloss of the form:\n\nLsim = £3 + Loi (7)\n3.3, Supersense Aware Input Embeddings\n\nThough in principle two different matrices could\nhave been used for converting in and out of the\nTranformer encoder, the BERT architecture em-\nploys the same mapping W. This approach, re-\nferred to as weight tying, was shown to yield the-\noretical and pracrical benefits (Inan et al., 2017;\nPress and Wolf, 2017). Intuitively, constructing the\nTransformer encoder’s input embeddings from the\nsame mapping with which the scores are computed\nimproves their quality as it makes the input more\nsensitive to the training signal.\n\n4659\n", "vlm_text": "3.2 Weakly-Supervised Supersense Prediction Task \nJointly with the above procedure for training the word-level language model of SenseBERT, we train the model to predict the supersense of every masked word, thereby training a semantic-level lan- guage model. This is done by adding a parallel ex- ternal mapping to the words supersenses space, de- noted    $S\\in\\mathbb{R}^{d\\times D_{S}}$    [see illustration in ﬁgure  1(b) ], where    $D_{S}=45$   is the size of supersenses vocabu- lary. Ideally, the objective is to have the softmax of the sense-score vector  $y^{\\mathrm{sending}}\\in\\mathbb{R}^{D_{S}}:=S^{\\top}v_{\\mathrm{outp}}^{(j)}$  output get as close as possible to a 1-hot vector correspond- ing to the word’s supersense in the given context. \nFor each word    $w$   in our vocabulary, we employ the WordNet word-sense inventory for constructing  $A(w)$  , the set of its “allowed” supersenses. Speciﬁ- cally, we apply a WordNet Lemmatizer on    $w$  , ex- tract the different synsets that are mapped to the lemmatized word in WordNet, and deﬁne    $A(w)$   as the union of supersenses coupled to each of these synsets. As exceptions, we set    $A(w)\\;=\\;\\emptyset$  for the following: (i) short words (up to 3 characters), since they are often treated as abbreviations, (ii) stop words, as WordNet does not contain their main synset (e.g. ‘he’ is either the element helium or the hebrew language according to WordNet), and (iii) tokens that represent part-of-word (see section  3.4 for further discussion on these tokens). \nGiven the above construction, we employ a com- bination of two loss terms for the supersense-level language model. The following  allowed-senses term  maximizes the probability that the predicted sense is in the set of allowed supersenses of the masked word    $w$  : \n\n$$\n\\begin{array}{r l}&{\\mathcal{L}_{\\mathrm{SLM}}^{\\mathrm{a||downed}}=-\\log p\\left(s\\in A(w)|\\mathrm{convex}\\right)}\\\\ &{\\qquad\\qquad=-\\log\\displaystyle\\sum_{s\\in A(w)}p(s|\\mathrm{convex}),}\\end{array}\n$$\n \nwhere the probability for a supersense    $s$   is given by: \n\n$$\np(s|\\mathrm{lceil})=\\frac{\\exp(y_{s}^{\\mathrm{senes}})}{\\sum_{s^{\\prime}}\\exp(y_{s^{\\prime}}^{\\mathrm{senes}})}.\n$$\n \nThe soft-labeling scheme given above, which treats all the allowed supersenses of the masked word equally, introduces noise to the supersense la- bels. We expect that encountering many contexts in a sufﬁciently large corpus will reinforce the correct labels whereas the signal of incorrect labels will diminish. To illustrate this, consider the following examples for the food context: \n1. “This  bass  is delicious” (supersenses: noun.food, noun.artifact,  etc. ) \n2. “This  chocolate  is delicious” (supersenses: noun.food, noun.attribute,  etc. ) \n3. “This  pickle  is delicious” (supersenses: noun.food, noun.state,  etc. ) \nMasking the marked word in each of the examples results in three identical input sequences, each with a different sets of labels. The ground truth label, noun.food, appears in all cases, so that its probabil- ity in contexts indicating food is increased whereas the signals supporting other labels cancel out. \nWhile    $\\mathcal{L}_{\\mathrm{SLM}}^{\\mathrm{allowed}}$  pushes the network in the right direction, minimizing this loss could result in the network becoming overconﬁdent in predicting a strict subset of the allowed senses for a given word, i.e., a collapse of the prediction distribution. This is especially acute in the early stages of the training procedure, when the network could converge to the noisy signal of the soft-labeling scheme. \nTo mitigate this issue, the following  regulariza- tion term  is added to the loss, which encourages a uniform prediction distribution over the allowed supersenses: \n\n$$\n\\mathcal{L}_{\\mathrm{SLM}}^{\\mathrm{reg}}=-\\sum_{s\\in A(w)}\\frac{1}{|A(w)|}\\log p(s|\\mathrm{convex}),\n$$\n \ni.e. , a cross-entropy loss with a uniform distribution over the allowed supersenses. \nOverall, jointly with the regular word level lan- guage model trained with the loss in eq.  2 , we train the semantic level language model with a combined loss of the form: \n\n$$\n\\mathcal{L}_{\\mathrm{SLM}}=\\mathcal{L}_{\\mathrm{SLM}}^{\\mathrm{allowed}}+\\mathcal{L}_{\\mathrm{SLM}}^{\\mathrm{reg}}\\quad.\n$$\n \n3.3 Supersense Aware Input Embeddings \nThough in principle two different matrices could have been used for converting in and out of the Tranformer encoder, the BERT architecture em- ploys the same mapping    $W$  . This approach, re- ferred to as weight tying, was shown to yield the- oretical and pracrical beneﬁts ( Inan et al. ,  2017 ; Press and Wolf ,  2017 ). Intuitively, constructing the Transformer encoder’s input embeddings from the same mapping with which the scores are computed improves their quality as it makes the input more sensitive to the training signal. "}
{"page": 4, "image_path": "doc_images/2020.acl-main.423_4.jpg", "ocr_text": "(a) All Supersenses\n\nMI Verb Supersenses Ill Noun Supersenses\n\nI Other (adv./adj.)\n\n(b) Noun Supersenses\n\n. enoun.artifact\n\nnoun.attribute e @ noun.group\n\nenoun.location\nnoun.person e\nnoun.shapee ¢noun.animal\n noun.object\n\n noun.food\n\nnoun. feeling noun. body «\nad . ° t\nnoun.plante\n°\n. ° noun.substance\n.\ni Abstract ™@ Concrete Ml Concrete - Entities\n\nFigure 2: UMAP visualization of supersense vectors (rows of the classifier 5’) learned by SenseBERT at pre-training.\n(a) Clustering by the supersense’s part-of speech. (b) Within noun supersenses, semantically similar supersenses\nare clustered together (see more details in the supplementary materials).\n\nWe follow this approach, and insert our newly\nproposed semantic-level language model matrix\nS in the input in addition to W [as depicted in\nfigure 1(b)], such that the input vector to the Trans-\nformer encoder (eq. 1) is modified to obey:\n\nv= (W+SM)29 +p, 8)\nwhere p\\) are the regular positional embeddings\nas used in BERT, and M € R?s*Pw isa static 0/1\nmatrix converting between words and their allowed\nWordNet supersenses A(w) (see construction de-\ntails above).\n\nThe above strategy for constructing ww allows\nfor the semantic level vectors in S' to come into play\nand shape the input embeddings even for words\nwhich are rarely observed in the training corpus.\nFor such a word, the corresponding row in W is\npotentially less informative, since due to the low\nword frequency the model did not have sufficient\nchance to adequately learn it. However, since the\nmodel learns a representation of its supersense, the\ncorresponding row in S is informative of the se-\nmantic category of the word. Therefore, the input\nembedding in eq. 8 can potentially help the model\nto elicit meaningful information even when the\nmasked word is rare, allowing for better exploita-\ntion of the training corpus.\n\n3.4 Rare Words Supersense Prediction\n\nAt the pre-processing stage, when an out-of-\nvocabulary (OOV) word is encountered in the cor-\npus, it is divided into several in-vocabulary sub-\nword tokens. For the self-supervised word pre-\n\ndiction task (eq. 2) masked sub-word tokens are\nstraightforwardly predicted as described in sec-\ntion 3.1. In contrast, word-sense supervision is\nonly meaningful at the word level. We compare\ntwo alternatives for dealing with tokenized OOV\nwords for the supersense prediction task (eq. 7).\n\nIn the first alternative, called 60K vocabulary, we\naugment BERT’s original 30K-token vocabulary\n(which roughly contained the most frequent words)\nwith additional 30K new words, chosen according\nto their frequency in Wikipedia. This vocabulary\nincrease allows us to see more of the corpus as\nwhole words for which supersense prediction is a\nmeaningful operation. Additionally, in accordance\nwith the discussion in the previous subsection, our\nsense-aware input embedding mechanism can help\nthe model extract more information from lower-\nfrequency words. For the cases where a sub-word\ntoken is chosen for masking, we only propagate\nthe regular word level loss and do not train the\nsupersense prediction task.\n\nThe above addition to the vocabulary results in\nan increase of approximately 23M parameters over\nthe 110M parameters of BERT gasp and an increase\nof approximately 30M parameters over the 340M\nparameters of BERT; arce (due to different embed-\nding dimensions d = 768 and d = 1024, respec-\ntively). It is worth noting that similar vocabulary\nsizes in leading models have not resulted in in-\ncreased sense awareness, as reflected for example\nin the WiC task results (Liu et al., 2019).\n\nAs a second alternative, referred to as average\nembedding, we employ BERT’s regular 30K-token\n\n4660\n", "vlm_text": "The image consists of two scatter plots related to linguistic supersenses. \n\nOn the left side:\n\n- There are colored dots representing different categories of linguistic supersenses, specifically:\n  - Grey dots labeled as \"Verb Supersenses.\"\n  - Yellow dots labeled as \"Noun Supersenses.\"\n  - Teal dots labeled as \"Other (adv./adj.)\"\n\nOn the right side:\n\n- The dots are labeled with more specific noun supersense categories, divided into three types based on color:\n  - Red dots indicate \"Abstract\" categories (e.g., noun.attribute, noun.shape, noun.feeling).\n  - Green dots indicate \"Concrete\" categories (e.g., noun.artifact, noun.animal, noun.food, noun.body, noun.plant, noun.substance).\n  - Blue dots indicate \"Concrete - Entities\" categories (noun.group, noun.location, noun.person).\n\nThis visualization seems to categorize various noun and verb supersenses, providing a visual representation of their distinctions and possibly clustering patterns.\nWe follow this approach, and insert our newly proposed semantic-level language model matrix  $S$   in the input in addition to    $W$   [as depicted in ﬁgure  1(b) ], such that the input vector to the Trans- former encoder (eq.  1 ) is modiﬁed to obey: \n\n$$\nv_{\\mathrm{input}}^{(j)}=(W+S M)x^{(j)}+p^{(j)},\n$$\n \nwhere  $p^{(j)}$    are the regular positional embeddings as used in BERT, and  $M\\in\\mathbb{R}^{D_{S}\\times D_{W}}$    is a static  $0/1$  matrix converting between words and their allowed WordNet supersenses    $A(w)$   (see construction de- tails above). \nThe above strategy for constructing  $v_{\\mathrm{input}}^{(j)}$    allows for the semantic level vectors in    $S$   to come into play and shape the input embeddings even for words which are rarely observed in the training corpus. For such a word, the corresponding row in    $W$   is potentially less informative, since due to the low word frequency the model did not have sufﬁcient chance to adequately learn it. However, since the model learns a representation of its supersense, the corresponding row in    $S$   is informative of the se- mantic category of the word. Therefore, the input embedding in eq.  8  can potentially help the model to elicit meaningful information even when the masked word is rare, allowing for better exploita- tion of the training corpus. \n3.4 Rare Words Supersense Prediction \nAt the pre-processing stage, when an out-of- vocabulary (OOV) word is encountered in the cor- pus, it is divided into several in-vocabulary sub- word tokens. For the self-supervised word pre- diction task (eq.  2 ) masked sub-word tokens are straightforwardly predicted as described in sec- tion  3.1 . In contrast, word-sense supervision is only meaningful at the word level. We compare two alternatives for dealing with tokenized OOV words for the supersense prediction task (eq.  7 ). \n\nIn the ﬁrst alternative, called  $60K$   vocabulary , we augment BERT’s original  30 K-token vocabulary (which roughly contained the most frequent words) with additional 30K new words, chosen according to their frequency in Wikipedia. This vocabulary increase allows us to see more of the corpus as whole words for which supersense prediction is a meaningful operation. Additionally, in accordance with the discussion in the previous subsection, our sense-aware input embedding mechanism can help the model extract more information from lower- frequency words. For the cases where a sub-word token is chosen for masking, we only propagate the regular word level loss and do not train the supersense prediction task. \nThe above addition to the vocabulary results in an increase of approximately  23 M parameters over the  110 M parameters of  $\\mathrm{BERT_{BSE}}$   and an increase of approximately  30 M parameters over the  340 M parameters of BERT LARGE  (due to different embed- ding dimensions    $d=768$   and    $d=1024$  , respec- tively). It is worth noting that similar vocabulary sizes in leading models have not resulted in in- creased sense awareness, as reﬂected for example in the WiC task results ( Liu et al. ,  2019 ). \nAs a second alternative, referred to as  average embedding , we employ BERT’s regular  30 K-token "}
{"page": 5, "image_path": "doc_images/2020.acl-main.423_5.jpg", "ocr_text": "(a)\nThe [MASK] fell to the floor.\n|\n52% noun.artifact (sword, chair, ...)\n17% noun.person (man, girl, ...)\n\nGill [MASK] the bread.\n\n|\n33% verb.contact (cut, buttered, ...)\n20% verb.consumption (ate, chewed, ...)\n11% verb.change (heated, baked, ...)\n6% verb.possession (took, bought, ...)\n\n(b)\nnoun.person noun.food\n| |\nDan cooked a bass on the grill.\n\nverb.creation noun.artifact\n\nnoun.artifact adj.all\n| |\nThe bass player was exceptional.\n\nnoun.person\n\nFigure 3: (a) A demonstration of supersense probabilities assigned to a masked position within context, as given\nby SenseBERT’s word-supersense level semantic language model (capped at 5%). Example words corresponding\nto each supersense are presented in parentheses. (b) Examples of SenseBERT’s prediction on raw text, when the\nunmasked input sentence is given to the model. This beyond word-form abstraction ability facilitates a more natural\n\nelicitation of semantic content at pre-training.\n\nvocabulary and employ a whole-word-masking\nstrategy. Accordingly, all of the tokens of a to-\nkenized OOV word are masked together. In this\ncase, we train the supersense prediction task to pre-\ndict the WordNet supersenses of this word from the\naverage of the output embeddings at the location\nof the masked sub-words tokens.\n\n3.5 Single-Supersensed Word Masking\n\nWords that have a single supersense are good an-\nchors for obtaining an unambiguous semantic sig-\nnal. These words teach the model to accurately\nmap contexts to supersenses, such that it is then\nable to make correct context-based predictions even\nwhen a masked word has several supersenses. We\ntherefore favor such words in the masking strategy,\nchoosing 50% of the single-supersensed words in\neach input sequence to be masked. We stop if\n40% of the overall 15% masking budget is filled\nwith single-supersensed words (this rarly happens),\nand in any case we randomize the choice of the\nremaining words to complete this budget. As in\nthe original BERT, 1 out of 10 words chosen for\nmasking is shown to the model as itself rather than\nreplaced with [MASK].\n\n4 Semantic Language Model\nVisualization\n\nA SenseBERT pretrained as described in section 3\n(with training hyperparameters as in Devlin et al.\n(2019)), has an immediate non-trivial bi-product.\nThe pre-trained mapping to the supersenses space,\ndenoted S, acts as an additional head predicting a\nword’s supersense given context [see figure 1(b)].\nWe thereby effectively attain a semantic-level lan-\n\nSenseBERT pase SemEval-SS Fine-tuned\n30K no OOV 81.9\n\n30K average OOV 82.7\n\n60K no OOV 83\n\nTable 1: Testing variants for predicting supersenses\nof rare words during SenseBERT’s pretraining, as de-\nscribed in section 5.1. Results are reported on the\nSemEval-SS task (see section 5.2). 30K/60K stand for\nvocabulary size, and no/average OOV stand for not pre-\ndicting senses for OOV words or predicting senses from\nthe average of the sub-word token embeddings, respec-\ntively.\n\nguage model that predicts the missing word’s mean-\ning jointly with the standard word-form level lan-\nguage model.\n\nWe illustrate the resultant mapping in fig-\nure 2, showing a UMAP dimensionality reduc-\ntion (McInnes et al., 2018) of the rows of S,\nwhich corresponds to the different supersenses. A\nclear clustering according to the supersense part-\nof-speech is apparent in figure 2(a). We further\nidentify finer-grained semantic clusters, as shown\nfor example in figure 2(b) and given in more detail\nin the supplementary materials.\n\nSenseBERT’s semantic language model allows\npredicting a distribution over supersenses rather\nthan over words in a masked position. Figure 3(a)\nshows the supersense probabilities assigned by\nSenseBERT in several contexts, demonstrating the\nmodel’s ability to assign semantically meaningful\ncategories to the masked position.\n\nFinally, we demonstrate that SenseBERT enjoys\n\n4661\n", "vlm_text": "The image shows two incomplete sentences with a placeholder \"[MASK]\" and a list of potential word categories along with their probabilities for what could fill the placeholder.\n\n1. For the sentence \"The [MASK] fell to the floor.\"\n   - 52% probability it is a \"noun.artifact\" (e.g., sword, chair, ...)\n   - 17% probability it is a \"noun.person\" (e.g., man, girl, ...)\n\n2. For the sentence \"Gill [MASK] the bread.\"\n   - 33% probability it is a \"verb.contact\" (e.g., cut, buttered, ...)\n   - 20% probability it is a \"verb.consumption\" (e.g., ate, chewed, ...)\n   - 11% probability it is a \"verb.change\" (e.g., heated, baked, ...)\n   - 6% probability it is a \"verb.possession\" (e.g., took, bought, ...)\n\nThe list serves as suggestions for what kind of word could logically fit in the sentence in place of \"[MASK]\" based on context and probability.\nThe image contains two sentences with parts of speech and word sense annotations for specific words. \n\n1. \"Dan cooked a bass on the grill.\"\n   - \"Dan\" is labeled as a \"noun.person\".\n   - \"cooked\" is labeled as a \"verb.creation\".\n   - \"bass\" is labeled as a \"noun.food\".\n   - \"grill\" is labeled as a \"noun.artifact\".\n\n2. \"The bass player was exceptional.\"\n   - \"bass\" is labeled as a \"noun.artifact\".\n   - \"player\" is labeled as a \"noun.person\".\n   - \"exceptional\" is labeled as an \"adj.all\". \n\nThe annotations indicate the parts of speech and specific meanings of the words \"bass\" in different contexts (as food and as an artifact, likely referring to a musical instrument).\nFigure 3:  (a)  A demonstration of supersense probabilities assigned to a masked position within context, as given by SenseBERT’s word-supersense level semantic language model (capped at    $5\\%$  ). Example words corresponding to each supersense are presented in parentheses.  (b)  Examples of SenseBERT’s prediction on raw text, when the unmasked input sentence is given to the model. This beyond word-form abstraction ability facilitates a more natural elicitation of semantic content at pre-training. \nvocabulary and employ a whole-word-masking strategy. Accordingly, all of the tokens of a to- kenized OOV word are masked together. In this case, we train the supersense prediction task to pre- dict the WordNet supersenses of this word from the average  of the output embeddings at the location of the masked sub-words tokens. \n3.5 Single-Supersensed Word Masking \nWords that have a single supersense are good an- chors for obtaining an unambiguous semantic sig- nal. These words teach the model to accurately map contexts to supersenses, such that it is then able to make correct context-based predictions even when a masked word has several supersenses. We therefore favor such words in the masking strategy, choosing  $50\\%$   of the single-supersensed words in each input sequence to be masked. We stop if  $40\\%$   of the overall    $15\\%$   masking budget is ﬁlled with single-supersensed words (this rarly happens), and in any case we randomize the choice of the remaining words to complete this budget. As in the original BERT,  1  out of  10  words chosen for masking is shown to the model as itself rather than replaced with [MASK]. \n4 Semantic Language Model Visualization \nA SenseBERT pretrained as described in section  3\n\n (with training hyperparameters as in  Devlin et al.\n\n ( 2019 )), has an immediate non-trivial bi-product. The pre-trained mapping to the supersenses space, denoted  $S$  , acts as an additional head predicting a word’s supersense given context [see ﬁgure  1(b) ]. We thereby effectively attain a semantic-level lan- \nThe table shows performance metrics for a model named SenseBERT (BASE), fine-tuned on SemEval-SS. It presents results for different dataset configurations:\n\n- **30K no OOV**: 81.9\n- **30K average OOV**: 82.7\n- **60K no OOV**: 83\n\n\"OOV\" likely stands for \"Out Of Vocabulary,\" indicating how the model performs with respect and without unknown words.\nTable 1: Testing variants for predicting supersenses of rare words during SenseBERT’s pretraining, as de- scribed in section  5.1 . Results are reported on the SemEval-SS task (see section  5.2 ).  30 K/ 60 K stand for vocabulary size, and no/average OOV stand for not pre- dicting senses for OOV words or predicting senses from the average of the sub-word token embeddings, respec- tively. \nguage model that predicts the missing word’s mean- ing jointly with the standard word-form level lan- guage model. \nWe illustrate the resultant mapping in ﬁg- ure  2 , showing a UMAP dimensionality reduc- tion ( McInnes et al. ,  2018 ) of the rows of    $S$  , which corresponds to the different supersenses. A clear clustering according to the supersense part- of-speech is apparent in ﬁgure  2(a) . We further identify ﬁner-grained semantic clusters, as shown for example in ﬁgure  2(b)  and given in more detail in the supplementary materials. \nSenseBERT’s semantic language model allows predicting a distribution over supersenses rather than over words in a masked position. Figure  3(a) shows the supersense probabilities assigned by SenseBERT in several contexts, demonstrating the model’s ability to assign semantically meaningful categories to the masked position. \nFinally, we demonstrate that SenseBERT enjoys "}
{"page": 6, "image_path": "doc_images/2020.acl-main.423_6.jpg", "ocr_text": "(a) The team used a battery of the newly developed “gene probes”\n\nSemEval-SS\n\nTen shirt-sleeved ringers stand in a circle, one foot ahead of the\n\nother in a prize-fighter's stance\n\nBERT SenseBERT\n\nnoun. artifact noun.group\n\nnoun. quantity noun. body\n\n(b) Sent. A: Sent. B:\nWwic The kick must be synchronized A sidecar is a smooth drink Same Different\nwith the arm movements. but it has a powerful kick.\nSent. A: Sent. B:\nPlant bugs in the dissident’s Plant a spy in Moscow. Different Same\napartment.\n\nFigure 4: Example entries of (a) the SemEval-SS task, where a model is to predict the supersense of the marked\nword, and (b) the Word in Context (WiC) task where a model must determine whether the underlined word is used\nin the same/different supersense within sentences A and B. In all displayed examples, taken from the corresponding\ndevelopment sets, SenseBERT predicted the correct label while BERT failed to do so. A quantitative comparison\n\nbetween models is presented in table 2.\n\nan ability to view raw text at a lexical semantic\nlevel. Figure 3(b) shows example sentences and\ntheir supersense prediction by the pretrained model.\nWhere a vanilla BERT would see only the words\nof the sentence “Dan cooked a bass on the grill’,\nSenseBERT would also have access to the super-\nsense abstraction: “[Person] [created] [food] on the\n[artifact]”. This sense-level perspective can help\nthe model extract more knowledge from every train-\ning example, and to generalize semantically similar\nnotions which do not share the same phrasing.\n\n5 Lexical Semantics Experiments\n\nIn this section, we present quantitative evaluations\nof SenseBERT, pre-trained as described in sec-\ntion 3. We test the model’s performance on a\nsupersense-based variant of the SemEval WSD test\nsets standardized in Raganato et al. (2017), and\non the Word in Context (WiC) task (Pilehvar and\nCamacho-Collados, 2019) (included in the recently\nintroduced SuperGLUE benchmark (Wang et al.,\n2019)), both directly relying on the network’s abil-\nity to perform lexical semantic categorization.\n\n5.1 Comparing Rare Words Supersense\nPrediction Methods\n\nWe first report a comparison of the two methods de-\nscribed in section 3.4 for predicting the supersenses\nof rare words which do not appear in BERT’s origi-\nnal vocabulary. The first 6(0K vocabulary method\nenriches the vocabulary and the second average\nembedding method predicts a supersense from the\naverage embeddings of the sub-word tokens com-\n\nprising an OOV word. During fine-tuning, when\nencountering an OOV word we predict the super-\nsenses from the rightmost sub-word token in the\n60K vocabulary method and from the average of\nthe sub-word tokens in the average embedding\nmethod.\n\nAs shown in table 1, both methods perform com-\nparably on the SemEval supersense disambigua-\ntion task (see following subsection), yielding an\nimprovement over the baseline of learning super-\nsense information only for whole words in BERT’s\noriginal 30K-token vocabulary. We continue with\nthe 60K-token vocabulary for the rest of the ex-\nperiments, but note the average embedding option\nas a viable competitor for predicting word-level\nsemantics.\n\n5.2. SemEval-SS: Supersense Disambiguation\n\nWe test SenseBERT on a Word Supersense Dis-\nambiguation task, a coarse grained variant of the\ncommon WSD task. We use SemCor (Miller\net al., 1993) as our training dataset (226, 036 an-\nnotated examples), and the SenseEval (Edmonds\nand Cotton, 2001; Snyder and Palmer, 2004) / Se-\nmEval (Pradhan et al., 2007; Navigli et al., 2013;\nMoro and Navigli, 2015) suite for evaluation (over-\nall 7253 annotated examples), following Raganato\net al. (2017). For each word in both training and test\nsets, we change its fine-grained sense label to its\ncorresponding WordNet supersense, and therefore\ntrain the network to predict a given word’s super-\nsense. We name this Supersense disambiguation\ntask SemEval-SS. See figure 4(a) for an example\n\n4662\n", "vlm_text": "The table compares the performance of BERT and SenseBERT on two tasks: SemEval-SS and WiC.\n\n### (a) SemEval-SS\n- **Sentence 1:** \"The team used a **battery** of the newly developed 'gene probes'\"\n  - **BERT:** noun.artifact\n  - **SenseBERT:** noun.group\n- **Sentence 2:** \"Ten shirt-sleeved ringers stand in a circle, one **foot** ahead of the other in a prize-fighter's stance\"\n  - **BERT:** noun.quantity\n  - **SenseBERT:** noun.body\n\n### (b) WiC\n- **Pair 1:**\n  - **Sent. A:** \"The **kick** must be synchronized with the arm movements.\"\n  - **Sent. B:** \"A sidecar is a smooth drink but it has a powerful **kick**.\"\n  - **BERT:** Same\n  - **SenseBERT:** Different\n- **Pair 2:**\n  - **Sent. A:** \"**Plant** bugs in the dissident’s apartment.\"\n  - **Sent. B:** \"**Plant** a spy in Moscow.\"\n  - **BERT:** Different\n  - **SenseBERT:** Same\n\nThe BERT and SenseBERT columns indicate the model's interpretation or classification of the ambiguous words in context.\nFigure 4: Example entries of  (a)  the SemEval-SS task, where a model is to predict the supersense of the marked word, and  (b)  the Word in Context (WiC) task where a model must determine whether the underlined word is used in the same/different supersense within sentences A and B. In all displayed examples, taken from the corresponding development sets, SenseBERT predicted the correct label while BERT failed to do so. A quantitative comparison between models is presented in table  2 . \nan ability to view raw text at a lexical semantic level. Figure  3(b)  shows example sentences and their supersense prediction by the pretrained model. Where a vanilla BERT would see only the words of the sentence “Dan cooked a bass on the grill”, SenseBERT would also have access to the super- sense abstraction: “[Person] [created] [food] on the [artifact]”. This sense-level perspective can help the model extract more knowledge from every train- ing example, and to generalize semantically similar notions which do not share the same phrasing. \n5 Lexical Semantics Experiments \nIn this section, we present quantitative evaluations of SenseBERT, pre-trained as described in sec- tion  3 . We test the model’s performance on a supersense-based variant of the SemEval WSD test sets standardized in  Raganato et al.  ( 2017 ), and on the Word in Context (WiC) task ( Pilehvar and Camacho-Collados ,  2019 ) (included in the recently introduced SuperGLUE benchmark ( Wang et al. , 2019 )), both directly relying on the network’s abil- ity to perform lexical semantic categorization. \n5.1 Comparing Rare Words Supersense Prediction Methods \nWe ﬁrst report a comparison of the two methods de- scribed in section  3.4  for predicting the supersenses of rare words which do not appear in BERT’s origi- nal vocabulary. The ﬁrst    $60K$   vocabulary  method enriches the vocabulary and the second  average embedding  method predicts a supersense from the average embeddings of the sub-word tokens com- prising an OOV word. During ﬁne-tuning, when encountering an OOV word we predict the super- senses from the rightmost sub-word token in the 60 K vocabulary method and from the average of the sub-word tokens in the average embedding method. \n\nAs shown in table  1 , both methods perform com- parably on the SemEval supersense disambigua- tion task (see following subsection), yielding an improvement over the baseline of learning super- sense information only for whole words in BERT’s original  30 K-token vocabulary. We continue with the  60 K-token vocabulary for the rest of the ex- periments, but note the average embedding option as a viable competitor for predicting word-level semantics. \n5.2 SemEval-SS: Supersense Disambiguation \nWe test SenseBERT on a Word Supersense Dis- ambiguation task, a coarse grained variant of the common WSD task. We use SemCor ( Miller et al. ,  1993 ) as our training dataset ( 226 ,  036  an- notated examples), and the SenseEval ( Edmonds and Cotton ,  2001 ;  Snyder and Palmer ,  2004 ) / Se- mEval ( Pradhan et al. ,  2007 ;  Navigli et al. ,  2013 ; Moro and Navigli ,  2015 ) suite for evaluation (over- all  7253  annotated examples), following  Raganato et al.  ( 2017 ). For each word in both training and test sets, we change its ﬁne-grained sense label to its corresponding WordNet supersense, and therefore train the network to predict a given word’s super- sense. We name this Supersense disambiguation task SemEval-SS. See ﬁgure  4(a)  for an example "}
{"page": 7, "image_path": "doc_images/2020.acl-main.423_7.jpg", "ocr_text": "SemEval-SS Frozen SemEval-SS Fine-tuned Word in Context\n\nBERTaase 65.1 79.2 -\n\nBERT, arcs 67.3 81.1 69.6\nSenseBERT ase 75.6 83.0 70.3\nSenseBERT | arce 79.5 83.7 72.1\n\nTable 2: Results on a supersense variant of the SemEval WSD test set standardized in Raganato et al. (2017), which\nwe denote SemEval-SS, and on the Word in Context (WiC) dataset (Pilehvar and Camacho-Collados, 2019) included\nin the recently introduced SuperGLUE benchmark (Wang et al., 2019). These tasks require a high level of lexical\nsemantic understanding, as can be seen in the examples in figure 4. For both tasks, SenseBERT demonstrates a\nclear improvement over BERT in the regular fine-tuning setup, where network weights are modified during training\non the task. Notably, SenseBERT, aggre achieves state of the art performance on the WiC task. In the SemEval-SS\nFrozen setting, we train a linear classifier over pretrained embeddings, without changing the network weights. The\nresults show that SenseBERT introduces a dramatic improvement in this setting, implying that its word-sense aware\npre-training (section 3) yields embeddings that carries lexical semantic information which is easily extractable\nfor the benefits of downstream tasks. Results for BERT on the SemEval-SS task are attained by employing the\npublished pre-trained BERT models, and the BERT; arce result on WiC is taken from the baseline scores published\non the SuperGLUE benchmark (Wang et al., 2019) (no result has been published for BERT g,4s5)-\n\nthe model’s potential to acquire word-supersense\ninformation given its pre-training.\n\nWord in Context\n\nbowel or 7 Table 2 shows a comparison between vanilla\nBERT nt embeddings ! oe BERT and SenseBERT on the supersense dis-\nRoBERTa‘? 69.9 ambiguation task. Our semantic level pre-\nKnowBERT-W+W? 70.9 training signal clearly yields embeddings with\nSenseBERT 21 enhanced word-meaning awareness, relative to\n\nembeddings trained with BERT’s vanilla word-\nlevel signal. SenseBERT gasp improves the score\nof BERTgase in the Frozen setting by over 10\npoints and SenseBERT,,rcz improves that of\nBERT arce by over 12 points, demonstrating com-\npetitive results even without fine-tuning. In the\nsetting of model fine-tuning, we see a clear demon-\nstration of the model’s ability to learn word-level\nsemantics, as SenseBERT gasp surpasses the score\nof BERT, arce by 2 points.\n\nTable 3: Test set results for the WiC dataset.\ntPilehvar and Camacho-Collados (2019)\nitLoureiro and Jorge (2019)\n\nFWang et al. (2019)\n\nLiu et al. (2019)\n\nPeters et al. (2019)\n\nfrom this modified data set.\n\nWe show results on the SemEval-SS task for\ntwo different training schemes. In the first, we\ntrained a linear classifier over the ‘frozen’ output\nembeddings of the examined model — we do not\nchange the the trained SenseBERT’s parameters in\nthis scheme. This Frozen setting is a test for the\namount of basic lexical semantics readily present\nin the pre-trained model, easily extricable by fur-\nther downstream tasks (reminiscent of the semantic\nprobes employed in Hewitt and Manning (2019);\n\n5.3. Word in Context (WiC) Task\n\nWe test our model on the recently introduced WiC\nbinary classification task. Each instance in WiC\nhas a target word w for which two contexts are\nprovided, each invoking a specific meaning of w.\nThe task is to determine whether the occurrences\nof w in the two contexts share the same meaning\nor not, clearly requiring an ability to identify the\n\nReif et al. (2019).\n\nIn the second training scheme we fine-tuned the\nexamined model on the task, allowing its param-\neters to change during training (see full training\ndetails in the supplementary materials). Results\nattained by employing this training method reflect\n\nword’s semantic category. The WiC task is defined\nover supersenses (Pilehvar and Camacho-Collados,\n2019) — the negative examples include a word used\nin two different supersenses and the positive ones\ninclude a word used in the same supersense. See\nfigure 4(b) for an example from this data set.\n\n4663\n", "vlm_text": "The table presents the performance of BERT and SenseBERT models across three scenarios: SemEval-SS Frozen, SemEval-SS Fine-tuned, and Word in Context. The models evaluated are BERT_BASE, BERT_LARGE, SenseBERT_BASE, and SenseBERT_LARGE. The displayed values are presumably scores or performance measures for each model in the respective scenarios. Here's a summary of the scores:\n\n- **SemEval-SS Frozen:**\n  - BERT_BASE: 65.1\n  - BERT_LARGE: 67.3\n  - SenseBERT_BASE: 75.6\n  - SenseBERT_LARGE: 79.5\n\n- **SemEval-SS Fine-tuned:**\n  - BERT_BASE: 79.2\n  - BERT_LARGE: 81.1\n  - SenseBERT_BASE: 83.0\n  - SenseBERT_LARGE: 83.7\n\n- **Word in Context:**\n  - BERT_LARGE: 69.6\n  - SenseBERT_BASE: 70.3\n  - SenseBERT_LARGE: 72.1\n\nNote that there is no data for BERT_BASE in the \"Word in Context\" column.\nTable 2: Results on a supersense variant of the SemEval WSD test set standardized in  Raganato et al.  ( 2017 ), which we denote SemEval-SS, and on the Word in Context (WiC) dataset ( Pilehvar and Camacho-Collados ,  2019 ) included in the recently introduced SuperGLUE benchmark ( Wang et al. ,  2019 ). These tasks require a high level of lexical semantic understanding, as can be seen in the examples in ﬁgure  4 . For both tasks, SenseBERT demonstrates a clear improvement over BERT in the regular ﬁne-tuning setup, where network weights are modiﬁed during training on the task. Notably, SenseBERT LARGE  achieves state of the art performance on the WiC task. In the SemEval-SS Frozen setting, we train a linear classiﬁer over pretrained embeddings, without changing the network weights. The results show that SenseBERT introduces a dramatic improvement in this setting, implying that its word-sense aware pre-training (section  3 ) yields embeddings that carries lexical semantic information which is easily extractable for the beneﬁts of downstream tasks. Results for BERT on the SemEval-SS task are attained by employing the published pre-trained BERT models, and the  $\\mathrm{BERT_{LARGE}}$   result on WiC is taken from the baseline scores published on the SuperGLUE benchmark ( Wang et al. ,  2019 ) (no result has been published for   $\\mathrm{BERT_{BSE}}.$  ). \nThe table presents a comparison of various language models and their performance on the \"Word in Context\" task. The models listed in the table are:\n\n1. ELMo\n2. BERT with sense embeddings\n3. BERT Large\n4. RoBERTa\n5. KnowBERT-W+W\n6. SenseBERT\n\nEach model has a corresponding score:\n\n- ELMo: 57.7\n- BERT sense embeddings: 67.7\n- BERT Large: 69.6\n- RoBERTa: 69.9\n- KnowBERT-W+W: 70.9\n- SenseBERT: 72.1\n\nThe scores appear to represent the performance of each model on a specific task, likely evaluating their contextual understanding or semantic interpretation abilities, with SenseBERT achieving the highest score among the models listed.\nfrom this modiﬁed data set. \nWe show results on the SemEval-SS task for two different training schemes. In the ﬁrst, we trained a linear classiﬁer over the ‘frozen’ output embeddings of the examined model – we do not change the the trained SenseBERT’s parameters in this scheme. This Frozen setting is a test for the amount of basic lexical semantics readily present in the pre-trained model, easily extricable by fur- ther downstream tasks (reminiscent of the semantic probes employed in  Hewitt and Manning  ( 2019 ); Reif et al.  ( 2019 ). \nIn the second training scheme we ﬁne-tuned the examined model on the task, allowing its param- eters to change during training (see full training details in the supplementary materials). Results attained by employing this training method reﬂect the model’s potential to acquire word-supersense information given its pre-training. \n\nTable  2  shows a comparison between vanilla BERT and SenseBERT on the supersense dis- ambiguation task. Our semantic level pre- training signal clearly yields embeddings with enhanced word-meaning awareness, relative to embeddings trained with BERT’s vanilla word- level signal. SenseBERT BASE  improves the score of   $\\mathrm{BERT_{BSE}}$   in the Frozen setting by over  10 points and SenseBERT LARGE improves that of  $\\mathrm{BRT_{\\mathrm{LRGE}}}$   by over  12  points, demonstrating com- petitive results even without ﬁne-tuning. In the setting of model ﬁne-tuning, we see a clear demon- stration of the model’s ability to learn word-level semantics, as SenseBERT BASE  surpasses the score of BERT LARGE  by  2  points. \n5.3 Word in Context (WiC) Task \nWe test our model on the recently introduced WiC binary classiﬁcation task. Each instance in WiC has a target word    $w$   for which two contexts are provided, each invoking a speciﬁc meaning of    $w$  . The task is to determine whether the occurrences of  $w$   in the two contexts share the same meaning or not, clearly requiring an ability to identify the word’s semantic category. The WiC task is deﬁned over supersenses ( Pilehvar and Camacho-Collados , 2019 ) – the negative examples include a word used in two different supersenses and the positive ones include a word used in the same supersense. See ﬁgure  4(b)  for an example from this data set. "}
{"page": 8, "image_path": "doc_images/2020.acl-main.423_8.jpg", "ocr_text": "Score CoLA SST-2 MRPC STS-B QQP MNLI QNLI RTE\nBERTaase (OURS) — (77.5 50.1 92.6  88.7/84.3 85.7/84.6 71.0/88.9 83.6 89.4 67.9\nSenseBERTaase 71.9 54.6 92.2 89.2/85.2  83.5/82.3.  70.3/88.8 83.6 90.6 67.5\n\nTable 4: Results on the GLUE benchmark test set.\n\nResults on the WiC task comparing Sense-\nBERT to vanilla BERT are shown in table 2.\nSenseBERT asx surpasses a larger vanilla model,\nBERT arce- As shown in table 3, a single\nSenseBERT, arcre model achieves the state of the\nart score in this task, demonstrating unprecedented\nlexical semantic awareness.\n\n5.4 GLUE\n\nThe General Language Understanding Evaluation\n(GLUE; Wang et al. (2018)) benchmark is a popu-\nlar testbed for language understanding models. It\nconsists of 9 different NLP tasks, covering different\nlinguistic phenomena. We evaluate our model on\nGLUE, in order to verify that SenseBERT gains its\nlexical semantic knowledge without compromising\nperformance on other downstream tasks. Due to\nslight differences in the data used for pretraining\nBERT and SenseBERT (BookCorpus is not pub-\nlicly available), we trained a BERT; 4s model with\nthe same data used for our models. BERTgase and\nSenseBERTsgase were both finetuned using the ex-\nact same procedures and hyperparameters. The\nresults are presented in table 4. Indeed, Sense-\nBERT performs on par with BERT, achieving an\noverall score of 77.9, compared to 77.5 achieved\nby BERT sasz-\n\n6 Conclusion\n\nWe introduce lexical semantic information into\na neural language model’s pre-training objective.\nThis results in a boosted word-level semantic aware-\nness of the resultant model, named SenseBERT,\nwhich considerably outperforms a vanilla BERT on\na SemEval based Supersense Disambiguation task\nand achieves state of the art results on the Word\nin Context task. This improvement was obtained\nwithout human annotation, but rather by harnessing\nan external linguistic knowledge source. Our work\nindicates that semantic signals extending beyond\nthe lexical level can be similarly introduced at the\npre-training stage, allowing the network to elicit\nfurther insight without human supervision.\n\nAcknowledgments\n\nWe acknowledge useful comments and assistance\nfrom our colleagues at AI21 Labs. We would also\nlike to thank the anonymous reviewers for their\nvaluable feedback.\n\nReferences\n\nSanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma,\nand Andrej Risteski. 2018. Linear algebraic struc-\nture of word senses, with applications to polysemy.\nTransactions of the Association for Computational\nLinguistics, 6:483—-495.\n\nPierpaolo Basile. 2012. Super-sense tagging using sup-\nport vector machines and distributional features. In\nInternational Workshop on Evaluation of Natural\nLanguage and Speech Tool for Italian, pages 176—\n185. Springer.\n\nWilliam Chan, Nikita Kitaev, Kelvin Guu, Mitchell\nStern, and Jakob Uszkoreit. 2019. KERMIT: Genera-\ntive insertion-based modeling for sequences. arXiv\npreprint arXiv: 1906.01604.\n\nXinxiong Chen, Zhiyuan Liu, and Maosong Sun. 2014.\nA unified model for word sense representation and\ndisambiguation. In Proceedings of the 2014 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 1025-1035, Doha, Qatar.\nAssociation for Computational Linguistics.\n\nMassimiliano Ciaramita and Mark Johnson. 2003. Su-\npersense tagging of unknown nouns in WordNet. In\nProceedings of the 2003 Conference on Empirical\nMethods in Natural Language Processing, pages 168—\n175.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171-4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\n\nPhilip Edmonds and Scott Cotton. 2001. SENSEVAL-\n2: Overview. In Proceedings of SENSEVAL-2 Sec-\nond International Workshop on Evaluating Word\nSense Disambiguation Systems, pages 1-5, Toulouse,\nFrance. Association for Computational Linguistics.\n\n4664\n", "vlm_text": "The table compares the performance of two models: BERT\\(_{\\text{BASE}}\\) (OURS) and SenseBERT\\(_{\\text{BASE}}\\). It includes scores across several tasks:\n\n- **Score**: Overall score (BERT\\(_{\\text{BASE}}\\): 77.5, SenseBERT\\(_{\\text{BASE}}\\): 77.9)\n- **CoLA**: BERT: 50.1, SenseBERT: 54.6\n- **SST-2**: BERT: 92.6, SenseBERT: 92.2\n- **MRPC**: BERT: 88.7/84.3, SenseBERT: 89.2/85.2\n- **STS-B**: BERT: 85.7/84.6, SenseBERT: 83.5/82.3\n- **QQP**: BERT: 71.0/88.9, SenseBERT: 70.3/88.8\n- **MNLI**: BERT: 83.6, SenseBERT: 83.6\n- **QNLI**: BERT: 89.4, SenseBERT: 90.6\n- **RTE**: BERT: 67.9, SenseBERT: 67.5\n\nThe tasks measure different NLP capabilities, and the scores might represent accuracy or F1 scores depending on the task.\nResults on the WiC task comparing Sense- BERT to vanilla BERT are shown in table  2 . SenseBERT BASE  surpasses a larger vanilla model, BERT LARGE . As shown in table  3 , a single SenseBERT LARGE  model achieves the state of the art score in this task, demonstrating unprecedented lexical semantic awareness. \n5.4 GLUE \nThe General Language Understanding Evaluation (GLUE;  Wang et al.  ( 2018 )) benchmark is a popu- lar testbed for language understanding models. It consists of 9 different NLP tasks, covering different linguistic phenomena. We evaluate our model on GLUE, in order to verify that SenseBERT gains its lexical semantic knowledge without compromising performance on other downstream tasks. Due to slight differences in the data used for pretraining BERT and SenseBERT (BookCorpus is not pub- licly available), we trained a BERT BASE  model with the same data used for our models. BERT BASE  and SenseBERT BASE  were both ﬁnetuned using the ex- act same procedures and hyperparameters. The results are presented in table  4 . Indeed, Sense- BERT performs on par with BERT, achieving an overall score of 77.9, compared to 77.5 achieved by BERT BASE . \n6 Conclusion \nWe introduce lexical semantic information into a neural language model’s pre-training objective. This results in a boosted word-level semantic aware- ness of the resultant model, named SenseBERT, which considerably outperforms a vanilla BERT on a SemEval based Supersense Disambiguation task and achieves state of the art results on the Word in Context task. This improvement was obtained without human annotation, but rather by harnessing an external linguistic knowledge source. Our work indicates that semantic signals extending beyond the lexical level can be similarly introduced at the pre-training stage, allowing the network to elicit further insight without human supervision. \nAcknowledgments \nWe acknowledge useful comments and assistance from our colleagues at AI21 Labs. We would also like to thank the anonymous reviewers for their valuable feedback. \nReferences \nSanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma, and Andrej Risteski. 2018.  Linear algebraic struc- ture of word senses, with applications to polysemy . Transactions of the Association for Computational Linguistics , 6:483–495. \nPierpaolo Basile. 2012. Super-sense tagging using sup- port vector machines and distributional features. In International Workshop on Evaluation of Natural Language and Speech Tool for Italian , pages 176– 185. Springer. \nWilliam Chan, Nikita Kitaev, Kelvin Guu, Mitchell Stern, and Jakob Uszkoreit. 2019.  KERMIT: Genera- tive insertion-based modeling for sequences .  arXiv preprint arXiv:1906.01604 . \nXinxiong Chen, Zhiyuan Liu, and Maosong Sun. 2014. A uniﬁed model for word sense representation and disambiguation . In  Proceedings of the 2014 Con- ference on Empirical Methods in Natural Language Processing (EMNLP) , pages 1025–1035, Doha, Qatar. Association for Computational Linguistics. \nMassimiliano Ciaramita and Mark Johnson. 2003.  Su- persense tagging of unknown nouns in WordNet . In Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing , pages 168– 175. \nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.  BERT: Pre-training of deep bidirectional transformers for language under- standing . In  Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech- nologies, Volume 1 (Long and Short Papers) , pages 4171–4186, Minneapolis, Minnesota. Association for Computational Linguistics. \nPhilip Edmonds and Scott Cotton. 2001.  SENSEVAL- 2: Overview . In  Proceedings of SENSEVAL-2 Sec- ond International Workshop on Evaluating Word Sense Disambiguation Systems , pages 1–5, Toulouse, France. Association for Computational Linguistics. "}
{"page": 9, "image_path": "doc_images/2020.acl-main.423_9.jpg", "ocr_text": "John Hewitt and Christopher D. Manning. 2019. A\nstructural probe for finding syntax in word represen-\ntations. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume I (Long and Short Papers), pages\n4129-4138, Minneapolis, Minnesota. Association for\nComputational Linguistics.\n\nIgnacio Iacobacci, Mohammad Taher Pilehvar, and\nRoberto Navigli. 2016. Embeddings for word sense\ndisambiguation: An evaluation study. In Proceed-\nings of the 54th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 897-907, Berlin, Germany. Association\nfor Computational Linguistics.\n\nHakan Inan, Khashayar Khosravi, and Richard Socher.\n2017. Tying word vectors and word classifiers: A\nloss framework for language modeling. In JCLR.\n\nAdam Kilgarriff. 1997. I don’t believe in word senses.\nComputers and the Humanities, 31(2):91-113.\n\nMinh Le, Marten Postma, Jacopo Urbani, and Piek\nVossen. 2018. A deep dive into word sense disam-\nbiguation with LSTM. In Proceedings of the 27th\nInternational Conference on Computational Linguis-\ntics, pages 354-365, Santa Fe, New Mexico, USA.\nAssociation for Computational Linguistics.\n\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Dangi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\n\nDaniel Loureiro and Alipio Jorge. 2019. Language\nmodelling makes sense: Propagating representations\nthrough WordNet for full-coverage word sense disam-\nbiguation. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics,\npages 5682-5691, Florence, Italy. Association for\nComputational Linguistics.\n\nLeland McInnes, John Healy, and James Melville. 2018.\nUMAP: Uniform manifold approximation and pro-\njection for dimension reduction. arXiv preprint\narXiv: 1802.03426.\n\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013. Distributed representa-\ntions of words and phrases and their compositionality.\nIn Advances in Neural Information Processing Sys-\ntems 26, pages 3111-3119. Curran Associates, Inc.\n\nGeorge A Miller. 1998. WordNet: An electronic lexical\ndatabase. MIT press.\n\nGeorge A. Miller, Claudia Leacock, Randee Tengi, and\nRoss T. Bunker. 1993. A semantic concordance.\nIn Human Language Technology: Proceedings of\na Workshop Held at Plainsboro, New Jersey, March\n21-24, 1993.\n\nAndrea Moro and Roberto Navigli. 2015. SemEval-\n2015 task 13: Multilingual all-words sense disam-\nbiguation and entity linking. In Proceedings of the\n9th International Workshop on Semantic Evaluation\n(SemEval 2015), pages 288-297, Denver, Colorado.\nAssociation for Computational Linguistics.\n\nRoberto Navigli. 2009. Word sense disambiguation: A\nsurvey. ACM Comput. Surv., 41(2).\n\nRoberto Navigli, David Jurgens, and Daniele Vannella.\n2013. SemEval-2013 task 12: Multilingual word\nsense disambiguation. In Second Joint Conference\non Lexical and Computational Semantics (*SEM),\nVolume 2: Proceedings of the Seventh International\nWorkshop on Semantic Evaluation (SemEval 2013),\npages 222-231, Atlanta, Georgia, USA. Association\nfor Computational Linguistics.\n\nJeffrey Pennington, Richard Socher, and Christopher\nManning. 2014. Glove: Global vectors for word\nrepresentation. In Proceedings of the 2014 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing (EMNLP), pages 1532-1543, Doha, Qatar.\nAssociation for Computational Linguistics.\n\nMatthew Peters, Mark Neumann, Mohit lyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. In Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume I (Long Papers), pages 2227-2237,\nNew Orleans, Louisiana. Association for Computa-\ntional Linguistics.\n\nMatthew E. Peters, Mark Neumann, Robert Logan, Roy\nSchwartz, Vidur Joshi, Sameer Singh, and Noah A.\nSmith. 2019. Knowledge enhanced contextual word\nrepresentations. In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP),\npages 43-54, Hong Kong, China. Association for\nComputational Linguistics.\n\nMohammad Taher Pilehvar and Jose Camacho-Collados.\n2019. WiC: the word-in-context dataset for evalu-\nating context-sensitive meaning representations. In\nProceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume I (Long and Short Papers), pages 1267-1273,\nMinneapolis, Minnesota. Association for Computa-\ntional Linguistics.\n\nSameer Pradhan, Edward Loper, Dmitriy Dligach, and\nMartha Palmer. 2007. SemEval-2007 task-17: En-\nglish lexical sample, SRL and all words. In Pro-\nceedings of the Fourth International Workshop on\nSemantic Evaluations (SemEval-2007), pages 87-92,\nPrague, Czech Republic. Association for Computa-\ntional Linguistics.\n\nOfir Press and Lior Wolf. 2017. Using the output em-\nbedding to improve language models. In Proceedings\n\n4665\n", "vlm_text": "John Hewitt and Christopher D. Manning. 2019.  A structural probe for ﬁnding syntax in word represen- tations . In  Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech- nologies, Volume 1 (Long and Short Papers) , pages 4129–4138, Minneapolis, Minnesota. Association for Computational Linguistics. \nIgnacio Iacobacci, Mohammad Taher Pilehvar, and Roberto Navigli. 2016.  Embeddings for word sense disambiguation: An evaluation study . In  Proceed- ings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Pa- pers) , pages 897–907, Berlin, Germany. Association for Computational Linguistics. \nHakan Inan, Khashayar Khosravi, and Richard Socher. 2017.  Tying word vectors and word classiﬁers: A loss framework for language modeling . In  ICLR . \nAdam Kilgarriff. 1997. I don’t believe in word senses. Computers and the Humanities , 31(2):91–113. \nMinh Le, Marten Postma, Jacopo Urbani, and Piek Vossen. 2018.  A deep dive into word sense disam- biguation with LSTM . In  Proceedings of the   $27t h$  International Conference on Computational Linguis- tics , pages 354–365, Santa Fe, New Mexico, USA. Association for Computational Linguistics. \nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man- dar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A robustly optimized bert pretraining ap- proach .  arXiv preprint arXiv:1907.11692 . \nDaniel Loureiro and Al ıpio Jorge. 2019. Language modelling makes sense: Propagating representations through WordNet for full-coverage word sense disam- biguation . In  Proceedings of the 57th Annual Meet- ing of the Association for Computational Linguistics , pages 5682–5691, Florence, Italy. Association for Computational Linguistics. \nLeland McInnes, John Healy, and James Melville. 2018. UMAP: Uniform manifold approximation and pro- jection for dimension reduction . arXiv preprint arXiv:1802.03426 . \nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor- rado, and Jeff Dean. 2013.  Distributed representa- tions of words and phrases and their compositional it y . In  Advances in Neural Information Processing Sys- tems 26 , pages 3111–3119. Curran Associates, Inc. \nGeorge A Miller. 1998.  WordNet: An electronic lexical database . MIT press. \nGeorge A. Miller, Claudia Leacock, Randee Tengi, and Ross T. Bunker. 1993. A semantic concordance . In  Human Language Technology: Proceedings of a Workshop Held at Plainsboro, New Jersey, March 21-24, 1993 . \nAndrea Moro and Roberto Navigli. 2015.  SemEval- 2015 task 13: Multilingual all-words sense disam- biguation and entity linking . In  Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015) , pages 288–297, Denver, Colorado. Association for Computational Linguistics. \nRoberto Navigli. 2009.  Word sense disambiguation: A survey. ACM Comput. Surv., 41(2).\nRoberto Navigli, David Jurgens, and Daniele Vannella. 2013.  SemEval-2013 task 12: Multilingual word sense disambiguation . In  Second Joint Conference on Lexical and Computational Semantics (\\*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval 2013) , pages 222–231, Atlanta, Georgia, USA. Association for Computational Linguistics. \nJeffrey Pennington, Richard Socher, and Christopher Manning. 2014.  Glove: Global vectors for word representation . In  Proceedings of the 2014 Confer- ence on Empirical Methods in Natural Language Pro- cessing (EMNLP) , pages 1532–1543, Doha, Qatar. Association for Computational Linguistics. \nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018.  Deep contextualized word repre- sentations . In  Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech- nologies, Volume 1 (Long Papers) , pages 2227–2237, New Orleans, Louisiana. Association for Computa- tional Linguistics. \nMatthew E. Peters, Mark Neumann, Robert Logan, Roy Schwartz, Vidur Joshi, Sameer Singh, and Noah A. Smith. 2019.  Knowledge enhanced contextual word representations . In  Proceedings of the 2019 Confer- ence on Empirical Methods in Natural Language Pro- cessing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) , pages 43–54, Hong Kong, China. Association for Computational Linguistics. \nMohammad Taher Pilehvar and Jose Camacho-Collados. 2019.  WiC: the word-in-context dataset for evalu- ating context-sensitive meaning representations . In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computa- tional Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) , pages 1267–1273, Minneapolis, Minnesota. Association for Computa- tional Linguistics. \nSameer Pradhan, Edward Loper, Dmitriy Dligach, and Martha Palmer. 2007.  SemEval-2007 task-17: En- glish lexical sample, SRL and all words . In  Pro- ceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007) , pages 87–92, Prague, Czech Republic. Association for Computa- tional Linguistics. \nOﬁr Press and Lior Wolf. 2017.  Using the output em- bedding to improve language models . In  Proceedings "}
{"page": 10, "image_path": "doc_images/2020.acl-main.423_10.jpg", "ocr_text": "of the 15th Conference of the European Chapter of\nthe Association for Computational Linguistics: Vol-\nume 2, Short Papers, pages 157-163, Valencia, Spain.\nAssociation for Computational Linguistics.\n\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\n\nAlessandro Raganato, Jose Camacho-Collados, and\nRoberto Navigli. 2017. Word sense disambiguation:\nA unified evaluation framework and empirical com-\nparison. In Proceedings of the 15th Conference of\nthe European Chapter of the Association for Compu-\ntational Linguistics: Volume 1, Long Papers, pages\n99-110, Valencia, Spain. Association for Computa-\ntional Linguistics.\n\nEmily Reif, Ann Yuan, Martin Wattenberg, Fernanda B\nViegas, Andy Coenen, Adam Pearce, and Been Kim.\n2019. Visualizing and measuring the geometry of\nBERT. In Advances in Neural Information Process-\ning Systems 32, pages 8594-8603. Curran Associates,\nInc.\n\nSascha Rothe and Hinrich Schiitze. 2015. AutoEx-\ntend: Extending word embeddings to embeddings\nfor synsets and lexemes. In Proceedings of the 53rd\nAnnual Meeting of the Association for Computational\nLinguistics and the 7th International Joint Confer-\nence on Natural Language Processing (Volume 1:\nLong Papers), pages 1793-1803, Beijing, China. As-\nsociation for Computational Linguistics.\n\nNathan Schneider. 2014. Lexical semantic analysis in\nnatural language text. Unpublished Doctoral Disser-\ntation, Carnegie Mellon University.\n\nNathan Schneider and Noah A. Smith. 2015. A corpus\nand model integrating multiword expressions and\nsupersenses. In Proceedings of the 2015 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, pages 1537-1547, Denver, Colorado.\nAssociation for Computational Linguistics.\n\nBenjamin Snyder and Martha Palmer. 2004. The En-\nglish all-words task. In Proceedings of SENSEVAL-3,\nthe Third International Workshop on the Evaluation\nof Systems for the Semantic Analysis of Text, pages\n41-43, Barcelona, Spain. Association for Computa-\ntional Linguistics.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, L ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30, pages 5998-6008. Curran Asso-\nciates, Inc.\n\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Aman-\npreet Singh, Julian Michael, Felix Hill, Omer Levy,\nand Samuel Bowman. 2019. SuperGLUE: A stickier\nbenchmark for general-purpose language understand-\ning systems. In Advances in Neural Information\n\nProcessing Systems 32, pages 3266-3280. Curran\nAssociates, Inc.\n\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel Bowman. 2018. GLUE:\nA multi-task benchmark and analysis platform for nat-\nural language understanding. In Proceedings of the\n2018 EMNLP Workshop BlackboxNLP: Analyzing\nand Interpreting Neural Networks for NLP, pages\n353-355, Brussels, Belgium. Association for Com-\nputational Linguistics.\n\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXLNet: Generalized autoregressive pretraining for\nlanguage understanding. In Advances in Neural In-\nformation Processing Systems 32, pages 5753-5763.\nCurran Associates, Inc.\n\nDayu Yuan, Julian Richardson, Ryan Doherty, Colin\nEvans, and Eric Altendorf. 2016. Semi-supervised\nword sense disambiguation with neural models. In\nProceedings of COLING 2016, the 26th International\nConference on Computational Linguistics: Technical\nPapers, pages 1374-1385, Osaka, Japan. The COL-\nING 2016 Organizing Committee.\n\nA_ Supersenses and Their Representation\nin SenseBERT\n\nWe present in table 5 a comprehensive list of Word-\nNet supersenses, as they appear in the WordNet\ndocumentation. In fig. 5 we present a Dendro-\ngram of an Agglomerative hierarchical clustering\nover the supersense embedding vectors learned by\nSenseBERT in pre-training. The clustering shows\na clear separation between Noun senses and Verb\nsenses. Furthermore, we can observe that semanti-\ncally related supersenses are clustered together (i.e,\nnoun.animal and noun.plant).\n\nB_ Training Details\n\nAs hyperparameters for the fine-tuning, we used\nmax_seq_length = 128, chose learning rates from\n{5e—6, le—5, 2e —5, 3e — 5, 5e — 5}, batch sizes\nfrom {16, 32}, and fine-tuned up to 10 epochs for\nall the datasets.\n\n4666\n", "vlm_text": "of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Vol- ume 2, Short Papers , pages 157–163, Valencia, Spain. Association for Computational Linguistics. \nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. \nAlessandro Raganato, Jose Camacho-Collados, and Roberto Navigli. 2017.  Word sense disambiguation: A uniﬁed evaluation framework and empirical com- parison . In  Proceedings of the 15th Conference of the European Chapter of the Association for Compu- tational Linguistics: Volume 1, Long Papers , pages 99–110, Valencia, Spain. Association for Computa- tional Linguistics. \nEmily Reif, Ann Yuan, Martin Wattenberg, Fernanda B Viegas, Andy Coenen, Adam Pearce, and Been Kim. 2019.  Visualizing and measuring the geometry of BERT . In  Advances in Neural Information Process- ing Systems 32 , pages 8594–8603. Curran Associates, Inc. \nSascha Rothe and Hinrich Sch utze. 2015. AutoEx- tend: Extending word embeddings to embeddings for synsets and lexemes . In  Proceedings of the  $53r d$  Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Confer- ence on Natural Language Processing (Volume 1: Long Papers) , pages 1793–1803, Beijing, China. As- sociation for Computational Linguistics. \nNathan Schneider. 2014. Lexical semantic analysis in natural language text.  Unpublished Doctoral Disser- tation, Carnegie Mellon University . \nNathan Schneider and Noah A. Smith. 2015.  A corpus and model integrating multiword expressions and supersenses . In  Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies , pages 1537–1547, Denver, Colorado. Association for Computational Linguistics. \nBenjamin Snyder and Martha Palmer. 2004.  The En- glish all-words task . In  Proceedings of SENSEVAL-3, the Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text , pages 41–43, Barcelona, Spain. Association for Computa- tional Linguistics. \nProcessing Systems 32 , pages 3266–3280. Curran Associates, Inc. \nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2018.  GLUE: A multi-task benchmark and analysis platform for nat- ural language understanding . In  Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP , pages 353–355, Brussels, Belgium. Association for Com- putational Linguistics. \nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car- bonell, Russ R Salakhutdinov, and Quoc V Le. 2019. XLNet: Generalized autoregressive pretraining for language understanding . In  Advances in Neural In- formation Processing Systems 32 , pages 5753–5763. Curran Associates, Inc. \nDayu Yuan, Julian Richardson, Ryan Doherty, Colin Evans, and Eric Altendorf. 2016.  Semi-supervised word sense disambiguation with neural models . In Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers , pages 1374–1385, Osaka, Japan. The COL- ING 2016 Organizing Committee. \nA Supersenses and Their Representation in SenseBERT \nWe present in table  5  a comprehensive list of Word- Net supersenses, as they appear in the WordNet documentation. In ﬁg.  5  we present a Dendro- gram of an Agglomerative hierarchical clustering over the supersense embedding vectors learned by SenseBERT in pre-training. The clustering shows a clear separation between Noun senses and Verb senses. Furthermore, we can observe that semanti- cally related supersenses are clustered together (i.e, noun.animal and noun.plant). \nB Training Details \nAs hyperparameters for the ﬁne-tuning, we used max seq lengt  $h=128$  , chose learning rates from  $\\{5e-6,1e-5,2e-5,3e-5,5e-5\\}$  , batch sizes from    $\\{16,32\\}$  , and ﬁne-tuned up to  10  epochs for all the datasets. \nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,  Ł  ukasz Kaiser, and Illia Polosukhin. 2017.  Attention is all you need . In  Advances in Neural Information Pro- cessing Systems 30 , pages 5998–6008. Curran Asso- ciates, Inc. \nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Aman- preet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2019.  SuperGLUE: A stickier benchmark for general-purpose language understand- ing systems . In  Advances in Neural Information "}
{"page": 11, "image_path": "doc_images/2020.acl-main.423_11.jpg", "ocr_text": "Figure 5: Dendrogram visualization of an Agglomerative hierarchical clustering over the supersense vectors (rows\nof the classifier S) learned by SenseBERT.\n\nName Content Name Content\n\nadj.all All adjective clusters noun.quantity Nouns denoting quantities and units\nof measure\n\nadj.pert Relational adjectives (pertainyms) noun.relation Nouns denoting relations between\npeople or things or ideas\n\nadv.all All adverbs noun.shape Nouns denoting two and three\ndimensional shapes\n\nnoun.Tops Unique beginner for nouns noun.state Nouns denoting stable states of affairs\n\nnoun.act Nouns denoting acts or actions noun.substance Nouns denoting substances\n\nnoun.animal Nouns denoting animals noun.time Nouns denoting time and temporal\nrelations\nnoun.artifact Nouns denoting man-made objects verb.body Verbs of grooming, dressing\n\nand bodily care\n\nnoun.attribute\n\nNouns denoting attributes of people\nand objects\n\nverb.change\n\nVerbs of size, temperature change,\nintensifying, etc.\n\nnoun.body\n\nNouns denoting body parts\n\nverb.cognition\n\nVerbs of thinking, judging, analyzing,\ndoubting\n\nnoun.cognition\n\nNouns denoting cognitive\nprocesses and contents\n\nverb.communication\n\nVerbs of telling, asking, ordering,\nsinging\n\nnoun.communication\n\nNouns denoting communicative\nprocesses and contents\n\nverb.competition\n\nVerbs of fighting, athletic activities\n\nnoun.event\n\nNouns denoting natural events\n\nverb.consumption\n\nVerbs of eating and drinking\n\nnoun.feeling\n\nNouns denoting feelings\n\nverb.contact\n\nVerbs of touching, hitting, tying,\n\nand emotions digging\nnoun.food Nouns denoting foods and drinks. verb.creation Verbs of sewing, baking, painting,\nperforming\nnoun.group Nouns denoting groupings of people | verb.emotion Verbs of feeling\n\nor objects\n\nnoun.location\n\nNouns denoting spatial position\n\nverb.motion\n\nVerbs of walking, flying, swimming\n\nnoun.motive\n\nNouns denoting goals\n\nverb.perception\n\nVerbs of seeing, hearing, feeling\n\nnoun.object\n\nNouns denoting natural objects\n(not man-made)\n\nverb.possession\n\nVerbs of buying, selling, owning\n\nnoun.person\n\nNouns denoting people\n\nverb.social\n\nVerbs of political and social\nactivities and events\n\nnoun.phenomenon\n\nNouns denoting natural phenomena\n\nverb.stative\n\nVerbs of being, having, spatial relations\n\nnoun.plant\n\nNouns denoting plants\n\nverb.weather\n\nVerbs of raining, snowing, thawing,\nthundering\n\nnoun.possession\n\nNouns denoting possession\nand transfer of possession\n\nadj.ppl\n\nParticipial adjectives\n\nnoun.process\n\nNouns denoting natural processes\n\nTable 5: A list of supersense categories from WordNet lexicographer.\n\n4667\n", "vlm_text": "This image is a hierarchical diagram categorizing words into two main groups: \"Nouns\" and \"Verbs,\" each with further subdivisions. \n\n**Nouns** are divided into categories like:\n- State, Cognition, Group, Person, Location, Time, Animal, Plant, Body, Object, Event, Phenomenon, and more.\n\n**Verbs** are divided into categories like:\n- Possession, Social, Emotion, Competition, Cognition, Communication, Change, Perception, Creation, Weather, Body, and more.\n\nEach category branches out into more specific subcategories. Some labels are in different colors for emphasis: black for main categories, while subcategories use grey, green, blue, and red.\nFigure 5: Dendrogram visualization of an Agglomerative hierarchical clustering over the supersense vectors (rows of the classiﬁer S) learned by SenseBERT. \nThe table provides a classification of parts of speech and semantic categories. It is divided into three columns: \"Name,\" \"Content,\" and a repeat of \"Name.\" Here's a summary of each category and its description:\n\n1. **adj.all** - All adjective clusters\n2. **adj.pert** - Relational adjectives (pertainyms)\n3. **adv.all** - All adverbs\n4. **noun.Tops** - Unique beginner for nouns\n5. **noun.act** - Nouns denoting acts or actions\n6. **noun.animal** - Nouns denoting animals\n7. **noun.artifact** - Nouns denoting man-made objects\n8. **noun.attribute** - Nouns denoting attributes of people and objects\n9. **noun.body** - Nouns denoting body parts\n10. **noun.cognition** - Nouns denoting cognitive processes and contents\n11. **noun.communication** - Nouns denoting communicative processes and contents\n12. **noun.event** - Nouns denoting natural events\n13. **noun.feeling** - Nouns denoting feelings and emotions\n14. **noun.food** - Nouns denoting foods and drinks\n15. **noun.group** - Nouns denoting groupings of people or objects\n16. **noun.location** - Nouns denoting spatial position\n17. **noun.motive** - Nouns denoting goals\n18. **noun.object** - Nouns denoting natural objects (not man-made)\n19. **noun.person** - Nouns denoting people\n20. **noun.phenomenon** - Nouns denoting natural phenomena\n21. **noun.plant** - Nouns denoting plants\n22. **noun.possession** - Nouns denoting possession and transfer of possession\n23. **noun.process** - Nouns denoting natural processes\n24. **noun.quantity** - Nouns denoting quantities and units of measure\n25. **noun.relation** - Nouns denoting relations between people or things or ideas\n26. **noun.shape** - Nouns denoting two and three dimensional shapes\n27. **noun.state** - Nouns denoting stable states of affairs\n28. **noun.substance** - Nouns denoting substances\n29. **noun.time** - Nouns denoting time and temporal relations\n30. **verb.body** - Verbs of grooming, dressing, and bodily care\n31. **verb.change** - Verbs of size, temperature change, intensifying, etc.\n32. **verb.cognition** - Verbs of thinking, judging, analyzing, doubting\n33. **verb.communication** - Verbs of telling, asking, ordering, singing\n34. **verb.competition** - Verbs of fighting, athletic activities\n35. **verb.consumption** - Verbs of eating and drinking\n36."}
