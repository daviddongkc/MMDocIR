{"page": 0, "image_path": "doc_images/N19-1240_0.jpg", "ocr_text": "Question Answering by Reasoning Across Documents\nwith Graph Convolutional Networks\n\nNicola De Cao\nUniversity of Edinburgh\nUniversity of Amsterdam\nnicola.decao@uva.nl\n\nAbstract\n\nMost research in reading comprehension has\nfocused on answering questions based on in-\ndividual documents or even single paragraphs.\nWe introduce a neural model which integrates\nand reasons relying on information spread\nwithin documents and across multiple docu-\nments. We frame it as an inference problem on\na graph. Mentions of entities are nodes of this\ngraph while edges encode relations between\ndifferent mentions (e.g., within- and cross-\ndocument coreference). Graph convolutional\nnetworks (GCNs) are applied to these graphs\nand trained to perform multi-step reasoning.\nOur Entity-GCN method is scalable and com-\npact, and it achieves state-of-the-art results on\na multi-document question answering dataset,\nWIKIHOopP (Welb!l et al., 2018).\n\n1 Introduction\n\nThe long-standing goal of natural language under-\nstanding is the development of systems which can\nacquire knowledge from text collections. Fresh in-\nterest in reading comprehension tasks was sparked\nby the availability of large-scale datasets, such as\nSQuAD (Rajpurkar et al., 2016) and CNN/Daily\nMail (Hermann et al., 2015), enabling end-to-end\ntraining of neural models (Seo et al., 2016; Xiong\net al., 2016; Shen et al., 2017). These systems,\ngiven a text and a question, need to answer the\nquery relying on the given document. Recently,\nit has been observed that most questions in these\ndatasets do not require reasoning across the doc-\nument, but they can be answered relying on in-\nformation contained in a single sentence (Weis-\nsenborn et al., 2017). The last generation of\nlarge-scale reading comprehension datasets, such\nas a NarrativeQA (Kocisky et al., 2018), Trivi-\naQA (Joshi et al., 2017), and RACE (Lai et al.,\n2017), have been created in such a way as to ad-\ndress this shortcoming and to ensure that systems\n\nWilker Aziz\nUniversity of Amsterdam\nw.aziz@uva.nl\n\nIvan Titov\nUniversity of Edinburgh\nUniversity of Amsterdam\nititov@inf.ed.ac.uk\n\nthorildsplan is a small park in Kristineberg in\nStockholm, named in 1925 after the writer [..]\n\nStockholm is the capital of Sweden q\n] H\n\n‘ and the most populous city in [..\n\nquery: country Thorildsplan\ncandidates: (Denmark, Finland, Sweden, Italy, ...}\nanswer: Sweden\n\nFigure 1: A sample from WIKIHOP where multi-step\nreasoning and information combination from different\ndocuments is necessary to infer the correct answer.\n\nrelying only on local information cannot achieve\ncompetitive performance.\n\nEven though these new datasets are challeng-\ning and require reasoning within documents, many\nquestion answering and search applications re-\nquire aggregation of information across multiple\ndocuments. The WIKIHOoP dataset (Welbl et al.,\n2018) was explicitly created to facilitate the devel-\nopment of systems dealing with these scenarios.\nEach example in WIKIHOP consists of a collec-\ntion of documents, a query and a set of candidate\nanswers (Figure 1). Though there is no guaran-\ntee that a question cannot be answered by relying\njust on a single sentence, the authors ensure that it\nis answerable using a chain of reasoning crossing\ndocument boundaries.\n\nThough an important practical problem, the\nmulti-hop setting has so far received little at-\ntention. The methods reported by Welbl et al.\n(2018) approach the task by merely concatenat-\ning all documents into a single long text and train-\ning a standard RNN-based reading comprehen-\nsion model, namely, BiDAF (Seo et al., 2016)\nand FastQA (Weissenborn et al., 2017). Docu-\nment concatenation in this setting is also used in\nWeaver (Raison et al., 2018) and MHPGM (Bauer\net al., 2018). The only published paper which\n\n2306\n\nProceedings of NAACL-HLT 2019, pages 2306-2317\nMinneapolis, Minnesota, June 2 - June 7, 2019. ©2019 Association for Computational Linguistics\n", "vlm_text": "Question Answering by Reasoning Across Documents with Graph Convolutional Networks \nIvan Titov University of Edinburgh University of Amsterdam ititov@inf.ed.ac.uk \nWilker Aziz University of Amsterdam w.aziz@uva.nl \nNicola De Cao University of Edinburgh University of Amsterdam nicola.decao@uva.nl \nThe image is a visual representation of a query process to determine the country associated with Thorildsplan, which is a small park in Kristineberg, Stockholm. The text explains how information from sentences is used to deduce that Stockholm is in Sweden. The query asks for the country of Thorildsplan, and among the candidate countries (Denmark, Finland, Sweden, Italy, etc.), the correct answer is identified as Sweden. The image conveys this logical inference through highlighted keywords and arrows.\nAbstract \nMost research in reading comprehension has focused on answering questions based on in- dividual documents or even single paragraphs. We introduce a neural model which integrates and reasons relying on information spread within documents and across multiple docu- ments. We frame it as an inference problem on a graph. Mentions of entities are nodes of this graph while edges encode relations between different mentions (e.g., within- and cross- document coreference). Graph convolutional networks (GCNs) are applied to these graphs and trained to perform multi-step reasoning. Our Entity-GCN method is scalable and com- pact, and it achieves state-of-the-art results on a multi-document question answering dataset, W IKI H OP  ( Welbl et al. ,  2018 ). \nFigure 1: A sample from W IKI H OP  where multi-step reasoning and information combination from different documents is necessary to infer the correct answer. \nrelying only on local information cannot achieve competitive performance. \nEven though these new datasets are challeng- ing and require reasoning within documents, many question answering and search applications re- quire aggregation of information across multiple documents. The W IKI H OP  dataset ( Welbl et al. , 2018 ) was explicitly created to facilitate the devel- opment of systems dealing with these scenarios. Each example in W IKI H OP  consists of a collec- tion of documents, a query and a set of candidate answers (Figure  1 ). Though there is no guaran- tee that a question cannot be answered by relying just on a single sentence, the authors ensure that it is answerable using a chain of reasoning crossing document boundaries. \n1 Introduction \nThe long-standing goal of natural language under- standing is the development of systems which can acquire knowledge from text collections. Fresh in- terest in reading comprehension tasks was sparked by the availability of large-scale datasets, such as SQuAD ( Rajpurkar et al. ,  2016 ) and CNN/Daily Mail ( Hermann et al. ,  2015 ), enabling end-to-end training of neural models ( Seo et al. ,  2016 ;  Xiong et al. ,  2016 ;  Shen et al. ,  2017 ). These systems, given a text and a question, need to answer the query relying on the given document. Recently, it has been observed that most questions in these datasets do not require reasoning across the doc- ument, but they can be answered relying on in- formation contained in a single sentence ( Weis- senborn et al. ,  2017 ). The last generation of large-scale reading comprehension datasets, such as a NarrativeQA ( Kocisky et al. ,  2018 ), Trivi- aQA ( Joshi et al. ,  2017 ), and RACE ( Lai et al. , 2017 ), have been created in such a way as to ad- dress this shortcoming and to ensure that systems \nThough an important practical problem, the multi-hop setting has so far received little at- tention. The methods reported by  Welbl et al. ( 2018 ) approach the task by merely concatenat- ing all documents into a single long text and train- ing a standard RNN-based reading comprehen- sion model, namely, BiDAF ( Seo et al. ,  2016 ) and FastQA ( Weissenborn et al. ,  2017 ). Docu- ment concatenation in this setting is also used in Weaver ( Raison et al. ,  2018 ) and MHPGM ( Bauer et al. ,  2018 ). The only published paper which goes beyond concatenation is due to  Dhingra et al.  ( 2018 ), where they augment RNNs with jump-links corresponding to co-reference edges. Though these edges provide a structural bias, the RNN states are still tasked with passing the infor- mation across the document and performing multi- hop reasoning. "}
{"page": 1, "image_path": "doc_images/N19-1240_1.jpg", "ocr_text": "goes beyond concatenation is due to Dhingra\net al. (2018), where they augment RNNs with\njump-links corresponding to co-reference edges.\nThough these edges provide a structural bias, the\nRNN states are still tasked with passing the infor-\nmation across the document and performing multi-\nhop reasoning.\n\nInstead, we frame question answering as an\ninference problem on a graph representing the\ndocument collection. Nodes in this graph corre-\nspond to named entities in a document whereas\nedges encode relations between them (e.g., cross-\nand within-document coreference links or simply\nco-occurrence in a document). We assume that\nreasoning chains can be captured by propagat-\ning local contextual information along edges in\nthis graph using a graph convolutional network\n(GCN) (Kipf and Welling, 2017).\n\nThe multi-document setting imposes scalabil-\nity challenges. In realistic scenarios, a system\nneeds to learn to answer a query for a given col-\nlection (e.g., Wikipedia or a domain-specific set\nof documents). In such scenarios one cannot af-\nford to run expensive document encoders (e.g.,\nRNN or transformer-like self-attention (Vaswani\net al., 2017)), unless the computation can be pre-\nprocessed both at train and test time. Even if\n(similarly to WIKIHOP creators) one considers a\ncoarse-to-fine approach, where a set of potentially\nrelevant documents is provided, re-encoding them\nin a query-specific way remains the bottleneck. In\ncontrast to other proposed methods (e.g., (Dhingra\net al., 2018; Raison et al., 2018; Seo et al., 2016)),\nwe avoid training expensive document encoders.\n\nIn our approach, only a small query encoder,\nthe GCN layers and a simple feed-forward an-\nswer selection component are learned. Instead\nof training RNN encoders, we use contextualized\nembeddings (ELMo) to obtain initial (local) rep-\nresentations of nodes. This implies that only a\nlightweight computation has to be performed on-\nline, both at train and test time, whereas the rest\nis preprocessed. Even in the somewhat contrived\nWIKIHOP setting, where fairly small sets of can-\ndidates are provided, the model is at least 5 times\nfaster to train than BiDAF! Interestingly, when\nwe substitute ELMo with simple pre-trained word\nembeddings, Entity-GCN still performs on par\n\n'When compared to the ‘small’ and hence fast BiDAF\nmodel reported in Welbl et al. (2018), which is 25% less ac-\ncurate than our Entity-GCN. Larger RNN models are prob-\nlematic also because of GPU memory constraints.\n\nwith many techniques that use expensive question-\naware recurrent document encoders.\n\nDespite not using recurrent document encoders,\nthe full Entity-GCN model achieves over 2% im-\nprovement over the best previously-published re-\nsults. As our model is efficient, we also reported\nresults of an ensemble which brings further 3.6%\nof improvement and only 3% below the human\nperformance reported by Welbl et al. (2018). Our\ncontributions can be summarized as follows:\n\n© we present a novel approach for multi-hop\nQA that relies on a (pre-trained) document\nencoder and information propagation across\nmultiple documents using graph neural net-\nworks;\n\ne we provide an efficient training technique\nwhich relies on a slower offline and a faster\non-line computation that does not require ex-\npensive document processing;\n\n© we empirically show that our algorithm is ef-\nfective, presenting an improvement over pre-\nvious results.\n\n2 Method\n\nIn this section we explain our method. We firs\nintroduce the dataset we focus on, WIKIHOP\nby Welbl et al. (2018), as well as the task ab-\nstraction. We then present the building blocks thai\nmake up our Entity-GCN model, namely, an en-\ntity graph used to relate mentions to entities within\nand across documents, a document encoder used\nto obtain representations of mentions in context,\nand a relational graph convolutional network that\npropagates information through the entity graph.\n\n2.1 Dataset and Task Abstraction\n\nData The WIKIHOP dataset comprises of tuples\n(q, Sq, Cq,a*) where: q is a query/question, Sq is\na set of supporting documents, Cy is a set of candi-\ndate answers (all of which are entities mentioned\nin S,), and a* € C;, is the entity that correctly\nanswers the question. WIKIHOP is assembled as-\nsuming that there exists a corpus and a knowledge\nbase (KB) related to each other. The KB contains\ntriples (s, 7,0) where s is a subject entity, o an ob-\nject entity, and r a unidirectional relation between\nthem. Welbl et al. (2018) used WIKIPEDIA as cor-\npus and WIKIDATA (Vrande¢ié, 2012) as KB. The\nKB is only used for constructing WIKIHOP: Welbl\n\n2307\n", "vlm_text": "\nInstead, we frame question answering as an inference problem on a graph representing the document collection. Nodes in this graph corre- spond to named entities in a document whereas edges encode relations between them (e.g., cross- and within-document coreference links or simply co-occurrence in a document). We assume that reasoning chains can be captured by propagat- ing local contextual information along edges in this graph using a graph convolutional network (GCN) ( Kipf and Welling ,  2017 ). \nThe multi-document setting imposes scalabil- ity challenges. In realistic scenarios, a system needs to learn to answer a query for a given col- lection (e.g., Wikipedia or a domain-speciﬁc set of documents). In such scenarios one cannot af- ford to run expensive document encoders (e.g., RNN or transformer-like self-attention ( Vaswani et al. ,  2017 )), unless the computation can be pre- processed both at train and test time. Even if (similarly to W IKI H OP  creators) one considers a coarse-to-ﬁne approach, where a set of potentially relevant documents is provided, re-encoding them in a query-speciﬁc way remains the bottleneck. In contrast to other proposed methods (e.g., ( Dhingra et al. ,  2018 ;  Raison et al. ,  2018 ;  Seo et al. ,  2016 )), we avoid training expensive document encoders. \nIn our approach, only a small query encoder, the GCN layers and a simple feed-forward an- swer selection component are learned. Instead of training RNN encoders, we use contextualized embeddings (ELMo) to obtain initial (local) rep- resentations of nodes. This implies that only a lightweight computation has to be performed on- line, both at train and test time, whereas the rest is preprocessed. Even in the somewhat contrived W IKI H OP  setting, where fairly small sets of can- didates are provided, the model is at least 5 times faster to train than BiDAF.   Interestingly, when we substitute ELMo with simple pre-trained word embeddings, Entity-GCN still performs on par with many techniques that use expensive question- aware recurrent document encoders. \n\nDespite not using recurrent document encoders, the full Entity-GCN model achieves over   $2\\%$   im- provement over the best previously-published re- sults. As our model is efﬁcient, we also reported results of an ensemble which brings further  $3.6\\%$  of improvement and only   $3\\%$   below the human performance reported by  Welbl et al.  ( 2018 ). Our contributions can be summarized as follows: \n•  we present a novel approach for multi-hop QA that relies on a (pre-trained) document encoder and information propagation across multiple documents using graph neural net- works; •  we provide an efﬁcient training technique which relies on a slower ofﬂine and a faster on-line computation that does not require ex- pensive document processing; •  we empirically show that our algorithm is ef- fective, presenting an improvement over pre- vious results. \n2 Method \nIn this section we explain our method. We ﬁrst introduce the dataset we focus on, W IKI H OP by  Welbl et al.  ( 2018 ), as well as the task ab- straction. We then present the building blocks that make up our Entity-GCN model, namely, an  en- tity graph  used to relate mentions to entities within and across documents, a  document encoder  used to obtain representations of mentions in context, and a  relational graph convolutional network  that propagates information through the entity graph. \n2.1 Dataset and Task Abstraction \nData The W IKI H OP  dataset comprises of tuples  $\\langle q,S_{q},C_{q},a^{\\star}\\rangle$  where:    $q$   is a query/question,    $S_{q}$   is a set of supporting documents,  $C_{q}$   is a set of candi- date answers (all of which are entities mentioned in    $S_{q})$  ), and    $a^{\\star}\\,\\in\\,C_{q}$   is the entity that correctly answers the question. W IKI H OP  is assembled as- suming that there exists a corpus and a knowledge base (KB) related to each other. The KB contains triples    $\\langle s,r,o\\rangle$  where    $s$   is a subject entity,    $o$   an ob- ject entity, and    $r$   a unidirectional relation between them.  Welbl et al.  ( 2018 ) used W IKIPEDIA  as cor- pus and W IKIDATA  ( Vrandeˇ ci´ ,  2012 ) as KB. The KB is only used for constructing W IKI H OP :  Welbl et al.  ( 2018 ) retrieved the supporting documents  $S_{q}$   from the corpus looking at mentions of subject and object entities in the text. Note that the set    $S_{q}$  (not the KB) is provided to the QA system, and not all of the supporting documents are relevant for the query but some of them act as distractors. Queries, on the other hand, are not expressed in natural lan- guage, but instead consist of tuples    $\\langle s,r,?\\rangle$  where the object entity is unknown and it has to be in- ferred by reading the support documents. There- fore, answering a query corresponds to ﬁnding the entity    $a^{\\star}$  that is the object of a tuple in the KB with subject  $s$   and relation    $r$   among the provided set of candidate answers  $C_{q}$  . "}
{"page": 2, "image_path": "doc_images/N19-1240_2.jpg", "ocr_text": "et al. (2018) retrieved the supporting documents\nSq from the corpus looking at mentions of subject\nand object entities in the text. Note that the set Sy\n(not the KB) is provided to the QA system, and not\nall of the supporting documents are relevant for the\nquery but some of them act as distractors. Queries,\non the other hand, are not expressed in natural lan-\nguage, but instead consist of tuples (s,7,?) where\nthe object entity is unknown and it has to be in-\nferred by reading the support documents. There-\nfore, answering a query corresponds to finding the\nentity a* that is the object of a tuple in the KB with\nsubject s and relation r among the provided set of\ncandidate answers Cy.\n\nTask The goal is to learn a model that can iden-\ntify the correct answer a* from the set of support-\ning documents S,. To that end, we exploit the\navailable supervision to train a neural network that\ncomputes scores for candidates in C,. We estimate\nthe parameters of the architecture by maximizing\nthe likelihood of observations. For prediction, we\nthen output the candidate that achieves the high-\nest probability. In the following, we present our\nmodel discussing the design decisions that enable\nmulti-step reasoning and an efficient computation.\n\n2.2. Reasoning on an Entity Graph\n\nEntity graph In an offline step, we organize the\ncontent of each training instance in a graph con-\nnecting mentions of candidate answers within and\nacross supporting documents. For a given query\nq = (s,r,?), we identify mentions in S, of the en-\ntities in C, U {s} and create one node per mention.\nThis process is based on the following heuristic:\n\n1. we consider mentions spans in S, exactly\nmatching an element of Cy U {s}. Admit-\ntedly, this is a rather simple strategy which\nmay suffer from low recall.\n\n2. we use predictions from a coreference reso-\nlution system to add mentions of elements in\nCq U {s} beyond exact matching (including\nboth noun phrases and anaphoric pronouns).\nIn particular, we use the end-to-end corefer-\nence resolution by Lee et al. (2017).\n\n3. we discard mentions which are ambiguously\nresolved to multiple coreference chains; this\nmay sacrifice recall, but avoids propagating\nambiguity.\n\nFigure 2: Supporting documents (dashed ellipses) or-\nganized as a graph where nodes are mentions of ei-\nther candidate entities or query entities. Nodes with the\nsame color indicates they refer to the same entity (ex-\nact match, coreference or both). Nodes are connected\nby three simple relations: one indicating co-occurrence\nin the same document (solid edges), another connect-\ning mentions that exactly match (dashed edges), and a\nthird one indicating a coreference (bold-red line).\n\nTo each node v;, we associate a continuous an-\nnotation x; € R? which represents an entity in\nthe context where it was mentioned (details in Sec-\ntion 2.3). We then proceed to connect these men-\ntions i) if they co-occur within the same document\n(we will refer to this as DOC-BASED edges), ii)\nif the pair of named entity mentions is identical\n(MATCH edges—these may connect nodes across\nand within documents), or iii) if they are in the\nsame coreference chain, as predicted by the exter-\nnal coreference system (COREF edges). Note that\nMATCH edges when connecting mentions in the\nsame document are mostly included in the set of\nedges predicted by the coreference system. Hav-\ning the two types of edges lets us distinguish be-\ntween less reliable edges provided by the coref-\nerence system and more reliable (but also more\nsparse) edges given by the exact-match heuristic.\nWe treat these three types of connections as three\ndifferent types of relations. See Figure 2 for an\nillustration. In addition to that, and to prevent hav-\ning disconnected graphs, we add a fourth type of\nrelation (COMPLEMENT edge) between any two\nnodes that are not connected with any of the other\nrelations. We can think of these edges as those\nin the complement set of the entity graph with re-\nspect to a fully connected graph.\n\nMulti-step reasoning Our model then ap-\nproaches multi-step reasoning by transforming\nnode representations (Section 2.3 for details)\nwith a differentiable message passing algorithm\nthat propagates information through the entity\n\n2308\n", "vlm_text": "\nTask The goal is to learn a model that can iden- tify the correct answer    $a^{\\star}$  from the set of support- ing documents    $S_{q}$  . To that end, we exploit the available supervision to train a neural network that computes scores for candidates in  $C_{q}$  . We estimate the parameters of the architecture by maximizing the likelihood of observations. For prediction, we then output the candidate that achieves the high- est probability. In the following, we present our model discussing the design decisions that enable multi-step reasoning and an efﬁcient computation. \n2.2 Reasoning on an Entity Graph \nEntity graph In an ofﬂine step, we organize the content of each training instance in a graph con- necting mentions of candidate answers within and across supporting documents. For a given query  $q=\\langle s,r,?\\rangle$  , we identify mentions in    $S_{q}$   of the en- tities in  $C_{q}\\cup\\{s\\}$   and create one node per mention. This process is based on the following heuristic: \n1. we consider mentions spans in    $S_{q}$   exactly matching an element of    $C_{q}\\cup\\{s\\}$  . Admit- tedly, this is a rather simple strategy which may suffer from low recall. 2. we use predictions from a coreference reso- lution system to add mentions of elements in  $C_{q}\\cup\\{s\\}$   beyond exact matching (including both noun phrases and anaphoric pronouns). In particular, we use the end-to-end corefer- ence resolution by  Lee et al.  ( 2017 ). 3. we discard mentions which are ambiguously resolved to multiple coreference chains; this may sacriﬁce recall, but avoids propagating ambiguity. \nThe image is a graph representing supporting documents as nodes, where nodes are mentions of either candidate entities or query entities. Nodes are color-coded to show they refer to the same entity based on exact match, coreference, or both. There are three types of connections between the nodes:\n\n- Solid edges indicate co-occurrence in the same document.\n- Dashed edges connect mentions that exactly match.\n- A bold-red line indicates a coreference.\n\nThe nodes are contained within dashed ellipses, organizing them as a graph.\nTo each node    $v_{i}$  , we associate a continuous an- notation  $\\mathbf{x}_{i}\\,\\in\\,\\mathbb{R}^{D}$    which represents an entity in the context where it was mentioned (details in Sec- tion  2.3 ). We then proceed to connect these men- tions i) if they co-occur within the same document (we will refer to this as  DOC-BASED  edges), ii) if the pair of named entity mentions is identical ( MATCH  edges—these may connect nodes across and within documents), or iii) if they are in the same coreference chain, as predicted by the exter- nal coreference system ( COREF  edges). Note that MATCH  edges when connecting mentions in the same document are mostly included in the set of edges predicted by the coreference system. Hav- ing the two types of edges lets us distinguish be- tween less reliable edges provided by the coref- erence system and more reliable (but also more sparse) edges given by the exact-match heuristic. We treat these three types of connections as three different types of relations. See Figure  2  for an illustration. In addition to that, and to prevent hav- ing disconnected graphs, we add a fourth type of relation ( COMPLEMENT  edge) between any two nodes that are not connected with any of the other relations. We can think of these edges as those in the complement set of the entity graph with re- spect to a fully connected graph. \nMulti-step reasoning Our model then ap- proaches multi-step reasoning by transforming node representations (Section  2.3  for details) with a differentiable message passing algorithm that propagates information through the entity graph. The algorithm is parameterized by a graph convolutional network (GCN) ( Kipf and Welling ,  2017 ), in particular, we employ relational-GCNs ( Schlichtkrull et al. ,  2018 ), an ex- tended version that accommodates edges of differ- ent types. In Section  2.4  we describe the propaga- tion rule. "}
{"page": 3, "image_path": "doc_images/N19-1240_3.jpg", "ocr_text": "graph. The algorithm is parameterized by\na graph convolutional network (GCN) (Kipf\nand Welling, 2017), in particular, we employ\nrelational-GCNs (Schlichtkrull et al., 2018), an ex-\ntended version that accommodates edges of differ-\nent types. In Section 2.4 we describe the propaga-\ntion rule.\n\nEach step of the algorithm (also referred to as\na hop) updates all node representations in parallel.\nIn particular, a node is updated as a function of\nmessages from its direct neighbours, and a mes-\nsage is possibly specific to a certain relation. At\nthe end of the first step, every node is aware of ev-\nery other node it connects directly to. Besides, the\nneighbourhood of a node may include mentions\nof the same entity as well as others (e.g., same-\ndocument relation), and these mentions may have\noccurred in different documents. Taking this idea\nrecursively, each further step of the algorithm al-\nlows a node to indirectly interact with nodes al-\nready known to their neighbours. After L layers of\nR-GCN, information has been propagated through\npaths connecting up to L + 1 nodes.\n\nWe start with node representations (HOW,\nand transform them by applying L layers of R-\nGCN obtaining th. Together with a rep-\nresentation q of the query, we define a distribution\nover candidate answers and we train maximizing\nthe likelihood of observations. The probability of\nselecting a candidate c¢ € Cy as an answer is then\n\nP(clq, Cq, Sq) « exp (9x fo({a; n{))) ;\nqd)\n\nwhere f, is a parameterized affine transforma-\ntion, and M, is the set of node indices such that\ni € M, only if node v; is a mention of c. The\nmax operator in Equation | is necessary to select\nthe node with highest predicted probability since a\ncandidate answer is realized in multiple locations\nvia different nodes.\n\n2.3 Node Annotations\n\nKeeping in mind we want an efficient model, we\nencode words in supporting documents and in the\nquery using only a pre-trained model for contex-\ntualized word representations rather than training\nour own encoder. Specifically, we use ELMo? (Pe-\nters et al., 2018), a pre-trained bi-directional lan-\n\n>The use of ELMo is an implementation choice, and, in\n\nprinciple, any other contextual pre-trained model could be\nused (Radford et al., 2018; Devlin et al., 2018).\n\nguage model that relies on character-based input\nrepresentation. ELMo representations, differently\nrom other pre-trained word-based models (e.g.,\nword2vec (Mikolov et al., 2013) or GloVe (Pen-\nnington et al., 2014)), are contextualized since\neach token representation depends on the entire\next excerpt (i.e., the whole sentence).\n\nWe choose not to fine tune nor propagate gradi-\nents through the ELMo architecture, as it would\nhave defied the goal of not having specialized\nRNN encoders. In the experiments, we will also\nablate the use of ELMo showing how our model\nbehaves using non-contextualized word represen-\nations (we use GloVe).\n\nDocuments pre-processing ELMo encodings\nare used to produce a set of representations\n{ xi} 1, where x; € R? denotes the ith candidate\nmention in context. Note that these representa-\nions do not depend on the query yet and no train-\nable model was used to process the documents so\nar, that is, we use ELMo as a fixed pre-trained en-\ncoder. Therefore, we can pre-compute representa-\nion of mentions once and store them for later use.\n\nQuery-dependent mention encodings ELMo\nencodings are used to produce a query represen-\nation q € R* as well. Here, q is a concatena-\nion of the final outputs from a bidirectional RNN\nlayer trained to re-encode ELMo representations\nof words in the query. The vector q is used to com-\npute a query-dependent representation of mentions\n{x;}_, as well as to compute a probability distri-\nbution over candidates (as in Equation 1). Query-\ndependent mention encodings X; = f,(q,x;) are\ngenerated by a trainable function f, which is pa-\nrameterized by a feed-forward neural network.\n\n2.4 Entity Relational Graph Convolutional\nNetwork\n\nOur model uses a gated version of the original\nR-GCN propagation rule. At the first layer, all\nhidden node representation are initialized with the\nquery-aware encodings n”) = %;. Then, at each\nlayer 0 < ¢ < L, the update message ul? to the\nith node is a sum of a transformation f, of the cur-\nrent node rey i\npresentation h;\n\nof its neighbours:\n\n1\nw= £601) + DDE folly?) 5\n\n| GEN, rERiy\n\nand transformations\n\n2309\n", "vlm_text": "\nEach step of the algorithm (also referred to as a  hop ) updates all node representations in parallel. In particular, a node is updated as a function of messages from its direct neighbours, and a mes- sage is possibly speciﬁc to a certain relation. At the end of the ﬁrst step, every node is aware of ev- ery other node it connects directly to. Besides, the neighbourhood of a node may include mentions of the same entity as well as others (e.g., same- document relation), and these mentions may have occurred in different documents. Taking this idea recursively, each further step of the algorithm al- lows a node to indirectly interact with nodes al- ready known to their neighbours. After    $L$   layers of R-GCN, information has been propagated through paths connecting up to    $L+1$   nodes. \nWe start with node representat ns    $\\{\\mathbf{h}_{i}^{(0)}\\}_{i=1}^{N}$    } , and transform them by applying  L  layers of R- GCN obtaining    $\\{\\mathbf{h}_{i}^{(L)}\\}_{i=1}^{N}$  . Together with a rep- resentation  q  of the query, we deﬁne a distribution over candidate answers and we train maximizing the likelihood of observations. The probability of selecting a candidate  $c\\in C_{q}$   as an answer is then \n\n$$\nP(c|q,C_{q},S_{q})\\propto\\exp\\left(\\operatorname*{max}_{i\\in\\mathcal{M}_{c}}f_{o}([\\mathbf{q},\\mathbf{h}_{i}^{(L)}])\\right)\\;,\n$$\n \nwhere    $f_{o}$   is a parameterized afﬁne transforma- tion, and    $\\mathcal{M}_{c}$   is the set of node indices such that  $i\\ \\in\\ \\mathcal{M}_{c}$   only if node    $v_{i}$   is a mention of    $c$  . The max  operator in Equation  1  is necessary to select the node with highest predicted probability since a candidate answer is realized in multiple locations via different nodes. \n2.3 Node Annotations \nKeeping in mind we want an efﬁcient model, we encode words in supporting documents and in the query using only a pre-trained model for contex- tualized word representations rather than training our own encoder. Speciﬁcally, we use ELMo 2   ( Pe- ters et al. ,  2018 ), a pre-trained bi-directional lan- guage model that relies on character-based input representation. ELMo representations, differently from other pre-trained word-based models (e.g., word2vec  ( Mikolov et al. ,  2013 ) or GloVe ( Pen- nington et al. ,  2014 )), are contextualized since each token representation depends on the entire text excerpt (i.e., the whole sentence). \n\nWe choose not to ﬁne tune nor propagate gradi- ents through the ELMo architecture, as it would have deﬁed the goal of not having specialized RNN encoders. In the experiments, we will also ablate the use of ELMo showing how our model behaves using non-contextualized word represen- tations (we use GloVe). \nDocuments pre-processing ELMo encodings are used to produce a set of representations  $\\{\\mathbf{x}_{i}\\}_{i=1}^{N}$  , where  $\\mathbf{x}_{i}\\in\\mathbb{R}^{D}$   denotes the  i th candidate mention in context. Note that these representa- tions do not depend on the query yet and no train- able model was used to process the documents so far, that is, we use ELMo as a ﬁxed pre-trained en- coder. Therefore, we can pre-compute representa- tion of mentions once and store them for later use. \nQuery-dependent mention encodings ELMo encodings are used to produce a query represen- tation    $\\mathbf{q}\\,\\in\\,\\mathbb{R}^{K}$    as well. Here,    $\\mathbf{q}$   is a concatena- tion of the ﬁnal outputs from a bidirectional RNN layer trained to re-encode ELMo representations of words in the query. The vector    $\\mathbf{q}$   is used to com- pute a query-dependent representation of mentions  $\\{\\hat{\\mathbf{x}}_{i}\\}_{i=1}^{N}$    as well as to compute a probability distri- bution over candidates (as in Equation  1 ). Query- dependent mention encodings    ${\\hat{\\mathbf{x}}}_{i}=f_{x}(\\mathbf{q},\\mathbf{x}_{i})$   are generated by a trainable function    $f_{x}$   which is pa- rameterized by a feed-forward neural network. \n2.4 Entity Relational Graph Convolutional Network \nOur model uses a gated version of the original R-GCN propagation rule. At the ﬁrst layer, all hidden node representation are initialized with the query-aware encodings    $\\mathbf{h}_{i}^{(0)}\\,=\\,\\hat{\\mathbf{x}}_{i}$  . Then, at each layer    $0\\,\\leq\\,\\ell\\,\\leq\\,L$  , the update message    $\\mathbf{u}_{i}^{(\\ell)}$  to the i th node is a sum of a transformation    $f_{s}$   of the cur- rent node representation    $\\mathbf{h}_{i}^{(\\ell)}$  and transformations of its neighbours: \n\n$$\n\\mathbf{u}_{i}^{(\\ell)}=f_{s}(\\mathbf{h}_{i}^{(\\ell)})+\\frac{1}{|\\mathcal{N}_{i}|}\\sum_{j\\in\\mathcal{N}_{i}}\\sum_{r\\in\\mathcal{R}_{i j}}f_{r}(\\mathbf{h}_{j}^{(\\ell)})\\ ,\n$$\n "}
{"page": 4, "image_path": "doc_images/N19-1240_4.jpg", "ocr_text": "where J; is the set of indices of nodes neighbour-\ning the ith node, R;; is the set of edge annotations\nbetween i and j, and f, is a parametrized func-\ntion specific to an edge type r € R. Recall the\navailable relations from Section 2.2, namely, R =\n{DOC-BASED, MATCH, COREF, COMPLEMENT}.\n\nA gating mechanism regulates how much of the\nupdate message propagates to the next step. This\nprovides the model a way to prevent completely\noverwriting past information. Indeed, if all neces-\nsary information to answer a question is present at\na layer which is not the last, then the model should\nlearn to stop using neighbouring information for\nthe next steps. Gate levels are computed as\n\na =o (fo (fw?.ni1)),\n\nwhere o(-) is the sigmoid function and fy, a\nparametrized transformation. Ultimately, the up-\ndated representation is a gated combination of the\nprevious representation and a non-linear transfor-\nmation of the update message:\n\nhy) = 6(u)?) ai? thi oa),\nwhere ¢(-) is any nonlinear function (we used\ntanh) and © stands for element-wise multiplica-\ntion. All transformations f, are affine and they are\nnot layer-dependent (since we would like to use\nas few parameters as possible to decrease model\ncomplexity promoting efficiency and scalability).\n\n3 Experiments\n\nIn this section, we compare our method against re-\ncent work as well as preforming an ablation study\nusing the WIKIHOP dataset (Welbl et al., 2018).\nSee Appendix A in the supplementary material for\na description of the hyper-parameters of our model\nand training details.\n\nWIKIHOP We use WIKIHOP for training, val-\nidation/development and test. The test set is not\npublicly available and therefore we measure per-\nformance on the validation set in almost all ex-\nperiments. WIKIHoP has 43,738/ 5,129/ 2,451\nquery-documents samples in the training, valida-\ntion and test sets respectively for a total of 51,318\nsamples. Authors constructed the dataset as de-\nscribed in Section 2.1 selecting samples with a\ngraph traversal up to a maximum chain length of\n3 documents (see Table | for additional dataset\nstatistics). WIKIHOP comes in two versions, a\n\nMin Max Avg. Median\n# candidates 2 719 19.8 14\n# documents 3 63 13.7 11\n# tokens/doc. 4 2,046 100.4 91\n\nTable 1: WIKIHOP dataset statistics from Welb] et al.\n(2018): number of candidates and documents per sam-\nple and document length.\n\nstandard (unmasked) one and a masked one. The\nmasked version was created by the authors to test\nwhether methods are able to learn lexical abstrac-\ntion. In this version, all candidates and all men-\ntions of them in the support documents are re-\nplaced by random but consistent placeholder to-\nkens. Thus, in the masked version, mentions are\nalways referred to via unambiguous surface forms.\nWe do not use coreference systems in the masked\nversion as they rely crucially on lexical realization\nof mentions and cannot operate on masked tokens.\n\n3.1 Comparison\n\nIn this experiment, we compare our Enitity-\nGCN against recent prior work on the same\ntask. We present test and development re-\nsults (when present) for both versions of the\ndataset in Table 2. From Welbl et al. (2018),\nwe list an oracle based on human performance\nas well as two standard reading comprehension\nmodels, namely BiDAF (Seo et al., 2016) and\nFastQA (Weissenborn et al., 2017). We also com-\npare against Coref-GRU (Dhingra et al., 2018),\nMHPGM (Bauer et al., 2018), and Weaver (Rai-\nson et al., 2018). Additionally, we include results\nof MHQA-GRN (Song et al., 2018), from a recent\narXiv preprint describing concurrent work. They\njointly train graph neural networks and recurrent\nencoders. We report single runs of our two best\nsingle models and an ensemble one on the un-\nmasked test set (recall that the test set is not pub-\nlicly available and the task organizers only report\nunmasked results) as well as both versions of the\nvalidation set.\n\nEntity-GCN (best single model without coref-\nerence edges) outperforms all previous work by\nover 2% points. We additionally re-ran BiDAF\nbaseline to compare training time: when using a\nsingle Titan X GPU, BiDAF and Entity-GCN pro-\ncess 12.5 and 57.8 document sets per second, re-\nspectively. Note that Welbl et al. (2018) had to\nuse BiDAF with very small state dimensionalities\n\n2310\n", "vlm_text": "where    $\\mathcal{N}_{i}$   is the set of indices of nodes neighbour- ing the  $i$  th node,  $\\mathcal{R}_{i j}$   is the set of edge annotations between  $i$   and    $j$  , and    $f_{r}$   is a parametrized func- tion speciﬁc to an edge type    $r\\,\\in\\,\\mathcal{R}$  . Recall the available relations from Section  2.2 , namely,  $\\mathcal{R}=$  { DOC-BASED ,  MATCH ,  COREF ,  $\\mathsf{C O M P L E M E N T}\\big\\}$  . \nA gating mechanism regulates how much of the update message propagates to the next step. This provides the model a way to prevent completely overwriting past information. Indeed, if all neces- sary information to answer a question is present at a layer which is not the last, then the model should learn to stop using neighbouring information for the next steps. Gate levels are computed as \n\n$$\n\\begin{array}{r}{\\mathbf{a}_{i}^{(\\ell)}=\\sigma\\left(f_{a}\\left([\\mathbf{u}_{i}^{(\\ell)},\\mathbf{h}_{i}^{(\\ell)}]\\right)\\right)\\ ,}\\end{array}\n$$\n \nwhere    $\\sigma(\\cdot)$   is the sigmoid function and    $f_{a}$   a parametrized transformation. Ultimately, the up- dated representation is a gated combination of the previous representation and a non-linear transfor- mation of the update message: \n\n$$\n\\mathbf{h}_{i}^{(\\ell+1)}=\\phi(\\mathbf{u}_{i}^{(\\ell)})\\odot\\mathbf{a}_{i}^{(\\ell)}+\\mathbf{h}_{i}^{(\\ell)}\\odot(1-\\mathbf{a}_{i}^{(\\ell)})\\ ,\n$$\n \nwhere    $\\phi(\\cdot)$   is any nonlinear function (we used tanh ) and    $\\odot$  stands for element-wise multiplica- tion. All transformations  $f_{*}$  are afﬁne and they are not layer-dependent (since we would like to use as few parameters as possible to decrease model complexity promoting efﬁciency and scalability). \n3 Experiments \nIn this section, we compare our method against re- cent work as well as preforming an ablation study using the W IKI H OP  dataset ( Welbl et al. ,  2018 ). See Appendix  A  in the supplementary material for a description of the hyper-parameters of our model and training details. \nW IKI H OP We use W IKI H OP  for training, val- idation/development and test. The test set is not publicly available and therefore we measure per- formance on the validation set in almost all ex- periments. W IKI H OP  has   $43{,}738/\\ 5{,}129/\\ 2{,}451$  query-documents samples in the training, valida- tion and test sets respectively for a total of 51,318 samples. Authors constructed the dataset as de- scribed in Section  2.1  selecting samples with a graph traversal up to a maximum chain length of 3 documents (see Table  1  for additional dataset statistics). W IKI H OP  comes in two versions, a \nThis table provides statistical data across three different categories: the number of candidates, the number of documents, and the number of tokens per document. For each category, the table displays the minimum value (Min), maximum value (Max), average value (Avg.), and the median value.\n\n- For \"# candidates\": \n  - Min: 2 \n  - Max: 79 \n  - Avg.: 19.8 \n  - Median: 14 \n\n- For \"# documents\": \n  - Min: 3 \n  - Max: 63 \n  - Avg.: 13.7 \n  - Median: 11 \n\n- For \"# tokens/doc.\": \n  - Min: 4 \n  - Max: 2,046 \n  - Avg.: 100.4 \n  - Median: 91\nstandard (unmasked) one and a masked one. The masked version was created by the authors to test whether methods are able to learn lexical abstrac- tion. In this version, all candidates and all men- tions of them in the support documents are re- placed by random but consistent placeholder to- kens. Thus, in the masked version, mentions are always referred to via unambiguous surface forms. We do not use coreference systems in the masked version as they rely crucially on lexical realization of mentions and cannot operate on masked tokens. \n3.1 Comparison \nIn this experiment, we compare our Enitity- GCN against recent prior work on the same task. We present test and development re- sults (when present) for both versions of the dataset in Table  2 . From  Welbl et al.  ( 2018 ), we list an oracle based on human performance as well as two standard reading comprehension models, namely BiDAF ( Seo et al. ,  2016 ) and FastQA ( Weissenborn et al. ,  2017 ). We also com- pare against Coref-GRU ( Dhingra et al. ,  2018 ), MHPGM ( Bauer et al. ,  2018 ), and Weaver ( Rai- son et al. ,  2018 ). Additionally, we include results of MHQA-GRN ( Song et al. ,  2018 ), from a recent arXiv preprint describing concurrent work. They jointly train graph neural networks and recurrent encoders. We report single runs of our two best single models and an ensemble one on the un- masked test set (recall that the test set is not pub- licly available and the task organizers only report unmasked results) as well as both versions of the validation set. \nEntity-GCN (best single model without coref- erence edges) outperforms all previous work by over   $2\\%$   points. We additionally re-ran BiDAF baseline to compare training time: when using a single Titan X GPU, BiDAF and Entity-GCN pro- cess 12.5 and 57.8 document sets per second, re- spectively. Note that  Welbl et al.  ( 2018 ) had to use BiDAF with very small state dimensional i ties "}
{"page": 5, "image_path": "doc_images/N19-1240_5.jpg", "ocr_text": "Unmasked Masked\nModel Test Dev | Test Dev\nHuman (Welbl et al., 2018) TAA - - -\nFastQA (Welbl et al., 2018) 25.7 - |358 —-\nBiDAF (Welbl et al., 2018) 42.9 - |545  -\nCoref-GRU (Dhingra et al., 2018) 59.3, 56.0 - -\nMHPGM (Bauer et al., 2018) - 58.2 - -\nWeaver / Jenga (Raison et al., 2018) 65.3 64.1 - -\nMHQA-GRN (Song et al., 2018) 65.4 62.8 - -\nEntity-GCN without coreference (single model) | 67.6 64.8 | -— 70.5\nEntity-GCN with coreference (single model) 66.4 65.3) — -\nEntity-GCN* (ensemble 5 models) 71.2 68.5 -— 71.6\n\nTable 2: Accuracy of different models on WIKIHOP closed test set and public validation set. Our Entity-GCN\noutperforms recent prior work without learning any language model to process the input but relying on a pre-\ntrained one (ELMo - without fine-tunning it) and applying R-GCN to reason among entities in the text. * with\ncoreference for unmasked dataset and without coreference for the masked one.\n\n(20), and smaller batch size due to the scalabil-\nity issues (both memory and computation costs).\nWe compare applying the same reductions.* Even-\ntually, we also report an ensemble of 5 indepen-\ndently trained models. All models are trained on\nthe same dataset splits with different weight ini-\ntializations. The ensemble prediction is obtained\n\n5\nas arg max |] P;(c|q, Cg, Sq) from each model.\n© isl\n\n3.2. Ablation Study\n\nTo help determine the sources of improvements,\nwe perform an ablation study using the publicly\navailable validation set (see Table 3). We per-\nform two groups of ablation, one on the embed-\nding layer, to study the effect of ELMo, and one\non the edges, to study how different relations af-\nfect the overall model performance.\n\nEmbedding ablation We argue that ELMo is\ncrucial, since we do not rely on any other context\nencoder. However, it is interesting to explore how\nour R-GCN performs without it. Therefore, in this\nexperiment, we replace the deep contextualized\nembeddings of both the query and the nodes with\nGloVe (Pennington et al., 2014) vectors (insensi-\ntive to context). Since we do not have any compo-\nnent in our model that processes the documents,\nwe expect a drop in performance. In other words,\nin this ablation our model tries to answer questions\n\nBesides, we could not run any other method we com-\n\npare with combined with ELMo without reducing the dimen-\nsionality further or having to implement a distributed version.\n\nwithout reading the context at all. For example, in\nFigure 1, our model would be aware that ““Stock-\nholm” and “Sweden” appear in the same document\nbut any context words, including the ones encod-\ning relations (e.g., “is the capital of”’) will be hid-\nden. Besides, in the masked case all mentions be-\ncome ‘unknown’ tokens with GloVe and therefore\nthe predictions are equivalent to a random guess.\nOnce the strong pre-trained encoder is out of the\nway, we also ablate the use of our R-GCN com-\nponent, thus completely depriving the model from\ninductive biases that aim at multi-hop reasoning.\n\nThe first important observation is that replacing\nELMo by GloVe (GloVe with R-GCN in Table 3)\nstill yields a competitive system that ranks far\nabove baselines from (Welbl et al., 2018) and even\nabove the Coref-GRU of Dhingra et al. (2018), in\nterms of accuracy on (unmasked) validation set.\nThe second important observation is that if we\nthen remove R-GCN (GloVe w/o R-GCN in Ta-\nble 3), we lose 8.0 points. That is, the R-GCN\ncomponent pushes the model to perform above\nCoref-GRU still without accessing context, but\nrather by updating mention representations based\non their relation to other ones. These results high-\nlight the impact of our R-GCN component.\n\nGraph edges ablation In this experiment we in-\nvestigate the effect of the different relations avail-\nable in the entity graph and processed by the R-\nGCN module. We start off by testing our stronger\nencoder (i.e., ELMo) in absence of edges connect-\ning mentions in the supporting documents (i.e., us-\n\n2311\n", "vlm_text": "The table presents a comparison of different models' performances in terms of test and development (Dev) accuracy scores across two tasks: Unmasked and Masked. The table includes the following models and their results:\n\n1. **Models Referenced from previous works (2018):**\n   - Human (Welbl et al., 2018)\n     - Unmasked Test: 74.1\n     - Other columns (-): No data provided\n   - FastQA (Welbl et al., 2018)\n     - Unmasked Test: 25.7\n     - Masked Test: 35.8\n   - BiDAF (Welbl et al., 2018)\n     - Unmasked Test: 42.9\n     - Masked Test: 54.5\n   - Coref-GRU (Dhingra et al., 2018)\n     - Unmasked Test: 59.3\n     - Unmasked Dev: 56.0\n   - MHPGM (Bauer et al., 2018)\n     - Unmasked Dev: 58.2\n     - Other columns (-): No data provided\n   - Weaver / Jenga (Raison et al., 2018)\n     - Unmasked Test: 65.3\n     - Unmasked Dev: 64.1\n   - MHQA-GRN (Song et al., 2018)\n     - Unmasked Test: 65.4\n     - Unmasked Dev: 62.8\n\n2. **Entity-GCN Models:**\n   - Entity-GCN without coreference (single model)\n     - Unmasked Test: 67.6\n     - Unmasked Dev: 64.8\n     - Masked Dev: 70.5\n   - Entity-GCN with coreference (single model)\n     - Unmasked Test: 66.4\n     - Unmasked Dev: 65.3\n   - Entity-GCN* (ensemble 5 models)\n     - Unmasked Test: 71.2\n     - Unmasked Dev: 68.5\n     - Masked Dev: 71.6\n\nThe table indicates the superior performance of the \"Entity-GCN\" models, particularly when using an ensemble model (Entity-GCN*), which achieves the highest scores on the Unmasked Test and Dev, and Masked Dev sets.\n(20), and smaller batch size due to the scalabil- ity issues (both memory and computation costs). We compare applying the same reductions.   Even- tually, we also report an ensemble of 5 indepen- dently trained models. All models are trained on the same dataset splits with different weight ini- tializations. The ensemble prediction is obtained as    $\\arg\\operatorname*{max}_{c}\\prod_{i=1}^{5}P_{i}(c|q,C_{q},\\bar{S_{q}})$   from each model. \n3.2 Ablation Study \nTo help determine the sources of improvements, we perform an ablation study using the publicly available validation set (see Table  3 ). We per- form two groups of ablation, one on the embed- ding layer, to study the effect of ELMo, and one on the edges, to study how different relations af- fect the overall model performance. \nEmbedding ablation We argue that ELMo is crucial, since we do not rely on any other context encoder. However, it is interesting to explore how our R-GCN performs without it. Therefore, in this experiment, we replace the deep contextualized embeddings of both the query and the nodes with GloVe ( Pennington et al. ,  2014 ) vectors (insensi- tive to context). Since we do not have any compo- nent in our model that processes the documents, we expect a drop in performance. In other words, in this ablation our model tries to answer questions without reading the context at all . For example, in Figure  1 , our model would be aware that “Stock- holm” and “Sweden” appear in the same document but any context words, including the ones encod- ing relations (e.g., “is the capital of”) will be hid- den. Besides, in the masked case all mentions be- come ‘unknown’ tokens with GloVe and therefore the predictions are equivalent to a random guess. Once the strong pre-trained encoder is out of the way, we also ablate the use of our R-GCN com- ponent, thus completely depriving the model from inductive biases that aim at multi-hop reasoning. \n\nThe ﬁrst important observation is that replacing ELMo by GloVe (GloVe with R-GCN in Table  3 ) still yields a competitive system that ranks far above baselines from ( Welbl et al. ,  2018 ) and even above the Coref-GRU of  Dhingra et al.  ( 2018 ), in terms of accuracy on (unmasked) validation set. The second important observation is that if we then remove R-GCN (GloVe w/o R-GCN in Ta- ble  3 ), we lose 8.0 points. That is, the R-GCN component pushes the model to perform above Coref-GRU still without accessing context, but rather by updating mention representations based on their relation to other ones. These results high- light the impact of our R-GCN component. \nGraph edges ablation In this experiment we in- vestigate the effect of the different relations avail- able in the entity graph and processed by the R- GCN module. We start off by testing our stronger encoder (i.e., ELMo) in absence of edges connect- ing mentions in the supporting documents (i.e., us- "}
{"page": 6, "image_path": "doc_images/N19-1240_6.jpg", "ocr_text": "Model unmasked masked\nfull (ensemble) 68.5 71.6\nfull (single) 65.1401 70.4 +012\nGloVe with R-GCN 59.2 11.1\nGloVe w/o R-GCN 51.2 11.6\nNo R-GCN 62.4 63.2\nNo relation types 62.7 63.9\nNo DOC-BASED 62.9 65.8\nNo MATCH 64.3 67.4\nNo COREF 64.8 -\nNo COMPLEMENT 64.1 70.3\nInduced edges 61.5 56.4\n\nTable 3: Ablation study on WIKIHOP validation set.\nThe full model is our Entity-GCN with all of its com-\nponents and other rows indicate models trained without\na component of interest. We also report baselines using\nGloVe instead of ELMo with and without R-GCN. For\nthe full model we report mean +1 std over 5 runs.\n\ning only self-loops —- No R-GCN in Table 3). The\nresults suggest that WIKIPHOP genuinely requires\nmultihop inference, as our best model is 6.1% and\n8.4% more accurate than this local model, in un-\nmasked and masked settings, respectively.* How-\never, it also shows that ELMo representations cap-\nture predictive context features, without being ex-\nplicitly trained for the task. It confirms that our\ngoal of getting away with training expensive doc-\nument encoders is a realistic one.\n\nWe then inspect our model’s effectiveness in\nmaking use of the structure encoded in the graph.\nWe start naively by fully-connecting all nodes\nwithin and across documents without distinguish-\ning edges by type (No relation types in Table 3).\nWe observe only marginal improvements with re-\nspect to ELMo alone (No R-GCN in Table 3) in\nboth the unmasked and masked setting suggest-\ning that a GCN operating over a naive entity graph\nwould not add much to this task and a more infor-\nmative graph construction and/or a more sophisti-\ncated parameterization is indeed needed.\n\nNext, we ablate each type of relations inde-\npendently, that is, we either remove connections\nof mentions that co-occur in the same docu-\nment (DOC-BASED), connections between men-\ntions matching exactly (MATCH), or edges pre-\ndicted by the coreference system (COREF). The\n\n‘Recall that all models in the ensemble use the same lo-\ncal representations, ELMo.\n\nfirst thing to note is that the model makes better\nuse of DOC-BASED connections than MATCH or\nCOREF connections. This is mostly because i) the\nmajority of the connections are indeed between\nmentions in the same document, and ii) withou\nconnecting mentions within the same documen\nwe remove important information since the model\nis unaware they appear closely in the document.\nSecondly, we notice that coreference links and\ncomplement edges seem to play a more marginal\nrole. Though it may be surprising for coreference\nedges, recall that the MATCH heuristic already cap-\ntures the easiest coreference cases, and for the res\nthe out-of-domain coreference system may not be\nreliable. Still, modelling all these different rela-\ntions together gives our Entity-GCN a clear advan-\ntage. This is our best system evaluating on the de-\nvelopment. Since Entity-GCN seems to gain little\nadvantage using the coreference system, we report\ntest results both with and without using it. Surpris-\ningly, with coreference, we observe performance\ndegradation on the test set. It is likely that the test\ndocuments are harder for the coreference system.\n\nWe do perform one last ablation, namely, we re-\nplace our heuristic for assigning edges and their\nlabels by a model component that predicts them.\nThe last row of Table 3 (Induced edges) shows\nmodel performance when edges are not predeter-\nmined but predicted. For this experiment, we use a\nbilinear function f.(X;,%j) = o (& We; that\npredicts the importance of a single edge connect-\ning two nodes i, 7 using the query-dependent rep-\nresentation of mentions (see Section 2.3). The\nperformance drops below ‘No R-GCN’ suggesting\nhat it cannot learn these dependencies on its own.\n\nMost results are stronger for the masked set-\nings even though we do not apply the coreference\nresolution system in this setting due to masking.\nIt is not surprising as coreferred mentions are la-\nbeled with the same identifier in the masked ver-\nsion, even if their original surface forms did not\nmatch (Welbl et al. (2018) used WIKIPEDIA links\nor masking). Indeed, in the masked version, an\nentity is always referred to via the same unique\nsurface form (e.g., MASK1) within and across doc-\numents. In the unmasked setting, on the other\nhand, mentions to an entity may differ (e.g., “US”\nvs “United States”) and they might not be retrieved\nby the coreference system we are employing, mak-\n\n5Since the test set is hidden from us, we cannot analyze\nthis difference further.\n\n2312\n", "vlm_text": "The table appears to show the performance of different models or configurations on some task, possibly related to natural language processing or machine learning, given the mention of GloVe, R-GCN, and different components like MATCH, COREF, and COMPLEMENT. The table is divided into columns labeled \"unmasked\" and \"masked,\" suggesting these are two different experimental conditions or evaluation settings. Each row represents a different model or model configuration, and the corresponding values in the \"unmasked\" and \"masked\" columns reflect the performance metrics, possibly accuracy or F1 score.\n\nHere's a breakdown of the rows:\n\n1. **`full (ensemble)`**: This configuration has the highest performance with values of 68.5 (unmasked) and 71.6 (masked).\n   \n2. **`full (single)`**: The single model configuration yields lower performance than the ensemble, with 65.1 ± 0.11 for unmasked and 70.4 ± 0.12 for masked.\n\n3. **`GloVe with R-GCN`**: Shows performance of 59.2 (unmasked) and 11.1 (masked), indicating potentially poor performance in the masked condition.\n\n4. **`GloVe w/o R-GCN`**: Offers 51.2 (unmasked) and 11.6 (masked), suggesting performance drops without R-GCN.\n\n5. **`No R-GCN`**: Performance of 62.4 (unmasked) and 63.2 (masked), showing the effect of removing R-GCN.\n\n6. **`No relation types`**: Scores of 62.7 (unmasked) and 63.9 (masked), indicating results without relation types.\n\n7. **`No DOC–BASED`**: Results of 62.9 (unmasked) and 65.8 (masked), evaluating performance without document-based feature/approach.\n\n8. **`No MATCH`**: Achieves 64.3 (unmasked) and 67.4 (masked), assessing the impact of removing match features.\n\n9. **`No COREF`**: Obtains 64.8 (unmasked) with no corresponding value for masked, testing without coreference features.\n\n10. **`No COMPLEMENT`**: Yields 64.1 (unmasked) and 70.3 (masked), evaluating without complementary features.\n\n11. **`Induced edges`**: Has performance of 61.5 (unmasked) and 56.4 (masked), reflecting results with induced edges.\n\nThe table provides comparative insights into how different features or configurations affect performance in two distinct settings, \"unmasked\" and \"masked.\"\ning only self-loops – No R-GCN in Table  3 ). The results suggest that W IKIP H OP  genuinely requires multihop inference, as our best model is   $6.1\\%$   and  $8.4\\%$   more accurate than this local model, in un- masked and masked settings, respectively.   How- ever, it also shows that ELMo representations cap- ture predictive context features, without being ex- plicitly trained for the task. It conﬁrms that our goal of getting away with training expensive doc- ument encoders is a realistic one. \nWe then inspect our model’s effectiveness in making use of the structure encoded in the graph. We start naively by fully-connecting all nodes within and across documents without distinguish- ing edges by type (No relation types in Table  3 ). We observe only marginal improvements with re- spect to ELMo alone (No R-GCN in Table  3 ) in both the unmasked and masked setting suggest- ing that a GCN operating over a naive entity graph would not add much to this task and a more infor- mative graph construction and/or a more sophisti- cated parameter iz ation is indeed needed. \nNext, we ablate each type of relations inde- pendently, that is, we either remove connections of mentions that co-occur in the same docu- ment ( DOC-BASED ), connections between men- tions matching exactly ( MATCH ), or edges pre- dicted by the coreference system ( COREF ). The ﬁrst thing to note is that the model makes better use of  DOC-BASED  connections than  MATCH  or COREF  connections. This is mostly because i) the majority of the connections are indeed between mentions in the same document, and ii) without connecting mentions within the same document we remove important information since the model is unaware they appear closely in the document. Secondly, we notice that coreference links and complement edges seem to play a more marginal role. Though it may be surprising for coreference edges, recall that the  MATCH  heuristic already cap- tures the easiest coreference cases, and for the rest the out-of-domain coreference system may not be reliable. Still, modelling all these different rela- tions together gives our Entity-GCN a clear advan- tage. This is our best system evaluating on the de- velopment. Since Entity-GCN seems to gain little advantage using the coreference system, we report test results both with and without using it. Surpris- ingly, with coreference, we observe performance degradation on the test set. It is likely that the test documents are harder for the coreference system. \n\nWe do perform one last ablation, namely, we re- place our heuristic for assigning edges and their labels by a model component that predicts them. The last row of Table  3  (Induced edges) shows model performance when edges are not predeter- mined but predicted. For this experiment, we use a bilinear function    $f_{e}\\big(\\hat{\\mathbf{x}}_{i},\\hat{\\mathbf{x}}_{j}\\big)\\,=\\,\\sigma\\left(\\hat{\\mathbf{x}}_{i}^{\\top}\\mathbf{W}_{e}\\hat{\\mathbf{x}}_{j}\\right)$  \u0000 \u0001 that predicts the importance of a single edge connect- ing two nodes    $i,j$   using the query-dependent rep- resentation of mentions (see Section  2.3 ). The performance drops below ‘No R-GCN’ suggesting that it cannot learn these dependencies on its own. \nMost results are stronger for the masked set- tings even though we do not apply the coreference resolution system in this setting due to masking. It is not surprising as coreferred mentions are la- beled with the same identiﬁer in the masked ver- sion, even if their original surface forms did not match ( Welbl et al.  ( 2018 ) used W IKIPEDIA  links for masking). Indeed, in the masked version, an entity is always referred to via the same unique surface form (e.g.,  MASK1 ) within and across doc- uments. In the unmasked setting, on the other hand, mentions to an entity may differ (e.g., “US” vs “United States”) and they might not be retrieved by the coreference system we are employing, mak- "}
{"page": 7, "image_path": "doc_images/N19-1240_7.jpg", "ocr_text": "Relation Accuracy P@2 P@5 Avg. |C,| Supports\n\noverall (ensemble) 68.5 81.0 94.1 20.4 +166 5129\n\noverall (single model) 65.3 79.7 92.9 20.4 +166 5129\nmember-of_political_party 85.5 95.7 98.6 5.4424 70\n\n3 best record_label 83.0 93.6 99.3 124461 283\npublisher 81.5 96.3 100.0 9.6451 54\nplace_of_birth 51.0 67.2 86.8 27.2 +145 309\n\n3 worst place_of_death 50.0 67.3 89.1 25.1 +143 159\ninception 29.9 53.2 83.1 21.9410 77\n\nTable 4: Accuracy and precision at K (P@K in the table) analysis overall and per query type. Avg. |C,| indicates\n\nthe average number of candidates with one standard deviation.\n\ning the task harder for all models. Therefore, as we\nrely mostly on exact matching when constructing\nour graph for the masked case, we are more effec-\ntive in recovering coreference links on the masked\nrather than unmasked version.°\n\n4 Error Analysis\n\nIn this section we provide an error analysis for\nour best single model predictions. First of all, we\nlook at which type of questions our model per-\nforms well or poorly. There are more than 150\nquery types in the validation set but we filtered\nthe three with the best and with the worst accu-\nracy that have at least 50 supporting documents\nand at least 5 candidates. We show results in Ta-\nble 4. We observe that questions regarding places\n(birth and death) are considered harder for Entity-\nGCN. We then inspect samples where our model\nfails while assigning highest likelihood and no-\nticed two principal sources of failure i) a mismatch\nbetween what is written in WIKIPEDIA and what is\nannotated in WIKIDATA, and ii) a different degree\nof granularity (e.g., born in “London” vs “UK”\ncould be considered both correct by a human but\nnot when measuring accuracy). See Table 6 in the\nsupplement material for some reported samples.\nSecondly, we study how the model performance\ndegrades when the input graph is large. In particu-\nlar, we observe a negative Pearson’s correlation (-\n0.687) between accuracy and the number of candi-\ndate answers. However, the performance does not\ndecrease steeply. The distribution of the number of\ncandidates in the dataset peaks at 5 and has an av-\nerage of approximately 20. Therefore, the model\n\n°Though other systems do not explicitly link matching\nmentions, they similarly benefit from masking (e.g., masks\nessentially single out spans that contain candidate answers).\n\ndoes not see many samples where there are a large\nnumber of candidate entities during training. Dif-\nferently, we notice that as the number of nodes in\nthe graph increases, the model performance drops\nbut more gently (negative but closer to zero Pear-\nson’s correlation). This is important as document\nsets can be large in practical applications. See Fig-\nure 3 in the supplemental material for plots.\n\n5 Related Work\n\nIn previous work, BiDAF (Seo et al., 2016),\nFastQA (Weissenborn et al., 2017), Coref-\nGRU (Dhingra et al., 2018), MHPGM (Bauer\net al., 2018), and Weaver / Jenga (Raison et al.,\n2018) have been applied to multi-document ques-\ntion answering. The first two mainly focus on sin-\ngle document QA and Welbl et al. (2018) adapted\nboth of them to work with WIKIHOP. They pro-\ncess each instance of the dataset by concatenat-\ning all d € Sq in a random order adding doc-\nument separator tokens. They trained using the\nfirst answer mention in the concatenated document\nand evaluating exact match at test time. Coref-\nGRU, similarly to us, encodes relations between\nentity mentions in the document. Instead of us-\ning graph neural network layers, as we do, they\naugment RNNs with jump links corresponding to\npairs of corefereed mentions. MHPGM uses a\nmulti-attention mechanism in combination with\nexternal commonsense relations to perform mul-\ntiple hops of reasoning. Weaver is a deep co-\nencoding model that uses several alternating bi-\nLSTMs to process the concatenated documents\nand the query.\n\nGraph neural networks have been shown suc-\ncessful on a number of NLP tasks (Marcheggiani\nand Titov, 2017; Bastings et al., 2017; Zhang et al.,\n\n2313\n", "vlm_text": "The table displays a comparison of model performance metrics for different relations. It includes measurements of accuracy and precision at 2 and 5 (P@2, P@5), the average size of some quantity (\\(|C_q|\\)), and the number of supports or instances.\n\n### Sections:\n\n1. **Overall Performance:**\n   - **Ensemble:**\n     - **Accuracy:** 68.5\n     - **P@2:** 81.0\n     - **P@5:** 94.1\n     - **Avg. \\(|C_q|\\):** 20.4 ± 16.6\n     - **Supports:** 5129\n   - **Single Model:**\n     - **Accuracy:** 65.3\n     - **P@2:** 79.7\n     - **P@5:** 92.9\n     - **Avg. \\(|C_q|\\):** 20.4 ± 16.6\n     - **Supports:** 5129\n\n2. **Top 3 Best Performing Relations:**\n   - **member_of_political_party:**\n     - **Accuracy:** 85.5\n     - **P@2:** 95.7\n     - **P@5:** 98.6\n     - **Avg. \\(|C_q|\\):** 5.4 ± 2.4\n     - **Supports:** 70\n   - **record_label:**\n     - **Accuracy:** 83.0\n     - **P@2:** 93.6\n     - **P@5:** 99.3\n     - **Avg. \\(|C_q|\\):** 12.4 ± 6.1\n     - **Supports:** 283\n   - **publisher:**\n     - **Accuracy:** 81.5\n     - **P@2:** 96.3\n     - **P@5:** 100.0\n     - **Avg. \\(|C_q|\\):** 9.6 ± 5.1\n     - **Supports:** 54\n\n3. **Top 3 Worst Performing Relations:**\n   - **place_of_birth:**\n     - **Accuracy:** 51.0\n     - **P@2:** 67.2\n     - **P@5:** 86.8\n     - **Avg. \\(|C_q|\\):** 27.2 ± 14.5\n     - **Supports:** 309\n   - **place_of_death:**\n     - **Accuracy:** 50.0\n     - **P@2:** 67.3\n     - **P@5:** 89.1\n     - **Avg. \\(|C_q|\\):** 25.1 ± 14.3\n     - **Supports:** 159\n   - **inception:**\n     -\ning the task harder for all models. Therefore, as we rely mostly on exact matching when constructing our graph for the masked case, we are more effec- tive in recovering coreference links on the masked rather than unmasked version. \n4 Error Analysis \nIn this section we provide an error analysis for our best single model predictions. First of all, we look at which type of questions our model per- forms well or poorly. There are more than 150 query types in the validation set but we ﬁltered the three with the best and with the worst accu- racy that have at least 50 supporting documents and at least 5 candidates. We show results in Ta- ble  4 . We observe that questions regarding places (birth and death) are considered harder for Entity- GCN. We then inspect samples where our model fails while assigning highest likelihood and no- ticed two principal sources of failure i) a mismatch between what is written in W IKIPEDIA  and what is annotated in W IKIDATA , and ii) a different degree of granularity (e.g., born in “London” vs “UK” could be considered both correct by a human but not when measuring accuracy). See Table  6  in the supplement material for some reported samples. \nSecondly, we study how the model performance degrades when the input graph is large. In particu- lar, we observe a negative Pearson’s correlation (- 0.687) between accuracy and the number of candi- date answers. However, the performance does not decrease steeply. The distribution of the number of candidates in the dataset peaks at 5 and has an av- erage of approximately 20. Therefore, the model does not see many samples where there are a large number of candidate entities during training. Dif- ferently, we notice that as the number of nodes in the graph increases, the model performance drops but more gently (negative but closer to zero Pear- son’s correlation). This is important as document sets can be large in practical applications. See Fig- ure  3  in the supplemental material for plots. \n\n5 Related Work \nIn previous work, BiDAF ( Seo et al. ,  2016 ), FastQA ( Weissenborn et al. , 2017 ), Coref- GRU ( Dhingra et al. ,  2018 ), MHPGM ( Bauer et al. ,  2018 ), and Weaver / Jenga ( Raison et al. , 2018 ) have been applied to multi-document ques- tion answering. The ﬁrst two mainly focus on sin- gle document QA and  Welbl et al.  ( 2018 ) adapted both of them to work with W IKI H OP . They pro- cess each instance of the dataset by concatenat- ing all    $d~\\in~S_{q}$   in a random order adding doc- ument separator tokens. They trained using the ﬁrst answer mention in the concatenated document and evaluating exact match at test time. Coref- GRU, similarly to us, encodes relations between entity mentions in the document. Instead of us- ing graph neural network layers, as we do, they augment RNNs with jump links corresponding to pairs of corefereed mentions. MHPGM uses a multi-attention mechanism in combination with external commonsense relations to perform mul- tiple hops of reasoning. Weaver is a deep co- encoding model that uses several alternating bi- LSTMs to process the concatenated documents and the query. \nGraph neural networks have been shown suc- cessful on a number of NLP tasks ( Marcheggiani and Titov ,  2017 ;  Bastings et al. ,  2017 ;  Zhang et al. , 2018a ), including those involving document level modeling ( Peng et al. ,  2017 ). They have also been applied in the context of asking questions about knowledge contained in a knowledge base ( Zhang et al. ,  2018b ). In  Schlichtkrull et al.  ( 2018 ), GCNs are used to capture reasoning chains in a knowl- edge base. Our work and unpublished concurrent work by  Song et al.  ( 2018 ) are the ﬁrst to study graph neural networks in the context of multi- document QA. Besides differences in the architec- ture,  Song et al.  ( 2018 ) propose to train a combi- nation of a graph recurrent network and an RNN encoder. We do not train any RNN document en- coders in this work. "}
{"page": 8, "image_path": "doc_images/N19-1240_8.jpg", "ocr_text": "2018a), including those involving document level\nmodeling (Peng et al., 2017). They have also been\napplied in the context of asking questions about\nknowledge contained in a knowledge base (Zhang\net al., 2018b). In Schlichtkrull et al. (2018), GCNs\nare used to capture reasoning chains in a knowl-\nedge base. Our work and unpublished concurrent\nwork by Song et al. (2018) are the first to study\ngraph neural networks in the context of multi-\ndocument QA. Besides differences in the architec-\nture, Song et al. (2018) propose to train a combi-\nnation of a graph recurrent network and an RNN\nencoder. We do not train any RNN document en-\ncoders in this work.\n\n6 Conclusion\n\nWe designed a graph neural network that oper-\nates over a compact graph representation of a set\nof documents where nodes are mentions to en-\ntities and edges signal relations such as within\nand cross-document coreference. The model\nlearns to answer questions by gathering evidence\nfrom different documents via a differentiable mes-\nsage passing algorithm that updates node repre-\nsentations based on their neighbourhood. Our\nmodel outperforms published results where abla-\ntions show substantial evidence in favour of multi-\nstep reasoning. Moreover, we make the model fast\nby using pre-trained (contextual) embeddings.\n\nAcknowledgments\n\nWe would like to thank Johannes Welb! for help-\ning to test our system on WIKIHoP. This\nproject is supported by SAP Innovation Center\nNetwork, ERC Starting Grant BroadSem (678254)\nand the Dutch Organization for Scientific Re-\nsearch (NWO) VIDI 639.022.518. Wilker Aziz is\nsupported by the Dutch Organisation for Scientific\nResearch (NWO) VICI Grant nr. 277-89-002.\n\nReferences\nJoost Bastings, Ivan Titov, Wilker Aziz, Diego\nMarcheggiani, and Khalil Simaan. 2017. Graph\n\nconvolutional encoders for syntax-aware neural ma-\nchine translation. In Proceedings of the 2017 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 1957-1967. Association for Com-\nputational Linguistics.\n\nLisa Bauer, Yicheng Wang, and Mohit Bansal. 2018.\nCommonsense for generative multi-hop question an-\nswering tasks. In Proceedings of the 2018 Con-\nference on Empirical Methods in Natural Language\n\nProcessing, pages 4220-4230. Association for Com-\nputational Linguistics.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. BERT: Pre-training of\nDeep Bidirectional Transformers for Language Un-\nderstanding. arXiv preprint arXiv:1810.04805.\n\nBhuwan Dhingra, Qiao Jin, Zhilin Yang, William Co-\nhen, and Ruslan Salakhutdinov. 2018. Neural mod-\nels for reasoning over multiple mentions using coref-\nerence. In Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 2 (Short Papers), pages 42-48,\nNew Orleans, Louisiana. Association for Computa-\ntional Linguistics.\n\nKarl Moritz Hermann, Tomas Kocisky, Edward\nGrefenstette, Lasse Espeholt, Will Kay, Mustafa Su-\nleyman, and Phil Blunsom. 2015. Teaching ma-\nchines to read and comprehend. In Advances in Neu-\nral Information Processing Systems, pages 1693-\n1701.\n\nMandar Joshi, Eunsol Choi, Daniel Weld, and Luke\nZettlemoyer. 2017. TriviaQA: A large scale dis-\ntantly supervised challenge dataset for reading com-\nprehension. In Proceedings of the 55th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), volume 1, pages\n1601-1611.\n\nDiederik P Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. Kinga, D., and\nJ. Ba Adam. ”A method for stochastic optimization.”\nInternational Conference on Learning Representa-\ntions (ICLR)., 5.\n\nThomas N Kipf and Max Welling. 2017. Semi-\nsupervised classification with graph convolutional\nnetworks. International Conference on Learning\nRepresentations (ICLR).\n\nTomas Kocisky, Jonathan Schwarz, Phil Blunsom,\nChris Dyer, Karl Moritz Hermann, Gabor Melis, and\nEdward Grefenstette. 2018. The NarrativeQA read-\ning comprehension challenge. Transactions of the\nAssociation for Computational Linguistics, 6:317-\n328.\n\nGuokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang,\nand Eduard Hovy. 2017. RACE: Large-scale read-\ning comprehension dataset from examinations. In\nProceedings of the 2017 Conference on Empirical\nMethods in Natural Language Processing, pages\n785-794, Copenhagen, Denmark. Association for\nComputational Linguistics.\n\nKenton Lee, Luheng He, Mike Lewis, and Luke Zettle-\nmoyer. 2017. End-to-end neural coreference reso-\nlution. In Proceedings of the 2017 Conference on\nEmpirical Methods in Natural Language Process-\ning, pages 188-197. Association for Computational\nLinguistics.\n\n2314\n", "vlm_text": "\n6 Conclusion \nWe designed a graph neural network that oper- ates over a compact graph representation of a set of documents where nodes are mentions to en- tities and edges signal relations such as within and cross-document coreference. The model learns to answer questions by gathering evidence from different documents via a differentiable mes- sage passing algorithm that updates node repre- sentations based on their neighbourhood. Our model outperforms published results where abla- tions show substantial evidence in favour of multi- step reasoning. Moreover, we make the model fast by using pre-trained (contextual) embeddings. \nAcknowledgments \nWe would like to thank Johannes Welbl for help- ing to test our system on W IKI H OP . This project is supported by SAP Innovation Center Network, ERC Starting Grant BroadSem (678254) and the Dutch Organization for Scientiﬁc Re- search (NWO) VIDI 639.022.518. Wilker Aziz is supported by the Dutch Organisation for Scientiﬁc Research (NWO) VICI Grant nr. 277-89-002. \nReferences \nJoost Bastings, Ivan Titov, Wilker Aziz, Diego Marcheggiani, and Khalil Simaan. 2017. Graph convolutional encoders for syntax-aware neural ma- chine translation . In  Proceedings of the 2017 Con- ference on Empirical Methods in Natural Language Processing , pages 1957–1967. Association for Com- putational Linguistics. \nLisa Bauer, Yicheng Wang, and Mohit Bansal. 2018. Commonsense for generative multi-hop question an- swering tasks . In  Proceedings of the 2018 Con- ference on Empirical Methods in Natural Language \nProcessing , pages 4220–4230. Association for Com- putational Linguistics. \nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. BERT: Pre-training of Deep Bidirectional Transformers for Language Un- derstanding.  arXiv preprint arXiv:1810.04805 . \nBhuwan Dhingra, Qiao Jin, Zhilin Yang, William Co- hen, and Ruslan Salakhutdinov. 2018.  Neural mod- els for reasoning over multiple mentions using coref- erence . In  Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech- nologies, Volume 2 (Short Papers) , pages 42–48, New Orleans, Louisiana. Association for Computa- tional Linguistics. \nKarl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Su- leyman, and Phil Blunsom. 2015. Teaching ma- chines to read and comprehend. In  Advances in Neu- ral Information Processing Systems , pages 1693– 1701. \nMandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. 2017. TriviaQA: A large scale dis- tantly supervised challenge dataset for reading com- prehension. In  Proceedings of the 55th Annual Meeting of the Association for Computational Lin- guistics (Volume 1: Long Papers) , volume 1, pages 1601–1611. \nDiederik P Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization.  Kinga, D., and J. Ba Adam. ”A method for stochastic optimization.” International Conference on Learning Representa- tions (ICLR). , 5. \nThomas N Kipf and Max Welling. 2017. Semi- supervised classiﬁcation with graph convolutional networks. International Conference on Learning Representations (ICLR) . \nTomas Kocisky, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gabor Melis, and Edward Grefenstette. 2018.  The NarrativeQA read- ing comprehension challenge .  Transactions of the Association for Computational Linguistics , 6:317– 328. \nGuokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. 2017.  RACE: Large-scale read- ing comprehension dataset from examinations . In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing , pages 785–794, Copenhagen, Denmark. Association for Computational Linguistics. \nKenton Lee, Luheng He, Mike Lewis, and Luke Zettle- moyer. 2017.  End-to-end neural coreference reso- lution . In  Proceedings of the 2017 Conference on Empirical Methods in Natural Language Process- ing , pages 188–197. Association for Computational Linguistics. "}
{"page": 9, "image_path": "doc_images/N19-1240_9.jpg", "ocr_text": "Diego Marcheggiani and Ivan Titov. 2017. Encoding\nsentences with graph convolutional networks for se-\nmantic role labeling. In Proceedings of the 2017\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 1506-1515. Association\nfor Computational Linguistics.\n\nTomas Mikoloy, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013. Distributed representa-\ntions of words and phrases and their compositional-\nity. In Advances in neural information processing\nsystems, pages 3111-3119.\n\nNanyun Peng, Hoifung Poon, Chris Quirk, Kristina\nToutanova, and Wen-tau Yih. 2017. Cross-sentence\nn-ary relation extraction with graph lstms. Transac-\ntions of the Association for Computational Linguis-\ntics, 5:101-115.\n\nJeffrey Pennington, Richard Socher, and Christopher\nManning. 2014. GloVe: Global vectors for word\nrepresentation. In Proceedings of the 2014 confer-\nence on empirical methods in natural language pro-\ncessing (EMNLP), pages 1532-1543.\n\nMatthew Peters, Mark Neumann, Mohit lyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long Papers), pages\n2227-2237, New Orleans, Louisiana. Association\nfor Computational Linguistics.\n\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding with unsupervised learning. Technical re-\nport, OpenAl.\n\nMartin Raison, Pierre-Emmanuel Mazaré, Rajarshi\nDas, and Antoine Bordes. 2018. Weaver: Deep co-\nencoding of questions and documents for machine\nreading. In Proceedings of the International Con-\nference on Machine Learning (ICML).\n\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQUAD: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2383-2392, Austin,\nTexas. Association for Computational Linguistics.\n\nMichael Schlichtkrull, Thomas N. Kipf, Peter Bloem,\nRianne van den Berg, Ivan Titov, and Max Welling.\n2018. Modeling relational data with graph convolu-\ntional networks. In The Semantic Web, pages 593—\n607, Cham. Springer International Publishing.\n\nMinjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and\nHannaneh Hajishirzi. 2016. Bidirectional atten-\ntion flow for machine comprehension. International\nConference on Learning Representations (ICLR).\n\nYelong Shen, Po-Sen Huang, Jianfeng Gao, and\nWeizhu Chen. 2017. Reasonet: Learning to stop\nreading in machine comprehension. In Proceedings\nof the 23rd ACM SIGKDD International Conference\non Knowledge Discovery and Data Mining, pages\n1047-1055. ACM.\n\nLinfeng Song, Zhiguo Wang, Mo Yu, Yue Zhang,\nRadu Florian, and Daniel Gildea. 2018. Exploring\nGraph-structured Passage Representation for Multi-\nhop Reading Comprehension with Graph Neural\nNetworks. arXiv preprint arXiv: 1809.02040.\n\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,\nIlya Sutskever, and Ruslan Salakhutdinov. 2014.\nDropout: A simple way to prevent neural networks\nfrom overfitting. The Journal of Machine Learning\nResearch, 15(1):1929-1958.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, pages 5998-6008.\n\nDenny Vrande¢ié. 2012. Wikidata: A new platform for\ncollaborative data collection. In Proceedings of the\n21st International Conference on World Wide Web,\npages 1063-1064. ACM.\n\nDirk Weissenborn, Georg Wiese, and Laura Seiffe.\n2017. Making neural qa as simple as possible but\nnot simpler. In Proceedings of the 21st Confer-\nence on Computational Natural Language Learn-\ning (CoNLL 2017), pages 271-280. Association for\nComputational Linguistics.\n\nJohannes Welbl, Pontus Stenetorp, and Sebastian\nRiedel. 2018. Constructing datasets for multi-hop\nreading comprehension across documents. Transac-\ntions of the Association for Computational Linguis-\ntics, 6:287-302.\n\nCaiming Xiong, Victor Zhong, and Richard Socher.\n2016. Dynamic coattention networks for question\nanswering. arXiv preprint arXiv: 1611.01604.\n\nYuhao Zhang, Peng Qi, and Christopher D. Manning.\n2018a. Graph convolution over pruned dependency\ntrees improves relation extraction. In Proceedings of\nthe 2018 Conference on Empirical Methods in Nat-\nural Language Processing, pages 2205-2215. Asso-\nciation for Computational Linguistics.\n\nYuyu Zhang, Hanjun Dai, Zornitsa Kozareva, Alexan-\nder J Smola, and Le Song. 2018b. Variational\nreasoning for question answering with knowledge\ngraph. The Thirty-Second AAAI Conference on Ar-\ntificial Intelligence (AAAI-18).\n\n2315\n", "vlm_text": "Diego Marcheggiani and Ivan Titov. 2017.  Encoding sentences with graph convolutional networks for se- mantic role labeling . In  Proceedings of the 2017 Conference on Empirical Methods in Natural Lan- guage Processing , pages 1506–1515. Association for Computational Linguistics. \nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor- rado, and Jeff Dean. 2013. Distributed representa- tions of words and phrases and their compositional- ity. In  Advances in neural information processing systems , pages 3111–3119. \nNanyun Peng, Hoifung Poon, Chris Quirk, Kristina Toutanova, and Wen-tau Yih. 2017.  Cross-sentence n-ary relation extraction with graph lstms .  Transac- tions of the Association for Computational Linguis- tics , 5:101–115. \nJeffrey Pennington, Richard Socher, and Christopher Manning. 2014. GloVe: Global vectors for word representation. In  Proceedings of the 2014 confer- ence on empirical methods in natural language pro- cessing (EMNLP) , pages 1532–1543. \nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018.  Deep contextualized word rep- resentations . In  Proceedings of the 2018 Confer- ence of the North American Chapter of the Associ- ation for Computational Linguistics: Human Lan- guage Technologies, Volume 1 (Long Papers) , pages 2227–2237, New Orleans, Louisiana. Association for Computational Linguistics. \nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language under- standing with unsupervised learning.  Technical re- port, OpenAI . \nMartin Raison, Pierre-Emmanuel Mazar´ e, Rajarshi Das, and Antoine Bordes. 2018. Weaver: Deep co- encoding of questions and documents for machine reading.  In Proceedings of the International Con- ference on Machine Learning (ICML) . \nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016.  SQuAD:   $100{,}000{+}$   questions for machine comprehension of text . In  Proceedings of the 2016 Conference on Empirical Methods in Natu- ral Language Processing , pages 2383–2392, Austin, Texas. Association for Computational Linguistics. \nMichael Schlichtkrull, Thomas N. Kipf, Peter Bloem, Rianne van den Berg, Ivan Titov, and Max Welling. 2018. Modeling relational data with graph convolu- tional networks. In  The Semantic Web , pages 593– 607, Cham. Springer International Publishing. \nYelong Shen, Po-Sen Huang, Jianfeng Gao, and Weizhu Chen. 2017. Reasonet: Learning to stop reading in machine comprehension. In  Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , pages 1047–1055. ACM. Linfeng Song, Zhiguo Wang, Mo Yu, Yue Zhang, Radu Florian, and Daniel Gildea. 2018. Exploring Graph-structured Passage Representation for Multi- hop Reading Comprehension with Graph Neural Networks.  arXiv preprint arXiv:1809.02040 . Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: A simple way to prevent neural networks from overﬁtting.  The Journal of Machine Learning Research , 15(1):1929–1958. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In  Advances in Neural Information Pro- cessing Systems , pages 5998–6008. Denny Vrandeˇ ci´ c. 2012. Wikidata: A new platform for collaborative data collection. In  Proceedings of the 21st International Conference on World Wide Web , pages 1063–1064. ACM. Dirk Weissenborn, Georg Wiese, and Laura Seiffe. 2017.  Making neural qa as simple as possible but not simpler . In  Proceedings of the 21st Confer- ence on Computational Natural Language Learn- ing (CoNLL 2017) , pages 271–280. Association for Computational Linguistics. Johannes Welbl, Pontus Stenetorp, and Sebastian Riedel. 2018.  Constructing datasets for multi-hop reading comprehension across documents .  Transac- tions of the Association for Computational Linguis- tics , 6:287–302. Caiming Xiong, Victor Zhong, and Richard Socher. 2016. Dynamic coattention networks for question answering.  arXiv preprint arXiv:1611.01604 . Yuhao Zhang, Peng Qi, and Christopher D. Manning. 2018a.  Graph convolution over pruned dependency trees improves relation extraction . In  Proceedings of the 2018 Conference on Empirical Methods in Nat- ural Language Processing , pages 2205–2215. Asso- ciation for Computational Linguistics. Yuyu Zhang, Hanjun Dai, Zornitsa Kozareva, Alexan- der J Smola, and Le Song. 2018b. Variational reasoning for question answering with knowledge graph.  The Thirty-Second AAAI Conference on Ar- tiﬁcial Intelligence (AAAI-18) . \nMinjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi. 2016. Bidirectional atten- tion ﬂow for machine comprehension.  International Conference on Learning Representations (ICLR) . "}
{"page": 10, "image_path": "doc_images/N19-1240_10.jpg", "ocr_text": "A_ Implementation and Experiments\nDetails\n\nA.1l_ Architecture\n\nSee table 5 for an outline of Entity-GCN architec-\ntural detail. Here the computational steps\n\n1. ELMo embeddings are a concatenation of\nthree 1024-dimensional vectors resulting in\n3072-dimensional input vectors {x;}*,.\n\nNo\n\n. For the query representation q, we apply 2\nbi-LSTM layers of 256 and 128 hidden units\nto its ELMo vectors. The concatenation of\nthe forward and backward states results in a\n256-dimensional question representation.\n\noS)\n\n. ELMo embeddings of candidates are pro-\njected to 256-dimensional vectors, concate-\nnated to the q, and further transformed with\na two layers MLP of 1024 and 512 hidden\nunits in 512-dimensional query aware entity\nrepresentations {%;}_, € R°!?.\n\n4. All transformations f,, in R-GCN-layers are\naffine and they do maintain the input and out-\nput dimensionality of node representations\nthe same (512-dimensional).\n\nmn\n\n. Eventually, a 2-layers MLP with [256, 128]\nhidden units takes the concatenation between\nphi) and q to predict the probability\nthat a candidate node v; may be the answer\nto the query q (see Equation 1).\n\nDuring preliminary trials, we experimented\nwith different numbers of R-GCN-layers (in the\nrange 1-7). We observed that with WIKIHOP, for\nL > 3 models reach essentially the same perfor-\nmance, but more layers increase the time required\nto train them. Besides, we observed that the gating\nmechanism learns to keep more and more informa-\ntion from the past at each layer making unneces-\nsary to have more layers than required.\n\nA.2_ Training Details\n\nWe train our models with a batch size of 32\nfor at most 20 epochs using the Adam opti-\nmizer (Kingma and Ba, 2015) with 6; = 0.9,\nBy = 0.999 and a learning rate of 10~*. To help\nagainst overfitting, we employ dropout (drop rate\n€ 0,0.1,0.15,0.2,0.25) (Srivastava et al., 2014)\nand early-stopping on validation accuracy. We re-\nport the best results of each experiment based on\naccuracy on validation set.\n\nB_ Error Analysis\n\nIn Table 6, we report three samples from WIKI-\nHop development set where out Entity-GCN fails.\nIn particular, we show two instances where our\nmodel presents high confidence on the answer,\nand one where is not. We commented these sam-\nples explaining why our model might fail in these\ncases.\n\nC Ablation Study\n\nIn Figure 3, we show how the model performance\ngoes when the input graph is large. In particular,\nhow Entity-GCN performs as the number of can-\ndidate answers or the number of nodes increases.\n\n1.0\n0.8\n\n0.6\n\n0.2\n\n0.0\n\n0 10 20 30 40 50 60 70\n\n(a) Candidates set size (x-axis) and accuracy (y-axis). Pear-\nson’s correlation of —0.687 (p < 107\").\n\n1.0\n0.8\n0.6\n0.4\n\n0.2\n\n9.0 0 50 100 150 200\n\n(b) Nodes set size (x-axis) and accuracy (y-axis). Pearson’s\ncorrelation of —0.385 (p < 1077).\n\nFigure 3: Accuracy (blue) of our best single model\nwith respect to the candidate set size (on the top) and\nnodes set size (on the bottom) on the validation set. Re-\nscaled data distributions (orange) per number of candi-\ndate (top) and nodes (bottom). Dashed lines indicate\naverage accuracy.\n\n2316\n", "vlm_text": "A Implementation and Experiments Details \nA.1 Architecture \nSee table  5  for an outline of Entity-GCN architec- tural detail. Here the computational steps \n1. ELMo embeddings are a concatenation of three 1024-dimensional vectors resulting in 3072-dimensional input vectors    $\\{\\mathbf{x}_{i}\\}_{i=1}^{N}$  . \n2. For the query representation  q , we apply 2 bi-LSTM layers of 256 and 128 hidden units to its ELMo vectors. The concatenation of the forward and backward states results in a 256-dimensional question representation. \n3. ELMo embeddings of candidates are pro- jected to 256-dimensional vectors, concate- nated to the    $\\mathbf{q}$  , and further transformed with a two layers MLP of 1024 and 512 hidden units in 512-dimensional query aware entity representations    $\\{\\hat{\\mathbf{x}}_{i}\\}_{i=1}^{N}\\in\\bar{\\mathbb{R}}^{51\\bar{2}}$  . \n4. All transformations    $f_{*}$  in R-GCN-layers are afﬁne and they do maintain the input and out- put dimensionality of node representations the same (512-dimensional). \n5. Eventually, a 2-layers MLP with [256, 128] hidden units takes the concatenation between  $\\{\\mathbf{h}_{i}^{(L)}\\}_{i=1}^{N}$    and    $\\mathbf{q}$   to predict the probability that a candidate node    $v_{i}$   may be the answer to the query  $q$   (see Equation  1 ). \nDuring preliminary trials, we experimented with different numbers of R-GCN-layers (in the range 1-7). We observed that with W IKI H OP , for  $L\\geq3$   models reach essentially the same perfor- mance, but more layers increase the time required to train them. Besides, we observed that the gating mechanism learns to keep more and more informa- tion from the past at each layer making unneces- sary to have more layers than required. \nB Error Analysis \nIn Table  6 , we report three samples from W IKI - H OP  development set where out Entity-GCN fails. In particular, we show two instances where our model presents high conﬁdence on the answer, and one where is not. We commented these sam- ples explaining why our model might fail in these cases. \nC Ablation Study \nIn Figure  3 , we show how the model performance goes when the input graph is large. In particular, how Entity-GCN performs as the number of can- didate answers or the number of nodes increases. \nThe image is a histogram plot showing two overlaid distributions, colored in blue and brown. The x-axis has a range from 0 to 70, and the y-axis ranges from 0.0 to 1.0, indicating normalized frequency or proportion. The plot also features a horizontal dashed line at approximately 0.6 on the y-axis. This line could indicate a threshold or reference value against which the data in the histogram is being compared. The blue histogram appears to extend higher and covers the entire range, while the brown histogram is shorter and more concentrated towards the left of the plot.\nThe image is a bar chart depicting two data sets represented by stacked blue and brown bars. The x-axis is labeled \"Candidates set size,\" which increases from 0 to 200. The y-axis is labeled \"accuracy,\" ranging from 0.0 to 1.0. A horizontal dashed line across the chart likely signifies a particular threshold or mean value for accuracy, around 0.6.\n\nThe chart also displays a summary statistic in the caption text, mentioning Pearson's correlation coefficient of -0.687 with a p-value of less than 10^-7, indicating a strong negative correlation between candidates set size and accuracy that is highly statistically significant.\nFigure 3: Accuracy (blue) of our best single model with respect to the candidate set size (on the  top ) and nodes set size (on the  bottom ) on the validation set. Re- scaled data distributions (orange) per number of candi- date   $(t o p)$   and nodes ( bottom ). Dashed lines indicate average accuracy. \nA.2 Training Details \nWe train our models with a batch size of 32 for at most 20 epochs using the Adam opti- mizer ( Kingma and Ba ,  2015 ) with    $\\beta_{1}~=~0.9,$  ,  $\\beta_{2}\\,=\\,0.999$   and a learning rate of    $10^{-4}$  . To help against overﬁtting, we employ dropout (drop rate  $\\in0,0.1,0.15,0.2,0.25)$   ( Srivastava et al. ,  2014 ) and early-stopping on validation accuracy. We re- port the best results of each experiment based on accuracy on validation set. "}
{"page": 11, "image_path": "doc_images/N19-1240_11.jpg", "ocr_text": "Input - q, {v;}.,\nquery ELMo 3072-dim | candidates ELMo 3072-dim\n2 layers bi-LSTM [256, 128]-dim | 1 layer FF 256-dim\n\nconcatenation 512-dim\n2 layer FF (1024, 512]-dim: : {%;}%,\n3 layers R-GCN 512-dim each (shared parameters)\n\nconcatenation with q 768-dim\n\n3 layers FF [256,128,1]-dim\n\nOutput - probabilities over C,\n\nTable 5: Model architecture.\n\nID | WH-dev_2257 | Gold answer | 2003 (p = 14.1)\nQuery | inception (of) Derrty Entertainment | Predicted answer | 2000 (p = 15.8)\nSupport 1 | Derrty Entertainment is a record label founded by [...]. The first album released under\nDerrty Entertainment was Nelly ’s Country Grammar.\nSupport 2 | Country Grammar is the debut single by American rapper Nelly. The song was pro-\n\nduced by Jason Epperson. It was released in 2000, [...]\n\n(a) In this example, the model predicts the answer correctly. However, there is a mismatch between what is written in\n\nWIKIPEDIA an\n\nwhat is annotated in WIKIDATA. In WIKIHOP, answers are generated with WIKIDATA.\n\nID | WH-dev_2401 | Gold answer | Adolph Zukor (p = 7.le—4%)\nQuery | producer (of) Forbidden Paradise | Predicted answer | Jesse L. Lask (p = 99.9%)\nSupport 1 | Forbidden Paradise is a [...] drama film produced by Famous Players-Lasky [...]\nSupport 2 | Famous Players-Lasky Corporation was [...] from the merger of Adolph Zukor’s Fa-\n\n(b) In this samp!\n\nmous Players Film Company [..] and the Jesse L. Lasky Feature Play Company.\n\nle, there is ambiguity between two entities since both are correct answers reading the passages but only one is\n\nmarked as correct. The model fails assigning very high probability to only on one of them.\n\nID | WH-dev-3030 | Gold answer | Scania (p = 0.029%)\nQuery | place_of_birth (of) Erik Penser | Predicted answer | Eslév (p = 97.3%)\nSupport 1 | Nils Wilhelm Erik Penser (born August 22, 1942, in Eslév, Skane) is a Swedish [...]\nSupport 2 | Skane County, sometimes referred to as “ Scania County ” in English, is the [...]\n\n(c) In this sample, there is ambiguity between two entities since the city Eslév is located in the Scania County (English name\nof Skane County). The model assigning high probability to the city and it cannot select the county.\n\nTable 6: Samples from WIKIHOP set where Entity-GCN fails. p indicates the predicted likelihood.\n\n2317\n", "vlm_text": "The table outlines a neural network architecture used for processing queries and candidates, leveraging ELMo embeddings and several neural network layers. Here's a breakdown of the components:\n\n1. **Input**:\n   - **q**: The query input.\n   - \\(\\{v_i\\}_{i=1}^N\\): The candidate set for each query.\n\n2. **Query and Candidates Representation**:\n   - Query and candidates are represented using **ELMo embeddings**, each having a 3072-dimensional vector.\n\n3. **Processing Layers**:\n   - The query is passed through **2 layers of bi-directional LSTM** resulting in dimensions [256, 128].\n   - The candidates are passed through **1 layer of a Feed-Forward network** with a 256-dimensional output.\n\n4. **Concatenation Step**:\n   - A concatenation operation resulting in a 512-dimensional vector.\n\n5. **Intermediate Processing**:\n   - The concatenated embeddings are further processed by a **2 layer Feed-Forward network** with dimensions [1024, 512], represented as \\(\\{\\hat{x}_i\\}_{i=1}^N\\).\n\n6. **Graph Convolutional Network**:\n   - A **3-layer Relational Graph Convolutional Network (R-GCN)** with each layer having 512 dimensions, utilizing shared parameters.\n\n7. **Integration with Query**:\n   - Another concatenation operation that combines the processed embedding from R-GCN with the query, resulting in a 768-dimensional representation.\n\n8. **Final Processing**:\n   - The concatenated result undergoes further processing through **3 layers of a Feed-Forward network** with dimensions [256, 128, 1].\n\n9. **Output**:\n   - The final output is a set of **probabilities over \\(C_q\\)**, where \\(C_q\\) likely represents the candidate space or classes related to the query.\n  \nThis architecture seems to be designed for tasks like question answering or selection of relevant candidates based on a query, employing ELMo for contextual embeddings and layers of LSTMs, R-GCN, and feed-forward networks for processing and decision making.\nThis table presents examples from a model's predictions versus gold (correct) answers across different queries. It shows:\n\n1. **First Example (ID: WH_dev_2257)**\n   - **Query:** Inception of Derrty Entertainment\n   - **Gold Answer:** 2003\n   - **Predicted Answer:** 2000\n   - **Support Passages:** Discuss the debut album \"Country Grammar.\"\n\n2. **Second Example (ID: WH_dev_2401)**\n   - **Query:** Producer of Forbidden Paradise\n   - **Gold Answer:** Adolph Zukor\n   - **Predicted Answer:** Jesse L. Lask\n   - **Support Passages:** Reference Famous Players-Lasky Corporation and its founders.\n\n3. **Third Example (ID: WH_dev_3030)**\n   - **Query:** Place of birth of Erik Penser\n   - **Gold Answer:** Scania\n   - **Predicted Answer:** Eslöv\n   - **Support Passages:** Mention Eslöv, Skåne as part of Scania County.\n\nFootnotes explain discrepancies, such as mismatches between Wikipedia and Wikidata for the first example, and ambiguity in support passages for the second example.\nTable 6: Samples from W IKI H OP  set where Entity-GCN fails.    $p$   indicates the predicted likelihood. "}
