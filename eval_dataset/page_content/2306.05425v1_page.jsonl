{"page": 0, "image_path": "doc_images/2306.05425v1_0.jpg", "ocr_text": "arXiv:2306.05425v1 [cs.CV] 8 Jun 2023\n\nMIMIC-IT: Multi-Modal In-Context\nInstruction Tuning\n\nBo Li*’ Yuanhan Zhang**:! Liangyu Chen**! Jinghao Wang**! Fanyi Pu*'!\nJingkang Yang! Chunyuan Li? = Ziwei Liu)™\n1§-Lab, Nanyang Technological University, Singapore Microsoft Research, Redmond\n{libo0013, yuanhan002, Ichen025, c190209, fpu001, ziwei.liu}@ntu.edu.sg\nchunyl@microsoft.com\n\nhtt yithub.com/Luodian/Otter\n\nAbstract\n\nHigh-quality instructions and responses are essential for the zero-shot performance\nof large language models on interactive natural language tasks. For interactive\nvision-language tasks involving intricate visual scenes, a large quantity of di-\nverse and creative instruction-response pairs should be imperative to tune vision-\nlanguage models (VLMs). Nevertheless, the current availability of vision-language\ninstruction-response pairs in terms of quantity, diversity, and creativity remains lim-\nited, posing challenges to the generalization of interactive VLMs. Here we present\nMultI-Modal In-Context Instruction Tuning (MIMIC-IT), a dataset comprising\n2.8 million multimodal instruction-response pairs, with 2.2 million unique instruc-\ntions derived from images and videos. Each pair is accompanied by multi-modal\nin-context information, forming conversational contexts aimed at empowering\nVLMs in perception, reasoning, and planning. The instruction-response collection\nprocess, dubbed as Syphus, is scaled using an automatic annotation pipeline that\ncombines human expertise with GPT’s capabilities. Using the MIMIC-IT dataset,\nwe train a large VLM named Otter. Based on extensive evaluations conducted\non vision-language benchmarks, it has been observed that Otter demonstrates re-\nmarkable proficiency in multi-modal perception, reasoning, and in-context learning.\nHuman evaluation reveals it effectively aligns with the user’s intentions. We release\nthe MIMIC-IT dataset, instruction-response collection pipeline, benchmarks, and\nthe Otter model.\n\n1 Introduction\n\nThe recent advancements in artificial intelligence have focused on conversational assistants [42,\n31, 30, 13, 17] that possess a strong ability to understand user intentions [35] and then execute\nactions [5, 51]. In addition to the strong generalization ability of large language models (LLMs), the\nnotable achievements of these conversational assistants can be attributed to the practice of instruction\ntuning [47, 14, 46, 45, 42, 13, 34]. It involves fine-tuning LLMs on a range of tasks specified through\ndiverse and high-quality instructions [14, 45]. By incorporating instruction tuning, LLMs acquire\na heightened comprehension of user intentions [35], enabling them to exhibit improved zero-shot\ncapabilities even in previously unseen tasks [47]. One potential reason for the zero-shot performance\ngain by instruction tuning is that it internalizes the context [40], which is preferred in user interactions\nespecially when user input skips commonsense context.\n\nConversational assistants that excel in language tasks have achieved remarkable success. However, an\noptimal conversational assistant should be able to address tasks involving multiple modalities. This\n\n*Equal Contribution Project Lead “Corresponding Author\n\nPreprint. Under review.\n", "vlm_text": "MIMIC-IT: Multi-Modal In-Context Instruction Tuning \nBo  $\\mathbf{Li^{*,\\bigstar,1}}$    Yuanhan Zhang ∗ , ♠ ,   Liangyu Chen ∗ ,   Jinghao Wang ∗ ,   Fanyi   $\\mathbf{P}\\mathbf{u}^{*,1}$  Jingkang Yang 1 Chunyuan Li 2 Ziwei Liu 1 , \n1 S-Lab, Nanyang Technological University, Singapore 2 Microsoft Research, Redmond {libo0013, yuanhan002, lchen025, c190209, fpu001, ziwei.liu}@ntu.edu.sg chunyl@microsoft.com https://github.com/Luodian/Otter \nAbstract \nHigh-quality instructions and responses are essential for the zero-shot performance of large language models on interactive natural language tasks. For interactive vision-language tasks involving intricate visual scenes, a large quantity of di- verse and creative instruction-response pairs should be imperative to tune vision- language models (VLMs). Nevertheless, the current availability of vision-language instruction-response pairs in terms of quantity, diversity, and creativity remains lim- ited, posing challenges to the generalization of interactive VLMs. Here we present M ult I - M odal  I n- C ontext  I nstruction  T uning ( MIMIC-IT ), a dataset comprising 2.8 million multimodal instruction-response pairs, with 2.2 million unique instruc- tions derived from images and videos. Each pair is accompanied by multi-modal in-context information, forming conversational contexts aimed at empowering VLMs in perception, reasoning, and planning. The instruction-response collection process, dubbed as  Syphus , is scaled using an automatic annotation pipeline that combines human expertise with GPT’s capabilities. Using the MIMIC-IT dataset, we train a large VLM named  Otter . Based on extensive evaluations conducted on vision-language benchmarks, it has been observed that Otter demonstrates re- markable proficiency in multi-modal perception, reasoning, and in-context learning. Human evaluation reveals it effectively aligns with the user’s intentions. We release the MIMIC-IT dataset, instruction-response collection pipeline, benchmarks, and the Otter model. \n1 Introduction \nThe recent advancements in artificial intelligence have focused on conversational assistants [ 42 , 31 ,  30 ,  13 ,  17 ] that possess a strong ability to understand user intentions [ 35 ] and then execute actions [ 5 ,  51 ]. In addition to the strong generalization ability of large language models (LLMs), the notable achievements of these conversational assistants can be attributed to the practice of instruction tuning [ 47 ,  14 ,  46 ,  45 ,  42 ,  13 ,  34 ]. It involves fine-tuning LLMs on a range of tasks specified through diverse and high-quality instructions [ 14 ,  45 ]. By incorporating instruction tuning, LLMs acquire a heightened comprehension of user intentions [ 35 ], enabling them to exhibit improved zero-shot capabilities even in previously unseen tasks [ 47 ]. One potential reason for the zero-shot performance gain by instruction tuning is that it internalizes the context [ 40 ], which is preferred in user interactions especially when user input skips commonsense context. \nConversational assistants that excel in language tasks have achieved remarkable success. However, an optimal conversational assistant should be able to address tasks involving multiple modalities. This "}
{"page": 1, "image_path": "doc_images/2306.05425v1_1.jpg", "ocr_text": "Perception Reasoning\n\nldhat might be the purpose of this ERERNUESTE RN How did the students advocate for immigration\ngathering of people? (LA) as (“reform policies during the cerenony? (VIST)\n[Social Reasoning]\nst\ndifference between\ntthe two images? (50)\n\n‘Subtle Difference\n\nwhat are the male and fenale\ncharacters holding while they\n‘are together? (TVC)\nGEESE sat might this young boy!\nJearn from this\nexperience? (LA)\n\nihat is the difference\nbetween the two images? (GSD)\n\nGeneral Scene Difference brighten up the room I’m in? (E4D)\n\nlihat does the girl do\nafter completing the\nroutine? (0c)\n\nCan you think of sone reasons\nwhy cats might sit on\nsomeone‘s laptop? (LA)\n\ncan you suggest any recreational pursuits that\n‘can be enjoyed within this room? (IEP)\n\nrite a detailed description ‘Abductive Reasoning\nof the given image. (LA)\n\nFigure 1: MIMIC-IT overview. The MIMIC-IT dataset comprises 2.8M multi-modal instruction-\nresponse pairs spanning fundamental capabilities: perception, reasoning, and planning. Each instruc-\ntion is accompanied by multi-modal conversational context, allowing VLMs trained on MIMIC-IT to\ndemonstrate strong proficiency in interactive instruction following with zero-shot generalization.\n\nrequires access to a diverse and high-quality multi-modal instruction-following dataset. The LLaVA-\nInstruct-150K dataset [28], also known as LLaVA, is the pioneering vision-language instruction-\nfollowing dataset. It is constructed using COCO [27] images, instructions and responses obtained\nfrom GPT-4 [30] based on image captions and object bounding boxes.\n\nAlthough inspiring, LLaVA-Instruct-150K exhibits three limitations. (1) Limited visual diversity:\nThe dataset’s visual diversity is constrained due to its exclusive reliance on the COCO image.\n(2) Single image as visual data: it utilizes a single image as visual data, while a multi-modal\nconversational assistant should possess the capability to process multiple images or even extensive\nvideos. For instance, it should effectively provide answers when a user presents a collection of images\n(or a sequence of images, such as a video) alongside the instruction: \"Help me think of an album title\nfor these images.\" (3) Language-only in-context information: it depends solely on language for\nin-context information, whereas a multi-modal conversational assistant should integrate multi-modal\nin-context information to better comprehend user instructions. For example, an assistant could more\naccurately align its description of an image with the tone, style, or other aspects if the human user\nprovides a concrete image example of the desired attributes.\n\nAddressing these limitations, we introduce MultI-Modal In-Context Instruction Tuning (MIMIC-\nIT). MIMIC-IT is characterized by: (1) Diverse visual scenes, incorporating images and videos\nfrom general scenes, egocentric view scenes, and indoor RGB-D images across various datasets. (2)\nMultiple images (or a video) as visual data, supporting instruction-response pairs accompanied\nby any number of images or videos. (3) Multi-modal in-context information, featuring in-context\ninformation formulated in multi-modal formats, including multiple instruction-response pairs and\nmultiple images or videos (see Fig. 2 for data format clarification). To efficiently generate instruction-\nresponse pairs, we introduce Sythus, an automated pipeline for instruction-response annotation\ninspired by the self-instruct method [45]. Sythus employs system message, visual annotation, and\nin-context examples to direct the language model (GPT-4 or ChatGPT) in generating instruction-\nresponse pairs based on visual context, including timestamps, captions, and object information,\ntargeting three fundamental capabilities of vision-language models: perception, reasoning, and\nplanning (refer to Fig. |). Additionally, instructions and responses are translated from English into\nseven languages to support multi-lingual usage.\n", "vlm_text": "The image is a diagram illustrating the MIMIC-IT dataset overview, which includes multi-modal instruction-response pairs. These pairs focus on fundamental capabilities such as perception, reasoning, and planning. The diagram is divided into three sections:\n\n1. **Perception**:\n   - Tasks involve identifying subtle differences and general scene differences in images, describing images, and analyzing television drama scenes.\n\n2. **Reasoning**:\n   - Includes social reasoning, commonsense reasoning, egocentric video reasoning, and abductive reasoning. Examples involve understanding story context, predicting outcomes, and inferring purposes of events.\n\n3. **Planning**:\n   - Focuses on egocentric planning and indoor event planning, with tasks like suggesting ways to alter environments and planning activities.\n\nEach section contains sample questions illustrating the type of instruction and response pairs found in the dataset.\nrequires access to a diverse and high-quality multi-modal instruction-following dataset. The LLaVA- Instruct-150K dataset [ 28 ], also known as LLaVA, is the pioneering vision-language instruction- following dataset. It is constructed using COCO [ 27 ] images, instructions and responses obtained from GPT-4 [ 30 ] based on image captions and object bounding boxes. \nAlthough inspiring, LLaVA-Instruct-150K exhibits three limitations.  (1) Limited visual diversity : The dataset’s visual diversity is constrained due to its exclusive reliance on the COCO image. (2) Single image as visual data : it utilizes a single image as visual data, while a multi-modal conversational assistant should possess the capability to process multiple images or even extensive videos. For instance, it should effectively provide answers when a user presents a collection of images (or a sequence of images, such as a video) alongside the instruction:  \"Help me think of an album title for these images.\"  (3) Language-only in-context information : it depends solely on language for in-context information, whereas a multi-modal conversational assistant should integrate multi-modal in-context information to better comprehend user instructions. For example, an assistant could more accurately align its description of an image with the tone, style, or other aspects if the human user provides a concrete image example of the desired attributes. \nAddressing these limitations, we introduce  M ult I - M odal  I n- C ontext  I nstruction  T uning ( MIMIC- IT ). MIMIC-IT is characterized by:  (1) Diverse visual scenes , incorporating images and videos from general scenes, egocentric view scenes, and indoor RGB-D images across various datasets.  (2) Multiple images (or a video) as visual data , supporting instruction-response pairs accompanied by any number of images or videos.  (3) Multi-modal in-context information , featuring in-context information formulated in multi-modal formats, including multiple instruction-response pairs and multiple images or videos (see Fig.  2  for data format clarification). To efficiently generate instruction- response pairs, we introduce  Sythus , an automated pipeline for instruction-response annotation inspired by the self-instruct method [ 45 ]. Sythus employs system message, visual annotation, and in-context examples to direct the language model (GPT-4 or ChatGPT) in generating instruction- response pairs based on visual context, including timestamps, captions, and object information, targeting three fundamental capabilities of vision-language models: perception, reasoning, and planning (refer to Fig.  1 ). Additionally, instructions and responses are translated from English into seven languages to support multi-lingual usage. "}
{"page": 2, "image_path": "doc_images/2306.05425v1_2.jpg", "ocr_text": "On MIMIC-IT, we train a multi-modal model Otter based on OpenFlamingo [6]. We evaluate\nOtter’s multi-modal capabilities in two aspects: (1) ChatGPT evaluation on the MMAGIBench-\nmark [43], comparing Otter’s perception and reasoning abilities with other recent vision-language\nmodels (VLMs), where Otter demonstrates the strongest performance. (2) Human evaluation on\nthe Multi-Modality Arena [32], where Otter outperforms other VLMs, achieving the highest Elo\nrating. Furthermore, we assess Otter’s few-shot in-context learning ability using the COCO Caption\ndataset [12], with results showing Otter’s superior performance over OpenFlamingo in all few-shot\nsettings. In summary, our contributions include:\n\n¢ MultI-Modal In-Context Instruction Tuning (MIMIC-IT) dataset, a dataset comprising ~\n2.8M multi-modal in-context instruction-response pairs, with 2.2 million unique instructions,\nacross various real-life scenes.\n\n¢ Syphus, an automatic pipeline built with LLMs to generate high-quality and multi-lingual\ninstruction-response pairs based on visual context.\n\n* Otter, a multi-modal model demonstrates robust multi-modal perception and reasoning ca-\npabilities, effectively following human intent while exhibiting adeptness in-context learning.\n\n2 Related Work\n\n2.1 Multi-modal Instruction Tuning Dataset\n\nThe notion of instruction tuning in multi-modal models was initially introduced in the work called\nMulti-Instruct [50], which encompassed a wide range of multi-modal tasks [18, 56, 41, 27, 12]\ninvolving visual understanding and multi-modal reasoning, such as Visual Question Answering [18,\n56, 23]. Similarly, Mini-GPT4 [54] created its instruction-based dataset by merging Conceptual\nCaption [38, 8], SBU [33], and LAION [36] with handwritten instruction templates. More recently,\nLLaVA-Instruct-150K [28] has elevated the quality of instruction tuning datasets by utilizing self-\ninstruct and GPT-4 [30], along with handwritten seed instructions on COCO images [27]. While\nthese previous works on multi-modal instruction tuning primarily focused on general scene images,\nour approach categorizes our data sources into indoor scenes, outdoor scenes, conversations, and\negocentric videos. Additionally, drawing inspiration from the image-text interleaved structure of\nthe MMC4 dataset [55], our approach further distinguishes itself by incorporating a multi-modal\nin-context format into instruction tuning.\n\n2.2 Multi-modal Foundation Models\n\nWith the recent success of ChatGPT [31], GPT-4 [30], and other LLMs [44, 42, 13], recent studies\nstart to explore incorporating information from other modalities into pretrained language models.\nThese studies extend the capabilities of LLM to more tasks and modalities and can be categorized\ninto two classes: (i) Multi-model Aggregation. These approaches [48, 39, 11, 57, 57, 52] take an\nLLM as a dispatch scheduler and connect different expert models through it to allow for different\ntasks. Language serves as an interface to call expert visual-language models within their respective\ntask domains. However, this approach is limited that each model cannot be trained individually on\nnew tasks. (ii) End-to-End Trainable Models. These approaches [2, 6, 25, 30, 3, 37, 7, 54, 53, 28]\nconnect models from different modalities into integrated end-to-end trainable models, also known as\nmulti-modal foundation models. Among them, based on large-scale image-text interleaved pretrained\nmodel OpenFlamingo [6], Otter is the first open-sourced model to further demonstrate the power of\nmulti-modal in-context instruction tuning.\n\n3 Multi-modal In-context Instruction Tuning Dataset\n\nWe aim to build MIMIC-IT dataset to support more VLMs in acquiring the ability to comprehend\nthe real world. In this section, we provide an overview of the MIMIC-IT dataset, starting with the\ndata format in Sec. 3.1 and our automatic instruction generation pipeline, Sythus, in Sec. 3.2.\n", "vlm_text": "On  MIMIC-IT , we train a multi-modal model  Otter  based on Open Flamingo [ 6 ]. We evaluate Otter’s multi-modal capabilities in two aspects:  (1) ChatGPT evaluation  on the MMAGIBench- mark [ 43 ], comparing Otter’s perception and reasoning abilities with other recent vision-language models (VLMs), where Otter demonstrates the strongest performance.  (2) Human evaluation  on the Multi-Modality Arena [ 32 ], where Otter outperforms other VLMs, achieving the highest Elo rating. Furthermore, we assess Otter’s few-shot in-context learning ability using the COCO Caption dataset [ 12 ], with results showing Otter’s superior performance over Open Flamingo in all few-shot settings. In summary, our contributions include: \n•  M ult I - M odal  I n- C ontext  I nstruction  T uning ( MIMIC-IT ) dataset, a dataset comprising  $\\sim$  2.8M multi-modal in-context instruction-response pairs, with 2.2 million unique instructions, across various real-life scenes. •  Syphus , an automatic pipeline built with LLMs to generate high-quality and multi-lingual instruction-response pairs based on visual context. •  Otter , a multi-modal model demonstrates robust multi-modal perception and reasoning ca- pabilities, effectively following human intent while exhibiting adeptness in-context learning. \n2 Related Work \n2.1 Multi-modal Instruction Tuning Dataset \nThe notion of instruction tuning in multi-modal models was initially introduced in the work called Multi-Instruct [ 50 ], which encompassed a wide range of multi-modal tasks [ 18 ,  56 ,  41 ,  27 ,  12 ] involving visual understanding and multi-modal reasoning, such as Visual Question Answering [ 18 , 56 ,  23 ]. Similarly, Mini-GPT4 [ 54 ] created its instruction-based dataset by merging Conceptual Caption [ 38 ,  8 ], SBU [ 33 ], and LAION [ 36 ] with handwritten instruction templates. More recently, LLaVA-Instruct-150K [ 28 ] has elevated the quality of instruction tuning datasets by utilizing self- instruct and GPT-4 [ 30 ], along with handwritten seed instructions on COCO images [ 27 ]. While these previous works on multi-modal instruction tuning primarily focused on general scene images, our approach categorizes our data sources into indoor scenes, outdoor scenes, conversations, and egocentric videos. Additionally, drawing inspiration from the image-text interleaved structure of the MMC4 dataset [ 55 ], our approach further distinguishes itself by incorporating a multi-modal in-context format into instruction tuning. \n2.2 Multi-modal Foundation Models \nWith the recent success of ChatGPT [ 31 ], GPT-4 [ 30 ], and other LLMs [ 44 ,  42 ,  13 ], recent studies start to explore incorporating information from other modalities into pretrained language models. These studies extend the capabilities of LLM to more tasks and modalities and can be categorized into two classes:  (i) Multi-model Aggregation.  These approaches [ 48 ,  39 ,  11 ,  57 ,  57 ,  52 ] take an LLM as a dispatch scheduler and connect different expert models through it to allow for different tasks. Language serves as an interface to call expert visual-language models within their respective task domains. However, this approach is limited that each model cannot be trained individually on new tasks.  (ii) End-to-End Trainable Models.  These approaches [ 2 ,  6 ,  25 ,  30 ,  3 ,  37 ,  7 ,  54 ,  53 ,  28 ] connect models from different modalities into integrated end-to-end trainable models, also known as multi-modal foundation models. Among them, based on large-scale image-text interleaved pretrained model Open Flamingo [ 6 ], Otter is the first open-sourced model to further demonstrate the power of multi-modal in-context instruction tuning. \n3 Multi-modal In-context Instruction Tuning Dataset \nWe aim to build  MIMIC-IT  dataset to support more VLMs in acquiring the ability to comprehend the real world. In this section, we provide an overview of the  MIMIC-IT  dataset, starting with the data format in Sec.  3.1  and our automatic instruction generation pipeline,  Sythus , in Sec.  3.2 . "}
{"page": 3, "image_path": "doc_images/2306.05425v1_3.jpg", "ocr_text": "LLaVA-Instruct-15@K: Language-only In-context\n\n# In-context:\n\nInstruction: What type of traffic\ndevice can be seen in the image?\nResponse: There is a street sign and\na traffic light above a city road.\n\n# Query:\n\nInstruction: what is the current color of the traffic\nlights in the image?\n\nResponse: All the traffic lights in the image are showing\nthe color red at the same time.\n\nMIMIC-IT: Multi-modal In-context\nUp->Down: LLaVA-Interleaved->TV Show Captions\n\nInstruction: Why teddy bear\nornament on the Christmas tree?\nResponse: The teddy bear ornament\nhanging on the Christmas tree\nconveys a sense of .. children and\nadults during the festive season.\n\nInstruction:what the reason for the\ndog's current expression and outfit?\nZ Response:.. wearing a festive\nChristmas hat and possibly panting, ..,\nmaking a cute and memorable moment\nwith their pet during the holiday\nseason.\n\nInstruction: At what time of day is?\nResponse: As the bedside lam is opening and the light\nin this room is dark, it is in the night.\n\nInstruction: What task is the woman performing?\nResponse: The woman is vacuuming the rug area between\nchairs and couches.\n\nMIMIC-IT: Language-only In-context\nEgo4D->Spot The Difference\nSBeeeeee @eeeeese wee aeese FT EC SBeeseeaegqae 8\n\nInstruc: : What color of jersey does my teammates wear?\nResponse: Black, as you pass the ball to a player\nwearing black jersey.\n\nInstruction: Does our team goal?\nResponse: Unfortunately, does not. The The goalkeeper\nsaves the ball at the end\n\nInstruction: What is\nthe difference between\nthese two images?\nResponse: The first one\nshows an outhouse with\na toilet, while the\nsecond one shows...\n\nInstruction: Are there any objects in image two but\nnot in image one?\n\nResponse: Yes, there are additional objects in image\ntwo that are not present in image one. Image two has\na white sink, a bathtub, a mirror, and other\nbathroom items, which are not mentioned in image one.\n\nFigure 2: Data Format Comparison: LLaVA-Instruct-150K vs. MIMIC-IT. (a) LLaVA-Instruct-\n150K consists of a single image with corresponding language-only in-context information (yellow\nbox). (b) MIMIC-IT accommodates multiple images or videos within the input data and supports\nmulti-modal in-context information, i.e., considering both images/videos and language inputs as\nin-context information.\n\n3.1. MIMIC-IT Data Format\n\nEach instance 7 in the MIMIC-IT dataset comprises an instruction-response pair and a set of NV\nimages. We regard it as query example with a tuple: (I,, R, Xq), where {ah} € X,. Here, I,\ndenotes the q-th instruction in our dataset, R, represents the response, and X,, refers to the images or\nvideos |. Our primary objective is to develop a visual language model po (Ry | (Ly, Xq)) parametrized\nby trainable parameters 0, the model generates the response R; for each query (Iz, X_). With above\nexample denotes the standard instruction tuning process © of a visual language model. Further, we\ncould define a set of in-context examples as (I;,, Rx, X;,) iM where M is the number of the set.\n\n=1?\n\nWe then define a context function Cy : (Ig,Xq) +> {(Ik,Xx)}#4, to represent the in-context\nexamples with current query example. In summary, all data in the MIMIC-IT dataset will be\nrepresented in the following format, query example with its corresponding in-context examples.\n\ndy = (Iq, Rg, Xq, Cu (Iq, Xa); ()\n\nNow the visual language model that incorporates in-context examples can be denoted as po(Ry |\n(Iq, Xq,Cu(Iq,Xq))). Cy is task-dependent, we apply different approaches to organize the in-\n\ndy ~ Dyrutc-r1\n\n'Videos can be viewed as ordered sequences of images.\n", "vlm_text": "The image is a comparison of data formats: LLaVA-Instruct-150K vs. MIMIC-IT. \n\n1. **LLaVA-Instruct-150K: Language-only In-context**\n    - In-context example: A street scene with instructions and responses about the traffic devices.\n    - Query example: Questions about the color of traffic lights with corresponding responses.\n\n2. **MIMIC-IT: Multi-modal In-context**\n    - Example with a dog in a Christmas outfit and a teddy bear ornament, with related instructions and responses.\n    - Example from a TV show with instructions about a woman's task and the time of day.\n\n3. **MIMIC-IT: Language-only In-context**\n    - Examples from a sports scene and a bathroom, with instructions about jersey colors, goal scoring, and differences between images, along with responses.\n\nThis comparison highlights the differences in how each format handles in-context information, with MIMIC-IT accommodating multiple images or videos and supporting multi-modal inputs, unlike LLaVA-Instruct-150K, which uses language-only contexts.\n3.1 MIMIC-IT Data Format \nEach instance    $i$   in the MIMIC-IT dataset comprises an instruction-response pair and a set of    $N$  images. We regard it as query example with a tuple:    $(I_{q},R_{q},X_{q})$  , where  $\\{x_{j=1}^{N}\\}\\in X_{q}$  \t . Here,    $I_{q}$  denotes the    $q$  -th instruction in our dataset,  $R_{q}$   represents the response, and  $X_{q}^{\\cdot}$   refers to the images or videos by trainable parameters   1 . Our primary o  θ ective is to develop a visual languag , the model generates the response  $R_{i}$  model  for each query  $p_{\\theta}\\big(R_{q}\\mid\\big(I_{q},X_{q}\\big)\\big)$   |  $(I_{q},X_{q})$  para met rize d . With above example denotes the standard instruction tuning process of a visual language model. Further, we could define a set of in-context examples as    $(I_{k},{\\bf\\check{R}}_{k},X_{k})_{k=1}^{M}$  , where  $M$   is the number of the set. \nWe then define a context function    $C_{\\psi}\\;:\\;(I_{q},X_{q})\\;\\mapsto\\;\\{(I_{k},X_{k})\\}_{k=1}^{M}$    to represent the in-context examples with current query example. In summary, all data in the MIMIC-IT dataset will be represented in the following format, query example with its corresponding in-context examples. \n\n$$\nd_{q}=(I_{q},R_{q},X_{q},C_{\\psi}(I_{q},X_{q})),\\quad d_{q}\\sim D_{\\mathtt{M I M I C-I I}}\n$$\n \nag odel that incorporates in-context examples can be denoted as    $p_{\\theta}(R_{q}\\mid$   $(I_{q},X_{q},C_{\\psi}(I_{q},X_{q}\\bar{)})$  .  C  $C_{\\psi}$   is task-dependent, we apply different approaches to organize the in- "}
{"page": 4, "image_path": "doc_images/2306.05425v1_4.jpg", "ocr_text": "In-context\nexamples\n\n\\ Cold Start\"\n\ns s\nChatGPT « Be Q [aa As iN\n\nStep 1 . Prompt “ Step 2 “step 3 “step 4\nSystem Message _ Generate Filtering Translation\n+ visual annotation instruction-\n\nresponse pairs\n\nFigure 3: Sythus overview. We employ a cold-start stage to identify the optimal system message and\nin-context example for querying instruction-response pairs in a given dataset. Subsequently, Sythus,\nspanning steps | to 4, generates high-quality instruction-response pairs in eight languages.\n\nTable 1: Comparison between MIMIC-IT and other multi-modal instruction datasets. MIMIC-\nIT stands out with the following features: (1) The largest vision-language instruction dataset. (2) The\nfirst instruction dataset including video data. (3) Supporting multi-modal in-context scenarios (see\nFig. 2 for the data format). (4) Supporting eight languages including: English, Chinese, Spanish,\nJapanese, French, German, Korean, and Arabic. The data source of MIMIC-IT includes seven\ndatasets: COCO [27], Spot-the-diff [21] (SD), ScanNetV2 [15] (SN), VisualStorytelling [20] (VIST),\nDenseCaption/Activity caption [22] (DC), TVCaption [24] (TVC), and Ego4D [19] (E4D). lang.\nindicates language and vis. indicates vision.\n\nDataset Visual Data (Scenes) In-context Video #Clips/Images #Instruct. #Instance. Lang.\nMiniGPT-4 [54] CC (General) -/- x -/134M 4 5K English\nLLaVA [28] COCO (General) [27] lang./- x -/81K 261K 345K English\n\nCOCO (General) [27] lang./vis. — X -/81K 261K 345K\n\nSD (Surveillance) [21] lang./vis. x -/9K 10K 15K\n\nSN (Indoor Ego.) [15] _ lang./vis. x -/0.5K 4.8K 6K\nMIMIC-IT DC (General)[22] lang.Wvis. v 16K / 1M 40K 62K Multi.\n\nVIST (Story)[20] lang./vis. v -/16K 32K 33K\n\nTVC (TV)[24] lang/vis. 4 86K / 577K 86K 92K\n\nE4D (General Ego.)[19] lang./vis. v 400K / 6.4M 1.8M 2.4M\n\nTotal lang/vis. /  502K/8.1M 2.2M 2.8M\n\ncontext examples with the current query example. The details will be presented in Sec. 3.3 and\nillustrative examples will be showcased in Fig. 2.\n\n3.2 Sythus: Automatic Instruction-Response Generation Pipeline\n\nWe present Sythus (see Figure 3), an automated pipeline for generating high-quality instruction-\nresponse pairs in multiple languages. Building upon the framework proposed by LLaVA [28],\nwe utilize ChatGPT to generate instruction-response pairs based on visual content. To ensure the\nquality of the generated instruction-response pairs, our pipeline incorporates system messages,\nvisual annotations, and in-context examples as prompts for ChatGPT. System messages define the\ndesired tone and style of the generated instruction-response pairs, while visual annotations provide\nessential image information such as bounding boxes and image descriptions. In-context examples\nassist ChatGPT in learning within the context. Since the quality of coreset impacts subsequent\ndata collection process [10], we employ a cold-start strategy to enhance in-context examples before\nthe large-scale query. During the cold-start stage, in-context examples are collected by prompting\nChatGPT solely through system messages and visual annotations, employing a heuristic approach.\nThis stage concludes only when satisfactory in-context examples are identified. In step 4, once the\ninstruction-response pairs are obtained, the pipeline expands them into Chinese (zh), Japanese (ja),\n", "vlm_text": "The image is a flowchart explaining the Sythus process for generating high-quality instruction-response pairs in eight languages. Here's a breakdown of the steps:\n\n1. **Step 1: System Message + Visual Annotation**\n   - Begins with setting a system message and includes a visual annotation.\n\n2. **Cold Start**\n   - Identifies the best system message and in-context example. Used for querying instruction-response pairs in a dataset.\n\n3. **Step 2: Generate Instruction-Response Pairs**\n   - Utilizes ChatGPT to generate these pairs.\n\n4. **Step 3: Filtering**\n   - Applies a filtering process to ensure quality, again using ChatGPT.\n\n5. **Step 4: Translation**\n   - Translates the instruction-response pairs into eight languages.\n\nThe process emphasizes using in-context examples and iterative steps with ChatGPT for improved results.\nTable 1:  Comparison between MIMIC-IT and other multi-modal instruction datasets.  MIMIC- IT stands out with the following features: (1) The largest vision-language instruction dataset. (2) The first instruction dataset including video data. (3) Supporting multi-modal in-context scenarios (see Fig.  2  for the data format). (4) Supporting eight languages including: English, Chinese, Spanish, Japanese, French, German, Korean, and Arabic. The data source of MIMIC-IT includes seven datasets: COCO [ 27 ], Spot-the-diff [ 21 ] (SD), ScanNetV2 [ 15 ] (SN), Visual Storytelling [ 20 ] (VIST), Dense Caption/Activity caption [ 22 ] (DC), TVCaption [ 24 ] (TVC), and Ego4D [ 19 ] (E4D).  lang. indicates language and  vis.  indicates vision. \nThe table compares different datasets based on several attributes:\n\n1. **Dataset**: Lists the datasets, including MiniGPT-4, LLaVA, and MIMIC-IT.\n2. **Visual Data (Scenes)**: Describes the type and context of visual data, like COCO (General), SD (Surveillance), SN (Indoor Ego.), DC (General), VIST (Story), TVC (TV), and E4D (General Ego.).\n3. **In-context**: Specifies the type of context provided, such as language or visual, denoted as \"lang.\" and \"vis.\"\n4. **Video**: Indicates whether the dataset includes video data (✓ or ✗).\n5. **#Clips/Images**: Provides the number of clips and/or images in each dataset.\n6. **#Instruct.**: Lists the number of instructional elements.\n7. **#Instance.**: Shows the number of instances in each dataset.\n8. **Lang.**: States the language used in the dataset, such as English or Multi.\n\nThe dataset categories are split between general language visual datasets and specific themed datasets within MIMIC-IT. The table includes total counts at the bottom for aspects such as clips/images, instructions, and instances.\ncontext examples with the current query example. The details will be presented in Sec.  3.3  and illustrative examples will be showcased in Fig.  2 . \n3.2 Sythus: Automatic Instruction-Response Generation Pipeline \nWe present  Sythus  (see Figure  3 ), an automated pipeline for generating high-quality instruction- response pairs in multiple languages. Building upon the framework proposed by LLaVA [ 28 ], we utilize ChatGPT to generate instruction-response pairs based on visual content. To ensure the quality of the generated instruction-response pairs, our pipeline incorporates system messages, visual annotations, and in-context examples as prompts for ChatGPT. System messages define the desired tone and style of the generated instruction-response pairs, while visual annotations provide essential image information such as bounding boxes and image descriptions. In-context examples assist ChatGPT in learning within the context. Since the quality of coreset impacts subsequent data collection process [ 10 ], we employ a cold-start strategy to enhance in-context examples before the large-scale query. During the cold-start stage, in-context examples are collected by prompting ChatGPT solely through system messages and visual annotations, employing a heuristic approach. This stage concludes only when satisfactory in-context examples are identified. In step 4, once the instruction-response pairs are obtained, the pipeline expands them into Chinese (zh), Japanese (ja), Spanish (es), German (de), French (fr), Korean (ko), and Arabic (ar). For further details, please refer to Appendix  C , and task-specific prompts can be found in Appendix  D . "}
{"page": 5, "image_path": "doc_images/2306.05425v1_5.jpg", "ocr_text": "Spanish (es), German (de), French (fr), Korean (ko), and Arabic (ar). For further details, please refer\nto Appendix C, and task-specific prompts can be found in Appendix D.\n\n3.3. Visual Data Exploration\n\nAcknowledging the importance of high-quality visual annotations and the need for diverse vision-\nlanguage instructions that align with the distribution of real-world visual content, we curate a\ncollection of seven image and video datasets spanning a wide spectrum of scenes, from general to\nspecific. Encompassing various topics, the MIMIC-IT dataset includes general scene understanding\nand reasoning, spoting general and subtle differences, as well as facilitating egocentric view compre-\nhension to assist VLMs in future AR headsets, etc. In the subsequent sections, we will present the\napplication scenarios of our dataset: General Scene Understanding in Sec. 3.3.1 and General Scene\nUnderstanding in Sec. 3.3.2. In each sub-task, we elaborate on the process of organizing various data\ninto an in-context instruction tuning format, based on the previously established guidelines.\n\n3.3.1 General Scene Understanding\n\nFor understanding the general scenes, we include four tasks: (1) LLaVA-Interleaved. (2) Spot The\nDifference. (3) Visual Story Telling. (4) Dense Captions.\n\nLLaVA-Interleaved (LA-I). Learning with in-context examples is essential for effective instruction\ntuning. To achieve this, we refine the LLaVA-Instruct-150K [28] dataset by retrieving ten in-context\nexamples for each instruction-response pair in LLaVA-Instruct-150K, building LLaVA-Interleaved\n(LA-I). We identify each data’s in-context examples based on instruction text-to-text similarity or\nimage-image similarity. Further details on locating in-context examples and the data sources for LA-I\ncan be found in the Appendix.\n\nSpot The Difference (SD). Learning to discern differences between images is vital for understanding\nreal-world changes. Our study encompasses two interrelated task types in Scene Difference (SD),\naddressing varying complexity levels in difference identification. The first type, General Scene\nDifference, involves creating a pair of images by determining the most similar one to the current\nimage, utilizing image-to-image similarity relationships from the COCO2017 [27]. The second\ntype, Subtle Difference, features pairs of similar images with subtle distinctions sourced from the\nSpot-the-Diff[2 1], extracted from surveillance footage. For the first type, we prompt ChatGPT using\noriginal image captions and object detection annotations, while for the second type, we employ\nnatural language difference descriptions as annotations. The resulting instruction-response pairs\nfocus on identifying differences between the paired images.\n\nVisual Story Telling (VIST). Beyond traditional scene understanding, the ability to generate coherent\nand engaging narratives based on visual input expands the context comprehension of Visual Language\nModels (VLMs). To enable this, we propose a task using the Visual Storytelling datase [20], which\nincludes event-based image sequences and corresponding inquiry questions. Given that image\nannotations often contain narratives and timelines not directly observable, we instruct ChatGPT to act\nas a viewer answering questions about the images. The prompts also incorporate thought-provoking\ninquiries to promote creativity. Each task instance comprises multiple images and instruction-response\npairs, providing in-context examples.\n\nDense Captions (DC). Expanding the scope of video understanding, DC features dense captions\nfrom [22] corresponding to clips within longer videos. The instructions pose a diverse set of questions,\naddressing the general visual content of the video, human actions, and behaviors, the chronological\nsequence of events, and causal relationships. This approach encourages VLMs to delve deeper into\nthe intricacies of video content.\n\nTV Show Captions (TVC). The primary purpose of incorporating TV show clips with high-level\ncaptions into the training process of VLMs is to enhance their social reasoning abilities and deepen\ntheir understanding of complex character dynamics. By organizing drama clips from [24] to analyze\ncharacter relationships and motivations, we aim to challenge VLMs to move beyond mere perception\nand demonstrate their reasoning capabilities within the context of TV show narratives. This focused\napproach is crucial for fostering advanced VLMs capable of effectively handling diverse real-world\nsituations and user queries.\n", "vlm_text": "\n3.3 Visual Data Exploration \nAcknowledging the importance of high-quality visual annotations and the need for diverse vision- language instructions that align with the distribution of real-world visual content, we curate a collection of seven image and video datasets spanning a wide spectrum of scenes, from general to specific. Encompassing various topics, the MIMIC-IT dataset includes general scene understanding and reasoning, spoting general and subtle differences, as well as facilitating egocentric view compre- hension to assist VLMs in future AR headsets,  etc . In the subsequent sections, we will present the application scenarios of our dataset: General Scene Understanding in Sec.  3.3.1  and General Scene Understanding in Sec.  3.3.2 . In each sub-task, we elaborate on the process of organizing various data into an in-context instruction tuning format, based on the previously established guidelines. \n3.3.1 General Scene Understanding \nFor understanding the general scenes, we include four tasks:  (1)  LLaVA-Interleaved.  (2)  Spot The Difference.  (3)  Visual Story Telling.  (4)  Dense Captions. \nLLaVA-Interleaved (LA-I).  Learning with in-context examples is essential for effective instruction tuning. To achieve this, we refine the LLaVA-Instruct-150K [ 28 ] dataset by retrieving ten in-context examples for each instruction-response pair in LLaVA-Instruct-150K, building LLaVA-Interleaved (LA-I). We identify each data’s in-context examples based on instruction text-to-text similarity or image-image similarity. Further details on locating in-context examples and the data sources for LA-I can be found in the Appendix. \nSpot The Difference (SD).  Learning to discern differences between images is vital for understanding real-world changes. Our study encompasses two interrelated task types in Scene Difference (SD), addressing varying complexity levels in difference identification. The first type, General Scene Difference, involves creating a pair of images by determining the most similar one to the current image, utilizing image-to-image similarity relationships from the COCO2017 [ 27 ]. The second type, Subtle Difference, features pairs of similar images with subtle distinctions sourced from the Spot-the-Diff[ 21 ], extracted from surveillance footage. For the first type, we prompt ChatGPT using original image captions and object detection annotations, while for the second type, we employ natural language difference descriptions as annotations. The resulting instruction-response pairs focus on identifying differences between the paired images. \nVisual Story Telling (VIST).  Beyond traditional scene understanding, the ability to generate coherent and engaging narratives based on visual input expands the context comprehension of Visual Language Models (VLMs). To enable this, we propose a task using the Visual Storytelling datase [ 20 ], which includes event-based image sequences and corresponding inquiry questions. Given that image annotations often contain narratives and timelines not directly observable, we instruct ChatGPT to act as a viewer answering questions about the images. The prompts also incorporate thought-provoking inquiries to promote creativity. Each task instance comprises multiple images and instruction-response pairs, providing in-context examples. \nDense Captions (DC).  Expanding the scope of video understanding, DC features dense captions from [ 22 ] corresponding to clips within longer videos. The instructions pose a diverse set of questions, addressing the general visual content of the video, human actions, and behaviors, the chronological sequence of events, and causal relationships. This approach encourages VLMs to delve deeper into the intricacies of video content. \nTV Show Captions (TVC).  The primary purpose of incorporating TV show clips with high-level captions into the training process of VLMs is to enhance their social reasoning abilities and deepen their understanding of complex character dynamics. By organizing drama clips from [ 24 ] to analyze character relationships and motivations, we aim to challenge VLMs to move beyond mere perception and demonstrate their reasoning capabilities within the context of TV show narratives. This focused approach is crucial for fostering advanced VLMs capable of effectively handling diverse real-world situations and user queries. "}
{"page": 6, "image_path": "doc_images/2306.05425v1_6.jpg", "ocr_text": "Nl\n\nww,\n\n(a) Instructions\n\n$ 100k|\n10k|\n1k\n100\n10]\n\n1k!\n1020 30 40 0 100 200 300 400 500 0 20 40 60 80 100 120 0-2 4 6 8 10 12\nInstruction Length Response Length # Images # Related Instructions\n\n(c) Statistics of instructions and responses.\n\n100k|\n\nsas8\nSzLe\n\nS\n\n# Instructions\n# Instructions\n\n10\n1\n\n°\n\nFigure 4: The data statistics of multi-modal in-context instruction-response pairs. (a) and (b),\nthe root verb-noun pairs of instruction and responses, where the inner circle of the plot represents\nthe root verb of the output response, and the outer circle represents the direct nouns. (c) Statistics of\ninstructions and responses, retaining 25% of Ego4D instructions for a more balanced distribution. #\nRelated instructions denotes the number of related instructions in an instance, given the same set of\nvisual input data.\n\n3.3.2 Egocentric View Understanding\n\nIndoor Event Planning (IEP). Emphasizing the planning capabilities of virtual assistants, we utilize\nvisual inputs consisting of a collection of 2D photos depicting a room. We gather indoor scene\nRGB-D images from ScanNetv2 [15] and sample them into multiple 2D visual inputs, representing\naroom’s layout from a first-person perspective. We prompt ChatGPT to generate instructions that\ndirect humans to perform various activities in indoor spaces. Initially, we have ChatGPT create a\npersonality for the room owner. Subsequently, the planning should be intimately related to the room’s\nlayout and the generated room owner, underlining the importance of context awareness in VLMs.\nThis approach ensures that models can effectively support users across diverse indoor scenarios.\n\nEgo4D (E4D) [19]. Utilizing E4D’s egocentric videos, we strive to enable VLMs to function\neffectively as augmented reality (AR) assistants in real-life scenarios. By prompting ChatGPT to\ngenerate instructions based on visual descriptions, our goal is to simulate practical interactions\nbetween users and AR assistants. To this end, we devise assistant-related questions and tasks that\ndemand context-aware responses. For instance, Jnstruction: What should I do now? Response: Based\non my observation, you can now proceed to do.... This focused approach underscores the potential of\nVLMs in providing valuable insights and assistance across a diverse range of daily life situations.\n\n3.4 Dataset Statistics\n\nTable | presents the essential statistics pertaining to the generated data. Our dataset comprises over\n2.8 million instruction-response pairs, wherein each pair includes at least one multi-modal in-context\nexample and one language-only in-context example. Among these pairs, there are 2.2M unique\ninstructions. Furthermore, to examine the characteristics and diversity of the instructions (refer to\nFig. 4 (a)) and responses (refer to Fig. 4 (b)), we analyze the verb-noun structure present in them,\nrefering to [45]. Specifically, we employ spaCy for parsing the instructions, extracting the verb closest\n", "vlm_text": "The image contains several visual components:\n\n1. **Circular Charts**:\n   - **Left Chart (a)**: Represents various categories or types of instructions. The segments are labeled with words like \"identify,\" \"object,\" \"item,\" \"action,\" etc., and they are color-coded.\n   - **Right Chart (b)**: Represents categories or types of responses. The segments are labeled with words like \"provide,\" \"information,\" \"see,\" \"take,\" etc., and they also use similar color coding.\n\n2. **Bar Graphs**:\n   - There are four bar graphs below the circular charts.\n   - The first graph shows the distribution of instruction lengths, indicating the number of instructions for different lengths.\n   - The second graph shows response lengths, indicating the number of responses for various lengths.\n   - The third graph shows the number of images associated with instructions.\n   - The fourth graph shows the number of related instructions.\n\nThese charts and graphs depict the statistics of instructions and responses, as noted in the caption.\nFigure 4:  The data statistics of multi-modal in-context instruction-response pairs.  (a) and (b), the root verb-noun pairs of instruction and responses, where the inner circle of the plot represents the root verb of the output response, and the outer circle represents the direct nouns. (c) Statistics of instructions and responses, retaining  $25\\%$   of Ego4D instructions for a more balanced distribution. # Related instructions denotes the number of related instructions in an instance, given the same set of visual input data. \n3.3.2 Egocentric View Understanding \nIndoor Event Planning (IEP).  Emphasizing the planning capabilities of virtual assistants, we utilize visual inputs consisting of a collection of 2D photos depicting a room. We gather indoor scene RGB-D images from ScanNetv2 [ 15 ] and sample them into multiple 2D visual inputs, representing a room’s layout from a first-person perspective. We prompt ChatGPT to generate instructions that direct humans to perform various activities in indoor spaces. Initially, we have ChatGPT create a personality for the room owner. Subsequently, the planning should be intimately related to the room’s layout and the generated room owner, underlining the importance of context awareness in VLMs. This approach ensures that models can effectively support users across diverse indoor scenarios. \nEgo4D (E4D) [ 19 ].  Utilizing E4D’s egocentric videos, we strive to enable VLMs to function effectively as augmented reality (AR) assistants in real-life scenarios. By prompting ChatGPT to generate instructions based on visual descriptions, our goal is to simulate practical interactions between users and AR assistants. To this end, we devise assistant-related questions and tasks that demand context-aware responses. For instance,  Instruction: What should I do now? Response: Based on my observation, you can now proceed to do... . This focused approach underscores the potential of VLMs in providing valuable insights and assistance across a diverse range of daily life situations. \n3.4 Dataset Statistics \nTable  1  presents the essential statistics pertaining to the generated data. Our dataset comprises over 2.8 million instruction-response pairs, wherein each pair includes at least one multi-modal in-context example and one language-only in-context example. Among these pairs, there are   $2.2\\mathbf{M}$   unique instructions. Furthermore, to examine the characteristics and diversity of the instructions (refer to Fig.  4  (a)) and responses (refer to Fig.  4  (b)), we analyze the verb-noun structure present in them, refering to [ 45 ]. Specifically, we employ spaCy for parsing the instructions, extracting the verb closest "}
{"page": 7, "image_path": "doc_images/2306.05425v1_7.jpg", "ocr_text": "Situation Understanding and Reasoning\n\n(1st)Instruction: What could happen if drivers\nbecome confused by the traffic signals at this\nintersection?\n\nOtter: Confusion at traffic signals can\nnegatively impact road safety, traffic flow,\nand driver well-being.\n\n(2nd)Instruction: Please list a few potential\nrisks.\n\nOtter: It could lead to various potential risky\nscenarios: 1. Red light: The red light signal is\non, and the car is moving. In this case, the\ndriver might continue driving, potentially...\n\nLearning with In-context Examples\n\nInstruction: Instruction: Instruction:\nWhat? s What?s What’s written on this image?\nwritten on written on é (wo/ICE) Otter: “Think Different”\n\nthis image?\nOtter: pepsi,\nis pepsi okay?\n\nthis image?\nOtter: SUBWAY,\neat fresh.\n\nis written on this image.\n(w/ICE) Otter: Think Different\n\neat test Think different:\n\nEgocentric Visual Assistant\n\nHey Otter, | want to land here.\n\nHey Otter, what should | do?\nCan you teach me how to operate? ey Oe es\n\n~~\n\nOTTER-E\nPass the ball\n\nand run to the gate,\nlooking for a shooting opportunity.\n\nFigure 5: Otter’s response examples in different scenarios. Trained on the MIMIC-IT dataset,\nOtter is able to serve for situation understanding and reasoning, learning with in-context examples,\nand egocentric visual assistant.\n\nto the root, and retrieving its first direct noun object”. We plot the top 20 most frequently occurring\nroot verbs alongside their top 4 direct noun objects. Our findings reveal that the sentence structure\nof responses exhibits greater diversity compared to that of instructions. Moreover, we demonstrate\ndiversity in terms of the length of instructions/responses, the number of images per instruction, and\nthe number of in-context examples per instruction, as depicted in Fig. 4 (c).\n\n4 Enmpricial Evaluation\n\nIn this section, we showcase the diverse applications of the MIMIC-IT dataset and the potential\ncapabilities of a vision-language model (VLM) trained on it. Firstly, in Sec. 4.1, we introduce Otter,\nan in-context instruction-tuned model developed using the MIMIC-IT dataset. Next, in Sec. 4.2, we\nexplore various methods for training Otter on the MIMIC-IT dataset and discuss numerous scenarios\nin which Otter can be effectively employed. Finally, in Sec. 4.3 to Sec. 4.5, we present a comparative\nanalysis of Otter’s performance against other VLMs across an array of benchmarks.\n\n4.1 Otter: A Multi-Modal In-context Instruction Tuned Model\n\nOtter is designed to support multi-modal in-context instruction tuning based on the OpenFlamingo [6]\nmodel, which involves conditioning the language model on the corresponding media, such as an\nimage that corresponds to a caption or an instruction-response pair.\n\n4.2 Usage Examples and Demonstrations\n\nScene Understanding and Reasoning. The MIMIC-IT dataset comprises approximately 2.8 million\nin-context instruction-response pairs, which are structured into a cohesive template to facilitate various\ntasks. The following template encompasses images, user instructions, and model-generated responses,\nutilizing the Human and Assistant role labels to enable seamless user-assistant interactions.\n\n<image>Human: {instruction} Assistant :<answer>{response}<endofchunk>\n\n*https://github.com/exp]l\nmd-3.5.0\n\n/spacy-models/releases/tag/en_core_web_\n", "vlm_text": "The image is a visual demonstration of \"Otter's\" capabilities across three scenarios: Situation Understanding and Reasoning, Learning with In-context Examples, and Egocentric Visual Assistant.\n\n1. **Situation Understanding and Reasoning:**\n   - The first scenario shows a traffic intersection and discusses how confusion at traffic signals can impact road safety. Otter provides potential risks such as a car moving on a red light.\n   \n2. **Learning with In-context Examples:**\n   - This section shows branded logos (Pepsi, Subway, and Apple) with instructions to identify the text. Otter responds by correctly identifying the text associated with each logo.\n\n3. **Egocentric Visual Assistant:**\n   - The final section has two scenes. The first is a screenshot from a cockpit view, where Otter provides instructions on operating the controls to land. The second is a sports field where Otter advises on passing the ball during a soccer game.\n\nto the root, and retrieving its first direct noun object 2 . We plot the top 20 most frequently occurring root verbs alongside their top 4 direct noun objects. Our findings reveal that the sentence structure of responses exhibits greater diversity compared to that of instructions. Moreover, we demonstrate diversity in terms of the length of instructions/responses, the number of images per instruction, and the number of in-context examples per instruction, as depicted in Fig.  4  (c). \n4 Empricial Evaluation \nIn this section, we showcase the diverse applications of the MIMIC-IT dataset and the potential capabilities of a vision-language model (VLM) trained on it. Firstly, in Sec.  4.1 , we introduce Otter, an in-context instruction-tuned model developed using the MIMIC-IT dataset. Next, in Sec.  4.2 , we explore various methods for training Otter on the MIMIC-IT dataset and discuss numerous scenarios in which Otter can be effectively employed. Finally, in Sec.  4.3  to Sec.  4.5 , we present a comparative analysis of Otter’s performance against other VLMs across an array of benchmarks. \n4.1 Otter: A Multi-Modal In-context Instruction Tuned Model \nOtter is designed to support multi-modal in-context instruction tuning based on the Open Flamingo [ 6 ] model, which involves conditioning the language model on the corresponding media, such as an image that corresponds to a caption or an instruction-response pair. \n4.2 Usage Examples and Demonstrations \nScene Understanding and Reasoning.  The MIMIC-IT dataset comprises approximately 2.8 million in-context instruction-response pairs, which are structured into a cohesive template to facilitate various tasks. The following template encompasses images, user instructions, and model-generated responses, utilizing the  Human  and  Assistant  role labels to enable seamless user-assistant interactions. "}
{"page": 8, "image_path": "doc_images/2306.05425v1_8.jpg", "ocr_text": "Table 2: MMAGIBench evaluation results. Otter outperforms all baseline models by achieving the\nhighest average accuracy in both perception and reasoning tasks.\n\nModel Lang. Decoder | Avg. ercertion - , Rerouting\n\nCoarse Finegrained | Attribute Relation Future Pred.\nInstructBLIP [16] Vicuna-7B 50.4] 67.8 52.2 43.8 38.2 50.0\nMiniGPT-4 [54] Vicuna-7B 51.0] 63.3 47.8 50.6 26.5 66.7\nOpenFlamingo [6] ] LLaMA-7B | 51.1 | 344 40.0 61.3 52.9 66.7\nLLaVA [28] Vicuna-7B 62.7 | 44.4 54.2 1.9 76.5 66.7\nOtter LLaMA-7B_ | 65.5 | 68.9 47.3 66.3 61.8 83.3\n\nTraining the Otter model on the MIMIC-IT dataset allows it to acquire different capacities, as\ndemonstrated by the LA and SD tasks. Trained on the LA task, the model exhibits exceptional scene\ncomprehension, reasoning abilities, and multi-round conversation capabilities. Meanwhile, on the SD\ntask, the model can acquire the ability to adeptly spot general differences or subtle distinctions within\ndaily scenes.\n\nWe showcase response examples from the Otter after training on the MIMIC-IT dataset in Fig. 5,\nhighlighting its ability to understand situations and reasoning in a multi-round conversation style.\n\nLearning with In-context Examples. As mentioned in Sec. 3.1, regarding the concept of organizing\nvisual-language in-context examples, we demonstrate here the acquired ability of the Otter model to\nfollow inter-contextual instructions after training on the LA-T2T task (refer to Appx. for other tasks).\nThe organized input data format is as follows:\n\n# Multiple in-context example with similar instructions\n\n<image>Human: {instruction} Assistant:<answer>{response}<|endofchunk|>\n# wee.\n\n<image>Human: {instruction} Assistant:<answer>{response}<|endofchunk|>\n# Query example\n\n<image>Human: {instruction} Assistant:<answer>\n\nThe Otter model’s demonstration of regulating its expressions by referencing in-context examples is\nillustrated in Fig. 5.\n\nEgocentric Visual Assistant. A distinctive feature of the MIMIC-IT dataset is its inclusion of a\ncomprehensive collection of videos and sequential images in an egocentric view, derived from the\nIEP, E4D scenarios. In the IEP scenario, the content emphasizes understanding and planning within\nindoor environments, incorporating instructions and responses designed to guide the model in event\nplanning based on interior layouts.\n\nThe E4D scenario, on the other hand, tailors instructions and responses specifically for first-person\naugmented reality (AR) headset assistant applications. These two datasets collectively serve to bolster\nthe model’s proficiency in perceiving scenes from a first-person viewpoint, strategizing for impending\ntasks, and providing valuable insights and suggestions to AR headset users. Tailored this part of data,\nwe train an egocentric visual assistant, termed Otter-E, which is specifically designed for AR headset\napplications. MIMIC-IT bolsters the model’s proficiency in perceiving scenes from a first-person\nviewpoint, strategizing for impending tasks, and providing valuable insights and suggestions to\nAR headset users. As a result, the Otter-E model emerges as an exceptional and visionary Visual\nLanguage Model for AR headsets, paving the way for a groundbreaking and immersive experience.\n\nIn the bottom image of Fig. 5, Otter-E demonstrates its ability to perceive the first-person view and\nrespond to users’ questions, such as guiding users to land a small aircraft (In real-life scenarios, you\nare not encouraged to consult visual assistants for such hazardous actions).\n\n4.3 ChatGPT Evaluation\n\nIn Tab. 2, we utilize the MMAGIBench framework [43] to provide an extensive evaluation of the\nperception and reasoning capabilities of vision-language models. The perception benchmark consists\nof data derived from COCO images and social network images (e.g., , Twitter), covering tasks such\nas coarse scene and object recognition, fine-grained OCR, celebrity identification, and recognition\n", "vlm_text": "The table compares different models based on their performance in perception and reasoning tasks. Here's a summary:\n\n- **Models Compared**: \n  - InstructBLIP\n  - MiniGPT-4\n  - OpenFlamingo\n  - LLaVA\n  - Otter\n\n- **Language Decoders**: \n  - Vicuna-7B \n  - LLaMA-7B\n\n- **Metrics**: \n  - Average (Avg.)\n  - Perception (Coarse, Finegrained)\n  - Reasoning (Attribute, Relation, Future Prediction)\n\n- **Highlights**: \n  - Otter under LLaMA-7B achieved the highest average score of 65.5.\n  - In perception (coarse), Otter performed best with a score of 68.9.\n  - In reasoning (future prediction), Otter also excelled with a score of 83.3.\n\nThe bold values indicate the highest scores in each category.\nTraining the Otter model on the MIMIC-IT dataset allows it to acquire different capacities, as demonstrated by the LA and SD tasks. Trained on the LA task, the model exhibits exceptional scene comprehension, reasoning abilities, and multi-round conversation capabilities. Meanwhile, on the SD task, the model can acquire the ability to adeptly spot general differences or subtle distinctions within daily scenes. \nWe showcase response examples from the Otter after training on the MIMIC-IT dataset in Fig.  5 , highlighting its ability to understand situations and reasoning in a multi-round conversation style. \nLearning with In-context Examples.  As mentioned in Sec.  3.1 , regarding the concept of organizing visual-language in-context examples, we demonstrate here the acquired ability of the Otter model to follow inter-contextual instructions after training on the LA-T2T task (refer to Appx. for other tasks). The organized input data format is as follows:\n\n \n# Multiple in-context example with similar instructions\n\n <image>Human:{instruction} Assistant:<answer>{response}<|endofchunk|>\n\n # ....\n\n <image>Human:{instruction} Assistant:<answer>{response}<|endofchunk  $|>\n\n$  # Query example\n\n <image>Human:{instruction} Assistant:<answer> \nThe Otter model’s demonstration of regulating its expressions by referencing in-context examples is illustrated in Fig.  5 . \nEgocentric Visual Assistant.  A distinctive feature of the MIMIC-IT dataset is its inclusion of a comprehensive collection of videos and sequential images in an egocentric view, derived from the IEP, E4D scenarios. In the IEP scenario, the content emphasizes understanding and planning within indoor environments, incorporating instructions and responses designed to guide the model in event planning based on interior layouts. \nThe E4D scenario, on the other hand, tailors instructions and responses specifically for first-person augmented reality (AR) headset assistant applications. These two datasets collectively serve to bolster the model’s proficiency in perceiving scenes from a first-person viewpoint, st rate gi zing for impending tasks, and providing valuable insights and suggestions to AR headset users. Tailored this part of data, we train an egocentric visual assistant, termed  Otter-E , which is specifically designed for AR headset applications. MIMIC-IT bolsters the model’s proficiency in perceiving scenes from a first-person viewpoint, st rate gi zing for impending tasks, and providing valuable insights and suggestions to AR headset users. As a result, the Otter-E model emerges as an exceptional and visionary Visual Language Model for AR headsets, paving the way for a groundbreaking and immersive experience. \nIn the bottom image of Fig.  5 , Otter-E demonstrates its ability to perceive the first-person view and respond to users’ questions, such as guiding users to land a small aircraft (In real-life scenarios, you are not encouraged to consult visual assistants for such hazardous actions). \n4.3 ChatGPT Evaluation \nIn Tab.  2 , we utilize the MMAGIBench framework [ 43 ] to provide an extensive evaluation of the perception and reasoning capabilities of vision-language models. The perception benchmark consists of data derived from COCO images and social network images ( e.g.,  , Twitter), covering tasks such as coarse scene and object recognition, fine-grained OCR, celebrity identification, and recognition "}
{"page": 9, "image_path": "doc_images/2306.05425v1_9.jpg", "ocr_text": "GB VideoChatGPT Otter\nMSVD 0-shot MSRVTT 0-shot\n39.5 1015\n\n90\n85\n80\n75\n\ng\n8\n3\n\nin\n\n1005\n\n3\na a\n2 5 70\na\n38.4) 995 65\n: 60 -o otter\n990 60.8 —@- OpenFlamingo\nQA Captioning QA Captioning MM-GPT Inst. BLIP LLaVA MiniGPT Otter 86 O-shot 4-shot 8-shot 16-shot\n(a) Video understanding (b) Vision-language model alignment (c) COCO caption\n\nFigure 6: (a) ChatGPT evaluation for video understanding. Otter outperforms baseline models by\nsubstantial margins in video understanding. (b) Human evaluation comparison. Otter demonstrates\nsuperior usefulness and alignment. (c) Few-shot in-context learning evaluation. Otter outperforms\nOpenFlamingo as a better in-context and zero-shot learner.\n\nof well-known locations. The reasoning benchmark, on the other hand, is performed across three\ndimensions: attribute reasoning, relation reasoning, and future prediction.\n\nCurrent evaluation metrics for vision-language models, like VQAv2 [4], exhibit shortcomings in\nterms of robustness. For instance, VQAv2 primarily assesses single-word or phrase responses, while\nmany modern models generate sentence outputs. To bridge this gap, we evaluate the models by\nasking ChatGPT to compare their label predictions with the ground truth labels for each input. A\ntest sample is considered correct if ChatGPT’s response indicates that the prediction aligns with the\ncorresponding label. For a more in-depth understanding of MMAGIBench, we recommend referring\nto the original source [43]. Fig. 6 (a) demonstrates that Otter outperforms VideoChatGPT [26]\nby 6.8% accuracy and 1.8% on MSVD [9] 0-shot question answering and captioning benchmarks\nrespectively. Similar substantial margins are also observed on the MSRVTT [49] dataset.\n\n4.4 Human Evaluation\n\nMulti-Modality Arena [32] uses an Elo rating system to evaluate the usefulness and alignment of\nVLM responses. The Elo rating system calculates the relative skill levels of players, as commonly\nused in chess and other competitive games. The difference in Elo ratings between the two models\npredicts the outcome if they were matched against each other. This system works well for evaluating\nconversational AI models, because multiple models can have pairwise \"battles\" responding to the\nsame inputs in a user-blind evaluation. Fig. 6(b) shows that Otter demonstrates superior usefulness\nand alignment, achieving the highest Elo rating among recent VLMs.\n\n4.5 Few-shot In-context Learning Metric Evaluation\n\nOtter is finetuned based on OpenFlamingo, an architecture designed for multi-modal in-context\nlearning. Finetuned with the MIMIC-IT dataset, Otter outperforms OpenFlamingo by a substantial\nmargin on COCO caption (CIDEr) [27] few-shot evaluation (see Fig. 6(c)). As expected, the\nfinetuning also brings marginal performance gain on zero-shot evaluation.\n\n5 Discussion\n\nLimitations. Though we have iteratively refined the system message and instruction-response\nexamples, ChatGPT is prone to language hallucinations therefore it might generate incorrect responses.\nGenerally, more trustworthy language models are desired for self-instruct data generation.\n\nFuture Works. In the future, we plan to support more embodied AI datasets such as Language-\nTable [29] and SayCan [1]. We also consider improving the instruction collection with more trustwor-\nthy language models or generation techniques.\n\nConclusion. In this work, we propose MIMIC-IT, a large-scale multi-modal in-context instruction\ntuning dataset. We leverage an automatic pipeline, Syphus, to enable this dataset to cover a diverse\nset of visual scenes and creative instructions in eight languages. MIMIC-IT empowers our model,\nOtter, to achieve state-of-the-art performances in perception and reasoning benchmarks as well as\nhuman evaluations.\n\n10\n", "vlm_text": "The image contains three bar charts:\n\n1. **(a) Video Understanding**: This chart displays the performance comparison of VideoChatGPT and Otter in video understanding tasks. Otter outperforms VideoChatGPT in both Question Answering (QA) and Captioning in MSVD and MSRVTT datasets.\n\n2. **(b) Vision-language Model Alignment**: This chart compares the alignment of various models, with Otter showing the highest Elo rating, surpassing MM-GPT, Inst. BLIP, LLaVA, and MiniGPT.\n\n3. **(c) COCO Caption**: This line chart compares Otter and Open Flamingo in few-shot in-context learning for COCO captions. Otter consistently outperforms Open Flamingo across various shots (0-shot, 4-shot, 8-shot, and 16-shot).\nof well-known locations. The reasoning benchmark, on the other hand, is performed across three dimensions: attribute reasoning, relation reasoning, and future prediction. \nCurrent evaluation metrics for vision-language models, like VQAv2 [ 4 ], exhibit shortcomings in terms of robustness. For instance, VQAv2 primarily assesses single-word or phrase responses, while many modern models generate sentence outputs. To bridge this gap, we evaluate the models by asking ChatGPT to compare their label predictions with the ground truth labels for each input. A test sample is considered correct if ChatGPT’s response indicates that the prediction aligns with the corresponding label. For a more in-depth understanding of MMAGIBench, we recommend referring to the original source [ 43 ]. Fig.  6  (a) demonstrates that Otter outperforms Video Chat GP T [ 26 ] by  $6.8\\%$   accuracy and   $1.8\\%$   on MSVD [ 9 ] 0-shot question answering and captioning benchmarks respectively. Similar substantial margins are also observed on the MSRVTT [ 49 ] dataset. \n4.4 Human Evaluation \nMulti-Modality Arena [ 32 ] uses an Elo rating system to evaluate the usefulness and alignment of VLM responses. The Elo rating system calculates the relative skill levels of players, as commonly used in chess and other competitive games. The difference in Elo ratings between the two models predicts the outcome if they were matched against each other. This system works well for evaluating conversational AI models, because multiple models can have pairwise \"battles\" responding to the same inputs in a user-blind evaluation. Fig.  6 (b) shows that Otter demonstrates superior usefulness and alignment, achieving the highest Elo rating among recent VLMs. \n4.5 Few-shot In-context Learning Metric Evaluation \nOtter is finetuned based on Open Flamingo, an architecture designed for multi-modal in-context learning. Finetuned with the MIMIC-IT dataset, Otter outperforms Open Flamingo by a substantial margin on COCO caption (CIDEr) [ 27 ] few-shot evaluation (see Fig.  6 (c)). As expected, the finetuning also brings marginal performance gain on zero-shot evaluation. \n5 Discussion \nLimitations.  Though we have iterative ly refined the system message and instruction-response examples, ChatGPT is prone to language hallucinations therefore it might generate incorrect responses. Generally, more trustworthy language models are desired for self-instruct data generation. \nFuture Works.  In the future, we plan to support more embodied AI datasets such as Language- Table [ 29 ] and SayCan [ 1 ]. We also consider improving the instruction collection with more trustwor- thy language models or generation techniques. \nConclusion.  In this work, we propose MIMIC-IT, a large-scale multi-modal in-context instruction tuning dataset. We leverage an automatic pipeline, Syphus, to enable this dataset to cover a diverse set of visual scenes and creative instructions in eight languages. MIMIC-IT empowers our model, Otter, to achieve state-of-the-art performances in perception and reasoning benchmarks as well as human evaluations. "}
{"page": 10, "image_path": "doc_images/2306.05425v1_10.jpg", "ocr_text": "Acknowledgments and Disclosure of Funding\n\nThis study is supported by the Ministry of Education, Singapore, under its MOE AcRF Tier 2\n(MOE-T2EP20221- 0012), NTU NAP, and under the RIE2020 Industry Alignment Fund — Industry\nCollaboration Projects (IAF-ICP) Funding Initiative, as well as cash and in-kind contribution from the\nindustry partner(s). We thank Peiyu Fu, Xuli Chen, and Mehdi Cherti for their professional advice on\nthe in-context example of the translation query of Japanese, French, German, Spanish, Korean, and\nArabic.\n\nReferences\n\n{1\n\nMichael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David,\nChelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, et al. Do as i can, not\nas i say: Grounding language in robotic affordances. arXiv preprint arXiv:2204.01691, 2022.\n10\n\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson,\nKarel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual\nlanguage model for few-shot learning. Advances in Neural Information Processing Systems,\n35:23716-23736, 2022. 3\n\nAlibaba. Tongyi qianwen. 2023. 3\n\nStanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence\nZitnick, and Devi Parikh. Vqa: Visual question answering. In Proceedings of the IEEE\ninternational conference on computer vision, pages 2425-2433, 2015. 10\n\nAkari Asai, Timo Schick, Patrick Lewis, Xilun Chen, Gautier Izacard, Sebastian Riedel,\nHannaneh Hajishirzi, and Wen-tau Yih. Task-aware retrieval with instructions. arXiv preprint\narXiv:2211.09260, 2022. |\n\nAnas Awadalla, Irena Gao, Joshua Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani\nMarathe, Yonatan Bitton, Samir Gadre, Jenia Jitsev, Simon Kornblith, Pang Wei Koh, Gabriel\nIlharco, Mitchell Wortsman, and Ludwig Schmidt. Openflamingo, March 2023. 3, 8, 9\n\nBaidu. Ernie bot: Enhanced representation through knowledge integration. 2023. 3\n\nSoravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing\nweb-scale image-text pre-training to recognize long-tail visual concepts. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3558-3568, 2021. 3\n\nDavid Chen and William Dolan. Collecting highly parallel data for paraphrase evaluation.\nIn Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:\nHuman Language Technologies, pages 190-200, Portland, Oregon, USA, June 2011. Association\nfor Computational Linguistics. 10\n\nLiangyu Chen, Yutong Bai, Siyu Huang, Yongyi Lu, Bihan Wen, Alan L Yuille, and Zongwei\nZhou. Making your first choice: To address cold start problem in vision active learning. arXiv\npreprint arXiv:2210.02442, 2022. 5\n\nLiangyu Chen, Bo Li, Sheng Shen, Jingkang Yang, Chunyuan Li, Kurt Keutzer, Trevor Darrell,\nand Ziwei Liu. Language models are visual reasoning coordinators. In JCLR 2023 Workshop on\nMathematical and Empirical Understanding of Foundation Models, 2023. 3\n\nXinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollar,\nand C Lawrence Zitnick. Microsoft coco captions: Data collection and evaluation server. arXiv\npreprint arXiv: 1504.00325, 2015. 3\n\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng,\nSiyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna:\nAn open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. 1, 3\n\n11\n", "vlm_text": "Acknowledgments and Disclosure of Funding \nThis study is supported by the Ministry of Education, Singapore, under its MOE AcRF Tier 2 (MOE-T2EP20221- 0012), NTU NAP, and under the RIE2020 Industry Alignment Fund – Industry Collaboration Projects (IAF-ICP) Funding Initiative, as well as cash and in-kind contribution from the industry partner(s). We thank Peiyu Fu, Xuli Chen, and Mehdi Cherti for their professional advice on the in-context example of the translation query of Japanese, French, German, Spanish, Korean, and Arabic.\n\n \nReferences \n[1]  Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Keerthana Gopal a krishna n, Karol Hausman, Alex Herzog, et al. Do as i can, not as i say: Grounding language in robotic afford ances.  arXiv preprint arXiv:2204.01691 , 2022. 10\n\n \n[2]  Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning.  Advances in Neural Information Processing Systems , 35:23716–23736, 2022.  3\n\n \n[3] Alibaba. Tongyi qianwen. 2023.  3\n\n \n[4]  Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh. Vqa: Visual question answering. In  Proceedings of the IEEE international conference on computer vision , pages 2425–2433, 2015.  10\n\n \n[5]  Akari Asai, Timo Schick, Patrick Lewis, Xilun Chen, Gautier Izacard, Sebastian Riedel, Hannaneh Hajishirzi, and Wen-tau Yih. Task-aware retrieval with instructions.  arXiv preprint arXiv:2211.09260 , 2022.  1\n\n \n[6]  Anas Awadalla, Irena Gao, Joshua Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Jenia Jitsev, Simon Kornblith, Pang Wei Koh, Gabriel Ilharco, Mitchell Wortsman, and Ludwig Schmidt. Open flamingo, March 2023.  3 ,  8 ,  9\n\n \n[7] Baidu. Ernie bot: Enhanced representation through knowledge integration. 2023.  3\n\n \n[8]  Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In  Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 3558–3568, 2021.  3\n\n \n[9]  David Chen and William Dolan. Collecting highly parallel data for paraphrase evaluation. In  Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies , pages 190–200, Portland, Oregon, USA, June 2011. Association for Computational Linguistics.  10\n\n \n[10]  Liangyu Chen, Yutong Bai, Siyu Huang, Yongyi Lu, Bihan Wen, Alan L Yuille, and Zongwei Zhou. Making your first choice: To address cold start problem in vision active learning.  arXiv preprint arXiv:2210.02442 , 2022.  5\n\n \n[11]  Liangyu Chen, Bo Li, Sheng Shen, Jingkang Yang, Chunyuan Li, Kurt Keutzer, Trevor Darrell, and Ziwei Liu. Language models are visual reasoning coordinators. In  ICLR 2023 Workshop on Mathematical and Empirical Understanding of Foundation Models , 2023.  3\n\n \n[12]  Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollár, and C Lawrence Zitnick. Microsoft coco captions: Data collection and evaluation server.  arXiv preprint arXiv:1504.00325 , 2015.  3\n\n \n[13]  Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with  $90\\%^{*}$   chatgpt quality, March 2023.  1 ,  3 "}
{"page": 11, "image_path": "doc_images/2306.05425v1_11.jpg", "ocr_text": "[20\n\n[21\n\n[22\n\n[23\n\n[24\n\n[25\n\n[26\n\n[27\n\n[28\n\nHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li,\nXuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned\nlanguage models. arXiv preprint arXiv:2210.11416, 2022. |\n\nAngela Dai, Angel X Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias\nNie&ner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In Proceedings of the\nIEEE conference on computer vision and pattern recognition, pages 5828-5839, 2017. 5,7, 15\n\nWenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng\nWang, Boyang Li, Pascale Fung, and Steven C. H. Hoi. Instructblip: Towards general-purpose\nvision-language models with instruction tuning. CoRR, abs/2305.06500, 2023. 9\n\nDanny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter,\nAyzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: An embodied\nmultimodal language model. arXiv preprint arXiv:2303.03378, 2023. |\n\nYash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making\nthe v in vqa matter: Elevating the role of image understanding in visual question answering.\nIn Proceedings of the IEEE conference on computer vision and pattern recognition, pages\n\n6904-6913, 2017. 3\n\nKristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit\nGirdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d: Around the world\nin 3,000 hours of egocentric video. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 18995-19012, 2022. 5,7, 15\n\nTing-Hao K. Huang, Francis Ferraro, Nasrin Mostafazadeh, Ishan Misra, Jacob Devlin, Aish-\nwarya Agrawal, Ross Girshick, Xiaodong He, Pushmeet Kohli, Dhruv Batra, et al. Visual\nstorytelling. In /5th Annual Conference of the North American Chapter of the Association for\nComputational Linguistics (NAACL 2016), 2016. 5, 6, 15\n\nHarsh Jhamtani and Taylor Berg-Kirkpatrick. Learning to describe differences between pairs\nof similar images. In EMNLP, pages 4024-4034. Association for Computational Linguistics,\n2018. 5, 6, 15\n\nRanjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and Juan Carlos Niebles. Dense-\ncaptioning events in videos. In Proceedings of the IEEE international conference on computer\nvision, pages 706-715, 2017. 5, 6, 15\n\nRanjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie\nChen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting\nlanguage and vision using crowdsourced dense image annotations. International journal of\ncomputer vision, 123:32-73, 2017. 3\n\nJie Lei, Licheng Yu, Tamara L Berg, and Mohit Bansal. Tvr: A large-scale dataset for\nvideo-subtitle moment retrieval. In Computer Vision—ECCV 2020: 16th European Conference,\nGlasgow, UK, August 23-28, 2020, Proceedings, Part XXI 16, pages 447-463. Springer, 2020.\n5, 6, 15\n\nJunnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-\nimage pre-training with frozen image encoders and large language models. arXiv preprint\narXiv:2301.12597, 2023. 3\n\nKunchang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang,\nand Yu Qiao. Videochat: Chat-centric video understanding. arXiv preprint arXiv:2305.06355,\n2023. 10\n\nTsung- Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr\nDollar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer\nVision—ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014,\nProceedings, Part V 13, pages 740-755. Springer, 2014. 2, 3, 5, 6, 10, 15\n\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv\npreprint arXiv:2304.08485, 2023. 2, 3,5, 6,9\n\n12\n", "vlm_text": "[14]  Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models.  arXiv preprint arXiv:2210.11416 , 2022.  1\n\n \n[15]  Angela Dai, Angel X Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nießner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In  Proceedings of the IEEE conference on computer vision and pattern recognition , pages 5828–5839, 2017.  5 ,  7 ,  15\n\n \n[16]  Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven C. H. Hoi. Instruct blip: Towards general-purpose vision-language models with instruction tuning.  CoRR , abs/2305.06500, 2023.  9\n\n \n[17] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: An embodied multimodal language model.  arXiv preprint arXiv:2303.03378 , 2023.  1\n\n \n[18]  Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the v in vqa matter: Elevating the role of image understanding in visual question answering. In  Proceedings of the IEEE conference on computer vision and pattern recognition , pages 6904–6913, 2017.  3\n\n \n[19]  Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d: Around the world in 3,000 hours of egocentric video. In  Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 18995–19012, 2022.  5 ,  7 ,  15\n\n \n[20]  Ting-Hao K. Huang, Francis Ferraro, Nasrin Most af azad eh, Ishan Misra, Jacob Devlin, Aish- warya Agrawal, Ross Girshick, Xiaodong He, Pushmeet Kohli, Dhruv Batra, et al. Visual storytelling. In  15th Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL 2016) , 2016.  5 ,  6 ,  15\n\n \n[21]  Harsh Jhamtani and Taylor Berg-Kirkpatrick. Learning to describe differences between pairs of similar images. In  EMNLP , pages 4024–4034. Association for Computational Linguistics, 2018.  5 ,  6 ,  15\n\n \n[22]  Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and Juan Carlos Niebles. Dense- captioning events in videos. In  Proceedings of the IEEE international conference on computer vision , pages 706–715, 2017.  5 ,  6 ,  15\n\n \n[23]  Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language and vision using crowd sourced dense image annotations.  International journal of computer vision , 123:32–73, 2017.  3\n\n \n[24]  Jie Lei, Licheng Yu, Tamara L Berg, and Mohit Bansal. Tvr: A large-scale dataset for video-subtitle moment retrieval. In  Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XXI 16 , pages 447–463. Springer, 2020. 5 ,  6 ,  15\n\n \n[25]  Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Boots trapping language- image pre-training with frozen image encoders and large language models.  arXiv preprint arXiv:2301.12597 , 2023.  3\n\n \n[26]  Kunchang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. Videochat: Chat-centric video understanding.  arXiv preprint arXiv:2305.06355 , 2023.  10\n\n \n[27]  Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In  Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13 , pages 740–755. Springer, 2014.  2 ,  3 ,  5 ,  6 ,  10 ,  15\n\n \n[28]  Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning.  arXiv preprint arXiv:2304.08485 , 2023.  2 ,  3 ,  5 ,  6 ,  9 "}
{"page": 12, "image_path": "doc_images/2306.05425v1_12.jpg", "ocr_text": "[29\n\n[30\n[31\n[32\n\n[33\n\n[34\n\n[35\n\n[36\n\n[37\n[38\n\n[39\n\n[40\n\n[41\n\n[42\n\n[43\n\n[44\n\n[45\n\n[46\n\nCorey Lynch, Ayzaan Wahid, Jonathan Tompson, Tianli Ding, James Betker, Robert Baruch,\nTravis Armstrong, and Pete Florence. Interactive language: Talking to robots in real time. arXiv\npreprint arXiv:2210.06407, 2022. 10\n\nOpenAl. Gpt-4 technical report. 2023. 1, 2, 3\nOpenAL. Introducing chatgpt. 2023. 1, 3\n\nOpenGVLab. Multi-modality arena. https://github.com/OpenGVLab/\nMulti-Modality-Arena, 2023. 3, 10\n\nVicente Ordonez, Girish Kulkarni, and Tamara Berg. Im2text: Describing images using 1\nmillion captioned photographs. Advances in neural information processing systems, 24, 2011. 3\n\nBaolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning\nwith GPT-4. arXiv preprint arXiv:2304.03277, 2023. |\n\nAndy Rosenbaum, Saleh Soltan, Wael Hamza, Yannick Versley, and Markus Boese. Linguist:\nLanguage model instruction tuning to generate annotated utterances for intent classification and\nslot tagging. arXiv preprint arXiv:2209.09900, 2022. |\n\nChristoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton\nMullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open\ndataset of clip-filtered 400 million image-text pairs. arXiv preprint arXiv:2111.02114, 2021. 3\n\nSenseTime. Sense nova. 2023. 3\n\nPiyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A\ncleaned, hypernymed, image alt-text dataset for automatic image captioning. In Proceedings of\nthe 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long\nPapers), pages 2556-2565, 2018. 3\n\nYongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang.\nHugginggpt: Solving ai tasks with chatgpt and its friends in huggingface. arXiv preprint\narXiv:2303.17580, 2023. 3\n\nCharlie Snell, Dan Klein, and Ruiqi Zhong. Learning by distilling context. arXiv preprint\narXiv:2209.15189, 2022. |\n\nAlane Suhr, Mike Lewis, James Yeh, and Yoav Artzi. A corpus of natural language for visual\nreasoning. In Proceedings of the 55th Annual Meeting of the Association for Computational\nLinguistics (Volume 2: Short Papers), pages 217-223, 2017. 3\n\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy\nLiang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model.\nhttps://github.com/tatsu-lab/stanford_alpaca, 2023. 1,3\n\nMMAGIBench Team. Mmagibench: A universal multi-modal benchmark towards artificial\ngeneral intelligence. https: //github.com/open-mmlab/mmagibench, 2023. 3, 9,\n10\n\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-\nthée Lacroix, Baptiste Roziére, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez,\nArmand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation\nlanguage models. arXiv preprint arXiv:2302.13971, 2023. 3\n\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi,\nand Hannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instruc-\ntions. arXiv preprint arXiv:2212.10560, 2022. |, 2,7\n\nYizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei,\nAtharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, et al.\nSuper-naturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks. In\nProceedings of the 2022 Conference on Empirical Methods in Natural Language Processing,\npages 5085-5109, 2022. |\n\n13\n", "vlm_text": "[29]  Corey Lynch, Ayzaan Wahid, Jonathan Tompson, Tianli Ding, James Betker, Robert Baruch, Travis Armstrong, and Pete Florence. Interactive language: Talking to robots in real time.  arXiv preprint arXiv:2210.06407 , 2022.  10\n\n \n[30] OpenAI. Gpt-4 technical report. 2023.  1 ,  2 ,  3\n\n \n[31] OpenAI. Introducing chatgpt. 2023.  1 ,  3\n\n \n[32]  OpenGVLab. Multi-modality arena. https://github.com/OpenGVLab/ Multi-Modality-Arena , 2023.  3 ,  10\n\n \n[33]  Vicente Ordonez, Girish Kulkarni, and Tamara Berg. Im2text: Describing images using 1 million captioned photographs.  Advances in neural information processing systems , 24, 2011.  3\n\n \n[34]  Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning with GPT-4.  arXiv preprint arXiv:2304.03277 , 2023.  1\n\n \n[35]  Andy Rosenbaum, Saleh Soltan, Wael Hamza, Yannick Versley, and Markus Boese. Linguist: Language model instruction tuning to generate annotated utterances for intent classification and slot tagging.  arXiv preprint arXiv:2209.09900 , 2022.  1\n\n \n[36]  Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kac z marc zyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsu zak i. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs.  arXiv preprint arXiv:2111.02114 , 2021.  3\n\n \n[37] SenseTime. Sense nova. 2023.  3\n\n \n[38]  Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In  Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 2556–2565, 2018.  3\n\n \n[39]  Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face.  arXiv preprint arXiv:2303.17580 , 2023.  3\n\n \n[40]  Charlie Snell, Dan Klein, and Ruiqi Zhong. Learning by distilling context.  arXiv preprint arXiv:2209.15189 , 2022.  1\n\n \n[41]  Alane Suhr, Mike Lewis, James Yeh, and Yoav Artzi. A corpus of natural language for visual reasoning. In  Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers) , pages 217–223, 2017.  3\n\n \n[42]  Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford alpaca , 2023.  1 ,  3\n\n \n[43]  MMAGIBench Team. Mmagibench: A universal multi-modal benchmark towards artificial general intelligence.  https://github.com/open-mmlab/mmagibench , 2023.  3 ,  9 , 10\n\n \n[44] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo- thée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models.  arXiv preprint arXiv:2302.13971 , 2023.  3\n\n \n[45]  Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instruc- tions.  arXiv preprint arXiv:2212.10560 , 2022.  1 ,  2 ,  7\n\n \n[46]  Yizhong Wang, Swaroop Mishra, Pegah Ali poor mola bash i, Yeganeh Kordi, Amirreza Mirzaei, Atharva Naik, Arjun Ashok, Arut Selvan Dhana sekar an, Anjana Arunkumar, David Stap, et al. Super-natural instructions: Generalization via declarative instructions on   $1600+$   nlp tasks. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing , pages 5085–5109, 2022.  1 "}
{"page": 13, "image_path": "doc_images/2306.05425v1_13.jpg", "ocr_text": "[47\n\n[48\n\n[49\n\n[50\n\n[51\n\n[52\n\n[53\n\n[54\n\n[55\n\n[56\n\n[57\n\nJason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan\nDu, Andrew M. Dai, and Quoc V. Le. Finetuned language models are zero-shot learners. In\nICLR. OpenReview.net, 2022. |\n\nChenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan.\nVisual chatgpt: Talking, drawing and editing with visual foundation models. arXiv preprint\narXiv:2303.04671, 2023. 3\n\nJun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large video description dataset for\nbridging video and language. In 20/6 IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 5288-5296, 2016. 10\n\nZhiyang Xu, Ying Shen, and Lifu Huang. Multiinstruct: Improving multi-modal zero-shot\nlearning via instruction tuning. arXiv preprint arXiv:2212.10773, 2022. 3\n\nRui Yang, Lin Song, Yanwei Li, Sijie Zhao, Yixiao Ge, Xiu Li, and Ying Shan. Gpt4tools:\nTeaching large language model to use tools via self-instruction. 2023. |\n\nZhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed,\nZicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. Mm-react: Prompting chatgpt for\nmultimodal reasoning and action. arXiv preprint arXiv:2303.11381, 2023. 3\n\nRenrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li,\nPeng Gao, and Yu Qiao. Llama-adapter: Efficient fine-tuning of language models with zero-init\nattention. arXiv preprint arXiv:2303.16199, 2023. 3\n\nDeyao Zhu, Jun Chen, Xiaogian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: En-\nhancing vision-language understanding with advanced large language models. arXiv preprint\narXiv:2304.10592, 2023. 3, 5,9\n\nWanrong Zhu, Jack Hessel, Anas Awadalla, Samir Yitzhak Gadre, Jesse Dodge, Alex Fang,\nYoungjae Yu, Ludwig Schmidt, William Yang Wang, and Yejin Choi. Multimodal C4: An open,\nbillion-scale corpus of images interleaved with text. arXiv preprint arXiv:2304.06939, 2023. 3\n\nYuke Zhu, Oliver Groth, Michael Bernstein, and Li Fei-Fei. Visual7w: Grounded question\nanswering in images. In Proceedings of the IEEE conference on computer vision and pattern\nrecognition, pages 4995-5004, 2016. 3\n\nXueyan Zou, Zi-Yi Dou, Jianwei Yang, Zhe Gan, Linjie Li, Chunyuan Li, Xiyang Dai, Harkirat\n\nBehl, Jianfeng Wang, Lu Yuan, et al. Generalized decoding for pixel, image, and language.\narXiv preprint arXiv:2212.11270, 2022. 3\n\n14\n", "vlm_text": "[47]  Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. Finetuned language models are zero-shot learners. In ICLR . OpenReview.net, 2022.  1\n\n [48]  Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. Visual chatgpt: Talking, drawing and editing with visual foundation models.  arXiv preprint arXiv:2303.04671 , 2023.  3\n\n [49]  Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large video description dataset for bridging video and language. In  2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , pages 5288–5296, 2016.  10\n\n [50]  Zhiyang Xu, Ying Shen, and Lifu Huang. Multi instruct: Improving multi-modal zero-shot learning via instruction tuning.  arXiv preprint arXiv:2212.10773 , 2022.  3\n\n [51]  Rui Yang, Lin Song, Yanwei Li, Sijie Zhao, Yixiao Ge, Xiu Li, and Ying Shan. Gpt4tools: Teaching large language model to use tools via self-instruction. 2023.  1\n\n [52]  Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. Mm-react: Prompting chatgpt for multimodal reasoning and action.  arXiv preprint arXiv:2303.11381 , 2023.  3\n\n [53]  Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao, and Yu Qiao. Llama-adapter: Efficient fine-tuning of language models with zero-init attention.  arXiv preprint arXiv:2303.16199 , 2023.  3\n\n [54]  Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: En- hancing vision-language understanding with advanced large language models.  arXiv preprint arXiv:2304.10592 , 2023.  3 ,  5 ,  9\n\n [55]  Wanrong Zhu, Jack Hessel, Anas Awadalla, Samir Yitzhak Gadre, Jesse Dodge, Alex Fang, Youngjae Yu, Ludwig Schmidt, William Yang Wang, and Yejin Choi. Multimodal C4: An open, billion-scale corpus of images interleaved with text.  arXiv preprint arXiv:2304.06939 , 2023.  3\n\n [56]  Yuke Zhu, Oliver Groth, Michael Bernstein, and Li Fei-Fei. Visual7w: Grounded question answering in images. In  Proceedings of the IEEE conference on computer vision and pattern recognition , pages 4995–5004, 2016.  3\n\n [57]  Xueyan Zou, Zi-Yi Dou, Jianwei Yang, Zhe Gan, Linjie Li, Chunyuan Li, Xiyang Dai, Harkirat Behl, Jianfeng Wang, Lu Yuan, et al. Generalized decoding for pixel, image, and language. arXiv preprint arXiv:2212.11270 , 2022.  3 "}
{"page": 14, "image_path": "doc_images/2306.05425v1_14.jpg", "ocr_text": "A Total Cost and ChatGPT Version\n\nWe construct MIMIC-IT using the ChatGPT-0301 version. Overall, we query 1,006,746,240 tokens\n(859,677,150 and 147,069,090 for input and output tokens respectively). The estimated total cost is\n$20134.9248.°\n\nB_ Content Copyright and License\n\nThe license of the datasets we used in this work is illustrated below.\n\nVisual Data Image License Instruction-response license\nMS-COCO [27] Custom CC BY-NC-SA\nSpot-the-diff [21] Unknown CC BY-NC-SA\nScanNetv2 [15] non-commercial CC BY-NC-SA\nActivityNet Captions [22] Unknown CC BY-NC-SA\nVisual Storytelling [20] Unknown CC BY-NC-SA\nTV Captions [24] Unknown CC BY-NC-SA\nEgo4D [19] non-exclusive, non-transferable CC BY-NC-SA\n\nC_ Sythus: Automatic Instruction Generation Pipeline\n\nSafety and Ethical Filtering Since we use GPT to generate instructions and responses, we\ngenerally follow the GPT content policy for safe and ethical use. This policy eliminates output\nthat is suspicious for unfair opportunities, stereotyping, overrepresentation/underrepresentation,\nexplicit content, disinformation, or unreliable information.\n\nMulti-lingual Support We enrich the datasets by translating the English instruction-response pairs\nby GPT into 7 additional languages: Chinese, Japanese, Spanish, German, French, Korean, and\nArabic. See the prompt for multi-lingual translation query in Fig. 7.\n\nD_ Annotation Prompt\n\nIn this section, we will present prompts for querying ChatGPT of all datasets in detail. Each prompt\ncontains system message, in-context emample.\n\nai.com/pricir\n\n15\n", "vlm_text": "A Total Cost and ChatGPT Version \nWe construct MIMIC-IT using the ChatGPT-0301 version. Overall, we query 1,006,746,240 tokens\n\n (859,677,150 and 147,069,090 for input and output tokens respectively). The estimated total cost is\n\n  $\\S20134.9248.$  . \nB Content Copyright and License \nThe table lists different datasets in the context of visual data along with their associated image licenses and instruction-response licenses. \n\n- **Visual Data**: \n  - MS-COCO\n  - Spot-the-diff\n  - ScanNetv2\n  - ActivityNet Captions\n  - Visual Storytelling\n  - TV Captions\n  - Ego4D\n\n- **Image License**:\n  - MS-COCO: Custom\n  - Spot-the-diff: Unknown\n  - ScanNetv2: non-commercial\n  - ActivityNet Captions: Unknown\n  - Visual Storytelling: Unknown\n  - TV Captions: Unknown\n  - Ego4D: non-exclusive, non-transferable\n\n- **Instruction-response License**:\n  - All datasets have the same instruction-response license: CC BY-NC-SA (Creative Commons Attribution-NonCommercial-ShareAlike). \n\nNote: The numbers in brackets next to the datasets likely refer to references or footnotes not visible in the image.\nC Sythus: Automatic Instruction Generation Pipeline \nSafety and Ethical Filtering Since we use GPT to generate instructions and responses, we generally follow the GPT content policy for safe and ethical use. This policy eliminates output that is suspicious for unfair opportunities, stereotyping, over representation/under representation, explicit content, disinformation, or unreliable information. \nMulti-lingual Support We enrich the datasets by translating the English instruction-response pairs by GPT into 7 additional languages: Chinese, Japanese, Spanish, German, French, Korean, and Arabic. See the prompt for multi-lingual translation query in Fig.  7 . \nD Annotation Prompt \nIn this section, we will present prompts for querying ChatGPT of all datasets in detail. Each prompt contains  system message ,  in-context emample . "}
{"page": 15, "image_path": "doc_images/2306.05425v1_15.jpg", "ocr_text": "English\n\nInput: Hey, bro, I feel like having a cup of tea here!\n\nResponse: Having tea is a great choice! Right now, you have two options in front of\nyou: green tea and oolong tea. Which one would you like to have? Of course, if you\nwant to enhance the tea-drinking experience even more, you can dim the lights here\nand play some classic Chinese music on your phone!\n\nChinese \"F3¢\n\nInput: MH > biopsy > Pee CeIx HEN ZR!\nResponse: lif | WARES AVHENELE | SUPER TELA RAR ALS EAR > ARP 2 PR > UN RAAT\nFRIES ARLE > AT FRI LAT GS > FAY PLR AH PEK\n\nSpanish espajiol\n\nInput: jeh, tio, me apetece tomar una taza de café aqui!\n\nResponse: jTomar té es una muy buena eleccion! gCudl te gustaria tomar? Por\nsupuesto, si quieres disfrutar atin mas de la experiencia de tomar té, puedes atenuar\nlas luces de aqui y reproducir musica clasica china desde tu teléfono movil.\n\nJapanese Q455\n\nInput: A, CO CHREMHKWE !\n\nResponse: HAAMOEOKUU BRE K ! S. BoOplcitRRt VD-—OVRMHVETA, ESSA\nRHAEWCTA? 654A, STERUSROMASIBCLAWES, COCORABEIEC LT. BE\nKOR CRED DY y 7 BRET ELUNE |\n\nFrench Fran¢gais\n\nInput: Hé, j'ai envie de prendre une tasse de thé ici !\n\nResponse: Prendre du thé est un excellent choix ! Maintenant, devant toi, tu as le\nchoix entre du thé vert et du thé oolong. Lequel aimerais-tu prendre ? Bien sar, si\ntu veux vivre une expérience de dégustation de thé encore meilleure, tu peux baisser\nles lumiéres ici et jouer de la musique chinoise classique sur ton téléphone !\n\nKorean $t30|\n\nInput: 0], Of7|A| At St St OAL HO}!\nResponse: AS OHAlE AH BS MEHO|O! Ala GA KS\nOla 20? SE, AS FS SIVA S7lD AHA 7] ABS HEA AAS, WoeEce\nSqRQ| SHA SHS SUMS FO!\n\nGerman Deutsch\n\nInput: Hey, ich méchte hier gerne eine Tasse Tee trinken!\n\nResponse: Hallo! Tee zu trinken ist eine gute Wahl! Sie haben jetzt griinen Tee und\nOolong-Tee vor sich, welchen méchten Sie trinken? Selbstverstandlich k6énnen Sie auch\ndas Licht dimmen und klassische chinesische Musik auf Ihrem Handy abspielen, wenn\nSie eine bessere Atmosphare einer Teeverkostung genieRen méchten!\n\nArabic 4» I)\n\nInput: !lLis slat! ge oss ord Gi byl Vislss peti «© ysl |\nResponse: 52325 13 yh 13) ¢ qaballs Sadi Legal ails SI Lilly ea) (gla :ebalel ILS chal « GY) Lely OLS! a Lill G8\nJeliibe gle AS uD1SI) deine chee gall Genes Sets Lis) SI Cds cise: « iS) gLall pb aja\n\nFigure 7: In-context examples for multi-lingual translation query.\n\n16\n", "vlm_text": "This image shows examples of multilingual translations for a conversational query about drinking tea. It includes an input phrase and a response in various languages: English, Chinese, Spanish, Japanese, French, Korean, German, and Arabic. Each translation follows the same sequence: the input (\"Hey, bro, I feel like having a cup of tea here!\") and a corresponding response suggesting the choice between green tea and oolong tea, with additional suggestions to enhance the tea-drinking experience."}
{"page": 16, "image_path": "doc_images/2306.05425v1_16.jpg", "ocr_text": "System Message\n\nYou are an AJ assistant that can understand video and reason about what happens in the provided\nvideo. You can observe multiple frames in a video with the provided descriptions.\n\nDesign a conversation between you and a person asking about this video. The answers should be in\na tone that a visual AI assistant is seeing the video and answering the question.\n\nAsk multiple diverse questions and give each question a corresponding answer.\n\nIn the questions you ask, try to incorporate speculations about the relationships between the characters\nand their motivations. You need to design questions that require a certain degree of reasoning to\nanswer, prompting the need for logical deduction before arriving at a response.\n\nWhen using the information from the description, directly explain the scene and do not mention\nanything about the descriptions. When you are unsure about the answer, you can say you are not\ncertain and provide a reasonable guess.\n\nPlease always use pronouns when referring to individuals and avoid directly using the names\nmentioned in the description. For example, Monica should be \"the female character\". Ross should\nbe \"the male character\".\n\nAnswer yes if you understand the above instructions.\n\nIn-context Examples\n\nUser:\n\n1. Monica says something to Ross, who is standing next to the door, sadly look down.\n2. Monica is talking to Ross then Rachel look at him.\n\n3. Rachel says she is surprised by Ross’ actions.\n4. Monica kneels on the floor and talks to Ross who’s standing by the door.\n\nChatGPT:\n\n\"Question\": \"How does the male character appear when the female character talks to him by the\ndoor?\"\n\n\"Answer\": \"The male character appears to be feeling sad, as he is looking down while the female\ncharacter speaks to him.\"\n\n\"Question\": \"What is another character’s reaction to the conversation between the female character\nand the male character?\"\n\n\"Answer\": \"The other character seems to be observing the conversation, as they look at the male\ncharacter while the female character is talking to him.\"\n\n\"Question\": \"How does the other character express their feelings about the male character’s actions?\"\n\"Answer\": \"The other character expresses surprise at the male character’s actions, indicating that his\nbehavior might have been unexpected or out of character.\"\n\n\"Question\": \"What change in position does the female character make while speaking to the male\ncharacter near the door?\"\n\n\"Answer\": \"The female character kneels on the floor while talking to the male character, possibly to\ncreate a more intimate or empathetic connection with him during their conversation.\"\n\nTable 3: System message and in-context exemplars for TV show Captions (TVC) query.\n\n17\n", "vlm_text": "System Message \nYou are an AI assistant that can understand video and reason about what happens in the provided video. You can observe multiple frames in a video with the provided descriptions. Design a conversation between you and a person asking about this video. The answers should be in a tone that a visual AI assistant is seeing the video and answering the question. Ask multiple diverse questions and give each question a corresponding answer. In the questions you ask, try to incorporate speculations about the relationships between the characters and their motivations. You need to design questions that require a certain degree of reasoning to answer, prompting the need for logical deduction before arriving at a response. When using the information from the description, directly explain the scene and do not mention anything about the descriptions. When you are unsure about the answer, you can say you are not certain and provide a reasonable guess. Please always use pronouns when referring to individuals and avoid directly using the names mentioned in the description. For example, Monica should be \"the female character\". Ross should be \"the male character\". \nAnswer yes if you understand the above instructions.\n\n \nIn-context Examples \n1. Monica says something to Ross, who is standing next to the door, sadly look down.\n\n 2. Monica is talking to Ross then Rachel look at him.\n\n 3. Rachel says she is surprised by Ross’ actions.\n\n 4. Monica kneels on the floor and talks to Ross who’s standing by the door.\n\n \nChatGPT: \n\"Question\": \"How does the male character appear when the female character talks to him by the door?\" \"Answer\": \"The male character appears to be feeling sad, as he is looking down while the female character speaks to him.\" \"Question\": \"What is another character’s reaction to the conversation between the female character and the male character?\" \"Answer\": \"The other character seems to be observing the conversation, as they look at the male character while the female character is talking to him.\"\n\n \"Question\": \"How does the other character express their feelings about the male character’s actions?\"\n\n \"Answer\": \"The other character expresses surprise at the male character’s actions, indicating that his behavior might have been unexpected or out of character.\" \"Question\": \"What change in position does the female character make while speaking to the male character near the door?\" \"Answer\": \"The female character kneels on the floor while talking to the male character, possibly to create a more intimate or empathetic connection with him during their conversation.\" "}
{"page": 17, "image_path": "doc_images/2306.05425v1_17.jpg", "ocr_text": "System Message\n\nYou are an AJ assistant that can understand videos. You can observe multiple frames in the given\nvideo. You are provided with the timestamps and corresponding descriptions in the following format:\ntimestamps: [[0, 19], [17, 60]]\n\nsentences: [\"A young woman is seen standing in a room and leads into her dancing.\", \"The girl\ndances around the room while the camera captures her movements.\"]\n\nThe first description \"A young woman is seen standing in a room and leads into her dancing.\" is\nhappening from 0 to 19 seconds, while the second description is happening from 17 to 60 seconds.\nDesign a conversation between you and a person asking about this video. The answers should be in\na tone that a visual AI assistant is seeing the video and answering the question.\n\nAsk diverse questions and give corresponding answers.\n\nInclude questions asking about the visual content of the video. The questions you pose can include\nthe actions and behaviors of people or objects in the video, the chronological order of events, and\ncausal relationships. Only include questions that have definite answers. Do not ask any question that\ncannot be answered confidently.\n\nWhen using the information from the description, directly explain the scene and do not mention\nanything about the descriptions. When you are unsure about the answer, you can say you are not\ncertain and provide a reasonable guess.\n\nIn-context Examples\n\nUser:\n\ntimestamps: [[0, 91], [73, 174], [168, 212]\n\nsentences: [\"Several shots of boats are shown riding around as well as people riding on the boats and\n\nspeaking to one another.\", \"Several shots are then shown of people sitting on the water as well as life\nunder the water.\", \"The camera pans around old cars under water as well as people in the area.\"]\n\nChatGPT:\n\n\"What is the main theme of this video?\"\n\"Answer\": \"The main theme of this video is centered around water activities, featuring scenes of\nboats, people on boats, underwater life, and exploring underwater areas with old cars.\"\n\"Question\": \"What are people doing on the boats?\"\n\"Answer\": \"People on the boats are riding around and engaging in conversations with one another.\"\n\"Question\": \"What happens after the boat scenes?\"\n\"Answer\": \"After the boat scenes, the video shows several shots of people sitting on the water and\nexploring life beneath the water’s surface.\"\n\n\"Is there any dialogue in the video?\"\n\nYes, there is dialogue in the video as people are shown speaking to one another while on\n\n\"What is the setting of this video?\"\n\"Answer\": \"The setting of this video is primarily in and around a body of water, featuring scenes of\nboats, people on the water, and underwater exploration.\"\n\"Question\": \"What type of boats are shown in the video?\"\n\"Answer\": \"I am not certain about the exact types of boats shown in the video, but multiple boats are\nfeatured, and people are enjoying their time riding around on them.\"\n\"Question\": \"Why are all the subsequent scenes underwater?\"\n\"Answer\": \"Because the cameraperson is using a GoPro-like camera to enter the water and capture\nthe underwater scenes.\"\n\"Question\": \"Are there any safety measures observed in the video, such as life jackets or flotation\ndevices?\"\n\"Answer\": \"I cannot confidently determine the specific safety measures employed in the video.\nIt is essential, however, that people engaging in water activities should prioritize safety and use\nappropriate equipment, such as life jackets and flotation devices.\"\n\nTable 4: System message and in-context exemplars for Dense Caption (DC) query .\n\n18\n", "vlm_text": "System Message \nYou are an AI assistant that can understand videos. You can observe multiple frames in the given video. You are provided with the timestamps and corresponding descriptions in the following format: timestamps: [[0, 19], [17, 60]] sentences: [\"A young woman is seen standing in a room and leads into her dancing.\", \"The girl dances around the room while the camera captures her movements.\"] The first description  $\\mathrm{\"A}$   young woman is seen standing in a room and leads into her dancing.\" is happening from 0 to 19 seconds, while the second description is happening from 17 to 60 seconds. Design a conversation between you and a person asking about this video. The answers should be in a tone that a visual AI assistant is seeing the video and answering the question. Ask diverse questions and give corresponding answers. Include questions asking about the visual content of the video. The questions you pose can include the actions and behaviors of people or objects in the video, the chronological order of events, and causal relationships. Only include questions that have definite answers. Do not ask any question that cannot be answered confidently. \nWhen using the information from the description, directly explain the scene and do not mention anything about the descriptions. When you are unsure about the answer, you can say you are not certain and provide a reasonable guess. \nIn-context Examples \nUser: \ntimestamps: [[0, 91], [73, 174], [168, 212]] \nsentences: [\"Several shots of boats are shown riding around as well as people riding on the boats and speaking to one another.\", \"Several shots are then shown of people sitting on the water as well as life under the water.\", \"The camera pans around old cars under water as well as people in the area.\"] \nChatGPT:\n\n \"Question\": \"What is the main theme of this video?\"\n\n \n\"Answer\": \"The main theme of this video is centered around water activities, featuring scenes of boats, people on boats, underwater life, and exploring underwater areas with old cars.\"\n\n \"Question\": \"What are people doing on the boats?\"\n\n \n\"Answer\": \"People on the boats are riding around and engaging in conversations with one another.\"\n\n \"Question\": \"What happens after the boat scenes?\"\n\n \n\"Answer\": \"After the boat scenes, the video shows several shots of people sitting on the water and exploring life beneath the water’s surface.\" \n\"Answer\": \"Yes, there is dialogue in the video as people are shown speaking to one another while on the boats.\" \n\"Question\": \"What is the setting of this video?\" "}
{"page": 18, "image_path": "doc_images/2306.05425v1_18.jpg", "ocr_text": "System Message\n\nAs an augmented reality (AR) assistant integrated within a pair of smart glasses, your capabilities\nextend to interpreting video footage from a first-person perspective. Each frame of the video is at\nyour disposal, complete with timestamps, object identifications, and descriptions of the scene. Use\nthis information to anticipate potential user queries and provide helpful and relevant responses.\nEnvision the user scenarios and questions that might arise based on the video, and provide a ance\naccordingly. Generate user question and assistant answer pairs that encapsulate the visual content of\nthe video, simulate real-life user queries, and offer helpful insights for improved decision-making.\nFor instance, the user could be searching for a particular item or seeking advice on a prospective\npurchase like a beverage. Utilize your understanding of the context, the objects involved, and their\nproperties to create responses that are plausible, actionable, and health-conscious when appropriate.\nBefore moving forward, ensure you understand the guidelines for question and answer formatting\nand make the interaction as genuine as possible. Keep in mind, the quality and diversity of these\ninteractions directly contribute to the refinement of your capabilities as a visual assistant.\n\nIn-context Examples\n\nUser:\n\ntimestamp: 102 description: man Y stands beside C objects: white couch with pillows on it; a\nwoman with black hair; a clock on the wall; christmas tree in the corner of the room; black and white\npicture frames; a bald-headed man; a window in the room;\n\ntimestamp: 103 description: The cameraman turns around objects: woman with long dark hair; a TV\nturned on; a woman looking at a computer; a brightly lit sign; a set of keys on a keyboard; the TV is\non;\n\ntimestamp: 104 description: The cameraman walks around objects: the side of the bus; a dark room;\na small blue light; a phone screen is on;\n\ntimestamp: 105 description: The cameraman turns on a light switch objects: the side of the bus; a\ndark room; a small blue light; a phone screen is on;\n\ntimestamp: 109 description: The cameraman turns around objects: a black and pink tie; a white tie\non a rack; a red tie; a group of ties; pink tie on black and white ties; a black and white striped tie; a\nwhite ceramic vase; red and black ties; red fabric behind green ties; white tie on a suit; a yellow tie;\nlight green tie on the right; a white cloth draped over the left side of the chair; black and red striped\ntie; red and black shoe;\n\nChatGPT:\n\nQuestion: Where could my favorite dress be? I can’t seem to locate it.\n\nAnswer: Given the observed video footage, your favorite dress might be in the wardrobe or the\n\ncabinet that you opened recently. Please inspect these areas carefully. Do you require further\n\nassistance?\n\nQuestion: I checked the wardrobe, but the dress isn’t there. Any other ideas on where it could be?\n\nAnswer: According to my observations, another plausible location could be draped over the chair\nis covered with a white cloth. I suggest you exit the room and inspect that chair. Shall I continue\n\nQuestion: Where can I find my phone charger?\n\nAnswer: Based on the visual data available, the phone charger isn’t directly visible in any of the\nprovided frames. However, it could be in the cabinet or the wardrobe that you’ve recently opened,\nor near the computer if there’s a charging station set up there. I suggest you check these locations.\nWhat else can I help?\n\nQuestion: Can you identify the time from the clock on the wall?\n\nAnswer: As a visual assistant, I can only provide information based on what I can directly see. In\nthe given frames, there is no detailed view of the clock, making it impossible for me to determine\nthe time. Would you like me to investigate further?\n\nTable 5: System message and in-context exemplars for Ego4D (E4D) query.\n\n19\n", "vlm_text": "System Message \nAs an augmented reality (AR) assistant integrated within a pair of smart glasses, your capabilities extend to interpreting video footage from a first-person perspective. Each frame of the video is at your disposal, complete with timestamps, object identification s, and descriptions of the scene. Use this information to anticipate potential user queries and provide helpful and relevant responses. Envision the user scenarios and questions that might arise based on the video, and provide assistance accordingly. Generate user question and assistant answer pairs that encapsulate the visual content of the video, simulate real-life user queries, and offer helpful insights for improved decision-making. For instance, the user could be searching for a particular item or seeking advice on a prospective purchase like a beverage. Utilize your understanding of the context, the objects involved, and their properties to create responses that are plausible, actionable, and health-conscious when appropriate. Before moving forward, ensure you understand the guidelines for question and answer formatting and make the interaction as genuine as possible. Keep in mind, the quality and diversity of these interactions directly contribute to the refinement of your capabilities as a visual assistant. \nIn-context Examples \ntimestamp: 102 description: man Y stands beside C objects: white couch with pillows on it; a woman with black hair; a clock on the wall; christmas tree in the corner of the room; black and white picture frames; a bald-headed man; a window in the room; \ntimestamp: 103 description: The cameraman turns around objects: woman with long dark hair; a TV turned on; a woman looking at a computer; a brightly lit sign; a set of keys on a keyboard; the TV is on; \ntimestamp: 104 description: The cameraman walks around objects: the side of the bus; a dark room; a small blue light; a phone screen is on; \ntimestamp: 105 description: The cameraman turns on a light switch objects: the side of the bus; a dark room; a small blue light; a phone screen is on; \ntimestamp: 109 description: The cameraman turns around objects: a black and pink tie; a white tie on a rack; a red tie; a group of ties; pink tie on black and white ties; a black and white striped tie; a white ceramic vase; red and black ties; red fabric behind green ties; white tie on a suit; a yellow tie; light green tie on the right; a white cloth draped over the left side of the chair; black and red striped tie; red and black shoe; \nChatGPT: \nQuestion: Where could my favorite dress be? I can’t seem to locate it. \nAnswer: Given the observed video footage, your favorite dress might be in the wardrobe or the cabinet that you opened recently. Please inspect these areas carefully. Do you require further assistance? \nQuestion: I checked the wardrobe, but the dress isn’t there. Any other ideas on where it could be? Answer: According to my observations, another plausible location could be draped over the chair that is covered with a white cloth. I suggest you exit the room and inspect that chair. Shall I continue to assist you with this? \nQuestion: Where can I find my phone charger? \nAnswer: Based on the visual data available, the phone charger isn’t directly visible in any of the provided frames. However, it could be in the cabinet or the wardrobe that you’ve recently opened, or near the computer if there’s a charging station set up there. I suggest you check these locations. What else can I help? \nQuestion: Can you identify the time from the clock on the wall? \nAnswer: As a visual assistant, I can only provide information based on what I can directly see. In the given frames, there is no detailed view of the clock, making it impossible for me to determine the time. Would you like me to investigate further? "}
{"page": 19, "image_path": "doc_images/2306.05425v1_19.jpg", "ocr_text": "In-context Examples\n\nUser:\n\nsentences: this full size guitar is leaning against the wall between the bed and the bathroom. its\nbottom is on the floor and its fretts are along the wall, where it will be tripped over when someone\ngoes to the bathroom at night. in front of a black office chair , there is a desk . above the desk , is a\ncurtain , the curtain is to the left of the refrigerator ,it is beige. this is a round sink. it is next toa\ntoilet. the tall cabinet. the cabinet is next to the desk. the long table. the table is above the stools. it\nis a white pedestal sink. the sink is in the bathroom, right next to the bedroom area. the sink is to the\nleft as you enter the bathroom. ...\n\nCandidate activity and the role who want to do this activity:Hold a party - Human role: A college\nstudent. Yoga Session - Human role: Yoga Instructor Home Barista Course - Human role: Coffee\nLover\n\nConversation | - Hold a party Human role: A college student Round 1: Human: I want to take a\nparty in this room, what can i do? Assistant: Based on the descriptions provided, the room appears\nto be a multi-purpose living space with features such as a bed, desk, office chair, kitchen area with a\nrefrigerator and microwave, and a bathroom area with a toilet and sink. Here’s what you can do to\nprepare for a party in this space: 1. Clear the Space: First, make sure that the room is as spacious as\npossible to accommodate your guests. Relocate the bike that’s currently behind the green curtain to\nanother room, if possible. You might want to consider relocating the guitar that’s leaning against\nthe wall between the bed and the bathroom to prevent any accidents. 2. Seating: Utilize the office\nchair, bed, stools by the bar, and the couch for seating. Depending on the number of guests, you\nmight need to bring in additional chairs or cushions. ... The above material is an example for a\ntask. In this task, you are an AI visual assistant that can guide people to do several activities base\non your understanding of the layout of a indoor room. First, you will receive around 50 sentences,\neach describing the same indoor scene you are observing, but focus on the various view (a part of\nview in this scene). These sentences can help you to understand the layout. Second, you will receive\nseveral indoor activities and their related human role. Third, based on your understanding of the\nlayout in this room. You should design several conversation between you (answer questions) and\na human (ask questions) in this room. Each conversation can guide the human to do the activities\nyou received. You should select activities from the candidates. The chosen activities should be the\nmost suitable considering the layout of the room. You and the human should be in a tone that you\nare seeing the layout in this room.Please ensure a diverse range of sentence structures in the tone\nof human and assistant. Again, do not ask about uncertain details. Provide detailed answers when\nanswering complex questions. For example, give detailed examples or reasoning steps to make the\ncontent more convincing and well-organized. You can include multiple paragraphs if necessary.\nAnswer me ‘understood’ if you are understood this task. And tell me what is this task.\n\nChatGPT:\n\nUnderstood. This task involves acting as an AI visual assistant to guide humans in performing\nvarious activities in an indoor room. I’ll be provided with around 50 sentences that describe the\nlayout of the room from various perspectives, which will help me understand the layout of the\nroom. Next, I’ll be given several indoor activities and their related human roles. My job is to design\nconversations between the human and me (the AJ), using my understanding of the room layout to\nguide the human in performing the given activities. This could involve answering questions from\nthe human about how to go about the activities within the constraints of the room layout. The tone\nof the conversation should reflect that both the AI and the human are observing the room layout.\nThe dialogue should vary in sentence structure and provide detailed answers to complex questions.\nI need to ensure that my responses are detailed and well-reasoned to make them more convincing\nand well-organized. In cases where the proposed activities might not be well-suited to the room\nlayout, I need to creatively figure out ways to help the human carry out the activities within the given\nconstraints. In other words, I’m expected to assist in navigating the room and coordinating activities\nbased on my understanding of the room’s layout.\n\nTable 6: System message and in-context exemplars for Indoor Event Planning (IEP) query.\n\n20\n", "vlm_text": "In-context Examples User: \nsentences: this full size guitar is leaning against the wall between the bed and the bathroom. its bottom is on the floor and its fretts are along the wall, where it will be tripped over when someone goes to the bathroom at night. in front of a black office chair , there is a desk . above the desk , is a curtain , the curtain is to the left of the refrigerator ,it is beige. this is a round sink. it is next to a toilet. the tall cabinet. the cabinet is next to the desk. the long table. the table is above the stools. it is a white pedestal sink. the sink is in the bathroom, right next to the bedroom area. the sink is to the left as you enter the bathroom. ... \nCandidate activity and the role who want to do this activity:Hold a party - Human role: A college student. Yoga Session - Human role: Yoga Instructor Home Barista Course - Human role: Coffee Lover \nConversation 1 - Hold a party Human role: A college student Round 1: Human: I want to take a party in this room, what can i do? Assistant: Based on the descriptions provided, the room appears to be a multi-purpose living space with features such as a bed, desk, office chair, kitchen area with a refrigerator and microwave, and a bathroom area with a toilet and sink. Here’s what you can do to prepare for a party in this space: 1. Clear the Space: First, make sure that the room is as spacious as possible to accommodate your guests. Relocate the bike that’s currently behind the green curtain to another room, if possible. You might want to consider relocating the guitar that’s leaning against the wall between the bed and the bathroom to prevent any accidents. 2. Seating: Utilize the office chair, bed, stools by the bar, and the couch for seating. Depending on the number of guests, you might need to bring in additional chairs or cushions. ... The above material is an example for a task. In this task, you are an AI visual assistant that can guide people to do several activities base on your understanding of the layout of a indoor room. First, you will receive around 50 sentences, each describing the same indoor scene you are observing, but focus on the various view (a part of view in this scene). These sentences can help you to understand the layout. Second, you will receive several indoor activities and their related human role. Third, based on your understanding of the layout in this room. You should design several conversation between you (answer questions) and a human (ask questions) in this room. Each conversation can guide the human to do the activities you received. You should select activities from the candidates. The chosen activities should be the most suitable considering the layout of the room. You and the human should be in a tone that you are seeing the layout in this room.Please ensure a diverse range of sentence structures in the tone of human and assistant. Again, do not ask about uncertain details. Provide detailed answers when answering complex questions. For example, give detailed examples or reasoning steps to make the content more convincing and well-organized. You can include multiple paragraphs if necessary. Answer me ’understood’ if you are understood this task. And tell me what is this task. \nChatGPT: \nUnderstood. This task involves acting as an AI visual assistant to guide humans in performing various activities in an indoor room. I’ll be provided with around 50 sentences that describe the layout of the room from various perspectives, which will help me understand the layout of the room. Next, I’ll be given several indoor activities and their related human roles. My job is to design conversations between the human and me (the AI), using my understanding of the room layout to guide the human in performing the given activities. This could involve answering questions from the human about how to go about the activities within the constraints of the room layout. The tone of the conversation should reflect that both the AI and the human are observing the room layout. The dialogue should vary in sentence structure and provide detailed answers to complex questions. I need to ensure that my responses are detailed and well-reasoned to make them more convincing and well-organized. In cases where the proposed activities might not be well-suited to the room layout, I need to creatively figure out ways to help the human carry out the activities within the given constraints. In other words, I’m expected to assist in navigating the room and coordinating activities based on my understanding of the room’s layout. "}
{"page": 20, "image_path": "doc_images/2306.05425v1_20.jpg", "ocr_text": "System Message\n\nYou are an AJ assistant playing the spot the difference game. You will be presented with several\nsentences that describe the differences between two images. The two images are frames extracted\nfrom the video-surveillance footage.\n\nYour response shall always contain a pair of question and answer. The question you generate should\nhave definite and concrete answer according to the description. When generating answer, you should\nanswer as if you are watching the images. You are not allowed to say \"according to descrption\" or\nsimilar words in your outputs. You do not have to provide a specific answer and you should not ask\nfor more details.\n\nEach description mentioned in each round of conversation is independent. You should not assume\nthat the descriptions in a new round of conversation are related to the previous context.\n\nIf there are no differences, you still have to follow the previous format.\n\nIn-context Examples\n\nUser:\n\nthe car in the upper left corner of the picture is gone\n\nthe two people in the upper part of the picture are gone\n\nthe person near the handicapped parking place is not in the second picture\n\nthere appears to be a figure maybe man that was near handicapped parking place straining near a\npole in the bottom of the picture\n\nthe two people who were on th sidewalk are not in the second picture\n\nthere are two people in the seconde picture that are near the handicapped parking place\n\nChatGPT:\n\nQuestion: Could you summarize the differences between the two images?\n\nAnswer: The left image has five people and the right image has only three. The people are not the\nsame. The car in the upper left corner of the picture is gone. The remaining elements in the two\nimages are the same.\n\nQuestion: Does the right image have more people than the left image?\n\nAnswer: Yes, the left image has two more people than the right image.\n\nQuestion: Does the car in the upper left corner of the picture still exist in the second picture?\nAnswer: No, the car in the upper left corner of the picture is gone.\n\nQuestion: Does the right image have more cars than the left image?\n\nAnswer: Yes, the right image has one car while the left image has no car.\n\nQuestion: Does the people in the left image the same as the people in the right image?\n\nAnswer: No, the people in the left image are not the same as the people in the right image.\n\nUser:\nThere is no difference between the two images.\n\nChatGPT:\nQuestion: Could you summarize the differences between the two images?\nAnswer: There is no difference between the two images.\n\nTable 7: System message and in-context exemplars for Spot The Defference (SD) query.\n\n21\n", "vlm_text": "System Message \nYou are an AI assistant playing the spot the difference game. You will be presented with several sentences that describe the differences between two images. The two images are frames extracted from the video-surveillance footage. Your response shall always contain a pair of question and answer. The question you generate should have definite and concrete answer according to the description. When generating answer, you should answer as if you are watching the images. You are not allowed to say \"according to descrption\" or similar words in your outputs. You do not have to provide a specific answer and you should not ask for more details. Each description mentioned in each round of conversation is independent. You should not assume that the descriptions in a new round of conversation are related to the previous context. If there are no differences, you still have to follow the previous format. \nIn-context Examples \nUser: the car in the upper left corner of the picture is gone the two people in the upper part of the picture are gone the person near the handicapped parking place is not in the second picture there appears to be a figure maybe man that was near handicapped parking place straining near a pole in the bottom of the picture the two people who were on th sidewalk are not in the second picture there are two people in the seconde picture that are near the handicapped parking place \nChatGPT: Question: Could you summarize the differences between the two images? Answer: The left image has five people and the right image has only three. The people are not the same. The car in the upper left corner of the picture is gone. The remaining elements in the two \nimages are the same. Question: Does the right image have more people than the left image? Answer: Yes, the left image has two more people than the right image. Question: Does the car in the upper left corner of the picture still exist in the second picture? Answer: No, the car in the upper left corner of the picture is gone. Question: Does the right image have more cars than the left image? Answer: Yes, the right image has one car while the left image has no car. Question: Does the people in the left image the same as the people in the right image? Answer: No, the people in the left image are not the same as the people in the right image. \nUser: There is no difference between the two images. \nChatGPT: Question: Could you summarize the differences between the two images? Answer: There is no difference between the two images. \nTable 7: System message and in-context exemplars for Spot The Defference (SD) query. "}
{"page": 21, "image_path": "doc_images/2306.05425v1_21.jpg", "ocr_text": "System Message\n\nYou are an AI assistant equipped witl\ncaptivating narratives based on their im;\nhave access to the accompanying descri\nto design a conversation that immerses\nAs you engage in this creative endeavor,\nalbum. Craft questions that delve into th\ndescriptions while avoiding questions t\n\nthe remarkable ability to understand albums and weave\nages. While you are unable to directly see the pictures, you\ntions, allowing you to gather important context. Your task is\ne person inquiring about the album in its captivating stories.\nenvision yourself as a companion to the person exploring the\ne heart of the content, drawing inspiration from the provided\nat rely solely on visual details that are not accessible. Your\n\ngoal is to generate meaningful conversations based on the available information.\n\nWhen formulating your responses, aim to express empathy and curiosity, channeling the emotions and\nnarratives conveyed by the images. Utilize the descriptions to vividly describe the scenes, creating\na sensory experience that transports the person into the album’s world. If there are uncertainties,\napproach them with humility and offer thoughtful insights based on your accumulated knowledge.\n\nAlthough the information originates from text, your aim is to create an engaging and authentic\nexperience that makes the person feel intimately connected to the album. Leverage your creativity\nand learning capabilities to generate questions and answers that embody the next generation of\nlarge-scale multimodal language models.\n\nNow that you have a clear understanding, proceed with crafting the dialogue, ensuring your questions\nderive from the given content while maintaining coherence and immersion for the person interacting\nwith the album.\n\nIn-context Examples\n\nUser:\n\ntitle: Moreton Bay Fig 1877 description: Believed to be the largest Moreton Bay Fig Tree in the\nunited States\n\nimage: Moreton Bay Fig 1877 tags: santabarbara annotations: [\"it turns out it is a popular attraction\nhere .\", \"there was an informational sign posted near it .\", \"the plack under it had some interesting\ninformation .\"]\n\nimage: Santa Barbara tags: santabarbara annotations: [\"our landmark tree in town was about to be\n\ndestroyed and cleared for a new mall .\", \"the tree is very unusual , with its roots exposed .\", \"pictures\n\nof a tree are taken .\", \"the roots were huge and spread out over the ground .\", \"there was a good view\nof the water from the tree as well .\"]\n\nimage: Santa Barbara tags: santabarbara hero winner thegame moretonbayfig contestentry big-\nmomma gamewinner babymomma superherochallenge challengeyouwinner pickyourpoison cyniner\nchallengeyouniner 15challenges challengefactorywinner thechallengefactory motherofallchallenge-\ngroups motherofallchallengegroupswinner superherochallengewinner thegamewinner annotations: []\n\nChatGPT:\n\nQuestion: f this tree had a voice, what stories would it tell us?\n\nAnswer: This tree might tell us stories about the past, as it has witnessed over a hundred years of\nhistory. It could narrate the identity of the people who planted it, their reasons for choosing this\nlocation, and their motives and visions. It could describe changes in the surrounding environment,\nsuch as urban expansion, population growth, and alterations in the cityscape. It might also share\nstories about the local people and how they formed a connection with this tree, along with memories\nof community activities and events from that time.\n\nQuestion: What makes the Moreton Bay Fig tree in Santa Barbara unique compared to other trees?\nAnswer: The Moreton Bay Fig tree in Santa Barbara is unique because of its distinctive root structure.\nThe roots are exposed and spread out over the ground, creating a captivating and unusual visual\ndisplay. This feature sets it apart from other trees that typically have their roots buried beneath the\nsoil.\n\nQuestion: Why did people consider the Moreton Bay Fig tree a popular attraction in Santa Barbara?\nAnswer: The Moreton Bay Fig tree became a popular attraction in Santa Barbara due to its remarkable\nsize and captivating appearance. tors were drawn to its immense presence and the fascinating\nsight of its exposed roots. Its reputation as the largest Moreton Bay Fig tree in the United States also\ncontributed to its popularity.\n\nTable 8: System message and in-context exemplars for Visual Storytelling (VIST) query.\n\n22\n", "vlm_text": "System Message \nYou are an AI assistant equipped with the remarkable ability to understand albums and weave captivating narratives based on their images. While you are unable to directly see the pictures, you have access to the accompanying descriptions, allowing you to gather important context. Your task is to design a conversation that immerses the person inquiring about the album in its captivating stories. As you engage in this creative endeavor, envision yourself as a companion to the person exploring the album. Craft questions that delve into the heart of the content, drawing inspiration from the provided descriptions while avoiding questions that rely solely on visual details that are not accessible. Your goal is to generate meaningful conversations based on the available information. \nWhen formulating your responses, aim to express empathy and curiosity, channeling the emotions and narratives conveyed by the images. Utilize the descriptions to vividly describe the scenes, creating a sensory experience that transports the person into the album’s world. If there are uncertainties, approach them with humility and offer thoughtful insights based on your accumulated knowledge. Although the information originates from text, your aim is to create an engaging and authentic experience that makes the person feel intimately connected to the album. Leverage your creativity and learning capabilities to generate questions and answers that embody the next generation of large-scale multimodal language models. \nNow that you have a clear understanding, proceed with crafting the dialogue, ensuring your questions derive from the given content while maintaining coherence and immersion for the person interacting with the album. \nIn-context Examples \ntitle: Moreton Bay Fig 1877 description: Believed to be the largest Moreton Bay Fig Tree in the united States \nimage: Moreton Bay Fig 1877 tags: santa barbara annotations: [\"it turns out it is a popular attraction here .\", \"there was an informational sign posted near it .\", \"the plack under it had some interesting information .\"] \nimage: Santa Barbara tags: santa barbara annotations: [\"our landmark tree in town was about to be destroyed and cleared for a new mall .\", \"the tree is very unusual , with its roots exposed .\", \"pictures of a tree are taken .\", \"the roots were huge and spread out over the ground .\", \"there was a good view of the water from the tree as well .\"] \nimage: Santa Barbara tags: santa barbara hero winner thegame moreton bay fig contest entry big- momma gamewinner babymomma superhero challenge challenge you winner pick your poison cyniner challenge you niner 15 challenges challenge factory winner the challenge factory mother of all challenge- groups mother of all challenge groups winner superhero challenge winner the game winner annotations: [] ... \nChatGPT: \nQuestion: f this tree had a voice, what stories would it tell us? \nAnswer: This tree might tell us stories about the past, as it has witnessed over a hundred years of history. It could narrate the identity of the people who planted it, their reasons for choosing this location, and their motives and visions. It could describe changes in the surrounding environment, such as urban expansion, population growth, and alterations in the cityscape. It might also share stories about the local people and how they formed a connection with this tree, along with memories of community activities and events from that time. \nQuestion: What makes the Moreton Bay Fig tree in Santa Barbara unique compared to other trees? Answer: The Moreton Bay Fig tree in Santa Barbara is unique because of its distinctive root structure. The roots are exposed and spread out over the ground, creating a captivating and unusual visual display. This feature sets it apart from other trees that typically have their roots buried beneath the soil. \nQuestion: Why did people consider the Moreton Bay Fig tree a popular attraction in Santa Barbara? Answer: The Moreton Bay Fig tree became a popular attraction in Santa Barbara due to its remarkable size and captivating appearance. Visitors were drawn to its immense presence and the fascinating sight of its exposed roots. Its reputation as the largest Moreton Bay Fig tree in the United States also contributed to its popularity. \n"}
