{"layout": 0, "type": "text", "text": "QAC HECK : A Demonstration System for Question-Guided Multi-Hop Fact-Checking ", "text_level": 1, "page_idx": 0, "bbox": [150, 75, 445, 109], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 1, "type": "text", "text": "Liangming   $\\mathbf{Pan}^{1,2}$    Xinyuan  $\\mathbf{L}\\mathbf{u}^{3}$  Min-Yen Kan 3 Preslav Nakov 1 1 MBZUAI 2 University of California, Santa Barbara 3  National University of Singapore ", "page_idx": 0, "bbox": [82.56500244140625, 116.02398681640625, 515.7009887695312, 149.14639282226562], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 2, "type": "text", "text": "liang ming pan@ucsb.edu luxinyuan@u.nus.edu kanmy@comp.nus.edu.sg preslav.nakov@mbzuai.ac.ae ", "page_idx": 0, "bbox": [148.6610107421875, 163.7003936767578, 449.6092529296875, 189.74607849121094], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 3, "type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0, "bbox": [158, 214, 202, 225], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 4, "type": "text", "text": "Fact-checking real-world claims often requires complex, multi-step reasoning due to the ab- sence of direct evidence to support or refute them. However, existing fact-checking sys- tems often lack transparency in their decision- making, making it challenging for users to com- prehend their reasoning process. To address this, we propose the  Question-guided Multi- hop Fact-Checking  (QAC HECK ) system, which guides the model’s reasoning process by ask- ing a series of questions critical for verifying a claim. QAC HECK  has five key modules: a claim verifier, a question generator, a question- answering module, a QA validator, and a rea- soner. Users can input a claim into QAC HECK , which then predicts its veracity and provides a comprehensive report detailing its reasoning process, guided by a sequence of (question, answer) pairs. QAC HECK 1   also provides the source of evidence supporting each question, fostering a transparent, explain able, and user- friendly fact-checking process. ", "page_idx": 0, "bbox": [87, 236.50555419921875, 273, 499.56951904296875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 5, "type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0, "bbox": [71, 510, 154, 523], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 6, "type": "text", "text": "In our age characterized by large amounts of both true and false information, fact-checking is not only crucial for counteracting misinformation but also plays a vital role in fostering trust in AI sys- tems. However, the process of validating real-world claims is rarely straightforward. Unlike the simplic- ity of supporting or refuting a claim with a single piece of direct evidence, real-world claims often resemble multi-layered puzzles that require com- plex and multi-step reasoning to solve ( Jiang et al. , 2020 ;  Nguyen et al. ,  2020 ;  Aly and Vlachos ,  2022 ; Chen et al. ,  2022 ;  Pan et al. ,  2023 ). ", "page_idx": 0, "bbox": [70, 531.8779907226562, 290, 694.064453125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 7, "type": "text", "text": "As an example, to verify the claim  “Sunlight can reach the deepest part of the Black Sea.” , it may be challenging to find direct evidence on the web that ", "page_idx": 0, "bbox": [70, 694.52099609375, 290, 734.7644653320312], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 8, "type": "image", "page_idx": 0, "img_path": "layout_images/2310.07609v1_0.jpg", "img_caption": "Figure 1: An example of  question-guided  reasoning for fact-checking complex real-world claims. ", "bbox": [305, 213, 525, 430], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "Claim: Sunlight can travel to the deepest part of the Black Sea.\n\n¥v\n\nQ1: What is the greatest depth »\nof the Black Sea? =\n\n221m\n\nA1: Black sea has a maximum «\ndepth of 2,212 meters.\n\n¥\n\nQ2: How far can sunlight »\npenetrate water? .\n\nA2: Sunlight does not penetrate «\nwater below 1,000 meters.\n\n¥\n\n2,212 is greater than 1,000. Therefore, the claim is 69.538\n\n", "vlm_text": "The image illustrates a process for fact-checking the claim: \"Sunlight can travel to the deepest part of the Black Sea.\"\n\n1. **Claim**: Sunlight can travel to the deepest part of the Black Sea.\n\n2. **Question 1 (Q1)**: What is the greatest depth of the Black Sea?\n   - **Answer 1 (A1)**: The Black Sea has a maximum depth of 2,212 meters.\n   - A search result image shows the depth as 2,212 meters.\n\n3. **Question 2 (Q2)**: How far can sunlight penetrate water?\n   - **Answer 2 (A2)**: Sunlight does not penetrate water below 1,000 meters.\n   - A search result image indicates sunlight penetrates up to 1,000 meters.\n\n4. **Conclusion**: Since 2,212 meters is greater than 1,000 meters, the claim is marked as false."}
{"layout": 9, "type": "text", "text": "refutes or supports this claim. Instead, a human fact-checker needs to decompose the claim, gather multiple pieces of evidence, and perform step-by- step reasoning ( Pan et al. ,  2023 ). This reasoning process can be formulated as  question-guided rea- soning , where the verification of the claim is guided by asking and answering a series of relevant ques- tions, as shown in Figure  1 . In this example, we se- quentially raise two questions:  “What is the great- est depth of the Black Sea?”  and  “How far can sunlight penetrate water?” . After independently answering these two questions by gathering rele- vant information from the Web, we can assert that the initial claim is  false  with simple reasoning. ", "page_idx": 0, "bbox": [305, 447.6600036621094, 526, 636.9454345703125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 10, "type": "text", "text": "While several models ( Liu et al. ,  2020 ;  Zhong et al. ,  2020 ;  Aly and Vlachos ,  2022 ) have been proposed to facilitate multi-step reasoning in fact- checking, they generally lack transparency in their reasoning processes. These models simply take a claim as input, then output a veracity label without an explicit explanation. Recent attempts, such as Quin+ (Samarinas et al., 2021) and What The Wiki-Fact  ( Cher nya v ski y et al. ,  2021 ), have aimed to de- velop more explain able fact-checking systems, by searching and visualizing the supporting evidence for a given claim. However, these systems primar- ily validate a claim from a  single  document, and do not provide a detailed, step-by-step visualization of the reasoning process as shown in Figure  1 . ", "page_idx": 0, "bbox": [305, 639.0009765625, 526, 774.0894775390625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 11, "type": "text", "text": "", "page_idx": 1, "bbox": [70, 71.74500274658203, 291, 139.08749389648438], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 12, "type": "text", "text": "We introduce the  Question-guided Multi-hop Fact-Checking  (QAC HECK ) system, which ad- dresses the aforementioned issues by generating multi-step explanations via question-guided rea- soning. To facilitate an explain able reasoning pro- cess, QAC HECK  manages the reasoning process by guiding the model to self-generate a series of ques- tions vital for claim verification. Our system, as depicted in Figure  2 , is composed of five modules: 1) a  claim verifier  that assesses whether sufficient information has been gathered to verify the claim, 2) a  question generator  to generate the next rele- vant question, 3) a  question-answering  module to answer the raised question, 4) a    $Q A$   validator  to evaluate the usefulness of the generated (Q, A) pair, and 5) a  reasoner  to output the final veracity label based on all collected contexts. ", "page_idx": 1, "bbox": [70, 140.1160430908203, 291, 370.0484619140625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 13, "type": "text", "text": "QAC HECK  offers enough adaptability, allowing users to customize the design of each module by integrating with different models. For example, we provide three alternative implementations for the QA component: the retriever–reader model, the FLAN-T5 model, and the GPT3-based reciter– reader model. Furthermore, we offer a user-friendly interface for users to fact-check any input claim and visualize its detailed question-guided reason- ing process. The screenshot of our user interface is shown in Figure  4 . We will discuss the implementa- tion details of the system modules in Section  3  and some evaluation results in Section  4 . Finally, we present the details of the user interface in Section  5 . and conclude and discuss future work in Section  6 . ", "page_idx": 1, "bbox": [70, 371.0780029296875, 291, 573.9114379882812], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 14, "type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1, "bbox": [70, 586, 161, 599], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 15, "type": "text", "text": "Fact-Checking Systems. The recent surge in automated fact-checking research aims to miti- gate the spread of misinformation. Various fact- checking systems, for example, T ANBIH 2   ( Zhang et al. ,  2019 ), PRTA 3   ( Martino et al. ,  2020 ), and W HAT T HE W IKI F ACT 4   ( Cher nya v ski y et al. , 2021 ) predominantly originating from Wikipedia and claims within political or scientific domains, have facilitated this endeavor. However, the major- ", "page_idx": 1, "bbox": [70, 608.7662963867188, 291, 730.6984252929688], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 16, "type": "image", "page_idx": 1, "img_path": "layout_images/2310.07609v1_1.jpg", "img_caption": "Figure 2: The architecture of our QAC HECK  system. ", "bbox": [308, 70, 523, 331], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "77” (Do we have sufficient ~S,\n\ncontexts to verify the claim?) *\n\ni\n1 Relevant {\nClaim Verifier |©<——— content '\n1 1\n\\ (What is the next question to ask?) i\noO}\nt i\nt i\nE fl\n| (Is the QA-pair ;\n| (What is the answer to the question?) correct and useful?) 1\nt\n\nt\n\\ Validator\n% (QA) Ps\n\nN\n\nReasoner\n\nws\n\nWikipedia Corpus \"Label\n\n", "vlm_text": "The image is a flowchart illustrating the architecture of the QAC HECK system. Here's a breakdown of the process:\n\n1. **Claim**: The starting point of the process.\n2. **Claim Verifier**: Assesses whether there are sufficient contexts to verify the claim and interacts with the \"Relevant Context\".\n3. **Question Generator**: Determines the next question to ask based on the verification process.\n4. **QA Model**: Answers the generated question.\n5. **Validator**: Checks if the QA pair (Question and Answer) is correct and useful.\n6. **Reasoner**: Uses information from the Wikipedia Corpus and QA to arrive at a conclusion.\n7. **Label**: The final outcome or classification of the claim.\n\nThe system relies on elements like the Wikipedia Corpus to provide context and evidence, and loops through its components to ensure accurate verification."}
{"layout": 17, "type": "text", "text": "ity of these systems limit the validation or refuta- tion of a claim to a single document, indicating a gap in systems for multi-step reasoning ( Pan et al. ,  2023 ). The system most similar to ours is    $Q u i n+$   ( Samarinas et al. ,  2021 ), which demon- strates evidence retrieval in a single step. In con- trast, our QAC HECK  shows a question-led multi- step reasoning process with explanations and re- trieved evidence for each reasoning step. In sum- mary, our system 1) supports fact-checking real- world claims that require multi-step reasoning, and 2) enhances transparency and helps users have a clear understanding of the reasoning process. ", "page_idx": 1, "bbox": [305, 352.12200927734375, 526, 527.857421875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 18, "type": "text", "text": "Explanation Generation. Simply predicting a veracity label to the claim is not persuasive, and can even enhance mistaken beliefs ( Guo et al. ,  2022 ). Hence, it is necessary for automated fact-checking methods to provide explanations to support model predictions. Traditional approaches have utilized attention weights, logic, or summary generation to provide post-hoc explanations for model pre- dictions ( Lu and Li ,  2020 ;  Ahmadi et al. ,  2019 ; Kotonya and Toni ,  2020 ;  Jolly et al. ,  2022 ;  Xing et al. ,  2022 ). In contrast, our approach employs question–answer  pair based explanations, offering more human-like and natural explanations. ", "page_idx": 1, "bbox": [305, 536.8892822265625, 526, 713.0184326171875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 19, "type": "text", "text": "3 System Architecture ", "text_level": 1, "page_idx": 1, "bbox": [305, 724, 429, 739], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 20, "type": "text", "text": "Figure  2  shows the general architecture of our sys- tem, comprised of five principal modules: a Claim Verifier    $\\mathcal{D}$  , a Que ion Generat    $\\mathcal{Q}$  , a Question- nswering Model  A , a Validator  V , and a R R . We first initialize an empty context  C  ${\\mathcal{C}}\\,=\\,\\emptyset$   ∅ . Upon the receipt of a new input claim    $c$  , the sys- tem first utilizes the  claim verifier  to determine the sufficiency of the existing context to validate the claim,  i.e. ,    $\\mathcal{D}(c,\\mathcal{C})\\,\\rightarrow\\,\\{\\mathsf{T r u e},\\mathsf{F a l s e}\\}$  . If the output is  False , the  question generator  learns to generate the next question that is necessary for ver- ifying the claim,  i.e. ,    $\\mathcal{Q}(c,\\mathcal{C})\\to q$  . The  question- answering  model is then applied to answer the question and provide the supported evidence,  i.e. ,  $\\mathcal{A}(q)\\rightarrow a,e$  , where  $a$   is the predicted answer, and  $e$   is the retrieved evidence that supports the an- swer. Afterward, the  validator  is used to validate the usefulness of the newly-generated (Q, A) pair based on the existing context and the claim,  i.e. ,  $\\mathcal{V}(c,\\{q,a\\},\\mathcal{C})\\,\\rightarrow\\,\\{\\mathsf{T r u e},\\mathsf{F a l s e}\\}$  . If the output is  True , the    $(q,a)$   pair is added into the context  $C$  . Otherwise, the question generator is asked to generate another question. We repeat this process of calling    ${\\mathcal{D}}\\,\\rightarrow\\,{\\mathcal{Q}}\\,\\rightarrow\\,{\\mathcal{A}}\\,\\rightarrow\\,{\\mathcal{V}}$   until the claim verifier returns a  True  indicating that the current context    $C$   contains sufficient information to ver- ify the claim    $c$  . In this case, the  reasoner  module is called to utilize the stored relevant context to justify the veracity of the claim and outputs the fi- nal label,  i.e. ,    $\\mathcal{R}(c,\\mathcal{C})\\rightarrow\\{\\mathsf{S u p p o r t e d},\\mathsf{R e f u t e d}\\}.$  . The subsequent sections provide a comprehensive description of the five key modules in QAC HECK . ", "page_idx": 1, "bbox": [306, 747.39501953125, 526, 774.0894775390625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 21, "type": "text", "text": "", "page_idx": 2, "bbox": [70, 71.74500274658203, 291, 477.8174743652344], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 22, "type": "text", "text": "3.1 Claim Verifier ", "text_level": 1, "page_idx": 2, "bbox": [70, 496, 162, 507], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 23, "type": "text", "text": "The claim verifier is a central component of QAC HECK , with the specific role of determining if the current context information is sufficient to verify the claim. This module is to ensure that the system can efficiently complete the claim verifica- tion process without redundant reasoning. We build the claim verifier based on Instruct GP T ( Ouyang et al. ,  2022 ), utilizing its powerful  in-context learn- ing  ability. Recent large language models such as Instruct GP T ( Ouyang et al. ,  2022 ) and GPT- 4 ( OpenAI ,  2023 ) have demonstrated strong few- shot generalization ability via  in-context learning , in which the model can efficiently learn a task when prompted with the instruction of the task together with a small number of demonstrations. We take ad- vantage of Instruct GP T’s in-context learning abil- ity to implement the claim verifier. We prompt Instruct GP T with ten distinct in-context examples as detailed in Appendix  A.1 , where each example consists of a claim and relevant question–answer pairs. We then prompt the model with the claim, the context, and the following instruction: ", "page_idx": 2, "bbox": [70, 517.0579833984375, 291, 774.0894775390625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 24, "type": "text", "text": "", "page_idx": 2, "bbox": [305, 71.74500274658203, 526, 111.98947143554688], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 25, "type": "image", "page_idx": 2, "img_path": "layout_images/2310.07609v1_2.jpg", "bbox": [325, 120, 503, 187], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "Claim = [@uNiy\n\nWe already know the following:\n\nCan we know whether the claim is\ntrue or false now? Yes or no?\n", "vlm_text": "The image contains text that discusses a process of evaluating a claim. It includes three main sections:\n\n1. \"Claim = CLAIM\" - indicates that a specific claim is being evaluated, although no concrete claim is provided in the image.\n\n2. \"We already know the following: CONTEXT\" - suggests that there is some pre-existing context or information related to the claim, which will be considered in the evaluation, but the actual context is not shown in the image.\n\n3. \"Can we know whether the claim is true or false now? Yes or no?\" - poses a question about the ability to determine the truthfulness of the claim given the context, asking for a binary answer (yes or no)."}
{"layout": 26, "type": "text", "text": "If the response is  ‘no’ , we proceed to the question generator module. Conversely, if the response is ‘yes’ , the process jumps to call the reasoner module. ", "page_idx": 2, "bbox": [305, 198.4459991455078, 526, 238.68948364257812], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 27, "type": "text", "text": "3.2 Question Generator ", "text_level": 1, "page_idx": 2, "bbox": [305, 249, 424, 261], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 28, "type": "text", "text": "The question generator module is called when the initial claim lacks the necessary context for veri- fication. This module aims to generate the next relevant question needed for verifying the claim. Similar to the claim verifier, we also leverage In- structGPT for in-context learning. We use slightly different prompts for generating the initial question and the follow-up questions. The detailed prompts are in Appendix  A.2 . For the  initial  question gen- eration, the instruction is: ", "page_idx": 2, "bbox": [305, 265.6610107421875, 526, 400.7494812011719], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 29, "type": "text", "text": "Claim   $=$   CLAIM To verify the above claim, we can first ask a simple question: ", "page_idx": 2, "bbox": [327, 411, 502.59283447265625, 449.4159240722656], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 30, "type": "text", "text": "For  follow-up  questions, the instruction is: ", "page_idx": 2, "bbox": [305, 460.10699462890625, 490, 473.2524719238281], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 31, "type": "image", "page_idx": 2, "img_path": "layout_images/2310.07609v1_3.jpg", "bbox": [325, 480, 505, 562], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "Claim = [@uNiy\n\nWe already know the following:\nTo verify the claim, what is the\nnext question we need to know the\nanswer to?\n", "vlm_text": "The image contains text related to verifying a claim. It presents a structure for analysis that includes:\n\n1. **Claim** labeled as \"CLAIM\"\n2. A statement, \"We already know the following:\" followed by \"CONTEXT\"\n3. A prompt asking, \"To verify the claim, what is the next question we need to know the answer to?\" \n\nThis layout suggests a focus on evaluating and analyzing claims through context and further questioning."}
{"layout": 32, "type": "text", "text": "3.3 Question Answering Model ", "text_level": 1, "page_idx": 2, "bbox": [305, 574, 459, 587], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 33, "type": "text", "text": "After generating a question, the Question Answer- ing (QA) module retrieves corresponding evidence and provides an answer as the output. The system’s reliability largely depends on the accuracy of the QA module’s responses. Understanding the need for different QA methods in various fact-checking scenarios, we introduce three different implemen- tations for the QA module, as shown in Figure  3 . ", "page_idx": 2, "bbox": [305, 590.8280029296875, 526, 698.8174438476562], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 34, "type": "text", "text": "Retriever–Reader. We first integrate the well- known  retriever–reader  framework, a prevalent QA paradigm originally introduced by  Chen et al. ( 2017 ). In this framework, a  retriever  first re- trieves relevant documents from a large evidence ", "page_idx": 2, "bbox": [305, 706.3543090820312, 526, 774.0894775390625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 35, "type": "image", "page_idx": 3, "img_path": "layout_images/2310.07609v1_4.jpg", "img_caption": "Figure 3: Illustrations of the three different implementa- tions of the Question Answering module in QAC HECK . ", "bbox": [70, 71, 291, 420], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "(2) (a) Retriever-Reader\n\nQuestion\n\nWikipedia a\n<Retrieved Evidence>\n\n‘ we] » Vd — Q: <Question>\n\nRetriever The answer is:\n\n@ Reader\nAnswer\n\n(b) FLAN-T5S Q: <Question>\ne-\n\nThe answer is:\n\nQuestion\n\nRetrieve a Wikipedia\n—— article relevant to\nthis question.\nInstructGPT\n\n2.\n", "vlm_text": "The image depicts three different implementations of a Question Answering (QA) module in a system referred to as QAC HECK. Here's a breakdown:\n\n1. **(a) Retriever–Reader:**\n   - The process begins with a question input.\n   - A retriever searches Wikipedia for relevant evidence.\n   - The retrieved evidence and the question are passed to a reader.\n   - The reader provides the answer.\n\n2. **(b) FLAN-T5:**\n   - The question is directly inputted into the FLAN-T5 model.\n   - The model generates an answer.\n\n3. **(c) GPT Reciter–Reader:**\n   - The question is inputted into InstructGPT.\n   - InstructGPT retrieves a relevant Wikipedia article.\n   - The retrieved information is sent to a reader.\n   - The reader outputs the answer.\n\nEach implementation depicts a different approach to answering questions using various methods and technologies."}
{"layout": 36, "type": "text", "text": "corpus, and then a  reader  predicts an answer con- ditioned on the retrieved documents. For the ev- idence corpus, we use the Wikipedia dump pro- vided by the Knowledge-Intensive Language Tasks (KILT) benchmark ( Petroni et al. ,  2021 ), in which the Wikipedia articles have been pre-processed and separated into paragraphs. For the retriever, we apply the widely-used sparse retrieval based on BM25 ( Robertson and Zaragoza ,  2009 ), imple- mented with the Pyserini toolkit ( Lin et al. ,  2021 ). For the reader, we use the  RoBERTa-large  ( Liu et al. ,  2019 ) model fine-tuned on the SQuAD dataset ( Rajpurkar et al. ,  2016 ), using the imple- mentation from  Prime  $Q A^{5}$    ( Sil et al. ,  2023 ). ", "page_idx": 3, "bbox": [70, 444.17401123046875, 291, 633.45947265625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 37, "type": "text", "text": "FLAN-T5. While effective, the retriever–reader framework is constrained by its reliance on the ev- idence corpus. In scenarios where a user’s claim is outside the scope of Wikipedia, the system might fail to produce a credible response. To en- hance flexibility, we also incorporate the  FLAN-T5 model ( Chung et al. ,  2022 ), a Seq2Seq model pre- trained on more than 1.8K tasks with instruction tuning. It directly takes the question as input and then generates the answer and the evidence, based on the model’s parametric knowledge. ", "page_idx": 3, "bbox": [70, 644.5162963867188, 291, 752.8994140625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 38, "type": "text", "text": "", "page_idx": 3, "bbox": [305, 71.74500274658203, 526, 111.98947143554688], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 39, "type": "text", "text": "GPT Reciter–Reader. Recent studies ( Sun et al. , 2023 ;  Yu et al. ,  2023 ) have demonstrated the great potential of the GPT series, such as Instruct- GPT ( Ouyang et al. ,  2022 ) and GPT-4 ( OpenAI , 2023 ), to function as robust knowledge reposito- ries. The knowledge can be retrieved by properly prompting the model. Drawing from this insight, we introduce the  GPT Reciter–Reader  approach. Given a question, we prompt the Instruct GP T to “recite”  the knowledge stored within it, and Instruct- GPT responds with relevant evidence. The evi- dence is then fed into a  reader  model to produce the corresponding answer. While this method, like FLAN-T5, does not rely on a specific corpus, it stands out by using Instruct GP T. This offers a more dependable parametric knowledge base than FLAN-T5. ", "page_idx": 3, "bbox": [305, 119.19425964355469, 526, 349.51947021484375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 40, "type": "text", "text": "The above three methods provide a flexible and robust QA module, allowing for switching between the methods as required, depending on the claim being verified and the available contextual informa- tion. In the following, we use GPT Reciter–Reader as the default implementation for our QA module. ", "page_idx": 3, "bbox": [305, 352.75799560546875, 526, 433.64947509765625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 41, "type": "text", "text": "3.4 QA Validator ", "text_level": 1, "page_idx": 3, "bbox": [306, 443, 394, 455], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 42, "type": "text", "text": "The validator module ensures the usefulness of the newly-generated QA pairs. For a QA pair to be valid, it must satisfy two criteria: 1) it brings addi- tional information to the current context    $\\mathcal{C}$  , and 2) it is useful for verifying the original claim. We again implement the validator by prompting Instruct GP T with a suite of ten demonstrations shown in Ap- pendix  A.3 . The instruction is as follows: ", "page_idx": 3, "bbox": [305, 460.1969909667969, 526, 568.1864013671875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 43, "type": "image", "page_idx": 3, "img_path": "layout_images/2310.07609v1_5.jpg", "bbox": [326, 574, 502, 683], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "Claim = [@mN\n\nWe already know the following:\n\nNow we further know:\n\nDoes the QA pair have additional\nknowledge useful for verifying\nthe claim?\n", "vlm_text": "The image contains text related to verifying a claim. It includes highlighted terms such as \"CLAIM,\" \"CONTEXT,\" and \"NEW QA PAIR,\" and asks whether the QA pair provides additional knowledge useful for verifying the claim."}
{"layout": 44, "type": "text", "text": "The validator acts as a safeguard against the system producing redundant or irrelevant QA pairs. Upon validation of a QA pair, it is added to the current context    $\\mathcal{C}$  . Subsequently, the system initiates an- other cycle of calling the claim verifier, question generator, question answering, and validation. ", "page_idx": 3, "bbox": [305, 693.197998046875, 526, 774.0894775390625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 45, "type": "image", "page_idx": 4, "img_path": "layout_images/2310.07609v1_6.jpg", "img_caption": "Figure 4: The screenshot of the QAC HECK  user interface showing its key annotated functions. First, users have the option to  select a claim  or  manually input  a claim that requires verification. Second, users can start the verification process by clicking the  Submit  button. Third, the system shows a step-by-step question-answering guided reasoning process. Each step includes the  reasoning depth , the  generated question , relevant retrieved  evidence , and the corresponding predicted  answer . Finally, it presents the final prediction  label  with the supporting  rationale . ", "bbox": [70, 73, 527, 614], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "QACheck: Question-Guided Multi-hop Fact-Checking Demo\ndesigned by\n\nInstructions: Select a claim or just enter your own claim otherwise, and then check the model's output.\n\n1. Select or input a custom claim\nQA Model:\n\n| GPT Reciter-Reader gi}\n\n© Please select a claim.\n\n[Ulrich Walter's employer is headquartered in Cologne.\n\n2. Submit to fact-check the input claim\n\nInput Claim:\n\nLars Onsager won the Nobel prize when he was 30 years old.\n\n3. Visualize the question-answering guided reasoning process\n\nQuestion Answering Decomposition:\n\nP> Reasoning depth: 0\n\n(>) Generated Question: In which year did Lars Onsager win the Nobel prize? The Nobel Prize in Chemistry 1968 was awarded to Lars\nOnsager for the discovery of the reciprocal relations\n\n@ Predicted Answer: 1968 bearing his name, which are fundamental for the\nthermodynamics of irreversible proces\n\nD> Reasoning depth\n\n(2) Generated Question: Which year was Lars Onsager born? Lars Onsager (27 November 1903~ 5 October 1976) was a\nNorwegian-American theoretical physicist and phys\n\n@ Predicted Answer: 1903 chemist.\n\n4. The final prediction result with rationale\n\nPrediction with rationale:\n\nLars Onsager won the Nobel prize in 1968. Lars Onsager was born in 1903. He was 65 when he won the Nobel prize. Therefore, the final answer is: False.\n\n", "vlm_text": "The image shows a user interface of a fact-checking demo called \"QACheck: Question-Guided Multi-hop Fact-Checking Demo.\" It's annotated to highlight its key functions:\n\n1. Users can either select a predefined claim or input a custom claim that they want to verify.\n2. To begin the verification process, users click the \"Submit\" button.\n3. The system then visualizes a step-by-step reasoning process for answering the query. This includes:\n   - Reasoning depth\n   - Generated question\n   - Relevant retrieved evidence\n   - Predicted answer\n4. Finally, it displays the prediction result with supporting rationale. In the example, the claim about Lars Onsager winning the Nobel prize at 30 is checked and found to be false, with an explanation provided."}
{"layout": 46, "type": "text", "text": "3.5 Reasoner ", "text_level": 1, "page_idx": 4, "bbox": [70, 634, 139, 647], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 47, "type": "text", "text": "The reasoner is called when the claim verifier deter- mines that the context  $\\mathcal{C}$   is sufficient to verify the claim or the system hits the maximum allowed iter- ations, set to 5 by default. The reasoner is a special question-answering model which takes the context  $\\mathcal{C}$   and the claim    $c$   as inputs and then answers the question  “Is the claim true or false?” . The model is also requested to output the rationale with the prediction. We provide two different implementa- tions for the reasoner: 1) the end-to-end QA model based on FLAN-T5, and 2) the Instruct GP T model with the prompts given in Appendix  A.4 . ", "page_idx": 4, "bbox": [70, 652.5499877929688, 290, 774.0894775390625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 48, "type": "text", "text": "", "page_idx": 4, "bbox": [305, 634.7990112304688, 526, 675.04248046875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 49, "type": "text", "text": "4 Performance Evaluation ", "text_level": 1, "page_idx": 4, "bbox": [306, 685, 448, 698], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 50, "type": "text", "text": "To evaluate the performance of our QAC HECK , we use two fact-checking datasets that contain complex claims requiring multi-step reasoning: HOVER ( Jiang et al. ,  2020 ) and FEVEROUS ( Aly et al. ,  2021 ), following the same experimental set- ", "page_idx": 4, "bbox": [305, 706.7470092773438, 526, 774.0894775390625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 51, "type": "table", "page_idx": 5, "img_path": "layout_images/2310.07609v1_7.jpg", "table_footnote": "Table 1: Evaluation of F1 scores for different models. The bold text shows the best results for each setting. ", "bbox": [70, 69, 291, 233], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "Model FEVEROUS\n\none 2-hop 3-hop 4-hop\nInstructGPT\n\n- Direct | 56.51 51.75 49.68 60.13\n\n- CoT 57.20 53.66 51.83 61.05\nCodex 55.57 53.42 45.59 57.85\nFLAN-T5 48.27 52.11 51.13 55.16\nProgramFC 54.27 54.18 52.88 59.66\nQACheck 55.67 54.67 52.35 59.47\n", "vlm_text": "The table presents the performance of different AI models on two datasets: HOVER and FEVEROUS. The metrics provided are likely accuracy or F1 scores, measured in percentage, over various tasks:\n\n1. **HOVER Task Types**:\n   - The tasks are divided into 2-hop, 3-hop, and 4-hop questions. These terms likely refer to the complexity or steps involved in reasoning required to answer the questions correctly.\n\n2. **Models and Their Performance**:\n   - **InstructGPT Direct**:\n     - 2-hop: 56.51\n     - 3-hop: 51.75\n     - 4-hop: 49.68\n     - FEVEROUS: 60.13\n   - **InstructGPT CoT** (Chain of Thought):\n     - 2-hop: 57.20\n     - 3-hop: 53.66\n     - 4-hop: 51.83\n     - FEVEROUS: 61.05\n   - **Codex**:\n     - 2-hop: 55.57\n     - 3-hop: 53.42\n     - 4-hop: 45.59\n     - FEVEROUS: 57.85\n   - **FLAN-T5**:\n     - 2-hop: 48.27\n     - 3-hop: 52.11\n     - 4-hop: 51.13\n     - FEVEROUS: 55.16\n   - **ProgramFC**:\n     - 2-hop: 54.27\n     - 3-hop: 54.18\n     - 4-hop: 52.88\n     - FEVEROUS: 59.66\n\n3. **QAcheck Model**:\n   - For HOVER:\n     - 2-hop: 55.67\n     - 3-hop: 54.67\n     - 4-hop: 52.35\n   - FEVEROUS: 59.47\n\nOverall, InstructGPT CoT appears to perform the best among the models listed, with the highest scores for the 2-hop and 3-hop HOVER tasks and the FEVEROUS dataset."}
{"layout": 52, "type": "text", "text": "tings used in  Pan et al.  ( 2023 ). HOVER con- tains 1,126 two-hop claims, 1,835 three-hop claims, and 1,039 four-hop claims, while FEVEROUS has 2,962 multi-hop claims. We compare our method with the baselines of directly applying Instruct GP T with two different prompting methods: ( i )  direct prompting with the claim, and   $(i i)$   CoT  ( Wei et al. , 2022 ) or chain-of-thought prompting with few- shot demonstrations of reasoning explanations. We also compare with  ProgramFC  ( Pan et al. ,  2023 ), FLAN-T5  ( Chung et al. ,  2022 ), and  Codex  ( Chen et al. ,  2021 ). We use the reported results for the baseline models from  Pan et al.  ( 2023 ). ", "page_idx": 5, "bbox": [70, 253.42298889160156, 291, 429.15948486328125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 53, "type": "text", "text": "The evaluation results are shown in Table  1 . Our QAC HECK  system achieves a macro-F1 score of 55.67, 54.67, and 52.35 on HOVER two-hop, three- hop, and four-hop claims, respectively. It achieves a 59.47 F1 score on FEVEROUS. These scores are better than directly using Instruct GP T, Codex, or FLAN-T5. They are also on par with the systems that apply claim decomposition strategies,  i.e. ,  CoT , and  ProgramFC . The results demonstrate the effec- tiveness of our QAC HECK  system. Especially, the QAC HECK  has better improvement over the end- to-end models on claims with high reasoning depth. This indicates that decomposing a complex claim into simpler steps with question-guided reasoning can facilitate more accurate reasoning. ", "page_idx": 5, "bbox": [70, 430.4840087890625, 291, 633.3184814453125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 54, "type": "text", "text": "5 User Interface ", "text_level": 1, "page_idx": 5, "bbox": [70, 646, 162, 660], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 55, "type": "text", "text": "We create a demo system based on Flask 6   for ver- ifying open-domain claims with QAC HECK , as shown in Figure  4 . The QAC HECK  demo is de- signed to be intuitive and user-friendly, enabling users to input any claim or select from a list of pre-defined claims (top half of Figure  4 ). ", "page_idx": 5, "bbox": [70, 666.364990234375, 291, 751.2154541015625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 56, "type": "text", "text": "Upon selecting or inputting a claim, the user can start the fact-checking process by clicking the “Submit”  button. The bottom half of Figure  4  shows a snapshot of QAC HECK ’s output for the input claim  “Lars Onsager won the Nobel prize when he was 30 years old” . The system visualizes the detailed question-guided reasoning process. For each reasoning step, the system shows the index of the reasoning step, the generated question, and the predicted answer to the question. The retrieved evidence to support the answer is shown on the right for each step. The system then shows the final veracity prediction for the original claim accom- panied by a comprehensive rationale in the  “Pre- diction with rationale”  section. This step-by-step illustration not only enhances the understanding of our system’s fact-checking process but also offers transparency to its functioning. ", "page_idx": 5, "bbox": [305, 71.74500274658203, 526, 315.2274475097656], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 57, "type": "text", "text": "QAC HECK  also allows users to change the un- derlying question–answering model. As shown at the top of Figure  4 , users can select between the three different QA models introduced in Sec- tion  3.3 , depending on their specific requirements or preferences. Our demo system will be open- sourced under the Apache-2.0 license. ", "page_idx": 5, "bbox": [305, 316.60101318359375, 526, 411.0414733886719], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 58, "type": "text", "text": "6 Conclusion and Future Works ", "text_level": 1, "page_idx": 5, "bbox": [306, 424, 479, 438], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 59, "type": "text", "text": "This paper presents the QAC HECK  system, a novel approach designed for verifying real-world com- plex claims. QAC HECK  conducts the reasoning process with the guidance of asking and answer- ing a series of questions and answers. Specifically, QAC HECK  iterative ly generates con textually rel- evant questions, retrieves and validates answers, judges the sufficiency of the context information, and finally, reasons out the claim’s truth value based on the accumulated knowledge. QAC HECK leverages a wide range of techniques, such as in- context learning, document retrieval, and question- answering, to ensure a precise, transparent, explain- able, and user-friendly fact-checking process. ", "page_idx": 5, "bbox": [305, 448.3420104980469, 526, 637.62646484375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 60, "type": "text", "text": "In the future, we plan to enhance QAC HECK 1) by integrating additional knowledge bases to further improve the breadth and depth of informa- tion accessible to the system ( Feng et al. ,  2023 ; Kim et al. ,  2023 ), and 2) by incorporating a multi- modal interface to support image ( Chakra bor ty et al. ,  2023 ), table ( Chen et al. ,  2020 ;  Lu et al. , 2023 ), and chart-based fact-checking ( Akhtar et al. , 2023 ), which can broaden the system’s utility in processing and analyzing different forms of data. ", "page_idx": 5, "bbox": [305, 639.0009765625, 526, 774.0894775390625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 61, "type": "text", "text": "Limitations ", "text_level": 1, "page_idx": 6, "bbox": [70, 71, 130, 83], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 62, "type": "text", "text": "We identify two main limitations of QAC HECK . First, several modules of our QAC HECK  currently utilize external API-based large language models, such as Instruct GP T. This reliance on external APIs tends to prolong the response time of our system. As a remedy, we are considering the integration of open-source, locally-run large language models like LLaMA ( Touvron et al. ,  2023 ). Secondly, the current scope of our QAC HECK  is confined to eval- uating  True/False  claims. Recognizing the signifi- cance of also addressing  Not Enough Information claims, we plan to devise strategies to incorporate these in upcoming versions of the system. ", "page_idx": 6, "bbox": [70, 92.23200225830078, 291, 267.9684753417969], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 63, "type": "text", "text": "Ethics Statement ", "text_level": 1, "page_idx": 6, "bbox": [70, 278, 158, 291], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 64, "type": "text", "text": "The use of large language models requires a signifi- cant amount of energy for computation for training, which contributes to global warming. Our work performs few-shot in-context learning instead of training models from scratch, so the energy foot- print of our work is less. The large language model (Instruct GP T) whose API we use for inference con- sumes significant energy. ", "page_idx": 6, "bbox": [70, 298.6500244140625, 291, 406.6404724121094], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 65, "type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 6, "bbox": [70, 417, 165, 429], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 66, "type": "text", "text": "This project is supported by the Ministry of Edu- cation, Singapore, under its MOE AcRF TIER 3 Grant (MOE-MOET32022-0001). The computa- tional work for this article was partially performed on resources of the National Super computing Cen- tre, Singapore. ", "page_idx": 6, "bbox": [70, 437.3219909667969, 291, 518.2134399414062], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 67, "type": "text", "text": "References ", "text_level": 1, "page_idx": 6, "bbox": [70, 541, 126, 553], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 68, "type": "text", "text": "Naser Ahmadi, Joohyung Lee, Paolo Papotti, and Mo- hammed Saeed. 2019.  Explain able fact checking with probabilistic answer set programming . In  Pro- ceedings of the 2019 Truth and Trust Online Confer- ence (TTO) . ", "page_idx": 6, "bbox": [70, 559.3115844726562, 291, 615.1525268554688], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 69, "type": "text", "text": "Mubashara Akhtar, Oana Cocarascu, and Elena Simperl. 2023.  Reading and reasoning over chart images for evidence-based automated fact-checking . In  Find- ings of the Association for Computational Linguistics (EACL) , pages 399–414. ", "page_idx": 6, "bbox": [70, 622.2085571289062, 291, 678.0494995117188], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 70, "type": "text", "text": "Rami Aly, Zhijiang Guo, Michael Sejr Sch licht kru ll, James Thorne, Andreas Vlachos, Christos Christo dou lo poul os, Oana Cocarascu, and Arpit Mittal. 2021. FEVEROUS: Fact Extraction and VERification Over Unstructured and Structured in- formation . In  Proceedings of the Neural Information Processing Systems (NeurIPS) Track on Datasets and Benchmarks . ", "page_idx": 6, "bbox": [70, 685.1065673828125, 291, 773.8235473632812], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 71, "type": "text", "text": "Rami Aly and Andreas Vlachos. 2022.  Natural logic- guided auto regressive multi-hop document retrieval for fact verification . In  Proceedings of the 2022 Con- ference on Empirical Methods in Natural Language Processing (EMNLP) , pages 6123–6135. ", "page_idx": 6, "bbox": [306, 72.61956787109375, 526, 128.4604949951172], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 72, "type": "text", "text": "Megha Chakra bor ty, Khusbu Pahwa, Anku Rani, Adarsh Mahor, Aditya Pakala, Arghya Sarkar, Harshit Dave, Ishan Paul, Janvita Reddy, Preethi Gu- rumurthy, Ritvik G, Samahriti Mukherjee, Shreyas Chatterjee, Kinjal Sensharma, Dwip Dalal, Suryavar- dan S, Shreyash Mishra, Parth Patwa, Aman Chadha, Amit P. Sheth, and Amitava Das. 2023. FAC- TIFY3M: A benchmark for multimodal fact ver- ification with explain ability through 5w question- answering .  CoRR , abs/2306.05523. ", "page_idx": 6, "bbox": [306, 138.17156982421875, 526, 248.80650329589844], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 73, "type": "text", "text": "Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. 2017.  Reading wikipedia to answer open- domain questions . In  Proceedings of the 55th Annual Meeting of the Association for Computational Lin- guistics (ACL) , pages 1870–1879. ", "page_idx": 6, "bbox": [306, 258.517578125, 526, 314.3575439453125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 74, "type": "text", "text": "Jifan Chen, Aniruddh Sriram, Eunsol Choi, and Greg Durrett. 2022.  Generating literal and implied sub- questions to fact-check complex claims . In  Proceed- ings of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pages 3495–3516. ", "page_idx": 6, "bbox": [306, 324.0685729980469, 526, 390.8675231933594], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 75, "type": "text", "text": "Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sas- try, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cum- mings, Matthias Plappert, Fotios Chantzis, Eliza- beth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021.  Evaluat- ing large language models trained on code .  ArXiv preprint , abs/2107.03374. ", "page_idx": 6, "bbox": [306, 400.5785827636719, 526, 631.7615356445312], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 76, "type": "text", "text": "Wenhu Chen, Hongmin Wang, Jianshu Chen, Yunkai Zhang, Hong Wang, Shiyang Li, Xiyou Zhou, and William Yang Wang. 2020.  Tabfact: A large-scale dataset for table-based fact verification . In  Proceed- ings of 8th International Conference on Learning Representations (ICLR) . ", "page_idx": 6, "bbox": [306, 641.4725341796875, 526, 708.2725219726562], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 77, "type": "text", "text": "Anton Cher nya v ski y, Dmitry Ilvovsky, and Preslav Nakov. 2021. What the wiki fact: Fact-checking claims against wikipedia . In  Proceedings of the 30th ACM International Conference on Information and Knowledge Management (CIKM) , pages 4690–4695. ", "page_idx": 6, "bbox": [306, 717.9825439453125, 526, 773.8235473632812], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 78, "type": "text", "text": "Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Web- son, Shixiang Shane Gu, Zhuyun Dai, Mirac Suz- gun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Y. Zhao, Yanping Huang, Andrew M. Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. 2022.  Scaling instruction-finetuned language models . CoRR , abs/2210.11416. ", "page_idx": 7, "bbox": [70, 72.61956787109375, 290, 194.21348571777344], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 79, "type": "text", "text": "Shangbin Feng, Vidhisha Bala chandra n, Yuyang Bai, and Yulia Tsvetkov. 2023.  Factkb: General iz able fac- tuality evaluation using language models enhanced with factual knowledge .  CoRR , abs/2305.08281. ", "page_idx": 7, "bbox": [70, 201.55657958984375, 290, 246.4385223388672], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 80, "type": "text", "text": "Zhijiang Guo, Michael Sejr Sch licht kru ll, and Andreas Vlachos. 2022.  A survey on automated fact-checking . Transactions of the Association for Computational Linguistics (TACL) , 10:178–206. ", "page_idx": 7, "bbox": [70, 253.78155517578125, 290, 298.66253662109375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 81, "type": "text", "text": "Yichen Jiang, Shikha Bordia, Zheng Zhong, Charles Dognin, Maneesh Singh, and Mohit Bansal. 2020. HoVer: A dataset for many-hop fact extraction and claim verification . In  Findings of the Association for Computational Linguistics (EMNLP) , pages 3441– 3460. ", "page_idx": 7, "bbox": [70, 306.00555419921875, 290, 372.8055114746094], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 82, "type": "text", "text": "Shailza Jolly, Pepa Atanasova, and Isabelle Augenstein. 2022.  Generating fluent fact checking explanations with unsupervised post-editing .  Information , 13:500. ", "page_idx": 7, "bbox": [70, 380.1475830078125, 290, 414.0705261230469], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 83, "type": "text", "text": "Jiho Kim, Sungjin Park, Yeonsu Kwon, Yohan Jo, James Thorne, and Edward Choi. 2023.  Factkg: Fact veri- fication via reasoning on knowledge graphs . In  Pro- ceedings of the 61st Annual Meeting of the Associ- ation for Computational Linguistics (ACL) , pages 16190–16206. ", "page_idx": 7, "bbox": [70, 421.41357421875, 290, 488.2135009765625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 84, "type": "text", "text": "Neema Kotonya and Francesca Toni. 2020.  Explain able automated fact-checking for public health claims . In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pages 7740–7754. ", "page_idx": 7, "bbox": [70, 495.5555725097656, 290, 551.3965454101562], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 85, "type": "text", "text": "Jimmy Lin, Xueguang Ma, Sheng-Chieh Lin, Jheng- Hong Yang, Ronak Pradeep, and Rodrigo Frassetto Nogueira. 2021.  Pyserini: A python toolkit for re- producible information retrieval research with sparse and dense representations . In  Proceedings of Inter- national ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR) , pages 2356–2362.", "page_idx": 7, "bbox": [70, 558.7395629882812, 290, 647.4564819335938], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 86, "type": "text", "text": "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man- dar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Z ett le moyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized BERT pre training approach .  CoRR , abs/1907.11692. ", "page_idx": 7, "bbox": [70, 654.799560546875, 290, 710.6395263671875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 87, "type": "text", "text": "Zhenghao Liu, Chenyan Xiong, Maosong Sun, and Zhiyuan Liu. 2020.  Fine-grained fact verification with kernel graph attention network . In  Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL) , pages 7342–7351. ", "page_idx": 7, "bbox": [70, 717.9825439453125, 290, 773.8235473632812], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 88, "type": "text", "text": "Xinyuan Lu, Liangming Pan, Qian Liu, Preslav Nakov, and Min-Yen Kan. 2023. SCITAB: A challeng- ing benchmark for compositional reasoning and claim verification on scientific tables . CoRR , abs/2305.13186. ", "page_idx": 7, "bbox": [306, 72.61956787109375, 526, 128.4604949951172], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 89, "type": "text", "text": "Yi-Ju Lu and Cheng-Te Li. 2020.  GCAN: graph-aware co-attention networks for explain able fake news de- tection on social media . In  Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL) , pages 505–514. ", "page_idx": 7, "bbox": [306, 136.8515625, 526, 192.69248962402344], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 90, "type": "text", "text": "Giovanni Da San Martino, Shaden Shaar, Yifan Zhang, Seunghak Yu, Alberto Barrón-Cedeño, and Preslav Nakov. 2020.  Prta: A system to support the analysis of propaganda techniques in the news . In  Proceed- ings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations (ACL) , pages 287–293. ", "page_idx": 7, "bbox": [306, 201.08355712890625, 526, 278.842529296875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 91, "type": "text", "text": "Van-Hoang Nguyen, Kazunari Sugiyama, Preslav Nakov, and Min-Yen Kan. 2020.  FANG: leveraging social context for fake news detection using graph representation . In  Proceedings of the 29th ACM Inter- national Conference on Information and Knowledge Management (CIKM) , pages 1165–1174. ", "page_idx": 7, "bbox": [306, 287.23358154296875, 526, 354.03350830078125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 92, "type": "text", "text": "OpenAI. 2023. GPT-4 technical report . CoRR , abs/2303.08774. ", "page_idx": 7, "bbox": [306, 362.424560546875, 526, 385.3885192871094], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 93, "type": "text", "text": "Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welin- der, Paul F. Christiano, Jan Leike, and Ryan Lowe. 2022.  Training language models to follow instruc- tions with human feedback . In  Proceedings of the 36th Conference on Neural Information Processing Systems (NeurIPS) . ", "page_idx": 7, "bbox": [306, 393.7795715332031, 526, 504.4145202636719], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 94, "type": "text", "text": "Liangming Pan, Xiaobao Wu, Xinyuan Lu, Anh Tuan Luu, William Yang Wang, Min-Yen Kan, and Preslav Nakov. 2023.  Fact-checking complex claims with program-guided reasoning . In  Proceedings of the 61st Annual Meeting of the Association for Computa- tional Linguistics (ACL) , pages 6981–7004. ", "page_idx": 7, "bbox": [306, 512.8065795898438, 526, 579.6055297851562], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 95, "type": "text", "text": "Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick S. H. Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard, Vassilis Plachouras, Tim Rock t s chel, and Sebastian Riedel. 2021. KILT: a benchmark for knowledge intensive language tasks . In  Proceed- ings of the 2021 Conference of the North American Chapter of the Association for Computational Lin- guistics: Human Language Technologies (NAACL- HLT) , pages 2523–2544. ", "page_idx": 7, "bbox": [306, 587.99755859375, 526, 698.6325073242188], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 96, "type": "text", "text": "Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,   ${000+}$   questions for machine comprehension of text . In  Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pages 2383– 2392. ", "page_idx": 7, "bbox": [306, 707.0235595703125, 526, 773.8235473632812], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 97, "type": "text", "text": "Stephen E. Robertson and Hugo Zaragoza. 2009.  The probabilistic relevance framework: BM25 and be- yond .  Journal of Foundations and Trends in Informa- tion Retrieval , 3(4):333–389. ", "page_idx": 8, "bbox": [70, 72.61956787109375, 290, 117.50151824951172], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 98, "type": "text", "text": "Chris Samarinas, Wynne Hsu, and Mong-Li Lee. 2021. Improving evidence retrieval for automated explain- able fact-checking . In  Proceedings of the 2021 Con- ference of the North American Chapter of the As- sociation for Computational Linguistics: Human Language Technologies: System Demonstrations, (NAACL-HLT) , pages 84–91. ", "page_idx": 8, "bbox": [70, 125.892578125, 290, 203.65147399902344], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 99, "type": "text", "text": "Natural Language Processing: System Demonstra- tions (EMNLP-IJCNLP) , pages 223–228. ", "page_idx": 8, "bbox": [316, 72.61956787109375, 526, 95.58348846435547], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 100, "type": "text", "text": "Wanjun Zhong, Jingjing Xu, Duyu Tang, Zenan Xu, Nan Duan, Ming Zhou, Jiahai Wang, and Jian Yin. 2020.  Reasoning over semantic-level graph for fact checking . In  Proceedings of the 58th Annual Meet- ing of the Association for Computational Linguistics (ACL) , pages 6170–6180. ", "page_idx": 8, "bbox": [306.1419982910156, 103.5035400390625, 526, 170.3035125732422], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 101, "type": "text", "text": "Avi Sil, Jaydeep Sen, Bhavani Iyer, Martin Franz, Kshi- tij Fadnis, Mihaela Bornea, Sara Rosenthal, J. Scott McCarley, Rong Zhang, Vishwajeet Kumar, Yulong Li, Md. Arafat Sultan, Riyaz Bhat, Jürgen Broß, Radu Florian, and Salim Roukos. 2023.  Primeqa: The prime repository for state-of-the-art multilingual question answering research and development . In Proceedings of the 61st Annual Meeting of the Associ- ation for Computational Linguistics: System Demon- strations (ACL) , pages 51–62. ", "page_idx": 8, "bbox": [70, 212.04254150390625, 290, 322.677490234375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 102, "type": "text", "text": "Zhiqing Sun, Xuezhi Wang, Yi Tay, Yiming Yang, and Denny Zhou. 2023.  Recitation-augmented language models . In  Proceedings of the 11th International Conference on Learning Representations (ICLR) . ", "page_idx": 8, "bbox": [70, 331.069580078125, 290, 375.95050048828125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 103, "type": "text", "text": "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurélien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023.  Llama: Open and efficient foundation language models .  CoRR , abs/2302.13971. ", "page_idx": 8, "bbox": [70, 384.3425598144531, 290, 462.10052490234375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 104, "type": "text", "text": "Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed H. Chi, Quoc Le, and Denny Zhou. 2022. Chain of thought prompting elicits reasoning in large language models .  CoRR , abs/2201.11903. ", "page_idx": 8, "bbox": [70, 470.4925842285156, 290, 515.3734741210938], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 105, "type": "text", "text": "Rui Xing, Shraey Bhatia, Timothy Baldwin, and Jey Han Lau. 2022.  Automatic explanation genera- tion for climate science claims . In  Proceedings of the The 20th Annual Workshop of the Australasian Lan- guage Technology Association (ALTA) , pages 122– 129. ", "page_idx": 8, "bbox": [70, 523.7655639648438, 290, 590.5645141601562], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 106, "type": "text", "text": "Wenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu, Mingxuan Ju, Soumya Sanyal, Chenguang Zhu, Michael Zeng, and Meng Jiang. 2023. Generate rather than retrieve: Large language models are strong context generators . In  Proceedings of the 11th International Conference on Learning Representa- tions (ICLR) . ", "page_idx": 8, "bbox": [70, 598.95654296875, 290, 676.7145385742188], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 107, "type": "text", "text": "Yifan Zhang, Giovanni Da San Martino, Alberto Barrón- Cedeño, Salvatore Romeo, Jisun An, Haewoon Kwak, Todor Staykovski, Israa Jaradat, Georgi Karadzhov, Ramy Baly, Kareem Darwish, James R. Glass, and Preslav Nakov. 2019.  Tanbih: Get to know what you are reading . In  Proceedings of the 2019 Conference on Empirical Methods in Natural Language Process- ing and the 9th International Joint Conference on ", "page_idx": 8, "bbox": [70, 685.1065673828125, 290, 773.7139282226562], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 108, "type": "text", "text": "A Prompts ", "text_level": 1, "page_idx": 9, "bbox": [70, 72, 136, 84], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 109, "type": "text", "text": "A.1 Prompts for Claim Verifier ", "text_level": 1, "page_idx": 9, "bbox": [69, 93, 223, 105], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 110, "type": "text", "text": "Claim  $=$   Superdrag and Collective Soul are both rock bands. We already know the following: Question   $\\begin{array}{r l}{1}&{{}=}\\end{array}$   Is Superdrag a rock band? Answer   $\\begin{array}{r l}{1}&{{}=}\\end{array}$   Yes Can we know whether the claim is true or false now? Yes or no? Prediction    $=\\textsf{N o}$   , we cannot know. Claim  $=$   Superdrag and Collective Soul are both rock bands. We already know the following: Question   $\\begin{array}{r l}{1}&{{}=}\\end{array}$   Is Superdrag a rock band? Answer   $\\begin{array}{r l}{1}&{{}=}\\end{array}$   Yes Question   $\\begin{array}{r l}{^2}&{{}=}\\end{array}$  = Is Collective Soul a rock band? Answer   $\\begin{array}{r l}{^2}&{{}=}\\end{array}$  = Yes Can we know whether the claim is true or false now? Yes or no? Prediction    $=$   Yes , we can know.\n\n  $<\\!1\\,\\emptyset$   demonstrations in total >\n\n -------- Claim  $=$   [[CLAIM]] Claim  $=$   CLAIM We already know the following: [[ QA CONTEXTS ]] Can we know whether the claim is true or false now? Yes or no? Prediction    $=$  ", "page_idx": 9, "bbox": [70, 116, 251.83673095703125, 347.4060363769531], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 111, "type": "text", "text": "A.2 Prompts for Question Generation Prompts for the initial question generation ", "text_level": 1, "page_idx": 9, "bbox": [69, 367, 269, 398], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 112, "type": "text", "text": "Claim  $=$   Superdrag and Collective Soul are both rock bands. To verify the above claim , we can first ask a simple question: Question    $=$   Is Superdrag a rock band?\n\n  $<\\!1\\,\\emptyset$   demonstrations in total >\n\n -------- Claim  $=$   [[CLAIM]] To verify the above claim , we can first ask a simple question: Question  $=$  ", "page_idx": 9, "bbox": [70, 404.7945556640625, 238, 499.78802490234375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 113, "type": "text", "text": "Question  $=$   Is Collective Soul a rock band? Answer  $=$   Yes Does the QA pair have additional knowledge useful for verifying the claim? The answer: Yes\n\n  $<\\!1\\,\\emptyset$   demonstrations in total >\n\n -------- Claim    $=$   [[ CLAIM ]] We already know the following: [[ QA CONTEXTS ]] Now we further know: [[ NEW QA PAIR ]] Does the QA pair have additional knowledge useful for verifying the claim? The answer: ", "page_idx": 9, "bbox": [306, 75.82754516601562, 477, 202.6113739013672], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 114, "type": "text", "text": "A.4 Prompts for Reasoner ", "text_level": 1, "page_idx": 9, "bbox": [305, 222, 436, 236], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 115, "type": "text", "text": "Contexts: Q1: When Lars Onsager won the Nobel Prize? A1: 1968 Q2: When was Lars Onsager born? A2: 1903 Claim  $=$   Lars Onsager won the Nobel Prize when he was 30 years old. Is this claim true or false? Answer : Lars Onsager won the Nobel Prize in 1968. Lars Onsager was born in 1903. Therefore , the final answer is: False.\n\n  $<\\!1\\,\\emptyset$   demonstrations in total >\n\n -------- Contexts: [[ CONTEXTS ]] Claim    $=$   [[ CLAIM ]] Is this claim true or false? Answer : Therefore , the final answer is ", "page_idx": 9, "bbox": [306, 246.00155639648438, 481.53802490234375, 436.5453796386719], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 116, "type": "text", "text": "Prompts for the follow-up question generation ", "text_level": 1, "page_idx": 9, "bbox": [70, 510, 287, 524], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 117, "type": "text", "text": "Claim  $=$   Superdrag and Collective Soul are both rock bands. We already know the following: Question   $\\begin{array}{r l}{1}&{{}=}\\end{array}$   Is Superdrag a rock band? Answer   $\\begin{array}{r l}{1}&{{}=}\\end{array}$   Yes To verify the claim , what is the next question we need to know the answer to? Question   $^{2\\ }=$   Is Collective Soul a rock band?\n\n  $<\\!1\\,\\emptyset$   demonstrations in total >\n\n -------- Claim  $=$   [[CLAIM]] We already know the following: [[ QA CONTEXTS ]] To verify the claim , what is the next question we need to know the answer to? Question  [[ Q_INDEX ]] = ", "page_idx": 9, "bbox": [70, 530, 247.65171813964844, 681.39404296875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 118, "type": "text", "text": "A.3 Prompts for Validator ", "text_level": 1, "page_idx": 9, "bbox": [70, 701, 200, 714], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 119, "type": "text", "text": "Claim  $=$   Superdrag and Collective Soul are both rock bands. We already know the following: Question    $=$   Is Superdrag a rock band? Answer  $=$   Yes Now we further know: ", "page_idx": 9, "bbox": [70, 725, 238, 772.2584228515625], "page_size": [595.2760009765625, 841.8900146484375]}
