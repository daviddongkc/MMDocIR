{"layout": 0, "type": "text", "text": "The Dawn of LMMs: Preliminary Explorations with GPT-4V(ision) ", "text_level": 1, "page_idx": 0, "bbox": [138, 96, 474, 138], "page_size": [612.0, 792.0]}
{"layout": 1, "type": "text", "text": "Zhengyuan Yang ∗ , Linjie  $\\mathbf{Li^{*}}$  , Kevin Lin ∗ , Jianfeng Wang ∗ , Chung-Ching Lin ∗ , Zicheng Liu, Lijuan Wang ∗♠ Microsoft Corporation ", "text_level": 1, "page_idx": 0, "bbox": [138, 178, 477, 213], "page_size": [612.0, 792.0]}
{"layout": 2, "type": "text", "text": "∗ Core Contributor ♠ Project Lead ", "page_idx": 0, "bbox": [239.16799926757812, 226.00357055664062, 368.10125732421875, 237.03054809570312], "page_size": [612.0, 792.0]}
{"layout": 3, "type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0, "bbox": [283, 284, 328, 297], "page_size": [612.0, 792.0]}
{"layout": 4, "type": "text", "text": "Large multimodal models (LMMs) extend large language models (LLMs) with multi-sensory skills, such as visual understanding, to achieve stronger generic in- telligence. In this paper, we analyze the latest model, GPT-4V(ision) [ 99 – 101 ,  1 ] 1 , to deepen the understanding of LMMs. The analysis focuses on the intriguing tasks that GPT-4V can perform, containing test samples to probe the quality and genericity of GPT-4V’s capabilities, its supported inputs and working modes, and the effective ways to prompt the model. In our approach to exploring GPT-4V, we curate and organize a collection of carefully designed qualitative samples spanning a variety of domains and tasks. Observations from these samples demon- strate that GPT-4V’s unprecedented ability in processing arbitrarily interleaved multimodal inputs and the genericity of its capabilities together make GPT-4V a powerful multimodal generalist system. Furthermore, GPT-4V’s unique capability of understanding visual markers drawn on input images can give rise to new human- computer interaction methods such as visual referring prompting. We conclude the report with in-depth discussions on the emerging application scenarios and the fu- ture research directions for GPT-4V-based systems. We hope that this preliminary exploration will inspire future research on the next-generation multimodal task formulation, new ways to exploit and enhance LMMs to solve real-world problems, and gaining better understanding of multimodal foundation models. Finally, we acknowledge that the model under our study is solely the product of OpenAI’s innovative work, and they should be fully credited for its development. Please see the GPT-4V contributions paper [ 101 ] for the authorship and credit attribution: ", "page_idx": 0, "bbox": [143, 309.0755615234375, 469, 550.1715087890625], "page_size": [612.0, 792.0]}
{"layout": 5, "type": "text", "text": "Contents ", "text_level": 1, "page_idx": 0, "bbox": [107, 579, 155, 593], "page_size": [612.0, 792.0]}
{"layout": 6, "type": "text", "text": "List of Figures ", "text_level": 1, "page_idx": 0, "bbox": [107, 607, 171, 619], "page_size": [612.0, 792.0]}
{"layout": 7, "type": "text", "text": "2 GPT-4V’s Input Modes 10 ", "text_level": 1, "page_idx": 1, "bbox": [104, 72, 506, 85], "page_size": [612.0, 792.0]}
{"layout": 8, "type": "text", "text": "2.1 Text-only Inputs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n\n 2.2 Single Image-text Pair . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n\n 2.3 Interleaved Image-text Inputs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\n\n ", "page_idx": 1, "bbox": [122, 89.3245849609375, 504, 134.4645233154297], "page_size": [612.0, 792.0]}
{"layout": 9, "type": "text", "text": "3 GPT-4V’s Working Modes and Prompting Techniques 12 ", "text_level": 1, "page_idx": 1, "bbox": [105, 148, 506, 162], "page_size": [612.0, 792.0]}
{"layout": 10, "type": "text", "text": "3.1 Following Text Instructions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n\n 3.2 Visual Pointing and Visual Referring Prompting . . . . . . . . . . . . . . . . . . . 15\n\n 3.3 Visual  $^+$   Text Prompting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\n\n 3.4 In-context Few-shot Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\n\n ", "page_idx": 1, "bbox": [122, 166.2725830078125, 504, 227.9795379638672], "page_size": [612.0, 792.0]}
{"layout": 11, "type": "text", "text": "4 Vision-Language Capability 26 ", "text_level": 1, "page_idx": 1, "bbox": [105, 242, 507, 255], "page_size": [612.0, 792.0]}
{"layout": 12, "type": "text", "text": "4.1 Image Description on Diverse Domains . . . . . . . . . . . . . . . . . . . . . . . 26\n\n 4.2 Object Localization, Counting, and Dense Captioning . . . . . . . . . . . . . . . . 38\n\n 4.3 Multimodal Knowledge and Commonsense . . . . . . . . . . . . . . . . . . . . . 43\n\n 4.4 Scene Text, Table, Chart, and Document Reasoning . . . . . . . . . . . . . . . . . 49\n\n 4.5 Multilingual Multimodal Understanding . . . . . . . . . . . . . . . . . . . . . . . 58\n\n 4.6 Coding Capability with Vision . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63\n\n ", "page_idx": 1, "bbox": [122, 259.78759765625, 504, 354.62847900390625], "page_size": [612.0, 792.0]}
{"layout": 13, "type": "text", "text": "5 Interaction with Humans: Visual Referring Prompting 66 ", "text_level": 1, "page_idx": 1, "bbox": [105, 369, 505, 382], "page_size": [612.0, 792.0]}
{"layout": 14, "type": "text", "text": "5.1 Understand Pointing Inputs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66\n\n 5.2 Visual Referring Prompting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69\n\n 5.3 Generate Pointing Outputs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69\n\n ", "page_idx": 1, "bbox": [122, 386.4365234375, 504, 431.5764465332031], "page_size": [612.0, 792.0]}
{"layout": 15, "type": "text", "text": "6 Temporal and Video Understanding 73 ", "text_level": 1, "page_idx": 1, "bbox": [105, 446, 507, 459], "page_size": [612.0, 792.0]}
{"layout": 16, "type": "text", "text": "6.1 Multi-image Sequencing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73\n\n 6.2 Video Understanding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73\n\n 6.3 Visual Referring Prompting for Grounded Temporal Understanding . . . . . . . . . 79\n\n ", "page_idx": 1, "bbox": [122, 463.3844909667969, 504, 508.5244140625], "page_size": [612.0, 792.0]}
{"layout": 17, "type": "text", "text": "7 Abstract Visual Reasoning and Intelligence Quotient Test 80 ", "text_level": 1, "page_idx": 1, "bbox": [105, 523, 507, 535], "page_size": [612.0, 792.0]}
{"layout": 18, "type": "text", "text": "7.1 Abstract Visual Stimuli . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80\n\n 7.2 Discovery and Association of Parts and Objects . . . . . . . . . . . . . . . . . . . 80\n\n 7.3 Wechsler Adult Intelligence Scale . . . . . . . . . . . . . . . . . . . . . . . . . . 80\n\n 7.4 Raven’s Progressive Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80\n\n ", "page_idx": 1, "bbox": [122, 540.3314819335938, 504, 602.0394287109375], "page_size": [612.0, 792.0]}
{"layout": 19, "type": "text", "text": "8 Emotional Quotient Test 86 ", "text_level": 1, "page_idx": 1, "bbox": [106, 616, 507, 629], "page_size": [612.0, 792.0]}
{"layout": 20, "type": "text", "text": "8.1 Read Emotion from Facial Expressions . . . . . . . . . . . . . . . . . . . . . . . 86\n\n 8.2 Understand How Visual Content Arouses Emotions . . . . . . . . . . . . . . . . . 87\n\n 8.3 Emotion Conditioned Output . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89\n\n ", "page_idx": 1, "bbox": [122, 633.8464965820312, 504, 678.9864501953125], "page_size": [612.0, 792.0]}
{"layout": 21, "type": "text", "text": "9 Emerging Application Highlights 90 ", "text_level": 1, "page_idx": 1, "bbox": [105, 693, 506, 706], "page_size": [612.0, 792.0]}
{"layout": 22, "type": "text", "text": "9.1 Spot the Difference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90 9.2 Industry . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90\n\n 9.3 Medical . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99\n\n 9.4 Auto Insurance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104\n\n 9.5 Customized Captioner . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107\n\n 9.6 Image Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111\n\n 9.7 Embodied Agent . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 116\n\n 9.8 GUI Navigation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121\n\n ", "page_idx": 1, "bbox": [122, 710.7944946289062, 504, 722.7994384765625], "page_size": [612.0, 792.0]}
{"layout": 23, "type": "text", "text": "", "page_idx": 2, "bbox": [122, 72.757568359375, 504, 183.0934295654297], "page_size": [612.0, 792.0]}
{"layout": 24, "type": "text", "text": "10 LMM Powered Agents 150 ", "text_level": 1, "page_idx": 2, "bbox": [106, 196, 506, 209], "page_size": [612.0, 792.0]}
{"layout": 25, "type": "text", "text": "10.1 Multimodal Plugins . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152\n\n 10.2 Multimodal Chains . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152\n\n 10.3 Self-Reflection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152\n\n 10.4 Self-Consistency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155\n\n 10.5 Retrieval-Augmented LMMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 156\n\n ", "page_idx": 2, "bbox": [122, 213.8284912109375, 504, 291.3874206542969], "page_size": [612.0, 792.0]}
{"layout": 26, "type": "text", "text": "11 Conclusions 156 ", "text_level": 1, "page_idx": 2, "bbox": [109, 305, 505, 315.75], "page_size": [612.0, 792.0]}
{"layout": 27, "type": "text", "text": "11.1 Summary and Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 156\n\n 11.2 Towards Future LMMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 156 ", "page_idx": 2, "bbox": [122, 322.1214904785156, 504, 350.51544189453125], "page_size": [612.0, 792.0]}
{"layout": 28, "type": "text", "text": "List of Figures ", "text_level": 1, "page_idx": 3, "bbox": [107, 71, 184, 85], "page_size": [612.0, 792.0]}
{"layout": 29, "type": "text", "text": "1 Section 2.3: multi-image inputs and interleaved image-text pairs. . . . . . . . . . . 11 2 Section 3.1: following text instructions. . . . . . . . . . . . . . . . . . . . . . . . 12 3 Section 3.1: constrained prompting. . . . . . . . . . . . . . . . . . . . . . . . . . 13 4 Section 3.1: condition on good performance. . . . . . . . . . . . . . . . . . . . . . 14 5 Section 3.2: different modes of visual pointing. . . . . . . . . . . . . . . . . . . . 16 6 Section 3.2: visual pointing and visual referring prompting. . . . . . . . . . . . . . 17 7 Section 3.3: multimodal example-grounded instruction. . . . . . . . . . . . . . . . 18 8 Section 3.4: few-shot comparison example 1, Part I: zero-shot. . . . . . . . . . . . 20 9 Section 3.4: few-shot comparison example 1, Part II: one-shot. . . . . . . . . . . . 21 10 Section 3.4: few-shot comparison example 1, Part III: two-shot. . . . . . . . . . . 22 11 Section 3.4: few-shot comparison example 2, Part I: zero-shot. . . . . . . . . . . . 23 12 Section 3.4: few-shot comparison example 2, Part II: one-shot. . . . . . . . . . . . 24 13 Section 3.4: few-shot comparison example 2, Part III: two-shot. . . . . . . . . . . 25 14 Section 4.1: celebrity recognition and description. . . . . . . . . . . . . . . . . . . 28 15 Section 4.1: landmark recognition and description. . . . . . . . . . . . . . . . . . 29 16 Section 4.1: landmark recognition and description. . . . . . . . . . . . . . . . . . 30 17 Section 4.1: food recognition and description. . . . . . . . . . . . . . . . . . . . . 31 18 Section 4.1: medical image understanding. . . . . . . . . . . . . . . . . . . . . . . 32 19 Section 4.1: medical image understanding. . . . . . . . . . . . . . . . . . . . . . . 33 20 Section 4.1: logo recognition and description. . . . . . . . . . . . . . . . . . . . . 34 21 Section 4.1: in-the-wild logo recognition and description. . . . . . . . . . . . . . . 35 22 Section 4.1: scene understanding. . . . . . . . . . . . . . . . . . . . . . . . . . . 36 23 Section 4.1: counter factual examples. . . . . . . . . . . . . . . . . . . . . . . . . 37 24 Section 4.2: spatial relationship understanding. . . . . . . . . . . . . . . . . . . . 39 25 Section 4.2: object counting. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40 26 Section 4.2: object localization. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41 27 Section 4.2: dense captioning. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42 28 Section 4.3: joke and meme understanding. . . . . . . . . . . . . . . . . . . . . . 44 29 Section 4.3: science and encyclopedia knowledge. . . . . . . . . . . . . . . . . . . 45 30 Section 4.3: science and encyclopedia knowledge. . . . . . . . . . . . . . . . . . . 46 31 Section 4.3: science and encyclopedia knowledge. . . . . . . . . . . . . . . . . . . 47 32 Section 4.3: multimodal commonsense. . . . . . . . . . . . . . . . . . . . . . . . 48 33 Section 4.4: scene text recognition. . . . . . . . . . . . . . . . . . . . . . . . . . . 50 34 Section 4.4: visual math reasoning. . . . . . . . . . . . . . . . . . . . . . . . . . . 51 35 Section 4.4: flow chart understanding. . . . . . . . . . . . . . . . . . . . . . . . . 52 36 Section 4.4: flow chart understanding. . . . . . . . . . . . . . . . . . . . . . . . . 53 37 Section 4.4: chart understanding and reasoning. . . . . . . . . . . . . . . . . . . . 54 38 Section 4.4: table understanding and reasoning. . . . . . . . . . . . . . . . . . . . 55 39 Section 4.4: document understanding. . . . . . . . . . . . . . . . . . . . . . . . . 56 40 Section 4.4: long document understanding. . . . . . . . . . . . . . . . . . . . . . 57 41 Section 4.5: multilingual image descriptions. . . . . . . . . . . . . . . . . . . . . 59 42 Section 4.5: multilingual scene text recognition. . . . . . . . . . . . . . . . . . . . 60 43 Section 4.5: multilingual text recognition, translation, and description reasoning. . 61 44 Section 4.5: multilingual multi culture understanding. . . . . . . . . . . . . . . . . 62 45 Section 4.6: generate LaTex codes based on the hand-written input. . . . . . . . . . 63 46 Section 4.6: reconstruct table in Markdown or LaTex code. . . . . . . . . . . . . . 64 47 Section 4.6: write codes to replicate the input figure. . . . . . . . . . . . . . . . . 65 48 Section 5.1: understand pointing inputs for grounded description. . . . . . . . . . . 67 49 Section 5.1: grounded description with text-format region coordinates. . . . . . . . 68 50 Section 5.2: examples of visual referring prompting. . . . . . . . . . . . . . . . . 70 51 Section 5.2: examples of visual referring prompting. . . . . . . . . . . . . . . . . 71 52 Section 5.3: generate pointing outputs. . . . . . . . . . . . . . . . . . . . . . . . . 72 53 Section 6.1: video frames understanding. . . . . . . . . . . . . . . . . . . . . . . . 74 54 Section 6.2: long-term temporal order reasoning. . . . . . . . . . . . . . . . . . . 75 55 Section 6.2: short-term temporal order reasoning. . . . . . . . . . . . . . . . . . . 76 56 Section 6.2: short-term and long-term temporal anticipation. . . . . . . . . . . . . 77 57 Section 6.2: temporal localization and reasoning. . . . . . . . . . . . . . . . . . . 78 58 Section 6.3: grounded temporal understanding. . . . . . . . . . . . . . . . . . . . 79 59 Section 7.1: abstract visual stimuli. . . . . . . . . . . . . . . . . . . . . . . . . . . 81 60 Section 7.2: part-object association. . . . . . . . . . . . . . . . . . . . . . . . . . 82 61 Section 7.3: WAIS IQ test. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83 62 Section 7.4: RPM IQ test, entire question page. . . . . . . . . . . . . . . . . . . . 84 63 Section 7.4: RPM IQ test, processed sub-figures. . . . . . . . . . . . . . . . . . . 85 64 Section 8.1: read emotions from facial expressions. . . . . . . . . . . . . . . . . . 86 65 Section 8.2: how different visual contents may arouse emotions. . . . . . . . . . . 87 66 Section 8.2: image aesthetics. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88 67 Section 8.3: emotion conditioned outputs. . . . . . . . . . . . . . . . . . . . . . . 89 68 Section 9.1: spot the differences. . . . . . . . . . . . . . . . . . . . . . . . . . . . 92 69 Section 9.1: spot the differences. . . . . . . . . . . . . . . . . . . . . . . . . . . . 93 70 Section 9.2: industrial defect detection. . . . . . . . . . . . . . . . . . . . . . . . 94 71 Section 9.2: industrial defect detection. . . . . . . . . . . . . . . . . . . . . . . . 95 72 Section 9.2: industrial defect detection. . . . . . . . . . . . . . . . . . . . . . . . 96 73 Section 9.2: safety inspection. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97 74 Section 9.2: grocery checkout. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98 75 Section 9.3: radiology report generation on medical images. . . . . . . . . . . . . 100 76 Section 9.3: radiology report generation on medical images. . . . . . . . . . . . . 101 77 Section 9.3: radiology report generation on medical images. . . . . . . . . . . . . 102 ", "page_idx": 3, "bbox": [122, 97.3505859375, 504, 722.7994995117188], "page_size": [612.0, 792.0]}
{"layout": 30, "type": "text", "text": "", "page_idx": 4, "bbox": [122, 72.757568359375, 504, 722.7994384765625], "page_size": [612.0, 792.0]}
{"layout": 31, "type": "text", "text": "78 Section 9.3: radiology report generation with diagnosis history. . . . . . . . . . . . 103 79 Section 9.4: auto damage evaluation. . . . . . . . . . . . . . . . . . . . . . . . . . 105 80 Section 9.4: insurance reporting. . . . . . . . . . . . . . . . . . . . . . . . . . . . 106 81 Section 9.5: customized captioner for photo organization. . . . . . . . . . . . . . . 108 82 Section 9.5: customized captioner for photo organization. . . . . . . . . . . . . . . 109 83 Section 9.5: dense captioning with segmentation cut-outs. . . . . . . . . . . . . . . 110 84 Section 9.6: evaluation of images generated from texts. . . . . . . . . . . . . . . . 112 85 Section 9.6: evaluation of images generated from texts. . . . . . . . . . . . . . . . 113 86 Section 9.6: image editing. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114 87 Section 9.6: image editing. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115 88 Section 9.7: embodied agent using coffee machine. . . . . . . . . . . . . . . . . . 117 89 Section 9.7: embodied agent using coffee machine. . . . . . . . . . . . . . . . . . 118 90 Section 9.7: embodied agent indoor navigation. . . . . . . . . . . . . . . . . . . . 119 91 Section 9.7: embodied agent indoor navigation. . . . . . . . . . . . . . . . . . . . 120 92 Section 9.8: web browsing for Mapo Tofu recipe. . . . . . . . . . . . . . . . . . . 122 93 Section 9.8: web browsing for Mapo Tofu recipe. . . . . . . . . . . . . . . . . . . 123 94 Section 9.8: web browsing for Mapo Tofu recipe. . . . . . . . . . . . . . . . . . . 124 95 Section 9.8: web browsing for Mapo Tofu recipe. . . . . . . . . . . . . . . . . . . 125 96 Section 9.8: web browsing for Mapo Tofu recipe. . . . . . . . . . . . . . . . . . . 126 97 Section 9.8: web browsing for today’s news. . . . . . . . . . . . . . . . . . . . . . 127 98 Section 9.8: web browsing for today’s news. . . . . . . . . . . . . . . . . . . . . . 128 99 Section 9.8: web browsing for today’s news. . . . . . . . . . . . . . . . . . . . . . 129 100 Section 9.8: web browsing for today’s news. . . . . . . . . . . . . . . . . . . . . . 130 101 Section 9.8: web browsing for today’s news. . . . . . . . . . . . . . . . . . . . . . 131 102 Section 9.8: web browsing for today’s news. . . . . . . . . . . . . . . . . . . . . . 132 103 Section 9.8: online shopping. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133 104 Section 9.8: online shopping. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 134 105 Section 9.8: online shopping. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135 106 Section 9.8: online shopping. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 136 107 Section 9.8: online shopping. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137 108 Section 9.8: online shopping. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138 109 Section 9.8: online shopping. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139 110 Section 9.8: online shopping. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140 111 Section 9.8: online shopping. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141 112 Section 9.8: notification understanding. . . . . . . . . . . . . . . . . . . . . . . . 142 113 Section 9.8: notification understanding. . . . . . . . . . . . . . . . . . . . . . . . 143 114 Section 9.8: notification understanding. . . . . . . . . . . . . . . . . . . . . . . . 144 115 Section 9.8: watching videos. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145 116 Section 9.8: watching videos. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 146 117 Section 9.8: watching videos. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147 118 Section 9.8: watching videos. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 148 119 Section 9.8: watching videos. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149 120 Section 10.1: multimodal plugins. . . . . . . . . . . . . . . . . . . . . . . . . . . 150 121 Section 10.2: multimodal chains. . . . . . . . . . . . . . . . . . . . . . . . . . . . 151 122 Section 10.3: self-reflection. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153 123 Section 10.3: self-reflection. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 154 124 Section 10.4: self-consistency. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155 ", "page_idx": 5, "bbox": [122, 72.757568359375, 504, 722.7994384765625], "page_size": [612.0, 792.0]}
{"layout": 32, "type": "text", "text": "", "page_idx": 6, "bbox": [122, 72.757568359375, 504, 199.48143005371094], "page_size": [612.0, 792.0]}
{"layout": 33, "type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 7, "bbox": [107, 73, 191, 85], "page_size": [612.0, 792.0]}
{"layout": 34, "type": "text", "text": "1.1 Motivation and Overview ", "text_level": 1, "page_idx": 7, "bbox": [107, 96, 239, 107], "page_size": [612.0, 792.0]}
{"layout": 35, "type": "text", "text": "The breakthroughs in large language models (LLMs) [ 23 ,  99 ,  30 ,  11 ,  123 ,  53 ] have shown remarkable versa til i ties and capabilities across various domains and tasks. The next evolution in this field, large multimodal models (LMMs), aims to expand upon the capabilities of LLMs by integrating multi-sensory skills to achieve even stronger general intelligence. Given the dominance of the visual in human senses [ 33 ,  58 ], many LMM studies start with extending the vision capability. Preliminary research investigations either finetune a vision encoder to align with a frozen pre-trained LLM [ 125 ,  7 ,  71 ,  55 ,  42 ,  13 ,  48 ,  157 ,  79 ,  35 ,  146 ], or use a vision-language model to convert visual inputs to text descriptions that LLMs can understand [ 149 ,  141 ,  131 ,  54 ,  113 ,  142 ]. However, most existing models [ 13 ,  48 ,  157 ,  79 ,  35 ,  69 ] are of limited model and data scales, potentially restricting the emergence of various intriguing abilities. Consequently, it remains unclear what are the status quo and emergent multimodal abilities of LMMs that are developed based on the state-of-the-art LLMs, such as GPT-4 (no vision) [ 99 ] and PaLM [ 30 ,  11 ]. In this paper, we report our preliminary explorations with (an early version of) GPT-4V, a state-of-the-art LMM with vision, built based on the SOTA LLM and trained with a large scale of multimodal data. ", "page_idx": 7, "bbox": [107, 116.08453369140625, 505, 269.907470703125], "page_size": [612.0, 792.0]}
{"layout": 36, "type": "text", "text": "Our exploration of GPT-4V is guided by the following questions. ", "page_idx": 7, "bbox": [107, 274.2915344238281, 367.3562316894531, 286.2964782714844], "page_size": [612.0, 792.0]}
{"layout": 37, "type": "text", "text": "1.  What are GPT-4V’s supported inputs and working modes?  The genericity of multimodal models inevitably requires the system to work with the arbitrary mix of different input modalities. GPT-4V shows unprecedented ability in understanding and processing an arbitrary mix of input images, sub-images, texts, scene texts, and visual pointers. We also demonstrate that GPT-4V well supports the test-time techniques observed in LLMs, including instruction following [ 102 ], chain-of-thoughts [ 136 ,  66 ], in-context few-shot learning [ 23 ],  etc . 2.  What are the quality and genericity of GPT-4V’s capabilities on different domains and tasks? We sample queries covering a wide range of domains and tasks to understand GPT-4V’s capabilities, including open-world visual understanding, visual description, multimodal knowledge, commonsense, scene text understanding, document reasoning, coding, temporal reasoning, abstract reasoning, emotion understanding, and many more. GPT-4V shows impressive human-level capabilities across many of the experimented domains. 3.  What are effective ways to use and prompt GPT-4V?  GPT-4V is strong in understanding pixel space edits, such as visual pointers and scene texts drawn on input images. Inspired by this capability, we discuss the “visual referring prompting” that directly edits input images to instruct the task of interest. Visual referring prompting can be seamlessly used together with other image and text prompts, presenting a nuanced interface for instruction and example demonstrations. 4.  What are promising future directions?  Given GPT-4V’s strong capability across domains and tasks, we ask what is the next step for multimodal learning, and more broadly for artificial intelligence. We organize our thoughts and explorations into two perspectives,  i.e ., emergent novel application scenarios to focus on, and the future research directions for GPT-4V-based systems. We present our preliminary explorations to inspire future studies. ", "page_idx": 7, "bbox": [131, 294.77154541015625, 505, 570.2785034179688], "page_size": [612.0, 792.0]}
{"layout": 38, "type": "text", "text": "Guided by the aforementioned problems, we comprehensively organize and list our explored quali- tative results. The report contains minimal quantitative benchmark results, and instead consists of mainly selected interesting qualitative examples. Despite being less rigorous, this design allows for providing a more comprehensive analysis covering a broad range of domains, tasks, working modes, and prompting techniques, under a fixed capacity. We believe this organized collection of explorations will inspire future works in emerging novel applications, next-generation multimodal task formulation, and developing advanced LMM-based intelligent systems. ", "page_idx": 7, "bbox": [107, 578.7535400390625, 505, 656.2135009765625], "page_size": [612.0, 792.0]}
{"layout": 39, "type": "text", "text": "1.2 Our Approach in Exploring GPT-4V ", "text_level": 1, "page_idx": 7, "bbox": [108, 669, 285, 681], "page_size": [612.0, 792.0]}
{"layout": 40, "type": "text", "text": "Goal of this report.  The standard approach for evaluating a system is by benchmarking it against a series of carefully designed datasets, each representing a specific domain and task. One challenge is that some of the existing benchmarks may not be suitable for evaluating LMMs anymore. For example, the image captioning outputs of LMMs are much richer and contain more detailed descriptions than the ground truths in the image captioning benchmark datasets [ 27 ]. There is also a lack of public information regarding GPT-4V’s large-scale pre-training, which may violate the train-test setup for certain existing datasets and invalidate those benchmark numbers. Because of this, restricting the evaluation to  existing  benchmarks and metrics may unintentionally narrow the scope of GPT-4V’s assessment. Developing a comprehensive list of next-generation evaluation tasks and benchmarks would be the ideal ultimate solution. However, we left those as future work due to the significant efforts required. ", "page_idx": 7, "bbox": [107, 688.617919921875, 505, 722.7994995117188], "page_size": [612.0, 792.0]}
{"layout": 41, "type": "text", "text": "", "page_idx": 8, "bbox": [107, 72.757568359375, 505, 161.12547302246094], "page_size": [612.0, 792.0]}
{"layout": 42, "type": "text", "text": "In lieu of quantitative benchmarking, this paper focuses on using qualitative results to provide a glimpse of GPT-4V’s new capabilities and potential emerging use cases. Our goal is to discover and preview what GPT-4V might already be capable of, even though these novel capabilities may not yet be entirely reliable. We hope this collection of explorations will inspire future research in establishing quantitative benchmarks for next-generation multimodal tasks, modernizing existing benchmarks, further improving model performance and system reliability, and sparkling innovation in emerging use cases. Following this, we will delve into the core designs for our approach to exploring GPT-4V. ", "page_idx": 8, "bbox": [107, 165.50958251953125, 505, 242.96849060058594], "page_size": [612.0, 792.0]}
{"layout": 43, "type": "text", "text": "Sample selection guidance.  This report focuses on presenting qualitative results to showcase the potential capabilities of GPT-4V, rather than providing comprehensive quantitative benchmark results. This naturally raises the question of the reliability of the showcased examples. The examples featured in this report may require careful instruction tuning to amplify GPT-4V’s corresponding capabilities. It should be noted that some complex cases may only work with the specifically designed prompts. As such, the capabilities demonstrated may not consistently work across different samples. Instead of showing only the reliable functionalities, the primary objective of this report is to provide readers with a list of our discovered potential capabilities of GPT-4V, which might otherwise be overlooked after a few unsuccessful trials. ", "page_idx": 8, "bbox": [107, 246.99388122558594, 505, 346.6304931640625], "page_size": [612.0, 792.0]}
{"layout": 44, "type": "text", "text": "Sample selection to prevent mere memorizing from training.  A fundamental design consideration in qualitative reports [ 24 ] is discerning models’ true capabilities from merely memorizing responses from training samples or making educated guesses based on hints from instructions and in-context examples. We carefully control both the images and text in the input prompts to prevent them from being seen during GPT-4V training. We generate original text queries from scratch, and try to use images that are either not accessible online or with a timestamp beyond April 2023. We will indicate instances where a specific sample does not meet this criterion,  e.g ., deliberately using samples from specific vision-language datasets. Beyond ensuring that samples are unseen, we incorporate rationale queries into the process. These queries are designed to probe the model’s reasoning process, thereby validating GPT-4V’s possession of the intended capability. ", "page_idx": 8, "bbox": [107, 350.6549072265625, 505, 461.20050048828125], "page_size": [612.0, 792.0]}
{"layout": 45, "type": "text", "text": "The default working mode.  As later detailed in Section  3 , GPT-4V works effectively in different working modes, including zero-shot learning with instructions, in-context few-shot learning,  etc . Among them, this report primarily focuses on zero-shot instruction tuning, as opposed to in-context few-shot learning. This design is to prevent potential information leakage from in-context examples. While in-context few-shot examples can enhance performance and reliability, they do not consistently engender new capabilities. As such, we designate zero-shot as the default working mode for presentation, and reduce the use of in-context examples to minimize examples’ impacts on the assessed capabilities. ", "page_idx": 8, "bbox": [107, 465.22491455078125, 505, 553.9525146484375], "page_size": [612.0, 792.0]}
{"layout": 46, "type": "text", "text": "1.3 How to Read this Report? ", "text_level": 1, "page_idx": 8, "bbox": [107, 567, 242, 578], "page_size": [612.0, 792.0]}
{"layout": 47, "type": "text", "text": "This report documents the explorations of GPT-4V conducted by researchers in the computer vision and vision-language multimodal field. It is primarily geared towards fellow researchers in related disciplines who seek to gain a qualitative impression of LMM’s capabilities and understand its difference from traditional vision-language models. The report is also prepared for professionals for whom AI or computer science may be outside their specialties, to assist them in conceptualizing ways LMMs can enhance their proficiency within their distinct domains of expertise. ", "page_idx": 8, "bbox": [107, 586.6685791015625, 505, 653.218505859375], "page_size": [612.0, 792.0]}
{"layout": 48, "type": "text", "text": "We give an overview of the report, structured around the four core questions that guide our exploration. ", "page_idx": 8, "bbox": [107, 657.6025390625, 505, 669.6074829101562], "page_size": [612.0, 792.0]}
{"layout": 49, "type": "text", "text": "1.  What are GPT-4V’s supported inputs and working modes?  Section  2  summarizes GPT-4V’s supported inputs and presents an overview of their corresponding use cases. Based on the flexible interleaved image-text inputs, Section  3  discusses GPT-4V’s different work- ing modes, such as instruction tuning, in-context learning, and other emergent usages. ", "page_idx": 8, "bbox": [131.41200256347656, 678.0675659179688, 505, 722.7994995117188], "page_size": [612.0, 792.0]}
{"layout": 50, "type": "text", "text": "The section covers the novel ways of using and prompting GPT-4V, aiming to provide a comprehensive overview of how we will use GPT-4V in subsequent sections. ", "page_idx": 9, "bbox": [143, 72.757568359375, 505, 95.67150115966797], "page_size": [612.0, 792.0]}
{"layout": 51, "type": "text", "text": "2.  What are the quality and genericity of GPT-4V’s capabilities on different domains and tasks? The exploration of this question makes up a large portion of the report. Section  4  provides a comprehensive analysis covering a wide range of vision and vision-language scenarios, in- cluding image description and recognition on different domains, dense visual understanding, multimodal knowledge, commonsense, scene text understanding, document reasoning, and many more. We also separate out several novel and interesting capabilities. Section  6  studies GPT-4V’s capability in temporal, motion, and video understanding. Section  7  explores the abstract visual understanding and reasoning capability, and Section  8  covers the emotion ", "page_idx": 9, "bbox": [131, 103.28057861328125, 505, 191.6484832763672], "page_size": [612.0, 792.0]}
{"layout": 52, "type": "text", "text": "and sentiment understanding. 3.  What are effective ways to use and prompt GPT-4V?  We start the discussion on this question from the working mode and prompting method introduction in Section  3 . In Section  5 , we highlight one novel promoting technique, namely visual referring prompting, which draws visual pointers and scene texts on input images to prompt GPT-4V. We demonstrate the flexi- ble prompting methods, such as the combination of instruction and example demonstrations, throughout the report in the given examples. 4.  What are promising future directions?  Section  9  focuses on the novel use cases facilitated by GPT-4V. We hope these initial examples could inspire future works to design new task setups and present rigorous benchmarks. Section  10  imagines powerful future systems that can be built based on GPT-4V, such as the multimodal plugins, multimodal chains, self-reflection, self-consistency, and retrieval-augmented LMMs,  etc . ", "page_idx": 9, "bbox": [131, 190.55255126953125, 505, 339.9674987792969], "page_size": [612.0, 792.0]}
{"layout": 53, "type": "text", "text": "In addition to this overview and the  table of contents , we have also included a  list of figures . The list enumerates the qualitative examples detailed within the report, serving as an additional tool to help readers navigate to their scenarios of interest. ", "page_idx": 9, "bbox": [107, 350.3369140625, 505, 384.51849365234375], "page_size": [612.0, 792.0]}
{"layout": 54, "type": "text", "text": "2 GPT-4V’s Input Modes ", "text_level": 1, "page_idx": 9, "bbox": [107, 404, 245, 418], "page_size": [612.0, 792.0]}
{"layout": 55, "type": "text", "text": "This section summarizes GPT-4V’s supported inputs,  i.e ., functioning as a uni-model language model with the text-only inputs, taking single image-text pair optionally with only a single image, and taking interleaved image-text pairs optionally with only multiple image inputs. We next highlight the representative use cases under these different input modes. ", "page_idx": 9, "bbox": [107, 432.51556396484375, 505, 477.2485046386719], "page_size": [612.0, 792.0]}
{"layout": 56, "type": "text", "text": "2.1 Text-only Inputs ", "text_level": 1, "page_idx": 9, "bbox": [107, 495, 202, 507], "page_size": [612.0, 792.0]}
{"layout": 57, "type": "text", "text": "GPT-4V’s strong language capability enables it to serve as an effective unimodal language model [ 38 , 108 ,  23 ] with text-only inputs. Operating exclusively with text for both input and output, GPT-4V is capable of performing a wide variety of language and coding tasks. We refer readers to the GPT-4 technical report [ 99 ] for the comprehensive and in-depth analysis of GPT-4V’s language and coding capabilities, as well as the comparison with GPT-4 (no vision). ", "page_idx": 9, "bbox": [107, 517.1095581054688, 505, 572.7515258789062], "page_size": [612.0, 792.0]}
{"layout": 58, "type": "text", "text": "2.2 Single Image-text Pair ", "text_level": 1, "page_idx": 9, "bbox": [107, 590, 226, 602], "page_size": [612.0, 792.0]}
{"layout": 59, "type": "text", "text": "GPT-4V, the latest large multimodal model, takes images and texts as inputs to generate textual outputs. In line with existing general-purpose vision-language models [ 9 ,  81 ,  73 ,  8 ,  70 ,  122 ,  120 , 155 ,  28 ,  83 ,  45 ,  74 ,  57 ,  64 ,  72 ,  132 ,  29 ,  140 ,  41 ,  7 ,  128 ,  46 ,  40 ,  158 ,  69 ], GPT-4V can take a single image-text pair or a single image as input to perform various vision and vision-language tasks, such as image recognition [ 37 ], object localization [ 153 ], image captioning [ 27 ], visual question answering [ 12 ], visual dialogue [ 36 ], dense caption [ 62 ], and so on. We note that the text in the image-text pair can be used either as instruction like “describe the image” for captioning, or as the query input like the question in visual question answering. GPT-4V’s exceptional intelligence is exemplified by its significantly enhanced performance and general iz ability compared to prior arts. A comprehensive analysis of its multimodal capabilities on various domains is detailed in Section  4 . ", "page_idx": 9, "bbox": [107, 612.612548828125, 505, 722.7994995117188], "page_size": [612.0, 792.0]}
{"layout": 60, "type": "image", "page_idx": 10, "img_path": "layout_images/2309.17421v2_0.jpg", "img_caption": "Figure 1: GPT-4V can work with multi-image and interleaved image-text inputs. Check Section  2.3 for detailed discussions. ", "bbox": [107, 68, 504, 472], "page_size": [612.0, 792.0], "ocr_text": "Sec. 2.3 Interleaved Image-text Inputs\n\nHow much should | pay for the beer on the table\n\nHow much did | pay for tax? P'\naccording to the price on the menu?\n\nWhere is the information?\n\n£1F woveisee. c cen |\nSes\n\nstat\nSe\nTat\nTh\n\nyottal Que: 560.59\n\nBkb8/8 S8Ssesass ©\n\nae\n\nHere is a photo of the menu.\n\nIn the first receipt, you paid $3.72 for tax. The\n\ninformation is located at the bottom of the receipt. In\n\nthe second receipt, you paid $42.23 for tax. The\n\ninformation is located near the bottom of the receipt, GPT-4V:\n\nunder the \"Sales Tax\" line. In the third receipt, you paid . A\n\n$4.58 for tax. The information is located at the bottom es \"Gine thostraing Magne boomseuine ie\nof the receipt, under the \"TAX\" line. In total, you paid the total cost for the beers would be $12 ,\n$50.53 in tax across all three receipts. .\n\n", "vlm_text": "The image presents an example from a document discussing GPT-4V's capability to process interleaved image-text inputs. It features two prompts with corresponding images and outputs from GPT-4V.\n\nOn the left side:\n- There is a prompt asking about the tax paid, accompanied by images of three different receipts.\n- GPT-4V responds by providing the tax amounts from each receipt, totaling $50.53.\n\nOn the right side:\n- A prompt inquires about the price of beer on a table, as per a menu.\n- An image shows two beers on a table.\n- Below it is an image of a menu displaying drink prices.\n- GPT-4V identifies the Magna beer's price as $6 each and calculates a total of $12 for the two beers.\n\nThe captions and text in the image demonstrate the model's functionality in interpreting and answering questions based on both textual prompts and visual cues."}
{"layout": 61, "type": "text", "text": "2.3 Interleaved Image-text Inputs ", "text_level": 1, "page_idx": 10, "bbox": [106, 494, 259, 506], "page_size": [612.0, 792.0]}
{"layout": 62, "type": "text", "text": "The generality of GPT-4V is further enhanced by its ability to handle flexibly interleaved image-text inputs. The interleaved image-text inputs can be either visually centric such as multiple images with a short question or instruction, text-centric such as a long webpage with two inserted images, or a balanced mixture of images and texts. This mode of mixed input provides flexibility for a wide array of applications. For example, it can compute the total tax paid across multiple receipt images, as shown in Figure  1 . It also enables processing multiple input images and extracting queried information. GPT-4V could also effectively associate information across interleaved image-text inputs, such as finding the beer price on the menu, counting the number of beers, and returning the total cost, as shown in Figure  1 . Beyond direct applications, processing interleaved image-text inputs serves as a fundamental component for in-context few-shot learning and other advanced test-time prompting techniques, thereby further boosting GPT-4V’s generality. We demonstrate these intriguing novel usages in the next section, Section  3 . ", "page_idx": 10, "bbox": [107, 513.9755859375, 505, 645.9805297851562], "page_size": [612.0, 792.0]}
{"layout": 63, "type": "text", "text": "3 GPT-4V’s Working Modes and Prompting Techniques ", "text_level": 1, "page_idx": 11, "bbox": [105, 71, 403, 85], "page_size": [612.0, 792.0]}
{"layout": 64, "type": "text", "text": "Sec. 3.1 Following Text Instructions ", "text_level": 1, "page_idx": 11, "bbox": [127, 133, 338, 149], "page_size": [612.0, 792.0]}
{"layout": 65, "type": "text", "text": "Following Text Instructions ", "text_level": 1, "page_idx": 11, "bbox": [117, 157, 243, 168], "page_size": [612.0, 792.0]}
{"layout": 66, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 11, "bbox": [117, 173, 156, 181.75], "page_size": [612.0, 792.0]}
{"layout": 67, "type": "text", "text": "Describe the image with a sentence consisting  of  three/six/nine  words. ", "page_idx": 11, "bbox": [118, 184.382080078125, 276, 201.6956329345703], "page_size": [612.0, 792.0]}
{"layout": 68, "type": "image", "page_idx": 11, "img_path": "layout_images/2309.17421v2_1.jpg", "bbox": [117, 206, 270, 320], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows an outdoor seating area of a restaurant or cafe near the beach. There are tables and chairs with umbrellas, and two tables are shown in the foreground. On one of the tables, there is a bottle of Magna beer, a bottle of water, and a glass. Napkins are placed on both tables. In the background, there is a view of the ocean, a few palm trees, and some string lights hanging across the seating area. The scene suggests a relaxed, beachside atmosphere."}
{"layout": 69, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 11, "bbox": [117, 330, 155, 340], "page_size": [612.0, 792.0]}
{"layout": 70, "type": "text", "text": "Three:  Seaside dining table Six:   Dining table overlooking ocean with beer Nine:  Outdoor restaurant table with ocean view  and a beer ", "page_idx": 11, "bbox": [118, 341.9303283691406, 276, 377.17620849609375], "page_size": [612.0, 792.0]}
{"layout": 71, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 11, "bbox": [116, 389, 155, 395.75], "page_size": [612.0, 792.0]}
{"layout": 72, "type": "text", "text": "Write a sentence for the image consisting only  of words starting with the letter  'B'/ 'D'/ 'T' . ", "page_idx": 11, "bbox": [118, 400.5043029785156, 276, 417.8178405761719], "page_size": [612.0, 792.0]}
{"layout": 73, "type": "image", "page_idx": 11, "img_path": "layout_images/2309.17421v2_2.jpg", "bbox": [117, 423, 270, 538], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows an outdoor dining area with a view of the ocean. There's a table with a bottle of beer labeled \"MAGNA\" and a bottle of water on it, along with napkins. In the background, there are white umbrellas, chairs, and palm trees, with string lights overhead. The setting has a relaxed, beachside atmosphere."}
{"layout": 74, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 11, "bbox": [117, 549, 155, 557.75], "page_size": [612.0, 792.0]}
{"layout": 75, "type": "text", "text": "'B':   Beachside bar boasts beautiful blue  backdrop. 'D':   Dining deck displays delightful drinks,  distant docks. 'T':   Tabletop terrace teeming with tropical  tranquility. ", "page_idx": 11, "bbox": [118, 560.6142578125, 257, 613.79248046875], "page_size": [612.0, 792.0]}
{"layout": 76, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 11, "bbox": [294, 157, 332, 166.75], "page_size": [612.0, 792.0]}
{"layout": 77, "type": "text", "text": "What would the missing image look like? ", "page_idx": 11, "bbox": [295, 168.33010864257812, 429.75177001953125, 176.67750549316406], "page_size": [612.0, 792.0]}
{"layout": 78, "type": "image", "page_idx": 11, "img_path": "layout_images/2309.17421v2_3.jpg", "bbox": [295, 178, 422, 293], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows a sequence puzzle with three shapes on a light blue background. The first shape is a diamond, the second is a four-pointed star, and the third is a diamond containing a small dot in the center. There is a question mark indicating where the next shape in the sequence should go. The puzzle likely requires identifying a pattern in the shapes."}
{"layout": 79, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 11, "bbox": [294, 297, 332, 305], "page_size": [612.0, 792.0]}
{"layout": 80, "type": "text", "text": "The missing image would likely be a variation of the  diamond or star shape, as seen in the other three images. It  could be a diamond with a different pattern or color, or a star  with a different number of points or color. It is difficult to say  for sure without more context, but it would likely follow the  same theme as the other images. ", "page_idx": 11, "bbox": [295, 307.946044921875, 497, 361.124267578125], "page_size": [612.0, 792.0]}
{"layout": 81, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 11, "bbox": [294, 375, 333, 385], "page_size": [612.0, 792.0]}
{"layout": 82, "type": "text", "text": "Here are three sub images arranged in a 2-by-2 matrix. First,  look at the two images in the first column, in the top left and  bottom left. Then, find out the pattern in the first column  based on the left two images. Next, use the found pattern  and the image in the top right to infer the missing figure.  Finally, describe what would the missing image look like? ", "page_idx": 11, "bbox": [295, 386.1521911621094, 497, 439.33038330078125], "page_size": [612.0, 792.0]}
{"layout": 83, "type": "image", "page_idx": 11, "img_path": "layout_images/2309.17421v2_4.jpg", "bbox": [296, 443, 421, 556], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows a 2x2 grid with three shapes and a question mark in the bottom right corner. \n\n- The top left shape is a diamond.\n- The top right shape is a star.\n- The bottom left shape is a diamond with a dot in the center.\n- The bottom right corner contains a question mark, suggesting a missing shape or pattern continuation is expected."}
{"layout": 84, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 11, "bbox": [294, 562, 333, 571], "page_size": [612.0, 792.0]}
{"layout": 85, "type": "text", "text": "The pattern in the first column is that the shape in the bottom  left image has a dot in the center of the shape in the top left  image. Therefore, the missing image in the bottom right  would be a star with a dot in the center, similar to the image  in the top right but with a dot in the center. ", "page_idx": 11, "bbox": [295, 573.1607055664062, 497, 617.3726806640625], "page_size": [612.0, 792.0]}
{"layout": 86, "type": "text", "text": "Figure 2: GPT-4V can understand and follow text instructions, to generate the desired text outputs or learn to perform a new task.  Red  highlights the less informative answer. Check Section  3.1  for detailed discussions. ", "page_idx": 11, "bbox": [108, 631.0535888671875, 504, 666.864501953125], "page_size": [612.0, 792.0]}
{"layout": 87, "type": "text", "text": "Constrained Prompting ", "text_level": 1, "page_idx": 12, "bbox": [130, 59, 229, 69], "page_size": [612.0, 792.0]}
{"layout": 88, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 12, "bbox": [130, 72, 163, 80.75], "page_size": [612.0, 792.0]}
{"layout": 89, "type": "text", "text": "Please read the text in this image and return the information in the following JSON format (note xxx is placeholder, if  the information is not available in the image, put \"N/A\" instead). {\"class\": xxx, \"DLN\": xxx, \"DOB\": xxx, \"Name\": xxx, \"Address\": xxx, \"EXP\": xxx, \"ISS\": xxx, \"SEX\": xxx, \"HGT\": xxx,  \"WGT\": xxx, \"EYES\": xxx, \"HAIR\": xxx, \"DONOR\": xxx} ", "page_idx": 12, "bbox": [129, 82.37741088867188, 480, 114.43550872802734], "page_size": [612.0, 792.0]}
{"layout": 90, "type": "image", "page_idx": 12, "img_path": "layout_images/2309.17421v2_5.jpg", "bbox": [133, 118, 293, 218], "page_size": [612.0, 792.0], "ocr_text": "fvuzona DRIVER LICENSE _\n\naeDLn D08954796\n» vos 01/01/1974\n\n» SAMPLE\n\n2 JELANI\n\n© 123 MAIN ST 4\nPHOENIX, AZ 85007\n\naw exe 03/01/2024 «iss 03/01/2016 _\n\nSEX Mw EYESBRO VETERA\nther $00 shanero VETERAN\n\n— ei 01/01/74\n\n5 DD 9001A96918142154\n\nsq END NONE\n1: RESTB\n\n", "vlm_text": "This is an image of an Arizona driver's license. It includes a photo and personal information typically found on such an ID, like name, address, birth date, height, weight, eye and hair color, and license number. It also shows the license class, expiration, and issue dates. The text indicates the individual is a veteran and an organ donor."}
{"layout": 91, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 12, "bbox": [130, 222, 164, 231], "page_size": [612.0, 792.0]}
{"layout": 92, "type": "text", "text": "{\"class\": \"D\", \"DLN\": \"D08954796\", \"DOB\": \"01/01/1974\", \"Name\": \"Jelani Sample\", \"Address\": \"123 Main St,  Phoenix, AZ 85007\", \"EXP\": \"03/01/2024\", \"ISS\": \"03/01/2016\", \"SEX\": \"M\", \"HGT\": \"5-09\", \"WGT\": \"185 lbs\", \"EYES\": \"BRO\", \"HAIR\": \"N/A\", \"DONOR\": \"Y\"}  ", "page_idx": 12, "bbox": [129, 233.23907470703125, 457, 257.1419677734375], "page_size": [612.0, 792.0]}
{"layout": 93, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 12, "bbox": [130, 271, 164, 280.75], "page_size": [612.0, 792.0]}
{"layout": 94, "type": "text", "text": "Please read the text in this image and return the information in the following JSON format (note xxx is placeholder, if  the information is not available in the image, put \"N/A\" instead). {\"class\": xxx, \"DLN\": xxx, \"DOB\": xxx, \"Name\": xxx, \"Address\": xxx, \"EXP\": xxx, \"ISS\": xxx, \"SEX\": xxx, \"HGT\": xxx,  \"WGT\": xxx, \"EYES\": xxx, \"HAIR\": xxx, \"DONOR\": xxx} ", "page_idx": 12, "bbox": [129, 281.6923522949219, 480, 313.75042724609375], "page_size": [612.0, 792.0]}
{"layout": 95, "type": "image", "page_idx": 12, "img_path": "layout_images/2309.17421v2_6.jpg", "bbox": [132, 319, 291, 422], "page_size": [612.0, 792.0], "ocr_text": "California °RVERUcENse }\n\nCLASS ¢\n\nKA\n\nEND NONE\n\n08311977\n\nSEX F 3 Bi EYES BRN\nMa Cond hallit— ct-s.05\" wer 1251\n\n1D oo/oooooonnNAWANForYY 08/31/2009\n\n", "vlm_text": "This is a California driver license. It includes details such as:\n\n- A photo of the cardholder\n- The cardholder's name: Ima Cardholder\n- Address: 2570 24th Street, Anytown, CA 95818\n- Date of birth: 08/31/1977\n- License number: I1234568\n- Expiration date: 08/31/2014\n- Additional details like height, weight, eye and hair color, and veteran status.\n\nThis appears to be a sample or simulated license for illustrative purposes."}
{"layout": 96, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 12, "bbox": [129, 429, 163, 438], "page_size": [612.0, 792.0]}
{"layout": 97, "type": "text", "text": "{\"class\": \"C\", \"DLN\": \"1234568\", \"DOB\": \"08/31/1977\", \"Name\": \"Ima Cardholder\", \"Address\": \"2570 24th Street,  Anytown, CA 95818\", \"EXP\": \"08/31/2014\", \"ISS\": \"08/31/2011\", \"SEX\": \"F\", \"HGT\": \"5-05\", \"WGT\": \"125 lbs\", \"EYES\": \"BRN\", \"HAIR\": \"BRN\", \"DONOR\": \"VETERAN\"}   ", "page_idx": 12, "bbox": [129, 440.1793212890625, 464.8107604980469, 464.0821838378906], "page_size": [612.0, 792.0]}
{"layout": 98, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 12, "bbox": [129, 477, 164, 486], "page_size": [612.0, 792.0]}
{"layout": 99, "type": "text", "text": "Please read the text in this image and return the information in the following JSON format (note xxx is placeholder, if  the information is not available in the image, put \"N/A\" instead). {\"Surname\": xxx, \"Given Name\": xxx, \"USCIS #\": xxx, \"Category\": xxx, \"Country of Birth\": xxx, \"Date of Birth\": xxx,  \"SEX\": xxx, \"Card Expires\": xxx, \"Resident Since\": xxx} ", "page_idx": 12, "bbox": [129, 487.39422607421875, 480, 519.4523315429688], "page_size": [612.0, 792.0]}
{"layout": 100, "type": "image", "page_idx": 12, "img_path": "layout_images/2309.17421v2_7.jpg", "bbox": [131, 524, 296, 630], "page_size": [612.0, 792.0], "ocr_text": "fee\n\nNENT OF AMERICA\n\nNT RESIDENT\n\n_ STEVENS\n\nais a\nCard Expires: 07/04/2027 ee\nResident Since: 07/05/2017-\n\naa Sim gage ge\n\nti\n000-000-000 Rte - i\n\n", "vlm_text": "The image is of a United States of America Permanent Resident Card, commonly known as a Green Card. It includes details such as the surname \"STEVENS,\" given name \"ICHIKA,\" USCIS number \"000-000-000,\" category \"IR1,\" country of birth \"Japan,\" date of birth \"18 AUG 1988,\" sex \"F,\" card expiration date \"07/04/2027,\" and resident since \"07/05/2017.\" It also has an image of the Statue of Liberty, the American flag, and a fingerprint."}
{"layout": 101, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 12, "bbox": [130, 635, 163, 644], "page_size": [612.0, 792.0]}
{"layout": 102, "type": "text", "text": "{\"Surname\": \"STEVENS\", \"Given Name\": \"ICHIKA\", \"USCIS #\": \"000-000-000\", \"Category\": \"IR1\", \"Country of Birth\":  \"Japan\", \"Date of Birth\": \"18 AUG 1988\", \"SEX\": \"F\", \"Card Expires\": \"07/04/2027\", \"Resident Since\": \"07/05/2017\"}  ", "page_idx": 12, "bbox": [129, 645.88134765625, 480, 661.6290283203125], "page_size": [612.0, 792.0]}
{"layout": 103, "type": "text", "text": "Figure 3: Constrained prompting to return in JSON format. Images are example IDs for samples. Red highlights the wrong answer. Check Section  3.1  for detailed discussions. ", "page_idx": 12, "bbox": [108, 674.5215454101562, 505.7454528808594, 699.4224853515625], "page_size": [612.0, 792.0]}
{"layout": 104, "type": "text", "text": "Sec. 3.1 Following Text Instructions ", "text_level": 1, "page_idx": 13, "bbox": [137, 39, 347, 53], "page_size": [612.0, 792.0]}
{"layout": 105, "type": "text", "text": "Condition on Good Performance ", "text_level": 1, "page_idx": 13, "bbox": [130, 59, 267, 69], "page_size": [612.0, 792.0]}
{"layout": 106, "type": "text", "text": "Prompt: Count the number of apples in the image. ", "page_idx": 13, "bbox": [130, 72.052490234375, 255.72320556640625, 90.04686737060547], "page_size": [612.0, 792.0]}
{"layout": 107, "type": "image", "page_idx": 13, "img_path": "layout_images/2309.17421v2_8.jpg", "bbox": [131, 92, 265, 160], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows a group of apples, varying in shades of red, yellow, and some green."}
{"layout": 108, "type": "text", "text": "GPT-4V: There are 12 apples in the image. ", "page_idx": 13, "bbox": [130, 167.69700622558594, 232.67025756835938, 185.69139099121094], "page_size": [612.0, 792.0]}
{"layout": 109, "type": "text", "text": "Prompt: Count the number of apples in the image. Let’s think step-by-step. ", "page_idx": 13, "bbox": [130, 198.71426391601562, 329.323974609375, 216.70864868164062], "page_size": [612.0, 792.0]}
{"layout": 110, "type": "image", "page_idx": 13, "img_path": "layout_images/2309.17421v2_9.jpg", "bbox": [131, 222, 176, 245], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows a collection of apples, mostly red with some yellowish tinges. There are eleven apples arranged closely together."}
{"layout": 111, "type": "text", "text": "Note: Same query image as shown above, resized to save space ", "page_idx": 13, "bbox": [219.21739196777344, 229.8432159423828, 464.1874084472656, 238.81756591796875], "page_size": [612.0, 792.0]}
{"layout": 112, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 13, "bbox": [130, 252, 163, 259.75], "page_size": [612.0, 792.0]}
{"layout": 113, "type": "text", "text": "Step 1: Look at the image and identify all the apples.  Step 2: Start counting the apples one by one.  Step 3: Make sure not to miss any apples or count the same apple twice.  Step 4: After counting all the apples, double check to make sure the count is correct.  Answer: There are 12 apples in the image. ", "page_idx": 13, "bbox": [130, 262.80914306640625, 386.0943908691406, 303.0737609863281], "page_size": [612.0, 792.0]}
{"layout": 114, "type": "text", "text": "Prompt: Count the number of apples in the image. Let’s count the apples row-by-row. ", "page_idx": 13, "bbox": [130, 318.2824401855469, 360, 336.2768249511719], "page_size": [612.0, 792.0]}
{"layout": 115, "type": "text", "text": "Note: Same query image as shown above, resized to save space ", "page_idx": 13, "bbox": [219, 349.4113464355469, 464, 358.3857116699219], "page_size": [612.0, 792.0]}
{"layout": 116, "type": "text", "text": "Prompt: Count the number of apples in the image. First count how many rows of apples are there, and then count the  apples in each row, and finally sum them up to get the total number.  ", "page_idx": 13, "bbox": [130, 429.1016540527344, 459.125244140625, 455.26165771484375], "page_size": [612.0, 792.0]}
{"layout": 117, "type": "text", "text": "Note: Same query image as shown above, resized to save space ", "page_idx": 13, "bbox": [219, 468.9794921875, 464, 477.953857421875], "page_size": [612.0, 792.0]}
{"layout": 118, "type": "text", "text": "GPT-4V: There are 3 rows of apples in the image.  The top row has 5 apples. The middle row has 4 apples. The bottom row has 6 apples. ", "page_idx": 13, "bbox": [130, 491.5531311035156, 254.57321166992188, 534.0443725585938], "page_size": [612.0, 792.0]}
{"layout": 119, "type": "text", "text": "Therefore, the total number of apples in the image is  $5+4+6=15$   apples.  ", "page_idx": 13, "bbox": [130, 533, 360, 542.2100219726562], "page_size": [612.0, 792.0]}
{"layout": 120, "type": "text", "text": "Prompt: You are an expert in counting things in the image. Let's count the number of apples in the image below row by row  to be sure we have the right answer. ", "page_idx": 13, "bbox": [130, 557.418701171875, 475.6822204589844, 583.5786743164062], "page_size": [612.0, 792.0]}
{"layout": 121, "type": "image", "page_idx": 13, "img_path": "layout_images/2309.17421v2_10.jpg", "bbox": [132, 589, 176, 613], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows a group of apples, varying in colors from green to red, arranged together."}
{"layout": 122, "type": "text", "text": "Note: Same query image as shown above, resized to save space ", "page_idx": 13, "bbox": [219.21739196777344, 597.2965087890625, 464.1874084472656, 606.2708740234375], "page_size": [612.0, 792.0]}
{"layout": 123, "type": "text", "text": "GPT-4V: Row 1: 4 apples  Row 2: 4 apples  Row 3: 3 apples  Total number of apples in the image: 11 apples ", "page_idx": 13, "bbox": [130, 619.8701782226562, 271.4772033691406, 662.3614501953125], "page_size": [612.0, 792.0]}
{"layout": 124, "type": "text", "text": "Figure 4: Condition on good performance to improve counting.  Green  (  Red  ) highlights the correct\n\n (wrong) answer.  Blue  indicates different ways to prompting in addition to the basic requirement of\n\n “Count the number of apples in the image.” Check Section  3.1  for detailed discussions. ", "page_idx": 13, "bbox": [106, 678.903564453125, 504, 715.6455078125], "page_size": [612.0, 792.0]}
{"layout": 125, "type": "text", "text": "3.1 Following Text Instructions ", "text_level": 1, "page_idx": 14, "bbox": [106, 73, 248, 84], "page_size": [612.0, 792.0]}
{"layout": 126, "type": "text", "text": "One unique strength of GPT-4V is its generality, partially achieved via its strong capability in understanding and following text instructions [ 102 ,  96 ,  134 ,  111 ]. Instructions provide a natural way to define and customize the desired output text for arbitrary vision-language use cases. Figure  2 shows an example of image descriptions with constraints on sentence length and the words to use. Alternatively, on the input side, GPT-4V could understand the detailed instructions to perform challenging tasks, such as enabling GPT-4V to better interpret the abstract reasoning question by providing instructions on intermediate steps. The ability to learn new tasks from instructions shows great potential in adapting to various unseen applications and tasks, as detailed in Section  9 . In line with recent studies [ 7 ,  13 ,  48 ,  157 ,  79 ,  35 ], the instructions discussed in this subsection are mostly in the text format, providing language descriptions of the interested task. We will discuss GPT-4V’s unique capability of following multimodal example-grounded instructions later in Section  3.3 . ", "page_idx": 14, "bbox": [107, 92.7325439453125, 505, 213.82847595214844], "page_size": [612.0, 792.0]}
{"layout": 127, "type": "text", "text": "In addition, we showcase how text instructions play an important role in shaping GPT-4V’s response with two techniques adopted from LLM literature [ 3 ,  156 ], ( i ) “constrained prompting” so that GPT-4V responds in a certain format; and ( ii ) “condition on good performance” that explicitly asks for good performance from GPT-4V. ", "page_idx": 14, "bbox": [107, 218.2115478515625, 505, 262.9434814453125], "page_size": [612.0, 792.0]}
{"layout": 128, "type": "text", "text": "Constrained prompting. In Figure  3 , we prompt GPT-4V to read the text in the image and return the information in a specific JSON format. Although GPT-4V makes some mistakes in extracting the corresponding information from driver’s licenses, the responses are constrained to the JSON format specified in the text instruction. We leverage this technique for certain application scenarios in Section  9 . ", "page_idx": 14, "bbox": [107, 273.1229248046875, 505, 329.12249755859375], "page_size": [612.0, 792.0]}
{"layout": 129, "type": "text", "text": "Condition on good performance. One observation about LLMs is that LLMs don’t want to succeed [ 10 ]. Rather, they want to imitate training sets with a spectrum of performance qualities. If the user wants to succeed in a task given to the model, the user should explicitly ask for it, which has proven useful in improving the performance of LLMs [ 156 ]. In the context of LMMs, we have similar observations. In Figure  4 , we compare the model’s response to different text instructions for counting. We start with a simple and clear prompt: “Count the number of apples in the image.” However, GPT-4V incorrectly counts a total of 12 apples in the image. To improve its performance, we explore the use of zero-shot chain-of-thought from [ 66 ] for LLMs by adding the phrase “Let’s think step-by-step.” Although GPT-4V’s predicted steps are generally correct, they are not very helpful for the final count, as it still arrives at the incorrect answer of “12 apples.” Next, we modify the instruction to “Let’s count the apples row-by-row,” which is more relevant to the visual input. While GPT-4V provides the correct total count, it makes mistakes in counting the second/third row. When we further expand the instruction to “First count how many rows of apples there are, then count the apples in each row, and finally sum them up to get the total number,” the final answer deviates even more from the correct answer (15 vs. 11). Finally, imitating “Let’s work this out in a step by step way to be sure we have the right answer.” in [ 156 ] for LLMs, we design the prompt as follows: “You are an expert in counting things in the image. Let’s count the number of apples in the image below row by row to be sure we have the right answer.”. The first sentence in our prompt asks GPT-4V to assume the role of an expert in counting, and the second sentence explicitly instructs GPT-4V to succeed. With this design, GPT-4V successfully returns the correct answer for each row as well as the total count. Throughout the paper, we employ this technique in various scenarios for better performance. ", "page_idx": 14, "bbox": [107, 339.3009033203125, 505, 580.7554931640625], "page_size": [612.0, 792.0]}
{"layout": 130, "type": "text", "text": "3.2 Visual Pointing and Visual Referring Prompting ", "text_level": 1, "page_idx": 14, "bbox": [107, 592, 336, 604], "page_size": [612.0, 792.0]}
{"layout": 131, "type": "text", "text": "Pointing is a fundamental aspect of human-human interaction [ 89 ]. To provide a comparable channel of interaction, various forms of “pointing” are studied to refer to an arbitrary spatial region of interest. For example, as depicted in Figure  5 , “pointing” can be represented as numerical spatial coordinates such as box coordinates and image crops, or visual markers overlaid on image pixels such as arrows, boxes, circles, and hand drawings. We observe that GPT-4V is particularly strong in understanding visual pointers drawn directly on images. Given the flexibility of drawing on images, this capability can be used as a natural approach for future human-computer interaction in the wild [ 90 ,  117 ,  157 ]. To this end, we explore a new prompting method named visual referring prompting, where people edit the pixel space of input images to specify the desired objective, such as drawing visual pointers or handwriting scene texts. As illustrated in Figure  6 , visual referring prompting edits the image pixels, instead of the conventional text prompts, to perform the task of interest. For example, it could be a simple grounded description, which focuses on describing the pointed object while maintaining the understanding of the global image context, as shown in Figure  6  (1,2). Visual referring prompting also enables other novel use cases, such as associating the pointed object with an index written in scene text (Figure  6  (3)), or solving the question asked near the queried edge or angle (Figure  6  (4)). Section  5  will discuss visual referring prompting in more detail. ", "page_idx": 14, "bbox": [107, 612.612548828125, 505, 722.7994995117188], "page_size": [612.0, 792.0]}
{"layout": 132, "type": "text", "text": "", "page_idx": 15, "bbox": [107, 168.73956298828125, 505, 235.2894744873047], "page_size": [612.0, 792.0]}
{"layout": 133, "type": "text", "text": "3.3 Visual  $^+$   Text Prompting ", "text_level": 1, "page_idx": 15, "bbox": [105, 248, 237, 260], "page_size": [612.0, 792.0]}
{"layout": 134, "type": "text", "text": "Visual referring prompting can be smoothly used together with other image-text prompts, presenting a nuanced interface that succinctly represents the problem of interest. Figure  7  presents two examples to showcase the flexibility of GPT-4V’s prompt, particularly its proficiency in integrating different input formats and seamlessly mixing instructions with examples in the inputs. GPT-4V’s genericity and flexibility result in a human-like comprehension of multimodal instructions and an unprecedented ability to adapt to unseen tasks. ", "page_idx": 15, "bbox": [107, 267.717529296875, 505, 334.26849365234375], "page_size": [612.0, 792.0]}
{"layout": 135, "type": "text", "text": "Integrated multimodal instruction inputs. Existing models usually have implicit constraints on how interleaved image-text inputs should be formatted,  e.g ., in-context few-shot learning requires image-text pairs to share a similar format as the query input. In contrast, GPT-4V shows the genericity in processing an arbitrary mix of images, sub-images, texts, scene texts, and visual pointers. For example, to illustrate the “adding a line” pattern in Figure  7 , one could either point to the first column in the matrix image with a circle as in sub-figure (1), or incorporate the sub-images inline as in sub-figure (2). Similarly, for input query, one could either present a large figure with the question as scene texts as in sub-figure (1), or send the mix of texts and sub-images as in sub-figure (2). In contrast to GPT-4V’s flexibility, existing multimodal models are highly restricted in terms of how they can combine images and texts, and the number of images they can process, thereby imposing limitations on the model’s capability and genericity. ", "page_idx": 15, "bbox": [107, 345.0179138183594, 505, 466.4725036621094], "page_size": [612.0, 792.0]}
{"layout": 136, "type": "text", "text": "Multimodal example-grounded instruction. In addition to supporting more flexible input formats, GPT-4V’s genericity also opens up more effective ways of illustrating the task to perform, compared with the instruction-following mode and in-context few-shot learning. Instruction-following tech- niques [ 102 ,  96 ,  134 ,  111 ], originally proposed for NLP tasks, intuitively focus on task instructions purely in the textual format. The text instruction is loosely related to the visual query input and thus may not provide a clear task demonstration. While in-context few-shot learning [ 23 ,  125 ,  7 ] provides test-time examples that contain both images and texts, these examples must align perfectly with the format of the inference query, making them complex and lengthy to incorporate. Furthermore, in-context examples are usually used separately from instructions, requiring the model to infer the task objective and thereby compromising the demonstration’s effectiveness. In contrast, GPT-4V’s capability to comprehend multimodal instructions enables task demonstrations to be grounded onto corresponding in-context examples, therefore more effectively illustrating the task of interest. For example, in Figure  7 , grounding instructions of “finding the pattern in the first column” onto the key steps in demonstration examples ( i.e ., the circled pattern in (1) and corresponding sub-figures in (2)) simplifies the learning process and enhances the model’s performance. This approach also mirrors the human learning process, which involves abstract instructions paired with intuitive examples. ", "page_idx": 15, "bbox": [107, 477.221923828125, 505, 653.2224731445312], "page_size": [612.0, 792.0]}
{"layout": 137, "type": "text", "text": "Sec. 3.2 Visual Referring Prompting ", "text_level": 1, "page_idx": 16, "bbox": [134, 108, 344, 124], "page_size": [612.0, 792.0]}
{"layout": 138, "type": "text", "text": "Visual Pointing and Visual Referring Prompting ", "text_level": 1, "page_idx": 16, "bbox": [117, 130, 336, 142], "page_size": [612.0, 792.0]}
{"layout": 139, "type": "text", "text": "(1) Prompt: ", "text_level": 1, "page_idx": 16, "bbox": [117, 147, 169, 157], "page_size": [612.0, 792.0]}
{"layout": 140, "type": "text", "text": "Describe the pointed region in the image. ", "page_idx": 16, "bbox": [118, 158.38818359375, 255, 166.73558044433594], "page_size": [612.0, 792.0]}
{"layout": 141, "type": "image", "page_idx": 16, "img_path": "layout_images/2309.17421v2_11.jpg", "bbox": [119, 171, 268, 282], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows an outdoor dining area near a beach or ocean. There are tables with umbrellas, and on the table in the foreground, there is a bottle of Magna beer, a glass bottle of water, and some napkins. In the background, there are palm trees and string lights."}
{"layout": 142, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 16, "bbox": [117, 286, 153, 296], "page_size": [612.0, 792.0]}
{"layout": 143, "type": "text", "text": "The pointed region in the image is a row of  hanging lights on a wire. The lights are small  and round, and they are evenly spaced along  the wire. The wire is stretched between two  poles and is located above a patio area with  tables and umbrellas. The lights appear to be  decorative and are likely used to provide  ambient lighting in the evening. ", "page_idx": 16, "bbox": [118, 298.0041198730469, 272, 369.1146240234375], "page_size": [612.0, 792.0]}
{"layout": 144, "type": "text", "text": "(3) Prompt: ", "text_level": 1, "page_idx": 16, "bbox": [116, 379, 170, 390], "page_size": [612.0, 792.0]}
{"layout": 145, "type": "text", "text": "What is in the circled glass? Object 1 or 2? Describe what is Object 1 and object 2. Then  check what is in the circled glass. ", "page_idx": 16, "bbox": [118, 391.64447021484375, 272, 417.9241638183594], "page_size": [612.0, 792.0]}
{"layout": 146, "type": "image", "page_idx": 16, "img_path": "layout_images/2309.17421v2_12.jpg", "bbox": [118, 422, 267, 533], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows a table set outdoors near the beach. Object 1 is a bottle of water labeled \"FIJI.\" Object 2 is a glass of water positioned next to the bottle, and there's also a bottle of \"MAGNA\" beer on the table. The setting includes white table settings, with umbrellas, some string lights, palm trees, and a view of the ocean in the background."}
{"layout": 147, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 16, "bbox": [117, 537, 154, 547], "page_size": [612.0, 792.0]}
{"layout": 148, "type": "text", "text": "Object 1 is a glass bottle of water. It is clear  and has a blue label on it. Object 2 is a glass bottle of Magna beer. It is  green and has a red and white label on it. In the circled glass, it appears to be a clear  liquid, likely water. So, it is likely that the  contents of the circled glass are from Object 1,  the glass bottle of water. ", "page_idx": 16, "bbox": [118, 549.1926879882812, 272, 620.30322265625], "page_size": [612.0, 792.0]}
{"layout": 149, "type": "text", "text": "(2) Prompt: ", "text_level": 1, "page_idx": 16, "bbox": [287, 147, 340, 157], "page_size": [612.0, 792.0]}
{"layout": 150, "type": "table", "page_idx": 16, "img_path": "layout_images/2309.17421v2_13.jpg", "table_caption": "Describe the pointed region in the image. ", "bbox": [287, 158, 494, 269], "page_size": [612.0, 792.0], "ocr_text": "Validataion set Test set\npiethes in, neat. out overall in near. out. overall\n\n¢ 5s ¢ 8 ¢ 8 ¢ Ss OG 8 CS sc 8\nOSCAR 854 119 840 117 803 100 834 114 82.1 115 fr38) 0.7 809 113\nHuman S44 143 85.0 143 95.7 140 87.1 142 846 147 142 853 146\nvivo 92.2 129 878 126 87.5 11.5 883 124 87.8 126 11 866 124\nVinVL. 1087 13.7 95.6 134 838 119 943 13.1 95.2 134 115 925 131\nUFO 1039 145 95.5 138 835 123 943 136 94.7 139 121 923 136\nmPLUG res 7. rrr ie\nSimVLM 1137 - 109 - 62 - 152 - 37 - 09 - - 52 -\nLEMON 1180 15.4 1163 15. 120.2 145 1173 150 1128 152 1155 154, 137 1143 14.9\nUniversalCap 1232 15.0 121.5 15.3 1234 144 1221 150 1189 15.4 1206 15. 141 1193 15.1\nCoCa so = 120.6 15.5\nGITs 100.7 138 97.7 13.5 896 125 66 14 - ~~ -\nom, 107.7 149 1078 145 1025 137 1069144 - - - -\\-[- - -\nor 129.8 16.8 124.1 160 1271 15.7 125.5 16.0 1224 16.2 123.9 160\\122.0{15.7 123.4 15.9\nGrr 126.9 16.1 125.8 16.2 190.6 15.8 126.9 16.1 124.2 16.4 125.5 16.1 122.8) 15.6 124.8 16.1\n\n", "vlm_text": "The table presents a comparison of different methods evaluated on both validation and test sets, with separate scores for in-distribution (in.), near-distribution (near.), out-of-distribution (out.), and overall performance.\n\n### Columns:\n- **Validation Set and Test Set**: Each divided into in., near., out., and overall.\n- **C**: Probably stands for a specific measure (e.g., accuracy, precision).\n- **S**: Another metric, possibly specificity or sensitivity.\n\n### Rows:\n- Methods listed include OSCAR, Human, VIVO, VinVL, UFO, mPLUG, SimVLM, LEMON, UniversalCap, CoCa, and variations of GIT (GIT_B, GIT_L, GIT, GIT2).\n\nValues are numerical, likely representing percentages or scores, and are compared across methods for a specific set and distribution condition.\n\nNote that the column grouping for the test set's \"out.\" segment is specifically highlighted, possibly indicating a focus or importance of these results."}
{"layout": 151, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 16, "bbox": [287, 274, 325, 283], "page_size": [612.0, 792.0]}
{"layout": 152, "type": "text", "text": "The highlighted red region in the image is a table cell that  contains a numerical value of 122.3. It is located in the \"Test  set\" section of the table, under the \"out\" column and the \"GIT2\"  row. This value represents the overall performance metric for  the GIT2 method on the test set, in the \"out\" category. ", "page_idx": 16, "bbox": [286, 285.154296875, 499, 329.3663635253906], "page_size": [612.0, 792.0]}
{"layout": 153, "type": "text", "text": "(4) Prompt: ", "text_level": 1, "page_idx": 16, "bbox": [288, 339, 339, 349], "page_size": [612.0, 792.0]}
{"layout": 154, "type": "text", "text": "Answer the question in the following figure. Provide  intermediate steps. ", "page_idx": 16, "bbox": [286, 351.01287841796875, 459.7657165527344, 368.326416015625], "page_size": [612.0, 792.0]}
{"layout": 155, "type": "image", "page_idx": 16, "img_path": "layout_images/2309.17421v2_14.jpg", "bbox": [291, 375, 492, 472], "page_size": [612.0, 792.0], "ocr_text": "How long is this edge?\n\nWhat is the angle here?\n", "vlm_text": "The image shows a right triangle with one side measuring 4 units, another side measuring 8 units, and a 90-degree angle marked. There are two questions: \"How long is this edge?\" pointing to the hypotenuse, and \"What is the angle here?\" indicating one of the acute angles.\n\nTo find the hypotenuse, use the Pythagorean theorem:\n\\[ c = \\sqrt{4^2 + 8^2} = \\sqrt{16 + 64} = \\sqrt{80} = 4\\sqrt{5} \\]\n\nTo find the acute angle (\\(\\theta\\)), use the tangent function:\n\\[ \\tan(\\theta) = \\frac{4}{8} = \\frac{1}{2} \\]\n\\[ \\theta = \\tan^{-1}\\left(\\frac{1}{2}\\right) \\]\n\nThe hypotenuse is \\(4\\sqrt{5}\\), and the angle can be found using the inverse tangent function."}
{"layout": 156, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 16, "bbox": [287, 483, 325, 492], "page_size": [612.0, 792.0]}
{"layout": 157, "type": "text", "text": "To find the length of the blue arrowed edge, we can use the  Pythagorean theorem since we have a right triangle. The  theorem states that  $\\mathsf{a}^{2}+\\mathsf{b}^{2}=\\mathsf{c}^{2}$  , where a and b are the two legs  of the right triangle and c is the hypotenuse. Plugging in the  values we have, we get:  $\\begin{array}{r l r}{4^{2}+8^{2}=\\mathtt{C}^{2}}&{{}\\,16+6\\overline{{4}}=\\mathtt{C}^{2}}&{80=\\mathtt{C}^{2}\\quad\\sqrt{80}=\\mathtt{C}\\quad8.94\\approx\\mathtt{C}}\\end{array}$  Therefore, the length of the blue arrowed edge is approximately  8.94. To find the angle at the green arrow, we can use the tangent  function since we have the opposite and adjacent sides of the  angle. The tangent function states that tanθ  $=$    opposite/adjacent. Plugging in the values we have, we get:  $\\mathsf{t a n}\\Theta=4/8$   $\\begin{array}{r l r}{{4}/{8}\\,}&{{}\\mathsf{t a n}\\Theta=1/2\\,}&{\\Theta=\\mathsf{t a n}^{\\wedge}{\\cdot}1(1/2)\\,}&{\\Theta\\approx26.57^{\\circ}\\,}\\end{array}$  Therefore, the angle at the green arrow is approximately   $26.57^{\\circ}$  . ", "page_idx": 16, "bbox": [286, 494.471435546875, 499, 628.344970703125], "page_size": [612.0, 792.0]}
{"layout": 158, "type": "text", "text": "Figure 6: GPT-4V demonstrates the unique capability of understanding visual pointing directly overlaid on images. Based on such capability, we explore visual referring prompting that edits input image pixels ( e.g ., drawing visual pointers and scene texts) to prompt the task of interest. Check Section  3.2  for detailed discussions. ", "page_idx": 16, "bbox": [108, 642.4185791015625, 504, 687.1505126953125], "page_size": [612.0, 792.0]}
{"layout": 159, "type": "text", "text": "Sec. 3.3 Visual + Text Prompting ", "text_level": 1, "page_idx": 17, "bbox": [124, 33, 352, 54], "page_size": [612.0, 792.0]}
{"layout": 160, "type": "text", "text": "Multimodal Example-grounded Instruction ", "page_idx": 17, "bbox": [122.18964385986328, 58.24357223510742, 312.35784912109375, 68.84724426269531], "page_size": [612.0, 792.0]}
{"layout": 161, "type": "text", "text": "(1) Prompt: ", "text_level": 1, "page_idx": 17, "bbox": [122, 74, 172, 84], "page_size": [612.0, 792.0]}
{"layout": 162, "type": "text", "text": "Find out the pattern in the first column, and use it to infer  the missing figure in the second column. ", "page_idx": 17, "bbox": [122, 85.46345520019531, 308, 102.56938934326172], "page_size": [612.0, 792.0]}
{"layout": 163, "type": "image", "page_idx": 17, "img_path": "layout_images/2309.17421v2_15.jpg", "bbox": [122, 105, 193, 169], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image depicts a pattern sequence puzzle, consisting of geometric shapes. It has three squares arranged in a 2x2 grid pattern, with one square left blank (having a question mark) which likely needs to be filled based on the pattern observed in the other squares.\n\nIn the left column, there are two triangles: \n- The top one is an inverted triangle with a horizontal line crossing through it.\n- The bottom one is also an inverted triangle with a line crossing it diagonally from top left to bottom right.\n\nIn the right column:\n- The top square contains a diamond shape. \n\nThe bottom right square contains a question mark, indicating the challenge is to determine the pattern and decide what shape should go in that position. Based on patterns in such puzzles, the likely shape for the question mark box is a diamond with a similar alteration, like a line crossing through it, following the pattern logic observed with the triangles."}
{"layout": 164, "type": "text", "text": "For example, the circled region in the first left column  shows the pattern of adding a line through it. Therefore,  the answer is a figure of a diamond ", "page_idx": 17, "bbox": [122, 174.04983520507812, 308, 200.01443481445312], "page_size": [612.0, 792.0]}
{"layout": 165, "type": "image", "page_idx": 17, "img_path": "layout_images/2309.17421v2_16.jpg", "bbox": [122, 202, 157, 235], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows a red diamond shape centered within a blue-bordered square on a beige background."}
{"layout": 166, "type": "text", "text": "with a line through it, looking like ", "page_idx": 17, "bbox": [122, 236.0603790283203, 228.38369750976562, 244.30767822265625], "page_size": [612.0, 792.0]}
{"layout": 167, "type": "image", "page_idx": 17, "img_path": "layout_images/2309.17421v2_17.jpg", "bbox": [123, 246, 156, 280], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows a red symbol resembling a diamond shape with a diagonal line crossing through it, all inside a blue-bordered square on a beige or light yellow background."}
{"layout": 168, "type": "text", "text": "Answer the question in the following figure. ", "page_idx": 17, "bbox": [122, 289.21221923828125, 262.42803955078125, 297.45953369140625], "page_size": [612.0, 792.0]}
{"layout": 169, "type": "image", "page_idx": 17, "img_path": "layout_images/2309.17421v2_18.jpg", "bbox": [123, 300, 193, 365], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows a 2x2 grid containing three shapes in the top left, top right, and bottom left squares. The top left square has an outline of a diamond shape, the top right square has an outline of a star shape, and the bottom left square has a diamond shape with a dot in the middle. The bottom right square contains a question mark, suggesting a pattern or sequence which needs to be completed or predicted. This image likely represents a visual puzzle asking for the next item in the pattern."}
{"layout": 170, "type": "text", "text": "", "page_idx": 17, "bbox": [129, 373, 294, 406], "page_size": [612.0, 792.0]}
{"layout": 171, "type": "image", "page_idx": 17, "img_path": "layout_images/2309.17421v2_19.jpg", "bbox": [126, 408, 297, 433], "page_size": [612.0, 792.0], "ocr_text": "7 ke & & .\n\n7\n", "vlm_text": "The image contains six different geometric shapes, each placed inside a light blue square. The shapes appear to be drawn in a brown color. Here's a description of each shape labeled a through f:\n\na. A star shape with four points, with additional lines creating a smaller star-like shape inside.\nb. A diamond shape.\nc. A star shape with four points, with a dot in the center.\nd. A single dot.\ne. A larger dot.\nf. A star shape with four points and a diamond outline around it.\n\nThe shapes are likely used for a visual comparison or pattern recognition task, but without more context or instructions, their specific purpose is not clear."}
{"layout": 172, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 17, "bbox": [121, 439, 158, 449], "page_size": [612.0, 792.0]}
{"layout": 173, "type": "text", "text": "The pattern in the first column is that a dot is added in  the center of the figure. Therefore, the missing figure in  the second column would be a star with a dot in the  center, which is option C. ", "page_idx": 17, "bbox": [122, 451.1985168457031, 308, 486.021728515625], "page_size": [612.0, 792.0]}
{"layout": 174, "type": "text", "text": "(2) Prompt: ", "text_level": 1, "page_idx": 17, "bbox": [122, 491, 173, 501], "page_size": [612.0, 792.0]}
{"layout": 175, "type": "text", "text": "Find out the pattern in the first column and use it to infer  the missing figure in the second column.  ", "page_idx": 17, "bbox": [122, 502.32568359375, 308, 519.431640625], "page_size": [612.0, 792.0]}
{"layout": 176, "type": "image", "page_idx": 17, "img_path": "layout_images/2309.17421v2_20.jpg", "img_caption": "For example,  ", "bbox": [122, 520.25, 193, 599], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "This image appears to be a pattern or sequence puzzle. There are three shapes:\n\n1. An upside-down triangle in the top left.\n2. A diamond in the top right.\n3. A combination of an upside-down triangle and another shape in the bottom left.\n\nThe bottom right has a question mark, indicating that you need to determine the next shape in the sequence or pattern."}
{"layout": 177, "type": "image", "page_idx": 17, "img_path": "layout_images/2309.17421v2_21.jpg", "bbox": [124, 601, 289, 634], "page_size": [612.0, 792.0], "ocr_text": "image 3 WwW adde a line through image 1 See\n", "vlm_text": "The image contains two symbolic images and a snippet of text. The first symbol (on the left) shows an inverted triangle with a line going through it. The second symbol (on the right) shows an inverted triangle. The handwritten text between these symbols reads: \"image 3 adds a line through image 1.\" This suggests that the first image (Image 3) is created by adding a line through the second image (Image 1)."}
{"layout": 178, "type": "image", "page_idx": 17, "img_path": "layout_images/2309.17421v2_22.jpg", "bbox": [123, 641, 231, 651.75], "page_size": [612.0, 792.0], "ocr_text": "Therefore, adds a line to image 2\n", "vlm_text": "The image contains a piece of text that reads, \"Therefore, adds a line to image 2.\" The word \"Therefore,\" has a blue underline beneath it."}
{"layout": 179, "type": "text", "text": "Prompt Continued 1. ", "page_idx": 17, "bbox": [318, 85.71813201904297, 377.8099670410156, 105.25965881347656], "page_size": [612.0, 792.0]}
{"layout": 180, "type": "image", "page_idx": 17, "img_path": "layout_images/2309.17421v2_23.jpg", "bbox": [318, 105, 353, 238], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image consists of three blue-bordered squares with light blue backgrounds, each containing a distinctive orange shape:\n\n2. A diamond shape.\n3. A star shape.\n4. A diamond shape with a dot in the center."}
{"layout": 181, "type": "text", "text": "Here are three sub images arranged in a 2-by-2 matrix. What is the pattern between image 3 and image 1? What would be the next image to complete the matrix? ", "page_idx": 17, "bbox": [318, 247.60926818847656, 498, 273.5738525390625], "page_size": [612.0, 792.0]}
{"layout": 182, "type": "image", "page_idx": 17, "img_path": "layout_images/2309.17421v2_24.jpg", "bbox": [319, 280, 352, 359], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image consists of two sections labeled \"a.\" and \"b.\" Both sections contain designs within a blue square on a light blue background.\n\n- In section \"a.\", there is a star-shaped design with four pointed corners and overlapping lines in a diamond-like configuration.\n- In section \"b.\", there is a simple diamond shape with no overlapping lines.\n\nThese designs are outlined in brown."}
{"layout": 183, "type": "image", "page_idx": 17, "img_path": "layout_images/2309.17421v2_25.jpg", "bbox": [319, 371, 351, 449], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows two sections. The top section contains a blue square with a star-shaped outline and a dot in the center, both in brown. The bottom section has a blue square with just a single brown dot in the center."}
{"layout": 184, "type": "image", "page_idx": 17, "img_path": "layout_images/2309.17421v2_26.jpg", "bbox": [318, 459, 353, 541], "page_size": [612.0, 792.0], "ocr_text": "P|\n", "vlm_text": "The image shows two shapes within squares. The top shape is a diamond with a dot in the center, and the bottom shape is a star. Both shapes are outlined in brown on a light blue background. The letter \"f.\" is positioned to the left of the bottom shape."}
{"layout": 185, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 17, "bbox": [318, 551, 354, 561], "page_size": [612.0, 792.0]}
{"layout": 186, "type": "text", "text": "The pattern between image 3 and image 1 is that  image 3 has a dot in the center, while image 1 does  not. The next image to complete the matrix would be  option c, a star with a dot in the center. This is because  the pattern seems to be that every other image has a  dot in the center, and the previous image (image 3)  had a dot in the center. Additionally, the images  alternate between a diamond and a star shape, so the  next image should be a star. ", "page_idx": 17, "bbox": [318, 562.7186889648438, 498, 641.8350830078125], "page_size": [612.0, 792.0]}
{"layout": 187, "type": "image", "page_idx": 17, "img_path": "layout_images/2309.17421v2_27.jpg", "bbox": [123, 652.25, 156, 686], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows a red diamond shape outlined in red, centered within a blue-bordered square with a beige background."}
{"layout": 188, "type": "text", "text": "will be a diamond with a line through it. ", "page_idx": 17, "bbox": [122.18964385986328, 688.3572387695312, 248.8890838623047, 696.6045532226562], "page_size": [612.0, 792.0]}
{"layout": 189, "type": "text", "text": "Figure 7: GPT-4V is strong in interpreting an arbitrary mix of images, sub-images, texts, scene texts, and visual pointer inputs. These elements could serve as instructions, examples, or input queries, helping GPT-4V to effectively perform novel tasks. Check Section  3.3  for detailed discussions. ", "page_idx": 17, "bbox": [108, 705.6395263671875, 505, 739.4625244140625], "page_size": [612.0, 792.0]}
{"layout": 190, "type": "text", "text": "3.4 In-context Few-shot Learning ", "text_level": 1, "page_idx": 18, "bbox": [106, 74, 257, 84], "page_size": [612.0, 792.0]}
{"layout": 191, "type": "text", "text": "In-context few-shot learning is another intriguing emergent ability observed in LLMs [ 23 ,  39 ,  135 ,  34 ]. That is, LLMs can generate desired outputs without parameter updates by prepending a few in-context examples at inference time. The examples share the same format as the input query, and serve as demonstrations to illustrate the desired outputs. Similar abilities were recently observed in multimodal models [ 125 ,  7 ,  55 ,  42 ,  151 ], where query inputs are formatted image-text pairs. Complementary to instruction tuning, in-context learning “teaches” model to perform new tasks by providing in-context examples with the same format during test time. We demonstrate the in-context few-shot learning capacity of GPT-4V through a few compelling examples. We emphasize that in certain scenarios, in-context few-shot learning with a sufficient number of examples becomes essential, particularly when zero-shot or one-shot instruction approaches fall short. Figures  8 - 10  explore a challenging scenario involving the reading of a speed meter. In Figure  8 , the zero-shot performance of GPT-4V on a screenshot of a speed meter image from a video is depicted. Despite numerous attempts to prompt GPT-4V in a zero-shot manner, it struggles to accurately read the current speed displayed in the image. The predictions it generates   $(22/30/40\\,\\mathrm{mph})$  ) deviate significantly from the actual human reading of “approximately 9 mph.” Even when employing a 1-shot in-context example, as shown in Figure  9 , using either a dissimilar example (Figure  9a ) or a similar example (Figure  9b ), GPT-4V still fails to accurately locate the two numbers on the left and right sides of the yellow pointer. In contrast, Figure  10  demonstrates that when provided with 2 in-context examples, one similar to the query image and the other dissimilar, GPT-4V successfully predicts the speed reading as “around 9 mph” by recognizing that the pointer is close to   $10\\,\\mathrm{mph}$   but not quite there yet. ", "page_idx": 18, "bbox": [107, 92.7325439453125, 505, 312.0094909667969], "page_size": [612.0, 792.0]}
{"layout": 192, "type": "text", "text": "The comparison between zero-shot, 1-shot, and 2-shot performance for reasoning over a complex line plot is illustrated in Figures  11 - 13 . The example we explore here presents a great difficulty level as it involves multi-hop reasoning. To answer the question “In the graph, which year has the highest average gas price for the month of June,” one needs to go through at least four steps: ( i ) locating the month of June on the  $\\mathbf{X}$  -axis,   $(i i)$   comparing data points for each line in June, ( iii ) identifying the color of the line with the highest value, and   $(i v)$   matching the color to the corresponding year in the legend at the top. Failure in any of these steps would lead to an incorrect prediction. As depicted in Figure  11 , even when prompted with “text instruction, think step-by-step” in a zero-shot manner, GPT-4V fails to correctly associate the colors with the years from the legend. Furthermore, it gets distracted by the highlighted gas price of  $\\S3.32$   in the graph. Similarly, in Figure  12 , although GPT-4V shows some improvement in reading the legend (correcting the corresponding colors for 2021 and 2022 compared to zero-shot), it still insists on answering with 2023 as the year with the highest average gas price for the month of June, despite the fact that the chart only includes data points until 01/17/2023. However, as we introduce another in-context example in Figure  13 , GPT-4V finally arrives at the correct answer (2022) and provides intermediate steps that explain its reasoning process, similar to the demonstration shown in the in-context examples. ", "page_idx": 18, "bbox": [107, 316.3935546875, 505, 492.03448486328125], "page_size": [612.0, 792.0]}
{"layout": 193, "type": "text", "text": "These proof-of-concept examples vividly demonstrate the rising significance of in-context few-shot learning for achieving improved performance with LMMs. This approach serves as a viable alternative to finetuning, analogous to the observations made in the context of LLMs [ 23 ,  39 ,  135 ,  34 ]. Despite the great importance of in-context few-shot learning in achieving better performance with LMMs, we limit its use in this report to prevent the potential information leakage or undesired hints from in-context examples. We also leave the quantitative evaluation of few-shot learning’s gain to future studies. ", "page_idx": 18, "bbox": [107, 496.4185485839844, 505, 573.8775024414062], "page_size": [612.0, 792.0]}
{"layout": 194, "type": "text", "text": "Sec. 3.4 In-context Few-shot Learning ", "text_level": 1, "page_idx": 19, "bbox": [125, 16, 376, 37], "page_size": [612.0, 792.0]}
{"layout": 195, "type": "text", "text": "Zero-shot  ", "text_level": 1, "page_idx": 19, "bbox": [121, 42, 167, 52], "page_size": [612.0, 792.0]}
{"layout": 196, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 19, "bbox": [122, 55, 159, 65], "page_size": [612.0, 792.0]}
{"layout": 197, "type": "text", "text": "What is the read of the speed meter? ", "page_idx": 19, "bbox": [122, 66.01820373535156, 245.1993865966797, 74.36560821533203], "page_size": [612.0, 792.0]}
{"layout": 198, "type": "image", "page_idx": 19, "img_path": "layout_images/2309.17421v2_28.jpg", "bbox": [123, 75, 269, 158], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows a vehicle's speedometer, which is part of a dashboard display in a car. The speedometer measures and displays the speed of the vehicle in miles per hour (mph). The needle is pointing slightly above 0 mph, indicating the car may be stationary or moving at a very low speed. Below the speedometer is an odometer showing a reading of 010582, representing the total miles the vehicle has traveled. On the right side, there is part of a fuel gauge, and just above that, a display indicating an average fuel efficiency of 32.1 miles per gallon (MPG) and that the vehicle is currently in 'Park' (P) gear. Additionally, the video player controls are visible at the bottom of the image, suggesting this is a still from a video titled \"Dashboard & Symbols\"."}
{"layout": 199, "type": "text", "text": "GPT-4V: The read of the speed meter is approximately 22 miles per hour (mph). ", "page_idx": 19, "bbox": [122, 161.9984893798828, 356.0540771484375, 181.75697326660156], "page_size": [612.0, 792.0]}
{"layout": 200, "type": "text", "text": "Zero-shot (think step-by-step) ", "text_level": 1, "page_idx": 19, "bbox": [121, 192, 259, 203], "page_size": [612.0, 792.0]}
{"layout": 201, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 19, "bbox": [122, 209, 159, 217.75], "page_size": [612.0, 792.0]}
{"layout": 202, "type": "text", "text": "What is the read of the speed meter? Let's think step-by-step. ", "page_idx": 19, "bbox": [122, 219.35708618164062, 325.8849182128906, 227.70448303222656], "page_size": [612.0, 792.0]}
{"layout": 203, "type": "image", "page_idx": 19, "img_path": "layout_images/2309.17421v2_29.jpg", "bbox": [123, 234, 190, 273], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image is a screenshot from a video titled \"Dashboard & Symbols.\" It shows a car's speedometer and some parts of the dashboard. The speedometer is analog, displaying speeds up to 140 miles per hour, and the needle is pointing slightly above 20, indicating the vehicle is moving at a little over 20 mph. Below the speedometer, there is a digital odometer or trip meter displaying 10582. In the top right, part of a digital display is visible showing a temperature reading of 32.1°F and a \"P\" indicating the car is in park, along with possibly a fuel gauge. There are various dashboard warning lights partially visible in the background."}
{"layout": 204, "type": "text", "text": "Note: Same query image as shown above, resized to save space ", "page_idx": 19, "bbox": [198.83041381835938, 248.3042755126953, 467.8160705566406, 258.158447265625], "page_size": [612.0, 792.0]}
{"layout": 205, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 19, "bbox": [122, 275, 159, 286], "page_size": [612.0, 792.0]}
{"layout": 206, "type": "text", "text": "There are two scales on this speedometer - one in miles per hour (mph) and one in kilometers per hour (km/h).  The needle is pointing between the 20 and 40 on the mph scale, and between the 40 and 60 on the km/h scale.  Since the numbers on the scales increase in increments of 20, we can estimate that the needle is pointing at  approximately 30 mph and  $50\\;\\mathsf{k m/h}$  . ", "page_idx": 19, "bbox": [122, 287.5425720214844, 493, 322.7884216308594], "page_size": [612.0, 792.0]}
{"layout": 207, "type": "text", "text": "Therefore, the read of the speed meter is approximately 30 mph (or 50 km/h). ", "page_idx": 19, "bbox": [122, 323.4071960449219, 378.0474548339844, 331.75457763671875], "page_size": [612.0, 792.0]}
{"layout": 208, "type": "text", "text": "Zero-shot (w/ text instructions)  ", "text_level": 1, "page_idx": 19, "bbox": [121, 344, 264, 355], "page_size": [612.0, 792.0]}
{"layout": 209, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 19, "bbox": [121, 359, 159, 368], "page_size": [612.0, 792.0]}
{"layout": 210, "type": "text", "text": "What is the read of the speed meter? To read this speed meter, observe the needle on the outer edge of the dial  which points to your current speed represented in miles per hour. Yellow is the color of the needle. ", "page_idx": 19, "bbox": [122, 369.63360595703125, 493, 386.9471435546875], "page_size": [612.0, 792.0]}
{"layout": 211, "type": "image", "page_idx": 19, "img_path": "layout_images/2309.17421v2_30.jpg", "bbox": [124, 391, 190, 429], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows a vehicle's speedometer. It includes gauge markings in kilometers per hour, with the needle pointing to roughly 28 km/h. Additionally, the display shows an odometer reading of 10582 kilometers. On the right, there are some dashboard indicators, including a gear position indicator showing \"P.\""}
{"layout": 212, "type": "text", "text": "Note: Same query image as shown above, resized to save space ", "page_idx": 19, "bbox": [198.7825927734375, 406.5715026855469, 467.76824951171875, 416.4256896972656], "page_size": [612.0, 792.0]}
{"layout": 213, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 19, "bbox": [122, 436, 159, 446], "page_size": [612.0, 792.0]}
{"layout": 214, "type": "text", "text": "In this example, the needle is pointing to approximately 40 mph, so the current speed is approximately 40 miles  per hour. Additionally, the inner circle of the dial shows the speed in kilometers per hour, so in this example, the  current speed is approximately  $64~\\mathsf{k m/h}$  . ", "page_idx": 19, "bbox": [122, 447.8143615722656, 493, 474.09405517578125], "page_size": [612.0, 792.0]}
{"layout": 215, "type": "text", "text": "Zero-shot (w/ text instructions, think step-by-step)  ", "text_level": 1, "page_idx": 19, "bbox": [121, 487, 353, 499], "page_size": [612.0, 792.0]}
{"layout": 216, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 19, "bbox": [122, 502, 159, 510.75], "page_size": [612.0, 792.0]}
{"layout": 217, "type": "text", "text": "What is the read of the speed meter? To read this speed meter, observe the needle on the outer edge of the dial  which points to your current speed represented in miles per hour. Yellow is the color of the needle. Let’s think  step-by-step. ", "page_idx": 19, "bbox": [122, 512.5328369140625, 493, 538.8125610351562], "page_size": [612.0, 792.0]}
{"layout": 218, "type": "image", "page_idx": 19, "img_path": "layout_images/2309.17421v2_31.jpg", "bbox": [123, 545, 191, 583], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows a vehicle's dashboard, focusing on the speedometer. The speedometer is analog and displays speeds in miles per hour, ranging from 0 to 140. The background of the dashboard includes various warning and indicator lights, such as a tire pressure warning light, fuel gauge, and parking indicator. The digital odometer reading is visible below the speedometer needle, showing \"10582.\" There is also some text at the bottom indicating \"Dashboard & Symbols,\" suggesting that the image might be from a tutorial or informational video or resource on dashboard symbols."}
{"layout": 219, "type": "text", "text": "Note: Same query image as shown above, resized to save space ", "page_idx": 19, "bbox": [199.4560089111328, 559.0773315429688, 468.4416809082031, 568.9315185546875], "page_size": [612.0, 792.0]}
{"layout": 220, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 19, "bbox": [122, 589, 159, 598], "page_size": [612.0, 792.0]}
{"layout": 221, "type": "text", "text": "1. First, look at the outer edge of the dial and find the numbers that represent miles per hour. 2. Next, find the yellow needle that points to the current speed. 3. Observe where the needle is pointing and read the number on the outer edge of the dial. 4. This number is the current speed in miles per hour. In this example, the needle is pointing to approximately 40 mph, so the current speed is 40 miles per hour. ", "page_idx": 19, "bbox": [122, 600.3201904296875, 472.2828063964844, 644.5322265625], "page_size": [612.0, 792.0]}
{"layout": 222, "type": "text", "text": "Figure 8: Zero-shot performance under the challenging scenario of reading a speed meter. GPT- 4V fails to read the speed meter accurately even with different ways of ZS prompting.  Red  highlights the wrong answer. Check Section  3.4  for detailed discussions. ", "page_idx": 19, "bbox": [107, 658.4895629882812, 505, 694.3004760742188], "page_size": [612.0, 792.0]}
{"layout": 223, "type": "text", "text": "Sec. 3.4 In-context Few-shot Learning ", "text_level": 1, "page_idx": 20, "bbox": [138, 43, 358, 58], "page_size": [612.0, 792.0]}
{"layout": 224, "type": "text", "text": "In-context Few-shot (1-shot) ", "text_level": 1, "page_idx": 20, "bbox": [124, 65, 255, 76], "page_size": [612.0, 792.0]}
{"layout": 225, "type": "text", "text": "Prompt: What is the read of the speed meter? ", "page_idx": 20, "bbox": [124, 81.61149597167969, 247, 101.39663696289062], "page_size": [612.0, 792.0]}
{"layout": 226, "type": "image", "page_idx": 20, "img_path": "layout_images/2309.17421v2_32.jpg", "bbox": [125, 104, 270, 185], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows a vehicle dashboard with a speedometer. The speedometer reads approximately 95 miles per hour. To the right is a section displaying \"P\" for park and an average miles per gallon (MPG) reading of 32.1. There's also an odometer reading of 010582. Various dashboard symbols are visible on the left."}
{"layout": 227, "type": "text", "text": "The yellow pointer is roughly at the middle between 80 and   $100\\,\\mathsf{m p h}$  . The middle between 80 and 100 is 90.  The speed is around 91 mph, as the pointer is just passing  $90\\,\\mathsf{m p h}$  . ", "page_idx": 20, "bbox": [124, 190, 488, 208], "page_size": [612.0, 792.0]}
{"layout": 228, "type": "image", "page_idx": 20, "img_path": "layout_images/2309.17421v2_33.jpg", "bbox": [126, 213, 270, 295], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows a car dashboard, specifically the speedometer, with the needle pointing at around 20 miles per hour. Below the speedometer, there's an odometer display showing 110582 miles. To the right of the speedometer, there's a fuel gauge and some additional information displaying \"P\" and \"32.1 Avg. MPG.\" Various dashboard symbols are visible on the left side. This appears to be from a video titled \"Dashboard & Symbols.\""}
{"layout": 229, "type": "text", "text": "GPT-4V: In this image, the yellow needle is roughly at the middle between 60 and 80 mph. The middle between 60 and  80 is 70. The speed is around 71 mph, as the needle is just passing 70 mph. ", "page_idx": 20, "bbox": [124, 301.74200439453125, 488, 330.50543212890625], "page_size": [612.0, 792.0]}
{"layout": 230, "type": "text", "text": "(a) ", "page_idx": 20, "bbox": [303.3659973144531, 338.5530700683594, 313.3186950683594, 349.3575744628906], "page_size": [612.0, 792.0]}
{"layout": 231, "type": "text", "text": "Sec. 3.4 In-context Few-shot Learning ", "text_level": 1, "page_idx": 20, "bbox": [147, 355, 358, 372], "page_size": [612.0, 792.0]}
{"layout": 232, "type": "text", "text": "In-context Few-shot (1-shot) ", "text_level": 1, "page_idx": 20, "bbox": [124, 378, 255, 389], "page_size": [612.0, 792.0]}
{"layout": 233, "type": "text", "text": "Prompt: What is the read of the speed meter? ", "page_idx": 20, "bbox": [124, 394.3735046386719, 247, 414.1586608886719], "page_size": [612.0, 792.0]}
{"layout": 234, "type": "image", "page_idx": 20, "img_path": "layout_images/2309.17421v2_34.jpg", "bbox": [125, 418, 269, 501], "page_size": [612.0, 792.0], "ocr_text": "Oesheoard &Syrboke\n\n", "vlm_text": "The image shows a car dashboard with a speedometer. The speedometer needle indicates a speed of 40 miles per hour. There is an odometer reading of 010582. Other dashboard indicators and symbols are faintly visible in the background, along with a fuel gauge showing \"F\" for full and an efficiency display indicating \"32.1 AVG. MPG.\""}
{"layout": 235, "type": "text", "text": "The yellow pointer is  at the long tick between 20 and 40 mph. The long tick at the middle between 20 and 40  is 30. The speed is around 29 mph, as the pointer is close to  $30\\,\\mathsf{m p h}$   but not quite 30 yet. ", "page_idx": 20, "bbox": [124, 506.36798095703125, 488, 524], "page_size": [612.0, 792.0]}
{"layout": 236, "type": "image", "page_idx": 20, "img_path": "layout_images/2309.17421v2_35.jpg", "bbox": [125, 529, 270, 610], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows a car dashboard, focusing mainly on the speedometer. The speedometer displays speeds ranging from 0 to 220 miles per hour. Below the speedometer, there's an odometer showing a reading of 010582 miles. On the right side, part of the fuel gauge is visible, and it indicates whether the tank is full or empty. The dashboard also displays the figure \"32.1 AVG. MPG\" related to average miles per gallon. There are several symbols and indicators visible in the background, likely representing various vehicle status notifications or alerts. The video timestamp suggests that this is a still from a YouTube video titled \"Dashboard & Symbols.\""}
{"layout": 237, "type": "text", "text": "GPT-4V: The yellow pointer is at the long tick between 40 and 60 mph. The long tick at the middle between 40 and 60 is  50. The speed is around 49 mph, as the pointer is close to 50 mph but not quite 50 yet. ", "page_idx": 20, "bbox": [124, 617.7105712890625, 492.2013244628906, 646.4739379882812], "page_size": [612.0, 792.0]}
{"layout": 238, "type": "text", "text": "Figure 9: One-shot (or prompting with multimodal example instruction) performance under the challenging scenario of reading a speed meter. GPT-4V still fails with (a) dissimilar or (b) similar 1-shot in-context example. Red  highlights the wrong answer. Check Section  3.4  for detailed discussions. ", "page_idx": 20, "bbox": [107, 671.7345581054688, 504, 718.4545288085938], "page_size": [612.0, 792.0]}
{"layout": 239, "type": "text", "text": "Sec. 3.4 In-context Few-shot Learning ", "text_level": 1, "page_idx": 21, "bbox": [141, 171, 356, 187], "page_size": [612.0, 792.0]}
{"layout": 240, "type": "text", "text": "In-context Few-shot (2-shot) ", "page_idx": 21, "bbox": [122, 193.9475860595703, 251.06219482421875, 204.69442749023438], "page_size": [612.0, 792.0]}
{"layout": 241, "type": "text", "text": "Prompt: What is the read of the speed meter? ", "page_idx": 21, "bbox": [122, 210.08775329589844, 245.38449096679688, 229.87289428710938], "page_size": [612.0, 792.0]}
{"layout": 242, "type": "image", "page_idx": 21, "img_path": "layout_images/2309.17421v2_36.jpg", "bbox": [123, 233, 268, 315], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image is of a vehicle dashboard, focusing on the speedometer. The speedometer is marked in both miles per hour (mph) and kilometers per hour (km/h). The needle points near 80 mph. There is an odometer reading of 015082. To the right, there is a digital display with a figure showing \"32.1 AVG. MPG,\" indicating average miles per gallon, and another display indicating the car is in \"P\" for Park. The fuel gauge shows the tank is closer to empty than full. Various warning and indicator lights are faintly visible on the left side of the image."}
{"layout": 243, "type": "text", "text": "The yellow pointer is roughly at the middle between 80 and  $100\\,\\mathsf{m p h}$  . The middle between 80 and 100 is 90.  The speed is around 91 mph, as the pointer is just passing 90 mph. ", "page_idx": 21, "bbox": [122, 318, 481, 336.21258544921875], "page_size": [612.0, 792.0]}
{"layout": 244, "type": "image", "page_idx": 21, "img_path": "layout_images/2309.17421v2_37.jpg", "bbox": [123, 341, 268, 424], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows a car dashboard with a focus on the speedometer. The speedometer measures speed in miles per hour and kilometers per hour, with the needle indicating a speed of about 38 miles per hour. Below the speedometer is an odometer displaying a reading of 010582 miles. To the right, there is a partial view of a fuel gauge, and above that, a digital display showing \"32.1 AVG. MPG.\" Various dashboard symbols and indicators are also visible, but not clearly discernible in the image."}
{"layout": 245, "type": "text", "text": "The yellow needle is roughly at the middle between 20 and 40 mph. The middle between 20 and 40 is 30. The  speed is around 29 mph, as the needle is close to 30 mph but not quite 30 yet. ", "page_idx": 21, "bbox": [122, 427.83319091796875, 487.4473571777344, 445.1700744628906], "page_size": [612.0, 792.0]}
{"layout": 246, "type": "image", "page_idx": 21, "img_path": "layout_images/2309.17421v2_38.jpg", "bbox": [123, 451, 269, 535], "page_size": [612.0, 792.0], "ocr_text": "Desnboard& Syreat\n\n", "vlm_text": "The image is a screenshot of a video titled \"Dashboard & Symbols,\" showing a car speedometer. The speedometer reads up to 140 mph. There is an odometer display showing a mileage of 10582, and nearby gauges indicate fuel levels and average miles per gallon (32.1 MPG). Various dashboard warning symbols are visible on the left side."}
{"layout": 247, "type": "text", "text": "GPT-4V: The yellow pointer is at the long tick between 0 and 20 mph. The long tick at the middle between 0 and 20 is  10. The speed is around 9 mph, as the pointer is close to 10 mph but not quite 10 yet. ", "page_idx": 21, "bbox": [122, 539.239990234375, 481, 568.0033569335938], "page_size": [612.0, 792.0]}
{"layout": 248, "type": "text", "text": "Figure 10: Two-shot performance under the challenging scenario of reading a speed meter. GPT- 4V now can read the speed accurately.  Green  highlights the correct answer. Check Section  3.4  for detailed discussions. ", "page_idx": 21, "bbox": [107, 588.18359375, 505, 623.9945068359375], "page_size": [612.0, 792.0]}
{"layout": 249, "type": "text", "text": "Zero-shot  ", "text_level": 1, "page_idx": 22, "bbox": [91, 40, 136, 51], "page_size": [612.0, 792.0]}
{"layout": 250, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 22, "bbox": [91, 54, 127, 64.75], "page_size": [612.0, 792.0]}
{"layout": 251, "type": "text", "text": "In the graph, which year has the highest average gas price for the month of June? ", "page_idx": 22, "bbox": [92, 65.31936645507812, 352.7525329589844, 73.35848999023438], "page_size": [612.0, 792.0]}
{"layout": 252, "type": "image", "page_idx": 22, "img_path": "layout_images/2309.17421v2_39.jpg", "bbox": [93, 77, 243, 187], "page_size": [612.0, 792.0], "ocr_text": "NATIONAL GAS PRICE COMPARISON | 2 01/11/23\n\n2019 me 2020 wm 2021 w 2022 w 2023\n\n‘$450 a i =<\n\nJ An\n$350 J 3 >\n‘$150\n\ntan | Feb | Mar Apr | May | tun ut aug | Sep\" oct | wow ' dee\n\n", "vlm_text": "The image is a graph comparing national gas prices in the United States from 2019 to 2023. The graph displays the prices per gallon for regular unleaded gasoline over the years, with each year represented by a different colored line: \n\n- 2019 is shown in black\n- 2020 is in orange\n- 2021 is in light blue\n- 2022 is in teal\n- 2023 is in red\n\nThe y-axis represents the price, ranging from $1.50 to $5.50 per gallon, and the x-axis represents the months of the year. As of January 17, 2023, the price is highlighted at $3.32 per gallon. The chart is sourced from AAA (American Automobile Association), as indicated at the bottom of the image."}
{"layout": 253, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 22, "bbox": [92, 193, 127, 202], "page_size": [612.0, 792.0]}
{"layout": 254, "type": "text", "text": "Based on the graph, the year with the highest average gas price for the month of June is 2021, with a price of approximately \\$3.32 per  gallon ", "page_idx": 22, "bbox": [92, 204.57321166992188, 520.7040405273438, 221.2473602294922], "page_size": [612.0, 792.0]}
{"layout": 255, "type": "text", "text": "Zero-shot (think step-by-step) ", "text_level": 1, "page_idx": 22, "bbox": [91, 231, 224, 242], "page_size": [612.0, 792.0]}
{"layout": 256, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 22, "bbox": [91, 245, 127, 254.75], "page_size": [612.0, 792.0]}
{"layout": 257, "type": "text", "text": "In the graph, which year has the highest average gas price for the month of June? Let's think step-by-step. ", "page_idx": 22, "bbox": [92, 255.9327392578125, 430.45806884765625, 263.97186279296875], "page_size": [612.0, 792.0]}
{"layout": 258, "type": "image", "page_idx": 22, "img_path": "layout_images/2309.17421v2_40.jpg", "bbox": [92, 267, 134, 299], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image is a graph titled \"National Gas Price Comparison | 2019-2023.\" It shows the trends in national gas prices over the years from 2019 to 2023. Each year is represented by a different colored line: 2019 is gray, 2020 is orange, 2021 is teal, 2022 is a darker blue, and 2023 is red. The vertical axis represents gas prices in dollars, while the horizontal axis shows the months of the year from January to December. The date in the top corner indicates the data is current as of August 15, 2023. There is a highlighted section showing a price of $3.82 in red, which appears to be emphasized as the current price. The chart includes a source citation to AAA and is branded with their logo at the bottom right corner."}
{"layout": 259, "type": "text", "text": "Note: Same query image as shown above, resized to save space ", "page_idx": 22, "bbox": [167.23220825195312, 279.7859191894531, 426.2834167480469, 289.2761535644531], "page_size": [612.0, 792.0]}
{"layout": 260, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 22, "bbox": [91, 303, 127, 312], "page_size": [612.0, 792.0]}
{"layout": 261, "type": "text", "text": "Step 1: Locate the month of June on the  $\\boldsymbol{\\mathsf{x}}$  -axis.  Step 2: Look at the data points for each year in the month of June.  Step 3: Compare the data points to see which year has the highest value.  Based on these steps, we can see that the year 2021 has the highest average gas price for the month of June, with a value of  approximately \\$3.32 per gallon. ", "page_idx": 22, "bbox": [92, 314.49090576171875, 493.5310363769531, 357.07012939453125], "page_size": [612.0, 792.0]}
{"layout": 262, "type": "text", "text": "Zero-shot (w/ text instructions)  ", "text_level": 1, "page_idx": 22, "bbox": [91, 365, 229, 376], "page_size": [612.0, 792.0]}
{"layout": 263, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 22, "bbox": [91, 380, 127, 389], "page_size": [612.0, 792.0]}
{"layout": 264, "type": "text", "text": "In the graph, which year has the highest average gas price for the month of June? Step 1, read the title of the graph; Step 2 read the  legend of the graph to learn the correspondence between year and line color; Step 3 look at the graph and identify the month of June on  the x-axis; Step 4, look at the data points for each line in the month of June; Step 5, Compare the data points of each line to determine  which one has the highest average gas price in June; Step 6, identify the color of that line, and match to the year in the legend on top.  ", "page_idx": 22, "bbox": [92, 390.4665832519531, 526.2490234375, 424.4107971191406], "page_size": [612.0, 792.0]}
{"layout": 265, "type": "image", "page_idx": 22, "img_path": "layout_images/2309.17421v2_41.jpg", "img_caption": "GPT-4V: ", "bbox": [92, 427, 135, 473], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image is a line graph that shows a national gas price comparison from 2019 to 2023. The graph displays price trends over several years, indicated by different colored lines for each year:\n\n- 2019 is represented by a black line.\n- 2020 is represented by a green line.\n- 2021 is represented by a blue line.\n- 2022 is represented by a light blue line.\n- 2023 is represented by a red line.\n\nPrice data is plotted on the y-axis, ranging from $1.00 to $6.00, with monthly increments along the x-axis from January to December.\n\nA significant detail highlighted in the image is the price of $3.32 associated with the red line for 2023, likely indicating a point of emphasis or notable observation for that year.\n\nThe image also contains some text at the top, \"NATIONAL GAS PRICE COMPARISON | 2019 - 2023,\" with the label \"ENV2023\" on the right side. There appears to be a logo on the bottom right corner."}
{"layout": 266, "type": "text", "text": "Note: Same query image as shown above, resized to save space Based on the above steps, we can see that the year 2021 (represented by the blue line) has the highest average gas price for the month  of June, at  $\\S3.32$   per gallon.  ", "page_idx": 22, "bbox": [167.23220825195312, 440.1507873535156, 426.2834167480469, 449.6410217285156], "page_size": [612.0, 792.0]}
{"layout": 267, "type": "text", "text": "", "page_idx": 22, "bbox": [92, 475.01171875, 527, 492], "page_size": [612.0, 792.0]}
{"layout": 268, "type": "text", "text": "Zero-shot (w/ text instructions, think step-by-step)  ", "page_idx": 22, "bbox": [92, 503.7420654296875, 315.0072937011719, 514.0780639648438], "page_size": [612.0, 792.0]}
{"layout": 269, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 22, "bbox": [91, 517, 127, 526.75], "page_size": [612.0, 792.0]}
{"layout": 270, "type": "text", "text": "In the graph, which year has the highest average gas price for the month of June? Step 1, read the title of the graph; Step 2 read the  legend of the graph to learn the correspondence between year and line color; Step 3 look at the graph and identify the month of June on  the x-axis; Step 4, look at the data points for each line in the month of June; Step 5, Compare the data points of each line to determine  which one has the highest average gas price in June; Step 6, identify the color of that line, and match to the year in the legend on top.  Let’s think step by step. ", "page_idx": 22, "bbox": [92, 528.08837890625, 527, 570.6676635742188], "page_size": [612.0, 792.0]}
{"layout": 271, "type": "image", "page_idx": 22, "img_path": "layout_images/2309.17421v2_42.jpg", "bbox": [92, 573, 134, 604], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "This image is a line graph showing the national average gas price comparison over the years 2019 to 2023. The x-axis represents the months of the year, while the y-axis represents the price in dollars (not fully visible, but inferred from context). Each year is represented by a different colored line: \n- 2019 is in dark gray\n- 2020 is in orange\n- 2021 is in light blue\n- 2022 is in blue\n- 2023 is in red\n\nThe graph shows fluctuations in gas prices over these years, with a highlighted label indicating that the price is $3.82 as of the date of the graph. At the top, it mentions \"NATIONAL GAS PRICE COMPARISON | 2019-2023\" and at the bottom right, there's an AAA logo suggesting that this data is sourced from the American Automobile Association."}
{"layout": 272, "type": "text", "text": "Note: Same query image as shown above, resized to save space ", "page_idx": 22, "bbox": [167.23220825195312, 585.0958862304688, 426.2834167480469, 594.586181640625], "page_size": [612.0, 792.0]}
{"layout": 273, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 22, "bbox": [92, 607, 127, 617], "page_size": [612.0, 792.0]}
{"layout": 274, "type": "text", "text": "Step 1: The title of the graph is \"National Gas Price Comparison / 2019-2023\". This tells us that the graph is comparing gas prices in the  United States from 2019 to 2023.  Step 2: The legend on top of the graph shows that the years 2019, 2020, 2021, 2022, and 2023 are represented by different colors:  orange, black, blue, green, and teal respectively.  Step 3: Looking at the  $\\boldsymbol{\\mathsf{x}}$  -axis, we can see that the month of June is represented.  Step 4: In the month of June, we can see that the data points for each line are as follows: - 2019 (orange): around  $\\S2.75\\cdot2020$   (black):  around  $\\S2.25\\cdot2021$   (blue): around  $\\S3.32\\cdot2022$   (green): around  $\\S3.00\\cdot2023$   (teal): around  $\\S2.90$  Step 5: Comparing these data points, we can see that the highest average gas price in June is   $\\S3.32.$  . Step 6: The color of the line representing  $\\S3.32$   is blue, which corresponds to the year 2021 in the legend.  Therefore, the year with the highest average gas price for the month of June is 2021.  ", "page_idx": 22, "bbox": [92, 618.8015747070312, 525, 704.5559692382812], "page_size": [612.0, 792.0]}
{"layout": 275, "type": "text", "text": "Figure 11: Zero-shot performance under the challenging scenario of reading a line plot. GPT-4V fails ", "page_idx": 22, "bbox": [108, 710.16455078125, 505, 722.1694946289062], "page_size": [612.0, 792.0]}
{"layout": 276, "type": "text", "text": "to answer the question even with different ways of ZS prompting.  Red  highlights the wrong answer. Check Section  3.4  for detailed discussions. ", "page_idx": 22, "bbox": [108, 723.060546875, 505, 745.9744873046875], "page_size": [612.0, 792.0]}
{"layout": 277, "type": "text", "text": "Sec. 3.4 In-context Few-shot Learning ", "text_level": 1, "page_idx": 23, "bbox": [139, 146, 362, 165], "page_size": [612.0, 792.0]}
{"layout": 278, "type": "text", "text": "In-context Few-shot (1-shot) ", "page_idx": 23, "bbox": [122, 170.7003173828125, 251.06219482421875, 181.44715881347656], "page_size": [612.0, 792.0]}
{"layout": 279, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 23, "bbox": [121, 186, 159, 196], "page_size": [612.0, 792.0]}
{"layout": 280, "type": "text", "text": "In the graph, which year has the highest average gas price for the month of June? ", "page_idx": 23, "bbox": [122, 197.5629119873047, 393.36102294921875, 205.92156982421875], "page_size": [612.0, 792.0]}
{"layout": 281, "type": "image", "page_idx": 23, "img_path": "layout_images/2309.17421v2_43.jpg", "bbox": [122, 213, 302, 343], "page_size": [612.0, 792.0], "ocr_text": "NATIONAL GAS PRICE COMPARISON | 2016-2019 02/04/19\n\n2016 2017 2018 w 2019\n$4.00\n\n$3.50\n$3.00\n$250\n$2.00\n\n‘$1.50 Y\nJan” Feb © Mar Apr = May Jun Jul-Aug Sep Oct-Nov Dec\n\n", "vlm_text": "The image is a line graph titled \"National Gas Price Comparison | 2016-2019\" showing the average gas prices over these years. It includes data from 2016 to 2019, represented by different colored lines: orange for 2016, green for 2017, blue for 2018, and red for 2019. The vertical axis shows the price per gallon in dollars, ranging from $1.50 to $4.00. The horizontal axis shows the months from January to December. On February 4, 2019, the gas price was $2.26, as indicated by a marker on the red line for 2019. The source of the data is AAA (GasPrices.AAA.com)."}
{"layout": 282, "type": "text", "text": "This graph is a line plot for national gas price comparison from 2016 until 02/04/2019. The legend on top  shows the line color of each year, red (2019), blue (2018), green (2017)  and orange (2016). Since the data  is reported until Feb. 2019, only 3 years have datapoints for the month of June, 2018 (blue), 2017 (green)  and 2016 (orange). Among them, blue line for 2018 is at the top for the month of June.  Hence, the year with  the highest average gas price for the month of June is 2018.  ", "page_idx": 23, "bbox": [122, 349.37103271484375, 481.43194580078125, 393.6427001953125], "page_size": [612.0, 792.0]}
{"layout": 283, "type": "text", "text": "", "text_level": 1, "page_idx": 23, "bbox": [124, 403, 300, 415.75], "page_size": [612.0, 792.0]}
{"layout": 284, "type": "image", "page_idx": 23, "img_path": "layout_images/2309.17421v2_44.jpg", "bbox": [124, 416.25, 301, 534], "page_size": [612.0, 792.0], "ocr_text": "92019 2020 2021 wm 2022 m 2023\n$5.50\n\n$450\n\n$350\n\n$250\n\n$150\n\nJan Feb Mar Apr May Jun Jul-Aug” “Sep Oct Nov Dec\n\n", "vlm_text": "The image is a line graph showing the average gas prices per gallon for regular unleaded gasoline from 2019 to 2023. Each year is represented by a different colored line:\n\n- 2019: Light blue\n- 2020: Orange\n- 2021: Black\n- 2022: Light green\n- 2023: Red\n\nThe graph tracks prices from January to December. In 2023, the price point highlighted is $3.32 per gallon at the beginning of the year. The source is noted as AAA (GasPrices.AAA.com)."}
{"layout": 285, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 23, "bbox": [122, 540, 159, 550], "page_size": [612.0, 792.0]}
{"layout": 286, "type": "text", "text": "This graph is a line plot for national gas price comparison from 2019 until 01/17/2023. The legend on top  shows the line color of each year, red (2023), blue (2022), green (2021), black (2020) and orange (2019).  Since the data is reported until Jan. 2023, only 4 year have datapoints for the month of June, 2022 (blue),  2021 (green), 2020 (black) and 2019 (orange). Among them, red line for 2023 is at the top for the month of  June. Hence, the year with the highest average gas price for the month of June is 2023.  ", "page_idx": 23, "bbox": [122, 552.3673095703125, 478, 596.6389770507812], "page_size": [612.0, 792.0]}
{"layout": 287, "type": "text", "text": "Figure 12: One-shot (or prompting with multimodal example instruction) performance under the challenging scenario of reading a line plot. GPT-4V still fails with 1-shot in-context example.  Red highlights the wrong answer. Check Section  3.4  for detailed discussions. ", "page_idx": 23, "bbox": [108, 611.4305419921875, 503, 647.2415161132812], "page_size": [612.0, 792.0]}
{"layout": 288, "type": "text", "text": "Sec. 3.4 In-context Few-shot Learning ", "text_level": 1, "page_idx": 24, "bbox": [128, 31, 373, 51], "page_size": [612.0, 792.0]}
{"layout": 289, "type": "text", "text": "In-context Few-shot (2-shot) ", "text_level": 1, "page_idx": 24, "bbox": [121, 55, 253, 66], "page_size": [612.0, 792.0]}
{"layout": 290, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 24, "bbox": [121, 72, 159, 81.75], "page_size": [612.0, 792.0]}
{"layout": 291, "type": "image", "page_idx": 24, "img_path": "layout_images/2309.17421v2_45.jpg", "img_caption": "In the graph, which year has the highest average gas price for the month of June? ", "bbox": [121, 82.25, 395, 230], "page_size": [612.0, 792.0], "ocr_text": "NATIONAL GAS PRICE COMPARISON | 2016-2019 02/04/19\n\n2016 2017 2018 w 2019\n$4.00\n\n$3.50\n\n$3.00\n\n$250\n\n$2.00\n\n$150\nJan Feb Mar Aug Sep Oct Nov Dec\n\n", "vlm_text": "The graph shows gas prices from 2016 to 2019. The year with the highest average gas price in June is 2018, represented by the blue line."}
{"layout": 292, "type": "text", "text": "This graph is a line plot for national gas price comparison from 2016 until 02/04/2019. The legend on top  shows the line color of each year, red (2019), blue (2018), green (2017)  and orange (2016). Since the data  is reported until Feb. 2019, only 3 years have datapoints for the month of June, 2018 (blue), 2017 (green)  and 2016 (orange). Among them, blue line for 2018 is at the top for the month of June.  Hence, the year with  the highest average gas price for the month of June is 2018.  ", "page_idx": 24, "bbox": [122, 235.27053833007812, 483, 279.542236328125], "page_size": [612.0, 792.0]}
{"layout": 293, "type": "image", "page_idx": 24, "img_path": "layout_images/2309.17421v2_46.jpg", "bbox": [123, 293, 302, 423], "page_size": [612.0, 792.0], "ocr_text": "NATIONAL GAS PRICE COMPARISON | 2 8 12/10/18\n\n2015 2016 2017 2018\n$4.00\n\n$3.50\n$3.00\n$250\n$2.00\n\n$150 1 Y\nJan Feb Mar Apr May Jun Jul-Aug -Sep Oct. Nov Dec\n\n", "vlm_text": "The image is a line graph titled \"National Gas Price Comparison | 2015-2018,\" showing the average gas prices for each year from 2015 to 2018. The prices are in dollars per gallon for regular unleaded gasoline. Each year is represented by a different colored line:\n\n- Blue for 2015\n- Green for 2016\n- Orange for 2017\n- Red for 2018\n\nThe graph tracks the prices from January to December, and the data point for December 10, 2018, indicates a price of $2.42. The source for the data is AAA (GasPrices.AAA.com)."}
{"layout": 294, "type": "text", "text": "This graph is a line plot for national gas price comparison from 2015 until 12/10/2018. The legend on top  shows the line color of each year, red (2018), orange (2017), green (2016)  and orange (2017). Since the  data is reported until Dec. 2018, all 4 years have datapoints for the month of June. Among them, red line for  2018 is at the top for the month of June.  Hence, the year with the highest average gas price for the month of  June is 2018.  ", "page_idx": 24, "bbox": [122, 430.1224365234375, 483, 474.39410400390625], "page_size": [612.0, 792.0]}
{"layout": 295, "type": "image", "page_idx": 24, "img_path": "layout_images/2309.17421v2_47.jpg", "bbox": [123, 483, 302, 617], "page_size": [612.0, 792.0], "ocr_text": "NATIONAL GAS PRICE COMPARISON | 2019-2023 01/17/23\n\n2019 2020 m 2021 2022 2023\n$550\n\n$450\n$350\n\n$150\n\nJan | Feb | Mar | Apr | May | Jun | tut | Aug | Sep ' Oct | Nov | Dec\n\n", "vlm_text": "The image is a line graph comparing national gas prices from 2019 to 2023. It shows:\n\n- Different colored lines representing each year: 2019 (black), 2020 (orange), 2021 (light blue), 2022 (teal), and 2023 (red).\n- The prices are measured in dollars per gallon for regular unleaded gasoline.\n- The graph depicts the changes in gas prices over the months of each year.\n- As of January 17, 2023, the gas price is at $3.32.\n\nSource is cited as AAA (GasPrices.AAA.com)."}
{"layout": 296, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 24, "bbox": [121, 621, 159, 632], "page_size": [612.0, 792.0]}
{"layout": 297, "type": "text", "text": "This graph is a line plot for national gas price comparison from 2019 until 01/17/2023. The legend on top  shows the line color of each year, red (2023), blue (2022), green (2021), orange (2020) and black (2019).  Since the data is reported until Jan. 2023, only 4 years have datapoints for the month of June, 2022 (blue),  2021 (green), 2020 (orange) and 2019 (black). Among them, blue line for 2022 is at the top for the month of  June. Hence, the year with the highest average gas price for the month of June is 2022.  ", "page_idx": 24, "bbox": [122, 633.8646240234375, 483, 678.1362915039062], "page_size": [612.0, 792.0]}
{"layout": 298, "type": "text", "text": "Figure 13: Two-shot performance under the challenging scenario of reading a line plot. GPT-4V now can answer the question of “which year has the highest average gas price for the month of June?” correctly. Check Section  3.4  for detailed discussions. Green highlights the correct answer. ", "page_idx": 24, "bbox": [108, 689.0185546875, 505, 724.8284912109375], "page_size": [612.0, 792.0]}
{"layout": 299, "type": "text", "text": "4 Vision-Language Capability ", "text_level": 1, "page_idx": 25, "bbox": [106, 72, 269, 85], "page_size": [612.0, 792.0]}
{"layout": 300, "type": "text", "text": "Understanding and describing visual information plays a crucial role in human cognition. In this section, we will investigate how GPT-4V can be utilized to comprehend and interpret the visual world. We will start by examining the model’s ability to generate open-ended descriptions for generic visual captioning. ", "page_idx": 25, "bbox": [107, 96.79156494140625, 505, 141.5234832763672], "page_size": [612.0, 792.0]}
{"layout": 301, "type": "text", "text": "Moving forward, in Section  4.2 , we will explore the application of GPT-4V in more advanced tasks, such as spatial relationship analysis, object localization, object counting, and dense captioning. In Section  4.3 , we will delve into the model’s capacity for multimodal knowledge and commonsense reasoning, and study whether the model can understand the context and relationships between different types of information. ", "page_idx": 25, "bbox": [107, 145.90655517578125, 505, 201.5485076904297], "page_size": [612.0, 792.0]}
{"layout": 302, "type": "text", "text": "Additionally, in Section  4.4 , we will assess the model’s capability to extract and analyze information from various sources, including scene text, tables, charts, and documents. In Section  4.5 , we will explore GPT-4V’s ability in comprehending and generating descriptions in multilingual scenarios. Lastly, in Section  4.6 , we will investigate the model’s coding proficiency with visual information, exploring its ability to perform tasks with selected examples. ", "page_idx": 25, "bbox": [107, 205.93157958984375, 505, 261.573486328125], "page_size": [612.0, 792.0]}
{"layout": 303, "type": "text", "text": "4.1 Image Description on Diverse Domains ", "text_level": 1, "page_idx": 25, "bbox": [107, 276, 297, 287], "page_size": [612.0, 792.0]}
{"layout": 304, "type": "text", "text": "We access the model’s capability and general iz ability by providing a  single image-text pair  as input. We prompt GPT-4V to generate natural language descriptions covering a variety of topics listed below. ", "page_idx": 25, "bbox": [107, 296.1495666503906, 505, 329.9725036621094], "page_size": [612.0, 792.0]}
{"layout": 305, "type": "text", "text": "Celebrity recognition.  Recognizing human appearance [ 49 ,  80 ] presents a significant challenge due to its inherent variability. To assess GPT-4V’s capabilities to recognize and describe the celebrities, we conduct an experiment by providing a text prompt, “Describe the image,” along with an input celebrity image. In the top row of Figure  14 , we observe that GPT-4V accurately identifies the eight celebrities, despite their diverse backgrounds and fields. Furthermore, when we present a more specific query, “Who is the person in the image and what is the person doing?,” as shown in the bottom row of Figure  14 , GPT-4V comprehends that the current President of the United States is delivering a speech at the 2023 G7 Summit. This illustrates the model’s ability to generalize and handle novel scenarios, such as the 2023 G7 Summit, which was not part of its training data. ", "page_idx": 25, "bbox": [107, 333.9979248046875, 505, 433.6335144042969], "page_size": [612.0, 792.0]}
{"layout": 306, "type": "text", "text": "Landmark recognition.  Landmarks exhibit considerable variations in appearance due to factors such as viewpoint changes, lighting conditions, occlusions, and seasonal changes. Recognizing landmarks under these variations requires models to generalize well and handle the vast range of visual appearances [ 152 ,  5 ]. In the experiments, we employ a straightforward text prompt, “Describe the landmark in the image,” to test the model’s capability. As shown in Figures  15 - 16 , GPT-4V generates accurate and open-ended descriptions for each test image. For example, it accurately recognizes Space Needle located in Seattle, Washington, understanding that the tower was built for the 1962 World’s Fair and has since become a symbol of the city. We have similar observations for other tested photos as well. The generated descriptions go beyond simple labels or generic phrases, providing vivid and detailed narratives that capture the essence of the landmark. ", "page_idx": 25, "bbox": [107, 437.6589050292969, 505, 548.2045288085938], "page_size": [612.0, 792.0]}
{"layout": 307, "type": "text", "text": "Food recognition.  Recognizing food or dishes is a fascinating task [ 20 ,  95 ], but it can be challenging to tackle due to the wide range of appearances and potential occlusions caused by other objects or overlapping ingredients. In our experiments, we employ a straightforward text prompt, asking the system to “Describe the name of the dish,” for testing purpose. Figure  17  demonstrates the accurate recognition of various dishes by GPT-4V. Additionally, GPT-4V effectively captures intricate details within the images, enabling it to identify specific ingredients, garnishes, or cooking techniques present in a dish. ", "page_idx": 25, "bbox": [107, 552.2288818359375, 505, 630.0474853515625], "page_size": [612.0, 792.0]}
{"layout": 308, "type": "text", "text": "Medical image understanding.  Medical images, such as X-rays and CT scans, can have large variability due to patient populations and imaging equipment. Additionally, interpreting the visual content of these images requires expert knowledge. In Figure  18 , we access GPT-4V’s performance by providing the prompt, “Describe the image.” The results show that GPT-4V recognizes both the teeth and jaw bones in the given   $\\mathrm{X}$  -ray. Furthermore, when we prompt with “Are there wisdom teeth that needs to be removed in this x-ray image?” GPT-4V performs reasoning with the visual context, and explains that the wisdom teeth on the bottom left and right sides of the jaw are not fully emerged from the gum line, and this could be a reason for removal. We also conduct testing with other medical images, as shown in Figure  19 . For these experiments, we use prompts such as “What’s wrong?” or “Look at the CT scan, tell me what’s wrong.” The observations reveal that GPT-4V can identify common conditions such as a Jones fracture. It could also point out potential concerns based on the CT scan of the lung. The experiments demonstrate GPT-4V’s basic understanding of medical images. We discuss the application of GPT-4V to the medical domain in Section  9.3 . ", "page_idx": 25, "bbox": [107, 634.0718994140625, 505, 722.7994995117188], "page_size": [612.0, 792.0]}
{"layout": 309, "type": "text", "text": "", "page_idx": 26, "bbox": [107, 72.757568359375, 505, 128.3984832763672], "page_size": [612.0, 792.0]}
{"layout": 310, "type": "text", "text": "Logo recognition.  We examine GPT-4V’s ability in logo recognition. In Figure  20 , we initiate the experiments by providing the text prompt, “Describe the image.” GPT-4V accurately identifies the three logos depicted in the image. We then proceed to ask a more specific question, “Describe the logos in details,” GPT-4V provides elaborate descriptions, including the design, style, and representation for each logo, respectively. Expanding the evaluation to a more challenging  in-the-wild scenario, as shown in Figure  21 , we experiment with logos that may be partially occluded, distorted, or situated in cluttered backgrounds. We employ the text prompt “Describe both the image and logo in details” for the  in-the-wild  experiment. As shown in Figure  21 , GPT-4V demonstrates strong capability in understanding logos in difficult scenarios. Notably, GPT-4V can also provide descriptions for novel or emerging logos and icons, such as the recently released Microsoft 365 Copilot. ", "page_idx": 26, "bbox": [107, 132.4238739013672, 505, 253.8774871826172], "page_size": [612.0, 792.0]}
{"layout": 311, "type": "text", "text": "Scene understanding.  Scene understanding [ 76 ,  32 ,  154 ] is an important task in computer vision. We examine the model’s capability by providing a simple query “Describe the image.” In Figure  22 , GPT-4V is able to describe the road and the location and color of the vehicles. It can also read the sign and notice the speed limit for this road. ", "page_idx": 26, "bbox": [107, 257.90289306640625, 505, 302.9934997558594], "page_size": [612.0, 792.0]}
{"layout": 312, "type": "text", "text": "Counter factual examples.  We conduct experiments by randomly selecting counter factual examples from [ 78 ]. In Figure  23 , we observe that GPT-4V correctly describes the image contents when faced with misleading questions or instructions. ", "page_idx": 26, "bbox": [107, 307.0189208984375, 505, 341.20050048828125], "page_size": [612.0, 792.0]}
{"layout": 313, "type": "text", "text": "Sec. 4.1 Image Description on Diverse Domains ", "text_level": 1, "page_idx": 27, "bbox": [128, 107, 397, 126], "page_size": [612.0, 792.0]}
{"layout": 314, "type": "text", "text": "Celebrity Recognition and Description  ", "text_level": 1, "page_idx": 27, "bbox": [120, 133, 297, 145], "page_size": [612.0, 792.0]}
{"layout": 315, "type": "text", "text": "Prompt: Describe the image ", "page_idx": 27, "bbox": [121, 149.7219696044922, 185.80096435546875, 169.48045349121094], "page_size": [612.0, 792.0]}
{"layout": 316, "type": "image", "page_idx": 27, "img_path": "layout_images/2309.17421v2_48.jpg", "bbox": [120, 175, 489, 358], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image appears to be a collage of several prominent figures in various domains such as sports and entertainment. It includes individuals wearing sports jerseys, formal suits, and elegant dresses, indicating a mix of athletes and celebrities. The presence of different backgrounds and types of outfits suggests diverse occasions like sports events and formal gatherings."}
{"layout": 317, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 27, "bbox": [120, 366, 158, 377], "page_size": [612.0, 792.0]}
{"layout": 318, "type": "text", "text": "The image shows a collage of 8 prominent personalities from different fields. On the top row, from left to right,  there is singer Justin Bieber, footballer Lionel Messi, footballer Cristiano Ronaldo, and entrepreneur Elon Musk.  On the bottom row, from left to right, there is model and reality TV star Kylie Jenner, singer Taylor Swift, actor  Robert Downey Jr., and actor Scarlett Johansson. All individuals are shown in different poses and outfits, with  various backgrounds. ", "page_idx": 27, "bbox": [121, 378.8827209472656, 490.4311828613281, 423.0946960449219], "page_size": [612.0, 792.0]}
{"layout": 319, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 27, "bbox": [120, 441, 158, 451], "page_size": [612.0, 792.0]}
{"layout": 320, "type": "text", "text": "Who is the person in the image and what is the  person doing? ", "page_idx": 27, "bbox": [121, 452.68243408203125, 278.7477111816406, 469.9959716796875], "page_size": [612.0, 792.0]}
{"layout": 321, "type": "image", "page_idx": 27, "img_path": "layout_images/2309.17421v2_49.jpg", "bbox": [121, 476, 296, 580], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows a person standing at a podium with microphones. Behind them is a backdrop with the text \"G7 Summit 2023\" and \"Hiroshima Japan.\" The podium features what appears to be a seal or emblem."}
{"layout": 322, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 27, "bbox": [120, 587, 158, 597], "page_size": [612.0, 792.0]}
{"layout": 323, "type": "text", "text": "The person in the image is the current President of  the United States, Joe Biden. He is standing at a  podium, likely delivering a speech, at the G7 Summit  in Hiroshima, Japan in 2023. ", "page_idx": 27, "bbox": [121, 598.7027587890625, 296, 633.9486694335938], "page_size": [612.0, 792.0]}
{"layout": 324, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 27, "bbox": [320, 441, 358, 451], "page_size": [612.0, 792.0]}
{"layout": 325, "type": "text", "text": "Who is the person in the image and what is the  person doing? ", "page_idx": 27, "bbox": [320, 452.7894287109375, 478.0783996582031, 470.10296630859375], "page_size": [612.0, 792.0]}
{"layout": 326, "type": "image", "page_idx": 27, "img_path": "layout_images/2309.17421v2_50.jpg", "bbox": [319, 475, 473, 580], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows a person holding an NVIDIA product in their hands. The background appears to be a modern, possibly tech-related environment with greenery and architectural elements. The focus is on the NVIDIA item being showcased."}
{"layout": 327, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 27, "bbox": [320, 587, 357, 597], "page_size": [612.0, 792.0]}
{"layout": 328, "type": "text", "text": "The person in the image is Jensen Huang, the CEO  and co-founder of NVIDIA. He is holding and  presenting a product from NVIDIA, likely a graphics  processing unit (GPU).   ", "page_idx": 27, "bbox": [320, 598.8097534179688, 493, 634.0556640625], "page_size": [612.0, 792.0]}
{"layout": 329, "type": "text", "text": "Figure 14: Results on celebrity recognition and description. GPT-4V can recognize a variety of celebrities and describe the visual information (including their profession, action, background, and the event) in details. Check Section  4.1  for detailed discussions. ", "page_idx": 27, "bbox": [108, 650.5415649414062, 503, 684.364501953125], "page_size": [612.0, 792.0]}
{"layout": 330, "type": "text", "text": "Sec. 4.1 Image Description on Diverse Domains ", "text_level": 1, "page_idx": 28, "bbox": [123, 72, 407, 93], "page_size": [612.0, 792.0]}
{"layout": 331, "type": "text", "text": "Landmark Recognition and Description ", "text_level": 1, "page_idx": 28, "bbox": [117, 99, 299, 110], "page_size": [612.0, 792.0]}
{"layout": 332, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 28, "bbox": [117, 115, 155, 125], "page_size": [612.0, 792.0]}
{"layout": 333, "type": "text", "text": "Describe the landmark in the image. ", "page_idx": 28, "bbox": [118, 126.70977783203125, 237.79803466796875, 135.0571746826172], "page_size": [612.0, 792.0]}
{"layout": 334, "type": "image", "page_idx": 28, "img_path": "layout_images/2309.17421v2_51.jpg", "bbox": [117, 140, 293, 270], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows the Space Needle in Seattle, Washington. It is set against a clear blue sky and is surrounded by trees with autumn foliage. Below the Space Needle, people are walking and enjoying the grassy park area."}
{"layout": 335, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 28, "bbox": [117, 279, 155, 289], "page_size": [612.0, 792.0]}
{"layout": 336, "type": "text", "text": "The landmark in the image is the Space Needle, an  observation tower located in Seattle, Washington. It  stands 605 feet tall and offers panoramic views of the  city and surrounding areas. The tower was built for  the 1962 World’s Fair and has since become a  symbol of Seattle and a popular tourist attraction. The  tower has a futuristic design with a large circular  observation deck at the top and a restaurant that  rotates 360 degrees. The base of the tower is  surrounded by a park with trees and green spaces. ", "page_idx": 28, "bbox": [118, 290.6624755859375, 296, 379.7052001953125], "page_size": [612.0, 792.0]}
{"layout": 337, "type": "text", "text": "Prompt: Describe the landmark in the image. ", "text_level": 1, "page_idx": 28, "bbox": [116, 402, 237, 422], "page_size": [612.0, 792.0]}
{"layout": 338, "type": "image", "page_idx": 28, "img_path": "layout_images/2309.17421v2_52.jpg", "bbox": [118, 426, 292, 557], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows a busy urban area with numerous bright, colorful billboards. It's likely a commercial hub with advertisements, featuring well-known brands like Sephora. There is a statue of a person in the center surrounded by people, suggesting it might be a popular tourist location. The environment suggests a lively atmosphere typical of a major city."}
{"layout": 339, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 28, "bbox": [117, 565, 155, 575], "page_size": [612.0, 792.0]}
{"layout": 340, "type": "text", "text": "The landmark in the image is the statue of George M.  Cohan in Times Square, New York City. It is a bronze  statue that stands on a granite pedestal in Duffy  Square, which is the northern part of Times Square.  The statue is a tribute to George M. Cohan, an  American entertainer, playwright, composer, and  producer, who is considered the father of American  musical comedy. The statue was unveiled in 1959  and has since become a popular tourist attraction in  Times Square. ", "page_idx": 28, "bbox": [118, 577.0460205078125, 296, 666.0888061523438], "page_size": [612.0, 792.0]}
{"layout": 341, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 28, "bbox": [316, 115, 354, 125], "page_size": [612.0, 792.0]}
{"layout": 342, "type": "text", "text": "Describe the landmark in the image. ", "page_idx": 28, "bbox": [317, 126.70977783203125, 437.1108093261719, 135.0571746826172], "page_size": [612.0, 792.0]}
{"layout": 343, "type": "image", "page_idx": 28, "img_path": "layout_images/2309.17421v2_53.jpg", "bbox": [318, 139, 493, 269], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows Kinkaku-ji, also known as the Golden Pavilion, which is a Zen Buddhist temple located in Kyoto, Japan. The temple is famous for its top two floors being covered in gold leaf, and it is a renowned historic building set amidst a beautiful garden and a reflective pond. The image captures the temple with its reflection in the water, surrounded by trees and mountains under a partly cloudy sky."}
{"layout": 344, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 28, "bbox": [316, 279, 354, 289], "page_size": [612.0, 792.0]}
{"layout": 345, "type": "text", "text": "This image shows the Kinkaku-ji Temple, also known  as the Golden Pavilion, located in Kyoto, Japan. The  temple is a three-story Buddhist temple that is  covered in gold leaf and sits on a pond, creating a  stunning reflection on the water. The surrounding  landscape is filled with trees and rocks, creating a  peaceful and serene atmosphere. It is a popular  tourist attraction and a designated National Special  Historic Site. ", "page_idx": 28, "bbox": [317, 290.6624755859375, 494, 370.73907470703125], "page_size": [612.0, 792.0]}
{"layout": 346, "type": "text", "text": "Prompt: Describe the landmark in the image. ", "text_level": 1, "page_idx": 28, "bbox": [316, 402, 436, 422], "page_size": [612.0, 792.0]}
{"layout": 347, "type": "image", "page_idx": 28, "img_path": "layout_images/2309.17421v2_54.jpg", "bbox": [319, 426, 493, 557], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows Lombard Street in San Francisco, California, which is famous for its steep, winding, and scenic route. The street features sharp curves, landscaped gardens, and beautiful views of the San Francisco cityscape in the background. The greenery and flowers along the edges of the street add to its charm. Residential buildings line the sides of the street, and people can be seen walking or observing the area. In the distance, iconic San Francisco landmarks like the Coit Tower can be seen."}
{"layout": 348, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 28, "bbox": [317, 565, 354, 575], "page_size": [612.0, 792.0]}
{"layout": 349, "type": "text", "text": "The landmark in the image is the famous Lombard  Street in San Francisco, California, known for its  steep hairpin turns. The street is lined with colorful  flowers and shrubbery, and there are houses and  buildings on both sides. In the background, you can  see the city skyline and the Coit Tower on Telegraph  Hill. This street is a popular tourist attraction, and it is  often considered one of the most crooked streets in  the world. ", "page_idx": 28, "bbox": [317, 577.0460205078125, 494, 657.1226806640625], "page_size": [612.0, 792.0]}
{"layout": 350, "type": "text", "text": "Figure 15: Results on landmark recognition and description. GPT-4V accurately recognizes the landmarks in the test images. It also generates vivid and detailed narratives that capture the essence of the landmarks. Check Section  4.1  for detailed discussions. ", "page_idx": 28, "bbox": [108, 684.965576171875, 504, 718.7885131835938], "page_size": [612.0, 792.0]}
{"layout": 351, "type": "text", "text": "Sec. 4.1 Image Description on Diverse Domains ", "text_level": 1, "page_idx": 29, "bbox": [126, 21, 398, 41], "page_size": [612.0, 792.0]}
{"layout": 352, "type": "text", "text": "Landmark Recognition and Description ", "text_level": 1, "page_idx": 29, "bbox": [117, 47, 299, 59], "page_size": [612.0, 792.0]}
{"layout": 353, "type": "text", "text": "Prompt: Describe the landmark in the image. ", "page_idx": 29, "bbox": [118, 63.53782653808594, 237, 83.29631805419922], "page_size": [612.0, 792.0]}
{"layout": 354, "type": "image", "page_idx": 29, "img_path": "layout_images/2309.17421v2_55.jpg", "bbox": [118, 91, 292, 220], "page_size": [612.0, 792.0], "ocr_text": "cotta Te une\nate De SOL\n\n", "vlm_text": "The image shows a nighttime city scene featuring a tall, illuminated skyscraper, which resembles Taipei 101 in Taipei, Taiwan. The street is bustling with cars and some pedestrians, and other buildings are visible along the road."}
{"layout": 355, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 29, "bbox": [117, 227, 154, 237], "page_size": [612.0, 792.0]}
{"layout": 356, "type": "text", "text": "The landmark in the image is the Taipei 101 building,  located in Taipei, Taiwan. This building, with a height  of 508 meters (1,667 feet), was the tallest building in  the world from 2004 to 2010. It has 101 floors above ground and 5 floors below ground, and is a popular  tourist attraction in Taipei. The design of the building  resembles a bamboo plant, symbolizing strength and  resilience. ", "page_idx": 29, "bbox": [118, 238.90159606933594, 295, 310.0120544433594], "page_size": [612.0, 792.0]}
{"layout": 357, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 29, "bbox": [116, 331, 155, 341.75], "page_size": [612.0, 792.0]}
{"layout": 358, "type": "text", "text": "Describe the landmark in the image. ", "page_idx": 29, "bbox": [118, 342.9093933105469, 237, 351.25677490234375], "page_size": [612.0, 792.0]}
{"layout": 359, "type": "image", "page_idx": 29, "img_path": "layout_images/2309.17421v2_56.jpg", "bbox": [117, 357, 292, 588], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows the interior of a building with luxurious marble walls and floors. In the center of the image, there is a sign that reads \"Trump Grill,\" indicating a dining area likely located within Trump Tower in New York City. The space is decorated for the holiday season with a Christmas tree, presents, and other festive decorations such as poinsettia flowers. There are also gold-colored railings and what appear to be mirrored ceiling panels, contributing to the opulent aesthetic of the setting."}
{"layout": 360, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 29, "bbox": [117, 595, 154, 605], "page_size": [612.0, 792.0]}
{"layout": 361, "type": "text", "text": "The landmark in the image is the interior of the  Trump Tower in New York City. The focal point is the  Trump Grill, which is a restaurant located on the  lower level of the tower. The interior is decorated with red and gold accents, marble walls and columns, and  a festive holiday display with a Christmas tree and  poinsettias. The space is opulent and luxurious,  typical of the Trump brand. ", "page_idx": 29, "bbox": [118, 606.7706909179688, 295, 677.881103515625], "page_size": [612.0, 792.0]}
{"layout": 362, "type": "text", "text": "Prompt: Describe the landmark in the image. ", "page_idx": 29, "bbox": [317, 63.53782653808594, 437, 83.29631805419922], "page_size": [612.0, 792.0]}
{"layout": 363, "type": "image", "page_idx": 29, "img_path": "layout_images/2309.17421v2_57.jpg", "bbox": [319, 90, 493, 221], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows a view of the Manhattan Bridge framed between two rows of red brick buildings. This is a popular viewpoint in the Dumbo neighborhood of Brooklyn, New York City. The sky is partly cloudy, and the bridge's towers are visible behind the buildings."}
{"layout": 364, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 29, "bbox": [317, 227, 353, 237], "page_size": [612.0, 792.0]}
{"layout": 365, "type": "text", "text": "The landmark in the image is the Manhattan Bridge,  as seen from the DUMBO neighborhood in Brooklyn,  New York. The bridge, which spans the East River,  connects lower Manhattan with Brooklyn. It is a suspension bridge with two large towers and was  completed in 1909. In the image, the bridge is framed  by two brick buildings, creating a unique view of the landmark. ", "page_idx": 29, "bbox": [317, 238.90159606933594, 496, 310.0120544433594], "page_size": [612.0, 792.0]}
{"layout": 366, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 29, "bbox": [316, 331, 354, 341], "page_size": [612.0, 792.0]}
{"layout": 367, "type": "text", "text": "Describe the landmark in the image. ", "page_idx": 29, "bbox": [317, 342.7612609863281, 437, 351.108642578125], "page_size": [612.0, 792.0]}
{"layout": 368, "type": "image", "page_idx": 29, "img_path": "layout_images/2309.17421v2_58.jpg", "bbox": [319, 357, 493, 588], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image depicts a series of vibrant orange torii gates at the Fushimi Inari Shrine in Kyoto, Japan. These gates form a path leading up the mountain to the shrine's summit. The path is known for its thousands of closely spaced torii gates, creating a stunning visual tunnel effect as seen in the photo. Torii gates are traditionally found at the entrance of Shinto shrines, marking the transition from the mundane to the sacred."}
{"layout": 369, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 29, "bbox": [317, 595, 354, 604], "page_size": [612.0, 792.0]}
{"layout": 370, "type": "text", "text": "The landmark in the image is the Fushimi Inari  Taisha Shrine in Kyoto, Japan. It is famous for its  thousands of vermilion torii gates that line the paths  leading up to the shrine. The gates represent the  boundary between the secular world and the sacred  world, and are donated by individuals and businesses  to show their devotion to the deity Inari. The image  shows a pathway lined with these gates leading up  towards the shrine, with stone steps in the center. ", "page_idx": 29, "bbox": [317, 606.62255859375, 496, 686.6990966796875], "page_size": [612.0, 792.0]}
{"layout": 371, "type": "text", "text": "Sec. 4.1 Image Description on Diverse Domains ", "text_level": 1, "page_idx": 30, "bbox": [122, 87, 408, 107], "page_size": [612.0, 792.0]}
{"layout": 372, "type": "text", "text": "Food Recognition and Description ", "text_level": 1, "page_idx": 30, "bbox": [117, 114, 276, 125], "page_size": [612.0, 792.0]}
{"layout": 373, "type": "text", "text": "Prompt: Describe the name of the dish. ", "page_idx": 30, "bbox": [118, 130.5085906982422, 219, 150.26707458496094], "page_size": [612.0, 792.0]}
{"layout": 374, "type": "image", "page_idx": 30, "img_path": "layout_images/2309.17421v2_59.jpg", "bbox": [118, 155, 226, 285], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows a dish of Mapo Tofu, a popular Sichuan cuisine. It consists of tofu set in a spicy chili and bean-based sauce, typically with minced meat. It is garnished with green onions."}
{"layout": 375, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 30, "bbox": [117, 291, 155, 301], "page_size": [612.0, 792.0]}
{"layout": 376, "type": "text", "text": "The dish in the image is a bowl of Mapo Tofu, a  popular Chinese dish from the Sichuan province. It  consists of soft tofu cubes in a rich and spicy sauce  made from chili peppers, fermented bean paste, and  Sichuan peppercorns. The dish is garnished with thin  slices of green onions. ", "page_idx": 30, "bbox": [118, 303.31060791015625, 296, 356.4888000488281], "page_size": [612.0, 792.0]}
{"layout": 377, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 30, "bbox": [116, 407, 155, 417.75], "page_size": [612.0, 792.0]}
{"layout": 378, "type": "text", "text": "Describe the name of the dish. ", "page_idx": 30, "bbox": [118, 418.71771240234375, 219, 427.0650939941406], "page_size": [612.0, 792.0]}
{"layout": 379, "type": "image", "page_idx": 30, "img_path": "layout_images/2309.17421v2_60.jpg", "bbox": [118, 437, 292, 567], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows a plate with four small bowls of food, each topped with rice. Surrounding the bowls are slices of bread. The bowls appear to contain different types of soups or stews, likely a variety of Cajun or Creole dishes such as gumbo, jambalaya, or similar, often served with rice. There is also a glass of water and another plate partially visible in the background."}
{"layout": 380, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 30, "bbox": [117, 572, 155, 582], "page_size": [612.0, 792.0]}
{"layout": 381, "type": "text", "text": "This dish is called a Cajun Sampler. It is a sampler  platter that consists of four small bowls of traditional  Cajun dishes - gumbo, jambalaya, red beans and  rice, and crawfish étouffée. The bowls are served  with a side of sliced bread for dipping. Each bowl is  filled with white rice and topped with green onions. It  is a popular dish in Louisiana and a great way to try a  variety of Cajun flavors. ", "page_idx": 30, "bbox": [118, 583.9512329101562, 296, 655.0617065429688], "page_size": [612.0, 792.0]}
{"layout": 382, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 30, "bbox": [316, 131, 354, 140], "page_size": [612.0, 792.0]}
{"layout": 383, "type": "text", "text": "Describe the name of the dish. ", "page_idx": 30, "bbox": [317, 141.919677734375, 423, 150.26707458496094], "page_size": [612.0, 792.0]}
{"layout": 384, "type": "image", "page_idx": 30, "img_path": "layout_images/2309.17421v2_61.jpg", "bbox": [319, 155, 493, 285], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows a dish of grilled eel, known as \"unagi,\" served over rice in a lacquered box. This Japanese dish is called \"unadon\" or \"unaju.\" The eel appears glazed with a sweet soy-based sauce, giving it a glossy appearance."}
{"layout": 385, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 30, "bbox": [316, 293, 354, 303], "page_size": [612.0, 792.0]}
{"layout": 386, "type": "text", "text": "Unagi Donburi is the name of the dish that is  originated from Japan. In Japanese cuisine, it is a  type of rice bowl dish typically served in a deep bowl.  The word “Unagi” means freshwater eel in Japanese  and “donburi” refers to a large bowl served with rice  and other toppings. Unagi Donburi is a dish made  with grilled eel and a type of sweet and savory sauce  called “unagi sauce” over white rice. It is often served  with pickled vegetables, sesame seeds, green  onions, and seaweed. ", "page_idx": 30, "bbox": [317, 304.59149169921875, 494, 393.6342468261719], "page_size": [612.0, 792.0]}
{"layout": 387, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 30, "bbox": [317, 407, 354, 415.75], "page_size": [612.0, 792.0]}
{"layout": 388, "type": "text", "text": "Describe the name of the dish.. ", "page_idx": 30, "bbox": [317, 418.6966857910156, 423, 427.0440673828125], "page_size": [612.0, 792.0]}
{"layout": 389, "type": "image", "page_idx": 30, "img_path": "layout_images/2309.17421v2_62.jpg", "bbox": [319, 437, 493, 567], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows a dish of braised beef short ribs served in a white bowl, accompanied by potatoes, mushrooms, and garnished with a green herb sauce. There is a sauce or broth pooling around the beef in the bottom of the bowl. In the background, there are glasses of water and other restaurant table settings."}
{"layout": 390, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 30, "bbox": [317, 573, 354, 583], "page_size": [612.0, 792.0]}
{"layout": 391, "type": "text", "text": "This dish is called Braised Short Rib with Chi mic hurri.  It features a succulent short rib that has been slow  cooked until tender and topped with a flavorful  chi mic hurri sauce. It is served alongside roasted  vegetables for a complete meal. ", "page_idx": 30, "bbox": [317, 585.2110595703125, 494, 629.4231567382812], "page_size": [612.0, 792.0]}
{"layout": 392, "type": "text", "text": "Sec. 4.1 Image Description on Diverse Domains ", "text_level": 1, "page_idx": 31, "bbox": [123, 44, 401, 64], "page_size": [612.0, 792.0]}
{"layout": 393, "type": "text", "text": "Medical Image Understanding ", "text_level": 1, "page_idx": 31, "bbox": [117, 66, 255, 77], "page_size": [612.0, 792.0]}
{"layout": 394, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 31, "bbox": [117, 87, 155, 96], "page_size": [612.0, 792.0]}
{"layout": 395, "type": "text", "text": "Describe the image ", "page_idx": 31, "bbox": [118, 97.6256103515625, 182.59877014160156, 105.97301483154297], "page_size": [612.0, 792.0]}
{"layout": 396, "type": "image", "page_idx": 31, "img_path": "layout_images/2309.17421v2_63.jpg", "bbox": [118, 112, 419, 297], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "This is a panoramic dental X-ray image, also known as a panoramic radiograph or orthopantomogram (OPG). It shows a broad view of the jaws, teeth, and surrounding structures such as the jawbone, nasal area, and the sinuses. Panoramic X-rays are useful in diagnosing dental issues, planning treatments, and monitoring the overall health of the mouth and associated structures."}
{"layout": 397, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 31, "bbox": [117, 307, 155, 318], "page_size": [612.0, 792.0]}
{"layout": 398, "type": "text", "text": "The image is a black and white X-ray of a human skull, viewed from the front. The top and bottom of the skull are  visible. The teeth and jaw bones are clearly visible. The nasal cavity and eye sockets are also visible but not as  clear. The image is labeled with an “L” on the bottom right corner, indicating that it is the left side of the skull. ", "page_idx": 31, "bbox": [118, 319.2179260253906, 492.42181396484375, 345.4975891113281], "page_size": [612.0, 792.0]}
{"layout": 399, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 31, "bbox": [117, 361, 155, 371.75], "page_size": [612.0, 792.0]}
{"layout": 400, "type": "text", "text": "Are there wisdom teeth that needs to be removed in this x-ray image? ", "page_idx": 31, "bbox": [118, 372.4548645019531, 348.59222412109375, 380.80224609375], "page_size": [612.0, 792.0]}
{"layout": 401, "type": "image", "page_idx": 31, "img_path": "layout_images/2309.17421v2_64.jpg", "bbox": [118, 386, 213, 445], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "This is a panoramic dental X-ray, often called a panorex or orthopantomogram. It shows a broad view of the upper and lower jaws, teeth, and other important structures such as the sinuses and temporomandibular joints. This type of X-ray is often used in dental examinations to check for issues with teeth alignment, jaw structure, or other dental conditions."}
{"layout": 402, "type": "text", "text": "Note: Same query image as shown above, resized to save space ", "page_idx": 31, "bbox": [219.55487060546875, 411.14300537109375, 488.54052734375, 420.9971923828125], "page_size": [612.0, 792.0]}
{"layout": 403, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 31, "bbox": [117, 453, 155, 463], "page_size": [612.0, 792.0]}
{"layout": 404, "type": "text", "text": "It is difficult to say for sure whether the wisdom teeth need to be removed from this X-ray image, as there are  other factors that should be considered, such as the age of the patient and the position of the teeth. However, it  appears that the wisdom teeth on the bottom left and right sides of the jaw are partially impacted, meaning they  are not fully emerged from the gum line. This could be a reason for removal, as impacted wisdom teeth can cause  pain, infection, and other problems. ", "page_idx": 31, "bbox": [118, 464.6781921386719, 495.8489074707031, 508.8902587890625], "page_size": [612.0, 792.0]}
{"layout": 405, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 31, "bbox": [117, 525, 154, 534.75], "page_size": [612.0, 792.0]}
{"layout": 406, "type": "text", "text": "Which wisdom tooth is hardest to remove? ", "page_idx": 31, "bbox": [118, 535.76708984375, 258.94561767578125, 544.114501953125], "page_size": [612.0, 792.0]}
{"layout": 407, "type": "image", "page_idx": 31, "img_path": "layout_images/2309.17421v2_65.jpg", "bbox": [118, 549, 213, 607], "page_size": [612.0, 792.0], "ocr_text": "ee\n", "vlm_text": "This image is a panoramic dental X-ray, also known as an orthopantomogram (OPG). It shows a comprehensive view of the upper and lower jaws, including the teeth, jawbones, and surrounding structures. This type of X-ray is commonly used by dentists to evaluate overall oral health, detect issues like impacted teeth, and plan treatments."}
{"layout": 408, "type": "text", "text": "Note: Same query image as shown above, resized to save space ", "page_idx": 31, "bbox": [219.55487060546875, 574.4552001953125, 488.54052734375, 584.3093872070312], "page_size": [612.0, 792.0]}
{"layout": 409, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 31, "bbox": [117, 612, 155, 622], "page_size": [612.0, 792.0]}
{"layout": 410, "type": "text", "text": "The lower wisdom teeth, especially those that are impacted, are typically considered the hardest to remove. This  is because the lower jawbone is denser than the upper jawbone, and the impacted wisdom teeth may be more  difficult to access. Additionally, the roots of the lower wisdom teeth are often longer and more curved than those  of the upper wisdom teeth, making them more challenging to remove. ", "page_idx": 31, "bbox": [118, 624.1477661132812, 491.2188415527344, 659.3936767578125], "page_size": [612.0, 792.0]}
{"layout": 411, "type": "text", "text": "Figure 18: Results on medical image understanding. GPT-4V recognizes both the teeth and jaw bones in the given  $\\mathrm{X}$  -ray, and explains that the partially emerged wisdom teeth on the bottom left and right sides of the jaw may necessitate removal. Check Section  4.1  for detailed discussions. The medical images are collected from the Internet. ", "page_idx": 31, "bbox": [108, 675.0945434570312, 504, 719.8264770507812], "page_size": [612.0, 792.0]}
{"layout": 412, "type": "text", "text": "Medical Image Understanding ", "page_idx": 32, "bbox": [118, 137.58544921875, 254.257080078125, 148.31781005859375], "page_size": [612.0, 792.0]}
{"layout": 413, "type": "text", "text": "Prompt: What’s wrong? ", "page_idx": 32, "bbox": [118, 153.7245330810547, 167.51321411132812, 173.48301696777344], "page_size": [612.0, 792.0]}
{"layout": 414, "type": "image", "page_idx": 32, "img_path": "layout_images/2309.17421v2_66.jpg", "bbox": [117, 178, 234, 361], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image is an X-ray of a human left foot. The red arrow is pointing to a fracture in one of the metatarsal bones. It appears to be the second metatarsal from the left, which corresponds to the bone in the area of the middle toe. The fracture seems to be located in the shaft of the metatarsal."}
{"layout": 415, "type": "text", "text": "GPT-4V: There appears to be a fracture or break in the fifth metatarsal bone (the bone on the outside of the foot that  connects to the pinky toe). This is commonly known as a \"Jones fracture.\"  ", "page_idx": 32, "bbox": [118, 366.3506774902344, 473.07733154296875, 395.0753173828125], "page_size": [612.0, 792.0]}
{"layout": 416, "type": "text", "text": "Prompt: Look at the CT scan of lung in the image below, tell me what's wrong. ", "page_idx": 32, "bbox": [118, 409.5159912109375, 346.8380432128906, 429.27447509765625], "page_size": [612.0, 792.0]}
{"layout": 417, "type": "image", "page_idx": 32, "img_path": "layout_images/2309.17421v2_67.jpg", "img_caption": "GPT-4V: There appears to be multiple areas of consolidation and ground-glass opacities in both lungs, which could  indicate a lung infection or inflammation. There is also a possible mass or nodule in the right upper lobe. Further  evaluation and clinical correlation is needed to determine the exact diagnosis.  ", "bbox": [116, 434, 488, 633], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image is a CT scan of the chest, showing multiple areas of consolidation and ground-glass opacities in both lungs. These findings may indicate a lung infection or inflammation. There's also a possible mass or nodule in the right upper lobe. Further evaluation and clinical correlation are recommended to determine the exact diagnosis."}
{"layout": 418, "type": "text", "text": "Figure 19: Results on medical image understanding. GPT-4V can identify common conditions like a Jones fracture. It could also point out potential concerns based on the CT scan of the lung. Check Section  4.1  for detailed discussions. The medical images are from the internet. ", "page_idx": 32, "bbox": [107, 646.53857421875, 504, 680.3624877929688], "page_size": [612.0, 792.0]}
{"layout": 419, "type": "text", "text": "Sec. 4.1 Image Description on Diverse Domains ", "text_level": 1, "page_idx": 33, "bbox": [128, 164, 404, 182], "page_size": [612.0, 792.0]}
{"layout": 420, "type": "text", "text": "Logo Recognition and Description ", "text_level": 1, "page_idx": 33, "bbox": [116, 189, 277, 201], "page_size": [612.0, 792.0]}
{"layout": 421, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 33, "bbox": [117, 206, 155, 215], "page_size": [612.0, 792.0]}
{"layout": 422, "type": "text", "text": "Describe the image ", "page_idx": 33, "bbox": [118, 216.90438842773438, 182.59877014160156, 225.2517852783203], "page_size": [612.0, 792.0]}
{"layout": 423, "type": "image", "page_idx": 33, "img_path": "layout_images/2309.17421v2_68.jpg", "bbox": [124, 232, 469, 323], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows three car logos:\n\n1. BMW (left)\n2. Infiniti (middle)\n3. Maserati (right)"}
{"layout": 424, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 33, "bbox": [117, 331, 155, 342], "page_size": [612.0, 792.0]}
{"layout": 425, "type": "text", "text": "The image is composed of three different icons arranged horizontally, with a thin vertical line separating each icon.  On the left is an icon resembling the letter  $\"\\bigcirc\"$   with a blue horizontal line running through the middle. This is the  logo of the car brand BMW. In the middle is an icon composed of two overlapping triangles, with the points facing  upwards. This is the logo of the car brand Infiniti. On the right is an icon composed of a shield-like shape with  three arrows pointing upwards. This is the logo of the car brand Maserati. ", "page_idx": 33, "bbox": [118, 343.7115173339844, 496.9630126953125, 387.923583984375], "page_size": [612.0, 792.0]}
{"layout": 426, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 33, "bbox": [117, 403, 155, 413.75], "page_size": [612.0, 792.0]}
{"layout": 427, "type": "text", "text": "Describe the logos in details ", "page_idx": 33, "bbox": [118, 415.0562744140625, 211.64797973632812, 423.4036560058594], "page_size": [612.0, 792.0]}
{"layout": 428, "type": "image", "page_idx": 33, "img_path": "layout_images/2309.17421v2_69.jpg", "bbox": [119, 428, 235, 456], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image contains three car logos:\n\n1. **Left**: BMW logo\n2. **Center**: Infiniti logo\n3. **Right**: Maserati logo"}
{"layout": 429, "type": "text", "text": "Note: Same query image as shown above, resized to  save space ", "page_idx": 33, "bbox": [247, 433.06671142578125, 468, 451.8870849609375], "page_size": [612.0, 792.0]}
{"layout": 430, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 33, "bbox": [117, 462, 155, 472], "page_size": [612.0, 792.0]}
{"layout": 431, "type": "text", "text": "Leftmost logo is BMW’s logo. It is a circle icon which is filled with black color. Inside the circle, there is a blue  horizontal line running through the middle. The area above the blue line is white and the area below the blue line  is light blue. This logo represents a spinning propeller of an airplane, which is a nod to the company’s history in  aviation. ", "page_idx": 33, "bbox": [118, 473.97674560546875, 495, 509.22259521484375], "page_size": [612.0, 792.0]}
{"layout": 432, "type": "text", "text": "The middle logo is the Infiniti logo. It is composed of two overlapping triangles, with the points facing upward. The  triangles are filled in with black color. The logo represents a mountain or a road, which is a nod to the company’s  ethos of power, strength, and performance. ", "page_idx": 33, "bbox": [118, 518.8074951171875, 495, 545.0872802734375], "page_size": [612.0, 792.0]}
{"layout": 433, "type": "text", "text": "The rightmost logo is the Maserati logo. It is composed of a shield-like shape and has three red arrows pointing  upward. The shield shape is filled in with a faint white color, and the area between the arrows is black. This logo is  a nod to the company’s history in racing, with the arrows representing the company’s victories. ", "page_idx": 33, "bbox": [118, 554.6721801757812, 495, 580.9519653320312], "page_size": [612.0, 792.0]}
{"layout": 434, "type": "text", "text": "Figure 20: Results on logo recognition. GPT-4V correctly recognizes the logos and provides detailed descriptions, including its design, color, shape, and symbol. Check Section  4.1  for detailed discussions. ", "page_idx": 33, "bbox": [108, 594.76953125, 504, 628.5924682617188], "page_size": [612.0, 792.0]}
{"layout": 435, "type": "text", "text": "In-the-Wild Logo Recognition and Description  ", "text_level": 1, "page_idx": 34, "bbox": [97, 39, 292, 50], "page_size": [612.0, 792.0]}
{"layout": 436, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 34, "bbox": [97, 55, 131, 64], "page_size": [612.0, 792.0]}
{"layout": 437, "type": "image", "page_idx": 34, "img_path": "layout_images/2309.17421v2_70.jpg", "img_caption": "Describe both the image and logo in details ", "bbox": [96, 65, 231, 155], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image features a serving of McDonald’s French fries, which are known for their long, thin, golden-brown appearance. The fries are held in a classic McDonald's red paper container. The red carton features the iconic McDonald’s logo prominently in the center, which is a large yellow \"M.\" This \"M,\" often referred to as the \"Golden Arches,\" is a globally recognized symbol of the McDonald's brand. The background of the image is a solid yellow, complementing the branding colors of McDonald's."}
{"layout": 438, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 34, "bbox": [97, 159, 131, 168.75], "page_size": [612.0, 792.0]}
{"layout": 439, "type": "text", "text": "The image is of a McDonald’s french fry container, filled with  golden brown french fries. The container is red, and has the  iconic McDonald’s “M” logo on it. The background is a bright  yellow, making the red container and fries stand out. The logo is the McDonald’s “M” which is a large golden arch,  forming the letter “M” for McDonald’s. It is one of the most  recognized logos in the world, and is often associated with the  fast food chain. ", "page_idx": 34, "bbox": [97, 170.38363647460938, 292, 235.9647216796875], "page_size": [612.0, 792.0]}
{"layout": 440, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 34, "bbox": [97, 249, 131, 258.75], "page_size": [612.0, 792.0]}
{"layout": 441, "type": "text", "text": "Describe both the image and logo in details ", "page_idx": 34, "bbox": [97, 259.635009765625, 229.61500549316406, 267.3333435058594], "page_size": [612.0, 792.0]}
{"layout": 442, "type": "image", "page_idx": 34, "img_path": "layout_images/2309.17421v2_71.jpg", "bbox": [98, 271, 216, 351], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows a person standing in front of large Olympic rings. The rings are colored blue, yellow, black, green, and red, which are the traditional colors of the Olympic symbol. The person appears to be wearing traditional clothing and is standing outdoors in a park-like setting."}
{"layout": 443, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 34, "bbox": [97, 356, 131, 366], "page_size": [612.0, 792.0]}
{"layout": 444, "type": "text", "text": "The image shows a woman in traditional Japanese clothing  standing in front of the Olympic rings logo. The woman is  wearing a purple kimono with a floral pattern and a red obi belt.  She is smiling and posing for the photo. The Olympic rings logo is made up of five interlocking rings of  different colors: blue, yellow, black, green, and red. The rings  are arranged in a way that they form a flower-like shape. The  logo is displayed on a grassy area with trees in the background.  ", "page_idx": 34, "bbox": [97, 367.1314697265625, 292, 432.71258544921875], "page_size": [612.0, 792.0]}
{"layout": 445, "type": "text", "text": "Prompt: Describe both the image and logo in details ", "text_level": 1, "page_idx": 34, "bbox": [95, 447, 230, 466], "page_size": [612.0, 792.0]}
{"layout": 446, "type": "image", "page_idx": 34, "img_path": "layout_images/2309.17421v2_72.jpg", "bbox": [99, 469, 240, 549], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows a close-up of a pair of white sneakers with the Nike logo. These are likely Nike Air Force 1 shoes, characterized by their distinctive design and logo."}
{"layout": 447, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 34, "bbox": [97, 552, 131, 561], "page_size": [612.0, 792.0]}
{"layout": 448, "type": "text", "text": "The image shows a close-up of a person’s foot wearing a white  Nike Air Force 1 sneaker. The sneaker is a low-top style with a  white rubber sole and white laces. The shoe’s upper is made of  a textured white leather, giving it a premium look. The logo shown on the sneaker is the iconic Nike Swoosh. It is  displayed on the side of the shoe in a silver color. The Swoosh  is a curved, elongated checkmark shape with a pointed tip at  the end. It represents speed and motion, embodying the brand’s  athletic and sporty aesthetic. ", "page_idx": 34, "bbox": [97, 562.7513427734375, 292, 636.6013793945312], "page_size": [612.0, 792.0]}
{"layout": 449, "type": "image", "page_idx": 34, "img_path": "layout_images/2309.17421v2_73.jpg", "img_caption": "Prompt: Describe both the image and logo in details ", "bbox": [304, 55, 438, 156], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image displays two Starbucks cups placed on a wooden surface, likely a table in a Starbucks café. The background shows a blurred view of shelves and a counter, typical of a café setting, with soft, warm lighting.\n\n1. The first cup on the left is a clear plastic cup filled with an iced beverage, possibly an iced coffee or latte. The cup is tall and cylindrical with a dome lid, indicating a cold drink. The beverage inside is light brown, suggesting it might contain coffee with cream or milk. The iconic Starbucks logo is prominently displayed on the cup. The logo features a two-tailed mermaid figure or siren, encircled by a green ring with the word \"Starbucks\" written above and \"Coffee\" written below.\n\n2. The second cup on the right is a standard Starbucks white paper cup with a cardboard sleeve wrapped around it, indicating a hot drink. The sleeve features the same Starbucks siren logo but is stylized in a more minimalist and monochromatic green design, likely as a more contemporary or artistic variation of the traditional logo. The lid is white and fits securely on top of the cup, typical of hot beverage cups.\n\nOverall, the image captures the quintessential Starbucks experience with both hot and cold drink options, showcasing the brand's recognizable and iconic logo."}
{"layout": 450, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 34, "bbox": [305, 159, 340, 169], "page_size": [612.0, 792.0]}
{"layout": 451, "type": "text", "text": "The image shows two Starbucks cups, one is a clear plastic cup with  a lid and straw, and the other is a white paper cup with a lid. The clear  plastic cup contains a cold drink with ice and whipped cream on top,  while the white paper cup contains a hot drink. The cups are placed  on a wooden table with a blurred background of a Starbucks store. The logo on the cups is the Starbucks logo, which is a green circle  with a white mermaid or siren in the center ", "page_idx": 34, "bbox": [305, 170.21214294433594, 519, 227.52426147460938], "page_size": [612.0, 792.0]}
{"layout": 452, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 34, "bbox": [305, 249, 340, 257.75], "page_size": [612.0, 792.0]}
{"layout": 453, "type": "text", "text": "Describe both the image and logo in details ", "page_idx": 34, "bbox": [305, 259.5165710449219, 437, 267.21490478515625], "page_size": [612.0, 792.0]}
{"layout": 454, "type": "image", "page_idx": 34, "img_path": "layout_images/2309.17421v2_74.jpg", "bbox": [306, 271, 424, 351], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows a Coca-Cola bottle and a Coca-Cola can lying on ice. The bottle has a label that reads \"New Grip Bottle\" and both the bottle and the can have droplets of condensation on them, suggesting they are cold."}
{"layout": 455, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 34, "bbox": [305, 354, 340, 363], "page_size": [612.0, 792.0]}
{"layout": 456, "type": "text", "text": "The image shows two Coca-Cola bottles lying on a bed of ice cubes.  The bottles are wet, indicating that they are chilled and ready to drink.  The bottle on the left is a new grip bottle, as mentioned on the label.  The bottle on the right is a regular Coca-Cola bottle. Both bottles  have the iconic Coca-Cola logo on them. The logo is written in a cursive font in white color, with a red  background. The first \"C\" in Coca is larger than the rest of the letters,  and the \"o\" in Cola is elongated. The logo is instantly recognizable  and is synonymous with the brand.  ", "page_idx": 34, "bbox": [305, 364.6504821777344, 519, 438.5005798339844], "page_size": [612.0, 792.0]}
{"layout": 457, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 34, "bbox": [304, 447, 338, 456], "page_size": [612.0, 792.0]}
{"layout": 458, "type": "text", "text": "Describe both the image and logo in details ", "page_idx": 34, "bbox": [305, 457.673583984375, 437, 465.3719177246094], "page_size": [612.0, 792.0]}
{"layout": 459, "type": "image", "page_idx": 34, "img_path": "layout_images/2309.17421v2_75.jpg", "bbox": [307, 469, 438, 549], "page_size": [612.0, 792.0], "ocr_text": "Microsoft 365 Copilot\n\n", "vlm_text": "The image shows a person standing next to a presentation slide. The slide is titled \"Microsoft 365 Copilot\" and includes a diagram labeled \"Natural Language\" with components such as \"Large Language Models,\" \"Microsoft Graph - Your Data,\" and \"Microsoft 365 Apps.\""}
{"layout": 460, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 34, "bbox": [304, 552, 339, 561], "page_size": [612.0, 792.0]}
{"layout": 461, "type": "text", "text": "The image is a screenshot from a presentation about Microsoft 365  Copilot. The image shows a man standing in front of a large screen  with the Microsoft 365 Copilot logo and some text on it. The man is  wearing a blue sweater and is gesturing with his hands as he speaks.  The background is a modern-looking set with a green plant and a  gray wall. ", "page_idx": 34, "bbox": [305, 562.8075561523438, 519, 611.8507080078125], "page_size": [612.0, 792.0]}
{"layout": 462, "type": "text", "text": "The logo for Microsoft 365 Copilot is a square with rounded corners.  The background of the logo is white, and the text \"Microsoft 365  Copilot\" is written in black. In the center of the logo, there is a colorful  shape that resembles a cube with three sides visible. The colors of  the shape are blue, purple, and pink, and the shape is slightly tilted to  the right. The logo is simple and modern, with a clean and  professional look.  ", "page_idx": 34, "bbox": [305, 612.42138671875, 519, 669.7333984375], "page_size": [612.0, 792.0]}
{"layout": 463, "type": "text", "text": "Sec. 4.1 Image Description on Diverse Domains ", "text_level": 1, "page_idx": 35, "bbox": [128, 79, 399, 97], "page_size": [612.0, 792.0]}
{"layout": 464, "type": "text", "text": "Scene Understanding ", "text_level": 1, "page_idx": 35, "bbox": [117, 104, 218, 115], "page_size": [612.0, 792.0]}
{"layout": 465, "type": "text", "text": "Prompt: Describe the image ", "page_idx": 35, "bbox": [118, 120.75272369384766, 182, 140.51121520996094], "page_size": [612.0, 792.0]}
{"layout": 466, "type": "image", "page_idx": 35, "img_path": "layout_images/2309.17421v2_76.jpg", "bbox": [117, 144, 495, 332], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows a view of a road taken from the perspective of a vehicle driver. It features a two-lane road with a clear dividing line in the middle and a speed limit sign indicating a speed limit of 35 mph. There are also some traffic cones visible further down the road, suggesting a construction zone or lane closure. Dense green trees and shrubbery line the sides of the road, and a large, dark cloud formation dominates the sky, indicating overcast or possibly stormy weather. The scene gives a sense of a rural or semi-rural area."}
{"layout": 467, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 35, "bbox": [117, 338, 154, 348], "page_size": [612.0, 792.0]}
{"layout": 468, "type": "text", "text": "The image is a screenshot from a dashcam, taken from the inside of a car, showing the view of the road ahead.  The car is driving on a two-lane road, with a speed limit sign of 35 mph visible on the right side of the road.  There are trees on both sides of the road, and the sky is filled with dark clouds, suggesting that it might rain  soon. Orange cones are visible on the left side of the road, indicating a construction zone. In the distance, there  is a bridge overpass. ", "page_idx": 35, "bbox": [118, 349.9134826660156, 489, 394.1254577636719], "page_size": [612.0, 792.0]}
{"layout": 469, "type": "text", "text": "Prompt: Describe the image ", "page_idx": 35, "bbox": [118, 409.69244384765625, 182, 429.450927734375], "page_size": [612.0, 792.0]}
{"layout": 470, "type": "image", "page_idx": 35, "img_path": "layout_images/2309.17421v2_77.jpg", "bbox": [117, 435, 495, 621], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows a road with a car in front, captured from another vehicle, possibly through a dashboard camera. There are cloudy skies overhead, suggesting overcast or impending rain. On both sides of the road, there are pedestrian crossing signs. The road has pedestrian islands and bollards separating lanes. Trees and vegetation are visible along the road, providing a natural backdrop."}
{"layout": 471, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 35, "bbox": [117, 631, 155, 641], "page_size": [612.0, 792.0]}
{"layout": 472, "type": "text", "text": "The image is a view of a road from the perspective of a car dashboard camera. The road has two lanes, and the  car is in the right lane behind a silver vehicle. There are two yellow signs on the side of the road warning drivers  of an upcoming pedestrian crossing and a curve to the left. The sky is cloudy and gray, and trees and bushes  line the sides of the road.  ", "page_idx": 35, "bbox": [118, 642.69580078125, 489, 677.941650390625], "page_size": [612.0, 792.0]}
{"layout": 473, "type": "text", "text": "Figure 22: Results on scene understanding. GPT-4V is able to provide a detailed description regarding the scenes and objects. Check Section  4.1  for detailed discussions. ", "page_idx": 35, "bbox": [108, 690.4205322265625, 504.00152587890625, 713.3345336914062], "page_size": [612.0, 792.0]}
{"layout": 474, "type": "text", "text": "Sec. 4.1 Image Description on Diverse Domains ", "text_level": 1, "page_idx": 36, "bbox": [131, 109, 398, 125], "page_size": [612.0, 792.0]}
{"layout": 475, "type": "text", "text": "Counter factual Examples ", "text_level": 1, "page_idx": 36, "bbox": [120, 133, 237, 144], "page_size": [612.0, 792.0]}
{"layout": 476, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 36, "bbox": [121, 150, 158, 159.75], "page_size": [612.0, 792.0]}
{"layout": 477, "type": "text", "text": "Can you find the red ball near the gray cat in the  image? ", "page_idx": 36, "bbox": [121, 161.133056640625, 288, 178.4466094970703], "page_size": [612.0, 792.0]}
{"layout": 478, "type": "image", "page_idx": 36, "img_path": "layout_images/2309.17421v2_78.jpg", "bbox": [121, 181, 231, 264], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows a gray cat lying on a cushion on a couch. In the foreground, there is a part of a laptop that appears to be facing the cat."}
{"layout": 479, "type": "text", "text": "GPT-4V: Sorry, I cannot answer this question as there is no  red ball visible in the image. ", "page_idx": 36, "bbox": [121, 268.84381103515625, 288, 297.5684509277344], "page_size": [612.0, 792.0]}
{"layout": 480, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 36, "bbox": [120, 305, 158, 315], "page_size": [612.0, 792.0]}
{"layout": 481, "type": "text", "text": "Can you spot the pink elephant playing in the  corner? ", "page_idx": 36, "bbox": [121, 316.3367919921875, 277, 333.65032958984375], "page_size": [612.0, 792.0]}
{"layout": 482, "type": "image", "page_idx": 36, "img_path": "layout_images/2309.17421v2_79.jpg", "bbox": [121, 336, 238, 422.75], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows a child wearing a red cap and orange shirt looking at an animal. The animal appears to be a bear, viewed from behind, in a zoo-like setting with rocky terrain. Part of an adult's shoulder is visible on the left side of the image."}
{"layout": 483, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 36, "bbox": [121, 423.25, 159, 433], "page_size": [612.0, 792.0]}
{"layout": 484, "type": "text", "text": "Sorry, I cannot because there is no pink elephant  playing in the corner of this image. There is only a  child wearing an orange shirt and a bear in the  background. ", "page_idx": 36, "bbox": [121, 435.4586486816406, 288, 470.70452880859375], "page_size": [612.0, 792.0]}
{"layout": 485, "type": "text", "text": "", "text_level": 1, "page_idx": 36, "bbox": [121, 482, 158, 488.75], "page_size": [612.0, 792.0]}
{"layout": 486, "type": "text", "text": "Try to locate a large waterfall behind the trees. ", "page_idx": 36, "bbox": [121, 493.5809020996094, 277, 501.92828369140625], "page_size": [612.0, 792.0]}
{"layout": 487, "type": "image", "page_idx": 36, "img_path": "layout_images/2309.17421v2_80.jpg", "bbox": [121, 505, 245, 598], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows an elephant in a grassy, wooded area with at least one person sitting on its back. There are three other people standing in the vicinity. The setting appears to be a natural or park environment, featuring lush greenery and trees around them."}
{"layout": 488, "type": "text", "text": "GPT-4V: Sorry, I am not able to complete this task as there is  no waterfall present in the given image. ", "page_idx": 36, "bbox": [121, 601.2916870117188, 295, 630.0162963867188], "page_size": [612.0, 792.0]}
{"layout": 489, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 36, "bbox": [321, 149, 359, 158.75], "page_size": [612.0, 792.0]}
{"layout": 490, "type": "text", "text": "Can you see the bright blue feathers on the bird's  wing? ", "page_idx": 36, "bbox": [322, 160.2967987060547, 489, 177.6103515625], "page_size": [612.0, 792.0]}
{"layout": 491, "type": "image", "page_idx": 36, "img_path": "layout_images/2309.17421v2_81.jpg", "bbox": [321, 181, 445, 264], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows a bird of prey, likely an eagle, flying over a grassy area. Its wings are spread wide as it glides through the air, and there's a blurred background of hills or mountains."}
{"layout": 492, "type": "text", "text": "GPT-4V: No, I cannot see bright blue feathers on the bird's  wing. It appears to have brown and white feathers. ", "page_idx": 36, "bbox": [322, 268.007568359375, 489, 296.732177734375], "page_size": [612.0, 792.0]}
{"layout": 493, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 36, "bbox": [321, 305, 359, 314], "page_size": [612.0, 792.0]}
{"layout": 494, "type": "text", "text": "Can you find the baseball player wearing a blue  shirt? ", "page_idx": 36, "bbox": [322, 315.50054931640625, 481.8407287597656, 332.8140869140625], "page_size": [612.0, 792.0]}
{"layout": 495, "type": "image", "page_idx": 36, "img_path": "layout_images/2309.17421v2_82.jpg", "bbox": [321, 332.25, 450, 430], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows a group of people wearing baseball uniforms on a field. They appear to be engaged in a practice or warm-up session. The field is surrounded by a chain-link fence, and there are trees and buildings in the background."}
{"layout": 496, "type": "text", "text": "GPT-4V: No, I cannot find a baseball player wearing a blue  shirt in this image. All the players appear to be  wearing black shirts. ", "page_idx": 36, "bbox": [322, 432.177490234375, 489, 469.8682861328125], "page_size": [612.0, 792.0]}
{"layout": 497, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 36, "bbox": [322, 481, 360, 491], "page_size": [612.0, 792.0]}
{"layout": 498, "type": "text", "text": "Can you find the blue lighting fixture mounted to the  ceiling? ", "page_idx": 36, "bbox": [322, 492.7445983886719, 496, 510.0581359863281], "page_size": [612.0, 792.0]}
{"layout": 499, "type": "image", "page_idx": 36, "img_path": "layout_images/2309.17421v2_83.jpg", "bbox": [322, 512, 432, 596], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows a kitchen with a central island countertop. There are three pendant lights hanging above the island. On the countertop, there are plates of sliced watermelon and stacked dishes. The kitchen has large windows in the back, cabinets, a coffee maker, a toaster, and a stainless steel refrigerator. The colors in the kitchen are warm, with wood and light yellow tones."}
{"layout": 500, "type": "text", "text": "GPT-4V: Sorry, I cannot find a blue lighting fixture mounted to  the ceiling in this image. All I see is a black wrought  iron chandelier with white glass shades. ", "page_idx": 36, "bbox": [322, 600.4553833007812, 496, 638.1461791992188], "page_size": [612.0, 792.0]}
{"layout": 501, "type": "text", "text": "Figure 23: Results on counter factual examples. GPT-4V is able to provide factual descriptions regarding the scenes and objects in the images. Example images are from [ 78 ]. Check Section  4.1  for detailed discussions. ", "page_idx": 36, "bbox": [108, 650.5415649414062, 504, 684.364501953125], "page_size": [612.0, 792.0]}
{"layout": 502, "type": "text", "text": "4.2 Object Localization, Counting, and Dense Captioning ", "text_level": 1, "page_idx": 37, "bbox": [106, 73, 359, 84], "page_size": [612.0, 792.0]}
{"layout": 503, "type": "text", "text": "Spatial relationship understanding.  Understanding the spatial relationship between humans and objects in the image is a vital aspect of visual intelligence [ 61 ,  14 ]. In Figure  24 , GPT-4V showcases promising capabilities in this regard. It can identify the spatial relationship between the frisbee and the man in the image. It can also recognize the spatial relationship between the man and the car in the image, and point out that the camera perspective may affect their perceived size. ", "page_idx": 37, "bbox": [108, 92.37389373779297, 505, 148.3734588623047], "page_size": [612.0, 792.0]}
{"layout": 504, "type": "text", "text": "Object counting.  Figure  25  highlights our exploration of GPT-4V’s capability in object counting. In our experiments, we employ the text prompt “Count the number of X in the image” to evaluate its performance. The results indicate that GPT-4V can successfully count the number of objects, such as apples, oranges, and people, present in the image. However, challenges arise when objects are occluded, or the scene is cluttered, which can result in errors in the counting process. In the bottom left of Figure  25 , GPT-4V identifies 12 people, but the correct answer should be 11. This may be due to our limited text prompt used in this experiment, and further investigation in prompting techniques is needed. ", "page_idx": 37, "bbox": [108, 152.39891052246094, 505, 241.12547302246094], "page_size": [612.0, 792.0]}
{"layout": 505, "type": "text", "text": "Object localization.  Object localization [ 153 ,  76 ,  51 ] is a fundamental challenge in the field of computer vision. In our preliminary experiments, we address this task by utilizing a simple text prompt, “Localize each person in the image using a bounding box.” The initial results of our object localization experiments are depicted in Figure  26 . The findings suggest that GPT-4V demonstrates the capability to generate bounding box coordinates in textual format, without separate textual i zed box tokens [ 25 ,  140 ,  129 ,  26 ,  82 ,  105 ]. However, it is important to note that the generated bounding box coordinates are not accurate. We rescaled the predicted bounding box coordinates during visualization. Promising localization results are observed when the scene or background is relatively simpler and less cluttered. Further prompting techniques are required to enhance object localization performance in more complex and crowded environments. ", "page_idx": 37, "bbox": [108, 245.1509246826172, 505, 355.6965026855469], "page_size": [612.0, 792.0]}
{"layout": 506, "type": "text", "text": "Dense captioning.  Dense captioning [ 62 ,  84 ] involves generating detailed description for each region of interest in the given image. This advanced task in vision-language field typically requires a complex system that integrates multiple experts, such as object detector, celebrity recognition model, and image captioning model. In order to explore GPT-4V’s capabilities in dense captioning, we use an instructional prompt, as shown in Figure  27 . The results are highly encouraging, as GPT-4V successfully localizes and recognizes the individuals within the image, and then provides concise descriptions for each scientist. ", "page_idx": 37, "bbox": [108, 359.7209167480469, 505, 437.53948974609375], "page_size": [612.0, 792.0]}
{"layout": 507, "type": "text", "text": "Spatial Relationship Understanding ", "page_idx": 38, "bbox": [121, 126.11089324951172, 283.1535339355469, 136.84326171875], "page_size": [612.0, 792.0]}
{"layout": 508, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 38, "bbox": [121, 143, 157, 152], "page_size": [612.0, 792.0]}
{"layout": 509, "type": "text", "text": "What is the spatial relation between the frisbee and the man? ", "page_idx": 38, "bbox": [121, 153.66107177734375, 323.9918518066406, 162.0084686279297], "page_size": [612.0, 792.0]}
{"layout": 510, "type": "image", "page_idx": 38, "img_path": "layout_images/2309.17421v2_84.jpg", "bbox": [120, 167, 246, 354], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image depicts an outdoor scene where a person is playing with a dog. The person, wearing a red sweatshirt, blue shorts, and a white cap and shoes, appears to be engaged in an activity similar to frisbee or disc play. They are crouched down, creating a platform with their back for the dog to leap from. A black and white dog is jumping in the air toward an orange frisbee or disc, which is captured mid-flight. The background shows a grassy field with trees, suggesting the setting is a park or open natural area."}
{"layout": 511, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 38, "bbox": [120, 359, 158, 369], "page_size": [612.0, 792.0]}
{"layout": 512, "type": "text", "text": "Based on the image provided, the spatial relation between the frisbee and the man is that the frisbee is above  and slightly in front of the man. It also appears to be moving away from the man, as if it has been thrown or  released by him. ", "page_idx": 38, "bbox": [121, 371.4107360839844, 485.03643798828125, 397.6903991699219], "page_size": [612.0, 792.0]}
{"layout": 513, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 38, "bbox": [120, 416, 158, 426], "page_size": [612.0, 792.0]}
{"layout": 514, "type": "text", "text": "Does the person bigger than the car? ", "page_idx": 38, "bbox": [121, 427.6614074707031, 244.74598693847656, 436.0087890625], "page_size": [612.0, 792.0]}
{"layout": 515, "type": "image", "page_idx": 38, "img_path": "layout_images/2309.17421v2_85.jpg", "img_caption": "GPT-4V: No, the car appears to be bigger than the person in this image. However, the perspective and distance of the  objects in the photo may affect their perceived size. ", "bbox": [119, 438, 481, 625], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows a man standing on the side of a road observing a cyclist and a car. The man is in the foreground, wearing a white t-shirt and jeans, and he is holding something in his hands. In the background, there is a cyclist on the road and a car approaching. The size of the car compared to the cyclist and the man is affected by the perspective and distance in the image, making the car appear larger relative to the other objects. The setting is outdoors on a clear day with a rocky hill or mountain in the background."}
{"layout": 516, "type": "text", "text": "Figure 24: Results on spatial relationship understanding. GPT-4V recognizes the spatial relationship between the objects in the images. Example images are from [ 67 ,  14 ]. Check Section  4.2  for detailed discussions. ", "page_idx": 38, "bbox": [108, 643.069580078125, 504, 676.8925170898438], "page_size": [612.0, 792.0]}
{"layout": 517, "type": "text", "text": "Sec. 4.2 Object Localization, Counting, and Dense Captioning  ", "text_level": 1, "page_idx": 39, "bbox": [122, 115, 476, 136], "page_size": [612.0, 792.0]}
{"layout": 518, "type": "text", "text": "Object Counting ", "text_level": 1, "page_idx": 39, "bbox": [116, 141, 194, 153], "page_size": [612.0, 792.0]}
{"layout": 519, "type": "text", "text": "Prompt: Count the number of apples in the image. ", "page_idx": 39, "bbox": [118, 158.2465057373047, 255, 178.00498962402344], "page_size": [612.0, 792.0]}
{"layout": 520, "type": "image", "page_idx": 39, "img_path": "layout_images/2309.17421v2_86.jpg", "bbox": [117, 179, 278, 286], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows a group of apples on a wooden surface, with some greenery in the background. The apples are red and yellow in color."}
{"layout": 521, "type": "text", "text": "GPT-4V: There are 7 apples in the image. ", "page_idx": 39, "bbox": [118, 286.33453369140625, 226, 306.0929870605469], "page_size": [612.0, 792.0]}
{"layout": 522, "type": "text", "text": "Prompt: Count the number of oranges in the image. ", "page_idx": 39, "bbox": [118, 324.5990905761719, 260.4488830566406, 344.3575744628906], "page_size": [612.0, 792.0]}
{"layout": 523, "type": "image", "page_idx": 39, "img_path": "layout_images/2309.17421v2_87.jpg", "bbox": [118, 349, 278, 450], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows three oranges placed on a red surface. One of the oranges is balanced on top of the other two. The top orange has a small branch with two green leaves still attached to it. In the background, there appears to be a piece of paper with black calligraphy or artwork on the red surface, a common sight during cultural or festive arrangements, likely linking to a context like a celebration or an event."}
{"layout": 524, "type": "text", "text": "GPT-4V: There are 3 oranges in the image. ", "page_idx": 39, "bbox": [118, 452.6871337890625, 230.98011779785156, 472.4455871582031], "page_size": [612.0, 792.0]}
{"layout": 525, "type": "text", "text": "Prompt: Count the number of people in the image. ", "page_idx": 39, "bbox": [118, 489.62689208984375, 255, 509.3853759765625], "page_size": [612.0, 792.0]}
{"layout": 526, "type": "image", "page_idx": 39, "img_path": "layout_images/2309.17421v2_88.jpg", "bbox": [117, 509.25, 261, 617.75], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows four individuals standing indoors, each wearing a name tag, suggesting they might be attending a conference or formal gathering. Behind them is a room with hanging lights and a partially opened door. The individuals are casually dressed in sweaters and shirts."}
{"layout": 527, "type": "text", "text": "GPT-4V: There are 4 people in the image. ", "page_idx": 39, "bbox": [118, 617.7149047851562, 226, 637.473388671875], "page_size": [612.0, 792.0]}
{"layout": 528, "type": "text", "text": "Prompt: Count the number of apples in the image. ", "page_idx": 39, "bbox": [311, 158.08628845214844, 448, 177.8447723388672], "page_size": [612.0, 792.0]}
{"layout": 529, "type": "image", "page_idx": 39, "img_path": "layout_images/2309.17421v2_89.jpg", "bbox": [311, 179, 420, 285.75], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows a collection of apples arranged in a grid pattern. The apples are of different varieties and colors, including shades of red, yellow, and green."}
{"layout": 530, "type": "text", "text": "GPT-4V: There are 16 apples in the image. ", "page_idx": 39, "bbox": [311, 286.17431640625, 423, 305.9327697753906], "page_size": [612.0, 792.0]}
{"layout": 531, "type": "text", "text": "Prompt: Count the number of people in the image. ", "page_idx": 39, "bbox": [311, 324.4388732910156, 448, 344.1973571777344], "page_size": [612.0, 792.0]}
{"layout": 532, "type": "image", "page_idx": 39, "img_path": "layout_images/2309.17421v2_90.jpg", "bbox": [312, 350, 470, 450], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows three people standing with their arms crossed, wearing casual clothing that includes two plaid shirts and one solid-colored shirt. The background is plain white, and there is a blue border surrounding the image."}
{"layout": 533, "type": "text", "text": "GPT-4V: There are 3 people in the image. ", "page_idx": 39, "bbox": [311, 452.52691650390625, 419.4640808105469, 472.2853698730469], "page_size": [612.0, 792.0]}
{"layout": 534, "type": "text", "text": "Prompt: Count the number of people in the image. ", "page_idx": 39, "bbox": [311, 489.4667053222656, 448, 509.2251892089844], "page_size": [612.0, 792.0]}
{"layout": 535, "type": "image", "page_idx": 39, "img_path": "layout_images/2309.17421v2_91.jpg", "bbox": [311, 509.25, 471, 617.75], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image depicts a group of people gathered around a dining table, which is set with plates of food, drinks, and cutlery. The table has several dishes, including bread, salad, and various condiments. There are also bottles of wine and glasses on the table, indicating a shared meal or gathering. The setting appears to be indoors, possibly in a home or restaurant."}
{"layout": 536, "type": "text", "text": "GPT-4V: There are 12 people in the image. ", "page_idx": 39, "bbox": [311, 617.5547485351562, 423, 637.313232421875], "page_size": [612.0, 792.0]}
{"layout": 537, "type": "text", "text": "Figure 25: Results on object counting. GPT-4V is able to determine the quantity of the specified objects the image. Red highlights the wrong answer. Check Section  4.2  for detailed discussions. ", "page_idx": 39, "bbox": [108, 650.9995727539062, 503.9972229003906, 675.9014892578125], "page_size": [612.0, 792.0]}
{"layout": 538, "type": "text", "text": "Object Localization ", "text_level": 1, "page_idx": 40, "bbox": [117, 50, 207, 61], "page_size": [612.0, 792.0]}
{"layout": 539, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 40, "bbox": [116, 67, 154, 76.75], "page_size": [612.0, 792.0]}
{"layout": 540, "type": "text", "text": "Localize each person in the image using bounding  box. The image size is (width, height)  $,=(998{,}786)$  ", "page_idx": 40, "bbox": [118, 77.78289794921875, 286, 95.0964584350586], "page_size": [612.0, 792.0]}
{"layout": 541, "type": "image", "page_idx": 40, "img_path": "layout_images/2309.17421v2_92.jpg", "bbox": [118, 99, 202, 164], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows a person wearing a sparkly purple outfit while holding a microphone on stage during a performance. It appears to be raining, as there are visible raindrops in the scene."}
{"layout": 542, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 40, "bbox": [117, 171, 155, 181], "page_size": [612.0, 792.0]}
{"layout": 543, "type": "text", "text": "There is only one person in the image, so the  bounding box for this person would be: - Top left: (254, 118) - Bottom right: (731, 693) ", "page_idx": 40, "bbox": [118, 182.81504821777344, 269.6244201660156, 218.06097412109375], "page_size": [612.0, 792.0]}
{"layout": 544, "type": "text", "text": "Explanation: The top left point of the bounding box is  at   $({\\mathsf{x}},{\\mathsf{y}})=(254,\\,118)$  , which is the top left corner of the  person in the image. The bottom right point of the  bounding box is at   $({\\sf x},{\\sf y})=(731,693)$  , which is the  bottom right corner of the person in the image. This  bounding box encompasses the entire person in the  image.  ", "page_idx": 40, "bbox": [118, 227.64588928222656, 297, 289.790283203125], "page_size": [612.0, 792.0]}
{"layout": 545, "type": "text", "text": "Visualization of Detected Bbox: ", "text_level": 1, "page_idx": 40, "bbox": [117, 299, 262, 310], "page_size": [612.0, 792.0]}
{"layout": 546, "type": "image", "page_idx": 40, "img_path": "layout_images/2309.17421v2_93.jpg", "bbox": [118, 313, 201, 379], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows a person performing on stage, wearing a sparkly purple outfit. It appears to be raining during the performance."}
{"layout": 547, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 40, "bbox": [311, 68, 349, 77], "page_size": [612.0, 792.0]}
{"layout": 548, "type": "text", "text": "Localize each car in the image using bounding box.  What is the image size of the input image? ", "page_idx": 40, "bbox": [312, 79.17445373535156, 482, 96.4880142211914], "page_size": [612.0, 792.0]}
{"layout": 549, "type": "image", "page_idx": 40, "img_path": "layout_images/2309.17421v2_94.jpg", "bbox": [313, 97.25, 411, 177], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows two cars driving on a multi-lane highway. The car on the left is black, and the car on the right is white. They are both traveling in the same direction. The background includes trees and a clear blue sky."}
{"layout": 550, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 40, "bbox": [312, 182, 349, 191], "page_size": [612.0, 792.0]}
{"layout": 551, "type": "text", "text": "Bounding box for the black car: - Top left corner: (63, 114) - Bottom right corner: (196, 220) ", "page_idx": 40, "bbox": [312, 211.10508728027344, 421, 237.38482666015625], "page_size": [612.0, 792.0]}
{"layout": 552, "type": "text", "text": "Bounding box for the white car: - Top left corner: (287, 96) - Bottom right corner: (474, 220)  ", "page_idx": 40, "bbox": [312, 246.9697723388672, 421, 273.24951171875], "page_size": [612.0, 792.0]}
{"layout": 553, "type": "text", "text": "Visualization of Detected Bbox: ", "text_level": 1, "page_idx": 40, "bbox": [312, 283, 456, 293], "page_size": [612.0, 792.0]}
{"layout": 554, "type": "image", "page_idx": 40, "img_path": "layout_images/2309.17421v2_95.jpg", "bbox": [313, 295, 411, 375], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows two cars on a multi-lane road. There are red boxes around each car, likely indicating they are being highlighted for some reason, such as object detection. The background includes trees and a highway setting."}
{"layout": 555, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 40, "bbox": [116, 391, 155, 400.75], "page_size": [612.0, 792.0]}
{"layout": 556, "type": "text", "text": "Localize each person in the image using bounding box. What is the image size of the input image? ", "page_idx": 40, "bbox": [118, 402.649658203125, 443.13677978515625, 410.9970397949219], "page_size": [612.0, 792.0]}
{"layout": 557, "type": "image", "page_idx": 40, "img_path": "layout_images/2309.17421v2_96.jpg", "bbox": [118, 413, 294, 481], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows a group of people walking across a crosswalk in a city setting. There are cars and a bus on the road, with more pedestrians and buildings visible in the background. Trees line the street, suggesting an urban environment."}
{"layout": 558, "type": "text", "text": "GPT-4V: The image size of the input image is   $1000\\times333$   pixels. ", "page_idx": 40, "bbox": [118, 483.4619140625, 299.9526062011719, 503.22039794921875], "page_size": [612.0, 792.0]}
{"layout": 559, "type": "text", "text": "Bounding boxes for each person: - Person 1 (leftmost):   $(\\mathsf{x}1,\\mathsf{y}1)=(22,\\,108)$  ,  $(\\mathsf{x}2,\\mathsf{y}2)=(170,300)$  - Person 2: (x1, y1)  $=$   (200, 80),  $(\\mathsf{x}2,\\mathsf{y}2)=(290,270)$  - Person 3: (x1, y1)  $=$   (350, 70),  $({\\mathsf{x}}2,{\\mathsf{y}}2)=(460,280)$  - Person 4: (x1, y1)  $=$   (540,   $\\mathsf{30),}\\,\\mathsf{(x2,y2)}=\\mathsf{(640,270)}$   $=$   $(\\mathsf{x}2,\\mathsf{y}2)=(920,290)$  ", "page_idx": 40, "bbox": [118, 512.8052978515625, 334, 567], "page_size": [612.0, 792.0]}
{"layout": 560, "type": "text", "text": "Note: These bounding box coordinates are approximate and may not be exact.  ", "page_idx": 40, "bbox": [118, 575.5684814453125, 379.9230651855469, 583.9158935546875], "page_size": [612.0, 792.0]}
{"layout": 561, "type": "text", "text": "Visualization of Detected Bbox: ", "text_level": 1, "page_idx": 40, "bbox": [117, 588, 261, 599], "page_size": [612.0, 792.0]}
{"layout": 562, "type": "image", "page_idx": 40, "img_path": "layout_images/2309.17421v2_97.jpg", "bbox": [117, 601, 294, 671], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows people crossing a street at a crosswalk. Red rectangles are overlaid on each person, likely indicating they are being detected or highlighted for some purpose, possibly related to pedestrian detection or analysis in an urban environment. There are vehicles and other pedestrians visible in the background."}
{"layout": 563, "type": "text", "text": "Figure 26: Results on object localization. GPT-4V is able to generate and approximate the bounding box coordinates for the specified objects in the image. When providing a simple text prompt only, the model may encounter challenges when dealing with more complex scenarios like object occlusions and cluttered scenes.  Red  highlights the wrong answer. We rescaled the predictions when visualizing the bounding boxes. Check Section  4.2  for detailed discussions. ", "page_idx": 40, "bbox": [108, 682.4715576171875, 504, 740.0994873046875], "page_size": [612.0, 792.0]}
{"layout": 564, "type": "text", "text": "Dense Captioning  ", "text_level": 1, "page_idx": 41, "bbox": [116, 35, 201, 47], "page_size": [612.0, 792.0]}
{"layout": 565, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 41, "bbox": [116, 53, 155, 61.75], "page_size": [612.0, 792.0]}
{"layout": 566, "type": "text", "text": "Please follow the instructions 1. Tell me the size of the input image; 2. Localize each person in the image using bounding box; 3. Recognize each person; ", "page_idx": 41, "bbox": [118, 64.2598876953125, 308.7132263183594, 103.54053497314453], "page_size": [612.0, 792.0]}
{"layout": 567, "type": "text", "text": "4. Generate detailed caption for each bounding box. ", "page_idx": 41, "bbox": [118, 105.50421142578125, 290, 113.85161590576172], "page_size": [612.0, 792.0]}
{"layout": 568, "type": "image", "page_idx": 41, "img_path": "layout_images/2309.17421v2_98.jpg", "bbox": [118, 117, 289, 246], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows four people standing together indoors. They are dressed in casual to semi-formal attire, with a couple of them wearing conference badges. The setting appears to be at an event or conference, given the presence of name tags and lanyards."}
{"layout": 569, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 41, "bbox": [117, 254, 155, 264], "page_size": [612.0, 792.0]}
{"layout": 570, "type": "text", "text": "1. The size of the input image is   $1600\\times1067$   pixels. ", "page_idx": 41, "bbox": [118, 264, 290, 274.0779724121094], "page_size": [612.0, 792.0]}
{"layout": 571, "type": "text", "text": "2. Bounding box 1: (x1, y1)  $=$  (120, 250),   $(\\mathsf{x}2,\\mathsf{y}2)=(420,950)$   - Person 1     Bounding box 2  $\\L_{:}({\\mathsf{x}}1,{\\mathsf{y}}1)=(500,240),({\\mathsf{x}}2,{\\mathsf{y}}2)=(800,940)$   - Person 2     Bounding box 3: (x1, y1)  $=$   (880, 230), (x2, y2) = (1180, 930) - Person 3     Bounding box 4: (x1, y1)  $=$   (1260, 220),  $({\\mathsf{x}}2,{\\mathsf{y}}2)=$  (1560, 920) - Person 4 ", "page_idx": 41, "bbox": [118, 280, 366.28204345703125, 320], "page_size": [612.0, 792.0]}
{"layout": 572, "type": "text", "text": "3. Person 1: Yann LeCun     Person 2: Geoffrey Hinton     Person 3: Yoshua Bengio     Person 4: Andrew Ng ", "page_idx": 41, "bbox": [118, 326.1240539550781, 214, 365.4046936035156], "page_size": [612.0, 792.0]}
{"layout": 573, "type": "text", "text": "4. Caption for bounding box 1: Yann LeCun, computer scientist and one of the founding fathers of artificial  ", "page_idx": 41, "bbox": [118, 371.78741455078125, 468.5609436035156, 380.1347961425781], "page_size": [612.0, 792.0]}
{"layout": 574, "type": "text", "text": "intelligence.     Caption for bounding box 2: Geoffrey Hinton, computer scientist and cognitive psychologist, known for his work  on artificial neural networks.     Caption for bounding box 3: Yoshua Bengio, computer scientist and co-founder of the Montreal Institute for  Learning Algorithms.     Caption for bounding box 4: Andrew Ng, computer scientist and entrepreneur, known for his work on deep  learning and artificial intelligence.  ", "page_idx": 41, "bbox": [118, 382.0985107421875, 493.99493408203125, 452.3124084472656], "page_size": [612.0, 792.0]}
{"layout": 575, "type": "text", "text": "Visualization of Dense Captioning: ", "text_level": 1, "page_idx": 41, "bbox": [117, 464, 276, 475], "page_size": [612.0, 792.0]}
{"layout": 576, "type": "image", "page_idx": 41, "img_path": "layout_images/2309.17421v2_99.jpg", "img_caption": "Figure 27: Results on dense captioning. GPT-4V follows the text prompt and successfully generates dense captions for the input image.  Red  highlights the wrong answer. We rescaled the predictions when visualizing the bounding boxes. Check Section  4.2  for detailed discussions. ", "bbox": [106, 478, 505, 744], "page_size": [612.0, 792.0], "ocr_text": "Yann LeCun, computer\nscientist and one of the\nfounding fathers of\nartificial intelligence.\n\nffrey Hinton, computer\n\nscientist and cognitive\n\npsychologist, known for\n\nhis work on artificial neural\nvorks.\n\nthe Montreal Institute for\nLearning Algorithms.\n\nNg, computer\n\nentrepreneur\n\nork on de\n\n", "vlm_text": "The image shows four individuals, each with a colored box and caption identifying them. From left to right: \n\n1. Yann LeCun, described as a computer scientist and one of the founding fathers of artificial intelligence.\n2. Geoffrey Hinton, identified as a computer scientist and cognitive psychologist, known for his work on artificial neural networks.\n3. Yoshua Bengio, described as a computer scientist and co-founder of the Montreal Institute for Learning Algorithms.\n4. Andrew Ng, identified as a computer scientist and entrepreneur, known for his work on deep learning and artificial intelligence.\n\nThe image is used to demonstrate dense captioning, with bounding boxes highlighting sections and captions explaining the content."}
{"layout": 577, "type": "text", "text": "4.3 Multimodal Knowledge and Commonsense ", "text_level": 1, "page_idx": 42, "bbox": [106, 73, 314, 85], "page_size": [612.0, 792.0]}
{"layout": 578, "type": "text", "text": "Joke and meme.  Jokes and memes often reference specific events, pop culture, or Internet trends. Un- der standing these references requires being familiar with the relevant context and cultural knowledge. Grasping the visual elements, their relationship to the text, and the intended humorous effect can be a complex task [ 99 ]. Moreover, memes are often user-generated, making them highly diverse and ever-expanding. To evaluate GPT-4V’s ability in this domain, we input a pair of meme and text prompt to GPT-4V. The example text prompts include “Can you explain the meme?” and “What is funny about the image?” Figure  28  shows the example results. We observe that GPT-4V has remarkable ability to gather information from both visual and textual modalities, and then comprehend the humor embedded within memes. ", "page_idx": 42, "bbox": [107, 92.37389373779297, 505, 192.00950622558594], "page_size": [612.0, 792.0]}
{"layout": 579, "type": "text", "text": "Science and knowledge.  We further investigate GPT-4V’s capability in tasks that requires reasoning with scientific knowledge [ 85 ]. We conduct experiments by providing a text prompt question and a corresponding image. The questions cover a wide range of topics, including geography, physics, biology, and earth science. In Figures  29 - 31 , we observe that GPT-4V is able to correctly answer the science questions based on the visual context. For instance, in the bottom row of Figure  29 , GPT-4V recognizes the average particle speed for both sample A and sample B. By considering the relationship among particle speed, kinetic energy, and temperature, GPT-4V answers the question correctly. For another instance, as shown in the bottom row of Figure  30 , GPT-4V takes into account the visual arrows presented in the figure to identify the producer in the specific food web. Moreover, as shown in Figure  31 , when we provide a more specific prompt, such as “Suppose you are a teacher, please use the figure to explain X,” we observe the generated answer adopts a tutorial format and explains the subject step by step. ", "page_idx": 42, "bbox": [107, 196.03489685058594, 505, 328.39849853515625], "page_size": [612.0, 792.0]}
{"layout": 580, "type": "text", "text": "Multimodal commonsense.  In Figure  32 , we access the ability of GPT-4V in multimodal common- sense reasoning [ 148 ,  52 ]. In our experiments, we observed that GPT-4V effectively utilizes the bounding boxes presented in the image as visual prompts ( e.g.,  [person1] and [person2]) to recognize the actions performed by the individuals. As shown in the second example in Figure  32 , based on the formal dress worn by [person1] and [person2] and the floral decorations present in the scene, it can be inferred that they are attending a wedding ceremony. Moreover, when we provide a more specific input prompt, such as \"Suppose you are a detective, what can you infer from the visual clues?\", GPT-4V demonstrates the ability to discern numerous nuanced visual cues within the image and offers a list of plausible hypotheses. ", "page_idx": 42, "bbox": [107, 332.42291259765625, 505, 432.05950927734375], "page_size": [612.0, 792.0]}
{"layout": 581, "type": "text", "text": "Sec. 4.3 Multimodal Knowledge and Commonsense ", "text_level": 1, "page_idx": 43, "bbox": [119, 29, 433, 49], "page_size": [612.0, 792.0]}
{"layout": 582, "type": "text", "text": "Joke and Meme  ", "text_level": 1, "page_idx": 43, "bbox": [117, 56, 191, 66], "page_size": [612.0, 792.0]}
{"layout": 583, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 43, "bbox": [117, 72, 154, 82], "page_size": [612.0, 792.0]}
{"layout": 584, "type": "text", "text": "Can you explain this meme? ", "page_idx": 43, "bbox": [118, 83.45262145996094, 212.3629608154297, 91.80000305175781], "page_size": [612.0, 792.0]}
{"layout": 585, "type": "text", "text": "", "page_idx": 43, "bbox": [126, 105, 198, 120.75], "page_size": [612.0, 792.0]}
{"layout": 586, "type": "text", "text": "", "page_idx": 43, "bbox": [125, 121.25, 280, 141], "page_size": [612.0, 792.0]}
{"layout": 587, "type": "image", "page_idx": 43, "img_path": "layout_images/2309.17421v2_100.jpg", "bbox": [119, 144, 290, 256], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows a stuffed toy of a green frog, suggesting it's relaxed with its arms behind its head, lying on a bed. It resembles a well-known frog character."}
{"layout": 588, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 43, "bbox": [117, 262, 154, 272], "page_size": [612.0, 792.0]}
{"layout": 589, "type": "text", "text": "This meme is poking fun at the idea of procrastination  and how someone may put off doing a task until a  certain time, but if they miss that time, they will push  it back even further. In this case, the person in the  meme planned to do something at 8 o’clock, but  since it is now 8:05, they have decided to wait until 9  o’clock to do it. The image of Kermit the Frog relaxing  in bed represents the person’s desire to procrastinate  and avoid the task. ", "page_idx": 43, "bbox": [118, 274.30352783203125, 296, 354.37994384765625], "page_size": [612.0, 792.0]}
{"layout": 590, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 43, "bbox": [121, 382, 158, 392], "page_size": [612.0, 792.0]}
{"layout": 591, "type": "text", "text": "What is funny about this image? ", "page_idx": 43, "bbox": [121, 393.3998718261719, 228.07052612304688, 401.74725341796875], "page_size": [612.0, 792.0]}
{"layout": 592, "type": "image", "page_idx": 43, "img_path": "layout_images/2309.17421v2_101.jpg", "bbox": [118, 407, 290, 574], "page_size": [612.0, 792.0], "ocr_text": "You see the world\nas you are!\n\n", "vlm_text": "The image is a cartoon featuring a rhino painting a landscape. The rhino's paintings, however, all depict a large rhino horn as a prominent feature in the scene. There are multiple canvases, each showing different versions of the landscape with a prominent horn in the middle. The caption at the bottom reads, \"You see the world as you are!\" This image humorously illustrates the concept that individuals often perceive the world through the lens of their own identity or characteristics."}
{"layout": 593, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 43, "bbox": [121, 581, 158, 591], "page_size": [612.0, 792.0]}
{"layout": 594, "type": "text", "text": "There are a couple of things that people might find  funny about this image: - It is a cartoon of a rhinoceros painting, which is an  unexpected and silly concept in itself. - The rhinoceros is painting a picture of itself, which  adds an extra layer of humor. - The caption, “You see the world as you are!”, is a  joke about how we all have our own unique  perspectives and can be a little self-centered at  times. ", "page_idx": 43, "bbox": [121, 593.2169189453125, 296, 682.259521484375], "page_size": [612.0, 792.0]}
{"layout": 595, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 43, "bbox": [317, 72, 354, 81.75], "page_size": [612.0, 792.0]}
{"layout": 596, "type": "text", "text": "Can you explain this meme? ", "page_idx": 43, "bbox": [317, 83.45262145996094, 411.7959899902344, 91.80000305175781], "page_size": [612.0, 792.0]}
{"layout": 597, "type": "image", "page_idx": 43, "img_path": "layout_images/2309.17421v2_102.jpg", "bbox": [319, 104, 475, 301], "page_size": [612.0, 792.0], "ocr_text": "My Handwriting In Exams\n\nFirst two pages\n: ba astm h\n\nHe Sn\n\nLast two pages\n", "vlm_text": "The image is a humorous depiction of how handwriting changes throughout an exam. It shows three sections labeled \"First two pages,\" \"Middle pages,\" and \"Last two pages.\"\n\n1. **First two pages**: The handwriting is neat and legible.\n2. **Middle pages**: The handwriting becomes messier.\n3. **Last two pages**: It resembles an ECG or heartbeat pattern, indicating the handwriting has become illegible and chaotic.\n\nThe image humorously illustrates the deterioration of handwriting over time during an exam."}
{"layout": 598, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 43, "bbox": [317, 307, 354, 317], "page_size": [612.0, 792.0]}
{"layout": 599, "type": "text", "text": "This meme highlights the deterioration of a person’s  handwriting during an exam. The first two pages  show neat and legible handwriting, the middle page  shows slightly messier handwriting, and the last two  pages show almost illegible handwriting. The meme is poking fun at how, as an exam  progresses, a person’s handwriting tends to get  worse as they become more tired and rushed to  finish the exam. It also implies that the person’s focus  and attention to detail decrease towards the end of  the exam. ", "page_idx": 43, "bbox": [317, 319.1341857910156, 495, 417.14288330078125], "page_size": [612.0, 792.0]}
{"layout": 600, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 43, "bbox": [317, 431, 354, 440.75], "page_size": [612.0, 792.0]}
{"layout": 601, "type": "text", "text": "What is funny about this image? ", "page_idx": 43, "bbox": [317, 442.23126220703125, 423.8106384277344, 450.5786437988281], "page_size": [612.0, 792.0]}
{"layout": 602, "type": "image", "page_idx": 43, "img_path": "layout_images/2309.17421v2_103.jpg", "bbox": [320, 454, 490, 541], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image is a popular meme format, featuring two parts:\n\n1. On the left, a woman appears to be yelling and pointing with another woman trying to calm her.\n2. On the right, a white cat sits at a table looking confused or disinterested, with a plate of food in front of it.\n\nThis meme is often used for humorous contrasts or to express disagreement over trivial matters."}
{"layout": 603, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 43, "bbox": [317, 549, 354, 559], "page_size": [612.0, 792.0]}
{"layout": 604, "type": "text", "text": "This image is funny for a couple of reasons. Firstly, it  is a meme template that has been widely used and  recognized on the internet. The left side of the image  is a screenshot from the reality TV show \"The Real  Housewives of Beverly Hills\" where one of the cast  members is angrily pointing and yelling. The right  side of the image is a picture of a cat sitting at a  dinner table with a plate of food in front of it, looking  unimpressed. The humor in this image comes from  the contrast between the two sides - the angry,  dramatic yelling on the left and the nonchalant cat on  the right. It also plays on the idea of cats acting like  humans and being unimpressed by our behavior.  Overall, the image creates a funny and absurd  situation that makes people laugh. ", "page_idx": 43, "bbox": [317, 561.3529052734375, 495, 695.226318359375], "page_size": [612.0, 792.0]}
{"layout": 605, "type": "text", "text": "Sec. 4.3 Multimodal Knowledge and Commonsense ", "text_level": 1, "page_idx": 44, "bbox": [132, 98, 421, 116], "page_size": [612.0, 792.0]}
{"layout": 606, "type": "text", "text": "Science and Knowledge ", "text_level": 1, "page_idx": 44, "bbox": [117, 124, 229, 135], "page_size": [612.0, 792.0]}
{"layout": 607, "type": "text", "text": "Prompt: Which of these oceans does the prime meridian intersect? ", "page_idx": 44, "bbox": [118, 140.11582946777344, 310.30340576171875, 159.8743133544922], "page_size": [612.0, 792.0]}
{"layout": 608, "type": "image", "page_idx": 44, "img_path": "layout_images/2309.17421v2_104.jpg", "bbox": [117, 163, 413, 336], "page_size": [612.0, 792.0], "ocr_text": "20°N\n0\n? 20°5)\n\n=\n3\n\n60°S\n\n40°N\n40°S\n\nbors\n", "vlm_text": "This image is a world map showing the continents and oceans. The continents labeled are North America, South America, Europe, Africa, Asia, Australia, and Antarctica. The oceans labeled are the Arctic Ocean, Pacific Ocean, Atlantic Ocean, Indian Ocean, and Southern Ocean. The map includes latitude and longitude lines."}
{"layout": 609, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 44, "bbox": [117, 339, 155, 349], "page_size": [612.0, 792.0]}
{"layout": 610, "type": "text", "text": "The prime meridian intersects the following oceans: - Atlantic Ocean - Southern Ocean - Arctic Ocean ", "page_idx": 44, "bbox": [118, 351.34429931640625, 288.3039855957031, 386.590087890625], "page_size": [612.0, 792.0]}
{"layout": 611, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 44, "bbox": [116, 398, 154, 408.75], "page_size": [612.0, 792.0]}
{"layout": 612, "type": "text", "text": "Compare the average kinetic energies of the particles in each sample. The diagrams below show two pure  samples of gas in identical closed, rigid containers. Each colored ball represents one gas particle. Both samples  have the same number of particles. Which sample has the higher temperature? Sample A or sample B? ", "page_idx": 44, "bbox": [118, 409.7379150390625, 489.1906433105469, 436.0176086425781], "page_size": [612.0, 792.0]}
{"layout": 613, "type": "image", "page_idx": 44, "img_path": "layout_images/2309.17421v2_105.jpg", "bbox": [120, 442, 351, 613], "page_size": [612.0, 792.0], "ocr_text": "@))\n\n= <a =\n\nSample A Sample B\n\nMass of each particle: 32 u Mass of each particle: 32 u\n\nAveraae particle speed: 600 m/s Average particle speed: 540 m/:\n", "vlm_text": "The image shows two jars with particles inside. \n\n- **Sample A** contains particles with:\n  - Mass of each particle: 32 u\n  - Average particle speed: 600 m/s\n\n- **Sample B** contains particles with:\n  - Mass of each particle: 32 u\n  - Average particle speed: 540 m/s\n\nThe particles in each jar are illustrated as small circles, and both samples have the same particle mass but different average speeds."}
{"layout": 614, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 44, "bbox": [117, 619, 155, 629.75], "page_size": [612.0, 792.0]}
{"layout": 615, "type": "text", "text": "Because the average particle speed in sample A is higher than in sample B (  $\\mathsf{600\\;m/s}$   vs.  $540\\;\\mathsf{m}/\\mathsf{s}.$  ), the particles in  sample A have a higher average kinetic energy. Therefore, sample A has a higher temperature than sample B. ", "page_idx": 44, "bbox": [118, 631, 493.7259521484375, 648.6437377929688], "page_size": [612.0, 792.0]}
{"layout": 616, "type": "text", "text": "Figure 29: Results on answering science questions. GPT-4V can understand the question textually and visually, and gather necessary information to answer the question. Example images are from [ 85 ]. Check Section  4.3  for detailed discussions. ", "page_idx": 44, "bbox": [108, 660.1485595703125, 504, 693.9714965820312], "page_size": [612.0, 792.0]}
{"layout": 617, "type": "text", "text": "Sec. 4.3 Multimodal Knowledge and Commonsense ", "text_level": 1, "page_idx": 45, "bbox": [135, 94, 418, 112], "page_size": [612.0, 792.0]}
{"layout": 618, "type": "text", "text": "Science and Knowledge ", "page_idx": 45, "bbox": [118, 119.70653533935547, 227.98789978027344, 130.43890380859375], "page_size": [612.0, 792.0]}
{"layout": 619, "type": "text", "text": "Prompt: Which of these states is farthest north? ", "page_idx": 45, "bbox": [118, 135.8456268310547, 247.18161010742188, 155.60411071777344], "page_size": [612.0, 792.0]}
{"layout": 620, "type": "image", "page_idx": 45, "img_path": "layout_images/2309.17421v2_106.jpg", "img_caption": "GPT-4V: Out of the three states shown in the map - Arizona, Oklahoma, and Delaware, Delaware is the farthest north. ", "bbox": [117, 161, 477, 347], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image is a map of the United States highlighting four states: Arizona, Oklahoma, North Carolina, and Delaware. The caption discusses three of these states - Arizona, Oklahoma, and Delaware - and states that Delaware is the farthest north among them."}
{"layout": 621, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 45, "bbox": [117, 366, 155, 375], "page_size": [612.0, 792.0]}
{"layout": 622, "type": "text", "text": "Which of the organisms is the producer in this food web? ", "page_idx": 45, "bbox": [118, 376.6479187011719, 305.72308349609375, 384.99530029296875], "page_size": [612.0, 792.0]}
{"layout": 623, "type": "image", "page_idx": 45, "img_path": "layout_images/2309.17421v2_107.jpg", "bbox": [119, 391, 370, 611], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "This image is a food web diagram showing the interactions between various organisms. It includes:\n\n- Grizzly bear\n- Red-tailed hawk\n- Chipmunk\n- Marmot\n- Grouse\n- Deer\n- Butterfly\n- Decomposers and detritivores\n- Berries and flowers\n- Grasses\n- Seeds\n\nArrows indicate the direction of energy flow, showing which organisms consume others."}
{"layout": 624, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 45, "bbox": [117, 617, 154, 629], "page_size": [612.0, 792.0]}
{"layout": 625, "type": "text", "text": "The producers in this food web are the berries and flowers, grasses, and seeds. These are the organisms that  use energy from the sun to produce their own food through photosynthesis. ", "page_idx": 45, "bbox": [118, 630.2621459960938, 482.8170471191406, 647.57568359375], "page_size": [612.0, 792.0]}
{"layout": 626, "type": "text", "text": "Figure 30: Results on answering science questions. GPT-4V can understand the question textually and visually, and gather necessary information to answer the question. Example images are from [ 85 ]. Check Section  4.3  for detailed discussions. ", "page_idx": 45, "bbox": [108, 664.4175415039062, 504, 698.2415161132812], "page_size": [612.0, 792.0]}
{"layout": 627, "type": "text", "text": "Sec. 4.3 Multimodal Knowledge and Commonsense ", "text_level": 1, "page_idx": 46, "bbox": [131, 155, 421, 172], "page_size": [612.0, 792.0]}
{"layout": 628, "type": "text", "text": "Science and Knowledge ", "text_level": 1, "page_idx": 46, "bbox": [116, 179, 229, 191], "page_size": [612.0, 792.0]}
{"layout": 629, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 46, "bbox": [117, 196, 155, 206], "page_size": [612.0, 792.0]}
{"layout": 630, "type": "text", "text": "Suppose you are a teacher, please use the figure to explain the distinction between evaporation and  evapotranspiration. ", "page_idx": 46, "bbox": [118, 207.29721069335938, 450.6286926269531, 224.6107635498047], "page_size": [612.0, 792.0]}
{"layout": 631, "type": "image", "page_idx": 46, "img_path": "layout_images/2309.17421v2_108.jpg", "bbox": [117, 230, 441, 427], "page_size": [612.0, 792.0], "ocr_text": "Condensation\n\nPrecipitation Evapotranspiration\n\nEvaporation\n\nGround Water\n\n", "vlm_text": "This image depicts the water cycle, showing processes such as precipitation, evaporation, evapotranspiration, and infiltration. It illustrates surface water, ground water, runoff, streamflow, and a dam. The water table is indicated below the ground level, while the sun and cloud formations indicate weather systems in action. Arrows are used to demonstrate the direction of each process."}
{"layout": 632, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 46, "bbox": [117, 430, 155, 440], "page_size": [612.0, 792.0]}
{"layout": 633, "type": "text", "text": "Okay class, today we are going to learn about the distinction between evaporation and evapotranspiration. Let’s  take a look at this figure. ", "page_idx": 46, "bbox": [118, 441.6982421875, 491, 459.01177978515625], "page_size": [612.0, 792.0]}
{"layout": 634, "type": "text", "text": "First, let’s understand what evaporation is. Evaporation is the process by which water is changed from its liquid  state to its gaseous state. This usually takes place at the surface of a liquid, like the ocean or a lake, when the  water molecules gain enough energy to break away and form water vapor. So in this figure, we can see that the  water in the lake and the dam is evaporating and becoming water vapor. ", "page_idx": 46, "bbox": [118, 468.5966796875, 491, 503.8424987792969], "page_size": [612.0, 792.0]}
{"layout": 635, "type": "text", "text": "Now, let’s take a look at evapotranspiration. This is a combination of two processes - evaporation and  transpiration. Transpiration is the process by which plants release water vapor through their leaves. So, in this  figure, we can see that the trees and plants are releasing water vapor into the air, which is the transpiration part  of evapotranspiration. At the same time, the water in the soil and the surface is also evaporating. This  combination of evaporation and transpiration is what we call evapotranspiration. ", "page_idx": 46, "bbox": [118, 513.4273681640625, 491, 557.639404296875], "page_size": [612.0, 792.0]}
{"layout": 636, "type": "text", "text": "In summary, evaporation is the process by which water is changed from its liquid state to its gaseous state, while  evapotranspiration is the combination of evaporation and transpiration. Both these processes contribute to the  water cycle and are important for maintaining the balance of water in our environment. ", "page_idx": 46, "bbox": [118, 567.2242431640625, 491, 593.5039672851562], "page_size": [612.0, 792.0]}
{"layout": 637, "type": "text", "text": "Figure 31: Results on answering science questions. When we use a more specific text prompt like “Suppose you are a teacher, please use the figure to explain X,” we observe that GPT-4V can generate a short tutorial for explaining the subject. Check Section  4.3  for detailed discussions. ", "page_idx": 46, "bbox": [106, 604.3765869140625, 504, 638.1995239257812], "page_size": [612.0, 792.0]}
{"layout": 638, "type": "text", "text": "Multimodal Commonsense ", "text_level": 1, "page_idx": 47, "bbox": [117, 64, 241, 75], "page_size": [612.0, 792.0]}
{"layout": 639, "type": "text", "text": "Prompt: What is [person3] doing? ", "page_idx": 47, "bbox": [118, 80.54612731933594, 202, 100.30461883544922], "page_size": [612.0, 792.0]}
{"layout": 640, "type": "image", "page_idx": 47, "img_path": "layout_images/2309.17421v2_109.jpg", "bbox": [118, 103, 332, 224], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows a group of people sitting at a diner table. The table has various items on it, including a bottle of syrup, condiments like ketchup and mustard, glasses of orange juice, cups, and silverware. One person is pointing at another individual across the table. The setting appears to be a casual dining environment with booths and a counter in the background. A server is holding plates of food, including a breakfast dish and a stack of pancakes."}
{"layout": 641, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 47, "bbox": [117, 229, 155, 240], "page_size": [612.0, 792.0]}
{"layout": 642, "type": "text", "text": "[person3] is carrying a plate of food, likely serving it to the customers at the table. It appears that they are working  as a waiter or server in a restaurant.  ", "page_idx": 47, "bbox": [118, 241.82020568847656, 494.5105285644531, 259.1337890625], "page_size": [612.0, 792.0]}
{"layout": 643, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 47, "bbox": [117, 273, 155, 283], "page_size": [612.0, 792.0]}
{"layout": 644, "type": "text", "text": "What are [person1] and [person2] doing? ", "page_idx": 47, "bbox": [118, 284.76165771484375, 253.868896484375, 293.1090393066406], "page_size": [612.0, 792.0]}
{"layout": 645, "type": "image", "page_idx": 47, "img_path": "layout_images/2309.17421v2_110.jpg", "bbox": [118, 295, 331, 413], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows a group of people dressed in formal attire, likely at a wedding or formal event. There are floral decorations in the background, and it appears that the scene is outdoors. Two people at the front are linked arm-in-arm and smiling, with others following behind."}
{"layout": 646, "type": "text", "text": "GPT-4V: [person1] and [person2] are walking down the aisle at a wedding ceremony. ", "page_idx": 47, "bbox": [118, 419.37091064453125, 367.4676513671875, 439.12939453125], "page_size": [612.0, 792.0]}
{"layout": 647, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 47, "bbox": [117, 455, 155, 464.75], "page_size": [612.0, 792.0]}
{"layout": 648, "type": "text", "text": "Suppose you are a detective, what can you infer from the visual clues in the image? ", "page_idx": 47, "bbox": [118, 465.6429138183594, 394.54779052734375, 473.99029541015625], "page_size": [612.0, 792.0]}
{"layout": 649, "type": "image", "page_idx": 47, "img_path": "layout_images/2309.17421v2_111.jpg", "bbox": [118, 478, 299, 615], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows a room with a green door on which several jackets are hanging. Adjacent to the door is a wooden shelving unit filled with various items, such as clothing and toys. A futon or couch sits nearby. In the corner of the room, there's a computer desk with a monitor, keyboard, mouse, and a black office chair. A metallic duct runs along one side of the room, indicating some ventilation or heating system. The walls have a textured, warm-toned finish, adding a cozy feel to the room."}
{"layout": 650, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 47, "bbox": [117, 617, 154, 628], "page_size": [612.0, 792.0]}
{"layout": 651, "type": "text", "text": "From the visual clues in the image, I can infer that: - The room belongs to someone who is likely a young adult or teenager, as evidenced by the casual clothing and  the desk with a computer.\n\n ", "page_idx": 47, "bbox": [118, 629.5955810546875, 491.16314697265625, 655.8753662109375], "page_size": [612.0, 792.0]}
{"layout": 652, "type": "text", "text": "- The person is likely a student or someone who works from home, as evidenced by the desk and computer.\n\n - The person is likely fashion-conscious, as evidenced by the multiple jackets and shoes.\n\n - The room is likely located in a colder climate, as evidenced by the jackets and boots.\n\n - The person is likely not very tidy, as evidenced by the cluttered desk and the clothes hanging on the door.\n\n - The room is likely located in an older or more rustic building, as evidenced by the exposed pipes and the  worn-looking walls.  ", "page_idx": 47, "bbox": [118, 656.4940795898438, 473.70159912109375, 709.6722412109375], "page_size": [612.0, 792.0]}
{"layout": 653, "type": "text", "text": "4.4 Scene Text, Table, Chart, and Document Reasoning ", "text_level": 1, "page_idx": 48, "bbox": [106, 73, 349, 85], "page_size": [612.0, 792.0]}
{"layout": 654, "type": "text", "text": "Scene text recognition.  Reading and understanding scene text in images is an important task in vision-language [ 118 – 120 ,  17 ]. In our experiments, we investigate GPT-4V’s ability to recognize scene text by utilizing the input prompt “What are all the scene text in the image?” Figure  33 shows the example results. We observe GPT-4V accurately identifies scene text in various scenarios, including both handwritten and printed text. In Section  4.5 , we present further results on multilingual scenarios. ", "page_idx": 48, "bbox": [106, 92.37389373779297, 505, 159.28245544433594], "page_size": [612.0, 792.0]}
{"layout": 655, "type": "text", "text": "Visual math reasoning.  In Figure  34 , GPT-4V demonstrates its capability in solving visual math problems. In our experiments, we observe GPT-4V is able to extract essential information from the image. For instance, in Figure  34 , GPT-4V correctly identifies the presence of a right triangle (or orthogonal triangle) and determines that AB is 4 units and BC is 3 units. In addition, we note that GPT-4V tends to present solutions in a well-structured manner, solving the problem step by step, thereby showcasing its ability to provide clear explanations. ", "page_idx": 48, "bbox": [106, 163.3079071044922, 505, 230.2164764404297], "page_size": [612.0, 792.0]}
{"layout": 656, "type": "text", "text": "Chart understanding and reasoning.  We further study GPT-4V’s ability in chart understanding and reasoning. Figures  35 - 37  show the example results. In our preliminary explorations, GPT-4V exhibits the ability to provide detailed descriptions of charts. For example, in Figure  35 , the model correctly explains the proposal process from the beginning to the end. In Figure  36 , the model not only understands the program in the given flow chat, but also translates the details to a python code. In the bottom row of Figure  37 , GPT-4V shows a clear understanding of both x- and y-axis, and explains the key insight presented in the chart. Furthermore, in our experiments, we observe that GPT-4V can answer questions based on the chart. In the top row of Figure  37 , GPT-4V correctly calculates the average total fueling cost, excluding the Ford F150. ", "page_idx": 48, "bbox": [106, 234.24192810058594, 505, 333.87750244140625], "page_size": [612.0, 792.0]}
{"layout": 657, "type": "text", "text": "Table understanding and reasoning.  In Figure  38 , we present our preliminary investigations into table understanding and reasoning. Similar to the findings from chart experiments, GPT-4V shows promising results in understanding the details in the table, as well as in reasoning and accurately responding to related questions. ", "page_idx": 48, "bbox": [106, 337.9029235839844, 505, 382.9934997558594], "page_size": [612.0, 792.0]}
{"layout": 658, "type": "text", "text": "Document understanding.  Figure  39  shows the results on various types of documents, such as floor plan, poster, and exam paper. We observe GPT-4V demonstrates an understanding of the documents and provides reasonable responses. For instance, it accurately identifies the location of the bathroom for the second bedroom in the floor plan. It also recognizes the Chinese dish “Hot dry noodles,” and associates it with the city of Wuhan by following the scene text. Moreover, GPT-4V is capable of reading an exam paper. It accurately reconstructs the table in Markdown, and then fills in the table with the correct answers. We present more explorations in its coding ability in Section  4.6 . ", "page_idx": 48, "bbox": [106, 387.0189208984375, 505, 464.83648681640625], "page_size": [612.0, 792.0]}
{"layout": 659, "type": "text", "text": "In Figure  40 , we explore a more challenging case by providing a technical report [ 126 ] with multiple pages as input. In our limited exploration, GPT-4V exhibits impressive results. It correctly describes the main idea and their proposed method by considering the context across multiple pages. However, it may occasionally miss some implementation details. Please note that the dataset should contain  $1196{+}665{=}1861$   examples, and the extracted features should include Histograms of Oriented Gradi- ents (HOG). Instead of prompting all pages to the model simultaneously, we believe that exploring more advanced prompting techniques, such as thinking step-by-step or employing in-context few-shot approaches, could potentially enhance the model’s performance. ", "page_idx": 48, "bbox": [106, 469.2205505371094, 505, 557.5885009765625], "page_size": [612.0, 792.0]}
{"layout": 660, "type": "text", "text": "Sec. 4.4 Scene Text, Table, Chart, and Document Reasoning ", "text_level": 1, "page_idx": 49, "bbox": [72, 73, 429, 92], "page_size": [612.0, 792.0]}
{"layout": 661, "type": "text", "text": "Scene Text Recognition ", "text_level": 1, "page_idx": 49, "bbox": [77, 99, 188, 110], "page_size": [612.0, 792.0]}
{"layout": 662, "type": "text", "text": "Prompt: What are all the scene text in the image? ", "page_idx": 49, "bbox": [78, 115.4296646118164, 214, 135.28627014160156], "page_size": [612.0, 792.0]}
{"layout": 663, "type": "image", "page_idx": 49, "img_path": "layout_images/2309.17421v2_112.jpg", "bbox": [79, 141, 185, 194], "page_size": [612.0, 792.0], "ocr_text": "san\n", "vlm_text": "The image shows a digital display with the number 3,642,039,031,055 in orange. It appears to be part of a ticker or electronic sign."}
{"layout": 664, "type": "text", "text": "GPT-4V: The scene text in the image is  \"3,642,039,031,055\" .  ", "page_idx": 49, "bbox": [78, 196.91195678710938, 180.34573364257812, 225.77923583984375], "page_size": [612.0, 792.0]}
{"layout": 665, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 49, "bbox": [77, 272, 115, 282.75], "page_size": [612.0, 792.0]}
{"layout": 666, "type": "text", "text": "What are all the scene text in the image? ", "page_idx": 49, "bbox": [78, 283.8938903808594, 214, 292.2827453613281], "page_size": [612.0, 792.0]}
{"layout": 667, "type": "image", "page_idx": 49, "img_path": "layout_images/2309.17421v2_113.jpg", "bbox": [79, 297, 229, 380], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows a view over a city at dusk or night from a bridge or viewpoint. On the railing in the foreground, there's graffiti that reads, \"The earth without art is just 'eh'.\""}
{"layout": 668, "type": "text", "text": "GPT-4V: The only scene text in the image is \"The  Earth without art is just 'eh'\".  ", "page_idx": 49, "bbox": [78, 387.31231689453125, 214, 416.17962646484375], "page_size": [612.0, 792.0]}
{"layout": 669, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 49, "bbox": [77, 442, 115, 452], "page_size": [612.0, 792.0]}
{"layout": 670, "type": "text", "text": "What are all the scene text in the image? ", "page_idx": 49, "bbox": [78, 453.97283935546875, 214, 462.3616943359375], "page_size": [612.0, 792.0]}
{"layout": 671, "type": "image", "page_idx": 49, "img_path": "layout_images/2309.17421v2_114.jpg", "bbox": [78, 466, 229, 579], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows a COVID-19 testing sign that indicates it's a drive-through location. The sign advises people to have their ID and insurance card ready. There is snow on the ground, and the picture seems to be taken from inside a vehicle, showing the side mirror and part of the car door. In the background, there are some trees and a building."}
{"layout": 672, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 49, "bbox": [78, 584, 115, 594], "page_size": [612.0, 792.0]}
{"layout": 673, "type": "text", "text": "There are three texts in the image: ", "page_idx": 49, "bbox": [78, 595.8910522460938, 193, 604.2799072265625], "page_size": [612.0, 792.0]}
{"layout": 674, "type": "text", "text": "1) “COVID-19 TESTING” 2) “PLEASE HAVE ID & INSURANCE  CARD READY” 3) “W   $@$  UW Medicine” (this is the  logo/branding for the University of  Washington Medicine  ", "page_idx": 49, "bbox": [78, 613.9124755859375, 205.9885711669922, 667.3546752929688], "page_size": [612.0, 792.0]}
{"layout": 675, "type": "text", "text": "Prompt: What are all the scene text in the image? ", "page_idx": 49, "bbox": [243, 115.43916320800781, 382, 135.2957763671875], "page_size": [612.0, 792.0]}
{"layout": 676, "type": "image", "page_idx": 49, "img_path": "layout_images/2309.17421v2_115.jpg", "bbox": [244, 140, 354, 176], "page_size": [612.0, 792.0], "ocr_text": "(NVDA)\n\nNVIDIA\n356.85 +51.47 [+16.85%]\n\n", "vlm_text": "The image shows stock information for NVIDIA (NVDA). The stock price is 356.85, with an increase of 51.47, which is a 16.85% rise."}
{"layout": 677, "type": "text", "text": "GPT-4V: The scene text in the image are:\n\n - (NVDA)\n\n - NVIDIA\n\n - 356.85\n\n -  $+51.47\n\n$  -  $+16.85\\%$    ", "page_idx": 49, "bbox": [243, 178.83572387695312, 350, 244], "page_size": [612.0, 792.0]}
{"layout": 678, "type": "text", "text": "Prompt: What are all the scene text in the image? ", "page_idx": 49, "bbox": [243, 272.4726257324219, 382, 292.3292236328125], "page_size": [612.0, 792.0]}
{"layout": 679, "type": "image", "page_idx": 49, "img_path": "layout_images/2309.17421v2_116.jpg", "bbox": [244, 296, 356, 380], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows a mosaic on the ground with the text \"The Road to Hollywood\" with the phrase \"how some of us got here\" written underneath."}
{"layout": 680, "type": "text", "text": "GPT-4V: The scene text in the image is “The Road  to Hollywood: how some of us got here” ", "page_idx": 49, "bbox": [243, 384.7843017578125, 382, 413.651611328125], "page_size": [612.0, 792.0]}
{"layout": 681, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 49, "bbox": [243, 443, 280, 452.75], "page_size": [612.0, 792.0]}
{"layout": 682, "type": "text", "text": "What are all the scene text in the image? ", "page_idx": 49, "bbox": [243, 454.11383056640625, 382, 462.502685546875], "page_size": [612.0, 792.0]}
{"layout": 683, "type": "image", "page_idx": 49, "img_path": "layout_images/2309.17421v2_117.jpg", "bbox": [244, 465, 344, 582], "page_size": [612.0, 792.0], "ocr_text": "ay VACOY ion\n\n", "vlm_text": "The image shows a framed chalkboard with handwritten words and drawings related to summer. The words are:\n\n- vacation\n- BEACH\n- smile (with a sun drawing)\n- SUMMER\n- watermelon\n- FLIP FLOPS\n- LEMONADE\n\nThe overall theme is summery and playful."}
{"layout": 684, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 49, "bbox": [243, 589, 281, 598.75], "page_size": [612.0, 792.0]}
{"layout": 685, "type": "text", "text": "The scene text in the image are:\n\n - Vacation\n\n - Beach\n\n - Smile\n\n - Summer\n\n - Watermelon\n\n - Flip flops\n\n - Lemonade ", "page_idx": 49, "bbox": [243, 601.1810302734375, 350, 672.6446533203125], "page_size": [612.0, 792.0]}
{"layout": 686, "type": "text", "text": "Prompt: What are all the scene text in the image? ", "page_idx": 49, "bbox": [398, 115.43916320800781, 538, 135.2957763671875], "page_size": [612.0, 792.0]}
{"layout": 687, "type": "image", "page_idx": 49, "img_path": "layout_images/2309.17421v2_118.jpg", "bbox": [399, 140, 484, 214], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows the words \"Royal London\" written on a window. There is a logo above the text, and you can see some buildings in the background through the glass."}
{"layout": 688, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 49, "bbox": [398, 218, 435, 224.75], "page_size": [612.0, 792.0]}
{"layout": 689, "type": "text", "text": "The scene text in the image are:\n\n - \"Royal\"\n\n - \"London\"  ", "page_idx": 49, "bbox": [398, 229.11376953125, 506, 255.52401733398438], "page_size": [612.0, 792.0]}
{"layout": 690, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 49, "bbox": [398, 273, 436, 282.75], "page_size": [612.0, 792.0]}
{"layout": 691, "type": "text", "text": "What are all the scene text in the image? ", "page_idx": 49, "bbox": [398, 284.06915283203125, 538, 292.4580078125], "page_size": [612.0, 792.0]}
{"layout": 692, "type": "image", "page_idx": 49, "img_path": "layout_images/2309.17421v2_119.jpg", "bbox": [399, 296, 523, 380], "page_size": [612.0, 792.0], "ocr_text": "piesttield\n\nwestfiei.0om 2\n\n", "vlm_text": "The image shows a sign for \"Westfield\" with the text \"westfield.com.au\" underneath. It appears to be on the exterior of a building."}
{"layout": 693, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 49, "bbox": [398, 387, 435, 395.75], "page_size": [612.0, 792.0]}
{"layout": 694, "type": "text", "text": "The scene text in the image are:\n\n - \"Westfield\"\n\n - \"westfield.com.au\"  ", "page_idx": 49, "bbox": [398, 398.955322265625, 506, 425.3655700683594], "page_size": [612.0, 792.0]}
{"layout": 695, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 49, "bbox": [398, 443, 436, 452.75], "page_size": [612.0, 792.0]}
{"layout": 696, "type": "text", "text": "What are all the scene text in the image? ", "page_idx": 49, "bbox": [398, 454.1142883300781, 538, 462.5031433105469], "page_size": [612.0, 792.0]}
{"layout": 697, "type": "image", "page_idx": 49, "img_path": "layout_images/2309.17421v2_120.jpg", "bbox": [400, 466, 536, 550], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows an informational sign about the Malayan Tapir (Tapirus indicus). The sign includes a picture of the animal and provides some details:\n\n- **Location**: Malay Peninsula and Sumatra, near water\n- **Relatives**: There are only five species of tapirs. They are most closely related to horses and rhinos.\n- **Vanishing?**: The text notes that forests provide tapirs with shelter, food, and water, but these forests are rapidly disappearing in Malaysia and Sumatra."}
{"layout": 698, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 49, "bbox": [398, 552, 435, 562], "page_size": [612.0, 792.0]}
{"layout": 699, "type": "text", "text": "The scene text in the image are:\n\n ", "page_idx": 49, "bbox": [398, 563.9158935546875, 506, 572.3047485351562], "page_size": [612.0, 792.0]}
{"layout": 700, "type": "text", "text": "- Malayan Tapir\n\n - Tapirus indicus\n\n - LOCATION: Malay Peninsula and  Sumatra, near water - RELATIVES: There are only five species  of tapirs. They are most closely related to  horses and rhinos. - VANISHING? The forest provides tapirs  with shelter, food, and water, but forests  are rapidly disappearing in Malaysia and  Sumatra.  ", "page_idx": 49, "bbox": [398, 581.937255859375, 538, 680.4329223632812], "page_size": [612.0, 792.0]}
{"layout": 701, "type": "text", "text": "Sec. 4.4 Scene Text, Table, Chart, and Document Reasoning ", "text_level": 1, "page_idx": 50, "bbox": [118, 113, 473, 132], "page_size": [612.0, 792.0]}
{"layout": 702, "type": "text", "text": "Visual Math ", "text_level": 1, "page_idx": 50, "bbox": [117, 139, 172, 150], "page_size": [612.0, 792.0]}
{"layout": 703, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 50, "bbox": [116, 156, 155, 165], "page_size": [612.0, 792.0]}
{"layout": 704, "type": "text", "text": "What is  $\\mathsf{x}$   in the equation? ", "page_idx": 50, "bbox": [116, 166.94515991210938, 208, 175.2925567626953], "page_size": [612.0, 792.0]}
{"layout": 705, "type": "image", "page_idx": 50, "img_path": "layout_images/2309.17421v2_121.jpg", "bbox": [117, 186, 222, 222], "page_size": [612.0, 792.0], "ocr_text": "(x +) =4\n\n", "vlm_text": "The image contains a mathematical equation: \\((x + 3)^2 = 4\\)."}
{"layout": 706, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 50, "bbox": [116, 232, 155, 242], "page_size": [612.0, 792.0]}
{"layout": 707, "type": "text", "text": "Without any context or  information about what the  equation is representing, we  can only assume that  $\"\\times\"$   is a  variable or unknown value in  the equation.  ", "page_idx": 50, "bbox": [116, 243.7979278564453, 221, 296.9761657714844], "page_size": [612.0, 792.0]}
{"layout": 708, "type": "text", "text": "In this equation,   $(\\mathsf{x}+\\mathsf{3})$   is  being squared and is equal to  4. To solve for  $\\mathsf{x}$  , we can take  the square root of both sides  of the equation to get rid of the  exponent on the left side. This  would give us: ", "page_idx": 50, "bbox": [116, 306, 221, 368.70550537109375], "page_size": [612.0, 792.0]}
{"layout": 709, "type": "equation", "text": "\n$$\n\\begin{array}{l}{{\\sqrt{((\\mathsf{x}+3)^{2})}=\\sqrt{4}}}\\\\ {{\\mathsf{x}+3}=\\pm2}\\end{array}\n$$\n ", "text_format": "latex", "page_idx": 50, "bbox": [113, 375, 169, 406], "page_size": [612.0, 792.0]}
{"layout": 710, "type": "text", "text": "From here, we can solve for  $\\mathsf{x}$  by subtracting 3 from both  sides. This would give us: ", "page_idx": 50, "bbox": [116, 414.15496826171875, 221, 440.4346618652344], "page_size": [612.0, 792.0]}
{"layout": 711, "type": "text", "text": " $\\times=\\cdot1$   or  $\\mathsf{x}=-5$  ", "page_idx": 50, "bbox": [116, 449, 167, 458.366943359375], "page_size": [612.0, 792.0]}
{"layout": 712, "type": "text", "text": "So, in this equation, x could be  either  $^{-1}$   or  $^{-5}$  .  ", "page_idx": 50, "bbox": [116, 467.95184326171875, 221, 485.265380859375], "page_size": [612.0, 792.0]}
{"layout": 713, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 50, "bbox": [229, 156, 266, 165], "page_size": [612.0, 792.0]}
{"layout": 714, "type": "text", "text": "What is the value of d in the last equation? ", "page_idx": 50, "bbox": [229, 166.85479736328125, 371, 175.2021942138672], "page_size": [612.0, 792.0]}
{"layout": 715, "type": "text", "text": "", "page_idx": 50, "bbox": [234, 189, 341, 199], "page_size": [612.0, 792.0]}
{"layout": 716, "type": "text", "text": " $\\mathbf{8x+11}=\\mathbf{4x+14}$   $\\mathbf{7}d-\\mathbf{4}=\\mathbf{11}d-\\mathbf{9}$  ", "page_idx": 50, "bbox": [249, 207, 318, 217], "page_size": [612.0, 792.0]}
{"layout": 717, "type": "text", "text": "", "page_idx": 50, "bbox": [249, 230, 313, 240], "page_size": [612.0, 792.0]}
{"layout": 718, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 50, "bbox": [229, 251, 266, 261], "page_size": [612.0, 792.0]}
{"layout": 719, "type": "text", "text": "1) To solve the first equation, we need to  isolate the  $\\mathsf{x}$   variable on one side of the  equation and the constants on the other  side. We can start by subtracting  $4\\times$   from  both sides to get rid of the  $\\mathsf{x}$   term on the  right side of the equation. This gives us: ", "page_idx": 50, "bbox": [229, 262.9208068847656, 371, 316.0989990234375], "page_size": [612.0, 792.0]}
{"layout": 720, "type": "equation", "text": "\n$$\n4\\mathsf{x}+11=14\n$$\n ", "text_format": "latex", "page_idx": 50, "bbox": [228, 324, 272, 334], "page_size": [612.0, 792.0]}
{"layout": 721, "type": "text", "text": "Next, we can subtract 11 from both sides  to get rid of the constant term on the left  side of the equation. This gives us: ", "page_idx": 50, "bbox": [229, 343.6163024902344, 371, 369.89599609375], "page_size": [612.0, 792.0]}
{"layout": 722, "type": "equation", "text": "\n$$\n4\\times=3\n$$\n ", "text_format": "latex", "page_idx": 50, "bbox": [228, 378, 252, 388], "page_size": [612.0, 792.0]}
{"layout": 723, "type": "text", "text": "Finally, we can divide both sides by 4 to  isolate the  $\\mathsf{x}$   variable on the left side of the  equation. This gives us: ", "page_idx": 50, "bbox": [229, 397.4132080078125, 371, 423.69287109375], "page_size": [612.0, 792.0]}
{"layout": 724, "type": "equation", "text": "\n$$\n\\times=3/4\n$$\n ", "text_format": "latex", "page_idx": 50, "bbox": [228, 432, 253, 441], "page_size": [612.0, 792.0]}
{"layout": 725, "type": "text", "text": "2) To solve the second equation, we can  start by subtracting 7d from both sides to  get rid of the d term on the left side of the  equation. This gives us: ", "page_idx": 50, "bbox": [229, 451.2100830078125, 371, 486.4559020996094], "page_size": [612.0, 792.0]}
{"layout": 726, "type": "equation", "text": "\n$$\n-4\\AA\\cdot9\n$$\n ", "text_format": "latex", "page_idx": 50, "bbox": [228, 495, 265, 505], "page_size": [612.0, 792.0]}
{"layout": 727, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 50, "bbox": [377, 156, 414, 165], "page_size": [612.0, 792.0]}
{"layout": 728, "type": "text", "text": "What is the value of  $\\mathsf{x}$   in the image? ", "page_idx": 50, "bbox": [378, 166.85479736328125, 497, 175.2021942138672], "page_size": [612.0, 792.0]}
{"layout": 729, "type": "image", "page_idx": 50, "img_path": "layout_images/2309.17421v2_122.jpg", "bbox": [379, 186, 463, 244], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows a right triangle with vertices labeled A, B, and C. The side AB is horizontal and has a length of 4 units. The side BC is vertical with a length of 3 units, and there is a right angle at B. The hypotenuse AC is labeled as \\( x \\). \n\nTo find the length of x, you can use the Pythagorean theorem:\n\n\\[\nx^2 = 4^2 + 3^2 \\implies x^2 = 16 + 9 \\implies x^2 = 25 \\implies x = 5\n\\]\n\nSo, \\( x \\) is 5 units."}
{"layout": 730, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 50, "bbox": [377, 251, 414, 260], "page_size": [612.0, 792.0]}
{"layout": 731, "type": "text", "text": "The value of  $\\mathsf{x}$   in the image can be  found using the Pythagorean  theorem, which states that in a right  triangle, the square of the  hypotenuse (the longest side,  opposite the right angle) is equal to  the sum of the squares of the other  two sides. ", "page_idx": 50, "bbox": [378, 262.9208068847656, 497, 334.0313720703125], "page_size": [612.0, 792.0]}
{"layout": 732, "type": "text", "text": "In this case, the hypotenuse is the  side AC, and the other two sides are  AB and BC. We know that AB is 4  and BC is 3, so we can plug these  values into the equation: ", "page_idx": 50, "bbox": [378, 343.6163024902344, 497, 387.82830810546875], "page_size": [612.0, 792.0]}
{"layout": 733, "type": "equation", "text": "\n$$\n{\\begin{array}{r l}&{\\mathsf{a}^{2}+\\mathsf{b}^{2}=\\mathsf{c}^{2}}\\\\ &{\\mathsf{3}^{2}+\\mathsf{4}^{2}=\\mathsf{c}^{2}}\\\\ &{\\mathsf{9}+\\mathsf{16}=\\mathsf{c}^{2}}\\\\ &{\\mathsf{25}=\\mathsf{c}^{2}}\\end{array}}\n$$\n ", "text_format": "latex", "page_idx": 50, "bbox": [375, 394, 416, 460], "page_size": [612.0, 792.0]}
{"layout": 734, "type": "text", "text": "Therefore, c (the length of the  hypotenuse) is 5. Since  $\\mathsf{x}$   is the  length of the hypotenuse,  $\\mathsf{x}$   is equal  to 5.  ", "page_idx": 50, "bbox": [378, 469.1423645019531, 497, 504.38818359375], "page_size": [612.0, 792.0]}
{"layout": 735, "type": "text", "text": "Next, we can add 9 to both sides to get rid  of the constant term on the right side of  the equation. This gives us: ", "page_idx": 50, "bbox": [229, 513.9730834960938, 371.3472595214844, 540.2528076171875], "page_size": [612.0, 792.0]}
{"layout": 736, "type": "equation", "text": "\n$$\n5=4{\\mathsf{d}}\n$$\n ", "text_format": "latex", "page_idx": 50, "bbox": [228, 548, 252, 558], "page_size": [612.0, 792.0]}
{"layout": 737, "type": "text", "text": "Finally, we can divide both sides by 4 to  isolate the d variable on the right side of  the equation. This gives us: ", "page_idx": 50, "bbox": [229, 567.77001953125, 363, 594.0497436523438], "page_size": [612.0, 792.0]}
{"layout": 738, "type": "equation", "text": "\n$$\n{\\mathsf{d}}=5/4\n$$\n ", "text_format": "latex", "page_idx": 50, "bbox": [228, 602, 253, 613], "page_size": [612.0, 792.0]}
{"layout": 739, "type": "text", "text": "Flow Chart Understanding ", "page_idx": 51, "bbox": [118, 138.770263671875, 238.1006622314453, 149.50262451171875], "page_size": [612.0, 792.0]}
{"layout": 740, "type": "image", "page_idx": 51, "img_path": "layout_images/2309.17421v2_123.jpg", "bbox": [117, 162, 301, 533], "page_size": [612.0, 792.0], "ocr_text": "Explain the figure\n\nYes jot enough info\n\n", "vlm_text": "This flowchart outlines the process of evaluating and developing a project idea within a team. Here's a breakdown:\n\n1. **Idea Suggestion**: A team member suggests a project.\n2. **Discussion**: The team discusses the idea in a meeting.\n3. **Evaluation**: Participants decide if it's a good idea. Three paths can follow:\n   - **No**: The team rejects the idea.\n   - **Yes**: A team member further defines the project.\n   - **Not Enough Information**: A team member conducts more research.\n4. **Proposal Development**: If approved, a team member writes a proposal.\n5. **Proposal Review**: The team reviews the proposal. Depending on the outcome:\n   - **No**: The team member revises the proposal.\n   - **Yes**: The team sends the proposal to the final decision-maker.\n6. **Conclusion**: The process ends after sending the proposal."}
{"layout": 741, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 51, "bbox": [117, 538, 155, 549], "page_size": [612.0, 792.0]}
{"layout": 742, "type": "text", "text": "This figure represents a flow chart of a team’s proposal process. The process begins with a team member  suggesting a project in a meeting. The team then discusses whether it is a good idea or not. If the idea is not  considered a good one, the team rejects it and the process ends. ", "page_idx": 51, "bbox": [118, 551.0960693359375, 477.8035888671875, 577.3757934570312], "page_size": [612.0, 792.0]}
{"layout": 743, "type": "text", "text": "If the team thinks it might be a good idea but needs more information, a team member conducts further research  and refines the idea into a proposal. The team then reviews the proposal and provides feedback. If the proposal  is not okay, then the team member revises it and the team reviews it again. ", "page_idx": 51, "bbox": [118, 586.960693359375, 491, 613.240478515625], "page_size": [612.0, 792.0]}
{"layout": 744, "type": "text", "text": "If the proposal is okay, the team sends it to a final decision-making process. If the proposal is approved, then the  team proceeds with the project. This is the end of the proposal process. ", "page_idx": 51, "bbox": [118, 622.8253784179688, 491, 640.1389770507812], "page_size": [612.0, 792.0]}
{"layout": 745, "type": "text", "text": "Figure 35: Results on flow chart understanding. GPT-4V correctly describes the proposal process in details. Check Section  4.4  for detailed discussions. ", "page_idx": 51, "bbox": [108, 656.2635498046875, 503.9961242675781, 679.177490234375], "page_size": [612.0, 792.0]}
{"layout": 746, "type": "text", "text": "Flow Chart Understanding ", "text_level": 1, "page_idx": 52, "bbox": [117, 139, 240, 150], "page_size": [612.0, 792.0]}
{"layout": 747, "type": "text", "text": "Prompt: Can you translate the flowchart to a python code? ", "page_idx": 52, "bbox": [118, 154.14041137695312, 282.4933166503906, 173.89889526367188], "page_size": [612.0, 792.0]}
{"layout": 748, "type": "image", "page_idx": 52, "img_path": "layout_images/2309.17421v2_124.jpg", "bbox": [117, 179, 316, 494], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image is a flowchart illustrating a simple program that compares two numbers, `num1` and `num2`. Below is a step-by-step explanation of the flowchart:\n\n1. **Start**: The process begins.\n2. **Input num1**: The program prompts for and receives the first number (`num1`).\n3. **Input num2**: The program prompts for and receives the second number (`num2`).\n4. **Decision (num1 > num2)**: The program checks if `num1` is greater than `num2`.\n   - If **True**: It proceeds to display `num1`.\n   - If **False**: It proceeds to display `num2`.\n5. **Display num1 or num2**: Based on the result of the comparison, the program displays the greater of the two numbers.\n6. **End**: The process stops. \n\nIn summary, the flowchart represents a simple decision-making process to determine which of the two input numbers is greater and displays that number."}
{"layout": 749, "type": "text", "text": "GPT-4V: Sure, here’s the Python code: ", "page_idx": 52, "bbox": [118, 501.25872802734375, 216.36553955078125, 521.0172119140625], "page_size": [612.0, 792.0]}
{"layout": 750, "type": "text", "text": "num1  $=$   input(\"Enter first number: \") num2  $=$   input(\"Enter second number: \") if num1  $>$   num2:  print(num1) else: print(num2) ", "page_idx": 52, "bbox": [118, 527.709228515625, 467.81561279296875, 543.8410034179688], "page_size": [612.0, 792.0]}
{"layout": 751, "type": "text", "text": "Explanation: ", "page_idx": 52, "bbox": [118, 551.09619140625, 159.23617553710938, 559.443603515625], "page_size": [612.0, 792.0]}
{"layout": 752, "type": "text", "text": "- The first two lines correspond to the “Input num1” and “Input num2” blocks in the flowchart, where the user is  prompted to enter two numbers. - The next two lines correspond to the “num1  $>$   num2?” block, where a conditional statement is used to check if  the first number is greater than the second number. - If the condition is true, the next line (“Display num1”) is executed, and the first number is printed to the screen. - If the condition is false, the else statement is executed, and the second number is printed to the screen  (“Display num2”). - The code then stops, corresponding to the “Stop” block in the flowchart. ", "page_idx": 52, "bbox": [118, 569.028564453125, 486, 640.13916015625], "page_size": [612.0, 792.0]}
{"layout": 753, "type": "text", "text": "Figure 36: Results on flow chart understanding. GPT-4V is able to translate the flow chart to a python code. Check Section  4.4  for detailed discussions. ", "page_idx": 52, "bbox": [108, 656.2635498046875, 504.0014953613281, 679.177490234375], "page_size": [612.0, 792.0]}
{"layout": 754, "type": "text", "text": "Chart Understanding and Reasoning ", "page_idx": 53, "bbox": [118, 125.16155242919922, 285.03851318359375, 135.8939208984375], "page_size": [612.0, 792.0]}
{"layout": 755, "type": "text", "text": "Prompt: ", "page_idx": 53, "bbox": [118, 140.5316925048828, 154.9283905029297, 151.26405334472656], "page_size": [612.0, 792.0]}
{"layout": 756, "type": "text", "text": "What is the average total fueling cost excluding the Ford F150? ", "page_idx": 53, "bbox": [118, 151.94277954101562, 327.30169677734375, 160.29017639160156], "page_size": [612.0, 792.0]}
{"layout": 757, "type": "image", "page_idx": 53, "img_path": "layout_images/2309.17421v2_125.jpg", "img_caption": "GPT-4V: ", "bbox": [117, 162, 239, 350], "page_size": [612.0, 792.0], "ocr_text": "$130.96\n\n$82.56\n\nFord Toyota Honda\nF150 RAV4 Civic\n(Regular Cab)\n\nTotal fueling cost\n\n", "vlm_text": "The image is a bar chart comparing the total fueling costs for three different vehicles: a Ford F150 (Regular Cab), a Toyota RAV4, and a Honda Civic. The costs are represented in purple bars, with the exact values displayed above each bar. The Ford F150 has the highest fueling cost at $130.96, followed by the Toyota RAV4 at $82.56 and the Honda Civic at $70.55. The title of the chart is \"Total fueling cost.\""}
{"layout": 758, "type": "text", "text": "To find the average total fueling cost excluding the Ford F150, we need to add the total fueling costs of the  Toyota RAV4 and Honda Civic and divide by 2 (since there are 2 vehicles in the calculation). ", "page_idx": 53, "bbox": [118, 351.7601623535156, 472, 369.07366943359375], "page_size": [612.0, 792.0]}
{"layout": 759, "type": "equation", "text": "\n$$\n(82.56+70.55)\\div2=76.55\n$$\n ", "text_format": "latex", "page_idx": 53, "bbox": [117, 377, 209, 388], "page_size": [612.0, 792.0]}
{"layout": 760, "type": "text", "text": "Therefore, the average total fueling cost excluding the Ford F150 is  $\\Updownarrow76.55$  . ", "page_idx": 53, "bbox": [118, 395, 367.15386962890625, 405], "page_size": [612.0, 792.0]}
{"layout": 761, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 53, "bbox": [117, 427, 155, 437], "page_size": [612.0, 792.0]}
{"layout": 762, "type": "image", "page_idx": 53, "img_path": "layout_images/2309.17421v2_126.jpg", "img_caption": "Explain the figure. ", "bbox": [116, 438, 248, 566], "page_size": [612.0, 792.0], "ocr_text": "A paper’s impact on your career\n\nLots of,\nimpact\n\nEffect on you\n\nnothing\n\n(Creative, original\n\n‘Ok Preity good\nand good.\n\nPaper quality\n\n", "vlm_text": "The image is a graph that illustrates the relationship between the quality of a paper and its impact on one's career. The x-axis represents the quality of the paper, ranging from \"Bad\" to \"Creative, original, and good.\" The y-axis represents the effect of the paper on your career, ranging from \"nothing\" to \"Lots of impact.\"\n\nThe graph is a step function that remains flat and at the lowest level on the y-axis from \"Bad\" through \"Pretty good\" paper quality, indicating that papers of those qualities have little to no impact on one's career. When the paper quality reaches \"Creative, original, and good,\" the graph jumps sharply to the highest level on the y-axis, suggesting that such papers significantly impact one's career. This figure implies that only papers of the highest quality have a substantial career impact."}
{"layout": 763, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 53, "bbox": [117, 572, 155, 582], "page_size": [612.0, 792.0]}
{"layout": 764, "type": "text", "text": "The figure is a graph that shows the relationship between the quality of a paper and its impact on a person's  career. The  $\\mathsf{x}.$  -axis represents the quality of the paper, which ranges from bad to creative and original. The  y-axis represents the impact the paper has on a person's career, which ranges from nothing to lots of impact. ", "page_idx": 53, "bbox": [118, 584.38232421875, 476, 610.662109375], "page_size": [612.0, 792.0]}
{"layout": 765, "type": "text", "text": "The graph shows that a bad paper has little to no impact on a person's career, while a creative and original  paper has a significant impact. The impact of the paper on a person's career increases as the quality of the  paper improves. This suggests that the quality of a person's work is an important factor in determining their  success in their career.  ", "page_idx": 53, "bbox": [118, 620.2470703125, 472, 655.492919921875], "page_size": [612.0, 792.0]}
{"layout": 766, "type": "text", "text": "Figure 37: GPT-4V shows promising results in understanding the details in the chart, as well as in reasoning and accurately responding to related questions. Check Section  4.4  for detailed discussions. ", "page_idx": 53, "bbox": [108, 669.87255859375, 504, 692.7864990234375], "page_size": [612.0, 792.0]}
{"layout": 767, "type": "text", "text": "Sec. 4.4 Scene Text, Table, Chart, and Document Reasoning ", "text_level": 1, "page_idx": 54, "bbox": [133, 151, 466, 168], "page_size": [612.0, 792.0]}
{"layout": 768, "type": "text", "text": "Table Understanding and Reasoning ", "page_idx": 54, "bbox": [117, 175.59585571289062, 284.3180236816406, 186.32821655273438], "page_size": [612.0, 792.0]}
{"layout": 769, "type": "text", "text": "Prompt: How many families are earning more than 13000 and owns more than 2 cars? ", "page_idx": 54, "bbox": [117, 190.96641540527344, 375.4871826171875, 210.7248992919922], "page_size": [612.0, 792.0]}
{"layout": 770, "type": "table", "page_idx": 54, "img_path": "layout_images/2309.17421v2_127.jpg", "table_caption": "GPT-4V: ", "bbox": [117, 219, 416, 386], "page_size": [612.0, 792.0], "ocr_text": "Monthly Vehicles per family\n160 25 0\n\nLess than 7000 10\n\n7000 — 10000 ie) 305 2\n10000 — 13000 i 535 i\n13000 — 16000 2 469 25\n16000 or more 1 579 82 88\n", "vlm_text": "The table presents data on the distribution of vehicles per family across different monthly income brackets, measured in Indian Rupees (Rs.). The columns represent the number of vehicles per family, organized into four categories: 0 vehicles, 1 vehicle, 2 vehicles, and more than 2 vehicles. The rows correspond to five different income ranges:\n\n1. Less than 7000 Rs.\n   - 0 vehicles: 10 families\n   - 1 vehicle: 160 families\n   - 2 vehicles: 25 families\n   - Above 2 vehicles: 0 families\n\n2. 7000 – 10000 Rs.\n   - 0 vehicles: 0 families\n   - 1 vehicle: 305 families\n   - 2 vehicles: 27 families\n   - Above 2 vehicles: 2 families\n   \n3. 10000 – 13000 Rs.\n   - 0 vehicles: 1 family\n   - 1 vehicle: 535 families\n   - 2 vehicles: 29 families (highlighted in the table)\n   - Above 2 vehicles: 1 family\n\n4. 13000 – 16000 Rs.\n   - 0 vehicles: 2 families\n   - 1 vehicle: 469 families\n   - 2 vehicles: 59 families\n   - Above 2 vehicles: 25 families\n\n5. 16000 or more Rs.\n   - 0 vehicles: 1 family\n   - 1 vehicle: 579 families\n   - 2 vehicles: 82 families\n   - Above 2 vehicles: 88 families\n\nThis table illustrates how vehicle ownership correlates with household income, and it shows that as income levels increase, the likelihood of owning more vehicles per family also tends to increase."}
{"layout": 771, "type": "text", "text": "From the given table, we can clearly see that there are a total of 113 families that earn more than Rs. 13000 and  own more than 2 cars (25 families in the 13000-16000 range and 88 families in the 16000 or more range). ", "page_idx": 54, "bbox": [117, 388.1692199707031, 490.2515563964844, 405.4827575683594], "page_size": [612.0, 792.0]}
{"layout": 772, "type": "table", "page_idx": 54, "img_path": "layout_images/2309.17421v2_128.jpg", "table_caption": "Prompt: Which country has the highest profit margin? ", "bbox": [116, 425, 430, 556], "page_size": [612.0, 792.0], "ocr_text": "USA\n\nChina\n\nAustralia\n\nIndia\n\nSouth Korea 494.948\n\n", "vlm_text": "The table presents data on sales performance across different countries, including:\n\n- **Country**: The regions covered are USA, China, Australia, India, and South Korea.\n- **Sales Volume**: Numeric values indicating units sold, with the USA having the highest at 40.080.\n- **Revenue**: The monetary revenue for each country, with the USA leading at $15,971,880.\n- **Profit**: The profit figures for each country, with the USA having the highest profit at $3,086,421.\n- **Profit Margin**: Percentage of profit margin for each country, with Australia having the highest at 19.4%.\n\nThe totals and averages for all countries combined are also shown:\n- Total sales volume: 141.282\n- Total revenue: $67,754,238\n- Total profit: $12,816,772\n- Average profit margin: 18.8%"}
{"layout": 773, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 54, "bbox": [116, 565, 155, 575], "page_size": [612.0, 792.0]}
{"layout": 774, "type": "text", "text": "Based on the data provided, the country with the highest profit margin is Australia, with a profit margin of   $19{,}4\\%$  . However, the difference between the top three countries (USA, China and Australia) is minimal, with  less than  $0{,}3\\%$   difference in profit margin. ", "page_idx": 54, "bbox": [117, 577.0826416015625, 474.4786376953125, 603.3624267578125], "page_size": [612.0, 792.0]}
{"layout": 775, "type": "text", "text": "Figure 38: We observe GPT-4V can understand the details in the table, and answer related questions. Check Section  4.4  for detailed discussions. ", "page_idx": 54, "bbox": [108, 619.4375610351562, 505.7440490722656, 642.3515014648438], "page_size": [612.0, 792.0]}
{"layout": 776, "type": "text", "text": "Sec. 4.4 Scene Text, Table, Chart, and Document Reasoning ", "text_level": 1, "page_idx": 55, "bbox": [113, 21, 480, 41], "page_size": [612.0, 792.0]}
{"layout": 777, "type": "text", "text": "Document Understanding ", "text_level": 1, "page_idx": 55, "bbox": [117, 47, 236, 58], "page_size": [612.0, 792.0]}
{"layout": 778, "type": "text", "text": "", "text_level": 1, "page_idx": 55, "bbox": [117, 63, 154, 68.75], "page_size": [612.0, 792.0]}
{"layout": 779, "type": "image", "page_idx": 55, "img_path": "layout_images/2309.17421v2_129.jpg", "img_caption": "Describe the image. ", "bbox": [116, 69.25, 316, 222], "page_size": [612.0, 792.0], "ocr_text": "TERRACE\n216\" x80\"\n\nSECOND BEDROOM\n1010\" x 147\"\n\nMASTER BEDROOM\n1210\" 179\n\nDINING AREA\n9110\" x 13°0\"\n\nLIVING ROOM\n146\" x 205\"\n\n", "vlm_text": "The image is a floor plan for what appears to be a residential apartment. It includes the following labeled areas and dimensions:\n\n- **Terrace**: 21'6\" x 8'0\"\n- **Dining Area**: 9'10\" x 13'0\"\n- **Living Room**: 14'6\" x 20'5\"\n- **Second Bedroom**: 10'10\" x 14'7\"\n- **Master Bedroom**: 12'10\" x 17'9\"\n- **Foyer**: 6'0\" x 6'3 1/2\"\n- **WC** (Water Closet): 5'2\" x 5'2\"\n- **Bathroom**: Equipped with a tub.\n- **Kitchen**: Includes appliances such as a refrigerator (FRIG), oven, and dishwasher (DW). There's also a built-in unit and closet space labeled \"CL\".\n- **Entrance**: Marked at the bottom, near the Foyer.\n- *Miscellaneous*: A laundry chute (LC) near the bathroom and a closet space (CL). \n\nThe plan also includes directional information indicating north (N), and it is noted as being \"For illustration purposes only\" at the bottom."}
{"layout": 780, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 55, "bbox": [117, 229, 154, 239], "page_size": [612.0, 792.0]}
{"layout": 781, "type": "text", "text": "This image is a floor plan for an apartment or  condominium. It shows the layout of the rooms and the  dimensions. There is a terrace, dining area, living room,  foyer, two bedrooms, a master bathroom, and a guest  bathroom. The floor plan is black and white and is a  two-dimensional drawing. ", "page_idx": 55, "bbox": [118, 240.69390869140625, 303, 293.8719787597656], "page_size": [612.0, 792.0]}
{"layout": 782, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 55, "bbox": [117, 308, 155, 318], "page_size": [612.0, 792.0]}
{"layout": 783, "type": "text", "text": "Answer the question in the image. ", "page_idx": 55, "bbox": [118, 319.46826171875, 230.6141815185547, 327.8156433105469], "page_size": [612.0, 792.0]}
{"layout": 784, "type": "image", "page_idx": 55, "img_path": "layout_images/2309.17421v2_130.jpg", "bbox": [120, 337, 318, 507], "page_size": [612.0, 792.0], "ocr_text": "ty\n\nFigure 1 shows the structure of the earth's interior. Write the number showing\neach layer in the table below.\n\nFigure 1\n\nLayer Number on figure 1\n\nOuter core\n\nMantle\n\nCrust\n\nInner core\n\n(4)\n\n", "vlm_text": "The image is a diagram showing the structure of the Earth's interior. It is divided into different layers with corresponding numbers. Below the diagram, there is a table that asks to match each layer name with the number shown on the diagram. The specified layers are:\n\n- Outer core\n- Mantle\n- Crust\n- Inner core\n\nThe numbers on the diagram corresponding to these layers are:\n\n- Crust is number 1\n- Mantle is number 2\n- Outer core is number 3\n- Inner core is number 4"}
{"layout": 785, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 55, "bbox": [117, 516, 154, 526], "page_size": [612.0, 792.0]}
{"layout": 786, "type": "text", "text": "Layer | Number on figure 1 --- | --- Outer core | 3 Mantle | 2 Crust | 1 Inner core | 4  ", "page_idx": 55, "bbox": [118, 528.2022094726562, 206.8189697265625, 581.38037109375], "page_size": [612.0, 792.0]}
{"layout": 787, "type": "text", "text": "Visualization of the generated results: ", "text_level": 1, "page_idx": 55, "bbox": [117, 585, 291, 597], "page_size": [612.0, 792.0]}
{"layout": 788, "type": "table", "page_idx": 55, "img_path": "layout_images/2309.17421v2_131.jpg", "bbox": [123, 607, 236, 675], "page_size": [612.0, 792.0], "ocr_text": "Layer Number on figure 1\nOutercore 3\nMantle 2\n\nCrust 1\n", "vlm_text": "The table lists layers of the Earth with corresponding numbers:\n\n- Crust: 1\n- Mantle: 2\n- Outer core: 3\n- Inner core: 4"}
{"layout": 789, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 55, "bbox": [330, 63, 367, 72.75], "page_size": [612.0, 792.0]}
{"layout": 790, "type": "image", "page_idx": 55, "img_path": "layout_images/2309.17421v2_132.jpg", "img_caption": "Where is the bathroom for the second bedroom? ", "bbox": [330, 73.25, 491, 149], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The floor plan shows a layout of a living space. The second bedroom does not have an en-suite bathroom. The bathroom is located near the entrance, next to the master bedroom, and accessible from the hallway close to both bedrooms."}
{"layout": 791, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 55, "bbox": [330, 157, 367, 166], "page_size": [612.0, 792.0]}
{"layout": 792, "type": "text", "text": "According to this floor plan, the bathroom for the  second bedroom appears to be located directly  adjacent to the bedroom. It is labeled \"WC\" and is  a small room measuring  $52\"\\times52\"$  . ", "page_idx": 55, "bbox": [330, 168.19622802734375, 498, 203.44207763671875], "page_size": [612.0, 792.0]}
{"layout": 793, "type": "text", "text": "Prompt: Describe the poster. Which city is this dish from? ", "page_idx": 55, "bbox": [330, 226.1971893310547, 498, 245.9556427001953], "page_size": [612.0, 792.0]}
{"layout": 794, "type": "image", "page_idx": 55, "img_path": "layout_images/2309.17421v2_133.jpg", "bbox": [331, 255, 481, 483], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "This image shows a bowl of hot dry noodles, known as \"Wuhan 热干面\" (Wuhan Re Gan Mian). The text is in Chinese and mentions that these noodles are a specialty snack, emphasizing their strong flavor and quality. It describes the dish as a well-known snack originating from Wuhan and outlines its characteristics and ingredients, including seasonings like sesame sauce and pickled vegetables."}
{"layout": 795, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 55, "bbox": [330, 489, 367, 498], "page_size": [612.0, 792.0]}
{"layout": 796, "type": "text", "text": "The poster is an advertisement for a Chinese dish  called \"Hot dry noodles.\" The poster has a red  background with white Chinese characters at the  top that read \"Hot dry noodles, the taste of China.\"  Below the text is a photo of the dish, which  consists of noodles with meat and vegetables on  top. The dish is served in a green bowl with red  chili peppers on the side. The text at the bottom of  the poster is in Chinese and describes the dish as  being spicy, fragrant, and delicious. ", "page_idx": 55, "bbox": [330, 500.6365051269531, 498, 589.6790771484375], "page_size": [612.0, 792.0]}
{"layout": 797, "type": "text", "text": "This dish is from the city of Wuhan, located in the  Hubei Province of China. It is a popular street food  in the city and is considered one of the  $\"10$   great  breakfasts\" in China.  ", "page_idx": 55, "bbox": [330, 599.263916015625, 498, 634.509765625], "page_size": [612.0, 792.0]}
{"layout": 798, "type": "text", "text": "Figure 39: Results on document understanding. GPT-4V recognizes three different types of document and answers the questions correctly. Check Section  4.4  for detailed discussions. ", "page_idx": 55, "bbox": [108, 698.8495483398438, 504.00152587890625, 721.7634887695312], "page_size": [612.0, 792.0]}
{"layout": 799, "type": "text", "text": "Sec. 4.4 Scene Text, Table, Chart, and Document Reasoning ", "text_level": 1, "page_idx": 56, "bbox": [124, 93, 462, 113], "page_size": [612.0, 792.0]}
{"layout": 800, "type": "text", "text": "Document Understanding ", "text_level": 1, "page_idx": 56, "bbox": [115, 120, 236, 131], "page_size": [612.0, 792.0]}
{"layout": 801, "type": "image", "page_idx": 56, "img_path": "layout_images/2309.17421v2_134.jpg", "img_caption": "Figure 40: Results on document understanding. GPT-4V reads a multi-page technical report, under- stands the content in each section, and provides a summary of the contribution of this technical report. Red highlights the wrong answer. Check Section  4.4  for detailed discussions. ", "bbox": [106, 134, 506, 699], "page_size": [612.0, 792.0], "ocr_text": "Prompt:\n\nThis is a paper \"Paper Gestalt.\"\nDescribe the paper in details\nand highlight their contribution.\n\nPrompt Continued\nPage 4\n\n", "vlm_text": "The image appears to be a compilation of pages from a multi-page technical report titled \"Paper Gestalt.\" Each page contains various elements including text, graphs, diagrams, and charts. The prompt on the left side asks for a detailed description and the highlighting of contributions of the paper. The red text highlights indicate areas where the information provided by GPT-4V was incorrect, suggesting this is an example of a document understanding task involving error identification and correction."}
{"layout": 802, "type": "text", "text": "4.5 Multilingual Multimodal Understanding ", "text_level": 1, "page_idx": 57, "bbox": [106, 73, 303, 85], "page_size": [612.0, 792.0]}
{"layout": 803, "type": "text", "text": "We assess GPT-4V’s ability in comprehending multiple languages and modalities. First, we explore this capability by evaluating natural images without scene text, as depicted in Figure  41 . In the first row of the figure, we provide the input text prompt “Describe the image” in Chinese, French, and Czech, respectively. GPT-4V recognizes the input text prompts in different languages, and generates correct image descriptions in corresponding languages. In the second row of Figure  41 , we provide the input text prompt in English and specify the output language. GPT-4V follows the instruction and generates correct descriptions in the desired languages. In the bottom row of Figure  41 , we provide an input prompt in Spanish, and ask GPT-4V to generate image descriptions in 20 different languages. We observe that GPT-4V can process both the input and output text in different languages. ", "page_idx": 57, "bbox": [107, 92.7325439453125, 505, 192.00950622558594], "page_size": [612.0, 792.0]}
{"layout": 804, "type": "text", "text": "Furthermore, we explore a scenario involving multilingual scene text recognition, where the input image may contain scene text in various languages. As shown in Figure  42 , GPT-4V correctly identifies and understands the scene text from different scenes. As shown in the first two rows of Figure  43 , we observe that GPT-4V can recognize the scene text, and translate it to a different language. In the bottom row of Figure  43 , we provide a screenshot of a Wikipedia website which is in Catalan, and instruct GPT-4V to summarize the information in 20 different languages. GPT-4V not only recognizes the text in Catalan but also generates precise summaries and translates them into different languages. This showcases GPT-4V’s ability to comprehend and translate multilingual scene text. ", "page_idx": 57, "bbox": [107, 196.3935546875, 505, 295.6715087890625], "page_size": [612.0, 792.0]}
{"layout": 805, "type": "text", "text": "We also explore the capability of multicultural understanding [ 147 ,  77 ]. Figure  44  shows the example results in such a scenario. We observe that GPT-4V is capable of understanding cultural nuances and generating reasonable multilingual descriptions for the wedding images given. ", "page_idx": 57, "bbox": [107, 300.0545654296875, 505, 333.87750244140625], "page_size": [612.0, 792.0]}
{"layout": 806, "type": "text", "text": "In our exploration, we found that GPT-4V seamlessly comprehends and correctly generates descrip- tions in different languages, highlighting its versatility in handling diverse linguistic contexts. ", "page_idx": 57, "bbox": [107, 338.2615661621094, 505, 361.1755065917969], "page_size": [612.0, 792.0]}
{"layout": 807, "type": "text", "text": "Multilingual Image Description ", "text_level": 1, "page_idx": 58, "bbox": [77, 59, 220, 70], "page_size": [612.0, 792.0]}
{"layout": 808, "type": "image", "page_idx": 58, "img_path": "layout_images/2309.17421v2_135.jpg", "img_caption": "Prompt: 描述這張圖片", "bbox": [77, 75, 189, 182], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows a cute orange tabby kitten with blue eyes walking through the grass. The kitten is looking downward as it walks, and the focus of the photograph is on the adorable feline against a blurred background of greenery."}
{"layout": 809, "type": "image", "page_idx": 58, "img_path": "layout_images/2309.17421v2_136.jpg", "img_caption": "Prompt: Décrire l’image ", "bbox": [229, 75, 341, 182], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows a small orange tabby kitten walking through grass. The kitten has a fluffy fur coat and blue eyes, and it appears to be young and curious, exploring its surroundings in a natural outdoor setting."}
{"layout": 810, "type": "image", "page_idx": 58, "img_path": "layout_images/2309.17421v2_137.jpg", "img_caption": "Prompt: Popište obrázek ", "bbox": [391, 75, 504, 182], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows a cute ginger kitten walking on grass. The kitten has a soft orange coat with subtle stripes and blue eyes, and the background is a blurry green, suggesting it is outside in a grassy area."}
{"layout": 811, "type": "text", "text": "", "text_level": 1, "page_idx": 58, "bbox": [78, 186, 115, 192.75], "page_size": [612.0, 792.0]}
{"layout": 812, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 58, "bbox": [229, 190, 267, 200], "page_size": [612.0, 792.0]}
{"layout": 813, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 58, "bbox": [392, 190, 429, 200], "page_size": [612.0, 792.0]}
{"layout": 814, "type": "text", "text": "這張圖片是一隻小貓在草地上走路的畫面。小貓是橘色的，有藍色的眼睛，看起來很可愛。它的身體很小，四肢纖細，正在小心翼翼地在草地上行走。 ", "page_idx": 58, "bbox": [78, 198.07546997070312, 198, 241.88522338867188], "page_size": [612.0, 792.0]}
{"layout": 815, "type": "text", "text": "Il s’agit d’une photo d’un chaton roux  marchant sur l’herbe verte. Le chaton a les  yeux bleus et semble être en train de  marcher avec prudence. Il est entouré  d’herbe et on peut voir une partie d’un  autre chat en arrière-plan.  ", "page_idx": 58, "bbox": [230, 203.45289611816406, 374.7613830566406, 263.5846862792969], "page_size": [612.0, 792.0]}
{"layout": 816, "type": "text", "text": "Na obrázku je malé oranžové kotě s  modrýma očima. Kotě stojí na zelené  trávě a směřuje dopředu. Jeho tělo je v  pohybu a vypadá, že právě běží. Na  pozadí je vidět další část trávy a nejasně i  další kočka.  ", "page_idx": 58, "bbox": [392, 203.45289611816406, 534, 256.83441162109375], "page_size": [612.0, 792.0]}
{"layout": 817, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 58, "bbox": [77, 277, 115, 287], "page_size": [612.0, 792.0]}
{"layout": 818, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 58, "bbox": [391, 278, 430, 287.75], "page_size": [612.0, 792.0]}
{"layout": 819, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 58, "bbox": [229, 279, 267, 288], "page_size": [612.0, 792.0]}
{"layout": 820, "type": "text", "text": "Describe the image in Chinese ", "page_idx": 58, "bbox": [78, 288.8809509277344, 180.66331481933594, 297.26025390625], "page_size": [612.0, 792.0]}
{"layout": 821, "type": "image", "page_idx": 58, "img_path": "layout_images/2309.17421v2_138.jpg", "img_caption": "Describe the image in Czech ", "bbox": [392, 288.25, 527, 390], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "Obrázek zobrazuje malé, nadšené štěně plemene corgi, které kráčí po trávě. Má hnědo-bílou srst a kolem krku obojek s přívěskem ve tvaru kosti. Štěně vypadá, že se raduje ze hry venku."}
{"layout": 822, "type": "text", "text": "Describe the image in French ", "page_idx": 58, "bbox": [230, 289.9334716796875, 328.5166015625, 298.3127746582031], "page_size": [612.0, 792.0]}
{"layout": 823, "type": "image", "page_idx": 58, "img_path": "layout_images/2309.17421v2_139.jpg", "bbox": [231, 301, 365, 390], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows a small, brown and white puppy walking on grass. The puppy appears to be a Corgi, a breed known for its short legs and long body. It is wearing a collar with a bone-shaped tag. The puppy has its mouth open, possibly panting or in the middle of barking. The background is blurred, focusing attention on the puppy as it trots along the grass."}
{"layout": 824, "type": "image", "page_idx": 58, "img_path": "layout_images/2309.17421v2_140.jpg", "bbox": [79, 302, 213, 390], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows a small dog with a light brown and white coat, possibly a corgi, standing in a grassy area. The dog is wearing a collar with a bone-shaped tag and appears to be yawning or panting."}
{"layout": 825, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 58, "bbox": [392, 397, 429, 406], "page_size": [612.0, 792.0]}
{"layout": 826, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 58, "bbox": [229, 397, 267, 407], "page_size": [612.0, 792.0]}
{"layout": 827, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 58, "bbox": [78, 397, 115, 407], "page_size": [612.0, 792.0]}
{"layout": 828, "type": "text", "text": "这是一只小柯基犬，它在草地上走着，嘴巴张开，舌头伸出来，好像很开心的样子。它的身上有一条项圈，上面挂着一个骨头形状的牌子。 ", "page_idx": 58, "bbox": [78, 409.3809814453125, 216, 444.1903076171875], "page_size": [612.0, 792.0]}
{"layout": 829, "type": "text", "text": "Na obrázku je malý hnědý pes s bílým  límcem a kostičkou na obojku. Pes je na  zelené trávě a má otevřenou tlamu.  Vypadá, že se baví.  ", "page_idx": 58, "bbox": [392, 408.5931701660156, 528, 443.97381591796875], "page_size": [612.0, 792.0]}
{"layout": 830, "type": "text", "text": "C'est une image d'un petit chien de race  Corgi. Il est de couleur marron et blanc et  porte un collier avec une médaille en  forme d'os. Il est en train de marcher sur  l'herbe verte avec la langue pendante. ", "page_idx": 58, "bbox": [230, 408.8677062988281, 370.1561584472656, 453.248779296875], "page_size": [612.0, 792.0]}
{"layout": 831, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 58, "bbox": [77, 470, 115, 480], "page_size": [612.0, 792.0]}
{"layout": 832, "type": "text", "text": "GPT-4V output continued ", "text_level": 1, "page_idx": 58, "bbox": [308, 473, 396, 482], "page_size": [612.0, 792.0]}
{"layout": 833, "type": "text", "text": "Describir la imagen en 20 idiomas diferentes ", "page_idx": 58, "bbox": [78, 481.63555908203125, 226.4892578125, 490.0148620605469], "page_size": [612.0, 792.0]}
{"layout": 834, "type": "text", "text": "4. Alemán: Dies ist eine schöne Aussicht auf Machu Picchu in Peru.  5. Italiano: Questa è una bella vista di Machu Picchu in Perù.  6. Portugués: Esta é uma bela vista de Machu Picchu no Peru.  7. Ruso: Это красивый вид на Мачу-Пикчу в Перу.  8. Chino: 这是秘鲁马丘比丘的美丽景色。 9. Japonés: これはペルーのマチュピチュの美しい景色です。 10. Coreano: 이것은 페루의 마추 피추의 아름다운 경치입니다. 11. Hindi:  यह   पेǾ   क   माचू   ͪपÍचू   का   सुंदर   Ǻæय   है।   12. Árabe: .ﺑﯾرو  $\\scriptstyle{\\varphi^{\\vec{\\mathbf{a}}}}$  ھذا ﻣﻧظر ﺟﻣﯾل ﻟﻣﺎﺷو ﺑﯾﺗﺷو  13. Turco: Bu, Peru'daki Machu Picchu'nun güzel bir man zara s d r.  14. Griego: Αυτή είναι μια όμορφη θέα του Μάτσου Πίτσου στο  Περού.  15. Hebreo: .זהו נוף יפה של מאצ'ו פיצ'ו בפרו  16. Sueco: Det här är en vacker vy av Machu Picchu i Peru.  17. Noruego: Dette er en vakker utsikt over Machu Picchu i Peru.  18. Danés: Dette er en smuk udsigt over Machu Picchu i Peru.  19. Holandés: Dit is een prachtig uitzicht op Machu Picchu in Peru.  20. Polaco: To piękny widok na Machu Picchu w Peru. \n\n ", "page_idx": 58, "bbox": [310, 483.6285095214844, 534, 667.9662475585938], "page_size": [612.0, 792.0]}
{"layout": 835, "type": "image", "page_idx": 58, "img_path": "layout_images/2309.17421v2_141.jpg", "bbox": [78, 495, 283, 631], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows Machu Picchu, an ancient Incan city located in the Andes Mountains of Peru. The picture showcases the well-preserved stone structures and terraces of the archaeological site, with a backdrop of lush green mountains and a clear blue sky. Machu Picchu is a UNESCO World Heritage Site and is renowned for its historical significance and architectural ingenuity."}
{"layout": 836, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 58, "bbox": [78, 638, 115, 647], "page_size": [612.0, 792.0]}
{"layout": 837, "type": "text", "text": "1. Inglés: This is a beautiful view of Machu Picchu in Peru. \n\n 2. Español: Esta es una hermosa vista de Machu Picchu en Perú.\n\n 3. Francés: C'est une belle vue de Machu Picchu au Pérou.  ", "page_idx": 58, "bbox": [78, 649.2365112304688, 296.46636962890625, 675.6166381835938], "page_size": [612.0, 792.0]}
{"layout": 838, "type": "text", "text": "Multilingual Scene Text Recognition ", "text_level": 1, "page_idx": 59, "bbox": [77, 59, 245, 70], "page_size": [612.0, 792.0]}
{"layout": 839, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 59, "bbox": [77, 75, 115, 85], "page_size": [612.0, 792.0]}
{"layout": 840, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 59, "bbox": [395, 75, 433, 85], "page_size": [612.0, 792.0]}
{"layout": 841, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 59, "bbox": [229, 75, 267, 85], "page_size": [612.0, 792.0]}
{"layout": 842, "type": "image", "page_idx": 59, "img_path": "layout_images/2309.17421v2_142.jpg", "img_caption": "What are all the scene text in the image? ", "bbox": [394, 86, 532, 180], "page_size": [612.0, 792.0], "ocr_text": "Tsim Sha Tsui\n\n", "vlm_text": "The text in the image says:\n\n失沙咀\nTsim Sha Tsui"}
{"layout": 843, "type": "text", "text": "What are all the scene text in the image? ", "page_idx": 59, "bbox": [230, 86.64017486572266, 369, 95.01948547363281], "page_size": [612.0, 792.0]}
{"layout": 844, "type": "text", "text": "What are all the scene text in the image? ", "page_idx": 59, "bbox": [78, 86.64017486572266, 214, 95.01948547363281], "page_size": [612.0, 792.0]}
{"layout": 845, "type": "image", "page_idx": 59, "img_path": "layout_images/2309.17421v2_143.jpg", "bbox": [231, 98, 368, 180], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows a glass entrance with the text \"Musée du Louvre,\" indicating that it is the entrance to the Louvre Museum in Paris, France. The structure in the background is part of the Louvre Pyramid, which is a well-known architectural feature of the museum."}
{"layout": 846, "type": "image", "page_idx": 59, "img_path": "layout_images/2309.17421v2_144.jpg", "bbox": [78, 100, 184, 180], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "This image shows an Islamic calligraphy tile featuring the phrase \"Bismillah ir-Rahman ir-Rahim\" in Arabic, which translates to \"In the name of Allah, the Most Gracious, the Most Merciful.\" The tile is decorated with floral patterns commonly found in Islamic art."}
{"layout": 847, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 59, "bbox": [395, 184, 433, 194], "page_size": [612.0, 792.0]}
{"layout": 848, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 59, "bbox": [229, 184, 267, 194], "page_size": [612.0, 792.0]}
{"layout": 849, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 59, "bbox": [78, 185, 115, 194], "page_size": [612.0, 792.0]}
{"layout": 850, "type": "text", "text": "The scene text in the image is an Arabic  phrase \" اﻟرﺣﻣن اﻟرﺣﯾم $\\scriptstyle\\mathtt{\\sim}$  (Bismillah al-Rahman al-Rahim). It translates to \"In  the name of Allah, the Most Merciful, the  Most Compassionate\". This phrase is  commonly used by Muslims at the  beginning of any action or speech as a way  to seek blessings and guidance from God.   ", "page_idx": 59, "bbox": [78, 196.3811492919922, 224.4180145263672, 267.7635192871094], "page_size": [612.0, 792.0]}
{"layout": 851, "type": "text", "text": "The scene text in the image is \"MUSEE  DU LOUVRE\" which is the name of the  famous art museum in Paris, France.    ", "page_idx": 59, "bbox": [230, 197.53831481933594, 369, 223.9185028076172], "page_size": [612.0, 792.0]}
{"layout": 852, "type": "text", "text": "The scene text in the image are: - \" 尖沙咀 \" (Chinese characters) - \"Tsim Sha Tsui\" (English text)  ", "page_idx": 59, "bbox": [396, 197.53831481933594, 503, 223.9185028076172], "page_size": [612.0, 792.0]}
{"layout": 853, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 59, "bbox": [77, 277, 115, 287], "page_size": [612.0, 792.0]}
{"layout": 854, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 59, "bbox": [394, 278, 432, 287], "page_size": [612.0, 792.0]}
{"layout": 855, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 59, "bbox": [229, 279, 267, 288], "page_size": [612.0, 792.0]}
{"layout": 856, "type": "text", "text": "What are all the scene text in the image? ", "page_idx": 59, "bbox": [396, 288.7691650390625, 532, 297.1484680175781], "page_size": [612.0, 792.0]}
{"layout": 857, "type": "text", "text": "What are all the scene text in the image? ", "page_idx": 59, "bbox": [78, 288.8809509277344, 214, 297.26025390625], "page_size": [612.0, 792.0]}
{"layout": 858, "type": "text", "text": "What are all the scene text in the image? ", "page_idx": 59, "bbox": [230, 289.9334716796875, 369, 298.3127746582031], "page_size": [612.0, 792.0]}
{"layout": 859, "type": "image", "page_idx": 59, "img_path": "layout_images/2309.17421v2_145.jpg", "bbox": [79, 302, 154, 402], "page_size": [612.0, 792.0], "ocr_text": "bienyenida\n\nBienvenue\nZ Wilhowmen\n\n", "vlm_text": "The image shows a signpost with the word \"Welcome\" in several languages, including Spanish (\"bienvenida\"), French (\"Bienvenue\"), German (\"Willkommen\"), and Italian (\"benvenuto\")."}
{"layout": 860, "type": "image", "page_idx": 59, "img_path": "layout_images/2309.17421v2_146.jpg", "bbox": [231, 302, 322, 402], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image depicts the entrance to Super Nintendo World, with a large colorful sign stating \"SUPER NINTENDO WORLD\" above a tunnel entrance that resembles a warp pipe from the Mario video games. Two people are visible from the back, standing in front of the entrance, and decorative elements such as a wreath with a red bow are also present, suggesting a festive or holiday theme. Super Nintendo World is a themed area within Universal Studios theme parks, based on Nintendo's video game franchises."}
{"layout": 861, "type": "image", "page_idx": 59, "img_path": "layout_images/2309.17421v2_147.jpg", "bbox": [396, 302, 471, 401], "page_size": [612.0, 792.0], "ocr_text": "GP PFIM DA EUROPA.\nATLETISMO\n\nBi. Antonio Agostinho _\n; 1987 2\n\n_ Fernando Domingos\n1988 i\n\nhs Om\n\n", "vlm_text": "The image shows a stone plaque with several smaller plaques attached to it. The text on the plaques is in Portuguese and reads:\n\n\"G.P. FIM DA EUROPA\nATLETISMO\n\nAntónio Agostinho\n1987\n\nFernando Domingos\n1988\"\n\nThis appears to commemorate athletic events or achievements."}
{"layout": 862, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 59, "bbox": [78, 407, 115, 417], "page_size": [612.0, 792.0]}
{"layout": 863, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 59, "bbox": [395, 407, 433, 416], "page_size": [612.0, 792.0]}
{"layout": 864, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 59, "bbox": [229, 407, 268, 417], "page_size": [612.0, 792.0]}
{"layout": 865, "type": "text", "text": "The scene text in the image are: - G.P. FIM DA EUROPA - ATLETISMO - António Agostinho 1987 - Fernando Domingos 1988 ", "page_idx": 59, "bbox": [396, 418.0539245605469, 503, 462.43499755859375], "page_size": [612.0, 792.0]}
{"layout": 866, "type": "text", "text": "There are two scene texts in the image: 1) \"SUPER NINTENDO WORLD\" 2) \"スーパーニンテンドーワールド\" (this is the Japanese version of \"Super Nintendo  World\")  ", "page_idx": 59, "bbox": [230, 419.2182312011719, 369, 463.59930419921875], "page_size": [612.0, 792.0]}
{"layout": 867, "type": "text", "text": "The scene text in the image are:\n\n - WELCOME (English)\n\n - bienvenida (Spanish)\n\n - Bienvenue (French)\n\n - Willkommen (German)\n\n - benvenuto (Italian)  ", "page_idx": 59, "bbox": [78, 419.3871765136719, 186, 472.7686767578125], "page_size": [612.0, 792.0]}
{"layout": 868, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 59, "bbox": [310, 483, 348, 493], "page_size": [612.0, 792.0]}
{"layout": 869, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 59, "bbox": [77, 483, 115, 493], "page_size": [612.0, 792.0]}
{"layout": 870, "type": "text", "text": "What are all the scene text in the image? ", "page_idx": 59, "bbox": [78, 494.4933166503906, 214, 502.87261962890625], "page_size": [612.0, 792.0]}
{"layout": 871, "type": "text", "text": "What are all the scene text in the image? ", "page_idx": 59, "bbox": [311, 494.4933166503906, 446.9740295410156, 502.87261962890625], "page_size": [612.0, 792.0]}
{"layout": 872, "type": "image", "page_idx": 59, "img_path": "layout_images/2309.17421v2_148.jpg", "bbox": [79, 508, 188, 588], "page_size": [612.0, 792.0], "ocr_text": "ENAER. TAXVAP. EYPKOT\nfee ~TOYDKIAZ| m0\n\n", "vlm_text": "This is an image of a Greek postage stamp. It features an illustration of the Parthenon with a zeppelin flying above it. The text includes a date \"29 May 1933\" and refers to destinations Athens, Rome, and Istanbul. The denomination is \"120 drachmas.\""}
{"layout": 873, "type": "image", "page_idx": 59, "img_path": "layout_images/2309.17421v2_149.jpg", "bbox": [311, 508, 494, 632], "page_size": [612.0, 792.0], "ocr_text": "vais © arma) tox umtder be ondex Arpinbo. no Quy tl!\n\n", "vlm_text": "The image shows an outdoor scene with a person walking on a bridge that has colorful glass panels. Below the bridge, there is a wall with graffiti that reads: \"não é normal ter medo de andar sozinha na rua!!!\" This translates to \"it's not normal to be afraid of walking alone on the street!!!\" in English. The message highlights a social issue regarding safety and fear when walking alone."}
{"layout": 874, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 59, "bbox": [78, 592, 115, 602], "page_size": [612.0, 792.0]}
{"layout": 875, "type": "text", "text": "The scene text in the image are:\n\n - ΑΘΗΝΑΙ-ΡΩΜΗ-ΡΙΟΝ ΙΑΠΕΤΡΟΝ (Athens-Rome-Rion Iapetron)\n\n - 29 ΜΑΪΟΥ 1933 (29 May 1933)\n\n - ΔΡΑΧ (Drachma)\n\n - ΕΛΛΑΣ (Greece)\n\n ", "page_idx": 59, "bbox": [78, 604.2343139648438, 297.6677551269531, 648.6153564453125], "page_size": [612.0, 792.0]}
{"layout": 876, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 59, "bbox": [310, 639, 348, 649], "page_size": [612.0, 792.0]}
{"layout": 877, "type": "text", "text": "- ΣΠΑΕΡ. ΤΑΧΥΔΡ. ΣΥΓΚΟΙΝ. ΙΤΑΛΙΑ-ΕΛΛΑΔΟΣ-ΤΟΥΡΚΙΑ \n\n (Airmail. Express. Communication. Italy-Greece-Turkey)\n\n - 120 (Value of the stamp in drachmas) ", "page_idx": 59, "bbox": [78, 649.2365112304688, 276.1404113769531, 675.6166381835938], "page_size": [612.0, 792.0]}
{"layout": 878, "type": "text", "text": "The scene text in the image is \"não é normal ter medo de andar  sozinho na rua!!\" which is in Portuguese and translates to \"It is not  normal to be afraid to walk alone on the street!!\"  ", "page_idx": 59, "bbox": [311, 652.7080688476562, 532, 679.0881958007812], "page_size": [612.0, 792.0]}
{"layout": 879, "type": "text", "text": "Multilingual Text Recognition, Translation, and Description ", "text_level": 1, "page_idx": 60, "bbox": [77, 56, 348, 67], "page_size": [612.0, 792.0]}
{"layout": 880, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 60, "bbox": [78, 75, 115, 85], "page_size": [612.0, 792.0]}
{"layout": 881, "type": "text", "text": "If the image is presented in a language  other than English, translate the content  to English. ", "page_idx": 60, "bbox": [78, 86.60327911376953, 215, 115.68357849121094], "page_size": [612.0, 792.0]}
{"layout": 882, "type": "image", "page_idx": 60, "img_path": "layout_images/2309.17421v2_150.jpg", "bbox": [79, 122, 149, 190], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image contains Japanese text and a drawing of a smiling sun. The text says \"おはようございます,\" which means \"Good morning\" in English."}
{"layout": 883, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 60, "bbox": [78, 194, 115, 203], "page_size": [612.0, 792.0]}
{"layout": 884, "type": "text", "text": "The text in the image is in Japanese and  translates to \"Good morning, have a nice  day.\"  ", "page_idx": 60, "bbox": [78, 205.5375518798828, 215, 231.91773986816406], "page_size": [612.0, 792.0]}
{"layout": 885, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 60, "bbox": [77, 252, 115, 262], "page_size": [612.0, 792.0]}
{"layout": 886, "type": "text", "text": "If the image is presented in a language  other than English, translate the content  to English. ", "page_idx": 60, "bbox": [78, 263.3976135253906, 215, 292.4779052734375], "page_size": [612.0, 792.0]}
{"layout": 887, "type": "image", "page_idx": 60, "img_path": "layout_images/2309.17421v2_151.jpg", "bbox": [79, 297, 158, 388], "page_size": [612.0, 792.0], "ocr_text": "1° Grawp Cru Ciassé\n\n", "vlm_text": "The image shows a wine label for Château Angélus, a 1er Grand Cru Classé wine from Saint-Émilion. It indicates the vintage year as 1996. The label features a prominent bell design and includes the text \"Appellation St-Emilion Grand Cru Contrôlée.\""}
{"layout": 888, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 60, "bbox": [78, 394, 115, 404], "page_size": [612.0, 792.0]}
{"layout": 889, "type": "text", "text": "Translation: Angelus 1st Grand Cru  Classé Chateau Angelus 1996  Saint-Emilion Grand Cru Appellation  Saint-Emilion Grand Cru Controlled ", "page_idx": 60, "bbox": [78, 405.5401306152344, 200.6418914794922, 440.9207763671875], "page_size": [612.0, 792.0]}
{"layout": 890, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 60, "bbox": [235, 75, 273, 85], "page_size": [612.0, 792.0]}
{"layout": 891, "type": "text", "text": "If the image is presented in a language  other than English, translate the content  to English. ", "page_idx": 60, "bbox": [236, 86.69291687011719, 374, 115.7732162475586], "page_size": [612.0, 792.0]}
{"layout": 892, "type": "image", "page_idx": 60, "img_path": "layout_images/2309.17421v2_152.jpg", "bbox": [237, 120, 318, 190], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "This image shows a dish of mapo tofu. It is a popular Chinese dish made with tofu set in a spicy sauce, typically containing minced meat. The text on the image reads \"麻婆豆腐\" which translates to \"mapo tofu.\""}
{"layout": 893, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 60, "bbox": [235, 194, 273, 204], "page_size": [612.0, 792.0]}
{"layout": 894, "type": "text", "text": "The text in the image is in Chinese and  translates to \"Mapo Tofu.\" Mapo tofu is a  popular Chinese dish made with tofu and  minced meat in a spicy sauce.   ", "page_idx": 60, "bbox": [236, 205.62718200683594, 374, 241.0078125], "page_size": [612.0, 792.0]}
{"layout": 895, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 60, "bbox": [235, 252, 273, 262], "page_size": [612.0, 792.0]}
{"layout": 896, "type": "text", "text": "If the image is presented in a language  other than English, translate the content  to English. ", "page_idx": 60, "bbox": [236, 263.12115478515625, 374, 292.2014465332031], "page_size": [612.0, 792.0]}
{"layout": 897, "type": "image", "page_idx": 60, "img_path": "layout_images/2309.17421v2_153.jpg", "bbox": [238, 301, 328, 387], "page_size": [612.0, 792.0], "ocr_text": "Kuvia [muokkaa | muokkaa wikitekstia |\n\nStegosauruksen aivot sijaitsivat sen\npienessa paassa.\n\n", "vlm_text": "The image shows a fossil or cast of a dinosaur, specifically the head and part of the neck. The caption is in Finnish and reads \"Stegosauruksen aivot sijaitsivat sen pienessä päässä,\" which translates to \"The stegosaurus's brain was located in its small head.\" The text suggests the image might be highlighting the skull of a stegosaurus."}
{"layout": 898, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 60, "bbox": [235, 397, 273, 407], "page_size": [612.0, 792.0]}
{"layout": 899, "type": "text", "text": "English translation: \"Pictures [edit | edit  wikitext] Stegosaurus's brain was located  in its small head.\"  ", "page_idx": 60, "bbox": [236, 409.12103271484375, 374, 435.501220703125], "page_size": [612.0, 792.0]}
{"layout": 900, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 60, "bbox": [384, 77, 421, 86], "page_size": [612.0, 792.0]}
{"layout": 901, "type": "text", "text": "If the image is presented in a language other  than English, translate the content to  English. ", "page_idx": 60, "bbox": [384, 87.53132629394531, 533, 116.61162567138672], "page_size": [612.0, 792.0]}
{"layout": 902, "type": "image", "page_idx": 60, "img_path": "layout_images/2309.17421v2_154.jpg", "bbox": [385, 122, 526, 184], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image contains Korean text that appears to read \"존맛탱\" (jonmattaeng). This is a slang term in Korean that is an abbreviation, used to express that something is extremely delicious. The term combines \"존맛\" (jonmat), which is short for \"존나 맛있다\" (jonna masitda, meaning \"really delicious\"), and \"탱\" (taeng), an emphatic suffix. The background of the image seems to be blurred, and there is a glimpse of what looks like food on the right side of the image."}
{"layout": 903, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 60, "bbox": [384, 193, 421, 203], "page_size": [612.0, 792.0]}
{"layout": 904, "type": "text", "text": "The text in the image is in Korean and  translates to \"Delicious food\".  ", "page_idx": 60, "bbox": [384, 205.11553955078125, 513.167236328125, 222.4952850341797], "page_size": [612.0, 792.0]}
{"layout": 905, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 60, "bbox": [384, 252, 421, 261], "page_size": [612.0, 792.0]}
{"layout": 906, "type": "text", "text": "If the image is presented in a language other  than English, translate the content to  English. ", "page_idx": 60, "bbox": [384, 263.0594482421875, 533, 292.1397399902344], "page_size": [612.0, 792.0]}
{"layout": 907, "type": "image", "page_idx": 60, "img_path": "layout_images/2309.17421v2_155.jpg", "bbox": [386, 297, 533, 377], "page_size": [612.0, 792.0], "ocr_text": "Faga duplo toque para\nadicionar um titulo\n\naga di eave para accra om sub\n\n", "vlm_text": "The image shows a PowerPoint slide in editing mode. The slide contains placeholder text in Portuguese: \"Faça duplo toque para adicionar um título\" and \"Faça duplo toque para adicionar um subtítulo,\" which means \"Double tap to add a title\" and \"Double tap to add a subtitle.\" The interface around the slide is typical of Microsoft PowerPoint, with toolbars and slide navigation on the side."}
{"layout": 908, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 60, "bbox": [384, 395, 421, 405], "page_size": [612.0, 792.0]}
{"layout": 909, "type": "text", "text": "The content in the image is in Portuguese  and translates to \"Double tap to add a title.  Double tap to add a subtitle.\"  ", "page_idx": 60, "bbox": [384, 406.5520324707031, 528.3046264648438, 432.9322204589844], "page_size": [612.0, 792.0]}
{"layout": 910, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 60, "bbox": [78, 450, 115, 460], "page_size": [612.0, 792.0]}
{"layout": 911, "type": "text", "text": "Summarize the info in 20 languages ", "text_level": 1, "page_idx": 60, "bbox": [78, 461, 199, 470], "page_size": [612.0, 792.0]}
{"layout": 912, "type": "image", "page_idx": 60, "img_path": "layout_images/2309.17421v2_156.jpg", "bbox": [79, 474, 270, 597], "page_size": [612.0, 792.0], "ocr_text": "Alps Neozelandesos 3a estngies »\ni ioe = a es\n\n7 est ale cn str i sortase necedlanesn Vogue ares sgntcts asl Sus Sesrio)\n\n| 2) repeat arc 0 socks no cts fort © recount mde referéies per 8a seve verticabat.\n[+ Ape de Sudo Mendon (Sater Absent) Alps Nesmangess sro AS ES\nserene Sco ty ctxt a he tS Nv ac Cao\n‘bors ata ntl tp anti cn tacoma each\nev a ani pee Crary aes.\n‘ee Ap Nees War anneal rr ma pen\nderma ep cadre matey pts nomen po oe a 8\nports earl pu arr Gos orion ok gees\n{notin men erge nb SE warn Eben ever wep eran oe\nase ar 300 rte aon\net ps Nec\nsenkiinniCok TEA\n\n", "vlm_text": "This image appears to be a screenshot of a Wikipedia page in Catalan about the \"Alps Neozelandesos\" or the Southern Alps of New Zealand. It includes a brief description and information about the mountains and their heights. There is also an infobox on the right with an image of the mountains, geographic details, and administrative locations."}
{"layout": 913, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 60, "bbox": [78, 603, 117, 612], "page_size": [612.0, 792.0]}
{"layout": 914, "type": "text", "text": "1. English: The Southern Alps are a mountain range in  New Zealand.  2. Spanish: Los Alpes del Sur son una cadena  montañosa en Nueva Zelanda.  3. French: Les Alpes du Sud sont une chaîne de  montagnes en Nouvelle-Zélande.  ", "page_idx": 60, "bbox": [79, 614.2607421875, 261.6918029785156, 674.3925170898438], "page_size": [612.0, 792.0]}
{"layout": 915, "type": "text", "text": "GPT-4V output continued ", "text_level": 1, "page_idx": 60, "bbox": [279, 477, 366, 486], "page_size": [612.0, 792.0]}
{"layout": 916, "type": "text", "text": "4. German: Die Südalpen sind ein Gebirgszug in Neuseeland. \n\n 5. Italian: Le Alpi meri dion ali sono una catena montuosa in Nuova Zelanda. \n\n 6. Portuguese: Os Alpes do Sul são uma cord il heir a na Nova Zelândia. \n\n 7. Russian: Южные Альпы - горный хребет в Новой Зеландии. \n\n 8. Chinese: 南阿尔卑斯山脉是新西兰的一条山脉。 \n\n9. Japanese: 南アルプスはニュージーランドの山脈です。 \n\n10. Korean: 남부 알프스는 뉴질랜드의 산맥입니다. \n\n11. Hindi:  दͯक्षिणी एल्प्स न्यूजीलैंड का एक पवर्वत श्रृंखला है। \n\n 12.  Bengali:  দিক্ষিণ   অ্যালপস   িনউিজল্যােন্ডের   এই   অিধক   পবর্বতমালা ।  \n\n 13. Arabic: ﻧﯾوزﯾﻠﻧدا  $\\varsigma^{\\frac{\\dot{\\mathsf{a}}}{\\varepsilon}}$  اﻷﻟب اﻟﺟﻧوﺑﯾﺔ ھﻲ ﺳﻠﺳﻠﺔ ﺟﺑﺎل. \n\n 14. Turkish: Güney Alpler Yeni Zelanda'da bir dağ sil si les i dir. \n\n 15. Indonesian: Alpen Selatan adalah sebuah pegunungan di Selandia Baru. \n\n 16. Vietnamese: Dãy núi Alpes phía Nam là một dãy núi  $\\grave{\\circ}$   New Zealand. \n\n 17. Thai:  เทือกเขาแอลปภาคใตเปนเทือกเขาในนิวซีแลนด  \n\n 18.  Malay: Alpen Selatan adalah satu rangkaian gunung di New Zealand. \n\n 19. Tagalog: Ang Timog Alps ay isang hanay ng mga bundok sa New  Zealand.  20. Swahili: Milima ya Alpi Kusini ni safu ya milima nchini New Zealand.  ", "page_idx": 60, "bbox": [280, 487.8669738769531, 534, 672.2047119140625], "page_size": [612.0, 792.0]}
{"layout": 917, "type": "text", "text": "Figure 43: Results on multilingual text recognition, translation, and description. GPT-4V is able to recognize, translate and generate descriptions in different languages. Check Section  4.5  for detailed discussions. ", "page_idx": 60, "bbox": [108, 690.6245727539062, 503, 724.447509765625], "page_size": [612.0, 792.0]}
{"layout": 918, "type": "text", "text": "Sec. 4.5 Multilingual Multimodal Understanding ", "text_level": 1, "page_idx": 61, "bbox": [122, 38, 433, 57], "page_size": [612.0, 792.0]}
{"layout": 919, "type": "text", "text": "Multilingual Multicultural Understanding ", "text_level": 1, "page_idx": 61, "bbox": [116, 64, 303, 75], "page_size": [612.0, 792.0]}
{"layout": 920, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 61, "bbox": [117, 81, 155, 90.75], "page_size": [612.0, 792.0]}
{"layout": 921, "type": "text", "text": "Describe the image in both English and the local  language corresponding to the geo location depicted  in the image. ", "page_idx": 61, "bbox": [118, 91.95664978027344, 296, 120.92616271972656], "page_size": [612.0, 792.0]}
{"layout": 922, "type": "image", "page_idx": 61, "img_path": "layout_images/2309.17421v2_157.jpg", "bbox": [117, 121.25, 208, 255], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows a couple dressed in traditional Chinese wedding attire. They are seated in front of a backdrop decorated with large red roses and a golden \"double happiness\" symbol, which is commonly used in Chinese weddings to symbolize joy and marital happiness. The clothing and decorations indicate that this is likely a wedding or pre-wedding photo session."}
{"layout": 923, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 61, "bbox": [117, 257, 155, 266], "page_size": [612.0, 792.0]}
{"layout": 924, "type": "text", "text": "English: This image shows a couple dressed in  traditional Chinese wedding attire, sitting in front of a  wall of red roses and a gold Chinese character for  \"double happiness.\" ", "page_idx": 61, "bbox": [118, 268.9100036621094, 296, 308.19061279296875], "page_size": [612.0, 792.0]}
{"layout": 925, "type": "text", "text": "Chinese (Simplified):  这张图 片展示了一 对 穿着中国 传 统婚礼服装的夫妇，坐在红色玫瑰和金色“双喜”汉字前面。  (Zhè zhāng túpiàn zhǎnshìle yī duì chuānzhe  zhōngguó chuántǒng hūnlǐ fúzhuāng de fūfù, zuò zài  hóngsè méiguī hé jīnsè \"shuāng xǐ\" hànzì qiánmiàn.)  ", "page_idx": 61, "bbox": [118, 320.4653625488281, 296, 370.0570373535156], "page_size": [612.0, 792.0]}
{"layout": 926, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 61, "bbox": [117, 381, 155, 391], "page_size": [612.0, 792.0]}
{"layout": 927, "type": "text", "text": "Describe the image in both English and the local  language corresponding to the geo location depicted  in the image. ", "page_idx": 61, "bbox": [118, 392.2973327636719, 296, 421.266845703125], "page_size": [612.0, 792.0]}
{"layout": 928, "type": "image", "page_idx": 61, "img_path": "layout_images/2309.17421v2_158.jpg", "bbox": [118, 422, 233, 562], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows a couple in traditional Japanese wedding attire. The man is wearing a black kimono with a family crest and striped hakama (trousers), while the woman is dressed in a white shiromuku (wedding kimono) with a wataboshi (bridal headdress). They are standing in a setting that appears to be a traditional or historical location."}
{"layout": 929, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 61, "bbox": [118, 566, 155, 576], "page_size": [612.0, 792.0]}
{"layout": 930, "type": "text", "text": "English: This image shows a Japanese couple in  traditional wedding attire. The man is wearing a black  kimono with a white hakama and the woman is  wearing a white kimono with a large headpiece. ", "page_idx": 61, "bbox": [118, 578.216796875, 296, 613.462646484375], "page_size": [612.0, 792.0]}
{"layout": 931, "type": "text", "text": "Japanese: この画像は伝統的な結婚式の衣装を着た日本のカップルを示しています。男性は白い袴と黒い着物を着ており、女性は大きな頭飾りと白い着物を着ています。  (Kono gazō wa dentō-teki na kekkon-shiki no  ishō o kita Nihon no kappuru o s hime shite i masu.  Dansei wa shiroi hakama to kuroi kimono o kite ori,  josei wa ōkina kazari to shiroi kimono o kiteimasu.)  ", "page_idx": 61, "bbox": [118, 623.0474853515625, 296, 693.2613525390625], "page_size": [612.0, 792.0]}
{"layout": 932, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 61, "bbox": [316, 81, 354, 90], "page_size": [612.0, 792.0]}
{"layout": 933, "type": "text", "text": "Describe the image in both English and the local  language corresponding to the geo location depicted  in the image. ", "page_idx": 61, "bbox": [317, 91.95664978027344, 494, 120.92616271972656], "page_size": [612.0, 792.0]}
{"layout": 934, "type": "image", "page_idx": 61, "img_path": "layout_images/2309.17421v2_159.jpg", "bbox": [316, 120.25, 419, 273], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows two people wearing traditional Ethiopian attire. The clothing features intricate patterns with a mix of colors, including blue, white, and gold. The woman is wearing a long dress with an embroidered design, and the man is wearing a matching outfit with a similar pattern. They are standing outdoors with greenery in the background."}
{"layout": 935, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 61, "bbox": [317, 277, 354, 287], "page_size": [612.0, 792.0]}
{"layout": 936, "type": "text", "text": "English: The image shows a couple wearing  traditional Ethiopian clothing, standing in a garden  with a tree in the background. The woman is wearing  a white dress with colorful patterns and the man is  wearing a white suit with similar patterns. ", "page_idx": 61, "bbox": [317, 289.404052734375, 494, 338.9957275390625], "page_size": [612.0, 792.0]}
{"layout": 937, "type": "text", "text": "Amharic (Ethiopia's official language):  ይህ   ምስል   በኢትዮጵያዊ   ባህል   ልብስ   የለበሱት   ባለቤትና   ባለቤትን   በገባር   ይታያል።   ሴትዮቱ   በነጻነት   ያሉ   አለባለባህር   ልብሶች   ለበሳለ ች   እና ወንድዮቱም   በመሳሳህ   አለባለባህር   ሱት   ለበሳለ ች።   ", "page_idx": 61, "bbox": [317, 350.8326721191406, 494, 390.7626953125], "page_size": [612.0, 792.0]}
{"layout": 938, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 61, "bbox": [317, 408, 354, 417], "page_size": [612.0, 792.0]}
{"layout": 939, "type": "text", "text": "Describe the image in both English and the local  language corresponding to the geo location depicted  in the image. ", "page_idx": 61, "bbox": [317, 418.71331787109375, 494, 447.6828308105469], "page_size": [612.0, 792.0]}
{"layout": 940, "type": "image", "page_idx": 61, "img_path": "layout_images/2309.17421v2_160.jpg", "bbox": [317, 450, 486, 562], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows a couple dressed in traditional Indian wedding attire. The individual on the left is wearing a red bridal saree with gold embellishments and traditional jewelry, including bangles and necklaces. The individual on the right is dressed in a white sherwani with gold detailing, and both are adorned with floral garlands. They are seated together on a decorated platform, suggesting a wedding ceremony setting, with floral arrangements in the background."}
{"layout": 941, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 61, "bbox": [317, 566, 354, 575], "page_size": [612.0, 792.0]}
{"layout": 942, "type": "text", "text": "English: This image shows a newlywed Indian couple  sharing a kiss. The bride is wearing a traditional red  sari and the groom is wearing a red turban and white  suit. They are sitting on a decorated stage with  flowers. ", "page_idx": 61, "bbox": [317, 577.734375, 494, 627.3260498046875], "page_size": [612.0, 792.0]}
{"layout": 943, "type": "text", "text": "Hindi (local language in India):  यह छͪव एक  नवͪववाǑहत भारतीय  जोड़े को एक चुंबन शेयर करते हुए Ǒदखाती है। दुल्हन एक  परंपरागत लाल साड़ी पहन रही है और दूल्हा एक लाल  पगड़ी और सफ ेद सूट पहन रहा है। वे फ ूलों से सजे हुए  एक मंच पर बैठे हैं।  ", "page_idx": 61, "bbox": [317, 637.0907592773438, 494, 701.1963500976562], "page_size": [612.0, 792.0]}
{"layout": 944, "type": "text", "text": "Figure 44: Results on multilingual multi culture understanding. Check Section  4.5  for detailed discussions. ", "page_idx": 61, "bbox": [108, 715.8575439453125, 503.9973449707031, 738.771484375], "page_size": [612.0, 792.0]}
{"layout": 945, "type": "image", "page_idx": 62, "img_path": "layout_images/2309.17421v2_161.jpg", "img_caption": "Figure 45: GPT-4V’s capability to generate LaTex codes based on the hand-written input. The instruction is ‘generate latex code.’ for each case. The output is the LaTeX code and we show the rendered result. Although the model fails to write the code for the complex equation (bottom), we can break it down into several simple equations, which GPT-4V is able to handle. Check Section  4.6 for detailed discussions. ", "bbox": [107, 97, 506, 470], "page_size": [612.0, 792.0], "ocr_text": "Sec. 4.6 Coding Capability with Vision\n\nPrompt:\n\nGPT-4V:\n\nN b\n{(c £0) =ex5 ix (i+) +u f feuds\n\ni=1\n\n", "vlm_text": "The image showcases the capability of GPT-4V (a variant of GPT-4 with vision capabilities) to generate LaTeX code from handwritten mathematical equations. There are several examples depicted in the image:\n\n1. **First Row:**\n   - **Left:** The handwritten input is \\(a^2 + b^2 = c^2\\). GPT-4V converts this to LaTeX, which is rendered as \\(a^2 + b^2 = c^2\\).\n   - **Center:** The handwritten input is \\(\\sum_{i=0}^{N}(i^2 + i + 1)^{0.5}\\). GPT-4V renders this as \\(\\sum_{i=0}^{N}(i^2 + i + 1)^{0.5}\\).\n   - **Right:** The handwritten input is an integral \\(y = \\int f(x, y) \\, dx\\). GPT-4V renders this as \\(y = \\int f(x, y) \\, dx\\).\n\n2. **Second Row:**\n   - A complex handwritten equation is shown: \n     \\[\n     a^2 + b^2 = c^2 + \\sum_{i=1}^{N}(i^2 + i + 1)^{0.5} + y = \\int f(x, y) \\, dx\n     \\]\n   - GPT-4V generates a more simplified LaTeX code that does not exactly match the handwritten input:\n     \\[\n     \\left( x \\neq 6 \\right) = c \\times 5 \\sum_{i=1}^{N} \\left( i \\times (i + 1) \\right) + y_s \\int_{a}^{b} f(x, y) \\, dx\n     \\]\n\nThe image illustrates that while GPT-4V successfully translates simpler equations, it struggles with more complex combinations and equations, as noted in the caption, detailed in Section 4.6 of the document."}
{"layout": 946, "type": "text", "text": "Figure  45  illustrates the ability to generate LaTeX code based on handwritten mathematical equations. This functionality can assist users in writing equations in LaTeX more efficiently. Although the model is unable to generate code for longer equations, it can handle shorter equations effectively. By breaking down longer equations into shorter components, the model is able to generate the appropriate code. Figure  46  further demonstrates how GPT-4V can reconstruct a table in the input image into MarkDown/LaTex code. ", "page_idx": 62, "bbox": [107, 497.1755676269531, 505, 563.7255249023438], "page_size": [612.0, 792.0]}
{"layout": 947, "type": "text", "text": "Figure  47  shows examples of writing code in Python, TikZ, and SVG to replicate the input figure. Although the resulting output is not an exact match, the layout is similar and the code can be easily modified to meet specific needs. ", "page_idx": 62, "bbox": [107, 568.1095581054688, 505, 601.9324951171875], "page_size": [612.0, 792.0]}
{"layout": 948, "type": "text", "text": "Sec. 4.6 Coding Capability with Vision ", "text_level": 1, "page_idx": 63, "bbox": [131, 53, 367, 69], "page_size": [612.0, 792.0]}
{"layout": 949, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 63, "bbox": [129, 76, 167, 84.75], "page_size": [612.0, 792.0]}
{"layout": 950, "type": "text", "text": "Please read the table in this image and return a markdown-style reconstructed table in text.\n\n ", "page_idx": 63, "bbox": [127, 87.24903106689453, 431.3265686035156, 95.596435546875], "page_size": [612.0, 792.0]}
{"layout": 951, "type": "table", "page_idx": 63, "img_path": "layout_images/2309.17421v2_162.jpg", "bbox": [131, 104, 414, 206], "page_size": [612.0, 792.0], "ocr_text": "Se\n\n40.080 $15.971.880 $3.086.421\nChina 35.070 $15.866.670 $3.032.162\nAustralia 27.054 $14.812.566 $2.868.636\nIndia 23.046 $10.608.174 $1.853.710\n\nSouth Korea 16.032 $10.494.948 $1.975.844\n\n", "vlm_text": "The table presents data for five countries (USA, China, Australia, India, South Korea) across three categories:\n\n1. **Sales Volume**\n   - USA: 40.080\n   - China: 35.070\n   - Australia: 27.054\n   - India: 23.046\n   - South Korea: 16.032\n\n2. **Revenue**\n   - USA: $15,971,880\n   - China: $15,866,670\n   - Australia: $14,812,566\n   - India: $10,608,174\n   - South Korea: $10,494,948\n\n3. **Profit**\n   - USA: $3,086,421\n   - China: $3,032,162\n   - Australia: $2,868,636\n   - India: $1,853,710\n   - South Korea: $1,975,844"}
{"layout": 952, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 63, "bbox": [128, 212, 166, 222], "page_size": [612.0, 792.0]}
{"layout": 953, "type": "text", "text": "| Country | Sales Volume | Revenue | Profit |\n\n  $\\mid\\dots\\mid\\dots\\mid\\dots\\mid\\dots\\mid\n\n$  ", "page_idx": 63, "bbox": [127, 224.45391845703125, 276, 242], "page_size": [612.0, 792.0]}
{"layout": 954, "type": "text", "text": "| USA | 40.080 | \\$15.971.421 | \\$3.086.421 |\n\n | China | 35.070 | \\$15.866.670 | \\$3.032.162 |\n\n | Australi  ${\\mathsf{a}}\\mid27.054\\mid\\S14.812.566\\mid\\S2.868.636\\mid{\\mathsf{T}}$  |\n\n | India | 23.046 | \\$10.608.174 | \\$1.853.710 |\n\n | South Korea | 16.032 | \\$10.494.948 | \\$1.975.844 |  ", "page_idx": 63, "bbox": [127, 242.38623046875, 301.00543212890625, 286.5982666015625], "page_size": [612.0, 792.0]}
{"layout": 955, "type": "text", "text": "Visualization of Reconstructed Table (Markdown): ", "text_level": 1, "page_idx": 63, "bbox": [128, 293, 356, 304], "page_size": [612.0, 792.0]}
{"layout": 956, "type": "table", "page_idx": 63, "img_path": "layout_images/2309.17421v2_163.jpg", "bbox": [132, 307, 320, 407], "page_size": [612.0, 792.0], "ocr_text": "Country\nUSA\nChina\nAustralia\nIndia\n\nSouth Korea\n\nSales Volume\n\n40.080\n\n35.070\n\n27.054\n\n23.046\n\n16.032\n\nRevenue\n$15.971.421\n$15.866.670\n$14.812.566\n$10.608.174\n\n$10.494.948\n\nProfit\n$3.086.421\n$3.032.162\n$2.868.636\n$1.853.710\n\n$1.975.844\n", "vlm_text": "The table displays data for five countries with columns for Sales Volume, Revenue, and Profit:\n\n- **USA**: \n  - Sales Volume: 40.080\n  - Revenue: $15,971,421\n  - Profit: $3,086,421\n\n- **China**: \n  - Sales Volume: 35.070\n  - Revenue: $15,866,670\n  - Profit: $3,032,162\n\n- **Australia**: \n  - Sales Volume: 27.054\n  - Revenue: $14,812,566\n  - Profit: $2,868,636\n\n- **India**: \n  - Sales Volume: 23.046\n  - Revenue: $10,608,174\n  - Profit: $1,853,710\n\n- **South Korea**: \n  - Sales Volume: 16.032\n  - Revenue: $10,494,948\n  - Profit: $1,975,844"}
{"layout": 957, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 63, "bbox": [129, 425, 167, 434.75], "page_size": [612.0, 792.0]}
{"layout": 958, "type": "text", "text": "Please read the table in this image and return a latex-style reconstructed table in text. ", "page_idx": 63, "bbox": [127, 436.4737548828125, 412.6396484375, 444.8211364746094], "page_size": [612.0, 792.0]}
{"layout": 959, "type": "table", "page_idx": 63, "img_path": "layout_images/2309.17421v2_164.jpg", "bbox": [131, 452, 239, 492], "page_size": [612.0, 792.0], "ocr_text": "usa\nchine\nAustrato\n\n‘South Korea\n\n40000\n35070\n27054\n2308\n16032\n\n$15971800\n515985670\ns1asi2s6s\n$10 600.174\nst04neou8\n\nsoe.\nsie\ns20e0606\ns19s3710\n1975.84\n", "vlm_text": "The table presents data for five countries: USA, China, Australia, India, and South Korea. It includes four columns:\n\n1. **Country**: Lists the countries.\n2. **Sales Volume**: Indicates the sales volume for each country.\n3. **Revenue**: Provides the revenue figures.\n4. **Profit**: Shows the profit amounts.\n\nFor example, the USA has a sales volume of 40,080, revenue of $15,971,880, and profit of $3,086,421."}
{"layout": 960, "type": "text", "text": "Note: Same query image as shown above, resized to  save space ", "page_idx": 63, "bbox": [253, 460.4430236816406, 474, 479.26336669921875], "page_size": [612.0, 792.0]}
{"layout": 961, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 63, "bbox": [129, 499, 166, 509], "page_size": [612.0, 792.0]}
{"layout": 962, "type": "text", "text": "Here is the reconstructed table in latex: \\begin{tabular},{|c|c|c|c|} \\hline  Country & Sales Volume & Revenue & Profit \\\\ \\hline  USA & 40.080 & \\\\$15.971.887 & \\\\$3.068.421 \\\\ \\hline  China & 35.070 & \\\\$15.866.670 & \\\\$3.032.162 \\\\ \\hline  Australia & 27.054 & \\\\$14.812.566 & \\\\$2.868.636 \\\\ \\hline  India & 23.046 & \\\\$10.608.174 & \\\\$1.853.710 \\\\ \\hline South Korea & 16.032 & \\\\$10.494.948 & \\\\$1.975.844 \\\\ \\hline \\end{tabular}  ", "page_idx": 63, "bbox": [130, 511.4523010253906, 330.98565673828125, 591.5289306640625], "page_size": [612.0, 792.0]}
{"layout": 963, "type": "text", "text": "Visualization of Reconstructed Table (LaTex): ", "text_level": 1, "page_idx": 63, "bbox": [129, 598, 336, 608], "page_size": [612.0, 792.0]}
{"layout": 964, "type": "table", "page_idx": 63, "img_path": "layout_images/2309.17421v2_165.jpg", "bbox": [132, 610, 362, 675], "page_size": [612.0, 792.0], "ocr_text": "Country Sales Volume Revenue Profit\nUSA 40.080 $15.971.887 | $3.068.421\nChina 35.070 $15.866.670 | $3.032.162\nAustralia 27.054 $14.812.566 | $2.868.636\nIndia 23.046 $10.608.174 | $1.853.710\nSouth Korea 16.032 $10.494.948 | $1.975.844\n\n", "vlm_text": "The table contains data on five countries, showing their sales volume, revenue, and profit. Here's the breakdown:\n\n- **USA**\n  - Sales Volume: 40,080\n  - Revenue: $15,971,887\n  - Profit: $3,068,421\n\n- **China**\n  - Sales Volume: 35,070\n  - Revenue: $15,866,670\n  - Profit: $3,032,162\n\n- **Australia**\n  - Sales Volume: 27,054\n  - Revenue: $14,812,566\n  - Profit: $2,868,636\n\n- **India**\n  - Sales Volume: 23,046\n  - Revenue: $10,608,174\n  - Profit: $1,853,710\n\n- **South Korea**\n  - Sales Volume: 16,032\n  - Revenue: $10,494,948\n  - Profit: $1,975,844"}
{"layout": 965, "type": "text", "text": "Figure 46: GPT-4V’s capability to generate Markdown/LaTex codes to reconstruct a table in the image. Red highlights the errors in reconstruction. Check Section  4.6  for detailed discussions. ", "page_idx": 63, "bbox": [108, 688.862548828125, 503.9973449707031, 713.7644653320312], "page_size": [612.0, 792.0]}
{"layout": 966, "type": "image", "page_idx": 64, "img_path": "layout_images/2309.17421v2_166.jpg", "img_caption": "Figure 47: GPT-4V’s capability to write codes to replicate the input figure. We directly show the rendered figures by python/TikZ/SVG as GPT-4V’s response. The rendered figure is roughly aligned with the input figure, and the code can be easily adapted. GPT-4V Chart. Check Section  4.6  for detailed discussions. 65 ", "bbox": [106, 22, 506, 753], "page_size": [612.0, 792.0], "ocr_text": "Sec. 4.6 Coding Capability with Vision\n\nPrompt:\n\nGenerate python code to draw similar curves.\n\n—t— Base\n=< Large\n—@ Huge\n\n10°\n\nmillion images in pre-training\n\n10?\nmillion images in pre-training\n\nGenerate the following image in tikz.\n\nPrompt:\n\nWrite python codes to generate similar figures.\n\nME OSCARs\nEE MiniVLM (ours)\n\n100% o50,\n\nef A\nTR 1R\n\nVOA NLVR2\n\nComparison of OSCarB and MiniVLM\n\nmm OsCarB\n(= MiniVLM (ours)\n\nWrite svg codes to generate following image.\n\n", "vlm_text": "The image shows a series of visual prompts and corresponding outputs generated by GPT-4V, demonstrating its coding capabilities with vision. Here's a breakdown:\n\n1. **Top Left:** \n   - **Prompt:** Generate Python code to draw similar curves.\n   - **Input Figure:** A line graph with three lines labeled \"Base,\" \"Large,\" and \"Huge\" plotted against \"million images in pre-training.\"\n   - **GPT-4V's Output:** A similar line graph using the same labels.\n\n2. **Top Right:** \n   - **Prompt:** Write Python code to generate similar figures.\n   - **Input Figure:** A bar chart comparing scores for different tasks (Caption, VQA, TR, etc.).\n   - **GPT-4V's Output:** A similar bar chart with comparable data.\n\n3. **Bottom Left:** \n   - **Prompt:** Generate the following image in TikZ.\n   - **Input Figure:** An abstract shape with ellipses and lines.\n   - **GPT-4V's Output:** A similar abstract image, with variations in positioning and color.\n\n4. **Bottom Right:** \n   - **Prompt:** Write SVG code to generate the following image.\n   - **Input Figure:** An emblem-like design.\n   - **GPT-4V's Output:** A similar design with notable adjustments in structure.\n\nOverall, the image highlights GPT-4V’s ability to generate code that creates graphics and figures similar to given inputs."}
{"layout": 967, "type": "text", "text": "5 Interaction with Humans: Visual Referring Prompting ", "text_level": 1, "page_idx": 65, "bbox": [106, 71, 406, 84], "page_size": [612.0, 792.0]}
{"layout": 968, "type": "text", "text": "Pointing to a specific spatial location is an essential capability in human-computer interaction with multimodal systems, such as conducting visually grounded dialogues. As shown in Section  5.1 , GPT-4V can well understand the visual pointers directly drawn on images. Based on this observation, we propose a novel model interaction method named “visual referring prompting.” The core idea is to directly edit image pixel space to draw visual pointers or scene texts as human referring instructions, as highlighted in Figure  50 . We detail its usages and advantages in Section  5.2 . Finally, Section  5.3 explores having GPT-4V generate visual pointer outputs to interact with humans. These visual pointers are intuitive for both humans and machines to generate and understand, making them a good channel for human-computer interaction. ", "page_idx": 65, "bbox": [107, 95.87054443359375, 505, 195.1484832763672], "page_size": [612.0, 792.0]}
{"layout": 969, "type": "text", "text": "5.1 Understand Pointing Inputs ", "text_level": 1, "page_idx": 65, "bbox": [106, 208, 250, 219], "page_size": [612.0, 792.0]}
{"layout": 970, "type": "text", "text": "As illustrated in Figure  48 , GPT-4V can understand different types of visual markers directly overlaid on images as a pointer, such as circles, boxes, and hand drawings. This ability helps GPT-4V generate grounded captioning, which is a known challenging problem to have conventional vision-language models [ 128 ] generating visual descriptions focused on a specific area of interest. Dense captioning methods [ 62 ,  138 ] use cropped boxes or mask regions to generate localized descriptions, but often ignore the global image context and produce sub-optimal descriptions. Visual pointing provides a natural way to indicate the area of interest while maintaining the global image context. For example, the top left example focuses on providing a comprehensive description of the pointed Magna beer, while also mentioning the global image context that the beer bottle is on the table. ", "page_idx": 65, "bbox": [107, 227.5765380859375, 505, 326.8544921875], "page_size": [612.0, 792.0]}
{"layout": 971, "type": "text", "text": "An intuitive alternative to visual pointers overlaid on images is the region coordinates represented in the numerical text format. As shown in Figure  49 , GPT-4V can understand the coordinates out-of-box, achieving the ability of spatial referring via text tokens without extra box token finetuning as in prior vision-language models [ 129 ,  143 ]. Despite the promising capability, we note that our current prompt is less precise spatially. For example, in the top left example in Figure  49 , GPT-4V mentions the surrounding objects napkin and water bottle, even though only the beer bottle is in the region (0.47, 0.48, 0.55, 0.87). Overall, with respect to our experimented prompts, GPT-4V works more reliably when prompted with overlaid visual pointers, compared with text coordinates. This unique capability motivates us to explore a new prompting method, namely visual referring prompting. ", "page_idx": 65, "bbox": [107, 331.237548828125, 505, 430.5155029296875], "page_size": [612.0, 792.0]}
{"layout": 972, "type": "text", "text": "Sec. 5.1 Understand Pointing Inputs ", "text_level": 1, "page_idx": 66, "bbox": [134, 78, 345, 95], "page_size": [612.0, 792.0]}
{"layout": 973, "type": "text", "text": "Grounded Description  (overlaid visual pointers) ", "page_idx": 66, "bbox": [117.1484375, 99.91224670410156, 305.03375244140625, 109.59994506835938], "page_size": [612.0, 792.0]}
{"layout": 974, "type": "text", "text": "", "text_level": 1, "page_idx": 66, "bbox": [116, 115, 150, 120.75], "page_size": [612.0, 792.0]}
{"layout": 975, "type": "image", "page_idx": 66, "img_path": "layout_images/2309.17421v2_167.jpg", "img_caption": "Describe the pointed region in the image. ", "bbox": [115, 121.25, 253, 236], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The pointed region in the image shows a bottle of Magna beer placed on a table."}
{"layout": 976, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 66, "bbox": [117, 240, 150, 249], "page_size": [612.0, 792.0]}
{"layout": 977, "type": "text", "text": "The pointed region in the image is a green glass bottle of  Magna beer, with a red label and a gold cap. It is sitting on  a black table, surrounded by other items such as a water  bottle, napkins, and umbrellas. ", "page_idx": 66, "bbox": [117, 250.8065185546875, 295, 282.62164306640625], "page_size": [612.0, 792.0]}
{"layout": 978, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 66, "bbox": [116, 292, 150, 301], "page_size": [612.0, 792.0]}
{"layout": 979, "type": "image", "page_idx": 66, "img_path": "layout_images/2309.17421v2_168.jpg", "img_caption": "Describe the pointed region in the image. ", "bbox": [116, 302, 252, 414], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The pointed region in the image shows a neatly rolled white napkin placed on the edge of a table."}
{"layout": 980, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 66, "bbox": [116, 419, 150, 428], "page_size": [612.0, 792.0]}
{"layout": 981, "type": "text", "text": "The pointed region in the image contains a white folded  napkin and a silverware set wrapped in another white  napkin, placed on a black table. This is likely a table setting  for a meal at a restaurant. ", "page_idx": 66, "bbox": [117, 429.5549621582031, 295, 461.3700866699219], "page_size": [612.0, 792.0]}
{"layout": 982, "type": "image", "page_idx": 66, "img_path": "layout_images/2309.17421v2_169.jpg", "img_caption": "Prompt: Describe the two pointed objects in the image. ", "bbox": [115, 471, 253, 592], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows an outdoor seating area with tables and umbrellas by the seaside. Two pointed objects are highlighted with arrows. The object pointed by the red arrow is a green bottle with a label that reads \"MAGNA,\" which is positioned on a table. The object indicated by the yellow arrow is the top of a large outdoor umbrella, providing shade to the seating area."}
{"layout": 983, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 66, "bbox": [116, 598, 150, 607], "page_size": [612.0, 792.0]}
{"layout": 984, "type": "text", "text": "The first pointed object (indicated by the yellow arrow) is a  white patio umbrella. It has a pointed top and is attached to  a metal pole. The umbrella is open and providing shade for  the outdoor seating area. The second pointed object  (indicated by the red arrow) is a green glass bottle of  Magna beer. The bottle has a pointed neck and is capped  with a gold-colored metal lid. It is sitting on a black table in  front of a white napkin. ", "page_idx": 66, "bbox": [117, 608.76611328125, 295, 672.9547729492188], "page_size": [612.0, 792.0]}
{"layout": 985, "type": "image", "page_idx": 66, "img_path": "layout_images/2309.17421v2_170.jpg", "img_caption": "Prompt: Describe the pointed region in the image. ", "bbox": [319, 114, 457, 237], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The pointed region in the image is highlighting a bottle of Magna, which is a type of beer. The bottle is placed on a table next to a bottle of water and some napkins, in an outdoor setting by the beach."}
{"layout": 986, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 66, "bbox": [320, 245, 354, 253], "page_size": [612.0, 792.0]}
{"layout": 987, "type": "text", "text": "The pointed region in the image is a green beer bottle with  a red label that reads \"MAGNA\". The bottle is standing  upright on a black table with a white napkin next to it. ", "page_idx": 66, "bbox": [321, 255.43130493164062, 497, 279.15301513671875], "page_size": [612.0, 792.0]}
{"layout": 988, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 66, "bbox": [320, 292, 354, 300], "page_size": [612.0, 792.0]}
{"layout": 989, "type": "text", "text": "Describe the pointed region in the image. ", "text_level": 1, "page_idx": 66, "bbox": [320, 301, 442, 310], "page_size": [612.0, 792.0]}
{"layout": 990, "type": "image", "page_idx": 66, "img_path": "layout_images/2309.17421v2_171.jpg", "bbox": [321, 311, 456, 414], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows an outdoor dining area with a table set up for a meal. The table has a bottle of water, a bottle of beer, and napkins. In the background, there's a view of a beach or seaside location, with palm trees and a clear sky. There are also string lights hanging, and a staircase with a handrail is marked with a red circle. Umbrellas and more tables and chairs are visible, suggesting a relaxed, beachfront setting."}
{"layout": 991, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 66, "bbox": [320, 419, 355, 427], "page_size": [612.0, 792.0]}
{"layout": 992, "type": "text", "text": "The pointed region in the image is a section of a wooden  railing with vertical slats. It is located on the edge of a patio  overlooking the ocean. The railing appears to be painted  white and is partially obscured. ", "page_idx": 66, "bbox": [321, 429.5553283691406, 497, 461.37042236328125], "page_size": [612.0, 792.0]}
{"layout": 993, "type": "text", "text": "Prompt: Describe the pointed region in the image. ", "page_idx": 66, "bbox": [321, 470.1275634765625, 444.1009216308594, 487.9627685546875], "page_size": [612.0, 792.0]}
{"layout": 994, "type": "image", "page_idx": 66, "img_path": "layout_images/2309.17421v2_172.jpg", "bbox": [321, 489, 456, 592], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows an outdoor seating area likely located at a beachside or waterfront venue. There is a dark-colored table with a bottle of water, a bottle of Magna beer, and a napkin on it, with an open umbrella above the table. In the background, there are more tables and chairs, some with umbrellas. The area overlooks a body of water, possibly the ocean, with palm trees along the edge of the venue. Along the top portion of the image, string lights are visible, with a section of them highlighted by a yellow oval shape."}
{"layout": 995, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 66, "bbox": [320, 599, 354, 608], "page_size": [612.0, 792.0]}
{"layout": 996, "type": "text", "text": "The pointed region in the image is a row of hanging lights  on a wire. The lights are small and round, and they are  evenly spaced along the wire. The wire is stretched  between two poles and is located above a patio area with  tables and umbrellas. The lights appear to be decorative  and are likely used to provide ambient lighting in the  evening. ", "page_idx": 66, "bbox": [321, 609.92236328125, 497, 666.0176391601562], "page_size": [612.0, 792.0]}
{"layout": 997, "type": "text", "text": "Figure 48: GPT-4V understands visual pointers directly overlaid on images. Conducting grounded description with both local and global visual information is one unique application scenario. Check Section  5.1  for detailed discussions. ", "page_idx": 66, "bbox": [108, 684.62353515625, 504, 718.4464721679688], "page_size": [612.0, 792.0]}
{"layout": 998, "type": "text", "text": "Sec. 5.1 Understand Pointing Inputs ", "text_level": 1, "page_idx": 67, "bbox": [141, 46, 327, 62], "page_size": [612.0, 792.0]}
{"layout": 999, "type": "text", "text": "Grounded Description  (text-format region coordinates) ", "page_idx": 67, "bbox": [117.1484375, 68.128173828125, 331.9934387207031, 77.81587219238281], "page_size": [612.0, 792.0]}
{"layout": 1000, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 67, "bbox": [116, 83, 150, 92], "page_size": [612.0, 792.0]}
{"layout": 1001, "type": "text", "text": "Locate the region based on given coordinates and  describe the contents in the region. Regions are  represented by (x1,y1,x2,y2) coordinates. x1 x2 are the left  and right most positions, normalized into 0 to 1, where 0 is  the left and 1 is the right. y1 y2 are the top and bottom  most positions, normalized into 0 to 1, where 0 is the top  and 1 is the bottom. Describe the relative position of region  (0.47, 0.48, 0.55, 0.87) in the image. Describe the contents  in that relative position. No need to mention contents in  other part of the image. ", "page_idx": 67, "bbox": [117, 92.99671936035156, 294, 173.37217712402344], "page_size": [612.0, 792.0]}
{"layout": 1002, "type": "image", "page_idx": 67, "img_path": "layout_images/2309.17421v2_173.jpg", "bbox": [118, 176, 283, 292], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows an outdoor dining setup with a table, chairs, and umbrellas. On the table, there's a bottle of water and a bottle of Magna beer, along with rolled-up napkins. In the background, there's a view of the ocean, palm trees, and string lights hanging above. The setting suggests a relaxed, beachside or coastal location."}
{"layout": 1003, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 67, "bbox": [116, 297, 149, 304], "page_size": [612.0, 792.0]}
{"layout": 1004, "type": "text", "text": "The region (0.47, 0.48, 0.55, 0.87) is located in the  center-right part of the image. It covers a small area of the  table and extends to the bottom of the image. The most  prominent object in this region is a green bottle of Magna  beer. There is also a white napkin and a part of a water  bottle visible in this region. The background shows a part  of the patio and the ocean. ", "page_idx": 67, "bbox": [117, 306.8937072753906, 294, 362.9889221191406], "page_size": [612.0, 792.0]}
{"layout": 1005, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 67, "bbox": [116, 371, 150, 380], "page_size": [612.0, 792.0]}
{"layout": 1006, "type": "text", "text": "Locate the region based on given coordinates and  describe the contents in the region. Regions are  represented by (x1,y1,x2,y2) coordinates. x1 x2 are the left  and right most positions, normalized into 0 to 1, where 0 is  the left and 1 is the right. y1 y2 are the top and bottom  most positions, normalized into 0 to 1, where 0 is the top  and 1 is the bottom. Describe the relative position of region  (0.01, 0.67, 0.36, 0.91) in the image. Describe the contents  in that relative position. No need to mention contents in  other part of the image. ", "page_idx": 67, "bbox": [117, 381.4686584472656, 294, 461.8441162109375], "page_size": [612.0, 792.0]}
{"layout": 1007, "type": "image", "page_idx": 67, "img_path": "layout_images/2309.17421v2_174.jpg", "bbox": [118, 467, 281, 585], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows an outdoor patio setting near a beach or waterfront. There's a table with a bottle of Magna beer, a bottle of water, and white napkins. The area has white patio umbrellas and chairs, with string lights overhead and palm trees in the background. The scene suggests a relaxed, sunny environment."}
{"layout": 1008, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 67, "bbox": [116, 595, 149, 603], "page_size": [612.0, 792.0]}
{"layout": 1009, "type": "text", "text": "The region (0.01, 0.67, 0.36, 0.91) in the image is located  at the bottom left corner, covering approximately one third  of the width and one fourth of the height of the image. In  this region, there is a black table with a white napkin, a  bottle of Magna beer, and a glass bottle of water. The table  is in the foreground, while the background shows a part of  the patio with white umbrellas and chairs. ", "page_idx": 67, "bbox": [117, 605.771484375, 294, 661.8666381835938], "page_size": [612.0, 792.0]}
{"layout": 1010, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 67, "bbox": [319, 83, 352, 91], "page_size": [612.0, 792.0]}
{"layout": 1011, "type": "text", "text": "Locate the region based on given coordinates and  describe the contents in the region. Regions are  represented by (x1,y1,x2,y2) coordinates. x1  $\\times2$   are the left  and right most positions, normalized into 0 to 1, where 0 is  the left and 1 is the right. y1 y2 are the top and bottom  most positions, normalized into 0 to 1, where 0 is the top  and 1 is the bottom. Describe the relative position of region  (0.01, 0.09, 0.29, 0.21) in the image. Describe the contents  in that relative position. No need to mention contents in  other part of the image. ", "page_idx": 67, "bbox": [319, 92.99671936035156, 497, 173.37217712402344], "page_size": [612.0, 792.0]}
{"layout": 1012, "type": "image", "page_idx": 67, "img_path": "layout_images/2309.17421v2_175.jpg", "bbox": [307, 175, 476, 298], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows an outdoor dining setup with a table that has a bottle of sparkling water and a bottle of Magna beer on it. There are rolled napkins on the table. In the background, you can see string lights, some palm trees, a staircase leading down, and a view of the ocean under a clear sky."}
{"layout": 1013, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 67, "bbox": [319, 310, 353, 318], "page_size": [612.0, 792.0]}
{"layout": 1014, "type": "text", "text": "The region (0.01, 0.09, 0.29, 0.21) is located in the top left  corner of the image. It covers approximately  $28\\%$   of the  width and  $12\\%$   of the height of the image. In this region,  there is a part of a white umbrella and a string of lights.  The background shows a clear blue sky and a palm tree. ", "page_idx": 67, "bbox": [319, 320.7681579589844, 497, 360.67657470703125], "page_size": [612.0, 792.0]}
{"layout": 1015, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 67, "bbox": [319, 371, 352, 380], "page_size": [612.0, 792.0]}
{"layout": 1016, "type": "text", "text": "Locate the region based on given coordinates and  describe the contents in the region. Regions are  represented by (x1,y1,x2,y2) coordinates. x1  $\\times2$   are the left  and right most positions, normalized into 0 to 1, where 0 is  the left and 1 is the right. y1  $\\mathsf{y}2$   are the top and bottom  most positions, normalized into 0 to 1, where 0 is the top  and 1 is the bottom. Describe the relative position of region  (0.47, 0.48, 0.55, 0.87) in the image. Describe the contents  in that relative position. No need to mention contents in  other part of the image. ", "page_idx": 67, "bbox": [319, 381.4686279296875, 497, 461.8441162109375], "page_size": [612.0, 792.0]}
{"layout": 1017, "type": "image", "page_idx": 67, "img_path": "layout_images/2309.17421v2_176.jpg", "bbox": [311, 467, 475, 590], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows an outdoor dining area by the water. There is a table with a bottle of water, a beer bottle, and napkins. Umbrellas provide shade, and string lights are visible in the background. Palm trees and a view of the ocean complete the setting."}
{"layout": 1018, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 67, "bbox": [319, 595, 352, 603], "page_size": [612.0, 792.0]}
{"layout": 1019, "type": "text", "text": "The region (0.47, 0.48, 0.55, 0.87) is located towards the  right side of the image, slightly below the center. It covers  a small portion of the table and the background. In this  region, there is a green bottle of Magna beer on the table,  with a white napkin next to it. In the background, there is a  part of an umbrella and a few chairs. ", "page_idx": 67, "bbox": [319, 605.771484375, 497, 653.7732543945312], "page_size": [612.0, 792.0]}
{"layout": 1020, "type": "text", "text": "Figure 49: An alternative to visual pointers overlaid on images is the region coordinates represented in the numerical text format. GPT-4V can understand the coordinates,  e.g ., (0.47, 0.48, 0.55, 0.87), (0.01, 0.09, 0.29, 0.21), and (0.01, 0.67, 0.36, 0.91) that correspond to the center beer bottle, top-left string lights, and bottom-left table set, respectively. We observe that GPT-4V works less reliably when prompted with text coordinates, compared with visual pointers in visual referring prompting. Check Section  5.1  for detailed discussions. ", "page_idx": 67, "bbox": [107, 674.0365600585938, 505, 740.5875244140625], "page_size": [612.0, 792.0]}
{"layout": 1021, "type": "text", "text": "5.2 Visual Referring Prompting ", "text_level": 1, "page_idx": 68, "bbox": [106, 73, 250, 84], "page_size": [612.0, 792.0]}
{"layout": 1022, "type": "text", "text": "Inspired by GPT-4V’s strong capability in understanding visual pointing and scene text, we explore a new method to interact with GPT-4V, namely the  visual referring prompting . Instead of conventional prompting techniques that edit text space, visual referring prompting is a complementary technique that directly edits the pixel space for input images for human-computer interaction. Such visual prompting could offer a more nuanced and comprehensive interaction with the image, potentially unlocking a wider array of responses from the model. For example, in Figure  50  (1), GPT-4V naturally associates the arrow-pointed objects with the given object indexes, easing the remaining visual reasoning and text outputs; in (2), GPT-4V understands the questions written on the image and pointed to the corresponding edge or angle, providing a nuanced interface for grounded visual dialogue; in (3), humans can point to arbitrary regions inside the figure to help GPT-4V better understand complicated documents and charts; in (4), the pattern can be concisely represented as an arrow and the scene text   $\\\"dot\"$  , therefore helping GPT-4V to predict the next image. Complementary to text prompts that are loosely grounded to images, visual referring prompting provides a novel interaction method that could facilitate various use cases, with additional demonstrations in Figure  51 and Section  9 . ", "page_idx": 68, "bbox": [107, 92.7325439453125, 505, 257.4644775390625], "page_size": [612.0, 792.0]}
{"layout": 1023, "type": "text", "text": "5.3 Generate Pointing Outputs ", "text_level": 1, "page_idx": 68, "bbox": [106, 270, 246, 282], "page_size": [612.0, 792.0]}
{"layout": 1024, "type": "text", "text": "Section  5.1  discusses the ability of GPT-4V to understand visual pointing generated by humans. A natural question is: Can GPT-4V generate its own pointing outputs, thereby facilitating a closed-loop interaction process in human-computer interaction? ", "page_idx": 68, "bbox": [107, 289.8925476074219, 505, 323.7154846191406], "page_size": [612.0, 792.0]}
{"layout": 1025, "type": "text", "text": "Figure  52  explores generating visual pointing outputs by letting GPT-4V predict region coordinates in the text format. We prompt GPT-4V to ground the object referred by text ( e.g ., the text of “blue Subaru SUV”) or a reference image ( e.g ., the image of “black Audi sedan”). Similar to the observation in having GPT-4V comprehend coordinates input, the model has a coarse understanding of spatial locations, but it wasn’t accurate with respect to the prompts used in the experiment. For example, in Figure  52 ’s “plot visualization s,” GPT-4V can approximately identify the blue SUV and black sedan mentioned in the query, but it struggles to create a closely-fitted bounding box. We observe that including example-grounded instructions in the prompt helps GPT-4V to understand the definition of coordinates and subsequently generate better pointing outputs. ", "page_idx": 68, "bbox": [107, 328.09954833984375, 505, 427.37750244140625], "page_size": [612.0, 792.0]}
{"layout": 1026, "type": "text", "text": "While the generated pointing outputs may not perfectly cover the queried region, they still provide a valuable tool for model interaction, interpretation, and helping multi-step visual reasoning. Specifi- cally, the pointing outputs can be interpreted by humans to better understand GPT-4V’s references, or by GPT-4V itself to enable further reasoning based on previous outputs. As shown in the bottom of Figure  52 , GPT-4V is capable of interpreting the pointers it generates, and providing grounded descriptions with the prompts in Figure  48 . Furthermore, the iterative pointing generation and understanding by GPT-4V itself can help complicated multi-hop grounded visual reasoning tasks. GPT-4V is capable of de constructing the question, generating distinct visual markers to iterative ly focus on different image regions for each sub-step, ultimately collating the information to formulate the final answer. ", "page_idx": 68, "bbox": [107, 431.76055908203125, 505, 541.947509765625], "page_size": [612.0, 792.0]}
{"layout": 1027, "type": "text", "text": "Sec. 5.2 Visual Referring Prompting ", "text_level": 1, "page_idx": 69, "bbox": [147, 83, 327, 95], "page_size": [612.0, 792.0]}
{"layout": 1028, "type": "text", "text": "(1) Prompt: ", "text_level": 1, "page_idx": 69, "bbox": [116, 106, 163, 115], "page_size": [612.0, 792.0]}
{"layout": 1029, "type": "text", "text": "What is in the circled glass? Object 1 or object 2? Describe what is Object 1 and object 2. Then check what  is in the circled glass. ", "page_idx": 69, "bbox": [117, 116.53103637695312, 294, 140.22415161132812], "page_size": [612.0, 792.0]}
{"layout": 1030, "type": "image", "page_idx": 69, "img_path": "layout_images/2309.17421v2_177.jpg", "bbox": [116, 141, 267, 253], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows an outdoor dining setup with a table. Object 1, marked with a green arrow, is a bottle of water. Object 2, marked with a red arrow, is a glass with a napkin next to it. There is also a bottle of Magners cider on the table. The background features a view of palm trees, the ocean, and outdoor seating with umbrellas."}
{"layout": 1031, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 69, "bbox": [117, 255, 149, 263], "page_size": [612.0, 792.0]}
{"layout": 1032, "type": "text", "text": "Object 1 is a glass bottle of water. It is clear and has a blue  label on it. Object 2 is a glass bottle of Magna beer. It is green and  has a red and white label on it. In the circled glass, it appears to be a clear liquid, likely  water. So, it is likely that the contents of the circled glass  are from Object 1, the glass bottle of water. ", "page_idx": 69, "bbox": [117, 265.50128173828125, 294, 321.5289611816406], "page_size": [612.0, 792.0]}
{"layout": 1033, "type": "text", "text": "(3) Prompt: ", "text_level": 1, "page_idx": 69, "bbox": [116, 360, 163, 369], "page_size": [612.0, 792.0]}
{"layout": 1034, "type": "text", "text": "Which circled line shows a different comparison trend?  Provide intermediate steps. ", "page_idx": 69, "bbox": [117, 370.6891784667969, 294, 386.2986145019531], "page_size": [612.0, 792.0]}
{"layout": 1035, "type": "table", "page_idx": 69, "img_path": "layout_images/2309.17421v2_178.jpg", "bbox": [117, 392, 313, 493], "page_size": [612.0, 792.0], "ocr_text": "Exam cpr GPT-4 (no vision)\nUniform Bar Exam (MBE+MEE+MPT) 298/400 (-90th) 298 / 400 (~90th)\n\n163 (-88th) 161 (-83rd)\nSAT Evidence-Based Reading & Writing 710/800(-93rd) 710/800 (~93rd)\n700 / 800 (-89th) 690/800 (~89th)\nGraduate Record Examination (GRE) Quantitative | 163/170(-80th) ‘157/170 (~62nd)\nGraduate Record Examination (GRE) Verbal | 169/170 (~99th) 165/170 (~96th)\n\nGraduate Record Examination (GRE) Writing 416 (-S4th) 416 (~54th)\n\nUSABO Semifinal Exam 2020 87/150 (99th 100th) 87/150 (99th - 10014)\nMedical Knowledge Self-Assessment Program 5% 5%\n\nCodeforces Rating 392 (below Sth) 392 (below Sth)\n", "vlm_text": "The table provides a comparison of GPT-4's performance on various exams, with and without vision capabilities. It includes scores and percentiles for exams such as the LSAT, SAT Math, GRE, USABO Semifinal Exam, and others. The highlighted sections are the LSAT (pink), SAT Math (green), and the USNCO Local Section Exam 2022 (blue)."}
{"layout": 1036, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 69, "bbox": [116, 501, 149, 509], "page_size": [612.0, 792.0]}
{"layout": 1037, "type": "text", "text": "The circled line that shows a different comparison trend is  the USNCO Local Section Exam 2022. Intermediate steps:  1. Analyze the data in the table and look for any  discrepancies in the comparison trends. 2. Notice that in  all the rows except for the USNCO Local Section Exam  2022, the GPT-4 (no vision) column has a lower score than  the GPT-4 column. 3. This means that in all the other rows,  the GPT-4 model with vision performed better than the  GPT-4 model without vision. 4. However, in the USNCO  Local Section Exam 2022 row, the GPT-4 (no vision)  column has a higher score than the GPT-4 column. 5. This  is a different comparison trend than the rest of the rows,  indicating that the GPT-4 model without vision performed  better than the GPT-4 model with vision in this particular  exam. ", "page_idx": 69, "bbox": [117, 511.5758056640625, 294, 632.2726440429688], "page_size": [612.0, 792.0]}
{"layout": 1038, "type": "text", "text": "(2) Prompt: ", "text_level": 1, "page_idx": 69, "bbox": [318, 106, 364, 115], "page_size": [612.0, 792.0]}
{"layout": 1039, "type": "image", "page_idx": 69, "img_path": "layout_images/2309.17421v2_179.jpg", "img_caption": "Answer the question in the following figure. Provide  intermediate steps. ", "bbox": [318, 116, 496, 220], "page_size": [612.0, 792.0], "ocr_text": "What ig the angle here?\n", "vlm_text": "To solve the problems in the image:\n\n1. **How long is this edge? (Hypotenuse)**\n\n   The triangle shown is a right triangle with legs of lengths 4 and 8. We can find the hypotenuse using the Pythagorean theorem:\n\n   \\[\n   c = \\sqrt{a^2 + b^2}\n   \\]\n\n   Substituting the given values:\n\n   \\[\n   c = \\sqrt{4^2 + 8^2} = \\sqrt{16 + 64} = \\sqrt{80} = \\sqrt{16 \\times 5} = 4\\sqrt{5}\n   \\]\n\n2. **What is the angle here?**\n\n   To find the angle adjacent to the side of length 8, we use the tangent function, which is the ratio of the opposite side to the adjacent side:\n\n   \\[\n   \\tan(\\theta) = \\frac{\\text{opposite}}{\\text{adjacent}} = \\frac{4}{8} = \\frac{1}{2}\n   \\]\n\n   Solve for the angle using the arctangent function:\n\n   \\[\n   \\theta = \\tan^{-1}\\left(\\frac{1}{2}\\right)\n   \\]\n\n   You can find \\(\\theta\\) using a calculator:\n\n   \\[\n   \\theta \\approx 26.57^\\circ\n   \\]\n\nBy these steps, we find the hypotenuse is \\(4\\sqrt{5}\\) and the angle is approximately \\(26.57^\\circ\\)."}
{"layout": 1040, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 69, "bbox": [318, 228, 351, 237], "page_size": [612.0, 792.0]}
{"layout": 1041, "type": "text", "text": "To find the length of the blue arrowed edge, we can use  the Pythagorean theorem since we have a right triangle.  The theorem states that  $\\mathsf{a}^{2}+\\mathsf{b}^{2}=\\mathsf{c}^{2}$  , where a and b are the  two legs of the right triangle and c is the hypotenuse.  Plugging in the values we have, we get:  $\\begin{array}{r l r}{4^{2}+8^{2}=\\mathsf{c}^{2}}&{{}\\,16+64=\\mathsf{c}^{2}}&{80=\\mathsf{c}^{2}}&{{}\\,\\sqrt{80}=\\mathsf{c}\\quad8.94\\approx\\mathsf{c}}\\end{array}$  Therefore, the length of the blue arrowed edge is  approximately 8.94. To find the angle at the green arrow, we can use the  tangent function since we have the opposite and adjacent  sides of the angle. The tangent function states that tanθ  $=$    opposite/adjacent. Plugging in the values we have, we get:  $\\mathsf{t a n}\\Theta=4/8$      $\\begin{array}{r l r}{{\\mathsf{t a n}}\\Theta=1/2}&{{}\\Theta={\\mathsf{t a n}}^{\\wedge}\\!-1(1/2)}&{\\Theta\\approx26.57^{\\circ}}\\end{array}$  Therefore, the angle at the green arrow is approximately   $26.57^{\\circ}$  . ", "page_idx": 69, "bbox": [318, 238.940673828125, 496, 359.6376037597656], "page_size": [612.0, 792.0]}
{"layout": 1042, "type": "text", "text": "(4) Prompt: ", "text_level": 1, "page_idx": 69, "bbox": [318, 375, 364, 383], "page_size": [612.0, 792.0]}
{"layout": 1043, "type": "text", "text": "Answer questions in the following figure. Provide  intermediate steps. ", "page_idx": 69, "bbox": [318, 385.12420654296875, 466, 400.733642578125], "page_size": [612.0, 792.0]}
{"layout": 1044, "type": "image", "page_idx": 69, "img_path": "layout_images/2309.17421v2_180.jpg", "bbox": [319, 402, 468, 536], "page_size": [612.0, 792.0], "ocr_text": "What would be\nthe next image?\n\n", "vlm_text": "The image presents a pattern sequence puzzle consisting of two primary shapes, one of which has been modified, and a question mark indicating the next shape. The first shape is a diamond without a dot. The second image shows a star shape beside it. Below the diamond shape is another diamond but with a dot inside, suggesting the modification of the former shape by adding a dot. There's a question mark adjacent to the star shape, suggesting the viewer complete the pattern. Following the pattern established, the next image would logically be a star with a dot inside it."}
{"layout": 1045, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 69, "bbox": [318, 542, 352, 551], "page_size": [612.0, 792.0]}
{"layout": 1046, "type": "text", "text": "The next image in the sequence would be a star with a dot  in the center. This is because the sequence appears to be  following a pattern where the shape alternates between a  diamond and a star, and a dot is added to the center of the  shape every other image. Intermediate steps: - First, we can observe that the sequence alternates  between a diamond and a star shape. - Next, we can see that a dot is added to the center of the  shape every other image. - Based on this pattern, we can predict that the next image  will be a star with a dot in the center. ", "page_idx": 69, "bbox": [318, 552.5714111328125, 496, 640.9336547851562], "page_size": [612.0, 792.0]}
{"layout": 1047, "type": "text", "text": "Figure 50: Visual referring prompting directly edits the input image as input prompts, such as drawing visual pointers and scene texts. Complementary to text prompts, visual referring prompting provides a more nuanced and natural interaction,  e.g ., (1) associating pointed objects with an index, (2) pointing to the image for questioning, (3) highlighting lines in documents and tables, (4) drawing the pattern on the image, and many other novel use cases. Check Section  5.2  for detailed discussions. ", "page_idx": 69, "bbox": [107, 656.4595336914062, 504, 712.1015014648438], "page_size": [612.0, 792.0]}
{"layout": 1048, "type": "text", "text": "Sec. 5.2 Visual Referring Prompting ", "text_level": 1, "page_idx": 70, "bbox": [145, 94, 325, 107], "page_size": [612.0, 792.0]}
{"layout": 1049, "type": "image", "page_idx": 70, "img_path": "layout_images/2309.17421v2_181.jpg", "img_caption": "Prompt: What is the icon used for? ", "bbox": [113, 118, 304, 270], "page_size": [612.0, 792.0], "ocr_text": "Welcome to the new Bing\noir Arona cs ha\n\n© Ask complex questions “+ Get better answers “© Get creative inspiration\n\n“at ar ome meals can make army\nPicky toler ha ony eat orange\n\n“what we te pros andconefthetop3_-Witea ha about crocodes outer\nseling pet vocusre™ space nthe yl of a rate\n\nLets lea ogee ngs powered by I that can understand and generate text an ages so sures and\n", "vlm_text": "The icon highlighted in the image is likely used to enable voice input, allowing users to speak their queries instead of typing them."}
{"layout": 1050, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 70, "bbox": [116, 278, 149, 287], "page_size": [612.0, 792.0]}
{"layout": 1051, "type": "text", "text": "The icon circled in the image is a microphone icon. It is  used to activate the voice search feature on Bing. Users  can click on this icon and speak into their device's  microphone to ask a question or search for something  instead of typing it in. This can be helpful for hands-free  searching or for users who may have difficulty typing.  ", "page_idx": 70, "bbox": [117, 288.9803771972656, 291, 336.2906799316406], "page_size": [612.0, 792.0]}
{"layout": 1052, "type": "text", "text": "Prompt: What does this part of the poster highlights? ", "page_idx": 70, "bbox": [117, 350.33270263671875, 246.6075439453125, 367.9109802246094], "page_size": [612.0, 792.0]}
{"layout": 1053, "type": "image", "page_idx": 70, "img_path": "layout_images/2309.17421v2_182.jpg", "bbox": [113, 380, 304, 473], "page_size": [612.0, 792.0], "ocr_text": "i ReCo: Region-Controlled Text-to-Image Generation oan\n\n", "vlm_text": "This image is a research poster titled \"ReCo: Region-Controlled Text-to-Image Generation.\" It includes sections on:\n\n1. **Text in T2I Generation**: Discusses image-level and region-level grounded descriptions to enhance text-to-image generation.\n\n2. **ReCo Framework**: Explains the method with features like flexible box tokens, seamless integration of these tokens with text, and improvements in semantic coherence and region controllability.\n\n3. **Experiment Results**: Shows comparisons and results with datasets like COCO, illustrating improvements in image generation quality.\n\n4. **Key Takeaways**: Summarizes the benefits of region-controlling techniques, spatial controllability, and improved semantic coherence and quality.\n\n5. **Grounded Descriptions**: Highlights the importance of regional and grounded text descriptions for better image relevance and detail.\n\nThe poster includes visuals, graphs, and tables to support these concepts."}
{"layout": 1054, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 70, "bbox": [116, 487, 149, 495], "page_size": [612.0, 792.0]}
{"layout": 1055, "type": "text", "text": "This part of the poster highlights the ReCo Framework,  which is the main contribution of the research. It explains  how the framework works and what it does. The ReCo  Framework is a region-controlled text-to-image generation  model that can generate high-quality images based on text  descriptions. It also shows the different components of the  framework and how they interact with each other.  The ReCo Framework is designed to be flexible and easy  to extend, and it has been shown to improve image  generation quality and achieve better semantic coherence  compared to other text-to-image generation methods.  ", "page_idx": 70, "bbox": [117, 497.2299499511719, 291, 584.42431640625], "page_size": [612.0, 792.0]}
{"layout": 1056, "type": "text", "text": "Prompt: What does the pointed news talk about? ", "page_idx": 70, "bbox": [308, 117.01313018798828, 426.4226989746094, 134.59141540527344], "page_size": [612.0, 792.0]}
{"layout": 1057, "type": "image", "page_idx": 70, "img_path": "layout_images/2309.17421v2_183.jpg", "bbox": [309, 142, 496, 295], "page_size": [612.0, 792.0], "ocr_text": "Dispenibits ftée de Phone\n15 laquelle serait in mins\nfrustrate?\n\n&\n= UBISOF?\n\noverran\nLacqustion gactvision\nBizzard por Microsot est de\n‘bon augure pou Vbisot.\n\n(Meneses trwupmae Nectecioge Okemo Desens Seance seme\n\nom\n\ncecege arena ent cn\n\n", "vlm_text": "The image shows a French webpage highlighting tech news articles. Topics include the release of the iPhone 15, the acquisition of Activision Blizzard by Microsoft, and updates on ChatGPT by OpenAI. The section related to ChatGPT discusses new personalized instructions to improve preferences and behavior."}
{"layout": 1058, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 70, "bbox": [308, 301, 340, 309], "page_size": [612.0, 792.0]}
{"layout": 1059, "type": "text", "text": "The pointed news is talking about the new feature of ChatGPT,  which allows users to create personalized instructions. This  feature is said to help users better manage their conversations  and improve their experience with the chatbot.  ", "page_idx": 70, "bbox": [308, 311.7712707519531, 498, 343.1279602050781], "page_size": [612.0, 792.0]}
{"layout": 1060, "type": "text", "text": "Prompt: What are the differences between these two experiments in the  slides? ", "page_idx": 70, "bbox": [308, 350.33270263671875, 498, 375.8877868652344], "page_size": [612.0, 792.0]}
{"layout": 1061, "type": "image", "page_idx": 70, "img_path": "layout_images/2309.17421v2_184.jpg", "bbox": [310, 380, 498, 485], "page_size": [612.0, 792.0], "ocr_text": "UniTAB: A Single Model for Different Tasks\n\n+ visual grounding eels\n\n+ Refcoco/+/g, Flickr30k Entities 0)\":\n\nText, box, alignment\nGrounded captioning\nFlickr30k Entities\n\n*{ Text =\n+ \\umage captioning, VOA ewer\n\n* MSCOCO, VaAv2 te\n\n", "vlm_text": "The image is a slide titled \"UniTAB: A Single Model for Different Tasks,\" showing how the UniTAB model can perform various tasks involving box, text, and alignment. \n\n1. **Box, alignment** tasks include:\n   - Visual grounding\n   - Datasets like Refcoco/+/g and Flickr30k Entities\n   - Shows a comparison table and example image with box annotations.\n\n2. **Text, box, alignment** tasks include:\n   - Grounded captioning\n   - Flickr30k Entities\n   - Shows a comparison table and example image with both text and box annotations.\n\n3. **Text** tasks include:\n   - Image captioning and VQA (Visual Question Answering)\n   - Datasets like MSCOCO and VQAv2\n   - Shows a comparison table and example image with text annotations.\n\nThe slide includes performance tables comparing the UniTAB model to other methods and visual examples of the model’s outputs."}
{"layout": 1062, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 70, "bbox": [308, 487, 340, 494], "page_size": [612.0, 792.0]}
{"layout": 1063, "type": "text", "text": "There are a few key differences between the two experiments  shown in the slides: 1. The tasks: The first experiment (on the left) is focused on the  task of visual grounding, where the model is trained to identify  and localize objects in an image. The second experiment (on the  right) is focused on the task of image captioning, where the  model is trained to generate a descriptive caption for an image. 2. The datasets: The first experiment uses the Flickr30k Entities  dataset, which contains images with annotated object bounding  boxes. The second experiment uses the MSCOCO and VQA2  datasets, which contain images with associated captions. 3. The evaluation metrics: The first experiment uses the mean  Average Precision metric to evaluate the model's performance  on the visual grounding task. The second experiment uses the  BLEU and CIDEr metrics to evaluate the model's performance  on the image captioning task. Overall, the two experiments are designed to evaluate the  UniTAB model's performance on different tasks and datasets,  and to demonstrate its ability to handle multiple tasks with a  single model.  ", "page_idx": 70, "bbox": [308, 497.2299499511719, 498, 656.2154541015625], "page_size": [612.0, 792.0]}
{"layout": 1064, "type": "text", "text": "Figure 51: Visual referring prompts enhance the seamless interaction between humans and computers. This is evident in the integration with computer and mobile Graphical User Interfaces (GUIs), and the support provided in understanding documents and slides. Check Section  5.2  for detailed discussions. ", "page_idx": 70, "bbox": [107, 667.1505737304688, 505, 700.9735107421875], "page_size": [612.0, 792.0]}
{"layout": 1065, "type": "text", "text": "Sec. 5.3 Generate Pointing Outputs ", "text_level": 1, "page_idx": 71, "bbox": [141, 33, 323, 46], "page_size": [612.0, 792.0]}
{"layout": 1066, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 71, "bbox": [115, 53, 149, 62], "page_size": [612.0, 792.0]}
{"layout": 1067, "type": "text", "text": "Locate the referred object and represent the location of  the region. Regions are represented by (x1,y1,x2,y2)  coordinates. x1 x2 are the left and right most positions,  normalized into 0 to 1, where 0 is the left and 1 is the right.  y1 y2 are the top and bottom most positions, normalized  into 0 to 1, where 0 is the top and 1 is the bottom. For  example, the  $(\\mathsf{x}1,\\mathsf{y}1,\\mathsf{x}2,\\mathsf{y}2)$  ) coordinates for that region with  the beer bottle is (0.47, 0.48, 0.55, 0.87). ", "page_idx": 71, "bbox": [117, 63.071922302246094, 290, 126.7196273803711], "page_size": [612.0, 792.0]}
{"layout": 1068, "type": "image", "page_idx": 71, "img_path": "layout_images/2309.17421v2_185.jpg", "bbox": [116, 130, 266, 241], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows an outdoor patio area with a table. On the table, there is a bottle of Magners cider and a bottle of water, along with napkins. In the background, there are umbrellas, tables, chairs, and a view of the ocean with palm trees and string lights."}
{"layout": 1069, "type": "text", "text": "For the image below, what is the   $(\\mathsf{x}1,\\mathsf{y}1,\\mathsf{x}2,\\mathsf{y}2)$   coordinates  for the region with the blue Subaru SUV? ", "page_idx": 71, "bbox": [117, 247, 290, 263.14794921875], "page_size": [612.0, 792.0]}
{"layout": 1070, "type": "image", "page_idx": 71, "img_path": "layout_images/2309.17421v2_186.jpg", "bbox": [117, 266, 248, 367], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows a large mural painted on the side of a building. The mural depicts a scenic coastal landscape with a view of the ocean, coastal town, forested hills, and a lighthouse. Several ships and boats are visible in the water, and a few birds are flying in the sky. In front of the mural, several cars are parked in a parking lot. The building and parking lot suggest an urban or semi-urban area."}
{"layout": 1071, "type": "text", "text": "GPT-4V: The (x1,y1,x2,y2) coordinates for the region with the blue  Subaru SUV would be (0.68, 0.57, 0.95, 0.95). ", "page_idx": 71, "bbox": [117, 376.158935546875, 290, 401.8690185546875], "page_size": [612.0, 792.0]}
{"layout": 1072, "type": "text", "text": "Plot Visualization: Refer to the query image below ", "page_idx": 71, "bbox": [117, 410.5526123046875, 238, 428.6326599121094], "page_size": [612.0, 792.0]}
{"layout": 1073, "type": "text", "text": "Grounded Description ", "text_level": 1, "page_idx": 71, "bbox": [116, 446, 208, 456], "page_size": [612.0, 792.0]}
{"layout": 1074, "type": "text", "text": "Prompt: Describe the pointed region in the image. ", "page_idx": 71, "bbox": [117, 456.4109802246094, 238, 474.0959167480469], "page_size": [612.0, 792.0]}
{"layout": 1075, "type": "image", "page_idx": 71, "img_path": "layout_images/2309.17421v2_187.jpg", "bbox": [116, 480, 249, 582], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows several cars parked in front of a large mural on the side of a building. The mural depicts a scenic landscape with water, trees, and mountains. One of the cars is highlighted with a red box."}
{"layout": 1076, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 71, "bbox": [116, 587, 150, 595], "page_size": [612.0, 792.0]}
{"layout": 1077, "type": "text", "text": "The pointed region in red is a part of a larger mural on the  side of a building. The content in this specific region  appears to be a depiction of a blue car, possibly an SUV,  parked in a parking lot. The car is facing towards the right  side of the image and appears to be in a parking spot with  a white line on the left side. ", "page_idx": 71, "bbox": [117, 597.3207397460938, 290, 644.9180908203125], "page_size": [612.0, 792.0]}
{"layout": 1078, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 71, "bbox": [303, 53, 336, 61], "page_size": [612.0, 792.0]}
{"layout": 1079, "type": "text", "text": "Locate the referred object and represent the location of the  region. Regions are represented by (x1,y1,x2,y2) coordinates.   $\\times1\\times2$   are the left and right most positions, normalized into 0 to  1, where 0 is the left and 1 is the right. y1 y2 are the top and  bottom most positions, normalized into 0 to 1, where 0 is the top  and 1 is the bottom. For example, the (x1,y1,x2,y2) coordinates  for that region with the beer bottle is (0.47, 0.48, 0.55, 0.87). ", "page_idx": 71, "bbox": [302, 63.071922302246094, 494, 118.6944351196289], "page_size": [612.0, 792.0]}
{"layout": 1080, "type": "image", "page_idx": 71, "img_path": "layout_images/2309.17421v2_188.jpg", "img_caption": "For the image below, what is the (x1,y1,x2,y2) coordinates for  the region with object ", "bbox": [302, 122, 486, 251], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows an outdoor dining area with a table in the foreground. On the table, there is a bottle of Magna beer, a napkin, and a bottle of water. In the background, there are tables and chairs with umbrellas, some palm trees, and a view of the sea. The coordinates (x1, y1, x2, y2) for the region with the Magna beer bottle, as indicated by the blue markings, are approximately (0.47, 0.55, 0.87, 0.48)."}
{"layout": 1081, "type": "image", "page_idx": 71, "img_path": "layout_images/2309.17421v2_189.jpg", "img_caption": "in it? ", "bbox": [303, 253, 363, 298], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows a black Audi car on a dirt path."}
{"layout": 1082, "type": "image", "page_idx": 71, "img_path": "layout_images/2309.17421v2_190.jpg", "bbox": [304, 299, 437, 396], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image features a large mural painted on the side of a building. The mural depicts a coastal scene with a body of water, boats, land, and some greenery, possibly representing a harbor or bay area. In front of the mural, several cars are parked in a parking lot. The mural seems to be in an urban setting, with another building visible in the background. A sign with text is affixed to the building to the right of the mural, although the text is not legible in this image."}
{"layout": 1083, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 71, "bbox": [303, 403, 336, 412], "page_size": [612.0, 792.0]}
{"layout": 1084, "type": "text", "text": "The  $(\\mathsf{x}1,\\mathsf{y}1,\\mathsf{x}2,\\mathsf{y}2)$  ) coordinates for the region with the black Audi  car in it would be approximately (0.35, 0.55, 0.55, 0.85). This is  based on the left and right most positions of the car being  around  $35\\%$   and  $55\\%$   of the image width, respectively, and the  top and bottom most positions being around  $55\\%$   and  $85\\%$   of  the image height, respectively. ", "page_idx": 71, "bbox": [302, 413, 494, 461.4847717285156], "page_size": [612.0, 792.0]}
{"layout": 1085, "type": "text", "text": "Plot Visualization: ", "text_level": 1, "page_idx": 71, "bbox": [303, 468, 377, 477.75], "page_size": [612.0, 792.0]}
{"layout": 1086, "type": "text", "text": "Refer to the query image below ", "page_idx": 71, "bbox": [302, 478.28204345703125, 424, 487.1020202636719], "page_size": [612.0, 792.0]}
{"layout": 1087, "type": "text", "text": "Grounded Description Prompt: Describe the pointed region in the image. ", "text_level": 1, "page_idx": 71, "bbox": [302, 499, 424, 526], "page_size": [612.0, 792.0]}
{"layout": 1088, "type": "image", "page_idx": 71, "img_path": "layout_images/2309.17421v2_191.jpg", "bbox": [304, 527, 436, 628], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows a parking lot with several cars parked in front of a building. The building features a large mural depicting a landscape scene, including a body of water, a dock, and surrounding greenery."}
{"layout": 1089, "type": "text", "text": "GPT-4V: The object in the pointed region in red is a black car, specifically  a sedan, parked in a parking lot. It appears to be a newer model  with a shiny exterior and tinted windows. ", "page_idx": 71, "bbox": [302, 629.526123046875, 494, 663.261474609375], "page_size": [612.0, 792.0]}
{"layout": 1090, "type": "text", "text": "Figure 52: GPT-4V can use its understanding of coordinates to generate visual pointing output, thereby grounding the textual or visually queried object. Using example-grounded instructions can help GPT-4V understand coordinate definitions and therefore generate better pointing. While output spatial regions are not precise, the approach enables an “understanding ( i.e ., grounded description) and generation” loop for visual pointing, leading to an effective way of human-computer interaction. Check Section  5.3  for detailed discussions. ", "page_idx": 71, "bbox": [108, 674.590576171875, 505, 741.1414794921875], "page_size": [612.0, 792.0]}
{"layout": 1091, "type": "text", "text": "6 Temporal and Video Understanding ", "text_level": 1, "page_idx": 72, "bbox": [106, 72, 311, 84], "page_size": [612.0, 792.0]}
{"layout": 1092, "type": "text", "text": "In this section, we discuss temporal and video understanding capabilities. Even though GPT- 4V operates primarily on images as inputs, evaluating its understanding of temporal sequences and video content remains a crucial aspect of its overall assessment. This is because real-world events unfold over time, and an AI system’s ability to understand these dynamic processes is instrumental in real-world applications. Capabilities like temporal anticipation, temporal ordering, temporal localization, temporal reasoning, and grounded temporal understanding help to gauge the model’s proficiency in comprehending the sequence of events, anticipating future occurrences, and con textually analyzing activities over time, all within a series of static images. In spite of its image-centric focus, GPT-4V is able to comprehend video and temporal sequences in a way that’s similar to human comprehension. To enhance the versatility and applicability of a sophisticated AI model like GPT-4V, this aspect of testing is critical to its development and refinement. For the upcoming experiments in this section, we will use multiple selected video frames as inputs to test the model’s abilities in understanding temporal sequences and video content. ", "page_idx": 72, "bbox": [107, 95.8365478515625, 505, 238.75047302246094], "page_size": [612.0, 792.0]}
{"layout": 1093, "type": "text", "text": "6.1 Multi-image Sequencing ", "text_level": 1, "page_idx": 72, "bbox": [106, 251, 236, 263], "page_size": [612.0, 792.0]}
{"layout": 1094, "type": "text", "text": "In this subsection, we demonstrate that GPT-4V can accurately comprehend and analyze sequences of video frames. Within this frame-by-frame analysis, GPT-4V recognizes the scene in which the activity is taking place, delivering a deeper contextual understanding. As shown in Figure  53 , the model is not just confined to recognizing the environment; it also accurately interprets the actions being performed by individuals in the video. GPT-4V understands the sequence and context of various human poses and intelligently correlates them with the ongoing activity. By understanding pose variations beyond just identification, GPT-4V can derive meaning from the subtleties of human movement and action. As a result of this level of detailed understanding, GPT-4V can capture the essence of what’s happening in videos, offering rich and nuanced insights that go beyond just identifying objects and scenes. ", "page_idx": 72, "bbox": [107, 271.14556884765625, 505, 381.3315124511719], "page_size": [612.0, 792.0]}
{"layout": 1095, "type": "text", "text": "6.2 Video Understanding ", "text_level": 1, "page_idx": 72, "bbox": [106, 393, 223, 405], "page_size": [612.0, 792.0]}
{"layout": 1096, "type": "text", "text": "Temporal ordering. Temporal ordering is a crucial element of temporal commonsense and forms an essential part of GPT-4V’s capabilities evaluation. This involves providing the model with a series of shuffled images and gauging its ability to discern cause and effect relationships as well as time progressions. An understanding of such relationships requires the ability to reorder the sequence in a logically coherent and temporally accurate manner. Figure  54  illustrates an example of long-term temporal ordering where GPT-4V is presented with a series of shuffled image frames depicting a sushi-making event. Despite the disorder, GPT-4V effectively identifies the event and determines the appropriate temporal sequence of the sushi-making process. In addition, Figure  55 provides an example of short-term temporal ordering. Given a designated action, such as opening or closing a door, GPT-4V demonstrates its capability to comprehend the image’s content and determine the correct sequential order of the events. These examples highlight GPT-4V’s capability in temporal commonsense, reinforcing its ability to comprehend both long-term and short-term sequences accurately. ", "page_idx": 72, "bbox": [107, 413.367919921875, 505, 556.6405029296875], "page_size": [612.0, 792.0]}
{"layout": 1097, "type": "text", "text": "Temporal anticipation. We demonstrate GPT-4V’s ability to anticipate future events given a set of initial frames. Long- and short-term examples are used to validate this capacity for anticipating future events. The right side of Figure  56  illustrates GPT-4V’s ability to anticipate short-term events with a soccer penalty kick example. Given the first few frames, it accurately foresees the typical next actions of both the kicker and the goalkeeper, due to its understanding of the inherent structure and rules of the game. In addition, as shown in The left side of Figure  56 , the sushi preparation sequence illustrates GPT-4V’s long-term anticipation capability. By understanding the activity based on visual cues, GPT-4V not only recognizes the current progress in sushi preparation but also accurately anticipates the subsequent steps, demonstrating its capacity to interpret and predict complex, multi-step processes over an extended period. This combination of short-term and long-term temporal anticipation allows GPT-4V to capture and understand activities with varying temporal structures and complexities. ", "page_idx": 72, "bbox": [107, 567.35693359375, 505, 688.8104858398438], "page_size": [612.0, 792.0]}
{"layout": 1098, "type": "text", "text": "Temporal localization and reasoning. Figure  57  illustrates GPT-4V’s capabilities in temporal localization and reasoning. It accurately identifies the precise moment the player strikes the ball. Fur- ", "page_idx": 72, "bbox": [107, 699.5269165039062, 505, 722.7994995117188], "page_size": [612.0, 792.0]}
{"layout": 1099, "type": "text", "text": "Sec. 6.1 Multi-image Sequencing ", "text_level": 1, "page_idx": 73, "bbox": [124, 73, 377, 94], "page_size": [612.0, 792.0]}
{"layout": 1100, "type": "text", "text": "Action Recognition ", "text_level": 1, "page_idx": 73, "bbox": [120, 97, 211, 109], "page_size": [612.0, 792.0]}
{"layout": 1101, "type": "text", "text": "Prompt: Explain the images and the action the person is taking. ", "page_idx": 73, "bbox": [121, 114.4997787475586, 302.1747741699219, 134.25827026367188], "page_size": [612.0, 792.0]}
{"layout": 1102, "type": "image", "page_idx": 73, "img_path": "layout_images/2309.17421v2_192.jpg", "bbox": [121, 136.25, 255, 443], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image sequence shows a person in a room equipped with motion capture cameras. The person is doing a push-up exercise while wearing a motion capture suit. You can see various cameras and a monitor set up around the room, likely to record and analyze the movements."}
{"layout": 1103, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 73, "bbox": [121, 444, 159, 454.75], "page_size": [612.0, 792.0]}
{"layout": 1104, "type": "text", "text": "The images show a person in a motion capture suit in a studio with multiple cameras and sensors. The person  is performing a push-up exercise. In the first image, the person is in the starting position with their arms  extended and body in a straight line. In the second image, the person is lowering their body towards the ground.  In the third image, the person is at the bottom of the push-up with their chest close to the ground. In the fourth  image, the person is pushing their body back up to the starting position.  ", "page_idx": 73, "bbox": [121, 456.6147155761719, 493.5496520996094, 500.82672119140625], "page_size": [612.0, 792.0]}
{"layout": 1105, "type": "text", "text": "Figure 53: Sequences of video frames understanding: Interpreting human poses and deriving relevant insights from video sequences. Check Section  6.1  for detailed discussions. ", "page_idx": 73, "bbox": [108, 515.0435791015625, 504, 537.9574584960938], "page_size": [612.0, 792.0]}
{"layout": 1106, "type": "text", "text": "thermore, GPT-4V showcases its understanding of cause and effect by inferring from the relationship between the goalkeeper and the ball to determine if the goalkeeper successfully blocks the ball. In the context of the example given, understanding whether the goalkeeper can block the ball involves not only recognizing the spatial positions of the goalkeeper and the ball but also understanding the dynamics of their interaction and predicting the outcome of these dynamics. This demonstrates a considerable level of sophistication in the model’s reasoning abilities. ", "page_idx": 73, "bbox": [108, 559.112548828125, 504, 625.6635131835938], "page_size": [612.0, 792.0]}
{"layout": 1107, "type": "text", "text": "Sec. 6.2 Video Understanding ", "text_level": 1, "page_idx": 74, "bbox": [164, 200, 334, 213], "page_size": [612.0, 792.0]}
{"layout": 1108, "type": "text", "text": "Temporal Ordering ", "text_level": 1, "page_idx": 74, "bbox": [120, 221, 209, 232], "page_size": [612.0, 792.0]}
{"layout": 1109, "type": "text", "text": "Prompt: Analyze the images and decide the correct order in  which they should be displayed. ", "page_idx": 74, "bbox": [121, 237.18881225585938, 291, 265.9134521484375], "page_size": [612.0, 792.0]}
{"layout": 1110, "type": "text", "text": "Prompt Continued ", "text_level": 1, "page_idx": 74, "bbox": [320, 256, 382, 263], "page_size": [612.0, 792.0]}
{"layout": 1111, "type": "image", "page_idx": 74, "img_path": "layout_images/2309.17421v2_193.jpg", "bbox": [118, 268, 484, 553], "page_size": [612.0, 792.0], "ocr_text": "The correct order in which the images should be\ndisplayed is:\n\n. @- Mixing the rice\n\n. © - Placing the rice on the seaweed\n. b - Adding the fillings\n\n. a - Cutting the roll\n\n. d - Finished sushi rolls on a plate.\n\nAWN\n", "vlm_text": "The image is a series of photographs illustrating the process of making sushi. Here is a breakdown of each step and the corresponding images in order:\n\n1. **e - Mixing the rice:** This shows the preparation stage where rice is being mixed, possibly seasoning it with vinegar, sugar, and salt.\n2. **c - Placing the rice on the seaweed:** Sushi rice is being spread on a sheet of seaweed (nori) as the initial step of sushi rolling.\n3. **b - Adding the fillings:** Ingredients such as fish and vegetables are placed on top of the rice as fillings for the sushi roll.\n4. **a - Cutting the roll:** The rolled sushi, wrapped in seaweed, is being sliced into smaller pieces with a knife on a cutting board.\n5. **d - Finished sushi rolls on a plate:** The final product is displayed with neatly arranged sushi rolls served on a plate, ready to eat."}
{"layout": 1112, "type": "text", "text": "Figure 54: Long-term temporal ordering: GPT-4V is presented with shuffled image frames depicting a sushi-making event. While the sushi-making process is disordered, GPT-4V is able to identify the event and determine the correct temporal sequence. Check Section  6.2  for detailed discussions. ", "page_idx": 74, "bbox": [108, 563.0135498046875, 504, 596.8374633789062], "page_size": [612.0, 792.0]}
{"layout": 1113, "type": "text", "text": "Sec. 6.2 Video Understanding ", "text_level": 1, "page_idx": 75, "bbox": [164, 150, 332, 165], "page_size": [612.0, 792.0]}
{"layout": 1114, "type": "text", "text": "Temporal Ordering ", "text_level": 1, "page_idx": 75, "bbox": [120, 172, 207, 184], "page_size": [612.0, 792.0]}
{"layout": 1115, "type": "text", "text": "Prompt: For opening a door, decide the correct order in which  they should be displayed. ", "page_idx": 75, "bbox": [122, 188.79981994628906, 296, 217.2941131591797], "page_size": [612.0, 792.0]}
{"layout": 1116, "type": "image", "page_idx": 75, "img_path": "layout_images/2309.17421v2_194.jpg", "bbox": [122, 222, 280, 528], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image sequence shows three frames labeled a, b, and c. It depicts a scene where a person, seen from the back, knocks on a door. In frames b and c, another person opens the door and appears to engage in a conversation with the first person. The setting seems to be indoors, possibly in a hallway or doorway."}
{"layout": 1117, "type": "text", "text": "GPT-4V: The correct order would be: \n\n ", "page_idx": 75, "bbox": [122, 532.6780395507812, 214.87939453125, 552.2780151367188], "page_size": [612.0, 792.0]}
{"layout": 1118, "type": "text", "text": "1. a - Person approaching the door \n\n 2. b - Person opening the door \n\n 3. c - Person entering through the door.   ", "page_idx": 75, "bbox": [122, 561.7860717773438, 255.2581329345703, 587.8550415039062], "page_size": [612.0, 792.0]}
{"layout": 1119, "type": "text", "text": "Prompt: For closing a door, decide the correct order in which  they should be displayed. ", "page_idx": 75, "bbox": [311, 188.79930114746094, 482, 217.29359436035156], "page_size": [612.0, 792.0]}
{"layout": 1120, "type": "image", "page_idx": 75, "img_path": "layout_images/2309.17421v2_195.jpg", "bbox": [311, 221, 467, 528], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image is a sequence of three frames labeled a, b, and c. In the first frame (a), a person wearing a brown and white shirt is standing in front of a door, suggesting they may be knocking or waiting. In the second frame (b), the door appears to be slightly open with some visible light from inside, and the person outside the door is still present. In the third frame (c), the door is more open, allowing more light to reveal part of the interior and the person standing outside the door continues to face inward."}
{"layout": 1121, "type": "text", "text": "GPT-4V: Correct order:  ", "page_idx": 75, "bbox": [311, 532.6774291992188, 359.0667419433594, 552.2774047851562], "page_size": [612.0, 792.0]}
{"layout": 1122, "type": "text", "text": "1) c - Person standing in doorway  2) b - Person starting to close door  3) a - Person closing door completely ", "page_idx": 75, "bbox": [311, 561.7854614257812, 433.5943908691406, 587.8544311523438], "page_size": [612.0, 792.0]}
{"layout": 1123, "type": "text", "text": "Figure 55: Short-term temporal ordering: given a specified action, such as opening or closing a door, GPT-4V demonstrates its capability to comprehend the images’ content and determine the correct sequential order corresponding to the specified action. Check Section  6.2  for detailed discussions. ", "page_idx": 75, "bbox": [108, 611.0565185546875, 505, 644.8795166015625], "page_size": [612.0, 792.0]}
{"layout": 1124, "type": "text", "text": "Sec. 6.2 Video Understanding ", "text_level": 1, "page_idx": 76, "bbox": [164, 146, 336, 163], "page_size": [612.0, 792.0]}
{"layout": 1125, "type": "text", "text": "Temporal Anticipation ", "text_level": 1, "page_idx": 76, "bbox": [121, 170, 223, 181], "page_size": [612.0, 792.0]}
{"layout": 1126, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 76, "bbox": [120, 186, 158, 196], "page_size": [612.0, 792.0]}
{"layout": 1127, "type": "text", "text": "Predict what will happen next based on the images. ", "page_idx": 76, "bbox": [121, 197.46688842773438, 291, 205.80287170410156], "page_size": [612.0, 792.0]}
{"layout": 1128, "type": "image", "page_idx": 76, "img_path": "layout_images/2309.17421v2_196.jpg", "bbox": [121, 219, 283, 521], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows three sequential frames of a soccer match where a player is taking a penalty kick. In the first frame, the player is approaching the ball to take the penalty. In the second frame, the player is about to strike the ball, and the goalkeeper is prepared to react. In the third frame, the ball has been kicked and is heading towards the goal, with the goalkeeper diving to attempt a save. The players on the pitch are wearing maroon and white kits."}
{"layout": 1129, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 76, "bbox": [120, 529, 159, 540], "page_size": [612.0, 792.0]}
{"layout": 1130, "type": "text", "text": "Based on the images, it appears that the player in  the white jersey is taking a penalty kick. The next  likely event is that the player will kick the ball  towards the goal, attempting to score. The  goalkeeper in the black jersey will try to block the  shot.  ", "page_idx": 76, "bbox": [121, 541.9722290039062, 287, 595.0776977539062], "page_size": [612.0, 792.0]}
{"layout": 1131, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 76, "bbox": [310, 186, 348, 196], "page_size": [612.0, 792.0]}
{"layout": 1132, "type": "text", "text": "Analyze the images and decide what the next steps  should be. ", "page_idx": 76, "bbox": [311, 197.4663543701172, 483, 214.75624084472656], "page_size": [612.0, 792.0]}
{"layout": 1133, "type": "image", "page_idx": 76, "img_path": "layout_images/2309.17421v2_197.jpg", "bbox": [310, 219, 469, 519], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows a step-by-step process of making sushi rolls. The first part of the image depicts someone mixing rice in a bowl. The second part shows rice placed on a sheet of seaweed (nori) on top of a bamboo sushi mat. The third part illustrates additional sushi ingredients, like sliced imitation crab meat and cucumber, being added on top of the rice and seaweed, getting ready to be rolled into a sushi roll."}
{"layout": 1134, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 76, "bbox": [310, 530, 348, 540], "page_size": [612.0, 792.0]}
{"layout": 1135, "type": "text", "text": "The images show the process of making sushi rolls.  The first image shows rice being mixed with vinegar  in a bowl. The second image shows the rice being  spread onto a sheet of seaweed. The third image  shows the addition of salmon and cucumber to the  roll.  The next steps would be to roll the sushi using a  bamboo mat, cut the roll into smaller pieces, and  serve with soy sauce and wasabi.  ", "page_idx": 76, "bbox": [311, 541.9716796875, 487, 612.9849243164062], "page_size": [612.0, 792.0]}
{"layout": 1136, "type": "text", "text": "Figure 56: Short-term and long-term temporal anticipation: GPT-4V captures and understands activi- ties with varying temporal structures and complexities. Check Section  6.2  for detailed discussions. ", "page_idx": 76, "bbox": [108, 624.9815673828125, 505.64923095703125, 647.8955078125], "page_size": [612.0, 792.0]}
{"layout": 1137, "type": "text", "text": "Temporal Localization and Reasoning  ", "page_idx": 77, "bbox": [121.20907592773438, 176.65643310546875, 295.9227294921875, 187.3887939453125], "page_size": [612.0, 792.0]}
{"layout": 1138, "type": "image", "page_idx": 77, "img_path": "layout_images/2309.17421v2_198.jpg", "img_caption": "Figure 57: Temporal localization and reasoning: GPT-4V shows the capability in temporal localization by accurately identifying when the player strikes the ball. It also demonstrates cause-and-effect reasoning by determining whether the ball was blocked based on the goalkeeper-ball interaction. Check Section  6.2  for detailed discussions. ", "bbox": [107, 192, 506, 641], "page_size": [612.0, 792.0], "ocr_text": "Prompt:\n\nIn which frame does the player kick the ball? Did ;\n\nthe goalkeeper block the ball? Maret ere Lat\na\n\nGPT-4V:\n\nThe player kicks the ball at frame c. The ball was not\nblocked by the goalkeeper as it can be seen in frame\nthat the ball is in the net and the goalkeeper is on the\nground.\n\n", "vlm_text": "The image presents a series of frames (a through f) capturing a moment in a soccer game where the player kicks the ball towards the goal. The caption describes GPT-4V's ability to determine the moment the player kicks the ball and whether the goalkeeper blocks the shot. The response from GPT-4V is also included, explaining that the player kicks the ball in frame c, and the ball is not blocked by the goalkeeper, as shown by the ball in the net and the goalkeeper on the ground in frame f."}
{"layout": 1139, "type": "text", "text": "Sec. 6.3 Video Visual Referring Prompting ", "text_level": 1, "page_idx": 78, "bbox": [125, 31, 375, 51], "page_size": [612.0, 792.0]}
{"layout": 1140, "type": "text", "text": "Grounded Temporal Understanding ", "text_level": 1, "page_idx": 78, "bbox": [120, 55, 284, 66], "page_size": [612.0, 792.0]}
{"layout": 1141, "type": "text", "text": "", "text_level": 1, "page_idx": 78, "bbox": [120, 72, 158, 77.75], "page_size": [612.0, 792.0]}
{"layout": 1142, "type": "text", "text": "Describe and summarize what the circled person  doing? ", "page_idx": 78, "bbox": [121, 83.39179229736328, 284.75054931640625, 100.70535278320312], "page_size": [612.0, 792.0]}
{"layout": 1143, "type": "image", "page_idx": 78, "img_path": "layout_images/2309.17421v2_199.jpg", "img_caption": "GPT-4V: The circled person is a man who is standing next to  a bench and appears to be in a conversation with  another man who is sitting on the bench. In the  second image, the circled man is pointing towards  something and in the third image, he is playfully  pretending to punch the other man. Overall, it  seems like the circled man is engaging in a friendly  conversation and joking around with the other man.  ", "bbox": [121, 104, 297, 490], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image sequence shows two men interacting near a bench in a park-like setting. The man who is circled is:\n\n1. Standing and talking to another man sitting on the bench.\n2. Pointing towards something as he talks.\n3. Playfully pretending to punch the seated man, indicating a friendly and joking interaction."}
{"layout": 1144, "type": "text", "text": "Prompt: Summarize the activities of the person. ", "page_idx": 78, "bbox": [313, 72.03953552246094, 441.9892272949219, 91.79802703857422], "page_size": [612.0, 792.0]}
{"layout": 1145, "type": "image", "page_idx": 78, "img_path": "layout_images/2309.17421v2_200.jpg", "img_caption": "GPT-4V: The circled person is sitting on a bench and then  stands up and adjusts his shirt.  ", "bbox": [311, 105, 485, 437], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image sequence depicts a person sitting on a bench, standing up, and then adjusting their shirt. The setting appears to be an outdoor area with trees and hills in the background. Another person is standing nearby, observing the actions of the circled individual."}
{"layout": 1146, "type": "text", "text": "Figure 58: Grounded temporal understanding: GPT-4V can apply a temporal understanding to a specific person of interest, indicated by a circle. Check Section  6.3  for detailed discussions. ", "page_idx": 78, "bbox": [108, 511.4835205078125, 505, 534.3975219726562], "page_size": [612.0, 792.0]}
{"layout": 1147, "type": "text", "text": "6.3 Visual Referring Prompting for Grounded Temporal Understanding ", "text_level": 1, "page_idx": 78, "bbox": [106, 541, 421, 554], "page_size": [612.0, 792.0]}
{"layout": 1148, "type": "text", "text": "Section  5  illustrates GPT-4V’s capabilities in visual referring prompting. In this section, we aim to extend this capability by testing visual referring prompting for temporal understanding. This advancement offers enhanced control over video comprehension tasks. ", "page_idx": 78, "bbox": [108, 561.6305541992188, 505, 595.4534912109375], "page_size": [612.0, 792.0]}
{"layout": 1149, "type": "text", "text": "Grounded temporal understanding.  Grounded temporal understanding forms another crucial aspect of GPT-4V’s capabilities, which we explore using pointing input in a sequence of image frames. Figure  58  exemplifies this by demonstrating how GPT-4V can apply a temporal understanding to a specific person of interest, indicated by a circle. GPT-4V can accurately describe events in a way that aligns with the corresponding temporal order, focusing on the activities of the circled individual. Beyond this, GPT-4V demonstrates a more refined understanding of the event, recognizing the nature of the interactions. For instance, GPT-4V can distinguish between friendly interactions and violent incidents, illustrating an ability to not only comprehend the temporal flow of events but also to interpret the tone and nature of the interactions taking place. This indicates GPT-4V’s capacity to process and comprehend complex temporal and social cues within a given sequence, adding a layer of depth to its understanding. ", "page_idx": 78, "bbox": [108, 599.4788818359375, 505, 720.9335327148438], "page_size": [612.0, 792.0]}
{"layout": 1150, "type": "text", "text": "7 Abstract Visual Reasoning and Intelligence Quotient Test ", "text_level": 1, "page_idx": 79, "bbox": [105, 72, 419, 84], "page_size": [612.0, 792.0]}
{"layout": 1151, "type": "text", "text": "Understanding and reasoning over abstract visual stimuli and symbols is one fundamental ability for human intelligence. This section examines if GPT-4V can abstract semantics from visual signals and can perform different types of human Intelligence Quotient (IQ) tests. ", "page_idx": 79, "bbox": [108, 95.87054443359375, 505, 129.69346618652344], "page_size": [612.0, 792.0]}
{"layout": 1152, "type": "text", "text": "7.1 Abstract Visual Stimuli ", "text_level": 1, "page_idx": 79, "bbox": [106, 141, 233, 154], "page_size": [612.0, 792.0]}
{"layout": 1153, "type": "text", "text": "Humans can infer semantics from abstract and often ambiguous visual stimuli. Figure  59  explores having GPT-4V interpret tangram [ 31 ,  97 ,  43 ,  59 ]. A tangram is a traditional geometric puzzle that consists of seven flat pieces called tans, which are put together to form shapes without overlapping the pieces. For example, GPT-4V interprets that sub-figure 7 in Figure  59  best illustrates a flying goose and provides reasoning descriptions for other sub-figure,  e.g ., 4. person or robot, 9. boat or hat, and 10. dog or fox. GPT-4V also has the ability to understand other formats of abstract visual diagrams [ 127 ,  16 ,  150 ], such as ASCII text art of cartoon characters in Figure  59  and symbolic inputs in Figures  61 - 62 . ", "page_idx": 79, "bbox": [108, 162.12158203125, 505, 250.49046325683594], "page_size": [612.0, 792.0]}
{"layout": 1154, "type": "text", "text": "7.2 Discovery and Association of Parts and Objects ", "text_level": 1, "page_idx": 79, "bbox": [106, 263, 333, 275], "page_size": [612.0, 792.0]}
{"layout": 1155, "type": "text", "text": "Discovering and associating object parts [ 139 ,  44 ] is another important abstract visual reasoning capability. Humans can easily discover how object parts may compose a semantically meaningful object. Figure  60  designs examples to probe GPT-4V’s capability in associating object parts. In the left example, we ask GPT-4V to localize an object part based on its semantic meaning. In the right example, GPT-4V is asked to associate object parts segmented by SAM [ 65 ]. GPT-4V can process figures for all object parts and associate them in a semantically meaningful to form the boy visualized in the bottom right. ", "page_idx": 79, "bbox": [108, 282.9185485839844, 505, 360.3785095214844], "page_size": [612.0, 792.0]}
{"layout": 1156, "type": "text", "text": "7.3 Wechsler Adult Intelligence Scale ", "text_level": 1, "page_idx": 79, "bbox": [106, 373, 274, 384], "page_size": [612.0, 792.0]}
{"layout": 1157, "type": "text", "text": "Section  7.1  demonstrates the abstract visual understanding capability of GPT-4V. As a further challenge, GPT-4V is asked to perform different abstract reasoning tasks, sourced from human Intelligence Quotient (IQ) tests. The Wechsler Adult Intelligence Scale [ 133 ] is recognized as one of the “gold standard IQ tests,” and is designed to provide a comprehensive measurement of an individual’s cognitive abilities using a series of sub-tests. Figure  61  shows representative questions and GPT-4V’s outputs from each sub-test category. GPT-4V shows promises in abstract reasoning, answering questions with texts only, symbolic visual inputs, and natural images. For example, the bottom right sample shows that GPT-4V can interpret the analogy question and find the best comparison of shoes. ", "page_idx": 79, "bbox": [108, 392.8065490722656, 505, 492.0845031738281], "page_size": [612.0, 792.0]}
{"layout": 1158, "type": "text", "text": "7.4 Raven’s Progressive Matrices ", "text_level": 1, "page_idx": 79, "bbox": [106, 504, 257, 516], "page_size": [612.0, 792.0]}
{"layout": 1159, "type": "text", "text": "Raven’s Progressive Matrices (RPM) [ 109 ] is another well-known non-verbal intelligence test devel- oped to measure abstract reasoning and problem-solving abilities. The test is designed to minimize the influence of language, culture, and formal education on test performance, making it suitable for testing AI models [ 16 ,  150 ,  55 ]. Each test sample contains three or eight images, arranged in 2-by-2 or 3-by-3 matrices with one figure missing. The goal is to select the next image from multiple candidate images by identifying patterns in the provided samples. In our approach, we challenge GPT-4V by sending the entire question page as a single image, instead of converting it into interleaved image-text pairs, similar to the human approach to IQ tests. As shown in Figure  62 , GPT-4V can generate reasonable answers without processed text descriptions or sub-figures. However, we also notice that breaking down the entire question image into interleaved text and sub-figures, such as in Figure  63 , does simplify the task and let GPT-4V produce more reliable answers. ", "page_idx": 79, "bbox": [108, 524.5125732421875, 505, 645.6084594726562], "page_size": [612.0, 792.0]}
{"layout": 1160, "type": "text", "text": "Sec. 7.1 Abstract Visual Stimuli ", "text_level": 1, "page_idx": 80, "bbox": [124, 37, 375, 58], "page_size": [612.0, 792.0]}
{"layout": 1161, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 80, "bbox": [116, 64, 154, 73], "page_size": [612.0, 792.0]}
{"layout": 1162, "type": "text", "text": "A tangram is a geometric puzzle that consists of  seven flat pieces to form shapes. Here are ten  abstract tangram. Which one best illustrates a  flying goose? What do other figures look like? ", "page_idx": 80, "bbox": [118, 75.21190643310547, 277, 110.4095687866211], "page_size": [612.0, 792.0]}
{"layout": 1163, "type": "image", "page_idx": 80, "img_path": "layout_images/2309.17421v2_201.jpg", "bbox": [117, 114, 182, 540], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image consists of six shapes, each composed of a set of geometric pieces arranged within six separate squares. These shapes resemble puzzles created using tangram pieces, which include triangles, a square, and a parallelogram. Each shape is unique:\n\n1. The first shape resembles an arrow pointing upwards.\n2. The second shape resembles a star.\n3. The third shape resembles a question mark.\n4. The fourth shape resembles a person standing upright.\n5. The fifth shape resembles a cat's face with pointed ears on top.\n6. The sixth shape resembles a house with a triangular roof."}
{"layout": 1164, "type": "image", "page_idx": 80, "img_path": "layout_images/2309.17421v2_202.jpg", "bbox": [302, 103, 369, 394], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows four tangram puzzles, each with a different arrangement of shapes. They're numbered 7 to 10. Tangrams are puzzles consisting of flat pieces called tans, which are put together to form shapes. Each puzzle consists of these geometric shapes arranged to form figures or patterns."}
{"layout": 1165, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 80, "bbox": [304, 398, 343, 407.75], "page_size": [612.0, 792.0]}
{"layout": 1166, "type": "text", "text": "The tangram that best illustrates a flying goose is number  7. It has the triangular shape of a goose's body and  wings, and the smaller triangle at the top could represent  the head and neck. The other figures could represent various things:\n\n 1. A rocket or a tower\n\n 2. A butterfly or a bat\n\n 3. A question mark or a snake\n\n 4. A person or a robot\n\n 5. A mountain range or a crown\n\n 6. A house or a temple\n\n 7. A flying goose (as mentioned above)\n\n 8. A bird or a fish\n\n 9. A boat or a hat\n\n 10. A dog or a fox. ", "page_idx": 80, "bbox": [306, 410.100830078125, 497, 543.7911987304688], "page_size": [612.0, 792.0]}
{"layout": 1167, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 80, "bbox": [116, 557, 155, 567], "page_size": [612.0, 792.0]}
{"layout": 1168, "type": "text", "text": "Here is an abstract figure. Can you identify how many characters are there, and the cartoon from which they  originate? ", "page_idx": 80, "bbox": [118, 568.4662475585938, 476.2408447265625, 585.756103515625], "page_size": [612.0, 792.0]}
{"layout": 1169, "type": "image", "page_idx": 80, "img_path": "layout_images/2309.17421v2_203.jpg", "bbox": [119, 590, 325, 660], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image is an ASCII art representation of the four main characters from the animated television show \"South Park.\" From left to right, the characters are Stan Marsh, Kyle Broflovski, Kenny McCormick, and Eric Cartman."}
{"layout": 1170, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 80, "bbox": [117, 667, 155, 678], "page_size": [612.0, 792.0]}
{"layout": 1171, "type": "text", "text": "There are four characters in this abstract figure. They originate from the cartoon \"South Park.\" The characters  are Eric Cartman, Kyle Broflovski, Stan Marsh, and Kenny McCormick. ", "page_idx": 80, "bbox": [118, 679.7503662109375, 480.80731201171875, 697.040283203125], "page_size": [612.0, 792.0]}
{"layout": 1172, "type": "text", "text": "Figure 59: Understanding abstract visual stimuli such as tangram [ 59 ] and ASCII text art. Check Section  7.1  for detailed discussions. ", "page_idx": 80, "bbox": [108, 709.3525390625, 504.2449645996094, 732.2664794921875], "page_size": [612.0, 792.0]}
{"layout": 1173, "type": "text", "text": "Sec. 7.2 Association of Parts and Objects ", "text_level": 1, "page_idx": 81, "bbox": [124, 33, 368, 54], "page_size": [612.0, 792.0]}
{"layout": 1174, "type": "text", "text": "Discovery and Association of Parts and Objects ", "text_level": 1, "page_idx": 81, "bbox": [116, 58, 334, 69], "page_size": [612.0, 792.0]}
{"layout": 1175, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 81, "bbox": [116, 74, 154, 84], "page_size": [612.0, 792.0]}
{"layout": 1176, "type": "text", "text": "A tangram is a geometric puzzle that consists of seven  flat pieces to form shapes. Locate the referred object  and represent the location of the region. Regions are  represented by   $(\\mathsf{x}1,\\mathsf{y}1,\\mathsf{x}2,\\mathsf{y}2)$   coordinates. x1  $\\times2$   are  the left and right most positions, normalized into 0 to 1,  where 0 is the left and 1 is the right. y1 y2 are the top  and bottom most positions, normalized into 0 to 1,  where 0 is the top and 1 is the bottom. For example,  the  $(\\mathsf{x}1,\\mathsf{y}1,\\mathsf{x}2,\\mathsf{y}2)$  ) coordinates for that region with the  beer bottle is (0.47, 0.48, 0.55, 0.87). ", "page_idx": 81, "bbox": [117, 85.29788970947266, 297, 172.8057861328125], "page_size": [612.0, 792.0]}
{"layout": 1177, "type": "image", "page_idx": 81, "img_path": "layout_images/2309.17421v2_204.jpg", "img_caption": "The image below that shows a flying goose. Which  part represents the head of the bird?  What is the   $(\\mathsf{x}1,\\mathsf{y}1,\\mathsf{x}2,\\mathsf{y}2)$   coordinates for the region  with the head of the bird? ", "bbox": [117, 176, 287, 341], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image appears to be of an outdoor dining setup with tables and a bottle in the center. There are no visible geese or birds in this image. \n\nThe $(\\mathsf{x}1,\\mathsf{y}1,\\mathsf{x}2,\\mathsf{y}2)$ coordinates you mentioned do not correspond to any bird's head, as there isn't one in the picture."}
{"layout": 1178, "type": "image", "page_idx": 81, "img_path": "layout_images/2309.17421v2_205.jpg", "bbox": [119, 346, 222, 444], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "This image shows a tangram puzzle arranged in the shape of a bird. Tangrams are made up of seven geometric pieces called tans, which are used to form various shapes and figures."}
{"layout": 1179, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 81, "bbox": [118, 446, 153, 456], "page_size": [612.0, 792.0]}
{"layout": 1180, "type": "text", "text": "The head of the bird is represented by the small  triangle at the top of the image. The   $(\\mathsf{x}1,\\mathsf{y}1,\\mathsf{x}2,\\mathsf{y}2)$    coordinates for the region with the head of the bird  would be approximately (0.42, 0.15, 0.58, 0.35). ", "page_idx": 81, "bbox": [117, 457.9026184082031, 283, 492.54095458984375], "page_size": [612.0, 792.0]}
{"layout": 1181, "type": "text", "text": "Plot Visualization: Refer to the query image below ", "page_idx": 81, "bbox": [117, 497.040283203125, 250.1346893310547, 516.8920288085938], "page_size": [612.0, 792.0]}
{"layout": 1182, "type": "text", "text": "Grounded VQA ", "text_level": 1, "page_idx": 81, "bbox": [117, 532, 186, 542], "page_size": [612.0, 792.0]}
{"layout": 1183, "type": "text", "text": "Prompt: What is the highlighted part?  ", "page_idx": 81, "bbox": [117, 543.6160278320312, 212.99192810058594, 563.033935546875], "page_size": [612.0, 792.0]}
{"layout": 1184, "type": "image", "page_idx": 81, "img_path": "layout_images/2309.17421v2_206.jpg", "bbox": [118, 566, 242, 669], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows a tangram puzzle arranged to resemble a bird-like figure. It includes geometric shapes like triangles and parallelograms. An area at the top is highlighted with a red box. There are numerical markers on the side, 0.15 and 0.35, possibly indicating positions or dimensions."}
{"layout": 1185, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 81, "bbox": [116, 678, 154, 687], "page_size": [612.0, 792.0]}
{"layout": 1186, "type": "text", "text": "The highlighted part is the head of the goose. ", "page_idx": 81, "bbox": [117, 689.5221557617188, 265.21514892578125, 697.7256469726562], "page_size": [612.0, 792.0]}
{"layout": 1187, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 81, "bbox": [319, 74, 357, 84], "page_size": [612.0, 792.0]}
{"layout": 1188, "type": "text", "text": "How are those visual parts related, and can they be  combined to form a single object such as a boy? If so,  how to arrange them? ", "page_idx": 81, "bbox": [321, 85.29788970947266, 497, 111.12459564208984], "page_size": [612.0, 792.0]}
{"layout": 1189, "type": "image", "page_idx": 81, "img_path": "layout_images/2309.17421v2_207.jpg", "bbox": [323, 113, 424, 423], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "This image is a collage of three separate sections:\n\n1. The top section shows a piece of cardboard decorated with geometric shapes, resembling the pattern of a face or mask. The shapes include orange, yellow, blue, and green elements, and it appears as a creative art piece or craft.\n\n2. The middle section depicts a pair of jeans worn by a person, but with the rest of the body (the upper body and feet) outside the frame, giving the impression of disembodied legs.\n\n3. The bottom section shows a different angle of a similar cardboard piece from the first section, tilted and with decorative shapes like an artistic depiction of a face or expression.\n\nThe overall arrangement and artistic elements suggest a playful or abstract theme, possibly involving art or a costume."}
{"layout": 1190, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 81, "bbox": [320, 427, 357, 437], "page_size": [612.0, 792.0]}
{"layout": 1191, "type": "text", "text": "The three visual parts are related in that they are all  components that could be used to create a single  object. In this case, the object is a boy. To combine these parts to form a boy, we can arrange  them in the following way: - The first image (the cardboard with orange and  green shapes) could be used as the body of the boy.  - The second image (the pair of jeans) could be used  as the legs of the boy.  - The third image (the cardboard with yellow, red, and  blue shapes) could be used as the head of the boy.  By arranging these parts in this way, we can create a  single object that resembles a boy. ", "page_idx": 81, "bbox": [321, 439.0206298828125, 497, 552.96337890625], "page_size": [612.0, 792.0]}
{"layout": 1192, "type": "text", "text": "Plot Visualization: ", "text_level": 1, "page_idx": 81, "bbox": [320, 562, 402, 573], "page_size": [612.0, 792.0]}
{"layout": 1193, "type": "image", "page_idx": 81, "img_path": "layout_images/2309.17421v2_208.jpg", "bbox": [336, 577, 403, 698], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows a person dressed in a homemade robot costume made out of cardboard and colorful paper. The costume consists of two large cardboard boxes: one for the body and one for the head. The head box is decorated with facial features like eyes, a mouth, and a nose made from colorful paper cutouts. The body box also has colorful paper decorations on it. The person is wearing jeans and standing with their arms slightly out to the sides."}
{"layout": 1194, "type": "text", "text": "Figure 60: Understanding part-object association in abstract and natural images. Check Section  7.2 for detailed discussions. ", "page_idx": 81, "bbox": [108, 709.5515747070312, 503.9972229003906, 732.4655151367188], "page_size": [612.0, 792.0]}
{"layout": 1195, "type": "text", "text": "Sec. 7.3 Wechsler Adult Intelligence Scale ", "text_level": 1, "page_idx": 82, "bbox": [126, 18, 368, 34], "page_size": [612.0, 792.0]}
{"layout": 1196, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 82, "bbox": [262, 43, 298, 51], "page_size": [612.0, 792.0]}
{"layout": 1197, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 82, "bbox": [116, 43, 153, 52], "page_size": [612.0, 792.0]}
{"layout": 1198, "type": "text", "text": "Tom, twelve years old, is three times as old as his sister. How old will  Tom be when he is twice as old as his sister? ", "page_idx": 82, "bbox": [262, 53.29606628417969, 489, 70.33387756347656], "page_size": [612.0, 792.0]}
{"layout": 1199, "type": "text", "text": "Which one of the five is least like the  other four? a. Carrot b. Potato c. Corn d. Apple e. Bean ", "page_idx": 82, "bbox": [117, 53.29606628417969, 238.83770751953125, 114.4506607055664], "page_size": [612.0, 792.0]}
{"layout": 1200, "type": "text", "text": "a. 18 b. 21 c. 16 d. 20 e. 15 ", "page_idx": 82, "bbox": [262, 70.94277954101562, 279, 114.4506607055664], "page_size": [612.0, 792.0]}
{"layout": 1201, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 82, "bbox": [262, 123, 299, 133], "page_size": [612.0, 792.0]}
{"layout": 1202, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 82, "bbox": [116, 123, 154, 134], "page_size": [612.0, 792.0]}
{"layout": 1203, "type": "text", "text": "If Tom is three times as old as his sister, then his sister is one-third of  his age. So, his sister is 4 years old. Now, we need to figure out when  Tom will be twice as old as his sister. This means that his sister's age  will be half of his age. So, we need to find a number that is double the  age of 4. The answer is 8. So, Tom will be 16 years old when he is  twice as old as his sister. The correct answer is c. 16. ", "page_idx": 82, "bbox": [262, 135.2272491455078, 489, 187.5585479736328], "page_size": [612.0, 792.0]}
{"layout": 1204, "type": "text", "text": "d. Apple Explanation: Carrot, potato, corn, and  bean are all vegetables, while apple is  a fruit. ", "page_idx": 82, "bbox": [117, 135.2272491455078, 244, 169.91180419921875], "page_size": [612.0, 792.0]}
{"layout": 1205, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 82, "bbox": [116, 202, 153, 212], "page_size": [612.0, 792.0]}
{"layout": 1206, "type": "text", "text": "Which one of the five is least like the  other four? ", "page_idx": 82, "bbox": [117, 213.37705993652344, 238, 230.41488647460938], "page_size": [612.0, 792.0]}
{"layout": 1207, "type": "image", "page_idx": 82, "img_path": "layout_images/2309.17421v2_209.jpg", "bbox": [116, 238, 179, 583], "page_size": [612.0, 792.0], "ocr_text": "Od <P of) <\n", "vlm_text": "The image contains five different abstract symbols labeled from a to e. \n\n- **a.** A central diamond shape with four arms extending outwards, each ending in a rectangular shape.\n- **b.** A Y-shaped symbol with a central vertical arm splitting into three branches, each ending in a triangular shape.\n- **c.** Similar to symbol a, but each arm ends in a square shape.\n- **d.** A vertical arrangement of three circles connected by lines, with additional arms at the sides, each ending in a circle.\n- **e.** Similar to symbol b, a Y-shaped structure with a central vertical arm splitting into branches, each branch finishing with a triangular shape.\n\nThese geometric designs vary in the shapes and arrangements of their components."}
{"layout": 1208, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 82, "bbox": [116, 593, 154, 603], "page_size": [612.0, 792.0]}
{"layout": 1209, "type": "text", "text": "Answer: d.  is the least like the other  four because it is the only one that  does not have a central point  connecting all the other points. ", "page_idx": 82, "bbox": [117, 605.3862915039062, 238, 640.0708618164062], "page_size": [612.0, 792.0]}
{"layout": 1210, "type": "text", "text": "Prompt: Which one of the five designs makes the best comparison? ", "page_idx": 82, "bbox": [262, 214.75253295898438, 454.77069091796875, 234.1963348388672], "page_size": [612.0, 792.0]}
{"layout": 1211, "type": "image", "page_idx": 82, "img_path": "layout_images/2309.17421v2_210.jpg", "bbox": [259, 236, 493, 650], "page_size": [612.0, 792.0], "ocr_text": "> vihie\n", "vlm_text": "The image shows an analogy puzzle. The comparison is between a hand and a glove, and a foot and an item of clothing. The options are:\n\na. Cap\nb. Soccer ball\nc. Shoe\nd. T-shirt\ne. Shorts\n\nThe correct answer is c. Shoe, as a glove is to a hand as a shoe is to a foot."}
{"layout": 1212, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 82, "bbox": [262, 656, 299, 665], "page_size": [612.0, 792.0]}
{"layout": 1213, "type": "text", "text": "Answer: c. The black dress shoe Explanation: The analogy is comparing a hand to a glove and a foot to  a shoe. The best comparison would be the black dress shoe as it is  an item of clothing that is worn on the foot, just like a glove is worn on  the hand. ", "page_idx": 82, "bbox": [262, 667.1499633789062, 492, 710.6578979492188], "page_size": [612.0, 792.0]}
{"layout": 1214, "type": "text", "text": "Sec. 7.4 Raven's Progressive Matrices ", "text_level": 1, "page_idx": 83, "bbox": [146, 91, 357, 106], "page_size": [612.0, 792.0]}
{"layout": 1215, "type": "text", "text": "RPM: Entire Question Page ", "text_level": 1, "page_idx": 83, "bbox": [117, 113, 243, 125], "page_size": [612.0, 792.0]}
{"layout": 1216, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 83, "bbox": [117, 130, 154, 139.75], "page_size": [612.0, 792.0]}
{"layout": 1217, "type": "text", "text": "Find out the pattern in the first column and use it to infer the missing figure in the second column. ", "page_idx": 83, "bbox": [118, 141.11962890625, 438.0835266113281, 149.46702575683594], "page_size": [612.0, 792.0]}
{"layout": 1218, "type": "image", "page_idx": 83, "img_path": "layout_images/2309.17421v2_211.jpg", "bbox": [117, 152, 487, 350], "page_size": [612.0, 792.0], "ocr_text": "5. Choose a drawing that fits the empty space:\n\nfolly |[o][A] fa] s|\n", "vlm_text": "The image is a puzzle asking you to choose a drawing that fits the empty space in a 2x2 grid. The top row of the grid displays a semicircle on the left and a triangle on the right, while the bottom row has a semicircle on the left and an empty space marked with a question mark on the right. Below the grid, there are six shape options to choose from: a circle, an upside-down triangle, a diamond, a triangle, a semicircle facing right, and a semicircle facing left.\n\nThe sequence appears to alternate between semicircles and triangles comparing across rows. The correct answer, following the pattern displayed, is most likely the triangle, which is the fourth option from the left in the row of choices."}
{"layout": 1219, "type": "text", "text": "GPT-4V: The correct drawing that fits the empty space is the upside-down triangle. This is because the pattern in the grid is  that each row and column has one of each shape in different orientations. ", "page_idx": 83, "bbox": [118, 356.4243469238281, 496.05718994140625, 385.1489562988281], "page_size": [612.0, 792.0]}
{"layout": 1220, "type": "text", "text": "Prompt: Find out the pattern in the first two rows and use it to infer the missing figure in the third row. ", "page_idx": 83, "bbox": [118, 397.4124755859375, 421.44384765625, 417.1709289550781], "page_size": [612.0, 792.0]}
{"layout": 1221, "type": "text", "text": "", "page_idx": 83, "bbox": [118, 422, 482, 429.75], "page_size": [612.0, 792.0]}
{"layout": 1222, "type": "image", "page_idx": 83, "img_path": "layout_images/2309.17421v2_212.jpg", "bbox": [123, 430.25, 479, 604], "page_size": [612.0, 792.0], "ocr_text": "“igure Out wrich igure Ora tO FI tial snoulra fogicany replace ne TMMSSING NgUre/ QuUesSuOr-iMalrk\nJ/\\\\ |?\n\n", "vlm_text": "The image is a puzzle with a 3x3 grid containing arrows. Eight arrows are given, and one position is marked with a question mark. Below the grid, there are eight options, labeled A to H, each showing a different arrow direction.\n\nTo solve the puzzle, look for a pattern in the grid:\n\n- Each row seems to follow a rotation pattern for the arrows.\n- Row 1: Right, Down-right, Down-left\n- Row 2: Left, Up-right, ?\n- Row 3: Up-left, Down, Right\n\nBy examining the patterns:\n- The arrows in each row seem to follow a consistent clockwise rotation.\n- The missing arrow in Row 2 should be one that logically completes this pattern, which is \"Up.\"\n\nTherefore, the arrow in option G fits the pattern and should replace the question mark."}
{"layout": 1223, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 83, "bbox": [117, 610, 155, 620.75], "page_size": [612.0, 792.0]}
{"layout": 1224, "type": "text", "text": "The pattern in the first two rows is that the figures are rotated 90 degrees clockwise in each subsequent cell.  Therefore, the missing figure in the third row should be a rotated version of the figure in the previous cell.  Looking at the options, the correct answer is OC, as it is the figure that is rotated 90 degrees clockwise from the  figure in the previous cell. ", "page_idx": 83, "bbox": [118, 622.7305908203125, 488.2864990234375, 657.9765014648438], "page_size": [612.0, 792.0]}
{"layout": 1225, "type": "text", "text": "Figure 62: Example questions from the Raven’s Progressive Matrices [ 109 ,  55 ]. We challenge GPT-4V by sending the entire question page as a single image, mimicking how humans look at the IQ tests. Check Section  7.4  for detailed discussions. ", "page_idx": 83, "bbox": [108, 670.5555419921875, 503, 704.3784790039062], "page_size": [612.0, 792.0]}
{"layout": 1226, "type": "text", "text": "Sec. 7.4 Raven's Progressive Matrices ", "text_level": 1, "page_idx": 84, "bbox": [134, 78, 371, 96], "page_size": [612.0, 792.0]}
{"layout": 1227, "type": "text", "text": "RPM: Processed Sub-figures ", "page_idx": 84, "bbox": [118, 102.62871551513672, 250.40553283691406, 113.36109161376953], "page_size": [612.0, 792.0]}
{"layout": 1228, "type": "text", "text": "Prompt: ", "page_idx": 84, "bbox": [118, 118.76780700683594, 154.92831420898438, 129.50018310546875], "page_size": [612.0, 792.0]}
{"layout": 1229, "type": "image", "page_idx": 84, "img_path": "layout_images/2309.17421v2_213.jpg", "bbox": [117, 139, 156, 263], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows three panels with different geometric symbols:\n\n1. An inverted triangle in a box.\n2. A diamond shape in a box.\n3. An inverted triangle overlapped by a diagonal line in a box."}
{"layout": 1230, "type": "text", "text": "Here are three sub images arranged in a 2-by-2  matrix. Figure out which figure of a to f that should  logically replace the missing figure. What would be  the next image to complete the matrix? ", "page_idx": 84, "bbox": [118, 273.63751220703125, 288, 308.8833923339844], "page_size": [612.0, 792.0]}
{"layout": 1231, "type": "image", "page_idx": 84, "img_path": "layout_images/2309.17421v2_214.jpg", "bbox": [118, 313, 157, 575], "page_size": [612.0, 792.0], "ocr_text": "[Be] [Sl INT IB] fo} [>\n", "vlm_text": "The image shows six individual boxes, each containing a unique red geometric figure against a light yellow background and outlined with a blue border. The boxes are labeled with letters from a to f:\n\na. A complex overlapping pattern consisting of multiple interlaced lines forming a symmetrical shape.\nb. A diamond-like shape with an additional intersecting line inside it.\nc. A simple diagonal line running from the bottom left to the top right.\nd. A shape resembling a triangle with additional crossing lines.\ne. A symmetrical diamond shape with its sides equal and centered.\nf. An inverted triangle or \"V\" shape.\n\nEach distinct shape appears to be drawn in a consistent red color."}
{"layout": 1232, "type": "text", "text": "The first image in the first column, image 1, is a  triangle. The first image in the second column,  image 3, is a triangle with a line through it. The  second image in the first column, image 2, is a  diamond. Therefore, the second image in the  second column, image 4 should continue this  pattern and be a diamond with a line through it.  Option b is the only image that fits this pattern, as it  is a diamond with a line through it. Therefore, the  correct answer is b. ", "page_idx": 84, "bbox": [118, 578.4866943359375, 288, 667.5296630859375], "page_size": [612.0, 792.0]}
{"layout": 1233, "type": "text", "text": "Prompt Continued ", "text_level": 1, "page_idx": 84, "bbox": [321, 103, 384, 113], "page_size": [612.0, 792.0]}
{"layout": 1234, "type": "image", "page_idx": 84, "img_path": "layout_images/2309.17421v2_215.jpg", "bbox": [322, 124, 361, 259], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image contains a vertical sequence of three figures on a light blue background, each enclosed in a blue border:\n\n1. The first figure is a yellow diamond shape.\n2. The second figure is a yellow star-like shape with four pointed ends.\n3. The third figure is a yellow diamond shape with a small yellow dot in its center.\n\nEach figure is enumerated from 1 to 3 on the left side."}
{"layout": 1235, "type": "text", "text": "Here are three sub images arranged in a 2-by-2  matrix. Figure out which figure of a to f that should  logically replace the missing figure. What would be  the next image to complete the matrix? ", "page_idx": 84, "bbox": [322, 262.1095886230469, 494, 297.35546875], "page_size": [612.0, 792.0]}
{"layout": 1236, "type": "image", "page_idx": 84, "img_path": "layout_images/2309.17421v2_216.jpg", "bbox": [322, 299, 362, 571], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows six boxes labeled a to f, each containing a geometric design on a light blue background. Here's a description of each:\n\na. A complex overlapping diamond pattern.\nb. A single diamond shape.\nc. A diamond shape with a small square in the center.\nd. A single dot in the center.\ne. A diamond with a small diamond in the center.\nf. A star shape with an outline."}
{"layout": 1237, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 84, "bbox": [322, 576, 360, 585], "page_size": [612.0, 792.0]}
{"layout": 1238, "type": "text", "text": "The first image in the first column, image 1, is a  diamond. The first image in the second column,  image 3, is a diamond with a dot in the middle. The  second image in the first column, image 2, is a star.  Therefore, the second image in the second column,  image 4 should continue this pattern and be a star  with a dot in the middle. Option c is the only image  that fits this pattern, as it is a star with a dot in the  middle. Therefore, the correct answer is c. ", "page_idx": 84, "bbox": [322, 587.452880859375, 494, 667.5296630859375], "page_size": [612.0, 792.0]}
{"layout": 1239, "type": "image", "page_idx": 85, "img_path": "layout_images/2309.17421v2_217.jpg", "img_caption": "Figure 64: GPT-4V can reliably identify and read the emotions of people from their facial expressions. Check Section  8.1  for detailed discussions. ", "bbox": [107, 110, 505, 572], "page_size": [612.0, 792.0], "ocr_text": "Sec. 8.1 Read Emotion from Facial Expressions\n\nIdentify and read emotions of people from their faces\nas shown in the image below.\n\nIdentify and read emotions of people from their faces\nas shown in the image below.\n\nThe person in the image appears to be displaying a\ncombination of emotions. The furrowed brows and\ndownturned mouth suggest anger or frustration, while\nthe tense jaw and narrowed eyes could indicate\ndisgust or contempt. Overall, it seems like the person\nis experiencing negative emotions and is likely upset or\nunhappy.\n\nIdentify and read emotions of people from their faces\nas shown in the image below.\n\nThe person in the image is displaying a happy emotion.\nThis can be seen from the wide smile and the crinkles\naround the eyes.\n\nThe emotion shown in the image is fear or anxiety. The\nperson's eyes are wide open and their mouth is slightly\nopen, indicating a sense of shock or surprise. Their\nhands are covering their mouth, which could suggest\nthat they are trying to suppress a scream or gasp.\n\nIdentify and read emotions of people from their faces\nas shown in the image below.\n\nThe person in the image appears to be experiencing a\nnegative emotion, likely frustration or disappointment.\nThis is evident from the furrowed brows, downward\ngaze, and hand on the forehead.\n", "vlm_text": "The image contains a series of prompts and responses related to identifying emotions from facial expressions. There are four different images, each depicting a person's face with a specific expression:\n\n1. **Top Left:**\n   - **Prompt:** Identify and read emotions from the face shown.\n   - **Response:** The expression suggests a combination of anger, frustration, disgust, or contempt, indicating the person is upset or unhappy.\n\n2. **Top Right:**\n   - **Prompt:** Identify and read emotions from the face shown.\n   - **Response:** The expression indicates fear or anxiety, with wide eyes and a slightly open mouth suggesting shock or surprise.\n\n3. **Bottom Left:**\n   - **Prompt:** Identify and read emotions from the face shown.\n   - **Response:** The person is displaying happiness, evident from a wide smile and crinkles around the eyes.\n\n4. **Bottom Right:**\n   - **Prompt:** Identify and read emotions from the face shown.\n   - **Response:** The expression shows frustration or disappointment, with furrowed brows, a downward gaze, and a hand on the forehead."}
{"layout": 1240, "type": "text", "text": "When interacting with humans, it is important that GPT-4V has the empathy and Emotional Quotient (EQ) to understand and share the feelings of humans. Inspired by the definition of the human EQ test [ 92 ,  91 ,  21 ], we examine GPT-4V’s capability in (1) identifying and reading human emotions from their facial expressions, (2) understanding how different visual contents may arouse emotions, and (3) generating proper text outputs conditioned on the desired emotional and sentiment. ", "page_idx": 85, "bbox": [107, 590.6615600585938, 505, 646.302490234375], "page_size": [612.0, 792.0]}
{"layout": 1241, "type": "text", "text": "8.1 Read Emotion from Facial Expressions ", "text_level": 1, "page_idx": 85, "bbox": [106, 665, 298, 678], "page_size": [612.0, 792.0]}
{"layout": 1242, "type": "text", "text": "As shown in Figure  64 , GPT-4V can reliably identify and read the emotions of people from their facial expressions. It also provides reasonable rationales for the visual cues observed to make the emotion interpretation, indicating a good understanding of the facial emotions. ", "page_idx": 85, "bbox": [107, 688.9765625, 505, 722.7994995117188], "page_size": [612.0, 792.0]}
{"layout": 1243, "type": "text", "text": "Sec. 8.2 How Visual Content Arouses Emotions ", "text_level": 1, "page_idx": 86, "bbox": [125, 53, 394, 72], "page_size": [612.0, 792.0]}
{"layout": 1244, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 86, "bbox": [120, 79, 157, 89], "page_size": [612.0, 792.0]}
{"layout": 1245, "type": "text", "text": "After seeing the below image, how people might  emotionally feel and react? ", "page_idx": 86, "bbox": [121, 90.61992645263672, 282, 107.90980529785156], "page_size": [612.0, 792.0]}
{"layout": 1246, "type": "image", "page_idx": 86, "img_path": "layout_images/2309.17421v2_218.jpg", "bbox": [121, 111, 222, 261], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image depicts a couple embracing at an outdoor event or festival. There are flags and banners, and a stage in the background with lights and possibly performance equipment. The audience is gathered in the foreground, seated or standing, indicating a live music or cultural event."}
{"layout": 1247, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 86, "bbox": [120, 269, 158, 279], "page_size": [612.0, 792.0]}
{"layout": 1248, "type": "text", "text": "People might feel happy, content, and relaxed when  seeing this image. They might react with a smile or a  feeling of warmth and comfort. This image appears to  be of a couple at a music festival, so people might also  feel a sense of excitement and anticipation for the  music and performances. ", "page_idx": 86, "bbox": [121, 281.2100830078125, 304, 334.3154602050781], "page_size": [612.0, 792.0]}
{"layout": 1249, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 86, "bbox": [120, 349, 158, 358], "page_size": [612.0, 792.0]}
{"layout": 1250, "type": "text", "text": "After seeing the below image, how people might  emotionally feel and react? ", "page_idx": 86, "bbox": [121, 359.5263977050781, 282, 376.8162536621094], "page_size": [612.0, 792.0]}
{"layout": 1251, "type": "image", "page_idx": 86, "img_path": "layout_images/2309.17421v2_219.jpg", "bbox": [122, 382, 267, 478], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows a scenic view of a coastal landscape during sunset. The foreground features rocks and patches of green seaweed or moss, with shallow pools of water reflecting the sky. In the background, the sun is setting on the horizon, casting a warm glow across the sky with colorful clouds. The overall atmosphere is serene and picturesque."}
{"layout": 1252, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 86, "bbox": [120, 484, 158, 494], "page_size": [612.0, 792.0]}
{"layout": 1253, "type": "text", "text": "People might feel a sense of awe and wonder at the  beauty of the sunset and the colors in the sky. They  may feel peaceful and calm, or even inspired. Some  may react with a sense of gratitude for being able to  witness such a beautiful moment in nature. Others may  feel a sense of longing to be in a place like this, or a  desire to travel and see more of the world's beauty. ", "page_idx": 86, "bbox": [121, 496.3930969238281, 304, 558.4525146484375], "page_size": [612.0, 792.0]}
{"layout": 1254, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 86, "bbox": [314, 79, 352, 89], "page_size": [612.0, 792.0]}
{"layout": 1255, "type": "text", "text": "After seeing the below image, how people might  emotionally feel and react? ", "page_idx": 86, "bbox": [314, 90.61992645263672, 478, 107.90980529785156], "page_size": [612.0, 792.0]}
{"layout": 1256, "type": "image", "page_idx": 86, "img_path": "layout_images/2309.17421v2_220.jpg", "bbox": [315, 111, 415, 261], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "This image depicts a group of people, one of whom is being held back by police officers wearing reflective jackets with the \"Metropolitan Police\" label. The scene appears to involve public demonstration or protest, as suggested by the presence of banners and flags."}
{"layout": 1257, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 86, "bbox": [314, 270, 352, 279], "page_size": [612.0, 792.0]}
{"layout": 1258, "type": "text", "text": "People might feel angry, frustrated, or upset after  seeing this image. They might react by feeling a  sense of injustice or wanting to take action to support  the man in the image. Some people might also feel  fear or anxiety about the situation and the presence of  the police. ", "page_idx": 86, "bbox": [314, 281.2100830078125, 494, 334.3154602050781], "page_size": [612.0, 792.0]}
{"layout": 1259, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 86, "bbox": [314, 349, 352, 358], "page_size": [612.0, 792.0]}
{"layout": 1260, "type": "text", "text": "After seeing the below image, how people might  emotionally feel and react? ", "page_idx": 86, "bbox": [314, 359.5263977050781, 478, 376.8162536621094], "page_size": [612.0, 792.0]}
{"layout": 1261, "type": "image", "page_idx": 86, "img_path": "layout_images/2309.17421v2_221.jpg", "bbox": [316, 381, 461, 478], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image depicts a dark, possibly haunted staircase. The shadowy, low-lit environment creates an eerie, mysterious atmosphere, with a focus on the railing and the wall where light and shadow interact to form intriguing shapes. The use of black and white adds to the overall haunting and dramatic feel of the image."}
{"layout": 1262, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 86, "bbox": [314, 485, 352, 494], "page_size": [612.0, 792.0]}
{"layout": 1263, "type": "text", "text": "People might feel scared, anxious, or uneasy after  seeing this image. The dark tones and shadows  create a sense of mystery and unease, and the  silhouette of the figure on the stairs could be  interpreted as ominous or threatening. Some people  might react by feeling a sense of dread or fear, while  others might be intrigued or curious about the context  of the image. ", "page_idx": 86, "bbox": [314, 496.3930969238281, 494, 567.4063720703125], "page_size": [612.0, 792.0]}
{"layout": 1264, "type": "text", "text": "Figure 65: GPT-4V understands how different visual contents may arouse human emotions. Check Section  8.2  for detailed discussions. ", "page_idx": 86, "bbox": [107, 582.9905395507812, 504, 605.9044799804688], "page_size": [612.0, 792.0]}
{"layout": 1265, "type": "text", "text": "8.2 Understand How Visual Content Arouses Emotions ", "text_level": 1, "page_idx": 86, "bbox": [106, 637, 351, 649], "page_size": [612.0, 792.0]}
{"layout": 1266, "type": "text", "text": "We next analyze GPT-4V’s ability on visual sentiment analysis,  i.e ., understanding humans’ emotional response after seeing the visual contents. Such ability is critical for GPT-4V to anticipate how visual contents may arouse human emotions and thereby react properly. As shown in Figure  65 , GPT-4V can interpret visual sentiments such as content, anger, awe, and fear, based on both the semantic contents and the image style. These capabilities are essential in use cases such as home robots. ", "page_idx": 86, "bbox": [107, 667.1585693359375, 504, 722.7994995117188], "page_size": [612.0, 792.0]}
{"layout": 1267, "type": "text", "text": "Sec. 8.2 How Visual Content Arouses Emotions ", "text_level": 1, "page_idx": 87, "bbox": [124, 132, 395, 153], "page_size": [612.0, 792.0]}
{"layout": 1268, "type": "text", "text": "Image Aesthetics ", "text_level": 1, "page_idx": 87, "bbox": [120, 158, 201, 169], "page_size": [612.0, 792.0]}
{"layout": 1269, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 87, "bbox": [120, 174, 158, 184], "page_size": [612.0, 792.0]}
{"layout": 1270, "type": "text", "text": "Aesthetics reflects the perceived beauty or visual  appeal of an object, design, or piece of art by human. Among the images provided below, based on societal  standards and norms, which one more people will  consider having a higher aesthetics? ", "page_idx": 87, "bbox": [121, 185.74609375, 298, 229.95811462402344], "page_size": [612.0, 792.0]}
{"layout": 1271, "type": "image", "page_idx": 87, "img_path": "layout_images/2309.17421v2_222.jpg", "bbox": [120, 235, 287, 486], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows two different buildings. The top image is of a modern, monochromatic house with a flat roof, and the bottom image depicts colorful buildings with vibrant facades in pink, yellow, and orange, featuring traditional-style windows and doors."}
{"layout": 1272, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 87, "bbox": [120, 493, 158, 504], "page_size": [612.0, 792.0]}
{"layout": 1273, "type": "text", "text": "Based on societal standards and norms, more people  will likely consider the second image (the colorful  houses) as having a higher aesthetics. This is  because the image is more visually appealing and  has a more artistic and creative feel to it. The first  image (the house) is more plain and ordinary, and  while it may be considered aesthetically pleasing to  some, it is not as striking or eye-catching as the  second image. ", "page_idx": 87, "bbox": [121, 505.9659729003906, 298, 586.0425415039062], "page_size": [612.0, 792.0]}
{"layout": 1274, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 87, "bbox": [314, 174, 352, 184], "page_size": [612.0, 792.0]}
{"layout": 1275, "type": "text", "text": "Aesthetics reflects the perceived beauty or visual  appeal of an object, design, or piece of art by human. Among the images provided below, based on societal  standards and norms, which one more people will  consider having a higher aesthetics? ", "page_idx": 87, "bbox": [315, 185.74609375, 493, 229.95811462402344], "page_size": [612.0, 792.0]}
{"layout": 1276, "type": "image", "page_idx": 87, "img_path": "layout_images/2309.17421v2_223.jpg", "bbox": [315, 233, 482, 479], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image consists of two separate photographs placed one above the other.\n\nThe top photograph showcases a view of New York City, featuring a part of Central Park with its lush green trees on the left side and a row of high-rise buildings along the edge of the park on the right. The sky is partly cloudy, and the scene captures the contrast between the greenery of the park and the urban landscape of the city.\n\nThe bottom photograph displays a cityscape of Vancouver, with an expansive view of roads, buildings, and a distant view of water and mountains. The image includes a clear blue sky above, and prominent features include a stadium with a white dome roof on the left and several bridges spanning over roads below. The combination of natural and urban elements is typical of Vancouver's landscapes."}
{"layout": 1277, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 87, "bbox": [314, 490, 352, 500], "page_size": [612.0, 792.0]}
{"layout": 1278, "type": "text", "text": "Most people would likely consider the first image to  have a higher aesthetic value. This is because the  image is more visually appealing, with a variety of  colors, buildings, and greenery. The second image is  less visually appealing, with a more monochromatic  color scheme and less variety in the buildings and  landscape. Additionally, the first image is of a more  iconic and recognizable location (New York City), which  may also contribute to its perceived aesthetic value. ", "page_idx": 87, "bbox": [315, 502.12335205078125, 498, 582.1998901367188], "page_size": [612.0, 792.0]}
{"layout": 1279, "type": "text", "text": "Figure 66: GPT-4V judges image aesthetics based on societal standards and norms. Check Section  8.2 for detailed discussions. ", "page_idx": 87, "bbox": [108, 600.4365234375, 504.0014343261719, 623.3505249023438], "page_size": [612.0, 792.0]}
{"layout": 1280, "type": "text", "text": "Sec. 8.3 Emotion Conditioned Output ", "text_level": 1, "page_idx": 88, "bbox": [143, 76, 377, 91], "page_size": [612.0, 792.0]}
{"layout": 1281, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 88, "bbox": [120, 100, 158, 109.75], "page_size": [612.0, 792.0]}
{"layout": 1282, "type": "text", "text": "Describe the image in a humorous way. ", "page_idx": 88, "bbox": [121, 110.60016632080078, 251.8223419189453, 118.94757080078125], "page_size": [612.0, 792.0]}
{"layout": 1283, "type": "image", "page_idx": 88, "img_path": "layout_images/2309.17421v2_224.jpg", "bbox": [121, 125, 278, 230], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows a scenic landscape of a coastal area at sunset. The foreground features a rocky shoreline with patches of seaweed or algae, and shallow pools of water reflecting the sky. In the background, the sun is setting on the horizon, casting a vibrant orange and pink glow across the sky and water. The overall mood is serene and tranquil, capturing the natural beauty of a seaside sunset. The name \"Stevie C\" is inscribed in the lower-left corner, likely indicating the photographer or artist."}
{"layout": 1284, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 88, "bbox": [120, 236, 158, 245], "page_size": [612.0, 792.0]}
{"layout": 1285, "type": "text", "text": "It looks like the sun is trying to hide behind the  horizon, but it's not doing a very good job. It's like it's  playing peek-a-boo with us. And the seaweed is just  chilling there, like \"Hey, don't mind us, we're just  hanging out.\" The rocks are probably feeling left out,  they're like \"Hey, what about us? We're part of this  beach too! ", "page_idx": 88, "bbox": [121, 247.6543426513672, 295, 309.7987365722656], "page_size": [612.0, 792.0]}
{"layout": 1286, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 88, "bbox": [120, 330, 158, 340], "page_size": [612.0, 792.0]}
{"layout": 1287, "type": "text", "text": "Describe the image to lessen someone's desire to  visit. ", "page_idx": 88, "bbox": [121, 341.4481201171875, 289, 358.76165771484375], "page_size": [612.0, 792.0]}
{"layout": 1288, "type": "image", "page_idx": 88, "img_path": "layout_images/2309.17421v2_225.jpg", "bbox": [122, 363, 278, 468], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image displays a scenic view of a coastal landscape during sunset or sunrise. In the foreground, there is a rocky shoreline with pools of water and patches of green moss or algae. In the background, the sun is setting or rising near the horizon, casting a warm glow over the sky and creating a reflection on the water's surface. The sky is filled with clouds, adding texture and color to the scene. On the horizon, distant landforms or hills are visible. Overall, the image captures the natural beauty of a coastal environment at dusk or dawn."}
{"layout": 1289, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 88, "bbox": [120, 475, 158, 483], "page_size": [612.0, 792.0]}
{"layout": 1290, "type": "text", "text": "The image shows a rocky beach at sunset with  seaweed and algae covering the ground. The water  is murky and there are no clear areas to swim or  relax. The rocks are sharp and slippery, making it  difficult to walk or explore. The sunset is partially  blocked by clouds. It seems like a difficult and  unpleasant place to navigate. ", "page_idx": 88, "bbox": [121, 486.1875915527344, 295, 548.3319702148438], "page_size": [612.0, 792.0]}
{"layout": 1291, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 88, "bbox": [314, 100, 352, 109], "page_size": [612.0, 792.0]}
{"layout": 1292, "type": "text", "text": "Describe the image to make people feel uneasy and  anxious. ", "page_idx": 88, "bbox": [314, 110.59977722167969, 489, 127.91333770751953], "page_size": [612.0, 792.0]}
{"layout": 1293, "type": "image", "page_idx": 88, "img_path": "layout_images/2309.17421v2_226.jpg", "bbox": [316, 130, 466, 231], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image appears to be a dimly lit staircase with ornate balusters. There is a prominent shadow on the wall, possibly cast by the railing or another object. The lighting creates a mysterious and somewhat eerie atmosphere."}
{"layout": 1294, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 88, "bbox": [314, 236, 351, 245], "page_size": [612.0, 792.0]}
{"layout": 1295, "type": "text", "text": "The image is of a dark, shadowy staircase. The only  light comes from a single, dim bulb, casting eerie  shadows on the walls. The railing is old and rusted,  with peeling paint and jagged edges. The stairs  themselves are uneven and creak underfoot. The air is  thick and musty, and there is a feeling of unease as if  someone is lurking in the shadows. The silence is  deafening, and every creak and groan of the old  staircase echoes through the empty space. ", "page_idx": 88, "bbox": [314, 247.65394592285156, 498, 327.73065185546875], "page_size": [612.0, 792.0]}
{"layout": 1296, "type": "text", "text": "", "text_level": 1, "page_idx": 88, "bbox": [314, 346, 352, 351.75], "page_size": [612.0, 792.0]}
{"layout": 1297, "type": "text", "text": "Describe the image to make people feel relax and  comforting. ", "page_idx": 88, "bbox": [314, 357.4591369628906, 489, 374.7726745605469], "page_size": [612.0, 792.0]}
{"layout": 1298, "type": "image", "page_idx": 88, "img_path": "layout_images/2309.17421v2_227.jpg", "bbox": [316, 377, 466, 477], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "This is a black and white image of a staircase with a shadow cast on the wall. The design of the railing is visible, creating an interesting pattern in the shadows. The overall atmosphere is dark and somewhat mysterious."}
{"layout": 1299, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 88, "bbox": [315, 483, 351, 492], "page_size": [612.0, 792.0]}
{"layout": 1300, "type": "text", "text": "The image is of a dimly lit staircase with ornate railings.  The light is soft and creates a peaceful atmosphere.  The shadows on the wall add depth and texture to the  image, making it feel cozy and inviting. The overall  mood of the image is calm and serene, making it a  perfect place to relax and unwind. ", "page_idx": 88, "bbox": [314, 494.5133361816406, 498, 547.6915283203125], "page_size": [612.0, 792.0]}
{"layout": 1301, "type": "text", "text": "Figure 67: GPT-4V generates proper text based on the perceived or desired emotions, making its communication with humans comforting and effective. Check Section  8.3  for detailed discussions. ", "page_idx": 88, "bbox": [108, 564.1435546875, 503, 587.0585327148438], "page_size": [612.0, 792.0]}
{"layout": 1302, "type": "text", "text": "8.3 Emotion Conditioned Output ", "text_level": 1, "page_idx": 88, "bbox": [105, 608, 256, 620], "page_size": [612.0, 792.0]}
{"layout": 1303, "type": "text", "text": "Based on the perceived emotions, GPT-4V effectively generates proper text outputs conditioned on the desired emotion. For example, in Figure  67 , GPT-4V can follow the prompt to describe the right-side scary image in a way that makes it more horrifying or becoming comforting. This demonstrates GPT-4V’s potential to enable emotion-aware human-robot communication. ", "page_idx": 88, "bbox": [108, 628.1885375976562, 503, 672.9215087890625], "page_size": [612.0, 792.0]}
{"layout": 1304, "type": "text", "text": "9 Emerging Application Highlights ", "text_level": 1, "page_idx": 89, "bbox": [106, 71, 296, 85], "page_size": [612.0, 792.0]}
{"layout": 1305, "type": "text", "text": "In this section, we showcase a myriad of high-value application scenarios and new use cases that can be potentially enabled by the remarkable capabilities of GPT-4V. While it is true that some of these application scenarios can be accomplished by meticulously curating the training data for finetuning existing Vision and Language (VL) models, we want to emphasize that the true power of GPT-4V lies in its ability to perform effortlessly right out of the box. Moreover, we present how GPT-4V seamlessly integrates with external tools and plugins, further expanding its potential and enabling even more innovative and collaborative applications. ", "page_idx": 89, "bbox": [107, 96.757568359375, 505, 174.21751403808594], "page_size": [612.0, 792.0]}
{"layout": 1306, "type": "text", "text": "9.1 Spot the Difference ", "text_level": 1, "page_idx": 89, "bbox": [106, 188, 213, 199], "page_size": [612.0, 792.0]}
{"layout": 1307, "type": "text", "text": "We begin with a generic use case inspired by the brain-teasing game “Spot the Difference.” In Figures  68 - 69 , we provide GPT-4V two visually similar images that contain subtle differences in certain regions. The task given to GPT-4V is to identify all the differences between the two images. Among the four examples, GPT-4V successfully identifies the regions or components that differ in the images. However, it falls short in providing accurate explanations for what is depicted in each image. To delve deeper into GPT-4V’s capabilities, let’s focus on the first example shown in Figure  68 . Although GPT-4V fails to recognize that the discrepancy lies in the number of cuts in the hairband rather than the shade of the hair, it correctly identifies that the crown, the bow of the dress, and the hair differ between the two images. While GPT-4V’s predictions in the “Spot the Difference” game are not perfect, its ability to compare the content in two images proves valuable in real-life applications, such as defect detection, which we will explore in the following subsections. ", "page_idx": 89, "bbox": [107, 208.715576171875, 505, 329.8114929199219], "page_size": [612.0, 792.0]}
{"layout": 1308, "type": "text", "text": "9.2 Industry ", "text_level": 1, "page_idx": 89, "bbox": [106, 343, 168, 355], "page_size": [612.0, 792.0]}
{"layout": 1309, "type": "text", "text": "Defect detection. Throughout the history of manufacturing, computer vision techniques have played a crucial role. One specific application scenario is defect detection, which is an essential step in manufacturing processes to ensure product quality. Detecting faults or defects in a timely manner and taking appropriate actions are vital for minimizing operational and quality-related costs. ", "page_idx": 89, "bbox": [107, 363.950927734375, 505, 409.0425109863281], "page_size": [612.0, 792.0]}
{"layout": 1310, "type": "text", "text": "In this scenario, we demonstrate the defect detection capabilities of GPT-4V by presenting images of defective products in Figures  70 - 71 . For commonly encountered products in real-life (e.g., hazelnut, fabric, screw, and car bumper in Figure  70 ), GPT-4V confidently identifies the defects such as small holes in the hazelnut/fabric, stripped heads of screws, and dents in car bumpers. However, when it comes to uncommon product images (e.g., the metal parts in Figures  70 - 71 ) or products with variations in appearance (e.g., the pill in Figure  71 ), GPT-4V may hesitate or even refuse to make predictions. An interesting case in Figure  71  involves a car tire, where multiple defects can be observed in the image, including dirt on the wheel, damage to the outer edge of the rim, and signs of wear on the tire. GPT-4V only focuses on the minor defect (dirt on the wheel) and fails to mention the major defect (damage to the outer edge of the rim) that would require repair. ", "page_idx": 89, "bbox": [107, 413.4255676269531, 505, 523.6124877929688], "page_size": [612.0, 792.0]}
{"layout": 1311, "type": "text", "text": "Given the success of GPT-4V in “Spot the Difference” scenario shown in Section  9.1 , we explore the idea of incorporating a reference image to illustrate what a defect-free product should look like, with the aim of improving the failure cases depicted in Figure  71 . The results of this approach are presented in Figure  72 . By including the reference image and refining the prompt, GPT-4V successfully identifies defects in all three failure cases in single-image defect detection. These promising findings highlight a potential high-value application of GPT-4V for defect detection in the manufacturing industry. ", "page_idx": 89, "bbox": [107, 527.9955444335938, 505, 605.4555053710938], "page_size": [612.0, 792.0]}
{"layout": 1312, "type": "text", "text": "Safety inspection. Figure  73  presents an exploration of Personal Protective Equipment (PPE) counting for safety inspection. The inadequate usage or failure to wear PPE, such as helmets, harnesses, and gloves, in work environments like construction sites, significantly increases the risk level associated with work activities. To effectively address this issue, computer vision techniques have been employed as a solution to monitor PPE compliance and promptly identify any violations of safety regulations. Taking helmets as an example, a safety inspection system is necessary to accurately detect and report the number of employees who are not wearing helmets. ", "page_idx": 89, "bbox": [107, 617.6838989257812, 505, 695.5015258789062], "page_size": [612.0, 792.0]}
{"layout": 1313, "type": "text", "text": "In Figure  73a , we assess the performance of GPT-4V by directly instructing it to count the individuals wearing helmets. GPT-4V provides a response of  $^{\\ast8}$   persons wearing helmets,” which matches the total count of people shown in the image, suggesting there is no alerting safety violations. Obviously, GPT-4V fails to detect the 3 individuals who are not wearing helmets, thus compromising their personal safety. This task poses a considerable challenge for GPT-4V, as it involves detecting people in the image, determining whether they are wearing helmets, and calculating the final count of people who are not wearing the helmets. ", "page_idx": 89, "bbox": [107, 699.8855590820312, 505, 722.7994995117188], "page_size": [612.0, 792.0]}
{"layout": 1314, "type": "text", "text": "", "page_idx": 90, "bbox": [107, 72.757568359375, 505, 128.3984832763672], "page_size": [612.0, 792.0]}
{"layout": 1315, "type": "text", "text": "In Figure  73b , instead of presenting GPT-4V with the original image containing all 8 individuals, we provide cropped regions of the detected persons with an external person detector. This approach divides the PPE counting workload into two steps: relying on an off-the-shelf person detector for person detection and leveraging GPT-4V’s robust visual reasoning capabilities and its ability to handle interleaved image-text inputs for identifying the safety issues. As we can see, GPT-4V can correctly count the person who is not wearing the helmet, also demonstrating the benefit of tool use and divide-and-conquer. ", "page_idx": 90, "bbox": [107, 132.78253173828125, 505, 210.2415008544922], "page_size": [612.0, 792.0]}
{"layout": 1316, "type": "text", "text": "Grocery checkout. Self-checkout machines have become increasingly popular in major retailers like Walmart, Target and CVS to expedite the checkout process for customers and reduce the workload for employees. However, the actual experience with self-checkout machines may be frustrating for customers. Users still need to search for the product barcode or manually enter codes for fresh items like apples, which can be time-consuming, particularly for those unfamiliar with the system. In Figure  74 , we provide a simplified prototype to demonstrate the potential of GPT-4V in enabling an automatic self-checkout system that can identify and ring up items without user intervention. ", "page_idx": 90, "bbox": [107, 220.9908905029297, 505, 298.80950927734375], "page_size": [612.0, 792.0]}
{"layout": 1317, "type": "text", "text": "When presented with a photograph of a shopping basket containing five grocery items, as shown in Figure  74a , GPT-4V fails to accurately identify the products within the basket. It mistakenly identifies strawberries as raspberries, crab dip as Greek yogurt, and includes salmon fillets that are not even present in the basket. However, in Figure  74b , we improve the prompt by augmenting it with catalog images of grocery products retrieved from the retail website. As a result, GPT-4V successfully identifies all five items in the basket. This successful demonstration allows the self-checkout system to proceed with retrieving the prices for each identified product from the database. While this is a simple example, it represents a significant step forward toward an automated self-checkout system. Further research and development can explore more complex and realistic scenarios to fully automate the self-checkout process, making it more efficient and convenient for customers. ", "page_idx": 90, "bbox": [107, 303.19256591796875, 505, 413.3794860839844], "page_size": [612.0, 792.0]}
{"layout": 1318, "type": "text", "text": "Sec. 9.1 Spot the Difference ", "text_level": 1, "page_idx": 91, "bbox": [135, 34, 299, 49], "page_size": [612.0, 792.0]}
{"layout": 1319, "type": "text", "text": "Prompt: ", "page_idx": 91, "bbox": [122, 55.972267150878906, 159.15509033203125, 66.70464324951172], "page_size": [612.0, 792.0]}
{"layout": 1320, "type": "text", "text": "Compare Image 1 and image 2, tell me about the differences between image 1 and image 2. ", "page_idx": 91, "bbox": [122, 67.38335418701172, 427.2987365722656, 75.73075866699219], "page_size": [612.0, 792.0]}
{"layout": 1321, "type": "text", "text": "Image 1 ", "page_idx": 91, "bbox": [122, 85.315673828125, 149.3435516357422, 93.66307830810547], "page_size": [612.0, 792.0]}
{"layout": 1322, "type": "image", "page_idx": 91, "img_path": "layout_images/2309.17421v2_228.jpg", "bbox": [123, 92.25, 216, 196], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image is an illustration of a cartoon princess. She is wearing a pink dress with a red bow, a heart-shaped necklace, and a pink crown. She has brown hair and is shown in a simplistic, cute style."}
{"layout": 1323, "type": "image", "page_idx": 91, "img_path": "layout_images/2309.17421v2_229.jpg", "bbox": [123, 208, 216, 313], "page_size": [612.0, 792.0], "ocr_text": "eed\n", "vlm_text": "This image shows an illustration of a girl wearing a pink dress and a red crown with a cross on top. She has light brown hair and is smiling."}
{"layout": 1324, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 91, "bbox": [122, 317, 159, 327], "page_size": [612.0, 792.0]}
{"layout": 1325, "type": "text", "text": "There are a few differences between image 1 and image 2: 1. The crown: in image 1, the crown is pink with a blue gem in the center. In image 2, the crown is gold with  red gems and a cross on top. 2. The dress: in image 1 the dress has a red bow in the center. In image 2, the bow is missing. 3. The hair: in image 1, the hair is a lighter shade of brown compared to image 2.   ", "page_idx": 91, "bbox": [122, 329.4981994628906, 488.4884948730469, 373.710205078125], "page_size": [612.0, 792.0]}
{"layout": 1326, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 91, "bbox": [121, 386, 159, 395.75], "page_size": [612.0, 792.0]}
{"layout": 1327, "type": "text", "text": "Compare Image 1 and image 2, tell me about 3 differences between image 1 and image 2. ", "page_idx": 91, "bbox": [122, 397.2099609375, 421.0747375488281, 405.5573425292969], "page_size": [612.0, 792.0]}
{"layout": 1328, "type": "image", "page_idx": 91, "img_path": "layout_images/2309.17421v2_230.jpg", "bbox": [123, 424, 276, 527], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image is a colorful cartoon illustration depicting an underwater scene. It features a fish and a turtle swimming among coral. The coral is in various bright colors like pink, red, purple, yellow, and blue. The background is a light blue with white bubbles, representing water."}
{"layout": 1329, "type": "image", "page_idx": 91, "img_path": "layout_images/2309.17421v2_231.jpg", "bbox": [123, 540, 276, 644], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image is a colorful illustration of an underwater scene. It features a green turtle with yellow spots swimming alongside a yellow fish. There are coral reefs in various colors, including pink, red, purple, and orange, with a few green aquatic plants. The background is blue with white bubbles, suggesting water."}
{"layout": 1330, "type": "text", "text": "1) In image 1, there are two blue coral reefs while in Image 2, there is only one blue coral reef. 2) In image 2, there is an additional green plant that is not present in image 1. 3) In image 1, the turtle’s head is facing downwards. In image 2 the turtle’s head is facing upwards. ", "page_idx": 91, "bbox": [122, 659.3247680664062, 458.7882080078125, 685.6044921875], "page_size": [612.0, 792.0]}
{"layout": 1331, "type": "text", "text": "Figure 68: Spot the differences.  Red  highlights the inaccurate description about the differences. Check Section  9.1  for detailed discussions. ", "page_idx": 91, "bbox": [108, 701.45556640625, 505.7466735839844, 724.3695068359375], "page_size": [612.0, 792.0]}
{"layout": 1332, "type": "text", "text": "Sec. 9.1 Spot the Difference ", "text_level": 1, "page_idx": 92, "bbox": [124, 31, 318, 52], "page_size": [612.0, 792.0]}
{"layout": 1333, "type": "text", "text": "Prompt: ", "page_idx": 92, "bbox": [122, 55.972267150878906, 159, 66.70464324951172], "page_size": [612.0, 792.0]}
{"layout": 1334, "type": "text", "text": "Compare Image 1 and image 2, tell me about the 3 differences between image 1 and image 2. ", "page_idx": 92, "bbox": [122, 67.38335418701172, 433.522705078125, 75.73075866699219], "page_size": [612.0, 792.0]}
{"layout": 1335, "type": "text", "text": "Image 1 ", "page_idx": 92, "bbox": [122, 85.315673828125, 149, 93.66307830810547], "page_size": [612.0, 792.0]}
{"layout": 1336, "type": "image", "page_idx": 92, "img_path": "layout_images/2309.17421v2_232.jpg", "bbox": [122, 103, 176, 317], "page_size": [612.0, 792.0], "ocr_text": "Image 2\n\n", "vlm_text": "The image shows two cartoon illustrations of a panda. In both illustrations, the panda is sitting and eating bamboo. The top image shows the panda with a leafy bamboo branch, while the bottom image includes a small brown object near the panda and a sun in the background."}
{"layout": 1337, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 92, "bbox": [122, 318, 159, 327], "page_size": [612.0, 792.0]}
{"layout": 1338, "type": "text", "text": "1) In image 1, there is no sun in the background, while in image 2, there is a sun in the top left corner. 2) In image 1, there is a single bamboo leaf on the ground, while in image 2, there is a pile of bamboo  leaves on the ground. 3) In image 1, the panda is holding a single bamboo stalk, while in image 2, the panda is holding multiple  bamboo stalks. ", "page_idx": 92, "bbox": [122, 329.4981994628906, 480.3207702636719, 373.710205078125], "page_size": [612.0, 792.0]}
{"layout": 1339, "type": "text", "text": "Prompt: Compare Image 1 and image 2, tell me about 3 differences between image 1 and image 2. ", "page_idx": 92, "bbox": [122, 385.7988586425781, 421.0747375488281, 405.5573425292969], "page_size": [612.0, 792.0]}
{"layout": 1340, "type": "text", "text": "Image 1 ", "text_level": 1, "page_idx": 92, "bbox": [121, 415, 150, 421.75], "page_size": [612.0, 792.0]}
{"layout": 1341, "type": "image", "page_idx": 92, "img_path": "layout_images/2309.17421v2_233.jpg", "bbox": [122, 422.25, 244, 525], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image is a simple, cartoon-style drawing of a dinosaur. It features an oval body, a circular head with a long neck, spiked plates along its back, and four legs with spiky feet, resembling a stegosaurus."}
{"layout": 1342, "type": "text", "text": "Image 2 ", "page_idx": 92, "bbox": [122, 529.781005859375, 149, 538.12841796875], "page_size": [612.0, 792.0]}
{"layout": 1343, "type": "image", "page_idx": 92, "img_path": "layout_images/2309.17421v2_234.jpg", "bbox": [122, 540, 245, 643], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image is a simple, abstract line drawing of a turtle. It features a large oval for the turtle's shell, a smaller circle with a face at the front representing the turtle's head, four semicircles with points representing the turtle's legs and feet, and a pointed tail at the back. The turtle appears to have a closed eye on its face, suggesting it might be sleeping or simply styled with a relaxed, peaceful expression."}
{"layout": 1344, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 92, "bbox": [122, 648, 158, 657], "page_size": [612.0, 792.0]}
{"layout": 1345, "type": "text", "text": "1) Image 1 has spikes on its back while image 2 does not. 2) Image 2 has a tail while image 1 does not. 3) Image 2 has a mouth with teeth while image 1 does not have a mouth ", "page_idx": 92, "bbox": [122, 659.3247680664062, 371.5341796875, 685.6044921875], "page_size": [612.0, 792.0]}
{"layout": 1346, "type": "text", "text": "Figure 69: Spot the differences.  Red  highlights the inaccurate description about the differences. Check Section  9.1  for detailed discussions. ", "page_idx": 92, "bbox": [108, 701.45556640625, 505.7466735839844, 724.3695068359375], "page_size": [612.0, 792.0]}
{"layout": 1347, "type": "text", "text": "Sec. 9.2 Industry ", "text_level": 1, "page_idx": 93, "bbox": [140, 75, 239, 92], "page_size": [612.0, 792.0]}
{"layout": 1348, "type": "text", "text": "Defect Detection ", "text_level": 1, "page_idx": 93, "bbox": [121, 97, 199, 109], "page_size": [612.0, 792.0]}
{"layout": 1349, "type": "text", "text": "Prompt: What is wrong with the object in the image? ", "page_idx": 93, "bbox": [122, 114.50170135498047, 268, 134.26019287109375], "page_size": [612.0, 792.0]}
{"layout": 1350, "type": "image", "page_idx": 93, "img_path": "layout_images/2309.17421v2_235.jpg", "bbox": [123, 138, 225, 241], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows a hazelnut with a small hole in it."}
{"layout": 1351, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 93, "bbox": [122, 245, 159, 254.75], "page_size": [612.0, 792.0]}
{"layout": 1352, "type": "text", "text": "There is a small hole in the object, which appears  to be a hazelnut. This could be a sign of insect  damage or rot.  ", "page_idx": 93, "bbox": [122, 256.7375793457031, 287, 283.01727294921875], "page_size": [612.0, 792.0]}
{"layout": 1353, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 93, "bbox": [121, 291, 159, 300.75], "page_size": [612.0, 792.0]}
{"layout": 1354, "type": "text", "text": "What is wrong with the object in the image? ", "page_idx": 93, "bbox": [122, 302.0338134765625, 268, 310.3811950683594], "page_size": [612.0, 792.0]}
{"layout": 1355, "type": "image", "page_idx": 93, "img_path": "layout_images/2309.17421v2_236.jpg", "bbox": [124, 314, 223, 416], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows the head of a Phillips screw. It's a metal screw with a cross-shaped slot, commonly used for fastening materials."}
{"layout": 1356, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 93, "bbox": [122, 421, 159, 431], "page_size": [612.0, 792.0]}
{"layout": 1357, "type": "text", "text": "The object in the image appears to be a screw or  bolt with a damaged or stripped head. The  grooves in the head, which would normally allow  a screwdriver or wrench to grip and turn the  screw, are worn away and no longer functional.  ", "page_idx": 93, "bbox": [122, 432.85858154296875, 287, 477.07061767578125], "page_size": [612.0, 792.0]}
{"layout": 1358, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 93, "bbox": [121, 486, 159, 495.75], "page_size": [612.0, 792.0]}
{"layout": 1359, "type": "text", "text": "What is wrong with the object in the image? ", "page_idx": 93, "bbox": [122, 497.36798095703125, 268, 505.7153625488281], "page_size": [612.0, 792.0]}
{"layout": 1360, "type": "image", "page_idx": 93, "img_path": "layout_images/2309.17421v2_237.jpg", "bbox": [123, 509, 229, 613], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows a cross-section of an electrical cable consisting of three colored insulated copper wires. The wires have green, blue, and brown insulation, which are typical colors for electrical wiring indicating different functions such as grounding and conducting electricity."}
{"layout": 1361, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 93, "bbox": [122, 616, 158, 625], "page_size": [612.0, 792.0]}
{"layout": 1362, "type": "text", "text": "It is difficult to say for sure what is wrong with the  object in the image as there is no context or  information about what the object is or what it is  supposed to look like. However, it appears that  the copper wires in the blue and brown sections  are frayed and not neatly arranged, which could  indicate damage or poor quality. ", "page_idx": 93, "bbox": [122, 628.1927490234375, 287, 690.3370971679688], "page_size": [612.0, 792.0]}
{"layout": 1363, "type": "text", "text": "Prompt: What is wrong with the object in the image? ", "page_idx": 93, "bbox": [316, 114.56053161621094, 463, 134.3190155029297], "page_size": [612.0, 792.0]}
{"layout": 1364, "type": "image", "page_idx": 93, "img_path": "layout_images/2309.17421v2_238.jpg", "bbox": [318, 136, 424, 244], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows a piece of woven fabric with a grid-like texture, possibly burlap or linen. There is a small hole or imperfection in the weave."}
{"layout": 1365, "type": "text", "text": "GPT-4V: There is a small tear or hole in the fabric. ", "page_idx": 93, "bbox": [316, 245.3853302001953, 452, 265.143798828125], "page_size": [612.0, 792.0]}
{"layout": 1366, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 93, "bbox": [316, 291, 353, 300], "page_size": [612.0, 792.0]}
{"layout": 1367, "type": "text", "text": "What is wrong with the object in the image? ", "page_idx": 93, "bbox": [316, 302.0338134765625, 463, 310.3811950683594], "page_size": [612.0, 792.0]}
{"layout": 1368, "type": "image", "page_idx": 93, "img_path": "layout_images/2309.17421v2_239.jpg", "bbox": [317, 310.25, 440, 416], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows a close-up of a dent in a metallic surface. The reflection and texture indicate that it could be part of a vehicle, like a car bumper or fender."}
{"layout": 1369, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 93, "bbox": [316, 421, 353, 431], "page_size": [612.0, 792.0]}
{"layout": 1370, "type": "text", "text": "It is difficult to tell exactly what the object in the image  is, but it appears to be a car bumper or fender with a  large dent in it. The problem with the object is that it is  damaged and dented, which may affect the  appearance and functionality of the car.  ", "page_idx": 93, "bbox": [316, 432.85858154296875, 496, 477.07061767578125], "page_size": [612.0, 792.0]}
{"layout": 1371, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 93, "bbox": [316, 486, 353, 495.75], "page_size": [612.0, 792.0]}
{"layout": 1372, "type": "text", "text": "What is wrong with the object in the image? ", "page_idx": 93, "bbox": [316, 497.36798095703125, 463, 505.7153625488281], "page_size": [612.0, 792.0]}
{"layout": 1373, "type": "image", "page_idx": 93, "img_path": "layout_images/2309.17421v2_240.jpg", "bbox": [318, 509, 421, 612], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows an object that appears to be a metallic or ceramic component, possibly a cutting tool insert or a part of a machine. The object has a circular center hole and is symmetrical with four protruding edges or flanges. Each edge has a curved shape that resembles a cloverleaf or a cross. The material seems to have a glossy finish with some texturing or minor imperfections. There is also a small black mark or detail on the left side of the object."}
{"layout": 1374, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 93, "bbox": [316, 616, 353, 625], "page_size": [612.0, 792.0]}
{"layout": 1375, "type": "text", "text": "It is difficult to say for sure what is wrong with the  object in the image as there is no context or  information about what the object is or what it is  supposed to look like. However, it appears that there  may be a crack on one of the edges of the object. ", "page_idx": 93, "bbox": [316, 628.1927490234375, 492, 672.40478515625], "page_size": [612.0, 792.0]}
{"layout": 1376, "type": "text", "text": "Figure 70: Defect detection with a single image.  Yellow  highlights the cases when GPT-4V is hesitating to make the predictions. Check Section  9.2  for detailed discussions. ", "page_idx": 93, "bbox": [108, 705.548583984375, 503.9972839355469, 728.4625244140625], "page_size": [612.0, 792.0]}
{"layout": 1377, "type": "text", "text": "Sec. 9.2 Industry ", "text_level": 1, "page_idx": 94, "bbox": [140, 135, 237, 151], "page_size": [612.0, 792.0]}
{"layout": 1378, "type": "text", "text": "Defect Detection ", "text_level": 1, "page_idx": 94, "bbox": [121, 157, 199, 168], "page_size": [612.0, 792.0]}
{"layout": 1379, "type": "text", "text": "Prompt: What is wrong with the object in the image? ", "page_idx": 94, "bbox": [122, 173.27761840820312, 270, 193.03610229492188], "page_size": [612.0, 792.0]}
{"layout": 1380, "type": "image", "page_idx": 94, "img_path": "layout_images/2309.17421v2_241.jpg", "bbox": [123, 195, 229, 300], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows a capsule-shaped pill. It appears to have two distinct colored halves: one half is black and the other half is reddish-brown with a white marking that reads \"500\". This type of pill is usually associated with a specific dosage of medication, although the exact type of medication cannot be determined solely from the image. To identify the pill accurately, one should refer to a pill identifier tool or consult with a healthcare professional, using the pill's imprint, color, and shape as identifying factors."}
{"layout": 1381, "type": "text", "text": "GPT-4V: There is not enough information provided to  determine if there is anything wrong with the  object in the image. It appears to be a capsule  medication with the number 500 printed on it, but  without further context, it is impossible to know if  there is anything wrong with it.  ", "page_idx": 94, "bbox": [122, 304.1023864746094, 284, 368.691650390625], "page_size": [612.0, 792.0]}
{"layout": 1382, "type": "text", "text": "Prompt: What is wrong with the object in the image? ", "page_idx": 94, "bbox": [316, 173.27761840820312, 460.9884948730469, 193.03610229492188], "page_size": [612.0, 792.0]}
{"layout": 1383, "type": "image", "page_idx": 94, "img_path": "layout_images/2309.17421v2_242.jpg", "bbox": [317, 195, 420, 298], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows the top view of a dark, round container or bottle, likely made of glass or plastic. It's difficult to identify the exact contents without more context, but it appears to be a closed or empty opening."}
{"layout": 1384, "type": "text", "text": "GPT-4V: Sorry, I cannot answer this question as there is no  context or information about what the object is or what  it is supposed to look like.   ", "page_idx": 94, "bbox": [316, 304.1023864746094, 496, 341.7931823730469], "page_size": [612.0, 792.0]}
{"layout": 1385, "type": "text", "text": "Prompt: What is wrong with the object in the image? ", "page_idx": 94, "bbox": [122, 381.42059326171875, 266.35968017578125, 401.1790771484375], "page_size": [612.0, 792.0]}
{"layout": 1386, "type": "image", "page_idx": 94, "img_path": "layout_images/2309.17421v2_243.jpg", "bbox": [123, 405, 256, 582], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows a car wheel, specifically an alloy rim with five spokes. It appears to be somewhat dirty, with signs of wear on the rim. The tire has the writing \"GOODYEAR WRANGLER\" on it."}
{"layout": 1387, "type": "text", "text": "GPT-4V: There is large amount of brake on the rim of the wheel. This is typically caused by the brake pads wearing  down and creating dust as they come in contact with the rotors. Additionally, the tire appears to be low on air  pressure, which can affect the vehicle’s handling and fuel economy. ", "page_idx": 94, "bbox": [122, 586.3613891601562, 481.2010498046875, 624.0521850585938], "page_size": [612.0, 792.0]}
{"layout": 1388, "type": "text", "text": "Figure 71: Failure examples of defect detection with a single image.  Red  highlights the cases when GPT-4V fails. Check Section  9.2  for detailed discussions. ", "page_idx": 94, "bbox": [108, 637.8365478515625, 504.00238037109375, 660.75048828125], "page_size": [612.0, 792.0]}
{"layout": 1389, "type": "text", "text": "Sec. 9.2 Industry ", "text_level": 1, "page_idx": 95, "bbox": [142, 26, 238, 40], "page_size": [612.0, 792.0]}
{"layout": 1390, "type": "text", "text": "Defect Detection ", "text_level": 1, "page_idx": 95, "bbox": [122, 47, 199, 58], "page_size": [612.0, 792.0]}
{"layout": 1391, "type": "text", "text": "Prompt: Given Image 1, tell me what is wrong in image 2. ", "page_idx": 95, "bbox": [122, 63.47959899902344, 283.3703918457031, 83.23809814453125], "page_size": [612.0, 792.0]}
{"layout": 1392, "type": "image", "page_idx": 95, "img_path": "layout_images/2309.17421v2_244.jpg", "img_caption": "Image 1 ", "bbox": [121, 92, 229, 209], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows a capsule-shaped pill that is half black and half brown. It has the number \"500\" printed on the brown side. This marking is often a dosage indicator in milligrams for the medication within the capsule, and the pill's color and marking can be used to identify the specific medication, which in this case could be Ciprofloxacin hydrochloride with a dose of 500 mg. However, for accurate identification, one should consult a medical professional or a pill identifier resource."}
{"layout": 1393, "type": "image", "page_idx": 95, "img_path": "layout_images/2309.17421v2_245.jpg", "img_caption": "Image 2 ", "bbox": [121, 216, 229, 333], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows a capsule-shaped pill that is half black and half brownish-orange. The brownish-orange half has the number \"500\" printed on it, along with a small icon that resembles an hourglass or sand timer. This pill appears to be an antibiotic known as Ciprofloxacin hydrochloride 500 mg."}
{"layout": 1394, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 95, "bbox": [121, 338, 159, 348], "page_size": [612.0, 792.0]}
{"layout": 1395, "type": "text", "text": "In image 2, there is a small white mark on the  orange part of the pill. This could be a defect or  damage to the pil. ", "page_idx": 95, "bbox": [122, 349.8143310546875, 280.3370666503906, 376.0940246582031], "page_size": [612.0, 792.0]}
{"layout": 1396, "type": "text", "text": "Prompt: Given Image 1, tell me what is wrong in image 2. ", "page_idx": 95, "bbox": [316, 63.47959899902344, 477.8079528808594, 83.23809814453125], "page_size": [612.0, 792.0]}
{"layout": 1397, "type": "image", "page_idx": 95, "img_path": "layout_images/2309.17421v2_246.jpg", "bbox": [317, 95, 424, 207], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image appears to show the top view of a dark-colored liquid inside what seems to be a round container or bottle. The circular shape and dark center suggest it could be a bottle with a dark liquid, such as a soda or beer, seen from above. The inner black circle may indicate the neck of the bottle. The gradient and rings may be due to reflections or the lighting conditions."}
{"layout": 1398, "type": "text", "text": "Image 2 ", "page_idx": 95, "bbox": [316, 217.06837463378906, 343.78125, 225.415771484375], "page_size": [612.0, 792.0]}
{"layout": 1399, "type": "image", "page_idx": 95, "img_path": "layout_images/2309.17421v2_247.jpg", "img_caption": "GPT-4V: In image 2, there is a small white spot on the  lower left side of the inner circle. This could be a  defect or damage on the surface of the object. ", "bbox": [315, 224.25, 477, 376], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows a circular object that appears to have a small white spot on the lower left side of the inner circle. This white spot is identified as potentially being a defect or damage on the surface of the object. The circular object could be anything from a component part, a lens, or a piece of machinery or equipment, but the specific nature of the object is not clear just from the image. The caption accurately describes this visible imperfection."}
{"layout": 1400, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 95, "bbox": [121, 390, 159, 400], "page_size": [612.0, 792.0]}
{"layout": 1401, "type": "text", "text": "Given image 1, tell me if there are any defects  seen on the object in image 2? ", "page_idx": 95, "bbox": [122, 401.5151062011719, 276.72796630859375, 418.8286437988281], "page_size": [612.0, 792.0]}
{"layout": 1402, "type": "image", "page_idx": 95, "img_path": "layout_images/2309.17421v2_248.jpg", "img_caption": "Image 1 ", "bbox": [121, 428, 268, 631], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows a car wheel with a five-spoke alloy rim. The tire has the brand name \"WRANGLER\" visible on it. The center cap of the wheel has a logo on it. It is placed on a tiled floor."}
{"layout": 1403, "type": "text", "text": "Image 2 ", "page_idx": 95, "bbox": [316.7994384765625, 425.18511962890625, 343.78125, 433.5325012207031], "page_size": [612.0, 792.0]}
{"layout": 1404, "type": "image", "page_idx": 95, "img_path": "layout_images/2309.17421v2_249.jpg", "bbox": [317, 438, 463, 631], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows a car wheel with an alloy rim. The tire is mounted on the rim, and the word \"WRANGLER\" is visible on the tire. The center cap of the rim has a logo on it. The rim appears to have some dirt or wear marks."}
{"layout": 1405, "type": "text", "text": "GPT-4V: Yes, there are a few defects seen on the object in image 2:\n\n ", "page_idx": 95, "bbox": [122, 640.3735961914062, 316.3212585449219, 660.132080078125], "page_size": [612.0, 792.0]}
{"layout": 1406, "type": "text", "text": "- There is a significant amount of dirt and grime on the wheel, which is not present in image 1.\n\n - The center cap is missing from the wheel in image 2.\n\n - There appears to be some damage to the outer edge of the rim, which is not present in image 1.\n\n - The tire tread appears to be more worn in image 2 than in image 1. ", "page_idx": 95, "bbox": [122, 669.7169799804688, 443.8647155761719, 704.9628295898438], "page_size": [612.0, 792.0]}
{"layout": 1407, "type": "image", "page_idx": 96, "img_path": "layout_images/2309.17421v2_250.jpg", "img_caption": "Figure 73: Application Highlights on Safety Inspection: Personal Protective Equipment (PPE) Counting. GPT-4V fails with zero-shot prompting in (a), while succeeds with single person crops in (b).  Red  (  Green  ) highlights the wrong (correct) answer. Check Section  9.2  for detailed discussions. ", "bbox": [105, 2, 507, 745], "page_size": [612.0, 792.0], "ocr_text": "Sec. 9.2 Industry\n\nSafety Inspection\n\nPrompt:\nHow many people are wearing helmet?\n\nThere are\n\nSec. 9.2 Industry\n\nSafety Inspection\n\nPrompt:\n\nPlease determine whether the person in the\nimage wears a helmet or not. And summarize\nhow many people are wearing helmets.\n\n(b)\n\nPrompt Continued\n\nImage 5\n\nGPT-4V:\nImage 1: Not wearing a helmet\nImage 2: Not wearing a helmet\nImage 3: Wearing a helmet\nImage 4: Wearing a helmet\nImage 5: Wearing a helmet\nImage 6: Wearing a helmet\nImage 7: Wearing a helmet\nImage 8: Not wearing a helmet\nSummary: Out of 8 people, 5 are wearing helmets\nnd 3 are not wearing helmets.\n\n", "vlm_text": "The image is a comparison of two scenarios demonstrating the identification of Personal Protective Equipment (PPE), particularly helmets, in a safety inspection context.\n\nIn part (a), a group picture of several people is analyzed. GPT-4V initially fails to correctly count the number of people wearing helmets, incorrectly stating that 8 people are wearing them. This mistake is highlighted in red.\n\nIn part (b), individual crops of each person in the image are analyzed. Here, GPT-4V correctly identifies which individuals are wearing helmets and which are not. It summarizes that out of 8 people, 5 are wearing helmets and 3 are not, with the correct outcome highlighted in green."}
{"layout": 1408, "type": "image", "page_idx": 97, "img_path": "layout_images/2309.17421v2_251.jpg", "img_caption": "Figure 74: Application Highlights on Grocery Checkout. GPT-4V fails with zero-shot prompting in (a), while succeeds when prompting with reference product images in (b).  Red  highlights the products that are not in the basket. Check Sections  9.2 ,  10.5  for detailed discussions. ", "bbox": [107, 2, 505, 739], "page_size": [612.0, 792.0], "ocr_text": "Sec. 9.2 Industry\n\nGrocery Checkout\nPrompt:\n\nFor below image, find what products are presented in the basket. Try to be accurate\n\nPink tulips, Salmon fillets, strawberries, apple, greek yogurt, sliced ham, raspberries\n\nSec. 9.2 Industry\nGrocery Checkout\n\nPrompt:\nHere is a list of reference images:\n\nShrimp\n\nSmoked Ham\n\n=>.\n\nBanana\n\nPrompt Continued\n\nCheese\n\nPink Tulips\n\nFor below image, find what products are presented in the basket\naccording to reference image above. Only tell me products that\nare exactly same type of reference images.\n\nGPT-4V:\n\nThere are 5 products in the basket that are exactly the same\ntype as the reference images:\n\n- Smoked ham\n\n- Apple\n\n- Pink tulips\n\n- Crab dip\n\n- Strawberries\n\n(b)\n\n", "vlm_text": "The image shows a comparison of two scenarios for identifying products in a grocery basket using GPT-4V, depicted as parts (a) and (b).\n\n**(a) Zero-Shot Prompting:**\n- A prompt asks to identify products in the basket from the image without any reference.\n- GPT-4V identifies: pink tulips, salmon fillets, strawberries, apple, Greek yogurt, sliced ham, raspberries.\n- Misidentified items (highlighted in red) include salmon fillets, Greek yogurt, and raspberries.\n\n**(b) Prompt with Reference Images:**\n- Reference images of shrimp, cheese, crab dip, strawberries, smoked ham, pink tulips, banana, apple, and bread are provided.\n- GPT-4V identifies products in the basket by matching them to reference images.\n- Correctly identified products: smoked ham, apple, pink tulips, crab dip, strawberries.\n\nThis demonstrates that providing reference images improves the accuracy of product identification in the basket."}
{"layout": 1409, "type": "text", "text": "9.3 Medical ", "text_level": 1, "page_idx": 98, "bbox": [107, 73, 166, 84], "page_size": [612.0, 792.0]}
{"layout": 1410, "type": "text", "text": "In Section  4.1 , the effectiveness of GPT-4V in medical image understanding is demonstrated through Figures  18 - 19 . Furthermore, we conducted a detailed investigation into the application of GPT-4V in radiology report generation, as depicted in Figures  75 - 78 . In this scenario, we provided GPT-4V with various medical images and tasked it with generating complete radiology reports. Since assessing the accuracy of the generated reports requires domain knowledge, we sought the evaluation of a medical professional. ", "page_idx": 98, "bbox": [107, 92.7325439453125, 505, 159.28245544433594], "page_size": [612.0, 792.0]}
{"layout": 1411, "type": "text", "text": "Figure  75  showcases two accurate examples: one involving an abdominal X-ray image and another featuring an MRI of the right knee. In both cases, GPT-4V correctly identified the study and provided an accurate diagnosis. Moving on to Figure  76 , we observe the generated radiology reports by GPT- 4V for X-ray images of the hand/wrist. While GPT-4V successfully diagnosed the recommended management based on the first X-ray image, it missed the obvious distal radial fracture present in the second X-ray image. Nevertheless, the generated reports maintain a high-quality format that can serve as a template, thus reducing the workload for medical professionals when drafting reports. ", "page_idx": 98, "bbox": [107, 163.66656494140625, 505, 241.12547302246094], "page_size": [612.0, 792.0]}
{"layout": 1412, "type": "text", "text": "In Figure  77 , we explore two additional examples involving a Chest CT and a Brain MRI. In the Chest CT case, GPT-4V mistakenly identified the mentioned nodule on the left side instead of the right side, and it also hallucinated the measurements. The ability to process interleaved image-text pairs also allows GPT-4V to reference prior medical scans and diagnosis histories, which is shown to be critical in medical professionals’ diagnosing processes [ 15 ]. Figure  78  shows examples of understanding the symptom progression from multiple Chest X-Ray scans [ 60 ,  15 ]. These illustrations shed light on the potential of GPT-4V to serve as an AI assistant for radiology report generation. However, it is crucial to have the generated reports evaluated by medical professionals to ensure their correctness and accuracy. ", "page_idx": 98, "bbox": [107, 245.50958251953125, 505, 344.7864990234375], "page_size": [612.0, 792.0]}
{"layout": 1413, "type": "text", "text": "Radiology Report Generation ", "text_level": 1, "page_idx": 99, "bbox": [91, 55, 221, 65], "page_size": [612.0, 792.0]}
{"layout": 1414, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 99, "bbox": [91, 74, 127, 83], "page_size": [612.0, 792.0]}
{"layout": 1415, "type": "text", "text": "Write a radiology report for this image as an expert radiologist.. ", "page_idx": 99, "bbox": [92, 84.67356872558594, 292.2338562011719, 92.71269226074219], "page_size": [612.0, 792.0]}
{"layout": 1416, "type": "image", "page_idx": 99, "img_path": "layout_images/2309.17421v2_252.jpg", "bbox": [92, 92.25, 230, 264], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "This image appears to be an X-ray of the abdomen, often referred to as an abdominal X-ray or KUB (Kidney, Ureter, Bladder). The image shows the skeletal structure of the spine and pelvis, as well as the outline of the intestines and, potentially, other abdominal organs.\n\nObservations:\n1. **Spinal Alignment**: The lumbar spine is visible, with vertebral bodies identifiable. Look for any signs of abnormal curvature or misalignment.\n   \n2. **Bowel Gas Patterns**: The intestines can be seen filled with various degrees of gas, giving an indication of bowel structure. Look for any signs of abnormal distension which could suggest bowel obstruction or ileus.\n\n3. **Calcifications**: Any notable calcifications or radiopaque objects should be examined. This could indicate gallstones, kidney stones, or other abnormalities.\n\n4. **Soft Tissue Shadows**: The image may show outlines of soft tissue structures. This may help in identifying masses or organ enlargement.\n\n5. **Bones**: Check for any fractures or bone lesions in the visible skeletal structures.\n\n6. **Foreign Objects**: Look for any visible medical devices or foreign materials.\n\nIt's important to note that a definitive diagnosis or complete analysis should be performed by a qualified radiologist, who would consider both this image and clinical findings."}
{"layout": 1417, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 99, "bbox": [91, 267, 127, 276], "page_size": [612.0, 792.0]}
{"layout": 1418, "type": "text", "text": "Radiology Report Date: 03/23/2021 Exam: Abdominal x-ray History: Patient presents with abdominal pain and distention Findings: The abdominal x-ray demonstrates a dilated stomach and small bowel loops, suggestive of a partial small bowel obstruction.  There is also a small amount of air in the rectum. No free intra peritoneal air or p neum o peritoneum is noted. The liver, spleen, and  kidneys appear unremarkable. Impression: Findings are suggestive of a partial small bowel obstruction. Further imaging studies and clinical correlation are  recommended.   ", "page_idx": 99, "bbox": [92, 278.5135498046875, 520, 355.6329040527344], "page_size": [612.0, 792.0]}
{"layout": 1419, "type": "text", "text": "Evaluation from Professionals: Correct study identification, correct diagnosis ", "text_level": 1, "page_idx": 99, "bbox": [91, 360, 235, 379], "page_size": [612.0, 792.0]}
{"layout": 1420, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 99, "bbox": [91, 391, 127, 400.75], "page_size": [612.0, 792.0]}
{"layout": 1421, "type": "text", "text": "Write a radiology report as an expert radiologist for the following case. ", "page_idx": 99, "bbox": [92, 402.3193664550781, 314.6273193359375, 410.3584899902344], "page_size": [612.0, 792.0]}
{"layout": 1422, "type": "image", "page_idx": 99, "img_path": "layout_images/2309.17421v2_253.jpg", "bbox": [93, 412, 252, 579], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "This image appears to be an MRI scan of a knee joint. It shows structures like the femur, tibia, and joint space, which includes the meniscus. MRI images are commonly used to assess joint conditions, such as tears or degeneration."}
{"layout": 1423, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 99, "bbox": [91, 584, 127, 595], "page_size": [612.0, 792.0]}
{"layout": 1424, "type": "text", "text": "Report: ", "page_idx": 99, "bbox": [92, 596.1593017578125, 115.72001647949219, 604.1984252929688], "page_size": [612.0, 792.0]}
{"layout": 1425, "type": "text", "text": "MRI of the right knee demonstrates a large joint effusion, which is most pronounced in the supra patella r region. There is also evidence  of a tear in the medial meniscus, as well as mild degenerative changes in the articular cartilage. The anterior cruciate ligament and  posterior cruciate ligament are intact. The extensor mechanism is also intact. No other significant abnormalities are seen. Impression: Large joint effusion and medial meniscal tear in the right knee, with mild degenerative changes in the articular cartilage.  ", "page_idx": 99, "bbox": [92, 604.7943725585938, 520, 638.7385864257812], "page_size": [612.0, 792.0]}
{"layout": 1426, "type": "text", "text": "Evaluation from Professionals: ", "text_level": 1, "page_idx": 99, "bbox": [91, 646, 228, 656], "page_size": [612.0, 792.0]}
{"layout": 1427, "type": "text", "text": "Correct study identification, correct diagnosis (effusion and meniscal tear) ", "page_idx": 99, "bbox": [92, 657.838134765625, 325.8251953125, 665.8772583007812], "page_size": [612.0, 792.0]}
{"layout": 1428, "type": "text", "text": "Figure 75: Application Highlights on Radiology Report Generation. The generated report is reviewed by a medical professional to evaluate its correctness.  Green  highlights that a medical professional has confirmed the described part of the report is correct. Check Section  9.3  for detailed discussions. The medical images are from the internet. ", "page_idx": 99, "bbox": [107, 672.6155395507812, 508, 719.3355102539062], "page_size": [612.0, 792.0]}
{"layout": 1429, "type": "text", "text": "Radiology Report Generation ", "text_level": 1, "page_idx": 100, "bbox": [92, 26, 221, 37], "page_size": [612.0, 792.0]}
{"layout": 1430, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 100, "bbox": [91, 46, 127, 54.75], "page_size": [612.0, 792.0]}
{"layout": 1431, "type": "text", "text": "Write a radiology report as an expert radiologist for the following case: ", "page_idx": 100, "bbox": [92, 56.135231018066406, 314.6268005371094, 64.17435455322266], "page_size": [612.0, 792.0]}
{"layout": 1432, "type": "text", "text": "Case: ", "page_idx": 100, "bbox": [92, 73.40528869628906, 110.92646026611328, 81.44441223144531], "page_size": [612.0, 792.0]}
{"layout": 1433, "type": "text", "text": "A 32-year-old woman presented with a complaint of pain in her right wrist. The patient reported that she fell on her outstretched hand  while playing basketball. An X-ray of the right wrist was performed. ", "page_idx": 100, "bbox": [92, 82.04031372070312, 516, 98.71446990966797], "page_size": [612.0, 792.0]}
{"layout": 1434, "type": "image", "page_idx": 100, "img_path": "layout_images/2309.17421v2_254.jpg", "bbox": [93, 102, 188, 259], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "This is an X-ray image of a forearm and wrist. It shows the bones, including the radius and ulna of the forearm, and the carpal bones of the wrist. This X-ray appears to show a fracture in the radius near the wrist."}
{"layout": 1435, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 100, "bbox": [92, 264, 127, 270.75], "page_size": [612.0, 792.0]}
{"layout": 1436, "type": "text", "text": "Radiology Report: Date: 01/01/2021 Exam: X-ray of the right wrist Indication: Right wrist pain after falling on outstretched hand while playing basketball Findings: There is a transverse fracture of the distal radius with mild dorsal angulation. No other fractures or dislocations are seen. The  carpal bones are in normal alignment. No significant soft tissue swelling is noted. Impression: Transverse fracture of the distal radius with mild dorsal angulation. No other fractures or dislocations are seen. Recommendation: Orthopedic consultation for management of the fracture.  ", "page_idx": 100, "bbox": [92, 274.646728515625, 521.9188842773438, 343.13104248046875], "page_size": [612.0, 792.0]}
{"layout": 1437, "type": "text", "text": "Evaluation from Professionals: Correct diagnosis and management ", "text_level": 1, "page_idx": 100, "bbox": [91, 347.25, 227, 366], "page_size": [612.0, 792.0]}
{"layout": 1438, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 100, "bbox": [91, 379, 127, 387.75], "page_size": [612.0, 792.0]}
{"layout": 1439, "type": "text", "text": "Write a radiology report for this image as an expert radiologist. ", "page_idx": 100, "bbox": [92, 389.20074462890625, 290.2406005859375, 397.2398681640625], "page_size": [612.0, 792.0]}
{"layout": 1440, "type": "image", "page_idx": 100, "img_path": "layout_images/2309.17421v2_255.jpg", "bbox": [93, 400, 176, 560], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "This image is an X-ray of a hand and wrist. It shows the bones including the radius and ulna (forearm bones), carpal bones (wrist bones), metacarpals, and phalanges (finger bones)."}
{"layout": 1441, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 100, "bbox": [91, 565, 127, 575], "page_size": [612.0, 792.0]}
{"layout": 1442, "type": "text", "text": "Report: Date: 01/01/2021 Exam: X-ray of the right hand and wrist Technique: AP and lateral views were obtained Findings: The bones of the right hand and wrist appear intact and in proper alignment. There is no evidence of fracture or dislocation.  The joint spaces are preserved and there is no evidence of degenerative changes. No soft tissue abnormalities are seen. Impression: Normal X-ray of the right hand and wrist. ", "page_idx": 100, "bbox": [92, 576.872802734375, 516, 636.7221069335938], "page_size": [612.0, 792.0]}
{"layout": 1443, "type": "text", "text": "Evaluation from Professionals: ", "text_level": 1, "page_idx": 100, "bbox": [91, 642, 227, 653], "page_size": [612.0, 792.0]}
{"layout": 1444, "type": "text", "text": "Missed the obvious distal radial fracture Correct identification of the type of study ", "page_idx": 100, "bbox": [92, 653.9713134765625, 220, 670.6455078125], "page_size": [612.0, 792.0]}
{"layout": 1445, "type": "text", "text": "Figure 76: Application Highlights on Radiology Report Generation. The generated report is reviewed by a medical professional to evaluate its correctness. Green  (  Red  ) highlights that a medical professional has confirmed the described part of the report is correct (incorrect). Check Section  9.3 for detailed discussions. The medical images are from the Internet. ", "page_idx": 100, "bbox": [108, 681.0855712890625, 504, 727.8045043945312], "page_size": [612.0, 792.0]}
{"layout": 1446, "type": "text", "text": "Sec. 9.3 Medical ", "text_level": 1, "page_idx": 101, "bbox": [94, 30, 259, 51], "page_size": [612.0, 792.0]}
{"layout": 1447, "type": "text", "text": "Radiology Report Generation ", "text_level": 1, "page_idx": 101, "bbox": [91, 55, 221, 65], "page_size": [612.0, 792.0]}
{"layout": 1448, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 101, "bbox": [91, 73, 127, 82.75], "page_size": [612.0, 792.0]}
{"layout": 1449, "type": "image", "page_idx": 101, "img_path": "layout_images/2309.17421v2_256.jpg", "img_caption": "Write a radiology report for this image as an expert radiologist.. ", "bbox": [91, 83.25, 291, 237], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "## Radiology Report\n\n**Patient Details:**\n- [Patient Name/ID: Redacted]\n- [Age/Gender: Redacted]\n- [Date of Examination: Redacted]\n- [Modality: CT Scan (Chest)]\n\n**Clinical Indication:**\n- Evaluation of respiratory symptoms.\n- Rule out neoplastic or infectious process.\n\n**Technique:**\nCT scan of the chest was performed with contrast enhancement. Axial images were obtained.\n\n**Findings:**\n\n### Lungs and Airways:\n- There is a well-circumscribed, round nodule measuring approximately [size not measured from image] located in the left upper lobe. The nodule exhibits spiculated margins, raising suspicion for a primary neoplastic process, such as a lung carcinoma.\n- No cavitation or calcification noted within the nodule.\n- Multiple smaller nodular opacities observed in the peripheral regions of both the left and right upper lobes, suggestive of possible satellite lesions or metastatic nodules.\n- No evidence of pleural effusion or pneumothorax.\n- The central airways are patent.\n\n### Mediastinum and Heart:\n- The mediastinum appears within normal limits in size. No lymphadenopathy is identified in the sampled image.\n- The heart is normal in size and configuration. No pericardial effusion is noted.\n\n### Bones and Soft Tissues:\n- No bony lesions or significant degenerative changes observed in the visible osseous structures.\n- The soft tissues of the thoracic wall appear unremarkable.\n\n**Impression:**\n- Spiculated nodule in the left upper lobe is suspicious for a neoplastic process, most likely a primary lung carcinoma. Further evaluation with a PET scan or biopsy is recommended for definitive diagnosis.\n- Note of multiple smaller nodules raises concern for metastatic disease; correlation with clinical history and further investigation is advised.\n\n**Recommendations:**\n- Recommend follow-up with pulmonology or oncology for further diagnostic workup.\n- Consider PET-CT for metabolic assessment of the identified nodule.\n- Tissue diagnosis via CT-guided biopsy may be warranted depending on multidisciplinary team suggestions.\n\n**Radiologist:**\n- [Name]\n- [Contact Information]\n- [Date]\n\n**Note:** This is a mock report created for educational purposes based on the provided image. Actual patient care decisions should be made based on the full clinical context and consultation with appropriate medical professionals."}
{"layout": 1450, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 101, "bbox": [91, 242, 127, 251], "page_size": [612.0, 792.0]}
{"layout": 1451, "type": "text", "text": "Report: Date: 6/25/2021 Study Type: Chest CT Subject: 42-year-old male ", "page_idx": 101, "bbox": [92, 253.65020751953125, 174.0807647705078, 287.59442138671875], "page_size": [612.0, 792.0]}
{"layout": 1452, "type": "text", "text": "Findings: The chest CT images demonstrate a nodular opacity in the left upper lobe, measuring approximately  $1.3\\,\\mathsf{c m}$   in diameter. The  opacity appears to have spiculated margins and is located adjacent to the pleura. There is no evidence of media st in al or hilar  lymph aden opa thy. ", "page_idx": 101, "bbox": [92, 288, 527, 313.49951171875], "page_size": [612.0, 792.0]}
{"layout": 1453, "type": "text", "text": "Impression: The findings are concerning for a primary lung malignancy. Further evaluation with a biopsy is recommended to confirm the  diagnosis.  ", "page_idx": 101, "bbox": [92, 314.09539794921875, 527, 330.7695617675781], "page_size": [612.0, 792.0]}
{"layout": 1454, "type": "text", "text": "Evaluation from Professionals: Correct study type and identification Mensions nodule but in the left rather than in the right. Gives measurements (hallucination) ", "page_idx": 101, "bbox": [92, 335.1787109375, 263.9895935058594, 371.4775695800781], "page_size": [612.0, 792.0]}
{"layout": 1455, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 101, "bbox": [91, 382, 128, 391.75], "page_size": [612.0, 792.0]}
{"layout": 1456, "type": "text", "text": "Write a radiology report as an expert radiologist for the following case. ", "page_idx": 101, "bbox": [92, 392.8757019042969, 314.6273193359375, 400.9148254394531], "page_size": [612.0, 792.0]}
{"layout": 1457, "type": "image", "page_idx": 101, "img_path": "layout_images/2309.17421v2_257.jpg", "bbox": [93, 405, 213, 546], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "This image appears to be an MRI scan of a human brain, specifically an axial view (looking at the brain from the top down). The scan shows the presence of a large mass or lesion on the right side of the brain (which appears on the left side of the image due to the orientation). This mass is irregular in shape and contains regions of varying intensity, which could suggest a tumor or other abnormality. The MRI shows cross sections through structures such as the eyes, skull, and brain tissue. For an accurate diagnosis, the scan should be assessed by a radiologist or medical professional with expertise in interpreting medical images."}
{"layout": 1458, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 101, "bbox": [91, 550, 127, 560], "page_size": [612.0, 792.0]}
{"layout": 1459, "type": "text", "text": "Report: Date: 01/01/2021 Modality: Magnetic Resonance Imaging (MRI) Body Part: Brain Technique: Axial T1-weighted Findings: The MRI of the brain demonstrates a large intra-axial mass in the left parietal lobe with surrounding edema. The mass appears  to be heterogeneous in signal intensity with areas of high and low signal intensity. There is a significant mass effect with midline shift to  the right. The ventricles appear to be compressed and there is evidence of hydro ce ph alus. No other significant abnormalities are noted. Impression: Large intra-axial mass in the left parietal lobe with surrounding edema and significant mass effect. Likely high-grade glioma.  Further imaging and clinical correlation are recommended.  ", "page_idx": 101, "bbox": [92, 562.0441284179688, 527, 647.7985229492188], "page_size": [612.0, 792.0]}
{"layout": 1460, "type": "text", "text": "Evaluation from Professionals: Correct study identification, correct mass description and location, correct diagnosis ", "page_idx": 101, "bbox": [92, 655.9085083007812, 357.9330749511719, 674.937255859375], "page_size": [612.0, 792.0]}
{"layout": 1461, "type": "text", "text": "Figure 77: Application Highlights on Radiology Report Generation. The generated report is reviewed by a medical professional to evaluate its correctness. Green  (  Red  ) highlights that a medical professional has confirmed the described part of the report is correct (incorrect).  Yellow  indicates that the model is hallucinating. Check Section  9.3  for detailed discussions. The medical images are from the internet. ", "page_idx": 101, "bbox": [108, 681.7395629882812, 504, 742.2874755859375], "page_size": [612.0, 792.0]}
{"layout": 1462, "type": "text", "text": "Sec. 9.3 Medical ", "text_level": 1, "page_idx": 102, "bbox": [132, 45, 222, 59], "page_size": [612.0, 792.0]}
{"layout": 1463, "type": "text", "text": "Radiology Report Generation with Diagnosis History ", "text_level": 1, "page_idx": 102, "bbox": [91, 66, 324, 77], "page_size": [612.0, 792.0]}
{"layout": 1464, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 102, "bbox": [91, 82, 127, 91], "page_size": [612.0, 792.0]}
{"layout": 1465, "type": "text", "text": "The first image is a chest x-ray scan the patient took last time.  The previous diagnoses mention pleural effusion. ", "page_idx": 102, "bbox": [92, 92.74632263183594, 294, 109.42047119140625], "page_size": [612.0, 792.0]}
{"layout": 1466, "type": "image", "page_idx": 102, "img_path": "layout_images/2309.17421v2_258.jpg", "img_caption": "The second image is a chest x-ray scan the patient took this  time. ", "bbox": [92, 112, 284, 299], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image is a chest X-ray, which is a type of medical imaging used to examine the chest, including the heart, lungs, and bones. The X-ray shows the expected anatomy: two lungs, heart, and the surrounding structures. The darker areas represent the air-filled lungs, while the lighter areas show denser structures like bones and the heart. The ribs and spine can also be faintly seen. Interpreting the details of such an image requires medical expertise, as it involves looking for abnormalities or signs of medical conditions."}
{"layout": 1467, "type": "image", "page_idx": 102, "img_path": "layout_images/2309.17421v2_259.jpg", "bbox": [93, 302, 262, 469], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "This image appears to be a chest X-ray displaying the thoracic cavity, likely showing the lungs and heart. Generally, such images are used to assess the condition of the lungs or heart for medical diagnosis."}
{"layout": 1468, "type": "text", "text": "Write a radiology report as an expert radiologist for the  symptoms and changes. ", "page_idx": 102, "bbox": [92, 472.6875, 269, 489.36163330078125], "page_size": [612.0, 792.0]}
{"layout": 1469, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 102, "bbox": [91, 498, 126, 507], "page_size": [612.0, 792.0]}
{"layout": 1470, "type": "text", "text": "Radiology Report Date: 01/01/2021 Patient: John Doe Exam: Chest X-ray ", "page_idx": 102, "bbox": [92, 509.27142333984375, 152.469970703125, 543.2156372070312], "page_size": [612.0, 792.0]}
{"layout": 1471, "type": "text", "text": "Findings: The chest x-ray scan taken this time shows a  significant improvement in the patient's condition compared to  the previous scan. The previous scan showed a pleural effusion,  which is a buildup of fluid in the space between the lungs and  chest wall. However, the current scan shows that the pleural  effusion has resolved. There is no evidence of any new  abnormalities in the lungs or chest wall. The lungs appear to be  well-expanded and the heart size is normal. The media st in um,  which is the space between the lungs, is also within normal  limits. ", "page_idx": 102, "bbox": [92, 543.8115844726562, 294, 629.5659790039062], "page_size": [612.0, 792.0]}
{"layout": 1472, "type": "text", "text": "Conclusion: The patient's condition has improved since the last  chest x-ray scan, with resolution of the previously noted pleural  effusion. No new abnormalities are seen. ", "page_idx": 102, "bbox": [92, 630.1619262695312, 294, 655.4711303710938], "page_size": [612.0, 792.0]}
{"layout": 1473, "type": "text", "text": "Annotations from Professionals: Improving pleural effusion ", "text_level": 1, "page_idx": 102, "bbox": [91, 663, 234, 682], "page_size": [612.0, 792.0]}
{"layout": 1474, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 102, "bbox": [314, 82, 350, 91], "page_size": [612.0, 792.0]}
{"layout": 1475, "type": "text", "text": "The first image is a chest x-ray scan the patient took last time.  The previous diagnoses mention pleural effusion. ", "page_idx": 102, "bbox": [314, 92.74632263183594, 515, 109.42047119140625], "page_size": [612.0, 792.0]}
{"layout": 1476, "type": "image", "page_idx": 102, "img_path": "layout_images/2309.17421v2_260.jpg", "img_caption": "The second image is a chest x-ray scan the patient took this  time. ", "bbox": [314, 112, 507, 299], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image is a chest X-ray, typically used to examine the health of the lungs, heart, and chest wall. It shows the ribcage and the outline of the lungs and heart. Please consult a medical professional for interpretation and diagnosis."}
{"layout": 1477, "type": "image", "page_idx": 102, "img_path": "layout_images/2309.17421v2_261.jpg", "bbox": [315, 302, 484, 469], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "This is an X-ray image of a chest. It shows the ribcage, lungs, and other structures within the thoracic cavity."}
{"layout": 1478, "type": "text", "text": "Write a radiology report as an expert radiologist for the  symptoms and changes. ", "page_idx": 102, "bbox": [314, 472.6875, 493, 489.36163330078125], "page_size": [612.0, 792.0]}
{"layout": 1479, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 102, "bbox": [314, 498, 349, 507], "page_size": [612.0, 792.0]}
{"layout": 1480, "type": "text", "text": "Radiology Report Date: 12/12/2021 Patient: Jane Doe Exam: Chest X-ray ", "page_idx": 102, "bbox": [314, 509.27142333984375, 375.2540588378906, 543.2156372070312], "page_size": [612.0, 792.0]}
{"layout": 1481, "type": "text", "text": "Findings: The chest x-ray scan taken this time shows a  significant worsening of the patient's pleural effusion. The fluid  accumulation in the pleural space is more prominent than in the  previous scan. The lung fields are also more hazy and  congested, indicating a possible infection or inflammation. The  heart and media st in al contours are obscured by the fluid, making  it difficult to assess their size and shape.  Impression: The patient's pleural effusion has worsened, and  there is a possibility of an underlying infection or inflammation.  Further evaluation and management are recommended. ", "page_idx": 102, "bbox": [314, 543.8115844726562, 522, 629.5659790039062], "page_size": [612.0, 792.0]}
{"layout": 1482, "type": "text", "text": "Annotations from Professionals: Worsening pleural effusion ", "page_idx": 102, "bbox": [314, 662.716064453125, 458.19927978515625, 681.7448120117188], "page_size": [612.0, 792.0]}
{"layout": 1483, "type": "text", "text": "9.4 Auto Insurance ", "text_level": 1, "page_idx": 103, "bbox": [107, 74, 196, 84], "page_size": [612.0, 792.0]}
{"layout": 1484, "type": "text", "text": "In this section, we explore another practical application of GPT-4V in the field of auto insurance, focusing specifically on car accident reporting. Within this context, we can further delineate two distinct sub-categories: ( i ) Damage Evaluation and   $(i i)$   Insurance Reporting. The former involves the crucial task of accurately identifying and assessing the extent of damages sustained by vehicles, while the latter encompasses not only damage identification but also the recognition of vehicle-specific information depicted in images, such as the make, model, license plate, and other relevant details. By addressing both aspects, we aim to demonstrate the comprehensive capabilities of GPT-4V within the auto insurance domain. ", "page_idx": 103, "bbox": [106, 92.7325439453125, 505, 181.1005096435547], "page_size": [612.0, 792.0]}
{"layout": 1485, "type": "text", "text": "Damage evaluation. We present an image depicting car damage to GPT-4V and prompt it with “Imagine that you are an expert in evaluating the car damage from car accident for auto insurance reporting. Please evaluate the damage seen in the image below.” in Figure  79 . GPT-4V has demonstrated remarkable proficiency in accurately identifying and precisely localizing the damages depicted in all four images. Furthermore, it impresses with its ability to provide detailed descriptions of each specific damage instance. In some instances, GPT-4V even endeavors to estimate the potential cost of repair. ", "page_idx": 103, "bbox": [106, 191.8508758544922, 505, 269.66851806640625], "page_size": [612.0, 792.0]}
{"layout": 1486, "type": "text", "text": "Insurance reporting. Building on the success in damage evaluation, we modify our prompt to ask GPT-4V to identify the make, model, and license plate of the vehicle depicted in the image, and return the obtained information in JSON format. The examples depicted in Figure  80  showcase this capability. In both instances, GPT-4V attempts to extract all the requested details from the image. However, it should be noted that certain information may be unavailable, such as the estimated cost of repair, or challenging to discern due to occlusion, as observed with the license plate in the second image. It is important to note that real-life insurance reporting typically involves multiple images capturing the car from various angles, a scenario that is usually not publicly accessible on the Internet. Nevertheless, the examples in Figures  79 - 80  vividly illustrate the potential of GPT-4V in automating the insurance reporting process for car accidents. ", "page_idx": 103, "bbox": [106, 280.41790771484375, 505, 390.9635009765625], "page_size": [612.0, 792.0]}
{"layout": 1487, "type": "text", "text": "Sec. 9.4 Auto Insurance ", "text_level": 1, "page_idx": 104, "bbox": [144, 47, 277, 63], "page_size": [612.0, 792.0]}
{"layout": 1488, "type": "text", "text": "Damage Evaluation ", "text_level": 1, "page_idx": 104, "bbox": [121, 70, 212, 81], "page_size": [612.0, 792.0]}
{"layout": 1489, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 104, "bbox": [121, 86, 159, 96], "page_size": [612.0, 792.0]}
{"layout": 1490, "type": "text", "text": "Imagine that you are an expert in evaluating the  car damage from car accident for auto insurance  reporting. Please evaluate the damage seen in the  image below. ", "page_idx": 104, "bbox": [122, 97.56739044189453, 291, 132.81326293945312], "page_size": [612.0, 792.0]}
{"layout": 1491, "type": "image", "page_idx": 104, "img_path": "layout_images/2309.17421v2_262.jpg", "bbox": [123, 136, 295, 233], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows a close-up view of a car's front bumper and wheel. There are visible scratches and paint damage on the bumper."}
{"layout": 1492, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 104, "bbox": [121, 238, 158, 248], "page_size": [612.0, 792.0]}
{"layout": 1493, "type": "text", "text": "Based on the image, it appears that the car has  sustained damage to the front passenger side  bumper. The damage appears to be in the form of  scrapes and scratches, with some areas of the  paint chipped away. The damage appears to be  cosmetic and does not appear to have affected  the structural integrity of the bumper or the car.  However, it is important to note that the image  only shows one angle of the car and does not  provide a full view of the damage. A more  thorough inspection of the car would be needed  to fully assess the extent of the damage and  determine if there are any underlying issues that  are not visible in the image.  ", "page_idx": 104, "bbox": [122, 250.8075714111328, 291, 375.7150573730469], "page_size": [612.0, 792.0]}
{"layout": 1494, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 104, "bbox": [121, 403, 159, 413], "page_size": [612.0, 792.0]}
{"layout": 1495, "type": "text", "text": "Imagine that you are an expert in evaluating the  car damage from car accident for auto insurance  reporting. Please evaluate the damage seen in the  image below. ", "page_idx": 104, "bbox": [122, 414.585205078125, 291, 449.8310546875], "page_size": [612.0, 792.0]}
{"layout": 1496, "type": "image", "page_idx": 104, "img_path": "layout_images/2309.17421v2_263.jpg", "bbox": [123, 451, 268, 549], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows two cars, with the focus on the rear section of a gray car that appears to have some damage. The other car is in the background, slightly out of focus. The setting appears to be a street or parking area."}
{"layout": 1497, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 104, "bbox": [121, 553, 159, 562], "page_size": [612.0, 792.0]}
{"layout": 1498, "type": "text", "text": "Based on the image, it appears that the car has  sustained damage to the rear bumper and quarter  panel on the driver's side. The bumper appears to  be dented and scratched, and the quarter panel  has a large dent and some scuff marks. It is likely  that the car was hit from the side, causing the  damage to the quarter panel and bumper. In  terms of repair costs, the bumper may be able to  be repaired with some minor body work and  repainting. However, the quarter panel may  require more extensive repairs or even  replacement, which could be more costly. Overall,  I would estimate the damage to be in the  moderate to severe range, with repair costs likely  in the thousands of dollars.  ", "page_idx": 104, "bbox": [122, 564.6231689453125, 291, 698.496826171875], "page_size": [612.0, 792.0]}
{"layout": 1499, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 104, "bbox": [316, 86, 353, 96], "page_size": [612.0, 792.0]}
{"layout": 1500, "type": "text", "text": "Imagine that you are an expert in evaluating the car  damage from car accident for auto insurance  reporting. Please evaluate the damage seen in the  image below. ", "page_idx": 104, "bbox": [316, 97.626220703125, 493, 132.87208557128906], "page_size": [612.0, 792.0]}
{"layout": 1501, "type": "image", "page_idx": 104, "img_path": "layout_images/2309.17421v2_264.jpg", "bbox": [317, 135, 463, 232], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows a blue car with significant front-end damage, likely from a collision. The hood is crumpled and bent upwards, and the front bumper appears to be damaged as well. The car is parked on a street, with other vehicles visible in the background."}
{"layout": 1502, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 104, "bbox": [316, 239, 352, 249], "page_size": [612.0, 792.0]}
{"layout": 1503, "type": "text", "text": "Based on the image, it appears that the vehicle has  sustained significant damage to the front end. The  hood is crumpled and the front bumper is partially  detached. The headlights also appear to be damaged.  It is likely that the vehicle was involved in a front-end  collision. The damage appears to be primarily  cosmetic, but it is possible that there may be  underlying structural damage to the frame or engine.  Further inspection would be required to assess the full  extent of the damage. In terms of repair costs, the  hood and bumper would likely need to be replaced,  and the headlights may also need to be replaced or  repaired. The cost of these repairs would depend on  the make and model of the vehicle, but it is likely that  the cost would be several thousand dollars at  minimum.  ", "page_idx": 104, "bbox": [316, 250.8664093017578, 497, 393.70623779296875], "page_size": [612.0, 792.0]}
{"layout": 1504, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 104, "bbox": [316, 403, 353, 413], "page_size": [612.0, 792.0]}
{"layout": 1505, "type": "text", "text": "Imagine that you are an expert in evaluating the car  damage from car accident for auto insurance  reporting. Please evaluate the damage seen in the  image below. ", "page_idx": 104, "bbox": [316, 414.585205078125, 493, 449.8310546875], "page_size": [612.0, 792.0]}
{"layout": 1506, "type": "image", "page_idx": 104, "img_path": "layout_images/2309.17421v2_265.jpg", "bbox": [317, 451, 447, 549], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows a white sedan with significant front-end damage, suggesting it was involved in a collision. The car is on a roadway with emergency responders, including police and a fire truck, visible in the background. The scene appears to be an accident on a road where assistance is being provided."}
{"layout": 1507, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 104, "bbox": [316, 553, 353, 562], "page_size": [612.0, 792.0]}
{"layout": 1508, "type": "text", "text": "Based on the image, it appears that the vehicle has  sustained significant damage to the front end. The  hood is crumpled and the front bumper is completely  detached from the vehicle. The headlights and grille  are also damaged and the engine appears to be  exposed. Additionally, there is damage to the driver's  side fender and door. Given the extent of the damage,  it is likely that the vehicle was involved in a  high-speed collision or a head-on collision. The cost  of repairs for this vehicle will likely be significant and it  may be deemed a total loss by the insurance  company.  ", "page_idx": 104, "bbox": [316, 564.6231689453125, 497, 671.5983276367188], "page_size": [612.0, 792.0]}
{"layout": 1509, "type": "text", "text": "Sec. 9.4 Auto Insurance ", "text_level": 1, "page_idx": 105, "bbox": [136, 111, 277, 128], "page_size": [612.0, 792.0]}
{"layout": 1510, "type": "text", "text": "Insurance Report Generation ", "text_level": 1, "page_idx": 105, "bbox": [121, 134, 256, 145], "page_size": [612.0, 792.0]}
{"layout": 1511, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 105, "bbox": [121, 154, 159, 163.75], "page_size": [612.0, 792.0]}
{"layout": 1512, "type": "text", "text": "Imagine that you are an expert in evaluating the car damage from car accident for auto insurance reporting.  Please fill the incident report for the accident shown in image below, with the following format in JSON (note  xxx is placeholder, if you cannot determine that based on the image, put \"N/A\" instead). {\"make\": xxx, \"model\": xxx, \"license plate\": xxx, \"damage description\": xxx, \"estimated cost of repair\": xxx} ", "page_idx": 105, "bbox": [121, 164.941650390625, 479, 200.18751525878906], "page_size": [612.0, 792.0]}
{"layout": 1513, "type": "image", "page_idx": 105, "img_path": "layout_images/2309.17421v2_266.jpg", "bbox": [122, 204, 360, 339], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows a heavily damaged car with significant front-end and roof damage. The windshield is shattered, and the hood is crumpled. The license plate reads \"KL 52 N 5305.\" The car is parked on the side of a road, with other vehicles visible in the background."}
{"layout": 1514, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 105, "bbox": [122, 345, 159, 354.75], "page_size": [612.0, 792.0]}
{"layout": 1515, "type": "text", "text": "{\"make\": \"Maruti Suzuki\", \"model\": \"Celerio\", \"license plate\": \"KL 52 N 5305\", \"damage description\": \"Front  bumper, hood, and windshield damaged. Airbags deployed.\", \"estimated cost of repair\": \"N/A\"}  ", "page_idx": 105, "bbox": [121, 356.60809326171875, 472, 373.921630859375], "page_size": [612.0, 792.0]}
{"layout": 1516, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 105, "bbox": [121, 391, 159, 400.75], "page_size": [612.0, 792.0]}
{"layout": 1517, "type": "text", "text": "Imagine that you are an expert in evaluating the car damage from car accident for auto insurance reporting.  Please evaluate the damage seen in the image below. For filing the incident report, please follow the following  format in JSON (note xxx is placeholder, if the information is not available in the image, put \"N/A\" instead). {\"make\": xxx, \"model\": xxx, \"license plate\": xxx, \"damage description\": xxx, \"estimated cost of repair\": xxx} ", "page_idx": 105, "bbox": [121, 401.9044494628906, 486.7675476074219, 437.1502990722656], "page_size": [612.0, 792.0]}
{"layout": 1518, "type": "image", "page_idx": 105, "img_path": "layout_images/2309.17421v2_267.jpg", "bbox": [123, 441, 359, 596], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows a black Toyota car that has been involved in a collision. The front of the vehicle is damaged, with a crumpled hood and bumper. A person is sitting inside the car, and the driver's side door is open. There is another vehicle visible in the background. The accident appears to have occurred on a roadway."}
{"layout": 1519, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 105, "bbox": [121, 601, 159, 610.75], "page_size": [612.0, 792.0]}
{"layout": 1520, "type": "text", "text": "{\"make\": \"Toyota\", \"model\": \"Corolla\", \"license plate\": \"N/A\", \"damage description\": \"Front bumper and hood  damage, driver side fender damage, driver side headlight damage\", \"estimated cost of repair\":   $\\\"\\S2{,}500{-}\\S3{,}500\"\\}$    ", "page_idx": 105, "bbox": [121, 612.7840576171875, 479, 640], "page_size": [612.0, 792.0]}
{"layout": 1521, "type": "text", "text": "Figure 80: Application Highlights on Insurance Reporting. For the highlighted text in  red  , GPT-4V fails to read the license plate, potentially due to occlusion. Check Section  9.4  for detailed discussions. ", "page_idx": 105, "bbox": [108, 660.7855224609375, 504, 683.6995239257812], "page_size": [612.0, 792.0]}
{"layout": 1522, "type": "text", "text": "9.5 Customized Captioner ", "text_level": 1, "page_idx": 106, "bbox": [106, 74, 226, 84], "page_size": [612.0, 792.0]}
{"layout": 1523, "type": "text", "text": "Photo organization. In this scenario, let’s picture that we have a family photo album. We demon- strate how GPT-4V can enhance the album by generating captions that explicitly mention the name of each family member shown in the photo. This personalized approach facilitates more precise and tailored photo organization, as illustrated in Figures  81 - 82 . By providing GPT-4V with visual prompts for each family member, along with their respective names, GPT-4V can precisely identify the family members (including person, cat, and dog) to generate detailed and customized captions. Storing such captions for all the images in the family album holds the potential to enable highly personalized image search. For instance, a user could search for “a family photo of Linda, Cotton, Max, Sam, and Emma” and easily locate the corresponding family photo shown in Figure  81 , or “Max lying between Sam and Emma” and locate the family photo in Figure  82 . ", "page_idx": 106, "bbox": [106, 92.37389373779297, 505, 202.9194793701172], "page_size": [612.0, 792.0]}
{"layout": 1524, "type": "text", "text": "Dense captioning w/ segmentation. We demonstrate the enhanced performance of GPT-4V in dense captioning by harnessing powerful segmentation models [ 65 ,  159 ,  160 ]. Figure  83  illustrates the dense captioning results by incorporating object cut-outs generated by SAM [ 65 ] into the prompt to extract more detailed captions for each object of interest. In addition, we provide GPT-4V with the original image as the global context and ask it to describe four object cut-outs as detailed as possible, and incorporating references to the context image. ", "page_idx": 106, "bbox": [106, 213.6688690185547, 505, 280.5775146484375], "page_size": [612.0, 792.0]}
{"layout": 1525, "type": "text", "text": "The results show GPT-4V can generate highly intricate dense captions for each object, some of which are accompanied by relevant references to the context image. For instance, when describing object 3 (a frog), the dense caption makes mention of a close-up shot of a frog with a snail perched on its head, despite the absence of the snail in the corresponding cut-out for object 3. Similarly, when referring to object 4 (a turtle), GPT-4V recognizes from the context image that the turtle is floating in water, thereby further enriching the generated caption. ", "page_idx": 106, "bbox": [106, 284.9615478515625, 505, 351.5115051269531], "page_size": [612.0, 792.0]}
{"layout": 1526, "type": "text", "text": "Sec. 9.5 Customized Captioner ", "text_level": 1, "page_idx": 107, "bbox": [126, 31, 328, 51], "page_size": [612.0, 792.0]}
{"layout": 1527, "type": "text", "text": "Photo Organization ", "page_idx": 107, "bbox": [122, 55.86431121826172, 210.8563232421875, 66.59666442871094], "page_size": [612.0, 792.0]}
{"layout": 1528, "type": "text", "text": "Prompt: Here are the photos of my family members. ", "page_idx": 107, "bbox": [122, 71.98271179199219, 265.5352478027344, 91.74116516113281], "page_size": [612.0, 792.0]}
{"layout": 1529, "type": "text", "text": "Linda ", "page_idx": 107, "bbox": [122, 96.20256042480469, 140.63902282714844, 104.54994201660156], "page_size": [612.0, 792.0]}
{"layout": 1530, "type": "image", "page_idx": 107, "img_path": "layout_images/2309.17421v2_268.jpg", "bbox": [122, 102.25, 176, 475], "page_size": [612.0, 792.0], "ocr_text": "Cotton\n\njax\n\nEmma\n", "vlm_text": "The image displays a series of photographs labeled with names. In addition to some blurred faces, it includes a clear picture of a white and orange cat labeled \"Cotton\" and a golden retriever dog labeled \"Max.\""}
{"layout": 1531, "type": "text", "text": "Please describe the image with as many details as possible, using my family members’ names. ", "page_idx": 107, "bbox": [122, 478.5446472167969, 435.8547668457031, 486.89202880859375], "page_size": [612.0, 792.0]}
{"layout": 1532, "type": "image", "page_idx": 107, "img_path": "layout_images/2309.17421v2_269.jpg", "bbox": [128, 491, 339, 633], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image features a group of people sitting indoors on a carpeted floor in front of a white couch. They are accompanied by a golden retriever and a cat. The setting appears to be a cozy living room with decorative elements such as a vase with flowers and a shelving unit. The cat is perched on one individual's knee, and the golden retriever is seated with its tongue out, facing the camera."}
{"layout": 1533, "type": "text", "text": "GPT-4V: This image is a family portrait of Linda and her children, Emma and Sam, along with their pets, Max the dog and  Cotton the cat. The family is sitting in their living room, with Linda on the couch and Emma and Sam on the floor.  Max is sitting in between Linda and Emma, with his tongue sticking out, while Cotton is perched on Linda's  shoulder. All family members are smiling and looking at the camera. The room is bright and airy, with a vase of  yellow flowers on the coffee table and a white bookshelf in the background.  ", "page_idx": 107, "bbox": [122, 638.3056030273438, 494, 693.9286499023438], "page_size": [612.0, 792.0]}
{"layout": 1534, "type": "text", "text": "Figure 81: Customized Captioner for photo organization (the reference images are cropped from the query image).  Blue  highlights the mention of family names. Check Sections  9.5 ,  10.5  for detailed discussions. ", "page_idx": 107, "bbox": [108, 707.3535766601562, 503, 743.1644897460938], "page_size": [612.0, 792.0]}
{"layout": 1535, "type": "text", "text": "Sec. 9.5 Customized Captioner ", "text_level": 1, "page_idx": 108, "bbox": [126, 31, 328, 50], "page_size": [612.0, 792.0]}
{"layout": 1536, "type": "text", "text": "Photo Organization ", "text_level": 1, "page_idx": 108, "bbox": [121, 56, 212, 66], "page_size": [612.0, 792.0]}
{"layout": 1537, "type": "text", "text": "Prompt: Here are the photos of my family members. ", "page_idx": 108, "bbox": [122, 71.98320770263672, 265.5352478027344, 91.74169921875], "page_size": [612.0, 792.0]}
{"layout": 1538, "type": "text", "text": "Linda ", "page_idx": 108, "bbox": [122, 96.20309448242188, 140.63902282714844, 104.55049896240234], "page_size": [612.0, 792.0]}
{"layout": 1539, "type": "image", "page_idx": 108, "img_path": "layout_images/2309.17421v2_270.jpg", "bbox": [122, 102.25, 175, 475], "page_size": [612.0, 792.0], "ocr_text": "Lech hahaa\n\n=\n\nCotton\n\nEmma\n\n", "vlm_text": "The image consists of a series of names listed vertically alongside corresponding blurred photographs. The names are Linda, Cotton, Max, Sam, and Emma. Among these, Cotton is associated with an image of a cat."}
{"layout": 1540, "type": "text", "text": "Please describe the image with as many details as possible, using my family members’ names. ", "page_idx": 108, "bbox": [122, 478.5457763671875, 435.8547668457031, 486.8931579589844], "page_size": [612.0, 792.0]}
{"layout": 1541, "type": "image", "page_idx": 108, "img_path": "layout_images/2309.17421v2_271.jpg", "img_caption": "GPT-4V: This image shows three members of my family - my brother Sam, our dog Max, and myself, Emma. We are lying  on a grassy field with a clear blue sky in the background. Sam and I are resting our heads on our hands and  smiling at the camera, while Max is lying between us with his tongue out. We are all enjoying the beautiful  weather and spending time together as a family.  ", "bbox": [121, 489, 495, 683], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows two children and a golden retriever lying on a grassy field with a clear blue sky in the background. The children are resting their heads on their hands and appear to be smiling, while the dog is positioned between them with its tongue out. They seem to be enjoying the outdoor setting and each other's company."}
{"layout": 1542, "type": "text", "text": "Figure 82: Customized Captioner for photo organization (the reference images are cropped from a different image than the query image)  Blue  highlights the mention of family names. Check Section  9.5  for detailed discussions. ", "page_idx": 108, "bbox": [108, 695.612548828125, 504, 731.4224853515625], "page_size": [612.0, 792.0]}
{"layout": 1543, "type": "text", "text": "Sec. 9.5 Customized Captioner ", "text_level": 1, "page_idx": 109, "bbox": [125, 31, 329, 51], "page_size": [612.0, 792.0]}
{"layout": 1544, "type": "text", "text": "Dense Captioning w/ Segmentation ", "page_idx": 109, "bbox": [122, 55.86431121826172, 285, 66.59666442871094], "page_size": [612.0, 792.0]}
{"layout": 1545, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 109, "bbox": [121, 72, 159, 82], "page_size": [612.0, 792.0]}
{"layout": 1546, "type": "text", "text": "Describe the objects below with as many details as possible, Reference to the content shown in the context  image but focus on describing each object below. ", "page_idx": 109, "bbox": [122, 83.39378356933594, 487, 100.70731353759766], "page_size": [612.0, 792.0]}
{"layout": 1547, "type": "image", "page_idx": 109, "img_path": "layout_images/2309.17421v2_272.jpg", "bbox": [122, 109, 220, 333], "page_size": [612.0, 792.0], "ocr_text": "Object 3\n\n", "vlm_text": "The image contains four objects labeled from top to bottom:\n\n1. Object 2: A snail.\n2. Object 3: A different snail.\n3. Object 4: A frog.\n4. Object 5: A turtle.\n\nEach object is outlined within a blue rectangle."}
{"layout": 1548, "type": "image", "page_idx": 109, "img_path": "layout_images/2309.17421v2_273.jpg", "img_caption": "Context image ", "bbox": [121, 340, 372, 518], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows a small turtle in the water with a frog sitting on top of it. Additionally, there is a snail on the frog's head. The turtle, frog, and snail are all captured with their reflections visible in the water."}
{"layout": 1549, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 109, "bbox": [122, 521, 159, 532], "page_size": [612.0, 792.0]}
{"layout": 1550, "type": "text", "text": "Object 1 is a close-up of a snail shell. The shell is a light beige color with a spiral pattern on the top. The shell  appears to be slightly weathered, with some small chips and cracks visible on the edges. The background is  white, making the shell the focal point of the image. The shell is positioned at a slight angle, with the spiral  pattern on the top facing towards the right side of the image.  ", "page_idx": 109, "bbox": [122, 533.7455444335938, 487, 568.9913940429688], "page_size": [612.0, 792.0]}
{"layout": 1551, "type": "text", "text": "Object 2 is a close-up of a snail. The snail has a light orange-colored shell with a spiral pattern. The body of the  snail is a darker orange color with small bumps on its skin. The snail's head is extended forward and its antennae  are visible. The background of the image is white, making the snail the focal point of the image.  ", "page_idx": 109, "bbox": [122, 578.5762939453125, 497.6014099121094, 604.85595703125], "page_size": [612.0, 792.0]}
{"layout": 1552, "type": "text", "text": "Object 3 is a close-up of a frog with a snail on its head. The frog is light green in color with a pinkish hue on its  underside. It has large, bulging eyes that are black and white. The snail is orange and is perched on top of the  frog's head. The frog is sitting on a rock or other flat surface with its legs tucked underneath its body. The  background is white, making the frog and snail stand out.  ", "page_idx": 109, "bbox": [122, 614.4408569335938, 487, 649.6867065429688], "page_size": [612.0, 792.0]}
{"layout": 1553, "type": "text", "text": "Object 4 shows a close-up of a turtle's head and neck, with its body partially visible. The turtle appears to be  floating in water, with its head and neck extended upwards. The turtle's skin is green and its shell is grey with  black stripes. The shell appears to be damaged, with a large chunk missing from the top. The turtle's eyes are  closed and its mouth is slightly open.  ", "page_idx": 109, "bbox": [122, 659.2716064453125, 487, 694.5175170898438], "page_size": [612.0, 792.0]}
{"layout": 1554, "type": "text", "text": "Figure 83: Dense captioning w/ segmentation cut-outs from SAM [ 65 ]  Blue  highlights the references to the context image. Check Section  9.5  for detailed discussions. ", "page_idx": 109, "bbox": [108, 709.4605712890625, 503.99530029296875, 732.37451171875], "page_size": [612.0, 792.0]}
{"layout": 1555, "type": "text", "text": "9.6 Image Generation ", "text_level": 1, "page_idx": 110, "bbox": [107, 74, 208, 84], "page_size": [612.0, 792.0]}
{"layout": 1556, "type": "text", "text": "In this section, we make connections with another prominent area of multimodal research: visual synthesis. By delving into the realm of image generation, we explore how GPT-4V can contribute to this field through various avenues, including evaluation and prompting. ", "page_idx": 110, "bbox": [106, 92.7325439453125, 505, 126.55547332763672], "page_size": [612.0, 792.0]}
{"layout": 1557, "type": "text", "text": "Evaluation of generated images. Figure  66  in Section  8.2  demonstrates the capability of GPT-4V in assessing the aesthetics of images. Here, we show how we employ GPT-4V to evaluate the generated images based on their alignment with the given prompts for text-to-image generation, inspired by RL-Diffusion [ 18 ]. RL-Diffusion leverages a VL model LLAVA [ 79 ] to describe the generated image, followed by text similarity computation between the prompt and the image description using BERT [ 38 ]. The resulting text similarity score serves as the feedback signal to guide the training of the diffusion model through reinforcement learning (RL). Notably, Figures  84 - 85  exhibit how GPT-4V, as a single model, can effectively rate the similarity between the generated image and the prompt. Moreover, GPT-4V provides explanations for the deduction in similarity score, which can potentially be used as a feedback to improve the image generation. ", "page_idx": 110, "bbox": [106, 137.30491638183594, 505, 247.8505096435547], "page_size": [612.0, 792.0]}
{"layout": 1558, "type": "text", "text": "In Figure  84 , we present the evaluation of image similarity using the prompt, “What is happening in the image? From a scale of 1 to 10, rate the similarity between the image and the text prompt ’a parrot driving a car’.” GPT-4V assigns a score of 1 to the most irrelevant image (a dolphin jumping over the water), while rating the most relevant image at the bottom with a score of 9. Notably, the last three images in Figure  84  are shown in RL-Diffusion as gradually improved generation results for the prompt “a parrot driving a car.” The ratings assigned by GPT-4V to these three images   $4\\rightarrow8$   $\\to9$  9) align with the refinement process. ", "page_idx": 110, "bbox": [106, 252.23455810546875, 505, 329.6935119628906], "page_size": [612.0, 792.0]}
{"layout": 1559, "type": "text", "text": "Figure  85  showcases the evaluation of image generation results that involve text rendering on a cake. Leveraging its robust optical character recognition (OCR) capabilities, GPT-4V accurately recognizes the rendered texts in the generated images, such as “Azuze Research,” “ARAUIE,” and “Azure Azure,” and compares them to the text prompt requirement, which is “Azure Research.” ", "page_idx": 110, "bbox": [106, 334.0775451660156, 505, 378.80950927734375], "page_size": [612.0, 792.0]}
{"layout": 1560, "type": "text", "text": "Prompt generation for image editing. In addition to its remarkable ability to evaluate generated images, GPT-4V offers a valuable feature that can greatly enhance image editing. By generating or rewriting the text prompt used for editing, GPT-4V can refine the edited image, resulting in a more visually appealing outcome. Figure  86  provides a demonstration of how we can harness the power of GPT-4V to generate a text prompt specifically tailored for image editing. By providing the original image and text requirements that describe the desired edits, GPT-4V produces an optimized prompt for the task at hand. This optimized prompt takes into account the unique characteristics of the image, ensuring that the subsequent editing process is well-informed and effective. ", "page_idx": 110, "bbox": [106, 389.55889892578125, 505, 478.2864990234375], "page_size": [612.0, 792.0]}
{"layout": 1561, "type": "text", "text": "Moreover, Figure  87  showcases another use case of GPT-4V to improve image editing by rewriting the editing prompt. By considering the original image, the initial prompt, and the edited image, GPT-4V can generate an improved version of the prompt that incorporates the changes made during the previous editing process. One can alternate the processes depicted in Figures  86 - 87 , allowing users to refine their edits repeatedly until they achieve a satisfying outcome. Consequently, this iterative process has the potential to significantly enhance the overall quality of the edited image, providing users with more control and creative freedom in their image editing endeavors. ", "page_idx": 110, "bbox": [106, 482.6695556640625, 505, 560.1295166015625], "page_size": [612.0, 792.0]}
{"layout": 1562, "type": "text", "text": "Evaluation of Generated Images (Prompt-Image Alignment)  ", "text_level": 1, "page_idx": 111, "bbox": [121, 32, 392, 45], "page_size": [612.0, 792.0]}
{"layout": 1563, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 111, "bbox": [121, 49, 159, 59], "page_size": [612.0, 792.0]}
{"layout": 1564, "type": "text", "text": "What is happening in the image? From scale 1 to 10,  decide how similar the image is to the text prompt \"a  parrot driving a car\"? ", "page_idx": 111, "bbox": [122, 60.71717071533203, 299, 86.99688720703125], "page_size": [612.0, 792.0]}
{"layout": 1565, "type": "image", "page_idx": 111, "img_path": "layout_images/2309.17421v2_274.jpg", "bbox": [123, 88, 222, 188], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows a dolphin leaping out of the water. In the background, there is a shoreline with green vegetation. The composition captures the dolphin in mid-air, showcasing its streamlined body and characteristic dorsal fin."}
{"layout": 1566, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 111, "bbox": [122, 192, 159, 202], "page_size": [612.0, 792.0]}
{"layout": 1567, "type": "text", "text": "In the image, there is a dolphin jumping out of the  water. On a scale of 1 to 10, I would rate this image a  1 in similarity to the text prompt \"a parrot driving a  car\" as there is no parrot or car present in the image.  ", "page_idx": 111, "bbox": [122, 204.3507537841797, 299, 239.59661865234375], "page_size": [612.0, 792.0]}
{"layout": 1568, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 111, "bbox": [121, 257, 159, 267], "page_size": [612.0, 792.0]}
{"layout": 1569, "type": "text", "text": "What is happening in the image? From scale 1 to 10,  decide how similar the image is to the text prompt \"a  parrot driving a car\"? ", "page_idx": 111, "bbox": [122, 268.86016845703125, 299, 295.1398620605469], "page_size": [612.0, 792.0]}
{"layout": 1570, "type": "image", "page_idx": 111, "img_path": "layout_images/2309.17421v2_275.jpg", "bbox": [123, 298, 221, 396], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows a colorful parrot, specifically a macaw, perched on the edge of an open car window. The macaw is vibrant, with bright red, green, and some blue plumage, and it is holding onto the window with its claws."}
{"layout": 1571, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 111, "bbox": [122, 400, 159, 410], "page_size": [612.0, 792.0]}
{"layout": 1572, "type": "text", "text": "In the image, there is a parrot perched on the side of  a car. It is not driving the car, but it is on the car. I  would rate this image a 4 in terms of similarity to the  text prompt \"a parrot driving a car\" because there is a  parrot and a car present, but the parrot is not driving  the car.  ", "page_idx": 111, "bbox": [122, 412.4937744140625, 299, 465.67193603515625], "page_size": [612.0, 792.0]}
{"layout": 1573, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 111, "bbox": [316, 49, 353, 59], "page_size": [612.0, 792.0]}
{"layout": 1574, "type": "text", "text": "What is happening in the image? From scale 1 to 10,  decide how similar the image is to the text prompt \"a  parrot driving a car\"? ", "page_idx": 111, "bbox": [316, 60.7760009765625, 492, 87.05571746826172], "page_size": [612.0, 792.0]}
{"layout": 1575, "type": "image", "page_idx": 111, "img_path": "layout_images/2309.17421v2_276.jpg", "bbox": [316, 88, 405, 188], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image is a cartoon illustration of a toucan flying through a scenic background that includes a blue sky with clouds and green trees. The toucan is depicted with a large, colorful beak, and its body is primarily black with some white and orange accents. The scene is vibrant and cheerful."}
{"layout": 1576, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 111, "bbox": [316, 192, 354, 202], "page_size": [612.0, 792.0]}
{"layout": 1577, "type": "text", "text": "In the image, there is a parrot flying in the sky with  trees and clouds in the background. I would rate this  image a 2 in similarity to the text prompt \"a parrot  driving a car\" as there is a parrot present in the  image, but it is not driving a car.  ", "page_idx": 111, "bbox": [316, 204.4095916748047, 492, 248.62161254882812], "page_size": [612.0, 792.0]}
{"layout": 1578, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 111, "bbox": [316, 257, 353, 267], "page_size": [612.0, 792.0]}
{"layout": 1579, "type": "text", "text": "What is happening in the image? From scale 1 to 10,  decide how similar the image is to the text prompt \"a  parrot driving a car\"? ", "page_idx": 111, "bbox": [316, 268.91900634765625, 492, 295.1986999511719], "page_size": [612.0, 792.0]}
{"layout": 1580, "type": "image", "page_idx": 111, "img_path": "layout_images/2309.17421v2_277.jpg", "bbox": [316, 298, 415, 396], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows a green car with artistic depictions of two birds in the background: one bird is red and orange, and the other is a green and white parrot. The design is colorful and stylized."}
{"layout": 1581, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 111, "bbox": [316, 400, 354, 411], "page_size": [612.0, 792.0]}
{"layout": 1582, "type": "text", "text": "In the image, there is a parrot's head superimposed  onto a car driving on a road. The parrot's head is in  the driver's seat and the car appears to be in motion.  On a scale of 1 to 10, I would rate this image an 8 in  terms of similarity to the text prompt \"a parrot driving  a car\". The image clearly shows a parrot in the  driver's seat of a car, but the parrot's body is not  visible and the car is not clearly a \"car\" but more of a  blurred image. ", "page_idx": 111, "bbox": [316, 412.5525817871094, 492, 492.6292419433594], "page_size": [612.0, 792.0]}
{"layout": 1583, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 111, "bbox": [121, 504, 159, 514], "page_size": [612.0, 792.0]}
{"layout": 1584, "type": "text", "text": "What is happening in the image? From scale 1 to 10, decide how similar the image is to the text prompt \"a  parrot driving a car\"? ", "page_idx": 111, "bbox": [122, 515.4295654296875, 474.0131530761719, 532.7431030273438], "page_size": [612.0, 792.0]}
{"layout": 1585, "type": "image", "page_idx": 111, "img_path": "layout_images/2309.17421v2_278.jpg", "bbox": [123, 534, 221, 633], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image features two parrots: one is brightly colored with red and green, sitting on top of a car, and the other is yellow, sitting inside the car at the steering wheel. There are palm trees and a blue sky with green hills in the background, suggesting a tropical setting."}
{"layout": 1586, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 111, "bbox": [122, 638, 159, 648], "page_size": [612.0, 792.0]}
{"layout": 1587, "type": "text", "text": "In the image, there are two parrots, one red and one yellow, on top of a car. The yellow parrot is holding the  steering wheel and appears to be driving the car. On a scale of 1 to 10, I would rate this image a  $\\mathfrak{g}$   in terms of  similarity to the text prompt \"a parrot driving a car\". The only reason it is not a perfect 10 is because there are two  parrots in the image, rather than just one as the prompt suggests.  ", "page_idx": 111, "bbox": [122, 649.45654296875, 497.6611328125, 684.702392578125], "page_size": [612.0, 792.0]}
{"layout": 1588, "type": "text", "text": "Figure 84: Prompt GPT-4V to give a score from 1 to 10 on how similar the generated image is to the prompt.  Blue  highlights the rating given by GPT-4V. The last three images are generated from RL-Diffusion [ 18 ]. Check Section  9.6  for detailed discussions. ", "page_idx": 111, "bbox": [108, 695.884521484375, 504, 731.6945190429688], "page_size": [612.0, 792.0]}
{"layout": 1589, "type": "text", "text": "Sec. 9.6 Image Generation ", "text_level": 1, "page_idx": 112, "bbox": [145, 85, 299, 101], "page_size": [612.0, 792.0]}
{"layout": 1590, "type": "text", "text": "Evaluation of Generated Images (Prompt-Image Alignment)  ", "text_level": 1, "page_idx": 112, "bbox": [121, 107, 392, 119], "page_size": [612.0, 792.0]}
{"layout": 1591, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 112, "bbox": [121, 124, 159, 134], "page_size": [612.0, 792.0]}
{"layout": 1592, "type": "text", "text": "What is happening in the image? From scale 1 to  10, decide how similar the image is to the text  prompt “a cake on the table with word “Azure  Research” written on it\"? ", "page_idx": 112, "bbox": [122, 135.41285705566406, 288, 170.65872192382812], "page_size": [612.0, 792.0]}
{"layout": 1593, "type": "image", "page_idx": 112, "img_path": "layout_images/2309.17421v2_279.jpg", "bbox": [122, 176, 230, 282], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows a square-shaped cake with white icing. On top of the cake, the text \"Azuze Research\" is written, with \"Azuze\" in blue and \"Research\" in black. The cake is placed on a wooden surface, and there is a small \"if\" logo at the bottom right corner of the image."}
{"layout": 1594, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 112, "bbox": [122, 286, 159, 296], "page_size": [612.0, 792.0]}
{"layout": 1595, "type": "text", "text": "In the image, there is a cake on a table with the  words \"Azuze Research\" written on it. I would rate  this image an 8 out of 10 in similarity to the text  prompt \"a cake on the table with word 'Azure  Research' written on it\" because the only  difference is the misspelling of \"Azure\" as  \"Azuze.\"  ", "page_idx": 112, "bbox": [122, 298.2596435546875, 288, 360.40399169921875], "page_size": [612.0, 792.0]}
{"layout": 1596, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 112, "bbox": [121, 400, 158, 409], "page_size": [612.0, 792.0]}
{"layout": 1597, "type": "text", "text": "What is happening in the image? From scale 1 to  10, decide how similar the image is to the text  prompt “a cake on the table with word “Azure  Research” written on it\"? ", "page_idx": 112, "bbox": [122, 410.80206298828125, 288, 446.04791259765625], "page_size": [612.0, 792.0]}
{"layout": 1598, "type": "image", "page_idx": 112, "img_path": "layout_images/2309.17421v2_280.jpg", "bbox": [122, 452, 230, 559], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows a cake on a table. The cake has a blue and brown design, and there is a word written on it that reads \"Azzure\" and \"Azure.\" The table also has some utensils and a cup."}
{"layout": 1599, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 112, "bbox": [121, 565, 159, 575], "page_size": [612.0, 792.0]}
{"layout": 1600, "type": "text", "text": "In the image, there is a cake on a table with the  words \"Azure Aurre\" written on it. I would rate this  image an 8 out of 10 in similarity to the text  prompt \"a cake on the table with word 'Azure  Research' written on it\" because the image does  show a cake on a table with the word \"Azure\"  written on it, but the word \"Research\" is not  present.   ", "page_idx": 112, "bbox": [122, 576.85107421875, 288, 647.9615478515625], "page_size": [612.0, 792.0]}
{"layout": 1601, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 112, "bbox": [316, 124, 354, 134], "page_size": [612.0, 792.0]}
{"layout": 1602, "type": "text", "text": "What is happening in the image? From scale 1 to  10, decide how similar the image is to the text  prompt “a cake on the table with word “Azure  Research” written on it\"? ", "page_idx": 112, "bbox": [316, 135.4716796875, 485, 170.71754455566406], "page_size": [612.0, 792.0]}
{"layout": 1603, "type": "image", "page_idx": 112, "img_path": "layout_images/2309.17421v2_281.jpg", "bbox": [316, 176, 424, 282], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "This image shows a blue frosted cake with decorative pearls on top. There appears to be a plastic fish as part of the decoration. The cake is sitting on a circular board, and the word \"ARAUJE\" is written on the side. In the background, there is another dessert item and an open notebook."}
{"layout": 1604, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 112, "bbox": [316, 286, 353, 296], "page_size": [612.0, 792.0]}
{"layout": 1605, "type": "text", "text": "In the image, there is a blue cake on a table with  the word \"ARAUIE\" written on it. On a scale of 1 to  10, I would rate this image a 6 in terms of similarity  to the text prompt \"a cake on the table with word  'Azure Research' written on it\".  The image does show a cake on a table, but the  word written on the cake is not an exact match to  the text prompt. The color of the cake is also blue,  which could be seen as similar to the word \"Azure\"  in the prompt.   ", "page_idx": 112, "bbox": [316, 298.3184814453125, 485, 387.3612976074219], "page_size": [612.0, 792.0]}
{"layout": 1606, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 112, "bbox": [315, 400, 353, 409], "page_size": [612.0, 792.0]}
{"layout": 1607, "type": "text", "text": "What is happening in the image? From scale 1 to  10, decide how similar the image is to the text  prompt “a cake on the table with word “Azure  Research” written on it\"? ", "page_idx": 112, "bbox": [316, 410.8608703613281, 485, 446.1067199707031], "page_size": [612.0, 792.0]}
{"layout": 1608, "type": "image", "page_idx": 112, "img_path": "layout_images/2309.17421v2_282.jpg", "bbox": [316, 452, 423, 558], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows two layered rectangular cakes. The top cake is decorated with colorful small candies and objects. The bottom cake has the words \"Azure Research\" written on it in blue icing, with a red border and a small decorative element."}
{"layout": 1609, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 112, "bbox": [316, 565, 354, 575], "page_size": [612.0, 792.0]}
{"layout": 1610, "type": "text", "text": "In the image, there is a two-tiered cake on a table  with the words \"Azure Research\" written on it in  blue icing. I would rate this image a 9 out of 10 in  terms of similarity to the text prompt, as it very  closely matches the description given. The only  reason I would not give it a perfect 10 is because  the text prompt does not mention the two tiers or  the additional decorations on the cake.  ", "page_idx": 112, "bbox": [316, 576.909912109375, 485, 648.0203857421875], "page_size": [612.0, 792.0]}
{"layout": 1611, "type": "text", "text": "Figure 85: Prompt GPT-4V to give a score from 1 to 10 on how similar the generated image is to the prompt.  Blue  highlights the rating given by GPT-4V.  Red  (  Green  ) indicate wrong (correct) rendered text. Generated images are from DeepFloyd IF [ 2 ], Midjourney V5.1 [ 4 ], SDXL [ 110 ], and ReCo [ 143 ]. Check Section  9.6  for detailed discussions. ", "page_idx": 112, "bbox": [108, 663.3065795898438, 504, 710.0264892578125], "page_size": [612.0, 792.0]}
{"layout": 1612, "type": "text", "text": "Sec. 9.6 Image Generation ", "text_level": 1, "page_idx": 113, "bbox": [149, 104, 297, 120], "page_size": [612.0, 792.0]}
{"layout": 1613, "type": "text", "text": "Prompt Generation for Image Editing ", "page_idx": 113, "bbox": [122, 126.94731903076172, 290.31256103515625, 137.6796875], "page_size": [612.0, 792.0]}
{"layout": 1614, "type": "text", "text": "Prompt: This is the original image: ", "page_idx": 113, "bbox": [122, 143.06549072265625, 207.02357482910156, 162.823974609375], "page_size": [612.0, 792.0]}
{"layout": 1615, "type": "image", "page_idx": 113, "img_path": "layout_images/2309.17421v2_283.jpg", "bbox": [122, 163.25, 296, 338], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows the upper torso of Michelangelo's famous marble sculpture, \"David.\" The statue is renowned for its detailed depiction of the human form, showcasing muscular structure and anatomical precision. It represents the biblical hero David, known for his victory against Goliath. The sculpture is an iconic work of Renaissance art."}
{"layout": 1616, "type": "text", "text": "I want to make this image look like graphic novel cover. The prompt should be a single sentence in less  than 15 words with descriptive words. For example: “have it look like a graphic novel”. Based on the above prompt examples and the original image, help me to write a better prompt to an  image editing model.  ", "page_idx": 113, "bbox": [122, 344.687255859375, 464.6360778808594, 379.9331359863281], "page_size": [612.0, 792.0]}
{"layout": 1617, "type": "text", "text": "GPT-4V: How about: “Transform this image of a statue into a dramatic graphic novel cover with a dark tone.”  ", "page_idx": 113, "bbox": [122, 389.1694641113281, 450, 408.9279479980469], "page_size": [612.0, 792.0]}
{"layout": 1618, "type": "text", "text": "Instruct Pix2Pix: ", "text_level": 1, "page_idx": 113, "bbox": [121, 427, 198, 438], "page_size": [612.0, 792.0]}
{"layout": 1619, "type": "text", "text": "With original image and prompt “Transform this image of a statue into a  dramatic graphic novel cover with a dark tone” as the input ", "page_idx": 113, "bbox": [125, 442.2376708984375, 450, 461.05804443359375], "page_size": [612.0, 792.0]}
{"layout": 1620, "type": "image", "page_idx": 113, "img_path": "layout_images/2309.17421v2_284.jpg", "img_caption": "Figure 86: Improving the text prompt for image editing, given the original image and textual requirement.  Blue  highlights the suggested editing prompt by GPT-4V. Original image/exemplary editing prompt are from Instruct Pix2Pix [ 22 ]. Check Section  9.6  for detailed discussions. ", "bbox": [107, 465, 505, 691], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image depicts the famous statue of David by Michelangelo. The sculpture is a renowned masterpiece of Renaissance art, known for its detailed depiction of the human form and its representation of the biblical figure, David. The figure is carved from marble and stands in a contrapposto pose, displaying muscular detail and anatomical precision."}
{"layout": 1621, "type": "text", "text": "Sec. 9.6 Image Generation ", "text_level": 1, "page_idx": 114, "bbox": [143, 77, 298, 93], "page_size": [612.0, 792.0]}
{"layout": 1622, "type": "text", "text": "Prompt Generation for Image Editing ", "page_idx": 114, "bbox": [122, 99.99559783935547, 290.31256103515625, 110.72797393798828], "page_size": [612.0, 792.0]}
{"layout": 1623, "type": "text", "text": "Prompt: This is the original image: ", "page_idx": 114, "bbox": [122, 116.11376190185547, 207, 135.87225341796875], "page_size": [612.0, 792.0]}
{"layout": 1624, "type": "image", "page_idx": 114, "img_path": "layout_images/2309.17421v2_285.jpg", "bbox": [123, 138, 220, 234], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows the sculpture of David by Michelangelo. The statue is a renowned masterpiece of Renaissance art, depicting the biblical hero David with remarkable detail in the anatomy. It is carved from marble and exemplifies the ideals of beauty and proportion from that era."}
{"layout": 1625, "type": "text", "text": "The prompt I used to edit this image is “have it look like a graphic novel”. ", "page_idx": 114, "bbox": [122, 237.68052673339844, 363, 246.02792358398438], "page_size": [612.0, 792.0]}
{"layout": 1626, "type": "text", "text": "The edited image I got is: ", "page_idx": 114, "bbox": [122, 255.6128387451172, 207, 263.9602355957031], "page_size": [612.0, 792.0]}
{"layout": 1627, "type": "image", "page_idx": 114, "img_path": "layout_images/2309.17421v2_286.jpg", "bbox": [123, 268, 219, 365], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image features the upper part of Michelangelo's famous sculpture, David, showcasing detailed anatomical structure, particularly the torso and arm holding a slingshot. This Renaissance masterpiece is renowned for its depiction of the biblical hero David, traditionally associated with his fight against Goliath."}
{"layout": 1628, "type": "text", "text": "Tell me how I should change my prompt to make the edited image better. ", "page_idx": 114, "bbox": [122, 368.9707336425781, 363, 377.318115234375], "page_size": [612.0, 792.0]}
{"layout": 1629, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 114, "bbox": [121, 385, 159, 395], "page_size": [612.0, 792.0]}
{"layout": 1630, "type": "text", "text": "Instead of using the prompt “have it look like a graphic novel”, you could try being more specific with the  prompt to guide the AI in the direction you want the image to go. For example, you could use prompts  such as “make the image look like a black and white comic book illustration” or “add bold lines and high  contrast to the image to make it look like a graphic novel cover”. This could help the AI better understand  the aesthetic you are going for and produce a more refined edited image. ", "page_idx": 114, "bbox": [122, 397.1399841308594, 469, 441.35198974609375], "page_size": [612.0, 792.0]}
{"layout": 1631, "type": "text", "text": "Instruct Pix2Pix: ", "text_level": 1, "page_idx": 114, "bbox": [121, 451, 198, 462], "page_size": [612.0, 792.0]}
{"layout": 1632, "type": "text", "text": "With original image and prompt “make  the image look like a black and  white comic book illustration” as  the input ", "page_idx": 114, "bbox": [125, 469.7233581542969, 288, 506.4760437011719], "page_size": [612.0, 792.0]}
{"layout": 1633, "type": "image", "page_idx": 114, "img_path": "layout_images/2309.17421v2_287.jpg", "bbox": [123, 510, 283, 669], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image depicts a famous marble statue of a male figure. It is characterized by its detailed anatomical precision, particularly in the musculature and posture. The statue is often associated with Renaissance art."}
{"layout": 1634, "type": "text", "text": "With original image and prompt “add  bold lines and high contrast to the  image to make it look like a graphic  novel cover” as the input ", "page_idx": 114, "bbox": [316, 470.5953369140625, 479, 507.3480224609375], "page_size": [612.0, 792.0]}
{"layout": 1635, "type": "image", "page_idx": 114, "img_path": "layout_images/2309.17421v2_288.jpg", "bbox": [317, 510, 476, 669], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image depicts a famous sculpture, specifically Michelangelo's Statue of David. The sculpture is known for its detailed representation of the human form, particularly in the depiction of muscles and posture."}
{"layout": 1636, "type": "text", "text": "Figure 87: Improving the editing prompt, given the original image, the editing prompt, and the edited image.  Blue  highlights the suggested editing prompt by GPT-4V. Original image/editing prompt/edited image are from Instruct Pix2Pix [ 22 ]. Check Section  9.6  for detailed discussions. ", "page_idx": 114, "bbox": [108, 682.1035766601562, 503, 717.9144897460938], "page_size": [612.0, 792.0]}
{"layout": 1637, "type": "text", "text": "9.7 Embodied Agent ", "text_level": 1, "page_idx": 115, "bbox": [107, 73, 203, 84], "page_size": [612.0, 792.0]}
{"layout": 1638, "type": "text", "text": "In this section, we delve into the exciting applications and implications of GPT-4V for embodied AI, exploring how it is poised to bridge the gap between multimodal understanding on static inputs and physical interaction with dynamic environments. To provide a concrete illustration, let us consider the scenario of GPT-4V assuming the role of a home robot. Within this context, we witness how it can read the menu to operate household appliances ( e.g ., coffee machine), and perform task-oriented navigation through the house. ", "page_idx": 115, "bbox": [106, 92.7325439453125, 505, 159.28245544433594], "page_size": [612.0, 792.0]}
{"layout": 1639, "type": "text", "text": "Operating machine. Imagine you’ve just acquired a brand-new coffee machine, and to your delight, your trusty home robot, GPT-4V, learns how to operate it on your behalf. In our experiment, we provide GPT-4V with a single image (Figure  88 ) featuring an operating menu with both illustrations and texts. Our task for GPT-4V is to identify the button that corresponds to the “8 OZ coffee” option within the coffee machine’s operating panel. Surprisingly, GPT-4V not only accurately locates the “8 OZ coffee” button but also successfully recognizes the button for   ${}^{**}10\\;\\mathrm{OZ}$   coffee.” However, it mistakenly identifies the power button as the “6 OZ coffee” button, potentially due to the visual confusion caused by the positioning of the “6 OZ coffee” option on both the menu and the coffee machine itself. To address this specific failure case, we devise a solution by isolating the operating menu for each button and presenting them all to GPT-4V in a single prompt (Figure  89 ). With this revised approach, GPT-4V now can recognize the precise position of the “6 OZ coffee” button. ", "page_idx": 115, "bbox": [106, 170.0319061279297, 505, 291.48651123046875], "page_size": [612.0, 792.0]}
{"layout": 1640, "type": "text", "text": "Navigation.  In order to explore navigation capabilities, we utilized Redfin virtual house tour as a means to replicate interactive environments for embodied agents. The objective was to assess the performance of GPT-4V in a task-oriented scenario. To illustrate this, we present an example depicted in Figures  90 - 91 . Initially, we provided GPT-4V with the entry image of a virtual house tour, offering a view from one corner into the living room. The task assigned to GPT-4V was to “go to the kitchen and retrieve an item from the fridge.” Our aim was to prompt GPT-4V to predict the subsequent actions. ", "page_idx": 115, "bbox": [106, 295.51190185546875, 505, 373.3294982910156], "page_size": [612.0, 792.0]}
{"layout": 1641, "type": "text", "text": "In the first step, as shown in the first half of Figure  90 , GPT-4V anticipated the initial action by suggesting to “turn right and move forward towards the hallway.” This prediction was based on GPT-4V’s hypothesis that the kitchen would likely be located in that direction. We then manually executed this action using the visual house touring portal, capturing the resulting view after the action was taken. This view was then used to prompt GPT-4V for the next action, as displayed in the second half of Figure  90 . It’s important to note that throughout the process, we maintained a record of the immediate previous turn to provide context for GPT-4V’s subsequent actions. ", "page_idx": 115, "bbox": [106, 377.71356201171875, 505, 455.1724853515625], "page_size": [612.0, 792.0]}
{"layout": 1642, "type": "text", "text": "As the navigation process unfolded, we successfully reached the fridge within the third turn, as indicated by the query image in the second half of Figure  91 . The final action predicted by GPT-4V was to “move forward and slightly to the left in order to align myself with the fridge door. Then, use my robotic arm to open the fridge door and retrieve the requested item.” This decisive action marked the accomplishment of GPT-4V in this task-oriented navigation scenario. ", "page_idx": 115, "bbox": [106, 459.5565490722656, 505, 515.197509765625], "page_size": [612.0, 792.0]}
{"layout": 1643, "type": "text", "text": "Sec. 9.7 Embodied Agent ", "page_idx": 116, "bbox": [144.62362670898438, 76.90789794921875, 280.1152038574219, 91.09264373779297], "page_size": [612.0, 792.0]}
{"layout": 1644, "type": "text", "text": "Operating Machine ", "text_level": 1, "page_idx": 116, "bbox": [122, 98, 210, 110], "page_size": [612.0, 792.0]}
{"layout": 1645, "type": "text", "text": "Prompt: First, read the operating menu below for coffee machine buttons. ", "page_idx": 116, "bbox": [122, 114.50176239013672, 335.96435546875, 134.26025390625], "page_size": [612.0, 792.0]}
{"layout": 1646, "type": "image", "page_idx": 116, "img_path": "layout_images/2309.17421v2_289.jpg", "img_caption": "Menu: ", "bbox": [121, 143, 289, 317], "page_size": [612.0, 792.0], "ocr_text": "é\n\nD 602\n(dd\n\nSy\n", "vlm_text": "The image shows a menu with three options for beverage sizes, indicated by coffee cup icons and their corresponding fluid ounces: 6 oz, 8 oz, and 10 oz."}
{"layout": 1647, "type": "text", "text": "Now, let's look at the operating panel of the coffee machine in the image below, which button should I use for  a 8 OZ coffee: left, right, top or bottom button? ", "page_idx": 116, "bbox": [122, 325.7301330566406, 482, 343.043701171875], "page_size": [612.0, 792.0]}
{"layout": 1648, "type": "image", "page_idx": 116, "img_path": "layout_images/2309.17421v2_290.jpg", "bbox": [123, 351, 263, 604], "page_size": [612.0, 792.0], "ocr_text": "JoooDece\n\n72\ney ©\nsCa\n\n", "vlm_text": "This is an image of a coffee machine. The display on the machine has icons likely representing different coffee options and the name \"JoooDeee\" is visible at the top. The design appears to be modern, featuring sleek metal sides and a drip tray at the bottom."}
{"layout": 1649, "type": "text", "text": "GPT-4V: You should use the bottom button for an  $8\\,\\mathrm{OZ}$   coffee. According to the menu, the bottom button corresponds  to a 8 OZ coffee, while the top button is for a 6 OZ coffee and the right button is for a 10 OZ coffee. ", "page_idx": 116, "bbox": [122, 608.2709350585938, 482, 636.9955444335938], "page_size": [612.0, 792.0]}
{"layout": 1650, "type": "text", "text": "Figure 88: Reading a full menu of coffee machine buttons, GPT-4V recognizes which button to choose for  $8\\,\\mathrm{OZ}$   coffee.  Green  (  Red  ) highlights the correct (wrong) answer. Check Section  9.7  for detailed discussions. ", "page_idx": 116, "bbox": [108, 653.8065185546875, 504, 689.6174926757812], "page_size": [612.0, 792.0]}
{"layout": 1651, "type": "text", "text": "Operating Machine ", "text_level": 1, "page_idx": 117, "bbox": [122, 47, 210, 58], "page_size": [612.0, 792.0]}
{"layout": 1652, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 117, "bbox": [121, 63, 159, 73], "page_size": [612.0, 792.0]}
{"layout": 1653, "type": "text", "text": "First, read the operating menus below for coffee machine buttons. Menu 1: ", "page_idx": 117, "bbox": [122, 74.89026641845703, 339.70025634765625, 92.20382690429688], "page_size": [612.0, 792.0]}
{"layout": 1654, "type": "image", "page_idx": 117, "img_path": "layout_images/2309.17421v2_291.jpg", "bbox": [121, 91.25, 296, 366], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image displays three menu options, each featuring an icon of a steaming cup and a corresponding size in ounces. Menu 1 shows \"6 OZ\", Menu 2 shows \"8 OZ\", and Menu 3 shows \"10 OZ\". These options likely represent different serving sizes for a beverage, possibly coffee."}
{"layout": 1655, "type": "text", "text": "Now, let's look at the operating panel of the coffee machine in the image below, which button should I use for  a 6 OZ coffee: left, right, top or bottom button? ", "page_idx": 117, "bbox": [122, 370.7735595703125, 482.64312744140625, 388.08709716796875], "page_size": [612.0, 792.0]}
{"layout": 1656, "type": "image", "page_idx": 117, "img_path": "layout_images/2309.17421v2_292.jpg", "img_caption": "GPT-4V: You should use the left button for a 6 OZ coffee. ", "bbox": [121, 392, 281, 675], "page_size": [612.0, 792.0], "ocr_text": "JoooDeee\nta ©.\n7.\"\n)\n\n", "vlm_text": "The image shows a coffee maker with the brand name \"JoooDeee\" displayed on it. It has a control panel with three buttons that likely correspond to different coffee sizes. The caption suggests using the left button for a 6 oz coffee."}
{"layout": 1657, "type": "text", "text": "Figure 89: Converting the full menu of coffee machine buttons to interleaved image-text instructions, GPT-4V can recognizes which button to choose for  $6~\\mathrm{OZ}$   coffee, which GPT-4V failed to do so with full menu instruction.  Green  highlights the correct answer. Check Section  9.7  for detailed discussions. ", "page_idx": 117, "bbox": [107, 687.1085815429688, 505, 733.8275146484375], "page_size": [612.0, 792.0]}
{"layout": 1658, "type": "text", "text": "Sec. 9.7 Embodied Agent ", "text_level": 1, "page_idx": 118, "bbox": [141, 77, 280, 90], "page_size": [612.0, 792.0]}
{"layout": 1659, "type": "text", "text": "Navigation ", "text_level": 1, "page_idx": 118, "bbox": [122, 98, 172, 109], "page_size": [612.0, 792.0]}
{"layout": 1660, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 118, "bbox": [121, 114, 159, 124.75], "page_size": [612.0, 792.0]}
{"layout": 1661, "type": "text", "text": "Imagine that you are a home robot, and is asked to go to the kitchen to fetch something from the fridge. The image below shows your current position. Please plan your next action. ", "page_idx": 118, "bbox": [122, 125.91297912597656, 463, 143.22653198242188], "page_size": [612.0, 792.0]}
{"layout": 1662, "type": "image", "page_idx": 118, "img_path": "layout_images/2309.17421v2_293.jpg", "bbox": [123, 147, 480, 298], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows a cozy living room with a modern yet inviting design. Key features include:\n\n- A wooden floor that adds warmth to the room.\n- A matching set of tan leather couches: a loveseat and a larger sofa.\n- A large flat-screen TV mounted on the wall facing the sofas. \n- A neatly arranged, minimalistic TV console beneath the television.\n- A decorative rug centered in the seating area.\n- A window that allows natural light to brighten the space, with curtains hanging on either side.\n- A few pieces of furniture for added functionality, like a side table with a lamp and a smaller table opposite it.\n- Potted plants placed strategically, adding a touch of greenery.\n- Shelves on the wall with decorations and the word \"Thankful\" prominently displayed.\n- The entrance door to the room has a glass window with a \"360\" decal on it.\n- Recessed lighting in the ceiling provides ample illumination."}
{"layout": 1663, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 118, "bbox": [121, 304, 159, 314], "page_size": [612.0, 792.0]}
{"layout": 1664, "type": "text", "text": "My next action would be turn right and move forward towards the hallway, as I can see that the kitchen is  likely located in that direction. I would then continue to navigate through the hallway until I reach the kitchen  and locate the fridge. ", "page_idx": 118, "bbox": [122, 316.1286926269531, 478.86260986328125, 342.40838623046875], "page_size": [612.0, 792.0]}
{"layout": 1665, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 118, "bbox": [121, 361, 159, 371], "page_size": [612.0, 792.0]}
{"layout": 1666, "type": "text", "text": "Imagine that you are a home robot, and is asked to go to the kitchen to fetch something from the fridge. At the previous turn, you were at the position shown in the below image: ", "page_idx": 118, "bbox": [122, 372.48236083984375, 463, 389.7958984375], "page_size": [612.0, 792.0]}
{"layout": 1667, "type": "image", "page_idx": 118, "img_path": "layout_images/2309.17421v2_294.jpg", "bbox": [123, 395, 218, 435], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "This image depicts a living room. It features a couch and a loveseat arranged around a rug with a television mounted above a fireplace. There is a window letting in natural light, various pieces of furniture, and decorations around the room, including plants on a shelf. The flooring appears to be hardwood."}
{"layout": 1668, "type": "text", "text": "Note: Same image from the previous turn, resized  to save space ", "page_idx": 118, "bbox": [249, 405.80413818359375, 466, 424.62451171875], "page_size": [612.0, 792.0]}
{"layout": 1669, "type": "text", "text": "The action taken in previous turn is: turn right and move forward towards the hallway. ", "page_idx": 118, "bbox": [122, 441.00946044921875, 402.7696838378906, 449.3568420410156], "page_size": [612.0, 792.0]}
{"layout": 1670, "type": "text", "text": "Now you are at the hallway, this is what you can see from there. Can you see the kitchen from here? What is  your next action? ", "page_idx": 118, "bbox": [122, 458.9418029785156, 482.42645263671875, 476.2553405761719], "page_size": [612.0, 792.0]}
{"layout": 1671, "type": "image", "page_idx": 118, "img_path": "layout_images/2309.17421v2_295.jpg", "bbox": [122, 478, 480, 629], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows an interior of a dining area that includes a wooden table with chairs, a wall mirror, and bar stools at a kitchen counter. There are wooden floors and some decor elements like wall art, hanging plants, and possibly a small table with items on it. The space has a cozy and welcoming feel."}
{"layout": 1672, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 118, "bbox": [122, 637, 159, 647], "page_size": [612.0, 792.0]}
{"layout": 1673, "type": "text", "text": "From the current position, I can see the kitchen on the right side, My next action would be to turn right and  move forward towards the kitchen. ", "page_idx": 118, "bbox": [122, 649.1575317382812, 473.9472351074219, 666.4710693359375], "page_size": [612.0, 792.0]}
{"layout": 1674, "type": "text", "text": "Figure 90: Acting as an embodied agent to navigate through a house to fetch something from the fridge (the 1st and 2nd turn).  Blue  highlights the predicted actions. Check Section  9.7  for detailed discussions. ", "page_idx": 118, "bbox": [108, 684.2275390625, 504, 720.0385131835938], "page_size": [612.0, 792.0]}
{"layout": 1675, "type": "text", "text": "Sec. 9.7 Embodied Agent ", "text_level": 1, "page_idx": 119, "bbox": [139, 39, 281, 54], "page_size": [612.0, 792.0]}
{"layout": 1676, "type": "text", "text": "Navigation ", "text_level": 1, "page_idx": 119, "bbox": [122, 61, 172, 73], "page_size": [612.0, 792.0]}
{"layout": 1677, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 119, "bbox": [122, 78, 159, 87.75], "page_size": [612.0, 792.0]}
{"layout": 1678, "type": "text", "text": "Imagine that you are a home robot, and is asked to go to the kitchen to fetch something from the fridge. At the previous turn, you were at the position shown in the below image: ", "page_idx": 119, "bbox": [122, 89.1221923828125, 463.073486328125, 106.43575286865234], "page_size": [612.0, 792.0]}
{"layout": 1679, "type": "image", "page_idx": 119, "img_path": "layout_images/2309.17421v2_296.jpg", "bbox": [122, 112, 209, 150], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image depicts an interior space, likely a dining area. There is a wooden floor and a dining table with four chairs, with the chairs having white frames and wooden seats. A round mirror is mounted on the wall above the table, and there are two light fixtures on either side of the mirror. On the left side of the image, there are some plants, both hanging and potted. On the right side, a countertop with two stools is visible, suggesting the presence of a kitchen area adjacent to the dining space. The overall decor features a mix of natural and bright elements."}
{"layout": 1680, "type": "text", "text": "Note: Same image from the previous turn, resized  to save space ", "page_idx": 119, "bbox": [249, 122.3850326538086, 466, 141.20535278320312], "page_size": [612.0, 792.0]}
{"layout": 1681, "type": "text", "text": "The action taken in previous turn is: turn right and move forward towards the kitchen. ", "page_idx": 119, "bbox": [122, 154.38824462890625, 402.0740051269531, 162.7356414794922], "page_size": [612.0, 792.0]}
{"layout": 1682, "type": "text", "text": "Now you are at the kitchen, this is what you can see from there. Can you see the fridge from here? What is  your next action? ", "page_idx": 119, "bbox": [122, 172.320556640625, 476.7469177246094, 189.63412475585938], "page_size": [612.0, 792.0]}
{"layout": 1683, "type": "image", "page_idx": 119, "img_path": "layout_images/2309.17421v2_297.jpg", "bbox": [123, 194, 479, 346], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows a kitchen with white cabinets and wood accents. There's a small island with two wooden stools, a sink with a window above it, and various kitchen appliances such as a microwave and stove. There's also a refrigerator and a countertop with items like a water dispenser and a blender. The floor is made of wood or wood-like material, and there are some wall-mounted planters on the left side."}
{"layout": 1684, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 119, "bbox": [122, 351, 159, 361], "page_size": [612.0, 792.0]}
{"layout": 1685, "type": "text", "text": "From this position, I can see the fridge on the right side of the kitchen. My next action would be to move  forward and turn slightly to the right to approach the fridge.  ", "page_idx": 119, "bbox": [122, 362.5362854003906, 465, 379.8498229980469], "page_size": [612.0, 792.0]}
{"layout": 1686, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 119, "bbox": [121, 394, 159, 404.75], "page_size": [612.0, 792.0]}
{"layout": 1687, "type": "text", "text": "Imagine that you are a home robot, and is asked to go to the kitchen to fetch something from the fridge. At the previous turn, you were at the position shown in the below image: ", "page_idx": 119, "bbox": [122, 406.0811767578125, 465, 423.39471435546875], "page_size": [612.0, 792.0]}
{"layout": 1688, "type": "image", "page_idx": 119, "img_path": "layout_images/2309.17421v2_298.jpg", "bbox": [123, 427, 208, 465], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "This image depicts a kitchen interior. Here are some features visible in the image:\n\n1. There is a kitchen island with two high stools with wooden seats and white backs.\n2. Various kitchen appliances are visible, such as a microwave and a stove/oven unit.\n3. There appears to be a water dispenser or jug on the countertop.\n4. Kitchen cabinets with white fronts and perhaps wooden trims surround the area.\n5. The flooring appears to be made of wood or wood-like material.\n6. Some additional accessories like a hanging wall basket and countertop items can also be seen, likely for storage or decoration purposes.\n\nOverall, it appears to be a well-organized, modern kitchen."}
{"layout": 1689, "type": "text", "text": "Note: Same image from the previous turn, resized  to save space ", "page_idx": 119, "bbox": [249, 436.2005920410156, 466, 455.02093505859375], "page_size": [612.0, 792.0]}
{"layout": 1690, "type": "text", "text": "The action taken in previous turn is: move forward and turn slightly to the right to approach the fridge. Now this is what you can see from there. What is your next action? ", "page_idx": 119, "bbox": [122, 471.4060363769531, 456.0201721191406, 479.75341796875], "page_size": [612.0, 792.0]}
{"layout": 1691, "type": "text", "text": "", "page_idx": 119, "bbox": [122, 489.33837890625, 342.7056884765625, 497.6857604980469], "page_size": [612.0, 792.0]}
{"layout": 1692, "type": "image", "page_idx": 119, "img_path": "layout_images/2309.17421v2_299.jpg", "bbox": [122, 502, 480, 654], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image depicts a kitchen scene. On the left, there is a gas stove with four burners, and above it is a built-in range hood. Beside the stove, there is a countertop with various items, including bottles and containers, indicating cooking supplies. Above the stove is a decorative sign with an inspirational or religious message. To the right of the stove is a large white refrigerator with a couple of magnets or decorative items attached to the door. There are cabinets above both the stove and the refrigerator. The image is framed so that the viewer is looking at a corner of the kitchen."}
{"layout": 1693, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 119, "bbox": [122, 661, 158, 671], "page_size": [612.0, 792.0]}
{"layout": 1694, "type": "text", "text": "My next action would be to move forward and slightly to the left to align myself with the fridge door. Then, I  would use my robotic arm to open the fridge door and retrieve the requested item.  ", "page_idx": 119, "bbox": [122, 673.1497192382812, 474.6311950683594, 690.4632568359375], "page_size": [612.0, 792.0]}
{"layout": 1695, "type": "text", "text": "Figure 91: Acting as an embodied agent to navigate through a house to fetch something from the fridge (the 3rd and 4th turn).  Blue  highlights the predicted actions. Check Section  9.7  for detailed discussions. ", "page_idx": 119, "bbox": [108, 705.017578125, 503, 740.8284912109375], "page_size": [612.0, 792.0]}
{"layout": 1696, "type": "text", "text": "9.8 GUI Navigation ", "text_level": 1, "page_idx": 120, "bbox": [107, 73, 199, 84], "page_size": [612.0, 792.0]}
{"layout": 1697, "type": "text", "text": "Beyond navigating the physical world, this section showcases the capability of GPT-4V to interact with and navigate through the Graphical User Interface (GUI) of a computer or smartphone. We explore the potential for GPT-4V to complete complex tasks, such as web browsing, online shopping, and  etc . ", "page_idx": 120, "bbox": [107, 92.7325439453125, 505, 137.46446228027344], "page_size": [612.0, 792.0]}
{"layout": 1698, "type": "text", "text": "Web browsing. We assess the performance of GPT-4V on computer GUI navigation under a task-oriented setting. The model was provided with the screenshot of current computer screen, the end goal of the navigation ( e.g ., finding a cooking recipe or reading today’s news), the list of possible actions ( e.g ., move the mouse, click an icon with the mouse, or type some texts with the keyboard). The model is then instructed to predict the subsequent actions (refer to Figure  92  for a complete prompt example). Upon the model’s prediction, we manually execute the predicted action and capture a screenshot, which served as the input for GPT-4V for the next turn. When the predicted action is to move the mouse, GPT-4V is specifically instructed to detail the mouse’s position. Hence, the predicted actions are grounded, showing the potential of automating the whole process without human in the loop. ", "page_idx": 120, "bbox": [107, 148.2139129638672, 505, 258.759521484375], "page_size": [612.0, 792.0]}
{"layout": 1699, "type": "text", "text": "In Figures  92 - 96 , GPT-4V predicts reasonable actions to operate a computer GUI, and finally accomplish the end goal of finding a recipe of Mapo Tofu and print out a copy of the recipe in Figure  95 . We then provide GPT-4V a screenshot of the printed recipe and ask it to describe the printout as detailed as possible. As shown in Figure  96 , GPT-4V is able to recognize the details presented in the printout, including the cooking time, the list of ingredients, the author of the recipe, the link to the original recipe and  etc . Figures  97 - 102  present how GPT-4V can navigate through GUI to browse the web to “read today’s news”. Despite the minor errors in Figure  100  when it tries to return to the previous search result page to continue browsing for more news articles, GPT-4V can perform the navigation and read two news articles reasonably well. ", "page_idx": 120, "bbox": [107, 263.1435546875, 505, 362.4205017089844], "page_size": [612.0, 792.0]}
{"layout": 1700, "type": "text", "text": "Online shopping. Figures  103 - 111  illustrates how GPT-4V can navigate a smartphone GUI for online shopping. Similarly, we provide GPT-4V with the screenshot of the current phone screen, the list of possible actions ( e.g ., move your finger to an icon, click an icon with your finger, scroll down a screen, or type some texts with the keyboard) and ask it to predict the subsequent actions to shop for an ergonomic keyboard with a budget between   $\\S50$   and   $\\S100$  . GPT-4V predicts to open the Amazon app (Figure  103 ), search ergonomic keyboard (Figure  104 ), open the filter options (Figure  105 ), set the price range filter between  $\\S50$   and   $\\S100$   (Figure  106 ), show filtered results (Figure  107 ), select the top search result (Figure  108 ), view product details (Figure  109 ), add product to the shopping cart (Figure  110 ) and finally proceed to checkout (Figure  111 ). ", "page_idx": 120, "bbox": [107, 373.1708984375, 505, 472.8064880371094], "page_size": [612.0, 792.0]}
{"layout": 1701, "type": "text", "text": "Notification understanding. Notifications are integral to modern human-computer interactions. GPT-4V has demonstrated its capacity to interpret notification content and respond accordingly. As shown in Figure  112 , the model can read and respond to a notification, such as suggesting to open the Maps app in response to a meeting proposal in Seattle. It also handles call (Figure  113 ) and message (Figure  114 ) notifications on a computer screen effectively. ", "page_idx": 120, "bbox": [107, 483.5569152832031, 505, 539.5565185546875], "page_size": [612.0, 792.0]}
{"layout": 1702, "type": "text", "text": "Watching videos. Alongside web browsing, videos are a key source of online information. GPT-4V has shown its capability to describe video content based on a series of screenshots from popular short-form videos. Regardless of whether the video has subtitle overlay (Figure  115  and  116 ) or not (Figure  117 ,  118 ,  119 ), GPT-4V can generate insightful descriptions about the video content, demonstrating its potential in automatic transcript generation for user-generated video content. ", "page_idx": 120, "bbox": [107, 550.305908203125, 505, 606.3065185546875], "page_size": [612.0, 792.0]}
{"layout": 1703, "type": "text", "text": "Sec. 9.8 GUI Navigation ", "text_level": 1, "page_idx": 121, "bbox": [144, 60, 279, 75], "page_size": [612.0, 792.0]}
{"layout": 1704, "type": "text", "text": "Web Browsing (Mapo Tofu Recipe) ", "text_level": 1, "page_idx": 121, "bbox": [122, 82, 281, 93], "page_size": [612.0, 792.0]}
{"layout": 1705, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 121, "bbox": [121, 98, 159, 108.75], "page_size": [612.0, 792.0]}
{"layout": 1706, "type": "text", "text": "Imagine that you are a robot operating a computer. Like how humans operate the computer, you can move the  mouse, click an icon with the mouse, or type some texts with the keyboard. ", "page_idx": 121, "bbox": [122, 109.85193634033203, 488, 128.5104217529297], "page_size": [612.0, 792.0]}
{"layout": 1707, "type": "text", "text": "You are asked to find a detailed recipe of Mapo Tofu. ", "page_idx": 121, "bbox": [122, 133.4201202392578, 296, 141.76751708984375], "page_size": [612.0, 792.0]}
{"layout": 1708, "type": "text", "text": "Below is what you see on the computer screen, predict your next move. If the action involves moving the mouse,  describe the location as detailed as possible. ", "page_idx": 121, "bbox": [122, 146.67723083496094, 495.4569091796875, 165.33570861816406], "page_size": [612.0, 792.0]}
{"layout": 1709, "type": "image", "page_idx": 121, "img_path": "layout_images/2309.17421v2_300.jpg", "bbox": [122, 170, 280, 300], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "This image shows a computer desktop background featuring a scenic landscape of a purple flower field, likely lavender, with a stone wall in the foreground and trees in the background under a clear sky. There are icons on the left side of the screen, indicating various applications and software."}
{"layout": 1710, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 121, "bbox": [122, 304, 159, 314], "page_size": [612.0, 792.0]}
{"layout": 1711, "type": "text", "text": "Next move: I would move the mouse to the Google Chrome icon located on the left side of the screen,  approximately 1/3 of the way down from the top. I would then click on the icon to open the web browser.  ", "page_idx": 121, "bbox": [122, 316.07867431640625, 469, 333.3922119140625], "page_size": [612.0, 792.0]}
{"layout": 1712, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 121, "bbox": [121, 345, 159, 355], "page_size": [612.0, 792.0]}
{"layout": 1713, "type": "text", "text": "Imagine that you are a robot operating a computer. Like how humans operate the computer, you can move the  mouse, click an icon with the mouse, or type some texts with the keyboard.  ", "page_idx": 121, "bbox": [122, 356.42132568359375, 488, 373.73486328125], "page_size": [612.0, 792.0]}
{"layout": 1714, "type": "text", "text": "You are asked to find a detailed recipe of Mapo Tofu. ", "page_idx": 121, "bbox": [122, 376.9154052734375, 296, 385.2627868652344], "page_size": [612.0, 792.0]}
{"layout": 1715, "type": "text", "text": "Previous action: \"I would move the mouse to the Google Chrome icon located on the left side of the screen,  approximately   $1/3$   of the way down from the top. I would then click on the icon to open the web browser. \" You have now opened the web browser. Below is what you see on the computer screen. What will you do next?  If the action involves moving the mouse, describe the location as detailed as possible. ", "page_idx": 121, "bbox": [122, 388.4433288574219, 491, 426.2509460449219], "page_size": [612.0, 792.0]}
{"layout": 1716, "type": "image", "page_idx": 121, "img_path": "layout_images/2309.17421v2_301.jpg", "bbox": [123, 432, 476, 613], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows a web browser's new tab page open to Google. In the background, there's a blurred cityscape with bokeh lights, focusing on wet pavement. The Google search bar is in the center, and below it are icons for Gmail, Drive, Course Planner, and other services. The address bar has YouTube, Maps, and Gmail shortcuts. The background photo is credited to Ev Tchebotarev."}
{"layout": 1717, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 121, "bbox": [122, 617, 158, 626], "page_size": [612.0, 792.0]}
{"layout": 1718, "type": "text", "text": "Next, I would move the mouse to the search bar located in the center of the screen, approximately  $_{1/2}$   of the  way down from the top. I would then click on the search bar and type in \"Mapo Tofu recipe\" using the keyboard.  After typing in the search query, I would press the \"Enter\" key on the keyboard to initiate the search.  ", "page_idx": 121, "bbox": [122, 628, 491, 654.740966796875], "page_size": [612.0, 792.0]}
{"layout": 1719, "type": "text", "text": "Figure 92: GPT-4V navigates through GUI to browse the web to search for the recipe of Mapo Tofu. Blue highlights the predicted actions. Check Section  9.8  for detailed discussions. ", "page_idx": 121, "bbox": [108, 668.1665649414062, 505.7424621582031, 693.0684814453125], "page_size": [612.0, 792.0]}
{"layout": 1720, "type": "text", "text": "Sec. 9.8 GUI Navigation ", "text_level": 1, "page_idx": 122, "bbox": [145, 66, 281, 81], "page_size": [612.0, 792.0]}
{"layout": 1721, "type": "text", "text": "Web Browsing (Mapo Tofu Recipe) ", "text_level": 1, "page_idx": 122, "bbox": [122, 87, 281, 99], "page_size": [612.0, 792.0]}
{"layout": 1722, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 122, "bbox": [122, 104, 159, 113.75], "page_size": [612.0, 792.0]}
{"layout": 1723, "type": "text", "text": "Imagine that you are a robot operating a computer. Like how humans operate the computer, you can move the  mouse, click an icon with the mouse, or type some texts with the keyboard.  ", "page_idx": 122, "bbox": [122, 115.45569610595703, 489, 134.11416625976562], "page_size": [612.0, 792.0]}
{"layout": 1724, "type": "text", "text": "You are asked to find a detailed recipe of Mapo Tofu. ", "page_idx": 122, "bbox": [122, 139.0238800048828, 296.2813415527344, 147.37127685546875], "page_size": [612.0, 792.0]}
{"layout": 1725, "type": "text", "text": "Previous action: \" I would move the mouse to the search bar located in the center of the screen, approximately  1/2 of the way down from the top. I would then click on the search bar and type in \"Mapo Tofu recipe\" using the  keyboard. After typing in the search query, I would press the \"Enter\" key on the keyboard to initiate the search.\" ", "page_idx": 122, "bbox": [122, 152.28099060058594, 489, 181.25054931640625], "page_size": [612.0, 792.0]}
{"layout": 1726, "type": "text", "text": "You have now completed the previous action as described above. Below is what you see on the computer  screen. What will you do next? If the action involves moving the mouse, describe the location as detailed as  possible. ", "page_idx": 122, "bbox": [122, 186.16026306152344, 479.2804870605469, 215.12982177734375], "page_size": [612.0, 792.0]}
{"layout": 1727, "type": "image", "page_idx": 122, "img_path": "layout_images/2309.17421v2_302.jpg", "bbox": [125, 222, 440, 607], "page_size": [612.0, 792.0], "ocr_text": "Google —_maportotu recipe x $a\n\nVideos Vegetarian Simple. ~— Japanese. ~— Korean. Not Images | Madewithlau Recipe\n\nAbout 9,850,000 results (0.41 seconds)\n\n‘The Woks of Life\nhitps:/thewoksofife.com Recipes Tofu 3\n\nMapo Tofu Recipe: The Real Deal\n\nJun 28, 2019 — Mapo Tofuis @ popular Chinese dish from Sichuan Province, where\nspicy food is king and the signature spice of the region--the Sichuan\n\nte te  # Rating: 4.9- 299 votes - 35 min\n\nVegan Mapo Tofu - Cheat's Mapo Tofu Packet - Tofu, Bean Curd & Seitan\n\nRecipes :\n\n”,\n\nMapo Tofu Authentic Mapo Tofu (#2 © Mapo Tofu\n\nThe Woks of Life 1) NYT Cooking - The New York T.\n4.9 sete tee (299) Omnivore's Cookbook 4.0 tee eH * (2K)\n\n35min 49 tek tee & (69) 30 min\n\n‘Spicy bean sauce, ground pork, | 25min Bean sauce, ground beef,\nsilken tofu, sesame oil, chicken Five spice powder, ground pork, _fetmented black beans, soy\n\nsoy sauce, homemade chili oil\n\nShow more v\n\nfe] The New York Times\n\nhttps://cooking nytimes.com > 1021459-mapo-tofu }\nMapo Tofu Recipe - NYT Cooking\nIngredients - 16ounces medium or medium-firm tofu (if unavailable, go with firm) 7\n‘rounded teaspoon Sichuan peppercorns - 3tablespoons canola oil - 6ounces ground\n+t & tee» Rating: 4 2,040 votes - 30 min\n\nPeople also ask\n\nWhat is mapo tofu sauce made of? v\nIs Szechuan tofu the same as mapo tofu? v\n\nWhy do people like mapo tofu? v\n\n", "vlm_text": "The image shows a Google search results page for \"mapo tofu recipe.\" It includes links to various mapo tofu recipes from different websites such as The Woks of Life and The New York Times. There are ratings, brief descriptions, and images of the dish. The page also includes a \"People also ask\" section with related questions."}
{"layout": 1728, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 122, "bbox": [122, 612, 159, 622], "page_size": [612.0, 792.0]}
{"layout": 1729, "type": "text", "text": "Next action: \"I would move the mouse to the first search result under the \"Recipes\" section, which is titled \"Mapo  Tofu Recipe - The Woks of Life\". I would then click on the link to open the webpage and view the detailed  recipe.\"  ", "page_idx": 122, "bbox": [122, 624.4584350585938, 494.9067077636719, 650.7381591796875], "page_size": [612.0, 792.0]}
{"layout": 1730, "type": "text", "text": "Figure 93: GPT-4V navigates through GUI to browse the web to search for the recipe of Mapo Tofu. Blue highlights the predicted actions. Check Section  9.8  for detailed discussions. ", "page_idx": 122, "bbox": [108, 662.5625610351562, 505.7424621582031, 687.4644775390625], "page_size": [612.0, 792.0]}
{"layout": 1731, "type": "text", "text": "Sec. 9.8 GUI Navigation ", "text_level": 1, "page_idx": 123, "bbox": [144, 66, 281, 81], "page_size": [612.0, 792.0]}
{"layout": 1732, "type": "text", "text": "Web Browsing (Mapo Tofu Recipe) ", "text_level": 1, "page_idx": 123, "bbox": [122, 87, 281, 99], "page_size": [612.0, 792.0]}
{"layout": 1733, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 123, "bbox": [121, 104, 159, 113.75], "page_size": [612.0, 792.0]}
{"layout": 1734, "type": "text", "text": "Imagine that you are a robot operating a computer. Like how humans operate the computer, you can move  the mouse, click an icon with the mouse, or type some texts with the keyboard.  ", "page_idx": 123, "bbox": [122, 115.45569610595703, 475, 134.11416625976562], "page_size": [612.0, 792.0]}
{"layout": 1735, "type": "text", "text": "You are asked to find a detailed recipe of Mapo Tofu. ", "page_idx": 123, "bbox": [122, 139.0238800048828, 296.2813415527344, 147.37127685546875], "page_size": [612.0, 792.0]}
{"layout": 1736, "type": "text", "text": "Previous action: “I would move the mouse to the first search result under the \"Recipes\" section, which is  titled \"Mapo Tofu Recipe - The Woks of Life\". I would then click on the link to open the webpage and view the  detailed recipe.\" ", "page_idx": 123, "bbox": [122, 152.28099060058594, 482, 181.25054931640625], "page_size": [612.0, 792.0]}
{"layout": 1737, "type": "text", "text": "You have now completed the previous action as described above. Below is what you see on the computer  screen. Based on the image below, what will you do next? If the action involves moving the mouse, describe  the location as detailed as possible. ", "page_idx": 123, "bbox": [122, 186.16026306152344, 482, 215.12982177734375], "page_size": [612.0, 792.0]}
{"layout": 1738, "type": "image", "page_idx": 123, "img_path": "layout_images/2309.17421v2_303.jpg", "bbox": [127, 223, 343, 265.75], "page_size": [612.0, 792.0], "ocr_text": "home > recipes > tofu > mapo tofu\n\nMAPO TOFU\n\n$ 8y: KAITLIN 894 COMMENTS\n\nJUMP TO RECIPE &\n\n", "vlm_text": "The image shows a webpage section for a recipe titled \"Mapo Tofu.\" It includes navigation links: home > recipes > tofu > mapo tofu. The author of the recipe is listed as \"Kaitlin,\" and there are 894 comments. There's also a button to \"Jump to Recipe.\""}
{"layout": 1739, "type": "image", "page_idx": 123, "img_path": "layout_images/2309.17421v2_304.jpg", "bbox": [128, 266.25, 340, 582.75], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image depicts a bowl of Mapo Tofu, a spicy and flavorful Chinese dish. Mapo Tofu typically consists of cubes of tofu set in a spicy sauce, typically a thin, oily, and bright red suspension based on doubanjiang (fermented broad bean and chili paste) and douchi (fermented black beans), along with minced meat, usually pork or beef. It's often garnished with chopped green onions or Sichuan peppercorns, lending a characteristic numbing and spicy flavor. The dish is traditionally served over or alongside steamed rice."}
{"layout": 1740, "type": "text", "text": "", "page_idx": 123, "bbox": [129, 583.25, 341, 609], "page_size": [612.0, 792.0]}
{"layout": 1741, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 123, "bbox": [121, 618, 159, 628.75], "page_size": [612.0, 792.0]}
{"layout": 1742, "type": "text", "text": "Next action: \"I would move the mouse to the \"Jump to Recipe\" button located at the top right corner of the  webpage. I would then click on the button to jump to the detailed recipe section of the page.\" ", "page_idx": 123, "bbox": [122, 630.86279296875, 475, 648.1763305664062], "page_size": [612.0, 792.0]}
{"layout": 1743, "type": "text", "text": "Figure 94: GPT-4V navigates through GUI to browse the web to search for the recipe of Mapo Tofu. Blue highlights the predicted actions. Check Section  9.8  for detailed discussions. ", "page_idx": 123, "bbox": [108, 662.5625610351562, 505.7424621582031, 687.4644775390625], "page_size": [612.0, 792.0]}
{"layout": 1744, "type": "text", "text": "Sec. 9.8 GUI Navigation ", "text_level": 1, "page_idx": 124, "bbox": [145, 66, 281, 81], "page_size": [612.0, 792.0]}
{"layout": 1745, "type": "text", "text": "Web Browsing (Mapo Tofu Recipe) ", "text_level": 1, "page_idx": 124, "bbox": [122, 87, 281, 99], "page_size": [612.0, 792.0]}
{"layout": 1746, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 124, "bbox": [122, 104, 159, 113.75], "page_size": [612.0, 792.0]}
{"layout": 1747, "type": "text", "text": "Imagine that you are a robot operating a computer. Like how humans operate the computer, you can move  the mouse, click an icon with the mouse, or type some texts with the keyboard.  ", "page_idx": 124, "bbox": [122, 115.45569610595703, 475, 134.11416625976562], "page_size": [612.0, 792.0]}
{"layout": 1748, "type": "text", "text": "You are asked to find a detailed recipe of Mapo Tofu. ", "page_idx": 124, "bbox": [122, 139.0238800048828, 296.2813415527344, 147.37127685546875], "page_size": [612.0, 792.0]}
{"layout": 1749, "type": "text", "text": "Previous action: “I would move the mouse to the \"Jump to Recipe\" button located at the top right corner of  the webpage. I would then click on the button to jump to the detailed recipe section of the page.\" ", "page_idx": 124, "bbox": [122, 152.28099060058594, 475, 170.93946838378906], "page_size": [612.0, 792.0]}
{"layout": 1750, "type": "text", "text": "You have now completed the previous action as described above. Below is what you see on the computer  screen. Based on the image below, what will you do next? If the action involves moving the mouse, describe  the location as detailed as possible. ", "page_idx": 124, "bbox": [122, 175.8491668701172, 483, 204.81874084472656], "page_size": [612.0, 792.0]}
{"layout": 1751, "type": "image", "page_idx": 124, "img_path": "layout_images/2309.17421v2_305.jpg", "bbox": [124, 212, 353, 608], "page_size": [612.0, 792.0], "ocr_text": "RECIPE\n\nWe Ke Ke 439 from 299 votes\nMAPO TOFU\n\nThis mapo tofu recipe is the true blue, authentic real deal\n—the spicy, tongue-numbing, rice-is-absolutely-not-\noptional mapo tofu that you get in the restaurants!\n\nby: kaitlin course: tofu cuisine: chinese\n\nminutes TOTAL: 35 minutes\n\n© PREP: 10 minutes COOK:\n\nINGREDIENTS BrRINT Q WrATE\n%4 cup oil\n1-2 fresh Thai bird chili\npeppers (ti ed)\n\n6-8 dried red chilies (roughly\n\n1/2- 1 1/2 tablespoons Sichuan\nPeppercorns (powdered or finely\n\nground, reserving 1/4 tea\ngarnish at the end; if you w\n\nflavor use 1/2 or 1 teaspoon ground\nSichuan peppercorw\n\n3 tablespoons ginger (finely minced)\n\n3 tablespoons garlic (\n8 ounces ground pork\n\n1-2 tablespoons spicy bean sauce (depending on your\n\n2/3 cup low sodium chicken broth (or water)\n1 pound silken tofu (cut into 1 indv/2.5em cubes!\n1/4 cup water\n\n1 1/2 teaspoons cornstarch\n\n1/4 teaspoon sesame oil (optional)\n\n1/4 teaspoon sugar (option\n\n1 scallion (finely cropped)\n\nus cusToMaRY [iiEuGia\n\nINSTRUCTIONS\n@ First, we toast the chilies. If you have homemade toasted chili oil, you can\n\nskip this step. Heat your wok or a small saucepan over low heat. Add half\n\nof the oil and throw in the fresh and dried peppers. Stir occasionally and\n\n", "vlm_text": "The image is a recipe for Mapo Tofu. It shows the following details:\n\n- **Rating**: 4.89 from 299 votes\n- **Description**: Describes it as an authentic spicy, tongue-numbing dish.\n- **Author**: Kaitlin\n- **Course**: Tofu\n- **Cuisine**: Chinese\n- **Servings**: 6\n- **Prep Time**: 10 minutes\n- **Cook Time**: 25 minutes\n- **Total Time**: 35 minutes\n\n**Ingredients** include:\n\n- Oil\n- Fresh Thai bird chili peppers\n- Dried red chilies\n- Sichuan peppercorns\n- Ginger\n- Garlic\n- Ground pork\n- Spicy bean sauce\n- Low sodium chicken broth\n- Silken tofu\n- Water\n- Cornstarch\n- Sesame oil (optional)\n- Sugar (optional)\n- Scallion\n\nThere are buttons for printing, rating, and converting measurements. There's a section for instructions starting with toasting chilies."}
{"layout": 1752, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 124, "bbox": [122, 612, 158, 622], "page_size": [612.0, 792.0]}
{"layout": 1753, "type": "text", "text": "Next action: \"I would move the mouse to the \"Print Recipe\" button located at the top right corner of the recipe  section, next to the \"Rate This Recipe\" button. I would then click on the button to print out a copy of the detailed  recipe for Mapo Tofu.”  ", "page_idx": 124, "bbox": [122, 624.4584350585938, 491.5921325683594, 650.7381591796875], "page_size": [612.0, 792.0]}
{"layout": 1754, "type": "text", "text": "Figure 95: GPT-4V navigates through GUI to browse the web to search for the recipe of Mapo Tofu. Blue highlights the predicted actions. Check Section  9.8  for detailed discussions. ", "page_idx": 124, "bbox": [108, 662.5625610351562, 505.7424621582031, 687.4644775390625], "page_size": [612.0, 792.0]}
{"layout": 1755, "type": "text", "text": "Sec. 9.8 GUI Navigation ", "text_level": 1, "page_idx": 125, "bbox": [133, 34, 287, 52], "page_size": [612.0, 792.0]}
{"layout": 1756, "type": "text", "text": "Web Browsing (Mapo Tofu Recipe) ", "text_level": 1, "page_idx": 125, "bbox": [122, 58, 281, 69], "page_size": [612.0, 792.0]}
{"layout": 1757, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 125, "bbox": [121, 74, 159, 84], "page_size": [612.0, 792.0]}
{"layout": 1758, "type": "text", "text": "Describe the image below as detailed as possible. ", "page_idx": 125, "bbox": [122, 85.33325958251953, 287.9877014160156, 93.6806640625], "page_size": [612.0, 792.0]}
{"layout": 1759, "type": "image", "page_idx": 125, "img_path": "layout_images/2309.17421v2_306.jpg", "bbox": [124, 99, 328, 494], "page_size": [612.0, 792.0], "ocr_text": "Mapo Tofu\n\nThis mapo tofu recipe is the true blue, authentic real deal—the spicy,\ntongue-numbing, rice-is-absolutely-not-optional mapo tofu that you get\nin the restaurants!\n\nPrep Time Cook Time Total Time\n10 mins 25 mins 35 mins\n\nCourse: Tofu Cuisine: Chinese Servings: 6 servings ‘4.89 from 200 voors\n\nCalories: 335keal Author: Kaitlin\n\nIngredients\n\n+ Y.cup oil (divided)\n\n‘1-2 fresh Thai bird chili peppers (thinly sliced)\n\n‘+ 6-8 dried red chilies (roughly chopped)\n\n‘+ 1/2- 1.1/2 tablespoons Sichuan peppercoms (powdered or finely ground, reserving 1/4\nteaspoon for gamish at the end; if you want a milder flavor use 1/2 oF 1 teaspoon ground\nSichuan peppercorn)\n\n+ 3 tablespoons ginger (finely minced)\n\n+ 3 tablespoons garlic (finely minced)\n\n* 8 ounces ground pork\n\n‘1-2 tablespoons spicy bean sauce (depending on your desired salt/spice levels)\n\n‘+ 2/3 cup low sodium chicken broth (or water)\n\n‘= 1 pound silken tofu (cut into 1 inch/2Sem cubes)\n\n+ 1/4 cup water\n\n* 11/2 teaspoons comstarch\n\n* 1/4 teaspoon sesame oil (optional)\n\n‘1/4 teaspoon sugar (optional)\n\n* 1 scallion (finely chopped)\n\nInstructions\n\n1. First, we toast the chilies. f you have homemade toasted chili oil, you can skip this step. Heat\nyour wok or a small saucepan over low heat. Add half of the oil and throw in the fresh and\ndried peppers. Stir occasionally and heat until fragrant, about 5 minutes, ensuring that the\npeppers don't bum. Remove from heat and set aside.\n\n2. Heat the remaining half of the oil in your wok over medium heat. Add the ginger. After 1\nminute, add the garlic. Fry for another minute, and then turn up the heat to high and add the\nground pork. Break up the meat and fry it until it's cooked through. Add your ground Sichuan\nPeppercoms and stir for about 15-30 seconds, taking care to not let it bum, as it will turn\nbitter ifit does.\n\n3. Add the spicy bean sauce to the mixture and stir tin well. Add the chicken broth to the wok\nand stir. Let this simmer for a minute or so, While that's happening, ready your tofu and\ncombine the water and comstarch in a small bowl.\n\n4. Add the comstarch mixture to your sauce and sti, Let it bubble away until the sauce starts to\nthicken. (fit gets too thick, splash in a little more water or chicken stock)\n\n5. Then add your chil oil from before—peppers and all! If you are using homemade chili oi,\nONLY use the standing oil, as it's likely that you have salted it and you only want the oil, not\nadditional salt. Stir the oil into the sauce, and add the tofu. Use your spatula to gently toss the\ntofu in the sauce. Let everything cook for 3-5 minutes. Add the sesame oil and sugar (if using)\nalong with the scallions and stir until the scallions are just wilted.\n\n6. Serve with a last sprinkle of Sichuan peppercorn powder as a gamish if desired.\n\nNutrition\n\nCalories: 335kcal | Carbohydrates: 8g | Protein: 11g | Fat: 29g | Saturated Fat: Sg | Cholesterot 27mg |\nSodium: 126mg | Potassium: 340mg | Fiber: 1g | Sugar: 2g | Vitamin A: 1651 | Vitamin C: 3.5mg |\nCalcium: 47mg | Iron: 14mg\n\n‘Mapo Tofu by The Woks of Life. Recipe URL: https/Ahewoksofif.cam/ma-po-tofu-real-dea\n\n", "vlm_text": "The image contains a recipe for Mapo Tofu. It includes details such as the prep time (10 minutes), cook time (25 minutes), total time (35 minutes), and servings (6 servings). The recipe is contributed by an author named Kaitlin and has a rating of 4.89 from 300 votes. \n\nThe ingredients listed for Mapo Tofu are:\n\n- ½ cup oil (divided)\n- 1-2 fresh Thai bird chili peppers (thinly sliced)\n- 6-8 dried red chilies (roughly chopped)\n- 1/2-1 ½ tablespoons Sichuan peppercorns\n- 3 tablespoons ginger (finely minced)\n- 3 tablespoons garlic (finely minced)\n- 8 ounces ground pork\n- 1-2 tablespoons spicy bean sauce\n- 2/3 cup low sodium chicken broth (or water)\n- 1 pound silken tofu (cut into 1 inch/2.5cm cubes)\n- 1/4 cup water\n- 1 ½ teaspoons cornstarch\n- 1/4 teaspoon sesame oil (optional)\n- 1/4 teaspoon sugar (optional)\n- 1 scallion (finely chopped)\n\nThe instructions to prepare the dish are provided in six steps, focusing on toasting chilies, cooking the ground pork with spices, adding ingredients like the spicy bean sauce and chicken broth, creating a cornstarch mixture, adding tofu, and finishing with garnishing. \n\nThe nutritional information per serving is also included, showing 335 kcal, 8g carbohydrates, 11g protein, and 29g fat, among other details. The recipe is attributed to The Woks of Life, with a specified URL for further details."}
{"layout": 1760, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 125, "bbox": [122, 503, 159, 514.75], "page_size": [612.0, 792.0]}
{"layout": 1761, "type": "text", "text": "The image is a recipe for Mapo Tofu, a Chinese dish. The recipe is presented in a structured format with a title,  description, prep time, cook time, total time, course, cuisine, servings, and rating.  ", "page_idx": 125, "bbox": [122, 516.028076171875, 488, 533.3416137695312], "page_size": [612.0, 792.0]}
{"layout": 1762, "type": "text", "text": "The title of the recipe is \"Mapo Tofu\" and it is written in bold black letters. Below the title, there is a description of  the dish which states that it is the true blue, authentic real deal - the spicy, tongue-numbing,  rice-is-absolutely-not-optional Mapo Tofu that you get in the restaurants.  ", "page_idx": 125, "bbox": [122, 536.5221557617188, 495, 562.8018798828125], "page_size": [612.0, 792.0]}
{"layout": 1763, "type": "text", "text": "The prep time for the dish is 10 minutes, the cook time is 25 minutes, and the total time is 35 minutes. The  course is listed as \"Main Course\" and the cuisine is \"Chinese\". The recipe serves 6 people and has a rating of  ", "page_idx": 125, "bbox": [122, 565.9823608398438, 488, 583.2958984375], "page_size": [612.0, 792.0]}
{"layout": 1764, "type": "text", "text": "4.69 stars out of 5. ", "text_level": 1, "page_idx": 125, "bbox": [122, 583.25, 181, 592], "page_size": [612.0, 792.0]}
{"layout": 1765, "type": "text", "text": "Below this information, there is a list of ingredients required for the dish, including tofu, ground pork, Sichuan  peppercorn, garlic, ginger, soy sauce, and cornstarch. The ingredients are listed in a bullet point format. At the bottom of the image, there is nutritional information for the dish, including calories, carbohydrates, protein,  fat, saturated fat, cholesterol, sodium, and calcium. The nutritional information is presented in a table format. The image also includes a credit to the author of the recipe, Kaitlin, and a link to the original recipe on the  website \"The Woks of Life\".  ", "page_idx": 125, "bbox": [122, 595.4425659179688, 495, 653.7443237304688], "page_size": [612.0, 792.0]}
{"layout": 1766, "type": "text", "text": "Figure 96: GPT-4V navigates through GUI to browse the web to search for the recipe of Mapo Tofu. As GPT-4V predicts to print out the recipe in the previous turn, we prompt it to read the screenshot of the printed recipe and summarize it.  Red  highlights the inaccurate description about the image. Check Section  9.8  for detailed discussions. ", "page_idx": 125, "bbox": [107, 670.8675537109375, 505, 717.5864868164062], "page_size": [612.0, 792.0]}
{"layout": 1767, "type": "text", "text": "Sec. 9.8 GUI Navigation ", "text_level": 1, "page_idx": 126, "bbox": [144, 59, 279, 75], "page_size": [612.0, 792.0]}
{"layout": 1768, "type": "text", "text": "Web Browsing (Today’s News) ", "text_level": 1, "page_idx": 126, "bbox": [122, 82, 262, 93], "page_size": [612.0, 792.0]}
{"layout": 1769, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 126, "bbox": [121, 98, 159, 108], "page_size": [612.0, 792.0]}
{"layout": 1770, "type": "text", "text": "Imagine that you are a robot operating a computer. Like how humans operate the computer, you can move the  mouse, click an icon with the mouse, or type some texts with the keyboard. ", "page_idx": 126, "bbox": [122, 109.79199981689453, 488, 128.4504852294922], "page_size": [612.0, 792.0]}
{"layout": 1771, "type": "text", "text": "You are asked to find what’s in the news today. ", "page_idx": 126, "bbox": [122, 133.3601837158203, 277, 141.70758056640625], "page_size": [612.0, 792.0]}
{"layout": 1772, "type": "text", "text": "Below is what you see on the computer screen, predict your next move. If the action involves moving the mouse,  describe the location as detailed as possible. ", "page_idx": 126, "bbox": [122, 149.56332397460938, 495, 168.2218017578125], "page_size": [612.0, 792.0]}
{"layout": 1773, "type": "image", "page_idx": 126, "img_path": "layout_images/2309.17421v2_307.jpg", "bbox": [122, 170, 280, 299], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows a computer desktop with various application icons on the left side of the screen. The background wallpaper features a scenic view of a field with rows of purple flowers, possibly lavender, in front of a stone wall. There are a few large trees in the background under an open sky. The desktop icons include familiar applications such as Recycle Bin, Google Chrome, Zoom, WhatsApp, and others, arranged vertically along the left side."}
{"layout": 1774, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 126, "bbox": [122, 304, 159, 314], "page_size": [612.0, 792.0]}
{"layout": 1775, "type": "text", "text": "My next move would be to move the mouse to the Google Chrome icon located on the left side of the screen,  approximately 1/3 of the way down from the top. I would then click on the icon to open the web browser and  search for news websites to find out what's in the news today.  ", "page_idx": 126, "bbox": [122, 316.01873779296875, 483, 342.2984313964844], "page_size": [612.0, 792.0]}
{"layout": 1776, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 126, "bbox": [121, 361, 159, 370.75], "page_size": [612.0, 792.0]}
{"layout": 1777, "type": "text", "text": "Imagine that you are a robot operating a computer. Like how humans operate the computer, you can move the  mouse, click an icon with the mouse, or type some texts with the keyboard.  ", "page_idx": 126, "bbox": [122, 372.3724060058594, 488, 389.6859436035156], "page_size": [612.0, 792.0]}
{"layout": 1778, "type": "text", "text": "You are asked to find what’s in the news today. ", "page_idx": 126, "bbox": [122, 395.4282531738281, 277, 403.775634765625], "page_size": [612.0, 792.0]}
{"layout": 1779, "type": "text", "text": "Previous action: \"My next move would be to move the mouse to the Google Chrome icon located on the left side  of the screen, approximately  $1/3$   of the way down from the top. I would then click on the icon to open the web  browser and search for news websites to find out what's in the news today. \" ", "page_idx": 126, "bbox": [122, 406.9561767578125, 495, 433.2358703613281], "page_size": [612.0, 792.0]}
{"layout": 1780, "type": "text", "text": "You have now opened the web browser. Below is what you see on the computer screen, what is your immediate  next step? If the action involves moving the mouse, describe the location as detailed as possible. ", "page_idx": 126, "bbox": [122, 436.4164123535156, 495, 453.7299499511719], "page_size": [612.0, 792.0]}
{"layout": 1781, "type": "image", "page_idx": 126, "img_path": "layout_images/2309.17421v2_308.jpg", "bbox": [123, 460, 415, 610], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows a screenshot of a Google Chrome web browser window. The browser is open to the new tab page, which features the Google logo and a search bar for searching Google or entering a URL. Below the search bar, there are several shortcuts to various Google services or apps such as Inbox, Sign in, Course Planner, and the Web Store, followed by an “Add shortcut” option. \n\nThe background image on the new tab page is a blurred cityscape featuring lights and reflections, giving a sense of an urban, possibly rainy environment. This image is credited to Ev Tchebotarev, whose name appears at the bottom left corner of the window.\n\nThere are additional tabs open in the browser but not visible in the screenshot. The browser window itself has a bluish hue, indicating that this is perhaps a customized theme. In the top right, the profile icon shows that the browser's sync feature is paused."}
{"layout": 1782, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 126, "bbox": [122, 617, 159, 626], "page_size": [612.0, 792.0]}
{"layout": 1783, "type": "text", "text": "My immediate next step would be to move the mouse to the search bar located in the center of the screen,  approximately 1/2 of the way down from the top. I would then click on the search bar and type in \"news\" and  press the enter key on the keyboard to search for news websites.  ", "page_idx": 126, "bbox": [122, 628.4013061523438, 483, 654.6810302734375], "page_size": [612.0, 792.0]}
{"layout": 1784, "type": "text", "text": "Figure 97: GPT-4V navigates through GUI to browse the web to read today’s news.  Blue  highlights the predicted actions. Check Section  9.8  for detailed discussions. ", "page_idx": 126, "bbox": [108, 670.2135620117188, 504.00018310546875, 693.1275024414062], "page_size": [612.0, 792.0]}
{"layout": 1785, "type": "text", "text": "Sec. 9.8 GUI Navigation ", "text_level": 1, "page_idx": 127, "bbox": [144, 110, 281, 125], "page_size": [612.0, 792.0]}
{"layout": 1786, "type": "text", "text": "Web Browsing (Today’s News) ", "text_level": 1, "page_idx": 127, "bbox": [121, 131, 263, 144], "page_size": [612.0, 792.0]}
{"layout": 1787, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 127, "bbox": [121, 148, 160, 158], "page_size": [612.0, 792.0]}
{"layout": 1788, "type": "text", "text": "Imagine that you are a robot operating a computer. Like how humans operate the computer, you can move the  mouse, click an icon with the mouse, or type some texts with the keyboard.  ", "page_idx": 127, "bbox": [122, 159.6929168701172, 494, 178.35137939453125], "page_size": [612.0, 792.0]}
{"layout": 1789, "type": "text", "text": "You are asked to find what’s in the news today. ", "page_idx": 127, "bbox": [122, 183.26109313964844, 277.0414733886719, 191.60848999023438], "page_size": [612.0, 792.0]}
{"layout": 1790, "type": "text", "text": "Previous action: \"My immediate next step would be to move the mouse to the search bar located in the center of  the screen, approximately   $_{1/2}$   of the way down from the top. I would then click on the search bar and type in  \"news\" and press the enter key on the keyboard to search for news websites. \" ", "page_idx": 127, "bbox": [122, 196.51820373535156, 494, 225.48776245117188], "page_size": [612.0, 792.0]}
{"layout": 1791, "type": "text", "text": "You have now completed the previous action. Below is what you see on the computer screen. Based on the  image below, what is your immediate next step? If the action involves moving the mouse, describe the location  as detailed as possible. ", "page_idx": 127, "bbox": [122, 230.39747619628906, 494, 259.3670349121094], "page_size": [612.0, 792.0]}
{"layout": 1792, "type": "image", "page_idx": 127, "img_path": "layout_images/2309.17421v2_309.jpg", "bbox": [126, 271, 484, 546], "page_size": [612.0, 792.0], "ocr_text": "Google\n\nnews\n\nNews Videos Images\n\nTop stories :\n\nPolitico\n\nState Department doubles\ndown after Zelenskyy slams\nreluctance ...\n\n@ Reuters\n\nUS says it no longer deems\nDonald Trump immune from\nE. Jean...\n\n2 hours ago\n\n& The New York Times\n\nHard Right Presses Culture\nWar Fights on Defense Bill,\nImperiling ...\n\n2 hours ago\n\nx\n\nShopping Maps _Flights\n\nBB cnn\n\nCatastrophic flooding\nswamped Vermont's capital\nas intense storms forced...\n\n1 hour ago\n\n& cist\n\nD.C. Council Approves\nEmergency Crime Bill\n\n5 hours ago\n\n@ NPR\n\nThe Anthropocene began in\n1950s, scientists say\n\n3 hours ago\n\n~\n\nFinance\n\n", "vlm_text": "The image shows a Google search results page for the query \"news.\" It displays a list of top stories from various news outlets, including:\n\n1. Politico: An article about the State Department and Zelenskyy.\n2. CNN: News about catastrophic flooding in Vermont.\n3. Reuters: Article about Donald Trump's immunity status.\n4. DCist: D.C. Council's approval of an emergency crime bill.\n5. The New York Times: Discussion on culture war and defense bill.\n6. NPR: Scientists identifying the start of the Anthropocene in the 1950s.\n\nEach news item includes a headline, a thumbnail image, the source, and how long ago it was published."}
{"layout": 1793, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 127, "bbox": [121, 556, 159, 567.75], "page_size": [612.0, 792.0]}
{"layout": 1794, "type": "text", "text": "My immediate next step would be to move the mouse to the first search result, which is \"State Department  doubles down on Zelensky slams\" located approximately 1/4 of the way down from the top of the screen and 1/8  of the way from the left side of the screen. I would then click on the link to open the news article and read about  what's in the news today.  ", "page_idx": 127, "bbox": [122, 569.4274291992188, 494, 604.6732788085938], "page_size": [612.0, 792.0]}
{"layout": 1795, "type": "text", "text": "Figure 98: GPT-4V navigates through GUI to browse the web to read today’s news.  Blue  highlights the predicted actions. Check Section  9.8  for detailed discussions. ", "page_idx": 127, "bbox": [108, 620.3125610351562, 504.00018310546875, 643.2265014648438], "page_size": [612.0, 792.0]}
{"layout": 1796, "type": "text", "text": "Sec. 9.8 GUI Navigation ", "text_level": 1, "page_idx": 128, "bbox": [146, 109, 279, 124], "page_size": [612.0, 792.0]}
{"layout": 1797, "type": "text", "text": "Web Browsing (Today’s News) ", "text_level": 1, "page_idx": 128, "bbox": [122, 131, 263, 143], "page_size": [612.0, 792.0]}
{"layout": 1798, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 128, "bbox": [121, 148, 160, 158], "page_size": [612.0, 792.0]}
{"layout": 1799, "type": "text", "text": "Read the article shown in the images below and summarize it. ", "page_idx": 128, "bbox": [122, 159.02769470214844, 327.3939514160156, 167.37509155273438], "page_size": [612.0, 792.0]}
{"layout": 1800, "type": "image", "page_idx": 128, "img_path": "layout_images/2309.17421v2_310.jpg", "bbox": [120, 166.25, 492, 462], "page_size": [612.0, 792.0], "ocr_text": "POLITICO\n\nFOREIGN AFFAIRS\n\nState Department doubles down after\nZelenskyy slams reluctance on\nUkraine’s entry to NATO\n\nState Department press secretary Matthew Miller said the\n\nc¥ HODGMAN DOQE\nThe State Department on Tuesday defended NATO's hesitation to invite\nUkraine into the alliance, even as Ukrainian President Volodymyr Zelenskyy\n\nvoiced his frustration with how the negotiations have unfolded.\n\nState Department press secretary Matthew Miller spoke to reporters on\nTuesday from the Lithuanian capital, Vilnius, where NATO's two-day summit\nis being held this week. Miller cited several conditions that NATO had decided\nUkraine must meet for an invitation to the alliance, including democratic\nreforms and the end of the war between Russia and Ukraine.\n\n“We are ready to extend an invitation when conditions are met,” Miller said.\n“There is an important reason why they are not becoming a member of NATO\nright now — because it would instantly put the United States in a shooting war\nwith Russia.”\n\nMiller echoed the sentiment of President Joe Biden, who told CNN on Sunday\nthat Ukraine could not be admitted into the alliance until the war with Russia\n\nhad ended, adding that “if the war is going on, then we're all in war.”\n\nThe U.S. has doubled down this week on military support of Ukraine in its war\nwith Russia, agreeing to include controversial cluster munitions in its next\narms package to Kyiv — a decision that Ukrainian Minister of Defense Oleksiy\nReznikov said would be a “game changer” as the Ukrainian counteroffensive\nstruggles to breach Russia’s front lines.\n\nIna summit communiqué released Tuesday, NATO leaders wrote that the\nalliance could not promise Ukraine membership until the war had ended and\n\nseveral democratic and security-sector reforms had been achieved.\n\n“We will be in a position to extend an invitation to Ukraine to join the Alliance\n\nwhen Allies agree and conditions are met,” the leaders wrote.\n\nIna statement posted to Twitter on Tuesday, Zelenskyy took issue with NATO’s\nreluctance to commit to admitting his country, and its unwillingness to provide\nan anticipated timeline for Ukraine's entry. He wrote that he did not see\nsufficient readiness on the part of NATO to integrate Ukraine into the alliance.\n\n“It’s unprecedented and absurd when time frame is not set neither for the\ninvitation nor for Ukraine’s membership,” Zelenskyy wrote. “While at the same\ntime vague wording about ‘conditions’ is added even for inviting Ukraine.”\n\n", "vlm_text": "The image is a screenshot from an article on Politico discussing the U.S. State Department's stance on Ukraine's entry into NATO. The article explains that the State Department, through press secretary Matthew Miller, has justified the U.S.'s hesitance to support Ukraine's immediate entry into NATO by citing the ongoing war with Russia. Miller reiterated President Biden's position that Ukraine could not join NATO until the war with Russia ends. Furthermore, the U.S. has reinforced its military support for Ukraine by agreeing to include controversial cluster munitions in its arms package to Kyiv. The NATO summit communiqué stated that membership for Ukraine could not be extended until the war ends and necessary reforms are achieved. Ukrainian President Volodymyr Zelenskyy criticized NATO's reluctance and the lack of a clear timeline for Ukraine's membership, calling it unprecedented and absurd."}
{"layout": 1801, "type": "text", "text": "GPT-4V: Summary: ", "page_idx": 128, "bbox": [122, 469.4366455078125, 159, 489.19512939453125], "page_size": [612.0, 792.0]}
{"layout": 1802, "type": "text", "text": "The article discusses the statement made by Ukrainian President Volodymyr Zelensky on Tuesday, where he  expressed his country's desire to join NATO. Zelensky stated that Ukraine cannot afford to go to war with Russia  and that joining NATO would provide the necessary security and protection. ", "page_idx": 128, "bbox": [122, 498.7800598144531, 494.3671569824219, 525.0597534179688], "page_size": [612.0, 792.0]}
{"layout": 1803, "type": "text", "text": "However, Matthew Miller, a spokesperson for NATO, stated that while the alliance is ready to extend an  invitation to Ukraine, there are certain conditions that need to be met. These conditions include democratic and  security-sector reforms. Miller also emphasized that Ukraine's entry into NATO is not a guarantee and that the  decision ultimately lies with the members of the alliance. ", "page_idx": 128, "bbox": [122, 534.6446533203125, 489.9512634277344, 569.8905639648438], "page_size": [612.0, 792.0]}
{"layout": 1804, "type": "text", "text": "The article also mentions that during a press conference on Tuesday, Zelensky reiterated his commitment to  strengthening democracy in Ukraine, which is a necessary condition for NATO membership.  ", "page_idx": 128, "bbox": [122, 579.4754638671875, 486, 596.7890625], "page_size": [612.0, 792.0]}
{"layout": 1805, "type": "text", "text": "Sec. 9.8 GUI Navigation ", "text_level": 1, "page_idx": 129, "bbox": [144, 99, 281, 114], "page_size": [612.0, 792.0]}
{"layout": 1806, "type": "text", "text": "Web Browsing (Today’s News) ", "text_level": 1, "page_idx": 129, "bbox": [121, 120, 263, 133], "page_size": [612.0, 792.0]}
{"layout": 1807, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 129, "bbox": [121, 138, 159, 147.75], "page_size": [612.0, 792.0]}
{"layout": 1808, "type": "text", "text": "Imagine that you are a robot operating a computer. Like how humans operate the computer, you can move the  mouse, click an icon with the mouse, or type some texts with the keyboard.  ", "page_idx": 129, "bbox": [122, 148.84288024902344, 491, 167.5013427734375], "page_size": [612.0, 792.0]}
{"layout": 1809, "type": "text", "text": "You are asked to find what’s in the news today. ", "page_idx": 129, "bbox": [122, 172.4110565185547, 279, 180.75845336914062], "page_size": [612.0, 792.0]}
{"layout": 1810, "type": "text", "text": "Previous action: \"My immediate next step would be to move the mouse to the first search result, which is \"State  Department doubles down on Zelensky slams\" located approximately 1/4 of the way down from the top of the  screen and 1/8 of the way from the left side of the screen. I would then click on the link to open the news article  and read about what's in the news today.\" ", "page_idx": 129, "bbox": [122, 185.6681671142578, 491, 224.9488067626953], "page_size": [612.0, 792.0]}
{"layout": 1811, "type": "text", "text": "You have now completed reading “State Department doubles down on Zelensky slams”. What is your immediate  next step? If the action involves moving the mouse, describe the location as detailed as possible. ", "page_idx": 129, "bbox": [122, 229.8585205078125, 494.3884582519531, 248.51699829101562], "page_size": [612.0, 792.0]}
{"layout": 1812, "type": "image", "page_idx": 129, "img_path": "layout_images/2309.17421v2_311.jpg", "bbox": [122, 254, 443, 553], "page_size": [612.0, 792.0], "ocr_text": "ea\n\n8 politco.com)\n\nPOLITICO\n\n{ne U.5. nas aouolea aown tms Week on muntary support or UkraIne in ws War\nwith Russia, agreeing to include controversial cluster munitions in its next\narms package to Kyiv — a decision that Ukrainian Minister of Defense Oleksiy\nReznikov said would be a “game changer” as the Ukrainian counteroffensive\nstruggles to breach Russia's front lines.\n\nIna summit communiqué released Tuesday, NATO leaders wrote that the\nalliance could not promise Ukraine membership until the war had ended and\nseveral democratic and security-sector reforms had been achieved.\n\n“We will be in a position to extend an invitation to Ukraine to join the Alliance\nwhen Allies agree and conditions are met,” the leaders wrote.\n\nIna statement posted to Twitter on Tuesday, Zelenskyy took issue with NATO's\nreluctance to commit to admitting his country, and its unwillingness to provide\nan anticipated timeline for Ukraine's entry. He wrote that he did not see\nsufficient readiness on the part of NATO to integrate Ukraine into the alliance.\n\n“It’s unprecedented and absurd when time frame is not set neither for the\ninvitation nor for Ukraine’s membership,” Zelenskyy wrote. “While at the same\ntime vague wording about ‘conditions’ is added even for inviting Ukraine.”\n\n‘Miller pointed to reforms around “anticorruption” and “strengthening\ndemocracy” that were necessary for Ukraine’s entry to NATO but declined to\ndetail what the specific conditions were in full. During his Tuesday press\nconference, Miller said that members of NATO stood ready to help Ukraine\nmeet the necessary conditions for its acceptance to the alliance.\n\n“One of the things Russia said before this war began is that they were firmly\n‘opposed to Ukraine joining NATO, and they wanted an ironclad commitment\nthat Ukraine would never join NATO,” Miller said. “And we made clear that\nwas not on the table, that we maintained NATO's open-door policy. We\nreiterate that commitment today, and we made clear today that Ukraine will\nbecome a member of NATO.”\n\nFILED UNDER: NATO, UKRAINE, VOLODYMYR ZELENSKYY, RUSSIA'S WAR ON UKRAINE\n\n", "vlm_text": "This image shows a Politico article discussing Ukraine's potential membership in NATO. It mentions the U.S. doubling down on military support for Ukraine, including sending cluster munitions. NATO leaders have stated conditions under which Ukraine might join the alliance, emphasizing the need for ending the war and achieving reforms. Ukrainian President Zelenskyy criticized NATO's reluctance on Twitter, seeking a clearer timeline. The article also mentions reforms needed for Ukraine's entry into NATO and reiterates NATO's open-door policy for Ukraine."}
{"layout": 1813, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 129, "bbox": [121, 556, 159, 566], "page_size": [612.0, 792.0]}
{"layout": 1814, "type": "text", "text": "My immediate next step would be to move the mouse to the top right corner of the screen, where I can see the  \"X\" icon to close the current tab. I would then click on the \"X\" icon to close the tab and return to the previous  page to continue browsing for more news articles.  ", "page_idx": 129, "bbox": [122, 568.1839599609375, 491, 594.4636840820312], "page_size": [612.0, 792.0]}
{"layout": 1815, "type": "text", "text": "Figure 100: GPT-4V navigates through GUI to browse the web to read today’s news. Upon finishing reading the first news article, GPT-4V predicts to close the tab and return to previous page to continue browsing more news articles (highlighted in  blue  ).  Red  highlights the inaccurate action prediction. Check Section  9.8  for detailed discussions. ", "page_idx": 129, "bbox": [108, 607.3565673828125, 505, 654.0755004882812], "page_size": [612.0, 792.0]}
{"layout": 1816, "type": "text", "text": "Sec. 9.8 GUI Navigation ", "text_level": 1, "page_idx": 130, "bbox": [144, 110, 281, 125], "page_size": [612.0, 792.0]}
{"layout": 1817, "type": "text", "text": "Browsing the Web (Today’s News) ", "text_level": 1, "page_idx": 130, "bbox": [121, 131, 280, 143], "page_size": [612.0, 792.0]}
{"layout": 1818, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 130, "bbox": [121, 148, 159, 158], "page_size": [612.0, 792.0]}
{"layout": 1819, "type": "text", "text": "Imagine that you are a robot operating a computer. Like how humans operate the computer, you can move the  mouse, click an icon with the mouse, or type some texts with the keyboard.  ", "page_idx": 130, "bbox": [122, 159.6929168701172, 489, 178.35137939453125], "page_size": [612.0, 792.0]}
{"layout": 1820, "type": "text", "text": "You are asked to find what’s in the news today. ", "page_idx": 130, "bbox": [122, 183.26109313964844, 277.0414733886719, 191.60848999023438], "page_size": [612.0, 792.0]}
{"layout": 1821, "type": "text", "text": "Previous action: \"My immediate next step would be to move the mouse to the top right corner of the screen,  where I can see the \"X\" icon to close the current tab. I would then click on the \"X\" icon to close the tab and  return to the previous page to continue browsing for more news articles. \" ", "page_idx": 130, "bbox": [122, 196.51820373535156, 478, 225.48776245117188], "page_size": [612.0, 792.0]}
{"layout": 1822, "type": "text", "text": "You have now returned to the previous page after reading the first article “State Department doubles down on  Zelensky slams”. Below is what you see on the computer screen. Based on the image below, what is your  immediate next step? If the action involves moving the mouse, describe the location as detailed as possible. ", "page_idx": 130, "bbox": [122, 230.39747619628906, 484, 259.3670349121094], "page_size": [612.0, 792.0]}
{"layout": 1823, "type": "image", "page_idx": 130, "img_path": "layout_images/2309.17421v2_312.jpg", "bbox": [127, 269, 483, 543], "page_size": [612.0, 792.0], "ocr_text": "Google news xlema\nNews Videos Images Books Shopping Maps Flights Finance\n\nAbout 25,270,000,000 results (0.46 seconds)\n\nTop stories :\n\nB Poitico EA cnn\n\nState Department doubles Catastrophic flooding\ndown after Zelenskyy slams swamped Vermont's capital\nreluctance .. as intense storms forced...\n6 hours ago 1 hour ago\n\n® Reuters & DCist\n\nUS says it no longer deems\nDonald Trump immune from\nE. Jean...\n\nD.C. Council Approves\nEmergency Crime Bill\n\n2 hours ago 5 hours ago\n\n€ The New York Times\n\nHard Right Presses Culture\nWar Fights on Defense Bill,\nImperiling ...\n\n© NPR\nThe Anthropocene began in\n1950s, scientists say\n\n2 hours ago 3 hours ago\n", "vlm_text": "The image shows a Google search result page for the term \"news.\" The top stories section includes several news articles:\n\n1. **Politico** - \"State Department doubles down after Zelenskyy slams reluctance...\" (6 hours ago)\n2. **CNN** - \"Catastrophic flooding swamped Vermont’s capital as intense storms forced...\" (1 hour ago)\n3. **Reuters** - \"US says it no longer deems Donald Trump immune from E. Jean...\" (2 hours ago)\n4. **DCist** - \"D.C. Council Approves Emergency Crime Bill\" (5 hours ago)\n5. **The New York Times** - \"Hard Right Presses Culture War Fights on Defense Bill, Imperiling...\" (2 hours ago)\n6. **NPR** - \"The Anthropocene began in 1950s, scientists say\" (3 hours ago)\n\nEach news article is accompanied by an image to the right of its title."}
{"layout": 1824, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 130, "bbox": [121, 553, 159, 564.75], "page_size": [612.0, 792.0]}
{"layout": 1825, "type": "text", "text": "My immediate next step would be to move the mouse to the second article on the page, \"Catastrophic flooding  swamps Florida's panhandle as intense storms force evacuations\". I would move the mouse to the left side of  the article title, approximately 1/3 of the way down the page, and click on the article title to open it and read the  news.  ", "page_idx": 130, "bbox": [122, 566.2252197265625, 489, 601.4710693359375], "page_size": [612.0, 792.0]}
{"layout": 1826, "type": "text", "text": "Figure 101: GPT-4V navigates through GUI to browse the web to read today’s news.  Blue  highlights the predicted actions. Check Section  9.8  for detailed discussions. ", "page_idx": 130, "bbox": [108, 620.3125610351562, 504.00018310546875, 643.2265014648438], "page_size": [612.0, 792.0]}
{"layout": 1827, "type": "text", "text": "Web Browsing (Today’s News) ", "text_level": 1, "page_idx": 131, "bbox": [121, 27, 262, 39], "page_size": [612.0, 792.0]}
{"layout": 1828, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 131, "bbox": [121, 44, 159, 53], "page_size": [612.0, 792.0]}
{"layout": 1829, "type": "image", "page_idx": 131, "img_path": "layout_images/2309.17421v2_313.jpg", "img_caption": "Read the article shown in the images below and summarize it. ", "bbox": [121, 54, 481, 508], "page_size": [612.0, 792.0], "ocr_text": "Catastrophic flooding swamped Vermont's capital as\nintense storms forced evacuations and closures in\nNortheast\n\n(cu) — intone rainstorms that nundate the Northeast tumod test nto\n‘ver, orced wiacuations and prompted oils varmont's epi, Montpeber,\n\n“Make no mlstak, the davastaton and flooding were experiencing across\n\\ormeont is historic and catastrophe,” Gov, Pl Scot told porters Tussday\n\nthe govemor sald. Kena ithe United States a a huricane in August 2031 and\nlet one communis submerged, ting mor than 40 pont\n\n‘The curet feeding clad thousands of homes and businesses, Scott aod\n\nIn downtown Montple, suthores ssc a travel ban that ated through\ntame feeding” could contaminate the drinking water supp\n\n“Te cy and surrounding communtine saw “record-breaking flood\nays, Montpller oficial sid Tuseday afternoon. Tha cy's downtown area\nremains flooded and eno sale for public trae, they added.\n\n‘The cty was hammered by a ecor-sating 528 Inches of ralnfall Monday, the\nNational Weather Servos Burlington sad, That's mer than anyother day on\nrecord, chung when Ine cropped 5.27 inches of an on the stat capital on\n‘August 28,2011\n\n“ene had outa 12-hour duration of rain and then twas ove” the governor\nsald, “This le tferent We've had he 48 hour of steady rin he sald, adelng\n‘more rains expected inthe coming days.\n\nVermont remains understate of emergency, with water in downtown Montel\nrunning from knee to walt dep Tusoday, standing resents in thelr homes and\nthan 300 rescues nthe state, Uran Search and Rescue manager Mike Cannon,\ntold onN,\n\n“rn many aed, the water conditions remain too dangerous for rescue by boat”\nstate pubic safety commissioner Jennifer Mrrison sald Tuesday,\n\nSires:\nWant stay updatod on to latst\nSowind toes ena?\n\n=Tere ae te theatening isolations that we are trying to dentty and rascue*\n\nPresident Joe Biden spoke with Scot and ater offi about the Nooding, the\n‘nit House sald Tuesday, adn tis coordinating wih state and focal tills\nand monitoring the od impacts\n\nfew Hampatre wil send awit boat rescus crows and Black Rawk helicopters to\nasst withthe response to the leading in Vermont, Gow, Chis Sununu sald\n\n| vormont to provide assistance and ethers are en route, the Vermont Department\not Putte Saty sla.\n\nIn Mootpoti, the pice department relocated is ispatch, police ad tke\noperations due to heavy flooding Inthe basement of Cty Hal andthe fire ane\npolice departments, Police Chet Eic Nordenson sal Tuesday.\n\nautores were also monitoring the Wrighteulle Dam in Montptr late Tuosdy\n\nern the day Inched toward the dam's slvay.\n\nnan 8:20 pam. update. Montpetir fills sald the dam's water tove was\n\nbeginning to recede and was not expected to breach the sptiway. On\n\n wecnosdy, crows wl begin clearing mul and dtr trom tho y's roadways,\n\nbutang inspections wil startin the downtown area and clean-up forts wil\niil sid\n\nFood alors affecting ovr 2 millon people across pars of Now England and\nokizhoma expired Tuesday, but food wamings remain in both regions\n\nckichoma city ator authorities sald thoy were swopt up Monday evening In\nong curens folowing heavy raifaln the cy ove the weekend\n\nThe citron, who authori sald were 10 and 13 yoars old, were among @ group\nct tour that was ising nthe area and entered tho wate.\n\nThe strong water curens alld two boys underwater while the oer two\nsatey made it 10a concrete td.\" Oktaoma Cy Fe Department Capt. John\n\nOne resident saw water ‘rising quickly’\n‘As the rin lessens in Vermont, Scott caution agalnt flee sense of\n‘complacency. “The wate has to go someplace, the reservoirs are ilng Yo, nd\nwe hve to detarmine the nat phase, the next wave he sl\n\nBetey Hart aed 913 when the locator suddenly started sing fast atthe\nbasement of her Chester property in Windsor County.\n\n“water was rising aul after being pretty tame most ofthe moming” Hart told\n(un's miguel Marquez. “Allo a sudden. wasn the house”\n\nart sid she's neer experienced flooding tke whet she saw Monday \"twas too\n\n3 wes raging ihe thi, bu It never relly got othe\n\n[ house” sho sal, standing on a rod near her home as wator rushed nearby\n\nTigher han level reached during Huricane rene, The Winooski erat\nMontpetir rose nearly 14 fet Monday and passed malo ood stage aa the watr|\ncontinued to lim, tireatening furthee oon,\n\nexpected to overiow their spitvays” the US Army Corps of Engineers warmed\n‘Monday night. Warning of emer Nooding- the agency urged resents\nthreatened loving areas of nearby Vermant and New Hampshire communis\n\n‘Seven dey ranfal! cals across much ofthe Northoast were aeady at 300% to\n‘500% of normal ves, the Weather Precio Center sd Monday\n\nWidespread rainfall of 2 to 4 inches fe across tho Northoast fom eastem |\n‘alta totale higher than 6 lachos have been Senn several stats.\n\nIn New York, & 35:0 old woman dad ater boing swept away by Hoodwater\nshe tld to evacuate her Orange County home Sunday. The flooding hes caused\neasly tons of milons of dollars in damage,\" county Execute Stove Nouhaus\nsalt Monday\n\n‘Some areas in Now Yorke wers hammered with mors than & Inches of rain within a\n] 2+:noue perio\n\n[tee nortorn part of te tte, stato of emergency wes doc\n\nof Long Lake duo to severe loosing that destroyed reads and bridges. downed\nct supply nos and forced some olden o evacuate. L009 Lake town\nsupardse Clay Arsenault alin declaration. stat of emergency lain ttact\nfor Hamiton County though Wednesday mening\n\nLong Lake offiiatsato said non-essential tavelnthe tn ls no alowed a the\n\nWater rushing into homes,\n\nIn Vermont state Rap. Kel Pala sald she woke up Monday mening to\nfloodwater already at the font step of her Londondeny apartment. She and her\nton packed up tei two cate and wracunted to higher ground\n\n“For people hat were here during Kone it fous ike u very sma experience”\nshe ald, Huricane rane brought deste foodng to the state 2033,\ncasing major nratrcture damage\n\nFloodwater could be aon gushing botweon homes in Chaser, whore some\nstructures were visibly damaged and trucks were whools-doep\n\n‘ben Hancock, dipping in water rom head to toe tokd CNN he has only ved\nis house for loss than a Yoar and watched floodwater enter the basement and\ngarage of his nw home.\n\n1 was @freightern New York. ve been thore many of tes to help paope out,\nbut ve noord this sid of.\" Hancock sald. “Once the water goes down, we\n\n", "vlm_text": "The article discusses severe flooding in Vermont and other parts of the Northeast, likening its impact to events like Hurricane Irene. The high rainfall levels, at 300-500% above normal, caused significant flooding across cities, including Montpelier, where emergency operations were activated. Roads and bridges were flooded, and rescue efforts were underway with aid from nearby states. Vermont's governor emphasized the situation's severity, urging caution as water levels were still rising in some areas. The state's emergency services, including swift water rescue teams, were actively responding, with additional federal support being coordinated. The floods disrupted travel and led to evacuations, with officials warning of continuing risks."}
{"layout": 1830, "type": "text", "text": "The article discusses the catastrophic flooding that occurred in Vermont's capital, Montpelier, on Tuesday, July  11, 2023. The flooding was caused by intense rainstorms that inundated the Northeast, leading to evacuations  and closures in the area. ", "page_idx": 131, "bbox": [122, 511.9855651855469, 494, 538.2652587890625], "page_size": [612.0, 792.0]}
{"layout": 1831, "type": "text", "text": "The floods were described as \"unprecedented\" and \"historic\" by officials, with some areas experiencing  record-breaking rainfall. The National Weather Service issued a travel advisory, warning residents to avoid  non-essential travel. ", "page_idx": 131, "bbox": [122, 541.4457397460938, 474, 567.7254638671875], "page_size": [612.0, 792.0]}
{"layout": 1832, "type": "text", "text": "Vermont Governor Phil Scott declared a state of emergency, and New Hampshire sent rescue crews and Black  Hawk helicopters to assist with the response. The Vermont Fire Department and other emergency services were  also dispatched to the affected areas. ", "page_idx": 131, "bbox": [122, 570.906005859375, 494, 597.1856689453125], "page_size": [612.0, 792.0]}
{"layout": 1833, "type": "text", "text": "The floods caused extensive damage to homes and businesses, with some residents being forced to evacuate.  A boil water notice was issued for Montpelier, and officials began clearing debris from roadways on Wednesday. In Oklahoma City, two children were rescued from a reservoir after being swept away by floodwaters. The  children, aged 10 and 11, were among a group of four that were fishing in the area. ", "page_idx": 131, "bbox": [122, 600.3662109375, 494, 638.1738891601562], "page_size": [612.0, 792.0]}
{"layout": 1834, "type": "text", "text": "In New York, Hurricane Irene caused 8 inches of rain within a 24-hour period, leading to major infrastructure  damage. A state of emergency was declared for Long Lake Town Supervisor Clay Arsenault, and residents were  urged to evacuate. ", "page_idx": 131, "bbox": [122, 641.3543701171875, 494, 667.6341552734375], "page_size": [612.0, 792.0]}
{"layout": 1835, "type": "text", "text": "The article concludes with a quote from Dan Hancock, a resident of Chester, Vermont, who watched floodwater  enter his basement. He expresses his frustration and helplessness, stating \"Once the water goes down, we go  back in, clean it up and move on. What can we do?\"                                             ", "page_idx": 131, "bbox": [122, 670.8146362304688, 494, 697.0943603515625], "page_size": [612.0, 792.0]}
{"layout": 1836, "type": "text", "text": "Figure 102: GPT-4V navigates through GUI to browse the web to read today’s news. We prompt GPT-4V to read the screenshots of the second news article and summarize it. Check Section  9.8  for detailed discussions. ", "page_idx": 131, "bbox": [108, 709.4285888671875, 504, 743.2515258789062], "page_size": [612.0, 792.0]}
{"layout": 1837, "type": "text", "text": "Sec. 9.8 GUI Navigation ", "text_level": 1, "page_idx": 132, "bbox": [145, 59, 279, 73], "page_size": [612.0, 792.0]}
{"layout": 1838, "type": "text", "text": "Online Shopping (Ergonomic Keyboard) ", "page_idx": 132, "bbox": [122, 80.95244598388672, 305.2043762207031, 91.68482208251953], "page_size": [612.0, 792.0]}
{"layout": 1839, "type": "text", "text": "Prompt: ", "page_idx": 132, "bbox": [122, 97.07061004638672, 159.1549072265625, 107.80298614501953], "page_size": [612.0, 792.0]}
{"layout": 1840, "type": "text", "text": "Imagine that you are a robot operating a phone. Like how humans operate the phone, you can move your finger  to an icon, click an icon with your finger, scroll down a screen, or type some texts with the keyboard.  ", "page_idx": 132, "bbox": [122, 108.48169708251953, 495, 127.14018249511719], "page_size": [612.0, 792.0]}
{"layout": 1841, "type": "text", "text": "You are asked to shop for an ergonomic keyboard, your budget is between  $\\S50$   and  $\\S100$  . ", "page_idx": 132, "bbox": [122, 138, 418.4900817871094, 147.76234436035156], "page_size": [612.0, 792.0]}
{"layout": 1842, "type": "text", "text": "Below is what you see on the phone  screen, predict your next move. If the action involves moving your finger to  an icon, describe the location of that icon as detailed as possible. ", "page_idx": 132, "bbox": [122, 160.037109375, 495, 178.69558715820312], "page_size": [612.0, 792.0]}
{"layout": 1843, "type": "image", "page_idx": 132, "img_path": "layout_images/2309.17421v2_314.jpg", "bbox": [122, 182, 315, 594], "page_size": [612.0, 792.0], "ocr_text": "1:56\n\nCalendar\n\nWallet\n\nWeather\n\nv\n\nTwitter\n\nYelp\n\nSettings\n\nClass Calculator\n\nVoice Memos\n\n* ©\n\nCamera\n\nApp Store YouTube\na\n=\nInstagram Chase Facebook\nQ Search\n\nOO =\n", "vlm_text": "The image shows a smartphone home screen with various app icons. These include:\n\n- Calendar\n- Settings\n- Clock\n- Calculator\n- Wallet\n- Voice Memos\n- Camera\n- Compass\n- Weather\n- App Store\n- YouTube\n- Amazon\n- Twitter\n- Instagram\n- Chase\n- Facebook\n- Yelp\n\nThere are also icons in the dock at the bottom:\n\n- Phone\n- Photos\n- Safari\n- Mail\n\nThe time is displayed as 1:56, and the battery percentage is 85%."}
{"layout": 1844, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 132, "bbox": [122, 602, 159, 612.75], "page_size": [612.0, 792.0]}
{"layout": 1845, "type": "text", "text": "My next move would be to move my finger to the Amazon icon, which is located in the third row and third column  of the screen. I would then click on the icon to open the app and search for an ergonomic keyboard within my  budget between  $\\S50$   and  $\\S100$  .  ", "page_idx": 132, "bbox": [122, 614.2822265625, 495, 640.5619506835938], "page_size": [612.0, 792.0]}
{"layout": 1846, "type": "text", "text": "Figure 103: GPT-4V navigates through GUI to shop for an ergonomic keyboard online. Blue highlights the predicted actions.  Red  highlights the inaccurate location of the Amazon icon. Check Section  9.8  for detailed discussions. ", "page_idx": 132, "bbox": [108, 657.695556640625, 504.2525634765625, 694.4384765625], "page_size": [612.0, 792.0]}
{"layout": 1847, "type": "text", "text": "Sec. 9.8 GUI Navigation ", "text_level": 1, "page_idx": 133, "bbox": [144, 55, 280, 71], "page_size": [612.0, 792.0]}
{"layout": 1848, "type": "text", "text": "Online Shopping (Ergonomic Keyboard) ", "text_level": 1, "page_idx": 133, "bbox": [121, 78, 306, 90], "page_size": [612.0, 792.0]}
{"layout": 1849, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 133, "bbox": [121, 94, 159, 104.75], "page_size": [612.0, 792.0]}
{"layout": 1850, "type": "text", "text": "Imagine that you are a robot operating a phone. Like how humans operate the phone, you can move your finger  to an icon, click an icon with your finger, scroll down a screen, or type some texts with the keyboard.  ", "page_idx": 133, "bbox": [122, 105.78949737548828, 495, 124.44798278808594], "page_size": [612.0, 792.0]}
{"layout": 1851, "type": "text", "text": "You are asked to shop for an ergonomic keyboard, your budget is between  $\\S50$   and  $\\S100$  . ", "page_idx": 133, "bbox": [122, 130, 418.4900817871094, 139.1781005859375], "page_size": [612.0, 792.0]}
{"layout": 1852, "type": "text", "text": "Previous action: \"My next move would be to move my finger to the Amazon icon, which is located in the third row  and third column of the screen. I would then click on the icon to open the app and search for an ergonomic  keyboard within my budget of  $\\S50$  . \" ", "page_idx": 133, "bbox": [122, 145.56082153320312, 495, 174.53036499023438], "page_size": [612.0, 792.0]}
{"layout": 1853, "type": "text", "text": "You have now opened the Amazon app. Below is what you see on the phone screen. What will you do next? If  the action involves moving your finger to an icon, describe the location of that icon as detailed as possible. ", "page_idx": 133, "bbox": [122, 180.91310119628906, 487, 199.5715789794922], "page_size": [612.0, 792.0]}
{"layout": 1854, "type": "image", "page_idx": 133, "img_path": "layout_images/2309.17421v2_315.jpg", "bbox": [122, 206, 309, 604], "page_size": [612.0, 792.0], "ocr_text": "Q Search Amazon.com\n\nShop school\nsupplies for less\n\nInspired by your Selected for you Buy it again\nWish List\n\nCrest Pro Health Advanced Multi-\nProtection Mouthwash, Alcohol Free, Ext...\n$27.16 vprime\n\nRecommended deal for you\n\n", "vlm_text": "The image shows a screenshot of the Amazon app on a mobile device. At the top, there is a search bar, and options for \"Groceries,\" \"Pharmacy,\" \"In-Store Code,\" and \"One Medical.\" Below that, there is a notification for delivery with a placeholder name \"Lin Jie.\" \n\nThere is a colorful banner with back-to-school supplies and the text \"Shop school supplies for less.\" \n\nBelow the banner, there are personalized product recommendations:\n- \"Inspired by your Wish List\" featuring ceramic ramekins.\n- \"Selected for you\" showing a plush toy pig.\n- \"Buy it again\" with Hero Mighty Patch for acne.\n\nThere's also a suggested product for Crest Pro Health Advanced Multi-Protection Mouthwash with a price of $27.16 and an \"Add to Cart\" button. \n\nThe bottom toolbar includes icons for the home page, user account, cart, and menu."}
{"layout": 1855, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 133, "bbox": [122, 612, 159, 622.75], "page_size": [612.0, 792.0]}
{"layout": 1856, "type": "text", "text": "My next move would be to move my finger to the search bar, which is located at the top of the screen. I would  then click on the search bar and type in \"ergonomic keyboard\" using the keyboard. After typing in the search  term, I would click on the \"Search\" button, which is located on the bottom right corner of the keyboard, to initiate  the search.  ", "page_idx": 133, "bbox": [122, 624.3988037109375, 495, 659.6446533203125], "page_size": [612.0, 792.0]}
{"layout": 1857, "type": "text", "text": "Figure 104: GPT-4V navigates through GUI to shop for an ergonomic keyboard online. Blue highlights the predicted actions. Check Section  9.8  for detailed discussions. ", "page_idx": 133, "bbox": [108, 674.216552734375, 501.0113220214844, 697.1304931640625], "page_size": [612.0, 792.0]}
{"layout": 1858, "type": "text", "text": "Sec. 9.8 GUI Navigation ", "text_level": 1, "page_idx": 134, "bbox": [144, 55, 280, 71], "page_size": [612.0, 792.0]}
{"layout": 1859, "type": "text", "text": "Online Shopping (Ergonomic Keyboard) ", "text_level": 1, "page_idx": 134, "bbox": [121, 78, 306, 90], "page_size": [612.0, 792.0]}
{"layout": 1860, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 134, "bbox": [121, 94, 159, 104.75], "page_size": [612.0, 792.0]}
{"layout": 1861, "type": "text", "text": "Imagine that you are a robot operating a phone. Like how humans operate the phone, you can move your finger  to an icon, click an icon with your finger, scroll down a screen, or type some texts with the keyboard.  ", "page_idx": 134, "bbox": [121, 105.78949737548828, 495, 124.44798278808594], "page_size": [612.0, 792.0]}
{"layout": 1862, "type": "text", "text": "You are asked to shop for an ergonomic keyboard, your budget is between  $\\S50$   and  $\\S100$  . ", "page_idx": 134, "bbox": [121, 130, 418.4900817871094, 139.1781005859375], "page_size": [612.0, 792.0]}
{"layout": 1863, "type": "text", "text": "Previous action: \"My next move would be to move my finger to the search bar, which is located at the top of the  screen. I would then click on the search bar and type in \"ergonomic keyboard\" using the keyboard. After typing  in the search term, I would click on the \"Search\" button, which is located on the bottom right corner of the  keyboard, to initiate the search. \" ", "page_idx": 134, "bbox": [121, 145.56082153320312, 495, 184.84146118164062], "page_size": [612.0, 792.0]}
{"layout": 1864, "type": "text", "text": "You have now completed the previous action as described above. Below is what you see on the phone screen.  Please predict your immediate next move. If the action involves moving your finger to an icon, describe the  location of that icon as detailed as possible. ", "page_idx": 134, "bbox": [121, 191.22418212890625, 495, 220.19375610351562], "page_size": [612.0, 792.0]}
{"layout": 1865, "type": "image", "page_idx": 134, "img_path": "layout_images/2309.17421v2_316.jpg", "bbox": [123, 227, 300, 607], "page_size": [612.0, 792.0], "ocr_text": "Q ergonomic keyboard\n\n© Deliver to Linjie -\n\nvprime tetetrk &Up = Mac_~— Windows Filters »\n\nExplore Related Brands\n\nSponsored ©\n\nSete tetety 11,057 tek teh\nvprime vprime\nShop the VIVO Store >\nSponsored\n\nX9 Performance Ergonomic\nKeyboard Wireless - Your Comfort\nMatters - Full Size Rechargeable...\nLaptop, PC\n\n4.3 tet te te Hz (495).\n\n", "vlm_text": "The image appears to be a screenshot of an Amazon shopping app interface on a mobile device. The search query \"ergonomic keyboard\" is visible at the top. The screen displays a section titled \"Explore Related Brands,\" showing logos of brands like VIVO, Perixx, Kinesis Gaming, and Macally. Below that, there's an image of a person typing on a keyboard with a pull-out tray. There are also product listings with ratings. The bottom section of the screen shows the app navigation menu with icons for home, categories, cart, and account."}
{"layout": 1866, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 134, "bbox": [122, 612, 159, 622.75], "page_size": [612.0, 792.0]}
{"layout": 1867, "type": "text", "text": "My immediate next move would be to move my finger to the \"Filters\" button, which is located at the top right  corner of the screen, next to the \"Windows\" button. I would then click on the \"Filters\" button to open the filter  options and narrow down my search results to only show ergonomic keyboards within my budget range of  $\\S50$   to   $\\S100$  .  ", "page_idx": 134, "bbox": [121, 624.3988037109375, 495, 659.6446533203125], "page_size": [612.0, 792.0]}
{"layout": 1868, "type": "text", "text": "Figure 105: GPT-4V navigates through GUI to shop for an ergonomic keyboard online. Blue highlights the predicted actions. Check Section  9.8  for detailed discussions. ", "page_idx": 134, "bbox": [108, 674.216552734375, 501.0113220214844, 697.1304931640625], "page_size": [612.0, 792.0]}
{"layout": 1869, "type": "text", "text": "Sec. 9.8 GUI Navigation ", "text_level": 1, "page_idx": 135, "bbox": [143, 54, 282, 72], "page_size": [612.0, 792.0]}
{"layout": 1870, "type": "text", "text": "Online Shopping (Ergonomic Keyboard) ", "text_level": 1, "page_idx": 135, "bbox": [121, 78, 307, 91], "page_size": [612.0, 792.0]}
{"layout": 1871, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 135, "bbox": [121, 94, 159, 104.75], "page_size": [612.0, 792.0]}
{"layout": 1872, "type": "text", "text": "Imagine that you are a robot operating a phone. Like how humans operate the phone, you can move your finger  to an icon, click an icon with your finger, scroll down a screen, or type some texts with the keyboard.  ", "page_idx": 135, "bbox": [122, 105.78949737548828, 495, 124.44798278808594], "page_size": [612.0, 792.0]}
{"layout": 1873, "type": "text", "text": "You are asked to shop for an ergonomic keyboard, your budget is between  $\\S50$   and  $\\S100$  . ", "page_idx": 135, "bbox": [122, 130, 418.4900817871094, 139.1781005859375], "page_size": [612.0, 792.0]}
{"layout": 1874, "type": "text", "text": "Previous action: \"My immediate next move would be to move my finger to the \"Filters\" button, which is located at  the top right corner of the screen, next to the \"Windows\" button. I would then click on the \"Filters\" button to open  the filter options and narrow down my search results to only show ergonomic keyboards within my budget range  of  $\\S50$   to  $\\S\\,100$  . \" ", "page_idx": 135, "bbox": [122, 145.56082153320312, 495, 185], "page_size": [612.0, 792.0]}
{"layout": 1875, "type": "text", "text": "You have now completed the previous action as described above. Below is what you see on the phone screen.  Please predict your immediate next move. If the action involves moving your finger to an icon, describe the  location of that icon as detailed as possible. ", "page_idx": 135, "bbox": [122, 191.22418212890625, 495, 220.19375610351562], "page_size": [612.0, 792.0]}
{"layout": 1876, "type": "image", "page_idx": 135, "img_path": "layout_images/2309.17421v2_317.jpg", "bbox": [123, 226, 301, 607], "page_size": [612.0, 792.0], "ocr_text": "Filters\n\nPrime &\nDelivery\n\nCategories\n\nBrands\n\nColors\n\nPrice & Deals\n\nCustomer\nReviews\n\nSort by\n\nMore-\nsustainable\nProducts\n\nKeyboard\nSpecial\nFeatures\n\nKeyboard\nConnectivity\nTechnology\n\nIncluded\n\newe &Up Mac _—- Windows Filters «\n\nClose\n\nPrime & Delivery\n\nDelivery\n\nGet It by Tomorrow Overnight by 11AM\nShipping Options\n\nFrustration-Free Packaging\n‘Amazon Global Store\nInternational Shipping Eligible\n\nPrime Programs\n\nAll Prime\n\n", "vlm_text": "The image displays a screenshot from a mobile device showing an Amazon search page for \"ergonomic keyboard.\" The page includes filter options for refining search results. On the top of the screen, the search term \"ergonomic keyboard\" is entered in the search bar, with some toggle options below, such as Prime, ratings, and specific operating systems (Mac, Windows). The main portion of the image shows the \"Filters\" menu open, specifically the \"Prime & Delivery\" options. Available delivery options include \"Get it by Tomorrow\" and \"Overnight by 11AM.\" There are also shipping options like \"Frustration-Free Packaging\" and \"Amazon Global Store.\" Other navigation categories can be seen on the left side bar, such as Categories, Brands, Colors, Price & Deals, and more. At the bottom, there is a button to show more than 10,000 results. The device information bar at the top shows it's 4:42, with a WiFi symbol and battery level of 79%."}
{"layout": 1877, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 135, "bbox": [122, 612, 159, 620.75], "page_size": [612.0, 792.0]}
{"layout": 1878, "type": "text", "text": "My immediate next move would be to move my finger to the \"Price & Deals\" filter option, which is located under  the \"Colors\" filter option and above the \"Sort by\" filter option. I would then click on the \"Price & Deals\" filter  option to open the price range filter and set my minimum price to  $\\S50$   and my maximum price to  $\\S100$  . This  would further narrow down my search results to only show ergonomic keyboards within my budget range.  ", "page_idx": 135, "bbox": [122, 624.3988037109375, 495, 659.6446533203125], "page_size": [612.0, 792.0]}
{"layout": 1879, "type": "text", "text": "Figure 106: GPT-4V navigates through GUI to shop for an ergonomic keyboard online. Blue highlights the predicted actions. Check Section  9.8  for detailed discussions. ", "page_idx": 135, "bbox": [108, 674.216552734375, 501.0113220214844, 697.1304931640625], "page_size": [612.0, 792.0]}
{"layout": 1880, "type": "text", "text": "Sec. 9.8 GUI Navigation ", "text_level": 1, "page_idx": 136, "bbox": [143, 54, 282, 72], "page_size": [612.0, 792.0]}
{"layout": 1881, "type": "text", "text": "Online Shopping (Ergonomic Keyboard) ", "text_level": 1, "page_idx": 136, "bbox": [121, 78, 306, 91], "page_size": [612.0, 792.0]}
{"layout": 1882, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 136, "bbox": [121, 94, 159, 104.75], "page_size": [612.0, 792.0]}
{"layout": 1883, "type": "text", "text": "Imagine that you are a robot operating a phone. Like how humans operate the phone, you can move your finger  to an icon, click an icon with your finger, scroll down a screen, or type some texts with the keyboard.  ", "page_idx": 136, "bbox": [121, 105.78949737548828, 495, 124.44798278808594], "page_size": [612.0, 792.0]}
{"layout": 1884, "type": "text", "text": "You are asked to shop for an ergonomic keyboard, your budget is between  $\\S50$   and  $\\S100$  . ", "page_idx": 136, "bbox": [121, 130, 418.4900817871094, 139.1781005859375], "page_size": [612.0, 792.0]}
{"layout": 1885, "type": "text", "text": "Previous action: \" My immediate next move would be to move my finger to the \"Price & Deals\" filter option, which  is located under the \"Colors\" filter option and above the \"Sort by\" filter option. I would then click on the \"Price &  Deals\" filter option to open the price range filter and set my minimum price to  $\\S50$   and my maximum price to   $\\S\\,100$  . This would further narrow down my search results to only show ergonomic keyboards within my budget  range. \" ", "page_idx": 136, "bbox": [121, 145.56082153320312, 495, 195.1525421142578], "page_size": [612.0, 792.0]}
{"layout": 1886, "type": "text", "text": "You have now completed the previous action as described above. Below is what you see on the phone screen.  What will you do next? If the action involves moving your finger to an icon, describe the location of that icon as  detailed as possible. ", "page_idx": 136, "bbox": [121, 201.53524780273438, 495, 230.50482177734375], "page_size": [612.0, 792.0]}
{"layout": 1887, "type": "image", "page_idx": 136, "img_path": "layout_images/2309.17421v2_318.jpg", "bbox": [123, 233, 301, 614], "page_size": [612.0, 792.0], "ocr_text": "Q ergonomic keyboard\n\nvprime( J) $50to $100 Computer Keyb... Filters (2) «\n\nFilters (2) Close\n\nPrime & Price & Deals\nDelivery\nPrice\n\nCategories AllPrices Under $25 $25 to $50\n$50to $100 X — $100 to $200\n$200 & Above\n\nDeals & Discounts\n\nPrice & Deals 1 All Discounts\n\nsustainable\nProducts\n\nSpecial\nFeatures\n\nConnectivity\nTechnology\n\nIncluded\n‘Components\n\nClear Filters\n\n", "vlm_text": "The image shows a mobile shopping app screen with filters applied for searching \"ergonomic keyboard.\" \n\n- The filters include:\n  - Price range: $50 to $100\n  - Prime delivery option activated\n  - Category filter: Computer Keyboards\n\n- Additional filter options visible include:\n  - Customer Reviews\n  - Sort by\n  - More-sustainable Products\n  - Special Features\n  - Connectivity Technology\n  - Included Components\n\nAt the bottom, there is an option to clear filters and a button showing 1,987 results available."}
{"layout": 1888, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 136, "bbox": [122, 618, 159, 627.75], "page_size": [612.0, 792.0]}
{"layout": 1889, "type": "text", "text": "My next move would be to move my finger to the \"Show 1,987 results\" button, which is located at the bottom of  the screen, below the \"Clear Filters\" button and above the navigation bar. I would then click on the \"Show 1,987  results\" button to view the search results for ergonomic keyboards within my budget range.  ", "page_idx": 136, "bbox": [121, 630.80322265625, 495, 657.0829467773438], "page_size": [612.0, 792.0]}
{"layout": 1890, "type": "text", "text": "Figure 107: GPT-4V navigates through GUI to shop for an ergonomic keyboard online. Blue highlights the predicted actions. Check Section  9.8  for detailed discussions. ", "page_idx": 136, "bbox": [108, 674.216552734375, 501.0113220214844, 697.1304931640625], "page_size": [612.0, 792.0]}
{"layout": 1891, "type": "text", "text": "Sec. 9.8 GUI Navigation ", "text_level": 1, "page_idx": 137, "bbox": [144, 49, 279, 64], "page_size": [612.0, 792.0]}
{"layout": 1892, "type": "text", "text": "Online Shopping (Ergonomic Keyboard) ", "text_level": 1, "page_idx": 137, "bbox": [122, 71, 306, 83], "page_size": [612.0, 792.0]}
{"layout": 1893, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 137, "bbox": [122, 87, 159, 97.75], "page_size": [612.0, 792.0]}
{"layout": 1894, "type": "text", "text": "Imagine that you are a robot operating a phone. Like how humans operate the phone, you can move your finger  to an icon, click an icon with your finger, scroll down a screen, or type some texts with the keyboard.  ", "page_idx": 137, "bbox": [122, 98.87549591064453, 495, 117.53398132324219], "page_size": [612.0, 792.0]}
{"layout": 1895, "type": "text", "text": "You are asked to shop for an ergonomic keyboard, your budget is between  $\\S50$   and  $\\S100$  . ", "page_idx": 137, "bbox": [122, 123, 418.4900817871094, 132.26409912109375], "page_size": [612.0, 792.0]}
{"layout": 1896, "type": "text", "text": "Previous action: \" My next move would be to move my finger to the \"Show 1,987 results\" button, which is located  at the bottom of the screen, below the \"Clear Filters\" button and above the navigation bar. I would then click on  the \"Show 1,987 results\" button to view the search results for ergonomic keyboards within my budget range. \" ", "page_idx": 137, "bbox": [122, 138.64682006835938, 495, 167.61636352539062], "page_size": [612.0, 792.0]}
{"layout": 1897, "type": "text", "text": "You have now completed the previous action as described above. Below is what you see on the phone screen.  What will you do next? If the action involves moving your finger to an icon, describe the location of that icon as  detailed as possible. ", "page_idx": 137, "bbox": [122, 173.9990997314453, 489, 202.96865844726562], "page_size": [612.0, 792.0]}
{"layout": 1898, "type": "image", "page_idx": 137, "img_path": "layout_images/2309.17421v2_319.jpg", "bbox": [123, 216, 297, 590], "page_size": [612.0, 792.0], "ocr_text": "Q ergonomic keyboard\n\n© Deliver to Linjie -\n\nvprime $50to $100 — Computer Keyb.. Filters (2) ¥\n\nExplore Related Brands\n\nSponsored @\n\nQmacatiy\n\nete tetetr 11,057\nvprime\n\nShop the VIVO Store >\n\nKensington Pro Fit Ergonomic\nWireless Keyboard - Black\n(K75401US)\n\nChrome OS, macOS, Windows\n\n3.8 Wete te try (1,508)\n£AN99\n\n", "vlm_text": "The image is a screenshot of an Amazon shopping app interface, showing search results for \"ergonomic keyboard.\" At the top, there is a search bar that contains the query. Delivery information is indicated next to the search bar, with a partial delivery location of \"Lin Jie.\"\n\nBelow are filtering options including \"Prime,\" price range \"$50 to $100,\" and \"Computer Keyboards.\" There is a section called \"Explore Related Brands\" with brand logos like VI:VO, Perixx, Kinesis Gaming, and Macally.\n\nAn image shows a person typing on a keyboard placed on an ergonomic desk attachment. A small product listing below this image highlights a \"Best seller\" item with 11,057 reviews, showing high ratings and a Prime logo. There is also an ergonomic desk device next to it.\n\nAnother product listing at the bottom shows a \"Kensington Pro Fit Ergonomic Wireless Keyboard\" with pricing at $40.99, reviews, Prime availability, and compatibility details (Chrome OS, macOS, Windows)."}
{"layout": 1899, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 137, "bbox": [122, 595, 159, 605.75], "page_size": [612.0, 792.0]}
{"layout": 1900, "type": "text", "text": "My next move would be to scroll down the screen to view the different options for ergonomic keyboards within  my budget range. I would then move my finger to the \"Kensington Pro Fit Ergonomic Wireless Keyboard - Black\"  option, which is located below the \"Explore Related Brands\" section and above the \"VIVO\" option. I would then  click on the \"Kensington Pro Fit Ergonomic Wireless Keyboard - Black\" option to view more details about the  product and potentially make a purchase.  ", "page_idx": 137, "bbox": [122, 607.8782348632812, 495, 652.0902709960938], "page_size": [612.0, 792.0]}
{"layout": 1901, "type": "text", "text": "Figure 108: GPT-4V navigates through GUI to shop for an ergonomic keyboard online. Blue highlights the predicted actions. Red highlights the inaccurate location of the product option to be selected. Check Section  9.8  for detailed discussions. ", "page_idx": 137, "bbox": [108, 667.3025512695312, 503, 704.0444946289062], "page_size": [612.0, 792.0]}
{"layout": 1902, "type": "text", "text": "Sec. 9.8 GUI Navigation ", "text_level": 1, "page_idx": 138, "bbox": [145, 49, 279, 64], "page_size": [612.0, 792.0]}
{"layout": 1903, "type": "text", "text": "Online Shopping (Ergonomic Keyboard) ", "text_level": 1, "page_idx": 138, "bbox": [122, 71, 306, 83], "page_size": [612.0, 792.0]}
{"layout": 1904, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 138, "bbox": [122, 87, 159, 97.75], "page_size": [612.0, 792.0]}
{"layout": 1905, "type": "text", "text": "Imagine that you are a robot operating a phone. Like how humans operate the phone, you can move your finger  to an icon, click an icon with your finger, scroll down a screen, or type some texts with the keyboard.  ", "page_idx": 138, "bbox": [122, 98.87549591064453, 493, 117.53398132324219], "page_size": [612.0, 792.0]}
{"layout": 1906, "type": "text", "text": "You are asked to shop for an ergonomic keyboard, your budget is between  $\\S50$   and  $\\S100$  . ", "page_idx": 138, "bbox": [122, 123, 418.4900817871094, 132.26409912109375], "page_size": [612.0, 792.0]}
{"layout": 1907, "type": "text", "text": "Previous action: \" My next move would be to scroll down the screen to view the different options for ergonomic  keyboards within my budget range. I would then move my finger to the \"Kensington Pro Fit Ergonomic Wireless  Keyboard - Black\" option, which is located below the \"Explore Related Brands\" section and above the \"VIVO\"  option. I would then click on the \"Kensington Pro Fit Ergonomic Wireless Keyboard - Black\" option to view more  details about the product and potentially make a purchase. \" ", "page_idx": 138, "bbox": [122, 138.64682006835938, 493, 188.23854064941406], "page_size": [612.0, 792.0]}
{"layout": 1908, "type": "text", "text": "You have now completed the previous action as described above. Below is what you see on the phone screen.  What will you do next? If the action involves moving your finger to an icon, describe the location of that icon as  detailed as possible. ", "page_idx": 138, "bbox": [122, 194.62124633789062, 493, 223.5908203125], "page_size": [612.0, 792.0]}
{"layout": 1909, "type": "image", "page_idx": 138, "img_path": "layout_images/2309.17421v2_320.jpg", "bbox": [123, 228, 297, 601], "page_size": [612.0, 792.0], "ocr_text": "© | Q Search Amazon.com\n\nVisit the Kensington Store 3.8 tri\n\nKensington Pro Fit Ergonomic Wireless Keyboard - Black\n(K75401US)\n\n9 A\n\nGet a $8 bonus when you reload $100 or more to your\ngift card balance (Restrictions apply).\n\nBuy New\n\n-6% °49\"\nList Price: $52.99 @\n\n“prime\n\na Q\n\n", "vlm_text": "The image is a screenshot of an Amazon product listing page. The product being displayed is the \"Kensington Pro Fit Ergonomic Wireless Keyboard - Black (K75401US).\" It has a 3.8 out of 5-star rating based on 1,508 reviews. The listing shows a price of $49.99, which is a 6% discount from the original list price of $52.99. Additionally, there's a promotion offering an $8 bonus when you reload $100 or more to your gift card balance. Near the bottom, there are icons for Amazon Prime, search, profile, and shopping cart. The time displayed on the mobile device is 5:39, and the battery level is at 75%."}
{"layout": 1910, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 138, "bbox": [122, 605, 158, 615.75], "page_size": [612.0, 792.0]}
{"layout": 1911, "type": "text", "text": "My next move would be to scroll down the screen to view more details about the \"Kensington Pro Fit Ergonomic  Wireless Keyboard - Black\" product. I would then move my finger to the \"Buy New\" button, which is located  below the product image and above the \"Get a  $\\S8$   bonus when you reload  $\\S100$   or more to your gift card  balance\" section. I would then click on the \"Buy New\" button to proceed with the purchase.  ", "page_idx": 138, "bbox": [122, 617.4848022460938, 493, 652.7306518554688], "page_size": [612.0, 792.0]}
{"layout": 1912, "type": "text", "text": "Figure 109: GPT-4V navigates through GUI to shop for an ergonomic keyboard online. Blue highlights the predicted actions.  Red  highlights the inaccurate action prediction (“Buy New” is not a clickable button). Check Section  9.8  for detailed discussions. ", "page_idx": 138, "bbox": [108, 667.3025512695312, 503, 704.0444946289062], "page_size": [612.0, 792.0]}
{"layout": 1913, "type": "text", "text": "Sec. 9.8 GUI Navigation ", "text_level": 1, "page_idx": 139, "bbox": [144, 55, 281, 71], "page_size": [612.0, 792.0]}
{"layout": 1914, "type": "text", "text": "Online Shopping (Ergonomic Keyboard) ", "text_level": 1, "page_idx": 139, "bbox": [121, 78, 306, 90], "page_size": [612.0, 792.0]}
{"layout": 1915, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 139, "bbox": [121, 94, 159, 104], "page_size": [612.0, 792.0]}
{"layout": 1916, "type": "text", "text": "Imagine that you are a robot operating a phone. Like how humans operate the phone, you can move your finger  to an icon, click an icon with your finger, scroll down a screen, or type some texts with the keyboard.  ", "page_idx": 139, "bbox": [122, 105.78949737548828, 495, 124.44798278808594], "page_size": [612.0, 792.0]}
{"layout": 1917, "type": "text", "text": "You are asked to shop for an ergonomic keyboard, your budget is between  $\\S50$   and  $\\S100$  . ", "page_idx": 139, "bbox": [122, 130, 418.4900817871094, 139.1781005859375], "page_size": [612.0, 792.0]}
{"layout": 1918, "type": "text", "text": "Previous action: \" My next move would be to scroll down the screen to view more details about the \"Kensington  Pro Fit Ergonomic Wireless Keyboard - Black\" product. I would then move my finger to the \"Buy New\" button,  which is located below the product image and above the \"Get a  $\\S8$   bonus when you reload  $\\S\\,100$   or more to your  gift card balance\" section. I would then click on the \"Buy New\" button to proceed with the purchase. \" ", "page_idx": 139, "bbox": [122, 145.56082153320312, 495, 184.84146118164062], "page_size": [612.0, 792.0]}
{"layout": 1919, "type": "text", "text": "You have now completed the previous action as described above. Below is what you see on the phone screen.  What will you do next? If the action involves moving your finger to an icon, describe the location of that icon as  detailed as possible. ", "page_idx": 139, "bbox": [122, 191.22418212890625, 495, 220.19375610351562], "page_size": [612.0, 792.0]}
{"layout": 1920, "type": "image", "page_idx": 139, "img_path": "layout_images/2309.17421v2_321.jpg", "bbox": [121, 227, 303, 611], "page_size": [612.0, 792.0], "ocr_text": "Q Search Amazon.com\n\nvprime\nFREE Returns\n\nFREE delivery Thursday, July 20. Order within 6 hrs\n20 mins\n\n© Deliver to Linjie\nIn Stock\n\nQty: 1 Vv\n\nPayment Secure transaction\nShips from Amazon.com\nSold by Amazon.com\n\nReturns _ Eligible for Return, Refund or Replacement\nwithin 30 days of receipt\n\nSupport Free Amazon tech support included\n\nPackaging Shows what's inside\n\nAdd a Protection Plan\n\nO 4-Year Protection for $8.99\n\nSave with Used - Like New\n-51% $2598\nFREE delivery: Monday, July 24\n\na Q\n\n", "vlm_text": "This image is a screenshot of an Amazon product page on a mobile device. It shows options to add an item to the cart or buy it now. The item is in stock, and free delivery is available by Thursday, July 20, if ordered within a specific time. Return and refund information is included, as well as details about a protection plan for $8.99. There's also an option to buy a \"Used - Like New\" version at a 51% discount, priced at $25.98, with free delivery by Monday, July 24."}
{"layout": 1921, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 139, "bbox": [122, 618, 158, 628.75], "page_size": [612.0, 792.0]}
{"layout": 1922, "type": "text", "text": "My next move would be to move my finger to the \"Add to Cart\" button, which is located below the \"Buy New\"  button and above the \"Ships from Amazon\" section. I would then click on the \"Add to Cart\" button to add the item  to my shopping cart.  ", "page_idx": 139, "bbox": [122, 630.80322265625, 495, 657.0829467773438], "page_size": [612.0, 792.0]}
{"layout": 1923, "type": "text", "text": "Figure 110: GPT-4V navigates through GUI to shop for an ergonomic keyboard online. Blue highlights the predicted actions. Check Section  9.8  for detailed discussions. ", "page_idx": 139, "bbox": [108, 674.216552734375, 501.0113220214844, 697.1304931640625], "page_size": [612.0, 792.0]}
{"layout": 1924, "type": "text", "text": "Sec. 9.8 GUI Navigation ", "text_level": 1, "page_idx": 140, "bbox": [144, 49, 280, 65], "page_size": [612.0, 792.0]}
{"layout": 1925, "type": "text", "text": "Online Shopping (Ergonomic Keyboard) ", "page_idx": 140, "bbox": [122, 71.34624481201172, 305.2043762207031, 82.07862091064453], "page_size": [612.0, 792.0]}
{"layout": 1926, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 140, "bbox": [122, 87, 159, 97.75], "page_size": [612.0, 792.0]}
{"layout": 1927, "type": "text", "text": "Imagine that you are a robot operating a phone. Like how humans operate the phone, you can move your finger  to an icon, click an icon with your finger, scroll down a screen, or type some texts with the keyboard.  ", "page_idx": 140, "bbox": [122, 98.87549591064453, 493, 117.53398132324219], "page_size": [612.0, 792.0]}
{"layout": 1928, "type": "text", "text": "You are asked to shop for an ergonomic keyboard, your budget is between  $\\S50$   and  $\\S100$  . ", "page_idx": 140, "bbox": [122, 123, 418.4900817871094, 132.26409912109375], "page_size": [612.0, 792.0]}
{"layout": 1929, "type": "text", "text": "Previous action: \" My next move would be to move my finger to the \"Add to Cart\" button, which is located below  the \"Buy New\" button and above the \"Ships from Amazon\" section. I would then click on the \"Add to Cart\" button  to add the item to my shopping cart. \" ", "page_idx": 140, "bbox": [122, 138.64682006835938, 493, 167.61636352539062], "page_size": [612.0, 792.0]}
{"layout": 1930, "type": "text", "text": "You have now completed the previous action as described above. Below is what you see on the phone screen.  What will you do next? If the action involves moving your finger to an icon, describe the location of that icon as  detailed as possible. ", "page_idx": 140, "bbox": [122, 173.9990997314453, 493, 202.96865844726562], "page_size": [612.0, 792.0]}
{"layout": 1931, "type": "image", "page_idx": 140, "img_path": "layout_images/2309.17421v2_322.jpg", "bbox": [123, 212, 307, 606], "page_size": [612.0, 792.0], "ocr_text": "Q Search Amazon.com\n\n© Deliver to Linjie -\n\nSubtotal $4999\n\nKensington Pro Fit Ergonomic\nWireless Keyboard - Black (K7540...\n\n$4999\nyprime & FREE Returns\nIn Stock\n\nU + Delete Save for later\n\nCompare with similar items\n\nReturns are easy\n30-day returns on millions of items\n\nAlways the perfect gift.\nShop gift cards\n\n", "vlm_text": "The image is a screenshot of an Amazon shopping cart page on a mobile device. The cart contains one item: a Kensington Pro Fit Ergonomic Wireless Keyboard - Black. The price of the keyboard is $49.99, and it is noted to be in stock with free Prime returns. The subtotal amount for the cart is $49.99. There are options to delete the item, save it for later, or compare it with similar items. Additionally, there's a button to proceed to checkout with the single item. The page also promotes easy returns with 30-day returns on millions of items and suggests shopping for gift cards."}
{"layout": 1932, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 140, "bbox": [122, 611, 158, 620.75], "page_size": [612.0, 792.0]}
{"layout": 1933, "type": "text", "text": "My next move would be to move my finger to the \"Proceed to Checkout\" button, which is located below the  \"Compare with similar items\" section and above the  $\"30.$  -day returns\" section. I would then click on the \"Proceed  to Checkout\" button to proceed with the purchase.  ", "page_idx": 140, "bbox": [122, 623.8892211914062, 493, 650.1689453125], "page_size": [612.0, 792.0]}
{"layout": 1934, "type": "text", "text": "Figure 111: GPT-4V navigates through GUI to shop for an ergonomic keyboard online. Blue highlights the predicted actions.  Red  highlights the inaccurate location of the “Proceed to checkout” buttion. Check Section  9.8  for detailed discussions. ", "page_idx": 140, "bbox": [108, 667.3025512695312, 505.7747802734375, 704.0444946289062], "page_size": [612.0, 792.0]}
{"layout": 1935, "type": "text", "text": "Sec. 9.8 GUI Navigation ", "text_level": 1, "page_idx": 141, "bbox": [143, 42, 279, 57], "page_size": [612.0, 792.0]}
{"layout": 1936, "type": "text", "text": "Notification Understanding ", "text_level": 1, "page_idx": 141, "bbox": [122, 64, 246, 75], "page_size": [612.0, 792.0]}
{"layout": 1937, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 141, "bbox": [121, 81, 159, 91], "page_size": [612.0, 792.0]}
{"layout": 1938, "type": "text", "text": "Imagine that you are a robot operating a phone. Like how humans operate the phone, you can move your finger  to an icon, click an icon with your finger, scroll down a screen, or type some texts with the keyboard.  ", "page_idx": 141, "bbox": [122, 92.45246124267578, 495, 111.11094665527344], "page_size": [612.0, 792.0]}
{"layout": 1939, "type": "text", "text": "Below is what you see on the phone screen. Describe the image and predict your next move. If the action  involves moving your finger to an icon, describe the location of that icon as detailed as possible. ", "page_idx": 141, "bbox": [122, 123.38571166992188, 471.8161926269531, 142.044189453125], "page_size": [612.0, 792.0]}
{"layout": 1940, "type": "image", "page_idx": 141, "img_path": "layout_images/2309.17421v2_323.jpg", "bbox": [122, 147, 311, 554], "page_size": [612.0, 792.0], "ocr_text": "o£ yykhahah\n<6) | see you are in Seattle, let's meet up.\n\nCalendar Settings Clock Wallet\n\nWeather App Store Teams Twitter\n\nOO€\n\nYouTube Amazon\n\n* ©\n\nCompass Voice Memos _Instagram\n\n", "vlm_text": "The image shows a smartphone home screen with various app icons, including:\n\n- Calendar\n- Settings\n- Clock\n- Wallet\n- Weather\n- App Store\n- Teams\n- Twitter\n- Chase\n- Camera\n- YouTube\n- Amazon\n- Maps\n- Compass\n- Voice Memos\n- Instagram (with a notification)\n\nAt the top, there's a notification from Instagram user \"yykhahah\" stating, \"I see you are in Seattle, let's meet up.\"\n\nThe dock at the bottom contains the Phone, Photos, Safari, and Mail apps. The time displayed is 11:01, with a battery percentage of 89%."}
{"layout": 1941, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 141, "bbox": [122, 561, 159, 571], "page_size": [612.0, 792.0]}
{"layout": 1942, "type": "text", "text": "The image on the phone screen is the home screen of an iPhone. There are multiple app icons arranged in a  grid-like pattern, including Calendar, Settings, Twitter, Amazon, Instagram, etc. At the top of the screen, there is  a notification from a user named \"yykhahah\" that reads \"I see you are in Seattle, let's meet up.\" At the bottom of  the screen, there is a search bar. ", "page_idx": 141, "bbox": [122, 572.6353759765625, 495, 607.8812255859375], "page_size": [612.0, 792.0]}
{"layout": 1943, "type": "text", "text": "Based on the notification, my next move would be to move my finger to the Maps app icon, which is located in  the second row and the first column of the app grid. This will allow me to search for a location in Seattle and plan  a meet up with the user.  ", "page_idx": 141, "bbox": [122, 617.4661254882812, 495, 643.745849609375], "page_size": [612.0, 792.0]}
{"layout": 1944, "type": "text", "text": "Figure 112: Prompting GPT-4V to predict the action upon receiving a notification. GPT-4V can accurately recognize the notification and the corresponding content (highlighted in  green  ).  Blue highlights the predicted actions.  Red  highlights the inaccurate location of the Maps app icon. Check Section  9.8  for detailed discussions. ", "page_idx": 141, "bbox": [108, 658.7725830078125, 504, 710.468505859375], "page_size": [612.0, 792.0]}
{"layout": 1945, "type": "text", "text": "Sec. 9.8 GUI Navigation ", "text_level": 1, "page_idx": 142, "bbox": [143, 128, 282, 145], "page_size": [612.0, 792.0]}
{"layout": 1946, "type": "text", "text": "Notification Understanding ", "text_level": 1, "page_idx": 142, "bbox": [121, 151, 247, 163], "page_size": [612.0, 792.0]}
{"layout": 1947, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 142, "bbox": [121, 167, 159, 178], "page_size": [612.0, 792.0]}
{"layout": 1948, "type": "text", "text": "Imagine that you are a robot operating a computer. Like how humans operate the computer, you can move the  mouse, click an icon with the mouse, or type some texts with the keyboard.  ", "page_idx": 142, "bbox": [122, 178.91026306152344, 488, 197.5687255859375], "page_size": [612.0, 792.0]}
{"layout": 1949, "type": "text", "text": "Below is what you see on the computer screen. Describe the image and predict your next move. If the action  involves moving the mouse, describe the location and the icon as detailed as possible. ", "page_idx": 142, "bbox": [122, 209.843505859375, 488, 228.50198364257812], "page_size": [612.0, 792.0]}
{"layout": 1950, "type": "image", "page_idx": 142, "img_path": "layout_images/2309.17421v2_324.jpg", "bbox": [122, 237, 494, 471], "page_size": [612.0, 792.0], "ocr_text": "YouTube\nMusic\n\nKevin Lin\nis calling you\n\n%\n\n&\n\nHQCH IMG > VOB - 4% v6 cow BEM\n\n", "vlm_text": "The image is a screenshot of a computer desktop featuring a scenic aerial wallpaper of a bridge over water connecting two rocky landmasses. On the desktop, there are several application icons on the left side, including YouTube Music, Recycle Bin, Abby Terminal, Google Chrome, Slack, Discord, and Adobe Acrobat Reader. The taskbar at the bottom displays various application icons, such as Microsoft Edge, File Explorer, Visual Studio, Teams, and others. Additionally, there is a notification in the bottom right corner indicating an incoming call from Kevin Lin via a communication application. The time and date in the corner show 11:18 AM on July 21, 2023."}
{"layout": 1951, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 142, "bbox": [122, 484, 159, 494], "page_size": [612.0, 792.0]}
{"layout": 1952, "type": "text", "text": "The image on the computer screen is a desktop background of a bridge over a body of water. There are several  icons on the desktop, including a YouTube icon, a Recycle Bin icon, and a Terminal icon. At the bottom of the  screen, there is a taskbar with several application icons, such as Microsoft Edge and File Explorer. On the  bottom right corner of the screen, there is a notification that says \"Kevin Lin is calling you.\" ", "page_idx": 142, "bbox": [122, 495.78094482421875, 492.4618225097656, 531.0267944335938], "page_size": [612.0, 792.0]}
{"layout": 1953, "type": "text", "text": "Based on the notification, my next move would be to move the mouse to the bottom right corner of the screen  and click on the notification. This would open the call application and allow me to answer Kevin Lin's call.  ", "page_idx": 142, "bbox": [122, 540.6116943359375, 488, 557.92529296875], "page_size": [612.0, 792.0]}
{"layout": 1954, "type": "text", "text": "Figure 113: Prompting GPT-4V to predict the action upon receiving a notification. GPT-4V can accurately recognize the notification and the corresponding content (highlighted in  green  ).  Blue highlights the predicted actions.  Red  highlights the inaccurate location of the Maps app icon. Check Section  9.8  for detailed discussions. ", "page_idx": 142, "bbox": [108, 572.3115234375, 504, 624.0075073242188], "page_size": [612.0, 792.0]}
{"layout": 1955, "type": "text", "text": "Sec. 9.8 GUI Navigation ", "text_level": 1, "page_idx": 143, "bbox": [144, 126, 282, 141], "page_size": [612.0, 792.0]}
{"layout": 1956, "type": "text", "text": "Notification Understanding ", "text_level": 1, "page_idx": 143, "bbox": [121, 148, 247, 160], "page_size": [612.0, 792.0]}
{"layout": 1957, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 143, "bbox": [121, 165, 158, 174.75], "page_size": [612.0, 792.0]}
{"layout": 1958, "type": "text", "text": "Imagine that you are a robot operating a computer. Like how humans operate the computer, you can move the  mouse, click an icon with the mouse, or type some texts with the keyboard.  ", "page_idx": 143, "bbox": [122, 176.2530975341797, 488, 194.91156005859375], "page_size": [612.0, 792.0]}
{"layout": 1959, "type": "text", "text": "Below is what you see on the computer screen. Describe the image and predict your next move. If the action  involves moving the mouse, describe the location as detailed as possible. ", "page_idx": 143, "bbox": [122, 207.18634033203125, 482, 225.84481811523438], "page_size": [612.0, 792.0]}
{"layout": 1960, "type": "image", "page_idx": 143, "img_path": "layout_images/2309.17421v2_325.jpg", "bbox": [122, 234, 493, 468], "page_size": [612.0, 792.0], "ocr_text": "Recycle Bin\n\ney Vee\n\nthis afternoon?\n\nReply\n\nHQC@QadImv2G>veorst A MG Fw Tries\n\n", "vlm_text": "The image depicts a computer desktop with a background showing an aerial view of a bridge crossing over a body of water with land on either side. The desktop has several application icons on the left, including YouTube Music, Recycle Bin, Tabby Terminal, Google Chrome, Slack, Discord, and Adobe Acrobat Reader. There is a taskbar at the bottom with several applications and a notification from \"Kevin Lin\" asking, \"Do you have time for a quick chat this afternoon?\" The time displayed on the taskbar is 11:26 AM on 7/21/2023."}
{"layout": 1961, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 143, "bbox": [122, 481, 159, 491], "page_size": [612.0, 792.0]}
{"layout": 1962, "type": "text", "text": "The image on the computer screen is a desktop background of a beautiful aerial view of a bridge over the  ocean. There are several icons on the left side of the screen, including the Recycle Bin, Terminal, and a few  others. On the bottom right corner, there is a notification from a messaging app from a user named Kevin Lin  asking if I am free for a chat this afternoon. ", "page_idx": 143, "bbox": [122, 493.123779296875, 482, 528.36962890625], "page_size": [612.0, 792.0]}
{"layout": 1963, "type": "text", "text": " My next move would be to move the mouse to the bottom right corner of the screen and click on the notification  from Kevin Lin to open the messaging app and respond to his message. I would move the mouse to the exact  location of the notification, which is approximately 2 inches from the right edge of the screen and 1 inch from the  bottom edge of the screen.  ", "page_idx": 143, "bbox": [122, 537.9545288085938, 493, 573.200439453125], "page_size": [612.0, 792.0]}
{"layout": 1964, "type": "text", "text": "Figure 114: Prompting GPT-4V to predict the action upon receiving a notification. GPT-4V can accurately recognize the notification and the corresponding content (highlighted in  green  ).  Blue highlights the predicted actions. Check Section  9.8  for detailed discussions. ", "page_idx": 143, "bbox": [108, 588.8675537109375, 503, 626.6654663085938], "page_size": [612.0, 792.0]}
{"layout": 1965, "type": "text", "text": "Sec. 9.8 GUI Navigation ", "text_level": 1, "page_idx": 144, "bbox": [139, 48, 281, 65], "page_size": [612.0, 792.0]}
{"layout": 1966, "type": "text", "text": "Watching Videos ", "text_level": 1, "page_idx": 144, "bbox": [121, 72, 200, 83], "page_size": [612.0, 792.0]}
{"layout": 1967, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 144, "bbox": [121, 88, 160, 98], "page_size": [612.0, 792.0]}
{"layout": 1968, "type": "text", "text": "Explain the story shown in the images below. ", "page_idx": 144, "bbox": [122, 99.30200958251953, 270.9519958496094, 107.6494140625], "page_size": [612.0, 792.0]}
{"layout": 1969, "type": "image", "page_idx": 144, "img_path": "layout_images/2309.17421v2_326.jpg", "bbox": [123, 109, 450, 278], "page_size": [612.0, 792.0], "ocr_text": "700 700 i” & 700 va\nILuON SS BILLION AND A BILLION ANDA\n\nHFRILLION 300 TRILLION 300\nMILLION\nvob |ON Vv\n\nrake tS in\n\na\n@\n\n", "vlm_text": "The image is a collage of four screenshots from a video reel. \n\n1. The first frame shows a person sitting on a bench with a dog, and the text overlay reads, \"When someone asks me how much I have spent on my dog.\"\n2. The other three frames feature a close-up of a person at a podium, with text overlays in each frame reading \"700 BILLION,\" \"700 BILLION AND A TRILLION 300,\" and \"700 BILLION AND A TRILLION 300 MILLION.\"\n\nThe reel seems to humorously compare the exaggerated amount spent on a dog to large financial figures."}
{"layout": 1970, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 144, "bbox": [121, 287, 159, 297], "page_size": [612.0, 792.0]}
{"layout": 1971, "type": "text", "text": "The images show a reel on Instagram, where a user is sharing a humorous post about how much they have  spent on their dog. The first image shows a man holding his dog with the caption \"When someone asks me how  much I have spent on my dog\". The next three images show a clip of Joe Biden saying \"700 billion\" followed by  \"700 billion and a trillion 300 million\" and \"700 billion and a trillion 300 million dollars\". The user is jokingly  exaggerating the amount they have spent on their dog by using the clip of Joe Biden saying a large sum of  money.   ", "page_idx": 144, "bbox": [122, 299.12432861328125, 492, 352.302490234375], "page_size": [612.0, 792.0]}
{"layout": 1972, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 144, "bbox": [121, 363, 159, 373], "page_size": [612.0, 792.0]}
{"layout": 1973, "type": "text", "text": "Tell me about the video content, given the frames below. ", "page_idx": 144, "bbox": [122, 374.69122314453125, 308.303466796875, 383.0386047363281], "page_size": [612.0, 792.0]}
{"layout": 1974, "type": "image", "page_idx": 144, "img_path": "layout_images/2309.17421v2_327.jpg", "bbox": [123, 387, 500, 515], "page_size": [612.0, 792.0], "ocr_text": "3-Fiahting aggressively\n\nforthe bi\n\n", "vlm_text": "This image is a series of six Instagram story screenshots. Each frame humorously highlights activities labeled as \"Things Asian people do for no reason.\" The activities depicted are:\n\n1. Grabbing products from the back of the shelf at a store.\n2. Rinsing a perfectly clean cup before using it.\n3. Asking \"have you eaten yet?\" as a form of greeting.\n4. Fighting aggressively for the bill.\n5. Wrapping pants around the neck to see if they fit.\n\nEach image features individuals performing these actions in various everyday settings."}
{"layout": 1975, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 144, "bbox": [122, 525, 159, 535], "page_size": [612.0, 792.0]}
{"layout": 1976, "type": "text", "text": "The video content is a compilation of things that Asian people do for no reason. The video is posted on TikTok by  user laparasian and has 1.3M views and 13.1K likes. The video is set to original audio by soph relabel o us and  has the hashtag #asian and #relatable. ", "page_idx": 144, "bbox": [122, 537.4570922851562, 495.9738464355469, 563.73681640625], "page_size": [612.0, 792.0]}
{"layout": 1977, "type": "text", "text": "The video is broken down into 5 different scenes: ", "page_idx": 144, "bbox": [122, 566.9172973632812, 284.5143127441406, 575.2647094726562], "page_size": [612.0, 792.0]}
{"layout": 1978, "type": "text", "text": "1. The first scene shows a girl grabbing products from the back of the shelf in a supermarket. 2. The second scene shows a girl rinsing a perfectly clean cup before using it. 3. The third scene shows two girls asking \"have you eaten yet?\" as a form of greeting. 4. The fourth scene shows two girls fighting aggressively for the bill in a restaurant. 5. The fifth scene shows a girl wrapping pants around her neck to see if it fits. ", "page_idx": 144, "bbox": [122, 578.4451904296875, 428.4957580566406, 622.6572265625], "page_size": [612.0, 792.0]}
{"layout": 1979, "type": "text", "text": "Overall, the video is a humorous take on some of the quirks and habits that are commonly seen in the Asian  community.  ", "page_idx": 144, "bbox": [122, 625.8377685546875, 480, 643.1513671875], "page_size": [612.0, 792.0]}
{"layout": 1980, "type": "text", "text": "Figure 115: Prompting GPT-4V to watch web videos. We present GPT-4V the screenshot of the video frames following their temporal order in the original video. To save space, we illustrate the frames in a row, where the leftmost one is the first frame. Check Section  9.8  for detailed discussions. Red highlights the inaccurate descriptions about the video. ", "page_idx": 144, "bbox": [107, 657.6165771484375, 505, 703.6184692382812], "page_size": [612.0, 792.0]}
{"layout": 1981, "type": "text", "text": "Sec. 9.8 GUI Navigation ", "text_level": 1, "page_idx": 145, "bbox": [142, 51, 282, 66], "page_size": [612.0, 792.0]}
{"layout": 1982, "type": "text", "text": "Watching Videos ", "text_level": 1, "page_idx": 145, "bbox": [121, 73, 200, 85], "page_size": [612.0, 792.0]}
{"layout": 1983, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 145, "bbox": [121, 90, 159, 99.75], "page_size": [612.0, 792.0]}
{"layout": 1984, "type": "text", "text": "Transcribe the video content, given the frames below. ", "page_idx": 145, "bbox": [122, 101.38849639892578, 298.8965148925781, 109.73590087890625], "page_size": [612.0, 792.0]}
{"layout": 1985, "type": "image", "page_idx": 145, "img_path": "layout_images/2309.17421v2_328.jpg", "bbox": [123, 114, 500, 224], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image contains a series of Instagram posts showcasing various scenic locations in Washington State. Each post features a different landscape, such as mountains, forests, and national parks. The places highlighted in the posts include:\n\n1. A mountain trail\n2. North Cascades National Park\n3. Olympic National Park\n4. Mt Baker Wilderness\n5. Mt Rainier National Park\n6. Mt Pilchuck State Park\n7. Gifford Pinchot National Forest\n\nThe theme of the series is \"7 Places you need to visit in Washington State.\""}
{"layout": 1986, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 145, "bbox": [121, 234, 159, 245], "page_size": [612.0, 792.0]}
{"layout": 1987, "type": "text", "text": "Hi everyone, it's nat y exp lora here and today I'm going to be sharing 7 places you need to visit in Washington  State.  ", "page_idx": 145, "bbox": [122, 246.77342224121094, 491, 264.08697509765625], "page_size": [612.0, 792.0]}
{"layout": 1988, "type": "text", "text": "First up, we have North Cascades National Park. This park is filled with beautiful mountain views and hiking  trails. It's a must-visit for any nature lover. Next, we have the Olympic Peninsula. This area is home to the famous Hoh Rainforest and beautiful beaches.  It's a great place to relax and enjoy the natural beauty of Washington. Third on the list is Mt Baker Wilderness. This area is known for its stunning mountain views and hiking trails. It's  a great place to escape the city and enjoy the great outdoors. Next up, we have Mt Rainier National Park. This park is home to the highest mountain in Washington and is a  great place for hiking and sightseeing. Fifth on the list is Alpine Lakes Wilderness. This area is filled with beautiful alpine lakes and hiking trails. It's a  great place to spend a day exploring and enjoying the natural beauty of Washington. Next, we have Mt Pilchuck State Park. This park is known for its beautiful views and hiking trails. It's a great  place to spend a day exploring and enjoying the natural beauty of Washington. Last but not least, we have Gifford Pinchot National Forest. This forest is home to beautiful mountain views and  hiking trails. It's a great place to escape the city and enjoy the great outdoors. And that concludes my list of 7 places you need to visit in Washington State. I hope you enjoyed it and are  inspired to explore more of this beautiful state.  ", "page_idx": 145, "bbox": [122, 267.26751708984375, 491, 428.03961181640625], "page_size": [612.0, 792.0]}
{"layout": 1989, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 145, "bbox": [122, 443, 159, 451.75], "page_size": [612.0, 792.0]}
{"layout": 1990, "type": "text", "text": "Tell me about the video content, given the frames below. ", "page_idx": 145, "bbox": [122, 453.6304931640625, 308.303466796875, 461.9778747558594], "page_size": [612.0, 792.0]}
{"layout": 1991, "type": "image", "page_idx": 145, "img_path": "layout_images/2309.17421v2_329.jpg", "bbox": [123, 467, 247, 595], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image is a humorous comparison of a dog involved in \"work from home\" activities. \n\nIn the left side image, a dog is comfortably sleeping in bed under the caption \"WFH Employees: 8:59 AM.\"\n\nIn the right side image, the same dog is sitting in front of a computer showing a video call, under the caption \"WFH Employees: 9:00 AM.\"\n\nThis plays on the joke that remote employees might be in bed until just before their workday starts."}
{"layout": 1992, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 145, "bbox": [122, 598, 159, 608], "page_size": [612.0, 792.0]}
{"layout": 1993, "type": "text", "text": "The video content is about a corgi dog named Tofu who is working from home. The first frame shows Tofu  sleeping on a bed with the caption \"WFH Employees: 8:59 AM\". The second frame shows Tofu sitting in front of  a computer with multiple screens, with the caption \"WFH Employees: 9:00 AM\". The video is likely a humorous  take on the work from home experience, highlighting how Tofu is just like any other employee who struggles to  get out of bed in the morning but is ready to work once they're in front of their computer.  ", "page_idx": 145, "bbox": [122, 609.991943359375, 491, 654.2039794921875], "page_size": [612.0, 792.0]}
{"layout": 1994, "type": "text", "text": "Figure 116: Watching web videos. We present GPT-4V the screenshot of the video frames following their temporal order in the original video. To save space, we illustrate the frames in a row, where the leftmost one is the first frame. Check Section  9.8  for detailed discussions. ", "page_idx": 145, "bbox": [108, 667.7085571289062, 503, 701.531494140625], "page_size": [612.0, 792.0]}
{"layout": 1995, "type": "text", "text": "Sec. 9.8 GUI Navigation ", "text_level": 1, "page_idx": 146, "bbox": [145, 103, 280, 117], "page_size": [612.0, 792.0]}
{"layout": 1996, "type": "text", "text": "Watching Videos ", "text_level": 1, "page_idx": 146, "bbox": [121, 125, 201, 137], "page_size": [612.0, 792.0]}
{"layout": 1997, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 146, "bbox": [121, 142, 159, 151.75], "page_size": [612.0, 792.0]}
{"layout": 1998, "type": "text", "text": "Tell me about the video content, given the frames below. Please include as many details as possible. ", "page_idx": 146, "bbox": [122, 153.4239959716797, 454.4068603515625, 161.77139282226562], "page_size": [612.0, 792.0]}
{"layout": 1999, "type": "image", "page_idx": 146, "img_path": "layout_images/2309.17421v2_330.jpg", "bbox": [122, 164, 486, 466], "page_size": [612.0, 792.0], "ocr_text": "Three Easy Breakfast Ideas\n\nFor BLW, Toddlers & Big Kids\n\n1. Raspberry Biss Balls\n\nGp memento\n\nSAVE een treat so\n\n", "vlm_text": "The image is a collage of screenshots from a video tutorial on making \"Raspberry Bliss Balls\" as part of \"Three Easy Breakfast Ideas For BLW, Toddlers & Big Kids\" by the Instagram user \"mealtimewithmummy.\"\n\n1. The first image shows raspberries in a bowl being mashed with a fork.\n2. The second image shows the raspberries being transferred into another bowl.\n3. The third image has additional ingredients like oats being added to the raspberries.\n4. The fourth image shows a bowl with chia seeds, oats, and possibly nut butter added to the mixture.\n5. The fifth image displays the ingredients being mixed together with a spoon.\n6. The sixth image shows the mixture being combined into a dough-like consistency.\n7. The seventh image shows a plate with shredded coconut in preparation for coating.\n8. The eighth image depicts the mixture being shaped into balls and coated with shredded coconut on the plate.\n9. The ninth image shows the finished raspberry bliss balls arranged on a plate.\n10. The tenth image displays a hand holding a bowl filled with the completed raspberry bliss balls.\n\nThe overall process highlights a simple, no-bake recipe suitable for children and those practicing baby-led weaning (BLW)."}
{"layout": 2000, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 146, "bbox": [122, 472, 160, 482.75], "page_size": [612.0, 792.0]}
{"layout": 2001, "type": "text", "text": "The video content is a tutorial on how to make Raspberry Bliss Balls, a healthy breakfast option for toddlers and  big kids. The video is posted by the Instagram account \"mealtime with mummy\" and has over 13k likes. ", "page_idx": 146, "bbox": [122, 484.5018310546875, 493, 501.81536865234375], "page_size": [612.0, 792.0]}
{"layout": 2002, "type": "text", "text": "The video starts with a bowl of fresh raspberries and a fork, which is then mashed into a puree. The next frame  shows a larger mixing bowl with oats, chia seeds, and peanut butter being added to the raspberry puree. The  mixture is then stirred together until well combined. ", "page_idx": 146, "bbox": [122, 511.4002990722656, 489.9054260253906, 537.6799926757812], "page_size": [612.0, 792.0]}
{"layout": 2003, "type": "text", "text": "The following frame shows a plate with shredded coconut on it, and the raspberry mixture being rolled into small  balls and coated in the coconut. The final frames show the finished Raspberry Bliss Balls being placed in a bowl  and ready to eat. ", "page_idx": 146, "bbox": [122, 547.264892578125, 493, 573.5446166992188], "page_size": [612.0, 792.0]}
{"layout": 2004, "type": "text", "text": "Throughout the video, there are text overlays with instructions and tips, making it easy for viewers to follow  along and make the recipe themselves.  ", "page_idx": 146, "bbox": [122, 583.1295776367188, 475.9256896972656, 600.443115234375], "page_size": [612.0, 792.0]}
{"layout": 2005, "type": "text", "text": "Figure 117: Watching web videos. We present GPT-4V the screenshot of the video frames following their temporal order in the original video. To save space, we illustrate frames 1-5 in the first row, and frames 6-9 in the second row. Check Section  9.8  for detailed discussions. ", "page_idx": 146, "bbox": [108, 615.6725463867188, 504, 649.4954833984375], "page_size": [612.0, 792.0]}
{"layout": 2006, "type": "text", "text": "Sec. 9.8 GUI Navigation ", "text_level": 1, "page_idx": 147, "bbox": [145, 100, 280, 114], "page_size": [612.0, 792.0]}
{"layout": 2007, "type": "text", "text": "Watching Videos ", "text_level": 1, "page_idx": 147, "bbox": [121, 121, 201, 133], "page_size": [612.0, 792.0]}
{"layout": 2008, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 147, "bbox": [121, 138, 159, 147.75], "page_size": [612.0, 792.0]}
{"layout": 2009, "type": "text", "text": "Tell me about the video content, given the frames below. Please include as many details as possible. ", "page_idx": 147, "bbox": [122, 149.4204864501953, 454.4068603515625, 157.76788330078125], "page_size": [612.0, 792.0]}
{"layout": 2010, "type": "image", "page_idx": 147, "img_path": "layout_images/2309.17421v2_331.jpg", "bbox": [121, 159, 489, 461], "page_size": [612.0, 792.0], "ocr_text": "2. Apple & Banana Pancakes 2. Apple & Banana Pancakes 2. Apple & Banana Pancakes\n\n-\n\n@ ~ gieee v ‘ (arden Yes\n\nsso 044 ‘IVE ere et no 4\n\n", "vlm_text": "This image sequence shows the process of making apple and banana pancakes. Here's a breakdown:\n\n1. A bowl with a whole apple and mashed bananas.\n2. Mixing the banana and apple together.\n3. Adding an egg to the mixture.\n4. Mixing in the egg.\n5. Adding oats to the mixture.\n6. Mixing everything together thoroughly.\n7. Cooking the mixture in a skillet to form pancakes.\n8. Serving the pancakes on a plate with sliced fruits and yogurt on the side."}
{"layout": 2011, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 147, "bbox": [122, 468, 159, 479], "page_size": [612.0, 792.0]}
{"layout": 2012, "type": "text", "text": "The video content is a recipe tutorial for apple and banana pancakes. The video is posted on Instagram Reels  by the user \"mealtime with mummy\" and has 13.2K views.  ", "page_idx": 147, "bbox": [122, 480.4983215332031, 486, 497.8118591308594], "page_size": [612.0, 792.0]}
{"layout": 2013, "type": "text", "text": "In the first frame, we see a glass bowl with a mashed banana and an apple being added to it. The second frame  shows the banana and apple being mixed together. The third frame shows an egg being added to the mixture.  The fourth frame shows the mixture being stirred with a fork.  ", "page_idx": 147, "bbox": [122, 507.3968200683594, 495, 533.676513671875], "page_size": [612.0, 792.0]}
{"layout": 2014, "type": "text", "text": "The fifth frame shows a bowl of oats being added to the mixture. The sixth frame shows the oats being mixed in  with a spoon. The seventh frame shows the mixture being spooned into a frying pan. The eighth frame shows  the pancakes being cooked in the frying pan.  ", "page_idx": 147, "bbox": [122, 543.2614135742188, 495, 569.5411376953125], "page_size": [612.0, 792.0]}
{"layout": 2015, "type": "text", "text": "The final frame shows the finished pancakes on a plate, served with sliced apples, bananas, and yogurt. The  caption on the video reads \"SAVE for easy breakfast inspo\" and the video is set to the song \"Stayin' Alive\" by the  Bee Gees.  ", "page_idx": 147, "bbox": [122, 579.1260375976562, 495, 605.4058227539062], "page_size": [612.0, 792.0]}
{"layout": 2016, "type": "text", "text": "Figure 118: Watching web videos. We present GPT-4V the screenshot of the video frames following their temporal order in the original video. To save space, we illustrate frames 1-5 in the first row, and frames 6-9 in the second row. Check Section  9.8  for detailed discussions. ", "page_idx": 147, "bbox": [108, 619.675537109375, 504, 653.4984741210938], "page_size": [612.0, 792.0]}
{"layout": 2017, "type": "text", "text": "Sec. 9.8 GUI Navigation ", "text_level": 1, "page_idx": 148, "bbox": [145, 100, 280, 113], "page_size": [612.0, 792.0]}
{"layout": 2018, "type": "text", "text": "Watching Videos ", "text_level": 1, "page_idx": 148, "bbox": [121, 121, 200, 132], "page_size": [612.0, 792.0]}
{"layout": 2019, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 148, "bbox": [121, 138, 159, 147], "page_size": [612.0, 792.0]}
{"layout": 2020, "type": "text", "text": "Tell me about the video content, given the 9 frames below. Please include as many details as possible. ", "page_idx": 148, "bbox": [122, 148.84288024902344, 460.630859375, 157.19027709960938], "page_size": [612.0, 792.0]}
{"layout": 2021, "type": "image", "page_idx": 148, "img_path": "layout_images/2309.17421v2_332.jpg", "bbox": [121, 161, 489, 463], "page_size": [612.0, 792.0], "ocr_text": "aT\n\nFor more easy recipes follow\n} Gmealtimewithmummy ,\\\n\n“Fotow\n\n‘hve ores bestot no 4\n\n", "vlm_text": "The image is a series of screenshots from an Instagram Reel by the user @mealtimewithmummy, demonstrating a recipe for \"Strawberry Stuffed French Toast.\" The sequence shows the following steps:\n\n1. Mixing strawberries with cream in a bowl.\n2. Spreading the mixture on a slice of bread.\n3. Placing another slice of bread on top.\n4. Using a fork to seal the edges of the sandwich.\n5. Dipping the stuffed sandwich in a beaten egg mixture.\n6. Cooking the dipped sandwich in a frying pan until golden brown.\n7. Displaying the final cooked Strawberry Stuffed French Toast along with a suggestion to follow @mealtimewithmummy for more recipes.\n\nThe images showcase a casual cooking process and emphasize ease of preparation for breakfast recipes."}
{"layout": 2022, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 148, "bbox": [122, 468, 159, 477.75], "page_size": [612.0, 792.0]}
{"layout": 2023, "type": "text", "text": "The video content is a recipe tutorial for strawberry stuffed French toast. The video is posted on Instagram Reels  by the user \"mealtime with mummy\". The video has 13.2K likes and 190 comments. ", "page_idx": 148, "bbox": [122, 479.92071533203125, 496, 497.2342529296875], "page_size": [612.0, 792.0]}
{"layout": 2024, "type": "text", "text": "In the first frame, we see a white plate with a dollop of red sauce and a dollop of white cream. The second frame  shows the plate with the red sauce and white cream mixed together. The third frame shows a slice of bread with  a knife spreading a mixture onto it.  ", "page_idx": 148, "bbox": [122, 501.6956787109375, 496, 527.9754028320312], "page_size": [612.0, 792.0]}
{"layout": 2025, "type": "text", "text": "The fourth frame shows a plate with a circular piece of bread on it. The fifth frame shows a hand using a knife to  spread a pink mixture onto the bread. The sixth frame shows the bread with the pink mixture covered with  another piece of bread, making a sandwich. ", "page_idx": 148, "bbox": [122, 532.436767578125, 496, 558.7164916992188], "page_size": [612.0, 792.0]}
{"layout": 2026, "type": "text", "text": "The seventh frame shows a hand dipping the sandwich into a bowl of beaten eggs. The eighth frame shows the  sandwich being fried in a pan. The final frame shows the finished strawberry stuffed French toast on a plate. ", "page_idx": 148, "bbox": [122, 563.1778564453125, 496, 580.491455078125], "page_size": [612.0, 792.0]}
{"layout": 2027, "type": "text", "text": "The video is set to the song \"Stayin' Alive\" by the Bee Gees. The caption reads \"SAVE for easy breakfast inspo \".  ", "page_idx": 148, "bbox": [122, 584.9528198242188, 496, 593.3002319335938], "page_size": [612.0, 792.0]}
{"layout": 2028, "type": "text", "text": "Figure 119: Watching web videos. We present GPT-4V the screenshot of the video frames following their temporal order in the original video. To save space, we illustrate frames 1-5 in the first row, and frames 6-9 in the second row.  Red  highlights the inaccurate descriptions about the video. Check Section  9.8  for detailed discussions. ", "page_idx": 148, "bbox": [108, 607.3565673828125, 504, 654.0755004882812], "page_size": [612.0, 792.0]}
{"layout": 2029, "type": "text", "text": "10 LMM Powered Agents ", "text_level": 1, "page_idx": 149, "bbox": [107, 72, 249, 84], "page_size": [612.0, 792.0]}
{"layout": 2030, "type": "text", "text": "In this section, we discuss possible future research directions that may further amplify GPT-4V’s capabilities. The discussion focuses on how the intriguing usages in LLMs may extend to the multimodal scenario and its enabled new abilities,  e.g ., multimodal plugins, multimodal chains, self-reflection, self-consistency, and retrieval-augmented LMMs,  etc . In the following sub-sections, we use  human-generated  examples to illustrate potential ways to enhance GPT-4V-based systems. ", "page_idx": 149, "bbox": [107, 98.9715576171875, 505, 154.6124725341797], "page_size": [612.0, 792.0]}
{"layout": 2031, "type": "text", "text": "Sec. 10.1 Multimodal Plugins ", "text_level": 1, "page_idx": 149, "bbox": [124, 171, 314, 192], "page_size": [612.0, 792.0]}
{"layout": 2032, "type": "text", "text": "Prompt: ", "page_idx": 149, "bbox": [122, 198.2327423095703, 160, 208.96510314941406], "page_size": [612.0, 792.0]}
{"layout": 2033, "type": "image", "page_idx": 149, "img_path": "layout_images/2309.17421v2_333.jpg", "img_caption": "GPT-4V: Sorry, I cannot answer this question accurately as no context or information is provided with the photo.  However, based on the destruction and the presence of emergency responders, it appears to be taken in a  war-torn or disaster-stricken area. ", "bbox": [120, 214.25, 476, 426], "page_size": [612.0, 792.0], "ocr_text": "Where is this photo taken?\n\n", "vlm_text": "The image shows a scene of destruction with emergency responders on site. It appears to be taken in a war-torn or disaster-stricken area, but without more specific details, it's not possible to determine the exact location."}
{"layout": 2034, "type": "text", "text": "Prompt: Where is this photo taken? ", "page_idx": 149, "bbox": [122, 442.9845886230469, 211.74627685546875, 462.7430725097656], "page_size": [612.0, 792.0]}
{"layout": 2035, "type": "image", "page_idx": 149, "img_path": "layout_images/2309.17421v2_334.jpg", "bbox": [123, 463.25, 239, 542], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows a scene of disaster recovery. There are rubble and debris from what appears to be a collapsed building. Several people, including some in bright outfits, are on-site, possibly engaged in search and rescue operations. Smoke or dust is also visible, adding to the chaotic atmosphere."}
{"layout": 2036, "type": "text", "text": "Note: Same query image as shown above, resized to  save space ", "page_idx": 149, "bbox": [251, 493.17633056640625, 473, 511.9967041015625], "page_size": [612.0, 792.0]}
{"layout": 2037, "type": "text", "text": "GPT-4V: - I do not have that information. - Enabling Bing Image Search Plugin. ", "page_idx": 149, "bbox": [122.36186981201172, 549.9851684570312, 261.1151428222656, 578.7097778320312], "page_size": [612.0, 792.0]}
{"layout": 2038, "type": "text", "text": "Figure 120: Illustration of using the Bing Image Search [ 94 ] plugin to enable GPT-4V with time- sensitive knowledge (bottom, highlighted in  green  ). Note that the earthquake happened on February 6, 2023, which is after GPT-4V’s training, thereby GPT-4V fails to identify the exact location without plugin (top). Check Section  10.1  for detailed discussions. ", "page_idx": 149, "bbox": [108, 656.258544921875, 505, 702.9774780273438], "page_size": [612.0, 792.0]}
{"layout": 2039, "type": "text", "text": "Sec. 10.2 Multimodal Chains ", "text_level": 1, "page_idx": 150, "bbox": [135, 77, 297, 91], "page_size": [612.0, 792.0]}
{"layout": 2040, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 150, "bbox": [122, 100, 160, 109], "page_size": [612.0, 792.0]}
{"layout": 2041, "type": "text", "text": "Please determine whether each person in the image wears a helmet or not. And summarize how many people  are wearing helmets. ", "page_idx": 150, "bbox": [123, 111.12523651123047, 488.19281005859375, 128.4387969970703], "page_size": [612.0, 792.0]}
{"layout": 2042, "type": "image", "page_idx": 150, "img_path": "layout_images/2309.17421v2_335.jpg", "bbox": [123, 130, 298, 250], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows a group of people wearing uniforms and hard hats, likely indicating they are workers or engineers, possibly in an industrial setting. They are walking together and appear to be in an outdoor location near large pipes or industrial equipment."}
{"layout": 2043, "type": "text", "text": "Thought 1 ", "text_level": 1, "page_idx": 150, "bbox": [121, 259, 164, 269], "page_size": [612.0, 792.0]}
{"layout": 2044, "type": "text", "text": "This question requires detecting people in the image. ", "page_idx": 150, "bbox": [188, 260.4311828613281, 363.96563720703125, 268.778564453125], "page_size": [612.0, 792.0]}
{"layout": 2045, "type": "text", "text": "Action 1 ", "text_level": 1, "page_idx": 150, "bbox": [120, 281, 158, 292], "page_size": [612.0, 792.0]}
{"layout": 2046, "type": "text", "text": "Assistant, detect the people shown in this image <ImagePath> ", "page_idx": 150, "bbox": [188, 282.8465881347656, 395.06439208984375, 291.1939697265625], "page_size": [612.0, 792.0]}
{"layout": 2047, "type": "text", "text": "Person [4, 126, 72, 175] Person [63, 121, 88, 181] Person [30, 107, 56, 88] Person [223, 123, 43, 78] Person [178, 122, 52, 180] Person [322, 118, 41, 68] Person [268, 109, 55, 178] Person [357, 127, 88, 173] ", "page_idx": 150, "bbox": [188, 301.9898986816406, 276, 373.10040283203125], "page_size": [612.0, 792.0]}
{"layout": 2048, "type": "text", "text": "Thought 2 ", "page_idx": 150, "bbox": [122.36186981201172, 380.95135498046875, 163.57334899902344, 390.4912414550781], "page_size": [612.0, 792.0]}
{"layout": 2049, "type": "text", "text": "There are 8 people detected in this image.  ", "page_idx": 150, "bbox": [188, 382.1147766113281, 330.7460632324219, 390.462158203125], "page_size": [612.0, 792.0]}
{"layout": 2050, "type": "text", "text": "Action 2 ", "text_level": 1, "page_idx": 150, "bbox": [120, 401, 163, 415], "page_size": [612.0, 792.0]}
{"layout": 2051, "type": "text", "text": "Assistant, crop the regions in this image specified by the bounding boxes <ImagePath> ", "page_idx": 150, "bbox": [188, 404.5301818847656, 476.4322814941406, 412.8775634765625], "page_size": [612.0, 792.0]}
{"layout": 2052, "type": "text", "text": "Observation 2 ", "text_level": 1, "page_idx": 150, "bbox": [120, 423, 180, 436], "page_size": [612.0, 792.0]}
{"layout": 2053, "type": "image", "page_idx": 150, "img_path": "layout_images/2309.17421v2_336.jpg", "bbox": [189, 433, 491, 524], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows a sequence of panels featuring individuals dressed in work uniforms, holding and wearing yellow safety helmets. The uniforms appear to be work attire, likely for a construction or industrial setting."}
{"layout": 2054, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 150, "bbox": [122, 534, 160, 545], "page_size": [612.0, 792.0]}
{"layout": 2055, "type": "text", "text": "Image 1: Not wearing a helmet Image 2: Not wearing a helmet Image 3: Wearing a helmet Image 4: Wearing a helmet Image 5: Wearing a helmet Image 6: Wearing a helmet Image 7: Wearing a helmet Image 8: Not wearing a helmet Summary: Out of 8 people, 5 are wearing helmets and 3 are not wearing helmets.  ", "page_idx": 150, "bbox": [123, 546.10546875, 395.021240234375, 626.18212890625], "page_size": [612.0, 792.0]}
{"layout": 2056, "type": "text", "text": "Figure 121: Extending GPT-4V to multimodal chains with ReAct [ 145 ,  142 ] for PPE Counting scenario. Check Section  10.2  for detailed discussions. ", "page_idx": 150, "bbox": [108, 642.0645751953125, 504.0011291503906, 664.9794921875], "page_size": [612.0, 792.0]}
{"layout": 2057, "type": "text", "text": "10.1 Multimodal Plugins ", "text_level": 1, "page_idx": 151, "bbox": [107, 74, 221, 84], "page_size": [612.0, 792.0]}
{"layout": 2058, "type": "text", "text": "In the context of LLMs, plugins [ 98 ,  56 ,  6 ,  112 ,  87 ,  103 ] play a crucial role in assisting LLMs for various tasks such as accessing the latest information, performing computations, or utilizing third- party services. These plugins are primarily designed to process inputs in natural language or inputs that can be interpreted as language, such as code and math equations. To illustrate the significance of multimodal plugins, such as Bing Image Search [ 94 ], especially in the context of LMMs, we present Figure  120 . By incorporating the Bing Image Search plugin, we empower GPT-4V to acquire time- sensitive knowledge related to the input image. In the upper part of the figure, we demonstrate the limitations of GPT-4V without Bing Image Search plugin. It fails to accurately answer the question, \"Where was this photo taken?\" due to the fact that the photo captures the aftermath of a massive earthquake that occurred on February 6, 2023, at the border of Turkey and Syria—a situation that took place after GPT-4V’s training. Since constantly retraining the model with current information can be computationally intensive and expensive, plugins like search engines prove to be invaluable resources for the model to access up-to-date information. In the lower part of Figure  120 , we showcase the capabilities of GPT-4V when equipped with the Bing Image Search plugin. It effectively leverages the retrieved information from the plugin, enabling accurate identification of the location  Izmir, Turkey. ", "page_idx": 151, "bbox": [107, 92.73553466796875, 505, 257.468505859375], "page_size": [612.0, 792.0]}
{"layout": 2059, "type": "text", "text": "10.2 Multimodal Chains ", "text_level": 1, "page_idx": 151, "bbox": [108, 270, 219, 281], "page_size": [612.0, 792.0]}
{"layout": 2060, "type": "text", "text": "Chaining with LLMs has been explored extensively in recent research [ 145 ,  47 ,  124 ,  107 ]. This approach goes beyond using a single plugin and instead establishes a system paradigm that integrates LLMs with a pool of plugins, enabling more advanced reasoning and interactions. By replacing language-only plugins with vision/multimodal experts such as image captioners, object detectors, or well-trained models for text-to-image generation and audio-to-text conversion, it becomes possible to construct a powerful multimodal chain with LLMs [ 137 ,  142 ,  121 ,  114 ,  75 ,  86 ]. ", "page_idx": 151, "bbox": [107, 289.9095458984375, 505, 356.46051025390625], "page_size": [612.0, 792.0]}
{"layout": 2061, "type": "text", "text": "However, the interactions within these chains between LLMs and the plugins typically take place in text format. Although the plugins may accept multimodal inputs, they return results in text to enhance the knowledge of LLMs. There is a notable exception in the case of image synthesis/editing [ 137 ], where the plugins can generate images, but these images are not fed back into LLMs for further analysis or knowledge augmentation, as LLMs can only process language-based inputs. ", "page_idx": 151, "bbox": [107, 360.84356689453125, 505, 416.4855041503906], "page_size": [612.0, 792.0]}
{"layout": 2062, "type": "text", "text": "In Figure  121 , we present an illustration of how GPT-4V, can be extended to support multimodal chains with ReAct [ 145 ,  142 ]. This extension enables the plugins in the chain to provide multimodal information, which can then be collectively processed by GPT-4V to achieve advanced reasoning in scenarios such as PPE counting. The entire chaining process shown in Figure  121  is divided into two rounds of thought, action, and observation, with each round involving the activation of a specific plugin. In the first round, GPT-4V deduces that person detection is necessary to count the number of people wearing helmets (Thought 1). Consequently, it calls the person detector tool (Action 1) and receives the coordinates of bounding boxes for each detected person in the image (Observation 1). Moving to the second round, based on the obtained bounding box information, GPT-4V infers that there are a total of 8 people in the image (Thought 2). It then utilizes the image cropping tool to crop out individual images of each person according to their corresponding bounding box coordinates (Action 2). The resulting outputs (Observation 2) consist of 8 labeled images, numbered from image 1 to image 8. GPT-4V subsequently determines whether each person in these images is wearing a helmet or not, and summarizes the total count of people wearing helmets. ", "page_idx": 151, "bbox": [107, 420.8685607910156, 505, 574.6914672851562], "page_size": [612.0, 792.0]}
{"layout": 2063, "type": "text", "text": "Overall, this integration of LMMs with a pool of multimodal plugins opens up new possibilities for enhanced reasoning and interaction, leveraging the strengths of both language and vision capabilities. The flexibility of multimodal chains allows for a more comprehensive understanding and analysis of multimodal data, and can potentially lead to improved performance in various applications. ", "page_idx": 151, "bbox": [107, 579.0755615234375, 505, 623.8074951171875], "page_size": [612.0, 792.0]}
{"layout": 2064, "type": "text", "text": "10.3 Self-Reflection ", "text_level": 1, "page_idx": 151, "bbox": [107, 636, 198, 647], "page_size": [612.0, 792.0]}
{"layout": 2065, "type": "text", "text": "Figure  122  demonstrates the application of self-reflection [ 116 ,  88 ,  63 ] to improve the results shown in Figure  47 . As we can see, the self-reflected result is better aligned with the reference image. For example, on the left side, the number of data points is corrected from 4 to 3, while on the right side, the percentage is added back above the bar. Although the result is still not exactly identical, it is evident that self-reflection can facilitate manual polishing. Figure  123  shows another example of self-reflection in improving the prompt generation for text-to-image models [ 106 ]. ", "page_idx": 151, "bbox": [107, 656.2495727539062, 505, 722.7994995117188], "page_size": [612.0, 792.0]}
{"layout": 2066, "type": "text", "text": "Sec. 10.3 Self-Reflection ", "text_level": 1, "page_idx": 152, "bbox": [144, 77, 279, 91], "page_size": [612.0, 792.0]}
{"layout": 2067, "type": "text", "text": "Self-Reflection to Improve Coding ", "text_level": 1, "page_idx": 152, "bbox": [117, 98, 275, 110], "page_size": [612.0, 792.0]}
{"layout": 2068, "type": "text", "text": "Prompt: Generate python code to draw similar curves. ", "page_idx": 152, "bbox": [118, 114.56059265136719, 268.3841247558594, 134.31907653808594], "page_size": [612.0, 792.0]}
{"layout": 2069, "type": "image", "page_idx": 152, "img_path": "layout_images/2309.17421v2_337.jpg", "img_caption": "GPT-4V: ", "bbox": [117, 139, 284, 283], "page_size": [612.0, 792.0], "ocr_text": "CIDEr\n\n140\n\n80\n\n—— Base\n< Large\n—@ Huge\n\n10! 10° 10\nmillion images in pre-training\n", "vlm_text": "The image is a line graph that compares the performance of three different models—Base, Large, and Huge—based on the number of million images used in pre-training. The y-axis represents the CIDEr score, which is a metric commonly used for evaluating the quality of image captioning. The x-axis is on a logarithmic scale and represents the number of million images used in pre-training.\n\nKey observations from the graph:\n- Each model shows an increase in CIDEr score as the number of pre-training images increases.\n- The Huge model performs the best, achieving the highest CIDEr scores at each data point.\n- The Base model starts with a lower score at the smallest dataset size and shows steady improvement, though it remains the lowest-performing among the three.\n- The Large model is in the middle, outperforming the Base model but not reaching the scores of the Huge model.\n\nAdditionally, each model is represented with different markers for their lines: stars for Base, crosses for Large, and circles for Huge."}
{"layout": 2070, "type": "image", "page_idx": 152, "img_path": "layout_images/2309.17421v2_338.jpg", "img_caption": "GPT-4V: ", "bbox": [117, 289, 284, 500], "page_size": [612.0, 792.0], "ocr_text": "160\n\n150\n\nBase\n“© Lage\n-e Huge\n\n10°\n\n10! 10?\nmillion images in pre-training\n\n10?\n\n", "vlm_text": "The image is a line graph that visualizes the relationship between the number of millions of images used in pre-training and the CIDEr score, which measures the quality of image captioning. The graph includes three lines, each representing a different model size: Base, Large, and Huge. \n\n- The x-axis is labeled \"million images in pre-training\" and uses a logarithmic scale ranging approximately from 10^0 (1) to 10^3 (1000).\n- The y-axis is labeled \"CIDEr\" and ranges from 80 to 160.\n- The blue line with x markers represents the Base model, the orange line with circular markers represents the Large model, and the green line with triangular markers represents the Huge model. \n\nThe lines demonstrate a positive trend, indicating that as the number of images used in pre-training increases, so does the CIDEr score, with the Huge model achieving the highest scores across the range."}
{"layout": 2071, "type": "image", "page_idx": 152, "img_path": "layout_images/2309.17421v2_339.jpg", "bbox": [119, 504, 279, 628], "page_size": [612.0, 792.0], "ocr_text": "160\n\n150\n\n80\n\nSe Base\n= Large\n-e Huge\n\n10?\n\n10?\nmillion images in pre-training\n\n", "vlm_text": "The image is a line graph showing the relationship between the number of million images in pre-training (on the x-axis, logarithmic scale) and the CIDEr score (on the y-axis). It has three lines representing different models or configurations: \n\n- Blue line with \"Base\"\n- Yellow line with \"Large\"\n- Green line with \"Huge\"\n\nAll lines show an upward trend, indicating that as the number of pre-training images increases, the CIDEr score also increases."}
{"layout": 2072, "type": "image", "page_idx": 152, "img_path": "layout_images/2309.17421v2_340.jpg", "img_caption": "Prompt: ", "bbox": [318, 115, 486, 218], "page_size": [612.0, 792.0], "ocr_text": "100%\n\n100% ogo\n\n100%\n\nMm OSCARS\nEE MiniVLM (ours)\n\n100% 494\n\nCaption\n\ni\nVOA\n\nTR\n\nNLVR2\n\n", "vlm_text": "The image is a bar chart comparing the performance of two models, OSCAR_B and MiniVLM (ours), across five different tasks: Caption, VQA, TR, IR, and NLVR2. The y-axis represents a performance metric, while the x-axis lists the tasks. Each task has two bars: blue for OSCAR_B and red for MiniVLM. In the chart:\n\n- For the Caption task, OSCAR_B scores 100% while MiniVLM scores 97%.\n- For the VQA task, OSCAR_B scores 100% while MiniVLM scores 94%.\n- For the TR task, OSCAR_B scores 100% while MiniVLM scores 96%.\n- For the IR task, OSCAR_B scores 100% while MiniVLM scores 95%.\n- For the NLVR2 task, OSCAR_B scores 100% while MiniVLM scores 94%.\n\nOverall, OSCAR_B outperforms MiniVLM, achieving 100% in all tasks compared to the slightly lower scores of MiniVLM."}
{"layout": 2073, "type": "image", "page_idx": 152, "img_path": "layout_images/2309.17421v2_341.jpg", "img_caption": "GPT-4V: ", "bbox": [318, 251, 486, 405], "page_size": [612.0, 792.0], "ocr_text": "Score\n\n20\n\n100\n\n80\n\n60\n\n20\n\nComparison of OSCarB and MiniVLM\n\nmm OSCarB\nSE MiniVLM (ours)\n\nCaption\n\nNwR2\n\n", "vlm_text": "The image is a bar graph comparing the performance of OSCarB and MiniVLM across different tasks: Caption, VQA, TR, IR, and NLVR2. The y-axis represents the score, ranging from 0 to 140. OSCarB is shown in blue, and MiniVLM is in orange. OSCarB scores higher in the Caption task, while the two models have similar scores in the other tasks."}
{"layout": 2074, "type": "text", "text": "Prompt:  Can you double-check the code and align it better  with the reference image? ", "page_idx": 152, "bbox": [319, 417.8524169921875, 485.55743408203125, 446.5770568847656], "page_size": [612.0, 792.0]}
{"layout": 2075, "type": "text", "text": "Note: Code from the above GPT-4V  response is omitted to save space ", "page_idx": 152, "bbox": [319, 455.0966491699219, 467, 473.9169921875], "page_size": [612.0, 792.0]}
{"layout": 2076, "type": "image", "page_idx": 152, "img_path": "layout_images/2309.17421v2_342.jpg", "img_caption": "GPT-4V: ", "bbox": [319, 484, 486, 623], "page_size": [612.0, 792.0], "ocr_text": "120\n\n100\n\n20\n\nComparison of OSCarB and MiniVLM\n\n100%\n\n100%\n\n100%\n\nim OSCarB\n|E_MiniVLM (ours)\n100%\n\n", "vlm_text": "The image is a bar chart comparing the performance of two models: OSCARb and MiniVLM, across five different tasks. The tasks are Caption, VQA, TR, IR, and NLVR2. The scores for MiniVLM are represented by teal bars, while the scores for OSCARb are represented by red bars. The exact scores are listed at the top of each bar for both models.\n\nHere's a breakdown of the scores for each task:\n\n1. **Caption**: \n   - OSCARb: 120%\n   - MiniVLM: 100%\n\n2. **VQA** (Visual Question Answering):\n   - OSCARb: 100%\n   - MiniVLM: 100%\n\n3. **TR** (Text Retrieval):\n   - OSCARb: 100%\n   - MiniVLM: 94%\n\n4. **IR** (Image Retrieval):\n   - OSCARb: 100%\n   - MiniVLM: 95%\n\n5. **NLVR2** (Natural Language for Visual Reasoning):\n   - OSCARb: 90%\n   - MiniVLM: 94%\n\nThe chart indicates that OscarB performs better in the Caption task, with a 20% higher score than MiniVLM. Both models perform equally well in the VQA task. However, MiniVLM outperforms OscarB in the NLVR2 task. For TR and IR tasks, OSCARb has a slight advantage over MiniVLM."}
{"layout": 2077, "type": "text", "text": "Figure 122: Illustration of using self-reflection to improve the code for figure drawing. Left: after reflection, the number of points in the curve aligns with the reference image. Right: the percentage is added to align with the reference image. Check Section  10.3  for detailed discussions. ", "page_idx": 152, "bbox": [108, 637.7955322265625, 504, 671.6184692382812], "page_size": [612.0, 792.0]}
{"layout": 2078, "type": "text", "text": "Self-Reflection to Improve Text-to-Image Generation ", "page_idx": 153, "bbox": [118.12110137939453, 98.38229370117188, 354.9437561035156, 109.09999084472656], "page_size": [612.0, 792.0]}
{"layout": 2079, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 153, "bbox": [116, 120, 154, 129.75], "page_size": [612.0, 792.0]}
{"layout": 2080, "type": "text", "text": "Instruction: Given a user imagined IDEA of the scene,  converting the IDEA into a self-contained sentence  prompt that will be used to generate an image. IDEA: photo of a dog looks like the one in the given  image running on the beach. ", "page_idx": 153, "bbox": [118, 131.05230712890625, 296, 175.203857421875], "page_size": [612.0, 792.0]}
{"layout": 2081, "type": "image", "page_idx": 153, "img_path": "layout_images/2309.17421v2_343.jpg", "img_caption": "SDXL: ", "bbox": [115, 177, 273, 329], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows a pug with large, expressive eyes looking directly at the camera. The background appears to be a paved surface."}
{"layout": 2082, "type": "image", "page_idx": 153, "img_path": "layout_images/2309.17421v2_344.jpg", "bbox": [116, 330, 215, 428.75], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows a dog with blue fur running on a beach, near the water, during what appears to be sunset or sunrise. The dog has a happy expression with its mouth open and tongue out, and it is wearing a collar. The background shows waves in the ocean and a cloudy sky, adding to the dynamic feel of the image. The colors have been edited to give the dog an unnatural blue hue, which is not typical for a real dog."}
{"layout": 2083, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 153, "bbox": [117, 429.25, 154, 439], "page_size": [612.0, 792.0]}
{"layout": 2084, "type": "text", "text": "I am iterative ly improving the sentence prompt by  looking at the images generated by an AI art  generation model and find out what is different from  the given IDEA. The IDEA is: photo of a dog … beach.  Generated sentence prompt for current round is: A  blue dog running on the beach with a happy  expression on its face. Corresponding image  generated by the AI art generation model: ", "page_idx": 153, "bbox": [118, 440.60113525390625, 296, 511.6142272949219], "page_size": [612.0, 792.0]}
{"layout": 2085, "type": "image", "page_idx": 153, "img_path": "layout_images/2309.17421v2_345.jpg", "bbox": [115, 515, 215, 616], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows a dog with blue fur running on a beach. The dog appears to be happy and in motion, with its ears flapping backward. The background includes a glimpse of the ocean with waves and a partly cloudy sky."}
{"layout": 2086, "type": "text", "text": "Prompt Continued ", "text_level": 1, "page_idx": 153, "bbox": [318, 115, 382, 125], "page_size": [612.0, 792.0]}
{"layout": 2087, "type": "text", "text": "The generated image is not perfect. Write key reasons  on how to further improve the image to better follow  the user imagined IDEA of the scene. ", "page_idx": 153, "bbox": [313, 137.40701293945312, 494, 163.65078735351562], "page_size": [612.0, 792.0]}
{"layout": 2088, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 153, "bbox": [312, 164.25, 349, 174], "page_size": [612.0, 792.0]}
{"layout": 2089, "type": "text", "text": "The dog in the generated image does not look like the  one in the given image. The dog in the given image is  a pug, while the dog in the generated image is a  different breed. To address this issue, we can modify  the prompt to specifically mention that the dog should  be a pug. ", "page_idx": 153, "bbox": [313, 175.7808380126953, 494, 228.88629150390625], "page_size": [612.0, 792.0]}
{"layout": 2090, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 153, "bbox": [312, 230, 350, 239], "page_size": [612.0, 792.0]}
{"layout": 2091, "type": "text", "text": "Instruction: Given a user imagined IDEA of the scene,  converting the IDEA into a sentence prompt that will  be used to generate an image. I am iterative ly  improving the sentence prompt by looking at the  images generated by an AI art generation model and  find out what is different from the given IDEA. The  IDEA is: photo of a dog … beach. Generated sentence  prompt for current round is: A blue … its face.  Corresponding image generated by the AI art  generation model: ", "page_idx": 153, "bbox": [313, 241.01637268066406, 494, 329.9374694824219], "page_size": [612.0, 792.0]}
{"layout": 2092, "type": "image", "page_idx": 153, "img_path": "layout_images/2309.17421v2_346.jpg", "bbox": [313, 330.25, 412, 428.75], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows a dog with blue fur running on a beach. The dog appears to be joyful and is wearing a collar. The background features a sandy shore and an ocean under a cloudy sky, likely during a sunset or sunrise. The blue coloring of the dog suggests that the image might be digitally altered or artistically edited."}
{"layout": 2093, "type": "text", "text": "The image can be improved because: The dog …  should be a pug. Based on the above information, to  improve the image, I wrote a detailed prompt exactly  about the IDEA follow the rules. ", "page_idx": 153, "bbox": [313, 429.04803466796875, 494, 464.2456970214844], "page_size": [612.0, 792.0]}
{"layout": 2094, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 153, "bbox": [312, 465.25, 350, 474], "page_size": [612.0, 792.0]}
{"layout": 2095, "type": "text", "text": "A blue pug running on the beach with a happy  expression on its face, with a sunset and palm trees in  the background. ", "page_idx": 153, "bbox": [313, 476.375732421875, 494, 502.6194763183594], "page_size": [612.0, 792.0]}
{"layout": 2096, "type": "text", "text": "SDXL: ", "text_level": 1, "page_idx": 153, "bbox": [312, 503.25, 342, 513], "page_size": [612.0, 792.0]}
{"layout": 2097, "type": "image", "page_idx": 153, "img_path": "layout_images/2309.17421v2_347.jpg", "bbox": [313, 515, 412, 616], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image depicts a happy pug running on a beach with palm trees in the background during sunset. The pug appears animated or digitally illustrated, and the scene conveys a cheerful and relaxed atmosphere."}
{"layout": 2098, "type": "text", "text": "Figure 123: Illustration of using self-reflection to improve the generated text prompts for a text-to- image model SDXL [ 106 ]. GPT-4V reflects the error in the initial prompt that it does not mention the dog’s breed, and makes the correct revision. Check Section  10.3  for detailed discussions. ", "page_idx": 153, "bbox": [108, 627.965576171875, 505, 661.7885131835938], "page_size": [612.0, 792.0]}
{"layout": 2099, "type": "text", "text": "Sec. 10.4 Self-Consistency ", "text_level": 1, "page_idx": 154, "bbox": [118, 43, 314, 65], "page_size": [612.0, 792.0]}
{"layout": 2100, "type": "text", "text": "Self-Consistency for Counting ", "page_idx": 154, "bbox": [118.10762023925781, 68.45171356201172, 255.83798217773438, 79.1551284790039], "page_size": [612.0, 792.0]}
{"layout": 2101, "type": "text", "text": "Sample 1 ", "text_level": 1, "page_idx": 154, "bbox": [117, 85, 160, 95], "page_size": [612.0, 792.0]}
{"layout": 2102, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 154, "bbox": [117, 96, 154, 106], "page_size": [612.0, 792.0]}
{"layout": 2103, "type": "text", "text": "Count the number of boats in the image. ", "page_idx": 154, "bbox": [118, 107.46531677246094, 250.9766387939453, 115.79019927978516], "page_size": [612.0, 792.0]}
{"layout": 2104, "type": "image", "page_idx": 154, "img_path": "layout_images/2309.17421v2_348.jpg", "bbox": [118, 119, 284, 245], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows a marina at sunset. Several boats are docked, and the sky is filled with dramatic clouds and vibrant colors from the setting sun, creating a picturesque scene. The reflection of the sky on the water adds to the beauty of the image."}
{"layout": 2105, "type": "text", "text": "GPT-4V: There are 6 boats in the image. ", "page_idx": 154, "bbox": [118, 249.3759307861328, 221.58737182617188, 269.08111572265625], "page_size": [612.0, 792.0]}
{"layout": 2106, "type": "text", "text": "Sample 3 ", "text_level": 1, "page_idx": 154, "bbox": [117, 304, 161, 314], "page_size": [612.0, 792.0]}
{"layout": 2107, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 154, "bbox": [116, 316, 155, 325.75], "page_size": [612.0, 792.0]}
{"layout": 2108, "type": "text", "text": "Count the number of boats in the image. Describe  each of their locations. ", "page_idx": 154, "bbox": [118, 327.013671875, 289, 344.280517578125], "page_size": [612.0, 792.0]}
{"layout": 2109, "type": "image", "page_idx": 154, "img_path": "layout_images/2309.17421v2_349.jpg", "bbox": [118, 349, 173, 390], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image depicts a scenic view of a marina during sunset. Several boats are docked, and the sky is filled with vibrant colors including orange, yellow, and blue, reflecting on the water."}
{"layout": 2110, "type": "text", "text": "Note: Same query image  as shown in Sample 1,  resized to save space ", "page_idx": 154, "bbox": [181, 355.3631896972656, 289, 383.0747375488281], "page_size": [612.0, 792.0]}
{"layout": 2111, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 154, "bbox": [117, 398, 154, 408], "page_size": [612.0, 792.0]}
{"layout": 2112, "type": "text", "text": "There are 4 boats in the image. 1) The first boat is on  the left side of the image, docked at the pier. It is a  larger boat with a tower on top. 2) The second boat is  in the center of the image, also docked at the pier. It is  a smaller boat with a white canopy on top. 3) The third  boat is on the right side of the image, docked at the  pier. It is a smaller boat with a white canopy on top,  similar to the second boat. 4) The fourth boat is in the  background, on the far right side of the image. It is  partially obscured by the third boat, but appears to be  a larger boat with a tower on top. ", "page_idx": 154, "bbox": [118, 410.0462341308594, 297, 507.7908630371094], "page_size": [612.0, 792.0]}
{"layout": 2113, "type": "text", "text": "Sample 2 ", "text_level": 1, "page_idx": 154, "bbox": [312, 84, 356, 95], "page_size": [612.0, 792.0]}
{"layout": 2114, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 154, "bbox": [311, 96, 350, 106], "page_size": [612.0, 792.0]}
{"layout": 2115, "type": "text", "text": "Count the number of boats in the image. Describe  each of their locations. ", "page_idx": 154, "bbox": [312, 107.42445373535156, 479, 124.6913070678711], "page_size": [612.0, 792.0]}
{"layout": 2116, "type": "image", "page_idx": 154, "img_path": "layout_images/2309.17421v2_350.jpg", "bbox": [314, 130, 367, 170], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image depicts a marina at sunset. There are several boats docked in the harbor, and the sky is filled with vibrant colors from the setting sun, creating a picturesque and serene scene."}
{"layout": 2117, "type": "text", "text": "Note: Same query image  as shown in Sample 1,  resized to save space ", "page_idx": 154, "bbox": [380, 135.77398681640625, 483, 163.48550415039062], "page_size": [612.0, 792.0]}
{"layout": 2118, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 154, "bbox": [312, 177, 349, 187], "page_size": [612.0, 792.0]}
{"layout": 2119, "type": "text", "text": "There are a total of 4 boats in the image. 1) The first  boat is on the left side of the image, docked at the  pier. It is a larger boat with a tower on top. 2) The  second boat is in the center of the image, also docked  at the pier. It is a smaller boat with a white canopy on  top. 3) The third boat is on the right side of the image,  docked at the pier. It is a smaller boat with a white  canopy on top, similar to the second boat. 4) The  fourth boat is in the background, on the far right side of  the image. It is a sailboat, and it appears to be sailing  on the water. ", "page_idx": 154, "bbox": [312, 189.17958068847656, 494, 286.9242248535156], "page_size": [612.0, 792.0]}
{"layout": 2120, "type": "text", "text": "Sample 4 ", "text_level": 1, "page_idx": 154, "bbox": [311, 304, 356, 314], "page_size": [612.0, 792.0]}
{"layout": 2121, "type": "text", "text": "Prompt: ", "text_level": 1, "page_idx": 154, "bbox": [311, 316, 350, 325.75], "page_size": [612.0, 792.0]}
{"layout": 2122, "type": "text", "text": "Count the number of boats in the image. Describe  each of their locations. ", "page_idx": 154, "bbox": [312, 326.8874206542969, 479, 344.15423583984375], "page_size": [612.0, 792.0]}
{"layout": 2123, "type": "image", "page_idx": 154, "img_path": "layout_images/2309.17421v2_351.jpg", "bbox": [313, 349, 367, 390], "page_size": [612.0, 792.0], "ocr_text": "", "vlm_text": "The image shows a picturesque scene of a marina during sunset. Several boats are docked at the pier, and the sky is filled with dramatic clouds, illuminated by the vibrant colors of the setting sun. The reflection of the sunset and clouds can be seen on the water, enhancing the serene and scenic atmosphere of the image."}
{"layout": 2124, "type": "text", "text": "Note: Same query image  as shown in Sample 1,  resized to save space ", "page_idx": 154, "bbox": [380, 356.17724609375, 483, 383.8887634277344], "page_size": [612.0, 792.0]}
{"layout": 2125, "type": "text", "text": "GPT-4V: ", "text_level": 1, "page_idx": 154, "bbox": [312, 398, 349, 408], "page_size": [612.0, 792.0]}
{"layout": 2126, "type": "text", "text": "There are 6 boats in the image. - 1 boat is on the left  side of the image, docked at the pier with a tower on  top - 2 boats are in the center of the image, docked at  the pier side by side - 1 boat is on the right side of the  image, docked at the pier with a cover on top - 2 boats  are in the background on the right side of the image,  docked at a different pier. ", "page_idx": 154, "bbox": [312, 409.9199523925781, 494, 471.8966979980469], "page_size": [612.0, 792.0]}
{"layout": 2127, "type": "text", "text": "Self Consistency: ", "text_level": 1, "page_idx": 154, "bbox": [116, 531, 198, 541.75], "page_size": [612.0, 792.0]}
{"layout": 2128, "type": "text", "text": "Sample 2:  There are a total of 4 boats in the image. 1) The first boat is on the left side of the image …… Sample 3:  There are 4 boats in the image. 1) The first boat is on the left side of the image, docked  …… Sample 4:  There are 6 boats in the image. - 1 boat is on the left side of the image, docked at the pier …… [Marginal ize out reasoning paths to aggregate final answers, e.g.,  majority vote ]  $=>$  ", "page_idx": 154, "bbox": [118, 542.9076538085938, 466, 578.0584106445312], "page_size": [612.0, 792.0]}
{"layout": 2129, "type": "text", "text": "There are 4 boats in the image. ", "page_idx": 154, "bbox": [118, 578.6754760742188, 221.4804229736328, 587.0003662109375], "page_size": [612.0, 792.0]}
{"layout": 2130, "type": "text", "text": "Figure 124: Improve the counting reliability with self-consistency [ 130 ], which aggregates multiple counting results repeated on the  same  image. Check Section  10.4  for detailed discussions. ", "page_idx": 154, "bbox": [108, 603.1915283203125, 504, 626.1055297851562], "page_size": [612.0, 792.0]}
{"layout": 2131, "type": "text", "text": "10.4 Self-Consistency ", "text_level": 1, "page_idx": 154, "bbox": [108, 644, 206, 655], "page_size": [612.0, 792.0]}
{"layout": 2132, "type": "text", "text": "Self-consistency [ 130 ] is a decoding strategy that aggregates multiple sampled outputs to produce the final answer, such as with the majority vote. Extended from marginal i zing to aggregating final answers, Tree-of-Thoughts [ 144 ] shows that the self-consistency idea can be applied to intermediate thoughts to improve the LLM reasoning performance. Figure  124  illustrates the use of self-consistency on GPT-4V for counting problems. We sample multiple counting results by asking GPT-4V to count the same image multiple times, either conducting multiple runs (Samples 2-4) or rephrasing the input text instruction (Samples 1,2). The example then uses the simple majority vote to aggregate the final answer of “4 boats.” We leave the comprehensive explorations of self-consistency LMMs to future works. ", "page_idx": 154, "bbox": [108, 667.1585693359375, 504, 722.7994995117188], "page_size": [612.0, 792.0]}
{"layout": 2133, "type": "text", "text": "", "page_idx": 155, "bbox": [107, 72.757568359375, 505, 117.48949432373047], "page_size": [612.0, 792.0]}
{"layout": 2134, "type": "text", "text": "10.5 Retrieval-Augmented LMMs ", "text_level": 1, "page_idx": 155, "bbox": [107, 129, 259, 140], "page_size": [612.0, 792.0]}
{"layout": 2135, "type": "text", "text": "Retrieval-Augmented LMMs [ 93 ,  68 ,  50 ,  19 ,  115 ,  104 ] enhances text generation by retrieving and integrating relevant information into prompts. The technique is particularly effective when specialized task-relevant information is needed, such as expert knowledge in a highly-specialized expert domain, the most recent information that may differ from LLMs’ memory, and the customizable information that varies from user to user. We imagine retrieval augmentation continues to play an essential role in LMMs. Figure  74  shows an example of retrieval-augmented LMMs helping grocery checkout. Since the produces’ image-text-price triplets are different in each store, it would be beneficial to retrieve them from the store’s database and yield the correct checkout information. Similarly, in Figure  81 ’s the customized captioning scenario, we imagine the system may automatically retrieve the family members’ photos from the album and achieve the customized captioning. ", "page_idx": 155, "bbox": [107, 149.1175537109375, 505, 259.30450439453125], "page_size": [612.0, 792.0]}
{"layout": 2136, "type": "text", "text": "11 Conclusions ", "text_level": 1, "page_idx": 155, "bbox": [108, 274, 195, 286], "page_size": [612.0, 792.0]}
{"layout": 2137, "type": "text", "text": "11.1 Summary and Conclusions ", "text_level": 1, "page_idx": 155, "bbox": [107, 297, 251, 309], "page_size": [612.0, 792.0]}
{"layout": 2138, "type": "text", "text": "In this report, our primary focus is on probing GPT-4V across various application scenarios. The findings reveal its remarkable capabilities, some of which have not been investigated or demonstrated in existing approaches. While we strive to uncover as many of these capabilities as possible, we acknowledge that our presentation may not be exhaustive. Nevertheless, this report can serve as a reference for future research aimed at exploring additional uses of GPT-4V, deepening the understanding of LMMs, and building even more powerful LMMs. ", "page_idx": 155, "bbox": [107, 317.1805419921875, 505, 383.73150634765625], "page_size": [612.0, 792.0]}
{"layout": 2139, "type": "text", "text": "11.2 Towards Future LMMs ", "text_level": 1, "page_idx": 155, "bbox": [107, 395, 237, 407], "page_size": [612.0, 792.0]}
{"layout": 2140, "type": "text", "text": "The weaknesses and limitations of GPT models have been extensively discussed in related reports [ 99 , 100 ,  24 ]. In this section, we briefly focus on presenting our perspective on future research directions. ", "page_idx": 155, "bbox": [107, 415.35955810546875, 505, 438.27349853515625], "page_size": [612.0, 792.0]}
{"layout": 2141, "type": "text", "text": "Models like GPT-1, GPT-2, and GPT-3 function primarily as text-in-text-out systems, capable of processing natural language only. GPT-4 (no vision) demonstrates unparalleled competence in text understanding and generation, while GPT-4V exhibits a strong ability to comprehend the image domain as well. ", "page_idx": 155, "bbox": [107, 442.65655517578125, 505, 487.38848876953125], "page_size": [612.0, 792.0]}
{"layout": 2142, "type": "text", "text": "As a natural progression, LMMs should be able to generate interleaved image-text content, such as producing vivid tutorials containing both text and images, to enable comprehensive multimodal content understanding and generation. Additionally, it would be beneficial to incorporate other modalities, such as video, audio, and other sensor data, to expand the capabilities of LMMs. ", "page_idx": 155, "bbox": [107, 491.7725524902344, 505, 536.5044555664062], "page_size": [612.0, 792.0]}
{"layout": 2143, "type": "text", "text": "Regarding the learning process, current approaches predominantly rely on well-organized data, such as image-tag or image-text datasets. However, a more versatile model may be able to learn from various sources, including online web content and even real-world physical environments, to facilitate continuous self-evolution. ", "page_idx": 155, "bbox": [107, 540.8885498046875, 505, 585.6204833984375], "page_size": [612.0, 792.0]}
{"layout": 2144, "type": "text", "text": "Acknowledgment ", "text_level": 1, "page_idx": 155, "bbox": [107, 600, 198, 613], "page_size": [612.0, 792.0]}
{"layout": 2145, "type": "text", "text": "We express our gratitude to all contributors from OpenAI for their technical efforts on the GPT-4V project [ 99 – 101 ,  1 ], and we are profoundly thankful to OpenAI for granting early access to their remarkable tool. Our sincere appreciation goes to Misha Bilenko for his invaluable guidance and support. We also extend heartfelt thanks to our Microsoft colleagues for their insights, with special acknowledgment to John Montgomery, Marco Casalaina, Gregory Buehrer, Nguyen Bach, Gopi Kumar, Luis Vargas, Kun Wu, Meenaz Merchant, Jianfeng Gao, Matt Lungren, Sheela Agarwal, Yumao Lu, Thomas Soemo, Fisayo Okikiolu, Ce Liu, Michael Zeng, Faisal Ahmed, Ehsan Azarnasab, and Lin Liang for their constructive feedback. We also thank Yingkai Yu for helping to create screenshots on GUI Navigation. ", "page_idx": 155, "bbox": [107, 623.5215454101562, 505, 722.7994995117188], "page_size": [612.0, 792.0]}
{"layout": 2146, "type": "text", "text": "References ", "text_level": 1, "page_idx": 156, "bbox": [107, 72, 165, 86], "page_size": [612.0, 792.0]}
{"layout": 2147, "type": "text", "text": "[1]  Chatgpt can now see, hear, and speak. https://openai.com/blog/ chatgpt-can-now-see-hear-and-speak , 2023.\n\n ", "page_idx": 156, "bbox": [117, 90.69854736328125, 505, 113.61345672607422], "page_size": [612.0, 792.0]}
{"layout": 2148, "type": "text", "text": "[2] Deepfloyd if.  https://github.com/deep-floyd/IF , 2023.\n\n [3] Guidance.  https://github.com/microsoft/guidance/ , 2023.\n\n [4] Midjourney.  https://www.midjourney.com/ , 2023.\n\n [5]  Sameer Agarwal, Yasutaka Furukawa, Noah Snavely, Ian Simon, Brian Curless, Steven M Seitz, and Richard Szeliski. Building rome in a day.  Communications of the ACM , 54(10):105–112, 2011.\n\n [6]  Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Keerthana Gopal a krishna n, Karol Hausman, Alex Herzog, et al. Do as i can, not as i say: Grounding language in robotic afford ances.  arXiv preprint arXiv:2204.01691 , 2022.\n\n [7]  Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning.  Advances in Neural Information Processing Systems , 35:23716–23736, 2022.\n\n [8]  Chris Alberti, Jeffrey Ling, Michael Collins, and David Reitter. Fusion of detected objects in text for visual question answering. In  EMNLP , 2019.\n\n [9]  Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and Lei Zhang. Bottom-up and top-down attention for image captioning and visual question answering. In  CVPR , 2018.\n\n [10] Karpathy Andrej. State of gpt.  https://karpathy.ai/stateofgpt.pdf , 2023.\n\n [11]  Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. arXiv preprint arXiv:2305.10403 , 2023.\n\n [12]  Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C. Lawrence Zitnick, and Devi Parikh. VQA: Visual Question Answering. In  ICCV , 2015.\n\n [13]  Anas Awadalla, Irena Gao, Joshua Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Jenia Jitsev, Simon Kornblith, Pang Wei Koh, Gabriel Ilharco, Mitchell Wortsman, and Ludwig Schmidt. Open flamingo, March 2023.\n\n [14]  Hessam Bag her inez had, Hannaneh Hajishirzi, Yejin Choi, and Ali Farhadi. Are elephants big- ger than butterflies? reasoning about sizes of objects. In  Proceedings of the AAAI Conference on Artificial Intelligence , volume 30, 2016.\n\n [15]  Shruthi Bannur, Stephanie Hyland, Qianchu Liu, Fernando Perez-Garcia, Maximilian Ilse, Daniel C Castro, Benedikt Boecking, Harshita Sharma, Kenza Bouzid, Anja Thieme, et al. Learning to exploit temporal structure for biomedical vision-language processing. In  Pro- ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 15016–15027, 2023.\n\n [16]  David Barrett, Felix Hill, Adam Santoro, Ari Morcos, and Timothy Lillicrap. Measuring abstract reasoning in neural networks. In  International conference on machine learning , pages 511–520. PMLR, 2018.\n\n [17]  Ali Furkan Biten, Ruben Tito, Andres Mafla, Lluis Gomez, Marçal Rusinol, Ernest Valveny, CV Jawahar, and Di most hen is Karatzas. Scene text visual question answering. In  ICCV , 2019.\n\n [18]  Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and Sergey Levine. Training diffusion models with reinforcement learning, 2023. ", "page_idx": 156, "bbox": [112, 121.3995361328125, 505, 722.7994995117188], "page_size": [612.0, 792.0]}
{"layout": 2149, "type": "text", "text": "[19]  Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al. Improving language models by retrieving from trillions of tokens. In  International conference on machine learning , pages 2206–2240. PMLR, 2022.\n\n [20]  Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101–mining disc rim i native components with random forests. In  Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part VI 13 , pages 446–461. Springer, 2014.\n\n [21]  Marc A Brackett and Peter Salovey. Measuring emotional intelligence with the mayer-salovery- caruso emotional intelligence test (msceit).  Psicothema , 18:34–41, 2006.\n\n [22]  Tim Brooks, Aleksander Holynski, and Alexei A. Efros. Instruct pix 2 pix: Learning to follow image editing instructions. In  CVPR , 2023.\n\n [23]  Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari- wal, Arvind Neel a kant an, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. In  NeurIPS , 2020.\n\n [24]  Sébastien Bubeck, Varun Chandra sekar an, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence: Early experiments with gpt-4.  arXiv preprint arXiv:2303.12712 , 2023.\n\n [25]  Ting Chen, Saurabh Saxena, Lala Li, David J Fleet, and Geoffrey Hinton. Pix2seq: A language modeling framework for object detection. In  ICLR , 2022.\n\n [26]  Ting Chen, Saurabh Saxena, Lala Li, Tsung-Yi Lin, David J Fleet, and Geoffrey E Hinton. A unified sequence interface for vision tasks.  Advances in Neural Information Processing Systems , 35:31333–31346, 2022.\n\n [27]  Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollár, and C Lawrence Zitnick. Microsoft coco captions: Data collection and evaluation server.  arXiv preprint arXiv:1504.00325 , 2015.\n\n [28]  Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. Uniter: Learning universal image-text representations. In  ECCV , 2020.\n\n [29]  Jaemin Cho, Jie Lei, Hao Tan, and Mohit Bansal. Unifying vision-and-language tasks via text generation. In  ICML , 2021.\n\n [30]  Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways.  arXiv preprint arXiv:2204.02311 , 2022.\n\n [31]  Herbert H Clark and Deanna Wilkes-Gibbs. Referring as a collaborative process.  Cognition , 22(1):1–39, 1986.\n\n [32]  Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In  Proceedings of the IEEE conference on computer vision and pattern recognition , pages 3213–3223, 2016.\n\n [33] Tom Cornsweet.  Visual perception . Academic press, 2012.\n\n [34]  Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Zhifang Sui, and Furu Wei. Why can gpt learn in-context? language models secretly perform gradient descent as meta optimizers.  arXiv preprint arXiv:2212.10559 , 2022.\n\n [35]  Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instruct blip: Towards general-purpose vision-language models with instruction tuning.  arXiv preprint arXiv:2305.06500 , 2023. ", "page_idx": 157, "bbox": [112, 72.757568359375, 505, 722.7994995117188], "page_size": [612.0, 792.0]}
{"layout": 2150, "type": "text", "text": "[36]  Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh, Deshraj Yadav, José MF Moura, Devi Parikh, and Dhruv Batra. Visual dialog. In  Proceedings of the IEEE conference on computer vision and pattern recognition , pages 326–335, 2017.\n\n [37]  Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In  CVPR , 2009.\n\n [38]  Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In  NAACL-HLT , 2019.\n\n [39]  Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. A survey for in-context learning.  arXiv preprint arXiv:2301.00234 , 2022.\n\n [40]  Zi-Yi Dou, Aishwarya Kamath, Zhe Gan, Pengchuan Zhang, Jianfeng Wang, Linjie Li, Zicheng Liu, Ce Liu, Yann LeCun, Nanyun Peng, et al. Coarse-to-fine vision-language pre-training with fusion in the backbone. In  Advances in Neural Information Processing Systems .\n\n [41]  Zi-Yi Dou, Yichong Xu, Zhe Gan, Jianfeng Wang, Shuohang Wang, Lijuan Wang, Chenguang Zhu, Pengchuan Zhang, Lu Yuan, Nanyun Peng, et al. An empirical study of training end- to-end vision-and-language transformers. In  Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 18166–18176, 2022.\n\n [42]  Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duckworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch, and Pete Florence. Palm-e: An embodied multimodal language model. In  arXiv preprint arXiv:2303.03378 , 2023.\n\n [43]  Alicia Fasquel, Angèle Brunel li re, and Dominique Knutsen. A modified procedure for naming 332 pictures and collecting norms: Using tangram pictures in psycho linguistic studies. Behavior Research Methods , pages 1–23, 2022.\n\n [44]  Samir Yitzhak Gadre, Kiana Ehsani, and Shuran Song. Act the part: Learning interaction strategies for articulated object part discovery. In  Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 15752–15761, 2021.\n\n [45]  Zhe Gan, Yen-Chun Chen, Linjie Li, Chen Zhu, Yu Cheng, and Jingjing Liu. Large-scale adversarial training for vision-and-language representation learning. In  NeurIPS , 2020.\n\n [46]  Zhe Gan, Linjie Li, Chunyuan Li, Lijuan Wang, Zicheng Liu, Jianfeng Gao, et al. Vision- language pre-training: Basics, recent advances, and future trends.  Foundations and Trends® in Computer Graphics and Vision , 14(3–4):163–352, 2022.\n\n [47]  Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. Pal: Program-aided language models. In  International Conference on Machine Learning , pages 10764–10799. PMLR, 2023.\n\n [48]  Tao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang, Miao Zheng, Qian Zhao, Kuikun Liu, Wenwei Zhang, Ping Luo, and Kai Chen. Multimodal-gpt: A vision and language model for dialogue with humans, 2023.\n\n [49]  Yandong Guo, Lei Zhang, Yuxiao Hu, Xiaodong He, and Jianfeng Gao. Ms-celeb-1m: A dataset and benchmark for large-scale face recognition. In  Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part III 14 , pages 87–102. Springer, 2016.\n\n [50]  Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. Retrieval augmented language model pre-training. In  International conference on machine learning , pages 3929–3938. PMLR, 2020.\n\n [51]  Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick. Mask r-cnn. In  Proceedings of the IEEE international conference on computer vision , pages 2961–2969, 2017. ", "page_idx": 158, "bbox": [112, 72.757568359375, 505, 722.7994995117188], "page_size": [612.0, 792.0]}
{"layout": 2151, "type": "text", "text": "[52]  Jack \\*Hessel, Jena D \\*Hwang, Jae Sung Park, Rowan Zellers, Chandra Bhaga va tula, Anna Rohrbach, Kate Saenko, and Yejin Choi. The Abduction of Sherlock Holmes: A Dataset for Visual Abductive Reasoning. In  ECCV , 2022.\n\n [53]  Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buch at s kaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models.  arXiv preprint arXiv:2203.15556 , 2022.\n\n [54]  Yushi Hu, Hang Hua, Zhengyuan Yang, Weijia Shi, Noah A Smith, and Jiebo Luo. Promptcap: Prompt-guided task-aware image captioning. In  Proceedings of International Conference on Computer Vision (ICCV) , 2023.\n\n [55]  Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Qiang Liu, et al. Language is not all you need: Aligning perception with language models.  arXiv preprint arXiv:2302.14045 , 2023.\n\n [56]  Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. In  International Conference on Machine Learning , pages 9118–9147. PMLR, 2022.\n\n [57]  Zhicheng Huang, Zhaoyang Zeng, Bei Liu, Dongmei Fu, and Jianlong Fu. Pixel-bert: Aligning image pixels with text by deep multi-modal transformers.  arXiv preprint arXiv:2004.00849 , 2020.\n\n [58]  Fabian Hutmacher. Why is there so much more research on vision than on any other sensory modality?  Frontiers in psychology , 10:2246, 2019.\n\n [59]  Anya Ji, Noriyuki Kojima, Noah Rush, Alane Suhr, Wai Keen Vong, Robert Hawkins, and Yoav Artzi. Abstract visual reasoning with tangram shapes. In  Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing , pages 582–601, 2022.\n\n [60]  Alistair EW Johnson, Tom J Pollard, Seth J Berkowitz, Nathaniel R Greenbaum, Matthew P Lungren, Chih-ying Deng, Roger G Mark, and Steven Horng. Mimic-cxr, a de-identified publicly available database of chest radio graphs with free-text reports.  Scientific data , 6(1):317, 2019.\n\n [61]  Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, C Lawrence Zitnick, and Ross Girshick. Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. In  Proceedings of the IEEE conference on computer vision and pattern recognition , pages 2901–2910, 2017.\n\n [62]  Justin Johnson, Andrej Karpathy, and Li Fei-Fei. Densecap: Fully convolutional localization networks for dense captioning. In  Proceedings of the IEEE conference on computer vision and pattern recognition , pages 4565–4574, 2016.\n\n [63]  Geunwoo Kim, Pierre Baldi, and Stephen McAleer. Language models can solve computer tasks.  arXiv preprint arXiv:2303.17491 , 2023.\n\n [64]  Wonjae Kim, Bokyung Son, and Ildoo Kim. Vilt: Vision-and-language transformer without convolution or region supervision. In  ICML , 2021.\n\n [65]  Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of International Conference on Computer Vision (ICCV) , 2023.\n\n [66]  Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners.  Advances in neural information processing systems , 35:22199–22213, 2022.\n\n [67]  Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language and vision using crowd sourced dense image annotations.  IJCV , 2017. ", "page_idx": 159, "bbox": [112, 72.757568359375, 505, 722.7994995117188], "page_size": [612.0, 792.0]}
{"layout": 2152, "type": "text", "text": "[68]  Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Na- man Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock t s chel, et al. Retrieval- augmented generation for knowledge-intensive nlp tasks.  Advances in Neural Information Processing Systems , 33:9459–9474, 2020.\n\n [69]  Chunyuan Li, Zhe Gan, Zhengyuan Yang, Jianwei Yang, Linjie Li, Lijuan Wang, and Jianfeng Gao. Multimodal foundation models: From specialists to general-purpose assistants.  arXiv preprint arXiv:2309.10020 , 2023.\n\n [70]  Gen Li, Nan Duan, Yuejian Fang, Ming Gong, Daxin Jiang, and Ming Zhou. Unicoder-vl: A universal encoder for vision and language by cross-modal pre-training. In  AAAI , 2020.\n\n [71]  Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Boots trapping language- image pre-training with frozen image encoders and large language models.  arXiv preprint arXiv:2301.12597 , 2023.\n\n [72]  Junnan Li, Ramp rasa a th R Selvaraju, Akhilesh Deepak Gotmare, Shafiq Joty, Caiming Xiong, and Steven Hoi. Align before fuse: Vision and language representation learning with momen- tum distillation. In  NeurIPS , 2021.\n\n [73]  Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. Visualbert: A simple and performant baseline for vision and language.  arXiv preprint arXiv:1908.03557 , 2019.\n\n [74]  Xiujun Li, Xi Yin, Chunyuan Li, Xiaowei Hu, Pengchuan Zhang, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, et al. Oscar: Object-semantics aligned pre-training for vision-language tasks. In  ECCV , 2020.\n\n [75]  Yaobo Liang, Chenfei Wu, Ting Song, Wenshan Wu, Yan Xia, Yu Liu, Yang Ou, Shuai Lu, Lei Ji, Shaoguang Mao, et al. Taskmatrix. ai: Completing tasks by connecting foundation models with millions of apis.  arXiv preprint arXiv:2303.16434 , 2023.\n\n [76]  Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In  ECCV , 2014.\n\n [77]  Fangyu Liu, Emanuele Bug liar ello, Edoardo Maria Ponti, Siva Reddy, Nigel Collier, and Desmond Elliott. Visually grounded reasoning across languages and cultures. In  Proceed- ings of the 2021 Conference on Empirical Methods in Natural Language Processing , pages 10467–10485, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.\n\n [78]  Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang. Aligning large multi-modal model with robust instruction tuning.  arXiv preprint arXiv:2306.14565 , 2023.\n\n [79]  Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning.  arXiv preprint arXiv:2304.08485 , 2023.\n\n [80] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In  Proceedings of International Conference on Computer Vision (ICCV) , December 2015.\n\n [81]  Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pre training task-agnostic vi sio linguistic representations for vision-and-language tasks. In  NeurIPS , 2019.\n\n [82]  Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mottaghi, and Aniruddha Kembhavi. Unified-io: A unified model for vision, language, and multi-modal tasks. In  The Eleventh International Conference on Learning Representations , 2022.\n\n [83]  Jiasen Lu, Vedanuj Goswami, Marcus Rohrbach, Devi Parikh, and Stefan Lee. 12-in-1: Multi-task vision and language representation learning. In  CVPR , 2020.\n\n [84] Jiasen Lu, Jianwei Yang, Dhruv Batra, and Devi Parikh. Neural baby talk. In  CVPR , 2018. ", "page_idx": 160, "bbox": [112, 72.757568359375, 505, 722.7994995117188], "page_size": [612.0, 792.0]}
{"layout": 2153, "type": "text", "text": "[85]  Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering.  Advances in Neural Information Processing Systems , 35:2507–2521, 2022.\n\n [86]  Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu, Song- Chun Zhu, and Jianfeng Gao. Chameleon: Plug-and-play compositional reasoning with large language models.  arXiv preprint arXiv:2304.09842 , 2023.\n\n [87]  Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter Clark, and Ashwin Kalyan. Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning.  arXiv preprint arXiv:2209.14610 , 2022.\n\n [88]  Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback.  arXiv preprint arXiv:2303.17651 , 2023.\n\n [89]  Bertram F Malle, Louis J Moses, and Dare A Baldwin.  Intentions and intentional it y: Founda- tions of social cognition . MIT press, 2001.\n\n [90]  Arjun Mani, Nobline Yoo, Will Hinthorn, and Olga Russ a kov sky. Point and ask: Incorporating pointing into visual question answering.  arXiv preprint arXiv:2011.13681 , 2020.\n\n [91]  John D Mayer. Msceit: Mayer-salovey-caruso emotional intelligence test.  Toronto, Canada: Multi-Health Systems , 2002.\n\n [92]  John D Mayer, Richard D Roberts, and Sigal G Barsade. Human abilities: Emotional intelli- gence.  Annu. Rev. Psychol. , 59:507–536, 2008.\n\n [93]  Grégoire Mialon, Roberto Dessì, Maria Lomeli, Christof oros Nalmpantis, Ram Pasunuru, Roberta Raileanu, Baptiste Rozière, Timo Schick, Jane Dwivedi-Yu, Asli Cel i kyi l maz, et al. Augmented language models: a survey.  arXiv preprint arXiv:2302.07842 , 2023.\n\n [94]  Microsoft. Bing image search api.  https://www.microsoft.com/en-us/bing/ apis/bing-image-search-api , 2023.\n\n [95]  Weiqing Min, Zhiling Wang, Yuxin Liu, Mengjiang Luo, Liping Kang, Xiaoming Wei, Xiaolin Wei, and Shuqiang Jiang. Large scale visual food recognition.  IEEE Transactions on Pattern Analysis and Machine Intelligence , 2023.\n\n [96]  Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. Cross-task general- ization via natural language crowd sourcing instructions. In  Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 3470–3487, 2022.\n\n [97]  Tara Murfitt and Jan McAllister. The effect of production variables in monolog and dialog on comprehension by novel listeners.  Language and Speech , 44(3):325–350, 2001.\n\n [98]  Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christo- pher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted question-answering with human feedback.  arXiv preprint arXiv:2112.09332 , 2021.\n\n [99] OpenAI. Gpt-4 technical report, 2023.\n\n [100] OpenAI. Gpt-4v(ision) system card. 2023.\n\n [101] OpenAI. Gpt-4v(ision) technical work and authors. 2023.\n\n [102]  Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback.  Advances in Neural Information Processing [103] Bhargavi Paranjape, Scott Lundberg, Sameer Singh, Hannaneh Hajishirzi, Luke Z ett le moyer, and Marco Tulio Ribeiro. Art: Automatic multi-step reasoning and tool-use for large language models.  arXiv preprint arXiv:2303.09014 , 2023.\n\n [104]  Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou Yu, Weizhu Chen, et al. Check your facts and try again: Improving large language models with external knowledge and automated feedback.  arXiv preprint arXiv:2302.12813 , 2023.\n\n [105]  Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. Kosmos-2: Grounding multimodal large language models to the world.  arXiv preprint arXiv:2306.14824 , 2023.\n\n [106]  Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis.  arXiv preprint arXiv:2307.01952 , 2023.\n\n [107]  Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Yufei Huang, Chaojun Xiao, Chi Han, Yi Ren Fung, Yusheng Su, Huadong Wang, Cheng Qian, Runchu Tian, Kunlun Zhu, Shihao Liang, Xingyu Shen, Bokai Xu, Zhen Zhang, Yining Ye, Bowen Li, Ziwei Tang, Jing Yi, Yuzhang Zhu, Zhenning Dai, Lan Yan, Xin Cong, Yaxi Lu, Weilin Zhao, Yuxiang Huang, Junxi Yan, Xu Han, Xian Sun, Dahai Li, Jason Phang, Cheng Yang, Tongshuang Wu, Heng Ji, Zhiyuan Liu, and Maosong Sun. Tool learning with foundation models, 2023.\n\n [108]  Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer.  The Journal of Machine Learning Research , 21(1):5485–5551, 2020.\n\n [109]  John C Raven and JH Court.  Raven’s progressive matrices . Western Psychological Services Los Angeles, 1938.\n\n [110]  Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In  Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 10684–10695, 2022.\n\n [111]  Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, et al. Multitask prompted training en- ables zero-shot task generalization. In  International Conference on Learning Representations , 2021.\n\n [112]  Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettle- moyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools.  arXiv preprint arXiv:2302.04761 , 2023.\n\n [113]  Zhenwei Shao, Zhou Yu, Meng Wang, and Jun Yu. Prompting large language models with answer heuristics for knowledge-based visual question answering. In  CVPR , pages 14974– 14983, 2023.\n\n [114]  Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face.  arXiv preprint arXiv:2303.17580 , 2023.\n\n [115]  Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Z ett le moyer, and Wen-tau Yih. Replug: Retrieval-augmented black-box language models. arXiv preprint arXiv:2301.12652 , 2023.\n\n [116]  Noah Shinn, Beck Labash, and Ashwin Gopinath. Reflexion: an autonomous agent with dynamic memory and self-reflection.  arXiv preprint arXiv:2303.11366 , 2023.\n\n [117]  Aleksandar Sh ted rit ski, Christian Rupprecht, and Andrea Vedaldi. What does clip know about a red circle? visual prompt engineering for vlms.  arXiv preprint arXiv:2304.06712 , 2023. ", "page_idx": 161, "bbox": [108, 72.757568359375, 505, 711.8905029296875], "page_size": [612.0, 792.0]}
{"layout": 2154, "type": "text", "text": "", "page_idx": 162, "bbox": [107, 72.757568359375, 505, 722.7994995117188], "page_size": [612.0, 792.0]}
{"layout": 2155, "type": "text", "text": "[118]  Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and Amanpreet Singh. Textcaps: a dataset for image captioning with reading comprehension. In  ECCV , pages 742–758, 2020.\n\n [119]  Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read. In  CVPR , 2019.\n\n [120]  Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, and Jifeng Dai. Vl-bert: Pre-training of generic visual-linguistic representations. In  ICLR , 2019.\n\n [121]  Dídac Surís, Sachit Menon, and Carl Vondrick. Vipergpt: Visual inference via python execution for reasoning.  arXiv preprint arXiv:2303.08128 , 2023.\n\n [122]  Hao Tan and Mohit Bansal. Lxmert: Learning cross-modality encoder representations from transformers. In  EMNLP , 2019.\n\n [123]  Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo- thée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models.  arXiv preprint arXiv:2302.13971 , 2023.\n\n [124]  Harsh Trivedi, Niranjan Bala subramania n, Tushar Khot, and Ashish Sabharwal. Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions.  arXiv preprint arXiv:2212.10509 , 2022.\n\n [125]  Maria Tsim po uk ell i, Jacob L Menick, Serkan Cabi, SM Eslami, Oriol Vinyals, and Felix Hill. Multimodal few-shot learning with frozen language models.  Advances in Neural Information Processing Systems , 34:200–212, 2021.\n\n [126]  Carven Von Bear nen squash. Paper gestalt.  Secret Proceedings of Computer Vision and Pattern Recognition (CVPR) , 2010.\n\n [127]  Hong Wang, Xuan Luo, Weizhi Wang, and Xifeng Yan. Bot or human? detecting chatgpt imposters with a single question.  arXiv preprint arXiv:2305.06424 , 2023.\n\n [128]  Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, and Lijuan Wang. Git: A generative image-to-text transformer for vision and language. Transactions on Machine Learning Research , 2022.\n\n [129]  Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang. Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework. In  International Conference on Machine Learning , pages 23318–23340. PMLR, 2022.\n\n [130]  Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In  The Eleventh International Conference on Learning Representations , 2022.\n\n [131]  Zhen hai long Wang, Manling Li, Ruochen Xu, Luowei Zhou, Jie Lei, Xudong Lin, Shuohang Wang, Ziyi Yang, Chenguang Zhu, Derek Hoiem, et al. Language models with image descriptors are strong few-shot video-language learners. In  Advances in Neural Information Processing Systems .\n\n [132]  Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, and Yuan Cao. Simvlm: Simple visual language model pre training with weak supervision. In  ICLR , 2022.\n\n [133]  David Wechsler. Wais-r: Manual: Wechsler adult intelligence scale-revised.  (No Title) , 1981.\n\n [134] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In ICLR , 2022.\n\n [135]  Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models.  arXiv preprint arXiv:2206.07682 , 2022. ", "page_idx": 163, "bbox": [107, 72.757568359375, 505, 722.7994995117188], "page_size": [612.0, 792.0]}
{"layout": 2156, "type": "text", "text": "[136]  Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems , 35:24824–24837, 2022.\n\n [137]  Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. Visual chatgpt: Talking, drawing and editing with visual foundation models.  arXiv preprint arXiv:2303.04671 , 2023.\n\n [138]  Jialian Wu, Jianfeng Wang, Zhengyuan Yang, Zhe Gan, Zicheng Liu, Junsong Yuan, and Lijuan Wang. Grit: A generative region-to-text transformer for object understanding.  arXiv preprint arXiv:2212.00280 , 2022.\n\n [139]  Zhenjia Xu, Zhijian Liu, Chen Sun, Kevin Murphy, William T Freeman, Joshua B Tenenbaum, and Jiajun Wu. Unsupervised discovery of parts, structure, and dynamics. In  International Conference on Learning Representations , 2018.\n\n [140]  Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Faisal Ahmed, Zicheng Liu, Yumao Lu, and Lijuan Wang. Unitab: Unifying text and box outputs for grounded vision-language modeling. In  European Conference on Computer Vision , pages 521–539. Springer, 2022.\n\n [141]  Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Yumao Lu, Zicheng Liu, and Lijuan Wang. An empirical study of gpt-3 for few-shot knowledge-based vqa. In  Proceedings of the AAAI Conference on Artificial Intelligence , volume 36, pages 3081–3089, 2022.\n\n [142]  Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. Mm-react: Prompting chatgpt for multimodal reasoning and action.  arXiv preprint arXiv:2303.11381 , 2023.\n\n [143]  Zhengyuan Yang, Jianfeng Wang, Zhe Gan, Linjie Li, Kevin Lin, Chenfei Wu, Nan Duan, Zicheng Liu, Ce Liu, Michael Zeng, et al. Reco: Region-controlled text-to-image generation. In  Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 14246–14255, 2023.\n\n [144]  Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models.  arXiv preprint arXiv:2305.10601 , 2023.\n\n [145]  Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R Narasimhan, and Yuan Cao. React: Syne rg i zing reasoning and acting in language models. In  The Eleventh International Conference on Learning Representations , 2022.\n\n [146]  Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. mplug-owl: Modular iz ation empowers large language models with multi modality.  arXiv preprint arXiv:2304.14178 , 2023.\n\n [147]  Da Yin, Liunian Harold Li, Ziniu Hu, Nanyun Peng, and Kai-Wei Chang. Broaden the vision: Geo-diverse visual commonsense reasoning.  arXiv preprint arXiv:2109.06860 , 2021.\n\n [148]  Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi. From recognition to cognition: Visual commonsense reasoning. In  CVPR , pages 6720–6731, 2019.\n\n [149]  Andy Zeng, Maria Attarian, Krzysztof Marcin Cho roman ski, Adrian Wong, Stefan Welker, Federico Tombari, Aveek Purohit, Michael S Ryoo, Vikas Sindhwani, Johnny Lee, et al. Socratic models: Composing zero-shot multimodal reasoning with language. In  The Eleventh International Conference on Learning Representations , 2022.\n\n [150]  Chi Zhang, Feng Gao, Baoxiong Jia, Yixin Zhu, and Song-Chun Zhu. Raven: A dataset for relational and analogical visual reasoning. In  Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 5317–5327, 2019.\n\n [151]  Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, and Alex Smola. Mul- timodal chain-of-thought reasoning in language models.  arXiv preprint arXiv:2302.00923 , 2023. ", "page_idx": 164, "bbox": [107, 72.757568359375, 505, 722.7994995117188], "page_size": [612.0, 792.0]}
{"layout": 2157, "type": "text", "text": "[152]  Yan-Tao Zheng, Ming Zhao, Yang Song, Hartwig Adam, Ulrich Buddemeier, Alessandro Bissacco, Fernando Brucher, Tat-Seng Chua, and Hartmut Neven. Tour the world: building a web-scale landmark recognition engine. In  2009 IEEE Conference on Computer Vision and Pattern Recognition , pages 1085–1092. IEEE, 2009.\n\n [153]  Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and Antonio Torralba. Learning deep features for disc rim i native localization. In  CVPR , 2016.\n\n [154]  Bolei Zhou, Agata Lapedriza, Antonio Torralba, and Aude Oliva. Places: An image database for deep scene understanding.  Journal of Vision , 17(10):296–296, 2017.\n\n [155]  Luowei Zhou, Hamid Palangi, Lei Zhang, Houdong Hu, Jason J Corso, and Jianfeng Gao. Unified vision-language pre-training for image captioning and vqa. In  AAAI , 2020.\n\n [156]  Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba. Large language models are human-level prompt engineers. In  The Eleventh International Conference on Learning Representations , 2022.\n\n [157]  Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592 , 2023.\n\n [158]  Xueyan Zou, Zi-Yi Dou, Jianwei Yang, Zhe Gan, Linjie Li, Chunyuan Li, Xiyang Dai, Harkirat Behl, Jianfeng Wang, Lu Yuan, et al. Generalized decoding for pixel, image, and language. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 15116–15127, 2023.\n\n [159]  Xueyan Zou, Zi-Yi Dou, Jianwei Yang, Zhe Gan, Linjie Li, Chunyuan Li, Xiyang Dai, Jianfeng Wang, Lu Yuan, Nanyun Peng, Lijuan Wang, Yong Jae Lee, and Jianfeng Gao. Generalized decoding for pixel, image and language. 2022.\n\n [160]  Xueyan Zou, Jianwei Yang, Hao Zhang, Feng Li, Linjie Li, Jianfeng Gao, and Yong Jae Lee. Segment everything everywhere all at once.  arXiv preprint arXiv:2304.06718 , 2023. ", "page_idx": 165, "bbox": [107, 72.757568359375, 505, 410.34149169921875], "page_size": [612.0, 792.0]}
