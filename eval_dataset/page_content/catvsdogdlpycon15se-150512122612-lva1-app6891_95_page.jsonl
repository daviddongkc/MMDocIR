{"page": 0, "image_path": "doc_images/catvsdogdlpycon15se-150512122612-lva1-app6891_95_0.jpg", "ocr_text": "5s W E- DBD E WN\n\nDeep Learnings\nand Deep Data-Science\n\n@graphific\n\n12 May 2015\nroelof@kth.se Graph Technologies R&D\nwww.csc.kth.se/-roelof/ roelof@eraph-technologies.com\n\nslides online at:\nhttps:/www-.sli hare.net/roelofp/ learning-as-a-cat -detector\n", "vlm_text": "Deep Learning: \nand Deep Data-Science \nThe image shows a cat and a dog facing each other against a green checkered background. The text \"CAT vs DOG\" is displayed below them.\nThe image shows a logo with a blue background. In the center, there is a crown at the top of a wreath. Inside the wreath, the text reads \"KTH VETENSKAP OCH KONST.\" This is the emblem of the KTH Royal Institute of Technology in Sweden.\n12May2015 \nROYAL INSTITUTE OF TECHNOLOGY \nGraph Technologies R&D roelof@graph-technologies.com \nroelof@kth.se www.csc.kth.se/-roelof/ \nslides online at: "}
{"page": 1, "image_path": "doc_images/catvsdogdlpycon15se-150512122612-lva1-app6891_95_1.jpg", "ocr_text": "Dee ee\n\nare yOu a...\nCAT PERSON? DOG PERSON?\n\n", "vlm_text": "BUT FIRST \nare you a CATPERSON? \n\nThis image is a humorous or edited photo showing a person sitting and holding a cat. The person's head has been replaced with the head of a cat, making it look like a human with a cat head holding a regular cat.\nDOG PERSON? \nThe image shows a digitally manipulated creature that has the body of a dog but with human facial features, like eyes and lips."}
{"page": 2, "image_path": "doc_images/catvsdogdlpycon15se-150512122612-lva1-app6891_95_2.jpg", "ocr_text": "", "vlm_text": "in the next few minutes we'llbe making a \nThe image shows a cat and a dog facing each other with the words \"CAT vs DOG\" below them. The background is a green pattern with squares and circles.\nDETECTOR "}
{"page": 3, "image_path": "doc_images/catvsdogdlpycon15se-150512122612-lva1-app6891_95_3.jpg", "ocr_text": "main Libraries «3»\n\nesckikit-learn (machine learning)\nhttp://scikit-learn.org\n\necaffe (deep learning) — for training deep neural nets\n(for today: loading a pre-trained one)\nhttp://caffe.berkeleyvision.org\n\netheano (efficient gou-powered math)\nhttp: //www.deeplearning. net/software/theano/\n\neipython notebook\nhttp: //ipython.org/notebook. html\n\n", "vlm_text": "main Libraries \nsckikit-learn(machine learning) http://scikit-learn.org caffe(deep learning)-for training deep neural nets （for today:loading a pre-trained one)http://caffe.berkeley vision.org \ntheano(efficient gpu-powered math) http://www.deep learning.net/software/theano/ \noi python notebook http://ipython.org/notebook.html "}
{"page": 4, "image_path": "doc_images/catvsdogdlpycon15se-150512122612-lva1-app6891_95_4.jpg", "ocr_text": "Classification\n\nIdentifying to which category an object\nbelongs to.\n\nApplications: Spam detection, Image\nrecognition.\n\nAlgorithms: SVM, nearest neighbors,\nrandom forest, ... — Examples\n\nDimensionality reduction\n\nReducing the number of random\nvariables to consider.\n\nApplications: Visualization, Increased\n\nefficiency\n\nAlgorithms: PCA, feature selection,\n\nnon-negative matrix factorization.\nExamples\n\nInstallation Documentation ~\n\nanalysis\n\nExamples\n\n+ Simple and efficient tools for data mining and data\n\n+ Accessible to everybody, and reusable in various contexts\n« Built on NumPy, SciPy, and matplotlib\n\n+ Open source, commercially usable - BSD license\n\nRegression\n\nPredicting a continuous-valued attribute\nassociated with an object.\n\nApplications: Drug response, Stock\nprices.\n\nAlgorithms: SVA, ridge regression,\nLasso, ... — Examples\n\nModel selection\n\nComparing, validating and choosing\nparameters and models.\n\nGoal: Improved accuracy via parameter\ntuning\nModules: grid search, cross validation,\n\nmetrics. — Examples\n\nClustering\n\nAutomatic grouping of similar objects\ninto sets.\n\nApplications: Customer segmentation,\nGrouping experiment outcomes\nAlgorithms: k-Means, spectral\nclustering, mean-shift, ... — Examples\n\nPreprocessing\n\nFeature extraction and normalization.\n\nApplication: Transforming input data\nsuch as text for use with machine\nlearning algorithms.\nModules: preprocessing, feature\nextraction.\n\n— Examples\n", "vlm_text": "scikit-learn \nThe image depicts several subplots showing decision boundaries of different machine learning models applied to a classification problem. Each subplot likely uses a different algorithm, such as k-Nearest Neighbors, Linear SVM, RBF SVM, Decision Trees, Random Forests, and AdaBoost. The regions are colored to indicate the areas classified for each class, with scatter plots of data points marked as well.\nMachine Leaming in Python \nSimple and efficient toolsfor data mining and data analysis Accessible to everybody,and reusable in various contexts Builton NumPy,SciPy.and matplotlib Open source,commercially usable-BSD license \nClassification \nClustering \nRegression \nPredicting a continuous-valued attribute associated with an object Applications:Drug response,Stock prices. Algorithms:SVR,ridge regression Lasso,.. -Examples \nAutomatic grouping of similar objects intosets. Applications:Customer segmentation, Grouping experiment outcomes Algorithms:k-Means,spectral clustering,mean-shift,... -Examples \nl dent if ying to which category anobject belongsto. Applications:Spam detection,lmage recognition. Algorithms:SVM,nearest neighbors random forest,... -Examples \nDimensionality reduction \nModel selection \nPreprocessing \nReducing the number of random variables to consider. \nFeature extraction and normalization \nComparing,validating and choosing parameters and models. Goal:lm proved accuracy via parameter tuning Modules:grid search,cross validation metrics. \nApplication:Transforming input data suchas text for use with machine learning algorithms. Modules:preprocessing,feature extraction. \nApplications:Visualization.Increased efficiency Algorithms:PCA,feature selection, non-negative matrix factorization "}
{"page": 5, "image_path": "doc_images/catvsdogdlpycon15se-150512122612-lva1-app6891_95_5.jpg", "ocr_text": "Caffe\n\nDeep learning framework\nby the BVLC\n\nCreated by\nYangqing Jia\nLead Developer\nEvan Shelhamer\n\nCaffe\n\nCaffe is a deep learning framework made with expression, speed, and modularity in mind. It is\ndeveloped by the Berkeley Vision and Learning Center (BVLC) and by community contributors.\nYangqing Jia created the project during his PhD at UC Berkeley. Caffe is released under the BSD 2-\nClause license.\n\nCheck out our web image classification demo!\n\nWhy Caffe?\n\nExpressive architecture encourages application and innovation. Models and optimization are\ndefined by configuration without hard-coding. Switch between CPU and GPU by setting a single flag\nto train on a GPU machine then deploy to commodity clusters or mobile devices.\n\nExtensible code fosters active development. In Caffe’s first year, it has been forked by over 1,000\ndevelopers and had many significant changes contributed back. Thanks to these contributors the\nframework tracks the state-of-the-art in both code and models.\n\nSpeed makes Caffe perfect for research experiments and industry deployment. Caffe can process\nover 60M images per day with a single NVIDIA K40 GPU*. That's 1 ms/image for inference and 4\nms/image for learning. We believe that Caffe is the fastest convnet implementation available.\n\nCommunity: Caffe already powers academic research projects, startup prototypes, and even large-\nscale industrial applications in vision, speech, and multimedia. Join our community of brewers on\nthe caffe-users group and Github.\n\n* With the ILSVRC2012-winning SuperVision model and caching |O. Consult performance details.\n", "vlm_text": "Caffe \nDeep learning framework bytheBVLC \nCreatedby Yang qing Jia Lead Developer Evan Shelhamer \nJMiew On GitHub \nCaffe \nCaffeisa deep learning framework made with expression,speed,and modular it yin mind.ltis developed by the Berkeley Vision and Learning Center(BVLC)andby community contributors. Yang qing Jia created theproject during his PhD at UCBerkeley.Caff e is released under theBSD 2- Clause license. \nCheckout our web image classification demo! \nWhyCaffe? \nExpressive architecture encourages application and innovation.Models and optimization are defined by configuration without hard-coding.Switch between CPU and GPUby setting a single flag to train on a GPumachine then deploy to commodity clusters or mobile devices \nExtensible code fosters active development.InCaffe'sfirstyear,it has been forked by over 1,oo0 developers andhad many significant changes contributed back.Thanksto these contributors the framework tracks the state-of-the-art in both code and models \nSpeedmakes Caff e perfect for research experiments and industry deployment.Caff e can process over 6 oM images per day with as in gleN viD lAK 40 GPU\\*.That's1ms/image for inference and 4 ms/image for learning.We believe that Caff e is the fastest con v net implementation available. \nCommunity:Caffe already powers academic research projects,startup prototypes,and even large scale industrial applications in vision,speech,and multimedia.Join our community of brewers on thecaffe-users group and G it hub "}
{"page": 6, "image_path": "doc_images/catvsdogdlpycon15se-150512122612-lva1-app6891_95_6.jpg", "ocr_text": "Theano 0.7 documentation » next | modules | index\n\nWelcome T h CQ ‘a O\n\nTheano is a Python library that allows you to define, optimize, and evaluate\nmathematical expressions involving multi-dimensional arrays efficiently. Theano\n\nfeatures: Welcome\n¢ tight integration with NumPy - Use numpy.ndarray in Theano-compiled nownloal\nfunctions. Status\n* transparent use of a GPU - Perform data-intensive calculations up to 140x Citing Theano\nfaster than with CPU.(float32 only) Documentation\n« efficient symbolic differentiation - Theano does your derivatives for function Community\n\nwith one or many inputs.\n\n* speed and stability optimizations - Get the right answer for log(1+x) even\nwhen x is really tiny. Release Notes\n\ne¢ dynamic C code generation - Evaluate expressions faster.\n\ne extensive unit-testing and self-verification - Detect and diagnose many types : Bd\nof mistake. Show Source\n\nTheano has been powering large-scale computationally intensive scientific\ninvestigations since 2007. But it is also approachable enough to be used in the\nclassroom (IFT6266 at the University of Montreal). Go\n\nEnter search terms or a module,\nclass or function name.\n\nNews\n\ne We support cuDNN if it is installed by the user.\n* Open Machine Learning Workshop 2014 presentation.\n\n* Colin Raffel tutorial on Theano.\n« lan Goodfellow did a 12h cl with exerci on Theano.\n", "vlm_text": "Welcome \nThe a no is a Python library that allows you to define,optimize,and evaluate mathematical expressions involving multi-dimensional arrays efficiently.Theano features: \ntight integration with Num Py-Usenumpy.nd array in The a no-compiled \nfunctions.  ${\\bf140x}$  faster than with CPU.(float 32 only) efficient symbolic differentiation-The a no does your derivatives for function with one or many inputs. speed and stability optimization s-Get the right answer for  $\\log(1+x)$  even when x is really tiny. dynamic C code generation-Evaluate expressions faster. extensive unit-testing andself-verification-Detect and diagnose many types ofmistake. \nThe a no has been powering large-scale computationally intensive scientific investigations since 2 oo 7.But it is also approachable enough to be used in the classroom(IF T 6266 at the University of Montreal) \nNews \nTable Of Contents \nWelcome News Download Status Citing Theano Documentation Community \nNext topic \nRelease Notes \nThis Page \nShow Source \nQuick search \nThe image shows a button or similar graphic with the word \"Go\" written on it.\nEnter search terms or a module class or function name. \nWe support cuD NN if it is installed by the user. Open Machine Learning Workshop 2 o 14 presentation Colin Raff el tutorial on The a no. lan Goodfellow did a 12 h class with exercises on The a no "}
{"page": 7, "image_path": "doc_images/catvsdogdlpycon15se-150512122612-lva1-app6891_95_7.jpg", "ocr_text": "I P Ly]: ee Computing\n\nInstall - Docs : Videos : News - Cite - Sponsors - Donate\n\nThe IPython Notebook eer oees .\n\n; ; ; ; ; VERSIONS\nThe [Python Notebook is an interactive computational environment, in which you can\ncombine code execution, rich text, mathematics, plots and rich media, as shown in’ Stable\nthis example session: 3.1 - April 2015\nInstall\n1Pur: atebook Specogran Lam uwed Ma OF 11 Pw :\n\nDevelopment\n4.0.dev\n\nSimple spectral analysis GitHub\n\nAn dunicrton of fon Decree Esgree Trgrato\n\nXi = Saat F- 4 6.....N-i\n=\n\nOffline Docs\nAll Versions\nGitHub\n\nweitg eetaneng 1 trees Me Pequeiy RMAT of a RENE eye\nie Den by nme a aamatie wing AP Y'Y meee Ra\n\nNOTEBOOK\nVIEWER\n\nShare your notebooks\n\nIt aims to be an agile tool for both exploratory computation and data analysis, and\nprovides a platform to support reproducible research, since all inputs and outputs\nmay be stored in a one-to-one way in notebook documents.\n\n", "vlm_text": "IP[y]: IPython Interactive Computing \nInstall·Docs·Videos·News·Cite·Sponsors  $^{\\ast}$  Donate \nThe I Python Notebook \nTheIPython Notebook is an interactive computational environment,inwhichyou can combine code execution,rich text,mathematics,plotsand rich media,asshownin this example session: \nThis image shows an IPython Notebook (now known as Jupyter Notebook) interface titled \"Simple spectral analysis\". It illustrates the Discrete Fourier Transform (DFT) with a mathematical equation and Python code snippets. The notebook imports a WAV file using the `scipy.io` module and visualizes its waveform and spectrogram using Matplotlib.\n\nThe interface displays:\n- Some description and mathematical equation for DFT.\n- Python code for importing and analyzing an audio file.\n- A plot of the audio signal waveform and a spectrogram.\n\nThe left plot shows the raw audio signal, and the right plot shows its spectrogram, representing frequency content over time.\nIt aims to bean agile tool for both exploratory computation and data analysis,and provides a platform to support reproducible research,since all inputs and outputs maybe stored in a one-to-one way in notebook documents. \nThe image shows a search box with the text \"Google™ Custom Search\" and a \"Search\" button next to it.\nVERSIONS \nStable 3.1-April2015 Install \nDevelopment 4.0.dev GitHub \nOffline Docs All Versions GitHub \nNOTEBOOK VIEWER \nShare your notebooks \nThe image appears to be a blurry bar chart with two sets of bars, likely representing data from the years 1950 and 2000, indicated by the color legend. Below the chart, there is some text about confidence sets and a mathematical expression, but it's too blurry to read clearly."}
{"page": 8, "image_path": "doc_images/catvsdogdlpycon15se-150512122612-lva1-app6891_95_8.jpg", "ocr_text": "", "vlm_text": "Code is ahead,soon... \nThe image features a cute cartoon bear sitting with a speech bubble above it. The text next to the bear reads \"BEAR WITH ME.\" The speech bubble contains the text \"I promise :)\" which is partially visible. The image is a playful pun using the bear to emphasize the phrase."}
{"page": 9, "image_path": "doc_images/catvsdogdlpycon15se-150512122612-lva1-app6891_95_9.jpg", "ocr_text": "Data Science ?\n\n“Data science is clearly a blend of the\nhackers: art, statistics and machine\nlearning...”\n\n—Hilary Mason & Chris Wiggins, 2010\n", "vlm_text": "DataScience？\n\"Data science is clearly a blend of the hackers'art,statistics and machine learning \n—Hilary Mason & Chris Wiggins, 2010 "}
{"page": 10, "image_path": "doc_images/catvsdogdlpycon15se-150512122612-lva1-app6891_95_10.jpg", "ocr_text": "(Drew Connoway 2010)\n", "vlm_text": "The image is a Venn diagram illustrating the intersection of three skill sets: \"Hacking Skills,\" \"Math & Statistics Knowledge,\" and \"Substantive Expertise.\" \n\n- The area where all three circles overlap is labeled \"Data Science.\"\n- The overlap between \"Hacking Skills\" and \"Math & Statistics Knowledge\" is labeled \"Machine Learning.\"\n- The overlap between \"Math & Statistics Knowledge\" and \"Substantive Expertise\" is labeled \"Traditional Research.\"\n- The overlap between \"Hacking Skills\" and \"Substantive Expertise\" is labeled \"Danger Zone!\""}
{"page": 11, "image_path": "doc_images/catvsdogdlpycon15se-150512122612-lva1-app6891_95_11.jpg", "ocr_text": "> Features = Awesomeness\n\n1 feature\n", "vlm_text": "Features=Awesome ness \nI feature "}
{"page": 12, "image_path": "doc_images/catvsdogdlpycon15se-150512122612-lva1-app6891_95_12.jpg", "ocr_text": "> Features = Awesomeness\n\nDD ages 8\n\nFeature 1\n\n2 features\n", "vlm_text": ">Features=Awesome ness \nThe image shows an illustration of cute, stylized dog and cat heads. The dogs have brown and beige features, while the cats are grey with stripes. They are arranged in a playful pattern.\nI feature \nThe image appears to be a 2D plot with two features labeled as \"Feature 1\" and \"Feature 2.\" The plot contains illustrations of cats and dogs scattered across the grid. Each point on the plot is represented by either a cat face or a dog face, indicating their position based on the two features.\n2features "}
{"page": 13, "image_path": "doc_images/catvsdogdlpycon15se-150512122612-lva1-app6891_95_13.jpg", "ocr_text": "Feature 1\n\n2 features\ntoo few features/dimensions = overfitting\n", "vlm_text": "The image features a cute row of cartoon-style animal faces, specifically dogs and cats, lined up in a pattern. The animals have round faces with simple, playful features.\n1 feature \nThe image is a two-dimensional plot with feature axes labeled \"Feature 1\" and \"Feature 2.\" It displays cartoon faces of cats and dogs scattered on a grid. The green shaded areas seem to separate groups of cats and dogs, likely representing different classifications or clusters in a feature space.\n2features \ntoo few features/dimensions=over fitting "}
{"page": 14, "image_path": "doc_images/catvsdogdlpycon15se-150512122612-lva1-app6891_95_14.jpg", "ocr_text": "> Features = Awesomeness\n\net\n\nFeature 1\n\n2 features 3 features\ntoo few features/dimensions = overfitting\n", "vlm_text": ">Features = Awesome ness \nThe image features a pattern with illustrated faces of dogs and cats arranged in a sequence. They are cartoon-like with simple features and are placed above a line that resembles a ruler.\nThe image depicts a 3D grid or plot with illustrations of cats and dogs. The plot is labeled with \"Feature 1\" and \"Feature 2\" on the axes. The animals are positioned in different regions, possibly to represent clusters or categories based on these features. There are shaded green areas highlighting certain regions on the plot.\n2features \nThe image depicts a three-dimensional graph with a grid and axes labeled \"Feature 1,\" \"Feature 2,\" and \"Feature 3.\" Within the graph are illustrated faces of cats and dogs placed at various points, likely representing data points based on the three features.\n3features too few features/dimensions=over fitting \n"}
{"page": 15, "image_path": "doc_images/catvsdogdlpycon15se-150512122612-lva1-app6891_95_15.jpg", "ocr_text": "More Features = Awesomeness!\n\nFeature 3\n\nFeature 1\n\nFeature 1\n\n2 features 3 features\n", "vlm_text": "More Features=Awesome ness! \nThe image features a row of stylized, cartoon-like dog and cat faces arranged along what looks like a measurement scale or ruler. The dog faces have brown and white colors, while the cat faces have gray markings. They all have friendly expressions.\nIfeature \nThe image is a 3D plot with two axes labeled \"Feature 1\" and \"Feature 2.\" It depicts a scatter plot with cartoon images of cats and dogs. Cats and dogs are placed in different areas, suggesting classification based on the two features. Some areas are highlighted to indicate clusters or decision boundaries within the plot.\nThe image is a three-dimensional graph with three labeled axes: Feature 1, Feature 2, and Feature 3. It depicts illustrations of cats and dogs positioned within the 3D space. A green plane is shown, separating cats from the dog, suggesting a decision boundary commonly used in classifications like those in machine learning."}
{"page": 16, "image_path": "doc_images/catvsdogdlpycon15se-150512122612-lva1-app6891_95_16.jpg", "ocr_text": "++ Data Needs also grow!\n\n", "vlm_text": "+十 Data Needs also grow! \nThe image contains three plots, each illustrating a different visual representation of data using icons of dogs and cats:\n\n1. **Left Plot**: A 1D plot with a linear gradient from red to green. Cat and dog icons are aligned along a horizontal axis, possibly denoting a range or category.\n\n2. **Middle Plot**: A 2D plot with a gradient background spanning from red to green, featuring cats and dogs scattered across a square grid. The numbering suggests value ranges on the axes.\n\n3. **Right Plot**: A 3D cube with a similar gradient effect and animal icons within the cube, depicting a volumetric space with labeled axes providing additional dimensions.\n\nThis visualization likely represents data in increasing complexity from 1D to 3D, combining both quantifiable values and categorical representations using the animal icons."}
{"page": 17, "image_path": "doc_images/catvsdogdlpycon15se-150512122612-lva1-app6891_95_17.jpg", "ocr_text": "(picture by Dato)\n", "vlm_text": "The image is a simple diagram of a classifier system. It shows an icon of a dog being input into a box labeled \"Simple Classifier.\" There are two output funnels labeled \"DOGS\" and \"CATS.\" The image of the dog exits through the \"DOGS\" funnel. This represents a basic classification task where an item (a dog) is correctly categorized."}
{"page": 18, "image_path": "doc_images/catvsdogdlpycon15se-150512122612-lva1-app6891_95_18.jpg", "ocr_text": "by Dato)\n\n(picture\n", "vlm_text": "The image shows a simple diagram of an animal classifier system with labels for \"CATS\" and \"DOGS\" on the outputs. There is a red prohibition symbol overlaying the diagram, indicating a restriction or negation."}
{"page": 19, "image_path": "doc_images/catvsdogdlpycon15se-150512122612-lva1-app6891_95_19.jpg", "ocr_text": "Deep Learning?\neA host of statistical machine learning\ntechniques\n\neEnables the automatic learning of feature\nhierarchies\n\neGenerally based on artificial neural\nnetworks\n", "vlm_text": "Deep Learning? \n·A host of statistical machine learning techniques \nEnables the automatic learning offeature hierarchies \nGenerally based on artificial neural networks "}
{"page": 20, "image_path": "doc_images/catvsdogdlpycon15se-150512122612-lva1-app6891_95_20.jpg", "ocr_text": "Deep Learning\n\n(picture by Dato)\n", "vlm_text": "Deep Learning \nThis image is an illustration of a \"Deep Learning Classifier.\" It shows various icons representing different objects like an anchor, dog, bicycle, hammer, glasses, apple, tree, mug, sailboat, and cat being fed into a funnel. The funnel is connected to a machine labeled \"Deep Learning Classifier,\" which categorizes items into groups such as animals, plants, food, tools, and insects. There's also a calendar in the background with pages for the 12th and 13th torn off, indicating the current date as the 14th."}
{"page": 21, "image_path": "doc_images/catvsdogdlpycon15se-150512122612-lva1-app6891_95_21.jpg", "ocr_text": "Deep Learning?\neManually designed features are often over-specified,\nincomplete and take a long time to design and validate\n\neLearned Features are easy to adapt, fast to learn\n\neDeep learning provides a very flexible, (almost?)\nuniversal, learnable framework for representing\nworld, visual and linguistic information.\n\neDeep learning can learn unsupervised (from raw\ntext/audio/images/whatever content) and\nsupervised (with specific labels like positive /\nnegative)\n\n", "vlm_text": "Deep Learning? \nManually designed features areoften over-specified, incomplete and take along time to design and validate Learned Features are easy to adapt,fastto learn \nDeep learning provides a very flexible,(almost?) universal,learn able framework for representing world,visual and T ingui stic information. \nDeep learning can learn unsupervised(fromraw text/audio/images/whatever content)and supervised(with specific labels like positive/ negative) "}
{"page": 22, "image_path": "doc_images/catvsdogdlpycon15se-150512122612-lva1-app6891_95_22.jpg", "ocr_text": "North Bay\n\nStacked\nAutoencoders\n\na “omputer Science\n\\9 J UNIVERSITY OF TORONTO\nGh\n\n| Restricted Boltzmann\nMachine\n\nRepresentations\nPittsburgh\nww\n\n", "vlm_text": "2006+:The Deep Learning Conspirators \nThe image appears to be a collage featuring three individuals associated with deep learning and artificial intelligence. It shows a map in the background with the following elements:\n\n1. **Hinton** (pictured)\n   - Associated logos: Google and University of Toronto\n   - Text: \"Restricted Boltzmann Machine\"\n\n2. **Bengio** (pictured)\n   - Associated logo: Université de Montréal\n   - Text: \"Stacked Autoencoders\"\n\n3. **LeCun** (pictured)\n   - Associated logos: Facebook and New York University\n   - Text: \"Sparse Representations\"\n\nThese names are likely linked to their contributions to AI and machine learning research."}
{"page": 23, "image_path": "doc_images/catvsdogdlpycon15se-150512122612-lva1-app6891_95_23.jpg", "ocr_text": "Error Rate\n\n1998\n\n2000\n\nAudio Recognition\n\n@ Traditional @ Deep Learning\n\n2002 2004 2006 2008 2010 2012 2014\n\n(chart by Clarifai)\n", "vlm_text": "Audio Recognition \nTraditional Deep Learning \nThe image is a scatter plot showing error rates from 1998 to 2014. The y-axis represents the error rate, ranging from 15 to 31, and the x-axis represents years from 1998 to 2014. The data points are represented by blue and orange dots. The blue dots are mostly clustered between 23 and 31 error rates from 1998 to 2010. Starting around 2010, orange dots appear, showing a decreasing trend in error rates, reaching below 19 by 2014."}
{"page": 24, "image_path": "doc_images/catvsdogdlpycon15se-150512122612-lva1-app6891_95_24.jpg", "ocr_text": "Error Rate\n\n(mage Recognition\n\nTraditional CV @ Deep Learning\n\n(chart by Clarifai)\n", "vlm_text": "Image Recognition \nThe image is a chart comparing error rates for traditional computer vision (CV) methods and deep learning from 2010 to 2014. \n\n- **Y-axis**: Error Rate (ranging from 7% to 79%)\n- **X-axis**: Years (2010 to 2014)\n- **Blue dots**: Represent traditional CV\n- **Orange dots**: Represent deep learning\n\nThe chart shows that error rates for traditional CV were high in 2010 and decreased over time. Deep learning began appearing around 2012 with lower error rates compared to traditional CV, continuing to improve through 2014."}
{"page": 25, "image_path": "doc_images/catvsdogdlpycon15se-150512122612-lva1-app6891_95_25.jpg", "ocr_text": "Natural Langauge Processing\n", "vlm_text": "Natural Langauge Processing \n"}
{"page": 26, "image_path": "doc_images/catvsdogdlpycon15se-150512122612-lva1-app6891_95_26.jpg", "ocr_text": "Natural Langauge Processing\n\ncloudy pon el forever.\n\n~ oe\n\n", "vlm_text": "Natural Langauge Processing \ncloudy days don'+ last forever. \nThe image is a simple illustration of a bright yellow sun with rays shining against a light blue sky, surrounded by dark clouds. "}
{"page": 27, "image_path": "doc_images/catvsdogdlpycon15se-150512122612-lva1-app6891_95_27.jpg", "ocr_text": "How ?\n\na\n\n[PPD PAP hp\no@® Dai: KDIDIE\n\nHE: ite Je ti re\n. a Be\nml pas\noa, Fo. ‘tH en |\nee Be ba\na Easier\nWP Pr ae DPh et\n\n", "vlm_text": "DL? How 2. \nThe image shows a collage of multiple faces arranged in a grid pattern. Each square in the grid contains a different face, displaying a variety of expressions and features.\nThe image shows an illustrated bear with a speech bubble. The bubble contains the text \"almost at the code...\" The bear appears to be sitting and has a friendly expression."}
{"page": 28, "image_path": "doc_images/catvsdogdlpycon15se-150512122612-lva1-app6891_95_28.jpg", "ocr_text": "input layer\n\n", "vlm_text": "The image shows a collage of human faces on the left side and a diagram of a neural network on the right side. The faces represent input data fed into the neural network, which has multiple layers, including an input layer, several hidden layers, and an output layer. This setup typically illustrates how neural networks are used in machine learning for tasks like image recognition or facial recognition."}
{"page": 29, "image_path": "doc_images/catvsdogdlpycon15se-150512122612-lva1-app6891_95_29.jpg", "ocr_text": "Deep neural\nnetworks learn\nhierarchical feature <2\nrepresentations a t\n\nhidden layer 1 hidden layer 2. hidden layer 3\n\ninput layer\n\nrm ee\n\n", "vlm_text": "The image appears to show visualizations related to a neural network or deep learning model, likely showcasing different layers of a convolutional neural network (CNN). \n\n- The first section seems to represent convolutional filters from an early layer, capturing simple features like edges. \n- The middle section may be from a deeper layer, showing combinations of features like parts of faces. \n- The last section looks like even deeper layers, where the model captures more complex representations resembling full faces.\n\nThese types of visualizations are often used to understand how CNNs process and learn different features from input data.\nDeepneural networks learn hierarchical feature represent at lons \nThe image contains a grid of multiple faces in various expressions and appearances.\nThis image depicts a diagram of a neural network. It shows multiple layers, including an input layer, several hidden layers, and an output layer. Each circle represents a neuron, and the lines represent connections (weights) between the neurons across layers. This structure illustrates how data flows through a neural network from input to output."}
{"page": 30, "image_path": "doc_images/catvsdogdlpycon15se-150512122612-lva1-app6891_95_30.jpg", "ocr_text": "", "vlm_text": "The image appears to be a grid of abstract or blurred faces. This type of image is often used in studies related to facial recognition or image processing, where the emphasis is on training algorithms to recognize patterns or features in facial structures. Each face seems deliberately blurred, possibly to test recognition capabilities or to anonymize individual identities.\nThe image appears to be a grid of abstract, distorted facial features. This could be a visual representation used in image processing or a neural network for recognizing or generating facial features.\nThe image appears to be a grid containing multiple small, abstract patterns. These patterns likely resemble Gabor filters or similar visual stimuli used in image processing or neural networks to detect edges or textures. Each square contains a unique pattern that may vary in orientation and intensity."}
{"page": 31, "image_path": "doc_images/catvsdogdlpycon15se-150512122612-lva1-app6891_95_31.jpg", "ocr_text": "‘oe = : ae\n\na i\noe\npe So\n\n<=\n\nfed\nRe es\nc? 4\n\noutput layer\ninput layer\n", "vlm_text": "input layer \nThis image shows a visualization of a neural network architecture. It features multiple layers of neurons connected with lines, illustrating the connections between layers. On the right, there are grids of images representing visual features that the network might learn at different layers. From bottom to top, the image depicts:\n\n1. Basic features like edges or gradients.\n2. More complex features like parts of faces.\n3. Full face reconstructions.\n\nThese representations illustrate how a neural network processes and identifies hierarchical features in images."}
{"page": 32, "image_path": "doc_images/catvsdogdlpycon15se-150512122612-lva1-app6891_95_32.jpg", "ocr_text": "Coding time!\n\n(picture by Dato)\n", "vlm_text": "The image is a cartoon illustration of two classification processes. On the left, there's a \"Deep Learning Classifier\" box that takes input from an image of a dog and categorizes it into broad categories like \"Animals,\" \"Plants,\" \"Food,\" \"Tools,\" and \"Insects.\" The output then goes to a \"Simple Classifier\" box that categorizes the image into more specific categories like \"Cats\" or \"Dogs,\" with the dog image resulting in the \"Dogs\" category.\nour "}
{"page": 33, "image_path": "doc_images/catvsdogdlpycon15se-150512122612-lva1-app6891_95_33.jpg", "ocr_text": "dataset (25k dog/cat\npictures)\n\n(picture by Dato)\n", "vlm_text": "Dog \nThe image is a humorous illustration of a two-step classification process. It includes a \"Deep Learning Classifier\" machine and a \"Simple Classifier\" machine, suggesting a system that first categorizes an image using deep learning into broad categories like \"animals, plants, food, tools, insects,\" and then further refines it into specific categories such as \"cats\" or \"dogs.\" The image specifically mentions Kaggle's Cat vs Dog dataset. The initial image input shows a dog silhouette, and the output label is \"DOG.\""}
{"page": 34, "image_path": "doc_images/catvsdogdlpycon15se-150512122612-lva1-app6891_95_34.jpg", "ocr_text": "Completed * Swag + 215 teams\nDogs vs. Cats\n\n13 - Sa\n\nib\n\nDashboard Competition Details » Getthe Data » Make a submission\nHome fs]\n\n“20 Create an algorithm to distinguish dogs\nInformation @ from cats\n\nIn this competition, you'll write an algorithm to classify whether images contain either a\ndog or a cat. This is easy for humans, dogs, and cats. Your computer will find it a bit\n\nPrizes\n\nWinners more difficult,\n\nForum *\n\nLeaderboard\nPublic\nPrivate\n\nVisualization\n\nMy Team\nGitHub\n\nMy Submissions a\n\nData Files\nLeaderboard\n\nFile Name Available Formats\nDeep Blue beat i bai t kb)\n|  sampleSubmission .csv (86.82\nWatson beat the brigh P\neupsicatuann Can you tell testi wip (271.15 mb)\ntrain Zip (543.16 mb)\n\nhttps://www.kaggle.com/c/dogs-vs-cats/data|\n", "vlm_text": "Wed 25 Sep 2013-S at 1 Feb 2014(15 months ago) \nDashboard \nHome \nData \nMake a submission \nInformation Description Evaluation Rules Prizes Winners \nForum \nVisual lz ation \nMy Submissions \nLeader board \n1.Pierre Sermanet 2orchld\n\n 3.Owen\n\n 4.Paul Covington \nCompetition Details Getthe Data \\*Makea submission \nCreate an algorithm to distinguish dogs fromcats \nIn this competition,you'll write an algorithm to classify whether images contain eithera dog or a cat. This is easy for humans, dogs,and cats. Your computer will find it a bit more difficult. \nThe image shows a bulldog and a cat facing each other in front of a wooden background.\nThe table lists files with their names and available formats:\n\n1. **File Name:** sampleSubmission\n   - **Available Formats:** .csv (86.82 kb)\n\n2. **File Name:** test1\n   - **Available Formats:** .zip (271.15 mb)\n\n3. **File Name:** train\n   - **Available Formats:** .zip (543.16 mb)\nDeep Bluebeat Watson beat the bright e Canyou tell Fi "}
{"page": 35, "image_path": "doc_images/catvsdogdlpycon15se-150512122612-lva1-app6891_95_35.jpg", "ocr_text": "Pretrained\nConvolutional\n\nNeural Net (CNN)\n\n", "vlm_text": "The image shows a diagram of a \"Deep Learning Classifier\" machine. There is a funnel on the left, with an image of a dog being input into it. The machine has various labeled categories on the side, such as \"Animals,\" \"Plants,\" \"Food,\" \"Tools,\" and \"Insects.\" This illustration represents a simplified concept of a classifier sorting inputs into different categories.\nPretrained Convolutional Neural Net (CNN) \nThe image shows a diagram labeled \"Simple Classifier.\" It appears to have two output channels marked \"DOGS\" and \"CATS.\" A card labeled \"DOG\" with a silhouette of a dog is coming out of the \"DOGS\" channel. This suggests a simplistic visual representation of a system that classifies inputs into categories of dogs or cats."}
{"page": 36, "image_path": "doc_images/catvsdogdlpycon15se-150512122612-lva1-app6891_95_36.jpg", "ocr_text": "(picture by Dato)\n", "vlm_text": "accuracy in < 1h \n"}
{"page": 37, "image_path": "doc_images/catvsdogdlpycon15se-150512122612-lva1-app6891_95_37.jpg", "ocr_text": "Cooking Instructions\n\n1 Load Pretrained Net\n\nS\no\n~\n&\n\necccccece |\nseeeoeeooo\n[reece\n\nQ, Go G3\n\nAy, Gn, Anz °**\n\nG3, G4, G3; °\"\n", "vlm_text": "1 Load Pre trained Net \nThe image represents a diagram of a convolutional neural network (CNN) architecture. It starts with an image of a cartoon cat and shows the process of feature extraction through various convolutional and pooling layers, labeled as conv1 through conv5. The diagram includes fully connected layers, labeled as fc6, fc7, and fc8, which finalize the feature extraction for classification. The network details such as convolution and max-pooling operations are indicated, with neuron counts shown in the fully connected layers. The text \"1 Load Pretrained Net\" and \"2 Extract features for all training images\" suggests that the network is used for feature extraction in a pretrained model workflow.\nThe image illustrates a process involving image feature extraction and classification using a neural network. \n\n1. **Image Features**: On the left, a matrix labeled \\(a_{ij}\\) represents features extracted from an image. The rows (\\(i\\)) correspond to different images, and the columns (\\(j\\)) correspond to different features.\n\n2. **Feature Selection**: A specific set of features (\\([a_{21}, a_{22}, a_{23}, \\ldots]\\)) is highlighted in blue, indicating the features that are being selected for further processing.\n\n3. **Training an MLP**: On the right, the text \"train MLP on those features\" appears, where MLP stands for Multi-Layer Perceptron, a type of neural network. The network diagram has nodes (neurons) with connections representing the flow of information.\n\n4. **Output Labels**: Two output nodes with pictures of a cat and a dog indicate that the MLP is used for classifying images into categories like 'cat' or 'dog'."}
{"page": 38, "image_path": "doc_images/catvsdogdlpycon15se-150512122612-lva1-app6891_95_38.jpg", "ocr_text": "1 Load Pretrained Net Cooking Instructions\n\nconvl conv2 conv3 conv4 conv5\n\no\n~\nre\nao\n\n|\n\nf\n3\neeceeeeoe &\n\njr\n\neceoeoeooo\njseece\n", "vlm_text": "1 Load Pre trained Net \nThis image depicts the architecture of a convolutional neural network (CNN), similar to AlexNet. It shows the various layers and operations applied to an input image of a cat:\n\n1. **Input Image**: The process starts with an image of a cat.\n2. **Convolutional Layers (conv1 to conv5)**: Various convolutional layers with different filter sizes and characteristics. Each layer progressively captures more abstract features. Max-pooling and response normalization are applied in some layers.\n3. **Fully Connected Layers (fc6, fc7, fc8)**: These layers follow the convolutional layers:\n   - **fc6 and fc7**: Each has 4096 neurons.\n   - **fc8**: The final layer, which has 1000 neurons, typically used for classification.\n\nThe structure is visualized with different colors for each layer, outlining the number of neurons and types of connections."}
{"page": 39, "image_path": "doc_images/catvsdogdlpycon15se-150512122612-lva1-app6891_95_39.jpg", "ocr_text": "No Free Lunch... But Free Models!\n\nBVLC / caffe @Watch+ 625 wUnstar 3,481 YY Fork\n\nModel Zoo\n\nLiwei Wang edited this page 3 days ago - 18 revisions\n\nTrained models are posted here as links to Github Gists. Check out the model zoo\ndocumentation for details.\n\nPages\n\nHome\nTo acquire a model: Caffe on EC2 Ubuntu 14.04 Cuda\n7\n1. download the model gist by ./scripts/download_model_from_gist.sh <gist_id>\n: : Development\n<dirname> to load the model metadata, architecture, solver configuration, and so on.\n4 A Installation\n(<dirname> is optional and defaults to caffe/models).\n. download the model weights by ./scripts/download_model_binary.py <model_dir> Installation (OSX)\nwhere <model_dir> is the gist directory from the first step. Model Zoo\n\nModels accuracy on ImageNet\nBerkeley-trained models areas\nPublications\n* Finetuning on Flickr Style: same as provided in models/ , but listed here as a Gist for\nan example.\ne BVLC GoogleNet\n\nRelated Projects\nUbuntu 14.04 ec2 instance\n\nUbuntu 14.04 VirtualBox VM\n\nNetwork in Network model\n\nClone this wiki locally\n\ns://sithub.com LC/cafte/wi\n\n", "vlm_text": "No Free Lunch... But Free Models! \nThe image contains icons and text typically seen on a GitHub repository page:\n\n- An eye icon with the text \"Watch\" followed by the number 625.\n- A star icon with the text \"Unstar\" followed by the number 3,481.\n- A fork icon with the text \"Fork\" and the number 2,057.\n\nThese elements represent the number of users watching, starring, and forking the repository.\nBVLC/caffe \nThe image shows two buttons. One says \"Edit\" and the other says \"New Page.\" The \"Edit\" button is white, and the \"New Page\" button is green.\nModelZoo \nLiwei Wang edited this page 3 days ago·18 revisions \nThe image shows a navigation menu titled \"Pages\" with 11 items listed. The items are:\n\n1. Home\n2. Caffe on EC2 Ubuntu 14.04 Cuda 7\n3. Development\n4. Installation\n5. Installation (OSX)\n6. Model Zoo\n7. Models accuracy on ImageNet 2012 val\n8. Publications\n9. Related Projects\n10. Ubuntu 14.04 ec2 instance\n11. Ubuntu 14.04 VirtualBox VM\nTrained models are posted here as links to Github Gists.Checkout the modelzoo documentation for details. \nTo acquire a model: \n1.download the model gist by./scripts/download model from gist.sh<gist_id> xdirname> to load the model metadata,architecture,solver configuration,and so on (<dirname> is optional and defaults to caff e/models) \n2.download themodel weightsby./scripts/download model binary.py <model_dir> where<model_dir>is the gist directory from the firststep \nBerkeley-trained models \nFinetuning on Flickr Style:same asprovided in models/,but listed hereasa Gist for anexample. ·B VLC Google Net \nNetwork in Network model \nClone this wiki locally "}
{"page": 40, "image_path": "doc_images/catvsdogdlpycon15se-150512122612-lva1-app6891_95_40.jpg", "ocr_text": "/scripts/download_model_binary.py ../models/bvlc_reference_caffenet\n\n> yupyter\n\nFiles Running Clusters.\n\nTo import a notebook, drag the file onto the listing below or click\n\n~ # / caffe / models / bvic_reference_caffenet\na Pe\n\n(5 bvic_reference_caffenet.caffemodel\n\n1 deploy.prototxt\n\n( readme.md\n\n(7 _eahior nentatet\n\n= jupyter solver.prototxt ~ 03/17/2015\n\n@ Menu ‘current mode\n\n1 nett\n“models/bvle_reference_caffenet/train_ val.prototx\nt\"\nteat_iter: 1000\ntest_interval: 1000\nbase_lrs 0.01\nlr_policy: \"step\"\ngammar 0.1\nstepsize: LOO00O\ndisplay: 20\n\n) max_iter: 450000\nmomentum: 0.9\nweight _decay: 0.0005\nsnapshot: 10000\nsnapshot_prefix:\n“models/bvlc_reference_caffenet/caffenet_train“\n\n1 solver_mode: GPU\n\nae ju pyter deploy.prototxt » 03/17/2015\n\nFile\n\nEdit View Language\n\nname: \"CaffeNet\"\ninput: \"data\"\n\n) input_dim: 10\n\ninput_dim: 3\n\n) input_dim: 227\n\ninput_dim: 227\nlayer {\nname: \"“convi\"\ntype: \"Convolution\"\nbottom: \"data\"\ntop: \"convl\"\nconvolution_param {\nnum_output: 96\nkernel size: 11\nstride: 4\n}\n}\n\nL8 layer {\n\nname: \"relul\"\ntype: \"ReLuU\"\nbottom: \"convl\"\ntop: \"convl\"\n}\nlayer {\nname: \"pooll\"\ntype: \"Pooling\"\nbottom: \"convl1\"\ntop: \"pooll\"\npooling param {\npool: MAX\nkernel_size: 3\nstride: 2\n1\n", "vlm_text": "jupyter \nRunning Clusters \nToimport anotebook,drag thefile onto the listing below or click \nb vlc reference caff e net.caffemodel \ndeploy.prototxt \nreadme.md \nsolvor nrntotyt \nJ up y ter solver.prototxto3/17/2015 \nMenu \ncurrent mode \nInet: \"models/b vlc reference caff e net/train_val.prototx 七\"testitor:1o00 test interval:looo bane1r:0.01 5 lr policy:\"step 6 qamma:0.1 7 stepsize: 100000 8 display: 20 9 maxiter:450000 10 momentum:0.9 11 v eight decay:0.0o05 12 snapshot:looo0 13 snapshot prefix: \"models/b vlc reference caffenet/caffenet train 14 solver mode: GPU \njupyter deploy.prototxt03/17/2015 \nFile Edit View Language \nname:\"CaffeNet\" input:\"data\" input dim:10input dim:3input dim:227input dim:227layer{ name: \"conv1\" type: \"Convolution\" bottom:\"data\" top:\"conv1\" convolution param num output:96 kernel size:11 stride:4 layer{ name: \"relul\" type: \"ReLU\" bottom:\"convl\" top:\"conv1\" layer{ \"pool1\" type: \"Pooling\" bottom:\"conv1\" top:\"pool1\" pooling param pool:MAX kernel size:3 stride:2 "}
{"page": 41, "image_path": "doc_images/catvsdogdlpycon15se-150512122612-lva1-app6891_95_41.jpg", "ocr_text": "# imports\n\n‘matplotlib inline\n\nimport logging\n\nfrom glob import glob\n\nfrom random import shuffle\nimport pickle\n\n# Make sure that caffe is on the python path:\n\ncaffe_root = '../'\n\nimport sys\nys.path.il t(@, caffe_root + 'python')\n\nimport caffe\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\n", "vlm_text": "工%matplotlib·inlineimport logging from·glob·import.glob 4 from random import shuffle 5 import·pickle 6 7 # Make sure that caffe is on the python path: 8 caffe_root·=.. 9 import sys sys.path.insert(o, caffe_root + 'python') import.caffe 2 import·numpy.as.np 4 import·matplotlib.pyplot·as plt 5 import.matplotlib.image·as mpimg "}
{"page": 42, "image_path": "doc_images/catvsdogdlpycon15se-150512122612-lva1-app6891_95_42.jpg", "ocr_text": "# load pretrained deep neural net\n\nMODEL_FILE = ‘../models/bvlc_reference_caffenet/depLloy.prototxt '\n\nPRETRAINED = '../models/bvlc_reference_caffenet/bvlc_reference_caffenet.\ncaffemodel'\n\npng_to_np(basedir, fetch_target=False):\nLogging.getLogger().setLevel( logging. INFO)\n\ncaffe.set_mode_gpu()\nnet = caffe.Classifier(MODEL_FILE, PRETRAINED,\nmean=np. Load(caffe_root + 'python/caffe/imagenet/\nilsvrc_2012_mean.npy').mean(1).mean(1),\nchannel_swap=(2,1,9),\nraw_scale=255,\nimage_dims=(256, 256) )\n\nAA\ni\\ a \\ a! ; \\ / x08 \\ / 2048 \\dense\n\\E ‘ \\ Nog \\ | /\\ \\\n‘ | | ; hs es —| |) =y) | --as rl 3 dense dense\n\\ AML | | ooo\nNu x \\ 2 128 Max 1. Ll\nnad Fan waite ae Max poaling 7°48 2048\nWor 4 \\ pooling pooling\n\n(convnet from Krizhevsky et al.'s NIPS 2012 ImageNet classification paper)\n", "vlm_text": "MoDEL_FILE·='../models/b vlc reference caff e net/deploy.prototxt PRETRAINED =.'../models/b vlc reference caff e net/b vlc reference caff e net caffemodel' \ndef png_to_np(basedir,fetch target=False): logging.getLogger().setLevel(logging.INFo) 6 caffe.set mode gpu() 广 net =-caffe.ClaSsifier(MODEL_FILE, PRETRAINED 8 mean=np.load(caffe_root +.python/caffe/imagenet/ il sv rc 2012 mean.npy').mean(1).mean(1), 9 channel swap=(2,1,0), 10 raw_scale=255 11 image_dims=（256,-256)) \nThis image depicts a diagram of a convolutional neural network (CNN) architecture. The structure includes layers such as convolutional layers with specifications (like filter sizes and depths), max pooling layers, and dense (fully connected) layers. The network probably ends with an output layer with 1000 units, which is typical for image classification tasks like those using the ImageNet dataset. Numbers on the diagram refer to dimensions of feature maps at each stage.\n(convnet from Krizhevsky et al's NIPS 2o12 ImageNet classification paper) "}
{"page": 43, "image_path": "doc_images/catvsdogdlpycon15se-150512122612-lva1-app6891_95_43.jpg", "ocr_text": "Cooking Instructions\n\n1 Load Pretrained Net\n\nconvl conv2 conv3 conv4 conv5 fc6 fc7 fc8\nee\nB 3 = e@e@\na a ——— * vurumy | @ @ @\n2 Extract features for all training images _.\n\nGi\n\na\n\n", "vlm_text": "1 Load Pre trained Net \nThe image is an illustration of a convolutional neural network (CNN) process.\n\n1. **Load Pretrained Net:** This refers to using a pre-trained CNN model. There's an icon of a cat, suggesting it's an example input image. \n   \n2. **Extract Features for All Training Images:** This shows a matrix representing features extracted from the image through different layers of the CNN. \n\n- The top part shows layers and operations like convolution and max-pooling.\n- The bottom part has a diagram showing how features are extracted from an image and stored in matrix form. \n\nThe visual suggests the steps involved in processing an image through a CNN to extract features."}
{"page": 44, "image_path": "doc_images/catvsdogdlpycon15se-150512122612-lva1-app6891_95_44.jpg", "ocr_text": "# feed image into the network and return internal feature\nrepresentation of layer fc6 demo\n\ndef activate(net, im):\ninput_image = caffe.io. load_image(im)\n# Resize the image to the standard (256, 256) and oversample net input\nSized crops.\ninput_ See ee\nit ‘data’ is the input. ‘blob name in as model definition, so we preprocess\nfor that input.\ncaffe_input = np.asarray([net.transformer.preprocess('data', in_) for in_\nin input_ pled] )\n# forward() takes keyword args for the input blobs with preprocessed input\narrays.\npredicted = net. for (data=caffe_input)\n# Aceivation of all feanvotutionat layers. and first fully connected\nfeat = net.blobs['fc6'].data[@]\nreturn feat\n\n", "vlm_text": "#feed image into the net re or k andreturn internal feature representation of layer fc 6 \ndef·activate(net,.im): 2 input image =.caffe.io.load_image（im) 3 #Resize the image to the standard (256,256） and oversample net input sized crops. 4 input over sampled = caffe.io.oversample([caffe.io.resize image(input image ,net.image_dims)], net.crop_dims) 5 #'data' is the input blob name in the model definition, so we preprocess for that input. 6 caff e input·=·np.asarray([net.transformer.preprocess('data', in_) for in_ in input over sampled]) # forward() takes keyword args for the input blobs with pre processed input arrays. 8 predicted = net.forward(data=caff e input) 9 # Activation of all convolutional layers and first fully connected 10 feat = net.blobs['fc6'].data[0] 11 return feat "}
{"page": 45, "image_path": "doc_images/catvsdogdlpycon15se-150512122612-lva1-app6891_95_45.jpg", "ocr_text": "extract features from images\n\nfeature_info = activate(net, files[@])\nfeature_count = feature_info.shape[@]\nfeature_dtype = feature_info.dtype\n\nfor n, im in enumerate(files):\ndata[n, :] = activate(net, im)\nif n % 1000 == @:\nprint ‘Reading in image’, n\n\nreturn data, target, files\n", "vlm_text": "#extract features from images \n15 feature info-=·activate（net, files[o]) 16 feature count = feature info.shape[o] 17 feature d type = feature info.dtype 18data·= np.zeros((len(files), feature count), dtype=feature d type）192 for·n,im in enumerate（files): data[n,:]·=·activate（net,·im) 21 if·n %·1000==·0: 22 print 'Reading in image',n 23 24 return data, target, files "}
{"page": 46, "image_path": "doc_images/catvsdogdlpycon15se-150512122612-lva1-app6891_95_46.jpg", "ocr_text": "#dump features as pickle file\n\nIn [*]:\n\nx, ¥, filenames = png_to_np(\n‘/mnt/pet/train/', fetch_target=True)\npickle.dump(x, open('saved_x_v2.pkl', ‘wb'))\npickle.dump(y, open('saved_y v2.pkl', ‘wb'))\npickle.dump(filenames, open('saved_filenames_v2.pkl', ‘wb'))\n\nReading in image 0\n\nReading in image 1000\nReading in image 2000\nReading in image 3000\nReading in image 4000\nReading in image 5000\nReading in image 6000\nReading in image 7000\nReading in image 8000\n\nbes)\n\ndemo\n", "vlm_text": "#dump features as pickle file \nIn[\\*]: x,Yfilenames  $=$  png_to_np( '/mnt/pet/train/',fetch target  $\\equiv$  True) pickle.dump(x,open('saved_x_v2.pkl','wb')) pickle.dump(y,open('saved_y_v2.pkl','wb')) pickle.dump（filenames,open('saved filenames v 2.pkl','wb')) \nReading in image 0 Reading in image 1000 Reading in image 2000 Reading in image 3000 Reading in image 4000 Reading in image 5000 Reading in image6000 Reading in image 7000 Reading in image 8000 \n.. "}
{"page": 47, "image_path": "doc_images/catvsdogdlpycon15se-150512122612-lva1-app6891_95_47.jpg", "ocr_text": "Cooking Instructions\n\n1 Load Pretrained Net\n\nes\ne¢\n$\n\nseeeceece\njfeeee\n\nG3, Ay 4G;\n", "vlm_text": "1 Load Pretrained Net \nThe image appears to be a diagram illustrating the process of using a pre-trained neural network (possibly a convolutional neural network, or CNN) for feature extraction. \n\n1. **Load Pretrained Net**: Shows an image of a cat being input into the network.\n2. **Convolutional Layers (conv1 to conv5)**: Demonstrates various layers with operations like convolution, max-pooling, and response normalization, reducing the size of the feature maps.\n3. **Fully Connected Layers (fc6, fc7, fc8)**: Represents dense connections with neuron counts mentioned (4096 and 1000 neurons).\n\nThis is likely part of a workflow for image recognition or classification tasks.\nThe image appears to illustrate a machine learning process involving images, features, and a neural network. \n\n1. An image is being processed to extract features, represented as a matrix with notation \\(a_{ij}\\).\n2. These features are used to train a multilayer perceptron (MLP), a type of neural network.\n3. The diagram includes nodes and layers connected in a network, with icons of a cat and a dog indicating possible classification outcomes."}
{"page": 48, "image_path": "doc_images/catvsdogdlpycon15se-150512122612-lva1-app6891_95_48.jpg", "ocr_text": "#imports\n\nfrom pylearn2.models import mlp\n\nfrom pylearn2.costs.mlp.dropout import Dropout\n\nfrom pylearn2.training_algorithms import sgd, learning_rule\nfrom pylearn2.termination_criteria import EpochCounter\n\nfrom pylearn2.datasets import DenseDesignMatrix\n\nfrom pylearn2.train import Train\nfrom pylearn2.train_extensions import best_params\nfrom pylearn2.space import VectorSpace\n\nimport pickle\nimport numpy as np\n\n", "vlm_text": "Pylearn2: \nMultilayer Perce ptr on(MLP)on topof extracted features \n#imports \n1 from·pylearn2.models·import mlp 2from pylearn2.costs.mlp.dropout import·Dropout 3 from·pylearn2.training algorithms·import·sgd,.learning rule 4 from pylearn2.termination criteria·import Epoch Counter 5 from·pylearn2.datasets·import·Dense Design Matrix 6 from pylearn2.train import Train 7 from pylearn2.train extensions import best params 8 from pylearn2.space import Vector Space 9 10 import.pickle import·numpy·as-np "}
{"page": 49, "image_path": "doc_images/catvsdogdlpycon15se-150512122612-lva1-app6891_95_49.jpg", "ocr_text": "#load earlier extracted features and labels\n#convert to input that pylearn understands\n\nx = pickle. lLoad(open( 'saved_x_v2.pkl', 'rb'))\ny = pickle. load(open('saved_y_v2.pkl', '‘rb'))\nfilenames = pickle. load(open('saved_filenames_v2.pkl', 'rb'))\n\ny = to_one_hot(y)\nin_space = VectorSpace(dim=x.shape[1] )\nfull = DenseDesignMatrix(X=x, y=y)\n\n", "vlm_text": "#load earlier extracted features and labels\n\n #convert to input that py learn understands \nx·=·pickle.load（open（'saved_x_v2.pkl','rb'))y=·pickle.load(open('saved_y_v2.pkl','rb')) 3 filenames =·pickle.load(open('saved filenames v 2.pkl',rb')) 一y·=·to_one_hot(y)5 in_space·=·Vector Space(dim=x.shape[1]) 6 full =·Dense Design Matrix（X=x, y=y) "}
{"page": 50, "image_path": "doc_images/catvsdogdlpycon15se-150512122612-lva1-app6891_95_50.jpg", "ocr_text": "11 = mlp.RectifiedLinear (le =\"11\",\n\n# create ee =12, demo\n\nlayers of\n12 = mlp.RectifiedLi ( =\"12',\nMLP mMip.RECTITLIEGLineéar : | eee\n\n. —\n# with “\n13 = mlp.RectifiedLinear ( ic ee\nsoftmax\n\nas final layer norm=.)\n\noutput = mlp.Softmax( lz ='y',\n\nlayers = [11, 12, 13, output]\n\nmdl = mlp.MLP( layers,\n\n? =in_space)\n#£ trainer\n\nr-= :\ninitialized eerie\n\nwith SGD, arn: rul =learning_ rule.Momentum(.5),\n# Remember, default dropout is .5\n\nmomentum, st=Dropout ( 5 ee\n={\"l1': }),\ndropout\n\nfy\n\n=EpochCounter(epochs),\n={\"train\": full})\n\n", "vlm_text": "#create \nlayers of MLP \n#with \nsoftmax as final layer \n#trainer initialized withSGD momentum, dropout \nThe image contains a code snippet written in Python. It appears to be configuring a multi-layer perceptron (MLP) neural network:\n\n1. **Layers**: \n   - `l1`, `l2`, and `l3` are Rectified Linear (ReLU) layers with a sparse initialization, each having a dimension of 5000 and a `max_col_norm` of 1.0.\n   - `output` is a Softmax layer for two classes.\n\n2. **Model Layers**:\n   - The layers are combined into a list: `[l1, l2, l3, output]`.\n\n3. **Model Initialization**:\n   - The model `mdl` is created from the layers using `mlp.MLP`.\n\n4. **Training Configuration**:\n   - Learning rate `lr` is set to 0.0001.\n   - Number of epochs is 100.\n   - Trainer is set up using stochastic gradient descent (SGD) with a learning rule and batch size of 128.\n\n5. **Dropout and Regularization**:\n   - Dropout is used with specified input inclusion probabilities.\n   - An epoch counter termination criterion is set, monitoring the training dataset.\n\nThis code is likely part of a machine learning training script."}
{"page": 51, "image_path": "doc_images/catvsdogdlpycon15se-150512122612-lva1-app6891_95_51.jpg", "ocr_text": "# train/test splits\n\n# no sklearn.cross_validation > train_test_spLit\n\n# own test/train split so we can also Link filenames\n\nsplitter = round(len(x)*®.8)\n\nX_train, X_test = x[:splitter],x[splitter:]\n\ny_train, y_test = y[:splitter],y[splitter: ]\n\nfilenames_train, filenames_test = filenames[:splitter], filenames [splitter: ]\n\npickle.dump(X_train, open('saved_feat_x_train_v2.pkL', ‘wb'))\npickle.dump(X_test, open('saved_feat_x_test_v2.pkLl', 'wb'))\nle.dump(y_train, open('saved_feat_y_train_v2.pkl', ‘'wb'))\ndump(y_test, open('saved_feat_y_test_v2.pkL', 'wb'))\npickle.dump(filenames_train, open('saved_feat_filenames_train_v2.pkl', ‘wb'))\npickle.dump(filenames_test, open('saved_feat_filenames_test_v2.pkl', ‘wb'))\n\n", "vlm_text": "1 # no sklearn.cross validation > train test split 2 # own test/train split so we can also link filenames 3 splitter= round（len(x)\\*0.8) 4X train,X test·=·x[:splitter],x[splitter:] 5 y_train,y_test =y[:splitter],y[splitter:] 6 filenames train, filenames test = filenames[:splitter],filenames[splitter:] 7 8pickle.dump(x_train,open('saved feat x train v 2.pkl',-wb')) 9 pickle.dump(x_test,open('saved feat x test v 2.pkl',.'wb')) 10 pickle.dump(y_train,open('saved feat y train v 2.pkl','wb')) 1 pickle.dump(y_test,open('saved feat y test v 2.pkl','wb')) 12 pickle.dump（filenames train, open('saved feat filenames train v 2.pkl','wb')）3 pickle.dump(filenames test,open('saved feat filenames test v 2.pkl','wb'))"}
{"page": 52, "image_path": "doc_images/catvsdogdlpycon15se-150512122612-lva1-app6891_95_52.jpg", "ocr_text": "#start our MLP (pylearn experiment method)\n\n#Liftoff!\ntrn = DenseDesignMatrix(X=X_train, y=y_train)\ntst = DenseDesignMatrix(X=X_test, y=y_test)\n\ntrainer.monitoring_dataset={ tst,\ntrai trn}\n\nexperiment.main_lLoop()\n\nIn [*]: trn = DenseDesignMatrix(X=xX_train, y=y_train)\ntet = DenseDesignMatrix(X=K_test, y=y_test)\ntrainer.monitoring_dataset={'valid’: tat,\n\n‘train’: trn}\nexperiment.main_loop()\n\nParameter and initial learning rate summary:\n\nll_W: 9.99999974738e-05\nll_b: 9.99999974738e-05\n12_W: 9.99999974738e-05\n12_b: 9.99999974738e-05\n13_W: 9.99999974738e-05\n13_b: 9.99999974738e-05\n\nsoftmax_b: 9.99999974738e-05\n\nsoftmax_W: 9.99999974738e-05\nCompiling sgd_update...\nCompiling sgd_update done. Time elapsed: 1.696666 seconds\ncompiling begin_record_entry...\ncompiling begin_record_entry done. Time elapsed: 0.548132 seconds\nMonitored channels:\n\nlearning_rate\n\nmomentum\n\ntotal seconds _last_epoch\n\n(...)\nalready after 5 min: valid_y misclass: 0.0619999989867\n", "vlm_text": "#start our MLP (pylearn experiment metbod) \n#liftoff! trn =·Dense Design Matrix（X=X_train, y=y_train) 3 tst =·Dense Design Matrix(X=X_test,y=y_test) trainer.monitoring data set='valid': tst, 'train':-trn} experiment.main_loop() \nIn [\\*]:trn  $=$   $\\tt X{=}{\\tt X}$  tst  $=$  Dense Design Matrix(  $\\tt X{=}{\\tt X}$  test,y-y_test) trainer.monitoring data set={'valid':tst, 'train':trn} experiment.main_loop() Parameter and initial learning rate summary: 11W:9.99999974738e-05 11_b:9.99999974738e-05 12_W:9.99999974738e-05 12_b:9.99999974738e-05 13_W:9.99999974738e-05 13 b:9.99999974738e-05 softmax b:9.99999974738e-05 softmax_W:9.99999974738e-05 Compiling sgd_update... Compiling sgd updated one.Time elapsed:1.686666 seconds compiling begin record entry... compiling begin record entry done.Time elapsed:0.548132 seconds Monitored channels: learning rate momentum total seconds last epoch "}
{"page": 53, "image_path": "doc_images/catvsdogdlpycon15se-150512122612-lva1-app6891_95_53.jpg", "ocr_text": "In [319]: | plt.plot(score_train[2:])\nplt.plot(score test[2:])\n\nOut[319]: [<matplotlib.lines.Line2D at 0x7£517c694450>]\naccuracy\n\n50% 05\n0.4\n0.3\n\n0.2\n\n90% 01\n\n1) 20 40 60 80 100\n1 hour\n", "vlm_text": "In[319]：\nplt.plot(score train[2:])plt.plot(score_test[2:]) \nOut[319]:[<matplotlib.lines.Line 2 Dat 0 x 7 f 517 c 694450>] \nThe image shows a line graph. The x-axis ranges from 0 to 100, and the y-axis has values between 0.0 and 0.5. Several lines are plotted, showing a trend that starts high and decreases rapidly, then levels out. There are also labels in a stylized font indicating \"50%\" at the top left and \"90%\" at the bottom left, and the title \"accuracy\" at the top center."}
{"page": 54, "image_path": "doc_images/catvsdogdlpycon15se-150512122612-lva1-app6891_95_54.jpg", "ocr_text": "In [319]: | plt.plot(score_train[2:])\nplt.plot(score test[2:])\n\nOut[319]: [<matplotlib.lines.Line2D at 0x7£517c694450>]\n\nr\n_ accuracy\n\n94% 0.060\n0.055\n0.050\n0.045\n0.040\n0.035\n97% 0.030 (...)\n\n0.025\n0 20 40 60 80 100\n\nstart at iteration #2 1 hour\n", "vlm_text": "In [319]: \nplt.plot(score train[2:])plt.plot(score_test[2:]) \nOut[319]:[<matplotlib.lines.Line 2 Dat 0 x 7 f 517 c 694450>] \nThe image is a graph showing some form of data over the range from 0 to 100 on the x-axis. The y-axis ranges from 0.025 to 0.065. Two lines are plotted, one in green and one in blue, both generally decreasing over the x-axis range. There are two percentages, 94% and 97%, located on the left side, but their context is not clear. The word \"accuracy\" is located at the top, suggesting the graph might relate to model performance accuracy over iterations or epochs.\nstart at iteration#2 "}
{"page": 55, "image_path": "doc_images/catvsdogdlpycon15se-150512122612-lva1-app6891_95_55.jpg", "ocr_text": "In [380]: input image = caffe.io.load_ image(‘'google-glasses-—cat-2.jpg')\nplt.imshow(input_image)\n\nOut[380]: <matplotlib.image.AxesImage at 0x7£51a017d410>\n\n0\n100\n200\n300\n\n400\n\nIn [381]: feat = getfeat_single_image('google-glasses-cat-2.jpg') #run image through cnn\n\nx = feat\ny = £([*]) #run feature through DBN > out: prediction\nif y:\nprint \"WOOF!\"\nelse:\n\nprint \"MEOW!\"\n\nMEOW!\n", "vlm_text": "In[380]: input image  $=$  caffe.io.load_image('google-glasses-cat-2.jpg') plt.imshow(input image) \nOut[380]:<matplotlib.image.Axes Image at 0 x 7 f 51 a 017 d 410> \nThe image shows a cat wearing round, oversized sunglasses. The photo is black and white, with a focus on the cat's face. The image also has a grid overlay with axes typically seen in graphical plots, indicating pixel dimensions.\nIn[381]: feat  $=$  get feat single image('google-glasses-cat-2.jpg')#run image through cnn  ${\\bf x}=$  feat  $\\textbf{y}=\\textbf{f}(~[\\mathbf{x}])$  #runfeature throughDBN  $>$  out: prediction ify: print \"WOOF!\" else: print \"MEOW!\" MEOW! "}
{"page": 56, "image_path": "doc_images/catvsdogdlpycon15se-150512122612-lva1-app6891_95_56.jpg", "ocr_text": "", "vlm_text": "So are You more like a Dogor Cat? \nThis image shows a cat and a dog facing each other. The background is a green and yellow checkerboard pattern. The text under the cat and dog reads \"CAT vs DOG.\"\nDETECTOR "}
{"page": 57, "image_path": "doc_images/catvsdogdlpycon15se-150512122612-lva1-app6891_95_57.jpg", "ocr_text": "€& > C5} pyeon:8005/index.htm!\n\nWhat\nabout\nme?\n\nAT or DOG, that's the question...\n\nwebrtc/w de adapted from quizduell\n\nRequires a webcam and a modern browser with WebRTC support such as Firefox or Chrome. Bui\n", "vlm_text": "What about me? \nCAT or DoG,that's the question... \nThe image shows a person with glasses wearing a hoodie, surrounded by other people in what appears to be an indoor setting. There is a computer screen or digital element at the bottom showing text related to WebRTC and a prompt to \"Share Snapshot.\" It seems to be a screenshot of a webcam capture during an event or meeting.\nI might put it up as a Flask site online,if people are interested?) "}
{"page": 58, "image_path": "doc_images/catvsdogdlpycon15se-150512122612-lva1-app6891_95_58.jpg", "ocr_text": "imgur = © uploadimages vy 3\n\nshare\n\nle\n\n", "vlm_text": "share \nThe image shows a person with glasses and long hair, wearing a hoodie. They appear to be in a seated area, possibly at an indoor event or meeting, with several other people visible in the background. The setting looks like it could be a conference or lecture hall."}
{"page": 59, "image_path": "doc_images/catvsdogdlpycon15se-150512122612-lva1-app6891_95_59.jpg", "ocr_text": "In [391]: !wget http://i.imgur.com/oMJyDO0. jpg\n\n--2015-05-12 12:12:00-- http://i.imgur.com/oMJyDO0. jpg\n\nResolving i.imgur.com (i.imgur.com)... 199.27.76.193\n\nConnecting to i.imgur.com (i.imgur.com) |199.27.76.193|:80... connected.\nHTTP request sent, awaiting response... 200 OK\n\nLength: 58456 (57K) [image/jpeg]\n\nSaving to: 'oMJyDOO.jpg'\n\n100% [ S*ssssssssssessessssessssessssssesssss>)] 58,456 m=, =K/s in 0.028\n\n2015-05-12 12:12:00 (2.97 MB/s) - ‘oMJyDO0.jpg' saved [(58456/58456)\n\nIn [393]: input_image = caffe.io.load_image('oMJyDO0.jpg')\nplt.imshow(input_image)\n\nOut[393]: <matplotlib.image.AxesImage at 0x7f5laefe5e50>\n\n0\n\n100\n\n200\n\n300\n\n400\n\n0 100 200 300 400 500 600\n", "vlm_text": "--2015-05-1212:12:00--http://i.imgur.com/oMJyD00.jpg Resolving i.imgur.com（i.imgur.com)...199.27.76.193 Connecting to i.imgur.com（i.imgur.com)|199.27.76.193|:80...connected. HTTP request sent,awaiting response...2oo oK Length:58456(57K)[image/jpeg] Savingto:'oMJyDoo.jpg \n1008 ===>]58,456 ==.-K/s in0.02s 2015-05-1212:12:00（2.97MB/s)-oMJyD00.jPg'SaVed[58456/58456] \n\nIn[393]:input image  $=$  caffe.io.load_image('oMJyDo0.jpg') plt.imshow(input image) \nThe image shows a person with long hair and glasses in the foreground, looking at the camera. There is another person with glasses in the background. The setting appears to be indoors, possibly a conference or lecture hall, given the seated arrangement and ceiling lights. There are axis labels along the sides, suggesting the image might be part of a plot or graph from a data visualization tool like Matplotlib."}
{"page": 60, "image_path": "doc_images/catvsdogdlpycon15se-150512122612-lva1-app6891_95_60.jpg", "ocr_text": "In [395]: feat = getfeat_single image('rQ4bKra.jpg') #run image 1\n\nx = feat\ny = £([x]) #run feature through DBN > out: prediction\nif y:\nprint \"WOOF I'm a Dog!\"\nelse:\n\nprint \"MEOW I'm a Cat!\"\n\nWOOF I'm a Dog!\n", "vlm_text": "In[395]：\nfeat  $\\equiv$  get feat single image('rQ4bKra.jpg') #run image  ${\\textbf{x}}=$  feat Y=±([x]) #run feature through DBN > out: prediction if y: print \"wooFI'maDog! else: print\"MEOwI'maCat!\" \nWOOFI'maDog! "}
{"page": 61, "image_path": "doc_images/catvsdogdlpycon15se-150512122612-lva1-app6891_95_61.jpg", "ocr_text": "In [395]: feat = getfeat_single image('rQ4bKra.jpg') #run image 1\n\nx = feat\ny = £([x]) #run feature through DBN > out: prediction\nif y:\nprint \"WOOF I'm a Dog!\"\nelse:\n\nprint \"MEOW I'm a Cat!\"\n\nWOOF I'm a Dog!\n\n", "vlm_text": "feat  $\\equiv$  get feat single image('rQ4bKra.jpg')#run image  ${\\textbf{x}}=$  feat \nY= f([x]) #run feature through DBN > out: prediction ify：print\"wooF I'maDog!\" else: print\"MEOWI'maCat!\" \nWOOFI'maDog! \nThe image features a comparison between a cat and a dog. The left side has an image of a cat's profile with a large red \"X\" over it, and the word \"CAT\" below. The right side shows a dog's profile with a green checkmark on it, accompanied by the word \"DOG.\" The background is a green and yellow geometric pattern. The text \"CAT vs DOG\" is centrally positioned."}
{"page": 62, "image_path": "doc_images/catvsdogdlpycon15se-150512122612-lva1-app6891_95_62.jpg", "ocr_text": "blirk.net\n", "vlm_text": "The image shows a painting of a cat and a dog lying close together, partially covered by fabric. At the top, blue text reads \"THATS ALL!\""}
{"page": 63, "image_path": "doc_images/catvsdogdlpycon15se-150512122612-lva1-app6891_95_63.jpg", "ocr_text": "(n Touch!\n\nAcademic/Research Data Science Consultancy\n\nas PhD candidate KTH/CSC: Gve Systems\n“Always interested in discussing Graph Technologies\nMachine Learning, Deep\nArchitectures, Graphs, and\nLanguage Technology”\n\ng pt,\nPTH\n\ni ee KONE wy\n\nsca\n\nROYAL INSTITUTE\nOF TECHNOLOGY\n\nMACHINE\nLEARNING\n\nPOWER TO THE VATA\n\nroelof@kth.se\nwww.csc.kth.se/~roelof/\n\nroelof@graph-systems.com\nwww.graph-technologies.com\n", "vlm_text": "In Touch! \nAcademic/Research \nData Science Consultancy \nGveSystems Graph Technologies \nas PhD candidate KTH/CSC: Always interested in discussing Machine Learning,Deep Architectures,Graphs,and Language Technology\" \nThe image features a stylized design of three raised fists, each surrounded by stars. Below the fists, the text reads \"MACHINE LEARNING\" and \"POWER TO THE DATA.\" The overall color scheme is red and white, giving it a bold, poster-like appearance.\nThe image appears to be an artistic, stylized portrait of a person with glasses. The image uses a sepia or monochromatic color scheme and has a textured, possibly halftone effect.\nThe image shows the logo of the Royal Institute of Technology, also known as KTH. The logo features a crown above the letters \"KTH,\" with a wreath encircling the text \"VETENSKAP OCH KONST,\" which translates to \"Science and Art\" in English. The background is blue.\nroelof@kth.se www.csc.kth.se/\\~roelof/ \nroelof@graph-systems.com www.graph-technologies.com "}
{"page": 64, "image_path": "doc_images/catvsdogdlpycon15se-150512122612-lva1-app6891_95_64.jpg", "ocr_text": "Wanna Know More?\n\nStockholm, Welcome! What's new\nSweden\n\nFounded Feb 21, 2015 * SCHEDULE A NEW MEETUP\n\nAbout us.. 4 Past\n\nDatamaniacs\n\nDeep Learning for Bioinformatics\n\nne An@Gennc 2\n\nPast Meetups\n\n103 Datamaniacs\nOur calendar\n\nAgenda: « 18:00 - 18:15 i rand get ready to\nOrganizer: rumble... - 18:15 - 18 30 “1 9:00 Roelof |\nRoelof Deep Learning, a birds eye view « 19:00 -... LEARN MORE\n\nPieters\n\nKickoff! Deep Learning: Revolution or Hype in Data-\n\nScience ?\nWe're about: MORE\n\nBig Data Analytics AD. ReBbasan\n\nArtificial Intelligence -\n\n1 i S tefan Ay n\nOpen Source - Software 144 Datamaniacs | 35 Photos ahi vesand Pi\n\n@ NEW MEMBER\n\nDevelopment - New es a bn -\nr a uy where ( this front\nTechnology - Big Data Deep Learning is kicking off everywhere (see this front page\n\narticle in the New York Times example)! There is good reas\nNatural Language article in the New York es for example)! There is good reason a. NEW MEMBER\n\nProcessing - Machine to be excited about deep learning, as it's... LEARN MORE Mans Magnusson E.\n=\n\nLearming - Data joined\n\n", "vlm_text": "Wanna Know More? \nbit.ly/SthlmDL \nThe image shows a webpage for the Stockholm Deep Learning Meetup. It includes details like:\n\n- **Location**: Stockholm, Sweden (founded February 21, 2015)\n- **Organizer**: Roelof Pieters\n- **Member Count**: 295 Datamaniacs\n- **Past Event**: \"Deep Learning for Bioinformatics\" on April 20 at 6:00 PM\n- **Agenda**: Starts with grabbing a coffee/beer, followed by a welcome speech and a talk by Roelof Pieters\n- **Another Past Event**: \"Kickoff! Deep Learning: Revolution or Hype in Data-Science?\" on March 10 at 6:00 PM\n- **Side Panel**: Features new members and images from presentations\n\nThe page also highlights topics of interest such as Big Data Analytics, Artificial Intelligence, and Machine Learning."}
{"page": 65, "image_path": "doc_images/catvsdogdlpycon15se-150512122612-lva1-app6891_95_65.jpg", "ocr_text": "Wanna Play ? General Deep Learning\n\n- Theano - CPU/GPU symbolic expression compiler in\npython (from LISA lab at University of Montreal).\n\n- Pylearn2 - library designed to make machine learning\nresearch easy. http://deeplearning.net/software/\npylearn2/\n\n- Torch - Matlab-like environment for state-of-the-art\nmachine learning algorithms in lua (from Ronan\nCollobert, Clement Farabet and Koray Kavukcuoglu)\nhttp://torch.ch/\n\n- more info: http://deeplearning.net/software links/\n", "vlm_text": "WannaPlay?General Deep Learning \nTheano - CPU/GPU symbolic expression compiler in python (from LISA lab at University of Montreal). http://deep learning.net/software/theano/ \nPylearn2 - library designed to make machine learning research easy.http://deep learning.net/software/ pylearn2/ \n:Torch-Matlab-like environment for state-of-the-art machine learning algorithms in lua (fromRonan Collobert, Clement Farabet and Koray Ka vuk cuo g lu http://torch.ch/ \nmoreinfo:http://deep learning.net/software links/ "}
{"page": 66, "image_path": "doc_images/catvsdogdlpycon15se-150512122612-lva1-app6891_95_66.jpg", "ocr_text": "Wanna Play ? NLP\n\n* RNNLM (Mikolov)\n\nhttp://rnnlm.org\n\n> NB-SVM\n\nhttps://sithub.com/mesniler/nbsvm\n\n* Word2Vec (skipgrams/cbow)\n\nhttps://code.google.com/p/word2vec/ (original)\nhttp://radimrehurek.com/gensim/models/word2vec.html (python)\n\n* GloVe\n\nhttp://nlp.stanford.edu/projects/glove/ (original)\nhttps://github.com/maciejkula/glove-python (python)\n\n* Socher et al / Stanford RNN Sentiment code:\n\nhttp://nlp.stanford.edu/sentiment/code.html\n\n- Deep Learning without Magic Tutorial:\nhttp://nlp.stanford.edu/courses/NAACL2013/\n\n67\n", "vlm_text": "Wanna Play ?NLP \nRNNLM (Mikolov) http://rnnlm.org \nNB-SVM https://github.com/mesnilgr/nbsvm \nWord2Vec(skipgrams/cbow) https://code.google.com/p/word2vec/(original) http://radi mre hur ek.com/gensim/models/word2vec.html(python) \nGloVe http://nlp.stanford.edu/projects/glove/(original) https://github.com/maciejkula/glove-python(python) \nSocheretal/Stanford RN N Sentiment code: http://nlp.stanford.edu/sentiment/code.html \nDeep Learning without Magic Tutorial: http://nlp.stanford.edu/courses/NAACL2o13/ "}
{"page": 67, "image_path": "doc_images/catvsdogdlpycon15se-150512122612-lva1-app6891_95_67.jpg", "ocr_text": "Wanna Play ? Computer Vision\n\ncuda-convnetz (Alex Krizhevsky, Toronto) (c++/\nCUDA, optimized for GTX 580)\n\nhttps://code.google.com/p/cuda-convnet2/\n\n* Caffe (Berkeley) (Cuda/OpenCL, Theano, Python)\nhttp://cafte.berkeleyvision.org/\n\n* OverFeat (NYU)\nhttp://cilvr.nyu.edu/doku.php?id=code:start\n", "vlm_text": "Wanna Play ? Computer Uision \ncuda-convnet2 (Alex Krizhevsky, Toronto) (c++/ CUDA,optimized for GTX58o) https://code.google.com/p/cuda-convnet2/ \nCaffe (Berkeley) (Cuda/OpenCL, Theano, Python) http://caffe.berkeley vision.org/ \nOverFeat(NYU)http://cilvr.nyu.edu/doku.php?id=code:start "}
