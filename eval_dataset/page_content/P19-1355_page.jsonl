{"page": 0, "image_path": "doc_images/P19-1355_0.jpg", "ocr_text": "Energy and Policy Considerations for Deep Learning in NLP\n\nEmma Strubell\n\nAnanya Ganesh\n\nAndrew McCallum\n\nCollege of Information and Computer Sciences\nUniversity of Massachusetts Amherst\n{strubell, aganesh, mccallum}@cs.umass.edu\n\nAbstract\n\nRecent progress in hardware and methodol-\nogy for training neural networks has ushered\nin a new generation of large networks trained\non abundant data. These models have ob-\ntained notable gains in accuracy across many\nNLP tasks. However, these accuracy improve-\nments depend on the availability of exception-\nally large computational resources that neces-\nsitate similarly substantial energy consump-\ntion. As a result these models are costly to\ntrain and develop, both financially, due to the\ncost of hardware and electricity or cloud com-\npute time, and environmentally, due to the car-\nbon footprint required to fuel modern tensor\nprocessing hardware. In this paper we bring\nthis issue to the attention of NLP researchers\nby quantifying the approximate financial and\nenvironmental costs of training a variety of re-\ncently successful neural network models for\nNLP. Based on these findings, we propose ac-\ntionable recommendations to reduce costs and\nimprove equity in NLP research and practice.\n\n1 Introduction\n\nAdvances in techniques and hardware for train-\ning deep neural networks have recently en-\nabled impressive accuracy improvements across\nmany fundamental NLP tasks (Bahdanau et al.,\n2015; Luong et al., 2015; Dozat and Man-\nning, 2017; Vaswani et al., 2017), with the\nmost computationally-hungry models obtaining\nthe highest scores (Peters et al., 2018; Devlin et al.,\n2019; Radford et al., 2019; So et al., 2019). As\na result, training a state-of-the-art model now re-\nquires substantial computational resources which\ndemand considerable energy, along with the as-\nsociated financial and environmental costs. Re-\nsearch and development of new models multiplies\nthese costs by thousands of times by requiring re-\ntraining to experiment with model architectures\nand hyperparameters. Whereas a decade ago most\n\nConsumption COze (Ibs)\nAir travel, 1 person, NY<+SF 1984\nHuman life, avg, 1 year 11,023\nAmerican life, avg, | year 36,156\nCar, avg incl. fuel, 1 lifetime 126,000\nTraining one model (GPU)\nNLP pipeline (parsing, SRL) 39\nw/ tuning & experiments 78,468\nTransformer (big) 192\nw/ neural arch. search 626,155\n\nTable 1: Estimated CO, emissions from training com-\nmon NLP models, compared to familiar consumption.!\n\nNLP models could be trained and developed on\na commodity laptop or server, many now require\nmultiple instances of specialized hardware such as\nGPUs or TPUs, therefore limiting access to these\nhighly accurate models on the basis of finances.\nEven when these expensive computational re-\nsources are available, model training also incurs a\nsubstantial cost to the environment due to the en-\nergy required to power this hardware for weeks or\nmonths at a time. Though some of this energy may\ncome from renewable or carbon credit-offset re-\nsources, the high energy demands of these models\nare still a concern since (1) energy is not currently\nderived from carbon-neural sources in many loca-\ntions, and (2) when renewable energy is available,\nit is still limited to the equipment we have to pro-\nduce and store it, and energy spent training a neu-\nral network might better be allocated to heating a\nfamily’s home. It is estimated that we must cut\ncarbon emissions by half over the next decade to\ndeter escalating rates of natural disaster, and based\non the estimated CO emissions listed in Table 1,\n‘Sources: (1) Air travel and per-capita consumption:\n\nhttps://bit.ly/2HwOxWe; (2) car lifetime: https:\n//bit.1y/2Qbrowl.\n\n3645\n\nProceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3645-3650\nFlorence, Italy, July 28 - August 2, 2019. ©2019 Association for Computational Linguistics\n", "vlm_text": "Energy and Policy Considerations for Deep Learning in NLP \nEmma Strubell Ananya Ganesh Andrew McCallum College of Information and Computer Sciences University of Massachusetts Amherst { strubell, aganesh, mccallum } @cs.umass.edu \nAbstract \nRecent progress in hardware and methodol- ogy for training neural networks has ushered in a new generation of large networks trained on abundant data. These models have ob- tained notable gains in accuracy across many NLP tasks. However, these accuracy improve- ments depend on the availability of exception- ally large computational resources that neces- sitate similarly substantial energy consump- tion. As a result these models are costly to train and develop, both ﬁnancially, due to the cost of hardware and electricity or cloud com- pute time, and environmentally, due to the car- bon footprint required to fuel modern tensor processing hardware. In this paper we bring this issue to the attention of NLP researchers by quantifying the approximate ﬁnancial and environmental costs of training a variety of re- cently successful neural network models for NLP. Based on these ﬁndings, we propose ac- tionable recommendations to reduce costs and improve equity in NLP research and practice. \n1 Introduction \nAdvances in techniques and hardware for train- ing deep neural networks have recently en- abled impressive accuracy improvements across many fundamental NLP tasks ( Bahdanau et al. , 2015 ;  Luong et al. ,  2015 ;  Dozat and Man- ning ,  2017 ;  Vaswani et al. ,  2017 ), with the most computationally-hungry models obtaining the highest scores ( Peters et al. ,  2018 ;  Devlin et al. , 2019 ;  Radford et al. ,  2019 ;  So et al. ,  2019 ). As a result, training a state-of-the-art model now re- quires substantial computational resources which demand considerable energy, along with the as- sociated ﬁnancial and environmental costs. Re- search and development of new models multiplies these costs by thousands of times by requiring re- training to experiment with model architectures and hyperparameters. Whereas a decade ago most \nThe table provides data on the carbon dioxide equivalent (CO₂e) emissions associated with different types of consumption. It lists four types of consumption with their corresponding CO₂e emissions measured in pounds (lbs):\n\n1. Air travel for one person between New York (NY) and San Francisco (SF) and back results in CO₂e emissions of 1,984 lbs.\n2. An average human life over one year is responsible for 11,023 lbs of CO₂e emissions.\n3. An average American life over one year produces 36,156 lbs of CO₂e emissions.\n4. The average emissions from a car, including fuel, over its lifetime amount to 126,000 lbs of CO₂e.\nThe table presents data related to natural language processing (NLP) tasks, specifically focusing on two main components: an NLP pipeline and a Transformer model. Each component has two associated metrics:\n\n1. **NLP Pipeline (parsing, SRL):** This likely refers to a traditional NLP approach involving parsing and semantic role labeling (SRL).\n   - The first value, \"39,\" might represent a baseline metric such as the number of parameters, iterations, or computational units involved in processing.\n   - The second value, \"78,468,\" corresponds to the same NLP pipeline with additional tuning and experiments, suggesting a substantial increase in resource usage or output.\n\n2. **Transformer (big):** This is indicative of a Transformer model, which is a type of neural network architecture commonly used in NLP tasks.\n   - The first value, \"192,\" could represent an initial metric related to this model's configuration or performance.\n   - The second value, \"626,155,\" pertains to the same Transformer model with neural architecture search applied, which likely implies an increase in complexity or performance optimization.\n\nOverall, the table captures the difference in resource metrics or performance indicators between a basic configuration and an optimized version of each model.\nTable 1: Estimated   $\\mathrm{CO_{2}}$   emissions from training com- mon NLP models, compared to familiar consumption. \nNLP models could be trained and developed on a commodity laptop or server, many now require multiple instances of specialized hardware such as GPUs or TPUs, therefore limiting access to these highly accurate models on the basis of ﬁnances. \nEven when these expensive computational re- sources are available, model training also incurs a substantial cost to the environment due to the en- ergy required to power this hardware for weeks or months at a time. Though some of this energy may come from renewable or carbon credit-offset re- sources, the high energy demands of these models are still a concern since (1) energy is not currently derived from carbon-neural sources in many loca- tions, and (2) when renewable energy is available, it is still limited to the equipment we have to pro- duce and store it, and energy spent training a neu- ral network might better be allocated to heating a family’s home. It is estimated that we must cut carbon emissions by half over the next decade to deter escalating rates of natural disaster, and based on the estimated  $\\mathrm{CO_{2}}$   emissions listed in Table  1 , model training and development likely make up a substantial portion of the greenhouse gas emis- sions attributed to many NLP researchers. "}
{"page": 1, "image_path": "doc_images/P19-1355_1.jpg", "ocr_text": "model training and development likely make up\na substantial portion of the greenhouse gas emis-\nsions attributed to many NLP researchers.\n\nTo heighten the awareness of the NLP commu-\nnity to this issue and promote mindful practice and\npolicy, we characterize the dollar cost and carbon\nemissions that result from training the neural net-\nworks at the core of many state-of-the-art NLP\nmodels. We do this by estimating the kilowatts\nof energy required to train a variety of popular\noff-the-shelf NLP models, which can be converted\nto approximate carbon emissions and electricity\ncosts. To estimate the even greater resources re-\nquired to transfer an existing model to a new task\nor develop new models, we perform a case study\nof the full computational resources required for the\ndevelopment and tuning of a recent state-of-the-art\nNLP pipeline (Strubell et al., 2018). We conclude\nwith recommendations to the community based on\nour findings, namely: (1) Time to retrain and sen-\nsitivity to hyperparameters should be reported for\nNLP machine learning models; (2) academic re-\nsearchers need equitable access to computational\nresources; and (3) researchers should prioritize de-\nveloping efficient models and hardware.\n\n2 Methods\n\nTo quantify the computational and environmen-\ntal cost of training deep neural network mod-\nels for NLP, we perform an analysis of the en-\nergy required to train a variety of popular off-\nthe-shelf NLP models, as well as a case study of\nthe complete sum of resources required to develop\nLISA (Strubell et al., 2018), a state-of-the-art NLP\nmodel from EMNLP 2018, including all tuning\nand experimentation.\n\nWe measure energy use as follows. We train the\nmodels described in §2.1 using the default settings\nprovided, and sample GPU and CPU power con-\nsumption during training. Each model was trained\nfor a maximum of | day. We train all models on\na single NVIDIA Titan X GPU, with the excep-\ntion of ELMo which was trained on 3 NVIDIA\nGTX 1080 Ti GPUs. While training, we repeat-\nedly query the NVIDIA System Management In-\nterface? to sample the GPU power consumption\nand report the average over all samples. To sample\nCPU power consumption, we use Intel’s Running\nAverage Power Limit interface.*\n\n*nvidia-smi: https://bit.ly/30sGEbi\n3RAPL power meter: https: //bit.1ly/2LObQhV\n\nConsumer Renew. Gas Coal Nuc.\nChina 22% 3% 65% 4%\nGermany 40% 7% 38% 13%\nUnited States 17% 35% 27% 19%\nAmazon-AWS 17% 24% 30% 26%\nGoogle 56% 14% 15% 10%\nMicrosoft 32% 23% 31% 10%\n\nTable 2: Percent energy sourced from: Renewable (e.g.\nhydro, solar, wind), natural gas, coal and nuclear for\nthe top 3 cloud compute providers (Cook et al., 2017),\ncompared to the United States,* China> and Germany\n(Burger, 2019).\n\nWe estimate the total time expected for mod-\nels to train to completion using training times and\nhardware reported in the original papers. We then\ncalculate the power consumption in kilowatt-hours\n(kWh) as follows. Let p, be the average power\ndraw (in watts) from all CPU sockets during train-\ning, let p, be the average power draw from all\nDRAM (main memory) sockets, let p, be the aver-\nage power draw of a GPU during training, and let\ng be the number of GPUs used to train. We esti-\nmate total power consumption as combined GPU,\nCPU and DRAM consumption, then multiply this\nby Power Usage Effectiveness (PUE), which ac-\ncounts for the additional energy required to sup-\nport the compute infrastructure (mainly cooling).\nWe use a PUE coefficient of 1.58, the 2018 global\naverage for data centers (Ascierto, 2018). Then the\ntotal power jp; required at a given instance during\ntraining is given by:\n\npy = BeBe + Pr + 9Po)\n1000\n\nThe U.S. Environmental Protection Agency (EPA)\n\nprovides average CO, produced (in pounds per\n\nkilowatt-hour) for power consumed in the U.S.\n\n(EPA, 2018), which we use to convert power to\n\nestimated CO2 emissions:\n\nqd)\n\nCOxe = 0.954p; (2)\n\nThis conversion takes into account the relative pro-\nportions of different energy sources (primarily nat-\nural gas, coal, nuclear and renewable) consumed\nto produce energy in the United States. Table 2\nlists the relative energy sources for China, Ger-\nmany and the United States compared to the top\n5U.S. Dept. of Energy: https: //bit.1ly/2JTbGnI\nChina Electricity Council; trans. China Energy Portal:\nhttps://bit.ly/2QHE503\n\n3646\n", "vlm_text": "\nTo heighten the awareness of the NLP commu- nity to this issue and promote mindful practice and policy, we characterize the dollar cost and carbon emissions that result from training the neural net- works at the core of many state-of-the-art NLP models. We do this by estimating the kilowatts of energy required to train a variety of popular off-the-shelf NLP models, which can be converted to approximate carbon emissions and electricity costs. To estimate the even greater resources re- quired to transfer an existing model to a new task or develop new models, we perform a case study of the full computational resources required for the development and tuning of a recent state-of-the-art NLP pipeline ( Strubell et al. ,  2018 ). We conclude with recommendations to the community based on our ﬁndings, namely: (1) Time to retrain and sen- sitivity to hyperparameters should be reported for NLP machine learning models; (2) academic re- searchers need equitable access to computational resources; and (3) researchers should prioritize de- veloping efﬁcient models and hardware. \n2 Methods \nTo quantify the computational and environmen- tal cost of training deep neural network mod- els for NLP, we perform an analysis of the en- ergy required to train a variety of popular off- the-shelf NLP models, as well as a case study of the complete sum of resources required to develop LISA ( Strubell et al. ,  2018 ), a state-of-the-art NLP model from EMNLP 2018, including all tuning and experimentation. \nWe measure energy use as follows. We train the models described in  $\\S2.1$   using the default settings provided, and sample GPU and CPU power con- sumption during training. Each model was trained for a maximum of 1 day. We train all models on a single NVIDIA Titan X GPU, with the excep- tion of ELMo which was trained on 3 NVIDIA GTX 1080 Ti GPUs. While training, we repeat- edly query the NVIDIA System Management In- terface 2   to sample the GPU power consumption and report the average over all samples. To sample CPU power consumption, we use Intel’s Running Average Power Limit interface. \nThe table shows the energy consumption distribution for various consumers, divided into four types of energy sources: Renewables, Gas, Coal, and Nuclear. The consumers include three countries (China, Germany, United States) and three companies (Amazon-AWS, Google, Microsoft). Here's the percentage breakdown:\n\n- **China:**\n  - Renewables: 22%\n  - Gas: 3%\n  - Coal: 65%\n  - Nuclear: 4%\n\n- **Germany:**\n  - Renewables: 40%\n  - Gas: 7%\n  - Coal: 38%\n  - Nuclear: 13%\n\n- **United States:**\n  - Renewables: 17%\n  - Gas: 35%\n  - Coal: 27%\n  - Nuclear: 19%\n\n- **Amazon-AWS:**\n  - Renewables: 17%\n  - Gas: 24%\n  - Coal: 30%\n  - Nuclear: 26%\n\n- **Google:**\n  - Renewables: 56%\n  - Gas: 14%\n  - Coal: 15%\n  - Nuclear: 10%\n\n- **Microsoft:**\n  - Renewables: 32%\n  - Gas: 23%\n  - Coal: 31%\n  - Nuclear: 10%\nWe estimate the total time expected for mod- els to train to completion using training times and hardware reported in the original papers. We then calculate the power consumption in kilowatt-hours  $(\\mathrm{kWh})$   as follows. Let    $p_{c}$   be the average power draw (in watts) from all CPU sockets during train- ing, let    $p_{r}$   be the average power draw from all DRAM (main memory) sockets, let  $p_{g}$   be the aver- age power draw of a GPU during training, and let  $g$   be the number of GPUs used to train. We esti- mate total power consumption as combined GPU, CPU and DRAM consumption, then multiply this by Power Usage Effectiveness (PUE), which ac- counts for the additional energy required to sup- port the compute infrastructure (mainly cooling). We use a PUE coefﬁcient of 1.58, the 2018 global average for data centers ( Ascierto ,  2018 ). Then the total power    $p_{t}$   required at a given instance during training is given by: \n\n$$\np_{t}=\\frac{1.58t(p_{c}+p_{r}+g p_{g})}{1000}\n$$\n \nThe U.S. Environmental Protection Agency (EPA) provides average   $\\mathrm{CO_{2}}$   produced (in pounds per kilowatt-hour) for power consumed in the U.S. ( EPA ,  2018 ), which we use to convert power to estimated  $\\mathrm{CO_{2}}$   emissions: \n\n$$\n\\mathrm{CO_{2}e}=0.954p_{t}\n$$\n \nThis conversion takes into account the relative pro- portions of different energy sources (primarily nat- ural gas, coal, nuclear and renewable) consumed to produce energy in the United States. Table  2 lists the relative energy sources for China, Ger- many and the United States compared to the top three cloud service providers. The U.S. break- down of energy is comparable to that of the most popular cloud compute service, Amazon Web Ser- vices, so we believe this conversion to provide a reasonable estimate of   $\\mathrm{CO_{2}}$   emissions per kilowatt hour of compute energy used. "}
{"page": 2, "image_path": "doc_images/P19-1355_2.jpg", "ocr_text": "three cloud service providers. The U.S. break-\ndown of energy is comparable to that of the most\npopular cloud compute service, Amazon Web Ser-\nvices, so we believe this conversion to provide a\nreasonable estimate of CO2 emissions per kilowatt\nhour of compute energy used.\n\n2.1 Models\n\nWe analyze four models, the computational re-\nquirements of which we describe below. All mod-\nels have code freely available online, which we\nused out-of-the-box. For more details on the mod-\nels themselves, please refer to the original papers.\nTransformer. The Transformer (T2T) model\n(Vaswani et al., 2017) is an encoder-decoder archi-\ntecture primarily recognized for efficient and accu-\nrate machine translation. The encoder and decoder\neach consist of 6 stacked layers of multi-head self-\nattention. Vaswani et al. (2017) report that the\nTransformer base model (T2Tpase; 65M param-\neters) was trained on 8 NVIDIA P100 GPUs for\n12 hours, and the Transformer big model (T2T;;,;\n213M parameters) was trained for 3.5 days (84\nhours; 300k steps). This model is also the ba-\nsis for recent work on neural architecture search\n(NAS) for machine translation and language mod-\neling (So et al., 2019), and the NLP pipeline that\nwe study in more detail in §4.2 (Strubell et al.,\n2018). So et al. (2019) report that their full ar-\nchitecture search ran for a total of 979M training\nsteps, and that their base model requires 10 hours\nto train for 300k steps on one TPUv2 core. This\nequates to 32,623 hours of TPU or 274,120 hours\non 8 P100 GPUs.\nELMo. The ELMo model (Peters et al., 2018)\nis based on stacked LSTMs and provides rich\nword representations in context by pre-training on\na large amount of data using a language model-\ning objective. Replacing context-independent pre-\ntrained word embeddings with ELMo has been\nshown to increase performance on downstream\ntasks such as named entity recognition, semantic\nrole labeling, and coreference. Peters et al. (2018)\nreport that ELMo was trained on 3 NVIDIA GTX\n1080 GPUs for 2 weeks (336 hours).\n\nBERT. The BERT model (Devlin et al., 2019) pro-\nvides a Transformer-based architecture for build-\ning contextual representations similar to ELMo,\nbut trained with a different language modeling ob-\njective. BERT substantially improves accuracy on\ntasks requiring sentence-level representations such\n\nas question answering and natural language infer-\nence. Devlin et al. (2019) report that the BERT\nbase model (BERT),;¢; 110M parameters) was\ntrained on 16 TPU chips for 4 days (96 hours).\nNVIDIA reports that they can train a BERT model\nin 3.3 days (79.2 hours) using 4 DGX-2H servers,\notaling 64 Tesla V100 GPUs (Forster et al., 2019).\nGPT-2. This model is the latest edition of\nOpenAI’s GPT general-purpose token encoder,\nalso based on Transformer-style self-attention and\ntrained with a language modeling objective (Rad-\nord et al., 2019). By training a very large model\non massive data, Radford et al. (2019) show high\nzero-shot performance on question answering and\nlanguage modeling benchmarks. The large model\ndescribed in Radford et al. (2019) has 1542M pa-\nrameters and is reported to require 1 week (168\nhours) of training on 32 TPUV3 chips. ©\n\n3 Related work\n\nThere is some precedent for work characterizing\nthe computational requirements of training and in-\nference in modern neural network architectures in\nthe computer vision community. Li et al. (2016)\npresent a detailed study of the energy use required\nfor training and inference in popular convolutional\nmodels for image classification in computer vi-\nsion, including fine-grained analysis comparing\ndifferent neural network layer types. Canziani\net al. (2016) assess image classification model ac-\ncuracy as a function of model size and gigaflops\nrequired during inference. They also measure av-\nerage power draw required during inference on\nGPUs as a function of batch size. Neither work an-\nalyzes the recurrent and self-attention models that\nhave become commonplace in NLP, nor do they\nextrapolate power to estimates of carbon and dol-\nlar cost of training.\n\nAnalysis of hyperparameter tuning has been\nperformed in the context of improved algorithms\nfor hyperparameter search (Bergstra et al., 2011;\nBergstra and Bengio, 2012; Snoek et al., 2012). To\nour knowledge there exists to date no analysis of\nhe computation required for R&D and hyperpa-\nrameter tuning of neural network models in NLP.\n\n°Via the authors on Reddit.\n\n7GPU lower bound computed using pre-emptible\nP100/V100 U.S. resources priced at $0.43-$0.74/hr, upper\nbound uses on-demand U.S. resources priced at $1.46—\n$2.48/hr. We similarly use pre-emptible ($1.46/hr-$2.40/hr)\nand on-demand ($4.50/hr-$8/hr) pricing as lower and upper\nbounds for TPU v2/3; cheaper bulk contracts are available.\n\n3647\n", "vlm_text": "\n2.1 Models \nWe analyze four models, the computational re- quirements of which we describe below. All mod- els have code freely available online, which we used out-of-the-box. For more details on the mod- els themselves, please refer to the original papers. \nTransformer . The Transformer (T2T) model ( Vaswani et al. ,  2017 ) is an encoder-decoder archi- tecture primarily recognized for efﬁcient and accu- rate machine translation. The encoder and decoder each consist of 6 stacked layers of multi-head self- attention. Vaswani et al.  ( 2017 ) report that the Transformer base model   $({\\bf T}2{\\bf T}_{b a s e}$  ; 65M param- eters) was trained on 8 NVIDIA P100 GPUs for 12 hours, and the Transformer big model   $({\\bf T}2{\\bf T}_{b i g}$  ; 213M parameters) was trained for 3.5 days (84 hours;   $300\\mathbf{k}$   steps). This model is also the ba- sis for recent work on neural architecture search ( NAS ) for machine translation and language mod- eling ( So et al. ,  2019 ), and the NLP pipeline that we study in more detail in    $\\S4.2$   ( Strubell et al. , 2018 ).  So et al.  ( 2019 ) report that their full ar- chitecture search ran for a total of 979M training steps, and that their base model requires 10 hours to train for  $300\\mathbf{k}$   steps on one TPUv2 core. This equates to 32,623 hours of TPU or 274,120 hours on 8 P100 GPUs. \nELMo.  The ELMo model ( Peters et al. ,  2018 ) is based on stacked LSTMs and provides rich word representations in context by pre-training on a large amount of data using a language model- ing objective. Replacing context-independent pre- trained word embeddings with ELMo has been shown to increase performance on downstream tasks such as named entity recognition, semantic role labeling, and coreference.  Peters et al.  ( 2018 ) report that ELMo was trained on 3 NVIDIA GTX 1080 GPUs for 2 weeks (336 hours). \nBERT.  The BERT model ( Devlin et al. ,  2019 ) pro- vides a Transformer-based architecture for build- ing contextual representations similar to ELMo, but trained with a different language modeling ob- jective. BERT substantially improves accuracy on tasks requiring sentence-level representations such as question answering and natural language infer- ence.  Devlin et al.  ( 2019 ) report that the BERT base model   $({\\bf B E R T}_{b a s e}$  ; 110M parameters) was trained on 16 TPU chips for 4 days (96 hours). NVIDIA reports that they can train a BERT model in 3.3 days (79.2 hours) using 4 DGX-2H servers, totaling 64 Tesla V100 GPUs ( Forster et al. ,  2019 ). \n\nGPT-2 . This model is the latest edition of OpenAI’s GPT general-purpose token encoder, also based on Transformer-style self-attention and trained with a language modeling objective ( Rad- ford et al. ,  2019 ). By training a very large model on massive data,  Radford et al.  ( 2019 ) show high zero-shot performance on question answering and language modeling benchmarks. The large model described in  Radford et al.  ( 2019 ) has 1542M pa- rameters and is reported to require 1 week (168 hours) of training on 32 TPUv3 chips.   6 \n3 Related work \nThere is some precedent for work characterizing the computational requirements of training and in- ference in modern neural network architectures in the computer vision community.  Li et al.  ( 2016 ) present a detailed study of the energy use required for training and inference in popular convolutional models for image classiﬁcation in computer vi- sion, including ﬁne-grained analysis comparing different neural network layer types. Canziani et al.  ( 2016 ) assess image classiﬁcation model ac- curacy as a function of model size and gigaﬂops required during inference. They also measure av- erage power draw required during inference on GPUs as a function of batch size. Neither work an- alyzes the recurrent and self-attention models that have become commonplace in NLP, nor do they extrapolate power to estimates of carbon and dol- lar cost of training. \nAnalysis of hyperparameter tuning has been performed in the context of improved algorithms for hyperparameter search ( Bergstra et al. ,  2011 ; Bergstra and Bengio ,  2012 ;  Snoek et al. ,  2012 ). To our knowledge there exists to date no analysis of the computation required for R&D and hyperpa- rameter tuning of neural network models in NLP. "}
{"page": 3, "image_path": "doc_images/P19-1355_3.jpg", "ocr_text": "Model Hardware Power (W) Hours kWh-PUE COze Cloud compute cost\nT2Tohase P100x8 1415.78 12 27 26 $41-$140\n\nT2T pig P100x8 1515.43 84 201 192 $289-$981\n\nELMo P100x3 517.66 336 275 262 $433-$1472\nBERTyase V100x64 12,041.51 79 1507 1438 $3751-$12,571\nBERTyase TPUv2x16 — 96 — — $2074-$6912\n\nNAS P100x8 1515.43 274,120 656,347 626,155 $942,973-$3,201,722\nNAS TPUv2x1 — 32,623 — — $44,055-$146,848\nGPT-2 TPUv3x32 — 168 — —  $12,902-$43,008\n\nTable 3: Estimated cost of training a model in terms of CO emissions (Ibs) and cloud compute cost (USD).” Power\nand carbon footprint are omitted for TPUs due to lack of public information on power draw for this hardware.\n\n4 Experimental results\n\n4.1 Cost of training\n\nTable 3 lists CO2 emissions and estimated cost of\ntraining the models described in §2.1. Of note is\nthat TPUs are more cost-efficient than GPUs on\nworkloads that make sense for that hardware (e.g.\nBERT). We also see that models emit substan-\ntial carbon emissions; training BERT on GPU is\nroughly equivalent to a trans-American flight. So\net al. (2019) report that NAS achieves a new state-\nof-the-art BLEU score of 29.7 for English to Ger-\nman machine translation, an increase of just 0.1\nBLEU at the cost of at least $150k in on-demand\ncompute time and non-trivial carbon emissions.\n\n4.2 Cost of development: Case study\n\nTo quantify the computational requirements of\nR&D for a new model we study the logs of\nall training required to develop Linguistically-\nInformed Self-Attention (Strubell et al., 2018), a\nmulti-task model that performs part-of-speech tag-\nging, labeled dependency parsing, predicate detec-\ntion and semantic role labeling. This model makes\nfor an interesting case study as a representative\nNLP pipeline and as a Best Long Paper at EMNLP.\nModel training associated with the project\nspanned a period of 172 days (approx. 6 months).\nDuring that time 123 small hyperparameter grid\nsearches were performed, resulting in 4789 jobs\nin total. Jobs varied in length ranging from a min-\nimum of 3 minutes, indicating a crash, to a maxi-\nmum of 9 days, with an average job length of 52\nhours. All training was done on a combination of\nNVIDIA Titan X (72%) and M40 (28%) GPUs.®\nThe sum GPU time required for the project\ntotaled 9998 days (27 years). This averages to\n\n8We approximate cloud compute cost using P100 pricing.\n\nEstimated cost (USD)\nModels Hours Cloud Electric\n1 120 $52-$175 $5\n24 2880 $1238-$4205 $118\n4789 239,942 $103k-$350k $9870\n\nTable 4: Estimated cost in terms of cloud compute and\nelectricity for training: (1) a single model (2) a single\ntune and (3) all models trained during R&D.\n\nabout 60 GPUs running constantly throughout the\n6 month duration of the project. Table 4 lists upper\nand lower bounds of the estimated cost in terms\nof Google Cloud compute and raw electricity re-\nquired to develop and deploy this model. We see\nthat while training a single model is relatively in-\nexpensive, the cost of tuning a model for a new\ndataset, which we estimate here to require 24 jobs,\nor performing the full R&D required to develop\nthis model, quickly becomes extremely expensive.\n\n5 Conclusions\n\nAuthors should report training time and\nsensitivity to hyperparameters.\n\nOur experiments suggest that it would be benefi-\ncial to directly compare different models to per-\nform a cost-benefit (accuracy) analysis. To ad-\ndress this, when proposing a model that is meant\nto be re-trained for downstream use, such as re-\ntraining on a new domain or fine-tuning on a new\ntask, authors should report training time and com-\nputational resources required, as well as model\nsensitivity to hyperparameters. This will enable\ndirect comparison across models, allowing subse-\nquent consumers of these models to accurately as-\nsess whether the required computational resources\n\n°Based on average U.S cost of electricity of $0.12/kWh.\n\n3648\n", "vlm_text": "The table provides information about different models and their associated hardware, power consumption, and costs:\n\n1. **Models**: T2T_base, T2T_big, ELMo, BERT_base, NAS, GPT-2.\n  \n2. **Hardware**: Details of the hardware used, including the type and number of processing units (e.g., P100x8, V100x64).\n\n3. **Power (W)**: The power consumption for each model.\n\n4. **MLU**: Some numerical value, possibly related to processing capability or usage.\n\n5. **TPUv2**: Values possibly indicating settings related to TPU version 2 usage.\n\n6. **Ops**: Number of operations or some measure of processing activity.\n\n7. **Cloud Compute Cost**: The estimated cost range for running each model in a cloud computing environment.\n\nThe table provides a comparison of different machine learning models in terms of resource requirements and potential operational costs.\n4 Experimental results \n4.1 Cost of training \nTable  3  lists  $\\mathrm{CO_{2}}$   emissions and estimated cost of training the models described in    $\\S2.1$  . Of note is that TPUs are more cost-efﬁcient than GPUs on workloads that make sense for that hardware (e.g. BERT). We also see that models emit substan- tial carbon emissions; training BERT on GPU is roughly equivalent to a trans-American ﬂight.  So et al.  ( 2019 ) report that NAS achieves a new state- of-the-art BLEU score of 29.7 for English to Ger- man machine translation, an increase of just 0.1 BLEU at the cost of at least  $\\mathbb{S}150\\mathbf{k}$   in on-demand compute time and non-trivial carbon emissions. \n4.2 Cost of development: Case study \nTo quantify the computational requirements of R&D for a new model we study the logs of all training required to develop Linguistically- Informed Self-Attention ( Strubell et al. ,  2018 ), a multi-task model that performs part-of-speech tag- ging, labeled dependency parsing, predicate detec- tion and semantic role labeling. This model makes for an interesting case study as a representative NLP pipeline and as a Best Long Paper at EMNLP. \nModel training associated with the project spanned a period of 172 days (approx. 6 months). During that time 123 small hyperparameter grid searches were performed, resulting in 4789 jobs in total. Jobs varied in length ranging from a min- imum of 3 minutes, indicating a crash, to a maxi- mum of 9 days, with an average job length of 52 hours. All training was done on a combination of NVIDIA Titan   $\\mathrm{X}\\,(72\\%)$   and M40   $(28\\%)$   GPUs. \nThe sum GPU time required for the project totaled 9998 days (27 years). This averages to \nThe table compares estimated costs for models in terms of hours and expenses for both cloud and electric options:\n\n- **1 Model**\n  - **Hours:** 120\n  - **Cloud Cost:** $52–$175\n  - **Electric Cost:** $5\n\n- **24 Models**\n  - **Hours:** 2880\n  - **Cloud Cost:** $1238–$4205\n  - **Electric Cost:** $118\n\n- **4789 Models**\n  - **Hours:** 239,942\n  - **Cloud Cost:** $103k–$350k\n  - **Electric Cost:** $9870\nabout 60 GPUs running constantly throughout the 6 month duration of the project. Table  4  lists upper and lower bounds of the estimated cost in terms of Google Cloud compute and raw electricity re- quired to develop and deploy this model.   We see that while training a single model is relatively in- expensive, the cost of tuning a model for a new dataset, which we estimate here to require 24 jobs, or performing the full R&D required to develop this model, quickly becomes extremely expensive. \n5 Conclusions \nAuthors should report training time and sensitivity to hyperparameters. \nOur experiments suggest that it would be beneﬁ- cial to directly compare different models to per- form a cost-beneﬁt (accuracy) analysis. To ad- dress this, when proposing a model that is meant to be re-trained for downstream use, such as re- training on a new domain or ﬁne-tuning on a new task, authors should report training time and com- putational resources required, as well as model sensitivity to hyperparameters. This will enable direct comparison across models, allowing subse- quent consumers of these models to accurately as- sess whether the required computational resources are compatible with their setting. More explicit characterization of tuning time could also reveal inconsistencies in time spent tuning baseline mod- els compared to proposed contributions. Realiz- ing this will require: (1) a standard, hardware- independent measurement of training time, such as gigaﬂops required to convergence, and (2) a standard measurement of model sensitivity to data and hyperparameters, such as variance with re- spect to hyperparameters searched. "}
{"page": 4, "image_path": "doc_images/P19-1355_4.jpg", "ocr_text": "are compatible with their setting. More explicit\ncharacterization of tuning time could also reveal\ninconsistencies in time spent tuning baseline mod-\nels compared to proposed contributions. Realiz-\ning this will require: (1) a standard, hardware-\nindependent measurement of training time, such\nas gigaflops required to convergence, and (2) a\nstandard measurement of model sensitivity to data\nand hyperparameters, such as variance with re-\nspect to hyperparameters searched.\n\nAcademic researchers need equitable access to\ncomputation resources.\n\nRecent advances in available compute come at a\nhigh price not attainable to all who desire access.\nMost of the models studied in this paper were de-\nveloped outside academia; recent improvements in\nstate-of-the-art accuracy are possible thanks to in-\ndustry access to large-scale compute.\n\nLimiting this style of research to industry labs\nhurts the NLP research community in many ways.\nFirst, it stifles creativity. Researchers with good\nideas but without access to large-scale compute\nwill simply not be able to execute their ideas,\ninstead constrained to focus on different prob-\nlems. Second, it prohibits certain types of re-\nsearch on the basis of access to financial resources.\nThis even more deeply promotes the already prob-\nlematic “rich get richer” cycle of research fund-\ning, where groups that are already successful and\nthus well-funded tend to receive more funding\ndue to their existing accomplishments. Third, the\nprohibitive start-up cost of building in-house re-\nsources forces resource-poor groups to rely on\ncloud compute services such as AWS, Google\nCloud and Microsoft Azure.\n\nWhile these services provide valuable, flexi-\nble, and often relatively environmentally friendly\ncompute resources, it is more cost effective for\nacademic researchers, who often work for non-\nprofit educational institutions and whose research\nis funded by government entities, to pool resources\nto build shared compute centers at the level of\nfunding agencies, such as the U.S. National Sci-\nence Foundation. For example, an off-the-shelf\nGPU server containing 8 NVIDIA 1080 Ti GPUs\nand supporting hardware can be purchased for\napproximately $20,000 USD. At that cost, the\nhardware required to develop the model in our\ncase study (approximately 58 GPUs for 172 days)\nwould cost $145,000 USD plus electricity, about\n\nhalf the estimated cost to use on-demand cloud\nGPUs. Unlike money spent on cloud compute,\nhowever, that invested in centralized resources\nwould continue to pay off as resources are shared\nacross many projects. A government-funded aca-\ndemic compute cloud would provide equitable ac-\ncess to all researchers.\n\nResearchers should prioritize computationally\nefficient hardware and algorithms.\n\nWe recommend a concerted effort by industry and\nacademia to promote research of more computa-\ntionally efficient algorithms, as well as hardware\nthat requires less energy. An effort can also be\nmade in terms of software. There is already a\nprecedent for NLP software packages prioritizing\nefficient models. An additional avenue through\nwhich NLP and machine learning software de-\nvelopers could aid in reducing the energy asso-\nciated with model tuning is by providing easy-\nto-use APIs implementing more efficient alterna-\ntives to brute-force grid search for hyperparameter\ntuning, e.g. random or Bayesian hyperparameter\nsearch techniques (Bergstra et al., 2011; Bergstra\nand Bengio, 2012; Snoek et al., 2012). While\nsoftware packages implementing these techniques\ndo exist,!° they are rarely employed in practice\n‘or tuning NLP models. This is likely because\nheir interoperability with popular deep learning\nrameworks such as PyTorch and TensorFlow is\nnot optimized, i.e. there are not simple exam-\nples of how to tune TensorFlow Estimators using\nBayesian search. Integrating these tools into the\nworkflows with which NLP researchers and practi-\nioners are already familiar could have notable im-\npact on the cost of developing and tuning in NLP.\n\nAcknowledgements\n\nWe are grateful to Sherief Farouk and the anony-\nmous reviewers for helpful feedback on earlier\ndrafts. This work was supported in part by the\nCenters for Data Science and Intelligent Infor-\nmation Retrieval, the Chan Zuckerberg Initiative\nunder the Scientific Knowledge Base Construc-\ntion project, the IBM Cognitive Horizons Network\nagreement no. W1668553, and National Science\nFoundation grant no. IIS-1514053. Any opinions,\nfindings and conclusions or recommendations ex-\npressed in this material are those of the authors and\ndo not necessarily reflect those of the sponsor.\n\n'0For example, the Hyperopt Python library.\n\n3649\n", "vlm_text": "\nAcademic researchers need equitable access to computation resources. \nRecent advances in available compute come at a high price not attainable to all who desire access. Most of the models studied in this paper were de- veloped outside academia; recent improvements in state-of-the-art accuracy are possible thanks to in- dustry access to large-scale compute. \nLimiting this style of research to industry labs hurts the NLP research community in many ways. First, it stiﬂes creativity. Researchers with good ideas but without access to large-scale compute will simply not be able to execute their ideas, instead constrained to focus on different prob- lems. Second, it prohibits certain types of re- search on the basis of access to ﬁnancial resources. This even more deeply promotes the already prob- lematic “rich get richer” cycle of research fund- ing, where groups that are already successful and thus well-funded tend to receive more funding due to their existing accomplishments. Third, the prohibitive start-up cost of building in-house re- sources forces resource-poor groups to rely on cloud compute services such as AWS, Google Cloud and Microsoft Azure. \nWhile these services provide valuable, ﬂexi- ble, and often relatively environmentally friendly compute resources, it is more cost effective for academic researchers, who often work for non- proﬁt educational institutions and whose research is funded by government entities, to pool resources to build shared compute centers at the level of funding agencies, such as the U.S. National Sci- ence Foundation. For example, an off-the-shelf GPU server containing 8 NVIDIA 1080 Ti GPUs and supporting hardware can be purchased for approximately   $\\mathbb{S}20{,}000$   USD. At that cost, the hardware required to develop the model in our case study (approximately 58 GPUs for 172 days) would cost   $\\S145{,}000$   USD plus electricity, about half the estimated cost to use on-demand cloud GPUs. Unlike money spent on cloud compute, however, that invested in centralized resources would continue to pay off as resources are shared across many projects. A government-funded aca- demic compute cloud would provide equitable ac- cess to all researchers. \n\nResearchers should prioritize computationally efﬁcient hardware and algorithms. \nWe recommend a concerted effort by industry and academia to promote research of more computa- tionally efﬁcient algorithms, as well as hardware that requires less energy. An effort can also be made in terms of software. There is already a precedent for NLP software packages prioritizing efﬁcient models. An additional avenue through which NLP and machine learning software de- velopers could aid in reducing the energy asso- ciated with model tuning is by providing easy- to-use APIs implementing more efﬁcient alterna- tives to brute-force grid search for hyperparameter tuning, e.g. random or Bayesian hyperparameter search techniques ( Bergstra et al. ,  2011 ;  Bergstra and Bengio ,  2012 ;  Snoek et al. ,  2012 ). While software packages implementing these techniques do exist,   they are rarely employed in practice for tuning NLP models. This is likely because their interoperability with popular deep learning frameworks such as PyTorch and TensorFlow is not optimized, i.e. there are not simple exam- ples of how to tune TensorFlow Estimators using Bayesian search. Integrating these tools into the workﬂows with which NLP researchers and practi- tioners are already familiar could have notable im- pact on the cost of developing and tuning in NLP. \nAcknowledgements \nWe are grateful to Sherief Farouk and the anony- mous reviewers for helpful feedback on earlier drafts. This work was supported in part by the Centers for Data Science and Intelligent Infor- mation Retrieval, the Chan Zuckerberg Initiative under the Scientiﬁc Knowledge Base Construc- tion project, the IBM Cognitive Horizons Network agreement no. W1668553, and National Science Foundation grant no. IIS-1514053. Any opinions, ﬁndings and conclusions or recommendations ex- pressed in this material are those of the authors and do not necessarily reﬂect those of the sponsor. "}
{"page": 5, "image_path": "doc_images/P19-1355_5.jpg", "ocr_text": "References\n\nRhonda Ascierto. 2018. Uptime Institute Global Data\nCenter Survey. Technical report, Uptime Institute.\n\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2015. Neural Machine Translation by Jointly\nLearning to Align and Translate. In 3rd Inter-\nnational Conference for Learning Representations\n(ICLR), San Diego, California, USA.\n\nJames Bergstra and Yoshua Bengio. 2012. Random\nsearch for hyper-parameter optimization. Journal of\nMachine Learning Research, 13(Feb):28 1-305.\n\nJames S Bergstra, Rémi Bardenet, Yoshua Bengio, and\nBalazs Kégl. 2011. Algorithms for hyper-parameter\noptimization. In Advances in neural information\nprocessing systems, pages 2546-2554.\n\nBruno Burger. 2019. Net Public Electricity Generation\nin Germany in 2018. Technical report, Fraunhofer\nInstitute for Solar Energy Systems ISE.\n\nAlfredo Canziani, Adam Paszke, and Eugenio Culur-\nciello. 2016. An analysis of deep neural network\nmodels for practical applications.\n\nGary Cook, Jude Lee, Tamina Tsai, Ada Kongn, John\nDeans, Brian Johnson, Elizabeth Jardim, and Brian\nJohnson. 2017. Clicking Clean: Who is winning\nthe race to build a green internet? Technical report,\nGreenpeace.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\nDeep Bidirectional Transformers for Language Un-\nderstanding. In NAACL.\n\nTimothy Dozat and Christopher D. Manning. 2017.\nDeep biaffine attention for neural dependency pars-\ning. In JCLR.\n\nEPA. 2018. Emissions & Generation Resource Inte-\ngrated Database (eGRID). Technical report, U.S.\nEnvironmental Protection Agency.\n\nChristopher Forster, Thor Johnsen, Swetha Man-\ndava, Sharath Turuvekere Sreenivas, Deyu Fu, Julie\nBernauer, Allison Gray, Sharan Chetlur, and Raul\nPuri. 2019. BERT Meets GPUs. Technical report,\nNVIDIA AIL.\n\nDa Li, Xinbo Chen, Michela Becchi, and Ziliang Zong.\n2016. Evaluating the energy efficiency of deep con-\nvolutional neural networks on cpus and gpus. 2016\nIEEE International Conferences on Big Data and\nCloud Computing (BDCloud), Social Computing\nand Networking (SocialCom), Sustainable Comput-\ning and Communications (SustainCom) (BDCloud-\nSocialCom-SustainCom), pages 477-484.\n\nThang Luong, Hieu Pham, and Christopher D. Man-\nning. 2015. Effective approaches to attention-based\nneural machine translation. In Proceedings of the\n2015 Conference on Empirical Methods in Natural\nLanguage Processing, pages 1412-1421. Associa-\ntion for Computational Linguistics.\n\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. In NAACL.\n\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\n\nJasper Snoek, Hugo Larochelle, and Ryan P Adams.\n2012. Practical bayesian optimization of machine\nlearning algorithms. In Advances in neural informa-\ntion processing systems, pages 2951-2959.\n\nDavid R. So, Chen Liang, and Quoc V. Le. 2019.\nThe evolved transformer. In Proceedings of the\n36th International Conference on Machine Learning\n(ICML).\n\nEmma Strubell, Patrick Verga, Daniel Andor,\nDavid Weiss, and Andrew McCallum. 2018.\nLinguistically-Informed Self-Attention for Se-\nmantic Role Labeling. In Conference on Empir-\nical Methods in Natural Language Processing\n(EMNLP), Brussels, Belgium.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In 3/st Conference on Neural Information\nProcessing Systems (NIPS).\n\n3650\n", "vlm_text": "References \nRhonda Ascierto. 2018.  Uptime Institute Global Data \nCenter Survey . Technical report, Uptime Institute. Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben- gio. 2015. Neural Machine Translation by Jointly Learning to Align and Translate. In  3rd Inter- national Conference for Learning Representations (ICLR) , San Diego, California, USA. James Bergstra and Yoshua Bengio. 2012. Random search for hyper-parameter optimization.  Journal of Machine Learning Research , 13(Feb):281–305. James S Bergstra, R´ emi Bardenet, Yoshua Bengio, and Bal´ azs K´ egl. 2011. Algorithms for hyper-parameter optimization. In  Advances in neural information processing systems , pages 2546–2554. Bruno Burger. 2019.  Net Public Electricity Generation in Germany in 2018 . Technical report, Fraunhofer Institute for Solar Energy Systems ISE. Alfredo Canziani, Adam Paszke, and Eugenio Culur- ciello. 2016. An analysis of deep neural network models for practical applications . Gary Cook, Jude Lee, Tamina Tsai, Ada Kongn, John Deans, Brian Johnson, Elizabeth Jardim, and Brian Johnson. 2017. Clicking Clean: Who is winning the race to build a green internet?  Technical report, Greenpeace. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Un- derstanding. In  NAACL . Timothy Dozat and Christopher D. Manning. 2017. Deep biafﬁne attention for neural dependency pars- ing. In  ICLR . EPA. 2018.  Emissions & Generation Resource Inte- grated Database (eGRID) . Technical report, U.S. Environmental Protection Agency. Christopher Forster, Thor Johnsen, Swetha Man- dava, Sharath Turuvekere Sreenivas, Deyu Fu, Julie Bernauer, Allison Gray, Sharan Chetlur, and Raul Puri. 2019.  BERT Meets GPUs . Technical report, NVIDIA AI. Da Li, Xinbo Chen, Michela Becchi, and Ziliang Zong. 2016. Evaluating the energy efﬁciency of deep con- volutional neural networks on cpus and gpus.  2016 IEEE International Conferences on Big Data and Cloud Computing (BDCloud), Social Computing and Networking (SocialCom), Sustainable Comput- ing and Communications (SustainCom) (BDCloud- SocialCom-SustainCom) , pages 477–484. Thang Luong, Hieu Pham, and Christopher D. Man- ning. 2015.  Effective approaches to attention-based neural machine translation . In  Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing , pages 1412–1421. Associa- tion for Computational Linguistics. \nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word rep- resentations. In  NAACL . Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019.  Language models are unsupervised multitask learners . Jasper Snoek, Hugo Larochelle, and Ryan P Adams. 2012. Practical bayesian optimization of machine learning algorithms. In  Advances in neural informa- tion processing systems , pages 2951–2959. David R. So, Chen Liang, and Quoc V. Le. 2019. The evolved transformer . In  Proceedings of the 36th International Conference on Machine Learning (ICML) . Emma Strubell, Patrick Verga, Daniel Andor, David Weiss, and Andrew McCallum. 2018. Linguistically-Informed Self-Attention for Se- mantic Role Labeling. In  Conference on Empir- ical Methods in Natural Language Processing (EMNLP) , Brussels, Belgium. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In  31st Conference on Neural Information Processing Systems (NIPS) . "}
