{"page": 0, "image_path": "doc_images/2309.17421v2_0.jpg", "ocr_text": "arX1v:2309.17421v2 [cs.CV] 11 Oct 2023\n\nThe Dawn of LMMs:\nPreliminary Explorations with GPT-4V(ision)\n\nZhengyuan Yang, Linjie Li*, Kevin Lin*, Jianfeng Wang*, Chung-Ching Lin‘,\nZicheng Liu, Lijuan Wang*®\nMicrosoft Corporation\n\n* Core Contributor ® Project Lead\n\nAbstract\n\nLarge multimodal models (LMMs) extend large language models (LLMs) with\nmulti-sensory skills, such as visual understanding, to achieve stronger generic in-\ntelligence. In this paper, we analyze the latest model, GPT-4V(ision) [99-101, iy,\nto deepen the understanding of LMMs. The analysis focuses on the intriguing\ntasks that GPT-4V can perform, containing test samples to probe the quality and\ngenericity of GPT-4V’s capabilities, its supported inputs and working modes, and\nthe effective ways to prompt the model. In our approach to exploring GPT-4V,\nwe curate and organize a collection of carefully designed qualitative samples\nspanning a variety of domains and tasks. Observations from these samples demon-\nstrate that GPT-4V’s unprecedented ability in processing arbitrarily interleaved\nmultimodal inputs and the genericity of its capabilities together make GPT-4V a\npowerful multimodal generalist system. Furthermore, GPT-4V’s unique capability\nof understanding visual markers drawn on input images can give rise to new human-\ncomputer interaction methods such as visual referring prompting. We conclude the\nreport with in-depth discussions on the emerging application scenarios and the fu-\nture research directions for GPT-4V-based systems. We hope that this preliminary\nexploration will inspire future research on the next-generation multimodal task\nformulation, new ways to exploit and enhance LMMs to solve real-world problems,\nand gaining better understanding of multimodal foundation models. Finally, we\nacknowledge that the model under our study is solely the product of OpenAI’s\ninnovative work, and they should be fully credited for its development. Please\nsee the GPT-4V contributions paper [101] for the authorship and credit attribution:\nhttps://cdn.openai.com/contributions/gpt—4v.pdf.\n\nContents\nList of Figures 4\n\n1 Introduction\n\n=\niv\nio)\n=\n5\n>\n3\n3\nS\n3\n5\nie}\n>\n=\nies)\n*\n2\n°\na.\n5\n0a\nQ\n~\na\n&\n<\nCw wo\n\n'This report explores GPT-4V(ision) with the vision capability and refers to the model as “GPT-4V,” following\nthe OpenAI reports [100, 99]. We refer to the text-only version of the model as “GPT-4 (no vision)” [99].\n", "vlm_text": "The Dawn of LMMs: Preliminary Explorations with GPT-4V(ision) \nZhengyuan Yang ∗ , Linjie  $\\mathbf{Li^{*}}$  , Kevin Lin ∗ , Jianfeng Wang ∗ , Chung-Ching Lin ∗ , Zicheng Liu, Lijuan Wang ∗♠ Microsoft Corporation \n∗ Core Contributor ♠ Project Lead \nAbstract \nLarge multimodal models (LMMs) extend large language models (LLMs) with multi-sensory skills, such as visual understanding, to achieve stronger generic in- telligence. In this paper, we analyze the latest model, GPT-4V(ision) [ 99 – 101 ,  1 ] 1 , to deepen the understanding of LMMs. The analysis focuses on the intriguing tasks that GPT-4V can perform, containing test samples to probe the quality and genericity of GPT-4V’s capabilities, its supported inputs and working modes, and the effective ways to prompt the model. In our approach to exploring GPT-4V, we curate and organize a collection of carefully designed qualitative samples spanning a variety of domains and tasks. Observations from these samples demon- strate that GPT-4V’s unprecedented ability in processing arbitrarily interleaved multimodal inputs and the genericity of its capabilities together make GPT-4V a powerful multimodal generalist system. Furthermore, GPT-4V’s unique capability of understanding visual markers drawn on input images can give rise to new human- computer interaction methods such as visual referring prompting. We conclude the report with in-depth discussions on the emerging application scenarios and the fu- ture research directions for GPT-4V-based systems. We hope that this preliminary exploration will inspire future research on the next-generation multimodal task formulation, new ways to exploit and enhance LMMs to solve real-world problems, and gaining better understanding of multimodal foundation models. Finally, we acknowledge that the model under our study is solely the product of OpenAI’s innovative work, and they should be fully credited for its development. Please see the GPT-4V contributions paper [ 101 ] for the authorship and credit attribution: \nContents \nList of Figures "}
{"page": 1, "image_path": "doc_images/2309.17421v2_1.jpg", "ocr_text": "GPT-4V’s Input Modes\n2.1 Text-only Inputs... 2... ee ee\n2.2 Single Image-text Par 2.2... ee\n\n2.3 Interleaved Image-text Inputs... 2... 0. ee ee\n\nGPT-4V’s Working Modes and Prompting Techniques\n\n3.1 Following Text Instructions... 2... 2. ee\n3.2 Visual Pointing and Visual Referring Prompting ................0..\n3.3. Visual+ Text Prompting ...........20. 0.000000. 0000.000.\n\n3.4 In-context Few-shot Learning... 2.2... 0.222 2. ee eee\n\nVision-Language Capability\n\n4.1 Image Description on Diverse Domains ....................000.\n4.2 Object Localization, Counting, and Dense Captioning. ...............\n4.3 Multimodal Knowledge and Commonsense ..................00.\n4.4 Scene Text, Table, Chart, and Document Reasoning .................\n4.5 Multilingual Multimodal Understanding .... 2... .....02.....00.0.\n4.6 Coding Capability with Vision .. 2.2... 2 ee ee\n\nInteraction with Humans: Visual Referring Prompting\n\n5.1 Understand Pointing Inputs... 2... 2... eee\n5.2 Visual Referring Prompting... 2... 2... 2 ee ee\n5.3. Generate Pointing Outputs ... 2.2... ee\n\nTemporal and Video Understanding\n\n6.1 Multi-image Sequencing ............2... 000000000000 000.\n6.2 Video Understanding .... 2.2... 2.00... 0.000000 0000000000.\n6.3 Visual Referring Prompting for Grounded Temporal Understanding. ........\n\nAbstract Visual Reasoning and Intelligence Quotient Test\n\n7.1 Abstract Visual Stimuli... ee eee\n7.2 Discovery and Association of Parts and Objects ...............000.\n7.3 Wechsler Adult Intelligence Scale 2.2... 2 ee\n\n7.4 Raven’s Progressive Matrices... 2... ee\n\nEmotional Quotient Test\n8.1 Read Emotion from Facial Expressions ................-0..000.4\n8.2 Understand How Visual Content Arouses Emotions .................\n\n8.3 Emotion Conditioned Output... 2... 2. eee\n\nEmerging Application Highlights\n9.1 Spot the Difference... 2... ee\n\n10\n10\n10\n11\n\n12\n15\n15\n16\n19\n\n26\n26\n38\n43\n49\n58\n63\n\n66\n66\n69\n69\n\n73\n73\n73\n79\n\n80\n80\n80\n80\n80\n\n86\n86\n87\n89\n\n90\n", "vlm_text": "2 GPT-4V’s Input Modes 10 \n2.1 Text-only Inputs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n\n 2.2 Single Image-text Pair . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n\n 2.3 Interleaved Image-text Inputs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\n\n \n3 GPT-4V’s Working Modes and Prompting Techniques 12 \n3.1 Following Text Instructions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n\n 3.2 Visual Pointing and Visual Referring Prompting . . . . . . . . . . . . . . . . . . . 15\n\n 3.3 Visual  $^+$   Text Prompting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\n\n 3.4 In-context Few-shot Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\n\n \n4 Vision-Language Capability 26 \n4.1 Image Description on Diverse Domains . . . . . . . . . . . . . . . . . . . . . . . 26\n\n 4.2 Object Localization, Counting, and Dense Captioning . . . . . . . . . . . . . . . . 38\n\n 4.3 Multimodal Knowledge and Commonsense . . . . . . . . . . . . . . . . . . . . . 43\n\n 4.4 Scene Text, Table, Chart, and Document Reasoning . . . . . . . . . . . . . . . . . 49\n\n 4.5 Multilingual Multimodal Understanding . . . . . . . . . . . . . . . . . . . . . . . 58\n\n 4.6 Coding Capability with Vision . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63\n\n \n5 Interaction with Humans: Visual Referring Prompting 66 \n5.1 Understand Pointing Inputs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66\n\n 5.2 Visual Referring Prompting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69\n\n 5.3 Generate Pointing Outputs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69\n\n \n6 Temporal and Video Understanding 73 \n6.1 Multi-image Sequencing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73\n\n 6.2 Video Understanding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73\n\n 6.3 Visual Referring Prompting for Grounded Temporal Understanding . . . . . . . . . 79\n\n \n7 Abstract Visual Reasoning and Intelligence Quotient Test 80 \n7.1 Abstract Visual Stimuli . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80\n\n 7.2 Discovery and Association of Parts and Objects . . . . . . . . . . . . . . . . . . . 80\n\n 7.3 Wechsler Adult Intelligence Scale . . . . . . . . . . . . . . . . . . . . . . . . . . 80\n\n 7.4 Raven’s Progressive Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80\n\n \n8 Emotional Quotient Test 86 \n8.1 Read Emotion from Facial Expressions . . . . . . . . . . . . . . . . . . . . . . . 86\n\n 8.2 Understand How Visual Content Arouses Emotions . . . . . . . . . . . . . . . . . 87\n\n 8.3 Emotion Conditioned Output . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89\n\n \n9 Emerging Application Highlights 90 \n9.1 Spot the Difference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90 9.2 Industry . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90\n\n 9.3 Medical . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99\n\n 9.4 Auto Insurance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104\n\n 9.5 Customized Captioner . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107\n\n 9.6 Image Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111\n\n 9.7 Embodied Agent . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 116\n\n 9.8 GUI Navigation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121\n\n "}
{"page": 2, "image_path": "doc_images/2309.17421v2_2.jpg", "ocr_text": "9.2 Industry... 2... ee 90\n\n9.3 Medical... 2... ee 99\n9.4 AutoInsurance 2... ee 104\n9.5 Customized Captioner . 2... 2... ee 107\n9.6 ImageGeneration...... 2... 0.000000. 000000002. eee 111\n9.7 Embodied Agent .. 2... .. 0.000000... 000000 00000000002 116\n9.8 GUI Navigation... 2... ee 121\nLMM Powered Agents 150\n10.1 Multimodal Plugins... 2... ee ee 152\n10.2 Multimodal Chains... 2... 2 ee ee 152\n10.3 Self-Reflection 2... 2... ee 152\n10.4 Self-Consistency .. 2.2.2... 0000000000000 000 22 eee 155\n10.5 Retrieval-Augmented LMMs ..........020.200 00000000000 00045 156\nConclusions 156\n11.1 Summary and Conclusions... 2... 2... 2 ee 156\n11.2 Towards Future LMMs ... 2... 2. ee 156\n", "vlm_text": "\n10 LMM Powered Agents 150 \n10.1 Multimodal Plugins . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152\n\n 10.2 Multimodal Chains . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152\n\n 10.3 Self-Reflection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152\n\n 10.4 Self-Consistency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155\n\n 10.5 Retrieval-Augmented LMMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 156\n\n \n11 Conclusions 156 \n11.1 Summary and Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 156\n\n 11.2 Towards Future LMMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 156 "}
{"page": 3, "image_path": "doc_images/2309.17421v2_3.jpg", "ocr_text": "List of Figures\n\nCo mrAN DU F&F WN —\n\nWw WW WW WwW WwW WwW YY YY MY YY KY KY KY KY KY FP eRe FP KF FP Re ee SS\nroranant WwWwNrFK DODO FAN DN FWY FH DUAN DN FWY KF CO\n\nSection 2.3:\nSection 3.1:\nSection 3.1:\nSection 3.1:\nSection 3.2:\nSection 3.2:\nSection 3.3:\nSection 3.4:\nSection 3.4:\nSection 3.4:\nSection 3.4:\nSection 3.4:\nSection 3.4:\nSection 4.\nSection 4.1:\nSection 4.1:\nSection 4.1:\nSection 4.\nSection 4.\nSection 4.1:\nSection 4.\n\nSection 4.\n\nSection 4.\nSection 4.2:\nSection 4.2:\nSection 4.2:\nSection 4.2:\nSection 4.3:\nSection 4.3:\nSection 4.3:\nSection 4.3:\nSection 4.3:\nSection 4.4:\nSection 4.4:\nSection 4.4:\nSection 4.4:\nSection 4.4:\nSection 4.4:\n\nmulti-image inputs and interleaved image-text pairs... ........ 11\nfollowing text instructions. ©... 2... ..........0..000. 12\nconstrained prompting. ..................00. 0000. 13\ncondition on good performance... ............00.0008.4 14\ndifferent modes of visual pointing. ................00. 16\nvisual pointing and visual referring prompting... ............ 17\nmultimodal example-grounded instruction... ............. 18\new-shot comparison example 1, Part I: zero-shot. .. 2.2... ..02. 20\new-shot comparison example 1, Part I: one-shot. .. 2... .....2. 21\new-shot comparison example 1, Part III: two-shot. ........2.. 22\new-shot comparison example 2, Part I: zero-shot. .. 2... 2.02. 23\new-shot comparison example 2, Part I: one-shot. .. 2... ...... 24\new-shot comparison example 2, Part III: two-shot. ........2.. 25\n\n: celebrity recognition and description... 2... .........000. 28\nlandmark recognition and description. ..............00-. 29\nlandmark recognition and description. ................0. 30\nood recognition and description. .. 2... 2... 2.2.2.0... .00.2 31\n\n: medical image understanding... .................00. 32\n: medical image understanding... .................00. 33\nlogo recognition and description. ................2000. 34\n\n: in-the-wild logo recognition and description... ..........0.. 35\n: scene understanding. . 2... 2.0... .00 0.020.000.0000. 36\n: counterfactual examples. ........... 0.02.00. .0000.2 37\nspatial relationship understanding. ...............00.. 39\nobject counting. ©... eee 40\nobject localization... 2... ee 41\ndense captioning. ... 2... 2.2... 0000.20.00. 0000. 42\njoke and meme understanding. .................008. 44\nscience and encyclopedia knowledge... .............00. 45\nscience and encyclopedia knowledge... .............00. 46\nscience and encyclopedia knowledge... .............00. 47\nmultimodal commonsense. .. 2... 2.00. eee ee eee 48\nscene text recognition... 2... 2... ee ee ee 50\nvisual math reasoning... . 2... 0.0.0.0. 0.2.0 00000084 51\nflow chart understanding. ... 2... ..........0..000. 52\nflow chart understanding. ... 2... ..........0..000. 53\nchart understanding and reasoning. ................00. 54\ntable understanding and reasoning. ................008. 55\n", "vlm_text": "List of Figures \n1 Section 2.3: multi-image inputs and interleaved image-text pairs. . . . . . . . . . . 11 2 Section 3.1: following text instructions. . . . . . . . . . . . . . . . . . . . . . . . 12 3 Section 3.1: constrained prompting. . . . . . . . . . . . . . . . . . . . . . . . . . 13 4 Section 3.1: condition on good performance. . . . . . . . . . . . . . . . . . . . . . 14 5 Section 3.2: different modes of visual pointing. . . . . . . . . . . . . . . . . . . . 16 6 Section 3.2: visual pointing and visual referring prompting. . . . . . . . . . . . . . 17 7 Section 3.3: multimodal example-grounded instruction. . . . . . . . . . . . . . . . 18 8 Section 3.4: few-shot comparison example 1, Part I: zero-shot. . . . . . . . . . . . 20 9 Section 3.4: few-shot comparison example 1, Part II: one-shot. . . . . . . . . . . . 21 10 Section 3.4: few-shot comparison example 1, Part III: two-shot. . . . . . . . . . . 22 11 Section 3.4: few-shot comparison example 2, Part I: zero-shot. . . . . . . . . . . . 23 12 Section 3.4: few-shot comparison example 2, Part II: one-shot. . . . . . . . . . . . 24 13 Section 3.4: few-shot comparison example 2, Part III: two-shot. . . . . . . . . . . 25 14 Section 4.1: celebrity recognition and description. . . . . . . . . . . . . . . . . . . 28 15 Section 4.1: landmark recognition and description. . . . . . . . . . . . . . . . . . 29 16 Section 4.1: landmark recognition and description. . . . . . . . . . . . . . . . . . 30 17 Section 4.1: food recognition and description. . . . . . . . . . . . . . . . . . . . . 31 18 Section 4.1: medical image understanding. . . . . . . . . . . . . . . . . . . . . . . 32 19 Section 4.1: medical image understanding. . . . . . . . . . . . . . . . . . . . . . . 33 20 Section 4.1: logo recognition and description. . . . . . . . . . . . . . . . . . . . . 34 21 Section 4.1: in-the-wild logo recognition and description. . . . . . . . . . . . . . . 35 22 Section 4.1: scene understanding. . . . . . . . . . . . . . . . . . . . . . . . . . . 36 23 Section 4.1: counter factual examples. . . . . . . . . . . . . . . . . . . . . . . . . 37 24 Section 4.2: spatial relationship understanding. . . . . . . . . . . . . . . . . . . . 39 25 Section 4.2: object counting. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40 26 Section 4.2: object localization. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41 27 Section 4.2: dense captioning. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42 28 Section 4.3: joke and meme understanding. . . . . . . . . . . . . . . . . . . . . . 44 29 Section 4.3: science and encyclopedia knowledge. . . . . . . . . . . . . . . . . . . 45 30 Section 4.3: science and encyclopedia knowledge. . . . . . . . . . . . . . . . . . . 46 31 Section 4.3: science and encyclopedia knowledge. . . . . . . . . . . . . . . . . . . 47 32 Section 4.3: multimodal commonsense. . . . . . . . . . . . . . . . . . . . . . . . 48 33 Section 4.4: scene text recognition. . . . . . . . . . . . . . . . . . . . . . . . . . . 50 34 Section 4.4: visual math reasoning. . . . . . . . . . . . . . . . . . . . . . . . . . . 51 35 Section 4.4: flow chart understanding. . . . . . . . . . . . . . . . . . . . . . . . . 52 36 Section 4.4: flow chart understanding. . . . . . . . . . . . . . . . . . . . . . . . . 53 37 Section 4.4: chart understanding and reasoning. . . . . . . . . . . . . . . . . . . . 54 38 Section 4.4: table understanding and reasoning. . . . . . . . . . . . . . . . . . . . 55 39 Section 4.4: document understanding. . . . . . . . . . . . . . . . . . . . . . . . . 56 40 Section 4.4: long document understanding. . . . . . . . . . . . . . . . . . . . . . 57 41 Section 4.5: multilingual image descriptions. . . . . . . . . . . . . . . . . . . . . 59 42 Section 4.5: multilingual scene text recognition. . . . . . . . . . . . . . . . . . . . 60 43 Section 4.5: multilingual text recognition, translation, and description reasoning. . 61 44 Section 4.5: multilingual multi culture understanding. . . . . . . . . . . . . . . . . 62 45 Section 4.6: generate LaTex codes based on the hand-written input. . . . . . . . . . 63 46 Section 4.6: reconstruct table in Markdown or LaTex code. . . . . . . . . . . . . . 64 47 Section 4.6: write codes to replicate the input figure. . . . . . . . . . . . . . . . . 65 48 Section 5.1: understand pointing inputs for grounded description. . . . . . . . . . . 67 49 Section 5.1: grounded description with text-format region coordinates. . . . . . . . 68 50 Section 5.2: examples of visual referring prompting. . . . . . . . . . . . . . . . . 70 51 Section 5.2: examples of visual referring prompting. . . . . . . . . . . . . . . . . 71 52 Section 5.3: generate pointing outputs. . . . . . . . . . . . . . . . . . . . . . . . . 72 53 Section 6.1: video frames understanding. . . . . . . . . . . . . . . . . . . . . . . . 74 54 Section 6.2: long-term temporal order reasoning. . . . . . . . . . . . . . . . . . . 75 55 Section 6.2: short-term temporal order reasoning. . . . . . . . . . . . . . . . . . . 76 56 Section 6.2: short-term and long-term temporal anticipation. . . . . . . . . . . . . 77 57 Section 6.2: temporal localization and reasoning. . . . . . . . . . . . . . . . . . . 78 58 Section 6.3: grounded temporal understanding. . . . . . . . . . . . . . . . . . . . 79 59 Section 7.1: abstract visual stimuli. . . . . . . . . . . . . . . . . . . . . . . . . . . 81 60 Section 7.2: part-object association. . . . . . . . . . . . . . . . . . . . . . . . . . 82 61 Section 7.3: WAIS IQ test. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83 62 Section 7.4: RPM IQ test, entire question page. . . . . . . . . . . . . . . . . . . . 84 63 Section 7.4: RPM IQ test, processed sub-figures. . . . . . . . . . . . . . . . . . . 85 64 Section 8.1: read emotions from facial expressions. . . . . . . . . . . . . . . . . . 86 65 Section 8.2: how different visual contents may arouse emotions. . . . . . . . . . . 87 66 Section 8.2: image aesthetics. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88 67 Section 8.3: emotion conditioned outputs. . . . . . . . . . . . . . . . . . . . . . . 89 68 Section 9.1: spot the differences. . . . . . . . . . . . . . . . . . . . . . . . . . . . 92 69 Section 9.1: spot the differences. . . . . . . . . . . . . . . . . . . . . . . . . . . . 93 70 Section 9.2: industrial defect detection. . . . . . . . . . . . . . . . . . . . . . . . 94 71 Section 9.2: industrial defect detection. . . . . . . . . . . . . . . . . . . . . . . . 95 72 Section 9.2: industrial defect detection. . . . . . . . . . . . . . . . . . . . . . . . 96 73 Section 9.2: safety inspection. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97 74 Section 9.2: grocery checkout. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98 75 Section 9.3: radiology report generation on medical images. . . . . . . . . . . . . 100 76 Section 9.3: radiology report generation on medical images. . . . . . . . . . . . . 101 77 Section 9.3: radiology report generation on medical images. . . . . . . . . . . . . 102 "}
{"page": 4, "image_path": "doc_images/2309.17421v2_4.jpg", "ocr_text": "39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n\nSection 4.4:\nSection 4.4:\nSection 4.5:\nSection 4.5:\nSection 4.5:\nSection 4.5:\nSection 4.6:\nSection 4.6:\nSection 4.6:\nSection 5.1:\nSection 5.1:\nSection 5.2:\nSection 5.2:\nSection 5.3:\nSection 6.1:\nSection 6.2:\nSection 6.2:\nSection 6.2:\nSection 6.2:\nSection 6.3:\nSection 7.1:\nSection 7.2:\nSection 7.3:\nSection 7.4:\nSection 7.4:\nSection 8.1:\nSection 8.2:\nSection 8.2:\nSection 8.3:\nSection 9.1:\nSection 9.1:\nSection 9.2:\nSection 9.2:\nSection 9.2:\nSection 9.2:\nSection 9.2:\nSection 9.3:\nSection 9.3:\n\nSection 9.3:\n\ndocument understanding. ... 2... 0200.00.20... 00 00008.\nlong document understanding. ..................00.\nmultilingual image descriptions. .................00.\nmultilingual scene text recognition... ..............00.\nmultilingual text recognition, translation, and description reasoning.\n\nmultilingual multiculture understanding. .............02.\ngenerate LaTex codes based on the hand-written input... .......\nreconstruct table in Markdown or LaTex code. .............\nwrite codes to replicate the input figure. 2... ..........02.\nunderstand pointing inputs for grounded description... ........\ngrounded description with text-format region coordinates... .....\nexamples of visual referring prompting. ................\nexamples of visual referring prompting. ................\ngenerate pointing outputs... 2... 2 ee\nvideo frames understanding... ..................00.\nlong-term temporal order reasoning. ...............00.\nshort-term temporal order reasoning. ...............00.\nshort-term and long-term temporal anticipation. ............\ntemporal localization and reasoning. ...............04.\ngrounded temporal understanding. ................00.\n\nabstract visual stimuli... 2... eee\n\nemotion conditioned outputs. . 2... 2... ee\nspot the differences... 2... 2 ee ee\nspot the differences... 2... 2 ee ee\nindustrial defect detection, 2... 2... .....0 00020000.\nindustrial defect detection, 2... 2... .....0 00020000.\nindustrial defect detection, 2... 2... .....0 00020000.\nsafety inspection. 2... . 2.2... eee ee\ngrocery checkout. .. 2... 2. ee eee\nradiology report generation on medical images. ............\n\nradiology report generation on medical images. ............\n\nradiology report generation on medical images. ............\n\n62\n\n68\n\n72\n\n76\n\n89\n92\n93\n", "vlm_text": ""}
{"page": 5, "image_path": "doc_images/2309.17421v2_5.jpg", "ocr_text": "78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n\nSection 9.3:\nSection 9.4:\nSection 9.4:\nSection 9.5:\nSection 9.5:\nSection 9.5:\nSection 9.6:\nSection 9.6:\nSection 9.6:\nSection 9.6:\nSection 9.7:\nSection 9.7:\nSection 9.7:\nSection 9.7:\nSection 9.8:\nSection 9.8:\nSection 9.8:\nSection 9.8:\nSection 9.8:\nSection 9.8:\nSection 9.8:\nSection 9.8:\nSection 9.8:\nSection 9.8:\nSection 9.8:\nSection 9.8:\nSection 9.8:\nSection 9.8:\nSection 9.8:\nSection 9.8:\nSection 9.8:\nSection 9.8:\nSection 9.8:\nSection 9.8:\nSection 9.8:\nSection 9.8:\nSection 9.8:\nSection 9.8:\n\nSection 9.8:\n\nradiology report generation with diagnosis history... ......... 103\nauto damage evaluation... 2... 2.2... 0.02.0..000 0000. 105\ninsurance reporting. .. 2... 2.2... 2.00.0. 00 000000004 106\ncustomized captioner for photo organization... ............ 108\ncustomized captioner for photo organization... ............ 109\ndense captioning with segmentation cut-outs... .........00. 110\nevaluation of images generated from texts... 2.2... .....00. 112\nevaluation of images generated from texts... 2.2... .....00. 113\nimageediting, .. 2.2... eee 114\nimageediting, .. 2.2... eee 115\nembodied agent using coffee machine. ................. 117\nembodied agent using coffee machine. ................. 118\nembodied agent indoor navigation. ...............000. 119\nembodied agent indoor navigation. ................00. 120\nweb browsing for Mapo Tofu recipe. ...............00. 122\nweb browsing for Mapo Tofu recipe. ...............00. 123\nweb browsing for Mapo Tofu recipe. ...............00. 124\nweb browsing for Mapo Tofu recipe. ...............00. 125\nweb browsing for Mapo Tofu recipe. ...............00. 126\nweb browsing for today’s news... .............0.000.4 127\nweb browsing for today’s news... .............0.000.4 128\nweb browsing for today’s news... 2... 2. ee ee eee 129\nweb browsing for today’s news... .............0.000.4 130\nweb browsing for today’s news... .............0.000.4 131\nweb browsing for today’s news... .............0.000.4 132\nonline shopping... 2... 2. eee 133\nonline shopping... 2... 2. eee 134\nonline shopping... 2... 2. eee 135\nonline shopping... 2... 2. eee 136\nonline shopping... 2... 2. eee 137\nonline shopping... 2... 2. eee 138\nonline shopping... 2... 2 ee 139\nonline shopping... 2... 2. eee 140\nonline shopping... 2... 2. eee 141\nnotification understanding. .................0..0008. 142\nnotification understanding. .................0..0008. 143\nnotification understanding. .................0..0008. 144\nwatching videos... 2... ee 145\nwatching videos... 2... ee 146\n", "vlm_text": "78 Section 9.3: radiology report generation with diagnosis history. . . . . . . . . . . . 103 79 Section 9.4: auto damage evaluation. . . . . . . . . . . . . . . . . . . . . . . . . . 105 80 Section 9.4: insurance reporting. . . . . . . . . . . . . . . . . . . . . . . . . . . . 106 81 Section 9.5: customized captioner for photo organization. . . . . . . . . . . . . . . 108 82 Section 9.5: customized captioner for photo organization. . . . . . . . . . . . . . . 109 83 Section 9.5: dense captioning with segmentation cut-outs. . . . . . . . . . . . . . . 110 84 Section 9.6: evaluation of images generated from texts. . . . . . . . . . . . . . . . 112 85 Section 9.6: evaluation of images generated from texts. . . . . . . . . . . . . . . . 113 86 Section 9.6: image editing. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114 87 Section 9.6: image editing. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115 88 Section 9.7: embodied agent using coffee machine. . . . . . . . . . . . . . . . . . 117 89 Section 9.7: embodied agent using coffee machine. . . . . . . . . . . . . . . . . . 118 90 Section 9.7: embodied agent indoor navigation. . . . . . . . . . . . . . . . . . . . 119 91 Section 9.7: embodied agent indoor navigation. . . . . . . . . . . . . . . . . . . . 120 92 Section 9.8: web browsing for Mapo Tofu recipe. . . . . . . . . . . . . . . . . . . 122 93 Section 9.8: web browsing for Mapo Tofu recipe. . . . . . . . . . . . . . . . . . . 123 94 Section 9.8: web browsing for Mapo Tofu recipe. . . . . . . . . . . . . . . . . . . 124 95 Section 9.8: web browsing for Mapo Tofu recipe. . . . . . . . . . . . . . . . . . . 125 96 Section 9.8: web browsing for Mapo Tofu recipe. . . . . . . . . . . . . . . . . . . 126 97 Section 9.8: web browsing for today’s news. . . . . . . . . . . . . . . . . . . . . . 127 98 Section 9.8: web browsing for today’s news. . . . . . . . . . . . . . . . . . . . . . 128 99 Section 9.8: web browsing for today’s news. . . . . . . . . . . . . . . . . . . . . . 129 100 Section 9.8: web browsing for today’s news. . . . . . . . . . . . . . . . . . . . . . 130 101 Section 9.8: web browsing for today’s news. . . . . . . . . . . . . . . . . . . . . . 131 102 Section 9.8: web browsing for today’s news. . . . . . . . . . . . . . . . . . . . . . 132 103 Section 9.8: online shopping. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133 104 Section 9.8: online shopping. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 134 105 Section 9.8: online shopping. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135 106 Section 9.8: online shopping. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 136 107 Section 9.8: online shopping. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137 108 Section 9.8: online shopping. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138 109 Section 9.8: online shopping. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139 110 Section 9.8: online shopping. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140 111 Section 9.8: online shopping. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141 112 Section 9.8: notification understanding. . . . . . . . . . . . . . . . . . . . . . . . 142 113 Section 9.8: notification understanding. . . . . . . . . . . . . . . . . . . . . . . . 143 114 Section 9.8: notification understanding. . . . . . . . . . . . . . . . . . . . . . . . 144 115 Section 9.8: watching videos. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145 116 Section 9.8: watching videos. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 146 117 Section 9.8: watching videos. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147 118 Section 9.8: watching videos. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 148 119 Section 9.8: watching videos. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149 120 Section 10.1: multimodal plugins. . . . . . . . . . . . . . . . . . . . . . . . . . . 150 121 Section 10.2: multimodal chains. . . . . . . . . . . . . . . . . . . . . . . . . . . . 151 122 Section 10.3: self-reflection. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153 123 Section 10.3: self-reflection. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 154 124 Section 10.4: self-consistency. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155 "}
{"page": 6, "image_path": "doc_images/2309.17421v2_6.jpg", "ocr_text": "117\n118\n119\n120\n121\n122\n123\n124\n\nSection 9.8: watching videos... 2... ee 147\nSection 9.8: watching videos... 2... ee 148\nSection 9.8: watching videos... 2... ee 149\n\nSection 10.1:\nSection 10.2:\nSection 10.3:\nSection 10.3:\nSection 10.4:\n\nmultimodal plugins. 2... . ee ee 150\nmultimodal chains... 2... ee 151\nself-reflection, . 2.2... 2. ee 153\nself-reflection, . 2.2... 2. ee 154\n\nself-consistency. 2.2... ee 155\n", "vlm_text": ""}
{"page": 7, "image_path": "doc_images/2309.17421v2_7.jpg", "ocr_text": "1 Introduction\n\n1.1 Motivation and Overview\n\nThe breakthroughs in large language models (LLMs) [23, 99, 30, 11, 123, 53] have shown remarkable\nversatilities and capabilities across various domains and tasks. The next evolution in this field,\nlarge multimodal models (LMMs), aims to expand upon the capabilities of LLMs by integrating\nmulti-sensory skills to achieve even stronger general intelligence. Given the dominance of the\nvisual in human senses [33, 58], many LMM studies start with extending the vision capability.\nPreliminary research investigations either finetune a vision encoder to align with a frozen pre-trained\nLLM [125, 7, 71, 55, 42, 13, 48, 157, 79, 35, 146], or use a vision-language model to convert visual\ninputs to text descriptions that LLMs can understand [149, 141, 131, 54, 113, 142]. However, most\nexisting models [13, 48, 157, 79, 35, 69] are of limited model and data scales, potentially restricting\nthe emergence of various intriguing abilities. Consequently, it remains unclear what are the status\nquo and emergent multimodal abilities of LMMs that are developed based on the state-of-the-art\nLLMs, such as GPT-4 (no vision) [99] and PaLM [30, | 1]. In this paper, we report our preliminary\nexplorations with (an early version of) GPT-4V, a state-of-the-art LMM with vision, built based on\nthe SOTA LLM and trained with a large scale of multimodal data.\n\nOur exploration of GPT-4V is guided by the following questions.\n\n1. What are GPT-4V’s supported inputs and working modes? The genericity of multimodal\nmodels inevitably requires the system to work with the arbitrary mix of different input\nmodalities. GPT-4V shows unprecedented ability in understanding and processing an\narbitrary mix of input images, sub-images, texts, scene texts, and visual pointers. We\nalso demonstrate that GPT-4V well supports the test-time techniques observed in LLMs,\nincluding instruction following [102], chain-of-thoughts [136, 66], in-context few-shot\nlearning [23], etc.\n\n2. What are the quality and genericity of GPT-4V’s capabilities on different domains and tasks?\nWe sample queries covering a wide range of domains and tasks to understand GPT-4V’s\ncapabilities, including open-world visual understanding, visual description, multimodal\nknowledge, commonsense, scene text understanding, document reasoning, coding, temporal\nreasoning, abstract reasoning, emotion understanding, and many more. GPT-4V shows\nimpressive human-level capabilities across many of the experimented domains.\n\n3. What are effective ways to use and prompt GPT-4V? GPT-AV is strong in understanding\npixel space edits, such as visual pointers and scene texts drawn on input images. Inspired by\nthis capability, we discuss the “visual referring prompting” that directly edits input images to\ninstruct the task of interest. Visual referring prompting can be seamlessly used together with\nother image and text prompts, presenting a nuanced interface for instruction and example\ndemonstrations.\n\n4. What are promising future directions? Given GPT-4V’s strong capability across domains and\ntasks, we ask what is the next step for multimodal learning, and more broadly for artificial\nintelligence. We organize our thoughts and explorations into two perspectives, i.e., emergent\nnovel application scenarios to focus on, and the future research directions for GPT-4V-based\nsystems. We present our preliminary explorations to inspire future studies.\n\nGuided by the aforementioned problems, we comprehensively organize and list our explored quali-\ntative results. The report contains minimal quantitative benchmark results, and instead consists of\nmainly selected interesting qualitative examples. Despite being less rigorous, this design allows\nfor providing a more comprehensive analysis covering a broad range of domains, tasks, working\nmodes, and prompting techniques, under a fixed capacity. We believe this organized collection of\nexplorations will inspire future works in emerging novel applications, next-generation multimodal\ntask formulation, and developing advanced LMM-based intelligent systems.\n\n1.2 Our Approach in Exploring GPT-4V\n\nGoal of this report. The standard approach for evaluating a system is by benchmarking it against a\nseries of carefully designed datasets, each representing a specific domain and task. One challenge is\nthat some of the existing benchmarks may not be suitable for evaluating LMMs anymore. For example,\n", "vlm_text": "1 Introduction \n1.1 Motivation and Overview \nThe breakthroughs in large language models (LLMs) [ 23 ,  99 ,  30 ,  11 ,  123 ,  53 ] have shown remarkable versa til i ties and capabilities across various domains and tasks. The next evolution in this field, large multimodal models (LMMs), aims to expand upon the capabilities of LLMs by integrating multi-sensory skills to achieve even stronger general intelligence. Given the dominance of the visual in human senses [ 33 ,  58 ], many LMM studies start with extending the vision capability. Preliminary research investigations either finetune a vision encoder to align with a frozen pre-trained LLM [ 125 ,  7 ,  71 ,  55 ,  42 ,  13 ,  48 ,  157 ,  79 ,  35 ,  146 ], or use a vision-language model to convert visual inputs to text descriptions that LLMs can understand [ 149 ,  141 ,  131 ,  54 ,  113 ,  142 ]. However, most existing models [ 13 ,  48 ,  157 ,  79 ,  35 ,  69 ] are of limited model and data scales, potentially restricting the emergence of various intriguing abilities. Consequently, it remains unclear what are the status quo and emergent multimodal abilities of LMMs that are developed based on the state-of-the-art LLMs, such as GPT-4 (no vision) [ 99 ] and PaLM [ 30 ,  11 ]. In this paper, we report our preliminary explorations with (an early version of) GPT-4V, a state-of-the-art LMM with vision, built based on the SOTA LLM and trained with a large scale of multimodal data. \nOur exploration of GPT-4V is guided by the following questions. \n1.  What are GPT-4V’s supported inputs and working modes?  The genericity of multimodal models inevitably requires the system to work with the arbitrary mix of different input modalities. GPT-4V shows unprecedented ability in understanding and processing an arbitrary mix of input images, sub-images, texts, scene texts, and visual pointers. We also demonstrate that GPT-4V well supports the test-time techniques observed in LLMs, including instruction following [ 102 ], chain-of-thoughts [ 136 ,  66 ], in-context few-shot learning [ 23 ],  etc . 2.  What are the quality and genericity of GPT-4V’s capabilities on different domains and tasks? We sample queries covering a wide range of domains and tasks to understand GPT-4V’s capabilities, including open-world visual understanding, visual description, multimodal knowledge, commonsense, scene text understanding, document reasoning, coding, temporal reasoning, abstract reasoning, emotion understanding, and many more. GPT-4V shows impressive human-level capabilities across many of the experimented domains. 3.  What are effective ways to use and prompt GPT-4V?  GPT-4V is strong in understanding pixel space edits, such as visual pointers and scene texts drawn on input images. Inspired by this capability, we discuss the “visual referring prompting” that directly edits input images to instruct the task of interest. Visual referring prompting can be seamlessly used together with other image and text prompts, presenting a nuanced interface for instruction and example demonstrations. 4.  What are promising future directions?  Given GPT-4V’s strong capability across domains and tasks, we ask what is the next step for multimodal learning, and more broadly for artificial intelligence. We organize our thoughts and explorations into two perspectives,  i.e ., emergent novel application scenarios to focus on, and the future research directions for GPT-4V-based systems. We present our preliminary explorations to inspire future studies. \nGuided by the aforementioned problems, we comprehensively organize and list our explored quali- tative results. The report contains minimal quantitative benchmark results, and instead consists of mainly selected interesting qualitative examples. Despite being less rigorous, this design allows for providing a more comprehensive analysis covering a broad range of domains, tasks, working modes, and prompting techniques, under a fixed capacity. We believe this organized collection of explorations will inspire future works in emerging novel applications, next-generation multimodal task formulation, and developing advanced LMM-based intelligent systems. \n1.2 Our Approach in Exploring GPT-4V \nGoal of this report.  The standard approach for evaluating a system is by benchmarking it against a series of carefully designed datasets, each representing a specific domain and task. One challenge is that some of the existing benchmarks may not be suitable for evaluating LMMs anymore. For example, the image captioning outputs of LMMs are much richer and contain more detailed descriptions than the ground truths in the image captioning benchmark datasets [ 27 ]. There is also a lack of public information regarding GPT-4V’s large-scale pre-training, which may violate the train-test setup for certain existing datasets and invalidate those benchmark numbers. Because of this, restricting the evaluation to  existing  benchmarks and metrics may unintentionally narrow the scope of GPT-4V’s assessment. Developing a comprehensive list of next-generation evaluation tasks and benchmarks would be the ideal ultimate solution. However, we left those as future work due to the significant efforts required. "}
{"page": 8, "image_path": "doc_images/2309.17421v2_8.jpg", "ocr_text": "the image captioning outputs of LMMs are much richer and contain more detailed descriptions than\nthe ground truths in the image captioning benchmark datasets [27]. There is also a lack of public\ninformation regarding GPT-4V’s large-scale pre-training, which may violate the train-test setup for\ncertain existing datasets and invalidate those benchmark numbers. Because of this, restricting the\nevaluation to existing benchmarks and metrics may unintentionally narrow the scope of GPT-4V’s\nassessment. Developing a comprehensive list of next-generation evaluation tasks and benchmarks\nwould be the ideal ultimate solution. However, we left those as future work due to the significant\nefforts required.\n\nIn lieu of quantitative benchmarking, this paper focuses on using qualitative results to provide a\nglimpse of GPT-4V’s new capabilities and potential emerging use cases. Our goal is to discover and\npreview what GPT-4V might already be capable of, even though these novel capabilities may not yet\nbe entirely reliable. We hope this collection of explorations will inspire future research in establishing\nquantitative benchmarks for next-generation multimodal tasks, modernizing existing benchmarks,\nfurther improving model performance and system reliability, and sparkling innovation in emerging\nuse cases. Following this, we will delve into the core designs for our approach to exploring GPT-4V.\n\nSample selection guidance. This report focuses on presenting qualitative results to showcase the\npotential capabilities of GPT-4V, rather than providing comprehensive quantitative benchmark results.\nThis naturally raises the question of the reliability of the showcased examples. The examples featured\nin this report may require careful instruction tuning to amplify GPT-4V’s corresponding capabilities.\nIt should be noted that some complex cases may only work with the specifically designed prompts.\nAs such, the capabilities demonstrated may not consistently work across different samples. Instead of\nshowing only the reliable functionalities, the primary objective of this report is to provide readers\nwith a list of our discovered potential capabilities of GPT-4V, which might otherwise be overlooked\nafter a few unsuccessful trials.\n\nSample selection to prevent mere memorizing from training. A fundamental design consideration\nin qualitative reports [24] is discerning models’ true capabilities from merely memorizing responses\nfrom training samples or making educated guesses based on hints from instructions and in-context\nexamples. We carefully control both the images and text in the input prompts to prevent them from\nbeing seen during GPT-4V training. We generate original text queries from scratch, and try to use\nimages that are either not accessible online or with a timestamp beyond April 2023. We will indicate\ninstances where a specific sample does not meet this criterion, e.g., deliberately using samples from\nspecific vision-language datasets. Beyond ensuring that samples are unseen, we incorporate rationale\nqueries into the process. These queries are designed to probe the model’s reasoning process, thereby\nvalidating GPT-4V’s possession of the intended capability.\n\nThe default working mode. As later detailed in Section 3, GPT-4V works effectively in different\nworking modes, including zero-shot learning with instructions, in-context few-shot learning, etc.\nAmong them, this report primarily focuses on zero-shot instruction tuning, as opposed to in-context\nfew-shot learning. This design is to prevent potential information leakage from in-context examples.\nWhile in-context few-shot examples can enhance performance and reliability, they do not consistently\nengender new capabilities. As such, we designate zero-shot as the default working mode for\npresentation, and reduce the use of in-context examples to minimize examples’ impacts on the\nassessed capabilities.\n\n1.3. How to Read this Report?\n\nThis report documents the explorations of GPT-4V conducted by researchers in the computer vision\nand vision-language multimodal field. It is primarily geared towards fellow researchers in related\ndisciplines who seek to gain a qualitative impression of LMM’s capabilities and understand its\ndifference from traditional vision-language models. The report is also prepared for professionals\nfor whom AI or computer science may be outside their specialties, to assist them in conceptualizing\nways LMMs can enhance their proficiency within their distinct domains of expertise.\n\nWe give an overview of the report, structured around the four core questions that guide our exploration.\n\n1. What are GPT-4V’s supported inputs and working modes? Section 2 summarizes GPT-4V’s\nsupported inputs and presents an overview of their corresponding use cases. Based on\nthe flexible interleaved image-text inputs, Section 3 discusses GPT-4V’s different work-\ning modes, such as instruction tuning, in-context learning, and other emergent usages.\n", "vlm_text": "\nIn lieu of quantitative benchmarking, this paper focuses on using qualitative results to provide a glimpse of GPT-4V’s new capabilities and potential emerging use cases. Our goal is to discover and preview what GPT-4V might already be capable of, even though these novel capabilities may not yet be entirely reliable. We hope this collection of explorations will inspire future research in establishing quantitative benchmarks for next-generation multimodal tasks, modernizing existing benchmarks, further improving model performance and system reliability, and sparkling innovation in emerging use cases. Following this, we will delve into the core designs for our approach to exploring GPT-4V. \nSample selection guidance.  This report focuses on presenting qualitative results to showcase the potential capabilities of GPT-4V, rather than providing comprehensive quantitative benchmark results. This naturally raises the question of the reliability of the showcased examples. The examples featured in this report may require careful instruction tuning to amplify GPT-4V’s corresponding capabilities. It should be noted that some complex cases may only work with the specifically designed prompts. As such, the capabilities demonstrated may not consistently work across different samples. Instead of showing only the reliable functionalities, the primary objective of this report is to provide readers with a list of our discovered potential capabilities of GPT-4V, which might otherwise be overlooked after a few unsuccessful trials. \nSample selection to prevent mere memorizing from training.  A fundamental design consideration in qualitative reports [ 24 ] is discerning models’ true capabilities from merely memorizing responses from training samples or making educated guesses based on hints from instructions and in-context examples. We carefully control both the images and text in the input prompts to prevent them from being seen during GPT-4V training. We generate original text queries from scratch, and try to use images that are either not accessible online or with a timestamp beyond April 2023. We will indicate instances where a specific sample does not meet this criterion,  e.g ., deliberately using samples from specific vision-language datasets. Beyond ensuring that samples are unseen, we incorporate rationale queries into the process. These queries are designed to probe the model’s reasoning process, thereby validating GPT-4V’s possession of the intended capability. \nThe default working mode.  As later detailed in Section  3 , GPT-4V works effectively in different working modes, including zero-shot learning with instructions, in-context few-shot learning,  etc . Among them, this report primarily focuses on zero-shot instruction tuning, as opposed to in-context few-shot learning. This design is to prevent potential information leakage from in-context examples. While in-context few-shot examples can enhance performance and reliability, they do not consistently engender new capabilities. As such, we designate zero-shot as the default working mode for presentation, and reduce the use of in-context examples to minimize examples’ impacts on the assessed capabilities. \n1.3 How to Read this Report? \nThis report documents the explorations of GPT-4V conducted by researchers in the computer vision and vision-language multimodal field. It is primarily geared towards fellow researchers in related disciplines who seek to gain a qualitative impression of LMM’s capabilities and understand its difference from traditional vision-language models. The report is also prepared for professionals for whom AI or computer science may be outside their specialties, to assist them in conceptualizing ways LMMs can enhance their proficiency within their distinct domains of expertise. \nWe give an overview of the report, structured around the four core questions that guide our exploration. \n1.  What are GPT-4V’s supported inputs and working modes?  Section  2  summarizes GPT-4V’s supported inputs and presents an overview of their corresponding use cases. Based on the flexible interleaved image-text inputs, Section  3  discusses GPT-4V’s different work- ing modes, such as instruction tuning, in-context learning, and other emergent usages. "}
{"page": 9, "image_path": "doc_images/2309.17421v2_9.jpg", "ocr_text": "The section covers the novel ways of using and prompting GPT-4V, aiming to provide a\ncomprehensive overview of how we will use GPT-4V in subsequent sections.\n\n2. What are the quality and genericity of GPT-4V’s capabilities on different domains and tasks?\nThe exploration of this question makes up a large portion of the report. Section 4 provides a\ncomprehensive analysis covering a wide range of vision and vision-language scenarios, in-\ncluding image description and recognition on different domains, dense visual understanding,\nmultimodal knowledge, commonsense, scene text understanding, document reasoning, and\nmany more. We also separate out several novel and interesting capabilities. Section 6 studies\nGPT-4V’s capability in temporal, motion, and video understanding. Section 7 explores the\nabstract visual understanding and reasoning capability, and Section 8 covers the emotion\nand sentiment understanding.\n\n3. What are effective ways to use and prompt GPT-4V? We start the discussion on this question\nfrom the working mode and prompting method introduction in Section 3. In Section 5, we\nhighlight one novel promoting technique, namely visual referring prompting, which draws\nvisual pointers and scene texts on input images to prompt GPT-4V. We demonstrate the flexi-\nble prompting methods, such as the combination of instruction and example demonstrations,\nthroughout the report in the given examples.\n\n4. What are promising future directions? Section 9 focuses on the novel use cases facilitated\nby GPT-4V. We hope these initial examples could inspire future works to design new task\nsetups and present rigorous benchmarks. Section 10 imagines powerful future systems\nthat can be built based on GPT-4V, such as the multimodal plugins, multimodal chains,\nself-reflection, self-consistency, and retrieval-augmented LMMs, etc.\n\nIn addition to this overview and the table of contents, we have also included a list of figures. The\nlist enumerates the qualitative examples detailed within the report, serving as an additional tool to\nhelp readers navigate to their scenarios of interest.\n\n2 GPT-4V’s Input Modes\n\nThis section summarizes GPT-4V’s supported inputs, i.e., functioning as a uni-model language model\nwith the text-only inputs, taking single image-text pair optionally with only a single image, and\ntaking interleaved image-text pairs optionally with only multiple image inputs. We next highlight the\nrepresentative use cases under these different input modes.\n\n2.1 Text-only Inputs\n\nGPT-4V’s strong language capability enables it to serve as an effective unimodal language model [38,\n108, 23] with text-only inputs. Operating exclusively with text for both input and output, GPT-4V is\ncapable of performing a wide variety of language and coding tasks. We refer readers to the GPT-4\ntechnical report [99] for the comprehensive and in-depth analysis of GPT-4V’s language and coding\ncapabilities, as well as the comparison with GPT-4 (no vision).\n\n2.2 Single Image-text Pair\n\nGPT-4V, the latest large multimodal model, takes images and texts as inputs to generate textual\noutputs. In line with existing general-purpose vision-language models [9, 81, 73, 8, 70, 122, 120,\n155, 28, 83, 45, 74, 57, 64, 72, 132, 29, 140, 41, 7, 128, 46, 40, 158, 69], GPT-4V can take a single\nimage-text pair or a single image as input to perform various vision and vision-language tasks,\nsuch as image recognition [37], object localization [153], image captioning [27], visual question\nanswering [12], visual dialogue [36], dense caption [62], and so on. We note that the text in the\nimage-text pair can be used either as instruction like “describe the image” for captioning, or as the\nquery input like the question in visual question answering. GPT-4V’s exceptional intelligence is\nexemplified by its significantly enhanced performance and generalizability compared to prior arts. A\ncomprehensive analysis of its multimodal capabilities on various domains is detailed in Section 4.\n\n10\n", "vlm_text": "The section covers the novel ways of using and prompting GPT-4V, aiming to provide a comprehensive overview of how we will use GPT-4V in subsequent sections. \n2.  What are the quality and genericity of GPT-4V’s capabilities on different domains and tasks? The exploration of this question makes up a large portion of the report. Section  4  provides a comprehensive analysis covering a wide range of vision and vision-language scenarios, in- cluding image description and recognition on different domains, dense visual understanding, multimodal knowledge, commonsense, scene text understanding, document reasoning, and many more. We also separate out several novel and interesting capabilities. Section  6  studies GPT-4V’s capability in temporal, motion, and video understanding. Section  7  explores the abstract visual understanding and reasoning capability, and Section  8  covers the emotion \nand sentiment understanding. 3.  What are effective ways to use and prompt GPT-4V?  We start the discussion on this question from the working mode and prompting method introduction in Section  3 . In Section  5 , we highlight one novel promoting technique, namely visual referring prompting, which draws visual pointers and scene texts on input images to prompt GPT-4V. We demonstrate the flexi- ble prompting methods, such as the combination of instruction and example demonstrations, throughout the report in the given examples. 4.  What are promising future directions?  Section  9  focuses on the novel use cases facilitated by GPT-4V. We hope these initial examples could inspire future works to design new task setups and present rigorous benchmarks. Section  10  imagines powerful future systems that can be built based on GPT-4V, such as the multimodal plugins, multimodal chains, self-reflection, self-consistency, and retrieval-augmented LMMs,  etc . \nIn addition to this overview and the  table of contents , we have also included a  list of figures . The list enumerates the qualitative examples detailed within the report, serving as an additional tool to help readers navigate to their scenarios of interest. \n2 GPT-4V’s Input Modes \nThis section summarizes GPT-4V’s supported inputs,  i.e ., functioning as a uni-model language model with the text-only inputs, taking single image-text pair optionally with only a single image, and taking interleaved image-text pairs optionally with only multiple image inputs. We next highlight the representative use cases under these different input modes. \n2.1 Text-only Inputs \nGPT-4V’s strong language capability enables it to serve as an effective unimodal language model [ 38 , 108 ,  23 ] with text-only inputs. Operating exclusively with text for both input and output, GPT-4V is capable of performing a wide variety of language and coding tasks. We refer readers to the GPT-4 technical report [ 99 ] for the comprehensive and in-depth analysis of GPT-4V’s language and coding capabilities, as well as the comparison with GPT-4 (no vision). \n2.2 Single Image-text Pair \nGPT-4V, the latest large multimodal model, takes images and texts as inputs to generate textual outputs. In line with existing general-purpose vision-language models [ 9 ,  81 ,  73 ,  8 ,  70 ,  122 ,  120 , 155 ,  28 ,  83 ,  45 ,  74 ,  57 ,  64 ,  72 ,  132 ,  29 ,  140 ,  41 ,  7 ,  128 ,  46 ,  40 ,  158 ,  69 ], GPT-4V can take a single image-text pair or a single image as input to perform various vision and vision-language tasks, such as image recognition [ 37 ], object localization [ 153 ], image captioning [ 27 ], visual question answering [ 12 ], visual dialogue [ 36 ], dense caption [ 62 ], and so on. We note that the text in the image-text pair can be used either as instruction like “describe the image” for captioning, or as the query input like the question in visual question answering. GPT-4V’s exceptional intelligence is exemplified by its significantly enhanced performance and general iz ability compared to prior arts. A comprehensive analysis of its multimodal capabilities on various domains is detailed in Section  4 . "}
{"page": 10, "image_path": "doc_images/2309.17421v2_10.jpg", "ocr_text": "Sec. 2.3 Interleaved Image-text Inputs\n\nInterleaved Image-text Pairs\n\nPrompt: Prompt:\nHow much did | pay for tax? Prompt Continued How much should | pay for the beer on the table\naccording to the price on the menu?\nioitetinas\nrd\n‘ome\netait\n\nHere is a photo of the menu.\n\nGPT-4V:\n\nIn the first receipt, you paid $3.72 for tax. The\n\ninformation is located at the bottom of the receipt. In\n\nthe second receipt, you paid $42.23 for tax. The\n\ninformation is located near the bottom of the receipt, Ie\nunder the \"Sales Tax\" line. In the third receipt, you paid GPT-4V:\n$4.58 for tax. The information is located at the bottom\nof the receipt, under the \"TAX\" line. In total, you paid\n$50.53 in tax across all three receipts.\n\nAccording to the menu, the price for a Magna beer\nis $6. Since there are two Magna beers on the table,\nthe total cost for the beers would be $12.\n\nFigure 1: GPT-4V can work with multi-image and interleaved image-text inputs. Check Section 2.3\nfor detailed discussions.\n\n2.3 Interleaved Image-text Inputs\n\nThe generality of GPT-4V is further enhanced by its ability to handle flexibly interleaved image-text\ninputs. The interleaved image-text inputs can be either visually centric such as multiple images\nwith a short question or instruction, text-centric such as a long webpage with two inserted images,\nor a balanced mixture of images and texts. This mode of mixed input provides flexibility for a\nwide array of applications. For example, it can compute the total tax paid across multiple receipt\nimages, as shown in Figure 1. It also enables processing multiple input images and extracting queried\ninformation. GPT-4V could also effectively associate information across interleaved image-text\ninputs, such as finding the beer price on the menu, counting the number of beers, and returning the\ntotal cost, as shown in Figure |. Beyond direct applications, processing interleaved image-text inputs\nserves as a fundamental component for in-context few-shot learning and other advanced test-time\nprompting techniques, thereby further boosting GPT-4V’s generality. We demonstrate these intriguing\nnovel usages in the next section, Section 3.\n\n11\n", "vlm_text": "The image presents an example from a document discussing GPT-4V's capability to process interleaved image-text inputs. It features two prompts with corresponding images and outputs from GPT-4V.\n\nOn the left side:\n- There is a prompt asking about the tax paid, accompanied by images of three different receipts.\n- GPT-4V responds by providing the tax amounts from each receipt, totaling $50.53.\n\nOn the right side:\n- A prompt inquires about the price of beer on a table, as per a menu.\n- An image shows two beers on a table.\n- Below it is an image of a menu displaying drink prices.\n- GPT-4V identifies the Magna beer's price as $6 each and calculates a total of $12 for the two beers.\n\nThe captions and text in the image demonstrate the model's functionality in interpreting and answering questions based on both textual prompts and visual cues.\n2.3 Interleaved Image-text Inputs \nThe generality of GPT-4V is further enhanced by its ability to handle flexibly interleaved image-text inputs. The interleaved image-text inputs can be either visually centric such as multiple images with a short question or instruction, text-centric such as a long webpage with two inserted images, or a balanced mixture of images and texts. This mode of mixed input provides flexibility for a wide array of applications. For example, it can compute the total tax paid across multiple receipt images, as shown in Figure  1 . It also enables processing multiple input images and extracting queried information. GPT-4V could also effectively associate information across interleaved image-text inputs, such as finding the beer price on the menu, counting the number of beers, and returning the total cost, as shown in Figure  1 . Beyond direct applications, processing interleaved image-text inputs serves as a fundamental component for in-context few-shot learning and other advanced test-time prompting techniques, thereby further boosting GPT-4V’s generality. We demonstrate these intriguing novel usages in the next section, Section  3 . "}
{"page": 11, "image_path": "doc_images/2309.17421v2_11.jpg", "ocr_text": "3. GPT-4V’s Working Modes and Prompting Techniques\n\nSec. 3.1 Following Text Instructions\n\nFollowing Text Instructions\n\nPrompt:\nDescribe the image with a sentence consisting\nof three/six/nine words.\n\nGPT-4V:\n\nThree: Seaside dining table\n\nSix: Dining table overlooking ocean with beer\nNine: Outdoor restaurant table with ocean view\nand a beer\n\nWrite a sentence for the image consisting only\nof words starting with the letter 'B'/ \"D'/ 'T’.\n\nGPT-4V:\n\n‘B': Beachside bar boasts beautiful blue\nbackdrop.\n\n\"D': Dining deck displays delightful drinks,\ndistant docks.\n\n'T': Tabletop terrace teeming with tropical\ntranquility.\n\nPrompt:\nWhat would the missing image look like?\n\n?\n\nGPT-4V:\n\nThe missing image would likely be a variation of the\ndiamond or star shape, as seen in the other three images. It\ncould be a diamond with a different pattern or color, or a star\nwith a different number of points or color.\n\nHere are three sub images arranged in a 2-by-2 matrix. First,\nlook at the two images in the first column, in the top left and\nbottom left. Then, find out the pattern in the first column.\nbased on the left two images. Next, use the found pattern\nand the image in the top right to infer the missing figure.\nFinally, describe what would the missing image look like?\n\n°?\n\nGPT-4V:\n\nThe pattern in the first column is that the shape in the bottom\nleft image has a dot in the center of the shape in the top left\nimage. Therefore, the missing image in the bottom right\nwould be a star with a dot in the center, similar to the image\nin the top right but with a dot in the center.\n\nFigure 2: GPT-4V can understand and follow text instructions, to generate the desired text outputs\n\nor learn to perform a new task. ‘Red highlights the less informative answer. Check Section 3.1 for\ndetailed discussions.\n\n12\n", "vlm_text": "3 GPT-4V’s Working Modes and Prompting Techniques \nSec. 3.1 Following Text Instructions \nFollowing Text Instructions \nPrompt: \nDescribe the image with a sentence consisting  of  three/six/nine  words. \nThe image shows an outdoor seating area of a restaurant or cafe near the beach. There are tables and chairs with umbrellas, and two tables are shown in the foreground. On one of the tables, there is a bottle of Magna beer, a bottle of water, and a glass. Napkins are placed on both tables. In the background, there is a view of the ocean, a few palm trees, and some string lights hanging across the seating area. The scene suggests a relaxed, beachside atmosphere.\nGPT-4V: \nThree:  Seaside dining table Six:   Dining table overlooking ocean with beer Nine:  Outdoor restaurant table with ocean view  and a beer \nPrompt: \nWrite a sentence for the image consisting only  of words starting with the letter  'B'/ 'D'/ 'T' . \nThe image shows an outdoor dining area with a view of the ocean. There's a table with a bottle of beer labeled \"MAGNA\" and a bottle of water on it, along with napkins. In the background, there are white umbrellas, chairs, and palm trees, with string lights overhead. The setting has a relaxed, beachside atmosphere.\nGPT-4V: \n'B':   Beachside bar boasts beautiful blue  backdrop. 'D':   Dining deck displays delightful drinks,  distant docks. 'T':   Tabletop terrace teeming with tropical  tranquility. \nPrompt: \nWhat would the missing image look like? \nThe image shows a sequence puzzle with three shapes on a light blue background. The first shape is a diamond, the second is a four-pointed star, and the third is a diamond containing a small dot in the center. There is a question mark indicating where the next shape in the sequence should go. The puzzle likely requires identifying a pattern in the shapes.\nGPT-4V: \nThe missing image would likely be a variation of the  diamond or star shape, as seen in the other three images. It  could be a diamond with a different pattern or color, or a star  with a different number of points or color. It is difficult to say  for sure without more context, but it would likely follow the  same theme as the other images. \nPrompt: \nHere are three sub images arranged in a 2-by-2 matrix. First,  look at the two images in the first column, in the top left and  bottom left. Then, find out the pattern in the first column  based on the left two images. Next, use the found pattern  and the image in the top right to infer the missing figure.  Finally, describe what would the missing image look like? \nThe image shows a 2x2 grid with three shapes and a question mark in the bottom right corner. \n\n- The top left shape is a diamond.\n- The top right shape is a star.\n- The bottom left shape is a diamond with a dot in the center.\n- The bottom right corner contains a question mark, suggesting a missing shape or pattern continuation is expected.\nGPT-4V: \nThe pattern in the first column is that the shape in the bottom  left image has a dot in the center of the shape in the top left  image. Therefore, the missing image in the bottom right  would be a star with a dot in the center, similar to the image  in the top right but with a dot in the center. \nFigure 2: GPT-4V can understand and follow text instructions, to generate the desired text outputs or learn to perform a new task.  Red  highlights the less informative answer. Check Section  3.1  for detailed discussions. "}
{"page": 12, "image_path": "doc_images/2309.17421v2_12.jpg", "ocr_text": "Sec. 3.1 Following Text Instructions\n\nConstrained Prompting\nPrompt:\n\nPlease read the text in this image and return the information in the following JSON format (note xxx is placeholder, if\nthe information is not available in the image, put \"N/A\" instead).\n\n{\"class\": xxx, \"DLN\": xxx, \"DOB\": xxx, \"Name\": xxx, \"Address\": xxx, \"EXP\": xxx, \"ISS\": xxx, \"SEX\": xxx, \"HGT\": xxx,\n\"WGT\": xxx, \"EYES\": xxx, \"HAIR\": xxx, \"DONOR\": xxx}\n\n'WGT\": \"185 Ibs\",\n\nPlease read the text in this image and return the information in the following JSON format (note xxx is placeholder, if\nthe information is not available in the image, put \"N/A\" instead).\n\n{\"class\": xxx, \"DLN\": xxx, \"DOB\": xxx, \"Name\": xxx, \"Address\": xxx, \"EXP\": xxx, \"ISS\": xxx, \"SEX\": xxx, \"HGT\": xxx,\n\"WGT\": xxx, \"EYES\": xxx, \"HAIR\": xxx, \"DONOR\": xxx}\n\nCalifornia* DRIVER LICENSE KA\n\n011234568 classe\nexe 08/31/2014. END NONE\n\nLNCARDHOLDER\nFNIMA }\nTR\n\n2570 247H\\STREET\nANYTOWN, CA 95018 .\n\npos 08/37/4977 zs A\nRSTRNONE 08311977\n\nise\nMa. Cordell scr 505 WoT 128 we\n\nGPT-4V:\n{\"class\":\nAnytown, CA 95818'\n\nPlease read the text in this image and return the information in the following JSON format (note xxx is placeholder, if\nthe information is not available in the image, put \"N/A\" instead).\n\n{\"Surname\": xxx, \"Given Name\": xxx, \"USCIS #\": xxx, \"Category\": xxx, \"Country of Birth\": xxx, \"Date of Birth\": xxx,\n\"SEX\": xxx, \"Card Expires\": xxx, \"Resident Since\": xxx}\n\nFigure 3: Constrained prompting to return in JSON format. Images are example IDs for samples.\n‘Red highlights the wrong answer. Check Section 3.1 for detailed discussions.\n\n13\n", "vlm_text": "Constrained Prompting \nPrompt: \nPlease read the text in this image and return the information in the following JSON format (note xxx is placeholder, if  the information is not available in the image, put \"N/A\" instead). {\"class\": xxx, \"DLN\": xxx, \"DOB\": xxx, \"Name\": xxx, \"Address\": xxx, \"EXP\": xxx, \"ISS\": xxx, \"SEX\": xxx, \"HGT\": xxx,  \"WGT\": xxx, \"EYES\": xxx, \"HAIR\": xxx, \"DONOR\": xxx} \nThis is an image of an Arizona driver's license. It includes a photo and personal information typically found on such an ID, like name, address, birth date, height, weight, eye and hair color, and license number. It also shows the license class, expiration, and issue dates. The text indicates the individual is a veteran and an organ donor.\nGPT-4V: \n{\"class\": \"D\", \"DLN\": \"D08954796\", \"DOB\": \"01/01/1974\", \"Name\": \"Jelani Sample\", \"Address\": \"123 Main St,  Phoenix, AZ 85007\", \"EXP\": \"03/01/2024\", \"ISS\": \"03/01/2016\", \"SEX\": \"M\", \"HGT\": \"5-09\", \"WGT\": \"185 lbs\", \"EYES\": \"BRO\", \"HAIR\": \"N/A\", \"DONOR\": \"Y\"}  \nPrompt: \nPlease read the text in this image and return the information in the following JSON format (note xxx is placeholder, if  the information is not available in the image, put \"N/A\" instead). {\"class\": xxx, \"DLN\": xxx, \"DOB\": xxx, \"Name\": xxx, \"Address\": xxx, \"EXP\": xxx, \"ISS\": xxx, \"SEX\": xxx, \"HGT\": xxx,  \"WGT\": xxx, \"EYES\": xxx, \"HAIR\": xxx, \"DONOR\": xxx} \nThis is a California driver license. It includes details such as:\n\n- A photo of the cardholder\n- The cardholder's name: Ima Cardholder\n- Address: 2570 24th Street, Anytown, CA 95818\n- Date of birth: 08/31/1977\n- License number: I1234568\n- Expiration date: 08/31/2014\n- Additional details like height, weight, eye and hair color, and veteran status.\n\nThis appears to be a sample or simulated license for illustrative purposes.\nGPT-4V: \n{\"class\": \"C\", \"DLN\": \"1234568\", \"DOB\": \"08/31/1977\", \"Name\": \"Ima Cardholder\", \"Address\": \"2570 24th Street,  Anytown, CA 95818\", \"EXP\": \"08/31/2014\", \"ISS\": \"08/31/2011\", \"SEX\": \"F\", \"HGT\": \"5-05\", \"WGT\": \"125 lbs\", \"EYES\": \"BRN\", \"HAIR\": \"BRN\", \"DONOR\": \"VETERAN\"}   \nPrompt: \nPlease read the text in this image and return the information in the following JSON format (note xxx is placeholder, if  the information is not available in the image, put \"N/A\" instead). {\"Surname\": xxx, \"Given Name\": xxx, \"USCIS #\": xxx, \"Category\": xxx, \"Country of Birth\": xxx, \"Date of Birth\": xxx,  \"SEX\": xxx, \"Card Expires\": xxx, \"Resident Since\": xxx} \nThe image is of a United States of America Permanent Resident Card, commonly known as a Green Card. It includes details such as the surname \"STEVENS,\" given name \"ICHIKA,\" USCIS number \"000-000-000,\" category \"IR1,\" country of birth \"Japan,\" date of birth \"18 AUG 1988,\" sex \"F,\" card expiration date \"07/04/2027,\" and resident since \"07/05/2017.\" It also has an image of the Statue of Liberty, the American flag, and a fingerprint.\nGPT-4V: \n{\"Surname\": \"STEVENS\", \"Given Name\": \"ICHIKA\", \"USCIS #\": \"000-000-000\", \"Category\": \"IR1\", \"Country of Birth\":  \"Japan\", \"Date of Birth\": \"18 AUG 1988\", \"SEX\": \"F\", \"Card Expires\": \"07/04/2027\", \"Resident Since\": \"07/05/2017\"}  \nFigure 3: Constrained prompting to return in JSON format. Images are example IDs for samples. Red highlights the wrong answer. Check Section  3.1  for detailed discussions. "}
{"page": 13, "image_path": "doc_images/2309.17421v2_13.jpg", "ocr_text": "Sec. 3.1 Following Text Instructions\n\nCondition on Good Performance\n\nPrompt:\nCount the number of apples in the image.\n\nGPT-4V:\nThere are 12/apples i\n\nPrompt:\nCount the number of apples in the image. Let's think step-by-step.\n\nBeg Note: Same query image as shown above, resized to save space\n\nGPT-4V:\n\nStep 1: Look at the image and identify all the apples.\n\nStep 2: Start counting the apples one by one.\n\nStep 3: Make sure not to miss any apples or count the same apple twice.\n\nStep 4: After counting all the apples, double check to make sure the count is correct.\nAnswer: There are 42/apples in the image.\n\nCount the number of apples in the image. Let's count the apples row-by-row.\n\nae | Note: Same query image as shown above, resized to save space\n\nGPT-4V:\n\nFirst row: 4 apples\n\nTotal number of apples in the image: 11 apples\n\nCount the number of apples in the image. First count how many rows of apples are there, and then count the\napples in each row, and finally sum them up to get the total number.\n\neer. | Note: Same query image as shown above, resized to save space\n\nGPT-4V:\n\nThere are 3 rows of apples in the image.\n\nThe top row has 5 apples.\n\nThe middle row has 4 apples.\n\nThe bottom row has 6 apples.\n\nTherefore, the total number of apples in the image is 54 #6 =15 apples!\n\nYou are an expert in counting things in the image. Let's count the number of apples in the image below row by row\nto be sure we have the right answer.\n\nSee Note: Same query image as shown above, resized to save space\n\nGPT-4V:\n\nRow 1: 4 apples\n\nRow 2: 4 apples\n\nRow 3: 3 apples\n\nTotal number of apples in the image: 11 apples\n\nFigure 4: Condition on good performance to improve counting. Green (Red!) highlights the correct\n\n(wrong) answer. Blue indicates different ways to prompting in addition to the basic requirement of\n“Count the number of apples in the image.” Check Section 3.1 for detailed discussions.\n\n14\n", "vlm_text": "Sec. 3.1 Following Text Instructions \nCondition on Good Performance \nPrompt: Count the number of apples in the image. \nThe image shows a group of apples, varying in shades of red, yellow, and some green.\nGPT-4V: There are 12 apples in the image. \nPrompt: Count the number of apples in the image. Let’s think step-by-step. \nThe image shows a collection of apples, mostly red with some yellowish tinges. There are eleven apples arranged closely together.\nNote: Same query image as shown above, resized to save space \nGPT-4V: \nStep 1: Look at the image and identify all the apples.  Step 2: Start counting the apples one by one.  Step 3: Make sure not to miss any apples or count the same apple twice.  Step 4: After counting all the apples, double check to make sure the count is correct.  Answer: There are 12 apples in the image. \nPrompt: Count the number of apples in the image. Let’s count the apples row-by-row. \nNote: Same query image as shown above, resized to save space \nPrompt: Count the number of apples in the image. First count how many rows of apples are there, and then count the  apples in each row, and finally sum them up to get the total number.  \nNote: Same query image as shown above, resized to save space \nGPT-4V: There are 3 rows of apples in the image.  The top row has 5 apples. The middle row has 4 apples. The bottom row has 6 apples. \nTherefore, the total number of apples in the image is  $5+4+6=15$   apples.  \nPrompt: You are an expert in counting things in the image. Let's count the number of apples in the image below row by row  to be sure we have the right answer. \nThe image shows a group of apples, varying in colors from green to red, arranged together.\nNote: Same query image as shown above, resized to save space \nGPT-4V: Row 1: 4 apples  Row 2: 4 apples  Row 3: 3 apples  Total number of apples in the image: 11 apples \nFigure 4: Condition on good performance to improve counting.  Green  (  Red  ) highlights the correct\n\n (wrong) answer.  Blue  indicates different ways to prompting in addition to the basic requirement of\n\n “Count the number of apples in the image.” Check Section  3.1  for detailed discussions. "}
{"page": 14, "image_path": "doc_images/2309.17421v2_14.jpg", "ocr_text": "3.1 Following Text Instructions\n\nOne unique strength of GPT-4V is its generality, partially achieved via its strong capability in\nunderstanding and following text instructions [102, 96, 134, 111]. Instructions provide a natural way\nto define and customize the desired output text for arbitrary vision-language use cases. Figure 2\nshows an example of image descriptions with constraints on sentence length and the words to use.\nAlternatively, on the input side, GPT-4V could understand the detailed instructions to perform\nchallenging tasks, such as enabling GPT-4V to better interpret the abstract reasoning question by\nproviding instructions on intermediate steps. The ability to learn new tasks from instructions shows\ngreat potential in adapting to various unseen applications and tasks, as detailed in Section 9. In line\nwith recent studies [7, 13, 48, 157, 79, 35], the instructions discussed in this subsection are mostly in\nthe text format, providing language descriptions of the interested task. We will discuss GPT-4V’s\nunique capability of following multimodal example-grounded instructions later in Section 3.3.\n\nIn addition, we showcase how text instructions play an important role in shaping GPT-4V’s response\nwith two techniques adopted from LLM literature [3, 156], (i) “constrained prompting” so that\nGPT-4V responds in a certain format; and (ii) “condition on good performance” that explicitly asks\nfor good performance from GPT-4V.\n\nConstrained prompting. In Figure 3, we prompt GPT-4V to read the text in the image and return\nthe information in a specific JSON format. Although GPT-4V makes some mistakes in extracting\nthe corresponding information from driver’s licenses, the responses are constrained to the JSON\nformat specified in the text instruction. We leverage this technique for certain application scenarios in\nSection 9.\n\nCondition on good performance. One observation about LLMs is that LLMs don’t want to\nsucceed [10]. Rather, they want to imitate training sets with a spectrum of performance qualities. If\nthe user wants to succeed in a task given to the model, the user should explicitly ask for it, which\nhas proven useful in improving the performance of LLMs [156]. In the context of LMMs, we have\nsimilar observations. In Figure 4, we compare the model’s response to different text instructions\nfor counting. We start with a simple and clear prompt: “Count the number of apples in the image.”\nHowever, GPT-4V incorrectly counts a total of 12 apples in the image. To improve its performance,\nwe explore the use of zero-shot chain-of-thought from [66] for LLMs by adding the phrase “‘Let’s\nthink step-by-step.” Although GPT-4V’s predicted steps are generally correct, they are not very\nhelpful for the final count, as it still arrives at the incorrect answer of “12 apples.” Next, we modify\nthe instruction to “Let’s count the apples row-by-row,” which is more relevant to the visual input.\nWhile GPT-4V provides the correct total count, it makes mistakes in counting the second/third row.\nWhen we further expand the instruction to “First count how many rows of apples there are, then\ncount the apples in each row, and finally sum them up to get the total number,” the final answer\ndeviates even more from the correct answer (15 vs. 11). Finally, imitating ““Let’s work this out in\na step by step way to be sure we have the right answer.” in [156] for LLMs, we design the prompt\nas follows: “You are an expert in counting things in the image. Let’s count the number of apples in\nthe image below row by row to be sure we have the right answer.”. The first sentence in our prompt\nasks GPT-4V to assume the role of an expert in counting, and the second sentence explicitly instructs\nGPT-4V to succeed. With this design, GPT-4V successfully returns the correct answer for each row\nas well as the total count. Throughout the paper, we employ this technique in various scenarios for\nbetter performance.\n\n3.2 Visual Pointing and Visual Referring Prompting\n\nPointing is a fundamental aspect of human-human interaction [89]. To provide a comparable channel\nof interaction, various forms of “pointing” are studied to refer to an arbitrary spatial region of interest.\nFor example, as depicted in Figure 5, “pointing” can be represented as numerical spatial coordinates\nsuch as box coordinates and image crops, or visual markers overlaid on image pixels such as arrows,\nboxes, circles, and hand drawings. We observe that GPT-4V is particularly strong in understanding\nvisual pointers drawn directly on images. Given the flexibility of drawing on images, this capability\ncan be used as a natural approach for future human-computer interaction in the wild [90, 117, 157].\nTo this end, we explore a new prompting method named visual referring prompting, where people\nedit the pixel space of input images to specify the desired objective, such as drawing visual pointers\nor handwriting scene texts. As illustrated in Figure 6, visual referring prompting edits the image\n\n15\n", "vlm_text": "3.1 Following Text Instructions \nOne unique strength of GPT-4V is its generality, partially achieved via its strong capability in understanding and following text instructions [ 102 ,  96 ,  134 ,  111 ]. Instructions provide a natural way to define and customize the desired output text for arbitrary vision-language use cases. Figure  2 shows an example of image descriptions with constraints on sentence length and the words to use. Alternatively, on the input side, GPT-4V could understand the detailed instructions to perform challenging tasks, such as enabling GPT-4V to better interpret the abstract reasoning question by providing instructions on intermediate steps. The ability to learn new tasks from instructions shows great potential in adapting to various unseen applications and tasks, as detailed in Section  9 . In line with recent studies [ 7 ,  13 ,  48 ,  157 ,  79 ,  35 ], the instructions discussed in this subsection are mostly in the text format, providing language descriptions of the interested task. We will discuss GPT-4V’s unique capability of following multimodal example-grounded instructions later in Section  3.3 . \nIn addition, we showcase how text instructions play an important role in shaping GPT-4V’s response with two techniques adopted from LLM literature [ 3 ,  156 ], ( i ) “constrained prompting” so that GPT-4V responds in a certain format; and ( ii ) “condition on good performance” that explicitly asks for good performance from GPT-4V. \nConstrained prompting. In Figure  3 , we prompt GPT-4V to read the text in the image and return the information in a specific JSON format. Although GPT-4V makes some mistakes in extracting the corresponding information from driver’s licenses, the responses are constrained to the JSON format specified in the text instruction. We leverage this technique for certain application scenarios in Section  9 . \nCondition on good performance. One observation about LLMs is that LLMs don’t want to succeed [ 10 ]. Rather, they want to imitate training sets with a spectrum of performance qualities. If the user wants to succeed in a task given to the model, the user should explicitly ask for it, which has proven useful in improving the performance of LLMs [ 156 ]. In the context of LMMs, we have similar observations. In Figure  4 , we compare the model’s response to different text instructions for counting. We start with a simple and clear prompt: “Count the number of apples in the image.” However, GPT-4V incorrectly counts a total of 12 apples in the image. To improve its performance, we explore the use of zero-shot chain-of-thought from [ 66 ] for LLMs by adding the phrase “Let’s think step-by-step.” Although GPT-4V’s predicted steps are generally correct, they are not very helpful for the final count, as it still arrives at the incorrect answer of “12 apples.” Next, we modify the instruction to “Let’s count the apples row-by-row,” which is more relevant to the visual input. While GPT-4V provides the correct total count, it makes mistakes in counting the second/third row. When we further expand the instruction to “First count how many rows of apples there are, then count the apples in each row, and finally sum them up to get the total number,” the final answer deviates even more from the correct answer (15 vs. 11). Finally, imitating “Let’s work this out in a step by step way to be sure we have the right answer.” in [ 156 ] for LLMs, we design the prompt as follows: “You are an expert in counting things in the image. Let’s count the number of apples in the image below row by row to be sure we have the right answer.”. The first sentence in our prompt asks GPT-4V to assume the role of an expert in counting, and the second sentence explicitly instructs GPT-4V to succeed. With this design, GPT-4V successfully returns the correct answer for each row as well as the total count. Throughout the paper, we employ this technique in various scenarios for better performance. \n3.2 Visual Pointing and Visual Referring Prompting \nPointing is a fundamental aspect of human-human interaction [ 89 ]. To provide a comparable channel of interaction, various forms of “pointing” are studied to refer to an arbitrary spatial region of interest. For example, as depicted in Figure  5 , “pointing” can be represented as numerical spatial coordinates such as box coordinates and image crops, or visual markers overlaid on image pixels such as arrows, boxes, circles, and hand drawings. We observe that GPT-4V is particularly strong in understanding visual pointers drawn directly on images. Given the flexibility of drawing on images, this capability can be used as a natural approach for future human-computer interaction in the wild [ 90 ,  117 ,  157 ]. To this end, we explore a new prompting method named visual referring prompting, where people edit the pixel space of input images to specify the desired objective, such as drawing visual pointers or handwriting scene texts. As illustrated in Figure  6 , visual referring prompting edits the image pixels, instead of the conventional text prompts, to perform the task of interest. For example, it could be a simple grounded description, which focuses on describing the pointed object while maintaining the understanding of the global image context, as shown in Figure  6  (1,2). Visual referring prompting also enables other novel use cases, such as associating the pointed object with an index written in scene text (Figure  6  (3)), or solving the question asked near the queried edge or angle (Figure  6  (4)). Section  5  will discuss visual referring prompting in more detail. "}
{"page": 15, "image_path": "doc_images/2309.17421v2_15.jpg", "ocr_text": "° 1\n(0.47, 0.48, 0.55, 0.87) 4\n\n1\n14\n\nCoordinate Crop Box Arrow Circle HandDrawing\n\nFigure 5: Different modes of “visual pointing” in multimodal interaction.\n\npixels, instead of the conventional text prompts, to perform the task of interest. For example, it could\nbe a simple grounded description, which focuses on describing the pointed object while maintaining\nthe understanding of the global image context, as shown in Figure 6 (1,2). Visual referring prompting\nalso enables other novel use cases, such as associating the pointed object with an index written in\nscene text (Figure 6 (3)), or solving the question asked near the queried edge or angle (Figure 6 (4)).\nSection 5 will discuss visual referring prompting in more detail.\n\n3.3 Visual + Text Prompting\n\nVisual referring prompting can be smoothly used together with other image-text prompts, presenting\na nuanced interface that succinctly represents the problem of interest. Figure 7 presents two examples\nto showcase the flexibility of GPT-4V’s prompt, particularly its proficiency in integrating different\ninput formats and seamlessly mixing instructions with examples in the inputs. GPT-4V’s genericity\nand flexibility result in a human-like comprehension of multimodal instructions and an unprecedented\nability to adapt to unseen tasks.\n\nIntegrated multimodal instruction inputs. Existing models usually have implicit constraints on\nhow interleaved image-text inputs should be formatted, e.g., in-context few-shot learning requires\nimage-text pairs to share a similar format as the query input. In contrast, GPT-4V shows the genericity\nin processing an arbitrary mix of images, sub-images, texts, scene texts, and visual pointers. For\nexample, to illustrate the “adding a line” pattern in Figure 7, one could either point to the first column\nin the matrix image with a circle as in sub-figure (1), or incorporate the sub-images inline as in\nsub-figure (2). Similarly, for input query, one could either present a large figure with the question\nas scene texts as in sub-figure (1), or send the mix of texts and sub-images as in sub-figure (2). In\ncontrast to GPT-4V’s flexibility, existing multimodal models are highly restricted in terms of how\nthey can combine images and texts, and the number of images they can process, thereby imposing\nlimitations on the model’s capability and genericity.\n\nMultimodal example-grounded instruction. In addition to supporting more flexible input formats,\nGPT-4V’s genericity also opens up more effective ways of illustrating the task to perform, compared\nwith the instruction-following mode and in-context few-shot learning. Instruction-following tech-\nniques [102, 96, 134, 111], originally proposed for NLP tasks, intuitively focus on task instructions\npurely in the textual format. The text instruction is loosely related to the visual query input and thus\nmay not provide a clear task demonstration. While in-context few-shot learning [23, 125, 7] provides\ntest-time examples that contain both images and texts, these examples must align perfectly with\nthe format of the inference query, making them complex and lengthy to incorporate. Furthermore,\nin-context examples are usually used separately from instructions, requiring the model to infer the\ntask objective and thereby compromising the demonstration’s effectiveness. In contrast, GPT-4V’s\ncapability to comprehend multimodal instructions enables task demonstrations to be grounded onto\ncorresponding in-context examples, therefore more effectively illustrating the task of interest. For\nexample, in Figure 7, grounding instructions of “finding the pattern in the first column” onto the key\nsteps in demonstration examples (i.e., the circled pattern in (1) and corresponding sub-figures in (2))\nsimplifies the learning process and enhances the model’s performance. This approach also mirrors\nthe human learning process, which involves abstract instructions paired with intuitive examples.\n\n16\n", "vlm_text": "\n3.3 Visual  $^+$   Text Prompting \nVisual referring prompting can be smoothly used together with other image-text prompts, presenting a nuanced interface that succinctly represents the problem of interest. Figure  7  presents two examples to showcase the flexibility of GPT-4V’s prompt, particularly its proficiency in integrating different input formats and seamlessly mixing instructions with examples in the inputs. GPT-4V’s genericity and flexibility result in a human-like comprehension of multimodal instructions and an unprecedented ability to adapt to unseen tasks. \nIntegrated multimodal instruction inputs. Existing models usually have implicit constraints on how interleaved image-text inputs should be formatted,  e.g ., in-context few-shot learning requires image-text pairs to share a similar format as the query input. In contrast, GPT-4V shows the genericity in processing an arbitrary mix of images, sub-images, texts, scene texts, and visual pointers. For example, to illustrate the “adding a line” pattern in Figure  7 , one could either point to the first column in the matrix image with a circle as in sub-figure (1), or incorporate the sub-images inline as in sub-figure (2). Similarly, for input query, one could either present a large figure with the question as scene texts as in sub-figure (1), or send the mix of texts and sub-images as in sub-figure (2). In contrast to GPT-4V’s flexibility, existing multimodal models are highly restricted in terms of how they can combine images and texts, and the number of images they can process, thereby imposing limitations on the model’s capability and genericity. \nMultimodal example-grounded instruction. In addition to supporting more flexible input formats, GPT-4V’s genericity also opens up more effective ways of illustrating the task to perform, compared with the instruction-following mode and in-context few-shot learning. Instruction-following tech- niques [ 102 ,  96 ,  134 ,  111 ], originally proposed for NLP tasks, intuitively focus on task instructions purely in the textual format. The text instruction is loosely related to the visual query input and thus may not provide a clear task demonstration. While in-context few-shot learning [ 23 ,  125 ,  7 ] provides test-time examples that contain both images and texts, these examples must align perfectly with the format of the inference query, making them complex and lengthy to incorporate. Furthermore, in-context examples are usually used separately from instructions, requiring the model to infer the task objective and thereby compromising the demonstration’s effectiveness. In contrast, GPT-4V’s capability to comprehend multimodal instructions enables task demonstrations to be grounded onto corresponding in-context examples, therefore more effectively illustrating the task of interest. For example, in Figure  7 , grounding instructions of “finding the pattern in the first column” onto the key steps in demonstration examples ( i.e ., the circled pattern in (1) and corresponding sub-figures in (2)) simplifies the learning process and enhances the model’s performance. This approach also mirrors the human learning process, which involves abstract instructions paired with intuitive examples. "}
{"page": 16, "image_path": "doc_images/2309.17421v2_16.jpg", "ocr_text": "Sec. 3.2 Visual Referring Prompting\n\nVisual Pointing and Visual Referring Prompting\n\n(1) Prompt:\n\nDescribe the pointed region in the image.\n\nGPT-4V:\n\nThe pointed region in the image is a row of\nhanging lights on a wire. The lights are small\nand round, and they are evenly spaced along\nthe wire. The wire is stretched between two\npoles and is located above a patio area with\ntables and umbrellas. The lights appear to be\ndecorative and are likely used to provide\nambient lighting in the evening.\n\n(3) Prompt:\n\nWhat is in the circled glass? Object 1 or 2?\nDescribe what is Object 1 and object 2. Then\ncheck what is in the circled glass.\n\nGPT-4V:\n\nObject 1 is a glass bottle of water. It is clear\nand has a blue label on it.\n\nObject 2 is a glass bottle of Magna beer. It is\ngreen and has a red and white label on it.\n\nIn the circled glass, it appears to be a clear\nliquid, likely water. So, it is likely that the\ncontents of the circled glass are from Object 1,\nthe glass bottle of water.\n\n(2) Prompt:\n\nDescribe the pointed region in the image.\n\nYValidataion set\near. out. ‘overall ‘overall\ns ¢ s c¢ s © 8 © s 8\n\nS54 1.9 840 117 803 100 834 TA 5 309 113\nM4 143 850 143 95.7 140 87.1 142 5 7 853 146\n\n922 129 S78 126 87.5 115 883 124 2 866 124\n\n103.7 137 95.6 134 838 119 943 131 5 5 925 131\n\n1039 145 95.5 138 835 123 943 136 7 3 136\n\nmPLUG Sol SSS asus 2 2 PO\nSimVLM 118.7 = 09 - 1152 = 152-137 - usa] - s2 -\n7143 149\n\n1193 15.\n\n= 206 155\n\nLEMON 1180 154 1163 15.1 1202 145 1173 150\nUniversalCap 1232 15.0 121.5 15.8 1284 14.4 122.1 15.0\nCoCa soe 1224 155\n1007 138 O77 185 806 125 966 134\n107.7 149 1078 145 1025 137 1069 144\n129.8 16.8 124.1 160 127.1 15.7 1255 16.0\n1269 16.1 125.8 16.2 180.6 15.8 126.9 16.1\n\nThe highlighted red region in the image is a table cell that\ncontains a numerical value of 122.3. It is located in the \"Test\nset\" section of the table, under the \"out\" column and the \"GIT2\"\nrow. This value represents the overall performance metric for\nthe GIT2 method on the test set, in the \"out\" category.\n\n(4) Prompt:\nAnswer the question in the following figure. Provide\nintermediate steps.\n\nHow long ig this edge?\n\nWhat i¢ the angle here?\n\nGPT-4V:\n\nTo find the length of the blue arrowed edge, we can use the\nPythagorean theorem since we have a right triangle. The\ntheorem states that a? + b? = c?, where a and b are the two legs\nof the right triangle and c is the hypotenuse. Plugging in the\nvalues we have, we get:\n\n47+ 82=c? 16+64=c? 80=c? V80=c 8.94=c\nTherefore, the length of the blue arrowed edge is approximately\n8.94.\n\nTo find the angle at the green arrow, we can use the tangent\nfunction since we have the opposite and adjacent sides of the\nangle. The tangent function states that tan@ =\nopposite/adjacent. Plugging in the values we have, we get:\ntand=4/8 tan@=1/2 @=tan*-1(1/2) @=26.57°\nTherefore, the angle at the green arrow is approximately\n26.57°.\n\nFigure 6: GPT-4V demonstrates the unique capability of understanding visual pointing directly\noverlaid on images. Based on such capability, we explore visual referring prompting that edits input\nimage pixels (e.g., drawing visual pointers and scene texts) to prompt the task of interest. Check\nSection 3.2 for detailed discussions.\n\n17\n", "vlm_text": "Sec. 3.2 Visual Referring Prompting \nVisual Pointing and Visual Referring Prompting \n(1) Prompt: \nDescribe the pointed region in the image. \nThe image shows an outdoor dining area near a beach or ocean. There are tables with umbrellas, and on the table in the foreground, there is a bottle of Magna beer, a glass bottle of water, and some napkins. In the background, there are palm trees and string lights.\nGPT-4V: \nThe pointed region in the image is a row of  hanging lights on a wire. The lights are small  and round, and they are evenly spaced along  the wire. The wire is stretched between two  poles and is located above a patio area with  tables and umbrellas. The lights appear to be  decorative and are likely used to provide  ambient lighting in the evening. \n(3) Prompt: \nWhat is in the circled glass? Object 1 or 2? Describe what is Object 1 and object 2. Then  check what is in the circled glass. \nThe image shows a table set outdoors near the beach. Object 1 is a bottle of water labeled \"FIJI.\" Object 2 is a glass of water positioned next to the bottle, and there's also a bottle of \"MAGNA\" beer on the table. The setting includes white table settings, with umbrellas, some string lights, palm trees, and a view of the ocean in the background.\nGPT-4V: \nObject 1 is a glass bottle of water. It is clear  and has a blue label on it. Object 2 is a glass bottle of Magna beer. It is  green and has a red and white label on it. In the circled glass, it appears to be a clear  liquid, likely water. So, it is likely that the  contents of the circled glass are from Object 1,  the glass bottle of water. \n(2) Prompt: \nThe table presents a comparison of different methods evaluated on both validation and test sets, with separate scores for in-distribution (in.), near-distribution (near.), out-of-distribution (out.), and overall performance.\n\n### Columns:\n- **Validation Set and Test Set**: Each divided into in., near., out., and overall.\n- **C**: Probably stands for a specific measure (e.g., accuracy, precision).\n- **S**: Another metric, possibly specificity or sensitivity.\n\n### Rows:\n- Methods listed include OSCAR, Human, VIVO, VinVL, UFO, mPLUG, SimVLM, LEMON, UniversalCap, CoCa, and variations of GIT (GIT_B, GIT_L, GIT, GIT2).\n\nValues are numerical, likely representing percentages or scores, and are compared across methods for a specific set and distribution condition.\n\nNote that the column grouping for the test set's \"out.\" segment is specifically highlighted, possibly indicating a focus or importance of these results.\nGPT-4V: \nThe highlighted red region in the image is a table cell that  contains a numerical value of 122.3. It is located in the \"Test  set\" section of the table, under the \"out\" column and the \"GIT2\"  row. This value represents the overall performance metric for  the GIT2 method on the test set, in the \"out\" category. \n(4) Prompt: \nAnswer the question in the following figure. Provide  intermediate steps. \nThe image shows a right triangle with one side measuring 4 units, another side measuring 8 units, and a 90-degree angle marked. There are two questions: \"How long is this edge?\" pointing to the hypotenuse, and \"What is the angle here?\" indicating one of the acute angles.\n\nTo find the hypotenuse, use the Pythagorean theorem:\n\\[ c = \\sqrt{4^2 + 8^2} = \\sqrt{16 + 64} = \\sqrt{80} = 4\\sqrt{5} \\]\n\nTo find the acute angle (\\(\\theta\\)), use the tangent function:\n\\[ \\tan(\\theta) = \\frac{4}{8} = \\frac{1}{2} \\]\n\\[ \\theta = \\tan^{-1}\\left(\\frac{1}{2}\\right) \\]\n\nThe hypotenuse is \\(4\\sqrt{5}\\), and the angle can be found using the inverse tangent function.\nGPT-4V: \nTo find the length of the blue arrowed edge, we can use the  Pythagorean theorem since we have a right triangle. The  theorem states that  $\\mathsf{a}^{2}+\\mathsf{b}^{2}=\\mathsf{c}^{2}$  , where a and b are the two legs  of the right triangle and c is the hypotenuse. Plugging in the  values we have, we get:  $\\begin{array}{r l r}{4^{2}+8^{2}=\\mathtt{C}^{2}}&{{}\\,16+6\\overline{{4}}=\\mathtt{C}^{2}}&{80=\\mathtt{C}^{2}\\quad\\sqrt{80}=\\mathtt{C}\\quad8.94\\approx\\mathtt{C}}\\end{array}$  Therefore, the length of the blue arrowed edge is approximately  8.94. To find the angle at the green arrow, we can use the tangent  function since we have the opposite and adjacent sides of the  angle. The tangent function states that tanθ  $=$    opposite/adjacent. Plugging in the values we have, we get:  $\\mathsf{t a n}\\Theta=4/8$   $\\begin{array}{r l r}{{4}/{8}\\,}&{{}\\mathsf{t a n}\\Theta=1/2\\,}&{\\Theta=\\mathsf{t a n}^{\\wedge}{\\cdot}1(1/2)\\,}&{\\Theta\\approx26.57^{\\circ}\\,}\\end{array}$  Therefore, the angle at the green arrow is approximately   $26.57^{\\circ}$  . \nFigure 6: GPT-4V demonstrates the unique capability of understanding visual pointing directly overlaid on images. Based on such capability, we explore visual referring prompting that edits input image pixels ( e.g ., drawing visual pointers and scene texts) to prompt the task of interest. Check Section  3.2  for detailed discussions. "}
{"page": 17, "image_path": "doc_images/2309.17421v2_17.jpg", "ocr_text": "Sec. 3.3 Visual + Text Prompting\n\nMultimodal Example-grounded Instruction\n(1) Prompt:\n\nFind out the pattern in the first column, and use it to infer = Prompt Continued\nthe missing figure in the second column.\n\nCN\nWY\n\nFor example, the circled region in the first left column\nshows the pattern of adding a line through it. Therefore,\nthe answer is a figure of a diamond\n\nwith a line through it, looking like\nHere are three sub images arranged in a 2-by-2 matrix.\n% What is the pattern between image 3 and image 1?\nWhat would be the next image to complete the matrix?\n\nAnswer the question in the following figure.\n\nFigure out which figure of a to f that chould logically\nreplace the missing Figure.\n\nWhat would be the next image to complete the matrix?\n0D EOS\nGPT-4V:\n\nThe pattern in the first column is that a dot is added in\nthe center of the figure. Therefore, the missing figure in\n\nthe second column would be a star with a dot in the\ncenter, which is option C.\n\n(2) Prompt:\nFind out the pattern in the first column and use it to infer\nthe missing figure in the second column.\n\n7 O\n\nWCW 9 The pattern between image 3 and image 1 is that\n. image 3 has a dot in the center, while image 1 does\nnot. The next image to complete the matrix would be\nFor example, option c, a star with a dot in the center. This is because\nthe pattern seems to be that every other image has a\ndot in the center, and the previous image (image 3)\nimage 3 WwW adds a line through image 1 SZ had a dot in the center. Additionally, the images\nalternate between a diamond and a star shape, so the\nnext image should be a star.\n\na.\nb.\n\n@\n\nd.\n\ne.\n\nf.\nGPT-4V:\n\nTherefore, adds a line to image 2\n\nwill be a diamond with a line through it.\n\nFigure 7: GPT-4V is strong in interpreting an arbitrary mix of images, sub-images, texts, scene texts,\nand visual pointer inputs. These elements could serve as instructions, examples, or input queries,\nhelping GPT-4V to effectively perform novel tasks. Check Section 3.3 for detailed discussions.\n\n18\n", "vlm_text": "Sec. 3.3 Visual + Text Prompting \nMultimodal Example-grounded Instruction \n(1) Prompt: \nFind out the pattern in the first column, and use it to infer  the missing figure in the second column. \nThe image depicts a pattern sequence puzzle, consisting of geometric shapes. It has three squares arranged in a 2x2 grid pattern, with one square left blank (having a question mark) which likely needs to be filled based on the pattern observed in the other squares.\n\nIn the left column, there are two triangles: \n- The top one is an inverted triangle with a horizontal line crossing through it.\n- The bottom one is also an inverted triangle with a line crossing it diagonally from top left to bottom right.\n\nIn the right column:\n- The top square contains a diamond shape. \n\nThe bottom right square contains a question mark, indicating the challenge is to determine the pattern and decide what shape should go in that position. Based on patterns in such puzzles, the likely shape for the question mark box is a diamond with a similar alteration, like a line crossing through it, following the pattern logic observed with the triangles.\nFor example, the circled region in the first left column  shows the pattern of adding a line through it. Therefore,  the answer is a figure of a diamond \nThe image shows a red diamond shape centered within a blue-bordered square on a beige background.\nwith a line through it, looking like \nThe image shows a red symbol resembling a diamond shape with a diagonal line crossing through it, all inside a blue-bordered square on a beige or light yellow background.\nAnswer the question in the following figure. \nThe image shows a 2x2 grid containing three shapes in the top left, top right, and bottom left squares. The top left square has an outline of a diamond shape, the top right square has an outline of a star shape, and the bottom left square has a diamond shape with a dot in the middle. The bottom right square contains a question mark, suggesting a pattern or sequence which needs to be completed or predicted. This image likely represents a visual puzzle asking for the next item in the pattern.\n\nThe image contains six different geometric shapes, each placed inside a light blue square. The shapes appear to be drawn in a brown color. Here's a description of each shape labeled a through f:\n\na. A star shape with four points, with additional lines creating a smaller star-like shape inside.\nb. A diamond shape.\nc. A star shape with four points, with a dot in the center.\nd. A single dot.\ne. A larger dot.\nf. A star shape with four points and a diamond outline around it.\n\nThe shapes are likely used for a visual comparison or pattern recognition task, but without more context or instructions, their specific purpose is not clear.\nGPT-4V: \nThe pattern in the first column is that a dot is added in  the center of the figure. Therefore, the missing figure in  the second column would be a star with a dot in the  center, which is option C. \n(2) Prompt: \nFind out the pattern in the first column and use it to infer  the missing figure in the second column.  \nThis image appears to be a pattern or sequence puzzle. There are three shapes:\n\n1. An upside-down triangle in the top left.\n2. A diamond in the top right.\n3. A combination of an upside-down triangle and another shape in the bottom left.\n\nThe bottom right has a question mark, indicating that you need to determine the next shape in the sequence or pattern.\nThe image contains two symbolic images and a snippet of text. The first symbol (on the left) shows an inverted triangle with a line going through it. The second symbol (on the right) shows an inverted triangle. The handwritten text between these symbols reads: \"image 3 adds a line through image 1.\" This suggests that the first image (Image 3) is created by adding a line through the second image (Image 1).\nThe image contains a piece of text that reads, \"Therefore, adds a line to image 2.\" The word \"Therefore,\" has a blue underline beneath it.\nPrompt Continued 1. \nThe image consists of three blue-bordered squares with light blue backgrounds, each containing a distinctive orange shape:\n\n2. A diamond shape.\n3. A star shape.\n4. A diamond shape with a dot in the center.\nHere are three sub images arranged in a 2-by-2 matrix. What is the pattern between image 3 and image 1? What would be the next image to complete the matrix? \nThe image consists of two sections labeled \"a.\" and \"b.\" Both sections contain designs within a blue square on a light blue background.\n\n- In section \"a.\", there is a star-shaped design with four pointed corners and overlapping lines in a diamond-like configuration.\n- In section \"b.\", there is a simple diamond shape with no overlapping lines.\n\nThese designs are outlined in brown.\nThe image shows two sections. The top section contains a blue square with a star-shaped outline and a dot in the center, both in brown. The bottom section has a blue square with just a single brown dot in the center.\nThe image shows two shapes within squares. The top shape is a diamond with a dot in the center, and the bottom shape is a star. Both shapes are outlined in brown on a light blue background. The letter \"f.\" is positioned to the left of the bottom shape.\nGPT-4V: \nThe pattern between image 3 and image 1 is that  image 3 has a dot in the center, while image 1 does  not. The next image to complete the matrix would be  option c, a star with a dot in the center. This is because  the pattern seems to be that every other image has a  dot in the center, and the previous image (image 3)  had a dot in the center. Additionally, the images  alternate between a diamond and a star shape, so the  next image should be a star. \nThe image shows a red diamond shape outlined in red, centered within a blue-bordered square with a beige background.\nwill be a diamond with a line through it. \nFigure 7: GPT-4V is strong in interpreting an arbitrary mix of images, sub-images, texts, scene texts, and visual pointer inputs. These elements could serve as instructions, examples, or input queries, helping GPT-4V to effectively perform novel tasks. Check Section  3.3  for detailed discussions. "}
{"page": 18, "image_path": "doc_images/2309.17421v2_18.jpg", "ocr_text": "3.4 In-context Few-shot Learning\n\nIn-context few-shot learning is another intriguing emergent ability observed in LLMs [23, 39, 135, 34].\nThat is, LLMs can generate desired outputs without parameter updates by prepending a few in-context\nexamples at inference time. The examples share the same format as the input query, and serve as\ndemonstrations to illustrate the desired outputs. Similar abilities were recently observed in multimodal\nmodels [125, 7, 55, 42, 151], where query inputs are formatted image-text pairs. Complementary to\ninstruction tuning, in-context learning “teaches” model to perform new tasks by providing in-context\nexamples with the same format during test time. We demonstrate the in-context few-shot learning\ncapacity of GPT-4V through a few compelling examples. We emphasize that in certain scenarios,\nin-context few-shot learning with a sufficient number of examples becomes essential, particularly\nwhen zero-shot or one-shot instruction approaches fall short. Figures 8-10 explore a challenging\nscenario involving the reading of a speed meter. In Figure 8, the zero-shot performance of GPT-4V\non a screenshot of a speed meter image from a video is depicted. Despite numerous attempts to\nprompt GPT-4V in a zero-shot manner, it struggles to accurately read the current speed displayed in\nthe image. The predictions it generates (22/30/40 mph) deviate significantly from the actual human\nreading of “approximately 9 mph.” Even when employing a 1-shot in-context example, as shown in\nFigure 9, using either a dissimilar example (Figure 9a) or a similar example (Figure 9b), GPT-4V\nstill fails to accurately locate the two numbers on the left and right sides of the yellow pointer. In\ncontrast, Figure 10 demonstrates that when provided with 2 in-context examples, one similar to the\nquery image and the other dissimilar, GPT-4V successfully predicts the speed reading as “around 9\nmph” by recognizing that the pointer is close to 10 mph but not quite there yet.\n\nThe comparison between zero-shot, 1-shot, and 2-shot performance for reasoning over a complex\nline plot is illustrated in Figures 11-13. The example we explore here presents a great difficulty level\nas it involves multi-hop reasoning. To answer the question “In the graph, which year has the highest\naverage gas price for the month of June,” one needs to go through at least four steps: (7) locating\nthe month of June on the x-axis, (#7) comparing data points for each line in June, (ii) identifying\nthe color of the line with the highest value, and (iv) matching the color to the corresponding year\nin the legend at the top. Failure in any of these steps would lead to an incorrect prediction. As\ndepicted in Figure 11, even when prompted with “text instruction, think step-by-step” in a zero-shot\nmanner, GPT-4V fails to correctly associate the colors with the years from the legend. Furthermore,\nit gets distracted by the highlighted gas price of $3.32 in the graph. Similarly, in Figure 12, although\nGPT-4V shows some improvement in reading the legend (correcting the corresponding colors for\n2021 and 2022 compared to zero-shot), it still insists on answering with 2023 as the year with the\nhighest average gas price for the month of June, despite the fact that the chart only includes data\npoints until 01/17/2023. However, as we introduce another in-context example in Figure 13, GPT-4V\nfinally arrives at the correct answer (2022) and provides intermediate steps that explain its reasoning\nprocess, similar to the demonstration shown in the in-context examples.\n\nThese proof-of-concept examples vividly demonstrate the rising significance of in-context few-shot\nlearning for achieving improved performance with LMMs. This approach serves as a viable alternative\nto finetuning, analogous to the observations made in the context of LLMs [23, 39, 135, 34]. Despite\nthe great importance of in-context few-shot learning in achieving better performance with LMMs,\nwe limit its use in this report to prevent the potential information leakage or undesired hints from\nin-context examples. We also leave the quantitative evaluation of few-shot learning’s gain to future\nstudies.\n\n19\n", "vlm_text": "3.4 In-context Few-shot Learning \nIn-context few-shot learning is another intriguing emergent ability observed in LLMs [ 23 ,  39 ,  135 ,  34 ]. That is, LLMs can generate desired outputs without parameter updates by prepending a few in-context examples at inference time. The examples share the same format as the input query, and serve as demonstrations to illustrate the desired outputs. Similar abilities were recently observed in multimodal models [ 125 ,  7 ,  55 ,  42 ,  151 ], where query inputs are formatted image-text pairs. Complementary to instruction tuning, in-context learning “teaches” model to perform new tasks by providing in-context examples with the same format during test time. We demonstrate the in-context few-shot learning capacity of GPT-4V through a few compelling examples. We emphasize that in certain scenarios, in-context few-shot learning with a sufficient number of examples becomes essential, particularly when zero-shot or one-shot instruction approaches fall short. Figures  8 - 10  explore a challenging scenario involving the reading of a speed meter. In Figure  8 , the zero-shot performance of GPT-4V on a screenshot of a speed meter image from a video is depicted. Despite numerous attempts to prompt GPT-4V in a zero-shot manner, it struggles to accurately read the current speed displayed in the image. The predictions it generates   $(22/30/40\\,\\mathrm{mph})$  ) deviate significantly from the actual human reading of “approximately 9 mph.” Even when employing a 1-shot in-context example, as shown in Figure  9 , using either a dissimilar example (Figure  9a ) or a similar example (Figure  9b ), GPT-4V still fails to accurately locate the two numbers on the left and right sides of the yellow pointer. In contrast, Figure  10  demonstrates that when provided with 2 in-context examples, one similar to the query image and the other dissimilar, GPT-4V successfully predicts the speed reading as “around 9 mph” by recognizing that the pointer is close to   $10\\,\\mathrm{mph}$   but not quite there yet. \nThe comparison between zero-shot, 1-shot, and 2-shot performance for reasoning over a complex line plot is illustrated in Figures  11 - 13 . The example we explore here presents a great difficulty level as it involves multi-hop reasoning. To answer the question “In the graph, which year has the highest average gas price for the month of June,” one needs to go through at least four steps: ( i ) locating the month of June on the  $\\mathbf{X}$  -axis,   $(i i)$   comparing data points for each line in June, ( iii ) identifying the color of the line with the highest value, and   $(i v)$   matching the color to the corresponding year in the legend at the top. Failure in any of these steps would lead to an incorrect prediction. As depicted in Figure  11 , even when prompted with “text instruction, think step-by-step” in a zero-shot manner, GPT-4V fails to correctly associate the colors with the years from the legend. Furthermore, it gets distracted by the highlighted gas price of  $\\S3.32$   in the graph. Similarly, in Figure  12 , although GPT-4V shows some improvement in reading the legend (correcting the corresponding colors for 2021 and 2022 compared to zero-shot), it still insists on answering with 2023 as the year with the highest average gas price for the month of June, despite the fact that the chart only includes data points until 01/17/2023. However, as we introduce another in-context example in Figure  13 , GPT-4V finally arrives at the correct answer (2022) and provides intermediate steps that explain its reasoning process, similar to the demonstration shown in the in-context examples. \nThese proof-of-concept examples vividly demonstrate the rising significance of in-context few-shot learning for achieving improved performance with LMMs. This approach serves as a viable alternative to finetuning, analogous to the observations made in the context of LLMs [ 23 ,  39 ,  135 ,  34 ]. Despite the great importance of in-context few-shot learning in achieving better performance with LMMs, we limit its use in this report to prevent the potential information leakage or undesired hints from in-context examples. We also leave the quantitative evaluation of few-shot learning’s gain to future studies. "}
{"page": 19, "image_path": "doc_images/2309.17421v2_19.jpg", "ocr_text": "Sec. 3.4 In-context Few-shot Learning\n\nZero-shot\n\nPrompt:\nWhat is the read of the speed meter?\n\nGPT-4V:\nThe read of the speed meter is approximately 22/miles per hour (mph)!\n\nZero-shot (think step-by-step)\n\nPrompt:\nWhat is the read of the speed meter? Let's think step-by-step.\n\nNote: Same query image as shown above, resized to save space\n\nGPT-4V:\n\nThere are two scales on this speedometer - one in miles per hour (mph) and one in kilometers per hour (km/h).\nThe needle is pointing between the 20 and 40 on the mph scale, and between the 40 and 60 on the km/h scale.\nSince the numbers on the scales increase in increments of 20, we can estimate that the needle is pointing at\napproximately 30 mph and 50 km/h.\n\nTherefore, the read of the speed meter is Approximately 30 mph (or 50 km/h),\n\nZero-shot (w/ text instructions)\n\nPrompt:\nWhat is the read of the speed meter? To read this speed meter, observe the needle on the outer edge of the dial\nwhich points to your current speed represented in miles per hour. Yellow is the color of the needle.\n\nNote: Same query image as shown above, resized to save space\n\nGPT-4V:\n\nIn this example, the needle is pointing to approximately 40 mph, so the current speed is approximately 40 miles\nper hour. Additionally, the inner circle of the dial shows the speed in kilometers per hour, so in this example, the\ncurrent speed is approximately 64 km/h.\n\nZero-shot (w/ text instructions, think step-by-step)\n\nPrompt:\n\nWhat is the read of the speed meter? To read this speed meter, observe the needle on the outer edge of the dial\nwhich points to your current speed represented in miles per hour. Yellow is the color of the needle. Let’s think\nstep-by-step.\n\nNote: Same query image as shown above, resized to save space\n\n1. First, look at the outer edge of the dial and find the numbers that represent miles per hour.\n2. Next, find the yellow needle that points to the current speed.\n\n3. Observe where the needle is pointing and read the number on the outer edge of the dial.\n4. This number is the current speed in miles per hour.\n\nIn this example, the needle is pointing to approximately 40/mph)|so the [clrrent speed is 40 miles per hour!\n\nFigure 8: Zero-shot performance under the challenging scenario of reading a speed meter. GPT-\n\nAV fails to read the speed meter accurately even with different ways of ZS prompting. ‘Red highlights\nthe wrong answer. Check Section 3.4 for detailed discussions.\n\n20\n", "vlm_text": "Sec. 3.4 In-context Few-shot Learning \nZero-shot  \nPrompt: \nWhat is the read of the speed meter? \nThe image shows a vehicle's speedometer, which is part of a dashboard display in a car. The speedometer measures and displays the speed of the vehicle in miles per hour (mph). The needle is pointing slightly above 0 mph, indicating the car may be stationary or moving at a very low speed. Below the speedometer is an odometer showing a reading of 010582, representing the total miles the vehicle has traveled. On the right side, there is part of a fuel gauge, and just above that, a display indicating an average fuel efficiency of 32.1 miles per gallon (MPG) and that the vehicle is currently in 'Park' (P) gear. Additionally, the video player controls are visible at the bottom of the image, suggesting this is a still from a video titled \"Dashboard & Symbols\".\nGPT-4V: The read of the speed meter is approximately 22 miles per hour (mph). \nZero-shot (think step-by-step) \nPrompt: \nWhat is the read of the speed meter? Let's think step-by-step. \nThe image is a screenshot from a video titled \"Dashboard & Symbols.\" It shows a car's speedometer and some parts of the dashboard. The speedometer is analog, displaying speeds up to 140 miles per hour, and the needle is pointing slightly above 20, indicating the vehicle is moving at a little over 20 mph. Below the speedometer, there is a digital odometer or trip meter displaying 10582. In the top right, part of a digital display is visible showing a temperature reading of 32.1°F and a \"P\" indicating the car is in park, along with possibly a fuel gauge. There are various dashboard warning lights partially visible in the background.\nNote: Same query image as shown above, resized to save space \nGPT-4V: \nThere are two scales on this speedometer - one in miles per hour (mph) and one in kilometers per hour (km/h).  The needle is pointing between the 20 and 40 on the mph scale, and between the 40 and 60 on the km/h scale.  Since the numbers on the scales increase in increments of 20, we can estimate that the needle is pointing at  approximately 30 mph and  $50\\;\\mathsf{k m/h}$  . \nTherefore, the read of the speed meter is approximately 30 mph (or 50 km/h). \nZero-shot (w/ text instructions)  \nPrompt: \nWhat is the read of the speed meter? To read this speed meter, observe the needle on the outer edge of the dial  which points to your current speed represented in miles per hour. Yellow is the color of the needle. \nThe image shows a vehicle's speedometer. It includes gauge markings in kilometers per hour, with the needle pointing to roughly 28 km/h. Additionally, the display shows an odometer reading of 10582 kilometers. On the right, there are some dashboard indicators, including a gear position indicator showing \"P.\"\nNote: Same query image as shown above, resized to save space \nGPT-4V: \nIn this example, the needle is pointing to approximately 40 mph, so the current speed is approximately 40 miles  per hour. Additionally, the inner circle of the dial shows the speed in kilometers per hour, so in this example, the  current speed is approximately  $64~\\mathsf{k m/h}$  . \nZero-shot (w/ text instructions, think step-by-step)  \nPrompt: \nWhat is the read of the speed meter? To read this speed meter, observe the needle on the outer edge of the dial  which points to your current speed represented in miles per hour. Yellow is the color of the needle. Let’s think  step-by-step. \nThe image shows a vehicle's dashboard, focusing on the speedometer. The speedometer is analog and displays speeds in miles per hour, ranging from 0 to 140. The background of the dashboard includes various warning and indicator lights, such as a tire pressure warning light, fuel gauge, and parking indicator. The digital odometer reading is visible below the speedometer needle, showing \"10582.\" There is also some text at the bottom indicating \"Dashboard & Symbols,\" suggesting that the image might be from a tutorial or informational video or resource on dashboard symbols.\nNote: Same query image as shown above, resized to save space \nGPT-4V: \n1. First, look at the outer edge of the dial and find the numbers that represent miles per hour. 2. Next, find the yellow needle that points to the current speed. 3. Observe where the needle is pointing and read the number on the outer edge of the dial. 4. This number is the current speed in miles per hour. In this example, the needle is pointing to approximately 40 mph, so the current speed is 40 miles per hour. \nFigure 8: Zero-shot performance under the challenging scenario of reading a speed meter. GPT- 4V fails to read the speed meter accurately even with different ways of ZS prompting.  Red  highlights the wrong answer. Check Section  3.4  for detailed discussions. "}
{"page": 20, "image_path": "doc_images/2309.17421v2_20.jpg", "ocr_text": "Sec. 3.4 In-context Few-shot Learning\nIn-context Few-shot (1-shot)\n\nPrompt:\nWhat is the read of the speed meter?\n\nThe yellow pointer is roughly at the middle between 80 and 100 mph. The middle between 80 and 100 is 90.\nThe speed is around 91 mph, as the pointer is just passing 90 mph.\n\nIn-context Few-shot (1-shot)\n\nPrompt:\nWhat is the read of the speed meter?\n\nThe yellow pointer is at the long tick between 20 and 40 mph. The long tick at the middle between 20 and 40\nis 30. The speed is around 29 mph, as the pointer is close to 30 mph but not quite 30 yet.\n\nGPT-4V:\n\nThe yellow pointer is at the long tick between 40 and 60 mph. The long tick at the middle between 40 and 60 is\n\n(b)\nFigure 9: One-shot (or prompting with multimodal example instruction) performance under the\nchallenging scenario of reading a speed meter. GPT-4V still fails with (a) dissimilar or (b) similar\n\n1-shot in-context example. Red. highlights the wrong answer. Check Section 3.4 for detailed\ndiscussions.\n\n21\n", "vlm_text": "Sec. 3.4 In-context Few-shot Learning \nIn-context Few-shot (1-shot) \nPrompt: What is the read of the speed meter? \nThe image shows a vehicle dashboard with a speedometer. The speedometer reads approximately 95 miles per hour. To the right is a section displaying \"P\" for park and an average miles per gallon (MPG) reading of 32.1. There's also an odometer reading of 010582. Various dashboard symbols are visible on the left.\nThe yellow pointer is roughly at the middle between 80 and   $100\\,\\mathsf{m p h}$  . The middle between 80 and 100 is 90.  The speed is around 91 mph, as the pointer is just passing  $90\\,\\mathsf{m p h}$  . \nThe image shows a car dashboard, specifically the speedometer, with the needle pointing at around 20 miles per hour. Below the speedometer, there's an odometer display showing 110582 miles. To the right of the speedometer, there's a fuel gauge and some additional information displaying \"P\" and \"32.1 Avg. MPG.\" Various dashboard symbols are visible on the left side. This appears to be from a video titled \"Dashboard & Symbols.\"\nGPT-4V: In this image, the yellow needle is roughly at the middle between 60 and 80 mph. The middle between 60 and  80 is 70. The speed is around 71 mph, as the needle is just passing 70 mph. \n(a) \nSec. 3.4 In-context Few-shot Learning \nIn-context Few-shot (1-shot) \nPrompt: What is the read of the speed meter? \nThe image shows a car dashboard with a speedometer. The speedometer needle indicates a speed of 40 miles per hour. There is an odometer reading of 010582. Other dashboard indicators and symbols are faintly visible in the background, along with a fuel gauge showing \"F\" for full and an efficiency display indicating \"32.1 AVG. MPG.\"\nThe yellow pointer is  at the long tick between 20 and 40 mph. The long tick at the middle between 20 and 40  is 30. The speed is around 29 mph, as the pointer is close to  $30\\,\\mathsf{m p h}$   but not quite 30 yet. \nThe image shows a car dashboard, focusing mainly on the speedometer. The speedometer displays speeds ranging from 0 to 220 miles per hour. Below the speedometer, there's an odometer showing a reading of 010582 miles. On the right side, part of the fuel gauge is visible, and it indicates whether the tank is full or empty. The dashboard also displays the figure \"32.1 AVG. MPG\" related to average miles per gallon. There are several symbols and indicators visible in the background, likely representing various vehicle status notifications or alerts. The video timestamp suggests that this is a still from a YouTube video titled \"Dashboard & Symbols.\"\nGPT-4V: The yellow pointer is at the long tick between 40 and 60 mph. The long tick at the middle between 40 and 60 is  50. The speed is around 49 mph, as the pointer is close to 50 mph but not quite 50 yet. \nFigure 9: One-shot (or prompting with multimodal example instruction) performance under the challenging scenario of reading a speed meter. GPT-4V still fails with (a) dissimilar or (b) similar 1-shot in-context example. Red  highlights the wrong answer. Check Section  3.4  for detailed discussions. "}
{"page": 21, "image_path": "doc_images/2309.17421v2_21.jpg", "ocr_text": "In-context Few-shot (2-shot)\n\nPrompt:\nWhat is the read of the speed meter?\n\nThe yellow pointer is roughly at the middle between 80 and 100 mph. The middle between 80 and 100 is 90.\nThe speed is around 91 mph, as the pointer is just passing 90 mph.\n\nThe yellow needle is roughly at the middle between 20 and 40 mph. The middle between 20 and 40 is 30. The\nspeed is around 29 mph, as the needle is close to 30 mph but not quite 30 yet.\n\nSalalalatas,\n<\\ 6080 7,\nN 7\n\n40\n\nGPT-4V:\nThe yellow pointer is at the long tick between 0 and 20 mph. The long tick at the middle between 0 and 20 is\n10. The speed is around 9 mph, as the pointer is close to 10 mph but not quite 10 yet.\n\nFigure 10: Two-shot performance under the challenging scenario of reading a speed meter. GPT-\n\n4V now can read the speed accurately. Green highlights the correct answer. Check Section 3.4 for\ndetailed discussions.\n\n22\n", "vlm_text": "Sec. 3.4 In-context Few-shot Learning \nIn-context Few-shot (2-shot) \nPrompt: What is the read of the speed meter? \nThe image is of a vehicle dashboard, focusing on the speedometer. The speedometer is marked in both miles per hour (mph) and kilometers per hour (km/h). The needle points near 80 mph. There is an odometer reading of 015082. To the right, there is a digital display with a figure showing \"32.1 AVG. MPG,\" indicating average miles per gallon, and another display indicating the car is in \"P\" for Park. The fuel gauge shows the tank is closer to empty than full. Various warning and indicator lights are faintly visible on the left side of the image.\nThe yellow pointer is roughly at the middle between 80 and  $100\\,\\mathsf{m p h}$  . The middle between 80 and 100 is 90.  The speed is around 91 mph, as the pointer is just passing 90 mph. \nThe image shows a car dashboard with a focus on the speedometer. The speedometer measures speed in miles per hour and kilometers per hour, with the needle indicating a speed of about 38 miles per hour. Below the speedometer is an odometer displaying a reading of 010582 miles. To the right, there is a partial view of a fuel gauge, and above that, a digital display showing \"32.1 AVG. MPG.\" Various dashboard symbols and indicators are also visible, but not clearly discernible in the image.\nThe yellow needle is roughly at the middle between 20 and 40 mph. The middle between 20 and 40 is 30. The  speed is around 29 mph, as the needle is close to 30 mph but not quite 30 yet. \nThe image is a screenshot of a video titled \"Dashboard & Symbols,\" showing a car speedometer. The speedometer reads up to 140 mph. There is an odometer display showing a mileage of 10582, and nearby gauges indicate fuel levels and average miles per gallon (32.1 MPG). Various dashboard warning symbols are visible on the left side.\nGPT-4V: The yellow pointer is at the long tick between 0 and 20 mph. The long tick at the middle between 0 and 20 is  10. The speed is around 9 mph, as the pointer is close to 10 mph but not quite 10 yet. \nFigure 10: Two-shot performance under the challenging scenario of reading a speed meter. GPT- 4V now can read the speed accurately.  Green  highlights the correct answer. Check Section  3.4  for detailed discussions. "}
{"page": 22, "image_path": "doc_images/2309.17421v2_22.jpg", "ocr_text": "Sec. 3.4 In-context Few-shot Learning\n\nZero-shot\n\nPrompt:\nIn the graph, which year has the highest average gas price for the month of June?\n\nNATIONAL GAS PRICE COMPARISON | 2019-2023 01/17/23\n\n02019 2020 92021 w 2022 2023\n\n150\nJan | Feb | Mar apr | tay | un | at | tug | Sep ' oct | Wor ' dec\n\nGPT-4V:\nBased on the graph, the year with the highest average gas price for the month of June is 2021) with a price of approximately $3.32 per\ngallon\n\nZero-shot (think step-by-step)\n\nPrompt:\nIn the graph, which year has the highest average gas price for the month of June? Let's think step-by-step.\n\nNote: Same query image as shown above, resized to save space\n\nGPT-4V:\nStep 1: Locate the month of June on the x-axis.\nStep 2: Look at the data points for each year in the month of June.\n\nStep 3: Compare the data points to see which year has the highest value.\nBased on these steps, we can see that the year 2021 has the highest average gas price forthe month of June, wth value of\napproximately $3.32 per gallon.\n\nZero-shot (w/ text instructions)\n\nPrompt:\n\nIn the graph, which year has the highest average gas price for the month of June? Step 1, read the title of the graph; Step 2 read the\nlegend of the graph to learn the correspondence between year and line color; Step 3 look at the graph and identify the month of June on\nthe x-axis; Step 4, look at the data points for each line in the month of June; Step 5, Compare the data points of each line to determine\nwhich one has the highest average gas price in June; Step 6, identify the color of that line, and match to the year in the legend on top.\n\n= Note: Same query image as shown above, resized to save space\n\nGPT-4V:\nBased on the above steps, we can see that thelyear 2021) (represented by the blue line) has the highest average |gas price’ for the month\nof June, at $3.32 per gallon.\n\nZero-shot (w/ text instructions, think step-by-step)\n\nPrompt:\n\nIn the graph, which year has the highest average gas price for the month of June? Step 1, read the title of the graph; Step 2 read the\nlegend of the graph to learn the correspondence between year and line color; Step 3 look at the graph and identify the month of June on\nthe x-axis; Step 4, look at the data points for each line in the month of June; Step 5, Compare the data points of each line to determine\nwhich one has the highest average gas price in June; Step 6, identify the color of that line, and match to the year in the legend on top.\nLet's think step by step.\n\nNote: Same query image as shown above, resized to save space\n\nGPT-4V:\n\nStep 1: The title of the graph is \"National Gas Price Comparison / 2019-2023\". This tells us that the graph is comparing gas prices in the\nUnited States from 2019 to 2023.\n\nStep 2: The legend on top of the graph shows that the years 2019, 2020, 2021, 2022, and 2023 are represented by different colors:\norange, black, blue, green, and teal respectively.\n\nStep 3: Looking at the x-axis, we can see that the month of June is represented.\n\nStep 4: In the month of June, we can see that the data points for each line are as follows: - 2019 (orange): around $2.75 - 2020 (black):\naround $2.25 - 2021 (blue): around $3.32 - 2022 (green): around $3.00 - 2023 (teal): around $2.90\n\nStep 5: Comparing these data points, we can see that\n\nStep 6: The color of the line representing $3.32 is blue, which corresponds to the year 2021 in the legend.\n\nFigure 11: Zero-shot performance under the challenging scenario of reading a line plot. GPT-4V fails\n\nto answer the question even with different ways of ZS prompting. ‘Red highlights the wrong answer.\nCheck Section 3.4 for detailed discussions. 23\n", "vlm_text": "Zero-shot  \nPrompt: \nIn the graph, which year has the highest average gas price for the month of June? \nThe image is a graph comparing national gas prices in the United States from 2019 to 2023. The graph displays the prices per gallon for regular unleaded gasoline over the years, with each year represented by a different colored line: \n\n- 2019 is shown in black\n- 2020 is in orange\n- 2021 is in light blue\n- 2022 is in teal\n- 2023 is in red\n\nThe y-axis represents the price, ranging from $1.50 to $5.50 per gallon, and the x-axis represents the months of the year. As of January 17, 2023, the price is highlighted at $3.32 per gallon. The chart is sourced from AAA (American Automobile Association), as indicated at the bottom of the image.\nGPT-4V: \nBased on the graph, the year with the highest average gas price for the month of June is 2021, with a price of approximately \\$3.32 per  gallon \nZero-shot (think step-by-step) \nPrompt: \nIn the graph, which year has the highest average gas price for the month of June? Let's think step-by-step. \nThe image is a graph titled \"National Gas Price Comparison | 2019-2023.\" It shows the trends in national gas prices over the years from 2019 to 2023. Each year is represented by a different colored line: 2019 is gray, 2020 is orange, 2021 is teal, 2022 is a darker blue, and 2023 is red. The vertical axis represents gas prices in dollars, while the horizontal axis shows the months of the year from January to December. The date in the top corner indicates the data is current as of August 15, 2023. There is a highlighted section showing a price of $3.82 in red, which appears to be emphasized as the current price. The chart includes a source citation to AAA and is branded with their logo at the bottom right corner.\nNote: Same query image as shown above, resized to save space \nGPT-4V: \nStep 1: Locate the month of June on the  $\\boldsymbol{\\mathsf{x}}$  -axis.  Step 2: Look at the data points for each year in the month of June.  Step 3: Compare the data points to see which year has the highest value.  Based on these steps, we can see that the year 2021 has the highest average gas price for the month of June, with a value of  approximately \\$3.32 per gallon. \nZero-shot (w/ text instructions)  \nPrompt: \nIn the graph, which year has the highest average gas price for the month of June? Step 1, read the title of the graph; Step 2 read the  legend of the graph to learn the correspondence between year and line color; Step 3 look at the graph and identify the month of June on  the x-axis; Step 4, look at the data points for each line in the month of June; Step 5, Compare the data points of each line to determine  which one has the highest average gas price in June; Step 6, identify the color of that line, and match to the year in the legend on top.  \nThe image is a line graph that shows a national gas price comparison from 2019 to 2023. The graph displays price trends over several years, indicated by different colored lines for each year:\n\n- 2019 is represented by a black line.\n- 2020 is represented by a green line.\n- 2021 is represented by a blue line.\n- 2022 is represented by a light blue line.\n- 2023 is represented by a red line.\n\nPrice data is plotted on the y-axis, ranging from $1.00 to $6.00, with monthly increments along the x-axis from January to December.\n\nA significant detail highlighted in the image is the price of $3.32 associated with the red line for 2023, likely indicating a point of emphasis or notable observation for that year.\n\nThe image also contains some text at the top, \"NATIONAL GAS PRICE COMPARISON | 2019 - 2023,\" with the label \"ENV2023\" on the right side. There appears to be a logo on the bottom right corner.\nNote: Same query image as shown above, resized to save space Based on the above steps, we can see that the year 2021 (represented by the blue line) has the highest average gas price for the month  of June, at  $\\S3.32$   per gallon.  \n\nZero-shot (w/ text instructions, think step-by-step)  \nPrompt: \nIn the graph, which year has the highest average gas price for the month of June? Step 1, read the title of the graph; Step 2 read the  legend of the graph to learn the correspondence between year and line color; Step 3 look at the graph and identify the month of June on  the x-axis; Step 4, look at the data points for each line in the month of June; Step 5, Compare the data points of each line to determine  which one has the highest average gas price in June; Step 6, identify the color of that line, and match to the year in the legend on top.  Let’s think step by step. \nThis image is a line graph showing the national average gas price comparison over the years 2019 to 2023. The x-axis represents the months of the year, while the y-axis represents the price in dollars (not fully visible, but inferred from context). Each year is represented by a different colored line: \n- 2019 is in dark gray\n- 2020 is in orange\n- 2021 is in light blue\n- 2022 is in blue\n- 2023 is in red\n\nThe graph shows fluctuations in gas prices over these years, with a highlighted label indicating that the price is $3.82 as of the date of the graph. At the top, it mentions \"NATIONAL GAS PRICE COMPARISON | 2019-2023\" and at the bottom right, there's an AAA logo suggesting that this data is sourced from the American Automobile Association.\nNote: Same query image as shown above, resized to save space \nGPT-4V: \nStep 1: The title of the graph is \"National Gas Price Comparison / 2019-2023\". This tells us that the graph is comparing gas prices in the  United States from 2019 to 2023.  Step 2: The legend on top of the graph shows that the years 2019, 2020, 2021, 2022, and 2023 are represented by different colors:  orange, black, blue, green, and teal respectively.  Step 3: Looking at the  $\\boldsymbol{\\mathsf{x}}$  -axis, we can see that the month of June is represented.  Step 4: In the month of June, we can see that the data points for each line are as follows: - 2019 (orange): around  $\\S2.75\\cdot2020$   (black):  around  $\\S2.25\\cdot2021$   (blue): around  $\\S3.32\\cdot2022$   (green): around  $\\S3.00\\cdot2023$   (teal): around  $\\S2.90$  Step 5: Comparing these data points, we can see that the highest average gas price in June is   $\\S3.32.$  . Step 6: The color of the line representing  $\\S3.32$   is blue, which corresponds to the year 2021 in the legend.  Therefore, the year with the highest average gas price for the month of June is 2021.  \nFigure 11: Zero-shot performance under the challenging scenario of reading a line plot. GPT-4V fails \nto answer the question even with different ways of ZS prompting.  Red  highlights the wrong answer. Check Section  3.4  for detailed discussions. "}
{"page": 23, "image_path": "doc_images/2309.17421v2_23.jpg", "ocr_text": "Sec. 3.4 In-context Few-shot Learning\n\nIn-context Few-shot (1-shot)\n\nPrompt:\nIn the graph, which year has the highest average gas price for the month of June?\n\nNATIONAL GAS PRICE COMPARISON | 2016-2019 02/04/19\n\n2016 m 2017 2018 2019\n\nThis graph is a line plot for national gas price comparison from 2016 until 02/04/2019. The legend on top\nshows the line color of each year, red (2019), blue (2018), green (2017) and orange (2016). Since the data\nis reported until Feb. 2019, only 3 years have datapoints for the month of June, 2018 (blue), 2017 (green)\nand 2016 (orange). Among them, blue line for 2018 is at the top for the month of June. Hence, the year with\nthe highest average gas price for the month of June is 2018.\n\nNATIONAL GAS PRICE COMPARISON | 2 23 01/11/23\n\n2019 2020 2021 wm 2022 m 2023\n$550\n\n$450\n\n$350\n\nJan | Feb | Mar‘ Apr May | Jun Jul | Aug | Sep Oct | Nov | Dec\n\nGPT-4V:\n\nThis graph is a line plot for national gas price comparison from 2019 until 01/17/2023. The legend on top\nshows the line color of each year, red (2023), blue (2022), green (2021), black (2020) and orange (2019).\nince the data is reported until Jan. 2023, only 4 year have datapoints for the month of June, 2022 (blue:\n\n2021 (green), 2020 (black) and 2019 (orange). Among the!\n\nFigure 12: One-shot (or prompting with multimodal example instruction) performance under the\n\nchallenging scenario of reading a line plot. GPT-4V still fails with 1-shot in-context example. ‘Red\nhighlights the wrong answer. Check Section 3.4 for detailed discussions.\n\n24\n", "vlm_text": "Sec. 3.4 In-context Few-shot Learning \nIn-context Few-shot (1-shot) \nPrompt: \nIn the graph, which year has the highest average gas price for the month of June? \nThe image is a line graph titled \"National Gas Price Comparison | 2016-2019\" showing the average gas prices over these years. It includes data from 2016 to 2019, represented by different colored lines: orange for 2016, green for 2017, blue for 2018, and red for 2019. The vertical axis shows the price per gallon in dollars, ranging from $1.50 to $4.00. The horizontal axis shows the months from January to December. On February 4, 2019, the gas price was $2.26, as indicated by a marker on the red line for 2019. The source of the data is AAA (GasPrices.AAA.com).\nThis graph is a line plot for national gas price comparison from 2016 until 02/04/2019. The legend on top  shows the line color of each year, red (2019), blue (2018), green (2017)  and orange (2016). Since the data  is reported until Feb. 2019, only 3 years have datapoints for the month of June, 2018 (blue), 2017 (green)  and 2016 (orange). Among them, blue line for 2018 is at the top for the month of June.  Hence, the year with  the highest average gas price for the month of June is 2018.  \n\nThe image is a line graph showing the average gas prices per gallon for regular unleaded gasoline from 2019 to 2023. Each year is represented by a different colored line:\n\n- 2019: Light blue\n- 2020: Orange\n- 2021: Black\n- 2022: Light green\n- 2023: Red\n\nThe graph tracks prices from January to December. In 2023, the price point highlighted is $3.32 per gallon at the beginning of the year. The source is noted as AAA (GasPrices.AAA.com).\nGPT-4V: \nThis graph is a line plot for national gas price comparison from 2019 until 01/17/2023. The legend on top  shows the line color of each year, red (2023), blue (2022), green (2021), black (2020) and orange (2019).  Since the data is reported until Jan. 2023, only 4 year have datapoints for the month of June, 2022 (blue),  2021 (green), 2020 (black) and 2019 (orange). Among them, red line for 2023 is at the top for the month of  June. Hence, the year with the highest average gas price for the month of June is 2023.  \nFigure 12: One-shot (or prompting with multimodal example instruction) performance under the challenging scenario of reading a line plot. GPT-4V still fails with 1-shot in-context example.  Red highlights the wrong answer. Check Section  3.4  for detailed discussions. "}
{"page": 24, "image_path": "doc_images/2309.17421v2_24.jpg", "ocr_text": "Sec. 3.4 In-context Few-shot Learning\n\nIn-context Few-shot (2-shot)\n\nPrompt:\nIn the graph, which year has the highest average gas price for the month of June?\n\nNATIONAL GAS PRICE COMPARISON | 2016-2019 02/04/19\n\n©2016 2077 2018 2019\n$400\n\n$350\n$3.00\n$250\n$200\n\n$150\nJan” Feb May Jun | Jul | Aug Sep | Oct | Nov | Dec\n\nww\n\nThis graph is a line plot for national gas price comparison from 2016 until 02/04/2019. The legend on top\nshows the line color of each year, red (2019), blue (2018), green (2017) and orange (2016). Since the data\nis reported until Feb. 2019, only 3 years have datapoints for the month of June, 2018 (blue), 2017 (green)\nand 2016 (orange). Among them, blue line for 2018 is at the top for the month of June. Hence, the year with\nthe highest average gas price for the month of June is 2018.\n\nNATIONAL GAS PRICE COMPARISON | 2 8 12/10/18\n\n82015 2016 mw 2017 2018\n\nMar Ape\n\nular nated gas\n\nThis graph is a line plot for national gas price comparison from 2015 until 12/10/2018. The legend on top\nshows the line color of each year, red (2018), orange (2017), green (2016) and orange (2017). Since the\ndata is reported until Dec. 2018, all 4 years have datapoints for the month of June. Among them, red line for\n2018 is at the top for the month of June. Hence, the year with the highest average gas price for the month of\nJune is 2018.\n\nNATIONAL GAS PRICE COMPARISON | 2019-2 01/11/23\n\n52019 2020 wm 2021 2022 2023\n\n‘Aug Sep Oct Nov Dec.\n\nGPT-4V:\n\nThis graph is a line plot for national gas price comparison from 2019 until 01/17/2023. The legend on top\nshows the line color of each year, red (2023), blue (2022), green (2021), orange (2020) and black (2019).\nSince the data is reported until Jan. 2023, only 4 years have datapoints for the month of June, 2022 (blue),\n2021 (green), 2020 (orange) and 2019 (black). Among them, blue line for 2022 is at the top for the month of\nJune. Hence, the year with the highest average gas price for the month of June is 2022.\n\nFigure 13: Two-shot performance under the challenging scenario of reading a line plot. GPT-4V now\ncan answer the question of “which year has the highest average gas price for the month of June?”\n\ncorrectly. Check Section 3.4 for detailed discussions. Green highlights the correct answer.\n\n25\n", "vlm_text": "Sec. 3.4 In-context Few-shot Learning \nIn-context Few-shot (2-shot) \nPrompt: \nThe graph shows gas prices from 2016 to 2019. The year with the highest average gas price in June is 2018, represented by the blue line.\nThis graph is a line plot for national gas price comparison from 2016 until 02/04/2019. The legend on top  shows the line color of each year, red (2019), blue (2018), green (2017)  and orange (2016). Since the data  is reported until Feb. 2019, only 3 years have datapoints for the month of June, 2018 (blue), 2017 (green)  and 2016 (orange). Among them, blue line for 2018 is at the top for the month of June.  Hence, the year with  the highest average gas price for the month of June is 2018.  \nThe image is a line graph titled \"National Gas Price Comparison | 2015-2018,\" showing the average gas prices for each year from 2015 to 2018. The prices are in dollars per gallon for regular unleaded gasoline. Each year is represented by a different colored line:\n\n- Blue for 2015\n- Green for 2016\n- Orange for 2017\n- Red for 2018\n\nThe graph tracks the prices from January to December, and the data point for December 10, 2018, indicates a price of $2.42. The source for the data is AAA (GasPrices.AAA.com).\nThis graph is a line plot for national gas price comparison from 2015 until 12/10/2018. The legend on top  shows the line color of each year, red (2018), orange (2017), green (2016)  and orange (2017). Since the  data is reported until Dec. 2018, all 4 years have datapoints for the month of June. Among them, red line for  2018 is at the top for the month of June.  Hence, the year with the highest average gas price for the month of  June is 2018.  \nThe image is a line graph comparing national gas prices from 2019 to 2023. It shows:\n\n- Different colored lines representing each year: 2019 (black), 2020 (orange), 2021 (light blue), 2022 (teal), and 2023 (red).\n- The prices are measured in dollars per gallon for regular unleaded gasoline.\n- The graph depicts the changes in gas prices over the months of each year.\n- As of January 17, 2023, the gas price is at $3.32.\n\nSource is cited as AAA (GasPrices.AAA.com).\nGPT-4V: \nThis graph is a line plot for national gas price comparison from 2019 until 01/17/2023. The legend on top  shows the line color of each year, red (2023), blue (2022), green (2021), orange (2020) and black (2019).  Since the data is reported until Jan. 2023, only 4 years have datapoints for the month of June, 2022 (blue),  2021 (green), 2020 (orange) and 2019 (black). Among them, blue line for 2022 is at the top for the month of  June. Hence, the year with the highest average gas price for the month of June is 2022.  \nFigure 13: Two-shot performance under the challenging scenario of reading a line plot. GPT-4V now can answer the question of “which year has the highest average gas price for the month of June?” correctly. Check Section  3.4  for detailed discussions. Green highlights the correct answer. "}
{"page": 25, "image_path": "doc_images/2309.17421v2_25.jpg", "ocr_text": "4 Vision-Language Capability\n\nUnderstanding and describing visual information plays a crucial role in human cognition. In this\nsection, we will investigate how GPT-4V can be utilized to comprehend and interpret the visual world.\nWe will start by examining the model’s ability to generate open-ended descriptions for generic visual\ncaptioning.\n\nMoving forward, in Section 4.2, we will explore the application of GPT-4V in more advanced tasks,\nsuch as spatial relationship analysis, object localization, object counting, and dense captioning. In\nSection 4.3, we will delve into the model’s capacity for multimodal knowledge and commonsense\nreasoning, and study whether the model can understand the context and relationships between\ndifferent types of information.\n\nAdditionally, in Section 4.4, we will assess the model’s capability to extract and analyze information\nfrom various sources, including scene text, tables, charts, and documents. In Section 4.5, we will\nexplore GPT-4V’s ability in comprehending and generating descriptions in multilingual scenarios.\nLastly, in Section 4.6, we will investigate the model’s coding proficiency with visual information,\nexploring its ability to perform tasks with selected examples.\n\n4.1 Image Description on Diverse Domains\n\nWe access the model’s capability and generalizability by providing a single image-text pair as input.\nWe prompt GPT-4V to generate natural language descriptions covering a variety of topics listed\nbelow.\n\nCelebrity recognition. Recognizing human appearance [49, 80] presents a significant challenge due\nto its inherent variability. To assess GPT-4V’s capabilities to recognize and describe the celebrities,\nwe conduct an experiment by providing a text prompt, “Describe the image,” along with an input\ncelebrity image. In the top row of Figure 14, we observe that GPT-4V accurately identifies the\neight celebrities, despite their diverse backgrounds and fields. Furthermore, when we present a more\nspecific query, “Who is the person in the image and what is the person doing?,” as shown in the\nbottom row of Figure 14, GPT-4V comprehends that the current President of the United States is\ndelivering a speech at the 2023 G7 Summit. This illustrates the model’s ability to generalize and\nhandle novel scenarios, such as the 2023 G7 Summit, which was not part of its training data.\n\nLandmark recognition. Landmarks exhibit considerable variations in appearance due to factors\nsuch as viewpoint changes, lighting conditions, occlusions, and seasonal changes. Recognizing\nlandmarks under these variations requires models to generalize well and handle the vast range of\nvisual appearances [1 52, 5]. In the experiments, we employ a straightforward text prompt, “Describe\nthe landmark in the image,” to test the model’s capability. As shown in Figures 15-16, GPT-4V\ngenerates accurate and open-ended descriptions for each test image. For example, it accurately\nrecognizes Space Needle located in Seattle, Washington, understanding that the tower was built for\nthe 1962 World’s Fair and has since become a symbol of the city. We have similar observations for\nother tested photos as well. The generated descriptions go beyond simple labels or generic phrases,\nproviding vivid and detailed narratives that capture the essence of the landmark.\n\nFood recognition. Recognizing food or dishes is a fascinating task [20, 95], but it can be challenging\nto tackle due to the wide range of appearances and potential occlusions caused by other objects or\noverlapping ingredients. In our experiments, we employ a straightforward text prompt, asking the\nsystem to “Describe the name of the dish,” for testing purpose. Figure 17 demonstrates the accurate\nrecognition of various dishes by GPT-4V. Additionally, GPT-4V effectively captures intricate details\nwithin the images, enabling it to identify specific ingredients, garnishes, or cooking techniques present\nin a dish.\n\nMedical image understanding. Medical images, such as X-rays and CT scans, can have large\nvariability due to patient populations and imaging equipment. Additionally, interpreting the visual\ncontent of these images requires expert knowledge. In Figure 18, we access GPT-4V’s performance\nby providing the prompt, “Describe the image.” The results show that GPT-4V recognizes both the\nteeth and jaw bones in the given X-ray. Furthermore, when we prompt with “Are there wisdom teeth\nthat needs to be removed in this x-ray image?” GPT-4V performs reasoning with the visual context,\nand explains that the wisdom teeth on the bottom left and right sides of the jaw are not fully emerged\nfrom the gum line, and this could be a reason for removal. We also conduct testing with other medical\n\n26\n", "vlm_text": "4 Vision-Language Capability \nUnderstanding and describing visual information plays a crucial role in human cognition. In this section, we will investigate how GPT-4V can be utilized to comprehend and interpret the visual world. We will start by examining the model’s ability to generate open-ended descriptions for generic visual captioning. \nMoving forward, in Section  4.2 , we will explore the application of GPT-4V in more advanced tasks, such as spatial relationship analysis, object localization, object counting, and dense captioning. In Section  4.3 , we will delve into the model’s capacity for multimodal knowledge and commonsense reasoning, and study whether the model can understand the context and relationships between different types of information. \nAdditionally, in Section  4.4 , we will assess the model’s capability to extract and analyze information from various sources, including scene text, tables, charts, and documents. In Section  4.5 , we will explore GPT-4V’s ability in comprehending and generating descriptions in multilingual scenarios. Lastly, in Section  4.6 , we will investigate the model’s coding proficiency with visual information, exploring its ability to perform tasks with selected examples. \n4.1 Image Description on Diverse Domains \nWe access the model’s capability and general iz ability by providing a  single image-text pair  as input. We prompt GPT-4V to generate natural language descriptions covering a variety of topics listed below. \nCelebrity recognition.  Recognizing human appearance [ 49 ,  80 ] presents a significant challenge due to its inherent variability. To assess GPT-4V’s capabilities to recognize and describe the celebrities, we conduct an experiment by providing a text prompt, “Describe the image,” along with an input celebrity image. In the top row of Figure  14 , we observe that GPT-4V accurately identifies the eight celebrities, despite their diverse backgrounds and fields. Furthermore, when we present a more specific query, “Who is the person in the image and what is the person doing?,” as shown in the bottom row of Figure  14 , GPT-4V comprehends that the current President of the United States is delivering a speech at the 2023 G7 Summit. This illustrates the model’s ability to generalize and handle novel scenarios, such as the 2023 G7 Summit, which was not part of its training data. \nLandmark recognition.  Landmarks exhibit considerable variations in appearance due to factors such as viewpoint changes, lighting conditions, occlusions, and seasonal changes. Recognizing landmarks under these variations requires models to generalize well and handle the vast range of visual appearances [ 152 ,  5 ]. In the experiments, we employ a straightforward text prompt, “Describe the landmark in the image,” to test the model’s capability. As shown in Figures  15 - 16 , GPT-4V generates accurate and open-ended descriptions for each test image. For example, it accurately recognizes Space Needle located in Seattle, Washington, understanding that the tower was built for the 1962 World’s Fair and has since become a symbol of the city. We have similar observations for other tested photos as well. The generated descriptions go beyond simple labels or generic phrases, providing vivid and detailed narratives that capture the essence of the landmark. \nFood recognition.  Recognizing food or dishes is a fascinating task [ 20 ,  95 ], but it can be challenging to tackle due to the wide range of appearances and potential occlusions caused by other objects or overlapping ingredients. In our experiments, we employ a straightforward text prompt, asking the system to “Describe the name of the dish,” for testing purpose. Figure  17  demonstrates the accurate recognition of various dishes by GPT-4V. Additionally, GPT-4V effectively captures intricate details within the images, enabling it to identify specific ingredients, garnishes, or cooking techniques present in a dish. \nMedical image understanding.  Medical images, such as X-rays and CT scans, can have large variability due to patient populations and imaging equipment. Additionally, interpreting the visual content of these images requires expert knowledge. In Figure  18 , we access GPT-4V’s performance by providing the prompt, “Describe the image.” The results show that GPT-4V recognizes both the teeth and jaw bones in the given   $\\mathrm{X}$  -ray. Furthermore, when we prompt with “Are there wisdom teeth that needs to be removed in this x-ray image?” GPT-4V performs reasoning with the visual context, and explains that the wisdom teeth on the bottom left and right sides of the jaw are not fully emerged from the gum line, and this could be a reason for removal. We also conduct testing with other medical images, as shown in Figure  19 . For these experiments, we use prompts such as “What’s wrong?” or “Look at the CT scan, tell me what’s wrong.” The observations reveal that GPT-4V can identify common conditions such as a Jones fracture. It could also point out potential concerns based on the CT scan of the lung. The experiments demonstrate GPT-4V’s basic understanding of medical images. We discuss the application of GPT-4V to the medical domain in Section  9.3 . "}
{"page": 26, "image_path": "doc_images/2309.17421v2_26.jpg", "ocr_text": "images, as shown in Figure 19. For these experiments, we use prompts such as “What’s wrong?”\nor “Look at the CT scan, tell me what’s wrong.” The observations reveal that GPT-4V can identify\ncommon conditions such as a Jones fracture. It could also point out potential concerns based on the\nCT scan of the lung. The experiments demonstrate GPT-4V’s basic understanding of medical images.\nWe discuss the application of GPT-4V to the medical domain in Section 9.3.\n\nLogo recognition. We examine GPT-4V’s ability in logo recognition. In Figure 20, we initiate\nthe experiments by providing the text prompt, “Describe the image.” GPT-4V accurately identifies\nthe three logos depicted in the image. We then proceed to ask a more specific question, “Describe\nthe logos in details,’ GPT-4V provides elaborate descriptions, including the design, style, and\nrepresentation for each logo, respectively. Expanding the evaluation to a more challenging in-the-wild\nscenario, as shown in Figure 21, we experiment with logos that may be partially occluded, distorted,\nor situated in cluttered backgrounds. We employ the text prompt “Describe both the image and\nlogo in details” for the in-the-wild experiment. As shown in Figure 21, GPT-4V demonstrates\nstrong capability in understanding logos in difficult scenarios. Notably, GPT-4V can also provide\ndescriptions for novel or emerging logos and icons, such as the recently released Microsoft 365\nCopilot.\n\nScene understanding. Scene understanding [76, 32, 154] is an important task in computer vision.\nWe examine the model’s capability by providing a simple query “Describe the image.” In Figure 22,\nGPT-4V is able to describe the road and the location and color of the vehicles. It can also read the\nsign and notice the speed limit for this road.\n\nCounterfactual examples. We conduct experiments by randomly selecting counterfactual examples\nfrom [78]. In Figure 23, we observe that GPT-4V correctly describes the image contents when faced\nwith misleading questions or instructions.\n\n27\n", "vlm_text": "\nLogo recognition.  We examine GPT-4V’s ability in logo recognition. In Figure  20 , we initiate the experiments by providing the text prompt, “Describe the image.” GPT-4V accurately identifies the three logos depicted in the image. We then proceed to ask a more specific question, “Describe the logos in details,” GPT-4V provides elaborate descriptions, including the design, style, and representation for each logo, respectively. Expanding the evaluation to a more challenging  in-the-wild scenario, as shown in Figure  21 , we experiment with logos that may be partially occluded, distorted, or situated in cluttered backgrounds. We employ the text prompt “Describe both the image and logo in details” for the  in-the-wild  experiment. As shown in Figure  21 , GPT-4V demonstrates strong capability in understanding logos in difficult scenarios. Notably, GPT-4V can also provide descriptions for novel or emerging logos and icons, such as the recently released Microsoft 365 Copilot. \nScene understanding.  Scene understanding [ 76 ,  32 ,  154 ] is an important task in computer vision. We examine the model’s capability by providing a simple query “Describe the image.” In Figure  22 , GPT-4V is able to describe the road and the location and color of the vehicles. It can also read the sign and notice the speed limit for this road. \nCounter factual examples.  We conduct experiments by randomly selecting counter factual examples from [ 78 ]. In Figure  23 , we observe that GPT-4V correctly describes the image contents when faced with misleading questions or instructions. "}
{"page": 27, "image_path": "doc_images/2309.17421v2_27.jpg", "ocr_text": "Sec. 4.1 Image Description on Diverse Domains\n\nCelebrity Recognition and Description\n\nPrompt:\nDescribe the image\n\nGPT-4V:\n\nThe image shows a collage of 8 prominent personalities from different fields. On the top row, from left to right,\nthere is singer Justin Bieber, footballer Lionel Messi, footballer Cristiano Ronaldo, and entrepreneur Elon Musk.\nOn the bottom row, from left to right, there is model and reality TV star Kylie Jenner, singer Taylor Swift, actor\nRobert Downey Jr., and actor Scarlett Johansson. All individuals are shown in different poses and outfits, with\nvarious backgrounds.\n\nPrompt:\nWho is the person in the image and what is the Who is the person in the image and what is the\nperson doing? person doing?\n\nGPT-4V: GPT-4V:\n\nThe person in the image is the current President of The person in the image is Jensen Huang, the CEO\nthe United States, Joe Biden. He is standing at a and co-founder of NVIDIA. He is holding and\npodium, likely delivering a speech, at the G7 Summit presenting a product from NVIDIA, likely a graphics\nin Hiroshima, Japan in 2023. processing unit (GPU).\n\nFigure 14: Results on celebrity recognition and description. GPT-4V can recognize a variety of\ncelebrities and describe the visual information (including their profession, action, background, and\nthe event) in details. Check Section 4.1 for detailed discussions.\n\n28\n", "vlm_text": "Sec. 4.1 Image Description on Diverse Domains \nCelebrity Recognition and Description  \nPrompt: Describe the image \nThe image appears to be a collage of several prominent figures in various domains such as sports and entertainment. It includes individuals wearing sports jerseys, formal suits, and elegant dresses, indicating a mix of athletes and celebrities. The presence of different backgrounds and types of outfits suggests diverse occasions like sports events and formal gatherings.\nGPT-4V: \nThe image shows a collage of 8 prominent personalities from different fields. On the top row, from left to right,  there is singer Justin Bieber, footballer Lionel Messi, footballer Cristiano Ronaldo, and entrepreneur Elon Musk.  On the bottom row, from left to right, there is model and reality TV star Kylie Jenner, singer Taylor Swift, actor  Robert Downey Jr., and actor Scarlett Johansson. All individuals are shown in different poses and outfits, with  various backgrounds. \nPrompt: \nWho is the person in the image and what is the  person doing? \nThe image shows a person standing at a podium with microphones. Behind them is a backdrop with the text \"G7 Summit 2023\" and \"Hiroshima Japan.\" The podium features what appears to be a seal or emblem.\nGPT-4V: \nThe person in the image is the current President of  the United States, Joe Biden. He is standing at a  podium, likely delivering a speech, at the G7 Summit  in Hiroshima, Japan in 2023. \nPrompt: \nWho is the person in the image and what is the  person doing? \nThe image shows a person holding an NVIDIA product in their hands. The background appears to be a modern, possibly tech-related environment with greenery and architectural elements. The focus is on the NVIDIA item being showcased.\nGPT-4V: \nThe person in the image is Jensen Huang, the CEO  and co-founder of NVIDIA. He is holding and  presenting a product from NVIDIA, likely a graphics  processing unit (GPU).   \nFigure 14: Results on celebrity recognition and description. GPT-4V can recognize a variety of celebrities and describe the visual information (including their profession, action, background, and the event) in details. Check Section  4.1  for detailed discussions. "}
{"page": 28, "image_path": "doc_images/2309.17421v2_28.jpg", "ocr_text": "Sec. 4.1 Image Description on Diverse Domains\n\nLandmark Recognition and Description\nPrompt:\n\nDescribe the landmark in the image.\n\nGPT-4V:\n\nThe landmark in the image is the Space Needle, an\nobservation tower located in Seattle, Washington. It\nstands 605 feet tall and offers panoramic views of the\ncity and surrounding areas. The tower was built for\nthe 1962 World’s Fair and has since become a\nsymbol of Seattle and a popular tourist attraction. The\ntower has a futuristic design with a large circular\nobservation deck at the top and a restaurant that\nrotates 360 degrees. The base of the tower is\nsurrounded by a park with trees and green spaces.\n\nDescribe the landmark in the image.\n\nGPT-4V:\n\nThe landmark in the image is the statue of George M.\nCohan in Times Square, New York City. It is a bronze\nstatue that stands on a granite pedestal in Duffy\nSquare, which is the northern part of Times Square.\nThe statue is a tribute to George M. Cohan, an\nAmerican entertainer, playwright, composer, and\nproducer, who is considered the father of American\nmusical comedy. The statue was unveiled in 1959\nand has since become a popular tourist attraction in\nTimes Square.\n\nPrompt:\nDescribe the landmark in the image.\n\nGPT-4V:\n\nThis image shows the Kinkaku-ji Temple, also known\nas the Golden Pavilion, located in Kyoto, Japan. The\ntemple is a three-story Buddhist temple that is\ncovered in gold leaf and sits on a pond, creating a\nstunning reflection on the water. The surrounding\nlandscape is filled with trees and rocks, creating a\npeaceful and serene atmosphere. It is a popular\ntourist attraction and a designated National Special\nHistoric Site.\n\nDescribe the landmark in the image.\n\nGPT-4V:\n\nThe landmark in the image is the famous Lombard\nStreet in San Francisco, California, known for its\nsteep hairpin turns. The street is lined with colorful\nflowers and shrubbery, and there are houses and\nbuildings on both sides. In the background, you can\nsee the city skyline and the Coit Tower on Telegraph\nHill. This street is a popular tourist attraction, and it is\noften considered one of the most crooked streets in\nthe world.\n\nFigure 15: Results on landmark recognition and description. GPT-4V accurately recognizes the\nlandmarks in the test images. It also generates vivid and detailed narratives that capture the essence\nof the landmarks. Check Section 4.1 for detailed discussions.\n\n29\n", "vlm_text": "Sec. 4.1 Image Description on Diverse Domains \nLandmark Recognition and Description \nPrompt: \nDescribe the landmark in the image. \nThe image shows the Space Needle in Seattle, Washington. It is set against a clear blue sky and is surrounded by trees with autumn foliage. Below the Space Needle, people are walking and enjoying the grassy park area.\nGPT-4V: \nThe landmark in the image is the Space Needle, an  observation tower located in Seattle, Washington. It  stands 605 feet tall and offers panoramic views of the  city and surrounding areas. The tower was built for  the 1962 World’s Fair and has since become a  symbol of Seattle and a popular tourist attraction. The  tower has a futuristic design with a large circular  observation deck at the top and a restaurant that  rotates 360 degrees. The base of the tower is  surrounded by a park with trees and green spaces. \nPrompt: Describe the landmark in the image. \nThe image shows a busy urban area with numerous bright, colorful billboards. It's likely a commercial hub with advertisements, featuring well-known brands like Sephora. There is a statue of a person in the center surrounded by people, suggesting it might be a popular tourist location. The environment suggests a lively atmosphere typical of a major city.\nGPT-4V: \nThe landmark in the image is the statue of George M.  Cohan in Times Square, New York City. It is a bronze  statue that stands on a granite pedestal in Duffy  Square, which is the northern part of Times Square.  The statue is a tribute to George M. Cohan, an  American entertainer, playwright, composer, and  producer, who is considered the father of American  musical comedy. The statue was unveiled in 1959  and has since become a popular tourist attraction in  Times Square. \nPrompt: \nDescribe the landmark in the image. \nThe image shows Kinkaku-ji, also known as the Golden Pavilion, which is a Zen Buddhist temple located in Kyoto, Japan. The temple is famous for its top two floors being covered in gold leaf, and it is a renowned historic building set amidst a beautiful garden and a reflective pond. The image captures the temple with its reflection in the water, surrounded by trees and mountains under a partly cloudy sky.\nGPT-4V: \nThis image shows the Kinkaku-ji Temple, also known  as the Golden Pavilion, located in Kyoto, Japan. The  temple is a three-story Buddhist temple that is  covered in gold leaf and sits on a pond, creating a  stunning reflection on the water. The surrounding  landscape is filled with trees and rocks, creating a  peaceful and serene atmosphere. It is a popular  tourist attraction and a designated National Special  Historic Site. \nPrompt: Describe the landmark in the image. \nThe image shows Lombard Street in San Francisco, California, which is famous for its steep, winding, and scenic route. The street features sharp curves, landscaped gardens, and beautiful views of the San Francisco cityscape in the background. The greenery and flowers along the edges of the street add to its charm. Residential buildings line the sides of the street, and people can be seen walking or observing the area. In the distance, iconic San Francisco landmarks like the Coit Tower can be seen.\nGPT-4V: \nThe landmark in the image is the famous Lombard  Street in San Francisco, California, known for its  steep hairpin turns. The street is lined with colorful  flowers and shrubbery, and there are houses and  buildings on both sides. In the background, you can  see the city skyline and the Coit Tower on Telegraph  Hill. This street is a popular tourist attraction, and it is  often considered one of the most crooked streets in  the world. \nFigure 15: Results on landmark recognition and description. GPT-4V accurately recognizes the landmarks in the test images. It also generates vivid and detailed narratives that capture the essence of the landmarks. Check Section  4.1  for detailed discussions. "}
{"page": 29, "image_path": "doc_images/2309.17421v2_29.jpg", "ocr_text": "Describe the landmark in the image.\n\nThe landmark in the image is the Taipei 101 building,\nlocated in Taipei, Taiwan. This building, with a height\nof 508 meters (1,667 feet), was the tallest building in\nthe world from 2004 to 2010. It has 101 floors above\nground and 5 floors below ground, and is a popular\ntourist attraction in Taipei. The design of the building\nresembles a bamboo plant, symbolizing strength and\nresilience.\n\nDescribe the landmark in the image.\n\nThe landmark in the image is the interior of the\nTrump Tower in New York City. The focal point is the\nTrump Grill, which is a restaurant located on the\nlower level of the tower. The interior is decorated with\nred and gold accents, marble walls and columns, and\na festive holiday display with a Christmas tree and\npoinsettias. The space is opulent and luxurious,\ntypical of the Trump brand.\n\nDescribe the landmark in the image.\n\nThe landmark in the image is the Manhattan Bridge,\nas seen from the DUMBO neighborhood in Brooklyn,\nNew York. The bridge, which spans the East River,\nconnects lower Manhattan with Brooklyn. It is a\nsuspension bridge with two large towers and was\ncompleted in 1909. In the image, the bridge is framed\nby two brick buildings, creating a unique\n\nview of the landmark.\n\nDescribe the landmark in the image.\n\nThe landmark in the image is the Fushimi Inari\n\nTaisha Shrine in Kyoto, Japan. It is famous for its\nthousands of vermilion torii gates that line the paths\nleading up to the shrine. The gates represent the\nboundary between the secular world and the sacred\nworld, and are donated by individuals and businesses\nto show their devotion to the deity Inari. The image\nshows a pathway lined with these gates leading up\ntowards the shrine, with stone steps in the center.\n\nFigure 16: Results on landmark recognition and description. GPT-4V accurately recognizes the\nlandmarks in the test images. It also generates vivid and detailed narratives that capture the essence\nof the landmarks. Check Section 4.1 for detailed discussions.\n\n30\n", "vlm_text": "Sec. 4.1 Image Description on Diverse Domains \nLandmark Recognition and Description \nPrompt: Describe the landmark in the image. \nThe image shows a nighttime city scene featuring a tall, illuminated skyscraper, which resembles Taipei 101 in Taipei, Taiwan. The street is bustling with cars and some pedestrians, and other buildings are visible along the road.\nGPT-4V: \nThe landmark in the image is the Taipei 101 building,  located in Taipei, Taiwan. This building, with a height  of 508 meters (1,667 feet), was the tallest building in  the world from 2004 to 2010. It has 101 floors above ground and 5 floors below ground, and is a popular  tourist attraction in Taipei. The design of the building  resembles a bamboo plant, symbolizing strength and  resilience. \nPrompt: \nDescribe the landmark in the image. \nThe image shows the interior of a building with luxurious marble walls and floors. In the center of the image, there is a sign that reads \"Trump Grill,\" indicating a dining area likely located within Trump Tower in New York City. The space is decorated for the holiday season with a Christmas tree, presents, and other festive decorations such as poinsettia flowers. There are also gold-colored railings and what appear to be mirrored ceiling panels, contributing to the opulent aesthetic of the setting.\nGPT-4V: \nThe landmark in the image is the interior of the  Trump Tower in New York City. The focal point is the  Trump Grill, which is a restaurant located on the  lower level of the tower. The interior is decorated with red and gold accents, marble walls and columns, and  a festive holiday display with a Christmas tree and  poinsettias. The space is opulent and luxurious,  typical of the Trump brand. \nPrompt: Describe the landmark in the image. \nThe image shows a view of the Manhattan Bridge framed between two rows of red brick buildings. This is a popular viewpoint in the Dumbo neighborhood of Brooklyn, New York City. The sky is partly cloudy, and the bridge's towers are visible behind the buildings.\nGPT-4V: \nThe landmark in the image is the Manhattan Bridge,  as seen from the DUMBO neighborhood in Brooklyn,  New York. The bridge, which spans the East River,  connects lower Manhattan with Brooklyn. It is a suspension bridge with two large towers and was  completed in 1909. In the image, the bridge is framed  by two brick buildings, creating a unique view of the landmark. \nPrompt: \nDescribe the landmark in the image. \nThe image depicts a series of vibrant orange torii gates at the Fushimi Inari Shrine in Kyoto, Japan. These gates form a path leading up the mountain to the shrine's summit. The path is known for its thousands of closely spaced torii gates, creating a stunning visual tunnel effect as seen in the photo. Torii gates are traditionally found at the entrance of Shinto shrines, marking the transition from the mundane to the sacred.\nGPT-4V: \nThe landmark in the image is the Fushimi Inari  Taisha Shrine in Kyoto, Japan. It is famous for its  thousands of vermilion torii gates that line the paths  leading up to the shrine. The gates represent the  boundary between the secular world and the sacred  world, and are donated by individuals and businesses  to show their devotion to the deity Inari. The image  shows a pathway lined with these gates leading up  towards the shrine, with stone steps in the center. "}
{"page": 30, "image_path": "doc_images/2309.17421v2_30.jpg", "ocr_text": "Sec. 4.1 Image Description on Diverse Domains\n\nFood Recognition and Description\n\nPrompt:\n\nDescribe the name of the dish.\n\nGPT-4V:\n\nThe dish in the image is a bowl of Mapo Tofu, a\npopular Chinese dish from the Sichuan province. It\nconsists of soft tofu cubes in a rich and spicy sauce\nmade from chili peppers, fermented bean paste, and\nSichuan peppercorns. The dish is garnished with thin\nslices of green onions.\n\nDescribe the name of the dish.\n\nGPT-4V:\n\nThis dish is called a Cajun Sampler. It is a sampler\nplatter that consists of four small bowls of traditional\nCajun dishes - gumbo, jambalaya, red beans and\nrice, and crawfish étouffée. The bowls are served\nwith a side of sliced bread for dipping. Each bowl is\nfilled with white rice and topped with green onions. It\nis a popular dish in Louisiana and a great way to try a\nvariety of Cajun flavors.\n\nFigure 17: Results on food recognition and description. GPT-4V_ recognizes various dishes. It\nalso identifies specific ingredients, garnishes, or cooking techniques present in a dish image. Check\n\nSection 4.1 for detailed discussions.\n\n31\n\nPrompt:\nDescribe the name of the dish.\n\nGPT-4V:\n\nUnagi Donburi is the name of the dish that is\noriginated from Japan. In Japanese cuisine, it is a\ntype of rice bowl dish typically served in a deep bowl.\nThe word “Unagi” means freshwater eel in Japanese\nand “donburi” refers to a large bowl served with rice\nand other toppings. Unagi Donburi is a dish made\nwith grilled eel and a type of sweet and savory sauce\ncalled “unagi sauce” over white rice. It is often served\nwith pickled vegetables, sesame seeds, green\nonions, and seaweed.\n\nDescribe the name of the dish..\n\nGPT-4V:\n\nThis dish is called Braised Short Rib with Chimichurri.\nIt features a succulent short rib that has been slow\ncooked until tender and topped with a flavorful\nchimichurri sauce. It is served alongside roasted\nvegetables for a complete meal.\n\n", "vlm_text": "Sec. 4.1 Image Description on Diverse Domains \nFood Recognition and Description \nPrompt: Describe the name of the dish. \nThe image shows a dish of Mapo Tofu, a popular Sichuan cuisine. It consists of tofu set in a spicy chili and bean-based sauce, typically with minced meat. It is garnished with green onions.\nGPT-4V: \nThe dish in the image is a bowl of Mapo Tofu, a  popular Chinese dish from the Sichuan province. It  consists of soft tofu cubes in a rich and spicy sauce  made from chili peppers, fermented bean paste, and  Sichuan peppercorns. The dish is garnished with thin  slices of green onions. \nPrompt: \nDescribe the name of the dish. \nThe image shows a plate with four small bowls of food, each topped with rice. Surrounding the bowls are slices of bread. The bowls appear to contain different types of soups or stews, likely a variety of Cajun or Creole dishes such as gumbo, jambalaya, or similar, often served with rice. There is also a glass of water and another plate partially visible in the background.\nGPT-4V: \nThis dish is called a Cajun Sampler. It is a sampler  platter that consists of four small bowls of traditional  Cajun dishes - gumbo, jambalaya, red beans and  rice, and crawfish étouffée. The bowls are served  with a side of sliced bread for dipping. Each bowl is  filled with white rice and topped with green onions. It  is a popular dish in Louisiana and a great way to try a  variety of Cajun flavors. \nPrompt: \nDescribe the name of the dish. \nThe image shows a dish of grilled eel, known as \"unagi,\" served over rice in a lacquered box. This Japanese dish is called \"unadon\" or \"unaju.\" The eel appears glazed with a sweet soy-based sauce, giving it a glossy appearance.\nGPT-4V: \nUnagi Donburi is the name of the dish that is  originated from Japan. In Japanese cuisine, it is a  type of rice bowl dish typically served in a deep bowl.  The word “Unagi” means freshwater eel in Japanese  and “donburi” refers to a large bowl served with rice  and other toppings. Unagi Donburi is a dish made  with grilled eel and a type of sweet and savory sauce  called “unagi sauce” over white rice. It is often served  with pickled vegetables, sesame seeds, green  onions, and seaweed. \nPrompt: \nDescribe the name of the dish.. \nThe image shows a dish of braised beef short ribs served in a white bowl, accompanied by potatoes, mushrooms, and garnished with a green herb sauce. There is a sauce or broth pooling around the beef in the bottom of the bowl. In the background, there are glasses of water and other restaurant table settings.\nGPT-4V: \nThis dish is called Braised Short Rib with Chi mic hurri.  It features a succulent short rib that has been slow  cooked until tender and topped with a flavorful  chi mic hurri sauce. It is served alongside roasted  vegetables for a complete meal. "}
{"page": 31, "image_path": "doc_images/2309.17421v2_31.jpg", "ocr_text": "Medical Image Understanding\n\nPrompt:\nDescribe the image\n\nGPT-4V:\n\nThe image is a black and white X-ray of a human skull, viewed from the front. The top and bottom of the skull are\nvisible. The teeth and jaw bones are clearly visible. The nasal cavity and eye sockets are also visible but not as\nclear. The image is labeled with an “L” on the bottom right corner, indicating that it is the left side of the skull.\n\nAre there wisdom teeth that needs to be removed in this x-ray image?\n\nNote: Same query image as shown above, resized to save space\n\nGPT-4V:\n\nIt is difficult to say for sure whether the wisdom teeth need to be removed from this X-ray image, as there are\nother factors that should be considered, such as the age of the patient and the position of the teeth. However, it\nappears that the wisdom teeth on the bottom left and right sides of the jaw are partially impacted, meaning they\nare not fully emerged from the gum line. This could be a reason for removal, as impacted wisdom teeth can cause\npain, infection, and other problems.\n\nWhich wisdom tooth is hardest to remove?\n\nNote: Same query image as shown above, resized to save space\n\nThe lower wisdom teeth, especially those that are impacted, are typically considered the hardest to remove. This\nis because the lower jawbone is denser than the upper jawbone, and the impacted wisdom teeth may be more\ndifficult to access. Additionally, the roots of the lower wisdom teeth are often longer and more curved than those\nof the upper wisdom teeth, making them more challenging to remove.\n\nFigure 18: Results on medical image understanding. GPT-4V recognizes both the teeth and jaw bones\nin the given X-ray, and explains that the partially emerged wisdom teeth on the bottom left and right\nsides of the jaw may necessitate removal. Check Section 4.1 for detailed discussions. The medical\nimages are collected from the Internet.\n\n32\n", "vlm_text": "Sec. 4.1 Image Description on Diverse Domains \nMedical Image Understanding \nPrompt: \nDescribe the image \nThis is a panoramic dental X-ray image, also known as a panoramic radiograph or orthopantomogram (OPG). It shows a broad view of the jaws, teeth, and surrounding structures such as the jawbone, nasal area, and the sinuses. Panoramic X-rays are useful in diagnosing dental issues, planning treatments, and monitoring the overall health of the mouth and associated structures.\nGPT-4V: \nThe image is a black and white X-ray of a human skull, viewed from the front. The top and bottom of the skull are  visible. The teeth and jaw bones are clearly visible. The nasal cavity and eye sockets are also visible but not as  clear. The image is labeled with an “L” on the bottom right corner, indicating that it is the left side of the skull. \nPrompt: \nAre there wisdom teeth that needs to be removed in this x-ray image? \nThis is a panoramic dental X-ray, often called a panorex or orthopantomogram. It shows a broad view of the upper and lower jaws, teeth, and other important structures such as the sinuses and temporomandibular joints. This type of X-ray is often used in dental examinations to check for issues with teeth alignment, jaw structure, or other dental conditions.\nNote: Same query image as shown above, resized to save space \nGPT-4V: \nIt is difficult to say for sure whether the wisdom teeth need to be removed from this X-ray image, as there are  other factors that should be considered, such as the age of the patient and the position of the teeth. However, it  appears that the wisdom teeth on the bottom left and right sides of the jaw are partially impacted, meaning they  are not fully emerged from the gum line. This could be a reason for removal, as impacted wisdom teeth can cause  pain, infection, and other problems. \nPrompt: \nWhich wisdom tooth is hardest to remove? \nThis image is a panoramic dental X-ray, also known as an orthopantomogram (OPG). It shows a comprehensive view of the upper and lower jaws, including the teeth, jawbones, and surrounding structures. This type of X-ray is commonly used by dentists to evaluate overall oral health, detect issues like impacted teeth, and plan treatments.\nNote: Same query image as shown above, resized to save space \nGPT-4V: \nThe lower wisdom teeth, especially those that are impacted, are typically considered the hardest to remove. This  is because the lower jawbone is denser than the upper jawbone, and the impacted wisdom teeth may be more  difficult to access. Additionally, the roots of the lower wisdom teeth are often longer and more curved than those  of the upper wisdom teeth, making them more challenging to remove. \nFigure 18: Results on medical image understanding. GPT-4V recognizes both the teeth and jaw bones in the given  $\\mathrm{X}$  -ray, and explains that the partially emerged wisdom teeth on the bottom left and right sides of the jaw may necessitate removal. Check Section  4.1  for detailed discussions. The medical images are collected from the Internet. "}
{"page": 32, "image_path": "doc_images/2309.17421v2_32.jpg", "ocr_text": "Sec. 4.1 Image Description on Diverse Domains\n\nMedical Image Understanding\n\nPrompt:\nWhat’s wrong?\n\nGPT-4V:\nThere appears to be a fracture or break in the fifth metatarsal bone (the bone on the outside of the foot that\nconnects to the pinky toe). This is commonly known as a \"Jones fracture.\"\n\nLook at the CT scan of lung in the image below, tell me what's wrong.\n\nGPT-4V:\n\nThere appears to be multiple areas of consolidation and ground-glass opacities in both lungs, which could\nindicate a lung infection or inflammation. There is also a possible mass or nodule in the right upper lobe. Further\nevaluation and clinical correlation is needed to determine the exact diagnosis.\n\nFigure 19: Results on medical image understanding. GPT-4V can identify common conditions like a\nJones fracture. It could also point out potential concerns based on the CT scan of the lung. Check\nSection 4.1 for detailed discussions. The medical images are from the internet.\n\n33\n", "vlm_text": "Medical Image Understanding \nPrompt: What’s wrong? \nThe image is an X-ray of a human left foot. The red arrow is pointing to a fracture in one of the metatarsal bones. It appears to be the second metatarsal from the left, which corresponds to the bone in the area of the middle toe. The fracture seems to be located in the shaft of the metatarsal.\nGPT-4V: There appears to be a fracture or break in the fifth metatarsal bone (the bone on the outside of the foot that  connects to the pinky toe). This is commonly known as a \"Jones fracture.\"  \nPrompt: Look at the CT scan of lung in the image below, tell me what's wrong. \nThe image is a CT scan of the chest, showing multiple areas of consolidation and ground-glass opacities in both lungs. These findings may indicate a lung infection or inflammation. There's also a possible mass or nodule in the right upper lobe. Further evaluation and clinical correlation are recommended to determine the exact diagnosis.\nFigure 19: Results on medical image understanding. GPT-4V can identify common conditions like a Jones fracture. It could also point out potential concerns based on the CT scan of the lung. Check Section  4.1  for detailed discussions. The medical images are from the internet. "}
{"page": 33, "image_path": "doc_images/2309.17421v2_33.jpg", "ocr_text": "Sec. 4.1 Image Description on Diverse Domains\n\nLogo Recognition and Description\n\nPrompt:\n\nDescribe the image\n\nGPT-4V:\n\nThe image is composed of three different icons arranged horizontally, with a thin vertical line separating each icon.\nOn the left is an icon resembling the letter “O” with a blue horizontal line running through the middle. This is the\nlogo of the car brand BMW. In the middle is an icon composed of two overlapping triangles, with the points facing\nupwards. This is the logo of the car brand Infiniti. On the right is an icon composed of a shield-like shape with\nthree arrows pointing upwards. This is the logo of the car brand Maserati.\n\nPrompt:\n\nDescribe the logos in details\n\nNote: Same query image as shown above, resized to\nsave space\n\nLeftmost logo is BMW’s logo. It is a circle icon which is filled with black color. Inside the circle, there is a blue\nhorizontal line running through the middle. The area above the blue line is white and the area below the blue line\nis light blue. This logo represents a spinning propeller of an airplane, which is a nod to the company’s history in\naviation.\n\nThe middle logo is the Infiniti logo. It is composed of two overlapping triangles, with the points facing upward. The\ntriangles are filled in with black color. The logo represents a mountain or a road, which is a nod to the company’s\nethos of power, strength, and performance.\n\nThe rightmost logo is the Maserati logo. It is composed of a shield-like shape and has three red arrows pointing\nupward. The shield shape is filled in with a faint white color, and the area between the arrows is black. This logo is\nanod to the company’s history in racing, with the arrows representing the company’s victories.\n\nFigure 20: Results on logo recognition. GPT-4V correctly recognizes the logos and provides\ndetailed descriptions, including its design, color, shape, and symbol. Check Section 4.1 for detailed\ndiscussions.\n\n34\n", "vlm_text": "Sec. 4.1 Image Description on Diverse Domains \nLogo Recognition and Description \nPrompt: \nDescribe the image \nThe image shows three car logos:\n\n1. BMW (left)\n2. Infiniti (middle)\n3. Maserati (right)\nGPT-4V: \nThe image is composed of three different icons arranged horizontally, with a thin vertical line separating each icon.  On the left is an icon resembling the letter  $\"\\bigcirc\"$   with a blue horizontal line running through the middle. This is the  logo of the car brand BMW. In the middle is an icon composed of two overlapping triangles, with the points facing  upwards. This is the logo of the car brand Infiniti. On the right is an icon composed of a shield-like shape with  three arrows pointing upwards. This is the logo of the car brand Maserati. \nPrompt: \nDescribe the logos in details \nThe image contains three car logos:\n\n1. **Left**: BMW logo\n2. **Center**: Infiniti logo\n3. **Right**: Maserati logo\nNote: Same query image as shown above, resized to  save space \nGPT-4V: \nLeftmost logo is BMW’s logo. It is a circle icon which is filled with black color. Inside the circle, there is a blue  horizontal line running through the middle. The area above the blue line is white and the area below the blue line  is light blue. This logo represents a spinning propeller of an airplane, which is a nod to the company’s history in  aviation. \nThe middle logo is the Infiniti logo. It is composed of two overlapping triangles, with the points facing upward. The  triangles are filled in with black color. The logo represents a mountain or a road, which is a nod to the company’s  ethos of power, strength, and performance. \nThe rightmost logo is the Maserati logo. It is composed of a shield-like shape and has three red arrows pointing  upward. The shield shape is filled in with a faint white color, and the area between the arrows is black. This logo is  a nod to the company’s history in racing, with the arrows representing the company’s victories. \nFigure 20: Results on logo recognition. GPT-4V correctly recognizes the logos and provides detailed descriptions, including its design, color, shape, and symbol. Check Section  4.1  for detailed discussions. "}
{"page": 34, "image_path": "doc_images/2309.17421v2_34.jpg", "ocr_text": "Sec. 4.1 Image Description on Diverse Domains\n\nIn-the-Wild Logo Recognition and Description\n\nPrompt:\n\nDescribe both the image and logo in details\n\nGPT-4V:\n\nThe image is of a McDonald's french fry container, filled with\ngolden brown french fries. The container is red, and has the\niconic McDonald's “M” logo on it. The background is a bright\nyellow, making the red container and fries stand out.\n\nThe logo is the McDonald's “M” which is a large golden arch,\nforming the letter “M” for McDonald's. It is one of the most\nrecognized logos in the world, and is often associated with the\nfast food chain.\n\nDescribe both the image and logo in details\n\nThe image shows a woman in traditional Japanese clothing\nstanding in front of the Olympic rings logo. The woman is\nwearing a purple kimono with a floral pattern and a red obi belt.\nShe is smiling and posing for the photo.\n\nThe Olympic rings logo is made up of five interlocking rings of\ndifferent colors: blue, yellow, black, green, and red. The rings\nare arranged in a way that they form a flower-like shape. The\nlogo is displayed on a grassy area with trees in the background.\n\nDescribe both the image and logo in details\n\nGPT-4V:\n\nThe image shows a close-up of a person’s foot wearing a white\nNike Air Force 1 sneaker. The sneaker is a low-top style with a\nwhite rubber sole and white laces. The shoe's upper is made of\na textured white leather, giving it a premium look.\n\nThe logo shown on the sneaker is the iconic Nike Swoosh. It is\ndisplayed on the side of the shoe in a silver color. The Swoosh\nis a curved, elongated checkmark shape with a pointed tip at\nthe end. It represents speed and motion, embodying the brand’s\nathletic and sporty aesthetic.\n\nPrompt:\n\nDescribe both the image and logo in details\n\nPiszre er 39\n\nThe image shows two Starbucks cups, one is a clear plastic cup with\na lid and straw, and the other is a white paper cup with a lid. The clear\nplastic cup contains a cold drink with ice and whipped cream on top,\nwhile the white paper cup contains a hot drink. The cups are placed\non a wooden table with a blurred background of a Starbucks store.\nThe logo on the cups is the Starbucks logo, which is a green circle\nwith a white mermaid or siren in the center\n\nDescribe both the image and logo in details\n\nThe image shows two Coca-Cola bottles lying on a bed of ice cubes.\nThe bottles are wet, indicating that they are chilled and ready to drink.\nThe bottle on the left is a new grip bottle, as mentioned on the label.\nThe bottle on the right is a regular Coca-Cola bottle. Both bottles\nhave the iconic Coca-Cola logo on them.\n\nThe logo is written in a cursive font in white color, with a red\nbackground. The first \"C\" in Coca is larger than the rest of the letters,\nand the in Cola is elongated. The logo is instantly recognizable\nand is synonymous with the brand.\n\nDescribe both the image and logo in details\n\n6\n\nMicrosoft 365 Copilot\n\n=|\n\nGPT-4V:\n\nThe image is a screenshot from a presentation about Microsoft 365\nCopilot. The image shows a man standing in front of a large screen\nwith the Microsoft 365 Copilot logo and some text on it. The man is\nwearing a blue sweater and is gesturing with his hands as he speaks.\nThe background is a modern-looking set with a green plant and a\ngray wall.\n\nThe logo for Microsoft 365 Copilot is a square with rounded corners.\nThe background of the logo is white, and the text \"Microsoft 365\nCopilot\" is written in black. In the center of the logo, there is a colorful\nshape that resembles a cube with three sides visible. The colors of\nthe shape are blue, purple, and pink, and the shape is slightly tilted to\nthe right. The logo is simple and modern, with a clean and\nprofessional look.\n\nFigure 21: Results on in-the-wild logo recognition and description. GPT-4V demonstrates strong\ncapability in understanding logos in many scenarios, including occlusions, lighting conditions, and\norientations. GPT-4V can also describe novel icons, such as the recently released Microsoft 365\nCopilot. Check Section 4.1 for detailed discussions.\n\n35\n", "vlm_text": "In-the-Wild Logo Recognition and Description  \nPrompt: \nThe image features a serving of McDonald’s French fries, which are known for their long, thin, golden-brown appearance. The fries are held in a classic McDonald's red paper container. The red carton features the iconic McDonald’s logo prominently in the center, which is a large yellow \"M.\" This \"M,\" often referred to as the \"Golden Arches,\" is a globally recognized symbol of the McDonald's brand. The background of the image is a solid yellow, complementing the branding colors of McDonald's.\nGPT-4V: \nThe image is of a McDonald’s french fry container, filled with  golden brown french fries. The container is red, and has the  iconic McDonald’s “M” logo on it. The background is a bright  yellow, making the red container and fries stand out. The logo is the McDonald’s “M” which is a large golden arch,  forming the letter “M” for McDonald’s. It is one of the most  recognized logos in the world, and is often associated with the  fast food chain. \nPrompt: \nDescribe both the image and logo in details \nThe image shows a person standing in front of large Olympic rings. The rings are colored blue, yellow, black, green, and red, which are the traditional colors of the Olympic symbol. The person appears to be wearing traditional clothing and is standing outdoors in a park-like setting.\nGPT-4V: \nThe image shows a woman in traditional Japanese clothing  standing in front of the Olympic rings logo. The woman is  wearing a purple kimono with a floral pattern and a red obi belt.  She is smiling and posing for the photo. The Olympic rings logo is made up of five interlocking rings of  different colors: blue, yellow, black, green, and red. The rings  are arranged in a way that they form a flower-like shape. The  logo is displayed on a grassy area with trees in the background.  \nPrompt: Describe both the image and logo in details \nThe image shows a close-up of a pair of white sneakers with the Nike logo. These are likely Nike Air Force 1 shoes, characterized by their distinctive design and logo.\nGPT-4V: \nThe image shows a close-up of a person’s foot wearing a white  Nike Air Force 1 sneaker. The sneaker is a low-top style with a  white rubber sole and white laces. The shoe’s upper is made of  a textured white leather, giving it a premium look. The logo shown on the sneaker is the iconic Nike Swoosh. It is  displayed on the side of the shoe in a silver color. The Swoosh  is a curved, elongated checkmark shape with a pointed tip at  the end. It represents speed and motion, embodying the brand’s  athletic and sporty aesthetic. \nThe image displays two Starbucks cups placed on a wooden surface, likely a table in a Starbucks café. The background shows a blurred view of shelves and a counter, typical of a café setting, with soft, warm lighting.\n\n1. The first cup on the left is a clear plastic cup filled with an iced beverage, possibly an iced coffee or latte. The cup is tall and cylindrical with a dome lid, indicating a cold drink. The beverage inside is light brown, suggesting it might contain coffee with cream or milk. The iconic Starbucks logo is prominently displayed on the cup. The logo features a two-tailed mermaid figure or siren, encircled by a green ring with the word \"Starbucks\" written above and \"Coffee\" written below.\n\n2. The second cup on the right is a standard Starbucks white paper cup with a cardboard sleeve wrapped around it, indicating a hot drink. The sleeve features the same Starbucks siren logo but is stylized in a more minimalist and monochromatic green design, likely as a more contemporary or artistic variation of the traditional logo. The lid is white and fits securely on top of the cup, typical of hot beverage cups.\n\nOverall, the image captures the quintessential Starbucks experience with both hot and cold drink options, showcasing the brand's recognizable and iconic logo.\nGPT-4V: \nThe image shows two Starbucks cups, one is a clear plastic cup with  a lid and straw, and the other is a white paper cup with a lid. The clear  plastic cup contains a cold drink with ice and whipped cream on top,  while the white paper cup contains a hot drink. The cups are placed  on a wooden table with a blurred background of a Starbucks store. The logo on the cups is the Starbucks logo, which is a green circle  with a white mermaid or siren in the center \nPrompt: \nDescribe both the image and logo in details \nThe image shows a Coca-Cola bottle and a Coca-Cola can lying on ice. The bottle has a label that reads \"New Grip Bottle\" and both the bottle and the can have droplets of condensation on them, suggesting they are cold.\nGPT-4V: \nThe image shows two Coca-Cola bottles lying on a bed of ice cubes.  The bottles are wet, indicating that they are chilled and ready to drink.  The bottle on the left is a new grip bottle, as mentioned on the label.  The bottle on the right is a regular Coca-Cola bottle. Both bottles  have the iconic Coca-Cola logo on them. The logo is written in a cursive font in white color, with a red  background. The first \"C\" in Coca is larger than the rest of the letters,  and the \"o\" in Cola is elongated. The logo is instantly recognizable  and is synonymous with the brand.  \nPrompt: \nDescribe both the image and logo in details \nThe image shows a person standing next to a presentation slide. The slide is titled \"Microsoft 365 Copilot\" and includes a diagram labeled \"Natural Language\" with components such as \"Large Language Models,\" \"Microsoft Graph - Your Data,\" and \"Microsoft 365 Apps.\"\nGPT-4V: \nThe image is a screenshot from a presentation about Microsoft 365  Copilot. The image shows a man standing in front of a large screen  with the Microsoft 365 Copilot logo and some text on it. The man is  wearing a blue sweater and is gesturing with his hands as he speaks.  The background is a modern-looking set with a green plant and a  gray wall. \nThe logo for Microsoft 365 Copilot is a square with rounded corners.  The background of the logo is white, and the text \"Microsoft 365  Copilot\" is written in black. In the center of the logo, there is a colorful  shape that resembles a cube with three sides visible. The colors of  the shape are blue, purple, and pink, and the shape is slightly tilted to  the right. The logo is simple and modern, with a clean and  professional look.  "}
{"page": 35, "image_path": "doc_images/2309.17421v2_35.jpg", "ocr_text": "Describe the image\n\nThe image is a screenshot from a dashcam, taken from the inside of a car, showing the view of the road ahead.\nThe car is driving on a two-lane road, with a speed limit sign of 35 mph visible on the right side of the road.\nThere are trees on both sides of the road, and the sky is filled with dark clouds, suggesting that it might rain\nsoon. Orange cones are visible on the left side of the road, indicating a construction zone. In the distance, there\nis a bridge overpass.\n\nDescribe the image\n\nThe image is a view of a road from the perspective of a car dashboard camera. The road has two lanes, and the\ncar is in the right lane behind a silver vehicle. There are two yellow signs on the side of the road warning drivers\nof an upcoming pedestrian crossing and a curve to the left. The sky is cloudy and gray, and trees and bushes\nline the sides of the road.\n\nFigure 22: Results on scene understanding. GPT-4V is able to provide a detailed description regarding\nthe scenes and objects. Check Section 4.1 for detailed discussions.\n\n36\n", "vlm_text": "Sec. 4.1 Image Description on Diverse Domains \nScene Understanding \nPrompt: Describe the image \nThe image shows a view of a road taken from the perspective of a vehicle driver. It features a two-lane road with a clear dividing line in the middle and a speed limit sign indicating a speed limit of 35 mph. There are also some traffic cones visible further down the road, suggesting a construction zone or lane closure. Dense green trees and shrubbery line the sides of the road, and a large, dark cloud formation dominates the sky, indicating overcast or possibly stormy weather. The scene gives a sense of a rural or semi-rural area.\nGPT-4V: \nThe image is a screenshot from a dashcam, taken from the inside of a car, showing the view of the road ahead.  The car is driving on a two-lane road, with a speed limit sign of 35 mph visible on the right side of the road.  There are trees on both sides of the road, and the sky is filled with dark clouds, suggesting that it might rain  soon. Orange cones are visible on the left side of the road, indicating a construction zone. In the distance, there  is a bridge overpass. \nPrompt: Describe the image \nThe image shows a road with a car in front, captured from another vehicle, possibly through a dashboard camera. There are cloudy skies overhead, suggesting overcast or impending rain. On both sides of the road, there are pedestrian crossing signs. The road has pedestrian islands and bollards separating lanes. Trees and vegetation are visible along the road, providing a natural backdrop.\nGPT-4V: \nThe image is a view of a road from the perspective of a car dashboard camera. The road has two lanes, and the  car is in the right lane behind a silver vehicle. There are two yellow signs on the side of the road warning drivers  of an upcoming pedestrian crossing and a curve to the left. The sky is cloudy and gray, and trees and bushes  line the sides of the road.  \nFigure 22: Results on scene understanding. GPT-4V is able to provide a detailed description regarding the scenes and objects. Check Section  4.1  for detailed discussions. "}
{"page": 36, "image_path": "doc_images/2309.17421v2_36.jpg", "ocr_text": "Sec. 4.1 Image Description on Diverse Domains\n\nCounterfactual Examples\n\nPrompt:\n\nCan you find the red ball near the gray cat in the\n\nSorry, | cannot answer this question as there is no\nred ball visible in the image.\n\nCan you spot the pink elephant playing in the\ncorner?\n\nGPT-4V:\n\nSorry, | cannot because there is no pink elephant\nplaying in the corner of this image. There is only a\nchild wearing an orange shirt and a bear in the\nbackground.\n\nTry to locate a large waterfall behind the trees.\n\nGPT-4V:\nSorry, | am not able to complete this task as there is\nno waterfall present in the given image.\n\nPrompt:\nCan you see the bright blue feathers on the bird's\nwing?\n\nGPT-4V:\nNo, | cannot see bright blue feathers on the bird's\nwing. It appears to have brown and white feathers.\n\nCan you find the baseball player wearing a blue\nshirt?\n\nGPT-4V:\n\nNo, | cannot find a baseball player wearing a blue\nshirt in this image. All the players appear to be\nwearing black shirts.\n\nCan you find the blue lighting fixture mounted to the\nceiling?\n\nSorry, | cannot find a blue lighting fixture mounted to\nthe ceiling in this image. All | see is a black wrought\niron chandelier with white glass shades.\n\nFigure 23: Results on counterfactual examples. GPT-4V is able to provide factual descriptions\nregarding the scenes and objects in the images. Example images are from [78]. Check Section 4.1 for\ndetailed discussions.\n\n37\n", "vlm_text": "Sec. 4.1 Image Description on Diverse Domains \nCounter factual Examples \nPrompt: \nCan you find the red ball near the gray cat in the  image? \nThe image shows a gray cat lying on a cushion on a couch. In the foreground, there is a part of a laptop that appears to be facing the cat.\nGPT-4V: Sorry, I cannot answer this question as there is no  red ball visible in the image. \nPrompt: \nCan you spot the pink elephant playing in the  corner? \nThe image shows a child wearing a red cap and orange shirt looking at an animal. The animal appears to be a bear, viewed from behind, in a zoo-like setting with rocky terrain. Part of an adult's shoulder is visible on the left side of the image.\nGPT-4V: \nSorry, I cannot because there is no pink elephant  playing in the corner of this image. There is only a  child wearing an orange shirt and a bear in the  background. \n\nTry to locate a large waterfall behind the trees. \nThe image shows an elephant in a grassy, wooded area with at least one person sitting on its back. There are three other people standing in the vicinity. The setting appears to be a natural or park environment, featuring lush greenery and trees around them.\nGPT-4V: Sorry, I am not able to complete this task as there is  no waterfall present in the given image. \nPrompt: \nCan you see the bright blue feathers on the bird's  wing? \nThe image shows a bird of prey, likely an eagle, flying over a grassy area. Its wings are spread wide as it glides through the air, and there's a blurred background of hills or mountains.\nGPT-4V: No, I cannot see bright blue feathers on the bird's  wing. It appears to have brown and white feathers. \nPrompt: \nCan you find the baseball player wearing a blue  shirt? \nThe image shows a group of people wearing baseball uniforms on a field. They appear to be engaged in a practice or warm-up session. The field is surrounded by a chain-link fence, and there are trees and buildings in the background.\nGPT-4V: No, I cannot find a baseball player wearing a blue  shirt in this image. All the players appear to be  wearing black shirts. \nPrompt: \nCan you find the blue lighting fixture mounted to the  ceiling? \nThe image shows a kitchen with a central island countertop. There are three pendant lights hanging above the island. On the countertop, there are plates of sliced watermelon and stacked dishes. The kitchen has large windows in the back, cabinets, a coffee maker, a toaster, and a stainless steel refrigerator. The colors in the kitchen are warm, with wood and light yellow tones.\nGPT-4V: Sorry, I cannot find a blue lighting fixture mounted to  the ceiling in this image. All I see is a black wrought  iron chandelier with white glass shades. \nFigure 23: Results on counter factual examples. GPT-4V is able to provide factual descriptions regarding the scenes and objects in the images. Example images are from [ 78 ]. Check Section  4.1  for detailed discussions. "}
{"page": 37, "image_path": "doc_images/2309.17421v2_37.jpg", "ocr_text": "4.2 Object Localization, Counting, and Dense Captioning\n\nSpatial relationship understanding. Understanding the spatial relationship between humans and\nobjects in the image is a vital aspect of visual intelligence [61, 14]. In Figure 24, GPT-4V showcases\npromising capabilities in this regard. It can identify the spatial relationship between the frisbee and\nthe man in the image. It can also recognize the spatial relationship between the man and the car in\nthe image, and point out that the camera perspective may affect their perceived size.\n\nObject counting. Figure 25 highlights our exploration of GPT-4V’s capability in object counting. In\nour experiments, we employ the text prompt “Count the number of X in the image” to evaluate its\nperformance. The results indicate that GPT-4V can successfully count the number of objects, such\nas apples, oranges, and people, present in the image. However, challenges arise when objects are\noccluded, or the scene is cluttered, which can result in errors in the counting process. In the bottom\nleft of Figure 25, GPT-4V identifies 12 people, but the correct answer should be 11. This may be due\nto our limited text prompt used in this experiment, and further investigation in prompting techniques\nis needed.\n\nObject localization. Object localization [153, 76, 51] is a fundamental challenge in the field of\ncomputer vision. In our preliminary experiments, we address this task by utilizing a simple text\nprompt, “Localize each person in the image using a bounding box.” The initial results of our object\nlocalization experiments are depicted in Figure 26. The findings suggest that GPT-4V demonstrates\nthe capability to generate bounding box coordinates in textual format, without separate textualized box\ntokens [25, 140, 129, 26, 82, 105]. However, it is important to note that the generated bounding box\ncoordinates are not accurate. We rescaled the predicted bounding box coordinates during visualization.\nPromising localization results are observed when the scene or background is relatively simpler and\nless cluttered. Further prompting techniques are required to enhance object localization performance\nin more complex and crowded environments.\n\nDense captioning. Dense captioning [62, 84] involves generating detailed description for each\nregion of interest in the given image. This advanced task in vision-language field typically requires a\ncomplex system that integrates multiple experts, such as object detector, celebrity recognition model,\nand image captioning model. In order to explore GPT-4V’s capabilities in dense captioning, we\nuse an instructional prompt, as shown in Figure 27. The results are highly encouraging, as GPT-4V\nsuccessfully localizes and recognizes the individuals within the image, and then provides concise\ndescriptions for each scientist.\n\n38\n", "vlm_text": "4.2 Object Localization, Counting, and Dense Captioning \nSpatial relationship understanding.  Understanding the spatial relationship between humans and objects in the image is a vital aspect of visual intelligence [ 61 ,  14 ]. In Figure  24 , GPT-4V showcases promising capabilities in this regard. It can identify the spatial relationship between the frisbee and the man in the image. It can also recognize the spatial relationship between the man and the car in the image, and point out that the camera perspective may affect their perceived size. \nObject counting.  Figure  25  highlights our exploration of GPT-4V’s capability in object counting. In our experiments, we employ the text prompt “Count the number of X in the image” to evaluate its performance. The results indicate that GPT-4V can successfully count the number of objects, such as apples, oranges, and people, present in the image. However, challenges arise when objects are occluded, or the scene is cluttered, which can result in errors in the counting process. In the bottom left of Figure  25 , GPT-4V identifies 12 people, but the correct answer should be 11. This may be due to our limited text prompt used in this experiment, and further investigation in prompting techniques is needed. \nObject localization.  Object localization [ 153 ,  76 ,  51 ] is a fundamental challenge in the field of computer vision. In our preliminary experiments, we address this task by utilizing a simple text prompt, “Localize each person in the image using a bounding box.” The initial results of our object localization experiments are depicted in Figure  26 . The findings suggest that GPT-4V demonstrates the capability to generate bounding box coordinates in textual format, without separate textual i zed box tokens [ 25 ,  140 ,  129 ,  26 ,  82 ,  105 ]. However, it is important to note that the generated bounding box coordinates are not accurate. We rescaled the predicted bounding box coordinates during visualization. Promising localization results are observed when the scene or background is relatively simpler and less cluttered. Further prompting techniques are required to enhance object localization performance in more complex and crowded environments. \nDense captioning.  Dense captioning [ 62 ,  84 ] involves generating detailed description for each region of interest in the given image. This advanced task in vision-language field typically requires a complex system that integrates multiple experts, such as object detector, celebrity recognition model, and image captioning model. In order to explore GPT-4V’s capabilities in dense captioning, we use an instructional prompt, as shown in Figure  27 . The results are highly encouraging, as GPT-4V successfully localizes and recognizes the individuals within the image, and then provides concise descriptions for each scientist. "}
{"page": 38, "image_path": "doc_images/2309.17421v2_38.jpg", "ocr_text": "Sec. 4.2 Object Localization, Counting, and Dense Captioning\n\nSpatial Relationship Understanding\nPrompt:\n\nWhat is the spatial relation between the frisbee and the man?\n\nGPT-4V:\n\nBased on the image provided, the spatial relation between the frisbee and the man is that the frisbee is above\nand slightly in front of the man. It also appears to be moving away from the man, as if it has been thrown or\nreleased by him.\n\nDoes the person bigger than the car?\n\nNo, the car appears to be bigger than the person in this image. However, the perspective and distance of the\nobjects in the photo may affect their perceived size.\n\nFigure 24: Results on spatial relationship understanding. GPT-4V recognizes the spatial relationship\nbetween the objects in the images. Example images are from [67, 14]. Check Section 4.2 for detailed\ndiscussions.\n\n39\n", "vlm_text": "Spatial Relationship Understanding \nPrompt: \nWhat is the spatial relation between the frisbee and the man? \nThe image depicts an outdoor scene where a person is playing with a dog. The person, wearing a red sweatshirt, blue shorts, and a white cap and shoes, appears to be engaged in an activity similar to frisbee or disc play. They are crouched down, creating a platform with their back for the dog to leap from. A black and white dog is jumping in the air toward an orange frisbee or disc, which is captured mid-flight. The background shows a grassy field with trees, suggesting the setting is a park or open natural area.\nGPT-4V: \nBased on the image provided, the spatial relation between the frisbee and the man is that the frisbee is above  and slightly in front of the man. It also appears to be moving away from the man, as if it has been thrown or  released by him. \nPrompt: \nDoes the person bigger than the car? \nThe image shows a man standing on the side of a road observing a cyclist and a car. The man is in the foreground, wearing a white t-shirt and jeans, and he is holding something in his hands. In the background, there is a cyclist on the road and a car approaching. The size of the car compared to the cyclist and the man is affected by the perspective and distance in the image, making the car appear larger relative to the other objects. The setting is outdoors on a clear day with a rocky hill or mountain in the background.\nFigure 24: Results on spatial relationship understanding. GPT-4V recognizes the spatial relationship between the objects in the images. Example images are from [ 67 ,  14 ]. Check Section  4.2  for detailed discussions. "}
{"page": 39, "image_path": "doc_images/2309.17421v2_39.jpg", "ocr_text": "Sec. 4.2 Object Localization, Counting, and Dense Captioning\n\nObject Counting\n\nPrompt:\nCount the number of apples in the image.\na\n\nGPT-4V:\n\nThere are 7 apples in the image.\n\nCount the number of oranges in the image.\n\nGPT-4V:\n\nThere are 3 oranges in the image.\n\nPrompt:\nCount the number of people in the image.\n\nGPT-4V:\n\nThere are 4 people in the image.\n\nPrompt:\nCount the number of apples in the image.\n\nGPT-4V:\n\nThere are 16 apples in the image.\n\nCount the number of people in the image.\n\nGPT-4V:\n\nThere are 3 people in the image.\n\nCount the number of people in the image.\n\nGPT-4V:\n\nThere are 42 people in the image.\n\nFigure 25: Results on object counting. GPT-4V is able to determine the quantity of the specified\nobjects the image. ‘Red highlights the wrong answer. Check Section 4.2 for detailed discussions.\n\n40\n", "vlm_text": "Sec. 4.2 Object Localization, Counting, and Dense Captioning  \nObject Counting \nPrompt: Count the number of apples in the image. \nThe image shows a group of apples on a wooden surface, with some greenery in the background. The apples are red and yellow in color.\nGPT-4V: There are 7 apples in the image. \nPrompt: Count the number of oranges in the image. \nThe image shows three oranges placed on a red surface. One of the oranges is balanced on top of the other two. The top orange has a small branch with two green leaves still attached to it. In the background, there appears to be a piece of paper with black calligraphy or artwork on the red surface, a common sight during cultural or festive arrangements, likely linking to a context like a celebration or an event.\nGPT-4V: There are 3 oranges in the image. \nPrompt: Count the number of people in the image. \nThe image shows four individuals standing indoors, each wearing a name tag, suggesting they might be attending a conference or formal gathering. Behind them is a room with hanging lights and a partially opened door. The individuals are casually dressed in sweaters and shirts.\nGPT-4V: There are 4 people in the image. \nPrompt: Count the number of apples in the image. \nThe image shows a collection of apples arranged in a grid pattern. The apples are of different varieties and colors, including shades of red, yellow, and green.\nGPT-4V: There are 16 apples in the image. \nPrompt: Count the number of people in the image. \nThe image shows three people standing with their arms crossed, wearing casual clothing that includes two plaid shirts and one solid-colored shirt. The background is plain white, and there is a blue border surrounding the image.\nGPT-4V: There are 3 people in the image. \nPrompt: Count the number of people in the image. \nThe image depicts a group of people gathered around a dining table, which is set with plates of food, drinks, and cutlery. The table has several dishes, including bread, salad, and various condiments. There are also bottles of wine and glasses on the table, indicating a shared meal or gathering. The setting appears to be indoors, possibly in a home or restaurant.\nGPT-4V: There are 12 people in the image. \nFigure 25: Results on object counting. GPT-4V is able to determine the quantity of the specified objects the image. Red highlights the wrong answer. Check Section  4.2  for detailed discussions. "}
{"page": 40, "image_path": "doc_images/2309.17421v2_40.jpg", "ocr_text": "Sec. 4.2 Object Localization, Counting, and Dense Captioning\n\nObject Localization\n\nPrompt: Prompt:\nLocalize each person in the image using bounding Localize each car in the image using bounding box.\nbox. The image size is (width, height) = (998,786) What is the image size of the input image?\n\nGPT-4V:\n\nThere is only one person in the image, so the\n\nbounding box for this person would be: The image size of the input image is 500X284 pixels.\n\n- Top left: (254, 118)\n\n- Bottom right: (731, 693) Bounding box for the black car:\n- Top left corner: (63, 114)\n\nExplanation: The top left point of the bounding box is - Bottom right corner: (196, 220)\n\nat (x, y) = (254, 118), which is the top left corner of the\n\nperson in the image. The bottom right point of the Bounding box for the white car:\n\nbounding box is at (x, y) = (731, 693), which is the - Top left corner: (287, 96)\n\nbottom right corner of the person in the image. This - Bottom right corner: (474, 220)\n\nbounding box encompasses the entire person in the\n\nimage. Visualization of Detected Bbox:\n\nVisualization of Detected Bbox:\n\nLocalize each person in the image using bounding box. What is the image size of the input image?\n\nGPT-4V:\nThe image size of the input image is 1000x333 pixels.\n\nBounding boxes for each person:\n\n- Person 1 (leftmost): (x1, y1) = (22, 108), (x2, y2) = (170, 300)\n\n- Person 2: (x1, y1) = (200, 80), (x2, y2) = (290, 270)\n\n- Person 3: (x1, y1) = (350, 70), (x2, y2) = (460, 280)\n\n- Person 4: (x1, y1) = (540, 80), (x2, y2) = (640, 270)\n\n- Person 5 (rightmost): (x1, y1) = (760, 100), (x2, y2) = (920, 290)\n\nNote: These bounding box coordinates are approximate and may not be exact.\n\nVisualization of Detected Bbox:\n\nFigure 26: Results on object localization. GPT-4V is able to generate and approximate the bounding\nbox coordinates for the specified objects in the image. When providing a simple text prompt only, the\nmodel may encounter challenges when dealing with more complex scenarios like object occlusions\nand cluttered scenes. ‘Red highlights the wrong answer. We rescaled the predictions when visualizing\nthe bounding boxes. Check Section 4.2 for detailed discussions.\n\n41\n", "vlm_text": "Object Localization \nPrompt: \nLocalize each person in the image using bounding  box. The image size is (width, height)  $,=(998{,}786)$  \nThe image shows a person wearing a sparkly purple outfit while holding a microphone on stage during a performance. It appears to be raining, as there are visible raindrops in the scene.\nGPT-4V: \nThere is only one person in the image, so the  bounding box for this person would be: - Top left: (254, 118) - Bottom right: (731, 693) \nExplanation: The top left point of the bounding box is  at   $({\\mathsf{x}},{\\mathsf{y}})=(254,\\,118)$  , which is the top left corner of the  person in the image. The bottom right point of the  bounding box is at   $({\\sf x},{\\sf y})=(731,693)$  , which is the  bottom right corner of the person in the image. This  bounding box encompasses the entire person in the  image.  \nVisualization of Detected Bbox: \nThe image shows a person performing on stage, wearing a sparkly purple outfit. It appears to be raining during the performance.\nPrompt: \nLocalize each car in the image using bounding box.  What is the image size of the input image? \nThe image shows two cars driving on a multi-lane highway. The car on the left is black, and the car on the right is white. They are both traveling in the same direction. The background includes trees and a clear blue sky.\nGPT-4V: \nBounding box for the black car: - Top left corner: (63, 114) - Bottom right corner: (196, 220) \nBounding box for the white car: - Top left corner: (287, 96) - Bottom right corner: (474, 220)  \nVisualization of Detected Bbox: \nThe image shows two cars on a multi-lane road. There are red boxes around each car, likely indicating they are being highlighted for some reason, such as object detection. The background includes trees and a highway setting.\nPrompt: \nLocalize each person in the image using bounding box. What is the image size of the input image? \nThe image shows a group of people walking across a crosswalk in a city setting. There are cars and a bus on the road, with more pedestrians and buildings visible in the background. Trees line the street, suggesting an urban environment.\nGPT-4V: The image size of the input image is   $1000\\times333$   pixels. \nBounding boxes for each person: - Person 1 (leftmost):   $(\\mathsf{x}1,\\mathsf{y}1)=(22,\\,108)$  ,  $(\\mathsf{x}2,\\mathsf{y}2)=(170,300)$  - Person 2: (x1, y1)  $=$   (200, 80),  $(\\mathsf{x}2,\\mathsf{y}2)=(290,270)$  - Person 3: (x1, y1)  $=$   (350, 70),  $({\\mathsf{x}}2,{\\mathsf{y}}2)=(460,280)$  - Person 4: (x1, y1)  $=$   (540,   $\\mathsf{30),}\\,\\mathsf{(x2,y2)}=\\mathsf{(640,270)}$   $=$   $(\\mathsf{x}2,\\mathsf{y}2)=(920,290)$  \nNote: These bounding box coordinates are approximate and may not be exact.  \nVisualization of Detected Bbox: \nThe image shows people crossing a street at a crosswalk. Red rectangles are overlaid on each person, likely indicating they are being detected or highlighted for some purpose, possibly related to pedestrian detection or analysis in an urban environment. There are vehicles and other pedestrians visible in the background.\nFigure 26: Results on object localization. GPT-4V is able to generate and approximate the bounding box coordinates for the specified objects in the image. When providing a simple text prompt only, the model may encounter challenges when dealing with more complex scenarios like object occlusions and cluttered scenes.  Red  highlights the wrong answer. We rescaled the predictions when visualizing the bounding boxes. Check Section  4.2  for detailed discussions. "}
{"page": 41, "image_path": "doc_images/2309.17421v2_41.jpg", "ocr_text": "Dense Captioning\n\nPrompt:\n\nPlease follow the instructions\n\n1. Tell me the size of the input image;\n\n2. Localize each person in the image using bounding box;\n3. Recognize each person;\n\n4. Generate detailed caption for each bounding box.\n\nGPT-4V:\n1. The size of the input image is 1600 x 1067 pixels.\n\n2. Bounding box 1: (x1, y1) = (120, 250), (x2, y2) = (420, 950) - Person 1\nBounding box 2: (x1, y1) = (500, 240), (x2, y2) = (800, 940) - Person 2\nBounding box 3: (x1, y1) = (880, 230), (x2, y2) = (1180, 930) - Person 3\nBounding box 4: (x1, y1) = (1260, 220), (x2, y2) = (1560, 920) - Person 4\n\n. Person 1: Yann LeCun\nPerson 2: Geoffrey Hinton\nPerson 3: Yoshua Bengio\nPerson 4: Andrew Ng\n\n4. Caption for bounding box 1: Yann LeCun, computer scientist and one of the founding fathers of artificial\nintelligence.\n\nCaption for bounding box 2: Geoffrey Hinton, computer scientist and cognitive psychologist, known for his work\non artificial neural networks.\n\nCaption for bounding box 3: Yoshua Bengio, computer scientist and co-founder of the Montreal Institute for\nLearning Algorithms.\n\nCaption for bounding box 4: Andrew Ng, computer scientist and entrepreneur, known for his work on deep\nlearning and artificial intelligence.\n\nVisualization of Dense Captioning:\n\n‘Andrew Ng, computer\nYoshua Bengio, computer _scientist and\n\nscientist and co-founder of _ entrepreneur, known for\nscientist and one of the ae oe erat the Montreal Institute for his work on deep leaming\nfounding fathers of rater Learning Algorithms. and artificial intelligence.\nartificial intelligence. s\n\nGeoffrey Hinton, computer\n\nVannilecUntGaalen scientist and cognitive\n\nFigure 27: Results on dense captioning. GPT-4V follows the text prompt and successfully generates\n\ndense captions for the input image. |Red highlights the wrong answer. We rescaled the predictions\nwhen visualizing the bounding boxes. Check Section 4.2 for detailed discussions.\n42\n", "vlm_text": "Dense Captioning  \nPrompt: \nPlease follow the instructions 1. Tell me the size of the input image; 2. Localize each person in the image using bounding box; 3. Recognize each person; \n4. Generate detailed caption for each bounding box. \nThe image shows four people standing together indoors. They are dressed in casual to semi-formal attire, with a couple of them wearing conference badges. The setting appears to be at an event or conference, given the presence of name tags and lanyards.\nGPT-4V: \n1. The size of the input image is   $1600\\times1067$   pixels. \n2. Bounding box 1: (x1, y1)  $=$  (120, 250),   $(\\mathsf{x}2,\\mathsf{y}2)=(420,950)$   - Person 1     Bounding box 2  $\\L_{:}({\\mathsf{x}}1,{\\mathsf{y}}1)=(500,240),({\\mathsf{x}}2,{\\mathsf{y}}2)=(800,940)$   - Person 2     Bounding box 3: (x1, y1)  $=$   (880, 230), (x2, y2) = (1180, 930) - Person 3     Bounding box 4: (x1, y1)  $=$   (1260, 220),  $({\\mathsf{x}}2,{\\mathsf{y}}2)=$  (1560, 920) - Person 4 \n3. Person 1: Yann LeCun     Person 2: Geoffrey Hinton     Person 3: Yoshua Bengio     Person 4: Andrew Ng \n4. Caption for bounding box 1: Yann LeCun, computer scientist and one of the founding fathers of artificial  \nintelligence.     Caption for bounding box 2: Geoffrey Hinton, computer scientist and cognitive psychologist, known for his work  on artificial neural networks.     Caption for bounding box 3: Yoshua Bengio, computer scientist and co-founder of the Montreal Institute for  Learning Algorithms.     Caption for bounding box 4: Andrew Ng, computer scientist and entrepreneur, known for his work on deep  learning and artificial intelligence.  \nVisualization of Dense Captioning: \nThe image shows four individuals, each with a colored box and caption identifying them. From left to right: \n\n1. Yann LeCun, described as a computer scientist and one of the founding fathers of artificial intelligence.\n2. Geoffrey Hinton, identified as a computer scientist and cognitive psychologist, known for his work on artificial neural networks.\n3. Yoshua Bengio, described as a computer scientist and co-founder of the Montreal Institute for Learning Algorithms.\n4. Andrew Ng, identified as a computer scientist and entrepreneur, known for his work on deep learning and artificial intelligence.\n\nThe image is used to demonstrate dense captioning, with bounding boxes highlighting sections and captions explaining the content."}
{"page": 42, "image_path": "doc_images/2309.17421v2_42.jpg", "ocr_text": "4.3 Multimodal Knowledge and Commonsense\n\nJoke and meme. Jokes and memes often reference specific events, pop culture, or Internet trends. Un-\nderstanding these references requires being familiar with the relevant context and cultural knowledge.\nGrasping the visual elements, their relationship to the text, and the intended humorous effect can be\na complex task [99]. Moreover, memes are often user-generated, making them highly diverse and\never-expanding. To evaluate GPT-4V’s ability in this domain, we input a pair of meme and text prompt\nto GPT-4V. The example text prompts include “Can you explain the meme?” and “What is funny\nabout the image?” Figure 28 shows the example results. We observe that GPT-4V has remarkable\nability to gather information from both visual and textual modalities, and then comprehend the humor\nembedded within memes.\n\nScience and knowledge. We further investigate GPT-4V’s capability in tasks that requires reasoning\nwith scientific knowledge [85]. We conduct experiments by providing a text prompt question and\na corresponding image. The questions cover a wide range of topics, including geography, physics,\nbiology, and earth science. In Figures 29-31, we observe that GPT-4V is able to correctly answer\nthe science questions based on the visual context. For instance, in the bottom row of Figure 29,\nGPT-4V recognizes the average particle speed for both sample A and sample B. By considering the\nrelationship among particle speed, kinetic energy, and temperature, GPT-4V answers the question\ncorrectly. For another instance, as shown in the bottom row of Figure 30, GPT-4V takes into account\nthe visual arrows presented in the figure to identify the producer in the specific food web. Moreover,\nas shown in Figure 31, when we provide a more specific prompt, such as “Suppose you are a teacher,\nplease use the figure to explain X,” we observe the generated answer adopts a tutorial format and\nexplains the subject step by step.\n\nMultimodal commonsense. In Figure 32, we access the ability of GPT-4V in multimodal common-\nsense reasoning [148, 52]. In our experiments, we observed that GPT-4V effectively utilizes the\nbounding boxes presented in the image as visual prompts (e.g., [person1] and [person2]) to recognize\nthe actions performed by the individuals. As shown in the second example in Figure 32, based on\nthe formal dress worn by [person1] and [person2] and the floral decorations present in the scene, it\ncan be inferred that they are attending a wedding ceremony. Moreover, when we provide a more\nspecific input prompt, such as \"Suppose you are a detective, what can you infer from the visual\nclues?\", GPT-4V demonstrates the ability to discern numerous nuanced visual cues within the image\nand offers a list of plausible hypotheses.\n\n4B\n", "vlm_text": "4.3 Multimodal Knowledge and Commonsense \nJoke and meme.  Jokes and memes often reference specific events, pop culture, or Internet trends. Un- der standing these references requires being familiar with the relevant context and cultural knowledge. Grasping the visual elements, their relationship to the text, and the intended humorous effect can be a complex task [ 99 ]. Moreover, memes are often user-generated, making them highly diverse and ever-expanding. To evaluate GPT-4V’s ability in this domain, we input a pair of meme and text prompt to GPT-4V. The example text prompts include “Can you explain the meme?” and “What is funny about the image?” Figure  28  shows the example results. We observe that GPT-4V has remarkable ability to gather information from both visual and textual modalities, and then comprehend the humor embedded within memes. \nScience and knowledge.  We further investigate GPT-4V’s capability in tasks that requires reasoning with scientific knowledge [ 85 ]. We conduct experiments by providing a text prompt question and a corresponding image. The questions cover a wide range of topics, including geography, physics, biology, and earth science. In Figures  29 - 31 , we observe that GPT-4V is able to correctly answer the science questions based on the visual context. For instance, in the bottom row of Figure  29 , GPT-4V recognizes the average particle speed for both sample A and sample B. By considering the relationship among particle speed, kinetic energy, and temperature, GPT-4V answers the question correctly. For another instance, as shown in the bottom row of Figure  30 , GPT-4V takes into account the visual arrows presented in the figure to identify the producer in the specific food web. Moreover, as shown in Figure  31 , when we provide a more specific prompt, such as “Suppose you are a teacher, please use the figure to explain X,” we observe the generated answer adopts a tutorial format and explains the subject step by step. \nMultimodal commonsense.  In Figure  32 , we access the ability of GPT-4V in multimodal common- sense reasoning [ 148 ,  52 ]. In our experiments, we observed that GPT-4V effectively utilizes the bounding boxes presented in the image as visual prompts ( e.g.,  [person1] and [person2]) to recognize the actions performed by the individuals. As shown in the second example in Figure  32 , based on the formal dress worn by [person1] and [person2] and the floral decorations present in the scene, it can be inferred that they are attending a wedding ceremony. Moreover, when we provide a more specific input prompt, such as \"Suppose you are a detective, what can you infer from the visual clues?\", GPT-4V demonstrates the ability to discern numerous nuanced visual cues within the image and offers a list of plausible hypotheses. "}
{"page": 43, "image_path": "doc_images/2309.17421v2_43.jpg", "ocr_text": "Sec. 4.3 Multimodal Knowledge and Commonsense\n\nJoke and Meme\n\nPrompt:\nCan you explain this meme?\n\nMe: I'll do it at 8\nTime: 8.05\nMe: looks like | gotta wait till 9 now\n\nGPT-4V:\n\nThis meme is poking fun at the idea of procrastination\nand how someone may put off doing a task until a\ncertain time, but if they miss that time, they will push\nit back even further. In this case, the person in the\nmeme planned to do something at 8 o’clock, but\nsince it is now 8:05, they have decided to wait until 9\no'clock to do it. The image of Kermit the Frog relaxing\nin bed represents the person’s desire to procrastinate\nand avoid the task.\n\nWhat is funny about this image?\n\nYou see the world\nas you are!\n\nGPT-4V:\n\nThere are a couple of things that people might find\nfunny about this image:\n\n- Itis a cartoon of a rhinoceros painting, which is an\nunexpected and silly concept in itself.\n\n- The rhinoceros is painting a picture of itself, which\nadds an extra layer of humor.\n\n- The caption, “You see the world as you are!”, isa\njoke about how we all have our own unique\nperspectives and can be a little self-centered at\ntimes.\n\nFigure 28: Results on joke and meme understanding. GPT-4V demonstrates the impressive capability\n\nPrompt:\nCan you explain this meme?\n\nMy Handwriting In Exams\n\nFirst two pages\n\nMiddle pages\n\nLast two pages\n\nGPT-4V:\n\nThis meme highlights the deterioration of a person’s\nhandwriting during an exam. The first two pages\nshow neat and legible handwriting, the middle page\nshows slightly messier handwriting, and the last two\npages show almost illegible handwriting.\n\nThe meme is poking fun at how, as an exam\nprogresses, a person's handwriting tends to get\nworse as they become more tired and rushed to\nfinish the exam. It also implies that the person’s focus\nand attention to detail decrease towards the end of\nthe exam.\n\nWhat is funny about this image?\n\nGPT-4V:\n\nThis image is funny for a couple of reasons. Firstly, it\nis a meme template that has been widely used and\nrecognized on the internet. The left side of the image\nis a screenshot from the reality TV show \"The Real\nHousewives of Beverly Hills\" where one of the cast\nmembers is angrily pointing and yelling. The right\nside of the image is a picture of a cat sitting ata\ndinner table with a plate of food in front of it, looking\nunimpressed. The humor in this image comes from\nthe contrast between the two sides - the angry,\ndramatic yelling on the left and the nonchalant cat on\nthe right. It also plays on the idea of cats acting like\nhumans and being unimpressed by our behavior.\nOverall, the image creates a funny and absurd\nsituation that makes people laugh.\n\nto comprehend the humor embedded within memes. Check Section 4.3 for detailed discussions.\n\n44\n", "vlm_text": "Sec. 4.3 Multimodal Knowledge and Commonsense \nJoke and Meme  \nPrompt: \nCan you explain this meme? \n\n\nThe image shows a stuffed toy of a green frog, suggesting it's relaxed with its arms behind its head, lying on a bed. It resembles a well-known frog character.\nGPT-4V: \nThis meme is poking fun at the idea of procrastination  and how someone may put off doing a task until a  certain time, but if they miss that time, they will push  it back even further. In this case, the person in the  meme planned to do something at 8 o’clock, but  since it is now 8:05, they have decided to wait until 9  o’clock to do it. The image of Kermit the Frog relaxing  in bed represents the person’s desire to procrastinate  and avoid the task. \nPrompt: \nWhat is funny about this image? \nThe image is a cartoon featuring a rhino painting a landscape. The rhino's paintings, however, all depict a large rhino horn as a prominent feature in the scene. There are multiple canvases, each showing different versions of the landscape with a prominent horn in the middle. The caption at the bottom reads, \"You see the world as you are!\" This image humorously illustrates the concept that individuals often perceive the world through the lens of their own identity or characteristics.\nGPT-4V: \nThere are a couple of things that people might find  funny about this image: - It is a cartoon of a rhinoceros painting, which is an  unexpected and silly concept in itself. - The rhinoceros is painting a picture of itself, which  adds an extra layer of humor. - The caption, “You see the world as you are!”, is a  joke about how we all have our own unique  perspectives and can be a little self-centered at  times. \nPrompt: \nCan you explain this meme? \nThe image is a humorous depiction of how handwriting changes throughout an exam. It shows three sections labeled \"First two pages,\" \"Middle pages,\" and \"Last two pages.\"\n\n1. **First two pages**: The handwriting is neat and legible.\n2. **Middle pages**: The handwriting becomes messier.\n3. **Last two pages**: It resembles an ECG or heartbeat pattern, indicating the handwriting has become illegible and chaotic.\n\nThe image humorously illustrates the deterioration of handwriting over time during an exam.\nGPT-4V: \nThis meme highlights the deterioration of a person’s  handwriting during an exam. The first two pages  show neat and legible handwriting, the middle page  shows slightly messier handwriting, and the last two  pages show almost illegible handwriting. The meme is poking fun at how, as an exam  progresses, a person’s handwriting tends to get  worse as they become more tired and rushed to  finish the exam. It also implies that the person’s focus  and attention to detail decrease towards the end of  the exam. \nPrompt: \nWhat is funny about this image? \nThe image is a popular meme format, featuring two parts:\n\n1. On the left, a woman appears to be yelling and pointing with another woman trying to calm her.\n2. On the right, a white cat sits at a table looking confused or disinterested, with a plate of food in front of it.\n\nThis meme is often used for humorous contrasts or to express disagreement over trivial matters.\nGPT-4V: \nThis image is funny for a couple of reasons. Firstly, it  is a meme template that has been widely used and  recognized on the internet. The left side of the image  is a screenshot from the reality TV show \"The Real  Housewives of Beverly Hills\" where one of the cast  members is angrily pointing and yelling. The right  side of the image is a picture of a cat sitting at a  dinner table with a plate of food in front of it, looking  unimpressed. The humor in this image comes from  the contrast between the two sides - the angry,  dramatic yelling on the left and the nonchalant cat on  the right. It also plays on the idea of cats acting like  humans and being unimpressed by our behavior.  Overall, the image creates a funny and absurd  situation that makes people laugh. "}
{"page": 44, "image_path": "doc_images/2309.17421v2_44.jpg", "ocr_text": "Sec. 4.3 Multimodal Knowledge and Commonsense\n\nScience and Knowledge\n\nPrompt:\nWhich of these oceans does the prime meridian intersect?\n\nThe prime meridian intersects the following oceans:\n- Atlantic Ocean\n\n- Southern Ocean\n\n- Arctic Ocean\n\nCompare the average kinetic energies of the particles in each sample. The diagrams below show two pure\nsamples of gas in identical closed, rigid containers. Each colored ball represents one gas particle. Both samples\nhave the same number of particles. Which sample has the higher temperature? Sample A or sample B?\n\n@)\n; |\no P\n\nSample A Sample B\n\nMass of each particle: 32 u Mass of each particle: 32 u\n\nBecause the average particle speed in sample A is higher than in sample B (600 m/s vs. 540 m/s), the particles in\nsample A have a higher average kinetic energy. Therefore, sample A has a higher temperature than sample B.\n\nFigure 29: Results on answering science questions. GPT-4V can understand the question textually\nand visually, and gather necessary information to answer the question. Example images are from\n[85]. Check Section 4.3 for detailed discussions.\n\n45\n", "vlm_text": "Sec. 4.3 Multimodal Knowledge and Commonsense \nScience and Knowledge \nPrompt: Which of these oceans does the prime meridian intersect? \nThis image is a world map showing the continents and oceans. The continents labeled are North America, South America, Europe, Africa, Asia, Australia, and Antarctica. The oceans labeled are the Arctic Ocean, Pacific Ocean, Atlantic Ocean, Indian Ocean, and Southern Ocean. The map includes latitude and longitude lines.\nGPT-4V: \nThe prime meridian intersects the following oceans: - Atlantic Ocean - Southern Ocean - Arctic Ocean \nPrompt: \nCompare the average kinetic energies of the particles in each sample. The diagrams below show two pure  samples of gas in identical closed, rigid containers. Each colored ball represents one gas particle. Both samples  have the same number of particles. Which sample has the higher temperature? Sample A or sample B? \nThe image shows two jars with particles inside. \n\n- **Sample A** contains particles with:\n  - Mass of each particle: 32 u\n  - Average particle speed: 600 m/s\n\n- **Sample B** contains particles with:\n  - Mass of each particle: 32 u\n  - Average particle speed: 540 m/s\n\nThe particles in each jar are illustrated as small circles, and both samples have the same particle mass but different average speeds.\nGPT-4V: \nBecause the average particle speed in sample A is higher than in sample B (  $\\mathsf{600\\;m/s}$   vs.  $540\\;\\mathsf{m}/\\mathsf{s}.$  ), the particles in  sample A have a higher average kinetic energy. Therefore, sample A has a higher temperature than sample B. \nFigure 29: Results on answering science questions. GPT-4V can understand the question textually and visually, and gather necessary information to answer the question. Example images are from [ 85 ]. Check Section  4.3  for detailed discussions. "}
{"page": 45, "image_path": "doc_images/2309.17421v2_45.jpg", "ocr_text": "Sec. 4.3 Multimodal Knowledge and Commonsense\n\nScience and Knowledge\n\nPrompt:\nWhich of these states is farthest north?\n\nGPT-4V:\n\nOut of the three states shown in the map - Arizona, Oklahoma, and Delaware, Delaware is the farthest north.\n\nWhich of the organisms is the producer in this food web?\n\nred-tailed\nhawk\n\nGPT-4V:\n\nThe producers in this food web are the berries and flowers, grasses, and seeds. These are the organisms that\nuse energy from the sun to produce their own food through photosynthesis.\n\nFigure 30: Results on answering science questions. GPT-4V can understand the question textually\nand visually, and gather necessary information to answer the question. Example images are from\n[85]. Check Section 4.3 for detailed discussions.\n\n46\n", "vlm_text": "Sec. 4.3 Multimodal Knowledge and Commonsense \nScience and Knowledge \nPrompt: Which of these states is farthest north? \nThe image is a map of the United States highlighting four states: Arizona, Oklahoma, North Carolina, and Delaware. The caption discusses three of these states - Arizona, Oklahoma, and Delaware - and states that Delaware is the farthest north among them.\nPrompt: \nWhich of the organisms is the producer in this food web? \nThis image is a food web diagram showing the interactions between various organisms. It includes:\n\n- Grizzly bear\n- Red-tailed hawk\n- Chipmunk\n- Marmot\n- Grouse\n- Deer\n- Butterfly\n- Decomposers and detritivores\n- Berries and flowers\n- Grasses\n- Seeds\n\nArrows indicate the direction of energy flow, showing which organisms consume others.\nGPT-4V: \nThe producers in this food web are the berries and flowers, grasses, and seeds. These are the organisms that  use energy from the sun to produce their own food through photosynthesis. \nFigure 30: Results on answering science questions. GPT-4V can understand the question textually and visually, and gather necessary information to answer the question. Example images are from [ 85 ]. Check Section  4.3  for detailed discussions. "}
{"page": 46, "image_path": "doc_images/2309.17421v2_46.jpg", "ocr_text": "Sec. 4.3 Multimodal Knowledge and Commonsense\n\nScience and Knowledge\n\nPrompt:\nSuppose you are a teacher, please use the figure to explain the distinction between evaporation and\nevapotranspiration.\n\nRunoff\n\nInfiltration [\n\nGPT-4V:\nOkay class, today we are going to learn about the distinction between evaporation and evapotranspiration. Let's\ntake a look at this figure.\n\nFirst, let’s understand what evaporation is. Evaporation is the process by which water is changed from its liquid\nstate to its gaseous state. This usually takes place at the surface of a liquid, like the ocean or a lake, when the\nwater molecules gain enough energy to break away and form water vapor. So in this figure, we can see that the\nwater in the lake and the dam is evaporating and becoming water vapor.\n\nNow, let's take a look at evapotranspiration. This is a combination of two processes - evaporation and\ntranspiration. Transpiration is the process by which plants release water vapor through their leaves. So, in this\nfigure, we can see that the trees and plants are releasing water vapor into the air, which is the transpiration part\nof evapotranspiration. At the same time, the water in the soil and the surface is also evaporating. This\ncombination of evaporation and transpiration is what we call evapotranspiration.\n\nIn summary, evaporation is the process by which water is changed from its liquid state to its gaseous state, while\nevapotranspiration is the combination of evaporation and transpiration. Both these processes contribute to the\nwater cycle and are important for maintaining the balance of water in our environment.\n\nFigure 31: Results on answering science questions. When we use a more specific text prompt like\n“Suppose you are a teacher, please use the figure to explain X,” we observe that GPT-4V can generate\na short tutorial for explaining the subject. Check Section 4.3 for detailed discussions.\n\n47\n", "vlm_text": "Sec. 4.3 Multimodal Knowledge and Commonsense \nScience and Knowledge \nPrompt: \nSuppose you are a teacher, please use the figure to explain the distinction between evaporation and  evapotranspiration. \nThis image depicts the water cycle, showing processes such as precipitation, evaporation, evapotranspiration, and infiltration. It illustrates surface water, ground water, runoff, streamflow, and a dam. The water table is indicated below the ground level, while the sun and cloud formations indicate weather systems in action. Arrows are used to demonstrate the direction of each process.\nGPT-4V: \nOkay class, today we are going to learn about the distinction between evaporation and evapotranspiration. Let’s  take a look at this figure. \nFirst, let’s understand what evaporation is. Evaporation is the process by which water is changed from its liquid  state to its gaseous state. This usually takes place at the surface of a liquid, like the ocean or a lake, when the  water molecules gain enough energy to break away and form water vapor. So in this figure, we can see that the  water in the lake and the dam is evaporating and becoming water vapor. \nNow, let’s take a look at evapotranspiration. This is a combination of two processes - evaporation and  transpiration. Transpiration is the process by which plants release water vapor through their leaves. So, in this  figure, we can see that the trees and plants are releasing water vapor into the air, which is the transpiration part  of evapotranspiration. At the same time, the water in the soil and the surface is also evaporating. This  combination of evaporation and transpiration is what we call evapotranspiration. \nIn summary, evaporation is the process by which water is changed from its liquid state to its gaseous state, while  evapotranspiration is the combination of evaporation and transpiration. Both these processes contribute to the  water cycle and are important for maintaining the balance of water in our environment. \nFigure 31: Results on answering science questions. When we use a more specific text prompt like “Suppose you are a teacher, please use the figure to explain X,” we observe that GPT-4V can generate a short tutorial for explaining the subject. Check Section  4.3  for detailed discussions. "}
{"page": 47, "image_path": "doc_images/2309.17421v2_47.jpg", "ocr_text": "Sec. 4.3 Multimodal Knowledge and Commonsense\n\nMultimodal Commonsense\n\nPrompt:\nWhat is [person3] doing?\n\nGPT-4V:\n\n[person3] is carrying a plate of food, likely serving it to the customers at the table. It appears that they are working\nas a waiter or server in a restaurant.\n\nWhat are [person1] and [person2] doing?\n\nGPT-4V:\n\n[person1] and [person2] are walking down the aisle at a wedding ceremony.\n\nSuppose you are a detective, what can you infer from the visual clues in the image?\n\nGPT-4V:\n\nFrom the visual clues in the image, | can infer that:\n\n- The room belongs to someone who is likely a young adult or teenager, as evidenced by the casual clothing and\nthe desk with a computer.\n\n- The person is likely a student or someone who works from home, as evidenced by the desk and computer.\n\n- The person is likely fashion-conscious, as evidenced by the multiple jackets and shoes.\n\n- The room is likely located in a colder climate, as evidenced by the jackets and boots.\n\n- The person is likely not very tidy, as evidenced by the cluttered desk and the clothes hanging on the door.\n\n- The room is likely located in an older or more rustic building, as evidenced by the exposed pipes and the\nworn-looking walls.\n\nFigure 32: Results on multimodal commonsense reasoning. Example images are from [148, 52].\nCheck Section 4.3 for detailed discussions. 48\n", "vlm_text": "Multimodal Commonsense \nPrompt: What is [person3] doing? \nThe image shows a group of people sitting at a diner table. The table has various items on it, including a bottle of syrup, condiments like ketchup and mustard, glasses of orange juice, cups, and silverware. One person is pointing at another individual across the table. The setting appears to be a casual dining environment with booths and a counter in the background. A server is holding plates of food, including a breakfast dish and a stack of pancakes.\nGPT-4V: \n[person3] is carrying a plate of food, likely serving it to the customers at the table. It appears that they are working  as a waiter or server in a restaurant.  \nPrompt: \nWhat are [person1] and [person2] doing? \nThe image shows a group of people dressed in formal attire, likely at a wedding or formal event. There are floral decorations in the background, and it appears that the scene is outdoors. Two people at the front are linked arm-in-arm and smiling, with others following behind.\nGPT-4V: [person1] and [person2] are walking down the aisle at a wedding ceremony. \nPrompt: \nSuppose you are a detective, what can you infer from the visual clues in the image? \nThe image shows a room with a green door on which several jackets are hanging. Adjacent to the door is a wooden shelving unit filled with various items, such as clothing and toys. A futon or couch sits nearby. In the corner of the room, there's a computer desk with a monitor, keyboard, mouse, and a black office chair. A metallic duct runs along one side of the room, indicating some ventilation or heating system. The walls have a textured, warm-toned finish, adding a cozy feel to the room.\nGPT-4V: \nFrom the visual clues in the image, I can infer that: - The room belongs to someone who is likely a young adult or teenager, as evidenced by the casual clothing and  the desk with a computer.\n\n \n- The person is likely a student or someone who works from home, as evidenced by the desk and computer.\n\n - The person is likely fashion-conscious, as evidenced by the multiple jackets and shoes.\n\n - The room is likely located in a colder climate, as evidenced by the jackets and boots.\n\n - The person is likely not very tidy, as evidenced by the cluttered desk and the clothes hanging on the door.\n\n - The room is likely located in an older or more rustic building, as evidenced by the exposed pipes and the  worn-looking walls.  "}
{"page": 48, "image_path": "doc_images/2309.17421v2_48.jpg", "ocr_text": "4.4 Scene Text, Table, Chart, and Document Reasoning\n\nScene text recognition. Reading and understanding scene text in images is an important task in\nvision-language [1 18-120, 17]. In our experiments, we investigate GPT-4V’s ability to recognize\nscene text by utilizing the input prompt “What are all the scene text in the image?” Figure 33\nshows the example results. We observe GPT-4V accurately identifies scene text in various scenarios,\nincluding both handwritten and printed text. In Section 4.5, we present further results on multilingual\nscenarios.\n\nVisual math reasoning. In Figure 34, GPT-4V demonstrates its capability in solving visual math\nproblems. In our experiments, we observe GPT-4V is able to extract essential information from the\nimage. For instance, in Figure 34, GPT-4V correctly identifies the presence of a right triangle (or\northogonal triangle) and determines that AB is 4 units and BC is 3 units. In addition, we note that\nGPT-4V tends to present solutions in a well-structured manner, solving the problem step by step,\nthereby showcasing its ability to provide clear explanations.\n\nChart understanding and reasoning. We further study GPT-4V’s ability in chart understanding\nand reasoning. Figures 35-37 show the example results. In our preliminary explorations, GPT-4V\nexhibits the ability to provide detailed descriptions of charts. For example, in Figure 35, the model\ncorrectly explains the proposal process from the beginning to the end. In Figure 36, the model not\nonly understands the program in the given flow chat, but also translates the details to a python code.\nIn the bottom row of Figure 37, GPT-4V shows a clear understanding of both x- and y-axis, and\nexplains the key insight presented in the chart. Furthermore, in our experiments, we observe that\nGPT-4V can answer questions based on the chart. In the top row of Figure 37, GPT-4V correctly\ncalculates the average total fueling cost, excluding the Ford F150.\n\nTable understanding and reasoning. In Figure 38, we present our preliminary investigations into\ntable understanding and reasoning. Similar to the findings from chart experiments, GPT-4V shows\npromising results in understanding the details in the table, as well as in reasoning and accurately\nresponding to related questions.\n\nDocument understanding. Figure 39 shows the results on various types of documents, such as floor\nplan, poster, and exam paper. We observe GPT-4V demonstrates an understanding of the documents\nand provides reasonable responses. For instance, it accurately identifies the location of the bathroom\nfor the second bedroom in the floor plan. It also recognizes the Chinese dish “Hot dry noodles,” and\nassociates it with the city of Wuhan by following the scene text. Moreover, GPT-4V is capable of\nreading an exam paper. It accurately reconstructs the table in Markdown, and then fills in the table\nwith the correct answers. We present more explorations in its coding ability in Section 4.6.\n\nIn Figure 40, we explore a more challenging case by providing a technical report [126] with multiple\npages as input. In our limited exploration, GPT-4V exhibits impressive results. It correctly describes\nthe main idea and their proposed method by considering the context across multiple pages. However,\nit may occasionally miss some implementation details. Please note that the dataset should contain\n1196+665=1861 examples, and the extracted features should include Histograms of Oriented Gradi-\nents (HOG). Instead of prompting all pages to the model simultaneously, we believe that exploring\nmore advanced prompting techniques, such as thinking step-by-step or employing in-context few-shot\napproaches, could potentially enhance the model’s performance.\n\n49\n", "vlm_text": "4.4 Scene Text, Table, Chart, and Document Reasoning \nScene text recognition.  Reading and understanding scene text in images is an important task in vision-language [ 118 – 120 ,  17 ]. In our experiments, we investigate GPT-4V’s ability to recognize scene text by utilizing the input prompt “What are all the scene text in the image?” Figure  33 shows the example results. We observe GPT-4V accurately identifies scene text in various scenarios, including both handwritten and printed text. In Section  4.5 , we present further results on multilingual scenarios. \nVisual math reasoning.  In Figure  34 , GPT-4V demonstrates its capability in solving visual math problems. In our experiments, we observe GPT-4V is able to extract essential information from the image. For instance, in Figure  34 , GPT-4V correctly identifies the presence of a right triangle (or orthogonal triangle) and determines that AB is 4 units and BC is 3 units. In addition, we note that GPT-4V tends to present solutions in a well-structured manner, solving the problem step by step, thereby showcasing its ability to provide clear explanations. \nChart understanding and reasoning.  We further study GPT-4V’s ability in chart understanding and reasoning. Figures  35 - 37  show the example results. In our preliminary explorations, GPT-4V exhibits the ability to provide detailed descriptions of charts. For example, in Figure  35 , the model correctly explains the proposal process from the beginning to the end. In Figure  36 , the model not only understands the program in the given flow chat, but also translates the details to a python code. In the bottom row of Figure  37 , GPT-4V shows a clear understanding of both x- and y-axis, and explains the key insight presented in the chart. Furthermore, in our experiments, we observe that GPT-4V can answer questions based on the chart. In the top row of Figure  37 , GPT-4V correctly calculates the average total fueling cost, excluding the Ford F150. \nTable understanding and reasoning.  In Figure  38 , we present our preliminary investigations into table understanding and reasoning. Similar to the findings from chart experiments, GPT-4V shows promising results in understanding the details in the table, as well as in reasoning and accurately responding to related questions. \nDocument understanding.  Figure  39  shows the results on various types of documents, such as floor plan, poster, and exam paper. We observe GPT-4V demonstrates an understanding of the documents and provides reasonable responses. For instance, it accurately identifies the location of the bathroom for the second bedroom in the floor plan. It also recognizes the Chinese dish “Hot dry noodles,” and associates it with the city of Wuhan by following the scene text. Moreover, GPT-4V is capable of reading an exam paper. It accurately reconstructs the table in Markdown, and then fills in the table with the correct answers. We present more explorations in its coding ability in Section  4.6 . \nIn Figure  40 , we explore a more challenging case by providing a technical report [ 126 ] with multiple pages as input. In our limited exploration, GPT-4V exhibits impressive results. It correctly describes the main idea and their proposed method by considering the context across multiple pages. However, it may occasionally miss some implementation details. Please note that the dataset should contain  $1196{+}665{=}1861$   examples, and the extracted features should include Histograms of Oriented Gradi- ents (HOG). Instead of prompting all pages to the model simultaneously, we believe that exploring more advanced prompting techniques, such as thinking step-by-step or employing in-context few-shot approaches, could potentially enhance the model’s performance. "}
{"page": 49, "image_path": "doc_images/2309.17421v2_49.jpg", "ocr_text": "Sec. 4.4 Scene Text, Table, Chart, and Document Reasoning\n\nScene Text Recognition\n\nPrompt:\n\nWhat are all the scene text in the image?\n\nGPT-4V:\nThe scene text in the image is\n\"3,642,039,031,055\"\n\nWhat are all the scene text in the image?\n\nGPT-4V:\nThe only scene text in the image is \"The\nEarth without art is just 'eh\".\n\nWhat are all the scene text in the image?\n\nThere are three texts in the image:\n\n1) “COVID-19 TESTING”\n\n2) “PLEASE HAVE ID & INSURANCE\nCARD READY”\n\n3) “W @UW Medicine’ (this is the\nlogo/branding for the University of\nWashington Medicine\n\nPrompt:\n\nWhat are all the scene text in the image?\n\nGPT-4V:\n\nThe scene text in the image are:\n- (NVDA)\n\n- NVIDIA\n\n- 356.85\n\n- +51.47\n\n- +16.85%\n\nWhat are all the scene text in the image?\n\nGPT-4V:\nThe scene text in the image is “The Road\nto Hollywood: how some of us got here”\n\nWhat are all the scene text in the image?\n\nThe scene text in the image are:\n- Vacation\n\n- Beach\n\n- Smile\n\n- Summer\n\n- Watermelon\n\n- Flip flops\n\n- Lemonade\n\nPrompt:\n\nWhat are all the scene text in the image?\n\nThe scene text in the image are:\n- \"Royal\"\n- \"London\"\n\nWhat are all the scene text in the image?\n\nGPT-4V:\n\nThe scene text in the image are:\n- \"Westfield\"\n\n- \"westfield.com.au\"\n\nWhat are all the scene text in the image?\n\nGPT-4V:\n\nThe scene text in the image are:\n\n- Malayan Tapir\n\n- Tapirus indicus\n\n- LOCATION: Malay Peninsula and\nSumatra, near water\n\n- RELATIVES: There are only five species\nof tapirs. They are most closely related to\nhorses and rhinos.\n\n- VANISHING? The forest provides tapirs\nwith shelter, food, and water, but forests\nare rapidly disappearing in Malaysia and\nSumatra.\n\nFigure 33: Results on scene text recognition. GPT-4V can recognize scene text in many challenging\nscenarios. Check Section 4.4 for detailed discussions.\n\n50\n", "vlm_text": "Sec. 4.4 Scene Text, Table, Chart, and Document Reasoning \nScene Text Recognition \nPrompt: What are all the scene text in the image? \nThe image shows a digital display with the number 3,642,039,031,055 in orange. It appears to be part of a ticker or electronic sign.\nGPT-4V: The scene text in the image is  \"3,642,039,031,055\" .  \nPrompt: \nWhat are all the scene text in the image? \nThe image shows a view over a city at dusk or night from a bridge or viewpoint. On the railing in the foreground, there's graffiti that reads, \"The earth without art is just 'eh'.\"\nGPT-4V: The only scene text in the image is \"The  Earth without art is just 'eh'\".  \nPrompt: \nWhat are all the scene text in the image? \nThe image shows a COVID-19 testing sign that indicates it's a drive-through location. The sign advises people to have their ID and insurance card ready. There is snow on the ground, and the picture seems to be taken from inside a vehicle, showing the side mirror and part of the car door. In the background, there are some trees and a building.\nGPT-4V: \nThere are three texts in the image: \n1) “COVID-19 TESTING” 2) “PLEASE HAVE ID & INSURANCE  CARD READY” 3) “W   $@$  UW Medicine” (this is the  logo/branding for the University of  Washington Medicine  \nPrompt: What are all the scene text in the image? \nThe image shows stock information for NVIDIA (NVDA). The stock price is 356.85, with an increase of 51.47, which is a 16.85% rise.\nGPT-4V: The scene text in the image are:\n\n - (NVDA)\n\n - NVIDIA\n\n - 356.85\n\n -  $+51.47\n\n$  -  $+16.85\\%$    \nPrompt: What are all the scene text in the image? \nThe image shows a mosaic on the ground with the text \"The Road to Hollywood\" with the phrase \"how some of us got here\" written underneath.\nGPT-4V: The scene text in the image is “The Road  to Hollywood: how some of us got here” \nPrompt: \nWhat are all the scene text in the image? \nThe image shows a framed chalkboard with handwritten words and drawings related to summer. The words are:\n\n- vacation\n- BEACH\n- smile (with a sun drawing)\n- SUMMER\n- watermelon\n- FLIP FLOPS\n- LEMONADE\n\nThe overall theme is summery and playful.\nGPT-4V: \nThe scene text in the image are:\n\n - Vacation\n\n - Beach\n\n - Smile\n\n - Summer\n\n - Watermelon\n\n - Flip flops\n\n - Lemonade \nPrompt: What are all the scene text in the image? \nThe image shows the words \"Royal London\" written on a window. There is a logo above the text, and you can see some buildings in the background through the glass.\nGPT-4V: \nThe scene text in the image are:\n\n - \"Royal\"\n\n - \"London\"  \nPrompt: \nWhat are all the scene text in the image? \nThe image shows a sign for \"Westfield\" with the text \"westfield.com.au\" underneath. It appears to be on the exterior of a building.\nGPT-4V: \nThe scene text in the image are:\n\n - \"Westfield\"\n\n - \"westfield.com.au\"  \nPrompt: \nWhat are all the scene text in the image? \nThe image shows an informational sign about the Malayan Tapir (Tapirus indicus). The sign includes a picture of the animal and provides some details:\n\n- **Location**: Malay Peninsula and Sumatra, near water\n- **Relatives**: There are only five species of tapirs. They are most closely related to horses and rhinos.\n- **Vanishing?**: The text notes that forests provide tapirs with shelter, food, and water, but these forests are rapidly disappearing in Malaysia and Sumatra.\nGPT-4V: \nThe scene text in the image are:\n\n \n- Malayan Tapir\n\n - Tapirus indicus\n\n - LOCATION: Malay Peninsula and  Sumatra, near water - RELATIVES: There are only five species  of tapirs. They are most closely related to  horses and rhinos. - VANISHING? The forest provides tapirs  with shelter, food, and water, but forests  are rapidly disappearing in Malaysia and  Sumatra.  "}
{"page": 50, "image_path": "doc_images/2309.17421v2_50.jpg", "ocr_text": "Sec. 4.4 Scene Text, Table, Chart, and Document Reasoning\n\nVisual Math\n\nPrompt:\nWhat is x in the equation?\n\n(x+3)°=4\n\nGPT-4V:\n\nWithout any context or\ninformation about what the\nequation is representing, we\ncan only assume that \"x\" is a\nvariable or unknown value in\nthe equation.\n\nIn this equation, (x + 3) is\nbeing squared and is equal to\n4. To solve for x, we can take\nthe square root of both sides\nof the equation to get rid of the\nexponent on the left side. This\nwould give us:\n\nV((x + 3)?) = V4\n\nx+3=42\n\nFrom here, we can solve for x\nby subtracting 3 from both\nsides. This would give us:\n\nx=-1orx=-5\n\nSo, in this equation, x could be\neither -1 or -5.\n\nPrompt:\n\nPrompt:\n\nWhat is the value of d in the last equation? What is the value of x in the image?\n\nSolve the following equations:\n\n1) 8@+11=4@414\n2) 7d-4=11d-9\n\nGPT-4V:\n\n1) To solve the first equation, we need to\nisolate the x variable on one side of the\nequation and the constants on the other\nside. We can start by subtracting 4x from\nboth sides to get rid of the x term on the\nright side of the equation. This gives us:\n\n4x+11=14\n\nNext, we can subtract 11 from both sides\nto get rid of the constant term on the left\nside of the equation. This gives us:\n\n4x=3\n\nFinally, we can divide both sides by 4 to\nisolate the x variable on the left side of the\nequation. This gives us:\n\nx= 3/4\n2) To solve the second equation, we can\nstart by subtracting 7d from both sides to\nget rid of the d term on the left side of the\nequation. This gives us:\n4=4d-9\nNext, we can add 9 to both sides to get rid\nof the constant term on the right side of\nthe equation. This gives us:\n\n4d\nFinally, we can divide both sides by 4 to\nisolate the d variable on the right side of\nthe equation. This gives us:\n\nd=5/4\n\nSo, the value of d in the last equation is\n5/4.\n\nGPT-4V:\n\nThe value of x in the image can be\nfound using the Pythagorean\ntheorem, which states that in a right\ntriangle, the square of the\nhypotenuse (the longest side,\nopposite the right angle) is equal to\nthe sum of the squares of the other\ntwo sides.\n\nIn this case, the hypotenuse is the\nside AC, and the other two sides are\nAB and BC. We know that AB is 4\nand BC is 3, so we can plug these\nvalues into the equation:\n\na? + b? = c?\n\n3? + 4? =c?\n\n9+16=c?\n\n25 = c?\n\nTherefore, c (the length of the\nhypotenuse) is 5. Since x is the\n\nlength of the hypotenuse, x is equal\nto 5.\n\nFigure 34: Results on visual math reasoning. GPT-4V is able to comprehend and solve visual math\nproblems with a well-structured solution. Check Section 4.4 for detailed discussions.\n\n51\n", "vlm_text": "Sec. 4.4 Scene Text, Table, Chart, and Document Reasoning \nVisual Math \nPrompt: \nWhat is  $\\mathsf{x}$   in the equation? \nThe image contains a mathematical equation: \\((x + 3)^2 = 4\\).\nGPT-4V: \nWithout any context or  information about what the  equation is representing, we  can only assume that  $\"\\times\"$   is a  variable or unknown value in  the equation.  \nIn this equation,   $(\\mathsf{x}+\\mathsf{3})$   is  being squared and is equal to  4. To solve for  $\\mathsf{x}$  , we can take  the square root of both sides  of the equation to get rid of the  exponent on the left side. This  would give us: \n\n$$\n\\begin{array}{l}{{\\sqrt{((\\mathsf{x}+3)^{2})}=\\sqrt{4}}}\\\\ {{\\mathsf{x}+3}=\\pm2}\\end{array}\n$$\n \nFrom here, we can solve for  $\\mathsf{x}$  by subtracting 3 from both  sides. This would give us: \n $\\times=\\cdot1$   or  $\\mathsf{x}=-5$  \nSo, in this equation, x could be  either  $^{-1}$   or  $^{-5}$  .  \nPrompt: \nWhat is the value of d in the last equation? \n\n $\\mathbf{8x+11}=\\mathbf{4x+14}$   $\\mathbf{7}d-\\mathbf{4}=\\mathbf{11}d-\\mathbf{9}$  \n\nGPT-4V: \n1) To solve the first equation, we need to  isolate the  $\\mathsf{x}$   variable on one side of the  equation and the constants on the other  side. We can start by subtracting  $4\\times$   from  both sides to get rid of the  $\\mathsf{x}$   term on the  right side of the equation. This gives us: \n\n$$\n4\\mathsf{x}+11=14\n$$\n \nNext, we can subtract 11 from both sides  to get rid of the constant term on the left  side of the equation. This gives us: \n\n$$\n4\\times=3\n$$\n \nFinally, we can divide both sides by 4 to  isolate the  $\\mathsf{x}$   variable on the left side of the  equation. This gives us: \n\n$$\n\\times=3/4\n$$\n \n2) To solve the second equation, we can  start by subtracting 7d from both sides to  get rid of the d term on the left side of the  equation. This gives us: \n\n$$\n-4\\AA\\cdot9\n$$\n \nPrompt: \nWhat is the value of  $\\mathsf{x}$   in the image? \nThe image shows a right triangle with vertices labeled A, B, and C. The side AB is horizontal and has a length of 4 units. The side BC is vertical with a length of 3 units, and there is a right angle at B. The hypotenuse AC is labeled as \\( x \\). \n\nTo find the length of x, you can use the Pythagorean theorem:\n\n\\[\nx^2 = 4^2 + 3^2 \\implies x^2 = 16 + 9 \\implies x^2 = 25 \\implies x = 5\n\\]\n\nSo, \\( x \\) is 5 units.\nGPT-4V: \nThe value of  $\\mathsf{x}$   in the image can be  found using the Pythagorean  theorem, which states that in a right  triangle, the square of the  hypotenuse (the longest side,  opposite the right angle) is equal to  the sum of the squares of the other  two sides. \nIn this case, the hypotenuse is the  side AC, and the other two sides are  AB and BC. We know that AB is 4  and BC is 3, so we can plug these  values into the equation: \n\n$$\n{\\begin{array}{r l}&{\\mathsf{a}^{2}+\\mathsf{b}^{2}=\\mathsf{c}^{2}}\\\\ &{\\mathsf{3}^{2}+\\mathsf{4}^{2}=\\mathsf{c}^{2}}\\\\ &{\\mathsf{9}+\\mathsf{16}=\\mathsf{c}^{2}}\\\\ &{\\mathsf{25}=\\mathsf{c}^{2}}\\end{array}}\n$$\n \nTherefore, c (the length of the  hypotenuse) is 5. Since  $\\mathsf{x}$   is the  length of the hypotenuse,  $\\mathsf{x}$   is equal  to 5.  \nNext, we can add 9 to both sides to get rid  of the constant term on the right side of  the equation. This gives us: \n\n$$\n5=4{\\mathsf{d}}\n$$\n \nFinally, we can divide both sides by 4 to  isolate the d variable on the right side of  the equation. This gives us: \n\n$$\n{\\mathsf{d}}=5/4\n$$\n "}
{"page": 51, "image_path": "doc_images/2309.17421v2_51.jpg", "ocr_text": "Sec. 4.4 Scene Text, Table, Chart, and Document Reasoning\n\nFlow Chart Understanding\n\nPrompt:\nExplain the figure\n\nGPT-4V:\n\nThis figure represents a flow chart of a team’s proposal process. The process begins with a team member\nsuggesting a project in a meeting. The team then discusses whether it is a good idea or not. If the idea is not\nconsidered a good one, the team rejects it and the process ends.\n\nIf the team thinks it might be a good idea but needs more information, a team member conducts further research\nand refines the idea into a proposal. The team then reviews the proposal and provides feedback. If the proposal\nis not okay, then the team member revises it and the team reviews it again.\n\nIf the proposal is okay, the team sends it to a final decision-making process. If the proposal is approved, then the\nteam proceeds with the project. This is the end of the proposal process.\n\nFigure 35: Results on flow chart understanding. GPT-4V correctly describes the proposal process in\ndetails. Check Section 4.4 for detailed discussions.\n\n52\n", "vlm_text": "Flow Chart Understanding \nThis flowchart outlines the process of evaluating and developing a project idea within a team. Here's a breakdown:\n\n1. **Idea Suggestion**: A team member suggests a project.\n2. **Discussion**: The team discusses the idea in a meeting.\n3. **Evaluation**: Participants decide if it's a good idea. Three paths can follow:\n   - **No**: The team rejects the idea.\n   - **Yes**: A team member further defines the project.\n   - **Not Enough Information**: A team member conducts more research.\n4. **Proposal Development**: If approved, a team member writes a proposal.\n5. **Proposal Review**: The team reviews the proposal. Depending on the outcome:\n   - **No**: The team member revises the proposal.\n   - **Yes**: The team sends the proposal to the final decision-maker.\n6. **Conclusion**: The process ends after sending the proposal.\nGPT-4V: \nThis figure represents a flow chart of a team’s proposal process. The process begins with a team member  suggesting a project in a meeting. The team then discusses whether it is a good idea or not. If the idea is not  considered a good one, the team rejects it and the process ends. \nIf the team thinks it might be a good idea but needs more information, a team member conducts further research  and refines the idea into a proposal. The team then reviews the proposal and provides feedback. If the proposal  is not okay, then the team member revises it and the team reviews it again. \nIf the proposal is okay, the team sends it to a final decision-making process. If the proposal is approved, then the  team proceeds with the project. This is the end of the proposal process. \nFigure 35: Results on flow chart understanding. GPT-4V correctly describes the proposal process in details. Check Section  4.4  for detailed discussions. "}
{"page": 52, "image_path": "doc_images/2309.17421v2_52.jpg", "ocr_text": "Sec. 4.4 Scene Text, Table, Chart, and Document Reasoning\n\nFlow Chart Understanding\n\nPrompt:\nCan you translate the flowchart to a python code?\n\nGPT-4V:\n\nSure, here’s the Python code:\n\nnum1 = input(\"Enter first number: \") num2 = input(\"Enter second number: \") if num1 > num2:\nprint(num1) else: print(num2)\n\nExplanation:\n\n- The first two lines correspond to the “Input num1” and “Input num2” blocks in the flowchart, where the user is\nprompted to enter two numbers.\n\n- The next two lines correspond to the “num1 > num2?” block, where a conditional statement is used to check if\nthe first number is greater than the second number.\n\n- If the condition is true, the next line (“Display num1”) is executed, and the first number is printed to the screen.\n- If the condition is false, the else statement is executed, and the second number is printed to the screen\n(“Display num2”).\n\n- The code then stops, corresponding to the “Stop” block in the flowchart.\n\nFigure 36: Results on flow chart understanding. GPT-4V is able to translate the flow chart to a python\ncode. Check Section 4.4 for detailed discussions.\n\n53\n", "vlm_text": "Flow Chart Understanding \nPrompt: Can you translate the flowchart to a python code? \nThe image is a flowchart illustrating a simple program that compares two numbers, `num1` and `num2`. Below is a step-by-step explanation of the flowchart:\n\n1. **Start**: The process begins.\n2. **Input num1**: The program prompts for and receives the first number (`num1`).\n3. **Input num2**: The program prompts for and receives the second number (`num2`).\n4. **Decision (num1 > num2)**: The program checks if `num1` is greater than `num2`.\n   - If **True**: It proceeds to display `num1`.\n   - If **False**: It proceeds to display `num2`.\n5. **Display num1 or num2**: Based on the result of the comparison, the program displays the greater of the two numbers.\n6. **End**: The process stops. \n\nIn summary, the flowchart represents a simple decision-making process to determine which of the two input numbers is greater and displays that number.\nGPT-4V: Sure, here’s the Python code: \nnum1  $=$   input(\"Enter first number: \") num2  $=$   input(\"Enter second number: \") if num1  $>$   num2:  print(num1) else: print(num2) \nExplanation: \n- The first two lines correspond to the “Input num1” and “Input num2” blocks in the flowchart, where the user is  prompted to enter two numbers. - The next two lines correspond to the “num1  $>$   num2?” block, where a conditional statement is used to check if  the first number is greater than the second number. - If the condition is true, the next line (“Display num1”) is executed, and the first number is printed to the screen. - If the condition is false, the else statement is executed, and the second number is printed to the screen  (“Display num2”). - The code then stops, corresponding to the “Stop” block in the flowchart. \nFigure 36: Results on flow chart understanding. GPT-4V is able to translate the flow chart to a python code. Check Section  4.4  for detailed discussions. "}
{"page": 53, "image_path": "doc_images/2309.17421v2_53.jpg", "ocr_text": "Sec. 4.4 Scene Text, Table, Chart, and Document Reasoning\n\nChart Understanding and Reasoning\n\nPrompt:\nWhat is the average total fueling cost excluding the Ford F150?\n\n$130.96\n\nFord Toyota Honda\nF150 RAV4 Civic\n(Regular Cab)\n\nTotal fueling cost\n\nGPT-4V:\nTo find the average total fueling cost excluding the Ford F150, we need to add the total fueling costs of the\nToyota RAV4 and Honda Civic and divide by 2 (since there are 2 vehicles in the calculation).\n\n(82.56 + 70.55) + 2 = 76.55\n\nTherefore, the average total fueling cost excluding the Ford F150 is $76.55.\n\nExplain the figure.\n\nA paper’s impact on your career\n\nLots of\nimpact\n\nEffect on your career\n\ni\n\n‘Ok Pretty good Creative, original\nand good.\n\nPaper quality\n\nGPT-4V:\n\nThe figure is a graph that shows the relationship between the quality of a paper and its impact on a person's\ncareer. The x-axis represents the quality of the paper, which ranges from bad to creative and original. The\ny-axis represents the impact the paper has on a person's career, which ranges from nothing to lots of impact.\n\nThe graph shows that a bad paper has little to no impact on a person's career, while a creative and original\npaper has a significant impact. The impact of the paper on a person's career increases as the quality of the\npaper improves. This suggests that the quality of a person's work is an important factor in determining their\nsuccess in their career.\n\nFigure 37: GPT-4V shows promising results in understanding the details in the chart, as well as in\nreasoning and accurately responding to related questions. Check Section 4.4 for detailed discussions.\n\n54\n", "vlm_text": "Chart Understanding and Reasoning \nPrompt: \nWhat is the average total fueling cost excluding the Ford F150? \nThe image is a bar chart comparing the total fueling costs for three different vehicles: a Ford F150 (Regular Cab), a Toyota RAV4, and a Honda Civic. The costs are represented in purple bars, with the exact values displayed above each bar. The Ford F150 has the highest fueling cost at $130.96, followed by the Toyota RAV4 at $82.56 and the Honda Civic at $70.55. The title of the chart is \"Total fueling cost.\"\nTo find the average total fueling cost excluding the Ford F150, we need to add the total fueling costs of the  Toyota RAV4 and Honda Civic and divide by 2 (since there are 2 vehicles in the calculation). \n\n$$\n(82.56+70.55)\\div2=76.55\n$$\n \nTherefore, the average total fueling cost excluding the Ford F150 is  $\\Updownarrow76.55$  . \nPrompt: \nThe image is a graph that illustrates the relationship between the quality of a paper and its impact on one's career. The x-axis represents the quality of the paper, ranging from \"Bad\" to \"Creative, original, and good.\" The y-axis represents the effect of the paper on your career, ranging from \"nothing\" to \"Lots of impact.\"\n\nThe graph is a step function that remains flat and at the lowest level on the y-axis from \"Bad\" through \"Pretty good\" paper quality, indicating that papers of those qualities have little to no impact on one's career. When the paper quality reaches \"Creative, original, and good,\" the graph jumps sharply to the highest level on the y-axis, suggesting that such papers significantly impact one's career. This figure implies that only papers of the highest quality have a substantial career impact.\nGPT-4V: \nThe figure is a graph that shows the relationship between the quality of a paper and its impact on a person's  career. The  $\\mathsf{x}.$  -axis represents the quality of the paper, which ranges from bad to creative and original. The  y-axis represents the impact the paper has on a person's career, which ranges from nothing to lots of impact. \nThe graph shows that a bad paper has little to no impact on a person's career, while a creative and original  paper has a significant impact. The impact of the paper on a person's career increases as the quality of the  paper improves. This suggests that the quality of a person's work is an important factor in determining their  success in their career.  \nFigure 37: GPT-4V shows promising results in understanding the details in the chart, as well as in reasoning and accurately responding to related questions. Check Section  4.4  for detailed discussions. "}
{"page": 54, "image_path": "doc_images/2309.17421v2_54.jpg", "ocr_text": "Sec. 4.4 Scene Text, Table, Chart, and Document Reasoning\n\nTable Understanding and Reasoning\n\nPrompt:\nHow many families are earning more than 13000 and owns more than 2 cars?\n\nMonthly Vehicles per family\n\nLess than 7000\n7000 — 10000 305 2/\n\n10000 — 13000 535 (29)\n\n13000 — 16000 469 sf)\n\n16000 or more qi 579 82\n\nGPT-4V:\nFrom the given table, we can clearly see that there are a total of 113 families that earn more than Rs. 13000 and\nown more than 2 cars (25 families in the 13000-16000 range and 88 families in the 16000 or more range).\n\nWhich country has the highest profit margin?\n\nSales Volume a Profit | Profit Margin\n\n$15.971.880 $3.086.421 19,3%\nChina 35.070 $15.866.670 $3.032.162 19,1%\nAustralia 27.054 $14.812.566 $2.868.636 19,4%\nIndia 23.046 $10.608.174 $1.853.710 17,5%\nSouth Korea 16.032 $10.494.948 $1.975.844 18,8%\n\nTotal / Avg 141.282 $67.754.238 $12.816.772\n\nGPT-4V:\n\nBased on the data provided, the country with the highest profit margin is Australia, with a profit margin of\n19,4%. However, the difference between the top three countries (USA, China and Australia) is minimal, with\nless than 0,3% difference in profit margin.\n\nFigure 38: We observe GPT-4V can understand the details in the table, and answer related questions.\nCheck Section 4.4 for detailed discussions.\n\n55\n", "vlm_text": "Sec. 4.4 Scene Text, Table, Chart, and Document Reasoning \nTable Understanding and Reasoning \nPrompt: How many families are earning more than 13000 and owns more than 2 cars? \nThe table presents data on the distribution of vehicles per family across different monthly income brackets, measured in Indian Rupees (Rs.). The columns represent the number of vehicles per family, organized into four categories: 0 vehicles, 1 vehicle, 2 vehicles, and more than 2 vehicles. The rows correspond to five different income ranges:\n\n1. Less than 7000 Rs.\n   - 0 vehicles: 10 families\n   - 1 vehicle: 160 families\n   - 2 vehicles: 25 families\n   - Above 2 vehicles: 0 families\n\n2. 7000 – 10000 Rs.\n   - 0 vehicles: 0 families\n   - 1 vehicle: 305 families\n   - 2 vehicles: 27 families\n   - Above 2 vehicles: 2 families\n   \n3. 10000 – 13000 Rs.\n   - 0 vehicles: 1 family\n   - 1 vehicle: 535 families\n   - 2 vehicles: 29 families (highlighted in the table)\n   - Above 2 vehicles: 1 family\n\n4. 13000 – 16000 Rs.\n   - 0 vehicles: 2 families\n   - 1 vehicle: 469 families\n   - 2 vehicles: 59 families\n   - Above 2 vehicles: 25 families\n\n5. 16000 or more Rs.\n   - 0 vehicles: 1 family\n   - 1 vehicle: 579 families\n   - 2 vehicles: 82 families\n   - Above 2 vehicles: 88 families\n\nThis table illustrates how vehicle ownership correlates with household income, and it shows that as income levels increase, the likelihood of owning more vehicles per family also tends to increase.\nFrom the given table, we can clearly see that there are a total of 113 families that earn more than Rs. 13000 and  own more than 2 cars (25 families in the 13000-16000 range and 88 families in the 16000 or more range). \nThe table presents data on sales performance across different countries, including:\n\n- **Country**: The regions covered are USA, China, Australia, India, and South Korea.\n- **Sales Volume**: Numeric values indicating units sold, with the USA having the highest at 40.080.\n- **Revenue**: The monetary revenue for each country, with the USA leading at $15,971,880.\n- **Profit**: The profit figures for each country, with the USA having the highest profit at $3,086,421.\n- **Profit Margin**: Percentage of profit margin for each country, with Australia having the highest at 19.4%.\n\nThe totals and averages for all countries combined are also shown:\n- Total sales volume: 141.282\n- Total revenue: $67,754,238\n- Total profit: $12,816,772\n- Average profit margin: 18.8%\nGPT-4V: \nBased on the data provided, the country with the highest profit margin is Australia, with a profit margin of   $19{,}4\\%$  . However, the difference between the top three countries (USA, China and Australia) is minimal, with  less than  $0{,}3\\%$   difference in profit margin. \nFigure 38: We observe GPT-4V can understand the details in the table, and answer related questions. Check Section  4.4  for detailed discussions. "}
{"page": 55, "image_path": "doc_images/2309.17421v2_55.jpg", "ocr_text": "Sec. 4.4 Scene Text, Table, Chart, and Document Reasoning\n\nDocument Understanding\n\nPrompt:\nDescribe the image.\n\nTERRACE\n216\" x 80\" ——\n\nwm\n\nSECOND BEDROOM | MASTER BEDROOM\nDINING AREA [= sO10\"\" 187 120017\"\n910\" x 130\"\n\nLIVING ROOM\n14'6\"x 205\"\n\nGPT-4V:\n\nThis image is a floor plan for an apartment or\ncondominium. It shows the layout of the rooms and the\ndimensions. There is a terrace, dining area, living room,\nfoyer, two bedrooms, a master bathroom, and a guest\nbathroom. The floor plan is black and white and is a\ntwo-dimensional drawing.\n\nAnswer the question in the image.\n\n1, Figure 1 shows the structure of the earth’s interior, Write the number showing\n\neach layer in the table below.\n\\») 2\n\nZY\n\n——\n\nFigure 1\n\nLayer Number on figure 1\n\nOuter core\n\nMantle\n\nCrust\n\nInner core\n\nGPT-4V:\nLayer | Number on figure 1\n\n=|\n\nOuter core | 3\nMantle | 2\nCrust | 1\nInner core | 4\n\nVisualization of the generated results:\n\nLayer Number on figure 1\nOutercore 3\nMantle 2\nCrust 1\n\nInnercore 4\n\nPrompt:\n\nWhere is the bathroom for the second bedroom?\n\nNote: Same query\nimage as shown on\nleft, resized to\nsave space\n\nAccording to this floor plan, the bathroom for the\nsecond bedroom appears to be located directly\nadjacent to the bedroom. It is labeled \"WC\" and is\na small room measuring 5'2\" x 5'2\".\n\nDescribe the poster. Which city is this dish from?\n\nFaeSEs\nARF\nMAS - Peo\n\nRF Hot dry noodles) P+ AMRZ—, MAAC RAMA\nRe2—, WSK, 60M, M.S, OITA, GM, HR, AT\n- SMNMINAN, ROARITAN, MNNR, FA\n\n5, AUUMOSR, RAR, HOMER,\n\nGPT-4V:\n\nThe poster is an advertisement for a Chinese dish\ncalled \"Hot dry noodles.\" The poster has a red\nbackground with white Chinese characters at the\ntop that read \"Hot dry noodles, the taste of China.\"\nBelow the text is a photo of the dish, which\nconsists of noodles with meat and vegetables on\ntop. The dish is served in a green bowl with red\nchili peppers on the side. The text at the bottom of\nthe poster is in Chinese and describes the dish as\nbeing spicy, fragrant, and delicious.\n\nThis dish is from the city of Wuhan, located in the\nHubei Province of China. It is a popular street food\nin the city and is considered one of the \"10 great\nbreakfasts\" in China.\n\nFigure 39: Results on document understanding. GPT-4V recognizes three different types of document\nand answers the questions correctly. Check Section 4.4 for detailed discussions.\n\n56\n", "vlm_text": "Sec. 4.4 Scene Text, Table, Chart, and Document Reasoning \nDocument Understanding \n\nThe image is a floor plan for what appears to be a residential apartment. It includes the following labeled areas and dimensions:\n\n- **Terrace**: 21'6\" x 8'0\"\n- **Dining Area**: 9'10\" x 13'0\"\n- **Living Room**: 14'6\" x 20'5\"\n- **Second Bedroom**: 10'10\" x 14'7\"\n- **Master Bedroom**: 12'10\" x 17'9\"\n- **Foyer**: 6'0\" x 6'3 1/2\"\n- **WC** (Water Closet): 5'2\" x 5'2\"\n- **Bathroom**: Equipped with a tub.\n- **Kitchen**: Includes appliances such as a refrigerator (FRIG), oven, and dishwasher (DW). There's also a built-in unit and closet space labeled \"CL\".\n- **Entrance**: Marked at the bottom, near the Foyer.\n- *Miscellaneous*: A laundry chute (LC) near the bathroom and a closet space (CL). \n\nThe plan also includes directional information indicating north (N), and it is noted as being \"For illustration purposes only\" at the bottom.\nGPT-4V: \nThis image is a floor plan for an apartment or  condominium. It shows the layout of the rooms and the  dimensions. There is a terrace, dining area, living room,  foyer, two bedrooms, a master bathroom, and a guest  bathroom. The floor plan is black and white and is a  two-dimensional drawing. \nPrompt: \nAnswer the question in the image. \nThe image is a diagram showing the structure of the Earth's interior. It is divided into different layers with corresponding numbers. Below the diagram, there is a table that asks to match each layer name with the number shown on the diagram. The specified layers are:\n\n- Outer core\n- Mantle\n- Crust\n- Inner core\n\nThe numbers on the diagram corresponding to these layers are:\n\n- Crust is number 1\n- Mantle is number 2\n- Outer core is number 3\n- Inner core is number 4\nGPT-4V: \nLayer | Number on figure 1 --- | --- Outer core | 3 Mantle | 2 Crust | 1 Inner core | 4  \nVisualization of the generated results: \nThe table lists layers of the Earth with corresponding numbers:\n\n- Crust: 1\n- Mantle: 2\n- Outer core: 3\n- Inner core: 4\nPrompt: \nThe floor plan shows a layout of a living space. The second bedroom does not have an en-suite bathroom. The bathroom is located near the entrance, next to the master bedroom, and accessible from the hallway close to both bedrooms.\nGPT-4V: \nAccording to this floor plan, the bathroom for the  second bedroom appears to be located directly  adjacent to the bedroom. It is labeled \"WC\" and is  a small room measuring  $52\"\\times52\"$  . \nPrompt: Describe the poster. Which city is this dish from? \nThis image shows a bowl of hot dry noodles, known as \"Wuhan 热干面\" (Wuhan Re Gan Mian). The text is in Chinese and mentions that these noodles are a specialty snack, emphasizing their strong flavor and quality. It describes the dish as a well-known snack originating from Wuhan and outlines its characteristics and ingredients, including seasonings like sesame sauce and pickled vegetables.\nGPT-4V: \nThe poster is an advertisement for a Chinese dish  called \"Hot dry noodles.\" The poster has a red  background with white Chinese characters at the  top that read \"Hot dry noodles, the taste of China.\"  Below the text is a photo of the dish, which  consists of noodles with meat and vegetables on  top. The dish is served in a green bowl with red  chili peppers on the side. The text at the bottom of  the poster is in Chinese and describes the dish as  being spicy, fragrant, and delicious. \nThis dish is from the city of Wuhan, located in the  Hubei Province of China. It is a popular street food  in the city and is considered one of the  $\"10$   great  breakfasts\" in China.  \nFigure 39: Results on document understanding. GPT-4V recognizes three different types of document and answers the questions correctly. Check Section  4.4  for detailed discussions. "}
{"page": 56, "image_path": "doc_images/2309.17421v2_56.jpg", "ocr_text": "Sec. 4.4 Scene Text, Table, Chart, and Document Reasoning\n\nDocument Understanding\n\nPrompt: Prompt Continued GPT-4V:\n\nThis is a paper \"Paper Gestalt.\". Page 4 The paper \"Paper Gestalt\" is a research\nDescribe the paper in details paper written by Carven von\n\nand highlight their contribution. (== SB Be L™ =e Ga FS | Bearnensquash from the Department of\nComputer Science at the University of\nPage 1 Phoenix. The paper discusses the visual\nquality of a computer science research\npaper and how it affects the overall\nimpression of the paper.\n\nThe paper begins with an introduction to\nthe concept of paper gestalt, which is the\noverall impression of a paper. The author\nargues that the visual quality of a paper is\nan important factor in creating a good\nimpression. The paper then discusses\nprevious work in this area and highlights\nthe contribution of this paper, which is the\nuse of a supervised learning algorithm to\nevaluate the visual quality of a paper.\n\nThe paper then presents the methodology\nand experimental results. The author\ncollected a dataset of 1196 computer\nscience research papers and used a\nsupervised learning algorithm to evaluate\nthe visual quality of the papers. The\nalgorithm was trained on a set of features\nextracted from the papers, such asthe\n\nThe results showed that the algorithm was\nable to accurately predict the visual quality\nof a paper with a high degree of accuracy.\n\nThe paper concludes with a discussion of\nthe results and future work. The author\nsuggests that future work could focus on\nimproving the algorithm by incorporating\nmore features and training it on a larger\ndataset. The paper also suggests that the\nalgorithm could be used to evaluate the\nvisual quality of other types of documents,\nsuch as technical reports and\ndissertations.\n\nOverall, the contribution of this paper is the\nuse of a supervised learning algorithm to\nevaluate the visual quality of a computer\nscience research paper. This is a novel\napproach that could help researchers.\nimprove the visual quality of their papers\nand make a better impression on their\nlh, wa\n\nFigure 40: Results on document understanding. GPT-4V reads a multi-page technical report, under-\nstands the content in each section, and provides a summary of the contribution of this technical report.\n\n‘Red highlights the wrong answer. Check Section 4.4 for detailed discussions.\n\n57\n", "vlm_text": "Sec. 4.4 Scene Text, Table, Chart, and Document Reasoning \nDocument Understanding \nThe image appears to be a compilation of pages from a multi-page technical report titled \"Paper Gestalt.\" Each page contains various elements including text, graphs, diagrams, and charts. The prompt on the left side asks for a detailed description and the highlighting of contributions of the paper. The red text highlights indicate areas where the information provided by GPT-4V was incorrect, suggesting this is an example of a document understanding task involving error identification and correction."}
{"page": 57, "image_path": "doc_images/2309.17421v2_57.jpg", "ocr_text": "4.5 Multilingual Multimodal Understanding\n\nWe assess GPT-4V’s ability in comprehending multiple languages and modalities. First, we explore\nthis capability by evaluating natural images without scene text, as depicted in Figure 41. In the first\nrow of the figure, we provide the input text prompt “Describe the image” in Chinese, French, and\nCzech, respectively. GPT-4V recognizes the input text prompts in different languages, and generates\ncorrect image descriptions in corresponding languages. In the second row of Figure 41, we provide\nthe input text prompt in English and specify the output language. GPT-4V follows the instruction and\ngenerates correct descriptions in the desired languages. In the bottom row of Figure 41, we provide\nan input prompt in Spanish, and ask GPT-4V to generate image descriptions in 20 different languages.\nWe observe that GPT-4V can process both the input and output text in different languages.\n\nFurthermore, we explore a scenario involving multilingual scene text recognition, where the input\nimage may contain scene text in various languages. As shown in Figure 42, GPT-4V correctly\nidentifies and understands the scene text from different scenes. As shown in the first two rows\nof Figure 43, we observe that GPT-4V can recognize the scene text, and translate it to a different\nlanguage. In the bottom row of Figure 43, we provide a screenshot of a Wikipedia website which is\nin Catalan, and instruct GPT-4V to summarize the information in 20 different languages. GPT-4V\nnot only recognizes the text in Catalan but also generates precise summaries and translates them into\ndifferent languages. This showcases GPT-4V’s ability to comprehend and translate multilingual scene\ntext.\n\nWe also explore the capability of multicultural understanding [147, 77]. Figure 44 shows the example\nresults in such a scenario. We observe that GPT-4V is capable of understanding cultural nuances and\ngenerating reasonable multilingual descriptions for the wedding images given.\n\nIn our exploration, we found that GPT-4V seamlessly comprehends and correctly generates descrip-\ntions in different languages, highlighting its versatility in handling diverse linguistic contexts.\n\n58\n", "vlm_text": "4.5 Multilingual Multimodal Understanding \nWe assess GPT-4V’s ability in comprehending multiple languages and modalities. First, we explore this capability by evaluating natural images without scene text, as depicted in Figure  41 . In the first row of the figure, we provide the input text prompt “Describe the image” in Chinese, French, and Czech, respectively. GPT-4V recognizes the input text prompts in different languages, and generates correct image descriptions in corresponding languages. In the second row of Figure  41 , we provide the input text prompt in English and specify the output language. GPT-4V follows the instruction and generates correct descriptions in the desired languages. In the bottom row of Figure  41 , we provide an input prompt in Spanish, and ask GPT-4V to generate image descriptions in 20 different languages. We observe that GPT-4V can process both the input and output text in different languages. \nFurthermore, we explore a scenario involving multilingual scene text recognition, where the input image may contain scene text in various languages. As shown in Figure  42 , GPT-4V correctly identifies and understands the scene text from different scenes. As shown in the first two rows of Figure  43 , we observe that GPT-4V can recognize the scene text, and translate it to a different language. In the bottom row of Figure  43 , we provide a screenshot of a Wikipedia website which is in Catalan, and instruct GPT-4V to summarize the information in 20 different languages. GPT-4V not only recognizes the text in Catalan but also generates precise summaries and translates them into different languages. This showcases GPT-4V’s ability to comprehend and translate multilingual scene text. \nWe also explore the capability of multicultural understanding [ 147 ,  77 ]. Figure  44  shows the example results in such a scenario. We observe that GPT-4V is capable of understanding cultural nuances and generating reasonable multilingual descriptions for the wedding images given. \nIn our exploration, we found that GPT-4V seamlessly comprehends and correctly generates descrip- tions in different languages, highlighting its versatility in handling diverse linguistic contexts. "}
{"page": 58, "image_path": "doc_images/2309.17421v2_58.jpg", "ocr_text": "Sec. 4.5 Multilingual Multimodal Understanding\n\nMultilingual Image Description\n\nPrompt:\nfae e A\n\nGPT-4V:\n\nSRB 2 — Sate ht ER\nBG. MiehEN, FEE MERE,\nBARRA. CHAAR, BAK\n\nMD, TEE Be ah HE ht fT\n\nDescribe the image in Chinese\n\nKE-RIMIEA, CHERMLES, WE\nKF, SAAR, HERA DHF. EC\nHSte-AWB, LweA-TA AW\n\nDescribir la imagen en 20 idiomas diferentes\n\nPrompt:\nDécrire l'image\n\nGPT-4V:\n\nIl s’agit d’une photo d’un chaton roux\nmarchant sur I'herbe verte. Le chaton a les\nyeux bleus et semble étre en train de\nmarcher avec prudence. Il est entouré\nd/herbe et on peut voir une partie d’un\nautre chat en arriére-plan.\n\nDescribe the image in French\n\nGPT-4V:\n\nC'est une image d'un petit chien de race\nCorgi. Il est de couleur marron et blanc et\nporte un collier avec une médaille en\nforme d'os. Il est en train de marcher sur\nI'herbe verte avec la langue pendante.\n\nPrompt:\n\nPopiSte obrazek\n\nGPT-4V:\n\nNa obrazku je malé oranzové koté s\nmodryma o¢ima. Koté stoji na zelené\ntravé a sméfuje dopredu. Jeho télo je v\npohybu a vypada, Ze pravé bézi. Na\npozadi je vidét dalSi Cast travy a nejasné i\ndal8i kocka.\n\nDescribe the image in Czech\n\nGPT-4V:\n\nNa obrazku je maly hnédy pes s bilym\nlimcem a kostiékou na obojku. Pes je na\nzelené travé a ma otevrenou tlamu.\nVypada, Ze se bavi.\n\nGPT-4V output continued\n\n4. Aleman: Dies ist eine sch6ne Aussicht auf Machu Picchu in Peru.\n5. Italiano: Questa é una bella vista di Machu Picchu in Peru.\n\n6. Portugués: Esta é uma bela vista de Machu Picchu no Peru.\n\n7. Ruso: 3To kpacusblii Bug Ha Mayy-Mukyy B Mepy.\n\nGPT-4V:\n\n1. Inglés: This is a beautiful view of Machu Picchu in Peru.\n\n2. Espafol: Esta es una hermosa vista de Machu Picchu en Pert.\n3. Francés: C'est une belle vue de Machu Picchu au Pérou.\n\n8. Chino: X#WES RRM MRE.\n\n9. Japonés: CHIEN IL—OVFAEFADKLURETCT.\n\n10. Coreano: OIAS HSS) OFF D2) SOS SASUC.\n11. Hindi: ae 8&8 & arg rex ar Yer Eka 1\n\n12. Arabe: .sou 8 sity sild Jee jis Ihe\n\n13. Turco: Bu, Peru'daki Machu Picchu'nun gizel bir manzarasidir.\n14. Griego: Auth eivai pia 6yope@n BEa Tou Matoou Nitoou oto\nNepou.\n\n15. Hebreo: .n92 1'y'9 I'yxn 7w AD! Qn 1AT\n\n16. Sueco: Det har ar en vacker vy av Machu Picchu i Peru.\n\n17. Noruego: Dette er en vakker utsikt over Machu Picchu i Peru.\n18. Danés: Dette er en smuk udsigt over Machu Picchu i Peru.\n\n19. Holandés: Dit is een prachtig uitzicht op Machu Picchu in Peru.\n20. Polaco: To piekny widok na Machu Picchu w Peru.\n\nFigure 41: Results on multilingual image descriptions. GPT-4V is able to generate image descriptions\nin different languages. Check Section 4.5 for detailed discussions.\n\n59\n", "vlm_text": "Multilingual Image Description \nThe image shows a cute orange tabby kitten with blue eyes walking through the grass. The kitten is looking downward as it walks, and the focus of the photograph is on the adorable feline against a blurred background of greenery.\nThe image shows a small orange tabby kitten walking through grass. The kitten has a fluffy fur coat and blue eyes, and it appears to be young and curious, exploring its surroundings in a natural outdoor setting.\nThe image shows a cute ginger kitten walking on grass. The kitten has a soft orange coat with subtle stripes and blue eyes, and the background is a blurry green, suggesting it is outside in a grassy area.\n\nGPT-4V: \nGPT-4V: \n這張圖片是一隻小貓在草地上走路的畫面。小貓是橘色的，有藍色的眼睛，看起來很可愛。它的身體很小，四肢纖細，正在小心翼翼地在草地上行走。 \nIl s’agit d’une photo d’un chaton roux  marchant sur l’herbe verte. Le chaton a les  yeux bleus et semble être en train de  marcher avec prudence. Il est entouré  d’herbe et on peut voir une partie d’un  autre chat en arrière-plan.  \nNa obrázku je malé oranžové kotě s  modrýma očima. Kotě stojí na zelené  trávě a směřuje dopředu. Jeho tělo je v  pohybu a vypadá, že právě běží. Na  pozadí je vidět další část trávy a nejasně i  další kočka.  \nPrompt: \nPrompt: \nPrompt: \nDescribe the image in Chinese \nObrázek zobrazuje malé, nadšené štěně plemene corgi, které kráčí po trávě. Má hnědo-bílou srst a kolem krku obojek s přívěskem ve tvaru kosti. Štěně vypadá, že se raduje ze hry venku.\nDescribe the image in French \nThe image shows a small, brown and white puppy walking on grass. The puppy appears to be a Corgi, a breed known for its short legs and long body. It is wearing a collar with a bone-shaped tag. The puppy has its mouth open, possibly panting or in the middle of barking. The background is blurred, focusing attention on the puppy as it trots along the grass.\nThe image shows a small dog with a light brown and white coat, possibly a corgi, standing in a grassy area. The dog is wearing a collar with a bone-shaped tag and appears to be yawning or panting.\nGPT-4V: \nGPT-4V: \nGPT-4V: \n这是一只小柯基犬，它在草地上走着，嘴巴张开，舌头伸出来，好像很开心的样子。它的身上有一条项圈，上面挂着一个骨头形状的牌子。 \nNa obrázku je malý hnědý pes s bílým  límcem a kostičkou na obojku. Pes je na  zelené trávě a má otevřenou tlamu.  Vypadá, že se baví.  \nC'est une image d'un petit chien de race  Corgi. Il est de couleur marron et blanc et  porte un collier avec une médaille en  forme d'os. Il est en train de marcher sur  l'herbe verte avec la langue pendante. \nPrompt: \nGPT-4V output continued \nDescribir la imagen en 20 idiomas diferentes \n4. Alemán: Dies ist eine schöne Aussicht auf Machu Picchu in Peru.  5. Italiano: Questa è una bella vista di Machu Picchu in Perù.  6. Portugués: Esta é uma bela vista de Machu Picchu no Peru.  7. Ruso: Это красивый вид на Мачу-Пикчу в Перу.  8. Chino: 这是秘鲁马丘比丘的美丽景色。 9. Japonés: これはペルーのマチュピチュの美しい景色です。 10. Coreano: 이것은 페루의 마추 피추의 아름다운 경치입니다. 11. Hindi:  यह   पेǾ   क   माचू   ͪपÍचू   का   सुंदर   Ǻæय   है।   12. Árabe: .ﺑﯾرو  $\\scriptstyle{\\varphi^{\\vec{\\mathbf{a}}}}$  ھذا ﻣﻧظر ﺟﻣﯾل ﻟﻣﺎﺷو ﺑﯾﺗﺷو  13. Turco: Bu, Peru'daki Machu Picchu'nun güzel bir man zara s d r.  14. Griego: Αυτή είναι μια όμορφη θέα του Μάτσου Πίτσου στο  Περού.  15. Hebreo: .זהו נוף יפה של מאצ'ו פיצ'ו בפרו  16. Sueco: Det här är en vacker vy av Machu Picchu i Peru.  17. Noruego: Dette er en vakker utsikt over Machu Picchu i Peru.  18. Danés: Dette er en smuk udsigt over Machu Picchu i Peru.  19. Holandés: Dit is een prachtig uitzicht op Machu Picchu in Peru.  20. Polaco: To piękny widok na Machu Picchu w Peru. \n\n \nThe image shows Machu Picchu, an ancient Incan city located in the Andes Mountains of Peru. The picture showcases the well-preserved stone structures and terraces of the archaeological site, with a backdrop of lush green mountains and a clear blue sky. Machu Picchu is a UNESCO World Heritage Site and is renowned for its historical significance and architectural ingenuity.\nGPT-4V: \n1. Inglés: This is a beautiful view of Machu Picchu in Peru. \n\n 2. Español: Esta es una hermosa vista de Machu Picchu en Perú.\n\n 3. Francés: C'est une belle vue de Machu Picchu au Pérou.  "}
{"page": 59, "image_path": "doc_images/2309.17421v2_59.jpg", "ocr_text": "Sec. 4.5 Multilingual Multimodal Understanding\n\nMultilingual Scene Text Recognition\n\nPrompt:\n\nWhat are all the scene text in the image?\n\nZaz\n\nGPT-4V:\n\nThe scene text in the image is an Arabic\nphrase \"as! Gt! ail aw\" (Bismillah\nal-Rahman al-Rahim). It translates to \"In\nthe name of Allah, the Most Merciful, the\nMost Compassionate\". This phrase is\ncommonly used by Muslims at the\nbeginning of any action or speech as a way\nto seek blessings and guidance from God.\n\nWhat are all the scene text in the image?\n\nWELCOME\n\nbenvenuto\n\nGPT-4V:\n\nThe scene text in the image are:\n- WELCOME (English)\n\n- bienvenida (Spanish)\n\n- Bienvenue (French)\n\n- Willkommen (German)\n\n- benvenuto (Italian)\n\nWhat are all the scene text in the image?\n\nThe scene text in the image are:\n\nPrompt:\n\nWhat are all the scene text in the image?\n\nThe scene text in the image is \"MUSEE\nDU LOUVRE\" which is the name of the\nfamous art museum in Paris, France.\n\nWhat are all the scene text in the image?\n\nGPT-4V:\n\nThere are two scene texts in the image:\n1) \"SUPER NINTENDO WORLD\"\n2)\"A-WN-LYFYE—T—ILE \" (this is\nthe Japanese version of \"Super Nintendo\n\nPrompt:\n\nWhat are all the scene text in the image?\n\nGPT-4V:\n\nThe scene text in the image are:\n- \"3820\" (Chinese characters)\n- \"Tsim Sha Tsui\" (English text)\n\nWhat are all the scene text in the image?\n\nGP. FIM DA EUROPA.\nATLETISMO a\n\n«Antonio Agostinho | [i\n1987 *\n\nFernando Domingos\n1988 o\n\nGPT-4V:\n\nThe scene text in the image are:\n- G.P. FIM DA EUROPA\n\n- ATLETISMO\n\n- Antonio Agostinho 1987\n\n- Fernando Domingos 1988\n\nWhat are all the scene text in the image?\n\n- AOHNAI-POMH-PION IANETPON (Athens-Rome-Rion lapetron)\n\n- 29 MAIOY 1933 (29 May 1933)\n- APAX (Drachma)\n- EAAAZ (Greece)\n\n- XNAEP. TAXYAP. ZYTKOIN. ITAAIA-EAAAAOZ-TOYPKIA\n(Airmail. Express. Communication. Italy-Greece-Turkey)\n\n- 120 (Value of the stamp in drachmas)\n\nGPT-4V:\n\nThe scene text in the image is \"nao é normal ter medo de andar\nsozinho na rua!!\" which is in Portuguese and translates to \"It is not\n\nnormal to be afraid to walk alone on the street!!\"\n\nFigure 42: Results on multilingual scene text recognition. GPT-4V can recognize scene text in\ndifferent languages. Check Section 4.5 for detailed discussions.\n\n60\n", "vlm_text": "Multilingual Scene Text Recognition \nPrompt: \nPrompt: \nPrompt: \nThe text in the image says:\n\n失沙咀\nTsim Sha Tsui\nWhat are all the scene text in the image? \nWhat are all the scene text in the image? \nThe image shows a glass entrance with the text \"Musée du Louvre,\" indicating that it is the entrance to the Louvre Museum in Paris, France. The structure in the background is part of the Louvre Pyramid, which is a well-known architectural feature of the museum.\nThis image shows an Islamic calligraphy tile featuring the phrase \"Bismillah ir-Rahman ir-Rahim\" in Arabic, which translates to \"In the name of Allah, the Most Gracious, the Most Merciful.\" The tile is decorated with floral patterns commonly found in Islamic art.\nGPT-4V: \nGPT-4V: \nGPT-4V: \nThe scene text in the image is an Arabic  phrase \" اﻟرﺣﻣن اﻟرﺣﯾم $\\scriptstyle\\mathtt{\\sim}$  (Bismillah al-Rahman al-Rahim). It translates to \"In  the name of Allah, the Most Merciful, the  Most Compassionate\". This phrase is  commonly used by Muslims at the  beginning of any action or speech as a way  to seek blessings and guidance from God.   \nThe scene text in the image is \"MUSEE  DU LOUVRE\" which is the name of the  famous art museum in Paris, France.    \nThe scene text in the image are: - \" 尖沙咀 \" (Chinese characters) - \"Tsim Sha Tsui\" (English text)  \nPrompt: \nPrompt: \nPrompt: \nWhat are all the scene text in the image? \nWhat are all the scene text in the image? \nWhat are all the scene text in the image? \nThe image shows a signpost with the word \"Welcome\" in several languages, including Spanish (\"bienvenida\"), French (\"Bienvenue\"), German (\"Willkommen\"), and Italian (\"benvenuto\").\nThe image depicts the entrance to Super Nintendo World, with a large colorful sign stating \"SUPER NINTENDO WORLD\" above a tunnel entrance that resembles a warp pipe from the Mario video games. Two people are visible from the back, standing in front of the entrance, and decorative elements such as a wreath with a red bow are also present, suggesting a festive or holiday theme. Super Nintendo World is a themed area within Universal Studios theme parks, based on Nintendo's video game franchises.\nThe image shows a stone plaque with several smaller plaques attached to it. The text on the plaques is in Portuguese and reads:\n\n\"G.P. FIM DA EUROPA\nATLETISMO\n\nAntónio Agostinho\n1987\n\nFernando Domingos\n1988\"\n\nThis appears to commemorate athletic events or achievements.\nGPT-4V: \nGPT-4V: \nGPT-4V: \nThe scene text in the image are: - G.P. FIM DA EUROPA - ATLETISMO - António Agostinho 1987 - Fernando Domingos 1988 \nThere are two scene texts in the image: 1) \"SUPER NINTENDO WORLD\" 2) \"スーパーニンテンドーワールド\" (this is the Japanese version of \"Super Nintendo  World\")  \nThe scene text in the image are:\n\n - WELCOME (English)\n\n - bienvenida (Spanish)\n\n - Bienvenue (French)\n\n - Willkommen (German)\n\n - benvenuto (Italian)  \nPrompt: \nPrompt: \nWhat are all the scene text in the image? \nWhat are all the scene text in the image? \nThis is an image of a Greek postage stamp. It features an illustration of the Parthenon with a zeppelin flying above it. The text includes a date \"29 May 1933\" and refers to destinations Athens, Rome, and Istanbul. The denomination is \"120 drachmas.\"\nThe image shows an outdoor scene with a person walking on a bridge that has colorful glass panels. Below the bridge, there is a wall with graffiti that reads: \"não é normal ter medo de andar sozinha na rua!!!\" This translates to \"it's not normal to be afraid of walking alone on the street!!!\" in English. The message highlights a social issue regarding safety and fear when walking alone.\nGPT-4V: \nThe scene text in the image are:\n\n - ΑΘΗΝΑΙ-ΡΩΜΗ-ΡΙΟΝ ΙΑΠΕΤΡΟΝ (Athens-Rome-Rion Iapetron)\n\n - 29 ΜΑΪΟΥ 1933 (29 May 1933)\n\n - ΔΡΑΧ (Drachma)\n\n - ΕΛΛΑΣ (Greece)\n\n \nGPT-4V: \n- ΣΠΑΕΡ. ΤΑΧΥΔΡ. ΣΥΓΚΟΙΝ. ΙΤΑΛΙΑ-ΕΛΛΑΔΟΣ-ΤΟΥΡΚΙΑ \n\n (Airmail. Express. Communication. Italy-Greece-Turkey)\n\n - 120 (Value of the stamp in drachmas) \nThe scene text in the image is \"não é normal ter medo de andar  sozinho na rua!!\" which is in Portuguese and translates to \"It is not  normal to be afraid to walk alone on the street!!\"  "}
{"page": 60, "image_path": "doc_images/2309.17421v2_60.jpg", "ocr_text": "Sec. 4.5 Multilingual Multimodal Understanding\n\nMultilingual Text Recognition, Translation, and Description\n\nPrompt:\n\nIf the image is presented in a language\nother than English, translate the content\nto English.\n\nThe text in the image is in Japanese and\ntranslates to \"Good morning, have a nice\nday.\"\n\nPrompt:\n\nIf the image is presented in a language\nother than English, translate the content\nto English.\n\nGPT-4V:\n\nTranslation: Angelus 1st Grand Cru\nClassé Chateau Angelus 1996\nSaint-Emilion Grand Cru Appellation\nSaint-Emilion Grand Cru Controlled\n\nSummarize the info in 20 languages\n\nAlps Neozelandesos\n\n1. English: The Southern Alps are a mountain range in\n\nNew Zealand.\n2. Spanish: Los Alpes del Sur son una cadena\nmontafiosa en Nueva Zelanda.\n\n3. French: Les Alpes du Sud sont une chaine de\n\nmontagnes en Nouvelle-Zélande.\n\nPrompt:\n\nIf the image is presented in a language\nother than English, translate the content\nto English.\n\nGPT-4V:\n\nThe text in the image is in Chinese and\ntranslates to \"Mapo Tofu.\" Mapo tofu is a\npopular Chinese dish made with tofu and\nminced meat in a spicy sauce.\n\nIf the image is presented in a language\nother than English, translate the content\nto English.\n\nKuvia__[muokkaa | muokkaa wikiteksta ]\n\n‘Stegosauruksen aivot sijaitsivat sen\npienessa paassa\n\nGPT-4V:\n\nEnglish translation: \"Pictures [edit | edit\nwikitext] Stegosaurus's brain was located\nin its small head.\"\n\nGPT-4V output continued\n\nPrompt:\n\nIf the image is presented in a language other\nthan English, translate the content to\nEnglish.\n\nGPT-4V:\nThe text in the image is in Korean and\ntranslates to \"Delicious food\".\n\nIf the image is presented in a language other\nthan English, translate the content to\nEnglish.\n\nFaga duplo toque para\nadicionar um titulo\n\nGPT-4V:\n\nThe content in the image is in Portuguese\nand translates to \"Double tap to add a title.\nDouble tap to add a subtitle.”\n\n. German: Die Siidalpen sind ein Gebirgszug in Neuseeland.\n. Italian: Le Alpi meridionali sono una catena montuosa in Nuova Zelanda.\n\n. Russian: KO>kHbIe Anbnbl - ropHbii xpe6eT B Hopoii SenaHaun.\n\n4\n5\n6. Portuguese: Os Alpes do Sul sao uma cordilheira na Nova Zelandia.\n7\n8\n\n. Chinese: FABRE ATLL bk 237 = A ALK.\n9. Japanese: MV JLFAlk=a—V—SYE OWA CF o\n10. Korean: 8 SHAS FAAS AWLIC.\n\n11. Hindi: af@ot vere aypites ar vH ude swat zl\n\n12. Bengali: “fis siren froRrenes 28 aie ae oTT!\n\n13. Arabic: tails 5935. Due Abad go Ay sill IYI,\n\n14. Turkish: Giney Alpler Yeni Zelanda'da bir dag silsilesidir.\n\n15. Indonesian: Alpen Selatan adalah sebuah pegunungan di Selandia Baru.\n\n16. Vietnamese: Day nui Alpes phia Nam la mét day nui @ New Zealand.\n\n17. Thai: Wantauaallmalsiuianaluiraduaud\n18. Malay: Alpen Selatan adalah satu rangkaian gunung di New Zealand.\n19. Tagalog: Ang Timog Alps ay isang hanay ng mga bundok sa New\n\nZealand.\n\n20. Swahili: Milima ya Alpi Kusini ni safu ya milima nchini New Zealand.\n\nFigure 43: Results on multilingual text recognition, translation, and description. GPT-4V is able to\nrecognize, translate and generate descriptions in different languages. Check Section 4.5 for detailed\n\ndiscussions.\n\n61\n", "vlm_text": "Multilingual Text Recognition, Translation, and Description \nPrompt: \nIf the image is presented in a language  other than English, translate the content  to English. \nThe image contains Japanese text and a drawing of a smiling sun. The text says \"おはようございます,\" which means \"Good morning\" in English.\nGPT-4V: \nThe text in the image is in Japanese and  translates to \"Good morning, have a nice  day.\"  \nPrompt: \nIf the image is presented in a language  other than English, translate the content  to English. \nThe image shows a wine label for Château Angélus, a 1er Grand Cru Classé wine from Saint-Émilion. It indicates the vintage year as 1996. The label features a prominent bell design and includes the text \"Appellation St-Emilion Grand Cru Contrôlée.\"\nGPT-4V: \nTranslation: Angelus 1st Grand Cru  Classé Chateau Angelus 1996  Saint-Emilion Grand Cru Appellation  Saint-Emilion Grand Cru Controlled \nPrompt: \nIf the image is presented in a language  other than English, translate the content  to English. \nThis image shows a dish of mapo tofu. It is a popular Chinese dish made with tofu set in a spicy sauce, typically containing minced meat. The text on the image reads \"麻婆豆腐\" which translates to \"mapo tofu.\"\nGPT-4V: \nThe text in the image is in Chinese and  translates to \"Mapo Tofu.\" Mapo tofu is a  popular Chinese dish made with tofu and  minced meat in a spicy sauce.   \nPrompt: \nIf the image is presented in a language  other than English, translate the content  to English. \nThe image shows a fossil or cast of a dinosaur, specifically the head and part of the neck. The caption is in Finnish and reads \"Stegosauruksen aivot sijaitsivat sen pienessä päässä,\" which translates to \"The stegosaurus's brain was located in its small head.\" The text suggests the image might be highlighting the skull of a stegosaurus.\nGPT-4V: \nEnglish translation: \"Pictures [edit | edit  wikitext] Stegosaurus's brain was located  in its small head.\"  \nPrompt: \nIf the image is presented in a language other  than English, translate the content to  English. \nThe image contains Korean text that appears to read \"존맛탱\" (jonmattaeng). This is a slang term in Korean that is an abbreviation, used to express that something is extremely delicious. The term combines \"존맛\" (jonmat), which is short for \"존나 맛있다\" (jonna masitda, meaning \"really delicious\"), and \"탱\" (taeng), an emphatic suffix. The background of the image seems to be blurred, and there is a glimpse of what looks like food on the right side of the image.\nGPT-4V: \nThe text in the image is in Korean and  translates to \"Delicious food\".  \nPrompt: \nIf the image is presented in a language other  than English, translate the content to  English. \nThe image shows a PowerPoint slide in editing mode. The slide contains placeholder text in Portuguese: \"Faça duplo toque para adicionar um título\" and \"Faça duplo toque para adicionar um subtítulo,\" which means \"Double tap to add a title\" and \"Double tap to add a subtitle.\" The interface around the slide is typical of Microsoft PowerPoint, with toolbars and slide navigation on the side.\nGPT-4V: \nThe content in the image is in Portuguese  and translates to \"Double tap to add a title.  Double tap to add a subtitle.\"  \nPrompt: \nSummarize the info in 20 languages \nThis image appears to be a screenshot of a Wikipedia page in Catalan about the \"Alps Neozelandesos\" or the Southern Alps of New Zealand. It includes a brief description and information about the mountains and their heights. There is also an infobox on the right with an image of the mountains, geographic details, and administrative locations.\nGPT-4V: \n1. English: The Southern Alps are a mountain range in  New Zealand.  2. Spanish: Los Alpes del Sur son una cadena  montañosa en Nueva Zelanda.  3. French: Les Alpes du Sud sont une chaîne de  montagnes en Nouvelle-Zélande.  \nGPT-4V output continued \n4. German: Die Südalpen sind ein Gebirgszug in Neuseeland. \n\n 5. Italian: Le Alpi meri dion ali sono una catena montuosa in Nuova Zelanda. \n\n 6. Portuguese: Os Alpes do Sul são uma cord il heir a na Nova Zelândia. \n\n 7. Russian: Южные Альпы - горный хребет в Новой Зеландии. \n\n 8. Chinese: 南阿尔卑斯山脉是新西兰的一条山脉。 \n\n9. Japanese: 南アルプスはニュージーランドの山脈です。 \n\n10. Korean: 남부 알프스는 뉴질랜드의 산맥입니다. \n\n11. Hindi:  दͯक्षिणी एल्प्स न्यूजीलैंड का एक पवर्वत श्रृंखला है। \n\n 12.  Bengali:  দিক্ষিণ   অ্যালপস   িনউিজল্যােন্ডের   এই   অিধক   পবর্বতমালা ।  \n\n 13. Arabic: ﻧﯾوزﯾﻠﻧدا  $\\varsigma^{\\frac{\\dot{\\mathsf{a}}}{\\varepsilon}}$  اﻷﻟب اﻟﺟﻧوﺑﯾﺔ ھﻲ ﺳﻠﺳﻠﺔ ﺟﺑﺎل. \n\n 14. Turkish: Güney Alpler Yeni Zelanda'da bir dağ sil si les i dir. \n\n 15. Indonesian: Alpen Selatan adalah sebuah pegunungan di Selandia Baru. \n\n 16. Vietnamese: Dãy núi Alpes phía Nam là một dãy núi  $\\grave{\\circ}$   New Zealand. \n\n 17. Thai:  เทือกเขาแอลปภาคใตเปนเทือกเขาในนิวซีแลนด  \n\n 18.  Malay: Alpen Selatan adalah satu rangkaian gunung di New Zealand. \n\n 19. Tagalog: Ang Timog Alps ay isang hanay ng mga bundok sa New  Zealand.  20. Swahili: Milima ya Alpi Kusini ni safu ya milima nchini New Zealand.  \nFigure 43: Results on multilingual text recognition, translation, and description. GPT-4V is able to recognize, translate and generate descriptions in different languages. Check Section  4.5  for detailed discussions. "}
{"page": 61, "image_path": "doc_images/2309.17421v2_61.jpg", "ocr_text": "Sec. 4.5 Multilingual Multimodal Understanding\n\nMultilingual Multicultural Understanding\n\nPrompt:\n\nDescribe the image in both English and the local\nlanguage corresponding to the geolocation depicted\nin the image.\n\n| Bae\nGPT-4V:\n\nEnglish: This image shows a couple dressed in\ntraditional Chinese wedding attire, sitting in front of a\nwall of red roses and a gold Chinese character for\n“double happiness.\"\n\nChinese (Simplified): kA HR” T — 3376 + BS\nSUSTLIRRH AE, MET AKRNEE ME RFA\n. (Zhé zhang tupian zhanshile yi dui chuanzhe\nzhdnggu6 chuantoéng hianli fuzhuang de fifu, zud zai\nhdongsé méigut hé jinsé \"shuang xi\" hanzi qianmian.)\n\nDescribe the image in both English and the local\nlanguage corresponding to the geolocation depicted\nin the image.\n\nEnglish: This image shows a Japanese couple in\ntraditional wedding attire. The man is wearing a black\nkimono with a white hakama and the woman is\nwearing a white kimono with a large headpiece.\n\nJapanese: COMMIS MH CHA OKRBEGKA\nAOAYANERLWET. HELA BER GD\n2 CEY, KH ASCMY CA BME CHE\nJ. (Kono gaz6 wa dent6-teki na kekkon-shiki no\nishd o kita Nihon no kappuru o shimeshiteimasu.\nDansei wa shiroi hakama to kuroi kimono o kite ori,\njosei wa Okina kazari to shiroi kimono o kiteimasu.)\n\nFigure 44: Results on multilingual multiculture understanding. Check Section 4.5 for detailed\n\ndiscussions.\n\n62\n\nPrompt:\n\nDescribe the image in both English and the local\nlanguage corresponding to the geolocation depicted\nin the image.\n\nGPT-4V:\n\nEnglish: The image shows a couple wearing\ntraditional Ethiopian clothing, standing in a garden\nwith a tree in the background. The woman is wearing\na white dress with colorful patterns and the man is\nwearing a white suit with similar patterns.\n\nAmharic (Ethiopia's official language): 2U PAA\nNATERIER VUA ANN PANT MARTE NAR? NIN\nQFLAH MEEK MIRE SA AAIATUC ANA ANAT AT\nDOIEERP NDAAU AANAIUC Mr ANAAT:\n\nDescribe the image in both English and the local\nlanguage corresponding to the geolocation depicted\nin the image.\n\nGPT-4V:\n\nEnglish: This image shows a newlywed Indian couple\nsharing a kiss. The bride is wearing a traditional red\nsari and the groom is wearing a red turban and white\nsuit. They are sitting on a decorated stage with\nflowers.\n\nHindi (local language in India): 4g ofa va\naafaantea secret\n\nais ar wp dat Me ara gu fear gl Geet WH\nNTT Ae AST Tea el S SI eel UH et\naarst se athe Ye eet LET Bl SF Het S Bot Br\nwh Ha Ww ad 1\n\n", "vlm_text": "Sec. 4.5 Multilingual Multimodal Understanding \nMultilingual Multicultural Understanding \nPrompt: \nDescribe the image in both English and the local  language corresponding to the geo location depicted  in the image. \nThe image shows a couple dressed in traditional Chinese wedding attire. They are seated in front of a backdrop decorated with large red roses and a golden \"double happiness\" symbol, which is commonly used in Chinese weddings to symbolize joy and marital happiness. The clothing and decorations indicate that this is likely a wedding or pre-wedding photo session.\nGPT-4V: \nEnglish: This image shows a couple dressed in  traditional Chinese wedding attire, sitting in front of a  wall of red roses and a gold Chinese character for  \"double happiness.\" \nChinese (Simplified):  这张图 片展示了一 对 穿着中国 传 统婚礼服装的夫妇，坐在红色玫瑰和金色“双喜”汉字前面。  (Zhè zhāng túpiàn zhǎnshìle yī duì chuānzhe  zhōngguó chuántǒng hūnlǐ fúzhuāng de fūfù, zuò zài  hóngsè méiguī hé jīnsè \"shuāng xǐ\" hànzì qiánmiàn.)  \nPrompt: \nDescribe the image in both English and the local  language corresponding to the geo location depicted  in the image. \nThe image shows a couple in traditional Japanese wedding attire. The man is wearing a black kimono with a family crest and striped hakama (trousers), while the woman is dressed in a white shiromuku (wedding kimono) with a wataboshi (bridal headdress). They are standing in a setting that appears to be a traditional or historical location.\nGPT-4V: \nEnglish: This image shows a Japanese couple in  traditional wedding attire. The man is wearing a black  kimono with a white hakama and the woman is  wearing a white kimono with a large headpiece. \nJapanese: この画像は伝統的な結婚式の衣装を着た日本のカップルを示しています。男性は白い袴と黒い着物を着ており、女性は大きな頭飾りと白い着物を着ています。  (Kono gazō wa dentō-teki na kekkon-shiki no  ishō o kita Nihon no kappuru o s hime shite i masu.  Dansei wa shiroi hakama to kuroi kimono o kite ori,  josei wa ōkina kazari to shiroi kimono o kiteimasu.)  \nPrompt: \nDescribe the image in both English and the local  language corresponding to the geo location depicted  in the image. \nThe image shows two people wearing traditional Ethiopian attire. The clothing features intricate patterns with a mix of colors, including blue, white, and gold. The woman is wearing a long dress with an embroidered design, and the man is wearing a matching outfit with a similar pattern. They are standing outdoors with greenery in the background.\nGPT-4V: \nEnglish: The image shows a couple wearing  traditional Ethiopian clothing, standing in a garden  with a tree in the background. The woman is wearing  a white dress with colorful patterns and the man is  wearing a white suit with similar patterns. \nAmharic (Ethiopia's official language):  ይህ   ምስል   በኢትዮጵያዊ   ባህል   ልብስ   የለበሱት   ባለቤትና   ባለቤትን   በገባር   ይታያል።   ሴትዮቱ   በነጻነት   ያሉ   አለባለባህር   ልብሶች   ለበሳለ ች   እና ወንድዮቱም   በመሳሳህ   አለባለባህር   ሱት   ለበሳለ ች።   \nPrompt: \nDescribe the image in both English and the local  language corresponding to the geo location depicted  in the image. \nThe image shows a couple dressed in traditional Indian wedding attire. The individual on the left is wearing a red bridal saree with gold embellishments and traditional jewelry, including bangles and necklaces. The individual on the right is dressed in a white sherwani with gold detailing, and both are adorned with floral garlands. They are seated together on a decorated platform, suggesting a wedding ceremony setting, with floral arrangements in the background.\nGPT-4V: \nEnglish: This image shows a newlywed Indian couple  sharing a kiss. The bride is wearing a traditional red  sari and the groom is wearing a red turban and white  suit. They are sitting on a decorated stage with  flowers. \nHindi (local language in India):  यह छͪव एक  नवͪववाǑहत भारतीय  जोड़े को एक चुंबन शेयर करते हुए Ǒदखाती है। दुल्हन एक  परंपरागत लाल साड़ी पहन रही है और दूल्हा एक लाल  पगड़ी और सफ ेद सूट पहन रहा है। वे फ ूलों से सजे हुए  एक मंच पर बैठे हैं।  \nFigure 44: Results on multilingual multi culture understanding. Check Section  4.5  for detailed discussions. "}
{"page": 62, "image_path": "doc_images/2309.17421v2_62.jpg", "ocr_text": "4.6 Coding Capability with Vision\n\nSec. 4.6 Coding Capability with Vision\n\nGPT-4V:\nN\n\nb\nGe 40) =ex5 Dix i+) tu | fewde\n\ni=1\n\nFigure 45: GPT-4V’s capability to generate LaTex codes based on the hand-written input. The\ninstruction is “generate latex code.’ for each case. The output is the LaTeX code and we show the\nrendered result. Although the model fails to write the code for the complex equation (bottom), we\ncan break it down into several simple equations, which GPT-4V is able to handle. Check Section 4.6\nfor detailed discussions.\n\nFigure 45 illustrates the ability to generate LaTeX code based on handwritten mathematical equations.\nThis functionality can assist users in writing equations in LaTeX more efficiently. Although the\nmodel is unable to generate code for longer equations, it can handle shorter equations effectively. By\nbreaking down longer equations into shorter components, the model is able to generate the appropriate\ncode. Figure 46 further demonstrates how GPT-4V can reconstruct a table in the input image into\nMarkDown/LaTex code.\n\nFigure 47 shows examples of writing code in Python, TikZ, and SVG to replicate the input figure.\nAlthough the resulting output is not an exact match, the layout is similar and the code can be easily\nmodified to meet specific needs.\n\n63\n", "vlm_text": "The image showcases the capability of GPT-4V (a variant of GPT-4 with vision capabilities) to generate LaTeX code from handwritten mathematical equations. There are several examples depicted in the image:\n\n1. **First Row:**\n   - **Left:** The handwritten input is \\(a^2 + b^2 = c^2\\). GPT-4V converts this to LaTeX, which is rendered as \\(a^2 + b^2 = c^2\\).\n   - **Center:** The handwritten input is \\(\\sum_{i=0}^{N}(i^2 + i + 1)^{0.5}\\). GPT-4V renders this as \\(\\sum_{i=0}^{N}(i^2 + i + 1)^{0.5}\\).\n   - **Right:** The handwritten input is an integral \\(y = \\int f(x, y) \\, dx\\). GPT-4V renders this as \\(y = \\int f(x, y) \\, dx\\).\n\n2. **Second Row:**\n   - A complex handwritten equation is shown: \n     \\[\n     a^2 + b^2 = c^2 + \\sum_{i=1}^{N}(i^2 + i + 1)^{0.5} + y = \\int f(x, y) \\, dx\n     \\]\n   - GPT-4V generates a more simplified LaTeX code that does not exactly match the handwritten input:\n     \\[\n     \\left( x \\neq 6 \\right) = c \\times 5 \\sum_{i=1}^{N} \\left( i \\times (i + 1) \\right) + y_s \\int_{a}^{b} f(x, y) \\, dx\n     \\]\n\nThe image illustrates that while GPT-4V successfully translates simpler equations, it struggles with more complex combinations and equations, as noted in the caption, detailed in Section 4.6 of the document.\nFigure  45  illustrates the ability to generate LaTeX code based on handwritten mathematical equations. This functionality can assist users in writing equations in LaTeX more efficiently. Although the model is unable to generate code for longer equations, it can handle shorter equations effectively. By breaking down longer equations into shorter components, the model is able to generate the appropriate code. Figure  46  further demonstrates how GPT-4V can reconstruct a table in the input image into MarkDown/LaTex code. \nFigure  47  shows examples of writing code in Python, TikZ, and SVG to replicate the input figure. Although the resulting output is not an exact match, the layout is similar and the code can be easily modified to meet specific needs. "}
{"page": 63, "image_path": "doc_images/2309.17421v2_63.jpg", "ocr_text": "Sec. 4.6 Coding Capability with Vision\n\nPrompt:\nPlease read the table in this image and return a markdown-style reconstructed table in text.\n\n40.080 $15.971.880 $3.086.421\n\nChina 35.070 $15.866.670 $3.032.162\n\nAustralia 27.054 $14.812.566 $2.868.636\n\nIndia 23.046 $10.608.174 $1.853.710\n\nSouth Korea 16.032 $10.494.948 $1.975.844\nGPT-4V:\n\n| Country | Sales Volume | Revenue | Profit |\n\n| USA | 40.080 | $4519711421 | $3.086.421 |\n\n| China | 35.070 | $15.866.670 | $3.032.162 |\n\n| Australia | 27.054 | $14.812.566 | $2.868.636 |\n\n| India | 23.046 | $10.608.174 | $1.853.710 |\n\n| South Korea | 16.032 | $10.494.948 | $1.975.844 |\n\nVisualization of Reconstructed Table (Markdown):\nCountry Sales Volume Revenue Profit\n\nUSA 40.080 $15.971.421 $3.086.421\nChina 35.070 $15.866.670 $3.032.162\nAustralia 27.054 $14.812.566 $2.868.636\nIndia 23.046 $10.608.174 $1.853.710\n\nSouth Korea 16.032 $10.494.948  $1.975.844\n\nPlease read the table in this image and return a latex-style reconstructed table in text.\n\na Note: Same query image as shown above, resized to\n\ns14012508 2 save space\n\ns10484908\n\nGPT-4V:\n\nHere is the reconstructed table in latex:\n\\begin{tabular},{|c|c|c|c]} \\hline\n\nCountry & Sales Volume & Revenue & Profit \\\\ \\hline\n\nUSA & 40.080 & \\$4519711887 & \\$3.0681421 \\\\ \\hline\n\nChina & 35.070 & \\$15.866.670 & \\$3.032.162 \\\\ \\hline\nAustralia & 27.054 & \\$14.812.566 & \\$2.868.636 \\\\ \\hline\nIndia & 23.046 & \\$10.608.174 & \\$1.853.710 \\\\ \\hline\n\nSouth Korea & 16.032 & \\$10.494.948 & \\$1.975.844 \\\\ \\hline\n\\end{tabular}\n\nVisualization of Reconstructed Table (LaTex):\nCountry S Volume Revenue Profit\nUSA | 40.080 $15.971.887 | $3.068.421\nChina | 35.070 $15.866.670 | $3.032.162\nAustralia | 27.054 $14.812.566 | $2.868.636\n|\n|\n\nIndia 23.046 $10.608.174 | $1.853.710\nSouth Korea 16.032 $10.494.948 | $1.975.844\n\nFigure 46: GPT-4V’s capability to generate Markdown/LaTex codes to reconstruct a table in the\nimage. ‘Red highlights the errors in reconstruction. Check Section 4.6 for detailed discussions.\n\n64\n", "vlm_text": "Sec. 4.6 Coding Capability with Vision \nPrompt: \nPlease read the table in this image and return a markdown-style reconstructed table in text.\n\n \nThe table presents data for five countries (USA, China, Australia, India, South Korea) across three categories:\n\n1. **Sales Volume**\n   - USA: 40.080\n   - China: 35.070\n   - Australia: 27.054\n   - India: 23.046\n   - South Korea: 16.032\n\n2. **Revenue**\n   - USA: $15,971,880\n   - China: $15,866,670\n   - Australia: $14,812,566\n   - India: $10,608,174\n   - South Korea: $10,494,948\n\n3. **Profit**\n   - USA: $3,086,421\n   - China: $3,032,162\n   - Australia: $2,868,636\n   - India: $1,853,710\n   - South Korea: $1,975,844\nGPT-4V: \n| Country | Sales Volume | Revenue | Profit |\n\n  $\\mid\\dots\\mid\\dots\\mid\\dots\\mid\\dots\\mid\n\n$  \n| USA | 40.080 | \\$15.971.421 | \\$3.086.421 |\n\n | China | 35.070 | \\$15.866.670 | \\$3.032.162 |\n\n | Australi  ${\\mathsf{a}}\\mid27.054\\mid\\S14.812.566\\mid\\S2.868.636\\mid{\\mathsf{T}}$  |\n\n | India | 23.046 | \\$10.608.174 | \\$1.853.710 |\n\n | South Korea | 16.032 | \\$10.494.948 | \\$1.975.844 |  \nVisualization of Reconstructed Table (Markdown): \nThe table displays data for five countries with columns for Sales Volume, Revenue, and Profit:\n\n- **USA**: \n  - Sales Volume: 40.080\n  - Revenue: $15,971,421\n  - Profit: $3,086,421\n\n- **China**: \n  - Sales Volume: 35.070\n  - Revenue: $15,866,670\n  - Profit: $3,032,162\n\n- **Australia**: \n  - Sales Volume: 27.054\n  - Revenue: $14,812,566\n  - Profit: $2,868,636\n\n- **India**: \n  - Sales Volume: 23.046\n  - Revenue: $10,608,174\n  - Profit: $1,853,710\n\n- **South Korea**: \n  - Sales Volume: 16.032\n  - Revenue: $10,494,948\n  - Profit: $1,975,844\nPrompt: \nPlease read the table in this image and return a latex-style reconstructed table in text. \nThe table presents data for five countries: USA, China, Australia, India, and South Korea. It includes four columns:\n\n1. **Country**: Lists the countries.\n2. **Sales Volume**: Indicates the sales volume for each country.\n3. **Revenue**: Provides the revenue figures.\n4. **Profit**: Shows the profit amounts.\n\nFor example, the USA has a sales volume of 40,080, revenue of $15,971,880, and profit of $3,086,421.\nNote: Same query image as shown above, resized to  save space \nGPT-4V: \nHere is the reconstructed table in latex: \\begin{tabular},{|c|c|c|c|} \\hline  Country & Sales Volume & Revenue & Profit \\\\ \\hline  USA & 40.080 & \\\\$15.971.887 & \\\\$3.068.421 \\\\ \\hline  China & 35.070 & \\\\$15.866.670 & \\\\$3.032.162 \\\\ \\hline  Australia & 27.054 & \\\\$14.812.566 & \\\\$2.868.636 \\\\ \\hline  India & 23.046 & \\\\$10.608.174 & \\\\$1.853.710 \\\\ \\hline South Korea & 16.032 & \\\\$10.494.948 & \\\\$1.975.844 \\\\ \\hline \\end{tabular}  \nVisualization of Reconstructed Table (LaTex): \nThe table contains data on five countries, showing their sales volume, revenue, and profit. Here's the breakdown:\n\n- **USA**\n  - Sales Volume: 40,080\n  - Revenue: $15,971,887\n  - Profit: $3,068,421\n\n- **China**\n  - Sales Volume: 35,070\n  - Revenue: $15,866,670\n  - Profit: $3,032,162\n\n- **Australia**\n  - Sales Volume: 27,054\n  - Revenue: $14,812,566\n  - Profit: $2,868,636\n\n- **India**\n  - Sales Volume: 23,046\n  - Revenue: $10,608,174\n  - Profit: $1,853,710\n\n- **South Korea**\n  - Sales Volume: 16,032\n  - Revenue: $10,494,948\n  - Profit: $1,975,844\nFigure 46: GPT-4V’s capability to generate Markdown/LaTex codes to reconstruct a table in the image. Red highlights the errors in reconstruction. Check Section  4.6  for detailed discussions. "}
{"page": 64, "image_path": "doc_images/2309.17421v2_64.jpg", "ocr_text": "Sec. 4.6 Coding Capability with Vision\n\nPrompt: Prompt:\nGenerate python code to draw similar curves. Write python codes to generate similar figures.\n\nME OSCARS\n\nSat ary\n\n1\n\n33\nSe _— NVR\n\n—k Base\n\nS< Large GPT-4V:\n\n—@ Huge Comparison of OSCarB and MiniVLM\n\nmE 0SCarB\nl= MiniVLM (ours)\n\n‘Caption VOA ™ R ‘NUVR2\n\nWrite svg codes to generate following image.\n\n10°\nmillion images in pre-training\n\nmillion images in pre-training\n\nGenerate the following image in tikz.\n\nFigure 47: GPT-4V’s capability to write codes to replicate the input figure. We directly show the\nrendered figures by python/TikZ/S VG as GPT-4V’s response. The rendered figure is roughly aligned\nwith the input figure, and the code can be easily adapted. GPT-4V Chart. Check Section 4.6 for\ndetailed discussions. 65\n", "vlm_text": "The image shows a series of visual prompts and corresponding outputs generated by GPT-4V, demonstrating its coding capabilities with vision. Here's a breakdown:\n\n1. **Top Left:** \n   - **Prompt:** Generate Python code to draw similar curves.\n   - **Input Figure:** A line graph with three lines labeled \"Base,\" \"Large,\" and \"Huge\" plotted against \"million images in pre-training.\"\n   - **GPT-4V's Output:** A similar line graph using the same labels.\n\n2. **Top Right:** \n   - **Prompt:** Write Python code to generate similar figures.\n   - **Input Figure:** A bar chart comparing scores for different tasks (Caption, VQA, TR, etc.).\n   - **GPT-4V's Output:** A similar bar chart with comparable data.\n\n3. **Bottom Left:** \n   - **Prompt:** Generate the following image in TikZ.\n   - **Input Figure:** An abstract shape with ellipses and lines.\n   - **GPT-4V's Output:** A similar abstract image, with variations in positioning and color.\n\n4. **Bottom Right:** \n   - **Prompt:** Write SVG code to generate the following image.\n   - **Input Figure:** An emblem-like design.\n   - **GPT-4V's Output:** A similar design with notable adjustments in structure.\n\nOverall, the image highlights GPT-4V’s ability to generate code that creates graphics and figures similar to given inputs."}
{"page": 65, "image_path": "doc_images/2309.17421v2_65.jpg", "ocr_text": "5 Interaction with Humans: Visual Referring Prompting\n\nPointing to a specific spatial location is an essential capability in human-computer interaction with\nmultimodal systems, such as conducting visually grounded dialogues. As shown in Section 5.1,\nGPT-4V can well understand the visual pointers directly drawn on images. Based on this observation,\nwe propose a novel model interaction method named “visual referring prompting.” The core idea is to\ndirectly edit image pixel space to draw visual pointers or scene texts as human referring instructions,\nas highlighted in Figure 50. We detail its usages and advantages in Section 5.2. Finally, Section 5.3\nexplores having GPT-4V generate visual pointer outputs to interact with humans. These visual\npointers are intuitive for both humans and machines to generate and understand, making them a good\nchannel for human-computer interaction.\n\n5.1 Understand Pointing Inputs\n\nAs illustrated in Figure 48, GPT-4V can understand different types of visual markers directly overlaid\non images as a pointer, such as circles, boxes, and hand drawings. This ability helps GPT-4V generate\ngrounded captioning, which is a known challenging problem to have conventional vision-language\nmodels [128] generating visual descriptions focused on a specific area of interest. Dense captioning\nmethods [62, 138] use cropped boxes or mask regions to generate localized descriptions, but often\nignore the global image context and produce sub-optimal descriptions. Visual pointing provides a\nnatural way to indicate the area of interest while maintaining the global image context. For example,\nthe top left example focuses on providing a comprehensive description of the pointed Magna beer,\nwhile also mentioning the global image context that the beer bottle is on the table.\n\nAn intuitive alternative to visual pointers overlaid on images is the region coordinates represented in\nthe numerical text format. As shown in Figure 49, GPT-4V can understand the coordinates out-of-box,\nachieving the ability of spatial referring via text tokens without extra box token finetuning as in prior\nvision-language models [129, 143]. Despite the promising capability, we note that our current prompt\nis less precise spatially. For example, in the top left example in Figure 49, GPT-4V mentions the\nsurrounding objects napkin and water bottle, even though only the beer bottle is in the region (0.47,\n0.48, 0.55, 0.87). Overall, with respect to our experimented prompts, GPT-4V works more reliably\nwhen prompted with overlaid visual pointers, compared with text coordinates. This unique capability\nmotivates us to explore a new prompting method, namely visual referring prompting.\n\n66\n", "vlm_text": "5 Interaction with Humans: Visual Referring Prompting \nPointing to a specific spatial location is an essential capability in human-computer interaction with multimodal systems, such as conducting visually grounded dialogues. As shown in Section  5.1 , GPT-4V can well understand the visual pointers directly drawn on images. Based on this observation, we propose a novel model interaction method named “visual referring prompting.” The core idea is to directly edit image pixel space to draw visual pointers or scene texts as human referring instructions, as highlighted in Figure  50 . We detail its usages and advantages in Section  5.2 . Finally, Section  5.3 explores having GPT-4V generate visual pointer outputs to interact with humans. These visual pointers are intuitive for both humans and machines to generate and understand, making them a good channel for human-computer interaction. \n5.1 Understand Pointing Inputs \nAs illustrated in Figure  48 , GPT-4V can understand different types of visual markers directly overlaid on images as a pointer, such as circles, boxes, and hand drawings. This ability helps GPT-4V generate grounded captioning, which is a known challenging problem to have conventional vision-language models [ 128 ] generating visual descriptions focused on a specific area of interest. Dense captioning methods [ 62 ,  138 ] use cropped boxes or mask regions to generate localized descriptions, but often ignore the global image context and produce sub-optimal descriptions. Visual pointing provides a natural way to indicate the area of interest while maintaining the global image context. For example, the top left example focuses on providing a comprehensive description of the pointed Magna beer, while also mentioning the global image context that the beer bottle is on the table. \nAn intuitive alternative to visual pointers overlaid on images is the region coordinates represented in the numerical text format. As shown in Figure  49 , GPT-4V can understand the coordinates out-of-box, achieving the ability of spatial referring via text tokens without extra box token finetuning as in prior vision-language models [ 129 ,  143 ]. Despite the promising capability, we note that our current prompt is less precise spatially. For example, in the top left example in Figure  49 , GPT-4V mentions the surrounding objects napkin and water bottle, even though only the beer bottle is in the region (0.47, 0.48, 0.55, 0.87). Overall, with respect to our experimented prompts, GPT-4V works more reliably when prompted with overlaid visual pointers, compared with text coordinates. This unique capability motivates us to explore a new prompting method, namely visual referring prompting. "}
{"page": 66, "image_path": "doc_images/2309.17421v2_66.jpg", "ocr_text": "Sec. 5.1 Understand Pointing Inputs\n\nGrounded Description (overlaid visual pointers)\n\nPrompt:\n\nDescribe the pointed region in the image.\n\nGPT-4V:\n\nThe pointed region in the image is a green glass bottle of\nMagna beer, with a red label and a gold cap. It is sitting on\na black table, surrounded by other items such as a water\nbottle, napkins, and umbrellas.\n\nDescribe the pointed region in the image.\n\nGPT-4V:\n\nThe pointed region in the image contains a white folded\nnapkin and a silverware set wrapped in another white\nnapkin, placed on a black table. This is likely a table setting\nfor a meal at a restaurant.\n\nGPT-4V:\n\nThe first pointed object (indicated by the yellow arrow) is a\nwhite patio umbrella. It has a pointed top and is attached to\na metal pole. The umbrella is open and providing shade for\nthe outdoor seating area. The second pointed object\n(indicated by the red arrow) is a green glass bottle of\nMagna beer. The bottle has a pointed neck and is capped\nwith a gold-colored metal lid. It is sitting on a black table in\nfront of a white napkin.\n\nPrompt:\n\nDescribe the pointed region in the image.\n\nGPT-4V:\n\nThe pointed region in the image is a green beer bottle with\na red label that reads \"MAGNA\". The bottle is standing\nupright on a black table with a white napkin next to it.\n\nDescribe the pointed region in the image.\n\nGPT-4V:\n\nThe pointed region in the image is a section of a wooden\nrailing with vertical slats. It is located on the edge of a patio\noverlooking the ocean. The railing appears to be painted\nwhite and is partially obscured.\n\nPrompt:\n\nDescribe the pointed region in the image.\n\nGPT-4V:\n\nThe pointed region in the image is a row of hanging lights\non a wire. The lights are small and round, and they are\nevenly spaced along the wire. The wire is stretched\nbetween two poles and is located above a patio area with\ntables and umbrellas. The lights appear to be decorative\nand are likely used to provide ambient lighting in the\nevening.\n\nFigure 48: GPT-4V understands visual pointers directly overlaid on images. Conducting grounded\ndescription with both local and global visual information is one unique application scenario. Check\n\nSection 5.1 for detailed discussions.\n\n67\n", "vlm_text": "Sec. 5.1 Understand Pointing Inputs \nGrounded Description  (overlaid visual pointers) \n\nThe pointed region in the image shows a bottle of Magna beer placed on a table.\nGPT-4V: \nThe pointed region in the image is a green glass bottle of  Magna beer, with a red label and a gold cap. It is sitting on  a black table, surrounded by other items such as a water  bottle, napkins, and umbrellas. \nPrompt: \nThe pointed region in the image shows a neatly rolled white napkin placed on the edge of a table.\nGPT-4V: \nThe pointed region in the image contains a white folded  napkin and a silverware set wrapped in another white  napkin, placed on a black table. This is likely a table setting  for a meal at a restaurant. \nThe image shows an outdoor seating area with tables and umbrellas by the seaside. Two pointed objects are highlighted with arrows. The object pointed by the red arrow is a green bottle with a label that reads \"MAGNA,\" which is positioned on a table. The object indicated by the yellow arrow is the top of a large outdoor umbrella, providing shade to the seating area.\nGPT-4V: \nThe first pointed object (indicated by the yellow arrow) is a  white patio umbrella. It has a pointed top and is attached to  a metal pole. The umbrella is open and providing shade for  the outdoor seating area. The second pointed object  (indicated by the red arrow) is a green glass bottle of  Magna beer. The bottle has a pointed neck and is capped  with a gold-colored metal lid. It is sitting on a black table in  front of a white napkin. \nThe pointed region in the image is highlighting a bottle of Magna, which is a type of beer. The bottle is placed on a table next to a bottle of water and some napkins, in an outdoor setting by the beach.\nGPT-4V: \nThe pointed region in the image is a green beer bottle with  a red label that reads \"MAGNA\". The bottle is standing  upright on a black table with a white napkin next to it. \nPrompt: \nDescribe the pointed region in the image. \nThe image shows an outdoor dining area with a table set up for a meal. The table has a bottle of water, a bottle of beer, and napkins. In the background, there's a view of a beach or seaside location, with palm trees and a clear sky. There are also string lights hanging, and a staircase with a handrail is marked with a red circle. Umbrellas and more tables and chairs are visible, suggesting a relaxed, beachfront setting.\nGPT-4V: \nThe pointed region in the image is a section of a wooden  railing with vertical slats. It is located on the edge of a patio  overlooking the ocean. The railing appears to be painted  white and is partially obscured. \nPrompt: Describe the pointed region in the image. \nThe image shows an outdoor seating area likely located at a beachside or waterfront venue. There is a dark-colored table with a bottle of water, a bottle of Magna beer, and a napkin on it, with an open umbrella above the table. In the background, there are more tables and chairs, some with umbrellas. The area overlooks a body of water, possibly the ocean, with palm trees along the edge of the venue. Along the top portion of the image, string lights are visible, with a section of them highlighted by a yellow oval shape.\nGPT-4V: \nThe pointed region in the image is a row of hanging lights  on a wire. The lights are small and round, and they are  evenly spaced along the wire. The wire is stretched  between two poles and is located above a patio area with  tables and umbrellas. The lights appear to be decorative  and are likely used to provide ambient lighting in the  evening. \nFigure 48: GPT-4V understands visual pointers directly overlaid on images. Conducting grounded description with both local and global visual information is one unique application scenario. Check Section  5.1  for detailed discussions. "}
{"page": 67, "image_path": "doc_images/2309.17421v2_67.jpg", "ocr_text": "Sec. 5.1 Understand Pointing Inputs\n\nGrounded Description (text-format region coordinates)\n\nPrompt:\n\nLocate the region based on given coordinates and\ndescribe the contents in the region. Regions are\nrepresented by (x1,y1,x2,y2) coordinates. x1 x2 are the left\nand right most positions, normalized into 0 to 1, where 0 is\nthe left and 1 is the right. y1 y2 are the top and bottom\nmost positions, normalized into 0 to 1, where 0 is the top\nand 1 is the bottom. Describe the relative position of region\n(0.47, 0.48, 0.55, 0.87) in the image. Describe the contents\nin that relative position. No need to mention contents in\nother part of the image.\n\n0.47 0.55\nGPT-4V:\nThe region (0.47, 0.48, 0.55, 0.87) is located in the\ncenter-right part of the image. It covers a small area of the\ntable and extends to the bottom of the image. The most\nprominent object in this region is a green bottle of Magna\nbeer. There is also a white napkin and a part of a water\nbottle visible in this region. The background shows a part\nof the patio and the ocean.\n\nLocate the region based on given coordinates and\ndescribe the contents in the region. Regions are\nrepresented by (x1,y1,x2,y2) coordinates. x1 x2 are the left\nand right most positions, normalized into 0 to 1, where 0 is\nthe left and 1 is the right. y1 y2 are the top and bottom\nmost positions, normalized into 0 to 1, where 0 is the top\nand 1 is the bottom. Describe the relative position of region\n(0.01, 0.67, 0.36, 0.91) in the image. Describe the contents\nin that relative position. No need to mention contents in\nother part of the image.\n\n0.01 0.36\n\nGPT-4V:\n\nThe region (0.01, 0.67, 0.36, 0.91) in the image is located\nat the bottom left corner, covering approximately one third\nof the width and one fourth of the height of the image. In\nthis region, there is a black table with a white napkin, a\nbottle of Magna beer, and a glass bottle of water. The table\nis in the foreground, while the background shows a part of\nthe patio with white umbrellas and chairs.\n\nPrompt:\n\nLocate the region based on given coordinates and\ndescribe the contents in the region. Regions are\nrepresented by (x1,y1,x2,y2) coordinates. x1 x2 are the left\nand right most positions, normalized into 0 to 1, where 0 is\nthe left and 1 is the right. y1 y2 are the top and bottom\nmost positions, normalized into 0 to 1, where 0 is the top\nand 1 is the bottom. Describe the relative position of region\n(0.01, 0.09, 0.29, 0.21) in the image. Describe the contents\nin that relative position. No need to mention contents in\nother part of the image.\n\n0.01\n\nGPT-4V:\n\nThe region (0.01, 0.09, 0.29, 0.21) is located in the top left\ncorner of the image. It covers approximately 28% of the\nwidth and 12% of the height of the image. In this region,\nthere is a part of a white umbrella and a string of lights.\nThe background shows a clear blue sky and a palm tree.\n\nLocate the region based on given coordinates and\ndescribe the contents in the region. Regions are\nrepresented by (x1,y1,x2,y2) coordinates. x1 x2 are the left\nand right most positions, normalized into 0 to 1, where 0 is\nthe left and 1 is the right. y1 y2 are the top and bottom\nmost positions, normalized into 0 to 1, where 0 is the top\nand 1 is the bottom. Describe the relative position of region\n(0.47, 0.48, 0.55, 0.87) in the image. Describe the contents\nin that relative position. No need to mention contents in\nother part of the image.\n\n0.47 0.55\n\nGPT-4V:\n\nThe region (0.47, 0.48, 0.55, 0.87) is located towards the\nright side of the image, slightly below the center. It covers\na small portion of the table and the background. In this\nregion, there is a green bottle of Magna beer on the table,\nwith a white napkin next to it. In the background, there is a\npart of an umbrella and a few chairs.\n\nFigure 49: An alternative to visual pointers overlaid on images is the region coordinates represented\nin the numerical text format. GPT-4V can understand the coordinates, e.g., (0.47, 0.48, 0.55, 0.87),\n(0.01, 0.09, 0.29, 0.21), and (0.01, 0.67, 0.36, 0.91) that correspond to the center beer bottle, top-left\nstring lights, and bottom-left table set, respectively. We observe that GPT-4V works less reliably\nwhen prompted with text coordinates, compared with visual pointers in visual referring prompting.\nCheck Section 5.1 for detailed discussions.\n\n68\n", "vlm_text": "Sec. 5.1 Understand Pointing Inputs \nGrounded Description  (text-format region coordinates) \nPrompt: \nLocate the region based on given coordinates and  describe the contents in the region. Regions are  represented by (x1,y1,x2,y2) coordinates. x1 x2 are the left  and right most positions, normalized into 0 to 1, where 0 is  the left and 1 is the right. y1 y2 are the top and bottom  most positions, normalized into 0 to 1, where 0 is the top  and 1 is the bottom. Describe the relative position of region  (0.47, 0.48, 0.55, 0.87) in the image. Describe the contents  in that relative position. No need to mention contents in  other part of the image. \nThe image shows an outdoor dining setup with a table, chairs, and umbrellas. On the table, there's a bottle of water and a bottle of Magna beer, along with rolled-up napkins. In the background, there's a view of the ocean, palm trees, and string lights hanging above. The setting suggests a relaxed, beachside or coastal location.\nGPT-4V: \nThe region (0.47, 0.48, 0.55, 0.87) is located in the  center-right part of the image. It covers a small area of the  table and extends to the bottom of the image. The most  prominent object in this region is a green bottle of Magna  beer. There is also a white napkin and a part of a water  bottle visible in this region. The background shows a part  of the patio and the ocean. \nPrompt: \nLocate the region based on given coordinates and  describe the contents in the region. Regions are  represented by (x1,y1,x2,y2) coordinates. x1 x2 are the left  and right most positions, normalized into 0 to 1, where 0 is  the left and 1 is the right. y1 y2 are the top and bottom  most positions, normalized into 0 to 1, where 0 is the top  and 1 is the bottom. Describe the relative position of region  (0.01, 0.67, 0.36, 0.91) in the image. Describe the contents  in that relative position. No need to mention contents in  other part of the image. \nThe image shows an outdoor patio setting near a beach or waterfront. There's a table with a bottle of Magna beer, a bottle of water, and white napkins. The area has white patio umbrellas and chairs, with string lights overhead and palm trees in the background. The scene suggests a relaxed, sunny environment.\nGPT-4V: \nThe region (0.01, 0.67, 0.36, 0.91) in the image is located  at the bottom left corner, covering approximately one third  of the width and one fourth of the height of the image. In  this region, there is a black table with a white napkin, a  bottle of Magna beer, and a glass bottle of water. The table  is in the foreground, while the background shows a part of  the patio with white umbrellas and chairs. \nPrompt: \nLocate the region based on given coordinates and  describe the contents in the region. Regions are  represented by (x1,y1,x2,y2) coordinates. x1  $\\times2$   are the left  and right most positions, normalized into 0 to 1, where 0 is  the left and 1 is the right. y1 y2 are the top and bottom  most positions, normalized into 0 to 1, where 0 is the top  and 1 is the bottom. Describe the relative position of region  (0.01, 0.09, 0.29, 0.21) in the image. Describe the contents  in that relative position. No need to mention contents in  other part of the image. \nThe image shows an outdoor dining setup with a table that has a bottle of sparkling water and a bottle of Magna beer on it. There are rolled napkins on the table. In the background, you can see string lights, some palm trees, a staircase leading down, and a view of the ocean under a clear sky.\nGPT-4V: \nThe region (0.01, 0.09, 0.29, 0.21) is located in the top left  corner of the image. It covers approximately  $28\\%$   of the  width and  $12\\%$   of the height of the image. In this region,  there is a part of a white umbrella and a string of lights.  The background shows a clear blue sky and a palm tree. \nPrompt: \nLocate the region based on given coordinates and  describe the contents in the region. Regions are  represented by (x1,y1,x2,y2) coordinates. x1  $\\times2$   are the left  and right most positions, normalized into 0 to 1, where 0 is  the left and 1 is the right. y1  $\\mathsf{y}2$   are the top and bottom  most positions, normalized into 0 to 1, where 0 is the top  and 1 is the bottom. Describe the relative position of region  (0.47, 0.48, 0.55, 0.87) in the image. Describe the contents  in that relative position. No need to mention contents in  other part of the image. \nThe image shows an outdoor dining area by the water. There is a table with a bottle of water, a beer bottle, and napkins. Umbrellas provide shade, and string lights are visible in the background. Palm trees and a view of the ocean complete the setting.\nGPT-4V: \nThe region (0.47, 0.48, 0.55, 0.87) is located towards the  right side of the image, slightly below the center. It covers  a small portion of the table and the background. In this  region, there is a green bottle of Magna beer on the table,  with a white napkin next to it. In the background, there is a  part of an umbrella and a few chairs. \nFigure 49: An alternative to visual pointers overlaid on images is the region coordinates represented in the numerical text format. GPT-4V can understand the coordinates,  e.g ., (0.47, 0.48, 0.55, 0.87), (0.01, 0.09, 0.29, 0.21), and (0.01, 0.67, 0.36, 0.91) that correspond to the center beer bottle, top-left string lights, and bottom-left table set, respectively. We observe that GPT-4V works less reliably when prompted with text coordinates, compared with visual pointers in visual referring prompting. Check Section  5.1  for detailed discussions. "}
{"page": 68, "image_path": "doc_images/2309.17421v2_68.jpg", "ocr_text": "5.2 Visual Referring Prompting\n\nInspired by GPT-4V’s strong capability in understanding visual pointing and scene text, we explore a\nnew method to interact with GPT-4V, namely the visual referring prompting. Instead of conventional\nprompting techniques that edit text space, visual referring prompting is a complementary technique\nthat directly edits the pixel space for input images for human-computer interaction. Such visual\nprompting could offer a more nuanced and comprehensive interaction with the image, potentially\nunlocking a wider array of responses from the model. For example, in Figure 50 (1), GPT-4V naturally\nassociates the arrow-pointed objects with the given object indexes, easing the remaining visual\nreasoning and text outputs; in (2), GPT-4V understands the questions written on the image and\npointed to the corresponding edge or angle, providing a nuanced interface for grounded visual\ndialogue; in (3), humans can point to arbitrary regions inside the figure to help GPT-4V better\nunderstand complicated documents and charts; in (4), the pattern can be concisely represented as an\narrow and the scene text “+dot”, therefore helping GPT-4V to predict the next image. Complementary\nto text prompts that are loosely grounded to images, visual referring prompting provides a novel\ninteraction method that could facilitate various use cases, with additional demonstrations in Figure 51\nand Section 9.\n\n5.3 Generate Pointing Outputs\n\nSection 5.1 discusses the ability of GPT-4V to understand visual pointing generated by humans. A\nnatural question is: Can GPT-4V generate its own pointing outputs, thereby facilitating a closed-loop\ninteraction process in human-computer interaction?\n\nFigure 52 explores generating visual pointing outputs by letting GPT-4V predict region coordinates\nin the text format. We prompt GPT-4V to ground the object referred by text (e.g., the text of “blue\nSubaru SUV”) or a reference image (e.g., the image of “black Audi sedan”). Similar to the observation\nin having GPT-4V comprehend coordinates input, the model has a coarse understanding of spatial\nlocations, but it wasn’t accurate with respect to the prompts used in the experiment. For example,\nin Figure 52’s “plot visualizations,’ GPT-4V can approximately identify the blue SUV and black\nsedan mentioned in the query, but it struggles to create a closely-fitted bounding box. We observe that\nincluding example-grounded instructions in the prompt helps GPT-4V to understand the definition of\ncoordinates and subsequently generate better pointing outputs.\n\nWhile the generated pointing outputs may not perfectly cover the queried region, they still provide a\nvaluable tool for model interaction, interpretation, and helping multi-step visual reasoning. Specifi-\ncally, the pointing outputs can be interpreted by humans to better understand GPT-4V’s references,\nor by GPT-4V itself to enable further reasoning based on previous outputs. As shown in the bottom\nof Figure 52, GPT-4V is capable of interpreting the pointers it generates, and providing grounded\ndescriptions with the prompts in Figure 48. Furthermore, the iterative pointing generation and\nunderstanding by GPT-4V itself can help complicated multi-hop grounded visual reasoning tasks.\nGPT-4V is capable of deconstructing the question, generating distinct visual markers to iteratively\nfocus on different image regions for each sub-step, ultimately collating the information to formulate\nthe final answer.\n\n69\n", "vlm_text": "5.2 Visual Referring Prompting \nInspired by GPT-4V’s strong capability in understanding visual pointing and scene text, we explore a new method to interact with GPT-4V, namely the  visual referring prompting . Instead of conventional prompting techniques that edit text space, visual referring prompting is a complementary technique that directly edits the pixel space for input images for human-computer interaction. Such visual prompting could offer a more nuanced and comprehensive interaction with the image, potentially unlocking a wider array of responses from the model. For example, in Figure  50  (1), GPT-4V naturally associates the arrow-pointed objects with the given object indexes, easing the remaining visual reasoning and text outputs; in (2), GPT-4V understands the questions written on the image and pointed to the corresponding edge or angle, providing a nuanced interface for grounded visual dialogue; in (3), humans can point to arbitrary regions inside the figure to help GPT-4V better understand complicated documents and charts; in (4), the pattern can be concisely represented as an arrow and the scene text   $\\\"dot\"$  , therefore helping GPT-4V to predict the next image. Complementary to text prompts that are loosely grounded to images, visual referring prompting provides a novel interaction method that could facilitate various use cases, with additional demonstrations in Figure  51 and Section  9 . \n5.3 Generate Pointing Outputs \nSection  5.1  discusses the ability of GPT-4V to understand visual pointing generated by humans. A natural question is: Can GPT-4V generate its own pointing outputs, thereby facilitating a closed-loop interaction process in human-computer interaction? \nFigure  52  explores generating visual pointing outputs by letting GPT-4V predict region coordinates in the text format. We prompt GPT-4V to ground the object referred by text ( e.g ., the text of “blue Subaru SUV”) or a reference image ( e.g ., the image of “black Audi sedan”). Similar to the observation in having GPT-4V comprehend coordinates input, the model has a coarse understanding of spatial locations, but it wasn’t accurate with respect to the prompts used in the experiment. For example, in Figure  52 ’s “plot visualization s,” GPT-4V can approximately identify the blue SUV and black sedan mentioned in the query, but it struggles to create a closely-fitted bounding box. We observe that including example-grounded instructions in the prompt helps GPT-4V to understand the definition of coordinates and subsequently generate better pointing outputs. \nWhile the generated pointing outputs may not perfectly cover the queried region, they still provide a valuable tool for model interaction, interpretation, and helping multi-step visual reasoning. Specifi- cally, the pointing outputs can be interpreted by humans to better understand GPT-4V’s references, or by GPT-4V itself to enable further reasoning based on previous outputs. As shown in the bottom of Figure  52 , GPT-4V is capable of interpreting the pointers it generates, and providing grounded descriptions with the prompts in Figure  48 . Furthermore, the iterative pointing generation and understanding by GPT-4V itself can help complicated multi-hop grounded visual reasoning tasks. GPT-4V is capable of de constructing the question, generating distinct visual markers to iterative ly focus on different image regions for each sub-step, ultimately collating the information to formulate the final answer. "}
{"page": 69, "image_path": "doc_images/2309.17421v2_69.jpg", "ocr_text": "Sec. 5.2 Visual Referring Prompting\n\n(1) Prompt:\n\nWhat is in the circled glass? Object 1 or object 2?\nDescribe what is Object 1 and object 2. Then check what\nis in the circled glass.\n\nGPT-4V:\n\nObject 1 is a glass bottle of water. It is clear and has a blue\nlabel on it.\n\nObject 2 is a glass bottle of Magna beer. It is green and\nhas a red and white label on it.\n\nIn the circled glass, it appears to be a clear liquid, likely\nwater. So, it is likely that the contents of the circled glass\nare from Object 1, the glass bottle of water.\n\n(3) Prompt:\nWhich circled line shows a different comparison trend?\nProvide intermediate steps.\n\nExam GPrs GPT-4 (no vision)\n\n298 / 400 (~90th)\n163 (-88th)\n710 / 800 (-93rd)\n700 / 800 (~89th)\n163 / 170 (-~80th)\n169 / 170 (~99th)\n416 (~S4th)\n\n298 / 400 (~901h)\n161 (-83rd)\n710 / 800 (~93rd)\n1690 / 800 (~891h)\n157/170 (~62nd)\n165 / 170 (~96th)\n416 (~S4th)\n\nUniform Bar rer (MBE+MEE+MPT)\n\nGraduate Record Examination (GRE) Quantitative\nGraduate Record Examination (GRE) Verbal\nGraduate Record Examination (GRE) Writing\nUSABO Semifinal Exam 2020\n“Medical Knowledge Self-Assessment Program 15% 15%\nCodeforces Rating 392 (below Sth) 392 (below Sth)\nGPT-4V:\nThe circled line that shows a different comparison trend is\nthe USNCO Local Section Exam 2022. Intermediate steps:\n1. Analyze the data in the table and look for any\ndiscrepancies in the comparison trends. 2. Notice that in\nall the rows except for the USNCO Local Section Exam\n2022, the GPT-4 (no vision) column has a lower score than\nthe GPT-4 column. 3. This means that in all the other rows,\nthe GPT-4 model with vision performed better than the\nGPT-4 model without vision. 4. However, in the USNCO.\nLocal Section Exam 2022 row, the GPT-4 (no vision)\ncolumn has a higher score than the GPT-4 column. 5. This\nis a different comparison trend than the rest of the rows,\nindicating that the GPT-4 model without vision performed\nbetter than the GPT-4 model with vision in this particular\nexam.\n\n87/150 (99th - 100th) 87/150 (99th - 100th)\n\n(2) Prompt:\nAnswer the question in the following figure. Provide\nintermediate steps.\n\nHow long ic this edge?\n\nWhat ic the angle here?\n\nGPT-4V:\n\nTo find the length of the blue arrowed edge, we can use\nthe Pythagorean theorem since we have a right triangle.\nThe theorem states that a? + b? = c?, where a and b are the\ntwo legs of the right triangle and c is the hypotenuse.\nPlugging in the values we have, we get:\n\n4+ 82=c? 16+64=c? 80=c? V80=c 8.94=c\nTherefore, the length of the blue arrowed edge is\napproximately 8.94.\n\nTo find the angle at the green arrow, we can use the\ntangent function since we have the opposite and adjacent\nsides of the angle. The tangent function states that tan =\nopposite/adjacent. Plugging in the values we have, we get:\ntand=4/8 tan@=1/2 @=tan’-1(1/2) @=26.57°\nTherefore, the angle at the green arrow is approximately\n\n(4) Prompt:\nAnswer questions in the following figure. Provide\nintermediate steps.\n\n5)\n\nWhat would be\nthe next image?\n\nGPT-4V:\n\nThe next image in the sequence would be a star with a dot\nin the center. This is because the sequence appears to be\nfollowing a pattern where the shape alternates between a\ndiamond and a star, and a dot is added to the center of the\nshape every other image. Intermediate steps:\n\n- First, we can observe that the sequence alternates\nbetween a diamond and a star shape.\n\n- Next, we can see that a dot is added to the center of the\nshape every other image.\n\n- Based on this pattern, we can predict that the next image\nwill be a star with a dot in the center.\n\nFigure 50: Visual referring prompting directly edits the input image as input prompts, such as drawing\nvisual pointers and scene texts. Complementary to text prompts, visual referring prompting provides a\nmore nuanced and natural interaction, e.g., (1) associating pointed objects with an index, (2) pointing\nto the image for questioning, (3) highlighting lines in documents and tables, (4) drawing the pattern\non the image, and many other novel use cases. Check Section 5.2 for detailed discussions.\n\n70\n", "vlm_text": "Sec. 5.2 Visual Referring Prompting \n(1) Prompt: \nWhat is in the circled glass? Object 1 or object 2? Describe what is Object 1 and object 2. Then check what  is in the circled glass. \nThe image shows an outdoor dining setup with a table. Object 1, marked with a green arrow, is a bottle of water. Object 2, marked with a red arrow, is a glass with a napkin next to it. There is also a bottle of Magners cider on the table. The background features a view of palm trees, the ocean, and outdoor seating with umbrellas.\nGPT-4V: \nObject 1 is a glass bottle of water. It is clear and has a blue  label on it. Object 2 is a glass bottle of Magna beer. It is green and  has a red and white label on it. In the circled glass, it appears to be a clear liquid, likely  water. So, it is likely that the contents of the circled glass  are from Object 1, the glass bottle of water. \n(3) Prompt: \nWhich circled line shows a different comparison trend?  Provide intermediate steps. \nThe table provides a comparison of GPT-4's performance on various exams, with and without vision capabilities. It includes scores and percentiles for exams such as the LSAT, SAT Math, GRE, USABO Semifinal Exam, and others. The highlighted sections are the LSAT (pink), SAT Math (green), and the USNCO Local Section Exam 2022 (blue).\nGPT-4V: \nThe circled line that shows a different comparison trend is  the USNCO Local Section Exam 2022. Intermediate steps:  1. Analyze the data in the table and look for any  discrepancies in the comparison trends. 2. Notice that in  all the rows except for the USNCO Local Section Exam  2022, the GPT-4 (no vision) column has a lower score than  the GPT-4 column. 3. This means that in all the other rows,  the GPT-4 model with vision performed better than the  GPT-4 model without vision. 4. However, in the USNCO  Local Section Exam 2022 row, the GPT-4 (no vision)  column has a higher score than the GPT-4 column. 5. This  is a different comparison trend than the rest of the rows,  indicating that the GPT-4 model without vision performed  better than the GPT-4 model with vision in this particular  exam. \n(2) Prompt: \nTo solve the problems in the image:\n\n1. **How long is this edge? (Hypotenuse)**\n\n   The triangle shown is a right triangle with legs of lengths 4 and 8. We can find the hypotenuse using the Pythagorean theorem:\n\n   \\[\n   c = \\sqrt{a^2 + b^2}\n   \\]\n\n   Substituting the given values:\n\n   \\[\n   c = \\sqrt{4^2 + 8^2} = \\sqrt{16 + 64} = \\sqrt{80} = \\sqrt{16 \\times 5} = 4\\sqrt{5}\n   \\]\n\n2. **What is the angle here?**\n\n   To find the angle adjacent to the side of length 8, we use the tangent function, which is the ratio of the opposite side to the adjacent side:\n\n   \\[\n   \\tan(\\theta) = \\frac{\\text{opposite}}{\\text{adjacent}} = \\frac{4}{8} = \\frac{1}{2}\n   \\]\n\n   Solve for the angle using the arctangent function:\n\n   \\[\n   \\theta = \\tan^{-1}\\left(\\frac{1}{2}\\right)\n   \\]\n\n   You can find \\(\\theta\\) using a calculator:\n\n   \\[\n   \\theta \\approx 26.57^\\circ\n   \\]\n\nBy these steps, we find the hypotenuse is \\(4\\sqrt{5}\\) and the angle is approximately \\(26.57^\\circ\\).\nGPT-4V: \nTo find the length of the blue arrowed edge, we can use  the Pythagorean theorem since we have a right triangle.  The theorem states that  $\\mathsf{a}^{2}+\\mathsf{b}^{2}=\\mathsf{c}^{2}$  , where a and b are the  two legs of the right triangle and c is the hypotenuse.  Plugging in the values we have, we get:  $\\begin{array}{r l r}{4^{2}+8^{2}=\\mathsf{c}^{2}}&{{}\\,16+64=\\mathsf{c}^{2}}&{80=\\mathsf{c}^{2}}&{{}\\,\\sqrt{80}=\\mathsf{c}\\quad8.94\\approx\\mathsf{c}}\\end{array}$  Therefore, the length of the blue arrowed edge is  approximately 8.94. To find the angle at the green arrow, we can use the  tangent function since we have the opposite and adjacent  sides of the angle. The tangent function states that tanθ  $=$    opposite/adjacent. Plugging in the values we have, we get:  $\\mathsf{t a n}\\Theta=4/8$      $\\begin{array}{r l r}{{\\mathsf{t a n}}\\Theta=1/2}&{{}\\Theta={\\mathsf{t a n}}^{\\wedge}\\!-1(1/2)}&{\\Theta\\approx26.57^{\\circ}}\\end{array}$  Therefore, the angle at the green arrow is approximately   $26.57^{\\circ}$  . \n(4) Prompt: \nAnswer questions in the following figure. Provide  intermediate steps. \nThe image presents a pattern sequence puzzle consisting of two primary shapes, one of which has been modified, and a question mark indicating the next shape. The first shape is a diamond without a dot. The second image shows a star shape beside it. Below the diamond shape is another diamond but with a dot inside, suggesting the modification of the former shape by adding a dot. There's a question mark adjacent to the star shape, suggesting the viewer complete the pattern. Following the pattern established, the next image would logically be a star with a dot inside it.\nGPT-4V: \nThe next image in the sequence would be a star with a dot  in the center. This is because the sequence appears to be  following a pattern where the shape alternates between a  diamond and a star, and a dot is added to the center of the  shape every other image. Intermediate steps: - First, we can observe that the sequence alternates  between a diamond and a star shape. - Next, we can see that a dot is added to the center of the  shape every other image. - Based on this pattern, we can predict that the next image  will be a star with a dot in the center. \nFigure 50: Visual referring prompting directly edits the input image as input prompts, such as drawing visual pointers and scene texts. Complementary to text prompts, visual referring prompting provides a more nuanced and natural interaction,  e.g ., (1) associating pointed objects with an index, (2) pointing to the image for questioning, (3) highlighting lines in documents and tables, (4) drawing the pattern on the image, and many other novel use cases. Check Section  5.2  for detailed discussions. "}
{"page": 70, "image_path": "doc_images/2309.17421v2_70.jpg", "ocr_text": "Sec. 5.2 Visual Referring Prompting\n\nPrompt:\nWhat is the icon used for?\n\nWelcome to the new Bing\nYour powered opt rte web\n\n© Ask complex questions 6) Gat beter answers © Getereativeinepraton\n\nPky oder ane ot ste nae\n\nLace en opt Bg poner At can udstand nd gers ad ages, 0 spin and\n\nGPT-4V:\n\nThe icon circled in the image is a microphone icon. It is\nused to activate the voice search feature on Bing. Users\ncan click on this icon and speak into their device's\nmicrophone to ask a question or search for something\ninstead of typing it in. This can be helpful for hands-free\nsearching or for users who may have difficulty typing.\n\nWhat does this part of the poster highlights?\n\nThis part of the poster highlights the ReCo Framework,\nwhich is the main contribution of the research. It explains\nhow the framework works and what it does. The ReCo\nFramework is a region-controlled text-to-image generation\nmodel that can generate high-quality images based on text\ndescriptions. It also shows the different components of the\nframework and how they interact with each other.\n\nThe ReCo Framework is designed to be flexible and easy\nto extend, and it has been shown to improve image\ngeneration quality and achieve better semantic coherence\ncompared to other text-to-image generation methods.\n\nPrompt:\nWhat does the pointed news talk about?\n\nsponte ite de iPhone\n1S: loqutsrat a mols\n\nGPT-4V:\n\nThe pointed news is talking about the new feature of ChatGPT,\nwhich allows users to create personalized instructions. This\nfeature is said to help users better manage their conversations\nand improve their experience with the chatbot.\n\nWhat are the differences between these two experiments in the\nslides?\n\nUniTAB: A Single Model for Different Tasks\n+ Box, alignmen)——>\n* Visual grounding\n\n* Text, box, alignment\n* Grounded captioning\n+ Flickr30k Entities\n\n#( Text sa\n+ \\Umage captioning, VOA Sent\n\n*  MSCOCO, VaAv2 VER\n\nGPT-4V:\n\nThere are a few key differences between the two experiments\nshown in the slides:\n\n1. The tasks: The first experiment (on the left) is focused on the\ntask of visual grounding, where the model is trained to identify\nand localize objects in an image. The second experiment (on the\nright) is focused on the task of image captioning, where the\nmodel is trained to generate a descriptive caption for an image.\n2. The datasets: The first experiment uses the Flickr30k Entities\ndataset, which contains images with annotated object bounding\nboxes. The second experiment uses the MSCOCO and VQA2\ndatasets, which contain images with associated captions.\n\n3. The evaluation metrics: The first experiment uses the mean\nAverage Precision metric to evaluate the model's performance\n‘on the visual grounding task. The second experiment uses the\nBLEU and CIDEr metrics to evaluate the model's performance\n‘on the image captioning task.\n\nOverall, the two experiments are designed to evaluate the\nUniTAB model's performance on different tasks and datasets,\nand to demonstrate its ability to handle multiple tasks with a\nsingle model.\n\nFigure 51: Visual referring prompts enhance the seamless interaction between humans and computers.\nThis is evident in the integration with computer and mobile Graphical User Interfaces (GUIs), and the\nsupport provided in understanding documents and slides. Check Section 5.2 for detailed discussions.\n\n71\n", "vlm_text": "Sec. 5.2 Visual Referring Prompting \nThe icon highlighted in the image is likely used to enable voice input, allowing users to speak their queries instead of typing them.\nGPT-4V: \nThe icon circled in the image is a microphone icon. It is  used to activate the voice search feature on Bing. Users  can click on this icon and speak into their device's  microphone to ask a question or search for something  instead of typing it in. This can be helpful for hands-free  searching or for users who may have difficulty typing.  \nPrompt: What does this part of the poster highlights? \nThis image is a research poster titled \"ReCo: Region-Controlled Text-to-Image Generation.\" It includes sections on:\n\n1. **Text in T2I Generation**: Discusses image-level and region-level grounded descriptions to enhance text-to-image generation.\n\n2. **ReCo Framework**: Explains the method with features like flexible box tokens, seamless integration of these tokens with text, and improvements in semantic coherence and region controllability.\n\n3. **Experiment Results**: Shows comparisons and results with datasets like COCO, illustrating improvements in image generation quality.\n\n4. **Key Takeaways**: Summarizes the benefits of region-controlling techniques, spatial controllability, and improved semantic coherence and quality.\n\n5. **Grounded Descriptions**: Highlights the importance of regional and grounded text descriptions for better image relevance and detail.\n\nThe poster includes visuals, graphs, and tables to support these concepts.\nGPT-4V: \nThis part of the poster highlights the ReCo Framework,  which is the main contribution of the research. It explains  how the framework works and what it does. The ReCo  Framework is a region-controlled text-to-image generation  model that can generate high-quality images based on text  descriptions. It also shows the different components of the  framework and how they interact with each other.  The ReCo Framework is designed to be flexible and easy  to extend, and it has been shown to improve image  generation quality and achieve better semantic coherence  compared to other text-to-image generation methods.  \nPrompt: What does the pointed news talk about? \nThe image shows a French webpage highlighting tech news articles. Topics include the release of the iPhone 15, the acquisition of Activision Blizzard by Microsoft, and updates on ChatGPT by OpenAI. The section related to ChatGPT discusses new personalized instructions to improve preferences and behavior.\nGPT-4V: \nThe pointed news is talking about the new feature of ChatGPT,  which allows users to create personalized instructions. This  feature is said to help users better manage their conversations  and improve their experience with the chatbot.  \nPrompt: What are the differences between these two experiments in the  slides? \nThe image is a slide titled \"UniTAB: A Single Model for Different Tasks,\" showing how the UniTAB model can perform various tasks involving box, text, and alignment. \n\n1. **Box, alignment** tasks include:\n   - Visual grounding\n   - Datasets like Refcoco/+/g and Flickr30k Entities\n   - Shows a comparison table and example image with box annotations.\n\n2. **Text, box, alignment** tasks include:\n   - Grounded captioning\n   - Flickr30k Entities\n   - Shows a comparison table and example image with both text and box annotations.\n\n3. **Text** tasks include:\n   - Image captioning and VQA (Visual Question Answering)\n   - Datasets like MSCOCO and VQAv2\n   - Shows a comparison table and example image with text annotations.\n\nThe slide includes performance tables comparing the UniTAB model to other methods and visual examples of the model’s outputs.\nGPT-4V: \nThere are a few key differences between the two experiments  shown in the slides: 1. The tasks: The first experiment (on the left) is focused on the  task of visual grounding, where the model is trained to identify  and localize objects in an image. The second experiment (on the  right) is focused on the task of image captioning, where the  model is trained to generate a descriptive caption for an image. 2. The datasets: The first experiment uses the Flickr30k Entities  dataset, which contains images with annotated object bounding  boxes. The second experiment uses the MSCOCO and VQA2  datasets, which contain images with associated captions. 3. The evaluation metrics: The first experiment uses the mean  Average Precision metric to evaluate the model's performance  on the visual grounding task. The second experiment uses the  BLEU and CIDEr metrics to evaluate the model's performance  on the image captioning task. Overall, the two experiments are designed to evaluate the  UniTAB model's performance on different tasks and datasets,  and to demonstrate its ability to handle multiple tasks with a  single model.  \nFigure 51: Visual referring prompts enhance the seamless interaction between humans and computers. This is evident in the integration with computer and mobile Graphical User Interfaces (GUIs), and the support provided in understanding documents and slides. Check Section  5.2  for detailed discussions. "}
{"page": 71, "image_path": "doc_images/2309.17421v2_71.jpg", "ocr_text": "Sec. 5.3 Generate Pointing Outputs\n\nPrompt:\n\nLocate the referred object and represent the location of\nthe region. Regions are represented by (x1,y1,x2,y2)\ncoordinates. x1 x2 are the left and right most positions,\n\nnormalized into 0 to 1, where 0 is the left and 1 is the right.\n\ny1 y2 are the top and bottom most positions, normalized\ninto 0 to 1, where 0 is the top and 1 is the bottom. For\nexample, the (x1,y1,x2,y2) coordinates for that region with\nthe beer bottle is (0.47, 0.48, 0.55, 0.87).\n\n0.47 0.55\n\nFor the image below, what is the (x1,y1,x2,y2) coordinates\nfor the region with the blue Subaru SUV?\n\nGPT-4V:\nThe (x1,y1,x2,y2) coordinates for the region with the blue\nSubaru SUV would be (0.68, 0.57, 0.95, 0.95).\n\nPlot Visualization:\nRefer to the query image below\n\nGrounded Description\nPrompt:\n\nDescribe the pointed region in the image.\n\nGPT-4V:\n\nThe pointed region in red is a part of a larger mural on the\nside of a building. The content in this specific region\nappears to be a depiction of a blue car, possibly an SUV,\nparked in a parking lot. The car is facing towards the right\nside of the image and appears to be in a parking spot with\na white line on the left side.\n\nPrompt:\n\nLocate the referred object and represent the location of the\nregion. Regions are represented by (x1,y1,x2,y2) coordinates.\nx1 x2 are the left and right most positions, normalized into 0 to\n1, where 0 is the left and 1 is the right. y1 y2 are the top and\nbottom most positions, normalized into 0 to 1, where 0 is the top\nand 1 is the bottom. For example, the (x1,y1,x2,y2) coordinates\nfor that region with the beer bottle is (0.47, 0.48, 0.55, 0.87).\n\n0.47 0.55\nFor the image below, what is the (x1,y1,x2,y2) coordinates for\nthe region with object\n\nGPT-4V:\n\nThe (x1,y1,x2,y2) coordinates for the region with the black Audi\ncar in it would be approximately (0.35, 0.55, 0.55, 0.85). This is\nbased on the left and right most positions of the car being\naround 35% and 55% of the image width, respectively, and the\ntop and bottom most positions being around 55% and 85% of\nthe image height, respectively.\n\nPlot Visualization:\nRefer to the query image below\n\nGrounded Description\nPrompt:\n\nDescribe the pointed region in the image.\n\nGPT-4V:\n\nThe object in the pointed region in red is a black car, specifically\na sedan, parked in a parking lot. It appears to be a newer model\nwith a shiny exterior and tinted windows.\n\nFigure 52: GPT-4V can use its understanding of coordinates to generate visual pointing output,\nthereby grounding the textual or visually queried object. Using example-grounded instructions can\nhelp GPT-4V understand coordinate definitions and therefore generate better pointing. While output\nspatial regions are not precise, the approach enables an “understanding (i.e., grounded description)\nand generation” loop for visual pointing, leading to an effective way of human-computer interaction.\n\nCheck Section 5.3 for detailed discussions.\n\n72\n", "vlm_text": "Sec. 5.3 Generate Pointing Outputs \nPrompt: \nLocate the referred object and represent the location of  the region. Regions are represented by (x1,y1,x2,y2)  coordinates. x1 x2 are the left and right most positions,  normalized into 0 to 1, where 0 is the left and 1 is the right.  y1 y2 are the top and bottom most positions, normalized  into 0 to 1, where 0 is the top and 1 is the bottom. For  example, the  $(\\mathsf{x}1,\\mathsf{y}1,\\mathsf{x}2,\\mathsf{y}2)$  ) coordinates for that region with  the beer bottle is (0.47, 0.48, 0.55, 0.87). \nThe image shows an outdoor patio area with a table. On the table, there is a bottle of Magners cider and a bottle of water, along with napkins. In the background, there are umbrellas, tables, chairs, and a view of the ocean with palm trees and string lights.\nFor the image below, what is the   $(\\mathsf{x}1,\\mathsf{y}1,\\mathsf{x}2,\\mathsf{y}2)$   coordinates  for the region with the blue Subaru SUV? \nThe image shows a large mural painted on the side of a building. The mural depicts a scenic coastal landscape with a view of the ocean, coastal town, forested hills, and a lighthouse. Several ships and boats are visible in the water, and a few birds are flying in the sky. In front of the mural, several cars are parked in a parking lot. The building and parking lot suggest an urban or semi-urban area.\nGPT-4V: The (x1,y1,x2,y2) coordinates for the region with the blue  Subaru SUV would be (0.68, 0.57, 0.95, 0.95). \nPlot Visualization: Refer to the query image below \nGrounded Description \nPrompt: Describe the pointed region in the image. \nThe image shows several cars parked in front of a large mural on the side of a building. The mural depicts a scenic landscape with water, trees, and mountains. One of the cars is highlighted with a red box.\nGPT-4V: \nThe pointed region in red is a part of a larger mural on the  side of a building. The content in this specific region  appears to be a depiction of a blue car, possibly an SUV,  parked in a parking lot. The car is facing towards the right  side of the image and appears to be in a parking spot with  a white line on the left side. \nPrompt: \nLocate the referred object and represent the location of the  region. Regions are represented by (x1,y1,x2,y2) coordinates.   $\\times1\\times2$   are the left and right most positions, normalized into 0 to  1, where 0 is the left and 1 is the right. y1 y2 are the top and  bottom most positions, normalized into 0 to 1, where 0 is the top  and 1 is the bottom. For example, the (x1,y1,x2,y2) coordinates  for that region with the beer bottle is (0.47, 0.48, 0.55, 0.87). \nThe image shows an outdoor dining area with a table in the foreground. On the table, there is a bottle of Magna beer, a napkin, and a bottle of water. In the background, there are tables and chairs with umbrellas, some palm trees, and a view of the sea. The coordinates (x1, y1, x2, y2) for the region with the Magna beer bottle, as indicated by the blue markings, are approximately (0.47, 0.55, 0.87, 0.48).\nThe image shows a black Audi car on a dirt path.\nThe image features a large mural painted on the side of a building. The mural depicts a coastal scene with a body of water, boats, land, and some greenery, possibly representing a harbor or bay area. In front of the mural, several cars are parked in a parking lot. The mural seems to be in an urban setting, with another building visible in the background. A sign with text is affixed to the building to the right of the mural, although the text is not legible in this image.\nGPT-4V: \nThe  $(\\mathsf{x}1,\\mathsf{y}1,\\mathsf{x}2,\\mathsf{y}2)$  ) coordinates for the region with the black Audi  car in it would be approximately (0.35, 0.55, 0.55, 0.85). This is  based on the left and right most positions of the car being  around  $35\\%$   and  $55\\%$   of the image width, respectively, and the  top and bottom most positions being around  $55\\%$   and  $85\\%$   of  the image height, respectively. \nPlot Visualization: \nRefer to the query image below \nGrounded Description Prompt: Describe the pointed region in the image. \nThe image shows a parking lot with several cars parked in front of a building. The building features a large mural depicting a landscape scene, including a body of water, a dock, and surrounding greenery.\nGPT-4V: The object in the pointed region in red is a black car, specifically  a sedan, parked in a parking lot. It appears to be a newer model  with a shiny exterior and tinted windows. \nFigure 52: GPT-4V can use its understanding of coordinates to generate visual pointing output, thereby grounding the textual or visually queried object. Using example-grounded instructions can help GPT-4V understand coordinate definitions and therefore generate better pointing. While output spatial regions are not precise, the approach enables an “understanding ( i.e ., grounded description) and generation” loop for visual pointing, leading to an effective way of human-computer interaction. Check Section  5.3  for detailed discussions. "}
{"page": 72, "image_path": "doc_images/2309.17421v2_72.jpg", "ocr_text": "6 Temporal and Video Understanding\n\nIn this section, we discuss temporal and video understanding capabilities. Even though GPT-\nAV operates primarily on images as inputs, evaluating its understanding of temporal sequences\nand video content remains a crucial aspect of its overall assessment. This is because real-world\nevents unfold over time, and an AI system’s ability to understand these dynamic processes is\ninstrumental in real-world applications. Capabilities like temporal anticipation, temporal ordering,\ntemporal localization, temporal reasoning, and grounded temporal understanding help to gauge\nthe model’s proficiency in comprehending the sequence of events, anticipating future occurrences,\nand contextually analyzing activities over time, all within a series of static images. In spite of its\nimage-centric focus, GPT-4V is able to comprehend video and temporal sequences in a way that’s\nsimilar to human comprehension. To enhance the versatility and applicability of a sophisticated\nAI model like GPT-4V, this aspect of testing is critical to its development and refinement. For the\nupcoming experiments in this section, we will use multiple selected video frames as inputs to test the\nmodel’s abilities in understanding temporal sequences and video content.\n\n6.1 Multi-image Sequencing\n\nIn this subsection, we demonstrate that GPT-4V can accurately comprehend and analyze sequences\nof video frames. Within this frame-by-frame analysis, GPT-4V recognizes the scene in which the\nactivity is taking place, delivering a deeper contextual understanding. As shown in Figure 53, the\nmodel is not just confined to recognizing the environment; it also accurately interprets the actions\nbeing performed by individuals in the video. GPT-4V understands the sequence and context of\nvarious human poses and intelligently correlates them with the ongoing activity. By understanding\npose variations beyond just identification, GPT-4V can derive meaning from the subtleties of human\nmovement and action. As a result of this level of detailed understanding, GPT-4V can capture\nthe essence of what’s happening in videos, offering rich and nuanced insights that go beyond just\nidentifying objects and scenes.\n\n6.2 Video Understanding\n\nTemporal ordering. Temporal ordering is a crucial element of temporal commonsense and forms\nan essential part of GPT-4V’s capabilities evaluation. This involves providing the model with a\nseries of shuffled images and gauging its ability to discern cause and effect relationships as well\nas time progressions. An understanding of such relationships requires the ability to reorder the\nsequence in a logically coherent and temporally accurate manner. Figure 54 illustrates an example\nof long-term temporal ordering where GPT-4V is presented with a series of shuffled image frames\ndepicting a sushi-making event. Despite the disorder, GPT-4V effectively identifies the event and\ndetermines the appropriate temporal sequence of the sushi-making process. In addition, Figure 55\nprovides an example of short-term temporal ordering. Given a designated action, such as opening\nor closing a door, GPT-4V demonstrates its capability to comprehend the image’s content and\ndetermine the correct sequential order of the events. These examples highlight GPT-4V’s capability\nin temporal commonsense, reinforcing its ability to comprehend both long-term and short-term\nsequences accurately.\n\nTemporal anticipation. We demonstrate GPT-4V’s ability to anticipate future events given a set of\ninitial frames. Long- and short-term examples are used to validate this capacity for anticipating future\nevents. The right side of Figure 56 illustrates GPT-4V’s ability to anticipate short-term events with a\nsoccer penalty kick example. Given the first few frames, it accurately foresees the typical next actions\nof both the kicker and the goalkeeper, due to its understanding of the inherent structure and rules of\nthe game. In addition, as shown in The left side of Figure 56, the sushi preparation sequence illustrates\nGPT-4V’s long-term anticipation capability. By understanding the activity based on visual cues,\nGPT-4V not only recognizes the current progress in sushi preparation but also accurately anticipates\nthe subsequent steps, demonstrating its capacity to interpret and predict complex, multi-step processes\nover an extended period. This combination of short-term and long-term temporal anticipation allows\nGPT-4V to capture and understand activities with varying temporal structures and complexities.\n\nTemporal localization and reasoning. Figure 57 illustrates GPT-4V’s capabilities in temporal\nlocalization and reasoning. It accurately identifies the precise moment the player strikes the ball. Fur-\n\n73\n", "vlm_text": "6 Temporal and Video Understanding \nIn this section, we discuss temporal and video understanding capabilities. Even though GPT- 4V operates primarily on images as inputs, evaluating its understanding of temporal sequences and video content remains a crucial aspect of its overall assessment. This is because real-world events unfold over time, and an AI system’s ability to understand these dynamic processes is instrumental in real-world applications. Capabilities like temporal anticipation, temporal ordering, temporal localization, temporal reasoning, and grounded temporal understanding help to gauge the model’s proficiency in comprehending the sequence of events, anticipating future occurrences, and con textually analyzing activities over time, all within a series of static images. In spite of its image-centric focus, GPT-4V is able to comprehend video and temporal sequences in a way that’s similar to human comprehension. To enhance the versatility and applicability of a sophisticated AI model like GPT-4V, this aspect of testing is critical to its development and refinement. For the upcoming experiments in this section, we will use multiple selected video frames as inputs to test the model’s abilities in understanding temporal sequences and video content. \n6.1 Multi-image Sequencing \nIn this subsection, we demonstrate that GPT-4V can accurately comprehend and analyze sequences of video frames. Within this frame-by-frame analysis, GPT-4V recognizes the scene in which the activity is taking place, delivering a deeper contextual understanding. As shown in Figure  53 , the model is not just confined to recognizing the environment; it also accurately interprets the actions being performed by individuals in the video. GPT-4V understands the sequence and context of various human poses and intelligently correlates them with the ongoing activity. By understanding pose variations beyond just identification, GPT-4V can derive meaning from the subtleties of human movement and action. As a result of this level of detailed understanding, GPT-4V can capture the essence of what’s happening in videos, offering rich and nuanced insights that go beyond just identifying objects and scenes. \n6.2 Video Understanding \nTemporal ordering. Temporal ordering is a crucial element of temporal commonsense and forms an essential part of GPT-4V’s capabilities evaluation. This involves providing the model with a series of shuffled images and gauging its ability to discern cause and effect relationships as well as time progressions. An understanding of such relationships requires the ability to reorder the sequence in a logically coherent and temporally accurate manner. Figure  54  illustrates an example of long-term temporal ordering where GPT-4V is presented with a series of shuffled image frames depicting a sushi-making event. Despite the disorder, GPT-4V effectively identifies the event and determines the appropriate temporal sequence of the sushi-making process. In addition, Figure  55 provides an example of short-term temporal ordering. Given a designated action, such as opening or closing a door, GPT-4V demonstrates its capability to comprehend the image’s content and determine the correct sequential order of the events. These examples highlight GPT-4V’s capability in temporal commonsense, reinforcing its ability to comprehend both long-term and short-term sequences accurately. \nTemporal anticipation. We demonstrate GPT-4V’s ability to anticipate future events given a set of initial frames. Long- and short-term examples are used to validate this capacity for anticipating future events. The right side of Figure  56  illustrates GPT-4V’s ability to anticipate short-term events with a soccer penalty kick example. Given the first few frames, it accurately foresees the typical next actions of both the kicker and the goalkeeper, due to its understanding of the inherent structure and rules of the game. In addition, as shown in The left side of Figure  56 , the sushi preparation sequence illustrates GPT-4V’s long-term anticipation capability. By understanding the activity based on visual cues, GPT-4V not only recognizes the current progress in sushi preparation but also accurately anticipates the subsequent steps, demonstrating its capacity to interpret and predict complex, multi-step processes over an extended period. This combination of short-term and long-term temporal anticipation allows GPT-4V to capture and understand activities with varying temporal structures and complexities. \nTemporal localization and reasoning. Figure  57  illustrates GPT-4V’s capabilities in temporal localization and reasoning. It accurately identifies the precise moment the player strikes the ball. Fur- "}
{"page": 73, "image_path": "doc_images/2309.17421v2_73.jpg", "ocr_text": "Sec. 6.1 Multi-image Sequencing\n\nAction Recognition\n\nPrompt:\n\nExplain the images and the action the person is taking.\n\nGPT-4V:\n\nThe images show a person in a motion capture suit in a studio with multiple cameras and sensors. The person\nis performing a push-up exercise. In the first image, the person is in the starting position with their arms\nextended and body in a straight line. In the second image, the person is lowering their body towards the ground.\nIn the third image, the person is at the bottom of the push-up with their chest close to the ground. In the fourth\nimage, the person is pushing their body back up to the starting position.\n\nFigure 53: Sequences of video frames understanding: Interpreting human poses and deriving relevant\ninsights from video sequences. Check Section 6.1 for detailed discussions.\n\nthermore, GPT-4V showcases its understanding of cause and effect by inferring from the relationship\nbetween the goalkeeper and the ball to determine if the goalkeeper successfully blocks the ball. In\nthe context of the example given, understanding whether the goalkeeper can block the ball involves\nnot only recognizing the spatial positions of the goalkeeper and the ball but also understanding the\ndynamics of their interaction and predicting the outcome of these dynamics. This demonstrates a\nconsiderable level of sophistication in the model’s reasoning abilities.\n\n74\n", "vlm_text": "Sec. 6.1 Multi-image Sequencing \nAction Recognition \nPrompt: Explain the images and the action the person is taking. \nThe image sequence shows a person in a room equipped with motion capture cameras. The person is doing a push-up exercise while wearing a motion capture suit. You can see various cameras and a monitor set up around the room, likely to record and analyze the movements.\nGPT-4V: \nThe images show a person in a motion capture suit in a studio with multiple cameras and sensors. The person  is performing a push-up exercise. In the first image, the person is in the starting position with their arms  extended and body in a straight line. In the second image, the person is lowering their body towards the ground.  In the third image, the person is at the bottom of the push-up with their chest close to the ground. In the fourth  image, the person is pushing their body back up to the starting position.  \nFigure 53: Sequences of video frames understanding: Interpreting human poses and deriving relevant insights from video sequences. Check Section  6.1  for detailed discussions. \nthermore, GPT-4V showcases its understanding of cause and effect by inferring from the relationship between the goalkeeper and the ball to determine if the goalkeeper successfully blocks the ball. In the context of the example given, understanding whether the goalkeeper can block the ball involves not only recognizing the spatial positions of the goalkeeper and the ball but also understanding the dynamics of their interaction and predicting the outcome of these dynamics. This demonstrates a considerable level of sophistication in the model’s reasoning abilities. "}
{"page": 74, "image_path": "doc_images/2309.17421v2_74.jpg", "ocr_text": "Sec. 6.2 Video Understanding\n\nTemporal Ordering\n\nPrompt:\nAnalyze the images and decide the correct order in .\nwhich they should be displayed. Prompt Continued\n\na\n\nThe correct order in which the images should be\ndisplayed is:\n\n. @ - Mixing the rice\n\n. ¢ - Placing the rice on the seaweed\n. b - Adding the fillings\n\n. a - Cutting the roll\n\n. d - Finished sushi rolls on a plate.\n\nFigure 54: Long-term temporal ordering: GPT-4V is presented with shuffled image frames depicting\na sushi-making event. While the sushi-making process is disordered, GPT-4V is able to identify the\nevent and determine the correct temporal sequence. Check Section 6.2 for detailed discussions.\n\n75\n", "vlm_text": "Sec. 6.2 Video Understanding \nTemporal Ordering \nPrompt: Analyze the images and decide the correct order in  which they should be displayed. \nPrompt Continued \nThe image is a series of photographs illustrating the process of making sushi. Here is a breakdown of each step and the corresponding images in order:\n\n1. **e - Mixing the rice:** This shows the preparation stage where rice is being mixed, possibly seasoning it with vinegar, sugar, and salt.\n2. **c - Placing the rice on the seaweed:** Sushi rice is being spread on a sheet of seaweed (nori) as the initial step of sushi rolling.\n3. **b - Adding the fillings:** Ingredients such as fish and vegetables are placed on top of the rice as fillings for the sushi roll.\n4. **a - Cutting the roll:** The rolled sushi, wrapped in seaweed, is being sliced into smaller pieces with a knife on a cutting board.\n5. **d - Finished sushi rolls on a plate:** The final product is displayed with neatly arranged sushi rolls served on a plate, ready to eat.\nFigure 54: Long-term temporal ordering: GPT-4V is presented with shuffled image frames depicting a sushi-making event. While the sushi-making process is disordered, GPT-4V is able to identify the event and determine the correct temporal sequence. Check Section  6.2  for detailed discussions. "}
{"page": 75, "image_path": "doc_images/2309.17421v2_75.jpg", "ocr_text": "Sec. 6.2 Video Understanding\n\nTemporal Ordering\n\nPrompt: Prompt:\n\nFor opening a door, decide the correct order in which For closing a door, decide the correct order in which\nthey should be displayed. they should be displayed.\n\na a\n\nGPT-4V: GPT-4V:\n\nThe correct order would be: Correct order:\n\n1. a- Person approaching the door 1) c - Person standing in doorway\n2. b - Person opening the door 2) b - Person starting to close door\n3. c - Person entering through the door. 3) a - Person closing door completely\n\nFigure 55: Short-term temporal ordering: given a specified action, such as opening or closing a door,\nGPT-4V demonstrates its capability to comprehend the images’ content and determine the correct\nsequential order corresponding to the specified action. Check Section 6.2 for detailed discussions.\n\n76\n", "vlm_text": "Sec. 6.2 Video Understanding \nTemporal Ordering \nPrompt: For opening a door, decide the correct order in which  they should be displayed. \nThe image sequence shows three frames labeled a, b, and c. It depicts a scene where a person, seen from the back, knocks on a door. In frames b and c, another person opens the door and appears to engage in a conversation with the first person. The setting seems to be indoors, possibly in a hallway or doorway.\nGPT-4V: The correct order would be: \n\n \n1. a - Person approaching the door \n\n 2. b - Person opening the door \n\n 3. c - Person entering through the door.   \nPrompt: For closing a door, decide the correct order in which  they should be displayed. \nThe image is a sequence of three frames labeled a, b, and c. In the first frame (a), a person wearing a brown and white shirt is standing in front of a door, suggesting they may be knocking or waiting. In the second frame (b), the door appears to be slightly open with some visible light from inside, and the person outside the door is still present. In the third frame (c), the door is more open, allowing more light to reveal part of the interior and the person standing outside the door continues to face inward.\nGPT-4V: Correct order:  \n1) c - Person standing in doorway  2) b - Person starting to close door  3) a - Person closing door completely \nFigure 55: Short-term temporal ordering: given a specified action, such as opening or closing a door, GPT-4V demonstrates its capability to comprehend the images’ content and determine the correct sequential order corresponding to the specified action. Check Section  6.2  for detailed discussions. "}
{"page": 76, "image_path": "doc_images/2309.17421v2_76.jpg", "ocr_text": "Sec. 6.2 Video Understanding\n\nTemporal Anticipation\n\nPrompt: Prompt:\nPredict what will happen next based on the images. Analyze the images and decide what the next steps\nshould be.\n\nGPT-4V:\nBased on the images, it appears that the player in The images show the process of making sushi rolls.\nthe white jersey is taking a penalty kick. The next The first image shows rice being mixed with vinegar\nlikely event is that the player will kick the ball in a bowl. The second image shows the rice being\ntowards the goal, attempting to score. The spread onto a sheet of seaweed. The third image\ngoalkeeper in the black jersey will try to block the shows the addition of salmon and cucumber to the\nshot. roll. The next steps would be to roll the sushi using a\nbamboo mat, cut the roll into smaller pieces, and\nserve with soy sauce and wasabi.\n\nFigure 56: Short-term and long-term temporal anticipation: GPT-4V captures and understands activi-\nties with varying temporal structures and complexities. Check Section 6.2 for detailed discussions.\n\n77\n", "vlm_text": "Sec. 6.2 Video Understanding \nTemporal Anticipation \nPrompt: \nPredict what will happen next based on the images. \nThe image shows three sequential frames of a soccer match where a player is taking a penalty kick. In the first frame, the player is approaching the ball to take the penalty. In the second frame, the player is about to strike the ball, and the goalkeeper is prepared to react. In the third frame, the ball has been kicked and is heading towards the goal, with the goalkeeper diving to attempt a save. The players on the pitch are wearing maroon and white kits.\nGPT-4V: \nBased on the images, it appears that the player in  the white jersey is taking a penalty kick. The next  likely event is that the player will kick the ball  towards the goal, attempting to score. The  goalkeeper in the black jersey will try to block the  shot.  \nPrompt: \nAnalyze the images and decide what the next steps  should be. \nThe image shows a step-by-step process of making sushi rolls. The first part of the image depicts someone mixing rice in a bowl. The second part shows rice placed on a sheet of seaweed (nori) on top of a bamboo sushi mat. The third part illustrates additional sushi ingredients, like sliced imitation crab meat and cucumber, being added on top of the rice and seaweed, getting ready to be rolled into a sushi roll.\nGPT-4V: \nThe images show the process of making sushi rolls.  The first image shows rice being mixed with vinegar  in a bowl. The second image shows the rice being  spread onto a sheet of seaweed. The third image  shows the addition of salmon and cucumber to the  roll.  The next steps would be to roll the sushi using a  bamboo mat, cut the roll into smaller pieces, and  serve with soy sauce and wasabi.  \nFigure 56: Short-term and long-term temporal anticipation: GPT-4V captures and understands activi- ties with varying temporal structures and complexities. Check Section  6.2  for detailed discussions. "}
{"page": 77, "image_path": "doc_images/2309.17421v2_77.jpg", "ocr_text": "Sec. 6.2 Video Understanding\n\nTemporal Localization and Reasoning\n\nPrompt:\n\nIn which frame does the player kick the ball? Did\nthe goalkeeper block the ball?\n\na\n\nPrompt Continued\n\nGPT-4V:\n\nThe player kicks the ball at frame c. The ball was not\nblocked by the goalkeeper as it can be seen in frame f\nthat the ball is in the net and the goalkeeper is on the\nground.\n\nFigure 57: Temporal localization and reasoning: GPT-4V shows the capability in temporal localization\nby accurately identifying when the player strikes the ball. It also demonstrates cause-and-effect\nreasoning by determining whether the ball was blocked based on the goalkeeper-ball interaction.\nCheck Section 6.2 for detailed discussions.\n\n78\n", "vlm_text": "Temporal Localization and Reasoning  \nThe image presents a series of frames (a through f) capturing a moment in a soccer game where the player kicks the ball towards the goal. The caption describes GPT-4V's ability to determine the moment the player kicks the ball and whether the goalkeeper blocks the shot. The response from GPT-4V is also included, explaining that the player kicks the ball in frame c, and the ball is not blocked by the goalkeeper, as shown by the ball in the net and the goalkeeper on the ground in frame f."}
{"page": 78, "image_path": "doc_images/2309.17421v2_78.jpg", "ocr_text": "Sec. 6.3 Video Visual Referring Prompting\n\nGrounded Temporal Understanding\n\nPrompt: Prompt:\n\nDescribe and summarize what the circled person Summarize the activities of the person.\n\nNe\n\ni\n\nGPT-4V: GPT-4V:\n\nThe circled person is a man who is standing next to The circled person is sitting on a bench and then\na bench and appears to be in a conversation with stands up and adjusts his shirt.\n\nanother man who is sitting on the bench. In the\n\nsecond image, the circled man is pointing towards\n\nsomething and in the third image, he is playfully\n\npretending to punch the other man. Overall, it\n\nseems like the circled man is engaging in a friendly\n\nconversation and joking around with the other man.\n\nFigure 58: Grounded temporal understanding: GPT-4V can apply a temporal understanding to a\nspecific person of interest, indicated by a circle. Check Section 6.3 for detailed discussions.\n\n6.3 Visual Referring Prompting for Grounded Temporal Understanding\n\nSection 5 illustrates GPT-4V’s capabilities in visual referring prompting. In this section, we aim\nto extend this capability by testing visual referring prompting for temporal understanding. This\nadvancement offers enhanced control over video comprehension tasks.\n\nGrounded temporal understanding. Grounded temporal understanding forms another crucial aspect\nof GPT-4V’s capabilities, which we explore using pointing input in a sequence of image frames.\nFigure 58 exemplifies this by demonstrating how GPT-4V can apply a temporal understanding to a\nspecific person of interest, indicated by a circle. GPT-4V can accurately describe events in a way\nthat aligns with the corresponding temporal order, focusing on the activities of the circled individual.\nBeyond this, GPT-4V demonstrates a more refined understanding of the event, recognizing the nature\nof the interactions. For instance, GPT-4V can distinguish between friendly interactions and violent\nincidents, illustrating an ability to not only comprehend the temporal flow of events but also to\ninterpret the tone and nature of the interactions taking place. This indicates GPT-4V’s capacity to\nprocess and comprehend complex temporal and social cues within a given sequence, adding a layer\nof depth to its understanding.\n\n79\n", "vlm_text": "Sec. 6.3 Video Visual Referring Prompting \nGrounded Temporal Understanding \n\nDescribe and summarize what the circled person  doing? \nThe image sequence shows two men interacting near a bench in a park-like setting. The man who is circled is:\n\n1. Standing and talking to another man sitting on the bench.\n2. Pointing towards something as he talks.\n3. Playfully pretending to punch the seated man, indicating a friendly and joking interaction.\nPrompt: Summarize the activities of the person. \nThe image sequence depicts a person sitting on a bench, standing up, and then adjusting their shirt. The setting appears to be an outdoor area with trees and hills in the background. Another person is standing nearby, observing the actions of the circled individual.\nFigure 58: Grounded temporal understanding: GPT-4V can apply a temporal understanding to a specific person of interest, indicated by a circle. Check Section  6.3  for detailed discussions. \n6.3 Visual Referring Prompting for Grounded Temporal Understanding \nSection  5  illustrates GPT-4V’s capabilities in visual referring prompting. In this section, we aim to extend this capability by testing visual referring prompting for temporal understanding. This advancement offers enhanced control over video comprehension tasks. \nGrounded temporal understanding.  Grounded temporal understanding forms another crucial aspect of GPT-4V’s capabilities, which we explore using pointing input in a sequence of image frames. Figure  58  exemplifies this by demonstrating how GPT-4V can apply a temporal understanding to a specific person of interest, indicated by a circle. GPT-4V can accurately describe events in a way that aligns with the corresponding temporal order, focusing on the activities of the circled individual. Beyond this, GPT-4V demonstrates a more refined understanding of the event, recognizing the nature of the interactions. For instance, GPT-4V can distinguish between friendly interactions and violent incidents, illustrating an ability to not only comprehend the temporal flow of events but also to interpret the tone and nature of the interactions taking place. This indicates GPT-4V’s capacity to process and comprehend complex temporal and social cues within a given sequence, adding a layer of depth to its understanding. "}
{"page": 79, "image_path": "doc_images/2309.17421v2_79.jpg", "ocr_text": "7 Abstract Visual Reasoning and Intelligence Quotient Test\n\nUnderstanding and reasoning over abstract visual stimuli and symbols is one fundamental ability for\nhuman intelligence. This section examines if GPT-4V can abstract semantics from visual signals and\ncan perform different types of human Intelligence Quotient (IQ) tests.\n\n7.1 Abstract Visual Stimuli\n\nHumans can infer semantics from abstract and often ambiguous visual stimuli. Figure 59 explores\nhaving GPT-4V interpret tangram [31, 97, 43, 59]. A tangram is a traditional geometric puzzle that\nconsists of seven flat pieces called tans, which are put together to form shapes without overlapping\nthe pieces. For example, GPT-4V interprets that sub-figure 7 in Figure 59 best illustrates a flying\ngoose and provides reasoning descriptions for other sub-figure, e.g., 4. person or robot, 9. boat or\nhat, and 10. dog or fox. GPT-4V also has the ability to understand other formats of abstract visual\ndiagrams [127, 16, 150], such as ASCII text art of cartoon characters in Figure 59 and symbolic\ninputs in Figures 61-62.\n\n7.2 Discovery and Association of Parts and Objects\n\nDiscovering and associating object parts [139, 44] is another important abstract visual reasoning\ncapability. Humans can easily discover how object parts may compose a semantically meaningful\nobject. Figure 60 designs examples to probe GPT-4V’s capability in associating object parts. In the\nleft example, we ask GPT-4V to localize an object part based on its semantic meaning. In the right\nexample, GPT-4V is asked to associate object parts segmented by SAM [65]. GPT-4V can process\nfigures for all object parts and associate them in a semantically meaningful to form the boy visualized\nin the bottom right.\n\n7.3 Wechsler Adult Intelligence Scale\n\nSection 7.1 demonstrates the abstract visual understanding capability of GPT-4V. As a further\nchallenge, GPT-4V is asked to perform different abstract reasoning tasks, sourced from human\nIntelligence Quotient (IQ) tests. The Wechsler Adult Intelligence Scale [133] is recognized as one\nof the “gold standard IQ tests,” and is designed to provide a comprehensive measurement of an\nindividual’s cognitive abilities using a series of sub-tests. Figure 61 shows representative questions\nand GPT-4V’s outputs from each sub-test category. GPT-4V shows promises in abstract reasoning,\nanswering questions with texts only, symbolic visual inputs, and natural images. For example,\nthe bottom right sample shows that GPT-4V can interpret the analogy question and find the best\ncomparison of shoes.\n\n7.4 Raven’s Progressive Matrices\n\nRaven’s Progressive Matrices (RPM) [109] is another well-known non-verbal intelligence test devel-\noped to measure abstract reasoning and problem-solving abilities. The test is designed to minimize\nthe influence of language, culture, and formal education on test performance, making it suitable\nfor testing AI models [16, 150, 55]. Each test sample contains three or eight images, arranged in\n2-by-2 or 3-by-3 matrices with one figure missing. The goal is to select the next image from multiple\ncandidate images by identifying patterns in the provided samples. In our approach, we challenge\nGPT-4V by sending the entire question page as a single image, instead of converting it into interleaved\nimage-text pairs, similar to the human approach to IQ tests. As shown in Figure 62, GPT-4V can\ngenerate reasonable answers without processed text descriptions or sub-figures. However, we also\nnotice that breaking down the entire question image into interleaved text and sub-figures, such as in\nFigure 63, does simplify the task and let GPT-4V produce more reliable answers.\n\n80\n", "vlm_text": "7 Abstract Visual Reasoning and Intelligence Quotient Test \nUnderstanding and reasoning over abstract visual stimuli and symbols is one fundamental ability for human intelligence. This section examines if GPT-4V can abstract semantics from visual signals and can perform different types of human Intelligence Quotient (IQ) tests. \n7.1 Abstract Visual Stimuli \nHumans can infer semantics from abstract and often ambiguous visual stimuli. Figure  59  explores having GPT-4V interpret tangram [ 31 ,  97 ,  43 ,  59 ]. A tangram is a traditional geometric puzzle that consists of seven flat pieces called tans, which are put together to form shapes without overlapping the pieces. For example, GPT-4V interprets that sub-figure 7 in Figure  59  best illustrates a flying goose and provides reasoning descriptions for other sub-figure,  e.g ., 4. person or robot, 9. boat or hat, and 10. dog or fox. GPT-4V also has the ability to understand other formats of abstract visual diagrams [ 127 ,  16 ,  150 ], such as ASCII text art of cartoon characters in Figure  59  and symbolic inputs in Figures  61 - 62 . \n7.2 Discovery and Association of Parts and Objects \nDiscovering and associating object parts [ 139 ,  44 ] is another important abstract visual reasoning capability. Humans can easily discover how object parts may compose a semantically meaningful object. Figure  60  designs examples to probe GPT-4V’s capability in associating object parts. In the left example, we ask GPT-4V to localize an object part based on its semantic meaning. In the right example, GPT-4V is asked to associate object parts segmented by SAM [ 65 ]. GPT-4V can process figures for all object parts and associate them in a semantically meaningful to form the boy visualized in the bottom right. \n7.3 Wechsler Adult Intelligence Scale \nSection  7.1  demonstrates the abstract visual understanding capability of GPT-4V. As a further challenge, GPT-4V is asked to perform different abstract reasoning tasks, sourced from human Intelligence Quotient (IQ) tests. The Wechsler Adult Intelligence Scale [ 133 ] is recognized as one of the “gold standard IQ tests,” and is designed to provide a comprehensive measurement of an individual’s cognitive abilities using a series of sub-tests. Figure  61  shows representative questions and GPT-4V’s outputs from each sub-test category. GPT-4V shows promises in abstract reasoning, answering questions with texts only, symbolic visual inputs, and natural images. For example, the bottom right sample shows that GPT-4V can interpret the analogy question and find the best comparison of shoes. \n7.4 Raven’s Progressive Matrices \nRaven’s Progressive Matrices (RPM) [ 109 ] is another well-known non-verbal intelligence test devel- oped to measure abstract reasoning and problem-solving abilities. The test is designed to minimize the influence of language, culture, and formal education on test performance, making it suitable for testing AI models [ 16 ,  150 ,  55 ]. Each test sample contains three or eight images, arranged in 2-by-2 or 3-by-3 matrices with one figure missing. The goal is to select the next image from multiple candidate images by identifying patterns in the provided samples. In our approach, we challenge GPT-4V by sending the entire question page as a single image, instead of converting it into interleaved image-text pairs, similar to the human approach to IQ tests. As shown in Figure  62 , GPT-4V can generate reasonable answers without processed text descriptions or sub-figures. However, we also notice that breaking down the entire question image into interleaved text and sub-figures, such as in Figure  63 , does simplify the task and let GPT-4V produce more reliable answers. "}
{"page": 80, "image_path": "doc_images/2309.17421v2_80.jpg", "ocr_text": "Sec. 7.1 Abstract Visual Stimuli\n\nPrompt:\n\nAtangram is a geometric puzzle that consists of\nseven flat pieces to form shapes. Here are ten\nabstract tangram. Which one best illustrates a\n\nflying goose? What do other figures look like? eB SS)\n\nGPT-4V:\n\nThe tangram that best illustrates a flying goose is number\n7. It has the triangular shape of a goose's body and\nwings, and the smaller triangle at the top could represent\nthe head and neck.\n\nThe other figures could represent various things:\n\n1. Arocket or a tower\n\n2. Abutterfly or a bat\n\n3. A question mark or a snake\n\n4. Aperson or a robot\n\n5. Amountain range or a crown\n\n6. Ahouse or a temple\n\n7. A flying goose (as mentioned above)\n\n8. A bird or a fish\n\n9. A boat or a hat\n\n10. Adog or a fox.\n\nA\nLad\nPa\nEa\nio\nrt\n\nHere is an abstract figure. Can you identify how many characters are there, and the cartoon from which they\n\nGPT-4V:\nThere are four characters in this abstract figure. They originate from the cartoon \"South Park.\" The characters\nare Eric Cartman, Kyle Broflovski, Stan Marsh, and Kenny McCormick.\n\nFigure 59: Understanding abstract visual stimuli such as tangram [59] and ASCII text art. Check\nSection 7.1 for detailed discussions.\n\n81\n", "vlm_text": "Sec. 7.1 Abstract Visual Stimuli \nPrompt: \nA tangram is a geometric puzzle that consists of  seven flat pieces to form shapes. Here are ten  abstract tangram. Which one best illustrates a  flying goose? What do other figures look like? \nThe image consists of six shapes, each composed of a set of geometric pieces arranged within six separate squares. These shapes resemble puzzles created using tangram pieces, which include triangles, a square, and a parallelogram. Each shape is unique:\n\n1. The first shape resembles an arrow pointing upwards.\n2. The second shape resembles a star.\n3. The third shape resembles a question mark.\n4. The fourth shape resembles a person standing upright.\n5. The fifth shape resembles a cat's face with pointed ears on top.\n6. The sixth shape resembles a house with a triangular roof.\nThe image shows four tangram puzzles, each with a different arrangement of shapes. They're numbered 7 to 10. Tangrams are puzzles consisting of flat pieces called tans, which are put together to form shapes. Each puzzle consists of these geometric shapes arranged to form figures or patterns.\nGPT-4V: \nThe tangram that best illustrates a flying goose is number  7. It has the triangular shape of a goose's body and  wings, and the smaller triangle at the top could represent  the head and neck. The other figures could represent various things:\n\n 1. A rocket or a tower\n\n 2. A butterfly or a bat\n\n 3. A question mark or a snake\n\n 4. A person or a robot\n\n 5. A mountain range or a crown\n\n 6. A house or a temple\n\n 7. A flying goose (as mentioned above)\n\n 8. A bird or a fish\n\n 9. A boat or a hat\n\n 10. A dog or a fox. \nPrompt: \nHere is an abstract figure. Can you identify how many characters are there, and the cartoon from which they  originate? \nThe image is an ASCII art representation of the four main characters from the animated television show \"South Park.\" From left to right, the characters are Stan Marsh, Kyle Broflovski, Kenny McCormick, and Eric Cartman.\nGPT-4V: \nThere are four characters in this abstract figure. They originate from the cartoon \"South Park.\" The characters  are Eric Cartman, Kyle Broflovski, Stan Marsh, and Kenny McCormick. \nFigure 59: Understanding abstract visual stimuli such as tangram [ 59 ] and ASCII text art. Check Section  7.1  for detailed discussions. "}
{"page": 81, "image_path": "doc_images/2309.17421v2_81.jpg", "ocr_text": "Sec. 7.2 Association of Parts and Objects\n\nDiscovery and Association of Parts and Objects\n\nPrompt:\n\nAtangram is a geometric puzzle that consists of seven\nflat pieces to form shapes. Locate the referred object\nand represent the location of the region. Regions are\nrepresented by (x1,y1,x2,y2) coordinates. x1 x2 are\nthe left and right most positions, normalized into 0 to 1,\nwhere 0 is the left and 1 is the right. y1 y2 are the top\nand bottom most positions, normalized into 0 to 1,\nwhere 0 is the top and 1 is the bottom. For example,\nthe (x1,y1,x2,y2) coordinates for that region with the\nbeer bottle is (0.47, 0.48, 0.55, 0.87).\n\n0.47 0.55\nThe image below that shows a flying goose. Which\npart represents the head of the bird?\nWhat is the (x1,y1,x2,y2) coordinates for the region\nwith the head of the bird?\n\nGPT-4V:\n\nThe head of the bird is represented by the small\ntriangle at the top of the image. The (x1,y1,x2,y2)\ncoordinates for the region with the head of the bird\nwould be approximately (0.42, 0.15, 0.58, 0.35).\n\nPlot Visualization:\nRefer to the query image below\n\nGrounded VQA\n\nPrompt:\nWhat is the highlighted part?\n\n0.42 0.58\nGPT-4V:\nThe highlighted part is the head of the goose.\n\nPrompt:\n\nHow are those visual parts related, and can they be\ncombined to form a single object such as a boy? If so,\nhow to arrange them?\n\nGPT-4V:\n\nThe three visual parts are related in that they are all\ncomponents that could be used to create a single\nobject. In this case, the object is a boy.\n\nTo combine these parts to form a boy, we can arrange\nthem in the following way:\n\n- The first image (the cardboard with orange and\ngreen shapes) could be used as the body of the boy.\n- The second image (the pair of jeans) could be used\nas the legs of the boy.\n\n- The third image (the cardboard with yellow, red, and\nblue shapes) could be used as the head of the boy.\nBy arranging these parts in this way, we can create a\nsingle object that resembles a boy.\n\nPlot Visualization:\n\nFigure 60: Understanding part-object association in abstract and natural images. Check Section 7.2\nfor detailed discussions.\n\n82\n", "vlm_text": "Sec. 7.2 Association of Parts and Objects \nDiscovery and Association of Parts and Objects \nPrompt: \nA tangram is a geometric puzzle that consists of seven  flat pieces to form shapes. Locate the referred object  and represent the location of the region. Regions are  represented by   $(\\mathsf{x}1,\\mathsf{y}1,\\mathsf{x}2,\\mathsf{y}2)$   coordinates. x1  $\\times2$   are  the left and right most positions, normalized into 0 to 1,  where 0 is the left and 1 is the right. y1 y2 are the top  and bottom most positions, normalized into 0 to 1,  where 0 is the top and 1 is the bottom. For example,  the  $(\\mathsf{x}1,\\mathsf{y}1,\\mathsf{x}2,\\mathsf{y}2)$  ) coordinates for that region with the  beer bottle is (0.47, 0.48, 0.55, 0.87). \nThe image appears to be of an outdoor dining setup with tables and a bottle in the center. There are no visible geese or birds in this image. \n\nThe $(\\mathsf{x}1,\\mathsf{y}1,\\mathsf{x}2,\\mathsf{y}2)$ coordinates you mentioned do not correspond to any bird's head, as there isn't one in the picture.\nThis image shows a tangram puzzle arranged in the shape of a bird. Tangrams are made up of seven geometric pieces called tans, which are used to form various shapes and figures.\nGPT-4V: \nThe head of the bird is represented by the small  triangle at the top of the image. The   $(\\mathsf{x}1,\\mathsf{y}1,\\mathsf{x}2,\\mathsf{y}2)$    coordinates for the region with the head of the bird  would be approximately (0.42, 0.15, 0.58, 0.35). \nPlot Visualization: Refer to the query image below \nGrounded VQA \nPrompt: What is the highlighted part?  \nThe image shows a tangram puzzle arranged to resemble a bird-like figure. It includes geometric shapes like triangles and parallelograms. An area at the top is highlighted with a red box. There are numerical markers on the side, 0.15 and 0.35, possibly indicating positions or dimensions.\nGPT-4V: \nThe highlighted part is the head of the goose. \nPrompt: \nHow are those visual parts related, and can they be  combined to form a single object such as a boy? If so,  how to arrange them? \nThis image is a collage of three separate sections:\n\n1. The top section shows a piece of cardboard decorated with geometric shapes, resembling the pattern of a face or mask. The shapes include orange, yellow, blue, and green elements, and it appears as a creative art piece or craft.\n\n2. The middle section depicts a pair of jeans worn by a person, but with the rest of the body (the upper body and feet) outside the frame, giving the impression of disembodied legs.\n\n3. The bottom section shows a different angle of a similar cardboard piece from the first section, tilted and with decorative shapes like an artistic depiction of a face or expression.\n\nThe overall arrangement and artistic elements suggest a playful or abstract theme, possibly involving art or a costume.\nGPT-4V: \nThe three visual parts are related in that they are all  components that could be used to create a single  object. In this case, the object is a boy. To combine these parts to form a boy, we can arrange  them in the following way: - The first image (the cardboard with orange and  green shapes) could be used as the body of the boy.  - The second image (the pair of jeans) could be used  as the legs of the boy.  - The third image (the cardboard with yellow, red, and  blue shapes) could be used as the head of the boy.  By arranging these parts in this way, we can create a  single object that resembles a boy. \nPlot Visualization: \nThe image shows a person dressed in a homemade robot costume made out of cardboard and colorful paper. The costume consists of two large cardboard boxes: one for the body and one for the head. The head box is decorated with facial features like eyes, a mouth, and a nose made from colorful paper cutouts. The body box also has colorful paper decorations on it. The person is wearing jeans and standing with their arms slightly out to the sides.\nFigure 60: Understanding part-object association in abstract and natural images. Check Section  7.2 for detailed discussions. "}
{"page": 82, "image_path": "doc_images/2309.17421v2_82.jpg", "ocr_text": "Sec. 7.3 Wechsler Adult Intelligence Scale\n\nPrompt: Prompt:\n\nWhich one of the five is least like the Tom, twelve years old, is three times as old as his sister. How old will\nother four? Tom be when he is twice as old as his sister?\n\na. Carrot a. 18\n\nb. Potato b. 21\n\nc. Corn c. 16\n\nd. Apple d. 20\n\ne. Bean e. 15\n\nGPT-4V: GPT-4V:\n\nd. Apple If Tom is three times as old as his sister, then his sister is one-third of\n\nExplanation: Carrot, potato, corn, and his age. So, his sister is 4 years old. Now, we need to figure out when\n\nbean are all vegetables, while apple is Tom will be twice as old as his sister. This means that his sister's age\nwill be half of his age. So, we need to find a number that is double the\nage of 4. The answer is 8. So, Tom will be 16 years old when he is\ntwice as old as his sister. The correct answer is c. 16.\n\nWhich one of the five is least like the\nWhich one of the five designs makes the best comparison?\n\nother four?\na.\na.\nb.\n@\nd.\ne.\nGPT-4V:\n\nAnswer: d. is the least like the other\nfour because it is the only one that\ndoes not have a central point\nconnecting all the other points.\n\nAnswer: c. The black dress shoe\n\nExplanation: The analogy is comparing a hand to a glove and a foot to\na shoe. The best comparison would be the black dress shoe as it is\nan item of clothing that is worn on the foot, just like a glove is worn on\nthe hand.\n\nFigure 61: Example questions from the Wechsler Adult Intelligence Scale (WAIS) [133]. Check\n\nSection 7.3 for detailed discussions.\n83\n", "vlm_text": "Sec. 7.3 Wechsler Adult Intelligence Scale \nPrompt: \nPrompt: \nTom, twelve years old, is three times as old as his sister. How old will  Tom be when he is twice as old as his sister? \nWhich one of the five is least like the  other four? a. Carrot b. Potato c. Corn d. Apple e. Bean \na. 18 b. 21 c. 16 d. 20 e. 15 \nGPT-4V: \nGPT-4V: \nIf Tom is three times as old as his sister, then his sister is one-third of  his age. So, his sister is 4 years old. Now, we need to figure out when  Tom will be twice as old as his sister. This means that his sister's age  will be half of his age. So, we need to find a number that is double the  age of 4. The answer is 8. So, Tom will be 16 years old when he is  twice as old as his sister. The correct answer is c. 16. \nd. Apple Explanation: Carrot, potato, corn, and  bean are all vegetables, while apple is  a fruit. \nPrompt: \nWhich one of the five is least like the  other four? \nThe image contains five different abstract symbols labeled from a to e. \n\n- **a.** A central diamond shape with four arms extending outwards, each ending in a rectangular shape.\n- **b.** A Y-shaped symbol with a central vertical arm splitting into three branches, each ending in a triangular shape.\n- **c.** Similar to symbol a, but each arm ends in a square shape.\n- **d.** A vertical arrangement of three circles connected by lines, with additional arms at the sides, each ending in a circle.\n- **e.** Similar to symbol b, a Y-shaped structure with a central vertical arm splitting into branches, each branch finishing with a triangular shape.\n\nThese geometric designs vary in the shapes and arrangements of their components.\nGPT-4V: \nAnswer: d.  is the least like the other  four because it is the only one that  does not have a central point  connecting all the other points. \nPrompt: Which one of the five designs makes the best comparison? \nThe image shows an analogy puzzle. The comparison is between a hand and a glove, and a foot and an item of clothing. The options are:\n\na. Cap\nb. Soccer ball\nc. Shoe\nd. T-shirt\ne. Shorts\n\nThe correct answer is c. Shoe, as a glove is to a hand as a shoe is to a foot.\nGPT-4V: \nAnswer: c. The black dress shoe Explanation: The analogy is comparing a hand to a glove and a foot to  a shoe. The best comparison would be the black dress shoe as it is  an item of clothing that is worn on the foot, just like a glove is worn on  the hand. "}
{"page": 83, "image_path": "doc_images/2309.17421v2_83.jpg", "ocr_text": "Sec. 7.4 Raven's Progressive Matrices\n\nRPM: Entire Question Page\n\nPrompt:\nFind out the pattern in the first column and use it to infer the missing figure in the second column.\n\n5. Choose a drawing that fits the empty space:\n\nThe correct drawing that fits the empty space is the upside-down triangle. This is because the pattern in the grid is\nthat each row and column has one of each shape in different orientations.\n\nFind out the pattern in the first two rows and use it to infer the missing figure in the third row.\n\nFigure out which figure of A to H that should logically replace the missing figure/question-mark.\nNE?\n\nA\naN\n\nGPT-4V:\n\nThe pattern in the first two rows is that the figures are rotated 90 degrees clockwise in each subsequent cell.\nTherefore, the missing figure in the third row should be a rotated version of the figure in the previous cell.\nLooking at the options, the correct answer is OC, as it is the figure that is rotated 90 degrees clockwise from the\nfigure in the previous cell.\n\nFigure 62: Example questions from the Raven’s Progressive Matrices [109, 55]. We challenge\nGPT-4V by sending the entire question page as a single image, mimicking how humans look at the\nIQ tests. Check Section 7.4 for detailed discussions.\n\n84\n", "vlm_text": "Sec. 7.4 Raven's Progressive Matrices \nRPM: Entire Question Page \nPrompt: \nFind out the pattern in the first column and use it to infer the missing figure in the second column. \nThe image is a puzzle asking you to choose a drawing that fits the empty space in a 2x2 grid. The top row of the grid displays a semicircle on the left and a triangle on the right, while the bottom row has a semicircle on the left and an empty space marked with a question mark on the right. Below the grid, there are six shape options to choose from: a circle, an upside-down triangle, a diamond, a triangle, a semicircle facing right, and a semicircle facing left.\n\nThe sequence appears to alternate between semicircles and triangles comparing across rows. The correct answer, following the pattern displayed, is most likely the triangle, which is the fourth option from the left in the row of choices.\nGPT-4V: The correct drawing that fits the empty space is the upside-down triangle. This is because the pattern in the grid is  that each row and column has one of each shape in different orientations. \nPrompt: Find out the pattern in the first two rows and use it to infer the missing figure in the third row. \n\nThe image is a puzzle with a 3x3 grid containing arrows. Eight arrows are given, and one position is marked with a question mark. Below the grid, there are eight options, labeled A to H, each showing a different arrow direction.\n\nTo solve the puzzle, look for a pattern in the grid:\n\n- Each row seems to follow a rotation pattern for the arrows.\n- Row 1: Right, Down-right, Down-left\n- Row 2: Left, Up-right, ?\n- Row 3: Up-left, Down, Right\n\nBy examining the patterns:\n- The arrows in each row seem to follow a consistent clockwise rotation.\n- The missing arrow in Row 2 should be one that logically completes this pattern, which is \"Up.\"\n\nTherefore, the arrow in option G fits the pattern and should replace the question mark.\nGPT-4V: \nThe pattern in the first two rows is that the figures are rotated 90 degrees clockwise in each subsequent cell.  Therefore, the missing figure in the third row should be a rotated version of the figure in the previous cell.  Looking at the options, the correct answer is OC, as it is the figure that is rotated 90 degrees clockwise from the  figure in the previous cell. \nFigure 62: Example questions from the Raven’s Progressive Matrices [ 109 ,  55 ]. We challenge GPT-4V by sending the entire question page as a single image, mimicking how humans look at the IQ tests. Check Section  7.4  for detailed discussions. "}
{"page": 84, "image_path": "doc_images/2309.17421v2_84.jpg", "ocr_text": "Sec. 7.4 Raven's Progressive Matrices\n\nRPM: Processed Sub-figures\nPrompt:\n\nnIGIGl\n\n»\n\nHere are three sub images arranged in a 2-by-2\nmatrix. Figure out which figure of a to f that should\nlogically replace the missing figure. What would be\nthe next image to complete the matrix?\n\n=\n=\n=\n=\ng\nIV]\n\nThe first image in the first column, image 1, is a\ntriangle. The first image in the second column,\nimage 3, is a triangle with a line through it. The\nsecond image in the first column, image 2, is a\ndiamond. Therefore, the second image in the\nsecond column, image 4 should continue this\npattern and be a diamond with a line through it.\nOption b is the only image that fits this pattern, as it\nis a diamond with a line through it. Therefore, the\ncorrect answer is b.\n\nFigure 63: Instead of sending the entire question page as a single image, we may also process the\nimage into multiple sub-figures and optionally provide detailed instructions and examples to further\n\nPrompt Continued\n\nrN\n\nHere are three sub images arranged in a 2-by-2\nmatrix. Figure out which figure of a to f that should\nlogically replace the missing figure. What would be\nthe next image to complete the matrix?\n\nKILI Siti\n\nQ\nvu\n7\n&\n<\n\nThe first image in the first column, image 1, is a\ndiamond. The first image in the second column,\nimage 3, is a diamond with a dot in the middle. The\nsecond image in the first column, image 2, is a star.\nTherefore, the second image in the second column,\nimage 4 should continue this pattern and be a star\nwith a dot in the middle. Option c is the only image\nthat fits this pattern, as it is a star with a dot in the\nmiddle. Therefore, the correct answer is c.\n\nboost the answer accuracy. Check Section 7.4 for detailed discussions.\n\n85\n", "vlm_text": "Sec. 7.4 Raven's Progressive Matrices \nRPM: Processed Sub-figures \nPrompt: \nThe image shows three panels with different geometric symbols:\n\n1. An inverted triangle in a box.\n2. A diamond shape in a box.\n3. An inverted triangle overlapped by a diagonal line in a box.\nHere are three sub images arranged in a 2-by-2  matrix. Figure out which figure of a to f that should  logically replace the missing figure. What would be  the next image to complete the matrix? \nThe image shows six individual boxes, each containing a unique red geometric figure against a light yellow background and outlined with a blue border. The boxes are labeled with letters from a to f:\n\na. A complex overlapping pattern consisting of multiple interlaced lines forming a symmetrical shape.\nb. A diamond-like shape with an additional intersecting line inside it.\nc. A simple diagonal line running from the bottom left to the top right.\nd. A shape resembling a triangle with additional crossing lines.\ne. A symmetrical diamond shape with its sides equal and centered.\nf. An inverted triangle or \"V\" shape.\n\nEach distinct shape appears to be drawn in a consistent red color.\nThe first image in the first column, image 1, is a  triangle. The first image in the second column,  image 3, is a triangle with a line through it. The  second image in the first column, image 2, is a  diamond. Therefore, the second image in the  second column, image 4 should continue this  pattern and be a diamond with a line through it.  Option b is the only image that fits this pattern, as it  is a diamond with a line through it. Therefore, the  correct answer is b. \nPrompt Continued \nThe image contains a vertical sequence of three figures on a light blue background, each enclosed in a blue border:\n\n1. The first figure is a yellow diamond shape.\n2. The second figure is a yellow star-like shape with four pointed ends.\n3. The third figure is a yellow diamond shape with a small yellow dot in its center.\n\nEach figure is enumerated from 1 to 3 on the left side.\nHere are three sub images arranged in a 2-by-2  matrix. Figure out which figure of a to f that should  logically replace the missing figure. What would be  the next image to complete the matrix? \nThe image shows six boxes labeled a to f, each containing a geometric design on a light blue background. Here's a description of each:\n\na. A complex overlapping diamond pattern.\nb. A single diamond shape.\nc. A diamond shape with a small square in the center.\nd. A single dot in the center.\ne. A diamond with a small diamond in the center.\nf. A star shape with an outline.\nGPT-4V: \nThe first image in the first column, image 1, is a  diamond. The first image in the second column,  image 3, is a diamond with a dot in the middle. The  second image in the first column, image 2, is a star.  Therefore, the second image in the second column,  image 4 should continue this pattern and be a star  with a dot in the middle. Option c is the only image  that fits this pattern, as it is a star with a dot in the  middle. Therefore, the correct answer is c. "}
{"page": 85, "image_path": "doc_images/2309.17421v2_85.jpg", "ocr_text": "8 Emotional Quotient Test\n\nSec. 8.1 Read Emotion from Facial Expressions\n\nPrompt:\nIdentify and read emotions of people from their faces\nas shown in the image below.\n\nGPT-4V:\n\nThe person in the image appears to be displaying a\ncombination of emotions. The furrowed brows and\ndownturned mouth suggest anger or frustration, while\nthe tense jaw and narrowed eyes could indicate\ndisgust or contempt. Overall, it seems like the person\n\nPrompt:\nIdentify and read emotions of people from their faces\nas shown in the image below.\n\nGPT-4V:\n\nThe emotion shown in the image is fear or anxiety. The\nperson's eyes are wide open and their mouth is slightly\nopen, indicating a sense of shock or surprise. Their\nhands are covering their mouth, which could suggest\nthat they are trying to suppress a scream or gasp.\n\nis experiencing negative emotions and is likely upset or\n\nIdentify and read emotions of people from their faces\nas shown in the image below.\n\nIdentify and read emotions of people from their faces\n\nas shown in the image below.\n\nGPT-4V:\n\nThe person in the image appears to be experiencing a\nnegative emotion, likely frustration or disappointment.\nThis is evident from the furrowed brows, downward\ngaze, and hand on the forehead.\n\nGPT-4V:\n\nThe person in the image is displaying a happy emotion.\nThis can be seen from the wide smile and the crinkles\naround the eyes.\n\nFigure 64: GPT-4V can reliably identify and read the emotions of people from their facial expressions.\nCheck Section 8.1 for detailed discussions.\n\nWhen interacting with humans, it is important that GPT-4V has the empathy and Emotional Quotient\n(EQ) to understand and share the feelings of humans. Inspired by the definition of the human EQ\ntest [92, 91, 21], we examine GPT-4V’s capability in (1) identifying and reading human emotions\nfrom their facial expressions, (2) understanding how different visual contents may arouse emotions,\nand (3) generating proper text outputs conditioned on the desired emotional and sentiment.\n\n8.1 Read Emotion from Facial Expressions\nAs shown in Figure 64, GPT-4V can reliably identify and read the emotions of people from their\n\nfacial expressions. It also provides reasonable rationales for the visual cues observed to make the\nemotion interpretation, indicating a good understanding of the facial emotions.\n\n86\n", "vlm_text": "The image contains a series of prompts and responses related to identifying emotions from facial expressions. There are four different images, each depicting a person's face with a specific expression:\n\n1. **Top Left:**\n   - **Prompt:** Identify and read emotions from the face shown.\n   - **Response:** The expression suggests a combination of anger, frustration, disgust, or contempt, indicating the person is upset or unhappy.\n\n2. **Top Right:**\n   - **Prompt:** Identify and read emotions from the face shown.\n   - **Response:** The expression indicates fear or anxiety, with wide eyes and a slightly open mouth suggesting shock or surprise.\n\n3. **Bottom Left:**\n   - **Prompt:** Identify and read emotions from the face shown.\n   - **Response:** The person is displaying happiness, evident from a wide smile and crinkles around the eyes.\n\n4. **Bottom Right:**\n   - **Prompt:** Identify and read emotions from the face shown.\n   - **Response:** The expression shows frustration or disappointment, with furrowed brows, a downward gaze, and a hand on the forehead.\nWhen interacting with humans, it is important that GPT-4V has the empathy and Emotional Quotient (EQ) to understand and share the feelings of humans. Inspired by the definition of the human EQ test [ 92 ,  91 ,  21 ], we examine GPT-4V’s capability in (1) identifying and reading human emotions from their facial expressions, (2) understanding how different visual contents may arouse emotions, and (3) generating proper text outputs conditioned on the desired emotional and sentiment. \n8.1 Read Emotion from Facial Expressions \nAs shown in Figure  64 , GPT-4V can reliably identify and read the emotions of people from their facial expressions. It also provides reasonable rationales for the visual cues observed to make the emotion interpretation, indicating a good understanding of the facial emotions. "}
{"page": 86, "image_path": "doc_images/2309.17421v2_86.jpg", "ocr_text": "Prompt:\nAfter seeing the below image, how people might\nemotionally feel and react?\n\nGPT-4V:\n\nPeople might feel happy, content, and relaxed when\nseeing this image. They might react with a smile or a\nfeeling of warmth and comfort. This image appears to\nbe of a couple at a music festival, so people might also\nfeel a sense of excitement and anticipation for the\nmusic and performances.\n\nAfter seeing the below image, how people might\nemotionally feel and react?\n\nGPT-4V:\n\nPeople might feel a sense of awe and wonder at the\nbeauty of the sunset and the colors in the sky. They\nmay feel peaceful and calm, or even inspired. Some\nmay react with a sense of gratitude for being able to\nwitness such a beautiful moment in nature. Others may\nfeel a sense of longing to be in a place like this, ora\ndesire to travel and see more of the world's beauty.\n\nPrompt:\nAfter seeing the below image, how people might\nemotionally feel and react?\n\nGPT-4V:\n\nPeople might feel angry, frustrated, or upset after\nseeing this image. They might react by feeling a\nsense of injustice or wanting to take action to support\nthe man in the image. Some people might also feel\nfear or anxiety about the situation and the presence of\nthe police.\n\nAfter seeing the below image, how people might\nemotionally feel and react?\n\nGPT-4V:\n\nPeople might feel scared, anxious, or uneasy after\nseeing this image. The dark tones and shadows\ncreate a sense of mystery and unease, and the\nsilhouette of the figure on the stairs could be\ninterpreted as ominous or threatening. Some people\nmight react by feeling a sense of dread or fear, while\nothers might be intrigued or curious about the context\nof the image.\n\nFigure 65: GPT-4V understands how different visual contents may arouse human emotions. Check\nSection 8.2 for detailed discussions.\n\n8.2. Understand How Visual Content Arouses Emotions\n\nWe next analyze GPT-4V’s ability on visual sentiment analysis, i.e., understanding humans’ emotional\nresponse after seeing the visual contents. Such ability is critical for GPT-4V to anticipate how visual\ncontents may arouse human emotions and thereby react properly. As shown in Figure 65, GPT-4V can\ninterpret visual sentiments such as content, anger, awe, and fear, based on both the semantic contents\nand the image style. These capabilities are essential in use cases such as home robots.\n\n87\n", "vlm_text": "Sec. 8.2 How Visual Content Arouses Emotions \nPrompt: \nAfter seeing the below image, how people might  emotionally feel and react? \nThe image depicts a couple embracing at an outdoor event or festival. There are flags and banners, and a stage in the background with lights and possibly performance equipment. The audience is gathered in the foreground, seated or standing, indicating a live music or cultural event.\nGPT-4V: \nPeople might feel happy, content, and relaxed when  seeing this image. They might react with a smile or a  feeling of warmth and comfort. This image appears to  be of a couple at a music festival, so people might also  feel a sense of excitement and anticipation for the  music and performances. \nPrompt: \nAfter seeing the below image, how people might  emotionally feel and react? \nThe image shows a scenic view of a coastal landscape during sunset. The foreground features rocks and patches of green seaweed or moss, with shallow pools of water reflecting the sky. In the background, the sun is setting on the horizon, casting a warm glow across the sky with colorful clouds. The overall atmosphere is serene and picturesque.\nGPT-4V: \nPeople might feel a sense of awe and wonder at the  beauty of the sunset and the colors in the sky. They  may feel peaceful and calm, or even inspired. Some  may react with a sense of gratitude for being able to  witness such a beautiful moment in nature. Others may  feel a sense of longing to be in a place like this, or a  desire to travel and see more of the world's beauty. \nPrompt: \nAfter seeing the below image, how people might  emotionally feel and react? \nThis image depicts a group of people, one of whom is being held back by police officers wearing reflective jackets with the \"Metropolitan Police\" label. The scene appears to involve public demonstration or protest, as suggested by the presence of banners and flags.\nGPT-4V: \nPeople might feel angry, frustrated, or upset after  seeing this image. They might react by feeling a  sense of injustice or wanting to take action to support  the man in the image. Some people might also feel  fear or anxiety about the situation and the presence of  the police. \nPrompt: \nAfter seeing the below image, how people might  emotionally feel and react? \nThe image depicts a dark, possibly haunted staircase. The shadowy, low-lit environment creates an eerie, mysterious atmosphere, with a focus on the railing and the wall where light and shadow interact to form intriguing shapes. The use of black and white adds to the overall haunting and dramatic feel of the image.\nGPT-4V: \nPeople might feel scared, anxious, or uneasy after  seeing this image. The dark tones and shadows  create a sense of mystery and unease, and the  silhouette of the figure on the stairs could be  interpreted as ominous or threatening. Some people  might react by feeling a sense of dread or fear, while  others might be intrigued or curious about the context  of the image. \nFigure 65: GPT-4V understands how different visual contents may arouse human emotions. Check Section  8.2  for detailed discussions. \n8.2 Understand How Visual Content Arouses Emotions \nWe next analyze GPT-4V’s ability on visual sentiment analysis,  i.e ., understanding humans’ emotional response after seeing the visual contents. Such ability is critical for GPT-4V to anticipate how visual contents may arouse human emotions and thereby react properly. As shown in Figure  65 , GPT-4V can interpret visual sentiments such as content, anger, awe, and fear, based on both the semantic contents and the image style. These capabilities are essential in use cases such as home robots. "}
{"page": 87, "image_path": "doc_images/2309.17421v2_87.jpg", "ocr_text": "Sec. 8.2 How Visual Content Arouses Emotions\n\nImage Aesthetics\nPrompt: Prompt:\n\nAesthetics reflects the perceived beauty or visual Aesthetics reflects the perceived beauty or visual\nappeal of an object, design, or piece of art by human. appeal of an object, design, or piece of art by human.\nAmong the images provided below, based on societal Among the images provided below, based on societal\nstandards and norms, which one more people will standards and norms, which one more people will\nconsider having a higher aesthetics? consider having a higher aesthetics?\n\nGPT-4V: GPT-AV:\n\nMost people would likely consider the first image to\nhave a higher aesthetic value. This is because the\nimage is more visually appealing, with a variety of\ncolors, buildings, and greenery. The second image is\nless visually appealing, with a more monochromatic\ncolor scheme and less variety in the buildings and\nlandscape. Additionally, the first image is of a more\niconic and recognizable location (New York City), which\nmay also contribute to its perceived aesthetic value.\n\nBased on societal standards and norms, more people\nwill likely consider the second image (the colorful\nhouses) as having a higher aesthetics. This is\nbecause the image is more visually appealing and\nhas a more artistic and creative feel to it. The first\nimage (the house) is more plain and ordinary, and\nwhile it may be considered aesthetically pleasing to\nsome, it is not as striking or eye-catching as the\nsecond image.\n\nFigure 66: GPT-4V judges image aesthetics based on societal standards and norms. Check Section 8.2\nfor detailed discussions.\n\nIn addition to interpreting visual sentiment, GPT-4V also aligns with human subjective judgments\nsuch as aesthetics. Figure 66 shows examples of GPT-4V judging image aesthetics based on societal\nstandards.\n\n88\n", "vlm_text": "Sec. 8.2 How Visual Content Arouses Emotions \nImage Aesthetics \nPrompt: \nAesthetics reflects the perceived beauty or visual  appeal of an object, design, or piece of art by human. Among the images provided below, based on societal  standards and norms, which one more people will  consider having a higher aesthetics? \nThe image shows two different buildings. The top image is of a modern, monochromatic house with a flat roof, and the bottom image depicts colorful buildings with vibrant facades in pink, yellow, and orange, featuring traditional-style windows and doors.\nGPT-4V: \nBased on societal standards and norms, more people  will likely consider the second image (the colorful  houses) as having a higher aesthetics. This is  because the image is more visually appealing and  has a more artistic and creative feel to it. The first  image (the house) is more plain and ordinary, and  while it may be considered aesthetically pleasing to  some, it is not as striking or eye-catching as the  second image. \nPrompt: \nAesthetics reflects the perceived beauty or visual  appeal of an object, design, or piece of art by human. Among the images provided below, based on societal  standards and norms, which one more people will  consider having a higher aesthetics? \nThe image consists of two separate photographs placed one above the other.\n\nThe top photograph showcases a view of New York City, featuring a part of Central Park with its lush green trees on the left side and a row of high-rise buildings along the edge of the park on the right. The sky is partly cloudy, and the scene captures the contrast between the greenery of the park and the urban landscape of the city.\n\nThe bottom photograph displays a cityscape of Vancouver, with an expansive view of roads, buildings, and a distant view of water and mountains. The image includes a clear blue sky above, and prominent features include a stadium with a white dome roof on the left and several bridges spanning over roads below. The combination of natural and urban elements is typical of Vancouver's landscapes.\nGPT-4V: \nMost people would likely consider the first image to  have a higher aesthetic value. This is because the  image is more visually appealing, with a variety of  colors, buildings, and greenery. The second image is  less visually appealing, with a more monochromatic  color scheme and less variety in the buildings and  landscape. Additionally, the first image is of a more  iconic and recognizable location (New York City), which  may also contribute to its perceived aesthetic value. \nFigure 66: GPT-4V judges image aesthetics based on societal standards and norms. Check Section  8.2 for detailed discussions. "}
{"page": 88, "image_path": "doc_images/2309.17421v2_88.jpg", "ocr_text": "Sec. 8.3 Emotion Conditioned Output\n\nPrompt:\nDescribe the image in a humorous way.\n\nGPT-4V:\n\nIt looks like the sun is trying to hide behind the\nhorizon, but it's not doing a very good job. It's like it's\nplaying peek-a-boo with us. And the seaweed is just\nchilling there, like \"Hey, don't mind us, we're just\nhanging out.\" The rocks are probably feeling left out,\nthey're like \"Hey, what about us? We're part of this\nbeach too!\n\nDescribe the image to lessen someone's desire to\nvisit.\n\nGPT-4V:\n\nThe image shows a rocky beach at sunset with\nseaweed and algae covering the ground. The water\nis murky and there are no clear areas to swim or\nrelax. The rocks are sharp and slippery, making it\ndifficult to walk or explore. The sunset is partially\nblocked by clouds. It seems like a difficult and\nunpleasant place to navigate.\n\nPrompt:\nDescribe the image to make people feel uneasy and\nanxious.\n\nGPT-4V:\n\nThe image is of a dark, shadowy staircase. The only\nlight comes from a single, dim bulb, casting eerie\nshadows on the walls. The railing is old and rusted,\nwith peeling paint and jagged edges. The stairs\nthemselves are uneven and creak underfoot. The air is\nthick and musty, and there is a feeling of unease as if\nsomeone is lurking in the shadows. The silence is\ndeafening, and every creak and groan of the old\nstaircase echoes through the empty space.\n\nDescribe the image to make people feel relax and\ncomforting.\n\nGPT-4V:\n\nThe image is of a dimly lit staircase with ornate railings.\nThe light is soft and creates a peaceful atmosphere.\nThe shadows on the wall add depth and texture to the\nimage, making it feel cozy and inviting. The overall\nmood of the image is calm and serene, making it a\nperfect place to relax and unwind.\n\nFigure 67: GPT-4V generates proper text based on the perceived or desired emotions, making its\ncommunication with humans comforting and effective. Check Section 8.3 for detailed discussions.\n\n8.3 Emotion Conditioned Output\n\nBased on the perceived emotions, GPT-4V effectively generates proper text outputs conditioned\non the desired emotion. For example, in Figure 67, GPT-4V can follow the prompt to describe\nthe right-side scary image in a way that makes it more horrifying or becoming comforting. This\ndemonstrates GPT-4V’s potential to enable emotion-aware human-robot communication.\n\n89\n", "vlm_text": "Sec. 8.3 Emotion Conditioned Output \nPrompt: \nDescribe the image in a humorous way. \nThe image shows a scenic landscape of a coastal area at sunset. The foreground features a rocky shoreline with patches of seaweed or algae, and shallow pools of water reflecting the sky. In the background, the sun is setting on the horizon, casting a vibrant orange and pink glow across the sky and water. The overall mood is serene and tranquil, capturing the natural beauty of a seaside sunset. The name \"Stevie C\" is inscribed in the lower-left corner, likely indicating the photographer or artist.\nGPT-4V: \nIt looks like the sun is trying to hide behind the  horizon, but it's not doing a very good job. It's like it's  playing peek-a-boo with us. And the seaweed is just  chilling there, like \"Hey, don't mind us, we're just  hanging out.\" The rocks are probably feeling left out,  they're like \"Hey, what about us? We're part of this  beach too! \nPrompt: \nDescribe the image to lessen someone's desire to  visit. \nThe image displays a scenic view of a coastal landscape during sunset or sunrise. In the foreground, there is a rocky shoreline with pools of water and patches of green moss or algae. In the background, the sun is setting or rising near the horizon, casting a warm glow over the sky and creating a reflection on the water's surface. The sky is filled with clouds, adding texture and color to the scene. On the horizon, distant landforms or hills are visible. Overall, the image captures the natural beauty of a coastal environment at dusk or dawn.\nGPT-4V: \nThe image shows a rocky beach at sunset with  seaweed and algae covering the ground. The water  is murky and there are no clear areas to swim or  relax. The rocks are sharp and slippery, making it  difficult to walk or explore. The sunset is partially  blocked by clouds. It seems like a difficult and  unpleasant place to navigate. \nPrompt: \nDescribe the image to make people feel uneasy and  anxious. \nThe image appears to be a dimly lit staircase with ornate balusters. There is a prominent shadow on the wall, possibly cast by the railing or another object. The lighting creates a mysterious and somewhat eerie atmosphere.\nGPT-4V: \nThe image is of a dark, shadowy staircase. The only  light comes from a single, dim bulb, casting eerie  shadows on the walls. The railing is old and rusted,  with peeling paint and jagged edges. The stairs  themselves are uneven and creak underfoot. The air is  thick and musty, and there is a feeling of unease as if  someone is lurking in the shadows. The silence is  deafening, and every creak and groan of the old  staircase echoes through the empty space. \n\nDescribe the image to make people feel relax and  comforting. \nThis is a black and white image of a staircase with a shadow cast on the wall. The design of the railing is visible, creating an interesting pattern in the shadows. The overall atmosphere is dark and somewhat mysterious.\nGPT-4V: \nThe image is of a dimly lit staircase with ornate railings.  The light is soft and creates a peaceful atmosphere.  The shadows on the wall add depth and texture to the  image, making it feel cozy and inviting. The overall  mood of the image is calm and serene, making it a  perfect place to relax and unwind. \nFigure 67: GPT-4V generates proper text based on the perceived or desired emotions, making its communication with humans comforting and effective. Check Section  8.3  for detailed discussions. \n8.3 Emotion Conditioned Output \nBased on the perceived emotions, GPT-4V effectively generates proper text outputs conditioned on the desired emotion. For example, in Figure  67 , GPT-4V can follow the prompt to describe the right-side scary image in a way that makes it more horrifying or becoming comforting. This demonstrates GPT-4V’s potential to enable emotion-aware human-robot communication. "}
{"page": 89, "image_path": "doc_images/2309.17421v2_89.jpg", "ocr_text": "9 Emerging Application Highlights\n\nIn this section, we showcase a myriad of high-value application scenarios and new use cases that can\nbe potentially enabled by the remarkable capabilities of GPT-4V. While it is true that some of these\napplication scenarios can be accomplished by meticulously curating the training data for finetuning\nexisting Vision and Language (VL) models, we want to emphasize that the true power of GPT-4V\nlies in its ability to perform effortlessly right out of the box. Moreover, we present how GPT-4V\nseamlessly integrates with external tools and plugins, further expanding its potential and enabling\neven more innovative and collaborative applications.\n\n9.1 Spot the Difference\n\nWe begin with a generic use case inspired by the brain-teasing game “Spot the Difference.” In\nFigures 68-69, we provide GPT-4V two visually similar images that contain subtle differences in\ncertain regions. The task given to GPT-4V is to identify all the differences between the two images.\nAmong the four examples, GPT-4V successfully identifies the regions or components that differ\nin the images. However, it falls short in providing accurate explanations for what is depicted in\neach image. To delve deeper into GPT-4V’s capabilities, let’s focus on the first example shown in\nFigure 68. Although GPT-4V fails to recognize that the discrepancy lies in the number of cuts in the\nhairband rather than the shade of the hair, it correctly identifies that the crown, the bow of the dress,\nand the hair differ between the two images. While GPT-4V’s predictions in the “Spot the Difference”\ngame are not perfect, its ability to compare the content in two images proves valuable in real-life\napplications, such as defect detection, which we will explore in the following subsections.\n\n9.2 Industry\n\nDefect detection. Throughout the history of manufacturing, computer vision techniques have\nplayed a crucial role. One specific application scenario is defect detection, which is an essential step\nin manufacturing processes to ensure product quality. Detecting faults or defects in a timely manner\nand taking appropriate actions are vital for minimizing operational and quality-related costs.\n\nIn this scenario, we demonstrate the defect detection capabilities of GPT-4V by presenting images of\ndefective products in Figures 70-71. For commonly encountered products in real-life (e.g., hazelnut,\nfabric, screw, and car bumper in Figure 70), GPT-4V confidently identifies the defects such as small\nholes in the hazelnut/fabric, stripped heads of screws, and dents in car bumpers. However, when\nit comes to uncommon product images (e.g., the metal parts in Figures 70-71) or products with\nvariations in appearance (e.g., the pill in Figure 71), GPT-4V may hesitate or even refuse to make\npredictions. An interesting case in Figure 71 involves a car tire, where multiple defects can be\nobserved in the image, including dirt on the wheel, damage to the outer edge of the rim, and signs of\nwear on the tire. GPT-4V only focuses on the minor defect (dirt on the wheel) and fails to mention\nthe major defect (damage to the outer edge of the rim) that would require repair.\n\nGiven the success of GPT-4V in “Spot the Difference” scenario shown in Section 9.1, we explore\nthe idea of incorporating a reference image to illustrate what a defect-free product should look like,\nwith the aim of improving the failure cases depicted in Figure 71. The results of this approach\nare presented in Figure 72. By including the reference image and refining the prompt, GPT-4V\nsuccessfully identifies defects in all three failure cases in single-image defect detection. These\npromising findings highlight a potential high-value application of GPT-4V for defect detection in the\nmanufacturing industry.\n\nSafety inspection. Figure 73 presents an exploration of Personal Protective Equipment (PPE)\ncounting for safety inspection. The inadequate usage or failure to wear PPE, such as helmets,\nharnesses, and gloves, in work environments like construction sites, significantly increases the risk\nlevel associated with work activities. To effectively address this issue, computer vision techniques\nhave been employed as a solution to monitor PPE compliance and promptly identify any violations\nof safety regulations. Taking helmets as an example, a safety inspection system is necessary to\naccurately detect and report the number of employees who are not wearing helmets.\n\nIn Figure 73a, we assess the performance of GPT-4V by directly instructing it to count the individuals\nwearing helmets. GPT-4V provides a response of “8 persons wearing helmets,” which matches the\n\n90\n", "vlm_text": "9 Emerging Application Highlights \nIn this section, we showcase a myriad of high-value application scenarios and new use cases that can be potentially enabled by the remarkable capabilities of GPT-4V. While it is true that some of these application scenarios can be accomplished by meticulously curating the training data for finetuning existing Vision and Language (VL) models, we want to emphasize that the true power of GPT-4V lies in its ability to perform effortlessly right out of the box. Moreover, we present how GPT-4V seamlessly integrates with external tools and plugins, further expanding its potential and enabling even more innovative and collaborative applications. \n9.1 Spot the Difference \nWe begin with a generic use case inspired by the brain-teasing game “Spot the Difference.” In Figures  68 - 69 , we provide GPT-4V two visually similar images that contain subtle differences in certain regions. The task given to GPT-4V is to identify all the differences between the two images. Among the four examples, GPT-4V successfully identifies the regions or components that differ in the images. However, it falls short in providing accurate explanations for what is depicted in each image. To delve deeper into GPT-4V’s capabilities, let’s focus on the first example shown in Figure  68 . Although GPT-4V fails to recognize that the discrepancy lies in the number of cuts in the hairband rather than the shade of the hair, it correctly identifies that the crown, the bow of the dress, and the hair differ between the two images. While GPT-4V’s predictions in the “Spot the Difference” game are not perfect, its ability to compare the content in two images proves valuable in real-life applications, such as defect detection, which we will explore in the following subsections. \n9.2 Industry \nDefect detection. Throughout the history of manufacturing, computer vision techniques have played a crucial role. One specific application scenario is defect detection, which is an essential step in manufacturing processes to ensure product quality. Detecting faults or defects in a timely manner and taking appropriate actions are vital for minimizing operational and quality-related costs. \nIn this scenario, we demonstrate the defect detection capabilities of GPT-4V by presenting images of defective products in Figures  70 - 71 . For commonly encountered products in real-life (e.g., hazelnut, fabric, screw, and car bumper in Figure  70 ), GPT-4V confidently identifies the defects such as small holes in the hazelnut/fabric, stripped heads of screws, and dents in car bumpers. However, when it comes to uncommon product images (e.g., the metal parts in Figures  70 - 71 ) or products with variations in appearance (e.g., the pill in Figure  71 ), GPT-4V may hesitate or even refuse to make predictions. An interesting case in Figure  71  involves a car tire, where multiple defects can be observed in the image, including dirt on the wheel, damage to the outer edge of the rim, and signs of wear on the tire. GPT-4V only focuses on the minor defect (dirt on the wheel) and fails to mention the major defect (damage to the outer edge of the rim) that would require repair. \nGiven the success of GPT-4V in “Spot the Difference” scenario shown in Section  9.1 , we explore the idea of incorporating a reference image to illustrate what a defect-free product should look like, with the aim of improving the failure cases depicted in Figure  71 . The results of this approach are presented in Figure  72 . By including the reference image and refining the prompt, GPT-4V successfully identifies defects in all three failure cases in single-image defect detection. These promising findings highlight a potential high-value application of GPT-4V for defect detection in the manufacturing industry. \nSafety inspection. Figure  73  presents an exploration of Personal Protective Equipment (PPE) counting for safety inspection. The inadequate usage or failure to wear PPE, such as helmets, harnesses, and gloves, in work environments like construction sites, significantly increases the risk level associated with work activities. To effectively address this issue, computer vision techniques have been employed as a solution to monitor PPE compliance and promptly identify any violations of safety regulations. Taking helmets as an example, a safety inspection system is necessary to accurately detect and report the number of employees who are not wearing helmets. \nIn Figure  73a , we assess the performance of GPT-4V by directly instructing it to count the individuals wearing helmets. GPT-4V provides a response of  $^{\\ast8}$   persons wearing helmets,” which matches the total count of people shown in the image, suggesting there is no alerting safety violations. Obviously, GPT-4V fails to detect the 3 individuals who are not wearing helmets, thus compromising their personal safety. This task poses a considerable challenge for GPT-4V, as it involves detecting people in the image, determining whether they are wearing helmets, and calculating the final count of people who are not wearing the helmets. "}
{"page": 90, "image_path": "doc_images/2309.17421v2_90.jpg", "ocr_text": "total count of people shown in the image, suggesting there is no alerting safety violations. Obviously,\nGPT-4V fails to detect the 3 individuals who are not wearing helmets, thus compromising their\npersonal safety. This task poses a considerable challenge for GPT-4V, as it involves detecting people\nin the image, determining whether they are wearing helmets, and calculating the final count of people\nwho are not wearing the helmets.\n\nIn Figure 73b, instead of presenting GPT-4V with the original image containing all 8 individuals,\nwe provide cropped regions of the detected persons with an external person detector. This approach\ndivides the PPE counting workload into two steps: relying on an off-the-shelf person detector for\nperson detection and leveraging GPT-4V’s robust visual reasoning capabilities and its ability to\nhandle interleaved image-text inputs for identifying the safety issues. As we can see, GPT-4V can\ncorrectly count the person who is not wearing the helmet, also demonstrating the benefit of tool use\nand divide-and-conquer.\n\nGrocery checkout. Self-checkout machines have become increasingly popular in major retailers\nlike Walmart, Target and CVS to expedite the checkout process for customers and reduce the workload\nfor employees. However, the actual experience with self-checkout machines may be frustrating for\ncustomers. Users still need to search for the product barcode or manually enter codes for fresh items\nlike apples, which can be time-consuming, particularly for those unfamiliar with the system. In\nFigure 74, we provide a simplified prototype to demonstrate the potential of GPT-4V in enabling an\nautomatic self-checkout system that can identify and ring up items without user intervention.\n\nWhen presented with a photograph of a shopping basket containing five grocery items, as shown\nin Figure 74a, GPT-4V fails to accurately identify the products within the basket. It mistakenly\nidentifies strawberries as raspberries, crab dip as Greek yogurt, and includes salmon fillets that are\nnot even present in the basket. However, in Figure 74b, we improve the prompt by augmenting it with\ncatalog images of grocery products retrieved from the retail website. As a result, GPT-4V successfully\nidentifies all five items in the basket. This successful demonstration allows the self-checkout system\nto proceed with retrieving the prices for each identified product from the database. While this is a\nsimple example, it represents a significant step forward toward an automated self-checkout system.\nFurther research and development can explore more complex and realistic scenarios to fully automate\nthe self-checkout process, making it more efficient and convenient for customers.\n\n91\n", "vlm_text": "\nIn Figure  73b , instead of presenting GPT-4V with the original image containing all 8 individuals, we provide cropped regions of the detected persons with an external person detector. This approach divides the PPE counting workload into two steps: relying on an off-the-shelf person detector for person detection and leveraging GPT-4V’s robust visual reasoning capabilities and its ability to handle interleaved image-text inputs for identifying the safety issues. As we can see, GPT-4V can correctly count the person who is not wearing the helmet, also demonstrating the benefit of tool use and divide-and-conquer. \nGrocery checkout. Self-checkout machines have become increasingly popular in major retailers like Walmart, Target and CVS to expedite the checkout process for customers and reduce the workload for employees. However, the actual experience with self-checkout machines may be frustrating for customers. Users still need to search for the product barcode or manually enter codes for fresh items like apples, which can be time-consuming, particularly for those unfamiliar with the system. In Figure  74 , we provide a simplified prototype to demonstrate the potential of GPT-4V in enabling an automatic self-checkout system that can identify and ring up items without user intervention. \nWhen presented with a photograph of a shopping basket containing five grocery items, as shown in Figure  74a , GPT-4V fails to accurately identify the products within the basket. It mistakenly identifies strawberries as raspberries, crab dip as Greek yogurt, and includes salmon fillets that are not even present in the basket. However, in Figure  74b , we improve the prompt by augmenting it with catalog images of grocery products retrieved from the retail website. As a result, GPT-4V successfully identifies all five items in the basket. This successful demonstration allows the self-checkout system to proceed with retrieving the prices for each identified product from the database. While this is a simple example, it represents a significant step forward toward an automated self-checkout system. Further research and development can explore more complex and realistic scenarios to fully automate the self-checkout process, making it more efficient and convenient for customers. "}
{"page": 91, "image_path": "doc_images/2309.17421v2_91.jpg", "ocr_text": "Sec. 9.1 Spot the Difference\n\nPrompt:\nCompare Image 1 and image 2, tell me about the differences between image 1 and image 2.\n\nGPT-4V:\n\nThere are a few differences between image 1 and image 2:\nThe crown: in image 1, the crown is pink with a blue gem in the center. In image 2, the crown is gold with\nred gems and a cross on top.\n\nThe dress: in image 1 the dress has a red bow in the center. In image 2, the bow is missing.\nThe hair: in image 1, the hairis a lighter shade of brown compared to image 2\n\nIn image 2, there is an additional green plant that is not present in image 1.\nIn image 1, the turtle’s head is facing downwards. In image 2 the turtle’s head is facing upwards.\n\nFigure 68: Spot the differences. ‘Red highlights the inaccurate description about the differences.\nCheck Section 9.1 for detailed discussions.\n\n92\n", "vlm_text": "Sec. 9.1 Spot the Difference \nPrompt: \nCompare Image 1 and image 2, tell me about the differences between image 1 and image 2. \nImage 1 \nThe image is an illustration of a cartoon princess. She is wearing a pink dress with a red bow, a heart-shaped necklace, and a pink crown. She has brown hair and is shown in a simplistic, cute style.\nThis image shows an illustration of a girl wearing a pink dress and a red crown with a cross on top. She has light brown hair and is smiling.\nGPT-4V: \nThere are a few differences between image 1 and image 2: 1. The crown: in image 1, the crown is pink with a blue gem in the center. In image 2, the crown is gold with  red gems and a cross on top. 2. The dress: in image 1 the dress has a red bow in the center. In image 2, the bow is missing. 3. The hair: in image 1, the hair is a lighter shade of brown compared to image 2.   \nPrompt: \nCompare Image 1 and image 2, tell me about 3 differences between image 1 and image 2. \nThe image is a colorful cartoon illustration depicting an underwater scene. It features a fish and a turtle swimming among coral. The coral is in various bright colors like pink, red, purple, yellow, and blue. The background is a light blue with white bubbles, representing water.\nThe image is a colorful illustration of an underwater scene. It features a green turtle with yellow spots swimming alongside a yellow fish. There are coral reefs in various colors, including pink, red, purple, and orange, with a few green aquatic plants. The background is blue with white bubbles, suggesting water.\n1) In image 1, there are two blue coral reefs while in Image 2, there is only one blue coral reef. 2) In image 2, there is an additional green plant that is not present in image 1. 3) In image 1, the turtle’s head is facing downwards. In image 2 the turtle’s head is facing upwards. \nFigure 68: Spot the differences.  Red  highlights the inaccurate description about the differences. Check Section  9.1  for detailed discussions. "}
{"page": 92, "image_path": "doc_images/2309.17421v2_92.jpg", "ocr_text": "Sec. 9.1 Spot the Difference\n\nPrompt:\nCompare Image 1 and image 2, tell me about the 3 differences between image 1 and image 2.\n\nImage 1\n\n1) In image 1, there is no sun in the background, while in image 2, there is a sun in the top left corner.\n2) Inimage 1, there is a single bamboo leaf on the ground, while in image 2, there is\n\n3) In image 1, the panda is holding a single bamboo stalk, while in image 2, the panda is holding multiple\n\nGPT-4V:\n\n1) Image 1 has spikes on its back while image 2 does not.\n\n2) Image 2 has a tail while image 1 does not.\n3) Image 2 has Simoutiathiieeti while image 1 does not have a mouth\n\nFigure 69: Spot the differences. ‘Red highlights the inaccurate description about the differences.\nCheck Section 9.1 for detailed discussions.\n\n93\n", "vlm_text": "Sec. 9.1 Spot the Difference \nPrompt: \nCompare Image 1 and image 2, tell me about the 3 differences between image 1 and image 2. \nImage 1 \nThe image shows two cartoon illustrations of a panda. In both illustrations, the panda is sitting and eating bamboo. The top image shows the panda with a leafy bamboo branch, while the bottom image includes a small brown object near the panda and a sun in the background.\nGPT-4V: \n1) In image 1, there is no sun in the background, while in image 2, there is a sun in the top left corner. 2) In image 1, there is a single bamboo leaf on the ground, while in image 2, there is a pile of bamboo  leaves on the ground. 3) In image 1, the panda is holding a single bamboo stalk, while in image 2, the panda is holding multiple  bamboo stalks. \nPrompt: Compare Image 1 and image 2, tell me about 3 differences between image 1 and image 2. \nImage 1 \nThe image is a simple, cartoon-style drawing of a dinosaur. It features an oval body, a circular head with a long neck, spiked plates along its back, and four legs with spiky feet, resembling a stegosaurus.\nImage 2 \nThe image is a simple, abstract line drawing of a turtle. It features a large oval for the turtle's shell, a smaller circle with a face at the front representing the turtle's head, four semicircles with points representing the turtle's legs and feet, and a pointed tail at the back. The turtle appears to have a closed eye on its face, suggesting it might be sleeping or simply styled with a relaxed, peaceful expression.\nGPT-4V: \n1) Image 1 has spikes on its back while image 2 does not. 2) Image 2 has a tail while image 1 does not. 3) Image 2 has a mouth with teeth while image 1 does not have a mouth \nFigure 69: Spot the differences.  Red  highlights the inaccurate description about the differences. Check Section  9.1  for detailed discussions. "}
{"page": 93, "image_path": "doc_images/2309.17421v2_93.jpg", "ocr_text": "Sec. 9.2 Industry\n\nDefect Detection\n\nPrompt:\nWhat is wrong with the object in the image?\n\nGPT-4V:\n\nThere is a small hole in the object, which appears\nto be a hazelnut. This could be a sign of insect\ndamage or rot.\n\nWhat is wrong with the object in the image?\n\nGPT-4V:\n\nThe object in the image appears to be a screw or\nbolt with a damaged or stripped head. The\ngrooves in the head, which would normally allow\na screwdriver or wrench to grip and turn the\nscrew, are worn away and no longer functional.\n\nWhat is wrong with the object in the image?\n\nGPT-4V:\n\nIt is difficult to say for sure what is wrong with the\nobject in the image as there is no context or\ninformation about what the object is or what it is\nsupposed to look like. However, it appears that\nthe copper wires in the blue and brown sections\nare frayed and not neatly arranged, which could\nindicate damage or poor quality.\n\nPrompt:\nWhat is wrong with the object in the image?\n\nGPT-4V:\n\nThere is a small tear or hole in the fabric.\n\nWhat is wrong with the object in the image?\n\nGPT-4V:\n\nIt is difficult to tell exactly what the object in the image\nis, but it appears to be a car bumper or fender with a\nlarge dent in it. The problem with the object is that it is\ndamaged and dented, which may affect the\nappearance and functionality of the car.\n\nWhat is wrong with the object in the image?\n\nGPT-4V:\n\nIt is difficult to say for sure what is wrong with the\nobject in the image as there is no context or\ninformation about what the object is or what it is\nsupposed to look like. However, it appears that there\nmay be a crack on one of the edges of the object.\n\nFigure 70: Defect detection with a single image. Yellow highlights the cases when GPT-4V is\nhesitating to make the predictions. Check Section 9.2 for detailed discussions.\n\n94\n", "vlm_text": "Sec. 9.2 Industry \nDefect Detection \nPrompt: What is wrong with the object in the image? \nThe image shows a hazelnut with a small hole in it.\nGPT-4V: \nThere is a small hole in the object, which appears  to be a hazelnut. This could be a sign of insect  damage or rot.  \nPrompt: \nWhat is wrong with the object in the image? \nThe image shows the head of a Phillips screw. It's a metal screw with a cross-shaped slot, commonly used for fastening materials.\nGPT-4V: \nThe object in the image appears to be a screw or  bolt with a damaged or stripped head. The  grooves in the head, which would normally allow  a screwdriver or wrench to grip and turn the  screw, are worn away and no longer functional.  \nPrompt: \nWhat is wrong with the object in the image? \nThe image shows a cross-section of an electrical cable consisting of three colored insulated copper wires. The wires have green, blue, and brown insulation, which are typical colors for electrical wiring indicating different functions such as grounding and conducting electricity.\nGPT-4V: \nIt is difficult to say for sure what is wrong with the  object in the image as there is no context or  information about what the object is or what it is  supposed to look like. However, it appears that  the copper wires in the blue and brown sections  are frayed and not neatly arranged, which could  indicate damage or poor quality. \nPrompt: What is wrong with the object in the image? \nThe image shows a piece of woven fabric with a grid-like texture, possibly burlap or linen. There is a small hole or imperfection in the weave.\nGPT-4V: There is a small tear or hole in the fabric. \nPrompt: \nWhat is wrong with the object in the image? \nThe image shows a close-up of a dent in a metallic surface. The reflection and texture indicate that it could be part of a vehicle, like a car bumper or fender.\nGPT-4V: \nIt is difficult to tell exactly what the object in the image  is, but it appears to be a car bumper or fender with a  large dent in it. The problem with the object is that it is  damaged and dented, which may affect the  appearance and functionality of the car.  \nPrompt: \nWhat is wrong with the object in the image? \nThe image shows an object that appears to be a metallic or ceramic component, possibly a cutting tool insert or a part of a machine. The object has a circular center hole and is symmetrical with four protruding edges or flanges. Each edge has a curved shape that resembles a cloverleaf or a cross. The material seems to have a glossy finish with some texturing or minor imperfections. There is also a small black mark or detail on the left side of the object.\nGPT-4V: \nIt is difficult to say for sure what is wrong with the  object in the image as there is no context or  information about what the object is or what it is  supposed to look like. However, it appears that there  may be a crack on one of the edges of the object. \nFigure 70: Defect detection with a single image.  Yellow  highlights the cases when GPT-4V is hesitating to make the predictions. Check Section  9.2  for detailed discussions. "}
{"page": 94, "image_path": "doc_images/2309.17421v2_94.jpg", "ocr_text": "Sec. 9.2 Industry\n\nDefect Detection\n\nPrompt: Prompt:\nWhat is wrong with the object in the image? What is wrong with the object in the image?\n\nWhat is wrong with the object in the image?\n\nGPT-4V:\n\nThere is large amount of brake on the rim of the wheel. This is typically caused by the brake pads wearing\ndown and creating dust as they come in contact with the rotors. Additionally, the tire appears to be low on air\npressure, which can affect the vehicle’s handling and fuel economy.\n\nFigure 71: Failure examples of defect detection with a single image./Red) highlights the cases when\nGPT-4V fails. Check Section 9.2 for detailed discussions.\n\n95\n", "vlm_text": "Sec. 9.2 Industry \nDefect Detection \nPrompt: What is wrong with the object in the image? \nThe image shows a capsule-shaped pill. It appears to have two distinct colored halves: one half is black and the other half is reddish-brown with a white marking that reads \"500\". This type of pill is usually associated with a specific dosage of medication, although the exact type of medication cannot be determined solely from the image. To identify the pill accurately, one should refer to a pill identifier tool or consult with a healthcare professional, using the pill's imprint, color, and shape as identifying factors.\nGPT-4V: There is not enough information provided to  determine if there is anything wrong with the  object in the image. It appears to be a capsule  medication with the number 500 printed on it, but  without further context, it is impossible to know if  there is anything wrong with it.  \nPrompt: What is wrong with the object in the image? \nThe image shows the top view of a dark, round container or bottle, likely made of glass or plastic. It's difficult to identify the exact contents without more context, but it appears to be a closed or empty opening.\nGPT-4V: Sorry, I cannot answer this question as there is no  context or information about what the object is or what  it is supposed to look like.   \nPrompt: What is wrong with the object in the image? \nThe image shows a car wheel, specifically an alloy rim with five spokes. It appears to be somewhat dirty, with signs of wear on the rim. The tire has the writing \"GOODYEAR WRANGLER\" on it.\nGPT-4V: There is large amount of brake on the rim of the wheel. This is typically caused by the brake pads wearing  down and creating dust as they come in contact with the rotors. Additionally, the tire appears to be low on air  pressure, which can affect the vehicle’s handling and fuel economy. \nFigure 71: Failure examples of defect detection with a single image.  Red  highlights the cases when GPT-4V fails. Check Section  9.2  for detailed discussions. "}
{"page": 95, "image_path": "doc_images/2309.17421v2_95.jpg", "ocr_text": "Sec. 9.2 Industry\n\nDefect Detection\n\nPrompt: Prompt:\nGiven Image 1, tell me what is wrong in image 2. Given Image 1, tell me what is wrong in image 2.\n\nImage 1 Image 1\n\n=\n=\n\nGPT-4V: GPT-4V:\nIn image 2, there is a small white mark on the In image 2, there is a small white spot on the\norange part of the pill. This could be a defect or lower left side of the inner circle. This could be a\n\ndamage to the pil. defect or damage on the surface of the object.\n\nGiven image 1, tell me if there are any defects\nseen on the object in image 2?\n\nImage 1\n\nGPT-4V:\n\nYes, there are a few defects seen on the object in image 2:\n- There is a significant amount of dirt and grime on the wheel, which is not present in image 1.\n\n- There appears to be some damage to the outer edge of the rim, which is not present in image 1.\n- The tire tread appears to be more worn in image 2 than in image 1.\n\nFigure 72: Defect detection with the help of a reference image./Red) highlights inaccurate descriptions.\n\nCheck Section 9.2 for detailed discussions.\n96\n", "vlm_text": "Sec. 9.2 Industry \nDefect Detection \nPrompt: Given Image 1, tell me what is wrong in image 2. \nThe image shows a capsule-shaped pill that is half black and half brown. It has the number \"500\" printed on the brown side. This marking is often a dosage indicator in milligrams for the medication within the capsule, and the pill's color and marking can be used to identify the specific medication, which in this case could be Ciprofloxacin hydrochloride with a dose of 500 mg. However, for accurate identification, one should consult a medical professional or a pill identifier resource.\nThe image shows a capsule-shaped pill that is half black and half brownish-orange. The brownish-orange half has the number \"500\" printed on it, along with a small icon that resembles an hourglass or sand timer. This pill appears to be an antibiotic known as Ciprofloxacin hydrochloride 500 mg.\nGPT-4V: \nIn image 2, there is a small white mark on the  orange part of the pill. This could be a defect or  damage to the pil. \nPrompt: Given Image 1, tell me what is wrong in image 2. \nThe image appears to show the top view of a dark-colored liquid inside what seems to be a round container or bottle. The circular shape and dark center suggest it could be a bottle with a dark liquid, such as a soda or beer, seen from above. The inner black circle may indicate the neck of the bottle. The gradient and rings may be due to reflections or the lighting conditions.\nImage 2 \nThe image shows a circular object that appears to have a small white spot on the lower left side of the inner circle. This white spot is identified as potentially being a defect or damage on the surface of the object. The circular object could be anything from a component part, a lens, or a piece of machinery or equipment, but the specific nature of the object is not clear just from the image. The caption accurately describes this visible imperfection.\nPrompt: \nGiven image 1, tell me if there are any defects  seen on the object in image 2? \nThe image shows a car wheel with a five-spoke alloy rim. The tire has the brand name \"WRANGLER\" visible on it. The center cap of the wheel has a logo on it. It is placed on a tiled floor.\nImage 2 \nThe image shows a car wheel with an alloy rim. The tire is mounted on the rim, and the word \"WRANGLER\" is visible on the tire. The center cap of the rim has a logo on it. The rim appears to have some dirt or wear marks.\nGPT-4V: Yes, there are a few defects seen on the object in image 2:\n\n \n- There is a significant amount of dirt and grime on the wheel, which is not present in image 1.\n\n - The center cap is missing from the wheel in image 2.\n\n - There appears to be some damage to the outer edge of the rim, which is not present in image 1.\n\n - The tire tread appears to be more worn in image 2 than in image 1. "}
{"page": 96, "image_path": "doc_images/2309.17421v2_96.jpg", "ocr_text": "Sec. 9.2 Industry\n\nSafety Inspection\nPrompt:\n\nHow many people are wearing helmet?\n\nThere are 8 persons wearing helmets in the image.\n\nSec. 9.2 Industry\n\nSafety Inspection Prompt Continued\nPrompt:\n\nPlease determine whether the person in the\nimage wears a helmet or not. And summarize\nhow many people are wearing helmets.\n\nImage 1\n\nGPT-4V:\n\nImage 1: Not wearing a helmet\nImage 2: Not wearing a helmet\nImage 3: Wearing a helmet\nImage 4: Wearing a helmet\nImage 5: Wearing a helmet\nImage 6: Wearing a helmet\nImage 7: Wearing a helmet\nImage 8: Not wearing a helmet\nSummary:\n\n(b)\nFigure 73: Application Highlights on Safety Inspection: Personal Protective Equipment (PPE)\nCounting. GPT-4V fails with zero-shot prompting in (a), while succeeds with single person crops in\n\n(b). ‘Red ( Green ) highlights the wrong (correct) answer. Check Section 9.2 for detailed discussions.\n97\n", "vlm_text": "The image is a comparison of two scenarios demonstrating the identification of Personal Protective Equipment (PPE), particularly helmets, in a safety inspection context.\n\nIn part (a), a group picture of several people is analyzed. GPT-4V initially fails to correctly count the number of people wearing helmets, incorrectly stating that 8 people are wearing them. This mistake is highlighted in red.\n\nIn part (b), individual crops of each person in the image are analyzed. Here, GPT-4V correctly identifies which individuals are wearing helmets and which are not. It summarizes that out of 8 people, 5 are wearing helmets and 3 are not, with the correct outcome highlighted in green."}
{"page": 97, "image_path": "doc_images/2309.17421v2_97.jpg", "ocr_text": "Sec. 9.2 Industry\nGrocery Checkout\n\nPrompt:\n\nFor below image, find what products are presented in the basket. Try to be accurate\n\nPink tulips, Salmon fillets, strawberries, apple, greek yogurt, sliced ham, raspberries\n\nSec. 9.2 Industry\n\nGrocery Checkout\n\nPrompt: Prompt Continued\nHere is a list of reference images:\n\nCheese\n\nPink Tulips\n\nFor below image, find what products are presented in the basket\naccording to reference image above. Only tell me products that\nare exactly same type of reference images.\n\nGPT-4V:\n\nThere are 5 products in the basket that are exactly the same\ntype as the reference images:\n\n- Smoked ham\n\n- Apple\n\n- Pink tulips\n\n- Crab dip\n\n- Strawberries\n\n(b)\nFigure 74: Application Highlights on Grocery Checkout. GPT-4V fails with zero-shot prompting\nin (a), while succeeds when prompting with reference product images in (b). ‘Red highlights the\nproducts that are not in the basket. Check Sections 9.2, 10.5 for detailed discussions.\n\n98\n", "vlm_text": "The image shows a comparison of two scenarios for identifying products in a grocery basket using GPT-4V, depicted as parts (a) and (b).\n\n**(a) Zero-Shot Prompting:**\n- A prompt asks to identify products in the basket from the image without any reference.\n- GPT-4V identifies: pink tulips, salmon fillets, strawberries, apple, Greek yogurt, sliced ham, raspberries.\n- Misidentified items (highlighted in red) include salmon fillets, Greek yogurt, and raspberries.\n\n**(b) Prompt with Reference Images:**\n- Reference images of shrimp, cheese, crab dip, strawberries, smoked ham, pink tulips, banana, apple, and bread are provided.\n- GPT-4V identifies products in the basket by matching them to reference images.\n- Correctly identified products: smoked ham, apple, pink tulips, crab dip, strawberries.\n\nThis demonstrates that providing reference images improves the accuracy of product identification in the basket."}
{"page": 98, "image_path": "doc_images/2309.17421v2_98.jpg", "ocr_text": "9.3. Medical\n\nIn Section 4.1, the effectiveness of GPT-4V in medical image understanding is demonstrated through\nFigures 18-19. Furthermore, we conducted a detailed investigation into the application of GPT-4V in\nradiology report generation, as depicted in Figures 75-78. In this scenario, we provided GPT-4V with\nvarious medical images and tasked it with generating complete radiology reports. Since assessing the\naccuracy of the generated reports requires domain knowledge, we sought the evaluation of a medical\nprofessional.\n\nFigure 75 showcases two accurate examples: one involving an abdominal X-ray image and another\nfeaturing an MRI of the right knee. In both cases, GPT-4V correctly identified the study and provided\nan accurate diagnosis. Moving on to Figure 76, we observe the generated radiology reports by GPT-\nAV for X-ray images of the hand/wrist. While GPT-4V successfully diagnosed the recommended\nmanagement based on the first X-ray image, it missed the obvious distal radial fracture present in\nthe second X-ray image. Nevertheless, the generated reports maintain a high-quality format that can\nserve as a template, thus reducing the workload for medical professionals when drafting reports.\n\nIn Figure 77, we explore two additional examples involving a Chest CT and a Brain MRI. In the Chest\nCT case, GPT-4V mistakenly identified the mentioned nodule on the left side instead of the right side,\nand it also hallucinated the measurements. The ability to process interleaved image-text pairs also\nallows GPT-4V to reference prior medical scans and diagnosis histories, which is shown to be critical\nin medical professionals’ diagnosing processes [15]. Figure 78 shows examples of understanding the\nsymptom progression from multiple Chest X-Ray scans [60, 15]. These illustrations shed light on\nthe potential of GPT-4V to serve as an AJ assistant for radiology report generation. However, it is\ncrucial to have the generated reports evaluated by medical professionals to ensure their correctness\nand accuracy.\n\n99\n", "vlm_text": "9.3 Medical \nIn Section  4.1 , the effectiveness of GPT-4V in medical image understanding is demonstrated through Figures  18 - 19 . Furthermore, we conducted a detailed investigation into the application of GPT-4V in radiology report generation, as depicted in Figures  75 - 78 . In this scenario, we provided GPT-4V with various medical images and tasked it with generating complete radiology reports. Since assessing the accuracy of the generated reports requires domain knowledge, we sought the evaluation of a medical professional. \nFigure  75  showcases two accurate examples: one involving an abdominal X-ray image and another featuring an MRI of the right knee. In both cases, GPT-4V correctly identified the study and provided an accurate diagnosis. Moving on to Figure  76 , we observe the generated radiology reports by GPT- 4V for X-ray images of the hand/wrist. While GPT-4V successfully diagnosed the recommended management based on the first X-ray image, it missed the obvious distal radial fracture present in the second X-ray image. Nevertheless, the generated reports maintain a high-quality format that can serve as a template, thus reducing the workload for medical professionals when drafting reports. \nIn Figure  77 , we explore two additional examples involving a Chest CT and a Brain MRI. In the Chest CT case, GPT-4V mistakenly identified the mentioned nodule on the left side instead of the right side, and it also hallucinated the measurements. The ability to process interleaved image-text pairs also allows GPT-4V to reference prior medical scans and diagnosis histories, which is shown to be critical in medical professionals’ diagnosing processes [ 15 ]. Figure  78  shows examples of understanding the symptom progression from multiple Chest X-Ray scans [ 60 ,  15 ]. These illustrations shed light on the potential of GPT-4V to serve as an AI assistant for radiology report generation. However, it is crucial to have the generated reports evaluated by medical professionals to ensure their correctness and accuracy. "}
{"page": 99, "image_path": "doc_images/2309.17421v2_99.jpg", "ocr_text": "Sec. 9.3 Medical\nRadiology Report Generation\n\nPrompt:\nWrite a radiology report for this image as an expert radiologist..\n\nGPT-4V:\n\nRadiology Report\n\nDate: 03/23/2021\n\nExam: Abdominal x-ray\n\nHistory: Patient presents with abdominal pain and distention\n\nFindings: The abdominal x-ray demonstrates a dilated stomach and small bowel loops, suggestive of a partial small bowel obstruction.\nThere is also a small amount of air in the rectum. No free intraperitoneal air or pneumoperitoneum is noted. The liver, spleen, and\nkidneys appear unremarkable.\n\nImpression: Findings are suggestive of a partial small bowel obstruction. Further imaging studies and clinical correlation are\nrecommended.\n\nEvaluation from Professionals:\nCorrect study identification, correct diagnosis\n\nWrite a radiology report as an expert radiologist for the following case.\n\nGPT-4V:\n\nReport:\n\nMRI of the right knee demonstrates a large joint effusion, which is most pronounced in the suprapatellar region. There is also evidence\nof a tear in the medial meniscus, as well as mild degenerative changes in the articular cartilage. The anterior cruciate ligament and\nposterior cruciate ligament are intact. The extensor mechanism is also intact. No other significant abnormalities are seen.\n\nImpression: Large joint effusion and medial meniscal tear in the right knee, with mild degenerative changes in the articular cartilage.\n\nEvaluation from Professionals:\nCorrect study identification, correct diagnosis (effusion and meniscal tear)\n\nFigure 75: Application Highlights on Radiology Report Generation. The generated report is reviewed\n\nby a medical professional to evaluate its correctness. Green highlights that a medical professional\nhas confirmed the described part of the report is correct. Check Section 9.3 for detailed discussions.\nThe medical images are from the internet.\n\n100\n", "vlm_text": "Radiology Report Generation \nPrompt: \nWrite a radiology report for this image as an expert radiologist.. \nThis image appears to be an X-ray of the abdomen, often referred to as an abdominal X-ray or KUB (Kidney, Ureter, Bladder). The image shows the skeletal structure of the spine and pelvis, as well as the outline of the intestines and, potentially, other abdominal organs.\n\nObservations:\n1. **Spinal Alignment**: The lumbar spine is visible, with vertebral bodies identifiable. Look for any signs of abnormal curvature or misalignment.\n   \n2. **Bowel Gas Patterns**: The intestines can be seen filled with various degrees of gas, giving an indication of bowel structure. Look for any signs of abnormal distension which could suggest bowel obstruction or ileus.\n\n3. **Calcifications**: Any notable calcifications or radiopaque objects should be examined. This could indicate gallstones, kidney stones, or other abnormalities.\n\n4. **Soft Tissue Shadows**: The image may show outlines of soft tissue structures. This may help in identifying masses or organ enlargement.\n\n5. **Bones**: Check for any fractures or bone lesions in the visible skeletal structures.\n\n6. **Foreign Objects**: Look for any visible medical devices or foreign materials.\n\nIt's important to note that a definitive diagnosis or complete analysis should be performed by a qualified radiologist, who would consider both this image and clinical findings.\nGPT-4V: \nRadiology Report Date: 03/23/2021 Exam: Abdominal x-ray History: Patient presents with abdominal pain and distention Findings: The abdominal x-ray demonstrates a dilated stomach and small bowel loops, suggestive of a partial small bowel obstruction.  There is also a small amount of air in the rectum. No free intra peritoneal air or p neum o peritoneum is noted. The liver, spleen, and  kidneys appear unremarkable. Impression: Findings are suggestive of a partial small bowel obstruction. Further imaging studies and clinical correlation are  recommended.   \nEvaluation from Professionals: Correct study identification, correct diagnosis \nPrompt: \nWrite a radiology report as an expert radiologist for the following case. \nThis image appears to be an MRI scan of a knee joint. It shows structures like the femur, tibia, and joint space, which includes the meniscus. MRI images are commonly used to assess joint conditions, such as tears or degeneration.\nGPT-4V: \nReport: \nMRI of the right knee demonstrates a large joint effusion, which is most pronounced in the supra patella r region. There is also evidence  of a tear in the medial meniscus, as well as mild degenerative changes in the articular cartilage. The anterior cruciate ligament and  posterior cruciate ligament are intact. The extensor mechanism is also intact. No other significant abnormalities are seen. Impression: Large joint effusion and medial meniscal tear in the right knee, with mild degenerative changes in the articular cartilage.  \nEvaluation from Professionals: \nCorrect study identification, correct diagnosis (effusion and meniscal tear) \nFigure 75: Application Highlights on Radiology Report Generation. The generated report is reviewed by a medical professional to evaluate its correctness.  Green  highlights that a medical professional has confirmed the described part of the report is correct. Check Section  9.3  for detailed discussions. The medical images are from the internet. "}
{"page": 100, "image_path": "doc_images/2309.17421v2_100.jpg", "ocr_text": "Sec. 9.3 Medical\nRadiology Report Generation\n\nPrompt:\nWrite a radiology report as an expert radiologist for the following case:\n\nCase:\nA 32-year-old woman presented with a complaint of pain in her right wrist. The patient reported that she fell on her outstretched hand\nwhile playing basketball. An X-ray of the right wrist was performed.\n\nRadiology Report:\n\nDate: 01/01/2021\n\nExam: X-ray of the right wrist\n\nIndication: Right wrist pain after falling on outstretched hand while playing basketball\n\nFindings: There is a transverse fracture of the distal radius with mild dorsal angulation. No other fractures or dislocations are seen. The\ncarpal bones are in normal alignment. No significant soft tissue swelling is noted.\n\nImpression: Transverse fracture of the distal radius with mild dorsal angulation. No other fractures or dislocations are seen.\nRecommendation: Orthopedic consultation for management of the fracture.\n\nEvaluation from Professionals:\nCorrect diagnosis and management\n\nWrite a radiology report for this image as an expert radiologist.\n\nGPT-4V:\n\nReport:\n\nDate: 01/01/2021\n\nExam: X-ray of the right hand and wrist\n\nTechnique: AP and lateral views were obtained\n\nFindings: The bones of the right hand and wrist appear intact and in proper alignment. There is no evidence of fracture or dislocation.\nThe joint spaces are preserved and there is no evidence of degenerative changes. No soft tissue abnormalities are seen.\nImpression: Normal X-ray of the right hand and wrist.\n\nEvaluation from Professionals:\n\nCorrect identification of the type of study\n\nFigure 76: Application Highlights on Radiology Report Generation. The generated report is reviewed\n\nby a medical professional to evaluate its correctness. Green (Red!) highlights that a medical\nprofessional has confirmed the described part of the report is correct (incorrect). Check Section 9.3\nfor detailed discussions. The medical images are from the Internet.\n\n101\n", "vlm_text": "Radiology Report Generation \nPrompt: \nWrite a radiology report as an expert radiologist for the following case: \nCase: \nA 32-year-old woman presented with a complaint of pain in her right wrist. The patient reported that she fell on her outstretched hand  while playing basketball. An X-ray of the right wrist was performed. \nThis is an X-ray image of a forearm and wrist. It shows the bones, including the radius and ulna of the forearm, and the carpal bones of the wrist. This X-ray appears to show a fracture in the radius near the wrist.\nGPT-4V: \nRadiology Report: Date: 01/01/2021 Exam: X-ray of the right wrist Indication: Right wrist pain after falling on outstretched hand while playing basketball Findings: There is a transverse fracture of the distal radius with mild dorsal angulation. No other fractures or dislocations are seen. The  carpal bones are in normal alignment. No significant soft tissue swelling is noted. Impression: Transverse fracture of the distal radius with mild dorsal angulation. No other fractures or dislocations are seen. Recommendation: Orthopedic consultation for management of the fracture.  \nEvaluation from Professionals: Correct diagnosis and management \nPrompt: \nWrite a radiology report for this image as an expert radiologist. \nThis image is an X-ray of a hand and wrist. It shows the bones including the radius and ulna (forearm bones), carpal bones (wrist bones), metacarpals, and phalanges (finger bones).\nGPT-4V: \nReport: Date: 01/01/2021 Exam: X-ray of the right hand and wrist Technique: AP and lateral views were obtained Findings: The bones of the right hand and wrist appear intact and in proper alignment. There is no evidence of fracture or dislocation.  The joint spaces are preserved and there is no evidence of degenerative changes. No soft tissue abnormalities are seen. Impression: Normal X-ray of the right hand and wrist. \nEvaluation from Professionals: \nMissed the obvious distal radial fracture Correct identification of the type of study \nFigure 76: Application Highlights on Radiology Report Generation. The generated report is reviewed by a medical professional to evaluate its correctness. Green  (  Red  ) highlights that a medical professional has confirmed the described part of the report is correct (incorrect). Check Section  9.3 for detailed discussions. The medical images are from the Internet. "}
{"page": 101, "image_path": "doc_images/2309.17421v2_101.jpg", "ocr_text": "Sec. 9.3 Medical\nRadiology Report Generation\n\nPrompt:\nWrite a radiology report for this image as an expert radiologist..\n\nGPT-4V:\n\nReport:\n\nDate: 6/25/2021\n\nStudy Type: Chest CT\n\nSubject: 42-year-old male\n\nFindings: The chest CT images demonstrate a nodular opacity in the left upper lobe, measuring approximately 1.3 cm in diameter. The\nopacity appears to have spiculated margins and is located adjacent to the pleura. There is no evidence of mediastinal or hilar\nlymphadenopathy.\n\nImpression: The findings are concerning for a primary lung malignancy. Further evaluation with a biopsy is recommended to confirm the\ndiagnosis.\n\nEvaluation from Professionals:\nCorrect study type and identification\nMensions nodule\n\nGives measurements (hallucination)\n\nWrite a radiology report as an expert radiologist for the following case.\n\nGPT-4V:\n\nReport:\n\nDate: 01/01/2021\n\nModality: Magnetic Resonance Imaging (MRI)\n\nBody Part: Brain\n\nTechnique: Axial T1-weighted\n\nFindings: The MRI of the brain demonstrates a large intra-axial mass in the left parietal lobe with surrounding edema. The mass appears\nto be heterogeneous in signal intensity with areas of high and low signal intensity. There is a significant mass effect with midline shift to\nthe right. The ventricles appear to be compressed and there is evidence of hydrocephalus. No other significant abnormalities are noted.\nImpression: Large intra-axial mass in the left parietal lobe with surrounding edema and significant mass effect. Likely high-grade glioma.\nFurther imaging and clinical correlation are recommended.\n\nEvaluation from Professionals:\nCorrect study identification, correct mass description and location, correct diagnosis\n\nFigure 77: Application Highlights on Radiology Report Generation. The generated report is reviewed\nby a medical professional to evaluate its correctness. Green (Red!) highlights that a medical\nprofessional has confirmed the described part of the report is correct (incorrect). Yellow indicates\nthat the model is hallucinating. Check Section 9.3 for detailed discussions. The medical images are\n\nfrom the internet.\n102\n", "vlm_text": "Sec. 9.3 Medical \nRadiology Report Generation \nPrompt: \n## Radiology Report\n\n**Patient Details:**\n- [Patient Name/ID: Redacted]\n- [Age/Gender: Redacted]\n- [Date of Examination: Redacted]\n- [Modality: CT Scan (Chest)]\n\n**Clinical Indication:**\n- Evaluation of respiratory symptoms.\n- Rule out neoplastic or infectious process.\n\n**Technique:**\nCT scan of the chest was performed with contrast enhancement. Axial images were obtained.\n\n**Findings:**\n\n### Lungs and Airways:\n- There is a well-circumscribed, round nodule measuring approximately [size not measured from image] located in the left upper lobe. The nodule exhibits spiculated margins, raising suspicion for a primary neoplastic process, such as a lung carcinoma.\n- No cavitation or calcification noted within the nodule.\n- Multiple smaller nodular opacities observed in the peripheral regions of both the left and right upper lobes, suggestive of possible satellite lesions or metastatic nodules.\n- No evidence of pleural effusion or pneumothorax.\n- The central airways are patent.\n\n### Mediastinum and Heart:\n- The mediastinum appears within normal limits in size. No lymphadenopathy is identified in the sampled image.\n- The heart is normal in size and configuration. No pericardial effusion is noted.\n\n### Bones and Soft Tissues:\n- No bony lesions or significant degenerative changes observed in the visible osseous structures.\n- The soft tissues of the thoracic wall appear unremarkable.\n\n**Impression:**\n- Spiculated nodule in the left upper lobe is suspicious for a neoplastic process, most likely a primary lung carcinoma. Further evaluation with a PET scan or biopsy is recommended for definitive diagnosis.\n- Note of multiple smaller nodules raises concern for metastatic disease; correlation with clinical history and further investigation is advised.\n\n**Recommendations:**\n- Recommend follow-up with pulmonology or oncology for further diagnostic workup.\n- Consider PET-CT for metabolic assessment of the identified nodule.\n- Tissue diagnosis via CT-guided biopsy may be warranted depending on multidisciplinary team suggestions.\n\n**Radiologist:**\n- [Name]\n- [Contact Information]\n- [Date]\n\n**Note:** This is a mock report created for educational purposes based on the provided image. Actual patient care decisions should be made based on the full clinical context and consultation with appropriate medical professionals.\nGPT-4V: \nReport: Date: 6/25/2021 Study Type: Chest CT Subject: 42-year-old male \nFindings: The chest CT images demonstrate a nodular opacity in the left upper lobe, measuring approximately  $1.3\\,\\mathsf{c m}$   in diameter. The  opacity appears to have spiculated margins and is located adjacent to the pleura. There is no evidence of media st in al or hilar  lymph aden opa thy. \nImpression: The findings are concerning for a primary lung malignancy. Further evaluation with a biopsy is recommended to confirm the  diagnosis.  \nEvaluation from Professionals: Correct study type and identification Mensions nodule but in the left rather than in the right. Gives measurements (hallucination) \nPrompt: \nWrite a radiology report as an expert radiologist for the following case. \nThis image appears to be an MRI scan of a human brain, specifically an axial view (looking at the brain from the top down). The scan shows the presence of a large mass or lesion on the right side of the brain (which appears on the left side of the image due to the orientation). This mass is irregular in shape and contains regions of varying intensity, which could suggest a tumor or other abnormality. The MRI shows cross sections through structures such as the eyes, skull, and brain tissue. For an accurate diagnosis, the scan should be assessed by a radiologist or medical professional with expertise in interpreting medical images.\nGPT-4V: \nReport: Date: 01/01/2021 Modality: Magnetic Resonance Imaging (MRI) Body Part: Brain Technique: Axial T1-weighted Findings: The MRI of the brain demonstrates a large intra-axial mass in the left parietal lobe with surrounding edema. The mass appears  to be heterogeneous in signal intensity with areas of high and low signal intensity. There is a significant mass effect with midline shift to  the right. The ventricles appear to be compressed and there is evidence of hydro ce ph alus. No other significant abnormalities are noted. Impression: Large intra-axial mass in the left parietal lobe with surrounding edema and significant mass effect. Likely high-grade glioma.  Further imaging and clinical correlation are recommended.  \nEvaluation from Professionals: Correct study identification, correct mass description and location, correct diagnosis \nFigure 77: Application Highlights on Radiology Report Generation. The generated report is reviewed by a medical professional to evaluate its correctness. Green  (  Red  ) highlights that a medical professional has confirmed the described part of the report is correct (incorrect).  Yellow  indicates that the model is hallucinating. Check Section  9.3  for detailed discussions. The medical images are from the internet. "}
{"page": 102, "image_path": "doc_images/2309.17421v2_102.jpg", "ocr_text": "Sec. 9.3 Medical\nRadiology Report Generation with Diagnosis History\n\nPrompt:\nThe first image is a chest x-ray scan the patient took last time.\nThe previous diagnoses mention pleural effusion.\n\nThe second image is a chest x-ray scan the patient took this\ntime.\n\nWrite a radiology report as an expert radiologist for the\nsymptoms and changes.\n\nGPT-4V:\n\nRadiology Report\n\nDate: 01/01/2021\n\nPatient: John Doe\n\nExam: Chest X-ray\n\nFindings: The chest x-ray scan taken this time shows a\nsignificant improvement in the patient's condition compared to\nthe previous scan. The previous scan showed a pleural effusion,\nwhich is a buildup of fluid in the space between the lungs and\nchest wall. However, the current scan shows that the pleural\neffusion has resolved. There is no evidence of any new\nabnormalities in the lungs or chest wall. The lungs appear to be\nwell-expanded and the heart size is normal. The mediastinum,\nwhich is the space between the lungs, is also within normal\nlimits.\n\nConclusion: The patient's condition has improved since the last\nchest x-ray scan, with resolution of the previously noted pleural\neffusion. No new abnormalities are seen.\n\nAnnotations from Professionals:\nImproving pleural effusion\n\nPrompt:\nThe first image is a chest x-ray scan the patient took last time.\nThe previous diagnoses mention pleural effusion.\n\nThe second image is a chest x-ray scan the patient took this\ntime.\n\nWrite a radiology report as an expert radiologist for the\nsymptoms and changes.\n\nGPT-4V:\n\nRadiology Report\n\nDate: 12/12/2021\n\nPatient: Jane Doe\n\nExam: Chest X-ray\n\nFindings: The chest x-ray scan taken this time shows a\nsignificant worsening of the patient's pleural effusion. The fluid\naccumulation in the pleural space is more prominent than in the\nprevious scan. The lung fields are also more hazy and\ncongested, indicating a possible infection or inflammation. The\nheart and mediastinal contours are obscured by the fluid, making\nit difficult to assess their size and shape.\n\nImpression: The patient's pleural effusion has worsened, and\nthere is a possibility of an underlying infection or inflammation.\nFurther evaluation and management are recommended.\n\nAnnotations from Professionals:\nWorsening pleural effusion\n\nFigure 78: Application Highlights on Radiology Report Generation with Diagnosis History. Check\nSection 9.3 for detailed discussions. The medical images are from MIMIC dataset [60].\n", "vlm_text": "Sec. 9.3 Medical \nRadiology Report Generation with Diagnosis History \nPrompt: \nThe first image is a chest x-ray scan the patient took last time.  The previous diagnoses mention pleural effusion. \nThe image is a chest X-ray, which is a type of medical imaging used to examine the chest, including the heart, lungs, and bones. The X-ray shows the expected anatomy: two lungs, heart, and the surrounding structures. The darker areas represent the air-filled lungs, while the lighter areas show denser structures like bones and the heart. The ribs and spine can also be faintly seen. Interpreting the details of such an image requires medical expertise, as it involves looking for abnormalities or signs of medical conditions.\nThis image appears to be a chest X-ray displaying the thoracic cavity, likely showing the lungs and heart. Generally, such images are used to assess the condition of the lungs or heart for medical diagnosis.\nWrite a radiology report as an expert radiologist for the  symptoms and changes. \nGPT-4V: \nRadiology Report Date: 01/01/2021 Patient: John Doe Exam: Chest X-ray \nFindings: The chest x-ray scan taken this time shows a  significant improvement in the patient's condition compared to  the previous scan. The previous scan showed a pleural effusion,  which is a buildup of fluid in the space between the lungs and  chest wall. However, the current scan shows that the pleural  effusion has resolved. There is no evidence of any new  abnormalities in the lungs or chest wall. The lungs appear to be  well-expanded and the heart size is normal. The media st in um,  which is the space between the lungs, is also within normal  limits. \nConclusion: The patient's condition has improved since the last  chest x-ray scan, with resolution of the previously noted pleural  effusion. No new abnormalities are seen. \nAnnotations from Professionals: Improving pleural effusion \nPrompt: \nThe first image is a chest x-ray scan the patient took last time.  The previous diagnoses mention pleural effusion. \nThe image is a chest X-ray, typically used to examine the health of the lungs, heart, and chest wall. It shows the ribcage and the outline of the lungs and heart. Please consult a medical professional for interpretation and diagnosis.\nThis is an X-ray image of a chest. It shows the ribcage, lungs, and other structures within the thoracic cavity.\nWrite a radiology report as an expert radiologist for the  symptoms and changes. \nGPT-4V: \nRadiology Report Date: 12/12/2021 Patient: Jane Doe Exam: Chest X-ray \nFindings: The chest x-ray scan taken this time shows a  significant worsening of the patient's pleural effusion. The fluid  accumulation in the pleural space is more prominent than in the  previous scan. The lung fields are also more hazy and  congested, indicating a possible infection or inflammation. The  heart and media st in al contours are obscured by the fluid, making  it difficult to assess their size and shape.  Impression: The patient's pleural effusion has worsened, and  there is a possibility of an underlying infection or inflammation.  Further evaluation and management are recommended. \nAnnotations from Professionals: Worsening pleural effusion "}
{"page": 103, "image_path": "doc_images/2309.17421v2_103.jpg", "ocr_text": "9.4 Auto Insurance\n\nIn this section, we explore another practical application of GPT-4V in the field of auto insurance,\nfocusing specifically on car accident reporting. Within this context, we can further delineate two\ndistinct sub-categories: (7) Damage Evaluation and (77) Insurance Reporting. The former involves the\ncrucial task of accurately identifying and assessing the extent of damages sustained by vehicles, while\nthe latter encompasses not only damage identification but also the recognition of vehicle-specific\ninformation depicted in images, such as the make, model, license plate, and other relevant details. By\naddressing both aspects, we aim to demonstrate the comprehensive capabilities of GPT-4V within the\nauto insurance domain.\n\nDamage evaluation. We present an image depicting car damage to GPT-4V and prompt it with\n“Imagine that you are an expert in evaluating the car damage from car accident for auto insurance\nreporting. Please evaluate the damage seen in the image below.” in Figure 79. GPT-4V has\ndemonstrated remarkable proficiency in accurately identifying and precisely localizing the damages\ndepicted in all four images. Furthermore, it impresses with its ability to provide detailed descriptions\nof each specific damage instance. In some instances, GPT-4V even endeavors to estimate the potential\ncost of repair.\n\nInsurance reporting. Building on the success in damage evaluation, we modify our prompt to\nask GPT-4V to identify the make, model, and license plate of the vehicle depicted in the image, and\nreturn the obtained information in JSON format. The examples depicted in Figure 80 showcase this\ncapability. In both instances, GPT-4V attempts to extract all the requested details from the image.\nHowever, it should be noted that certain information may be unavailable, such as the estimated cost\nof repair, or challenging to discern due to occlusion, as observed with the license plate in the second\nimage. It is important to note that real-life insurance reporting typically involves multiple images\ncapturing the car from various angles, a scenario that is usually not publicly accessible on the Internet.\nNevertheless, the examples in Figures 79-80 vividly illustrate the potential of GPT-4V in automating\nthe insurance reporting process for car accidents.\n\n104\n", "vlm_text": "9.4 Auto Insurance \nIn this section, we explore another practical application of GPT-4V in the field of auto insurance, focusing specifically on car accident reporting. Within this context, we can further delineate two distinct sub-categories: ( i ) Damage Evaluation and   $(i i)$   Insurance Reporting. The former involves the crucial task of accurately identifying and assessing the extent of damages sustained by vehicles, while the latter encompasses not only damage identification but also the recognition of vehicle-specific information depicted in images, such as the make, model, license plate, and other relevant details. By addressing both aspects, we aim to demonstrate the comprehensive capabilities of GPT-4V within the auto insurance domain. \nDamage evaluation. We present an image depicting car damage to GPT-4V and prompt it with “Imagine that you are an expert in evaluating the car damage from car accident for auto insurance reporting. Please evaluate the damage seen in the image below.” in Figure  79 . GPT-4V has demonstrated remarkable proficiency in accurately identifying and precisely localizing the damages depicted in all four images. Furthermore, it impresses with its ability to provide detailed descriptions of each specific damage instance. In some instances, GPT-4V even endeavors to estimate the potential cost of repair. \nInsurance reporting. Building on the success in damage evaluation, we modify our prompt to ask GPT-4V to identify the make, model, and license plate of the vehicle depicted in the image, and return the obtained information in JSON format. The examples depicted in Figure  80  showcase this capability. In both instances, GPT-4V attempts to extract all the requested details from the image. However, it should be noted that certain information may be unavailable, such as the estimated cost of repair, or challenging to discern due to occlusion, as observed with the license plate in the second image. It is important to note that real-life insurance reporting typically involves multiple images capturing the car from various angles, a scenario that is usually not publicly accessible on the Internet. Nevertheless, the examples in Figures  79 - 80  vividly illustrate the potential of GPT-4V in automating the insurance reporting process for car accidents. "}
{"page": 104, "image_path": "doc_images/2309.17421v2_104.jpg", "ocr_text": "Sec. 9.4 Auto Insurance\n\nDamage Evaluation\n\nPrompt:\n\nImagine that you are an expert in evaluating the\ncar damage from car accident for auto insurance\nreporting. Please evaluate the damage seen in the\nimage below.\n\nBased on the image, it appears that the car has\nsustained damage to the front passenger side\nbumper. The damage appears to be in the form of\nscrapes and scratches, with some areas of the\npaint chipped away. The damage appears to be\ncosmetic and does not appear to have affected\nthe structural integrity of the bumper or the car.\nHowever, it is important to note that the image\nonly shows one angle of the car and does not\nprovide a full view of the damage. A more\nthorough inspection of the car would be needed\nto fully assess the extent of the damage and\ndetermine if there are any underlying issues that\nare not visible in the image.\n\nImagine that you are an expert in evaluating the\ncar damage from car accident for auto insurance\nreporting. Please evaluate the damage seen in the\nimage below.\n\nBased on the image, it appears that the car has\nsustained damage to the rear bumper and quarter\npanel on the driver's side. The bumper appears to\nbe dented and scratched, and the quarter panel\nhas a large dent and some scuff marks. It is likely\nthat the car was hit from the side, causing the\ndamage to the quarter panel and bumper. In\nterms of repair costs, the bumper may be able to\nbe repaired with some minor body work and\nrepainting. However, the quarter panel may\nrequire more extensive repairs or even\nreplacement, which could be more costly. Overall,\n| would estimate the damage to be in the\nmoderate to severe range, with repair costs likely\nin the thousands of dollars.\n\nPrompt:\n\nImagine that you are an expert in evaluating the car\ndamage from car accident for auto insurance\nreporting. Please evaluate the damage seen in the\nimage below.\n\nGPT-4V:\n\nBased on the image, it appears that the vehicle has\nsustained significant damage to the front end. The\nhood is crumpled and the front bumper is partially\ndetached. The headlights also appear to be damaged.\nIt is likely that the vehicle was involved in a front-end\ncollision. The damage appears to be primarily\ncosmetic, but it is possible that there may be\nunderlying structural damage to the frame or engine.\nFurther inspection would be required to assess the full\nextent of the damage. In terms of repair costs, the\nhood and bumper would likely need to be replaced,\nand the headlights may also need to be replaced or\nrepaired. The cost of these repairs would depend on\nthe make and model of the vehicle, but it is likely that\nthe cost would be several thousand dollars at\nminimum.\n\nImagine that you are an expert in evaluating the car\ndamage from car accident for auto insurance\nreporting. Please evaluate the damage seen in the\n\nimage below.\n—\n\nGPT-4V:\n\nBased on the image, it appears that the vehicle has\nsustained significant damage to the front end. The\nhood is crumpled and the front bumper is completely\ndetached from the vehicle. The headlights and grille\nare also damaged and the engine appears to be\nexposed. Additionally, there is damage to the driver's\nside fender and door. Given the extent of the damage,\nit is likely that the vehicle was involved in a\nhigh-speed collision or a head-on collision. The cost\nof repairs for this vehicle will likely be significant and it\nmay be deemed a total loss by the insurance\ncompany.\n\nFigure 79: Application Highlights on Auto Damage Evaluation. Check Section 9.4 for detailed\n\ndiscussions.\n\n105\n", "vlm_text": "Sec. 9.4 Auto Insurance \nDamage Evaluation \nPrompt: \nImagine that you are an expert in evaluating the  car damage from car accident for auto insurance  reporting. Please evaluate the damage seen in the  image below. \nThe image shows a close-up view of a car's front bumper and wheel. There are visible scratches and paint damage on the bumper.\nGPT-4V: \nBased on the image, it appears that the car has  sustained damage to the front passenger side  bumper. The damage appears to be in the form of  scrapes and scratches, with some areas of the  paint chipped away. The damage appears to be  cosmetic and does not appear to have affected  the structural integrity of the bumper or the car.  However, it is important to note that the image  only shows one angle of the car and does not  provide a full view of the damage. A more  thorough inspection of the car would be needed  to fully assess the extent of the damage and  determine if there are any underlying issues that  are not visible in the image.  \nPrompt: \nImagine that you are an expert in evaluating the  car damage from car accident for auto insurance  reporting. Please evaluate the damage seen in the  image below. \nThe image shows two cars, with the focus on the rear section of a gray car that appears to have some damage. The other car is in the background, slightly out of focus. The setting appears to be a street or parking area.\nGPT-4V: \nBased on the image, it appears that the car has  sustained damage to the rear bumper and quarter  panel on the driver's side. The bumper appears to  be dented and scratched, and the quarter panel  has a large dent and some scuff marks. It is likely  that the car was hit from the side, causing the  damage to the quarter panel and bumper. In  terms of repair costs, the bumper may be able to  be repaired with some minor body work and  repainting. However, the quarter panel may  require more extensive repairs or even  replacement, which could be more costly. Overall,  I would estimate the damage to be in the  moderate to severe range, with repair costs likely  in the thousands of dollars.  \nPrompt: \nImagine that you are an expert in evaluating the car  damage from car accident for auto insurance  reporting. Please evaluate the damage seen in the  image below. \nThe image shows a blue car with significant front-end damage, likely from a collision. The hood is crumpled and bent upwards, and the front bumper appears to be damaged as well. The car is parked on a street, with other vehicles visible in the background.\nGPT-4V: \nBased on the image, it appears that the vehicle has  sustained significant damage to the front end. The  hood is crumpled and the front bumper is partially  detached. The headlights also appear to be damaged.  It is likely that the vehicle was involved in a front-end  collision. The damage appears to be primarily  cosmetic, but it is possible that there may be  underlying structural damage to the frame or engine.  Further inspection would be required to assess the full  extent of the damage. In terms of repair costs, the  hood and bumper would likely need to be replaced,  and the headlights may also need to be replaced or  repaired. The cost of these repairs would depend on  the make and model of the vehicle, but it is likely that  the cost would be several thousand dollars at  minimum.  \nPrompt: \nImagine that you are an expert in evaluating the car  damage from car accident for auto insurance  reporting. Please evaluate the damage seen in the  image below. \nThe image shows a white sedan with significant front-end damage, suggesting it was involved in a collision. The car is on a roadway with emergency responders, including police and a fire truck, visible in the background. The scene appears to be an accident on a road where assistance is being provided.\nGPT-4V: \nBased on the image, it appears that the vehicle has  sustained significant damage to the front end. The  hood is crumpled and the front bumper is completely  detached from the vehicle. The headlights and grille  are also damaged and the engine appears to be  exposed. Additionally, there is damage to the driver's  side fender and door. Given the extent of the damage,  it is likely that the vehicle was involved in a  high-speed collision or a head-on collision. The cost  of repairs for this vehicle will likely be significant and it  may be deemed a total loss by the insurance  company.  "}
{"page": 105, "image_path": "doc_images/2309.17421v2_105.jpg", "ocr_text": "Sec. 9.4 Auto Insurance\n\nInsurance Report Generation\n\nPrompt:\n\nImagine that you are an expert in evaluating the car damage from car accident for auto insurance reporting.\nPlease fill the incident report for the accident shown in image below, with the following format in JSON (note\nxxx is placeholder, if you cannot determine that based on the image, put \"N/A\" instead).\n\n{\"make\": xxx, \"model\": xxx, \"license plate\": xxx, \"damage description\": xxx, \"estimated cost of repair\": xxx}\n\nGPT-4V:\n{\"make\": \"Maruti Suzuki\", \"model\": \"Celerio\", \"license plate\": \"KL 52 N 5305\", \"damage description\": \"Front\nbumper, hood, and windshield damaged. Airbags deployed.\", \"estimated cost of repair\": \"N/A\"}\n\nPrompt:\n\nImagine that you are an expert in evaluating the car damage from car accident for auto insurance reporting.\nPlease evaluate the damage seen in the image below. For filing the incident report, please follow the following\nformat in JSON (note xxx is placeholder, if the information is not available in the image, put \"N/A\" instead).\n{\"make\": xxx, \"model\": xxx, \"license plate\": xxx, \"damage description\": xxx, \"estimated cost of repair\": xxx}\n\nGPT-4V:\n\n{\"make\": \"Toyota\" : \", ‘license plate\": \"N/A\", \"damage description\": \"Front bumper and hood\ndamage, driver side fender damage, driver side headlight damage\", \"estimated cost of repair\":\n\"$2,500-$3,500\"}\n\nFigure 80: Application Highlights on Insurance Reporting. For the highlighted text in red, GPT-4V\nfails to read the license plate, potentially due to occlusion. Check Section 9.4 for detailed discussions.\n\n106\n", "vlm_text": "Sec. 9.4 Auto Insurance \nInsurance Report Generation \nPrompt: \nImagine that you are an expert in evaluating the car damage from car accident for auto insurance reporting.  Please fill the incident report for the accident shown in image below, with the following format in JSON (note  xxx is placeholder, if you cannot determine that based on the image, put \"N/A\" instead). {\"make\": xxx, \"model\": xxx, \"license plate\": xxx, \"damage description\": xxx, \"estimated cost of repair\": xxx} \nThe image shows a heavily damaged car with significant front-end and roof damage. The windshield is shattered, and the hood is crumpled. The license plate reads \"KL 52 N 5305.\" The car is parked on the side of a road, with other vehicles visible in the background.\nGPT-4V: \n{\"make\": \"Maruti Suzuki\", \"model\": \"Celerio\", \"license plate\": \"KL 52 N 5305\", \"damage description\": \"Front  bumper, hood, and windshield damaged. Airbags deployed.\", \"estimated cost of repair\": \"N/A\"}  \nPrompt: \nImagine that you are an expert in evaluating the car damage from car accident for auto insurance reporting.  Please evaluate the damage seen in the image below. For filing the incident report, please follow the following  format in JSON (note xxx is placeholder, if the information is not available in the image, put \"N/A\" instead). {\"make\": xxx, \"model\": xxx, \"license plate\": xxx, \"damage description\": xxx, \"estimated cost of repair\": xxx} \nThe image shows a black Toyota car that has been involved in a collision. The front of the vehicle is damaged, with a crumpled hood and bumper. A person is sitting inside the car, and the driver's side door is open. There is another vehicle visible in the background. The accident appears to have occurred on a roadway.\nGPT-4V: \n{\"make\": \"Toyota\", \"model\": \"Corolla\", \"license plate\": \"N/A\", \"damage description\": \"Front bumper and hood  damage, driver side fender damage, driver side headlight damage\", \"estimated cost of repair\":   $\\\"\\S2{,}500{-}\\S3{,}500\"\\}$    \nFigure 80: Application Highlights on Insurance Reporting. For the highlighted text in  red  , GPT-4V fails to read the license plate, potentially due to occlusion. Check Section  9.4  for detailed discussions. "}
{"page": 106, "image_path": "doc_images/2309.17421v2_106.jpg", "ocr_text": "9.5 Customized Captioner\n\nPhoto organization. In this scenario, let’s picture that we have a family photo album. We demon-\nstrate how GPT-4V can enhance the album by generating captions that explicitly mention the name\nof each family member shown in the photo. This personalized approach facilitates more precise\nand tailored photo organization, as illustrated in Figures 81-82. By providing GPT-4V with visual\nprompts for each family member, along with their respective names, GPT-4V can precisely identify\nthe family members (including person, cat, and dog) to generate detailed and customized captions.\nStoring such captions for all the images in the family album holds the potential to enable highly\npersonalized image search. For instance, a user could search for “a family photo of Linda, Cotton,\nMax, Sam, and Emma” and easily locate the corresponding family photo shown in Figure 81, or\n“Max lying between Sam and Emma” and locate the family photo in Figure 82.\n\nDense captioning w/ segmentation. We demonstrate the enhanced performance of GPT-4V in\ndense captioning by harnessing powerful segmentation models [65, 159, 160]. Figure 83 illustrates\nthe dense captioning results by incorporating object cut-outs generated by SAM [65] into the prompt\nto extract more detailed captions for each object of interest. In addition, we provide GPT-4V with the\noriginal image as the global context and ask it to describe four object cut-outs as detailed as possible,\nand incorporating references to the context image.\n\nThe results show GPT-4V can generate highly intricate dense captions for each object, some of which\nare accompanied by relevant references to the context image. For instance, when describing object 3\n(a frog), the dense caption makes mention of a close-up shot of a frog with a snail perched on its head,\ndespite the absence of the snail in the corresponding cut-out for object 3. Similarly, when referring\nto object 4 (a turtle), GPT-4V recognizes from the context image that the turtle is floating in water,\nthereby further enriching the generated caption.\n\n107\n", "vlm_text": "9.5 Customized Captioner \nPhoto organization. In this scenario, let’s picture that we have a family photo album. We demon- strate how GPT-4V can enhance the album by generating captions that explicitly mention the name of each family member shown in the photo. This personalized approach facilitates more precise and tailored photo organization, as illustrated in Figures  81 - 82 . By providing GPT-4V with visual prompts for each family member, along with their respective names, GPT-4V can precisely identify the family members (including person, cat, and dog) to generate detailed and customized captions. Storing such captions for all the images in the family album holds the potential to enable highly personalized image search. For instance, a user could search for “a family photo of Linda, Cotton, Max, Sam, and Emma” and easily locate the corresponding family photo shown in Figure  81 , or “Max lying between Sam and Emma” and locate the family photo in Figure  82 . \nDense captioning w/ segmentation. We demonstrate the enhanced performance of GPT-4V in dense captioning by harnessing powerful segmentation models [ 65 ,  159 ,  160 ]. Figure  83  illustrates the dense captioning results by incorporating object cut-outs generated by SAM [ 65 ] into the prompt to extract more detailed captions for each object of interest. In addition, we provide GPT-4V with the original image as the global context and ask it to describe four object cut-outs as detailed as possible, and incorporating references to the context image. \nThe results show GPT-4V can generate highly intricate dense captions for each object, some of which are accompanied by relevant references to the context image. For instance, when describing object 3 (a frog), the dense caption makes mention of a close-up shot of a frog with a snail perched on its head, despite the absence of the snail in the corresponding cut-out for object 3. Similarly, when referring to object 4 (a turtle), GPT-4V recognizes from the context image that the turtle is floating in water, thereby further enriching the generated caption. "}
{"page": 107, "image_path": "doc_images/2309.17421v2_107.jpg", "ocr_text": "Sec. 9.5 Customized Captioner\n\nPhoto Organization\n\nPrompt:\nHere are the photos of my family members.\n\nGPT-4V:\n\nThis image is a family portrait of Linda and her children, Emma and Sam, along with their pets, Max the dog and\nCotton the cat. The family is sitting in their living room, with Linda on the couch and Emma and Sam on the floor.\nMax is sitting in between Linda and Emma, with his tongue sticking out, while Cotton is perched on Linda's\nshoulder. All family members are smiling and looking at the camera. The room is bright and airy, with a vase of\nyellow flowers on the coffee table and a white bookshelf in the background.\n\nFigure 81: Customized Captioner for photo organization (the reference images are cropped from the\nquery image). Blue highlights the mention of family names. Check Sections 9.5, 10.5 for detailed\n\ndiscussions.\n108\n", "vlm_text": "Sec. 9.5 Customized Captioner \nPhoto Organization \nPrompt: Here are the photos of my family members. \nLinda \nThe image displays a series of photographs labeled with names. In addition to some blurred faces, it includes a clear picture of a white and orange cat labeled \"Cotton\" and a golden retriever dog labeled \"Max.\"\nPlease describe the image with as many details as possible, using my family members’ names. \nThe image features a group of people sitting indoors on a carpeted floor in front of a white couch. They are accompanied by a golden retriever and a cat. The setting appears to be a cozy living room with decorative elements such as a vase with flowers and a shelving unit. The cat is perched on one individual's knee, and the golden retriever is seated with its tongue out, facing the camera.\nGPT-4V: This image is a family portrait of Linda and her children, Emma and Sam, along with their pets, Max the dog and  Cotton the cat. The family is sitting in their living room, with Linda on the couch and Emma and Sam on the floor.  Max is sitting in between Linda and Emma, with his tongue sticking out, while Cotton is perched on Linda's  shoulder. All family members are smiling and looking at the camera. The room is bright and airy, with a vase of  yellow flowers on the coffee table and a white bookshelf in the background.  \nFigure 81: Customized Captioner for photo organization (the reference images are cropped from the query image).  Blue  highlights the mention of family names. Check Sections  9.5 ,  10.5  for detailed discussions. "}
{"page": 108, "image_path": "doc_images/2309.17421v2_108.jpg", "ocr_text": "Sec. 9.5 Customized Captioner\n\nPhoto Organization\n\nPrompt:\nHere are the photos of my family members.\n\nPlease describe the image with as many details as possible, using my family members’ names.\n\nGPT-4V:\n\nThis image shows three members of my family - my brother Sam, our dog Max, and myself, Emma. We are lying\non a grassy field with a clear blue sky in the background. Sam and | are resting our heads on our hands and\nsmiling at the camera, while Max is lying between us with his tongue out. We are all enjoying the beautiful\nweather and spending time together as a family.\n\nFigure 82: Customized Captioner for photo organization (the reference images are cropped from\n\na different image than the query image) Blue highlights the mention of family names. Check\nSection 9.5 for detailed discussions.\n\n109\n", "vlm_text": "Sec. 9.5 Customized Captioner \nPhoto Organization \nPrompt: Here are the photos of my family members. \nLinda \nThe image consists of a series of names listed vertically alongside corresponding blurred photographs. The names are Linda, Cotton, Max, Sam, and Emma. Among these, Cotton is associated with an image of a cat.\nPlease describe the image with as many details as possible, using my family members’ names. \nThe image shows two children and a golden retriever lying on a grassy field with a clear blue sky in the background. The children are resting their heads on their hands and appear to be smiling, while the dog is positioned between them with its tongue out. They seem to be enjoying the outdoor setting and each other's company.\nFigure 82: Customized Captioner for photo organization (the reference images are cropped from a different image than the query image)  Blue  highlights the mention of family names. Check Section  9.5  for detailed discussions. "}
{"page": 109, "image_path": "doc_images/2309.17421v2_109.jpg", "ocr_text": "Sec. 9.5 Customized Captioner\n\nDense Captioning w/ Segmentation\n\nPrompt:\nDescribe the objects below with as many details as possible, Reference to the content shown in the context\nimage but focus on describing each object below.\n\nContext image\n\nGPT-4V:\n\nObject 1 is a close-up of a snail shell. The shell is a light beige color with a spiral pattern on the top. The shell\nappears to be slightly weathered, with some small chips and cracks visible on the edges. The background is\nwhite, making the shell the focal point of the image. The shell is positioned at a slight angle, with the spiral\npattern on the top facing towards the right side of the image.\n\nObject 2 is a close-up of a snail. The snail has a light orange-colored shell with a spiral pattern. The body of the\nsnail is a darker orange color with small bumps on its skin. The snail's head is extended forward and its antennae\nare visible. The background of the image is white, making the snail the focal point of the image.\n\nObject 3 is a close-up of a frog with a snail on its head. The frog is light green in color with a pinkish hue on its\nunderside. It has large, bulging eyes that are black and white. The snail is orange and is perched on top of the\nfrog's head. The frog is sitting on a rock or other flat surface with its legs tucked underneath its body. The\nbackground is white, making the frog and snail stand out.\n\nObject 4 shows a close-up of a turtle's head and neck, with its body partially visible. The turtle appears to be\nfloating in water, with its head and neck extended upwards. The turtle's skin is green and its shell is grey with\nblack stripes. The shell appears to be damaged, with a large chunk missing from the top. The turtle's eyes are\nclosed and its mouth is slightly open.\n\nFigure 83: Dense captioning w/ segmentation cut-outs from SAM [65] Blue highlights the references\nto the context image. Check Section 9.5 for detailed discussions.\n\n110\n", "vlm_text": "Sec. 9.5 Customized Captioner \nDense Captioning w/ Segmentation \nPrompt: \nDescribe the objects below with as many details as possible, Reference to the content shown in the context  image but focus on describing each object below. \nThe image contains four objects labeled from top to bottom:\n\n1. Object 2: A snail.\n2. Object 3: A different snail.\n3. Object 4: A frog.\n4. Object 5: A turtle.\n\nEach object is outlined within a blue rectangle.\nThe image shows a small turtle in the water with a frog sitting on top of it. Additionally, there is a snail on the frog's head. The turtle, frog, and snail are all captured with their reflections visible in the water.\nGPT-4V: \nObject 1 is a close-up of a snail shell. The shell is a light beige color with a spiral pattern on the top. The shell  appears to be slightly weathered, with some small chips and cracks visible on the edges. The background is  white, making the shell the focal point of the image. The shell is positioned at a slight angle, with the spiral  pattern on the top facing towards the right side of the image.  \nObject 2 is a close-up of a snail. The snail has a light orange-colored shell with a spiral pattern. The body of the  snail is a darker orange color with small bumps on its skin. The snail's head is extended forward and its antennae  are visible. The background of the image is white, making the snail the focal point of the image.  \nObject 3 is a close-up of a frog with a snail on its head. The frog is light green in color with a pinkish hue on its  underside. It has large, bulging eyes that are black and white. The snail is orange and is perched on top of the  frog's head. The frog is sitting on a rock or other flat surface with its legs tucked underneath its body. The  background is white, making the frog and snail stand out.  \nObject 4 shows a close-up of a turtle's head and neck, with its body partially visible. The turtle appears to be  floating in water, with its head and neck extended upwards. The turtle's skin is green and its shell is grey with  black stripes. The shell appears to be damaged, with a large chunk missing from the top. The turtle's eyes are  closed and its mouth is slightly open.  \nFigure 83: Dense captioning w/ segmentation cut-outs from SAM [ 65 ]  Blue  highlights the references to the context image. Check Section  9.5  for detailed discussions. "}
{"page": 110, "image_path": "doc_images/2309.17421v2_110.jpg", "ocr_text": "9.6 Image Generation\n\nIn this section, we make connections with another prominent area of multimodal research: visual\nsynthesis. By delving into the realm of image generation, we explore how GPT-4V can contribute to\nthis field through various avenues, including evaluation and prompting.\n\nEvaluation of generated images. Figure 66 in Section 8.2 demonstrates the capability of GPT-4V in\nassessing the aesthetics of images. Here, we show how we employ GPT-4V to evaluate the generated\nimages based on their alignment with the given prompts for text-to-image generation, inspired by\nRL-Diffusion [18]. RL-Diffusion leverages a VL model LLAVA [79] to describe the generated\nimage, followed by text similarity computation between the prompt and the image description using\nBERT [38]. The resulting text similarity score serves as the feedback signal to guide the training\nof the diffusion model through reinforcement learning (RL). Notably, Figures 84-85 exhibit how\nGPT-4V, as a single model, can effectively rate the similarity between the generated image and the\nprompt. Moreover, GPT-4V provides explanations for the deduction in similarity score, which can\npotentially be used as a feedback to improve the image generation.\n\nIn Figure 84, we present the evaluation of image similarity using the prompt, “What is happening\nin the image? From a scale of | to 10, rate the similarity between the image and the text prompt ’a\nparrot driving a car’.” GPT-4V assigns a score of | to the most irrelevant image (a dolphin jumping\nover the water), while rating the most relevant image at the bottom with a score of 9. Notably, the last\nthree images in Figure 84 are shown in RL-Diffusion as gradually improved generation results for the\ntext prompt “‘a parrot driving a car.” The ratings assigned by GPT-4V to these three images (4 — 8\n\n— 9) align with the refinement process.\n\nFigure 85 showcases the evaluation of image generation results that involve text rendering on a cake.\nLeveraging its robust optical character recognition (OCR) capabilities, GPT-4V accurately recognizes\nthe rendered texts in the generated images, such as “Azuze Research,” “ARAUIE,” and “Azure Azure,”\nand compares them to the text prompt requirement, which is “Azure Research.”\n\nPrompt generation for image editing. In addition to its remarkable ability to evaluate generated\nimages, GPT-4V offers a valuable feature that can greatly enhance image editing. By generating or\nrewriting the text prompt used for editing, GPT-4V can refine the edited image, resulting in a more\nvisually appealing outcome. Figure 86 provides a demonstration of how we can harness the power of\nGPT-4V to generate a text prompt specifically tailored for image editing. By providing the original\nimage and text requirements that describe the desired edits, GPT-4V produces an optimized prompt\nfor the task at hand. This optimized prompt takes into account the unique characteristics of the image,\nensuring that the subsequent editing process is well-informed and effective.\n\nMoreover, Figure 87 showcases another use case of GPT-4V to improve image editing by rewriting\nthe editing prompt. By considering the original image, the initial prompt, and the edited image,\nGPT-4V can generate an improved version of the prompt that incorporates the changes made during\nthe previous editing process. One can alternate the processes depicted in Figures 86-87, allowing\nusers to refine their edits repeatedly until they achieve a satisfying outcome. Consequently, this\niterative process has the potential to significantly enhance the overall quality of the edited image,\nproviding users with more control and creative freedom in their image editing endeavors.\n\n111\n", "vlm_text": "9.6 Image Generation \nIn this section, we make connections with another prominent area of multimodal research: visual synthesis. By delving into the realm of image generation, we explore how GPT-4V can contribute to this field through various avenues, including evaluation and prompting. \nEvaluation of generated images. Figure  66  in Section  8.2  demonstrates the capability of GPT-4V in assessing the aesthetics of images. Here, we show how we employ GPT-4V to evaluate the generated images based on their alignment with the given prompts for text-to-image generation, inspired by RL-Diffusion [ 18 ]. RL-Diffusion leverages a VL model LLAVA [ 79 ] to describe the generated image, followed by text similarity computation between the prompt and the image description using BERT [ 38 ]. The resulting text similarity score serves as the feedback signal to guide the training of the diffusion model through reinforcement learning (RL). Notably, Figures  84 - 85  exhibit how GPT-4V, as a single model, can effectively rate the similarity between the generated image and the prompt. Moreover, GPT-4V provides explanations for the deduction in similarity score, which can potentially be used as a feedback to improve the image generation. \nIn Figure  84 , we present the evaluation of image similarity using the prompt, “What is happening in the image? From a scale of 1 to 10, rate the similarity between the image and the text prompt ’a parrot driving a car’.” GPT-4V assigns a score of 1 to the most irrelevant image (a dolphin jumping over the water), while rating the most relevant image at the bottom with a score of 9. Notably, the last three images in Figure  84  are shown in RL-Diffusion as gradually improved generation results for the prompt “a parrot driving a car.” The ratings assigned by GPT-4V to these three images   $4\\rightarrow8$   $\\to9$  9) align with the refinement process. \nFigure  85  showcases the evaluation of image generation results that involve text rendering on a cake. Leveraging its robust optical character recognition (OCR) capabilities, GPT-4V accurately recognizes the rendered texts in the generated images, such as “Azuze Research,” “ARAUIE,” and “Azure Azure,” and compares them to the text prompt requirement, which is “Azure Research.” \nPrompt generation for image editing. In addition to its remarkable ability to evaluate generated images, GPT-4V offers a valuable feature that can greatly enhance image editing. By generating or rewriting the text prompt used for editing, GPT-4V can refine the edited image, resulting in a more visually appealing outcome. Figure  86  provides a demonstration of how we can harness the power of GPT-4V to generate a text prompt specifically tailored for image editing. By providing the original image and text requirements that describe the desired edits, GPT-4V produces an optimized prompt for the task at hand. This optimized prompt takes into account the unique characteristics of the image, ensuring that the subsequent editing process is well-informed and effective. \nMoreover, Figure  87  showcases another use case of GPT-4V to improve image editing by rewriting the editing prompt. By considering the original image, the initial prompt, and the edited image, GPT-4V can generate an improved version of the prompt that incorporates the changes made during the previous editing process. One can alternate the processes depicted in Figures  86 - 87 , allowing users to refine their edits repeatedly until they achieve a satisfying outcome. Consequently, this iterative process has the potential to significantly enhance the overall quality of the edited image, providing users with more control and creative freedom in their image editing endeavors. "}
{"page": 111, "image_path": "doc_images/2309.17421v2_111.jpg", "ocr_text": "Sec. 9.6 Image Generation\n\nEvaluation of Generated Images (Prompt-Image Alignment)\n\nPrompt:\n\nWhat is happening in the image? From scale 1 to 10,\ndecide how similar the image is to the text prompt \"a\nparrot driving a car\"?\n\nGPT-4V:\n\nIn the image, there is a dolphin jumping out of the\nwater. On a scale of 1 to 10, | would rate this image a\n1 in similarity to the text prompt \"a parrot driving a\ncar\" as there is no parrot or car present in the image.\n\nWhat is happening in the image? From scale 1 to 10,\ndecide how similar the image is to the text prompt \"a\nparrot driving a car\"?\n\nGPT-4V:\n\nIn the image, there is a parrot perched on the side of\na car. It is not driving the car, but it is on the car. |\nwould rate this image a 4 in terms of similarity to the\ntext prompt \"a parrot driving a car\" because there is a\nparrot and a car present, but the parrot is not driving\n\nPrompt:\n\nPrompt:\n\nWhat is happening in the image? From scale 1 to 10,\ndecide how similar the image is to the text prompt \"a\nparrot driving a car\"?\n\nIn the image, there is a parrot flying in the sky with\ntrees and clouds in the background. | would rate this\nimage a 2 in similarity to the text prompt \"a parrot\ndriving a car\" as there is a parrot present in the\nimage, but it is not driving a car.\n\nWhat is happening in the image? From scale 1 to 10,\ndecide how similar the image is to the text prompt \"a\nparrot driving a car\"?\n\nGPT-4V:\n\nIn the image, there is a parrot's head superimposed\nonto a car driving on a road. The parrot's head is in\nthe driver's seat and the car appears to be in motion.\nOn a scale of 1 to 10, | would rate this image an 8 in\nterms of similarity to the text prompt \"a parrot driving\na car\". The image clearly shows a parrot in the\ndriver's seat of a car, but the parrot's body is not\nvisible and the car is not clearly a \"car\" but more of a\nblurred image.\n\nWhat is happening in the image? From scale 1 to 10, decide how similar the image is to the text prompt \"a\n\nparrot driving a car\"?\n\nGPT-4V:\n\nIn the image, there are two parrots, one red and one yellow, on top of a car. The yellow parrot is holding the\nsteering wheel and appears to be driving the car. On a scale of 1 to 10, | would rate this image a 9 in terms of\nsimilarity to the text prompt \"a parrot driving a car\". The only reason it is not a perfect 10 is because there are two\nparrots in the image, rather than just one as the prompt suggests.\n\nFigure 84: Prompt GPT-4V to give a score from | to 10 on how similar the generated image is to\nthe prompt. Blue highlights the rating given by GPT-4V. The last three images are generated from\nRL-Diffusion [18]. Check Section 9.6 for detailed discussions.\n\n112\n", "vlm_text": "Evaluation of Generated Images (Prompt-Image Alignment)  \nPrompt: \nWhat is happening in the image? From scale 1 to 10,  decide how similar the image is to the text prompt \"a  parrot driving a car\"? \nThe image shows a dolphin leaping out of the water. In the background, there is a shoreline with green vegetation. The composition captures the dolphin in mid-air, showcasing its streamlined body and characteristic dorsal fin.\nGPT-4V: \nIn the image, there is a dolphin jumping out of the  water. On a scale of 1 to 10, I would rate this image a  1 in similarity to the text prompt \"a parrot driving a  car\" as there is no parrot or car present in the image.  \nPrompt: \nWhat is happening in the image? From scale 1 to 10,  decide how similar the image is to the text prompt \"a  parrot driving a car\"? \nThe image shows a colorful parrot, specifically a macaw, perched on the edge of an open car window. The macaw is vibrant, with bright red, green, and some blue plumage, and it is holding onto the window with its claws.\nGPT-4V: \nIn the image, there is a parrot perched on the side of  a car. It is not driving the car, but it is on the car. I  would rate this image a 4 in terms of similarity to the  text prompt \"a parrot driving a car\" because there is a  parrot and a car present, but the parrot is not driving  the car.  \nPrompt: \nWhat is happening in the image? From scale 1 to 10,  decide how similar the image is to the text prompt \"a  parrot driving a car\"? \nThe image is a cartoon illustration of a toucan flying through a scenic background that includes a blue sky with clouds and green trees. The toucan is depicted with a large, colorful beak, and its body is primarily black with some white and orange accents. The scene is vibrant and cheerful.\nGPT-4V: \nIn the image, there is a parrot flying in the sky with  trees and clouds in the background. I would rate this  image a 2 in similarity to the text prompt \"a parrot  driving a car\" as there is a parrot present in the  image, but it is not driving a car.  \nPrompt: \nWhat is happening in the image? From scale 1 to 10,  decide how similar the image is to the text prompt \"a  parrot driving a car\"? \nThe image shows a green car with artistic depictions of two birds in the background: one bird is red and orange, and the other is a green and white parrot. The design is colorful and stylized.\nGPT-4V: \nIn the image, there is a parrot's head superimposed  onto a car driving on a road. The parrot's head is in  the driver's seat and the car appears to be in motion.  On a scale of 1 to 10, I would rate this image an 8 in  terms of similarity to the text prompt \"a parrot driving  a car\". The image clearly shows a parrot in the  driver's seat of a car, but the parrot's body is not  visible and the car is not clearly a \"car\" but more of a  blurred image. \nPrompt: \nWhat is happening in the image? From scale 1 to 10, decide how similar the image is to the text prompt \"a  parrot driving a car\"? \nThe image features two parrots: one is brightly colored with red and green, sitting on top of a car, and the other is yellow, sitting inside the car at the steering wheel. There are palm trees and a blue sky with green hills in the background, suggesting a tropical setting.\nGPT-4V: \nIn the image, there are two parrots, one red and one yellow, on top of a car. The yellow parrot is holding the  steering wheel and appears to be driving the car. On a scale of 1 to 10, I would rate this image a  $\\mathfrak{g}$   in terms of  similarity to the text prompt \"a parrot driving a car\". The only reason it is not a perfect 10 is because there are two  parrots in the image, rather than just one as the prompt suggests.  \nFigure 84: Prompt GPT-4V to give a score from 1 to 10 on how similar the generated image is to the prompt.  Blue  highlights the rating given by GPT-4V. The last three images are generated from RL-Diffusion [ 18 ]. Check Section  9.6  for detailed discussions. "}
{"page": 112, "image_path": "doc_images/2309.17421v2_112.jpg", "ocr_text": "Sec. 9.6 Image Generation\n\nEvaluation of Generated Images (Prompt-Image Alignment)\n\nPrompt:\n\nWhat is happening in the image? From scale 1 to\n10, decide how similar the image is to the text\nprompt “a cake on the table with word “Azure\nResearch’ written on it\"?\n\nGPT-4V:\n\nIn the image, there is a cake on a table with the\nwords written on it. | would rate\nthis image an 8 out of 10 in similarity to the text\nprompt \"a cake on the table with word ‘Azure\nResearch’ written on it\" because the only\ndifference is the misspelling of \"Azure\" as\n\nWhat is happening in the image? From scale 1 to\n10, decide how similar the image is to the text\nprompt “a cake on the table with word “Azure\nResearch’ written on it\"?\n\nGPT-4V:\n\nIn the image, there is a cake on a table with the\nwords written on it. | would rate this\nimage an 8 out of 10 in similarity to the text\nprompt \"a cake on the table with word ‘Azure\nResearch’ written on it\" because the image does\nshow a cake on a table with the word \"Azure\"\nwritten on it, but the word \"Research\" is not\npresent.\n\nPrompt:\n\nWhat is happening in the image? From scale 1 to\n10, decide how similar the image is to the text\nprompt “a cake on the table with word “Azure\nResearch’ written on it\"?\n\nGPT-4V:\n\nIn the image, there is a blue cake on a table with\nthe word \"ARAUIE\" written on it. On a scale of 1 to\n10, | would rate this image a 6 in terms of similarity\nto the text prompt \"a cake on the table with word\n‘Azure Research’ written on it\".\n\nThe image does show a cake on a table, but the\nword written on the cake is not an exact match to\nthe text prompt. The color of the cake is also blue,\nwhich could be seen as similar to the word \"Azure\"\nin the prompt.\n\nWhat is happening in the image? From scale 1 to\n10, decide how similar the image is to the text\nprompt “a cake on the table with word “Azure\nResearch’ written on it\"?\n\nGPT-4V:\n\nIn the image, there is a two-tiered cake on a table\nwith the words \"Azure Research\" written on it in\nblue icing. | would rate this image a 9 out of 10 in\nterms of similarity to the text prompt, as it very\nclosely matches the description given. The only\nreason | would not give it a perfect 10 is because\nthe text prompt does not mention the two tiers or\nthe additional decorations on the cake.\n\nFigure 85: Prompt GPT-4V to give a score from | to 10 on how similar the generated image is to\n\nthe prompt. Blue highlights the rating given by GPT-4V. ‘Red ( Green ) indicate wrong (correct)\nrendered text. Generated images are from DeepFloyd IF [2], Midjourney V5.1 [4], SDXL [110], and\nReCo [143]. Check Section 9.6 for detailed discussions.\n\n113\n", "vlm_text": "Sec. 9.6 Image Generation \nEvaluation of Generated Images (Prompt-Image Alignment)  \nPrompt: \nWhat is happening in the image? From scale 1 to  10, decide how similar the image is to the text  prompt “a cake on the table with word “Azure  Research” written on it\"? \nThe image shows a square-shaped cake with white icing. On top of the cake, the text \"Azuze Research\" is written, with \"Azuze\" in blue and \"Research\" in black. The cake is placed on a wooden surface, and there is a small \"if\" logo at the bottom right corner of the image.\nGPT-4V: \nIn the image, there is a cake on a table with the  words \"Azuze Research\" written on it. I would rate  this image an 8 out of 10 in similarity to the text  prompt \"a cake on the table with word 'Azure  Research' written on it\" because the only  difference is the misspelling of \"Azure\" as  \"Azuze.\"  \nPrompt: \nWhat is happening in the image? From scale 1 to  10, decide how similar the image is to the text  prompt “a cake on the table with word “Azure  Research” written on it\"? \nThe image shows a cake on a table. The cake has a blue and brown design, and there is a word written on it that reads \"Azzure\" and \"Azure.\" The table also has some utensils and a cup.\nGPT-4V: \nIn the image, there is a cake on a table with the  words \"Azure Aurre\" written on it. I would rate this  image an 8 out of 10 in similarity to the text  prompt \"a cake on the table with word 'Azure  Research' written on it\" because the image does  show a cake on a table with the word \"Azure\"  written on it, but the word \"Research\" is not  present.   \nPrompt: \nWhat is happening in the image? From scale 1 to  10, decide how similar the image is to the text  prompt “a cake on the table with word “Azure  Research” written on it\"? \nThis image shows a blue frosted cake with decorative pearls on top. There appears to be a plastic fish as part of the decoration. The cake is sitting on a circular board, and the word \"ARAUJE\" is written on the side. In the background, there is another dessert item and an open notebook.\nGPT-4V: \nIn the image, there is a blue cake on a table with  the word \"ARAUIE\" written on it. On a scale of 1 to  10, I would rate this image a 6 in terms of similarity  to the text prompt \"a cake on the table with word  'Azure Research' written on it\".  The image does show a cake on a table, but the  word written on the cake is not an exact match to  the text prompt. The color of the cake is also blue,  which could be seen as similar to the word \"Azure\"  in the prompt.   \nPrompt: \nWhat is happening in the image? From scale 1 to  10, decide how similar the image is to the text  prompt “a cake on the table with word “Azure  Research” written on it\"? \nThe image shows two layered rectangular cakes. The top cake is decorated with colorful small candies and objects. The bottom cake has the words \"Azure Research\" written on it in blue icing, with a red border and a small decorative element.\nGPT-4V: \nIn the image, there is a two-tiered cake on a table  with the words \"Azure Research\" written on it in  blue icing. I would rate this image a 9 out of 10 in  terms of similarity to the text prompt, as it very  closely matches the description given. The only  reason I would not give it a perfect 10 is because  the text prompt does not mention the two tiers or  the additional decorations on the cake.  \nFigure 85: Prompt GPT-4V to give a score from 1 to 10 on how similar the generated image is to the prompt.  Blue  highlights the rating given by GPT-4V.  Red  (  Green  ) indicate wrong (correct) rendered text. Generated images are from DeepFloyd IF [ 2 ], Midjourney V5.1 [ 4 ], SDXL [ 110 ], and ReCo [ 143 ]. Check Section  9.6  for detailed discussions. "}
{"page": 113, "image_path": "doc_images/2309.17421v2_113.jpg", "ocr_text": "Sec. 9.6 Image Generation\n\nPrompt Generation for Image Editing\n\nPrompt:\nThis is the original image:\n\n| want to make this image look like graphic novel cover. The prompt should be a single sentence in less\nthan 15 words with descriptive words. For example: “have it look like a graphic novel”.\n\nBased on the above prompt examples and the original image, help me to write a better prompt to an\nimage editing model.\n\nGPT-4V:\n\nHow about: “Transform this image of a statue into a dramatic graphic novel cover with a dark tone.”\n\nInstruct Pix2Pix:\n\nWith original image and prompt “Transform this image of a statue into a\ndramatic graphic novel cover with a dark tone” as the input\n\nFigure 86: Improving the text prompt for image editing, given the original image and textual\n\nrequirement. Blue highlights the suggested editing prompt by GPT-4V. Original image/exemplary\nediting prompt are from Instruct Pix2Pix [22]. Check Section 9.6 for detailed discussions.\n\n114\n", "vlm_text": "Sec. 9.6 Image Generation \nPrompt Generation for Image Editing \nPrompt: This is the original image: \nThe image shows the upper torso of Michelangelo's famous marble sculpture, \"David.\" The statue is renowned for its detailed depiction of the human form, showcasing muscular structure and anatomical precision. It represents the biblical hero David, known for his victory against Goliath. The sculpture is an iconic work of Renaissance art.\nI want to make this image look like graphic novel cover. The prompt should be a single sentence in less  than 15 words with descriptive words. For example: “have it look like a graphic novel”. Based on the above prompt examples and the original image, help me to write a better prompt to an  image editing model.  \nGPT-4V: How about: “Transform this image of a statue into a dramatic graphic novel cover with a dark tone.”  \nInstruct Pix2Pix: \nWith original image and prompt “Transform this image of a statue into a  dramatic graphic novel cover with a dark tone” as the input \nThe image depicts the famous statue of David by Michelangelo. The sculpture is a renowned masterpiece of Renaissance art, known for its detailed depiction of the human form and its representation of the biblical figure, David. The figure is carved from marble and stands in a contrapposto pose, displaying muscular detail and anatomical precision."}
{"page": 114, "image_path": "doc_images/2309.17421v2_114.jpg", "ocr_text": "Sec. 9.6 Image Generation\n\nPrompt Generation for Image Editing\n\nPrompt:\nThis is the original image:\n\nThe prompt | used to edit this image is “have it look like a graphic novel’.\n\nThe edited image | got is:\n\nTell me how | should change my prompt to make the edited image better.\n\nGPT-4V:\n\nInstead of using the prompt “have it look like a graphic novel”, you could try being more specific with the\nprompt to guide the Al in the direction you want the image to go. For example, you could use prompts\nsuch as “make the image look like a black and white comic book illustration” or “add bold lines and high\ncontrast to the image to make it look like a graphic novel cover”. This could help the Al better understand\nthe aesthetic you are going for and produce a more refined edited image.\n\nInstruct Pix2P\n\nWith original image and prompt “make With original image and prompt “add\nthe image look like a black and bold lines and high contrast to the\nwhite comic book illustration” as image to make it look like a graphic\nthe input novel cover” as the input\n\nFigure 87: Improving the editing prompt, given the original image, the editing prompt, and the\nedited image. Blue highlights the suggested editing prompt by GPT-4V. Original image/editing\nprompt/edited image are from Instruct Pix2Pix [22]. Check Section 9.6 for detailed discussions.\n\n115\n", "vlm_text": "Sec. 9.6 Image Generation \nPrompt Generation for Image Editing \nPrompt: This is the original image: \nThe image shows the sculpture of David by Michelangelo. The statue is a renowned masterpiece of Renaissance art, depicting the biblical hero David with remarkable detail in the anatomy. It is carved from marble and exemplifies the ideals of beauty and proportion from that era.\nThe prompt I used to edit this image is “have it look like a graphic novel”. \nThe edited image I got is: \nThe image features the upper part of Michelangelo's famous sculpture, David, showcasing detailed anatomical structure, particularly the torso and arm holding a slingshot. This Renaissance masterpiece is renowned for its depiction of the biblical hero David, traditionally associated with his fight against Goliath.\nTell me how I should change my prompt to make the edited image better. \nGPT-4V: \nInstead of using the prompt “have it look like a graphic novel”, you could try being more specific with the  prompt to guide the AI in the direction you want the image to go. For example, you could use prompts  such as “make the image look like a black and white comic book illustration” or “add bold lines and high  contrast to the image to make it look like a graphic novel cover”. This could help the AI better understand  the aesthetic you are going for and produce a more refined edited image. \nInstruct Pix2Pix: \nWith original image and prompt “make  the image look like a black and  white comic book illustration” as  the input \nThe image depicts a famous marble statue of a male figure. It is characterized by its detailed anatomical precision, particularly in the musculature and posture. The statue is often associated with Renaissance art.\nWith original image and prompt “add  bold lines and high contrast to the  image to make it look like a graphic  novel cover” as the input \nThe image depicts a famous sculpture, specifically Michelangelo's Statue of David. The sculpture is known for its detailed representation of the human form, particularly in the depiction of muscles and posture.\nFigure 87: Improving the editing prompt, given the original image, the editing prompt, and the edited image.  Blue  highlights the suggested editing prompt by GPT-4V. Original image/editing prompt/edited image are from Instruct Pix2Pix [ 22 ]. Check Section  9.6  for detailed discussions. "}
{"page": 115, "image_path": "doc_images/2309.17421v2_115.jpg", "ocr_text": "9.7 Embodied Agent\n\nIn this section, we delve into the exciting applications and implications of GPT-4V for embodied AI,\nexploring how it is poised to bridge the gap between multimodal understanding on static inputs and\nphysical interaction with dynamic environments. To provide a concrete illustration, let us consider\nthe scenario of GPT-4V assuming the role of a home robot. Within this context, we witness how it\ncan read the menu to operate household appliances (e.g., coffee machine), and perform task-oriented\nnavigation through the house.\n\nOperating machine. Imagine you’ve just acquired a brand-new coffee machine, and to your delight,\nyour trusty home robot, GPT-4V, learns how to operate it on your behalf. In our experiment, we\nprovide GPT-4V with a single image (Figure 88) featuring an operating menu with both illustrations\nand texts. Our task for GPT-4V is to identify the button that corresponds to the “8 OZ coffee” option\nwithin the coffee machine’s operating panel. Surprisingly, GPT-4V not only accurately locates the\n“8 OZ coffee” button but also successfully recognizes the button for “10 OZ coffee.” However, it\nmistakenly identifies the power button as the “6 OZ coffee” button, potentially due to the visual\nconfusion caused by the positioning of the “6 OZ coffee” option on both the menu and the coffee\nmachine itself. To address this specific failure case, we devise a solution by isolating the operating\nmenu for each button and presenting them all to GPT-4V in a single prompt (Figure 89). With this\nrevised approach, GPT-4V now can recognize the precise position of the “6 OZ coffee” button.\n\nNavigation. In order to explore navigation capabilities, we utilized Redfin virtual house tour as a\nmeans to replicate interactive environments for embodied agents. The objective was to assess the\nperformance of GPT-4V in a task-oriented scenario. To illustrate this, we present an example depicted\nin Figures 90-91. Initially, we provided GPT-4V with the entry image of a virtual house tour, offering\na view from one corner into the living room. The task assigned to GPT-4V was to “go to the kitchen\nand retrieve an item from the fridge.’ Our aim was to prompt GPT-4V to predict the subsequent\nactions.\n\nIn the first step, as shown in the first half of Figure 90, GPT-4V anticipated the initial action by\nsuggesting to “turn right and move forward towards the hallway.” This prediction was based on\nGPT-4V’s hypothesis that the kitchen would likely be located in that direction. We then manually\nexecuted this action using the visual house touring portal, capturing the resulting view after the action\nwas taken. This view was then used to prompt GPT-4V for the next action, as displayed in the second\nhalf of Figure 90. It’s important to note that throughout the process, we maintained a record of the\nimmediate previous turn to provide context for GPT-4V’s subsequent actions.\n\nAs the navigation process unfolded, we successfully reached the fridge within the third turn, as\nindicated by the query image in the second half of Figure 91. The final action predicted by GPT-4V\nwas to “move forward and slightly to the left in order to align myself with the fridge door. Then, use\nmy robotic arm to open the fridge door and retrieve the requested item.” This decisive action marked\nthe accomplishment of GPT-4V in this task-oriented navigation scenario.\n\n116\n", "vlm_text": "9.7 Embodied Agent \nIn this section, we delve into the exciting applications and implications of GPT-4V for embodied AI, exploring how it is poised to bridge the gap between multimodal understanding on static inputs and physical interaction with dynamic environments. To provide a concrete illustration, let us consider the scenario of GPT-4V assuming the role of a home robot. Within this context, we witness how it can read the menu to operate household appliances ( e.g ., coffee machine), and perform task-oriented navigation through the house. \nOperating machine. Imagine you’ve just acquired a brand-new coffee machine, and to your delight, your trusty home robot, GPT-4V, learns how to operate it on your behalf. In our experiment, we provide GPT-4V with a single image (Figure  88 ) featuring an operating menu with both illustrations and texts. Our task for GPT-4V is to identify the button that corresponds to the “8 OZ coffee” option within the coffee machine’s operating panel. Surprisingly, GPT-4V not only accurately locates the “8 OZ coffee” button but also successfully recognizes the button for   ${}^{**}10\\;\\mathrm{OZ}$   coffee.” However, it mistakenly identifies the power button as the “6 OZ coffee” button, potentially due to the visual confusion caused by the positioning of the “6 OZ coffee” option on both the menu and the coffee machine itself. To address this specific failure case, we devise a solution by isolating the operating menu for each button and presenting them all to GPT-4V in a single prompt (Figure  89 ). With this revised approach, GPT-4V now can recognize the precise position of the “6 OZ coffee” button. \nNavigation.  In order to explore navigation capabilities, we utilized Redfin virtual house tour as a means to replicate interactive environments for embodied agents. The objective was to assess the performance of GPT-4V in a task-oriented scenario. To illustrate this, we present an example depicted in Figures  90 - 91 . Initially, we provided GPT-4V with the entry image of a virtual house tour, offering a view from one corner into the living room. The task assigned to GPT-4V was to “go to the kitchen and retrieve an item from the fridge.” Our aim was to prompt GPT-4V to predict the subsequent actions. \nIn the first step, as shown in the first half of Figure  90 , GPT-4V anticipated the initial action by suggesting to “turn right and move forward towards the hallway.” This prediction was based on GPT-4V’s hypothesis that the kitchen would likely be located in that direction. We then manually executed this action using the visual house touring portal, capturing the resulting view after the action was taken. This view was then used to prompt GPT-4V for the next action, as displayed in the second half of Figure  90 . It’s important to note that throughout the process, we maintained a record of the immediate previous turn to provide context for GPT-4V’s subsequent actions. \nAs the navigation process unfolded, we successfully reached the fridge within the third turn, as indicated by the query image in the second half of Figure  91 . The final action predicted by GPT-4V was to “move forward and slightly to the left in order to align myself with the fridge door. Then, use my robotic arm to open the fridge door and retrieve the requested item.” This decisive action marked the accomplishment of GPT-4V in this task-oriented navigation scenario. "}
{"page": 116, "image_path": "doc_images/2309.17421v2_116.jpg", "ocr_text": "Sec. 9.7 Embodied Agent\nOperating Machine\n\nPrompt:\nFirst, read the operating menu below for coffee machine buttons.\n\nMenu:\n\nNow, let's look at the operating panel of the coffee machine in the image below, which button should | use for\na 8 OZ coffee: left, right, top or bottom button?\n\nGPT-4V:\nYou should use the bottom button for an 8 OZ coffee. According to the menu, the bottom button corresponds\nto a 8 OZ coffee, while the top button is for a 6 OZ coffee and the right button is for a 10 OZ coffee.\n\nFigure 88: Reading a full menu of coffee machine buttons, GPT-4V recognizes which button to\n\nchoose for 8 OZ coffee. Green (Red) highlights the correct (wrong) answer. Check Section 9.7 for\ndetailed discussions.\n\n117\n", "vlm_text": "Sec. 9.7 Embodied Agent \nOperating Machine \nPrompt: First, read the operating menu below for coffee machine buttons. \nThe image shows a menu with three options for beverage sizes, indicated by coffee cup icons and their corresponding fluid ounces: 6 oz, 8 oz, and 10 oz.\nNow, let's look at the operating panel of the coffee machine in the image below, which button should I use for  a 8 OZ coffee: left, right, top or bottom button? \nThis is an image of a coffee machine. The display on the machine has icons likely representing different coffee options and the name \"JoooDeee\" is visible at the top. The design appears to be modern, featuring sleek metal sides and a drip tray at the bottom.\nGPT-4V: You should use the bottom button for an  $8\\,\\mathrm{OZ}$   coffee. According to the menu, the bottom button corresponds  to a 8 OZ coffee, while the top button is for a 6 OZ coffee and the right button is for a 10 OZ coffee. \nFigure 88: Reading a full menu of coffee machine buttons, GPT-4V recognizes which button to choose for  $8\\,\\mathrm{OZ}$   coffee.  Green  (  Red  ) highlights the correct (wrong) answer. Check Section  9.7  for detailed discussions. "}
{"page": 117, "image_path": "doc_images/2309.17421v2_117.jpg", "ocr_text": "Sec. 9.7 Embodied Agent\n\nOperating Machine\n\nPrompt:\nFirst, read the operating menus below for coffee machine buttons.\n\né\n\n 60z\n\nNow, let's look at the operating panel of the coffee machine in the image below, which button should | use for\na6 OZ coffee: left, right, top or bottom button?\n\nGPT-4V:\n\nYou should use the left button for a 6 OZ coffee.\n\nFigure 89: Converting the full menu of coffee machine buttons to interleaved image-text instructions,\nGPT-4V can recognizes which button to choose for 6 OZ coffee, which GPT-4V failed to do so\n\nwith full menu instruction. Green highlights the correct answer. Check Section 9.7 for detailed\ndiscussions.\n\n118\n", "vlm_text": "Operating Machine \nPrompt: \nFirst, read the operating menus below for coffee machine buttons. Menu 1: \nThe image displays three menu options, each featuring an icon of a steaming cup and a corresponding size in ounces. Menu 1 shows \"6 OZ\", Menu 2 shows \"8 OZ\", and Menu 3 shows \"10 OZ\". These options likely represent different serving sizes for a beverage, possibly coffee.\nNow, let's look at the operating panel of the coffee machine in the image below, which button should I use for  a 6 OZ coffee: left, right, top or bottom button? \nThe image shows a coffee maker with the brand name \"JoooDeee\" displayed on it. It has a control panel with three buttons that likely correspond to different coffee sizes. The caption suggests using the left button for a 6 oz coffee.\nFigure 89: Converting the full menu of coffee machine buttons to interleaved image-text instructions, GPT-4V can recognizes which button to choose for  $6~\\mathrm{OZ}$   coffee, which GPT-4V failed to do so with full menu instruction.  Green  highlights the correct answer. Check Section  9.7  for detailed discussions. "}
{"page": 118, "image_path": "doc_images/2309.17421v2_118.jpg", "ocr_text": "Sec. 9.7 Embodied Agent\n\nNavigation\n\nPrompt:\n\nImagine that you are a home robot, and is asked to go to the kitchen to fetch something from the fridge.\nThe image below shows your current position. Please plan your next action.\n\nGPT-4V:\n\nMy next action would be turn right and move forward towards the hallway, as | can see that the kitchen is\nlikely located in that direction. | would then continue to navigate through the hallway until | reach the kitchen\nand locate the fridge.\n\nImagine that you are a home robot, and is asked to go to the kitchen to fetch something from the fridge.\nAt the previous turn, you were at the position shown in the below image:\n\nNote: Same image from the previous turn, resized\nto save space\n\nThe action taken in previous turn is: turn right and move forward towards the hallway.\n\nNow you are at the hallway, this is what you can see from there. Can you see the kitchen from here? What is\nyour next action?\n\nGPT-4V:\nFrom the current position, | can see the kitchen on the right side, My next action would be to turn right and\nmove forward towards the kitchen.\n\nFigure 90: Acting as an embodied agent to navigate through a house to fetch something from the\n\nfridge (the Ist and 2nd turn). Blue highlights the predicted actions. Check Section 9.7 for detailed\ndiscussions.\n\n119\n", "vlm_text": "Sec. 9.7 Embodied Agent \nNavigation \nPrompt: \nImagine that you are a home robot, and is asked to go to the kitchen to fetch something from the fridge. The image below shows your current position. Please plan your next action. \nThe image shows a cozy living room with a modern yet inviting design. Key features include:\n\n- A wooden floor that adds warmth to the room.\n- A matching set of tan leather couches: a loveseat and a larger sofa.\n- A large flat-screen TV mounted on the wall facing the sofas. \n- A neatly arranged, minimalistic TV console beneath the television.\n- A decorative rug centered in the seating area.\n- A window that allows natural light to brighten the space, with curtains hanging on either side.\n- A few pieces of furniture for added functionality, like a side table with a lamp and a smaller table opposite it.\n- Potted plants placed strategically, adding a touch of greenery.\n- Shelves on the wall with decorations and the word \"Thankful\" prominently displayed.\n- The entrance door to the room has a glass window with a \"360\" decal on it.\n- Recessed lighting in the ceiling provides ample illumination.\nGPT-4V: \nMy next action would be turn right and move forward towards the hallway, as I can see that the kitchen is  likely located in that direction. I would then continue to navigate through the hallway until I reach the kitchen  and locate the fridge. \nPrompt: \nImagine that you are a home robot, and is asked to go to the kitchen to fetch something from the fridge. At the previous turn, you were at the position shown in the below image: \nThis image depicts a living room. It features a couch and a loveseat arranged around a rug with a television mounted above a fireplace. There is a window letting in natural light, various pieces of furniture, and decorations around the room, including plants on a shelf. The flooring appears to be hardwood.\nNote: Same image from the previous turn, resized  to save space \nThe action taken in previous turn is: turn right and move forward towards the hallway. \nNow you are at the hallway, this is what you can see from there. Can you see the kitchen from here? What is  your next action? \nThe image shows an interior of a dining area that includes a wooden table with chairs, a wall mirror, and bar stools at a kitchen counter. There are wooden floors and some decor elements like wall art, hanging plants, and possibly a small table with items on it. The space has a cozy and welcoming feel.\nGPT-4V: \nFrom the current position, I can see the kitchen on the right side, My next action would be to turn right and  move forward towards the kitchen. \nFigure 90: Acting as an embodied agent to navigate through a house to fetch something from the fridge (the 1st and 2nd turn).  Blue  highlights the predicted actions. Check Section  9.7  for detailed discussions. "}
{"page": 119, "image_path": "doc_images/2309.17421v2_119.jpg", "ocr_text": "Sec. 9.7 Embodied Agent\nNavigation\n\nPrompt:\n\nImagine that you are a home robot, and is asked to go to the kitchen to fetch something from the fridge.\nAt the previous turn, you were at the position shown in the below image:\n\nNote: Same image from the previous turn, resized\nto save space\n\nThe action taken in previous turn is: turn right and move forward towards the kitchen.\n\nNow you are at the kitchen, this is what you can see from there. Can you see the fridge from here? What is\nyour next action?\n\nGPT-4V:\nFrom this position, | can see the fridge on the right side of the kitchen. My next action would be to move\n\nImagine that you are a home robot, and is asked to go to the kitchen to fetch something from the fridge.\nAt the previous turn, you were at the position shown in the below image:\n\nNote: Same image from the previous turn, resized\nto save space\n\nThe action taken in previous turn is: move forward and turn slightly to the right to approach the fridge.\n\nNow this is what you can see from there. What is your next action?\n\nGPT-4V:\n\nMy next action would be to move forward and slightly to the left to align myself with the fridge door. Then, |\nwould use my robotic arm to open the fridge door and retrieve the requested item.\n\nFigure 91: Acting as an embodied agent to navigate through a house to fetch something from the\n\nfridge (the 3rd and 4th turn). Blue highlights the predicted actions. Check Section 9.7 for detailed\ndiscussions.\n\n120\n", "vlm_text": "Sec. 9.7 Embodied Agent \nNavigation \nPrompt: \nImagine that you are a home robot, and is asked to go to the kitchen to fetch something from the fridge. At the previous turn, you were at the position shown in the below image: \nThe image depicts an interior space, likely a dining area. There is a wooden floor and a dining table with four chairs, with the chairs having white frames and wooden seats. A round mirror is mounted on the wall above the table, and there are two light fixtures on either side of the mirror. On the left side of the image, there are some plants, both hanging and potted. On the right side, a countertop with two stools is visible, suggesting the presence of a kitchen area adjacent to the dining space. The overall decor features a mix of natural and bright elements.\nNote: Same image from the previous turn, resized  to save space \nThe action taken in previous turn is: turn right and move forward towards the kitchen. \nNow you are at the kitchen, this is what you can see from there. Can you see the fridge from here? What is  your next action? \nThe image shows a kitchen with white cabinets and wood accents. There's a small island with two wooden stools, a sink with a window above it, and various kitchen appliances such as a microwave and stove. There's also a refrigerator and a countertop with items like a water dispenser and a blender. The floor is made of wood or wood-like material, and there are some wall-mounted planters on the left side.\nGPT-4V: \nFrom this position, I can see the fridge on the right side of the kitchen. My next action would be to move  forward and turn slightly to the right to approach the fridge.  \nPrompt: \nImagine that you are a home robot, and is asked to go to the kitchen to fetch something from the fridge. At the previous turn, you were at the position shown in the below image: \nThis image depicts a kitchen interior. Here are some features visible in the image:\n\n1. There is a kitchen island with two high stools with wooden seats and white backs.\n2. Various kitchen appliances are visible, such as a microwave and a stove/oven unit.\n3. There appears to be a water dispenser or jug on the countertop.\n4. Kitchen cabinets with white fronts and perhaps wooden trims surround the area.\n5. The flooring appears to be made of wood or wood-like material.\n6. Some additional accessories like a hanging wall basket and countertop items can also be seen, likely for storage or decoration purposes.\n\nOverall, it appears to be a well-organized, modern kitchen.\nNote: Same image from the previous turn, resized  to save space \nThe action taken in previous turn is: move forward and turn slightly to the right to approach the fridge. Now this is what you can see from there. What is your next action? \n\nThe image depicts a kitchen scene. On the left, there is a gas stove with four burners, and above it is a built-in range hood. Beside the stove, there is a countertop with various items, including bottles and containers, indicating cooking supplies. Above the stove is a decorative sign with an inspirational or religious message. To the right of the stove is a large white refrigerator with a couple of magnets or decorative items attached to the door. There are cabinets above both the stove and the refrigerator. The image is framed so that the viewer is looking at a corner of the kitchen.\nGPT-4V: \nMy next action would be to move forward and slightly to the left to align myself with the fridge door. Then, I  would use my robotic arm to open the fridge door and retrieve the requested item.  \nFigure 91: Acting as an embodied agent to navigate through a house to fetch something from the fridge (the 3rd and 4th turn).  Blue  highlights the predicted actions. Check Section  9.7  for detailed discussions. "}
{"page": 120, "image_path": "doc_images/2309.17421v2_120.jpg", "ocr_text": "9.8 GUI Navigation\n\nBeyond navigating the physical world, this section showcases the capability of GPT-4V to interact\nwith and navigate through the Graphical User Interface (GUI) of a computer or smartphone. We\nexplore the potential for GPT-4V to complete complex tasks, such as web browsing, online shopping,\nand etc.\n\nWeb browsing. We assess the performance of GPT-4V on computer GUI navigation under a\ntask-oriented setting. The model was provided with the screenshot of current computer screen, the\nend goal of the navigation (e.g., finding a cooking recipe or reading today’s news), the list of possible\nactions (e.g., move the mouse, click an icon with the mouse, or type some texts with the keyboard).\nThe model is then instructed to predict the subsequent actions (refer to Figure 92 for a complete\nprompt example). Upon the model’s prediction, we manually execute the predicted action and capture\na screenshot, which served as the input for GPT-4V for the next turn. When the predicted action\nis to move the mouse, GPT-4V is specifically instructed to detail the mouse’s position. Hence, the\npredicted actions are grounded, showing the potential of automating the whole process without human\nin the loop.\n\nIn Figures 92-96, GPT-4V predicts reasonable actions to operate a computer GUI, and finally\naccomplish the end goal of finding a recipe of Mapo Tofu and print out a copy of the recipe in\nFigure 95. We then provide GPT-4V a screenshot of the printed recipe and ask it to describe the\nprintout as detailed as possible. As shown in Figure 96, GPT-4V is able to recognize the details\npresented in the printout, including the cooking time, the list of ingredients, the author of the recipe,\nthe link to the original recipe and etc. Figures 97-102 present how GPT-4V can navigate through\nGUI to browse the web to “read today’s news”. Despite the minor errors in Figure 100 when it tries\nto return to the previous search result page to continue browsing for more news articles, GPT-4V can\nperform the navigation and read two news articles reasonably well.\n\nOnline shopping. Figures 103-111 illustrates how GPT-4V can navigate a smartphone GUI for\nonline shopping. Similarly, we provide GPT-4V with the screenshot of the current phone screen, the\nlist of possible actions (e.g., move your finger to an icon, click an icon with your finger, scroll down a\nscreen, or type some texts with the keyboard) and ask it to predict the subsequent actions to shop for\nan ergonomic keyboard with a budget between $50 and $100. GPT-4V predicts to open the Amazon\napp (Figure 103), search ergonomic keyboard (Figure 104), open the filter options (Figure 105), set\nthe price range filter between $50 and $100 (Figure 106), show filtered results (Figure 107), select\nthe top search result (Figure 108), view product details (Figure 109), add product to the shopping cart\n(Figure 110) and finally proceed to checkout (Figure 111).\n\nNotification understanding. Notifications are integral to modern human-computer interactions.\nGPT-4V has demonstrated its capacity to interpret notification content and respond accordingly. As\nshown in Figure 112, the model can read and respond to a notification, such as suggesting to open the\nMaps app in response to a meeting proposal in Seattle. It also handles call (Figure 113) and message\n(Figure 1 14) notifications on a computer screen effectively.\n\nWatching videos. Alongside web browsing, videos are a key source of online information. GPT-4V\nhas shown its capability to describe video content based on a series of screenshots from popular\nshort-form videos. Regardless of whether the video has subtitle overlay (Figure 115 and 116) or\nnot (Figure 117, 118, 119), GPT-4V can generate insightful descriptions about the video content,\ndemonstrating its potential in automatic transcript generation for user-generated video content.\n\n121\n", "vlm_text": "9.8 GUI Navigation \nBeyond navigating the physical world, this section showcases the capability of GPT-4V to interact with and navigate through the Graphical User Interface (GUI) of a computer or smartphone. We explore the potential for GPT-4V to complete complex tasks, such as web browsing, online shopping, and  etc . \nWeb browsing. We assess the performance of GPT-4V on computer GUI navigation under a task-oriented setting. The model was provided with the screenshot of current computer screen, the end goal of the navigation ( e.g ., finding a cooking recipe or reading today’s news), the list of possible actions ( e.g ., move the mouse, click an icon with the mouse, or type some texts with the keyboard). The model is then instructed to predict the subsequent actions (refer to Figure  92  for a complete prompt example). Upon the model’s prediction, we manually execute the predicted action and capture a screenshot, which served as the input for GPT-4V for the next turn. When the predicted action is to move the mouse, GPT-4V is specifically instructed to detail the mouse’s position. Hence, the predicted actions are grounded, showing the potential of automating the whole process without human in the loop. \nIn Figures  92 - 96 , GPT-4V predicts reasonable actions to operate a computer GUI, and finally accomplish the end goal of finding a recipe of Mapo Tofu and print out a copy of the recipe in Figure  95 . We then provide GPT-4V a screenshot of the printed recipe and ask it to describe the printout as detailed as possible. As shown in Figure  96 , GPT-4V is able to recognize the details presented in the printout, including the cooking time, the list of ingredients, the author of the recipe, the link to the original recipe and  etc . Figures  97 - 102  present how GPT-4V can navigate through GUI to browse the web to “read today’s news”. Despite the minor errors in Figure  100  when it tries to return to the previous search result page to continue browsing for more news articles, GPT-4V can perform the navigation and read two news articles reasonably well. \nOnline shopping. Figures  103 - 111  illustrates how GPT-4V can navigate a smartphone GUI for online shopping. Similarly, we provide GPT-4V with the screenshot of the current phone screen, the list of possible actions ( e.g ., move your finger to an icon, click an icon with your finger, scroll down a screen, or type some texts with the keyboard) and ask it to predict the subsequent actions to shop for an ergonomic keyboard with a budget between   $\\S50$   and   $\\S100$  . GPT-4V predicts to open the Amazon app (Figure  103 ), search ergonomic keyboard (Figure  104 ), open the filter options (Figure  105 ), set the price range filter between  $\\S50$   and   $\\S100$   (Figure  106 ), show filtered results (Figure  107 ), select the top search result (Figure  108 ), view product details (Figure  109 ), add product to the shopping cart (Figure  110 ) and finally proceed to checkout (Figure  111 ). \nNotification understanding. Notifications are integral to modern human-computer interactions. GPT-4V has demonstrated its capacity to interpret notification content and respond accordingly. As shown in Figure  112 , the model can read and respond to a notification, such as suggesting to open the Maps app in response to a meeting proposal in Seattle. It also handles call (Figure  113 ) and message (Figure  114 ) notifications on a computer screen effectively. \nWatching videos. Alongside web browsing, videos are a key source of online information. GPT-4V has shown its capability to describe video content based on a series of screenshots from popular short-form videos. Regardless of whether the video has subtitle overlay (Figure  115  and  116 ) or not (Figure  117 ,  118 ,  119 ), GPT-4V can generate insightful descriptions about the video content, demonstrating its potential in automatic transcript generation for user-generated video content. "}
{"page": 121, "image_path": "doc_images/2309.17421v2_121.jpg", "ocr_text": "Web Browsing (Mapo Tofu Recipe)\n\nPrompt:\nImagine that you are a robot operating a computer. Like how humans operate the computer, you can move the\nmouse, click an icon with the mouse, or type some texts with the keyboard.\n\nYou are asked to find a detailed recipe of Mapo Tofu.\n\nBelow is what you see on the computer screen, predict your next move. If the action involves moving the mouse,\ndescribe the location as detailed as possible.\n\nGPT-4V:\nNext move: | would move the mouse to the Google Chrome icon located on the left side of the screen,\napproximately 1/3 of the way down from the top. | would then click on the icon to open the web browser.\n\nImagine that you are a robot operating a computer. Like how humans operate the computer, you can move the\nmouse, click an icon with the mouse, or type some texts with the keyboard.\n\nYou are asked to find a detailed recipe of Mapo Tofu.\nPrevious action: \"| would move the mouse to the Google Chrome icon located on the left side of the screen,\napproximately 1/3 of the way down from the top. | would then click on the icon to open the web browser. \"\n\nYou have now opened the web browser. Below is what you see on the computer screen. What will you do next?\nIf the action involves moving the mouse, describe the location as detailed as possible.\n\nGPT-4V:\n\nNext, | would move the mouse to the search bar located in the center of the screen, approximately 1/2 of the\nway down from the top. | would then click on the search bar and type in \"Mapo Tofu recipe\" using the keyboard.\nAfter typing in the search query, | would press the \"Enter\" key on the keyboard to initiate the search.\n\nFigure 92: GPT-4V navigates through GUI to browse the web to search for the recipe of Mapo Tofu.\nBlue highlights the predicted actions. Check Section 9.8 for detailed discussions.\n\n122\n", "vlm_text": "Sec. 9.8 GUI Navigation \nWeb Browsing (Mapo Tofu Recipe) \nPrompt: \nImagine that you are a robot operating a computer. Like how humans operate the computer, you can move the  mouse, click an icon with the mouse, or type some texts with the keyboard. \nYou are asked to find a detailed recipe of Mapo Tofu. \nBelow is what you see on the computer screen, predict your next move. If the action involves moving the mouse,  describe the location as detailed as possible. \nThis image shows a computer desktop background featuring a scenic landscape of a purple flower field, likely lavender, with a stone wall in the foreground and trees in the background under a clear sky. There are icons on the left side of the screen, indicating various applications and software.\nGPT-4V: \nNext move: I would move the mouse to the Google Chrome icon located on the left side of the screen,  approximately 1/3 of the way down from the top. I would then click on the icon to open the web browser.  \nPrompt: \nImagine that you are a robot operating a computer. Like how humans operate the computer, you can move the  mouse, click an icon with the mouse, or type some texts with the keyboard.  \nYou are asked to find a detailed recipe of Mapo Tofu. \nPrevious action: \"I would move the mouse to the Google Chrome icon located on the left side of the screen,  approximately   $1/3$   of the way down from the top. I would then click on the icon to open the web browser. \" You have now opened the web browser. Below is what you see on the computer screen. What will you do next?  If the action involves moving the mouse, describe the location as detailed as possible. \nThe image shows a web browser's new tab page open to Google. In the background, there's a blurred cityscape with bokeh lights, focusing on wet pavement. The Google search bar is in the center, and below it are icons for Gmail, Drive, Course Planner, and other services. The address bar has YouTube, Maps, and Gmail shortcuts. The background photo is credited to Ev Tchebotarev.\nGPT-4V: \nNext, I would move the mouse to the search bar located in the center of the screen, approximately  $_{1/2}$   of the  way down from the top. I would then click on the search bar and type in \"Mapo Tofu recipe\" using the keyboard.  After typing in the search query, I would press the \"Enter\" key on the keyboard to initiate the search.  \nFigure 92: GPT-4V navigates through GUI to browse the web to search for the recipe of Mapo Tofu. Blue highlights the predicted actions. Check Section  9.8  for detailed discussions. "}
{"page": 122, "image_path": "doc_images/2309.17421v2_122.jpg", "ocr_text": "Sec. 9.8 GUI Navigation\n\nWeb Browsing (Mapo Tofu Recipe)\n\nPrompt:\nImagine that you are a robot operating a computer. Like how humans operate the computer, you can move the\nmouse, click an icon with the mouse, or type some texts with the keyboard.\n\nYou are asked to find a detailed recipe of Mapo Tofu.\n\nPrevious action: \"| would move the mouse to the search bar located in the center of the screen, approximately\n1/2 of the way down from the top. | would then click on the search bar and type in \"Mapo Tofu recipe\" using the\nkeyboard. After typing in the search query, | would press the \"Enter\" key on the keyboard to initiate the search.\"\n\nYou have now completed the previous action as described above. Below is what you see on the computer\nscreen. What will you do next? If the action involves moving the mouse, describe the location as detailed as\npossible.\n\nmapo tofu recipe\nVideos || Vegetarian Simple | Japanese Korean Notspicy Images. | Madewithieu | Recipe\n\nAbout 9,850,000 results (0.41 seconds)\n\n‘The Woks of Life\nhttps://thewoksofife.com Recipes» Tofu\n\nMapo Tofu Recipe: The Real Deal\n\nJun 28,2019 ~ Mapo Tofu is a popular Chinese dish from Sichuan Province, where\nspley food is king and the signature spice of the region——the Sichuan\n\n+h ek He Rating: 4.9 299 votes - 35 min\n\nVegan Mapo Tofu: Cheat's Mapo Tofu Packet Tofu, Bean Curd & Seltan\n\nRecipes\n\n~~\n\nMapo Tofu Authentic Mapo Tofu (ff | Mapo Tofu\n\nThe Woks of Life Bm) NYT Cooking - The New York T.\n4.9 dee ee (299) Omnivore's Cookbook 4.0 de de eke (2K),\n\n35min 4.9 te tee ee (69) 30 min\n\n‘Spicy bean sauce, ground pork, 25min Bean sauce, ground beef,\nsilken tofu, sesame oll, chicken Five spice powder, ground pork, _ fermented black beans, soy\n\nsoy sauce, homemade chill oil\nShow more v\n\nThe New York Times\nhttps://cooking nytimes.com 1021459-mapotofu\n\nMapo Tofu Recipe - NYT Cooking\n\nIngredients - 16ounces medium or medium-firm tofu (if unavailable, go with firm)\n‘rounded teaspoon Sichuan peppercoms - Stablespoons canola oll- 6ounces ground\n+e ee Rating: 42,040 votes - 30 min\n\nPeople also ask\n\nWhat is mapo tofu sauce made of?\n\nIs Szechuan tofu the same as mapo tofu?\n\nWhy do people like mapo tofu?\n\n: \"| would move the mouse to the first search result under the \"Recipes\" section, which is titled \"Mapo\nTofu Recipe - The Woks of Life\". | would then click on the link to open the webpage and view the detailed\nrecipe.\"\n\nFigure 93: GPT-4V navigates through GUI to browse the web to search for the recipe of Mapo Tofu.\nBlue highlights the predicted actions. Check Section 9.8 for detailed discussions.\n\n123\n", "vlm_text": "Sec. 9.8 GUI Navigation \nWeb Browsing (Mapo Tofu Recipe) \nPrompt: \nImagine that you are a robot operating a computer. Like how humans operate the computer, you can move the  mouse, click an icon with the mouse, or type some texts with the keyboard.  \nYou are asked to find a detailed recipe of Mapo Tofu. \nPrevious action: \" I would move the mouse to the search bar located in the center of the screen, approximately  1/2 of the way down from the top. I would then click on the search bar and type in \"Mapo Tofu recipe\" using the  keyboard. After typing in the search query, I would press the \"Enter\" key on the keyboard to initiate the search.\" \nYou have now completed the previous action as described above. Below is what you see on the computer  screen. What will you do next? If the action involves moving the mouse, describe the location as detailed as  possible. \nThe image shows a Google search results page for \"mapo tofu recipe.\" It includes links to various mapo tofu recipes from different websites such as The Woks of Life and The New York Times. There are ratings, brief descriptions, and images of the dish. The page also includes a \"People also ask\" section with related questions.\nGPT-4V: \nNext action: \"I would move the mouse to the first search result under the \"Recipes\" section, which is titled \"Mapo  Tofu Recipe - The Woks of Life\". I would then click on the link to open the webpage and view the detailed  recipe.\"  \nFigure 93: GPT-4V navigates through GUI to browse the web to search for the recipe of Mapo Tofu. Blue highlights the predicted actions. Check Section  9.8  for detailed discussions. "}
{"page": 123, "image_path": "doc_images/2309.17421v2_123.jpg", "ocr_text": "Sec. 9.8 GUI Navigation\nWeb Browsing (Mapo Tofu Recipe)\n\nPrompt:\n\nImagine that you are a robot operating a computer. Like how humans operate the computer, you can move\nthe mouse, click an icon with the mouse, or type some texts with the keyboard.\n\nYou are asked to find a detailed recipe of Mapo Tofu.\n\nPrevious action: “I would move the mouse to the first search result under the \"Recipes\" section, which is\n\ntitled \"Mapo Tofu Recipe - The Woks of Life\". | would then click on the link to open the webpage and view the\ndetailed recipe.\"\n\nYou have now completed the previous action as described above. Below is what you see on the computer\nscreen. Based on the image below, what will you do next? If the action involves moving the mouse, describe\nthe location as detailed as possible.\n\nhome > recipes > tofu > mapo tofu\n\nMAPO TOFU\n\n8 by: KAITLIN © 894 COMMENTS JUMP TO RECIPE 4\n\nPOSTED: 6/28/2019 UPDATED: 3/22/2023\n\nThis mapo tofu recipe (fA) is the true blue, authentic real deal—the spicy,\ntongue-numbing, rice-is-absolutely-not-optional, can’t-have-just-one-scoop mapo\n\ntofu that you get in the restaurants.\n\nGPT-4V:\n\nNext action: \"| would move the mouse to the \"Jump to Recipe\" button located at the top right corner of the\nwebpage. | would then click on the button to jump to the detailed recipe section of the page.\"\n\nFigure 94: GPT-4V navigates through GUI to browse the web to search for the recipe of Mapo Tofu.\nBlue highlights the predicted actions. Check Section 9.8 for detailed discussions.\n\n124\n", "vlm_text": "Sec. 9.8 GUI Navigation \nWeb Browsing (Mapo Tofu Recipe) \nPrompt: \nImagine that you are a robot operating a computer. Like how humans operate the computer, you can move  the mouse, click an icon with the mouse, or type some texts with the keyboard.  \nYou are asked to find a detailed recipe of Mapo Tofu. \nPrevious action: “I would move the mouse to the first search result under the \"Recipes\" section, which is  titled \"Mapo Tofu Recipe - The Woks of Life\". I would then click on the link to open the webpage and view the  detailed recipe.\" \nYou have now completed the previous action as described above. Below is what you see on the computer  screen. Based on the image below, what will you do next? If the action involves moving the mouse, describe  the location as detailed as possible. \nThe image shows a webpage section for a recipe titled \"Mapo Tofu.\" It includes navigation links: home > recipes > tofu > mapo tofu. The author of the recipe is listed as \"Kaitlin,\" and there are 894 comments. There's also a button to \"Jump to Recipe.\"\nThe image depicts a bowl of Mapo Tofu, a spicy and flavorful Chinese dish. Mapo Tofu typically consists of cubes of tofu set in a spicy sauce, typically a thin, oily, and bright red suspension based on doubanjiang (fermented broad bean and chili paste) and douchi (fermented black beans), along with minced meat, usually pork or beef. It's often garnished with chopped green onions or Sichuan peppercorns, lending a characteristic numbing and spicy flavor. The dish is traditionally served over or alongside steamed rice.\n\nGPT-4V: \nNext action: \"I would move the mouse to the \"Jump to Recipe\" button located at the top right corner of the  webpage. I would then click on the button to jump to the detailed recipe section of the page.\" \nFigure 94: GPT-4V navigates through GUI to browse the web to search for the recipe of Mapo Tofu. Blue highlights the predicted actions. Check Section  9.8  for detailed discussions. "}
{"page": 124, "image_path": "doc_images/2309.17421v2_124.jpg", "ocr_text": "Sec. 9.8 GUI Navigation\n\nWeb Browsing (Mapo Tofu Recipe)\n\nPrompt:\nImagine that you are a robot operating a computer. Like how humans operate the computer, you can move\nthe mouse, click an icon with the mouse, or type some texts with the keyboard.\n\nYou are asked to find a detailed recipe of Mapo Tofu.\n\nPrevious action: “| would move the mouse to the \"Jump to Recipe\" button located at the top right corner of\nthe webpage. | would then click on the button to jump to the detailed recipe section of the page.\"\n\nYou have now completed the previous action as described above. Below is what you see on the computer\nscreen. Based on the image below, what will you do next? If the action involves moving the mouse, describe\nthe location as detailed as possible.\n\nAHA IC 459 from 299 votes\nMAPO TOFU\n\nThis mapo tofu recipe is the true blue, authentic real deal\n—the spicy, tongue-numbing, rice-is-absolutely-not-\noptional mapo tofu that you get in the restaurants!\n\nRECIPE\n\nby: kaitlin course: tofu cuisine: chinese\n\n© PREP: 10 minutes COOK: 25 minutes TOTAL: 35 minutes\n\nINGREDIENTS B print\n\n1-2 fresh Thai bird chili\n\npeppers (thinly sliced)\n6-8 dried red chilies (roughly\n\ncropped\n\n1/2- 1 1/2 tablespoons Sichuan\n\npeppercorns (powdered or finely\n\n‘ground, reserving 1/4 teaspoon\n\ngarnish at the end; if you want a milder\n\nflavor use 1/2 or 1 teaspoon ground\n\nSichuan peppercorn)\n\n3 tablespoons ginger (finely minced\n\n3 tablespoons garlic (finely minced\n\n8 ounces ground pork\n\n1-2 tablespoons spicy bean sauce (depending on your desired salt/spce levels)\n2/3 cup low sodium chicken broth (or water)\n\n1 pound silken tofu (cut into 1 inck/2Sem cubes)\n\n1/4 cup water\n\n1 1/2 teaspoons cornstarch\n\n1/4 teaspoon sesame oil (optional)\n\n1/4 teaspoon sugar (optional)\n\n1 scallion (finely hopped)\n\nus cusTomaRY [iin\n\nINSTRUCTIONS\n@ First, we toast the chilies. If you have homemade toasted chili oil, you can\n\nskip this step. Heat your wok or a small saucepan over low heat. Add half\n\nODOooOoOoOoooOoOoO\n\nof the oil and throw in the fresh and dried peppers. Stir occasionally and\n\nGPT-4V:\n\nNext action: \"| would move the mouse to the \"Print Recipe\" button located at the top right corner of the recipe\nsection, next to the \"Rate This Recipe\" button. | would then click on the button to print out a copy of the detailed\nrecipe for Mapo Tofu.”\n\nFigure 95: GPT-4V navigates through GUI to browse the web to search for the recipe of Mapo Tofu.\nBlue highlights the predicted actions. Check Section 9.8 for detailed discussions.\n\n125\n", "vlm_text": "Sec. 9.8 GUI Navigation \nWeb Browsing (Mapo Tofu Recipe) \nPrompt: \nImagine that you are a robot operating a computer. Like how humans operate the computer, you can move  the mouse, click an icon with the mouse, or type some texts with the keyboard.  \nYou are asked to find a detailed recipe of Mapo Tofu. \nPrevious action: “I would move the mouse to the \"Jump to Recipe\" button located at the top right corner of  the webpage. I would then click on the button to jump to the detailed recipe section of the page.\" \nYou have now completed the previous action as described above. Below is what you see on the computer  screen. Based on the image below, what will you do next? If the action involves moving the mouse, describe  the location as detailed as possible. \nThe image is a recipe for Mapo Tofu. It shows the following details:\n\n- **Rating**: 4.89 from 299 votes\n- **Description**: Describes it as an authentic spicy, tongue-numbing dish.\n- **Author**: Kaitlin\n- **Course**: Tofu\n- **Cuisine**: Chinese\n- **Servings**: 6\n- **Prep Time**: 10 minutes\n- **Cook Time**: 25 minutes\n- **Total Time**: 35 minutes\n\n**Ingredients** include:\n\n- Oil\n- Fresh Thai bird chili peppers\n- Dried red chilies\n- Sichuan peppercorns\n- Ginger\n- Garlic\n- Ground pork\n- Spicy bean sauce\n- Low sodium chicken broth\n- Silken tofu\n- Water\n- Cornstarch\n- Sesame oil (optional)\n- Sugar (optional)\n- Scallion\n\nThere are buttons for printing, rating, and converting measurements. There's a section for instructions starting with toasting chilies.\nGPT-4V: \nNext action: \"I would move the mouse to the \"Print Recipe\" button located at the top right corner of the recipe  section, next to the \"Rate This Recipe\" button. I would then click on the button to print out a copy of the detailed  recipe for Mapo Tofu.”  \nFigure 95: GPT-4V navigates through GUI to browse the web to search for the recipe of Mapo Tofu. Blue highlights the predicted actions. Check Section  9.8  for detailed discussions. "}
{"page": 125, "image_path": "doc_images/2309.17421v2_125.jpg", "ocr_text": "Sec. 9.8 GUI Navigation\n\nWeb Browsing (Mapo Tofu Recipe)\n\nPrompt:\nDescribe the image below as detailed as possible.\n\nMapo Tofu\n\nThis mapo tofu recipe is the true blue, authentic real deal—the spicy,\n‘tongue-numbing, rice-is-absolutely-not-optional mapo tofu that you get\nin the restaurants!\n\nPrep Time Cook Time Total Time\n10 mine 25 mins 35 mine\n\n‘ourse: Tofu Cuisine: Chinese Servings: 6 zeringe\nCalories: 335teal Author: Kaitlin\n\nIngredients\n\n+ Y2cup oil (divided)\n\n+ 1-2 fresh Thai bird chili peppers (thinly sliced)\n\n+ 6-8 dried red chilies (roughly chopped)\n\n* 1/2: 11/2 tablespoons Sichuan peppercoms (powdered or finely ground, reserving 1/4\nteaspoon for gamish atthe end; if you want a milder flavor use 1/2 or 1 teaspoon ground\nSichuan peppercor)\n\n3 tablespoons ginger (finely minced)\n\n3 tablespoons gatlic (finely minced)\n\n8 ounces ground pork\n\n1-2 tablespoons spicy bean sauce (depending on your desired salt/spice levels)\n2/3 cup low sodium chicken broth (or water)\n\n1 pound silken tofu (cut into 1 inchy/2.Sem cubes)\n1/4 cup water\n\n11/2 teaspoons comstarch\n\n1/4 teaspoon sesame oil (optional)\n\n1/4 teaspoon sugar (optional)\n\n1 scallion (finely chopped)\n\nInstructions\n\n1. First, we toast the chilies f you have homemade toasted chil cil, you can skip ths step. Heat\n‘your wok or a small saucepan over low heat. Add half of the oil and throw in the fresh and\ndried peppers. Stir occasionally and heat until fragrant, about S minutes, ensuring that the\nPeppers don't bum. Remove from heat and set aside.\n\n2. Heat the remaining half of the oil in your wok over medium heat. Add the ginger. After 1\nminute, add the garlic. Fry for another minute, and then tun up the heat to high and add the\n‘ground pork. Break up the meat and fry it until it's cooked through. Add your ground Sichuan\nPeppercoms and stir for about 15-30 seconds, taking care to not let it burn a it will tum\nbitter ifit does.\n\n3. Add the spicy bean sauce to the mixture and str it in well Add the chicken broth to the wok\nand sti. Let this simmer for a minute or so. While that's happening, ready your tofu and\ncombine the water and cornstarch in a small bow.\n\n4. Add the comstarch mixture to your sauce and stir Let it bubble away until the sauce starts to\nthicken (fit gets too thick, splash ina little more water or chicken stock)\n\n5. Then add your chil il from before—peppers and all! if you are using homemade chil cil,\n‘ONLY use the standing oil, s its likely that you have salted it and you only want the oil, not\n‘additional salt. tir the il into the sauce, and add the tofu. Use your spatula to gently toss the\ntofu in the sauce. Let everything cook for 3-5 minutes. Add the sesame oil and sugar if using)\n‘along with the scalions and str until the scallions are just wilted.\n\n6. Serve with a last sprinkle of Sichuan peppercom powder as a gamish if desired.\n\nNutrition\nCalories: 335kea | Carbohydrates: &g | Protein: 11g | Fat: 299 | Saturated Fat: Sg | Cholesterol: 27mg |\nSodium: 126mg | Potassium: 340mg | Fiber: 1g | Sugar: 2g | Vitamin A: 1651 | Vitamin C: 3.5m |\nCalcium 47mg | Iron: 14mg\n\n‘Mapo Tofu by The Woks of Life, Recipe URL htps//thewoksoffecom/ma-po-tofu-reai-dea\n\nGPT-4V:\n\nThe image is a recipe for Mapo Tofu, a Chinese dish. The recipe is presented in a structured format with a title,\ndescription, prep time, cook time, total time, course, cuisine, servings, and rating.\n\nThe title of the recipe is \"Mapo Tofu\" and it is written in bold black letters. Below the title, there is a description of\nthe dish which states that it is the true blue, authentic real deal - the spicy, tongue-numbing,\nrice-is-absolutely-not-optional Mapo Tofu that you get in the restaurants.\n\nThe prep time for the dish is 10 minutes, the cook time is 25 minutes, and the total time is 35 minutes. The\ncourse is listed as \"Main Course\" and the cuisine is \"Chinese\". The recipe serves 6 people and has a rating of\n\nBelow this information, there is a list of ingredients required for the dish, including tofu, ground pork, Sichuan\npeppercorn, garlic, ginger, soy sauce, and cornstarch. The ingredients are listed in a bullet point format.\n\nAt the bottom of the image, there is nutritional information for the dish, including calories, carbohydrates, protein,\nfat, saturated fat, cholesterol, sodium, and calcium. The nutritional information is presented in a table format.\nThe image also includes a credit to the author of the recipe, Kaitlin, and a link to the original recipe on the\nwebsite \"The Woks of Life\".\n\nFigure 96: GPT-4V navigates through GUI to browse the web to search for the recipe of Mapo Tofu.\nAs GPT-4V predicts to print out the recipe in the previous turn, we prompt it to read the screenshot\n\nof the printed recipe and summarize it. ‘Red highlights the inaccurate description about the image.\nCheck Section 9.8 for detailed discussions.\n\n126\n", "vlm_text": "Sec. 9.8 GUI Navigation \nWeb Browsing (Mapo Tofu Recipe) \nPrompt: \nDescribe the image below as detailed as possible. \nThe image contains a recipe for Mapo Tofu. It includes details such as the prep time (10 minutes), cook time (25 minutes), total time (35 minutes), and servings (6 servings). The recipe is contributed by an author named Kaitlin and has a rating of 4.89 from 300 votes. \n\nThe ingredients listed for Mapo Tofu are:\n\n- ½ cup oil (divided)\n- 1-2 fresh Thai bird chili peppers (thinly sliced)\n- 6-8 dried red chilies (roughly chopped)\n- 1/2-1 ½ tablespoons Sichuan peppercorns\n- 3 tablespoons ginger (finely minced)\n- 3 tablespoons garlic (finely minced)\n- 8 ounces ground pork\n- 1-2 tablespoons spicy bean sauce\n- 2/3 cup low sodium chicken broth (or water)\n- 1 pound silken tofu (cut into 1 inch/2.5cm cubes)\n- 1/4 cup water\n- 1 ½ teaspoons cornstarch\n- 1/4 teaspoon sesame oil (optional)\n- 1/4 teaspoon sugar (optional)\n- 1 scallion (finely chopped)\n\nThe instructions to prepare the dish are provided in six steps, focusing on toasting chilies, cooking the ground pork with spices, adding ingredients like the spicy bean sauce and chicken broth, creating a cornstarch mixture, adding tofu, and finishing with garnishing. \n\nThe nutritional information per serving is also included, showing 335 kcal, 8g carbohydrates, 11g protein, and 29g fat, among other details. The recipe is attributed to The Woks of Life, with a specified URL for further details.\nGPT-4V: \nThe image is a recipe for Mapo Tofu, a Chinese dish. The recipe is presented in a structured format with a title,  description, prep time, cook time, total time, course, cuisine, servings, and rating.  \nThe title of the recipe is \"Mapo Tofu\" and it is written in bold black letters. Below the title, there is a description of  the dish which states that it is the true blue, authentic real deal - the spicy, tongue-numbing,  rice-is-absolutely-not-optional Mapo Tofu that you get in the restaurants.  \nThe prep time for the dish is 10 minutes, the cook time is 25 minutes, and the total time is 35 minutes. The  course is listed as \"Main Course\" and the cuisine is \"Chinese\". The recipe serves 6 people and has a rating of  \n4.69 stars out of 5. \nBelow this information, there is a list of ingredients required for the dish, including tofu, ground pork, Sichuan  peppercorn, garlic, ginger, soy sauce, and cornstarch. The ingredients are listed in a bullet point format. At the bottom of the image, there is nutritional information for the dish, including calories, carbohydrates, protein,  fat, saturated fat, cholesterol, sodium, and calcium. The nutritional information is presented in a table format. The image also includes a credit to the author of the recipe, Kaitlin, and a link to the original recipe on the  website \"The Woks of Life\".  \nFigure 96: GPT-4V navigates through GUI to browse the web to search for the recipe of Mapo Tofu. As GPT-4V predicts to print out the recipe in the previous turn, we prompt it to read the screenshot of the printed recipe and summarize it.  Red  highlights the inaccurate description about the image. Check Section  9.8  for detailed discussions. "}
{"page": 126, "image_path": "doc_images/2309.17421v2_126.jpg", "ocr_text": "Sec. 9.8 GUI Navigation\nWeb Browsing (Today’s News)\n\nPrompt:\nImagine that you are a robot operating a computer. Like how humans operate the computer, you can move the\nmouse, click an icon with the mouse, or type some texts with the keyboard.\n\nYou are asked to find what's in the news today.\n\nBelow is what you see on the computer screen, predict your next move. If the action involves moving the mouse,\ndescribe the location as detailed as possible.\n\nGPT-4V:\n\nMy next move would be to move the mouse to the Google Chrome icon located on the left side of the screen,\napproximately 1/3 of the way down from the top. | would then click on the icon to open the web browser and\nsearch for news websites to find out what's in the news today.\n\nImagine that you are a robot operating a computer. Like how humans operate the computer, you can move the\nmouse, click an icon with the mouse, or type some texts with the keyboard.\n\nYou are asked to find what's in the news today.\n\nPrevious action: \"My next move would be to move the mouse to the Google Chrome icon located on the left side\nof the screen, approximately 1/3 of the way down from the top. | would then click on the icon to open the web\nbrowser and search for news websites to find out what's in the news today. \"\n\nYou have now opened the web browser. Below is what you see on the computer screen, what is your immediate\nnext step? If the action involves moving the mouse, describe the location as detailed as possible.\n\nGPT-4V:\n\nMy immediate next step would be to move the mouse to the search bar located in the center of the screen,\napproximately 1/2 of the way down from the top. | would then click on the search bar and type in \"news\" and\npress the enter key on the keyboard to search for news websites.\n\nFigure 97: GPT-4V navigates through GUI to browse the web to read today’s news. Blue highlights\nthe predicted actions. Check Section 9.8 for detailed discussions.\n\n127\n", "vlm_text": "Sec. 9.8 GUI Navigation \nWeb Browsing (Today’s News) \nPrompt: \nImagine that you are a robot operating a computer. Like how humans operate the computer, you can move the  mouse, click an icon with the mouse, or type some texts with the keyboard. \nYou are asked to find what’s in the news today. \nBelow is what you see on the computer screen, predict your next move. If the action involves moving the mouse,  describe the location as detailed as possible. \nThe image shows a computer desktop with various application icons on the left side of the screen. The background wallpaper features a scenic view of a field with rows of purple flowers, possibly lavender, in front of a stone wall. There are a few large trees in the background under an open sky. The desktop icons include familiar applications such as Recycle Bin, Google Chrome, Zoom, WhatsApp, and others, arranged vertically along the left side.\nGPT-4V: \nMy next move would be to move the mouse to the Google Chrome icon located on the left side of the screen,  approximately 1/3 of the way down from the top. I would then click on the icon to open the web browser and  search for news websites to find out what's in the news today.  \nPrompt: \nImagine that you are a robot operating a computer. Like how humans operate the computer, you can move the  mouse, click an icon with the mouse, or type some texts with the keyboard.  \nYou are asked to find what’s in the news today. \nPrevious action: \"My next move would be to move the mouse to the Google Chrome icon located on the left side  of the screen, approximately  $1/3$   of the way down from the top. I would then click on the icon to open the web  browser and search for news websites to find out what's in the news today. \" \nYou have now opened the web browser. Below is what you see on the computer screen, what is your immediate  next step? If the action involves moving the mouse, describe the location as detailed as possible. \nThe image shows a screenshot of a Google Chrome web browser window. The browser is open to the new tab page, which features the Google logo and a search bar for searching Google or entering a URL. Below the search bar, there are several shortcuts to various Google services or apps such as Inbox, Sign in, Course Planner, and the Web Store, followed by an “Add shortcut” option. \n\nThe background image on the new tab page is a blurred cityscape featuring lights and reflections, giving a sense of an urban, possibly rainy environment. This image is credited to Ev Tchebotarev, whose name appears at the bottom left corner of the window.\n\nThere are additional tabs open in the browser but not visible in the screenshot. The browser window itself has a bluish hue, indicating that this is perhaps a customized theme. In the top right, the profile icon shows that the browser's sync feature is paused.\nGPT-4V: \nMy immediate next step would be to move the mouse to the search bar located in the center of the screen,  approximately 1/2 of the way down from the top. I would then click on the search bar and type in \"news\" and  press the enter key on the keyboard to search for news websites.  \nFigure 97: GPT-4V navigates through GUI to browse the web to read today’s news.  Blue  highlights the predicted actions. Check Section  9.8  for detailed discussions. "}
{"page": 127, "image_path": "doc_images/2309.17421v2_127.jpg", "ocr_text": "Sec. 9.8 GUI Navigation\n\nWeb Browsing (Today’s News)\n\nPrompt:\nImagine that you are a robot operating a computer. Like how humans operate the computer, you can move the\nmouse, click an icon with the mouse, or type some texts with the keyboard.\n\nYou are asked to find what's in the news today.\nPrevious action: \"My immediate next step would be to move the mouse to the search bar located in the center of\n\nthe screen, approximately 1/2 of the way down from the top. | would then click on the search bar and type in\n\"news\" and press the enter key on the keyboard to search for news websites. \"\n\nYou have now completed the previous action. Below is what you see on the computer screen. Based on the\nimage below, what is your immediate next step? If the action involves moving the mouse, describe the location\nas detailed as possible.\n\nGoogle news\n\nNews Videos Images Books | Shopping Maps Flights _—_- Finance\n\nAbout 25,270,000,000 results (0.46 seconds)\n\nTop stories\n\nGPT-4V:\n\nB Politico\n\nState Department doubles\ndown after Zelenskyy slams\nreluctance ...\n\n6 hours ago\n\n@> Reuters\n\nUS says it no longer deems\nDonald Trump immune from\nE. Jean...\n\n2 hours ago\n\n€ The New York Times\n\nHard Right Presses Culture\nWar Fights on Defense Bill,\nImperiling ...\n\n2 hours ago\n\nEi cnn\n\nCatastrophic flooding\nswamped Vermont's capital\nas intense storms forced.\n\n1 hour ago\n\n& Dcist\nD.C. Council Approves\nEmergency Crime Bill\n\n5 hours ago\n\n© NPR\nThe Anthropocene began in\n1950s, scientists say\n\n3 hours ago\n\nMy immediate next step would be to move the mouse to the first search result, which is \"State Department\ndoubles down on Zelensky slams\" located approximately 1/4 of the way down from the top of the screen and 1/8\nof the way from the left side of the screen. | would then click on the link to open the news article and read about\nwhat's in the news today.\n\nFigure 98: GPT-4V navigates through GUI to browse the web to read today’s news. Blue highlights\nthe predicted actions. Check Section 9.8 for detailed discussions.\n\n128\n", "vlm_text": "Sec. 9.8 GUI Navigation \nWeb Browsing (Today’s News) \nPrompt: \nImagine that you are a robot operating a computer. Like how humans operate the computer, you can move the  mouse, click an icon with the mouse, or type some texts with the keyboard.  \nYou are asked to find what’s in the news today. \nPrevious action: \"My immediate next step would be to move the mouse to the search bar located in the center of  the screen, approximately   $_{1/2}$   of the way down from the top. I would then click on the search bar and type in  \"news\" and press the enter key on the keyboard to search for news websites. \" \nYou have now completed the previous action. Below is what you see on the computer screen. Based on the  image below, what is your immediate next step? If the action involves moving the mouse, describe the location  as detailed as possible. \nThe image shows a Google search results page for the query \"news.\" It displays a list of top stories from various news outlets, including:\n\n1. Politico: An article about the State Department and Zelenskyy.\n2. CNN: News about catastrophic flooding in Vermont.\n3. Reuters: Article about Donald Trump's immunity status.\n4. DCist: D.C. Council's approval of an emergency crime bill.\n5. The New York Times: Discussion on culture war and defense bill.\n6. NPR: Scientists identifying the start of the Anthropocene in the 1950s.\n\nEach news item includes a headline, a thumbnail image, the source, and how long ago it was published.\nGPT-4V: \nMy immediate next step would be to move the mouse to the first search result, which is \"State Department  doubles down on Zelensky slams\" located approximately 1/4 of the way down from the top of the screen and 1/8  of the way from the left side of the screen. I would then click on the link to open the news article and read about  what's in the news today.  \nFigure 98: GPT-4V navigates through GUI to browse the web to read today’s news.  Blue  highlights the predicted actions. Check Section  9.8  for detailed discussions. "}
{"page": 128, "image_path": "doc_images/2309.17421v2_128.jpg", "ocr_text": "Sec. 9.8 GUI Navigation\nWeb Browsing (Today’s News)\n\nPrompt:\nRead the article shown in the images below and summarize it.\n\n“We are ready to extend an invitation when conditions are met,” Miller said.\nPOLITICO “There isan important reson wy hey arent beeing « member of NATO\nright now — because it would instantly put the United States in a shooting war\n\nFOREIGN AFFAIRS with Russia.’\n\nState Department doubles down after Miller echoed the sentiment of President Joe Biden, who told CNN on Sunday\nZelenskyy slams reluctance on that Ukraine could not be admitted into the alliance until the war with Russia\nUkraine’s entry to NATO had ended, adding that “if the war is going on, then we're alin war.”\n\nvee as besa ‘ongoing war with Russia The U.S. has doubled down this week on military support of Ukraine in its war\nwith Russia, agreeing to include controversial cluster munitions in its next\narms package to Kyiv — a decision that Ukrainian Minister of Defense Oleksiy\nReznikov said would be a “game changer” as the Ukrainian counteroffensive\n\nstruggles to breach Russia's front lines.\n\nIna summit communiqué released Tuesday, NATO leaders wrote that the\nalliance could not promise Ukraine membership until the war had ended and\nseveral democratic and security-sector reforms had been achieved.\n\n“We will be in a position to extend an invitation to Ukraine to join the Alliance\nwhen Allies agree and conditions are met,” the leaders wrote.\n\nIna statement posted to Twitter on Tuesday, Zelenskyy took issue with NATO's\nSSS senna reluctance to commit to admitting his country, and its unwillingness to provide\nNATO relictanca to ammo ating our Fanci Seca /A an anticipated timeline for Ukraine's entry. He wrote that he did not see\n\noyu 7 sufficient readiness on the part of NATO to integrate Ukraine into the alliance.\ny LUCY HODGMAN °)\n\n“It’s unprecedented and absurd when time frame is not set neither for the\n\nThe State Department on Tuesday defended NATO's hesitation to invite\ninvitation nor for Ukraine’s membership,” Zelenskyy wrote. “While at the same\n\nUkraine into the alliance, even as Ukrainian President Volodymyr Zelenskyy\n-oceths etlo Wth Me he gutiin eNAa time vague wording about ‘conditions’ is added even for inviting Ukraine.”\n\nState Department press secretary Matthew Miller spoke to reporters on\nTuesday from the Lithuanian capital, Vilnius, where NATO's two-day summit\nis being held this week. Miller cited several conditions that NATO had decided\n‘Ukraine must meet for an invitation to the alliance, including democratic\nreforms and the end of the war between Russia and Ukraine.\n\nGPT-4V:\n\nSummary:\n\nThe article discusses the statement made by Ukrainian President Volodymyr Zelensky on Tuesday, where he\nexpressed his country's desire to join NATO. Zelensky stated that Ukraine cannot afford to go to war with Russia\nand that joining NATO would provide the necessary security and protection.\n\nHowever, Matthew Miller, a spokesperson for NATO, stated that while the alliance is ready to extend an\ninvitation to Ukraine, there are certain conditions that need to be met. These conditions include democratic and\nsecurity-sector reforms. Miller also emphasized that Ukraine's entry into NATO is not a guarantee and that the\ndecision ultimately lies with the members of the alliance.\n\nThe article also mentions that during a press conference on Tuesday, Zelensky reiterated his commitment to\nstrengthening democracy in Ukraine, which is a necessary condition for NATO membership.\n\nFigure 99: GPT-4V navigates through GUI to browse the web to read today’s news. We prompt\nGPT-4V to read the screenshots of the first news article and summarize it. Check Section 9.8 for\ndetailed discussions.\n\n129\n", "vlm_text": "Sec. 9.8 GUI Navigation \nWeb Browsing (Today’s News) \nPrompt: \nRead the article shown in the images below and summarize it. \nThe image is a screenshot from an article on Politico discussing the U.S. State Department's stance on Ukraine's entry into NATO. The article explains that the State Department, through press secretary Matthew Miller, has justified the U.S.'s hesitance to support Ukraine's immediate entry into NATO by citing the ongoing war with Russia. Miller reiterated President Biden's position that Ukraine could not join NATO until the war with Russia ends. Furthermore, the U.S. has reinforced its military support for Ukraine by agreeing to include controversial cluster munitions in its arms package to Kyiv. The NATO summit communiqué stated that membership for Ukraine could not be extended until the war ends and necessary reforms are achieved. Ukrainian President Volodymyr Zelenskyy criticized NATO's reluctance and the lack of a clear timeline for Ukraine's membership, calling it unprecedented and absurd.\nGPT-4V: Summary: \nThe article discusses the statement made by Ukrainian President Volodymyr Zelensky on Tuesday, where he  expressed his country's desire to join NATO. Zelensky stated that Ukraine cannot afford to go to war with Russia  and that joining NATO would provide the necessary security and protection. \nHowever, Matthew Miller, a spokesperson for NATO, stated that while the alliance is ready to extend an  invitation to Ukraine, there are certain conditions that need to be met. These conditions include democratic and  security-sector reforms. Miller also emphasized that Ukraine's entry into NATO is not a guarantee and that the  decision ultimately lies with the members of the alliance. \nThe article also mentions that during a press conference on Tuesday, Zelensky reiterated his commitment to  strengthening democracy in Ukraine, which is a necessary condition for NATO membership.  "}
{"page": 129, "image_path": "doc_images/2309.17421v2_129.jpg", "ocr_text": "Web Browsing (Today’s News)\n\nPrompt:\nImagine that you are a robot operating a computer. Like how humans operate the computer, you can move the\n\nmouse, click an icon with the mouse, or type some texts with the keyboard.\nYou are asked to find what’s in the news today.\n\nPrevious action: \"My immediate next step would be to move the mouse to the first search result, which is \"State\nDepartment doubles down on Zelensky slams\" located approximately 1/4 of the way down from the top of the\nscreen and 1/8 of the way from the left side of the screen. | would then click on the link to open the news article\nand read about what's in the news today.\"\n\nYou have now completed reading “State Department doubles down on Zelensky slams”. What is your immediate\nnext step? If the action involves moving the mouse, describe the location as detailed as possible.\n\nPOLITICO\n\nne U.. nas aouplea aown tis Week on mutary support or Ukraine 1n 1ts War\nwith Russia, agreeing to include controversial cluster munitions in its next\narms package to Kyiv — a decision that Ukrainian Minister of Defense Oleksiy\nReznikov said would be a “game changer” as the Ukrainian counteroffensive\nstruggles to breach Russia’s front lines.\n\nIna summit communiqué released Tuesday, NATO leaders wrote that the\nalliance could not promise Ukraine membership until the war had ended and\nseveral democratic and security-sector reforms had been achieved.\n\n“We will be in a position to extend an invitation to Ukraine to join the Alliance\nwhen Allies agree and conditions are met,” the leaders wrote.\n\nIna statement posted to Twitter on Tuesday, Zelenskyy took issue with NATO's\nreluctance to commit to admitting his country, and its unwillingness to provide\n‘an anticipated timeline for Ukraine's entry. He wrote that he did not see\nsufficient readiness on the part of NATO to integrate Ukraine into the alliance,\n\n“It's unprecedented and absurd when time frame is not set neither for the\ninvitation nor for Ukraine's membership,” Zelenskyy wrote. “While at the same\n‘time vague wording about ‘conditions’ is added even for inviting Ukraine.”\n\nMiller pointed to reforms around “anticorruption” and “strengthening\ndemocracy” that were necessary for Ukraine's entry to NATO but declined to\ndetail what the specific conditions were in full. During his Tuesday press\nconference, Miller said that members of NATO stood ready to help Ukraine\nmeet the necessary conditions for its acceptance to the alliance.\n\n“One of the things Russia said before this war began is that they were firmly\n‘opposed to Ukraine joining NATO, and they wanted an ironclad commitment\nthat Ukraine would never join NATO,” Miller said. “And we made clear that\nwas not on the table, that we maintained NATO's open-door policy. We\nreiterate that commitment today, and we made clear today that Ukraine will\nbecome a member of NATO.”\n\nGPT-4V:\n\nMy immediate next step would be to\n*X\"icon to close the current tab, | would then and return to the previous\n\npage to continue browsing for more news articles.\n\nFigure 100: GPT-4V navigates through GUI to browse the web to read today’s news. Upon finishing\nreading the first news article, GPT-4V predicts to close the tab and return to previous page to continue\n\nbrowsing more news articles (highlighted in blue ). ‘Red highlights the inaccurate action prediction.\nCheck Section 9.8 for detailed discussions.\n\n130\n", "vlm_text": "Sec. 9.8 GUI Navigation \nWeb Browsing (Today’s News) \nPrompt: \nImagine that you are a robot operating a computer. Like how humans operate the computer, you can move the  mouse, click an icon with the mouse, or type some texts with the keyboard.  \nYou are asked to find what’s in the news today. \nPrevious action: \"My immediate next step would be to move the mouse to the first search result, which is \"State  Department doubles down on Zelensky slams\" located approximately 1/4 of the way down from the top of the  screen and 1/8 of the way from the left side of the screen. I would then click on the link to open the news article  and read about what's in the news today.\" \nYou have now completed reading “State Department doubles down on Zelensky slams”. What is your immediate  next step? If the action involves moving the mouse, describe the location as detailed as possible. \nThis image shows a Politico article discussing Ukraine's potential membership in NATO. It mentions the U.S. doubling down on military support for Ukraine, including sending cluster munitions. NATO leaders have stated conditions under which Ukraine might join the alliance, emphasizing the need for ending the war and achieving reforms. Ukrainian President Zelenskyy criticized NATO's reluctance on Twitter, seeking a clearer timeline. The article also mentions reforms needed for Ukraine's entry into NATO and reiterates NATO's open-door policy for Ukraine.\nGPT-4V: \nMy immediate next step would be to move the mouse to the top right corner of the screen, where I can see the  \"X\" icon to close the current tab. I would then click on the \"X\" icon to close the tab and return to the previous  page to continue browsing for more news articles.  \nFigure 100: GPT-4V navigates through GUI to browse the web to read today’s news. Upon finishing reading the first news article, GPT-4V predicts to close the tab and return to previous page to continue browsing more news articles (highlighted in  blue  ).  Red  highlights the inaccurate action prediction. Check Section  9.8  for detailed discussions. "}
{"page": 130, "image_path": "doc_images/2309.17421v2_130.jpg", "ocr_text": "Sec. 9.8 GUI Navigation\n\nBrowsing the Web (Today’s News)\n\nPrompt:\n\nImagine that you are a robot operating a computer. Like how humans operate the computer, you can move the\nmouse, click an icon with the mouse, or type some texts with the keyboard.\n\nYou are asked to find what's in the news today.\n\nPrevious action: \"My immediate next step would be to move the mouse to the top right corner of the screen,\nwhere | can see the \"X\" icon to close the current tab. | would then click on the \"X\" icon to close the tab and\nreturn to the previous page to continue browsing for more news articles. \"\n\nYou have now returned to the previous page after reading the first article “State Department doubles down on\nZelensky slams”. Below is what you see on the computer screen. Based on the image below, what is your\nimmediate next step? If the action involves moving the mouse, describe the location as detailed as possible.\n\nGoogle news\n\nNews Videos Images Books | Shopping Maps Flights _—_ Finance\n\nAbout 25,270,000,000 results (0.46 seconds)\n\nTop stories\n\nGPT-4V:\n\nPolitico\n\nState Department doubles\ndown after Zelenskyy slams\nreluctance ...\n\n6 hours ago\n\n© Reuters\n\nUS says it no longer deems\nDonald Trump immune from\nE. Jean...\n\n2 hours ago\n\n€ The New York Times\n\nHard Right Presses Culture\nWar Fights on Defense Bill,\nImperiling ...\n\n2 hours ago\n\nEA cnn\n\nCatastrophic flooding\nswamped Vermont's capital\nas intense storms forced...\n\n1 hour ago\n\n& Dcist\nD.C. Council Approves\nEmergency Crime Bill\n\n5 hours ago\n\n© NPR\nThe Anthropocene began in\n1950s, scientists say\n\n3 hours ago\n\nMy immediate next step would be to move the mouse to the second article on the page, \"Catastrophic flooding\nswamps Florida's panhandle as intense storms force evacuations\". | would move the mouse to the left side of\nthe article title, approximately 1/3 of the way down the page, and click on the article title to open it and read the\nnews.\n\nFigure 101: GPT-4V navigates through GUI to browse the web to read today’s news. Blue highlights\nthe predicted actions. Check Section 9.8 for detailed discussions.\n\n131\n", "vlm_text": "Sec. 9.8 GUI Navigation \nBrowsing the Web (Today’s News) \nPrompt: \nImagine that you are a robot operating a computer. Like how humans operate the computer, you can move the  mouse, click an icon with the mouse, or type some texts with the keyboard.  \nYou are asked to find what’s in the news today. \nPrevious action: \"My immediate next step would be to move the mouse to the top right corner of the screen,  where I can see the \"X\" icon to close the current tab. I would then click on the \"X\" icon to close the tab and  return to the previous page to continue browsing for more news articles. \" \nYou have now returned to the previous page after reading the first article “State Department doubles down on  Zelensky slams”. Below is what you see on the computer screen. Based on the image below, what is your  immediate next step? If the action involves moving the mouse, describe the location as detailed as possible. \nThe image shows a Google search result page for the term \"news.\" The top stories section includes several news articles:\n\n1. **Politico** - \"State Department doubles down after Zelenskyy slams reluctance...\" (6 hours ago)\n2. **CNN** - \"Catastrophic flooding swamped Vermont’s capital as intense storms forced...\" (1 hour ago)\n3. **Reuters** - \"US says it no longer deems Donald Trump immune from E. Jean...\" (2 hours ago)\n4. **DCist** - \"D.C. Council Approves Emergency Crime Bill\" (5 hours ago)\n5. **The New York Times** - \"Hard Right Presses Culture War Fights on Defense Bill, Imperiling...\" (2 hours ago)\n6. **NPR** - \"The Anthropocene began in 1950s, scientists say\" (3 hours ago)\n\nEach news article is accompanied by an image to the right of its title.\nGPT-4V: \nMy immediate next step would be to move the mouse to the second article on the page, \"Catastrophic flooding  swamps Florida's panhandle as intense storms force evacuations\". I would move the mouse to the left side of  the article title, approximately 1/3 of the way down the page, and click on the article title to open it and read the  news.  \nFigure 101: GPT-4V navigates through GUI to browse the web to read today’s news.  Blue  highlights the predicted actions. Check Section  9.8  for detailed discussions. "}
{"page": 131, "image_path": "doc_images/2309.17421v2_131.jpg", "ocr_text": "Sec. 9.8 GUI Navigation\nWeb Browsing (Today’s News)\n\nPrompt:\nRead the article shown in the images below and summarize it.\nCatastrophic flooding swamped Vermont's capital as\n\nintense storms forced evacuations and closures in\nNortheast\n\nbegnang to cede and was not xpéctd to beach the spay. 00\nveanesday crows wi bg anny mud and Gers rom he cys ade\n\nstron cmon ftom haat etl the cy oe he wen\n\nottour tat was isting nthe area and onteres the watt\n\nTec downtown area\n\nrecor pang won ne opped 5:27 ches of rain on he sate apt on Ay ee ay aN\n\nAust 2, 2011 ‘One resident saw water ‘rising quickly’ Leng Lak fll to sid non essert! wael the ton not towed a\n‘Ate ran ose in Veen, Scot catoned again alee sense of ‘ime —\ncomplacency. “Te water ha fo go someplace. he reser ae ing up nd\n\nWater rushing into homes.\n\nmor rin expected inthe coming days\n\nWant stay updated onthe atest\nspace and sconce news?\n\n“Teer to thesening loons that we re yg to enty a rescue”\n\nGPT-4V:\n\nThe article discusses the catastrophic flooding that occurred in Vermont's capital, Montpelier, on Tuesday, July\n11, 2023. The flooding was caused by intense rainstorms that inundated the Northeast, leading to evacuations\nand closures in the area.\n\nThe floods were described as \"unprecedented\" and \"historic\" by officials, with some areas experiencing\nrecord-breaking rainfall. The National Weather Service issued a travel advisory, warning residents to avoid\nnon-essential travel.\n\nVermont Governor Phil Scott declared a state of emergency, and New Hampshire sent rescue crews and Black\nHawk helicopters to assist with the response. The Vermont Fire Department and other emergency services were\nalso dispatched to the affected areas.\n\nThe floods caused extensive damage to homes and businesses, with some residents being forced to evacuate.\nA boil water notice was issued for Montpelier, and officials began clearing debris from roadways on Wednesday.\n\nIn Oklahoma City, two children were rescued from a reservoir after being swept away by floodwaters. The\nchildren, aged 10 and 11, were among a group of four that were fishing in the area.\n\nIn New York, Hurricane Irene caused 8 inches of rain within a 24-hour period, leading to major infrastructure\ndamage. A state of emergency was declared for Long Lake Town Supervisor Clay Arsenault, and residents were\nurged to evacuate.\n\nThe article concludes with a quote from Dan Hancock, a resident of Chester, Vermont, who watched floodwater\n\nenter his basement. He expresses his frustration and helplessness, stating \"Once the water goes down, we go\nback in, clean it up and move on. What can we do?\"\n\nFigure 102: GPT-4V navigates through GUI to browse the web to read today’s news. We prompt\nGPT-4V to read the screenshots of the second news article and summarize it. Check Section 9.8 for\n\ndetailed discussions.\n132\n", "vlm_text": "Web Browsing (Today’s News) \nPrompt: \nThe article discusses severe flooding in Vermont and other parts of the Northeast, likening its impact to events like Hurricane Irene. The high rainfall levels, at 300-500% above normal, caused significant flooding across cities, including Montpelier, where emergency operations were activated. Roads and bridges were flooded, and rescue efforts were underway with aid from nearby states. Vermont's governor emphasized the situation's severity, urging caution as water levels were still rising in some areas. The state's emergency services, including swift water rescue teams, were actively responding, with additional federal support being coordinated. The floods disrupted travel and led to evacuations, with officials warning of continuing risks.\nThe article discusses the catastrophic flooding that occurred in Vermont's capital, Montpelier, on Tuesday, July  11, 2023. The flooding was caused by intense rainstorms that inundated the Northeast, leading to evacuations  and closures in the area. \nThe floods were described as \"unprecedented\" and \"historic\" by officials, with some areas experiencing  record-breaking rainfall. The National Weather Service issued a travel advisory, warning residents to avoid  non-essential travel. \nVermont Governor Phil Scott declared a state of emergency, and New Hampshire sent rescue crews and Black  Hawk helicopters to assist with the response. The Vermont Fire Department and other emergency services were  also dispatched to the affected areas. \nThe floods caused extensive damage to homes and businesses, with some residents being forced to evacuate.  A boil water notice was issued for Montpelier, and officials began clearing debris from roadways on Wednesday. In Oklahoma City, two children were rescued from a reservoir after being swept away by floodwaters. The  children, aged 10 and 11, were among a group of four that were fishing in the area. \nIn New York, Hurricane Irene caused 8 inches of rain within a 24-hour period, leading to major infrastructure  damage. A state of emergency was declared for Long Lake Town Supervisor Clay Arsenault, and residents were  urged to evacuate. \nThe article concludes with a quote from Dan Hancock, a resident of Chester, Vermont, who watched floodwater  enter his basement. He expresses his frustration and helplessness, stating \"Once the water goes down, we go  back in, clean it up and move on. What can we do?\"                                             \nFigure 102: GPT-4V navigates through GUI to browse the web to read today’s news. We prompt GPT-4V to read the screenshots of the second news article and summarize it. Check Section  9.8  for detailed discussions. "}
{"page": 132, "image_path": "doc_images/2309.17421v2_132.jpg", "ocr_text": "Sec. 9.8 GUI Navigation\n\nOnline Shopping (Ergonomic Keyboard)\nPrompt:\n\nImagine that you are a robot operating a phone. Like how humans operate the phone, you can move your finger\nto an icon, click an icon with your finger, scroll down a screen, or type some texts with the keyboard.\n\nYou are asked to shop for an ergonomic keyboard, your budget is between $50 and $100.\n\nBelow is what you see on the phone screen, predict your next move. If the action involves moving your finger to\nan icon, describe the location of that icon as detailed as possible.\n\n©) @L)\n\nCalendar Settings Clock Calculator\n\nWallet Voice Memos Camera Compass\n\na\n\nWeather App Store YouTube Amazon\n\ny ee\n\nTwitter Facebook\n\nQ Search\n\nO00»\nGPT-4V:\n\nMy next move would be to move my finger to the Amazon icon, which is located in the third row and fhird column:\nof the screen. | would then click on the icon to open the app and search for an ergonomic keyboard within my\nbudget between $50 and $100.\n\nFigure 103: GPT-4V navigates through GUI to shop for an ergonomic keyboard online. Blue\n\nhighlights the predicted actions. ‘Red highlights the inaccurate location of the Amazon icon. Check\nSection 9.8 for detailed discussions.\n\n133\n", "vlm_text": "Sec. 9.8 GUI Navigation \nOnline Shopping (Ergonomic Keyboard) \nPrompt: \nImagine that you are a robot operating a phone. Like how humans operate the phone, you can move your finger  to an icon, click an icon with your finger, scroll down a screen, or type some texts with the keyboard.  \nYou are asked to shop for an ergonomic keyboard, your budget is between  $\\S50$   and  $\\S100$  . \nBelow is what you see on the phone  screen, predict your next move. If the action involves moving your finger to  an icon, describe the location of that icon as detailed as possible. \nThe image shows a smartphone home screen with various app icons. These include:\n\n- Calendar\n- Settings\n- Clock\n- Calculator\n- Wallet\n- Voice Memos\n- Camera\n- Compass\n- Weather\n- App Store\n- YouTube\n- Amazon\n- Twitter\n- Instagram\n- Chase\n- Facebook\n- Yelp\n\nThere are also icons in the dock at the bottom:\n\n- Phone\n- Photos\n- Safari\n- Mail\n\nThe time is displayed as 1:56, and the battery percentage is 85%.\nGPT-4V: \nMy next move would be to move my finger to the Amazon icon, which is located in the third row and third column  of the screen. I would then click on the icon to open the app and search for an ergonomic keyboard within my  budget between  $\\S50$   and  $\\S100$  .  \nFigure 103: GPT-4V navigates through GUI to shop for an ergonomic keyboard online. Blue highlights the predicted actions.  Red  highlights the inaccurate location of the Amazon icon. Check Section  9.8  for detailed discussions. "}
{"page": 133, "image_path": "doc_images/2309.17421v2_133.jpg", "ocr_text": "Sec. 9.8 GUI Navigation\nOnline Shopping (Ergonomic Keyboard)\n\nPrompt:\nImagine that you are a robot operating a phone. Like how humans operate the phone, you can move your finger\nto an icon, click an icon with your finger, scroll down a screen, or type some texts with the keyboard.\n\nYou are asked to shop for an ergonomic keyboard, your budget is between $50 and $100.\n\nPrevious action: \"My next move would be to move my finger to the Amazon icon, which is located in the third row\nand third column of the screen. | would then click on the icon to open the app and search for an ergonomic\nkeyboard within my budget of $50.\"\n\nYou have now opened the Amazon app. Below is what you see on the phone screen. What will you do next? If\nthe action involves moving your finger to an icon, describe the location of that icon as detailed as possible.\n\nSearch Amazon.com\n\n=| Shop school\n\nsupplies for less\n\nInspired by your Selected for you Buy it again\n\nMIGHTY\nPATCH’\n\nCrest Pro Health Advanced Multi-\nProtection Mouthwash, Alcohol Free, Ext...\n*27.16 vprime\n\nRecommended deal for you\n\nGPT-4V:\n\nMy next move would be to move my finger to the search bar, which is located at the top of the screen. | would\nthen click on the search bar and type in \"ergonomic keyboard\" using the keyboard. After typing in the search\nterm, | would click on the \"Search\" button, which is located on the bottom right corner of the keyboard, to initiate\nthe search.\n\nFigure 104: GPT-4V navigates through GUI to shop for an ergonomic keyboard online. Blue\nhighlights the predicted actions. Check Section 9.8 for detailed discussions.\n\n134\n", "vlm_text": "Sec. 9.8 GUI Navigation \nOnline Shopping (Ergonomic Keyboard) \nPrompt: \nImagine that you are a robot operating a phone. Like how humans operate the phone, you can move your finger  to an icon, click an icon with your finger, scroll down a screen, or type some texts with the keyboard.  \nYou are asked to shop for an ergonomic keyboard, your budget is between  $\\S50$   and  $\\S100$  . \nPrevious action: \"My next move would be to move my finger to the Amazon icon, which is located in the third row  and third column of the screen. I would then click on the icon to open the app and search for an ergonomic  keyboard within my budget of  $\\S50$  . \" \nYou have now opened the Amazon app. Below is what you see on the phone screen. What will you do next? If  the action involves moving your finger to an icon, describe the location of that icon as detailed as possible. \nThe image shows a screenshot of the Amazon app on a mobile device. At the top, there is a search bar, and options for \"Groceries,\" \"Pharmacy,\" \"In-Store Code,\" and \"One Medical.\" Below that, there is a notification for delivery with a placeholder name \"Lin Jie.\" \n\nThere is a colorful banner with back-to-school supplies and the text \"Shop school supplies for less.\" \n\nBelow the banner, there are personalized product recommendations:\n- \"Inspired by your Wish List\" featuring ceramic ramekins.\n- \"Selected for you\" showing a plush toy pig.\n- \"Buy it again\" with Hero Mighty Patch for acne.\n\nThere's also a suggested product for Crest Pro Health Advanced Multi-Protection Mouthwash with a price of $27.16 and an \"Add to Cart\" button. \n\nThe bottom toolbar includes icons for the home page, user account, cart, and menu.\nGPT-4V: \nMy next move would be to move my finger to the search bar, which is located at the top of the screen. I would  then click on the search bar and type in \"ergonomic keyboard\" using the keyboard. After typing in the search  term, I would click on the \"Search\" button, which is located on the bottom right corner of the keyboard, to initiate  the search.  \nFigure 104: GPT-4V navigates through GUI to shop for an ergonomic keyboard online. Blue highlights the predicted actions. Check Section  9.8  for detailed discussions. "}
{"page": 134, "image_path": "doc_images/2309.17421v2_134.jpg", "ocr_text": "Sec. 9.8 GUI Navigation\n\nOnline Shopping (Ergonomic Keyboard)\n\nPrompt:\nImagine that you are a robot operating a phone. Like how humans operate the phone, you can move your finger\nto an icon, click an icon with your finger, scroll down a screen, or type some texts with the keyboard.\n\nYou are asked to shop for an ergonomic keyboard, your budget is between $50 and $100.\n\nPrevious action: \"My next move would be to move my finger to the search bar, which is located at the top of the\nscreen. | would then click on the search bar and type in \"ergonomic keyboard\" using the keyboard. After typing\nin the search term, | would click on the \"Search\" button, which is located on the bottom right corner of the\nkeyboard, to initiate the search. \"\n\nYou have now completed the previous action as described above. Below is what you see on the phone screen.\nPlease predict your immediate next move. If the action involves moving your finger to an icon, describe the\nlocation of that icon as detailed as possible.\n\nQ ergonomic keyboard\n© Deliver to Linjie -\nvprime Yet &Up = Mac ~—- Windows Filters ¥\n\nExplore Related Brands\n\nSponsored @\n\naS\n\ntet tte te 11,057\nvprime\n\nShop the VIVO Store >\n\nSponsored\nX9 Performance Ergonomic\nKeyboard Wireless - Your Comfort\nMatters - Full Size Rechargeable...\nLaptop, PC\n\n3 teste she te tz (495)\ne\n\n8 os\n\nGPT-4V:\n\nMy immediate next move would be to move my finger to the \"Filters\" button, which is located at the top right\ncorner of the screen, next to the \"Windows\" button. | would then click on the \"Filters\" button to open the filter\noptions and narrow down my search results to only show ergonomic keyboards within my budget range of $50 to\n$100.\n\nFigure 105: GPT-4V navigates through GUI to shop for an ergonomic keyboard online. Blue\nhighlights the predicted actions. Check Section 9.8 for detailed discussions.\n\n135\n", "vlm_text": "Sec. 9.8 GUI Navigation \nOnline Shopping (Ergonomic Keyboard) \nPrompt: \nImagine that you are a robot operating a phone. Like how humans operate the phone, you can move your finger  to an icon, click an icon with your finger, scroll down a screen, or type some texts with the keyboard.  \nYou are asked to shop for an ergonomic keyboard, your budget is between  $\\S50$   and  $\\S100$  . \nPrevious action: \"My next move would be to move my finger to the search bar, which is located at the top of the  screen. I would then click on the search bar and type in \"ergonomic keyboard\" using the keyboard. After typing  in the search term, I would click on the \"Search\" button, which is located on the bottom right corner of the  keyboard, to initiate the search. \" \nYou have now completed the previous action as described above. Below is what you see on the phone screen.  Please predict your immediate next move. If the action involves moving your finger to an icon, describe the  location of that icon as detailed as possible. \nThe image appears to be a screenshot of an Amazon shopping app interface on a mobile device. The search query \"ergonomic keyboard\" is visible at the top. The screen displays a section titled \"Explore Related Brands,\" showing logos of brands like VIVO, Perixx, Kinesis Gaming, and Macally. Below that, there's an image of a person typing on a keyboard with a pull-out tray. There are also product listings with ratings. The bottom section of the screen shows the app navigation menu with icons for home, categories, cart, and account.\nGPT-4V: \nMy immediate next move would be to move my finger to the \"Filters\" button, which is located at the top right  corner of the screen, next to the \"Windows\" button. I would then click on the \"Filters\" button to open the filter  options and narrow down my search results to only show ergonomic keyboards within my budget range of  $\\S50$   to   $\\S100$  .  \nFigure 105: GPT-4V navigates through GUI to shop for an ergonomic keyboard online. Blue highlights the predicted actions. Check Section  9.8  for detailed discussions. "}
{"page": 135, "image_path": "doc_images/2309.17421v2_135.jpg", "ocr_text": "Sec. 9.8 GUI Navigation\nOnline Shopping (Ergonomic Keyboard)\n\nPrompt:\nImagine that you are a robot operating a phone. Like how humans operate the phone, you can move your finger\nto an icon, click an icon with your finger, scroll down a screen, or type some texts with the keyboard.\n\nYou are asked to shop for an ergonomic keyboard, your budget is between $50 and $100.\n\nPrevious action: \"My immediate next move would be to move my finger to the \"Filters\" button, which is located at\nthe top right corner of the screen, next to the \"Windows\" button. | would then click on the \"Filters\" button to open\nthe filter options and narrow down my search results to only show ergonomic keyboards within my budget range\nof $50 to $100.\"\n\nYou have now completed the previous action as described above. Below is what you see on the phone screen.\nPlease predict your immediate next move. If the action involves moving your finger to an icon, describe the\nlocation of that icon as detailed as possible.\n\nQ ergonomic keyboard\nvprime( J) Weyiewievr &Up Mac Windows Filters «\n\nFilters Close\n\nPrime & Prime & Delivery\nDelivery\nDelivery\n\nCategories Get It by Tomorrow Overnight by 11AM\nShipping Options\n\nFrustration-Free Packaging\n‘Amazon Global Store\nPrice & Deals International Shipping Eligible\n\nGeemme Prime Programs\n\nReviews\nAll Prime\n\nSort by\n\nMore-\nsustainable\nProducts\n\nKeyboard\nSpecial\nFeatures\n\nKeyboard\nConnectivity\nTechnology\n\nIncluded\n\nGPT-4V:\n\nMy immediate next move would be to move my finger to the \"Price & Deals\" filter option, which is located under\nthe \"Colors\" filter option and above the \"Sort by\" filter option. | would then click on the \"Price & Deals\" filter\noption to open the price range filter and set my minimum price to $50 and my maximum price to $100. This\nwould further narrow down my search results to only show ergonomic keyboards within my budget range.\n\nFigure 106: GPT-4V navigates through GUI to shop for an ergonomic keyboard online. Blue\nhighlights the predicted actions. Check Section 9.8 for detailed discussions.\n\n136\n", "vlm_text": "Sec. 9.8 GUI Navigation \nOnline Shopping (Ergonomic Keyboard) \nPrompt: \nImagine that you are a robot operating a phone. Like how humans operate the phone, you can move your finger  to an icon, click an icon with your finger, scroll down a screen, or type some texts with the keyboard.  \nYou are asked to shop for an ergonomic keyboard, your budget is between  $\\S50$   and  $\\S100$  . \nPrevious action: \"My immediate next move would be to move my finger to the \"Filters\" button, which is located at  the top right corner of the screen, next to the \"Windows\" button. I would then click on the \"Filters\" button to open  the filter options and narrow down my search results to only show ergonomic keyboards within my budget range  of  $\\S50$   to  $\\S\\,100$  . \" \nYou have now completed the previous action as described above. Below is what you see on the phone screen.  Please predict your immediate next move. If the action involves moving your finger to an icon, describe the  location of that icon as detailed as possible. \nThe image displays a screenshot from a mobile device showing an Amazon search page for \"ergonomic keyboard.\" The page includes filter options for refining search results. On the top of the screen, the search term \"ergonomic keyboard\" is entered in the search bar, with some toggle options below, such as Prime, ratings, and specific operating systems (Mac, Windows). The main portion of the image shows the \"Filters\" menu open, specifically the \"Prime & Delivery\" options. Available delivery options include \"Get it by Tomorrow\" and \"Overnight by 11AM.\" There are also shipping options like \"Frustration-Free Packaging\" and \"Amazon Global Store.\" Other navigation categories can be seen on the left side bar, such as Categories, Brands, Colors, Price & Deals, and more. At the bottom, there is a button to show more than 10,000 results. The device information bar at the top shows it's 4:42, with a WiFi symbol and battery level of 79%.\nGPT-4V: \nMy immediate next move would be to move my finger to the \"Price & Deals\" filter option, which is located under  the \"Colors\" filter option and above the \"Sort by\" filter option. I would then click on the \"Price & Deals\" filter  option to open the price range filter and set my minimum price to  $\\S50$   and my maximum price to  $\\S100$  . This  would further narrow down my search results to only show ergonomic keyboards within my budget range.  \nFigure 106: GPT-4V navigates through GUI to shop for an ergonomic keyboard online. Blue highlights the predicted actions. Check Section  9.8  for detailed discussions. "}
{"page": 136, "image_path": "doc_images/2309.17421v2_136.jpg", "ocr_text": "Sec. 9.8 GUI Navigation\nOnline Shopping (Ergonomic Keyboard)\n\nPrompt:\nImagine that you are a robot operating a phone. Like how humans operate the phone, you can move your finger\nto an icon, click an icon with your finger, scroll down a screen, or type some texts with the keyboard.\n\nYou are asked to shop for an ergonomic keyboard, your budget is between $50 and $100.\n\nPrevious action: \" My immediate next move would be to move my finger to the \"Price & Deals\" filter option, which\nis located under the \"Colors\" filter option and above the \"Sort by\" filter option. | would then click on the \"Price &\nDeals\" filter option to open the price range filter and set my minimum price to $50 and my maximum price to\n$100. This would further narrow down my search results to only show ergonomic keyboards within my budget\nrange.\"\n\nYou have now completed the previous action as described above. Below is what you see on the phone screen.\nWhat will you do next? If the action involves moving your finger to an icon, describe the location of that icon as\ndetailed as possible.\n\nQ ergonomic keyboard\nvprime (_) $50to$100 Computer Keyb.. Filters (2) «\nFilters (2) Close\n\nPrime & Price & Deals\nDelivery\nPrice\n\nCategories AllPrices Under $25 $25 to $50\n$50to $100 X — $100 to $200\n$200 & Above\n\nDeals & Discounts\n\nPrice & Deals 1 All Discounts\n\nCustomer\nReviews\n\nSort by\n\nConnectivity\nTechnology\n\nIncluded\n‘Components\n\nClear Filters\n\nGPT-4V:\n\nMy next move would be to move my finger to the \"Show 1,987 results\" button, which is located at the bottom of\nthe screen, below the \"Clear Filters\" button and above the navigation bar. | would then click on the \"Show 1,987\nresults\" button to view the search results for ergonomic keyboards within my budget range.\n\nFigure 107: GPT-4V navigates through GUI to shop for an ergonomic keyboard online. Blue\nhighlights the predicted actions. Check Section 9.8 for detailed discussions.\n\n137\n", "vlm_text": "Sec. 9.8 GUI Navigation \nOnline Shopping (Ergonomic Keyboard) \nPrompt: \nImagine that you are a robot operating a phone. Like how humans operate the phone, you can move your finger  to an icon, click an icon with your finger, scroll down a screen, or type some texts with the keyboard.  \nYou are asked to shop for an ergonomic keyboard, your budget is between  $\\S50$   and  $\\S100$  . \nPrevious action: \" My immediate next move would be to move my finger to the \"Price & Deals\" filter option, which  is located under the \"Colors\" filter option and above the \"Sort by\" filter option. I would then click on the \"Price &  Deals\" filter option to open the price range filter and set my minimum price to  $\\S50$   and my maximum price to   $\\S\\,100$  . This would further narrow down my search results to only show ergonomic keyboards within my budget  range. \" \nYou have now completed the previous action as described above. Below is what you see on the phone screen.  What will you do next? If the action involves moving your finger to an icon, describe the location of that icon as  detailed as possible. \nThe image shows a mobile shopping app screen with filters applied for searching \"ergonomic keyboard.\" \n\n- The filters include:\n  - Price range: $50 to $100\n  - Prime delivery option activated\n  - Category filter: Computer Keyboards\n\n- Additional filter options visible include:\n  - Customer Reviews\n  - Sort by\n  - More-sustainable Products\n  - Special Features\n  - Connectivity Technology\n  - Included Components\n\nAt the bottom, there is an option to clear filters and a button showing 1,987 results available.\nGPT-4V: \nMy next move would be to move my finger to the \"Show 1,987 results\" button, which is located at the bottom of  the screen, below the \"Clear Filters\" button and above the navigation bar. I would then click on the \"Show 1,987  results\" button to view the search results for ergonomic keyboards within my budget range.  \nFigure 107: GPT-4V navigates through GUI to shop for an ergonomic keyboard online. Blue highlights the predicted actions. Check Section  9.8  for detailed discussions. "}
{"page": 137, "image_path": "doc_images/2309.17421v2_137.jpg", "ocr_text": "Sec. 9.8 GUI Navigation\n\nOnline Shopping (Ergonomic Keyboard)\n\nPrompt:\nImagine that you are a robot operating a phone. Like how humans operate the phone, you can move your finger\n\nto an icon, click an icon with your finger, scroll down a screen, or type some texts with the keyboard.\nYou are asked to shop for an ergonomic keyboard, your budget is between $50 and $100.\n\nPrevious action: \" My next move would be to move my finger to the \"Show 1,987 results\" button, which is located\nat the bottom of the screen, below the \"Clear Filters\" button and above the navigation bar. | would then click on\nthe \"Show 1,987 results\" button to view the search results for ergonomic keyboards within my budget range. \"\n\nYou have now completed the previous action as described above. Below is what you see on the phone screen.\nWhat will you do next? If the action involves moving your finger to an icon, describe the location of that icon as\ndetailed as possible.\n\nQ ergonomic keyboard\n© Deliver to Linjie -\nvprime $50to$100 © Computer Keyb.. Filters (2) »\n\nExplore Related Brands\n\nSponsored @\n\nQymacauy\n\nde tete tet 11,057\nvprime\n\nShop the VIVO Store >\n\nKensington Pro Fit Ergonomic\nWireless Keyboard - Black\n(k75401US)\n\nChrome OS, macOS, Windows\n\n3.8 We te tie teyy (1,508)\nAnno\n\nGPT-4V:\n\nMy next move would be to scroll down the screen to view the different options for ergonomic keyboards within\nmy budget range. | would then move my finger to the \"Kensington Pro Fit Ergonomic Wireless Keyboard - Black\"\noption, which is located below the \"Explore Related Brands\" section and above the \"VIVO\" option | would then\nclick on the \"Kensington Pro Fit Ergonomic Wireless Keyboard - Black\" option to view more details about the\nproduct and potentially make a purchase.\n\nFigure 108: GPT-4V navigates through GUI to shop for an ergonomic keyboard online. Blue\n\nhighlights the predicted actions. Red highlights the inaccurate location of the product option to be\nselected. Check Section 9.8 for detailed discussions.\n\n138\n", "vlm_text": "Sec. 9.8 GUI Navigation \nOnline Shopping (Ergonomic Keyboard) \nPrompt: \nImagine that you are a robot operating a phone. Like how humans operate the phone, you can move your finger  to an icon, click an icon with your finger, scroll down a screen, or type some texts with the keyboard.  \nYou are asked to shop for an ergonomic keyboard, your budget is between  $\\S50$   and  $\\S100$  . \nPrevious action: \" My next move would be to move my finger to the \"Show 1,987 results\" button, which is located  at the bottom of the screen, below the \"Clear Filters\" button and above the navigation bar. I would then click on  the \"Show 1,987 results\" button to view the search results for ergonomic keyboards within my budget range. \" \nYou have now completed the previous action as described above. Below is what you see on the phone screen.  What will you do next? If the action involves moving your finger to an icon, describe the location of that icon as  detailed as possible. \nThe image is a screenshot of an Amazon shopping app interface, showing search results for \"ergonomic keyboard.\" At the top, there is a search bar that contains the query. Delivery information is indicated next to the search bar, with a partial delivery location of \"Lin Jie.\"\n\nBelow are filtering options including \"Prime,\" price range \"$50 to $100,\" and \"Computer Keyboards.\" There is a section called \"Explore Related Brands\" with brand logos like VI:VO, Perixx, Kinesis Gaming, and Macally.\n\nAn image shows a person typing on a keyboard placed on an ergonomic desk attachment. A small product listing below this image highlights a \"Best seller\" item with 11,057 reviews, showing high ratings and a Prime logo. There is also an ergonomic desk device next to it.\n\nAnother product listing at the bottom shows a \"Kensington Pro Fit Ergonomic Wireless Keyboard\" with pricing at $40.99, reviews, Prime availability, and compatibility details (Chrome OS, macOS, Windows).\nGPT-4V: \nMy next move would be to scroll down the screen to view the different options for ergonomic keyboards within  my budget range. I would then move my finger to the \"Kensington Pro Fit Ergonomic Wireless Keyboard - Black\"  option, which is located below the \"Explore Related Brands\" section and above the \"VIVO\" option. I would then  click on the \"Kensington Pro Fit Ergonomic Wireless Keyboard - Black\" option to view more details about the  product and potentially make a purchase.  \nFigure 108: GPT-4V navigates through GUI to shop for an ergonomic keyboard online. Blue highlights the predicted actions. Red highlights the inaccurate location of the product option to be selected. Check Section  9.8  for detailed discussions. "}
{"page": 138, "image_path": "doc_images/2309.17421v2_138.jpg", "ocr_text": "Sec. 9.8 GUI Navigation\nOnline Shopping (Ergonomic Keyboard)\n\nPrompt:\nImagine that you are a robot operating a phone. Like how humans operate the phone, you can move your finger\nto an icon, click an icon with your finger, scroll down a screen, or type some texts with the keyboard.\n\nYou are asked to shop for an ergonomic keyboard, your budget is between $50 and $100.\n\nPrevious action: \" My next move would be to scroll down the screen to view the different options for ergonomic\nkeyboards within my budget range. | would then move my finger to the \"Kensington Pro Fit Ergonomic Wireless\nKeyboard - Black\" option, which is located below the \"Explore Related Brands\" section and above the \"VIVO\"\noption. | would then click on the \"Kensington Pro Fit Ergonomic Wireless Keyboard - Black\" option to view more\ndetails about the product and potentially make a purchase. \"\n\nYou have now completed the previous action as described above. Below is what you see on the phone screen.\nWhat will you do next? If the action involves moving your finger to an icon, describe the location of that icon as\ndetailed as possible.\n\nQ Search Amazon.com\n\nVisit the Kensington Store 3.8 wetter 1,508\nKensington Pro Fit Ergonomic Wireless Keyboard - Black\n(K75401US)\n\nfu)\n\n9 QA\n\nGet a $8 bonus when you reload $100 or more to your\ngift card balance (Restrictions apply).\n\nBuy New fe)\n-6% *49”\nList Price: $52:99 @\n\nvprime\n\na Q\n\nGPT-4V:\n\nMy next move would be to scroll down the screen to view more details about the \"Kensington Pro Fit Ergonomic\nWireless Keyboard - Black\" product. | would then move my finger to the \"Buy New\" button, which is located\nbelow the product image and above the \"Get a $8 bonus when you reload $100 or more to your gift card\nbalance\" section. | would then click on the \"Buy New\" button to proceed with the purchase.\n\nFigure 109: GPT-4V navigates through GUI to shop for an ergonomic keyboard online. Blue\n\nhighlights the predicted actions. ‘Red highlights the inaccurate action prediction (“Buy New” is not\na clickable button). Check Section 9.8 for detailed discussions.\n\n139\n", "vlm_text": "Sec. 9.8 GUI Navigation \nOnline Shopping (Ergonomic Keyboard) \nPrompt: \nImagine that you are a robot operating a phone. Like how humans operate the phone, you can move your finger  to an icon, click an icon with your finger, scroll down a screen, or type some texts with the keyboard.  \nYou are asked to shop for an ergonomic keyboard, your budget is between  $\\S50$   and  $\\S100$  . \nPrevious action: \" My next move would be to scroll down the screen to view the different options for ergonomic  keyboards within my budget range. I would then move my finger to the \"Kensington Pro Fit Ergonomic Wireless  Keyboard - Black\" option, which is located below the \"Explore Related Brands\" section and above the \"VIVO\"  option. I would then click on the \"Kensington Pro Fit Ergonomic Wireless Keyboard - Black\" option to view more  details about the product and potentially make a purchase. \" \nYou have now completed the previous action as described above. Below is what you see on the phone screen.  What will you do next? If the action involves moving your finger to an icon, describe the location of that icon as  detailed as possible. \nThe image is a screenshot of an Amazon product listing page. The product being displayed is the \"Kensington Pro Fit Ergonomic Wireless Keyboard - Black (K75401US).\" It has a 3.8 out of 5-star rating based on 1,508 reviews. The listing shows a price of $49.99, which is a 6% discount from the original list price of $52.99. Additionally, there's a promotion offering an $8 bonus when you reload $100 or more to your gift card balance. Near the bottom, there are icons for Amazon Prime, search, profile, and shopping cart. The time displayed on the mobile device is 5:39, and the battery level is at 75%.\nGPT-4V: \nMy next move would be to scroll down the screen to view more details about the \"Kensington Pro Fit Ergonomic  Wireless Keyboard - Black\" product. I would then move my finger to the \"Buy New\" button, which is located  below the product image and above the \"Get a  $\\S8$   bonus when you reload  $\\S100$   or more to your gift card  balance\" section. I would then click on the \"Buy New\" button to proceed with the purchase.  \nFigure 109: GPT-4V navigates through GUI to shop for an ergonomic keyboard online. Blue highlights the predicted actions.  Red  highlights the inaccurate action prediction (“Buy New” is not a clickable button). Check Section  9.8  for detailed discussions. "}
{"page": 139, "image_path": "doc_images/2309.17421v2_139.jpg", "ocr_text": "Sec. 9.8 GUI Navigation\n\nOnline Shopping (Ergonomic Keyboard)\n\nPrompt:\nImagine that you are a robot operating a phone. Like how humans operate the phone, you can move your finger\nto an icon, click an icon with your finger, scroll down a screen, or type some texts with the keyboard.\n\nYou are asked to shop for an ergonomic keyboard, your budget is between $50 and $100.\nPrevious action: \" My next move would be to scroll down the screen to view more details about the \"Kensington\nPro Fit Ergonomic Wireless Keyboard - Black\" product. | would then move my finger to the \"Buy New\" button,\n\nwhich is located below the product image and above the \"Get a $8 bonus when you reload $100 or more to your\ngift card balance\" section. | would then click on the \"Buy New\" button to proceed with the purchase. \"\n\nYou have now completed the previous action as described above. Below is what you see on the phone screen.\nWhat will you do next? If the action involves moving your finger to an icon, describe the location of that icon as\ndetailed as possible.\n\nQ Search Amazon.com\n\nvprime\n\nFREE Returns\n\nFREE delivery Thursday, July 20. Order within 6 hrs\n20 mins\n\n© Deliver to Linjie\nIn Stock\n\nQty: 1 v\n\nPayment Secure transaction\n\nShips from Amazon.com\n\nSold by Amazon.com\n\nReturns _ Eligible for Return, Refund or Replacement\nwithin 30 days of receipt\n\nSupport Free Amazon tech support included\n\nPackaging Shows what's inside\n\nAdd a Protection Plan\n\n(\"]  4-Year Protection for $8.99\n\nSave with Used - Like New\n-51% $2598\nFREE delivery: Monday, July 24\n\na 9 & w\n\nGPT-4V:\n\nMy next move would be to move my finger to the \"Add to Cart\" button, which is located below the \"Buy New\"\nbutton and above the \"Ships from Amazon\" section. | would then click on the \"Add to Cart\" button to add the item\nto my shopping cart.\n\nFigure 110: GPT-4V navigates through GUI to shop for an ergonomic keyboard online. Blue\nhighlights the predicted actions. Check Section 9.8 for detailed discussions.\n\n140\n", "vlm_text": "Sec. 9.8 GUI Navigation \nOnline Shopping (Ergonomic Keyboard) \nPrompt: \nImagine that you are a robot operating a phone. Like how humans operate the phone, you can move your finger  to an icon, click an icon with your finger, scroll down a screen, or type some texts with the keyboard.  \nYou are asked to shop for an ergonomic keyboard, your budget is between  $\\S50$   and  $\\S100$  . \nPrevious action: \" My next move would be to scroll down the screen to view more details about the \"Kensington  Pro Fit Ergonomic Wireless Keyboard - Black\" product. I would then move my finger to the \"Buy New\" button,  which is located below the product image and above the \"Get a  $\\S8$   bonus when you reload  $\\S\\,100$   or more to your  gift card balance\" section. I would then click on the \"Buy New\" button to proceed with the purchase. \" \nYou have now completed the previous action as described above. Below is what you see on the phone screen.  What will you do next? If the action involves moving your finger to an icon, describe the location of that icon as  detailed as possible. \nThis image is a screenshot of an Amazon product page on a mobile device. It shows options to add an item to the cart or buy it now. The item is in stock, and free delivery is available by Thursday, July 20, if ordered within a specific time. Return and refund information is included, as well as details about a protection plan for $8.99. There's also an option to buy a \"Used - Like New\" version at a 51% discount, priced at $25.98, with free delivery by Monday, July 24.\nGPT-4V: \nMy next move would be to move my finger to the \"Add to Cart\" button, which is located below the \"Buy New\"  button and above the \"Ships from Amazon\" section. I would then click on the \"Add to Cart\" button to add the item  to my shopping cart.  \nFigure 110: GPT-4V navigates through GUI to shop for an ergonomic keyboard online. Blue highlights the predicted actions. Check Section  9.8  for detailed discussions. "}
{"page": 140, "image_path": "doc_images/2309.17421v2_140.jpg", "ocr_text": "Sec. 9.8 GUI Navigation\n\nOnline Shopping (Ergonomic Keyboard)\n\nPrompt:\nImagine that you are a robot operating a phone. Like how humans operate the phone, you can move your finger\nto an icon, click an icon with your finger, scroll down a screen, or type some texts with the keyboard.\n\nYou are asked to shop for an ergonomic keyboard, your budget is between $50 and $100.\n\nPrevious action: \" My next move would be to move my finger to the \"Add to Cart\" button, which is located below\nthe \"Buy New\" button and above the \"Ships from Amazon\" section. | would then click on the \"Add to Cart\" button\nto add the item to my shopping cart.\"\n\nYou have now completed the previous action as described above. Below is what you see on the phone screen.\nWhat will you do next? If the action involves moving your finger to an icon, describe the location of that icon as\ndetailed as possible.\n\nQ Search Amazon.com\n\n© Deliver to Linjie -\n\nSubtotal $4999\n\nKensington Pro Fit Ergonomic\nWireless Keyboard - Black (K7540...\n\n$4999\nvprime & FREE Returns\nIn Stock\n\n1 in Delete Save for later\n\nCompare with similar items\n\nReturns are easy\n30-day returns on millions of items\n\nAlways the perfect gift.\nShop gift cards\n\nGPT-4V:\n\nMy next move would be to move my finger to the \"Proceed to Checkout\" button, which is located below the\nTSR | oc thon cick on the \"Proceed\n\nto Checkout\" button to proceed with the purchase.\n\nFigure 111: GPT-4V navigates through GUI to shop for an ergonomic keyboard online. Blue\n\nhighlights the predicted actions. Red highlights the inaccurate location of the “Proceed to checkout”\nbuttion. Check Section 9.8 for detailed discussions.\n\n141\n", "vlm_text": "Sec. 9.8 GUI Navigation \nOnline Shopping (Ergonomic Keyboard) \nPrompt: \nImagine that you are a robot operating a phone. Like how humans operate the phone, you can move your finger  to an icon, click an icon with your finger, scroll down a screen, or type some texts with the keyboard.  \nYou are asked to shop for an ergonomic keyboard, your budget is between  $\\S50$   and  $\\S100$  . \nPrevious action: \" My next move would be to move my finger to the \"Add to Cart\" button, which is located below  the \"Buy New\" button and above the \"Ships from Amazon\" section. I would then click on the \"Add to Cart\" button  to add the item to my shopping cart. \" \nYou have now completed the previous action as described above. Below is what you see on the phone screen.  What will you do next? If the action involves moving your finger to an icon, describe the location of that icon as  detailed as possible. \nThe image is a screenshot of an Amazon shopping cart page on a mobile device. The cart contains one item: a Kensington Pro Fit Ergonomic Wireless Keyboard - Black. The price of the keyboard is $49.99, and it is noted to be in stock with free Prime returns. The subtotal amount for the cart is $49.99. There are options to delete the item, save it for later, or compare it with similar items. Additionally, there's a button to proceed to checkout with the single item. The page also promotes easy returns with 30-day returns on millions of items and suggests shopping for gift cards.\nGPT-4V: \nMy next move would be to move my finger to the \"Proceed to Checkout\" button, which is located below the  \"Compare with similar items\" section and above the  $\"30.$  -day returns\" section. I would then click on the \"Proceed  to Checkout\" button to proceed with the purchase.  \nFigure 111: GPT-4V navigates through GUI to shop for an ergonomic keyboard online. Blue highlights the predicted actions.  Red  highlights the inaccurate location of the “Proceed to checkout” buttion. Check Section  9.8  for detailed discussions. "}
{"page": 141, "image_path": "doc_images/2309.17421v2_141.jpg", "ocr_text": "Sec. 9.8 GUI Navigation\nNotification Understanding\n\nPrompt:\nImagine that you are a robot operating a phone. Like how humans operate the phone, you can move your finger\nto an icon, click an icon with your finger, scroll down a screen, or type some texts with the keyboard.\n\nBelow is what you see on the phone screen. Describe the image and predict your next move. If the action\ninvolves moving your finger to an icon, describe the location of that icon as detailed as possible.\n\nCalendar Settings Wallet\n\n~ y\n\nWeather App Store Teams Twitter\n\n=\nyp O\n~w\nCamera YouTube Amazon\n\nCompass Voice Memos _ Instagram\n\nQ Search\n\n“OO.\n\nGPT-4V:\n\nThe image on the phone screen is the home screen of an iPhone. There are multiple app icons arranged ina\ngrid-like pattern, including Calendar, Settings, Twitter, Amazon, Instagram, etc. At the top of the screen, there is\na notification from a user named \"yykhahah\" that reads \"I see you are in Seattle, let's meet up.\" At the bottom of\nthe screen, there is a search bar.\n\nBased on the notification, my next move would be to move my finger to the Maps app icon, which is located in\nand the first column of the app grid. This will allow me to search for a location in Seattle and plan\na meet up with the user.\n\nFigure 112: Prompting GPT-4V to predict the action upon receiving a notification. GPT-4V can\naccurately recognize the notification and the corresponding content (highlighted in green ). Blue\n\nhighlights the predicted actions. ‘Red highlights the inaccurate location of the Maps app icon. Check\nSection 9.8 for detailed discussions.\n\n142\n", "vlm_text": "Sec. 9.8 GUI Navigation \nNotification Understanding \nPrompt: \nImagine that you are a robot operating a phone. Like how humans operate the phone, you can move your finger  to an icon, click an icon with your finger, scroll down a screen, or type some texts with the keyboard.  \nBelow is what you see on the phone screen. Describe the image and predict your next move. If the action  involves moving your finger to an icon, describe the location of that icon as detailed as possible. \nThe image shows a smartphone home screen with various app icons, including:\n\n- Calendar\n- Settings\n- Clock\n- Wallet\n- Weather\n- App Store\n- Teams\n- Twitter\n- Chase\n- Camera\n- YouTube\n- Amazon\n- Maps\n- Compass\n- Voice Memos\n- Instagram (with a notification)\n\nAt the top, there's a notification from Instagram user \"yykhahah\" stating, \"I see you are in Seattle, let's meet up.\"\n\nThe dock at the bottom contains the Phone, Photos, Safari, and Mail apps. The time displayed is 11:01, with a battery percentage of 89%.\nGPT-4V: \nThe image on the phone screen is the home screen of an iPhone. There are multiple app icons arranged in a  grid-like pattern, including Calendar, Settings, Twitter, Amazon, Instagram, etc. At the top of the screen, there is  a notification from a user named \"yykhahah\" that reads \"I see you are in Seattle, let's meet up.\" At the bottom of  the screen, there is a search bar. \nBased on the notification, my next move would be to move my finger to the Maps app icon, which is located in  the second row and the first column of the app grid. This will allow me to search for a location in Seattle and plan  a meet up with the user.  \nFigure 112: Prompting GPT-4V to predict the action upon receiving a notification. GPT-4V can accurately recognize the notification and the corresponding content (highlighted in  green  ).  Blue highlights the predicted actions.  Red  highlights the inaccurate location of the Maps app icon. Check Section  9.8  for detailed discussions. "}
{"page": 142, "image_path": "doc_images/2309.17421v2_142.jpg", "ocr_text": "Sec. 9.8 GUI Navigation\n\nNotification Understanding\n\nPrompt:\nImagine that you are a robot operating a computer. Like how humans operate the computer, you can move the\nmouse, click an icon with the mouse, or type some texts with the keyboard.\n\nBelow is what you see on the computer screen. Describe the image and predict your next move. If the action\ninvolves moving the mouse, describe the location and the icon as detailed as possible.\n\nGPT-4V:\n\nThe image on the computer screen is a desktop background of a bridge over a body of water. There are several\nicons on the desktop, including a YouTube icon, a Recycle Bin icon, and a Terminal icon. At the bottom of the\nscreen, there is a taskbar with several application icons, such as Microsoft Edge and File Explorer. On the\nbottom right corner of the screen, there is a notification that says \"Kevin Lin is calling you.\"\n\nBased on the notification, my next move would be to move the mouse to the bottom right corner of the screen\nand click on the notification. This would open the call application and allow me to answer Kevin Lin's call.\n\nFigure 113: Prompting GPT-4V to predict the action upon receiving a notification. GPT-4V can\naccurately recognize the notification and the corresponding content (highlighted in green ). Blue\n\nhighlights the predicted actions. ‘Red highlights the inaccurate location of the Maps app icon. Check\nSection 9.8 for detailed discussions.\n\n143\n", "vlm_text": "Sec. 9.8 GUI Navigation \nNotification Understanding \nPrompt: \nImagine that you are a robot operating a computer. Like how humans operate the computer, you can move the  mouse, click an icon with the mouse, or type some texts with the keyboard.  \nBelow is what you see on the computer screen. Describe the image and predict your next move. If the action  involves moving the mouse, describe the location and the icon as detailed as possible. \nThe image is a screenshot of a computer desktop featuring a scenic aerial wallpaper of a bridge over water connecting two rocky landmasses. On the desktop, there are several application icons on the left side, including YouTube Music, Recycle Bin, Abby Terminal, Google Chrome, Slack, Discord, and Adobe Acrobat Reader. The taskbar at the bottom displays various application icons, such as Microsoft Edge, File Explorer, Visual Studio, Teams, and others. Additionally, there is a notification in the bottom right corner indicating an incoming call from Kevin Lin via a communication application. The time and date in the corner show 11:18 AM on July 21, 2023.\nGPT-4V: \nThe image on the computer screen is a desktop background of a bridge over a body of water. There are several  icons on the desktop, including a YouTube icon, a Recycle Bin icon, and a Terminal icon. At the bottom of the  screen, there is a taskbar with several application icons, such as Microsoft Edge and File Explorer. On the  bottom right corner of the screen, there is a notification that says \"Kevin Lin is calling you.\" \nBased on the notification, my next move would be to move the mouse to the bottom right corner of the screen  and click on the notification. This would open the call application and allow me to answer Kevin Lin's call.  \nFigure 113: Prompting GPT-4V to predict the action upon receiving a notification. GPT-4V can accurately recognize the notification and the corresponding content (highlighted in  green  ).  Blue highlights the predicted actions.  Red  highlights the inaccurate location of the Maps app icon. Check Section  9.8  for detailed discussions. "}
{"page": 143, "image_path": "doc_images/2309.17421v2_143.jpg", "ocr_text": "Sec. 9.8 GUI Navigation\n\nNotification Understanding\n\nPrompt:\nImagine that you are a robot operating a computer. Like how humans operate the computer, you can move the\nmouse, click an icon with the mouse, or type some texts with the keyboard.\n\nBelow is what you see on the computer screen. Describe the image and predict your next move. If the action\ninvolves moving the mouse, describe the location as detailed as possible.\n\nGPT-4V:\n\nThe image on the computer screen is a desktop background of a beautiful aerial view of a bridge over the\nocean. There are several icons on the left side of the screen, including the Recycle Bin, Terminal, and a few\nothers. On the bottom right corner, there is a notification from a messaging app from a user named Kevin Lin\nasking if | am free for a chat this afternoon.\n\nMy next move would be to move the mouse to the bottom right corner of the screen and click on the notification\nfrom Kevin Lin to open the messaging app and respond to his message. | would move the mouse to the exact\nlocation of the notification, which is approximately 2 inches from the right edge of the screen and 1 inch from the\nbottom edge of the screen.\n\nFigure 114: Prompting GPT-4V to predict the action upon receiving a notification. GPT-4V can\naccurately recognize the notification and the corresponding content (highlighted in green ). Blue\nhighlights the predicted actions. Check Section 9.8 for detailed discussions.\n\n144\n", "vlm_text": "Sec. 9.8 GUI Navigation \nNotification Understanding \nPrompt: \nImagine that you are a robot operating a computer. Like how humans operate the computer, you can move the  mouse, click an icon with the mouse, or type some texts with the keyboard.  \nBelow is what you see on the computer screen. Describe the image and predict your next move. If the action  involves moving the mouse, describe the location as detailed as possible. \nThe image depicts a computer desktop with a background showing an aerial view of a bridge crossing over a body of water with land on either side. The desktop has several application icons on the left, including YouTube Music, Recycle Bin, Tabby Terminal, Google Chrome, Slack, Discord, and Adobe Acrobat Reader. There is a taskbar at the bottom with several applications and a notification from \"Kevin Lin\" asking, \"Do you have time for a quick chat this afternoon?\" The time displayed on the taskbar is 11:26 AM on 7/21/2023.\nGPT-4V: \nThe image on the computer screen is a desktop background of a beautiful aerial view of a bridge over the  ocean. There are several icons on the left side of the screen, including the Recycle Bin, Terminal, and a few  others. On the bottom right corner, there is a notification from a messaging app from a user named Kevin Lin  asking if I am free for a chat this afternoon. \n My next move would be to move the mouse to the bottom right corner of the screen and click on the notification  from Kevin Lin to open the messaging app and respond to his message. I would move the mouse to the exact  location of the notification, which is approximately 2 inches from the right edge of the screen and 1 inch from the  bottom edge of the screen.  \nFigure 114: Prompting GPT-4V to predict the action upon receiving a notification. GPT-4V can accurately recognize the notification and the corresponding content (highlighted in  green  ).  Blue highlights the predicted actions. Check Section  9.8  for detailed discussions. "}
{"page": 144, "image_path": "doc_images/2309.17421v2_144.jpg", "ocr_text": "Sec. 9.8 GUI Navigation\n\nWatching Videos\n\nPrompt:\nExplain the story shown in the images below.\n\nhow/muchil|have)\nspention myidog)\n\n700 a\nBILLION AND A BILLION AND A\n\nTRILLION 300 C TRILLION 300\nMILLION\n\nGPT-4V:\n\nThe images show a reel on Instagram, where a user is sharing a humorous post about how much they have\nspent on their dog. The first image shows a man holding his dog with the caption \"When someone asks me how\nmuch | have spent on my dog\". The next three images show a clip of Joe Biden saying \"700 billion\" followed by\n\"700 billion and a trillion 300 million\" and \"700 billion and a trillion 300 million dollars\". The user is jokingly\nexaggerating the amount they have spent on their dog by using the clip of Joe Biden saying a large sum of\n\nTell me about the video content, given the frames below.\n\nThe video content is a compilation of things that Asian people do for no reason. The video is posted on TikTok by\nuser laparasian and has 1.3M views and 13.1K likes. The video is set to original audio by sophrelabelous and\nhas the hashtag #asian and #relatable.\n\nThe video is broken down into 5 different scenes:\n\n1. The first scene shows a girl grabbing products from the back of the shelf in a supermarket.\n\n2. The second scene shows a girl rinsing a perfectly clean cup before using it.\n\n3. The third scene shows two girls asking \"have you eaten yet?\" as a form of greeting.\n\n4. The fourth scene shows two girls fighting aggressively for the bill in a restaurant.\n\n5. The fifth scene shows a girl wrapping pants around her neck to see if it fits.\n\nOverall, the video is a humorous take on some of the quirks and habits that are commonly seen in the Asian\n\ncommunity.\n\nFigure 115: Prompting GPT-4V to watch web videos. We present GPT-4V the screenshot of the\nvideo frames following their temporal order in the original video. To save space, we illustrate the\nframes in a row, where the leftmost one is the first frame. Check Section 9.8 for detailed discussions.\n\n‘Red highlights the inaccurate descriptions about the video.\n\n145\n", "vlm_text": "Sec. 9.8 GUI Navigation \nWatching Videos \nPrompt: \nExplain the story shown in the images below. \nThe image is a collage of four screenshots from a video reel. \n\n1. The first frame shows a person sitting on a bench with a dog, and the text overlay reads, \"When someone asks me how much I have spent on my dog.\"\n2. The other three frames feature a close-up of a person at a podium, with text overlays in each frame reading \"700 BILLION,\" \"700 BILLION AND A TRILLION 300,\" and \"700 BILLION AND A TRILLION 300 MILLION.\"\n\nThe reel seems to humorously compare the exaggerated amount spent on a dog to large financial figures.\nGPT-4V: \nThe images show a reel on Instagram, where a user is sharing a humorous post about how much they have  spent on their dog. The first image shows a man holding his dog with the caption \"When someone asks me how  much I have spent on my dog\". The next three images show a clip of Joe Biden saying \"700 billion\" followed by  \"700 billion and a trillion 300 million\" and \"700 billion and a trillion 300 million dollars\". The user is jokingly  exaggerating the amount they have spent on their dog by using the clip of Joe Biden saying a large sum of  money.   \nPrompt: \nTell me about the video content, given the frames below. \nThis image is a series of six Instagram story screenshots. Each frame humorously highlights activities labeled as \"Things Asian people do for no reason.\" The activities depicted are:\n\n1. Grabbing products from the back of the shelf at a store.\n2. Rinsing a perfectly clean cup before using it.\n3. Asking \"have you eaten yet?\" as a form of greeting.\n4. Fighting aggressively for the bill.\n5. Wrapping pants around the neck to see if they fit.\n\nEach image features individuals performing these actions in various everyday settings.\nGPT-4V: \nThe video content is a compilation of things that Asian people do for no reason. The video is posted on TikTok by  user laparasian and has 1.3M views and 13.1K likes. The video is set to original audio by soph relabel o us and  has the hashtag #asian and #relatable. \nThe video is broken down into 5 different scenes: \n1. The first scene shows a girl grabbing products from the back of the shelf in a supermarket. 2. The second scene shows a girl rinsing a perfectly clean cup before using it. 3. The third scene shows two girls asking \"have you eaten yet?\" as a form of greeting. 4. The fourth scene shows two girls fighting aggressively for the bill in a restaurant. 5. The fifth scene shows a girl wrapping pants around her neck to see if it fits. \nOverall, the video is a humorous take on some of the quirks and habits that are commonly seen in the Asian  community.  \nFigure 115: Prompting GPT-4V to watch web videos. We present GPT-4V the screenshot of the video frames following their temporal order in the original video. To save space, we illustrate the frames in a row, where the leftmost one is the first frame. Check Section  9.8  for detailed discussions. Red highlights the inaccurate descriptions about the video. "}
{"page": 145, "image_path": "doc_images/2309.17421v2_145.jpg", "ocr_text": "Sec. 9.8 GUI Navigation\n\nWatching Videos\n\nPrompt:\nTranscribe the video content, given the frames below.\n\nGPT-4V:\n\nHi everyone, it's natyexplora here and today I'm going to be sharing 7 places you need to visit in Washington\nState.\n\nFirst up, we have North Cascades National Park. This park is filled with beautiful mountain views and hiking\ntrails. It's a must-visit for any nature lover.\n\nNext, we have the Olympic Peninsula. This area is home to the famous Hoh Rainforest and beautiful beaches.\nIt's a great place to relax and enjoy the natural beauty of Washington.\n\nThird on the list is Mt Baker Wilderness. This area is known for its stunning mountain views and hiking trails. It's\na great place to escape the city and enjoy the great outdoors.\n\nNext up, we have Mt Rainier National Park. This park is home to the highest mountain in Washington and is a\ngreat place for hiking and sightseeing.\n\nFifth on the list is Alpine Lakes Wilderness. This area is filled with beautiful alpine lakes and hiking trails. It's a\n\ngreat place to spend a day exploring and enjoying the natural beauty of Washington.\n\nNext, we have Mt Pilchuck State Park. This park is known for its beautiful views and hiking trails. It's a great\nplace to spend a day exploring and enjoying the natural beauty of Washington.\n\nLast but not least, we have Gifford Pinchot National Forest. This forest is home to beautiful mountain views and\nhiking trails. It's a great place to escape the city and enjoy the great outdoors.\n\nAnd that concludes my list of 7 places you need to visit in Washington State. | hope you enjoyed it and are\ninspired to explore more of this beautiful state.\n\nTell me about the video content, given the frames below.\n\nGPT-4V:\n\nThe video content is about a corgi dog named Tofu who is working from home. The first frame shows Tofu\nsleeping on a bed with the caption \"WFH Employees: 8:59 AM\". The second frame shows Tofu sitting in front of\na computer with multiple screens, with the caption \"WFH Employees: 9:00 AM\". The video is likely a humorous\ntake on the work from home experience, highlighting how Tofu is just like any other employee who struggles to\nget out of bed in the morning but is ready to work once they're in front of their computer.\n\nFigure 116: Watching web videos. We present GPT-4V the screenshot of the video frames following\ntheir temporal order in the original video. To save space, we illustrate the frames in a row, where the\nleftmost one is the first frame. Check Section 9.8 for detailed discussions.\n\n146\n", "vlm_text": "Sec. 9.8 GUI Navigation \nWatching Videos \nPrompt: \nTranscribe the video content, given the frames below. \nThe image contains a series of Instagram posts showcasing various scenic locations in Washington State. Each post features a different landscape, such as mountains, forests, and national parks. The places highlighted in the posts include:\n\n1. A mountain trail\n2. North Cascades National Park\n3. Olympic National Park\n4. Mt Baker Wilderness\n5. Mt Rainier National Park\n6. Mt Pilchuck State Park\n7. Gifford Pinchot National Forest\n\nThe theme of the series is \"7 Places you need to visit in Washington State.\"\nGPT-4V: \nHi everyone, it's nat y exp lora here and today I'm going to be sharing 7 places you need to visit in Washington  State.  \nFirst up, we have North Cascades National Park. This park is filled with beautiful mountain views and hiking  trails. It's a must-visit for any nature lover. Next, we have the Olympic Peninsula. This area is home to the famous Hoh Rainforest and beautiful beaches.  It's a great place to relax and enjoy the natural beauty of Washington. Third on the list is Mt Baker Wilderness. This area is known for its stunning mountain views and hiking trails. It's  a great place to escape the city and enjoy the great outdoors. Next up, we have Mt Rainier National Park. This park is home to the highest mountain in Washington and is a  great place for hiking and sightseeing. Fifth on the list is Alpine Lakes Wilderness. This area is filled with beautiful alpine lakes and hiking trails. It's a  great place to spend a day exploring and enjoying the natural beauty of Washington. Next, we have Mt Pilchuck State Park. This park is known for its beautiful views and hiking trails. It's a great  place to spend a day exploring and enjoying the natural beauty of Washington. Last but not least, we have Gifford Pinchot National Forest. This forest is home to beautiful mountain views and  hiking trails. It's a great place to escape the city and enjoy the great outdoors. And that concludes my list of 7 places you need to visit in Washington State. I hope you enjoyed it and are  inspired to explore more of this beautiful state.  \nPrompt: \nTell me about the video content, given the frames below. \nThe image is a humorous comparison of a dog involved in \"work from home\" activities. \n\nIn the left side image, a dog is comfortably sleeping in bed under the caption \"WFH Employees: 8:59 AM.\"\n\nIn the right side image, the same dog is sitting in front of a computer showing a video call, under the caption \"WFH Employees: 9:00 AM.\"\n\nThis plays on the joke that remote employees might be in bed until just before their workday starts.\nGPT-4V: \nThe video content is about a corgi dog named Tofu who is working from home. The first frame shows Tofu  sleeping on a bed with the caption \"WFH Employees: 8:59 AM\". The second frame shows Tofu sitting in front of  a computer with multiple screens, with the caption \"WFH Employees: 9:00 AM\". The video is likely a humorous  take on the work from home experience, highlighting how Tofu is just like any other employee who struggles to  get out of bed in the morning but is ready to work once they're in front of their computer.  \nFigure 116: Watching web videos. We present GPT-4V the screenshot of the video frames following their temporal order in the original video. To save space, we illustrate the frames in a row, where the leftmost one is the first frame. Check Section  9.8  for detailed discussions. "}
{"page": 146, "image_path": "doc_images/2309.17421v2_146.jpg", "ocr_text": "Sec. 9.8 GUI Navigation\n\nWatching Videos\n\nPrompt:\nTell me about the video content, given the frames below. Please include as many details as possible.\n\nThree Easy Breakfast Ideas\n\nFor BLW, Toddlers & Bi For BLW, Toddlers & Big Kids\n\n1. Raspberry Biss Balls\n\nGPT-4V:\nThe video content is a tutorial on how to make Raspberry Bliss Balls, a healthy breakfast option for toddlers and\nbig kids. The video is posted by the Instagram account \"mealtime with mummy\" and has over 13k likes.\n\nThe video starts with a bowl of fresh raspberries and a fork, which is then mashed into a puree. The next frame\nshows a larger mixing bowl with oats, chia seeds, and peanut butter being added to the raspberry puree. The\nmixture is then stirred together until well combined.\n\nThe following frame shows a plate with shredded coconut on it, and the raspberry mixture being rolled into small\nballs and coated in the coconut. The final frames show the finished Raspberry Bliss Balls being placed in a bowl\nand ready to eat.\n\nThroughout the video, there are text overlays with instructions and tips, making it easy for viewers to follow\nalong and make the recipe themselves.\n\nFigure 117: Watching web videos. We present GPT-4V the screenshot of the video frames following\ntheir temporal order in the original video. To save space, we illustrate frames 1-5 in the first row, and\nframes 6-9 in the second row. Check Section 9.8 for detailed discussions.\n\n147\n", "vlm_text": "Sec. 9.8 GUI Navigation \nWatching Videos \nPrompt: \nTell me about the video content, given the frames below. Please include as many details as possible. \nThe image is a collage of screenshots from a video tutorial on making \"Raspberry Bliss Balls\" as part of \"Three Easy Breakfast Ideas For BLW, Toddlers & Big Kids\" by the Instagram user \"mealtimewithmummy.\"\n\n1. The first image shows raspberries in a bowl being mashed with a fork.\n2. The second image shows the raspberries being transferred into another bowl.\n3. The third image has additional ingredients like oats being added to the raspberries.\n4. The fourth image shows a bowl with chia seeds, oats, and possibly nut butter added to the mixture.\n5. The fifth image displays the ingredients being mixed together with a spoon.\n6. The sixth image shows the mixture being combined into a dough-like consistency.\n7. The seventh image shows a plate with shredded coconut in preparation for coating.\n8. The eighth image depicts the mixture being shaped into balls and coated with shredded coconut on the plate.\n9. The ninth image shows the finished raspberry bliss balls arranged on a plate.\n10. The tenth image displays a hand holding a bowl filled with the completed raspberry bliss balls.\n\nThe overall process highlights a simple, no-bake recipe suitable for children and those practicing baby-led weaning (BLW).\nGPT-4V: \nThe video content is a tutorial on how to make Raspberry Bliss Balls, a healthy breakfast option for toddlers and  big kids. The video is posted by the Instagram account \"mealtime with mummy\" and has over 13k likes. \nThe video starts with a bowl of fresh raspberries and a fork, which is then mashed into a puree. The next frame  shows a larger mixing bowl with oats, chia seeds, and peanut butter being added to the raspberry puree. The  mixture is then stirred together until well combined. \nThe following frame shows a plate with shredded coconut on it, and the raspberry mixture being rolled into small  balls and coated in the coconut. The final frames show the finished Raspberry Bliss Balls being placed in a bowl  and ready to eat. \nThroughout the video, there are text overlays with instructions and tips, making it easy for viewers to follow  along and make the recipe themselves.  \nFigure 117: Watching web videos. We present GPT-4V the screenshot of the video frames following their temporal order in the original video. To save space, we illustrate frames 1-5 in the first row, and frames 6-9 in the second row. Check Section  9.8  for detailed discussions. "}
{"page": 147, "image_path": "doc_images/2309.17421v2_147.jpg", "ocr_text": "Sec. 9.8 GUI Navigation\n\nWatching Videos\n\nPrompt:\nTell me about the video content, given the frames below. Please include as many details as possible.\n\nGPT-4V:\n\nThe video content is a recipe tutorial for apple and banana pancakes. The video is posted on Instagram Reels\nby the user \"mealtime with mummy\" and has 13.2K views.\n\nIn the first frame, we see a glass bowl with a mashed banana and an apple being added to it. The second frame\nshows the banana and apple being mixed together. The third frame shows an egg being added to the mixture.\nThe fourth frame shows the mixture being stirred with a fork.\n\nThe fifth frame shows a bowl of oats being added to the mixture. The sixth frame shows the oats being mixed in\nwith a spoon. The seventh frame shows the mixture being spooned into a frying pan. The eighth frame shows\nthe pancakes being cooked in the frying pan.\n\nThe final frame shows the finished pancakes on a plate, served with sliced apples, bananas, and yogurt. The\ncaption on the video reads \"SAVE for easy breakfast inspo\" and the video is set to the song \"Stayin' Alive\" by the\nBee Gees.\n\nFigure 118: Watching web videos. We present GPT-4V the screenshot of the video frames following\ntheir temporal order in the original video. To save space, we illustrate frames 1-5 in the first row, and\nframes 6-9 in the second row. Check Section 9.8 for detailed discussions.\n\n148\n", "vlm_text": "Sec. 9.8 GUI Navigation \nWatching Videos \nPrompt: \nTell me about the video content, given the frames below. Please include as many details as possible. \nThis image sequence shows the process of making apple and banana pancakes. Here's a breakdown:\n\n1. A bowl with a whole apple and mashed bananas.\n2. Mixing the banana and apple together.\n3. Adding an egg to the mixture.\n4. Mixing in the egg.\n5. Adding oats to the mixture.\n6. Mixing everything together thoroughly.\n7. Cooking the mixture in a skillet to form pancakes.\n8. Serving the pancakes on a plate with sliced fruits and yogurt on the side.\nGPT-4V: \nThe video content is a recipe tutorial for apple and banana pancakes. The video is posted on Instagram Reels  by the user \"mealtime with mummy\" and has 13.2K views.  \nIn the first frame, we see a glass bowl with a mashed banana and an apple being added to it. The second frame  shows the banana and apple being mixed together. The third frame shows an egg being added to the mixture.  The fourth frame shows the mixture being stirred with a fork.  \nThe fifth frame shows a bowl of oats being added to the mixture. The sixth frame shows the oats being mixed in  with a spoon. The seventh frame shows the mixture being spooned into a frying pan. The eighth frame shows  the pancakes being cooked in the frying pan.  \nThe final frame shows the finished pancakes on a plate, served with sliced apples, bananas, and yogurt. The  caption on the video reads \"SAVE for easy breakfast inspo\" and the video is set to the song \"Stayin' Alive\" by the  Bee Gees.  \nFigure 118: Watching web videos. We present GPT-4V the screenshot of the video frames following their temporal order in the original video. To save space, we illustrate frames 1-5 in the first row, and frames 6-9 in the second row. Check Section  9.8  for detailed discussions. "}
{"page": 148, "image_path": "doc_images/2309.17421v2_148.jpg", "ocr_text": "Sec. 9.8 GUI Navigation\n\nWatching Videos\n\nPrompt:\nTell me about the video content, given the 9 frames below. Please include as many details as possible.\n\nGPT-4V:\nThe video content is a recipe tutorial for strawberry stuffed French toast. The video is posted on Instagram Reels\nby the user \"mealtime with mummy\". The video has 13.2K likes and 190 comments.\n\nIn the first frame, we see a white plate with a dollop of red sauce and a dollop of white cream. The second frame\nshows the plate with the red sauce and white cream mixed together.\n\nThe fourth frame shows a plate with a circular piece of bread on it. The fifth frame shows a hand using a Knife'to\nspread a pink mixture onto the bread. The sixth frame shows the bread with the pink mixture covered with\nanother piece of bread, making a sandwich.\n\nThe seventh frame shows a hand dipping the sandwich into a bowl of beaten eggs. The eighth frame shows the\nsandwich being fried in a pan. The final frame shows the finished strawberry stuffed French toast on a plate.\n\nThe video is set to the song \"Stayin' Alive\" by the Bee Gees. The caption reads \"SAVE for easy breakfast inspo \".\n\nFigure 119: Watching web videos. We present GPT-4V the screenshot of the video frames following\ntheir temporal order in the original video. To save space, we illustrate frames 1-5 in the first row, and\n\nframes 6-9 in the second row. ‘Red highlights the inaccurate descriptions about the video. Check\nSection 9.8 for detailed discussions.\n\n149\n", "vlm_text": "Sec. 9.8 GUI Navigation \nWatching Videos \nPrompt: \nTell me about the video content, given the 9 frames below. Please include as many details as possible. \nThe image is a series of screenshots from an Instagram Reel by the user @mealtimewithmummy, demonstrating a recipe for \"Strawberry Stuffed French Toast.\" The sequence shows the following steps:\n\n1. Mixing strawberries with cream in a bowl.\n2. Spreading the mixture on a slice of bread.\n3. Placing another slice of bread on top.\n4. Using a fork to seal the edges of the sandwich.\n5. Dipping the stuffed sandwich in a beaten egg mixture.\n6. Cooking the dipped sandwich in a frying pan until golden brown.\n7. Displaying the final cooked Strawberry Stuffed French Toast along with a suggestion to follow @mealtimewithmummy for more recipes.\n\nThe images showcase a casual cooking process and emphasize ease of preparation for breakfast recipes.\nGPT-4V: \nThe video content is a recipe tutorial for strawberry stuffed French toast. The video is posted on Instagram Reels  by the user \"mealtime with mummy\". The video has 13.2K likes and 190 comments. \nIn the first frame, we see a white plate with a dollop of red sauce and a dollop of white cream. The second frame  shows the plate with the red sauce and white cream mixed together. The third frame shows a slice of bread with  a knife spreading a mixture onto it.  \nThe fourth frame shows a plate with a circular piece of bread on it. The fifth frame shows a hand using a knife to  spread a pink mixture onto the bread. The sixth frame shows the bread with the pink mixture covered with  another piece of bread, making a sandwich. \nThe seventh frame shows a hand dipping the sandwich into a bowl of beaten eggs. The eighth frame shows the  sandwich being fried in a pan. The final frame shows the finished strawberry stuffed French toast on a plate. \nThe video is set to the song \"Stayin' Alive\" by the Bee Gees. The caption reads \"SAVE for easy breakfast inspo \".  \nFigure 119: Watching web videos. We present GPT-4V the screenshot of the video frames following their temporal order in the original video. To save space, we illustrate frames 1-5 in the first row, and frames 6-9 in the second row.  Red  highlights the inaccurate descriptions about the video. Check Section  9.8  for detailed discussions. "}
{"page": 149, "image_path": "doc_images/2309.17421v2_149.jpg", "ocr_text": "10 LMM Powered Agents\n\nIn this section, we discuss possible future research directions that may further amplify GPT-4V’s\ncapabilities. The discussion focuses on how the intriguing usages in LLMs may extend to the\nmultimodal scenario and its enabled new abilities, e.g., multimodal plugins, multimodal chains,\nself-reflection, self-consistency, and retrieval-augmented LMMs, etc. In the following sub-sections,\nwe use human-generated examples to illustrate potential ways to enhance GPT-4V-based systems.\n\nSec. 10.1 Multimodal Plugins\n\nPrompt:\nWhere is this photo taken?\n\nGPT-4V:\n\nSorry, | cannot answer this question accurately as no context or information is provided with the photo.\nHowever, based on the destruction and the presence of emergency responders, it appears to be taken ina\nwar-torn or disaster-stricken area.\n\nWhere is this photo taken?\n\nNote: Same query image as shown above, resized to\nsave space\n\nGPT-4V:\n- | do not have that information.\n- Enabling Bing Image Search Plugin.\n\nCalling Bing Image Search...\n- This image is taken in izmir, Turkey\n- The image is related to an earthquake event\n- To summarize, this image was taken in izmir, Turkey, during a magnitude 6.6 earthquake in Turkey's\nAegean region. The scene shows the aftermath of the disaster with people standing in a pile of rubble, a\ncrane lifting a truck, and a red truck with a black front.\n\nFigure 120: Illustration of using the Bing Image Search [94] plugin to enable GPT-4V with time-\nsensitive knowledge (bottom, highlighted in green ). Note that the earthquake happened on February\n\n6, 2023, which is after GPT-4V’s training, thereby GPT-4V fails to identify the exact location without\nplugin (top). Check Section 10.1 for detailed discussions.\n\n150\n", "vlm_text": "10 LMM Powered Agents \nIn this section, we discuss possible future research directions that may further amplify GPT-4V’s capabilities. The discussion focuses on how the intriguing usages in LLMs may extend to the multimodal scenario and its enabled new abilities,  e.g ., multimodal plugins, multimodal chains, self-reflection, self-consistency, and retrieval-augmented LMMs,  etc . In the following sub-sections, we use  human-generated  examples to illustrate potential ways to enhance GPT-4V-based systems. \nSec. 10.1 Multimodal Plugins \nPrompt: \nThe image shows a scene of destruction with emergency responders on site. It appears to be taken in a war-torn or disaster-stricken area, but without more specific details, it's not possible to determine the exact location.\nPrompt: Where is this photo taken? \nThe image shows a scene of disaster recovery. There are rubble and debris from what appears to be a collapsed building. Several people, including some in bright outfits, are on-site, possibly engaged in search and rescue operations. Smoke or dust is also visible, adding to the chaotic atmosphere.\nNote: Same query image as shown above, resized to  save space \nGPT-4V: - I do not have that information. - Enabling Bing Image Search Plugin. \nFigure 120: Illustration of using the Bing Image Search [ 94 ] plugin to enable GPT-4V with time- sensitive knowledge (bottom, highlighted in  green  ). Note that the earthquake happened on February 6, 2023, which is after GPT-4V’s training, thereby GPT-4V fails to identify the exact location without plugin (top). Check Section  10.1  for detailed discussions. "}
{"page": 150, "image_path": "doc_images/2309.17421v2_150.jpg", "ocr_text": "Sec. 10.2 Multimodal Chains\nPrompt:\n\nPlease determine whether each person in the image wears a helmet or not. And summarize how many people\nare wearing helmets.\n\nThought 1 This question requires detecting people in the image.\n\nAction 1 Assistant, detect the people shown in this image <ImagePath>\n\nObservation 1 Person [4, 126, 72, 175]\nPerson [63, 121, 88, 181]\nPerson [30, 107, 56, 88]\nPerson [223, 123, 43, 78]\nPerson [178, 122, 52, 180]\nPerson [322, 118, 41, 68]\nPerson [268, 109, 55, 178]\nPerson [357, 127, 88, 173]\n\nThought 2 There are 8 people detected in this image.\n\nAction 2 Assistant, crop the regions in this image specified by the bounding boxes <ImagePath>\n\nObservation 2 Image 1 Image 2 Image 3 Image 4 Image 5 Image 6 Image 7 Image 8\n\nGPT-4V:\n\nImage 1: Not wearing a helmet\n\nImage 2: Not wearing a helmet\n\nImage 3: Wearing a helmet\n\nImage 4: Wearing a helmet\n\nImage 5: Wearing a helmet\n\nImage 6: Wearing a helmet\n\nImage 7: Wearing a helmet\n\nImage 8: Not wearing a helmet\n\nSummary: Out of 8 people, 5 are wearing helmets and 3 are not wearing helmets.\n\nFigure 121: Extending GPT-4V to multimodal chains with ReAct [145, 142] for PPE Counting\nscenario. Check Section 10.2 for detailed discussions.\n\n151\n", "vlm_text": "Sec. 10.2 Multimodal Chains \nPrompt: \nPlease determine whether each person in the image wears a helmet or not. And summarize how many people  are wearing helmets. \nThe image shows a group of people wearing uniforms and hard hats, likely indicating they are workers or engineers, possibly in an industrial setting. They are walking together and appear to be in an outdoor location near large pipes or industrial equipment.\nThought 1 \nThis question requires detecting people in the image. \nAction 1 \nAssistant, detect the people shown in this image <ImagePath> \nPerson [4, 126, 72, 175] Person [63, 121, 88, 181] Person [30, 107, 56, 88] Person [223, 123, 43, 78] Person [178, 122, 52, 180] Person [322, 118, 41, 68] Person [268, 109, 55, 178] Person [357, 127, 88, 173] \nThought 2 \nThere are 8 people detected in this image.  \nAction 2 \nAssistant, crop the regions in this image specified by the bounding boxes <ImagePath> \nObservation 2 \nThe image shows a sequence of panels featuring individuals dressed in work uniforms, holding and wearing yellow safety helmets. The uniforms appear to be work attire, likely for a construction or industrial setting.\nGPT-4V: \nImage 1: Not wearing a helmet Image 2: Not wearing a helmet Image 3: Wearing a helmet Image 4: Wearing a helmet Image 5: Wearing a helmet Image 6: Wearing a helmet Image 7: Wearing a helmet Image 8: Not wearing a helmet Summary: Out of 8 people, 5 are wearing helmets and 3 are not wearing helmets.  \nFigure 121: Extending GPT-4V to multimodal chains with ReAct [ 145 ,  142 ] for PPE Counting scenario. Check Section  10.2  for detailed discussions. "}
{"page": 151, "image_path": "doc_images/2309.17421v2_151.jpg", "ocr_text": "10.1 Multimodal Plugins\n\nIn the context of LLMs, plugins [98, 56, 6, 112, 87, 103] play a crucial role in assisting LLMs for\nvarious tasks such as accessing the latest information, performing computations, or utilizing third-\nparty services. These plugins are primarily designed to process inputs in natural language or inputs\nthat can be interpreted as language, such as code and math equations. To illustrate the significance of\nmultimodal plugins, such as Bing Image Search [94], especially in the context of LMMs, we present\nFigure 120. By incorporating the Bing Image Search plugin, we empower GPT-4V to acquire time-\nsensitive knowledge related to the input image. In the upper part of the figure, we demonstrate the\nlimitations of GPT-4V without Bing Image Search plugin. It fails to accurately answer the question,\n\"Where was this photo taken?\" due to the fact that the photo captures the aftermath of a massive\nearthquake that occurred on February 6, 2023, at the border of Turkey and Syria—a situation that took\nplace after GPT-4V’s training. Since constantly retraining the model with current information can be\ncomputationally intensive and expensive, plugins like search engines prove to be invaluable resources\nfor the model to access up-to-date information. In the lower part of Figure 120, we showcase the\ncapabilities of GPT-4V when equipped with the Bing Image Search plugin. It effectively leverages the\nretrieved information from the plugin, enabling accurate identification of the location Izmir, Turkey.\n\n10.2 Multimodal Chains\n\nChaining with LLMs has been explored extensively in recent research [145, 47, 124, 107]. This\napproach goes beyond using a single plugin and instead establishes a system paradigm that integrates\nLLMs with a pool of plugins, enabling more advanced reasoning and interactions. By replacing\nlanguage-only plugins with vision/multimodal experts such as image captioners, object detectors, or\nwell-trained models for text-to-image generation and audio-to-text conversion, it becomes possible to\nconstruct a powerful multimodal chain with LLMs [137, 142, 121, 114, 75, 86].\n\nHowever, the interactions within these chains between LLMs and the plugins typically take place in\ntext format. Although the plugins may accept multimodal inputs, they return results in text to enhance\nthe knowledge of LLMs. There is a notable exception in the case of image synthesis/editing [137],\nwhere the plugins can generate images, but these images are not fed back into LLMs for further\nanalysis or knowledge augmentation, as LLMs can only process language-based inputs.\n\nIn Figure 121, we present an illustration of how GPT-4V, can be extended to support multimodal\nchains with ReAct [145, 142]. This extension enables the plugins in the chain to provide multimodal\ninformation, which can then be collectively processed by GPT-4V to achieve advanced reasoning in\nscenarios such as PPE counting. The entire chaining process shown in Figure 121 is divided into\ntwo rounds of thought, action, and observation, with each round involving the activation of a specific\nplugin. In the first round, GPT-4V deduces that person detection is necessary to count the number of\npeople wearing helmets (Thought 1). Consequently, it calls the person detector tool (Action 1) and\nreceives the coordinates of bounding boxes for each detected person in the image (Observation 1).\nMoving to the second round, based on the obtained bounding box information, GPT-4V infers that\nthere are a total of 8 people in the image (Thought 2). It then utilizes the image cropping tool to crop\nout individual images of each person according to their corresponding bounding box coordinates\n(Action 2). The resulting outputs (Observation 2) consist of 8 labeled images, numbered from image\n1 to image 8. GPT-4V subsequently determines whether each person in these images is wearing a\nhelmet or not, and summarizes the total count of people wearing helmets.\n\nOverall, this integration of LMMs with a pool of multimodal plugins opens up new possibilities for\nenhanced reasoning and interaction, leveraging the strengths of both language and vision capabilities.\nThe flexibility of multimodal chains allows for a more comprehensive understanding and analysis of\nmultimodal data, and can potentially lead to improved performance in various applications.\n\n10.3. Self-Reflection\n\nFigure 122 demonstrates the application of self-reflection [116, 88, 63] to improve the results shown\nin Figure 47. As we can see, the self-reflected result is better aligned with the reference image. For\nexample, on the left side, the number of data points is corrected from 4 to 3, while on the right side,\nthe percentage is added back above the bar. Although the result is still not exactly identical, it is\nevident that self-reflection can facilitate manual polishing. Figure 123 shows another example of\nself-reflection in improving the prompt generation for text-to-image models [106].\n\n152\n", "vlm_text": "10.1 Multimodal Plugins \nIn the context of LLMs, plugins [ 98 ,  56 ,  6 ,  112 ,  87 ,  103 ] play a crucial role in assisting LLMs for various tasks such as accessing the latest information, performing computations, or utilizing third- party services. These plugins are primarily designed to process inputs in natural language or inputs that can be interpreted as language, such as code and math equations. To illustrate the significance of multimodal plugins, such as Bing Image Search [ 94 ], especially in the context of LMMs, we present Figure  120 . By incorporating the Bing Image Search plugin, we empower GPT-4V to acquire time- sensitive knowledge related to the input image. In the upper part of the figure, we demonstrate the limitations of GPT-4V without Bing Image Search plugin. It fails to accurately answer the question, \"Where was this photo taken?\" due to the fact that the photo captures the aftermath of a massive earthquake that occurred on February 6, 2023, at the border of Turkey and Syria—a situation that took place after GPT-4V’s training. Since constantly retraining the model with current information can be computationally intensive and expensive, plugins like search engines prove to be invaluable resources for the model to access up-to-date information. In the lower part of Figure  120 , we showcase the capabilities of GPT-4V when equipped with the Bing Image Search plugin. It effectively leverages the retrieved information from the plugin, enabling accurate identification of the location  Izmir, Turkey. \n10.2 Multimodal Chains \nChaining with LLMs has been explored extensively in recent research [ 145 ,  47 ,  124 ,  107 ]. This approach goes beyond using a single plugin and instead establishes a system paradigm that integrates LLMs with a pool of plugins, enabling more advanced reasoning and interactions. By replacing language-only plugins with vision/multimodal experts such as image captioners, object detectors, or well-trained models for text-to-image generation and audio-to-text conversion, it becomes possible to construct a powerful multimodal chain with LLMs [ 137 ,  142 ,  121 ,  114 ,  75 ,  86 ]. \nHowever, the interactions within these chains between LLMs and the plugins typically take place in text format. Although the plugins may accept multimodal inputs, they return results in text to enhance the knowledge of LLMs. There is a notable exception in the case of image synthesis/editing [ 137 ], where the plugins can generate images, but these images are not fed back into LLMs for further analysis or knowledge augmentation, as LLMs can only process language-based inputs. \nIn Figure  121 , we present an illustration of how GPT-4V, can be extended to support multimodal chains with ReAct [ 145 ,  142 ]. This extension enables the plugins in the chain to provide multimodal information, which can then be collectively processed by GPT-4V to achieve advanced reasoning in scenarios such as PPE counting. The entire chaining process shown in Figure  121  is divided into two rounds of thought, action, and observation, with each round involving the activation of a specific plugin. In the first round, GPT-4V deduces that person detection is necessary to count the number of people wearing helmets (Thought 1). Consequently, it calls the person detector tool (Action 1) and receives the coordinates of bounding boxes for each detected person in the image (Observation 1). Moving to the second round, based on the obtained bounding box information, GPT-4V infers that there are a total of 8 people in the image (Thought 2). It then utilizes the image cropping tool to crop out individual images of each person according to their corresponding bounding box coordinates (Action 2). The resulting outputs (Observation 2) consist of 8 labeled images, numbered from image 1 to image 8. GPT-4V subsequently determines whether each person in these images is wearing a helmet or not, and summarizes the total count of people wearing helmets. \nOverall, this integration of LMMs with a pool of multimodal plugins opens up new possibilities for enhanced reasoning and interaction, leveraging the strengths of both language and vision capabilities. The flexibility of multimodal chains allows for a more comprehensive understanding and analysis of multimodal data, and can potentially lead to improved performance in various applications. \n10.3 Self-Reflection \nFigure  122  demonstrates the application of self-reflection [ 116 ,  88 ,  63 ] to improve the results shown in Figure  47 . As we can see, the self-reflected result is better aligned with the reference image. For example, on the left side, the number of data points is corrected from 4 to 3, while on the right side, the percentage is added back above the bar. Although the result is still not exactly identical, it is evident that self-reflection can facilitate manual polishing. Figure  123  shows another example of self-reflection in improving the prompt generation for text-to-image models [ 106 ]. "}
{"page": 152, "image_path": "doc_images/2309.17421v2_152.jpg", "ocr_text": "Sec. 10.3 Self-Reflection\n\nSelf-Reflection to Improve Coding\n\nPrompt:\nGenerate python code to draw similar curves.\n\n[EE\n\n—k Base\nSe Large\nO Huge\n\n10° 10\nmillion images in pre-training\n\naot 10?\nmillion images in pre-training\n\nCan you double-check the code and align it better\nwith the reference image?\n\nNote: Code from the above GPT-4V\nresponse is omitted to save space\n\nGPT-4V:\n\n160\n\n150\n\n10?\nmillion images in pre-training\n\nPrompt:\nWrite python codes to generate similar figures.\n\nME OSCARS\nEE MiniVLM (ours)\n\n100%,\n930%\n100% o40g\n\nComparison of OSCarB and MiniVLM\n\nlt\n\n‘Caption VOA ™\n\nmm Oscars\nfmm MiniVLM (ours)\n\nR NwR2\n\nCan you double-check the code and align it better\nwith the reference image?\n\nNote: Code from the above GPT-4V\nresponse is omitted to save space\n\nGPT-4V:\n\nComparison of OSCarB and MiniVLM\n\nlm Oscars\nmE MiniVuM (ours)\n\nFigure 122: Illustration of using self-reflection to improve the code for figure drawing. Left: after\nreflection, the number of points in the curve aligns with the reference image. Right: the percentage is\nadded to align with the reference image. Check Section 10.3 for detailed discussions.\n\n153\n", "vlm_text": "Sec. 10.3 Self-Reflection \nSelf-Reflection to Improve Coding \nPrompt: Generate python code to draw similar curves. \nThe image is a line graph that compares the performance of three different models—Base, Large, and Huge—based on the number of million images used in pre-training. The y-axis represents the CIDEr score, which is a metric commonly used for evaluating the quality of image captioning. The x-axis is on a logarithmic scale and represents the number of million images used in pre-training.\n\nKey observations from the graph:\n- Each model shows an increase in CIDEr score as the number of pre-training images increases.\n- The Huge model performs the best, achieving the highest CIDEr scores at each data point.\n- The Base model starts with a lower score at the smallest dataset size and shows steady improvement, though it remains the lowest-performing among the three.\n- The Large model is in the middle, outperforming the Base model but not reaching the scores of the Huge model.\n\nAdditionally, each model is represented with different markers for their lines: stars for Base, crosses for Large, and circles for Huge.\nThe image is a line graph that visualizes the relationship between the number of millions of images used in pre-training and the CIDEr score, which measures the quality of image captioning. The graph includes three lines, each representing a different model size: Base, Large, and Huge. \n\n- The x-axis is labeled \"million images in pre-training\" and uses a logarithmic scale ranging approximately from 10^0 (1) to 10^3 (1000).\n- The y-axis is labeled \"CIDEr\" and ranges from 80 to 160.\n- The blue line with x markers represents the Base model, the orange line with circular markers represents the Large model, and the green line with triangular markers represents the Huge model. \n\nThe lines demonstrate a positive trend, indicating that as the number of images used in pre-training increases, so does the CIDEr score, with the Huge model achieving the highest scores across the range.\nThe image is a line graph showing the relationship between the number of million images in pre-training (on the x-axis, logarithmic scale) and the CIDEr score (on the y-axis). It has three lines representing different models or configurations: \n\n- Blue line with \"Base\"\n- Yellow line with \"Large\"\n- Green line with \"Huge\"\n\nAll lines show an upward trend, indicating that as the number of pre-training images increases, the CIDEr score also increases.\nThe image is a bar chart comparing the performance of two models, OSCAR_B and MiniVLM (ours), across five different tasks: Caption, VQA, TR, IR, and NLVR2. The y-axis represents a performance metric, while the x-axis lists the tasks. Each task has two bars: blue for OSCAR_B and red for MiniVLM. In the chart:\n\n- For the Caption task, OSCAR_B scores 100% while MiniVLM scores 97%.\n- For the VQA task, OSCAR_B scores 100% while MiniVLM scores 94%.\n- For the TR task, OSCAR_B scores 100% while MiniVLM scores 96%.\n- For the IR task, OSCAR_B scores 100% while MiniVLM scores 95%.\n- For the NLVR2 task, OSCAR_B scores 100% while MiniVLM scores 94%.\n\nOverall, OSCAR_B outperforms MiniVLM, achieving 100% in all tasks compared to the slightly lower scores of MiniVLM.\nThe image is a bar graph comparing the performance of OSCarB and MiniVLM across different tasks: Caption, VQA, TR, IR, and NLVR2. The y-axis represents the score, ranging from 0 to 140. OSCarB is shown in blue, and MiniVLM is in orange. OSCarB scores higher in the Caption task, while the two models have similar scores in the other tasks.\nPrompt:  Can you double-check the code and align it better  with the reference image? \nNote: Code from the above GPT-4V  response is omitted to save space \nThe image is a bar chart comparing the performance of two models: OSCARb and MiniVLM, across five different tasks. The tasks are Caption, VQA, TR, IR, and NLVR2. The scores for MiniVLM are represented by teal bars, while the scores for OSCARb are represented by red bars. The exact scores are listed at the top of each bar for both models.\n\nHere's a breakdown of the scores for each task:\n\n1. **Caption**: \n   - OSCARb: 120%\n   - MiniVLM: 100%\n\n2. **VQA** (Visual Question Answering):\n   - OSCARb: 100%\n   - MiniVLM: 100%\n\n3. **TR** (Text Retrieval):\n   - OSCARb: 100%\n   - MiniVLM: 94%\n\n4. **IR** (Image Retrieval):\n   - OSCARb: 100%\n   - MiniVLM: 95%\n\n5. **NLVR2** (Natural Language for Visual Reasoning):\n   - OSCARb: 90%\n   - MiniVLM: 94%\n\nThe chart indicates that OscarB performs better in the Caption task, with a 20% higher score than MiniVLM. Both models perform equally well in the VQA task. However, MiniVLM outperforms OscarB in the NLVR2 task. For TR and IR tasks, OSCARb has a slight advantage over MiniVLM.\nFigure 122: Illustration of using self-reflection to improve the code for figure drawing. Left: after reflection, the number of points in the curve aligns with the reference image. Right: the percentage is added to align with the reference image. Check Section  10.3  for detailed discussions. "}
{"page": 153, "image_path": "doc_images/2309.17421v2_153.jpg", "ocr_text": "Sec. 10.3 Self-Reflection\n\nSelf-Reflection to Improve Text-to-lmage Generation\n\nPrompt:\n\nInstruction: Given a user imagined IDEA of the scene,\nconverting the IDEA into a self-contained sentence\nprompt that will be used to generate an image.\n\nIDEA: photo of a dog looks like the one in the given\nimage running on the beach.\n\nBased on the above information, | wrote a detailed\nprompt exactly about the IDEA follow the rules.\nGPT-4V:\n\nA blue dog running on the beach with a happy\nexpression on its face.\n\nSDXL:\n\nPrompt:\n\n| am iteratively improving the sentence prompt by\nlooking at the images generated by an Al art\ngeneration model and find out what is different from\n\nthe given IDEA. The IDEA is: photo of a dog ... beach.\n\nGenerated sentence prompt for current round is: A\nblue dog running on the beach with a happy\nexpression on its face. Corresponding image\ngenerated by the Al art generation model:\n\nFigure 123: Illustration of using self-reflection to improve the generated text prompts for a text-to-\n\nPrompt Continued\n\nThe generated image is not perfect. Write key reasons\non how to further improve the image to better follow\nthe user imagined IDEA of the scene.\n\nGPT-4V:\n\nThe dog in the generated image does not look like the\none in the given image. The dog in the given image is\na pug, while the dog in the generated image is a\ndifferent breed. To address this issue, we can modify\nthe prompt to specifically mention that the dog should\nbe a pug.\n\nPrompt:\n\nInstruction: Given a user imagined IDEA of the scene,\nconverting the IDEA into a sentence prompt that will\nbe used to generate an image. | am iteratively\nimproving the sentence prompt by looking at the\nimages generated by an Al art generation model and\nfind out what is different from the given IDEA. The\nIDEA is: photo of a dog ... beach. Generated sentence\nprompt for current round is: A blue ... its face.\nCorresponding image generated by the Al art\ngeneration model:\n\nThe image can be improved because: The dog ...\nshould be a pug. Based on the above information, to\nimprove the image, | wrote a detailed prompt exactly\nabout the IDEA follow the rules.\n\nGPT-4V:\n\nA blue pug running on the beach with a happy\nexpression on its face, with a sunset and palm trees in\nthe background.\n\nSDXL:\n\nimage model SDXL [106]. GPT-4V reflects the error in the initial prompt that it does not mention the\ndog’s breed, and makes the correct revision. Check Section 10.3 for detailed discussions.\n\n154\n", "vlm_text": "Self-Reflection to Improve Text-to-Image Generation \nPrompt: \nInstruction: Given a user imagined IDEA of the scene,  converting the IDEA into a self-contained sentence  prompt that will be used to generate an image. IDEA: photo of a dog looks like the one in the given  image running on the beach. \nThe image shows a pug with large, expressive eyes looking directly at the camera. The background appears to be a paved surface.\nThe image shows a dog with blue fur running on a beach, near the water, during what appears to be sunset or sunrise. The dog has a happy expression with its mouth open and tongue out, and it is wearing a collar. The background shows waves in the ocean and a cloudy sky, adding to the dynamic feel of the image. The colors have been edited to give the dog an unnatural blue hue, which is not typical for a real dog.\nPrompt: \nI am iterative ly improving the sentence prompt by  looking at the images generated by an AI art  generation model and find out what is different from  the given IDEA. The IDEA is: photo of a dog … beach.  Generated sentence prompt for current round is: A  blue dog running on the beach with a happy  expression on its face. Corresponding image  generated by the AI art generation model: \nThe image shows a dog with blue fur running on a beach. The dog appears to be happy and in motion, with its ears flapping backward. The background includes a glimpse of the ocean with waves and a partly cloudy sky.\nPrompt Continued \nThe generated image is not perfect. Write key reasons  on how to further improve the image to better follow  the user imagined IDEA of the scene. \nGPT-4V: \nThe dog in the generated image does not look like the  one in the given image. The dog in the given image is  a pug, while the dog in the generated image is a  different breed. To address this issue, we can modify  the prompt to specifically mention that the dog should  be a pug. \nPrompt: \nInstruction: Given a user imagined IDEA of the scene,  converting the IDEA into a sentence prompt that will  be used to generate an image. I am iterative ly  improving the sentence prompt by looking at the  images generated by an AI art generation model and  find out what is different from the given IDEA. The  IDEA is: photo of a dog … beach. Generated sentence  prompt for current round is: A blue … its face.  Corresponding image generated by the AI art  generation model: \nThe image shows a dog with blue fur running on a beach. The dog appears to be joyful and is wearing a collar. The background features a sandy shore and an ocean under a cloudy sky, likely during a sunset or sunrise. The blue coloring of the dog suggests that the image might be digitally altered or artistically edited.\nThe image can be improved because: The dog …  should be a pug. Based on the above information, to  improve the image, I wrote a detailed prompt exactly  about the IDEA follow the rules. \nGPT-4V: \nA blue pug running on the beach with a happy  expression on its face, with a sunset and palm trees in  the background. \nSDXL: \nThe image depicts a happy pug running on a beach with palm trees in the background during sunset. The pug appears animated or digitally illustrated, and the scene conveys a cheerful and relaxed atmosphere.\nFigure 123: Illustration of using self-reflection to improve the generated text prompts for a text-to- image model SDXL [ 106 ]. GPT-4V reflects the error in the initial prompt that it does not mention the dog’s breed, and makes the correct revision. Check Section  10.3  for detailed discussions. "}
{"page": 154, "image_path": "doc_images/2309.17421v2_154.jpg", "ocr_text": "Sec. 10.4 Self-Consistency\n\nSelf-Consistency for Counting\n\nSample 1\n\nPrompt:\nCount the number of boats in the image.\n\nSample 3\n\nPrompt:\nCount the number of boats in the image. Describe\neach of their locations.\n\nNote: Same query image\nas shown in Sample 1,\nresized to save space\n\nGPT-4V:\n\nThere are 4 boats in the image. 1) The first boat is on\nthe left side of the image, docked at the pier. Itis a\nlarger boat with a tower on top. 2) The second boat is\nin the center of the image, also docked at the pier. It is\na smaller boat with a white canopy on top. 3) The third\nboat is on the right side of the image, docked at the\npier. It is a smaller boat with a white canopy on top,\nsimilar to the second boat. 4) The fourth boat is in the\nbackground, on the far right side of the image. It is\npartially obscured by the third boat, but appears to be\na larger boat with a tower on top.\n\nSelf Consistency:\nSample 2: There are a total of 4 boats in the image. 1) The first boat is on the left side of the image\nSample 3: There are 4 boats in the image. 1) The first boat is on the left side of the image, docked\nSample 4: There are 6 boats in the image. - 1 boat is on the left side of the image, docked at the pier\n[Marginalize out reasoning paths to aggregate final answers, e.g., majority vote] =>\n\nThere are 4 boats in the image.\n\nSample 2\n\nPrompt:\nCount the number of boats in the image. Describe\neach of their locations.\n\nNote: Same query image\nas shown in Sample 1,\nresized to save space\n\nGPT-4V:\n\nThere are a total of 4 boats in the image. 1) The first\nboat is on the left side of the image, docked at the\npier. It is a larger boat with a tower on top. 2) The\nsecond boat is in the center of the image, also docked\nat the pier. It is a smaller boat with a white canopy on\ntop. 3) The third boat is on the right side of the image,\ndocked at the pier. It is a smaller boat with a white\ncanopy on top, similar to the second boat. 4) The\nfourth boat is in the background, on the far right side of\nthe image. It is a sailboat, and it appears to be sailing\non the water.\n\nSample 4\n\nPrompt:\nCount the number of boats in the image. Describe\neach of their locations.\n\nNote: Same query image\nas shown in Sample 1,\nresized to save space\n\nGPT-4V:\n\n- 1 boat is on the left\nside of the image, docked at the pier with a tower on\ntop - 2 boats are in the center of the image, docked at\nthe pier side by side - 1 boat is on the right side of the\nimage, docked at the pier with a cover on top - 2 boats\nare in the background on the right side of the image,\ndocked at a different pier.\n\nFigure 124: Improve the counting reliability with self-consistency [130], which aggregates multiple\ncounting results repeated on the same image. Check Section 10.4 for detailed discussions.\n\n10.4 Self-Consistency\n\nSelf-consistency [130] is a decoding strategy that aggregates multiple sampled outputs to produce\nthe final answer, such as with the majority vote. Extended from marginalizing to aggregating final\nanswers, Tree-of-Thoughts [144] shows that the self-consistency idea can be applied to intermediate\nthoughts to improve the LLM reasoning performance. Figure 124 illustrates the use of self-consistency\non GPT-4V for counting problems. We sample multiple counting results by asking GPT-4V to count\n\n155\n", "vlm_text": "Sec. 10.4 Self-Consistency \nSelf-Consistency for Counting \nSample 1 \nPrompt: \nCount the number of boats in the image. \nThe image shows a marina at sunset. Several boats are docked, and the sky is filled with dramatic clouds and vibrant colors from the setting sun, creating a picturesque scene. The reflection of the sky on the water adds to the beauty of the image.\nGPT-4V: There are 6 boats in the image. \nSample 3 \nPrompt: \nCount the number of boats in the image. Describe  each of their locations. \nThe image depicts a scenic view of a marina during sunset. Several boats are docked, and the sky is filled with vibrant colors including orange, yellow, and blue, reflecting on the water.\nNote: Same query image  as shown in Sample 1,  resized to save space \nGPT-4V: \nThere are 4 boats in the image. 1) The first boat is on  the left side of the image, docked at the pier. It is a  larger boat with a tower on top. 2) The second boat is  in the center of the image, also docked at the pier. It is  a smaller boat with a white canopy on top. 3) The third  boat is on the right side of the image, docked at the  pier. It is a smaller boat with a white canopy on top,  similar to the second boat. 4) The fourth boat is in the  background, on the far right side of the image. It is  partially obscured by the third boat, but appears to be  a larger boat with a tower on top. \nSample 2 \nPrompt: \nCount the number of boats in the image. Describe  each of their locations. \nThe image depicts a marina at sunset. There are several boats docked in the harbor, and the sky is filled with vibrant colors from the setting sun, creating a picturesque and serene scene.\nNote: Same query image  as shown in Sample 1,  resized to save space \nGPT-4V: \nThere are a total of 4 boats in the image. 1) The first  boat is on the left side of the image, docked at the  pier. It is a larger boat with a tower on top. 2) The  second boat is in the center of the image, also docked  at the pier. It is a smaller boat with a white canopy on  top. 3) The third boat is on the right side of the image,  docked at the pier. It is a smaller boat with a white  canopy on top, similar to the second boat. 4) The  fourth boat is in the background, on the far right side of  the image. It is a sailboat, and it appears to be sailing  on the water. \nSample 4 \nPrompt: \nCount the number of boats in the image. Describe  each of their locations. \nThe image shows a picturesque scene of a marina during sunset. Several boats are docked at the pier, and the sky is filled with dramatic clouds, illuminated by the vibrant colors of the setting sun. The reflection of the sunset and clouds can be seen on the water, enhancing the serene and scenic atmosphere of the image.\nNote: Same query image  as shown in Sample 1,  resized to save space \nGPT-4V: \nThere are 6 boats in the image. - 1 boat is on the left  side of the image, docked at the pier with a tower on  top - 2 boats are in the center of the image, docked at  the pier side by side - 1 boat is on the right side of the  image, docked at the pier with a cover on top - 2 boats  are in the background on the right side of the image,  docked at a different pier. \nSelf Consistency: \nSample 2:  There are a total of 4 boats in the image. 1) The first boat is on the left side of the image …… Sample 3:  There are 4 boats in the image. 1) The first boat is on the left side of the image, docked  …… Sample 4:  There are 6 boats in the image. - 1 boat is on the left side of the image, docked at the pier …… [Marginal ize out reasoning paths to aggregate final answers, e.g.,  majority vote ]  $=>$  \nThere are 4 boats in the image. \nFigure 124: Improve the counting reliability with self-consistency [ 130 ], which aggregates multiple counting results repeated on the  same  image. Check Section  10.4  for detailed discussions. \n10.4 Self-Consistency \nSelf-consistency [ 130 ] is a decoding strategy that aggregates multiple sampled outputs to produce the final answer, such as with the majority vote. Extended from marginal i zing to aggregating final answers, Tree-of-Thoughts [ 144 ] shows that the self-consistency idea can be applied to intermediate thoughts to improve the LLM reasoning performance. Figure  124  illustrates the use of self-consistency on GPT-4V for counting problems. We sample multiple counting results by asking GPT-4V to count the same image multiple times, either conducting multiple runs (Samples 2-4) or rephrasing the input text instruction (Samples 1,2). The example then uses the simple majority vote to aggregate the final answer of “4 boats.” We leave the comprehensive explorations of self-consistency LMMs to future works. "}
{"page": 155, "image_path": "doc_images/2309.17421v2_155.jpg", "ocr_text": "the same image multiple times, either conducting multiple runs (Samples 2-4) or rephrasing the input\ntext instruction (Samples 1,2). The example then uses the simple majority vote to aggregate the final\nanswer of “4 boats.” We leave the comprehensive explorations of self-consistency LMMs to future\nworks.\n\n10.5 Retrieval-Augmented LMMs\n\nRetrieval-Augmented LMMs [93, 68, 50, 19, 115, 104] enhances text generation by retrieving and\nintegrating relevant information into prompts. The technique is particularly effective when specialized\ntask-relevant information is needed, such as expert knowledge in a highly-specialized expert domain,\nthe most recent information that may differ from LLMs’ memory, and the customizable information\nthat varies from user to user. We imagine retrieval augmentation continues to play an essential role in\nLMMs. Figure 74 shows an example of retrieval-augmented LMMs helping grocery checkout. Since\nthe produces’ image-text-price triplets are different in each store, it would be beneficial to retrieve\nthem from the store’s database and yield the correct checkout information. Similarly, in Figure 81’s\nthe customized captioning scenario, we imagine the system may automatically retrieve the family\nmembers’ photos from the album and achieve the customized captioning.\n\n11 Conclusions\n\n11.1. Summary and Conclusions\n\nIn this report, our primary focus is on probing GPT-4V across various application scenarios. The\nfindings reveal its remarkable capabilities, some of which have not been investigated or demonstrated\nin existing approaches. While we strive to uncover as many of these capabilities as possible, we\nacknowledge that our presentation may not be exhaustive. Nevertheless, this report can serve\nas a reference for future research aimed at exploring additional uses of GPT-4V, deepening the\nunderstanding of LMMs, and building even more powerful LMMs.\n\n11.2. Towards Future LMMs\n\nThe weaknesses and limitations of GPT models have been extensively discussed in related reports [99,\n100, 24]. In this section, we briefly focus on presenting our perspective on future research directions.\n\nModels like GPT-1, GPT-2, and GPT-3 function primarily as text-in-text-out systems, capable of\nprocessing natural language only. GPT-4 (no vision) demonstrates unparalleled competence in text\nunderstanding and generation, while GPT-4V exhibits a strong ability to comprehend the image\ndomain as well.\n\nAs a natural progression, LMMs should be able to generate interleaved image-text content, such\nas producing vivid tutorials containing both text and images, to enable comprehensive multimodal\ncontent understanding and generation. Additionally, it would be beneficial to incorporate other\nmodalities, such as video, audio, and other sensor data, to expand the capabilities of LMMs.\n\nRegarding the learning process, current approaches predominantly rely on well-organized data, such\nas image-tag or image-text datasets. However, a more versatile model may be able to learn from\nvarious sources, including online web content and even real-world physical environments, to facilitate\ncontinuous self-evolution.\n\nAcknowledgment\n\nWe express our gratitude to all contributors from OpenAI for their technical efforts on the GPT-4V\nproject [99-101, 1], and we are profoundly thankful to OpenAI for granting early access to their\nremarkable tool. Our sincere appreciation goes to Misha Bilenko for his invaluable guidance and\nsupport. We also extend heartfelt thanks to our Microsoft colleagues for their insights, with special\nacknowledgment to John Montgomery, Marco Casalaina, Gregory Buehrer, Nguyen Bach, Gopi\nKumar, Luis Vargas, Kun Wu, Meenaz Merchant, Jianfeng Gao, Matt Lungren, Sheela Agarwal,\nYumao Lu, Thomas Soemo, Fisayo Okikiolu, Ce Liu, Michael Zeng, Faisal Ahmed, Ehsan Azarnasab,\nand Lin Liang for their constructive feedback. We also thank Yingkai Yu for helping to create\nscreenshots on GUI Navigation.\n\n156\n", "vlm_text": "\n10.5 Retrieval-Augmented LMMs \nRetrieval-Augmented LMMs [ 93 ,  68 ,  50 ,  19 ,  115 ,  104 ] enhances text generation by retrieving and integrating relevant information into prompts. The technique is particularly effective when specialized task-relevant information is needed, such as expert knowledge in a highly-specialized expert domain, the most recent information that may differ from LLMs’ memory, and the customizable information that varies from user to user. We imagine retrieval augmentation continues to play an essential role in LMMs. Figure  74  shows an example of retrieval-augmented LMMs helping grocery checkout. Since the produces’ image-text-price triplets are different in each store, it would be beneficial to retrieve them from the store’s database and yield the correct checkout information. Similarly, in Figure  81 ’s the customized captioning scenario, we imagine the system may automatically retrieve the family members’ photos from the album and achieve the customized captioning. \n11 Conclusions \n11.1 Summary and Conclusions \nIn this report, our primary focus is on probing GPT-4V across various application scenarios. The findings reveal its remarkable capabilities, some of which have not been investigated or demonstrated in existing approaches. While we strive to uncover as many of these capabilities as possible, we acknowledge that our presentation may not be exhaustive. Nevertheless, this report can serve as a reference for future research aimed at exploring additional uses of GPT-4V, deepening the understanding of LMMs, and building even more powerful LMMs. \n11.2 Towards Future LMMs \nThe weaknesses and limitations of GPT models have been extensively discussed in related reports [ 99 , 100 ,  24 ]. In this section, we briefly focus on presenting our perspective on future research directions. \nModels like GPT-1, GPT-2, and GPT-3 function primarily as text-in-text-out systems, capable of processing natural language only. GPT-4 (no vision) demonstrates unparalleled competence in text understanding and generation, while GPT-4V exhibits a strong ability to comprehend the image domain as well. \nAs a natural progression, LMMs should be able to generate interleaved image-text content, such as producing vivid tutorials containing both text and images, to enable comprehensive multimodal content understanding and generation. Additionally, it would be beneficial to incorporate other modalities, such as video, audio, and other sensor data, to expand the capabilities of LMMs. \nRegarding the learning process, current approaches predominantly rely on well-organized data, such as image-tag or image-text datasets. However, a more versatile model may be able to learn from various sources, including online web content and even real-world physical environments, to facilitate continuous self-evolution. \nAcknowledgment \nWe express our gratitude to all contributors from OpenAI for their technical efforts on the GPT-4V project [ 99 – 101 ,  1 ], and we are profoundly thankful to OpenAI for granting early access to their remarkable tool. Our sincere appreciation goes to Misha Bilenko for his invaluable guidance and support. We also extend heartfelt thanks to our Microsoft colleagues for their insights, with special acknowledgment to John Montgomery, Marco Casalaina, Gregory Buehrer, Nguyen Bach, Gopi Kumar, Luis Vargas, Kun Wu, Meenaz Merchant, Jianfeng Gao, Matt Lungren, Sheela Agarwal, Yumao Lu, Thomas Soemo, Fisayo Okikiolu, Ce Liu, Michael Zeng, Faisal Ahmed, Ehsan Azarnasab, and Lin Liang for their constructive feedback. We also thank Yingkai Yu for helping to create screenshots on GUI Navigation. "}
{"page": 156, "image_path": "doc_images/2309.17421v2_156.jpg", "ocr_text": "References\n\n1\n\nun & WwW WN\n\n[16]\n\n[17]\n\n[18]\n\nChatgpt can now see, hear, and_ speak. https://openai.com/blog/\nchatgpt-can-now-see-hear-and- speak, 2023.\n\nDeepfloyd if. https: //github.com/deep-floyd/IF, 2023.\nGuidance. https: //github.com/microsoft/guidance/, 2023.\nMidjourney. https: //www.midjourney.com/, 2023.\n\nSameer Agarwal, Yasutaka Furukawa, Noah Snavely, Ian Simon, Brian Curless, Steven M Seitz,\nand Richard Szeliski. Building rome in a day. Communications of the ACM, 54(10):105-112,\n2011.\n\nMichael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David,\nChelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, et al. Do as i can, not\nas i say: Grounding language in robotic affordances. arXiv preprint arXiv:2204.01691, 2022.\n\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson,\nKarel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual\nlanguage model for few-shot learning. Advances in Neural Information Processing Systems,\n35:23716-23736, 2022.\n\nChris Alberti, Jeffrey Ling, Michael Collins, and David Reitter. Fusion of detected objects in\ntext for visual question answering. In EMNLP, 2019.\n\nPeter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould,\nand Lei Zhang. Bottom-up and top-down attention for image captioning and visual question\nanswering. In CVPR, 2018.\n\nKarpathy Andrej. State of gpt. https: //karpathy.ai/stateofgpt.pdf, 2023.\n\nRohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos,\nSiamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report.\narXiv preprint arXiv:2305.10403, 2023.\n\nStanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C. Lawrence\nZitnick, and Devi Parikh. VQA: Visual Question Answering. In ICCV, 2015.\n\nAnas Awadalla, Irena Gao, Joshua Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani\nMarathe, Yonatan Bitton, Samir Gadre, Jenia Jitsev, Simon Kornblith, Pang Wei Koh, Gabriel\nIlharco, Mitchell Wortsman, and Ludwig Schmidt. Openflamingo, March 2023.\n\nHessam Bagherinezhad, Hannaneh Hajishirzi, Yejin Choi, and Ali Farhadi. Are elephants big-\nger than butterflies? reasoning about sizes of objects. In Proceedings of the AAAI Conference\non Artificial Intelligence, volume 30, 2016.\n\nShruthi Bannur, Stephanie Hyland, Qianchu Liu, Fernando Perez-Garcia, Maximilian Ilse,\nDaniel C Castro, Benedikt Boecking, Harshita Sharma, Kenza Bouzid, Anja Thieme, et al.\nLearning to exploit temporal structure for biomedical vision-language processing. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages\n15016-15027, 2023.\n\nDavid Barrett, Felix Hill, Adam Santoro, Ari Morcos, and Timothy Lillicrap. Measuring\nabstract reasoning in neural networks. In International conference on machine learning, pages\n511-520. PMLR, 2018.\n\nAli Furkan Biten, Ruben Tito, Andres Mafla, Lluis Gomez, Margal Rusinol, Ernest Valveny,\nCV Jawahar, and Dimosthenis Karatzas. Scene text visual question answering. In ICCV, 2019.\n\nKevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and Sergey Levine. Training diffusion\nmodels with reinforcement learning, 2023.\n\n157\n", "vlm_text": "References \n[1]  Chatgpt can now see, hear, and speak. https://openai.com/blog/ chatgpt-can-now-see-hear-and-speak , 2023.\n\n \n[2] Deepfloyd if.  https://github.com/deep-floyd/IF , 2023.\n\n [3] Guidance.  https://github.com/microsoft/guidance/ , 2023.\n\n [4] Midjourney.  https://www.midjourney.com/ , 2023.\n\n [5]  Sameer Agarwal, Yasutaka Furukawa, Noah Snavely, Ian Simon, Brian Curless, Steven M Seitz, and Richard Szeliski. Building rome in a day.  Communications of the ACM , 54(10):105–112, 2011.\n\n [6]  Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Keerthana Gopal a krishna n, Karol Hausman, Alex Herzog, et al. Do as i can, not as i say: Grounding language in robotic afford ances.  arXiv preprint arXiv:2204.01691 , 2022.\n\n [7]  Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning.  Advances in Neural Information Processing Systems , 35:23716–23736, 2022.\n\n [8]  Chris Alberti, Jeffrey Ling, Michael Collins, and David Reitter. Fusion of detected objects in text for visual question answering. In  EMNLP , 2019.\n\n [9]  Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and Lei Zhang. Bottom-up and top-down attention for image captioning and visual question answering. In  CVPR , 2018.\n\n [10] Karpathy Andrej. State of gpt.  https://karpathy.ai/stateofgpt.pdf , 2023.\n\n [11]  Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. arXiv preprint arXiv:2305.10403 , 2023.\n\n [12]  Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C. Lawrence Zitnick, and Devi Parikh. VQA: Visual Question Answering. In  ICCV , 2015.\n\n [13]  Anas Awadalla, Irena Gao, Joshua Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Jenia Jitsev, Simon Kornblith, Pang Wei Koh, Gabriel Ilharco, Mitchell Wortsman, and Ludwig Schmidt. Open flamingo, March 2023.\n\n [14]  Hessam Bag her inez had, Hannaneh Hajishirzi, Yejin Choi, and Ali Farhadi. Are elephants big- ger than butterflies? reasoning about sizes of objects. In  Proceedings of the AAAI Conference on Artificial Intelligence , volume 30, 2016.\n\n [15]  Shruthi Bannur, Stephanie Hyland, Qianchu Liu, Fernando Perez-Garcia, Maximilian Ilse, Daniel C Castro, Benedikt Boecking, Harshita Sharma, Kenza Bouzid, Anja Thieme, et al. Learning to exploit temporal structure for biomedical vision-language processing. In  Pro- ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 15016–15027, 2023.\n\n [16]  David Barrett, Felix Hill, Adam Santoro, Ari Morcos, and Timothy Lillicrap. Measuring abstract reasoning in neural networks. In  International conference on machine learning , pages 511–520. PMLR, 2018.\n\n [17]  Ali Furkan Biten, Ruben Tito, Andres Mafla, Lluis Gomez, Marçal Rusinol, Ernest Valveny, CV Jawahar, and Di most hen is Karatzas. Scene text visual question answering. In  ICCV , 2019.\n\n [18]  Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and Sergey Levine. Training diffusion models with reinforcement learning, 2023. "}
{"page": 157, "image_path": "doc_images/2309.17421v2_157.jpg", "ocr_text": "[19\n\n[20\n\n[21\n\n[22\n\n[23\n\n[24\n\n[25\n\n[26\n\n[27\n\n[28\n\n[29\n\n[30\n\n[31\n\n[32\n\n[33\n[34\n\n[35\n\nSebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie\nMillican, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark,\net al. Improving language models by retrieving from trillions of tokens. In /nternational\nconference on machine learning, pages 2206-2240. PMLR, 2022.\n\nLukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101—mining discriminative\ncomponents with random forests. In Computer Vision—ECCV 2014: 13th European Conference,\nZurich, Switzerland, September 6-12, 2014, Proceedings, Part VI 13, pages 446-461. Springer,\n2014.\n\nMarc A Brackett and Peter Salovey. Measuring emotional intelligence with the mayer-salovery-\ncaruso emotional intelligence test (msceit). Psicothema, 18:34—41, 2006.\n\nTim Brooks, Aleksander Holynski, and Alexei A. Efros. Instructpix2pix: Learning to follow\nimage editing instructions. In CVPR, 2023.\n\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-\nwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language\nmodels are few-shot learners. In NeurIPS, 2020.\n\nSébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece\nKamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general\nintelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023.\n\nTing Chen, Saurabh Saxena, Lala Li, David J Fleet, and Geoffrey Hinton. Pix2seq: A language\nmodeling framework for object detection. In JCLR, 2022.\n\nTing Chen, Saurabh Saxena, Lala Li, Tsung-Yi Lin, David J Fleet, and Geoffrey E Hinton.\nA unified sequence interface for vision tasks. Advances in Neural Information Processing\nSystems, 35:31333-31346, 2022.\n\nXinlei Chen, Hao Fang, Tsung- Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollar,\nand C Lawrence Zitnick. Microsoft coco captions: Data collection and evaluation server. arXiv\npreprint arXiv: 1504.00325, 2015.\n\nYen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng,\nand Jingjing Liu. Uniter: Learning universal image-text representations. In ECCV, 2020.\n\nJaemin Cho, Jie Lei, Hao Tan, and Mohit Bansal. Unifying vision-and-language tasks via text\ngeneration. In JCML, 2021.\n\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\nRoberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm:\nScaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.\n\nHerbert H Clark and Deanna Wilkes-Gibbs. Referring as a collaborative process. Cognition,\n22(1):1-39, 1986.\n\nMarius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo\nBenenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic\nurban scene understanding. In Proceedings of the IEEE conference on computer vision and\npattern recognition, pages 3213-3223, 2016.\n\nTom Cornsweet. Visual perception. Academic press, 2012.\n\nDamai Dai, Yutao Sun, Li Dong, Yaru Hao, Zhifang Sui, and Furu Wei. Why can gpt learn\nin-context? language models secretly perform gradient descent as meta optimizers. arXiv\npreprint arXiv:2212.10559, 2022.\n\nWenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng\n\nWang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose\nvision-language models with instruction tuning. arXiv preprint arXiv:2305.06500, 2023.\n\n158\n", "vlm_text": "[19]  Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al. Improving language models by retrieving from trillions of tokens. In  International conference on machine learning , pages 2206–2240. PMLR, 2022.\n\n [20]  Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101–mining disc rim i native components with random forests. In  Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part VI 13 , pages 446–461. Springer, 2014.\n\n [21]  Marc A Brackett and Peter Salovey. Measuring emotional intelligence with the mayer-salovery- caruso emotional intelligence test (msceit).  Psicothema , 18:34–41, 2006.\n\n [22]  Tim Brooks, Aleksander Holynski, and Alexei A. Efros. Instruct pix 2 pix: Learning to follow image editing instructions. In  CVPR , 2023.\n\n [23]  Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari- wal, Arvind Neel a kant an, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. In  NeurIPS , 2020.\n\n [24]  Sébastien Bubeck, Varun Chandra sekar an, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence: Early experiments with gpt-4.  arXiv preprint arXiv:2303.12712 , 2023.\n\n [25]  Ting Chen, Saurabh Saxena, Lala Li, David J Fleet, and Geoffrey Hinton. Pix2seq: A language modeling framework for object detection. In  ICLR , 2022.\n\n [26]  Ting Chen, Saurabh Saxena, Lala Li, Tsung-Yi Lin, David J Fleet, and Geoffrey E Hinton. A unified sequence interface for vision tasks.  Advances in Neural Information Processing Systems , 35:31333–31346, 2022.\n\n [27]  Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollár, and C Lawrence Zitnick. Microsoft coco captions: Data collection and evaluation server.  arXiv preprint arXiv:1504.00325 , 2015.\n\n [28]  Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. Uniter: Learning universal image-text representations. In  ECCV , 2020.\n\n [29]  Jaemin Cho, Jie Lei, Hao Tan, and Mohit Bansal. Unifying vision-and-language tasks via text generation. In  ICML , 2021.\n\n [30]  Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways.  arXiv preprint arXiv:2204.02311 , 2022.\n\n [31]  Herbert H Clark and Deanna Wilkes-Gibbs. Referring as a collaborative process.  Cognition , 22(1):1–39, 1986.\n\n [32]  Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In  Proceedings of the IEEE conference on computer vision and pattern recognition , pages 3213–3223, 2016.\n\n [33] Tom Cornsweet.  Visual perception . Academic press, 2012.\n\n [34]  Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Zhifang Sui, and Furu Wei. Why can gpt learn in-context? language models secretly perform gradient descent as meta optimizers.  arXiv preprint arXiv:2212.10559 , 2022.\n\n [35]  Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instruct blip: Towards general-purpose vision-language models with instruction tuning.  arXiv preprint arXiv:2305.06500 , 2023. "}
{"page": 158, "image_path": "doc_images/2309.17421v2_158.jpg", "ocr_text": "[36\n\n[37\n\n[38\n\n[39\n\n[40\n\n[41\n\n[42\n\n[43\n\n[44\n\n[45\n\n[46\n\n[47\n\n[48\n\n[49\n\n[50\n\n[51\n\nAbhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh, Deshraj Yadav, José MF Moura, Devi\nParikh, and Dhruv Batra. Visual dialog. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 326-335, 2017.\n\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale\nhierarchical image database. In CVPR, 2009.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of\ndeep bidirectional transformers for language understanding. In NAACL-HLT, 2019.\n\nQingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing\nXu, and Zhifang Sui. A survey for in-context learning. arXiv preprint arXiv:2301.00234,\n2022.\n\nZi-Yi Dou, Aishwarya Kamath, Zhe Gan, Pengchuan Zhang, Jianfeng Wang, Linjie Li, Zicheng\nLiu, Ce Liu, Yann LeCun, Nanyun Peng, et al. Coarse-to-fine vision-language pre-training\nwith fusion in the backbone. In Advances in Neural Information Processing Systems.\n\nZi-Yi Dou, Yichong Xu, Zhe Gan, Jianfeng Wang, Shuohang Wang, Lijuan Wang, Chenguang\nZhu, Pengchuan Zhang, Lu Yuan, Nanyun Peng, et al. An empirical study of training end-\nto-end vision-and-language transformers. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 18166-18176, 2022.\n\nDanny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian\nIchter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang, Yevgen\nChebotar, Pierre Sermanet, Daniel Duckworth, Sergey Levine, Vincent Vanhoucke, Karol\nHausman, Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch, and Pete Florence. Palm-e:\nAn embodied multimodal language model. In arXiv preprint arXiv:2303.03378, 2023.\n\nAlicia Fasquel, Angéle Brunelliére, and Dominique Knutsen. A modified procedure for\nnaming 332 pictures and collecting norms: Using tangram pictures in psycholinguistic studies.\nBehavior Research Methods, pages 1-23, 2022.\n\nSamir Yitzhak Gadre, Kiana Ehsani, and Shuran Song. Act the part: Learning interaction\nstrategies for articulated object part discovery. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, pages 15752-15761, 2021.\n\nZhe Gan, Yen-Chun Chen, Linjie Li, Chen Zhu, Yu Cheng, and Jingjing Liu. Large-scale\nadversarial training for vision-and-language representation learning. In NeurIPS, 2020.\n\nZhe Gan, Linjie Li, Chunyuan Li, Lijuan Wang, Zicheng Liu, Jianfeng Gao, et al. Vision-\nlanguage pre-training: Basics, recent advances, and future trends. Foundations and Trends®\nin Computer Graphics and Vision, 14(3-4):163-352, 2022.\n\nLuyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan,\nand Graham Neubig. Pal: Program-aided language models. In International Conference on\nMachine Learning, pages 10764-10799. PMLR, 2023.\n\nTao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang, Miao Zheng, Qian Zhao, Kuikun Liu,\nWenwei Zhang, Ping Luo, and Kai Chen. Multimodal-gpt: A vision and language model for\ndialogue with humans, 2023.\n\nYandong Guo, Lei Zhang, Yuxiao Hu, Xiaodong He, and Jianfeng Gao. Ms-celeb-1m: A\ndataset and benchmark for large-scale face recognition. In Computer Vision—ECCV 2016: 14th\nEuropean Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part\nIII 14, pages 87-102. Springer, 2016.\n\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. Retrieval\naugmented language model pre-training. In International conference on machine learning,\npages 3929-3938. PMLR, 2020.\n\nKaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Girshick. Mask r-cnn. In Proceedings\nof the IEEE international conference on computer vision, pages 2961-2969, 2017.\n\n159\n", "vlm_text": "[36]  Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh, Deshraj Yadav, José MF Moura, Devi Parikh, and Dhruv Batra. Visual dialog. In  Proceedings of the IEEE conference on computer vision and pattern recognition , pages 326–335, 2017.\n\n [37]  Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In  CVPR , 2009.\n\n [38]  Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In  NAACL-HLT , 2019.\n\n [39]  Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. A survey for in-context learning.  arXiv preprint arXiv:2301.00234 , 2022.\n\n [40]  Zi-Yi Dou, Aishwarya Kamath, Zhe Gan, Pengchuan Zhang, Jianfeng Wang, Linjie Li, Zicheng Liu, Ce Liu, Yann LeCun, Nanyun Peng, et al. Coarse-to-fine vision-language pre-training with fusion in the backbone. In  Advances in Neural Information Processing Systems .\n\n [41]  Zi-Yi Dou, Yichong Xu, Zhe Gan, Jianfeng Wang, Shuohang Wang, Lijuan Wang, Chenguang Zhu, Pengchuan Zhang, Lu Yuan, Nanyun Peng, et al. An empirical study of training end- to-end vision-and-language transformers. In  Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 18166–18176, 2022.\n\n [42]  Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duckworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch, and Pete Florence. Palm-e: An embodied multimodal language model. In  arXiv preprint arXiv:2303.03378 , 2023.\n\n [43]  Alicia Fasquel, Angèle Brunel li re, and Dominique Knutsen. A modified procedure for naming 332 pictures and collecting norms: Using tangram pictures in psycho linguistic studies. Behavior Research Methods , pages 1–23, 2022.\n\n [44]  Samir Yitzhak Gadre, Kiana Ehsani, and Shuran Song. Act the part: Learning interaction strategies for articulated object part discovery. In  Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 15752–15761, 2021.\n\n [45]  Zhe Gan, Yen-Chun Chen, Linjie Li, Chen Zhu, Yu Cheng, and Jingjing Liu. Large-scale adversarial training for vision-and-language representation learning. In  NeurIPS , 2020.\n\n [46]  Zhe Gan, Linjie Li, Chunyuan Li, Lijuan Wang, Zicheng Liu, Jianfeng Gao, et al. Vision- language pre-training: Basics, recent advances, and future trends.  Foundations and Trends® in Computer Graphics and Vision , 14(3–4):163–352, 2022.\n\n [47]  Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. Pal: Program-aided language models. In  International Conference on Machine Learning , pages 10764–10799. PMLR, 2023.\n\n [48]  Tao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang, Miao Zheng, Qian Zhao, Kuikun Liu, Wenwei Zhang, Ping Luo, and Kai Chen. Multimodal-gpt: A vision and language model for dialogue with humans, 2023.\n\n [49]  Yandong Guo, Lei Zhang, Yuxiao Hu, Xiaodong He, and Jianfeng Gao. Ms-celeb-1m: A dataset and benchmark for large-scale face recognition. In  Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part III 14 , pages 87–102. Springer, 2016.\n\n [50]  Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. Retrieval augmented language model pre-training. In  International conference on machine learning , pages 3929–3938. PMLR, 2020.\n\n [51]  Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick. Mask r-cnn. In  Proceedings of the IEEE international conference on computer vision , pages 2961–2969, 2017. "}
{"page": 159, "image_path": "doc_images/2309.17421v2_159.jpg", "ocr_text": "[52\n\n[53\n\n[54\n\n[55\n\n[56\n\n[57\n\n[58\n\n[59\n\n[60\n\n[61\n\n[62\n\n[63\n\n[64\n\n[65\n\n[66\n\n[67\n\nJack *Hessel, Jena D *Hwang, Jae Sung Park, Rowan Zellers, Chandra Bhagavatula, Anna\nRohrbach, Kate Saenko, and Yejin Choi. The Abduction of Sherlock Holmes: A Dataset for\nVisual Abductive Reasoning. In ECCV, 2022.\n\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza\nRutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al.\nTraining compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022.\n\nYushi Hu, Hang Hua, Zhengyuan Yang, Weijia Shi, Noah A Smith, and Jiebo Luo. Promptcap:\nPrompt-guided task-aware image captioning. In Proceedings of International Conference on\nComputer Vision (ICCV), 2023.\n\nShaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao\nLy, Lei Cui, Owais Khan Mohammed, Qiang Liu, et al. Language is not all you need: Aligning\nperception with language models. arXiv preprint arXiv:2302. 14045, 2023.\n\nWenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as\nzero-shot planners: Extracting actionable knowledge for embodied agents. In /nternational\nConference on Machine Learning, pages 9118-9147. PMLR, 2022.\n\nZhicheng Huang, Zhaoyang Zeng, Bei Liu, Dongmei Fu, and Jianlong Fu. Pixel-bert: Aligning\nimage pixels with text by deep multi-modal transformers. arXiv preprint arXiv:2004.00849,\n2020.\n\nFabian Hutmacher. Why is there so much more research on vision than on any other sensory\nmodality? Frontiers in psychology, 10:2246, 2019.\n\nAnya Ji, Noriyuki Kojima, Noah Rush, Alane Suhr, Wai Keen Vong, Robert Hawkins, and\nYoav Artzi. Abstract visual reasoning with tangram shapes. In Proceedings of the 2022\nConference on Empirical Methods in Natural Language Processing, pages 582-601, 2022.\n\nAlistair EW Johnson, Tom J Pollard, Seth J Berkowitz, Nathaniel R Greenbaum, Matthew P\nLungren, Chih-ying Deng, Roger G Mark, and Steven Horng. Mimic-cxr, a de-identified\npublicly available database of chest radiographs with free-text reports. Scientific data, 6(1):317,\n2019.\n\nJustin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, C Lawrence Zitnick,\nand Ross Girshick. Clevr: A diagnostic dataset for compositional language and elementary\nvisual reasoning. In Proceedings of the IEEE conference on computer vision and pattern\nrecognition, pages 2901-2910, 2017.\n\nJustin Johnson, Andrej Karpathy, and Li Fei-Fei. Densecap: Fully convolutional localization\nnetworks for dense captioning. In Proceedings of the IEEE conference on computer vision and\npattern recognition, pages 4565-4574, 2016.\n\nGeunwoo Kim, Pierre Baldi, and Stephen McAleer. Language models can solve computer\ntasks. arXiv preprint arXiv:2303.17491, 2023.\n\nWonjae Kim, Bokyung Son, and Ildoo Kim. Vilt: Vision-and-language transformer without\nconvolution or region supervision. In JCML, 2021.\n\nAlexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson,\nTete Xiao, Spencer Whitehead, Alexander C Berg, Wan- Yen Lo, et al. Segment anything. In\nProceedings of International Conference on Computer Vision (ICCV), 2023.\n\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa.\nLarge language models are zero-shot reasoners. Advances in neural information processing\nsystems, 35:22199-22213, 2022.\n\nRanjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie\n\nChen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting\nlanguage and vision using crowdsourced dense image annotations. L/CV, 2017.\n\n160\n", "vlm_text": "[52]  Jack \\*Hessel, Jena D \\*Hwang, Jae Sung Park, Rowan Zellers, Chandra Bhaga va tula, Anna Rohrbach, Kate Saenko, and Yejin Choi. The Abduction of Sherlock Holmes: A Dataset for Visual Abductive Reasoning. In  ECCV , 2022.\n\n [53]  Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buch at s kaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models.  arXiv preprint arXiv:2203.15556 , 2022.\n\n [54]  Yushi Hu, Hang Hua, Zhengyuan Yang, Weijia Shi, Noah A Smith, and Jiebo Luo. Promptcap: Prompt-guided task-aware image captioning. In  Proceedings of International Conference on Computer Vision (ICCV) , 2023.\n\n [55]  Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Qiang Liu, et al. Language is not all you need: Aligning perception with language models.  arXiv preprint arXiv:2302.14045 , 2023.\n\n [56]  Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. In  International Conference on Machine Learning , pages 9118–9147. PMLR, 2022.\n\n [57]  Zhicheng Huang, Zhaoyang Zeng, Bei Liu, Dongmei Fu, and Jianlong Fu. Pixel-bert: Aligning image pixels with text by deep multi-modal transformers.  arXiv preprint arXiv:2004.00849 , 2020.\n\n [58]  Fabian Hutmacher. Why is there so much more research on vision than on any other sensory modality?  Frontiers in psychology , 10:2246, 2019.\n\n [59]  Anya Ji, Noriyuki Kojima, Noah Rush, Alane Suhr, Wai Keen Vong, Robert Hawkins, and Yoav Artzi. Abstract visual reasoning with tangram shapes. In  Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing , pages 582–601, 2022.\n\n [60]  Alistair EW Johnson, Tom J Pollard, Seth J Berkowitz, Nathaniel R Greenbaum, Matthew P Lungren, Chih-ying Deng, Roger G Mark, and Steven Horng. Mimic-cxr, a de-identified publicly available database of chest radio graphs with free-text reports.  Scientific data , 6(1):317, 2019.\n\n [61]  Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, C Lawrence Zitnick, and Ross Girshick. Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. In  Proceedings of the IEEE conference on computer vision and pattern recognition , pages 2901–2910, 2017.\n\n [62]  Justin Johnson, Andrej Karpathy, and Li Fei-Fei. Densecap: Fully convolutional localization networks for dense captioning. In  Proceedings of the IEEE conference on computer vision and pattern recognition , pages 4565–4574, 2016.\n\n [63]  Geunwoo Kim, Pierre Baldi, and Stephen McAleer. Language models can solve computer tasks.  arXiv preprint arXiv:2303.17491 , 2023.\n\n [64]  Wonjae Kim, Bokyung Son, and Ildoo Kim. Vilt: Vision-and-language transformer without convolution or region supervision. In  ICML , 2021.\n\n [65]  Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of International Conference on Computer Vision (ICCV) , 2023.\n\n [66]  Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners.  Advances in neural information processing systems , 35:22199–22213, 2022.\n\n [67]  Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language and vision using crowd sourced dense image annotations.  IJCV , 2017. "}
{"page": 160, "image_path": "doc_images/2309.17421v2_160.jpg", "ocr_text": "[68\n\n[69\n\n[70\n\n[71\n\n[72\n\n[73\n\n[74\n\n[75\n\n[76\n\n(77\n\n[78\n\n[79\n\n[80\n\n[81\n\n[82\n\n[83\n\n[84\n\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Na-\nman Goyal, Heinrich Kiittler, Mike Lewis, Wen-tau Yih, Tim Rocktischel, et al. Retrieval-\naugmented generation for knowledge-intensive nlp tasks. Advances in Neural Information\nProcessing Systems, 33:9459-9474, 2020.\n\nChunyuan Li, Zhe Gan, Zhengyuan Yang, Jianwei Yang, Linjie Li, Lijuan Wang, and Jianfeng\nGao. Multimodal foundation models: From specialists to general-purpose assistants. arXiv\npreprint arXiv:2309. 10020, 2023.\n\nGen Li, Nan Duan, Yuejian Fang, Ming Gong, Daxin Jiang, and Ming Zhou. Unicoder-vl: A\nuniversal encoder for vision and language by cross-modal pre-training. In AAAI, 2020.\n\nJunnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-\nimage pre-training with frozen image encoders and large language models. arXiv preprint\narXiv:2301.12597, 2023.\n\nJunnan Li, Ramprasaath R Selvaraju, Akhilesh Deepak Gotmare, Shafiq Joty, Caiming Xiong,\nand Steven Hoi. Align before fuse: Vision and language representation learning with momen-\ntum distillation. In NeurIPS, 2021.\n\nLiunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. Visualbert: A\nsimple and performant baseline for vision and language. arXiv preprint arXiv: 1908.03557,\n2019.\n\nXiujun Li, Xi Yin, Chunyuan Li, Xiaowei Hu, Pengchuan Zhang, Lei Zhang, Lijuan Wang,\nHoudong Hu, Li Dong, Furu Wei, et al. Oscar: Object-semantics aligned pre-training for\nvision-language tasks. In ECCV, 2020.\n\nYaobo Liang, Chenfei Wu, Ting Song, Wenshan Wu, Yan Xia, Yu Liu, Yang Ou, Shuai Lu, Lei\nJi, Shaoguang Mao, et al. Taskmatrix. ai: Completing tasks by connecting foundation models\nwith millions of apis. arXiv preprint arXiv:2303.16434, 2023.\n\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan,\nPiotr Dollar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV,\n2014.\n\nFangyu Liu, Emanuele Bugliarello, Edoardo Maria Ponti, Siva Reddy, Nigel Collier, and\nDesmond Elliott. Visually grounded reasoning across languages and cultures. In Proceed-\nings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages\n10467-10485, Online and Punta Cana, Dominican Republic, November 2021. Association for\nComputational Linguistics.\n\nFuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang. Aligning\nlarge multi-modal model with robust instruction tuning. arXiv preprint arXiv:2306.14565,\n2023.\n\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv\npreprint arXiv:2304.08485, 2023.\n\nZiwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the\nwild. In Proceedings of International Conference on Computer Vision (ICCV), December\n2015.\n\nJiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic\nvisiolinguistic representations for vision-and-language tasks. In NeurIPS, 2019.\n\nJiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mottaghi, and Aniruddha Kembhavi.\nUnified-io: A unified model for vision, language, and multi-modal tasks. In The Eleventh\nInternational Conference on Learning Representations, 2022.\n\nJiasen Lu, Vedanuj Goswami, Marcus Rohrbach, Devi Parikh, and Stefan Lee. 12-in-1:\nMulti-task vision and language representation learning. In CVPR, 2020.\n\nJiasen Lu, Jianwei Yang, Dhruv Batra, and Devi Parikh. Neural baby talk. In CVPR, 2018.\n\n161\n", "vlm_text": "[68]  Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Na- man Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock t s chel, et al. Retrieval- augmented generation for knowledge-intensive nlp tasks.  Advances in Neural Information Processing Systems , 33:9459–9474, 2020.\n\n [69]  Chunyuan Li, Zhe Gan, Zhengyuan Yang, Jianwei Yang, Linjie Li, Lijuan Wang, and Jianfeng Gao. Multimodal foundation models: From specialists to general-purpose assistants.  arXiv preprint arXiv:2309.10020 , 2023.\n\n [70]  Gen Li, Nan Duan, Yuejian Fang, Ming Gong, Daxin Jiang, and Ming Zhou. Unicoder-vl: A universal encoder for vision and language by cross-modal pre-training. In  AAAI , 2020.\n\n [71]  Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Boots trapping language- image pre-training with frozen image encoders and large language models.  arXiv preprint arXiv:2301.12597 , 2023.\n\n [72]  Junnan Li, Ramp rasa a th R Selvaraju, Akhilesh Deepak Gotmare, Shafiq Joty, Caiming Xiong, and Steven Hoi. Align before fuse: Vision and language representation learning with momen- tum distillation. In  NeurIPS , 2021.\n\n [73]  Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. Visualbert: A simple and performant baseline for vision and language.  arXiv preprint arXiv:1908.03557 , 2019.\n\n [74]  Xiujun Li, Xi Yin, Chunyuan Li, Xiaowei Hu, Pengchuan Zhang, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, et al. Oscar: Object-semantics aligned pre-training for vision-language tasks. In  ECCV , 2020.\n\n [75]  Yaobo Liang, Chenfei Wu, Ting Song, Wenshan Wu, Yan Xia, Yu Liu, Yang Ou, Shuai Lu, Lei Ji, Shaoguang Mao, et al. Taskmatrix. ai: Completing tasks by connecting foundation models with millions of apis.  arXiv preprint arXiv:2303.16434 , 2023.\n\n [76]  Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In  ECCV , 2014.\n\n [77]  Fangyu Liu, Emanuele Bug liar ello, Edoardo Maria Ponti, Siva Reddy, Nigel Collier, and Desmond Elliott. Visually grounded reasoning across languages and cultures. In  Proceed- ings of the 2021 Conference on Empirical Methods in Natural Language Processing , pages 10467–10485, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.\n\n [78]  Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang. Aligning large multi-modal model with robust instruction tuning.  arXiv preprint arXiv:2306.14565 , 2023.\n\n [79]  Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning.  arXiv preprint arXiv:2304.08485 , 2023.\n\n [80] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In  Proceedings of International Conference on Computer Vision (ICCV) , December 2015.\n\n [81]  Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pre training task-agnostic vi sio linguistic representations for vision-and-language tasks. In  NeurIPS , 2019.\n\n [82]  Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mottaghi, and Aniruddha Kembhavi. Unified-io: A unified model for vision, language, and multi-modal tasks. In  The Eleventh International Conference on Learning Representations , 2022.\n\n [83]  Jiasen Lu, Vedanuj Goswami, Marcus Rohrbach, Devi Parikh, and Stefan Lee. 12-in-1: Multi-task vision and language representation learning. In  CVPR , 2020.\n\n [84] Jiasen Lu, Jianwei Yang, Dhruv Batra, and Devi Parikh. Neural baby talk. In  CVPR , 2018. "}
{"page": 161, "image_path": "doc_images/2309.17421v2_161.jpg", "ocr_text": "[85\n\n[86\n\n[87\n\n[88\n\n[89\n\n[90\n\n[91\n\n[92\n\n[93\n\n[94\n\n[95\n\n[96\n\n[97\n\n[98\n\n[99\n[100\n[101\n[102\n\nPan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind\nTafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought\nchains for science question answering. Advances in Neural Information Processing Systems,\n35:2507-2521, 2022.\n\nPan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu, Song-\nChun Zhu, and Jianfeng Gao. Chameleon: Plug-and-play compositional reasoning with large\nlanguage models. arXiv preprint arXiv:2304.09842, 2023.\n\nPan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter\nClark, and Ashwin Kalyan. Dynamic prompt learning via policy gradient for semi-structured\nmathematical reasoning. arXiv preprint arXiv:2209. 14610, 2022.\n\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe,\nUri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative\nrefinement with self-feedback. arXiv preprint arXiv:2303.17651, 2023.\n\nBertram F Malle, Louis J Moses, and Dare A Baldwin. Intentions and intentionality: Founda-\ntions of social cognition. MIT press, 2001.\n\nArjun Mani, Nobline Yoo, Will Hinthorn, and Olga Russakovsky. Point and ask: Incorporating\npointing into visual question answering. arXiv preprint arXiv:2011.13681, 2020.\n\nJohn D Mayer. Msceit: Mayer-salovey-caruso emotional intelligence test. Toronto, Canada:\nMulti-Health Systems, 2002.\n\nJohn D Mayer, Richard D Roberts, and Sigal G Barsade. Human abilities: Emotional intelli-\ngence. Annu. Rev. Psychol., 59:507-536, 2008.\n\nGrégoire Mialon, Roberto Dessi, Maria Lomeli, Christoforos Nalmpantis, Ram Pasunuru,\nRoberta Raileanu, Baptiste Roziére, Timo Schick, Jane Dwivedi-Yu, Asli Celikyilmaz, et al.\nAugmented language models: a survey. arXiv preprint arXiv:2302.07842, 2023.\n\nMicrosoft. Bing image search api. https: //www.microsoft.com/en-us/bing/\napis/bing-image-search-api, 2023.\n\nWeiqing Min, Zhiling Wang, Yuxin Liu, Mengjiang Luo, Liping Kang, Xiaoming Wei, Xiaolin\nWei, and Shuqiang Jiang. Large scale visual food recognition. [EEE Transactions on Pattern\nAnalysis and Machine Intelligence, 2023.\n\nSwaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. Cross-task general-\nization via natural language crowdsourcing instructions. In Proceedings of the 60th Annual\nMeeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages\n3470-3487, 2022.\n\nTara Murfitt and Jan McAllister. The effect of production variables in monolog and dialog on\ncomprehension by novel listeners. Language and Speech, 44(3):325-350, 2001.\n\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christo-\npher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted\nquestion-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021.\n\nOpenAI. Gpt-4 technical report, 2023.\nOpenAI. Gpt-4v(ision) system card. 2023.\nOpenAI. Gpt-4v(ision) technical work and authors. 2023.\n\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin,\nChong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models\nto follow instructions with human feedback. Advances in Neural Information Processing\nSystems, 35:27730-27744, 2022.\n\n162\n", "vlm_text": "[85]  Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering.  Advances in Neural Information Processing Systems , 35:2507–2521, 2022.\n\n [86]  Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu, Song- Chun Zhu, and Jianfeng Gao. Chameleon: Plug-and-play compositional reasoning with large language models.  arXiv preprint arXiv:2304.09842 , 2023.\n\n [87]  Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter Clark, and Ashwin Kalyan. Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning.  arXiv preprint arXiv:2209.14610 , 2022.\n\n [88]  Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback.  arXiv preprint arXiv:2303.17651 , 2023.\n\n [89]  Bertram F Malle, Louis J Moses, and Dare A Baldwin.  Intentions and intentional it y: Founda- tions of social cognition . MIT press, 2001.\n\n [90]  Arjun Mani, Nobline Yoo, Will Hinthorn, and Olga Russ a kov sky. Point and ask: Incorporating pointing into visual question answering.  arXiv preprint arXiv:2011.13681 , 2020.\n\n [91]  John D Mayer. Msceit: Mayer-salovey-caruso emotional intelligence test.  Toronto, Canada: Multi-Health Systems , 2002.\n\n [92]  John D Mayer, Richard D Roberts, and Sigal G Barsade. Human abilities: Emotional intelli- gence.  Annu. Rev. Psychol. , 59:507–536, 2008.\n\n [93]  Grégoire Mialon, Roberto Dessì, Maria Lomeli, Christof oros Nalmpantis, Ram Pasunuru, Roberta Raileanu, Baptiste Rozière, Timo Schick, Jane Dwivedi-Yu, Asli Cel i kyi l maz, et al. Augmented language models: a survey.  arXiv preprint arXiv:2302.07842 , 2023.\n\n [94]  Microsoft. Bing image search api.  https://www.microsoft.com/en-us/bing/ apis/bing-image-search-api , 2023.\n\n [95]  Weiqing Min, Zhiling Wang, Yuxin Liu, Mengjiang Luo, Liping Kang, Xiaoming Wei, Xiaolin Wei, and Shuqiang Jiang. Large scale visual food recognition.  IEEE Transactions on Pattern Analysis and Machine Intelligence , 2023.\n\n [96]  Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. Cross-task general- ization via natural language crowd sourcing instructions. In  Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 3470–3487, 2022.\n\n [97]  Tara Murfitt and Jan McAllister. The effect of production variables in monolog and dialog on comprehension by novel listeners.  Language and Speech , 44(3):325–350, 2001.\n\n [98]  Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christo- pher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted question-answering with human feedback.  arXiv preprint arXiv:2112.09332 , 2021.\n\n [99] OpenAI. Gpt-4 technical report, 2023.\n\n [100] OpenAI. Gpt-4v(ision) system card. 2023.\n\n [101] OpenAI. Gpt-4v(ision) technical work and authors. 2023.\n\n [102]  Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback.  Advances in Neural Information Processing [103] Bhargavi Paranjape, Scott Lundberg, Sameer Singh, Hannaneh Hajishirzi, Luke Z ett le moyer, and Marco Tulio Ribeiro. Art: Automatic multi-step reasoning and tool-use for large language models.  arXiv preprint arXiv:2303.09014 , 2023.\n\n [104]  Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou Yu, Weizhu Chen, et al. Check your facts and try again: Improving large language models with external knowledge and automated feedback.  arXiv preprint arXiv:2302.12813 , 2023.\n\n [105]  Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. Kosmos-2: Grounding multimodal large language models to the world.  arXiv preprint arXiv:2306.14824 , 2023.\n\n [106]  Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis.  arXiv preprint arXiv:2307.01952 , 2023.\n\n [107]  Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Yufei Huang, Chaojun Xiao, Chi Han, Yi Ren Fung, Yusheng Su, Huadong Wang, Cheng Qian, Runchu Tian, Kunlun Zhu, Shihao Liang, Xingyu Shen, Bokai Xu, Zhen Zhang, Yining Ye, Bowen Li, Ziwei Tang, Jing Yi, Yuzhang Zhu, Zhenning Dai, Lan Yan, Xin Cong, Yaxi Lu, Weilin Zhao, Yuxiang Huang, Junxi Yan, Xu Han, Xian Sun, Dahai Li, Jason Phang, Cheng Yang, Tongshuang Wu, Heng Ji, Zhiyuan Liu, and Maosong Sun. Tool learning with foundation models, 2023.\n\n [108]  Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer.  The Journal of Machine Learning Research , 21(1):5485–5551, 2020.\n\n [109]  John C Raven and JH Court.  Raven’s progressive matrices . Western Psychological Services Los Angeles, 1938.\n\n [110]  Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In  Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 10684–10695, 2022.\n\n [111]  Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, et al. Multitask prompted training en- ables zero-shot task generalization. In  International Conference on Learning Representations , 2021.\n\n [112]  Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettle- moyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools.  arXiv preprint arXiv:2302.04761 , 2023.\n\n [113]  Zhenwei Shao, Zhou Yu, Meng Wang, and Jun Yu. Prompting large language models with answer heuristics for knowledge-based visual question answering. In  CVPR , pages 14974– 14983, 2023.\n\n [114]  Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face.  arXiv preprint arXiv:2303.17580 , 2023.\n\n [115]  Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Z ett le moyer, and Wen-tau Yih. Replug: Retrieval-augmented black-box language models. arXiv preprint arXiv:2301.12652 , 2023.\n\n [116]  Noah Shinn, Beck Labash, and Ashwin Gopinath. Reflexion: an autonomous agent with dynamic memory and self-reflection.  arXiv preprint arXiv:2303.11366 , 2023.\n\n [117]  Aleksandar Sh ted rit ski, Christian Rupprecht, and Andrea Vedaldi. What does clip know about a red circle? visual prompt engineering for vlms.  arXiv preprint arXiv:2304.06712 , 2023. "}
{"page": 162, "image_path": "doc_images/2309.17421v2_162.jpg", "ocr_text": "03\n\n04\n\n05\n\n06\n\n07\n\n08\n\n09\n\nBhargavi Paranjape, Scott Lundberg, Sameer Singh, Hannaneh Hajishirzi, Luke Zettlemoyer,\nand Marco Tulio Ribeiro. Art: Automatic multi-step reasoning and tool-use for large language\nmodels. arXiv preprint arXiv:2303.09014, 2023.\n\nBaolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Qiuyuan Huang,\nLars Liden, Zhou Yu, Weizhu Chen, et al. Check your facts and try again: Improving\nlarge language models with external knowledge and automated feedback. arXiv preprint\narXiv:2302.12813, 2023.\n\nZhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu\nWei. Kosmos-2: Grounding multimodal large language models to the world. arXiv preprint\narXiv:2306.14824, 2023.\n\nDustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Miiller,\nJoe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution\nimage synthesis. arXiv preprint arXiv:2307.01952, 2023.\n\nYujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng,\nYufei Huang, Chaojun Xiao, Chi Han, Yi Ren Fung, Yusheng Su, Huadong Wang, Cheng\nQian, Runchu Tian, Kunlun Zhu, Shihao Liang, Xingyu Shen, Bokai Xu, Zhen Zhang, Yining\nYe, Bowen Li, Ziwei Tang, Jing Yi, Yuzhang Zhu, Zhenning Dai, Lan Yan, Xin Cong, Yaxi\nLu, Weilin Zhao, Yuxiang Huang, Junxi Yan, Xu Han, Xian Sun, Dahai Li, Jason Phang,\nCheng Yang, Tongshuang Wu, Heng Ji, Zhiyuan Liu, and Maosong Sun. Tool learning with\nfoundation models, 2023.\n\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\nYanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified\ntext-to-text transformer. The Journal of Machine Learning Research, 21(1):5485-5551, 2020.\n\nJohn C Raven and JH Court. Raven’s progressive matrices. Western Psychological Services\nLos Angeles, 1938.\n\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj6r Ommer.\nHigh-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pages 10684-10695, 2022.\n\nVictor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai,\nAntoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, et al. Multitask prompted training en-\n\nables zero-shot task generalization. In International Conference on Learning Representations,\n2021.\n\nTimo Schick, Jane Dwivedi-Yu, Roberto Dessi, Roberta Raileanu, Maria Lomeli, Luke Zettle-\nmoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach\nthemselves to use tools. arXiv preprint arXiv:2302.04761, 2023.\n\nZhenwei Shao, Zhou Yu, Meng Wang, and Jun Yu. Prompting large language models with\nanswer heuristics for knowledge-based visual question answering. In CVPR, pages 14974—\n14983, 2023.\n\nYongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang.\nHugginggpt: Solving ai tasks with chatgpt and its friends in huggingface. arXiv preprint\narXiv:2303.17580, 2023.\n\nWeijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke\nZettlemoyer, and Wen-tau Yih. Replug: Retrieval-augmented black-box language models.\narXiv preprint arXiv:2301.12652, 2023.\n\nNoah Shinn, Beck Labash, and Ashwin Gopinath. Reflexion: an autonomous agent with\ndynamic memory and self-reflection. arXiv preprint arXiv:2303.11366, 2023.\n\nAleksandar Shtedritski, Christian Rupprecht, and Andrea Vedaldi. What does clip know about\nared circle? visual prompt engineering for vlms. arXiv preprint arXiv:2304.06712, 2023.\n\n163\n", "vlm_text": ""}
{"page": 163, "image_path": "doc_images/2309.17421v2_163.jpg", "ocr_text": "18\n\n19\n\n20\n\n21\n\n22\n\n23\n\n24\n\n25\n\n26\n\n27\n\n28\n\n29\n\n30\n\n31\n\n32\n\n33\n34\n\n35\n\nOleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and Amanpreet Singh. Textcaps: a dataset\nfor image captioning with reading comprehension. In ECCV, pages 742-758, 2020.\n\nAmanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi\nParikh, and Marcus Rohrbach. Towards vqa models that can read. In CVPR, 2019.\n\nWeijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, and Jifeng Dai. V1-bert:\nPre-training of generic visual-linguistic representations. In JCLR, 2019.\n\nDidac Suris, Sachit Menon, and Carl Vondrick. Vipergpt: Visual inference via python execution\nfor reasoning. arXiv preprint arXiv:2303.08128, 2023.\n\nHao Tan and Mohit Bansal. Lxmert: Learning cross-modality encoder representations from\ntransformers. In EMNLP, 2019.\n\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-\nthée Lacroix, Baptiste Roziére, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open\nand efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\n\nHarsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. Interleaving\nretrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions. arXiv\npreprint arXiv:2212.10509, 2022.\n\nMaria Tsimpoukelli, Jacob L Menick, Serkan Cabi, SM Eslami, Oriol Vinyals, and Felix Hill.\nMultimodal few-shot learning with frozen language models. Advances in Neural Information\nProcessing Systems, 34:200-212, 2021.\n\nCarven Von Bearnensquash. Paper gestalt. Secret Proceedings of Computer Vision and Pattern\nRecognition (CVPR), 2010.\n\nHong Wang, Xuan Luo, Weizhi Wang, and Xifeng Yan. Bot or human? detecting chatgpt\nimposters with a single question. arXiv preprint arXiv:2305.06424, 2023.\n\nJianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu,\nCe Liu, and Lijuan Wang. Git: A generative image-to-text transformer for vision and language.\nTransactions on Machine Learning Research, 2022.\n\nPeng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou,\nJingren Zhou, and Hongxia Yang. Ofa: Unifying architectures, tasks, and modalities through a\nsimple sequence-to-sequence learning framework. In Jnternational Conference on Machine\nLearning, pages 23318-23340. PMLR, 2022.\n\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H Chi, Sharan Narang, Aakanksha\nChowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in\nlanguage models. In The Eleventh International Conference on Learning Representations,\n2022.\n\nZhenhailong Wang, Manling Li, Ruochen Xu, Luowei Zhou, Jie Lei, Xudong Lin, Shuohang\nWang, Ziyi Yang, Chenguang Zhu, Derek Hoiem, et al. Language models with image\ndescriptors are strong few-shot video-language learners. In Advances in Neural Information\nProcessing Systems.\n\nZirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, and Yuan Cao. Simvlm:\nSimple visual language model pretraining with weak supervision. In JCLR, 2022.\n\nDavid Wechsler. Wais-r: Manual: Wechsler adult intelligence scale-revised. (No Title), 1981.\n\nJason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan\nDu, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In\nICLR, 2022.\n\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani\nYogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large\nlanguage models. arXiv preprint arXiv:2206.07682, 2022.\n\n164\n", "vlm_text": "[118]  Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and Amanpreet Singh. Textcaps: a dataset for image captioning with reading comprehension. In  ECCV , pages 742–758, 2020.\n\n [119]  Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read. In  CVPR , 2019.\n\n [120]  Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, and Jifeng Dai. Vl-bert: Pre-training of generic visual-linguistic representations. In  ICLR , 2019.\n\n [121]  Dídac Surís, Sachit Menon, and Carl Vondrick. Vipergpt: Visual inference via python execution for reasoning.  arXiv preprint arXiv:2303.08128 , 2023.\n\n [122]  Hao Tan and Mohit Bansal. Lxmert: Learning cross-modality encoder representations from transformers. In  EMNLP , 2019.\n\n [123]  Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo- thée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models.  arXiv preprint arXiv:2302.13971 , 2023.\n\n [124]  Harsh Trivedi, Niranjan Bala subramania n, Tushar Khot, and Ashish Sabharwal. Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions.  arXiv preprint arXiv:2212.10509 , 2022.\n\n [125]  Maria Tsim po uk ell i, Jacob L Menick, Serkan Cabi, SM Eslami, Oriol Vinyals, and Felix Hill. Multimodal few-shot learning with frozen language models.  Advances in Neural Information Processing Systems , 34:200–212, 2021.\n\n [126]  Carven Von Bear nen squash. Paper gestalt.  Secret Proceedings of Computer Vision and Pattern Recognition (CVPR) , 2010.\n\n [127]  Hong Wang, Xuan Luo, Weizhi Wang, and Xifeng Yan. Bot or human? detecting chatgpt imposters with a single question.  arXiv preprint arXiv:2305.06424 , 2023.\n\n [128]  Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, and Lijuan Wang. Git: A generative image-to-text transformer for vision and language. Transactions on Machine Learning Research , 2022.\n\n [129]  Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang. Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework. In  International Conference on Machine Learning , pages 23318–23340. PMLR, 2022.\n\n [130]  Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In  The Eleventh International Conference on Learning Representations , 2022.\n\n [131]  Zhen hai long Wang, Manling Li, Ruochen Xu, Luowei Zhou, Jie Lei, Xudong Lin, Shuohang Wang, Ziyi Yang, Chenguang Zhu, Derek Hoiem, et al. Language models with image descriptors are strong few-shot video-language learners. In  Advances in Neural Information Processing Systems .\n\n [132]  Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, and Yuan Cao. Simvlm: Simple visual language model pre training with weak supervision. In  ICLR , 2022.\n\n [133]  David Wechsler. Wais-r: Manual: Wechsler adult intelligence scale-revised.  (No Title) , 1981.\n\n [134] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In ICLR , 2022.\n\n [135]  Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models.  arXiv preprint arXiv:2206.07682 , 2022. "}
{"page": 164, "image_path": "doc_images/2309.17421v2_164.jpg", "ocr_text": "36\n\n37\n\n38\n\n39\n\n40\n\n4l\n\n42\n\n43\n\n44\n\n45\n\n46\n\n47\n\n48\n\n49\n\n50\n\n51\n\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le,\nDenny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models.\nAdvances in Neural Information Processing Systems, 35:24824—24837, 2022.\n\nChenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan.\nVisual chatgpt: Talking, drawing and editing with visual foundation models. arXiv preprint\narXiv:2303.04671, 2023.\n\nJialian Wu, Jianfeng Wang, Zhengyuan Yang, Zhe Gan, Zicheng Liu, Junsong Yuan, and\nLijuan Wang. Grit: A generative region-to-text transformer for object understanding. arXiv\npreprint arXiv:2212.00280, 2022.\n\nZhenjia Xu, Zhijian Liu, Chen Sun, Kevin Murphy, William T Freeman, Joshua B Tenenbaum,\nand Jiajun Wu. Unsupervised discovery of parts, structure, and dynamics. In Jnternational\nConference on Learning Representations, 2018.\n\nZhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Faisal Ahmed, Zicheng Liu, Yumao\nLu, and Lijuan Wang. Unitab: Unifying text and box outputs for grounded vision-language\nmodeling. In European Conference on Computer Vision, pages 521-539. Springer, 2022.\n\nZhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Yumao Lu, Zicheng Liu, and Lijuan\nWang. An empirical study of gpt-3 for few-shot knowledge-based vqa. In Proceedings of the\nAAAI Conference on Artificial Intelligence, volume 36, pages 3081-3089, 2022.\n\nZhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed,\nZicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. Mm-react: Prompting chatgpt for\nmultimodal reasoning and action. arXiv preprint arXiv:2303.11381, 2023.\n\nZhengyuan Yang, Jianfeng Wang, Zhe Gan, Linjie Li, Kevin Lin, Chenfei Wu, Nan Duan,\nZicheng Liu, Ce Liu, Michael Zeng, et al. Reco: Region-controlled text-to-image generation.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,\npages 14246-14255, 2023.\n\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, and Karthik\nNarasimhan. Tree of thoughts: Deliberate problem solving with large language models. arXiv\npreprint arXiv:2305.10601, 2023.\n\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R Narasimhan, and\nYuan Cao. React: Synergizing reasoning and acting in language models. In The Eleventh\nInternational Conference on Learning Representations, 2022.\n\nQinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang,\nAnwen Hu, Pengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization empowers large\nlanguage models with multimodality. arXiv preprint arXiv:2304.14178, 2023.\n\nDa Yin, Liunian Harold Li, Ziniu Hu, Nanyun Peng, and Kai-Wei Chang. Broaden the vision:\nGeo-diverse visual commonsense reasoning. arXiv preprint arXiv:2109.06860, 2021.\n\nRowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi. From recognition to cognition:\nVisual commonsense reasoning. In CVPR, pages 6720-6731, 2019.\n\nAndy Zeng, Maria Attarian, Krzysztof Marcin Choromanski, Adrian Wong, Stefan Welker,\nFederico Tombari, Aveek Purohit, Michael S Ryoo, Vikas Sindhwani, Johnny Lee, et al.\nSocratic models: Composing zero-shot multimodal reasoning with language. In The Eleventh\nInternational Conference on Learning Representations, 2022.\n\nChi Zhang, Feng Gao, Baoxiong Jia, Yixin Zhu, and Song-Chun Zhu. Raven: A dataset for\nrelational and analogical visual reasoning. In Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, pages 5317-5327, 2019.\n\nZhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, and Alex Smola. Mul-\ntimodal chain-of-thought reasoning in language models. arXiv preprint arXiv:2302.00923,\n2023.\n\n165\n", "vlm_text": "[136]  Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems , 35:24824–24837, 2022.\n\n [137]  Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. Visual chatgpt: Talking, drawing and editing with visual foundation models.  arXiv preprint arXiv:2303.04671 , 2023.\n\n [138]  Jialian Wu, Jianfeng Wang, Zhengyuan Yang, Zhe Gan, Zicheng Liu, Junsong Yuan, and Lijuan Wang. Grit: A generative region-to-text transformer for object understanding.  arXiv preprint arXiv:2212.00280 , 2022.\n\n [139]  Zhenjia Xu, Zhijian Liu, Chen Sun, Kevin Murphy, William T Freeman, Joshua B Tenenbaum, and Jiajun Wu. Unsupervised discovery of parts, structure, and dynamics. In  International Conference on Learning Representations , 2018.\n\n [140]  Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Faisal Ahmed, Zicheng Liu, Yumao Lu, and Lijuan Wang. Unitab: Unifying text and box outputs for grounded vision-language modeling. In  European Conference on Computer Vision , pages 521–539. Springer, 2022.\n\n [141]  Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Yumao Lu, Zicheng Liu, and Lijuan Wang. An empirical study of gpt-3 for few-shot knowledge-based vqa. In  Proceedings of the AAAI Conference on Artificial Intelligence , volume 36, pages 3081–3089, 2022.\n\n [142]  Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. Mm-react: Prompting chatgpt for multimodal reasoning and action.  arXiv preprint arXiv:2303.11381 , 2023.\n\n [143]  Zhengyuan Yang, Jianfeng Wang, Zhe Gan, Linjie Li, Kevin Lin, Chenfei Wu, Nan Duan, Zicheng Liu, Ce Liu, Michael Zeng, et al. Reco: Region-controlled text-to-image generation. In  Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 14246–14255, 2023.\n\n [144]  Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models.  arXiv preprint arXiv:2305.10601 , 2023.\n\n [145]  Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R Narasimhan, and Yuan Cao. React: Syne rg i zing reasoning and acting in language models. In  The Eleventh International Conference on Learning Representations , 2022.\n\n [146]  Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. mplug-owl: Modular iz ation empowers large language models with multi modality.  arXiv preprint arXiv:2304.14178 , 2023.\n\n [147]  Da Yin, Liunian Harold Li, Ziniu Hu, Nanyun Peng, and Kai-Wei Chang. Broaden the vision: Geo-diverse visual commonsense reasoning.  arXiv preprint arXiv:2109.06860 , 2021.\n\n [148]  Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi. From recognition to cognition: Visual commonsense reasoning. In  CVPR , pages 6720–6731, 2019.\n\n [149]  Andy Zeng, Maria Attarian, Krzysztof Marcin Cho roman ski, Adrian Wong, Stefan Welker, Federico Tombari, Aveek Purohit, Michael S Ryoo, Vikas Sindhwani, Johnny Lee, et al. Socratic models: Composing zero-shot multimodal reasoning with language. In  The Eleventh International Conference on Learning Representations , 2022.\n\n [150]  Chi Zhang, Feng Gao, Baoxiong Jia, Yixin Zhu, and Song-Chun Zhu. Raven: A dataset for relational and analogical visual reasoning. In  Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 5317–5327, 2019.\n\n [151]  Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, and Alex Smola. Mul- timodal chain-of-thought reasoning in language models.  arXiv preprint arXiv:2302.00923 , 2023. "}
{"page": 165, "image_path": "doc_images/2309.17421v2_165.jpg", "ocr_text": "52\n\n53\n\n54\n\n55\n\n56\n\n57\n\n58\n\n59\n\n60\n\nYan-Tao Zheng, Ming Zhao, Yang Song, Hartwig Adam, Ulrich Buddemeier, Alessandro\nBissacco, Fernando Brucher, Tat-Seng Chua, and Hartmut Neven. Tour the world: building a\nweb-scale landmark recognition engine. In 2009 IEEE Conference on Computer Vision and\nPattern Recognition, pages 1085-1092. IEEE, 2009.\n\nBolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and Antonio Torralba. Learning\ndeep features for discriminative localization. In CVPR, 2016.\n\nBolei Zhou, Agata Lapedriza, Antonio Torralba, and Aude Oliva. Places: An image database\nfor deep scene understanding. Journal of Vision, 17(10):296-296, 2017.\n\nLuowei Zhou, Hamid Palangi, Lei Zhang, Houdong Hu, Jason J Corso, and Jianfeng Gao.\nUnified vision-language pre-training for image captioning and vqa. In AAAI, 2020.\n\nYongchao Zhou, Andrei Joan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan,\nand Jimmy Ba. Large language models are human-level prompt engineers. In The Eleventh\nInternational Conference on Learning Representations, 2022.\n\nDeyao Zhu, Jun Chen, Xiaogian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4:\nEnhancing vision-language understanding with advanced large language models. arXiv\npreprint arXiv:2304. 10592, 2023.\n\nXueyan Zou, Zi-Yi Dou, Jianwei Yang, Zhe Gan, Linjie Li, Chunyuan Li, Xiyang Dai, Harkirat\nBehl, Jianfeng Wang, Lu Yuan, et al. Generalized decoding for pixel, image, and language. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages\n15116-15127, 2023.\n\nXueyan Zou, Zi-Yi Dou, Jianwei Yang, Zhe Gan, Linjie Li, Chunyuan Li, Xiyang Dai, Jianfeng\nWang, Lu Yuan, Nanyun Peng, Lijuan Wang, Yong Jae Lee, and Jianfeng Gao. Generalized\ndecoding for pixel, image and language. 2022.\n\nXueyan Zou, Jianwei Yang, Hao Zhang, Feng Li, Linjie Li, Jianfeng Gao, and Yong Jae Lee.\nSegment everything everywhere all at once. arXiv preprint arXiv:2304.06718, 2023.\n\n166\n", "vlm_text": "[152]  Yan-Tao Zheng, Ming Zhao, Yang Song, Hartwig Adam, Ulrich Buddemeier, Alessandro Bissacco, Fernando Brucher, Tat-Seng Chua, and Hartmut Neven. Tour the world: building a web-scale landmark recognition engine. In  2009 IEEE Conference on Computer Vision and Pattern Recognition , pages 1085–1092. IEEE, 2009.\n\n [153]  Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and Antonio Torralba. Learning deep features for disc rim i native localization. In  CVPR , 2016.\n\n [154]  Bolei Zhou, Agata Lapedriza, Antonio Torralba, and Aude Oliva. Places: An image database for deep scene understanding.  Journal of Vision , 17(10):296–296, 2017.\n\n [155]  Luowei Zhou, Hamid Palangi, Lei Zhang, Houdong Hu, Jason J Corso, and Jianfeng Gao. Unified vision-language pre-training for image captioning and vqa. In  AAAI , 2020.\n\n [156]  Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba. Large language models are human-level prompt engineers. In  The Eleventh International Conference on Learning Representations , 2022.\n\n [157]  Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592 , 2023.\n\n [158]  Xueyan Zou, Zi-Yi Dou, Jianwei Yang, Zhe Gan, Linjie Li, Chunyuan Li, Xiyang Dai, Harkirat Behl, Jianfeng Wang, Lu Yuan, et al. Generalized decoding for pixel, image, and language. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 15116–15127, 2023.\n\n [159]  Xueyan Zou, Zi-Yi Dou, Jianwei Yang, Zhe Gan, Linjie Li, Chunyuan Li, Xiyang Dai, Jianfeng Wang, Lu Yuan, Nanyun Peng, Lijuan Wang, Yong Jae Lee, and Jianfeng Gao. Generalized decoding for pixel, image and language. 2022.\n\n [160]  Xueyan Zou, Jianwei Yang, Hao Zhang, Feng Li, Linjie Li, Jianfeng Gao, and Yong Jae Lee. Segment everything everywhere all at once.  arXiv preprint arXiv:2304.06718 , 2023. "}
