{"page": 0, "image_path": "doc_images/P19-1164_0.jpg", "ocr_text": "Evaluating Gender Bias in Machine Translation\n\nGabriel Stanovsky!”, Noah A. Smith!”, and Luke Zettlemoyer!\n\n‘Paul G. Allen School of Computer Science & Engineering, University of Washington, Seattle, USA\n\n2 Allen Institute for Artificial Intelligence, Seattle, USA\n{gabis,nasmith, lsz}@cs.washington.edu\n\nAbstract\n\nWe present the first challenge set and eval-\nuation protocol for the analysis of gender\nbias in machine translation (MT). Our ap-\nproach uses two recent coreference resolution\ndatasets composed of English sentences which\ncast participants into non-stereotypical gender\nroles (e.g., “The doctor asked the nurse to help\nher in the operation”). We devise an automatic\ngender bias evaluation method for eight tar-\nget languages with grammatical gender, based\non morphological analysis (e.g., the use of fe-\nmale inflection for the word “doctor’). Our\nanalyses show that four popular industrial MT\nsystems and two recent state-of-the-art aca-\ndemic MT models are significantly prone to\ngender-biased translation errors for all tested\ntarget languages. Our data and code are pub-\nlicly available at https: //github.com/\ngabrielStanovsky/mt_gender.\n\n1 Introduction\n\nLearned models exhibit social bias when their\ntraining data encode stereotypes not relevant for\nthe task, but the correlations are picked up any-\nway. Notable examples include gender biases in\nvisual SRL (cooking is stereotypically done by\nwomen, construction workers are stereotypically\nmen; Zhao et al., 2017), lexical semantics (“man\nis to computer programmer as woman is to home-\nmaker”; Bolukbasi et al., 2016), and natural lan-\nguage inference (associating women with gossip-\ning and men with guitars; Rudinger et al., 2017).\nIn this work, we conduct the first large-scale\nmultilingual evaluation of gender-bias in machine\ntranslation (MT), following recent small-scale\nqualitative studies which observed that online MT\nservices, such as Google Translate or Microsoft\nTranslator, also exhibit biases, e.g., translating\nnurses as females and programmers as males, re-\ngardless of context (Alvarez-Melis and Jaakkola,\n\n‘The doctor asked the nurse to help her in the procedure\n\nEl doctor Ie pidio a la enfermera que le ayudara con el procedimiento\n\nFigure 1: An example of gender bias in machine trans-\nlation from English (top) to Spanish (bottom). In\nthe English source sentence, the nurse’s gender is un-\nknown, while the coreference link with “her” identi-\nfies the “doctor” as a female. On the other hand, the\nSpanish target sentence uses morphological features\nfor gender: “e/ doctor” (male), versus “/a enfermer-\na” (female). Aligning between source and target sen-\ntences reveals that a stereotypical assignment of gender\nroles changed the meaning of the translated sentence by\nchanging the doctor’s gender.\n\n2017; Font and Costa-Jussa, 2019). Google Trans-\nlate recently tried to mitigate these biases by al-\nlowing users to sometimes choose between gen-\ndered translations (Kuczmarski, 2018).\n\nAs shown in Figure 1, we use data introduced\nby two recent coreference gender-bias studies: the\nWinogender (Rudinger et al., 2018), and the Wino-\nBias (Zhao et al., 2018) datasets. Following the\nWinograd schema (Levesque, 2011), each instance\nin these datasets is an English sentence which de-\nscribes a scenario with human entities, who are\nidentified by their role (e.g., “the doctor” and “the\nnurse” in Figure 1), and a pronoun (“her” in the\nexample), which needs to be correctly resolved\nto one of the entities (“the doctor” in this case).\nRudinger et al. (2018) and Zhao et al. (2018) found\nthat while human agreement on the task was high\n(roughly 95%), coreference resolution models of-\nten ignore context and make socially biased pre-\ndictions, e.g., associating the feminine pronoun\n“her” with the stereotypically female “nurse.”\n\nWe observe that for many target languages, a\nfaithful translation requires a similar form of (at\n\n1679\n\nProceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1679-1684\nFlorence, Italy, July 28 - August 2, 2019. ©2019 Association for Computational Linguistics\n", "vlm_text": "Evaluating Gender Bias in Machine Translation \nGabriel Stanovsky ,  Noah A. Smith , and  Luke Zettlemoyer 1 \n1 Paul G. Allen School of Computer Science & Engineering, University of Washington, Seattle, USA 2 Allen Institute for Artiﬁcial Intelligence, Seattle, USA { gabis,nasmith,lsz } @cs.washington.edu \nAbstract \nWe present the ﬁrst challenge set and eval- uation protocol for the analysis of gender bias in machine translation (MT). Our ap- proach uses two recent coreference resolution datasets composed of English sentences which cast participants into non-stereotypical gender roles (e.g., “The doctor asked the nurse to help her  in the operation”). We devise an automatic gender bias evaluation method for eight tar- get languages with grammatical gender, based on morphological analysis (e.g., the use of fe- male inﬂection for the word “doctor”). Our analyses show that four popular industrial MT systems and two recent state-of-the-art aca- demic MT models are signiﬁcantly prone to gender-biased translation errors for all tested target languages. Our data and code are pub- licly available at  https://github.com/ gabriel St a nov sky/mt_gender . \n1 Introduction \nLearned models exhibit social bias when their training data encode stereotypes not relevant for the task, but the correlations are picked up any- way. Notable examples include gender biases in visual SRL (cooking is stereotypically done by women, construction workers are stereotypically men;  Zhao et al. ,  2017 ), lexical semantics (“man is to computer programmer as woman is to home- maker”;  Bolukbasi et al. ,  2016 ), and natural lan- guage inference (associating women with gossip- ing and men with guitars;  Rudinger et al. ,  2017 ). \nIn this work, we conduct the ﬁrst large-scale multilingual evaluation of gender-bias in machine translation (MT), following recent small-scale qualitative studies which observed that online MT services, such as Google Translate or Microsoft Translator, also exhibit biases, e.g., translating nurses as females and programmers as males, re- gardless of context ( Alvarez-Melis and Jaakkola , \nThe image shows a diagram with two sentences, one in English and one in Spanish, and arrows indicating the correspondence between certain words in both sentences. The English sentence is \"The doctor asked the nurse to help her in the procedure,\" and the Spanish sentence is \"El doctor le pidió a la enfermera que le ayudara con el procedimiento.\"\n\nThe words \"The doctor\" in the English sentence and \"El doctor\" in the Spanish sentence are highlighted in purple and light blue respectively, with an arrow indicating their correspondence. Similarly, the words \"the nurse\" in English and \"la enfermera\" in Spanish are highlighted in purple/pink, also linked by an arrow. Additionally, there is a dotted line connecting \"The doctor\" in the English sentence to \"asked\" further along in the sentence, indicating a relationship or structure within the sentence. The diagram appears to illustrate a bilingual alignment or syntactic parsing of the sentences.\nFigure 1: An example of gender bias in machine trans- lation from English (top) to Spanish (bottom). In the English source sentence, the nurse’s gender is un- known, while the coreference link with “her” identi- ﬁes the “doctor” as a female. On the other hand, the Spanish target sentence uses morphological features for gender: “ el  doctor” (male), versus “ la  enfermer - a ” (female). Aligning between source and target sen- tences reveals that a stereotypical assignment of gender roles changed the meaning of the translated sentence by changing the doctor’s gender. \n2017 ;  Font and Costa-Juss\\` ,  2019 ). Google Trans- late recently tried to mitigate these biases by al- lowing users to sometimes choose between gen- dered translations ( Kuczmarski ,  2018 ). \nAs shown in Figure  1 , we use data introduced by two recent coreference gender-bias studies: the Winogender ( Rudinger et al. ,  2018 ), and the Wino- Bias ( Zhao et al. ,  2018 ) datasets. Following the Winograd schema ( Levesque ,  2011 ), each instance in these datasets is an English sentence which de- scribes a scenario with human entities, who are identiﬁed by their role (e.g., “the doctor” and “the nurse” in Figure  1 ), and a pronoun (“her” in the example), which needs to be correctly resolved to one of the entities (“the doctor” in this case). Rudinger et al.  ( 2018 ) and  Zhao et al.  ( 2018 ) found that while human agreement on the task was high (roughly   $95\\%$  ), coreference resolution models of- ten ignore context and make socially biased pre- dictions, e.g., associating the feminine pronoun “her” with the stereotypically female “nurse.” \nWe observe that for many target languages, a faithful translation requires a similar form of (at least implicit) gender identiﬁcation. In addition, in the many languages which associate between biological and grammatical gender (e.g., most Ro- mance, Germanic, Slavic, and Semitic languages; Craig ,  1986 ;  Mucchi-Faina ,  2005 ;  Corbett ,  2007 ), the gender of an animate object can be identiﬁed via morphological markers. For instance, when translating our running example in Figure  1  to Spanish, a valid translation may be: “ La doc- tora  le pidio a la enfermera que le ayudara con el procedimiento,” which indicates that the doctor is a woman, by using a feminine sufﬁx inﬂection (“doctor a ”) and the feminine deﬁnite gendered ar- ticle (“ la ”). However, a biased translation system may ignore the given context and stereotypically translate the doctor as male, as shown at the bot- tom of the ﬁgure. "}
{"page": 1, "image_path": "doc_images/P19-1164_1.jpg", "ocr_text": "least implicit) gender identification. In addition,\nin the many languages which associate between\nbiological and grammatical gender (e.g., most Ro-\nmance, Germanic, Slavic, and Semitic languages;\nCraig, 1986; Mucchi-Faina, 2005; Corbett, 2007),\nthe gender of an animate object can be identified\nvia morphological markers. For instance, when\ntranslating our running example in Figure | to\nSpanish, a valid translation may be: “La doc-\ntora le pidio a la enfermera que le ayudara con\nel procedimiento,” which indicates that the doctor\nis a woman, by using a feminine suffix inflection\n(“doctora’’) and the feminine definite gendered ar-\nticle (“Ja”). However, a biased translation system\nmay ignore the given context and stereotypically\ntranslate the doctor as male, as shown at the bot-\ntom of the figure.\n\nFollowing these observations, we design a chal-\nlenge set approach for evaluating gender bias in\nMT using a concatenation of Winogender and\nWinoBias. We devise an automatic translation\nevaluation method for eight diverse target lan-\nguages, without requiring additional gold trans-\nlations, relying instead on automatic measures\nfor alignment and morphological analysis (Sec-\ntion 2). We find that four widely used commercial\nMT systems and two recent state-of-the-art aca-\ndemic models are significantly gender-biased on\nall tested languages (Section 3). Our method and\nbenchmarks are publicly available, and are easily\nextensible with more languages and MT models.\n\n2 Challenge Set for Gender Bias in MT\n\nWe compose a challenge set for gender bias in MT\n(which we dub ““WinoMT’”) by concatenating the\nWinogender and WinoBias coreference test sets.\nOverall, WinoMT contains 3,888 instances, and is\nequally balanced between male and female gen-\nders, as well as between stereotypical and non-\nstereotypical gender-role assignments (e.g., a fe-\nmale doctor versus a female nurse). Additional\ndataset statistics are presented in Table 1.\n\nWe use WinoMT to estimate the gender-bias of\nan MT model, M, in target-language L by per-\nforming following steps (exemplified in Figure 1):\n(1) Translate all of the sentences in WinoMT into\nLusing M, thus forming a bilingual corpus of En-\nglish and the target language L.\n\n(2) Align between the source and target transla-\ntions, using fast_align (Dyer et al., 2013), trained\non the automatic translations from from step (1).\n\nWinogender WinoBias WinoMT\nMale 240 1582 1826\nFemale 240 1586 1822\nNeutral 240 0 240\nTotal 720 3168 3888\n\nTable 1: The coreference test sets and resulting\nWinoMT corpus statistics (in number of instances).\n\nWe then map the English entity annotated in the\ncoreference datasets to its translation (e.g., align\nbetween “the doctor” and “el doctor” in Figure 1).\n(3) Finally, we extract the target-side entity’s\ngender using simple heuristics over language-\nspecific morphological analysis, which we per-\nform using off-the-shelf tools for each target lan-\nguage, as discussed in the following section.\n\nThis process extracts the translated genders, ac-\ncording to M, for all of the entities in WinoMT,\nwhich we can then evaluate against the gold anno-\ntations provided by the original English dataset.\n\nThis process can introduce noise into our eval-\nuation in steps (2) and (3), via wrong alignments\nor erroneous morphological analysis. In Section 3,\nwe will present a human evaluation showing these\nerrors are infrequent.\n\n3 Evaluation\n\nIn this section, we briefly describe the MT systems\nand the target languages we use, our main results,\nand their human validation.\n\n3.1 Experimental Setup\n\nMT systems We test six widely used MT mod-\nels, representing the state of the art in both\ncommercial and academic research: (1) Google\nTranslate,! (2) Microsoft Translator,2 (3) Amazon\nTranslate,> (4) SYSTRAN,* (5) the model of Ott\net al. (2018), which recently achieved the best per-\nformance on English-to-French translation on the\nWMT’ 14 test set, and (6) the model of Edunov\net al. (2018), the WMT’ 18 winner on English-to-\nGerman translation. We query the online API for\nthe first four commercial MT systems, while for\nthe latter two academic models we use the pre-\ntrained models provided by the Fairseq toolkit.\n\n‘https: //translate.google.com\n*https://www.bing.com/translator\nShttps://aws.amazon.com/translate\n‘http: //www.systransoft.com\nShttps://github.com/pytorch/fairseq\n\n1680\n", "vlm_text": "\nFollowing these observations, we design a chal- lenge set approach for evaluating gender bias in MT using a concatenation of Winogender and WinoBias. We devise an automatic translation evaluation method for eight diverse target lan- guages, without requiring additional gold trans- lations, relying instead on automatic measures for alignment and morphological analysis (Sec- tion  2 ). We ﬁnd that four widely used commercial MT systems and two recent state-of-the-art aca- demic models are signiﬁcantly gender-biased on all tested languages (Section  3 ). Our method and benchmarks are publicly available, and are easily extensible with more languages and MT models. \n2 Challenge Set for Gender Bias in MT \nWe compose a challenge set for gender bias in MT (which we dub “WinoMT”) by concatenating the Winogender and WinoBias coreference test sets. Overall, WinoMT contains 3,888 instances, and is equally balanced between male and female gen- ders, as well as between stereotypical and non- stereotypical gender-role assignments (e.g., a fe- male doctor versus a female nurse). Additional dataset statistics are presented in Table  1 . \nWe use WinoMT to estimate the gender-bias of an MT model,    $M$  , in target-language    $L$   by per- forming following steps (exempliﬁed in Figure  1 ): (1)  Translate  all of the sentences in WinoMT into  $L$   using    $M$  , thus forming a bilingual corpus of En- glish and the target language    $L$  . \nThis table presents a breakdown of data across three categories (Winogender, WinoBias, and WinoMT) by gender classification (Male, Female, Neutral).\n\n- For Winogender: \n  - Male: 240\n  - Female: 240\n  - Neutral: 240\n  - Total: 720\n\n- For WinoBias:\n  - Male: 1582\n  - Female: 1586\n  - Neutral: 0\n  - Total: 3168\n\n- For WinoMT:\n  - Male: 1826\n  - Female: 1822\n  - Neutral: 240\n  - Total: 3888\n\nThe total values represent the sum of the respective categories across all gender classifications.\nWe then map the English entity annotated in the coreference datasets to its translation (e.g., align between “the doctor” and “el doctor” in Figure  1 ). (3) Finally, we  extract the target-side entity’s gender  using simple heuristics over language- speciﬁc morphological analysis, which we per- form using off-the-shelf tools for each target lan- guage, as discussed in the following section. \nThis process extracts the translated genders, ac- cording to    $M$  , for all of the entities in WinoMT, which we can then evaluate against the gold anno- tations provided by the original English dataset. \nThis process can introduce noise into our eval- uation in steps (2) and (3), via wrong alignments or erroneous morphological analysis. In Section  3 , we will present a human evaluation showing these errors are infrequent. \n3 Evaluation \nIn this section, we brieﬂy describe the MT systems and the target languages we use, our main results, and their human validation. \n3.1 Experimental Setup \nMT systems We test six widely used MT mod- els, representing the state of the art in both commercial and academic research: (1) Google Translate,   (2) Microsoft Translator,   (3) Amazon Translate,   (4) SYSTRAN,   (5) the model of  Ott et al.  ( 2018 ), which recently achieved the best per- formance on English-to-French translation on the WMT’14 test set, and (6) the model of  Edunov et al.  ( 2018 ), the WMT’18 winner on English-to- German translation. We query the online API for the ﬁrst four commercial MT systems, while for the latter two academic models we use the pre- trained models provided by the Fairseq toolkit. \n(2)  Align  between the source and target transla- tions, using  fast align  ( Dyer et al. ,  2013 ), trained on the automatic translations from from step (1). "}
{"page": 2, "image_path": "doc_images/P19-1164_2.jpg", "ocr_text": "Google Translate Microsoft Translator Amazon Translate” SYSTRAN\nAcc Ag As Acc Ag Ag Acc Ag As Acc Ag Ag\nES | 53.1 234 21.3 | 47.3 36.8 23.2 | 59.4 15.4 22.3) 45.6 463 15.0\nFR | 63.6 64 26.7 | 44.7 364 29.7| 55.2 17.7 24.9 | 45.0 44.0 9.4\nIT | 39.6 32.9 21.5 | 39.8 39.8 17.0 | 42.4 27.8 18.5 | 38.9 47.5 9.4\nRU | 37.7 36.8 11.4 | 36.8 42.1 8.5 | 39.7 34.7 9.2 | 37.3 44.1 9.3\nUK | 384 43.6 10.8 | 41.3 469 11.8] —- - — | 289 224 12.9\nHE | 53.7) 7.9 37.8) 48.1 14.9 32.9} 50.5 10.3 47.3 | 46.6 20.5 24.5\nAR | 48.5 43.7 16.1 | 47.3 48.3 13.4 | 49.8 38.5 19.0 | 47.0 49.4 5.3\nDE |594 12.5 12.5 | 741 0.0 30.2 | 62.4 12.0 16.7 |] 486 345 10.3\n\nTable 2: Performance of commercial MT systems on the WinoMT corpus on all tested languages, categorized by\ntheir family: Spanish, French, Italian, Russian, Ukrainian, Hebrew, Arabic, and German. Acc indicates overall\ngender accuracy (% of instances the translation had the correct gender), Ag denotes the difference in performance\n(F\\ score) between masculine and feminine scores, and Ag is the difference in performance (F) score) between\npro-stereotypical and anti-stereotypical gender role assignments (higher numbers in the two latter metrics indicate\nstronger biases). Numbers in bold indicate best accuracy for the language across MT systems (row), and underlined\nnumbers indicate best accuracy for the MT system across languages (column). *Amazon Translate does not have\n\na trained model for English to Ukrainian.\n\nAcc Ag As\n\n494 2.6 16.1\n52.55 7.3 8.4\n\nFR (Ott et al., 2018)\nDE (Edunov et al., 2018)\n\nTable 3: Performance of recent state-of-the-art aca-\ndemic translation models from English to French and\nGerman. Metrics are the same as those in Table 2.\n\nTarget languages and morphological analysis\nWe selected a set of eight languages with gram-\nmatical gender which exhibit a wide range of\nother linguistic properties (e.g., in terms of al-\nphabet, word order, or grammar), while still al-\nlowing for highly accurate automatic morpholog-\nical analysis. These languages belong to four dif-\nferent families: (1) Romance languages: Span-\nish, French, and Italian, all of which have gen-\ndered noun-determiner agreement and spaCy mor-\nphological analysis support (Honnibal and Mon-\ntani, 2017). (2) Slavic languages (Cyrillic alpha-\nbet): Russian and Ukrainian, for which we use\nthe morphological analyzer developed by Korobov\n(2015). (3) Semitic languages: Hebrew and Ara-\nbic, each with a unique alphabet. For Hebrew,\nwe use the analyzer developed by Adler and El-\nhadad (2006), while gender inflection in Arabic\ncan be easily identified via the ta marbuta charac-\nter, which uniquely indicates feminine inflection.\n(4) Germanic languages: German, for which we\n\nuse the morphological analyzer developed by Al-\ntinok (2018).\n\n3.2 Results\n\nOur main findings are presented in Tables 2 and 3.\nFor each tested MT system and target language we\ncompute three metrics with respect to their abil-\nity to convey the correct gender in the target lan-\nguage. Ultimately, our analyses indicate that all\nested MT systems are indeed gender biased.\nFirst, the overall system Accuracy is calculated\nby the percentage of instances in which the trans-\nlation preserved the gender of the entity from\nhe original English sentence. We find that most\nested systems across eight tested languages per-\n‘orm quite poorly on this metric. The best per-\n‘orming model on each language often does not\ndo much better than a random guess for the correct\ninflection. An exception to this rule is the transla-\nion accuracies on German, where three out of four\nsystems acheive their best performance. This may\nbe explained by German’s similarity to the English\nsource language (Hawkins, 2015).\nIn Table 2, Ag denotes the difference in per-\normance (F; score) between male and female\ntranslations. Interestingly, all systems, except Mi-\ncrosoft Translator on German, perform signifi-\ncantly better on male roles, which may stem from\nhese being more frequent in the training set.\nPerhaps most tellingly, Ag measures the differ-\n\n1681\n", "vlm_text": "The table presents accuracy and error metrics for different translation services—Google Translate, Microsoft Translator, Amazon Translate, and SYSTRAN—across various languages. \n\n- The languages are English to Spanish (ES), French (FR), Italian (IT), Russian (RU), Ukrainian (UK), Hebrew (HE), Arabic (AR), and German (DE).\n- The metrics for each translation service include:\n  - \"Acc\": The accuracy percentage.\n  - \"Δ_G\": A measure indicating a specific type of change in the output, possibly related to grammatical or structural accuracy.\n  - \"Δ_S\": Another measure indicating a different type of change, possibly related to semantic or syntactic accuracy.\n\n### Breakdown by Service and Language:\n\n1. **Google Translate:**\n   - High accuracy score for FR (63.6) and HE (53.7).\n   - Significant \"Δ_G\" variations, especially notable in AR (43.7).\n   - \"Δ_S\" is also varied with HE showing a noticeable change (37.8).\n\n2. **Microsoft Translator:**\n   - Highest accuracy score is with DE (74.1).\n   - \"Δ_G\" scores are variable with AR language having a high score (48.3).\n   - \"Δ_S\" shows considerable changes in DE (30.2).\n\n3. **Amazon Translate:**\n   - Shows its highest accuracy with ES (59.4) and AR (49.8).\n   - Presents \"Δ_G\" scores that are relatively consistent, with AR being the most affected (38.5).\n   - \"Δ_S\" changes notable in HE (47.3).\n\n4. **SYSTRAN:**\n   - Achieves a higher accuracy score in DE (48.6).\n   - Displays significant \"Δ_G\" changes across various languages, with AR showing the most substantial change (49.4).\n   - \"Δ_S\" is more stable, with HE showing a higher rate of alteration (24.5).\n\nThese metrics reflect performance variations among translation services for each language, indicating how each service handles grammatical and semantic changes differently.\nTable 2: Performance of commercial MT systems on the WinoMT corpus on all tested languages, categorized by their family: Spanish, French, Italian, Russian, Ukrainian, Hebrew, Arabic, and German.    $A c c$   indicates overall gender accuracy (  $\\%$   of instances the translation had the correct gender),    $\\Delta_{G}$   denotes the difference in performance (  $F_{1}$   score) between masculine and feminine scores, and    $\\Delta_{S}$   is the difference in performance (  $F_{1}$   score) between pro-stereotypical and anti-stereotypical gender role assignments (higher numbers in the two latter metrics indicate stronger biases). Numbers in bold indicate best accuracy for the language across MT systems (row), and underlined numbers indicate best accuracy for the MT system across languages (column).   ∗ Amazon Translate does not have a trained model for English to Ukrainian. \nThe table shows data for two rows corresponding to two different experiments or analyses labeled \"FR\" and \"DE\". For each row, there are three columns of data labeled as \"Acc\", \"ΔG\", and \"ΔS\". The values in the table are as follows:\n\n1. For \"FR\" (Ott et al., 2018):\n   - Acc: 49.4\n   - ΔG: 2.6\n   - ΔS: 16.1\n\n2. For \"DE\" (Edunov et al., 2018):\n   - Acc: 52.5\n   - ΔG: 7.3\n   - ΔS: 8.4\n\nThe table entries suggest that it is comparing two studies or methods, possibly in the context of statistical or experimental results, given the reference to two different authors and dates in parentheses.\nTarget languages and morphological analysis We selected a set of eight languages with gram- matical gender which exhibit a wide range of other linguistic properties (e.g., in terms of al- phabet, word order, or grammar), while still al- lowing for highly accurate automatic morpholog- ical analysis. These languages belong to four dif- ferent families: (1)  Romance languages :  Span- ish, French, and Italian , all of which have gen- dered noun-determiner agreement and spaCy mor- phological analysis support ( Honnibal and Mon- tani ,  2017 ). (2)  Slavic languages  (Cyrillic alpha- bet):  Russian and Ukrainian , for which we use the morphological analyzer developed by  Korobov ( 2015 ). (3)  Semitic languages :  Hebrew and Ara- bic , each with a unique alphabet. For Hebrew, we use the analyzer developed by  Adler and El- hadad  ( 2006 ), while gender inﬂection in Arabic can be easily identiﬁed via the  ta marbuta  charac- ter, which uniquely indicates feminine inﬂection. (4)  Germanic languages :  German , for which we use the morphological analyzer developed by  Al- tinok  ( 2018 ). \n\n3.2 Results \nOur main ﬁndings are presented in Tables  2  and  3 . For each tested MT system and target language we compute three metrics with respect to their abil- ity to convey the correct gender in the target lan- guage. Ultimately, our analyses indicate that all tested MT systems are indeed gender biased. \nFirst, the overall system  Accuracy  is calculated by the percentage of instances in which the trans- lation preserved the gender of the entity from the original English sentence. We ﬁnd that most tested systems across eight tested languages per- form quite poorly on this metric. The best per- forming model on each language often does not do much better than a random guess for the correct inﬂection. An exception to this rule is the transla- tion accuracies on German, where three out of four systems acheive their best performance. This may be explained by German’s similarity to the English source language ( Hawkins ,  2015 ). \nIn Table  2 ,    $\\Delta_{G}$   denotes the difference in per- formance (  ${\\cal F}_{1}$   score) between male and female translations. Interestingly, all systems, except Mi- crosoft Translator on German, perform signiﬁ- cantly better on male roles, which may stem from these being more frequent in the training set. \nPerhaps most tellingly,    $\\Delta_{S}$   measures the differ- "}
{"page": 3, "image_path": "doc_images/P19-1164_3.jpg", "ocr_text": "100\n\n10 Stereotypical |] 1 Non-Stereotypical\n& 80 a 76\nZz 67 69\niy 60 57\n2 60 545\n5 46 44 46 44\n2 40 30 33 35 3\n20\nES FR IT RU UK HE AR DE\nFigure 2: Google Translate’s performance on gender translation on our tested languages. The performance on the\n\nstereotypical portion of WinoMT is consistently better than that on the non-stereotypical portion. The other MT\n\nsystems we tested display similar trends.\n\nOriginal +Adj A\nES 53.1 63.5 +10.4\nRU 37.7 48.9 411.2\nUK 38.4 42.9 +445\n\nTable 4: Performance of Google Translate on Spanish,\nRussian, and Ukranian gender prediction accuracy (%\ncorrect) on the original WinoMT corpus, versus a mod-\nified version of the dataset where we add sterotypical\ngender adjectives (see Section 3.3).\n\nence in performance (F| score) between stereo-\ntypical and non-stereotypical gender role assign-\nments, as defined by Zhao et al. (2018) who\nuse statistics provided by the US Department of\nLabor.® This metric shows that all tested sys-\ntems have a significant and consistently better per-\nformance when presented with pro-stereotypical\nassignments (e.g., a female nurse), while their\nperformance deteriorates when translating anti-\nstereotypical roles (e.g., a male receptionist).\nFor instance, Figure 2 depicts Google Trans-\nlate absolute accuracies on stereotypical and non-\nstereotypical gender roles across all tested lan-\nguages. Other tested systems show similar trends.\n\n3.3. Fighting Bias with Bias\n\nFinally, we tested whether we can affect the\ntranslations by automatically creating a version\nof WinoMT with the adjectives “handsome” and\n“pretty” prepended to male and female entities, re-\nspectively. For example, the sentence in Figure 1\nwill be converted to: “The pretty doctor asked the\nnurse to help her in the operation”. We are inter-\nested in evaluating whether this “corrects” the pro-\nfession bias by mixing signals, e.g., while “doc-\n\n®https://www.bls.gov/cps/cpsaatll.htm\n\ntor” biases towards a male translation, “pretty”\ntugs the translation towards a female inflection.\nOur results show that this improved performance\nin some languages, significantly reducing bias in\nSpanish, Russian, and Ukrainian (see Table 4).\nAdmittedly, this is impractical as a general debi-\nasing scheme, since it assumes oracle coreference\nresolution, yet it attests to the relation between\ncoreference resolution and MT, and serves as a fur-\nther indication of gender bias in MT.\n\n3.4 Human Validation\n\nWe estimate the accuracy of our gender bias evalu-\nation method by randomly sampling 100 instances\nof all translation systems and target languages, an-\nnotating each sample by two target-language na-\ntive speakers (resulting in 9,600 human annota-\ntions). Each instance conformed to a format sim-\nilar to that used by our automatic gender detec-\ntion algorithm: human annotators were asked to\nmark the gender of an entity within a given target-\nlanguage sentence. (e.g., see “el doctor” as high-\nlighted in the Spanish sentence in Figure 1). By\nannotating at the sentence-level, we can account\nfor both types of possible errors, i.e., alignment\nand gender extraction.\n\nWe compare the sentence-level human anno-\ntations to the output of our automatic method,\nand find that the levels of agreement for all lan-\nguages and systems were above 85%, with an\naverage agreement on 87% of the annotations.\nIn comparison, human inter-annotator agreement\nwas 90%, due to noise introduced by several inco-\nherent translations.\n\nOur errors occur when language-specific id-\niosyncrasies introduce ambiguity to the morpho-\nlogical analysis. For example, gender for certain\nwords in Hebrew cannot be distinguished without\n\n1682\n", "vlm_text": "The image is a bar chart displaying the accuracy of Google Translate's performance on gender translation for different languages, specifically comparing stereotypical and non-stereotypical translations. The languages tested are Spanish (ES), French (FR), Italian (IT), Russian (RU), Ukrainian (UK), Hebrew (HE), Arabic (AR), and German (DE).\n\nFor each language, there are two bars: one in purple representing the accuracy for stereotypical translations and another in red for non-stereotypical translations. The chart shows that the accuracy for stereotypical translations is consistently higher across all tested languages compared to non-stereotypical translations. Here are the specific accuracy percentages displayed in the chart:\n\n- Spanish (ES): Stereotypical 67%, Non-Stereotypical 46%\n- French (FR): Stereotypical 80%, Non-Stereotypical 54%\n- Italian (IT): Stereotypical 52%, Non-Stereotypical 30%\n- Russian (RU): Stereotypical 44%, Non-Stereotypical 33%\n- Ukrainian (UK): Stereotypical 46%, Non-Stereotypical 35%\n- Hebrew (HE): Stereotypical 76%, Non-Stereotypical 38%\n- Arabic (AR): Stereotypical 60%, Non-Stereotypical 44%\n- German (DE): Stereotypical 69%, Non-Stereotypical 57%\nThe table displays data for three entities, labeled as ES, RU, and UK. It consists of three columns: \"Original,\" \"+Adj,\" and \"Δ.\" \n\n- For ES, the \"Original\" value is 53.1, the \"+Adj\" value is 63.5, and the change (Δ) is +10.4.\n- For RU, the \"Original\" value is 37.7, the \"+Adj\" value is 48.9, and the change (Δ) is +11.2.\n- For UK, the \"Original\" value is 38.4, the \"+Adj\" value is 42.9, and the change (Δ) is +4.5.\n\nThe table shows the original values, adjusted values, and the difference between them for each entity.\nTable 4: Performance of Google Translate on Spanish, Russian, and Ukranian gender prediction accuracy (  $\\%$  correct) on the original WinoMT corpus, versus a mod- iﬁed version of the dataset where we add sterotypical gender adjectives (see Section  3.3 ). \nence in performance (  ${\\cal F}_{1}$   score) between stereo- typical and non-stereotypical gender role assign- ments, as deﬁned by  Zhao et al.  ( 2018 ) who use statistics provided by the US Department of Labor. This metric shows that all tested sys- tems have a signiﬁcant and consistently better per- formance when presented with pro-stereotypical assignments (e.g., a female nurse), while their performance deteriorates when translating anti- stereotypical roles (e.g., a male receptionist). For instance, Figure  2  depicts Google Trans- late absolute accuracies on stereotypical and non- stereotypical gender roles across all tested lan- guages. Other tested systems show similar trends. \n3.3 Fighting Bias with Bias \nFinally, we tested whether we can affect the translations by automatically creating a version of WinoMT with the adjectives “handsome” and “pretty” prepended to male and female entities, re- spectively. For example, the sentence in Figure  1 will be converted to: “The  pretty  doctor asked the nurse to help her in the operation”. We are inter- ested in evaluating whether this “corrects” the pro- fession bias by mixing signals, e.g., while “doc- tor” biases towards a male translation, “pretty” tugs the translation towards a female inﬂection. Our results show that this improved performance in some languages, signiﬁcantly reducing bias in Spanish, Russian, and Ukrainian (see Table  4 ). Admittedly, this is impractical as a general debi- asing scheme, since it assumes oracle coreference resolution, yet it attests to the relation between coreference resolution and MT, and serves as a fur- ther indication of gender bias in MT. \n\n3.4 Human Validation \nWe estimate the accuracy of our gender bias evalu- ation method by randomly sampling 100 instances of all translation systems and target languages, an- notating each sample by two target-language na- tive speakers (resulting in 9,600 human annota- tions). Each instance conformed to a format sim- ilar to that used by our automatic gender detec- tion algorithm: human annotators were asked to mark the gender of an entity within a given target- language sentence. (e.g., see “el doctor” as high- lighted in the Spanish sentence in Figure  1 ). By annotating at the sentence-level, we can account for both types of possible errors, i.e., alignment and gender extraction. \nWe compare the sentence-level human anno- tations to the output of our automatic method, and ﬁnd that the levels of agreement for all lan- guages and systems were above   $85\\%$  , with an average agreement on   $87\\%$   of the annotations. In comparison, human inter-annotator agreement was  $90\\%$  , due to noise introduced by several inco- herent translations. \nOur errors occur when language-speciﬁc id- iosyncrasies introduce ambiguity to the morpho- logical analysis. For example, gender for certain words in Hebrew cannot be distinguished without "}
{"page": 4, "image_path": "doc_images/P19-1164_4.jpg", "ocr_text": "Source\n\n[Target lang.] Predicted translation\n\nPhenomenon\n\nThe janitor does not like the baker\nbecause she always messes up the\nkitchen.\n\n[ES] Al conserje no le gusta el panadero\nporque ella siempre desordena la cocina.\n\nBiased translation, giving “baker” a\nmale inflection, with a mismatched pro-\nnoun reference.\n\nThe janitor does not like the pretty\nbaker because she always messes up\nthe kitchen.\n\n[ES] Al conserje no le gusta la panadera\nbonita porque ella siempre desordena la\ncocina.\n\nAdding a stereotypically female adjec-\ntive “fixes” the translation.\n\nThe counselor asked the guard a few\nquestions and praised her for the good\nwork.\n\n[FR] Le conseiller a posé quelques ques-\ntions a la garde et I’a louée pour le bon\ntravail.\n\nFrench uses “garde” for both male and\nfemale guards, allowing for a more di-\nrect translation from English.\n\nTable 5: Examples of Google Translate’s output for different sentences in the WinoMT corpus. Words in blue,\n\nred, and orange indicate male, female and neutral entities, respectively.\n\ndiacritics (e.g., the male and female versions of the\nword “baker” are spelled identically), and the con-\ntracted determiner in French and Italian (/’) is used\nfor both masculine and feminine nouns. In ad-\ndition, some languages have only male or female\ninflections for professions which were stereotypi-\ncally associated with one of the genders, for exam-\nple “sastre” (tailor) in Spanish or “soldat” (soldier)\nin French, which do not have female inflections.\nSee Table 5 for detailed examples.\n\n4 Discussion\n\nRelated work This work is most related to sev-\neral recent efforts which evaluate MT through\nthe use of challenge sets. Similarly to our use\nWinoMT, these works evaluate MT systems (ei-\nther manually or automatically) on test sets which\nare specially created to exhibit certain linguis-\ntic phenomena, thus going beyond the traditional\nBLEU metric (Papineni et al., 2002). These in-\nclude challenge sets for language-specific idiosyn-\ncrasies (Isabelle et al., 2017), discourse phenom-\nena (Bawden et al., 2018), pronoun translation\n(Miiller et al., 2018; Webster et al., 2018), or\ncoreference and multiword expressions (Burchardt\net al., 2017).\n\nLimitations and future work While our work\npresents the first large-scale evaluation of gender\nbias in MT, it still suffers from certain limitations\nwhich could be addressed in follow up work. First,\nlike some of the challenge sets discussed above,\nWinoMT is composed of synthetic English source-\nside examples. On the one hand, this allows for\na controlled experiment environment, while, on\nthe other hand, this might introduce some artifi-\ncial biases in our data and evaluation. Ideally,\nWinoMT could be augmented with natural “in the\nwild” instances, with many source languages, all\n\nannotated with ground truth entity gender. Sec-\nond, similar to any medium size test set, it is clear\nthat WinoMT serves only as a proxy estimation for\nthe phenomenon of gender bias, and would prob-\nably be easy to overfit. A larger annotated cor-\npus can perhaps provide a better signal for train-\ning. Finally, even though in Section 3.3 we show\na very rudimentary debiasing scheme which relies\non oracle coreference system, it is clear that this\nis not applicable in a real-world scenario. While\nrecent research has shown that getting rid of such\nbiases may prove to be very challenging (Elazar\nand Goldberg, 2018; Gonen and Goldberg, 2019),\nwe hope that this work will serve as a first step for\ndeveloping more gender-balanced MT models.\n\n5 Conclusions\n\nWe presented the first large-scale multilingual\nquantitative evidence for gender bias in MT,\nshowing that on eight diverse target languages,\nall four tested popular commercial systems and\ntwo recent state-of-the-art academic MT mod-\nels are significantly prone to translate based\non gender stereotypes rather than more mean-\ningful context. Our data and code are pub-\nlicly available at https://github.com/\ngabrielStanovsky/mt_gender.\n\nAcknowledgments\n\nWe would like to thank Mark Yatskar, Iz Beltagy,\nTim Dettmers, Ronan Le Bras, Kyle Richardson,\nAriel and Claudia Stanovsky, and Paola Virga for\nmany insightful discussions about the role gender\nplays in the languages evaluated in this work, as\nwell as the reviewers for their helpful comments.\n\n1683\n", "vlm_text": "The table demonstrates examples of translation bias and gender issues in machine translation from English to Spanish and French. It includes three columns: the \"Source\" text in English, the \"[Target lang.] Predicted translation,\" which shows the translated text, and the \"Phenomenon\" column that describes the translation issue observed.\n\n1. **First Row:**\n   - Source: \"The janitor does not like the baker because she always messes up the kitchen.\"\n   - Predicted Translation in Spanish: \"Al conserje no le gusta el panadero porque ella siempre desordena la cocina.\"\n   - Phenomenon: Biased translation, as the word \"baker\" is given a male inflection (\"el panadero\") in Spanish, which is mismatched with the female pronoun \"ella.\"\n\n2. **Second Row:**\n   - Source: \"The janitor does not like the pretty baker because she always messes up the kitchen.\"\n   - Predicted Translation in Spanish: \"Al conserje no le gusta la panadera bonita porque ella siempre desordena la cocina.\"\n   - Phenomenon: Adding the adjective \"pretty\" (a stereotypically female adjective) corrects the translation by properly matching the female noun (\"la panadera\") with the pronoun \"ella.\"\n\n3. **Third Row:**\n   - Source: \"The counselor asked the guard a few questions and praised her for the good work.\"\n   - Predicted Translation in French: \"Le conseiller a posé quelques questions à la garde et l’a louée pour le bon travail.\"\n   - Phenomenon: French uses the term \"garde\" for both male and female guards, allowing for a direct translation from English without gender bias, with \"l’a louée\" correctly reflecting the gender-specific direct object pronoun corresponding to \"her.\"\ndiacritics (e.g., the male and female versions of the word “baker” are spelled identically), and the con- tracted determiner in French and Italian   $(l^{,})$   is used for both masculine and feminine nouns. In ad- dition, some languages have only male or female inﬂections for professions which were stereotypi- cally associated with one of the genders, for exam- ple “sastre” (tailor) in Spanish or “soldat” (soldier) in French, which do not have female inﬂections. See Table  5  for detailed examples. \n4 Discussion \nRelated work This work is most related to sev- eral recent efforts which evaluate MT through the use of  challenge sets . Similarly to our use WinoMT, these works evaluate MT systems (ei- ther manually or automatically) on test sets which are specially created to exhibit certain linguis- tic phenomena, thus going beyond the traditional BLEU metric ( Papineni et al. ,  2002 ). These in- clude challenge sets for language-speciﬁc idiosyn- crasies ( Isabelle et al. ,  2017 ), discourse phenom- ena ( Bawden et al. ,  2018 ), pronoun translation ( M¨ uller et al. ,  2018 ;  Webster et al. ,  2018 ), or coreference and multiword expressions ( Burchardt et al. ,  2017 ). \nLimitations and future work While our work presents the ﬁrst large-scale evaluation of gender bias in MT, it still suffers from certain limitations which could be addressed in follow up work. First, like some of the challenge sets discussed above, WinoMT is composed of synthetic English source- side examples. On the one hand, this allows for a controlled experiment environment, while, on the other hand, this might introduce some artiﬁ- cial biases in our data and evaluation. Ideally, WinoMT could be augmented with natural “in the wild” instances, with many source languages, all annotated with ground truth entity gender. Sec- ond, similar to any medium size test set, it is clear that WinoMT serves only as a proxy estimation for the phenomenon of gender bias, and would prob- ably be easy to overﬁt. A larger annotated cor- pus can perhaps provide a better signal for train- ing. Finally, even though in Section  3.3  we show a very rudimentary debiasing scheme which relies on oracle coreference system, it is clear that this is not applicable in a real-world scenario. While recent research has shown that getting rid of such biases may prove to be very challenging ( Elazar and Goldberg ,  2018 ;  Gonen and Goldberg ,  2019 ), we hope that this work will serve as a ﬁrst step for developing more gender-balanced MT models. \n\n5 Conclusions \nWe presented the ﬁrst large-scale multilingual quantitative evidence for gender bias in MT, showing that on eight diverse target languages, all four tested popular commercial systems and two recent state-of-the-art academic MT mod- els are signiﬁcantly prone to translate based on gender stereotypes rather than more mean- ingful context. Our data and code are pub- licly available at https://github.com/ gabriel St a nov sky/mt_gender . \nAcknowledgments \nWe would like to thank Mark Yatskar, Iz Beltagy, Tim Dettmers, Ronan Le Bras, Kyle Richardson, Ariel and Claudia Stanovsky, and Paola Virga for many insightful discussions about the role gender plays in the languages evaluated in this work, as well as the reviewers for their helpful comments. "}
{"page": 5, "image_path": "doc_images/P19-1164_5.jpg", "ocr_text": "References\n\nMeni Adler and Michael Elhadad. 2006. An unsuper-\nvised morpheme-based HMM for Hebrew morpho-\nlogical disambiguation. In ACL.\n\nDuygu Altinok. 2018. DEMorphy, German language\nmorphological analyzer. CoRR, abs/1803.00902.\n\nDavid Alvarez-Melis and Tommi S. Jaakkola. 2017.\nA causal framework for explaining the predictions\nof black-box sequence-to-sequence models. In\nEMNLP.\n\nRachel Bawden, Rico Sennrich, Alexandra Birch, and\nBarry Haddow. 2018. Evaluating discourse phe-\nnomena in neural machine translation. In NAACL-\nHLT.\n\nTolga Bolukbasi, Kai-Wei Chang, James Y. Zou,\nVenkatesh Saligrama, and Adam Tauman Kalai.\n2016. Man is to computer programmer as woman\nis to homemaker? debiasing word embeddings. In\nNIPS.\n\nAljoscha Burchardt, Vivien Macketanz, Jon De-\nhdari, Georg Heigold, Jan-Thorsten Peter, and\nPhilip Williams. 2017. A linguistic evaluation of\nrule-based, phrase-based, and neural mt engines.\nThe Prague Bulletin of Mathematical Linguistics,\n108(1):159-170.\n\nGreville G Corbett. 2007. Gender and noun classes.\n\nColette G Craig. 1986. Noun Classes and Categoriza-\ntion: Proceedings of a Symposium on Categoriza-\ntion and Noun Classification, volume 7. John Ben-\njamins Publishing Company.\n\nChris Dyer, Victor Chahuneau, and Noah A. Smith.\n2013. A simple, fast, and effective reparameteriza-\ntion of ibm model 2. In HLT-NAACL.\n\nSergey Edunov, Myle Ott, Michael Auli, and David\nGrangier. 2018. Understanding back-translation at\nscale. arXiv preprint arXiv: 1808.09381.\n\nYanai Elazar and Yoav Goldberg. 2018. Adversarial\nremoval of demographic attributes from text data. In\nEMNLP.\n\nJoel Escudé Font and Marta R. Costa-Jussa. 2019.\nEqualizing gender biases in neural machine trans-\nlation with word embeddings techniques. CoRR,\nabs/1901.03116.\n\nHila Gonen and Yoav Goldberg. 2019. Lipstick on a\npig: Debiasing methods cover up systematic gender\nbiases in word embeddings but do not remove them.\nHLT-NAACL.\n\nJohn A Hawkins. 2015. A Comparative Typology of\nEnglish and German: Unifying the Contrasts. Rout-\nledge.\n\nMatthew Honnibal and Ines Montani. 2017. spaCy 2:\nNatural language understanding with Bloom embed-\ndings, convolutional neural networks and incremen-\ntal parsing. To appear.\n\nPierre Isabelle, Colin Cherry, and George F. Foster.\n2017. A challenge set approach to evaluating ma-\nchine translation. In EMNLP.\n\nMikhail Korobov. 2015. Morphological analyzer and\ngenerator for Russian and Ukrainian languages.\nIn Mikhail Yu. Khachay, Natalia Konstantinova,\nAlexander Panchenko, Dmitry I. Ignatov, and Va-\nleri G. Labunets, editors, Analysis of Images, Social\nNetworks and Texts, volume 542 of Communications\nin Computer and Information Science, pages 320-\n332. Springer International Publishing.\n\nJames Kuczmarski. 2018. Reducing gender bias in\ngoogle translate.\n\nHector J. Levesque. 2011. The Winograd schema chal-\nlenge. In AAAI Spring Symposium: Logical Formal-\nizations of Commonsense Reasoning.\n\nAngelica Mucchi-Faina. 2005. Visible or influential?\nlanguage reforms and gender (in) equality. Social\nScience Information, 44(1):189-215.\n\nMathias Miiller, Annette Rios, Elena Voita, and Rico\nSennrich. 2018. A large-scale test set for the evalu-\nation of context-aware pronoun translation in neural\nmachine translation. CoRR, abs/1810.02268.\n\nMyle Ott, Sergey Edunov, David Grangier, and\nMichael Auli. 2018. Scaling neural machine trans-\nlation. arXiv preprint arXiv: 1806.00187.\n\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic eval-\nuation of machine translation. In ACL.\n\nRachel Rudinger, Chandler May, and Benjamin Van\nDurme. 2017. Social bias in elicited natural lan-\nguage inferences. In EthNLP@EACL.\n\nRachel Rudinger, Jason Naradowsky, Brian Leonard,\nand Benjamin Van Durme. 2018. Gender bias in\ncoreference resolution. In NAACL-HLT.\n\nKellie Webster, Marta Recasens, Vera Axelrod, and Ja-\nson Baldridge. 2018. Mind the gap: A balanced\ncorpus of gendered ambiguous pronouns. Transac-\ntions of the Association for Computational Linguis-\ntics, 6:605-617.\n\nJieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Or-\ndonez, and Kai-Wei Chang. 2017. Men also like\nshopping: Reducing gender bias amplification using\ncorpus-level constraints. In EMNLP.\n\nJieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Or-\ndonez, and Kai-Wei Chang. 2018. Gender bias in\ncoreference resolution: Evaluation and debiasing\nmethods. In NAACL-HLT.\n\n1684\n", "vlm_text": "References \nMeni Adler and Michael Elhadad. 2006. An unsuper- vised morpheme-based HMM for Hebrew morpho- \nlogical disambiguation. In  ACL . Duygu Altinok. 2018. DEMorphy, German language morphological analyzer.  CoRR , abs/1803.00902. David Alvarez-Melis and Tommi S. Jaakkola. 2017. A causal framework for explaining the predictions of black-box sequence-to-sequence models. In EMNLP . Rachel Bawden, Rico Sennrich, Alexandra Birch, and Barry Haddow. 2018. Evaluating discourse phe- nomena in neural machine translation. In  NAACL- HLT . Tolga Bolukbasi, Kai-Wei Chang, James Y. Zou, Venkatesh Saligrama, and Adam Tauman Kalai. 2016. Man is to computer programmer as woman is to homemaker? debiasing word embeddings. In NIPS . Aljoscha Burchardt, Vivien Macketanz, Jon De- hdari, Georg Heigold, Jan-Thorsten Peter, and Philip Williams. 2017. A linguistic evaluation of rule-based, phrase-based, and neural mt engines. The Prague Bulletin of Mathematical Linguistics , 108(1):159–170. Greville G Corbett. 2007. Gender and noun classes. Colette G Craig. 1986.  Noun Classes and Categoriza- tion: Proceedings of a Symposium on Categoriza- tion and Noun Classiﬁcation , volume 7. John Ben- jamins Publishing Company. Chris Dyer, Victor Chahuneau, and Noah A. Smith. 2013. A simple, fast, and effective reparameteriza- tion of ibm model 2. In  HLT-NAACL . Sergey Edunov, Myle Ott, Michael Auli, and David Grangier. 2018. Understanding back-translation at scale.  arXiv preprint arXiv:1808.09381 . Yanai Elazar and Yoav Goldberg. 2018.  Adversarial removal of demographic attributes from text data . In EMNLP . Joel Escud´ e Font and Marta R. Costa-Juss\\` a. 2019. Equalizing gender biases in neural machine trans- lation with word embeddings techniques . CoRR , abs/1901.03116. Hila Gonen and Yoav Goldberg. 2019. Lipstick on a pig: Debiasing methods cover up systematic gender biases in word embeddings but do not remove them. HLT-NAACL . John A Hawkins. 2015.  A Comparative Typology of English and German: Unifying the Contrasts . Rout- ledge. \nMatthew Honnibal and Ines Montani. 2017. spaCy 2: Natural language understanding with Bloom embed- dings, convolutional neural networks and incremen- tal parsing.  To appear . Pierre Isabelle, Colin Cherry, and George F. Foster. 2017. A challenge set approach to evaluating ma- chine translation. In  EMNLP . Mikhail Korobov. 2015.  Morphological analyzer and generator for Russian and Ukrainian languages . In Mikhail Yu. Khachay, Natalia Konstantinova, Alexander Panchenko, Dmitry I. Ignatov, and Va- leri G. Labunets, editors,  Analysis of Images, Social Networks and Texts , volume 542 of  Communications in Computer and Information Science , pages 320– 332. Springer International Publishing. James Kuczmarski. 2018. Reducing gender bias in google translate . Hector J. Levesque. 2011. The Winograd schema chal- lenge. In  AAAI Spring Symposium: Logical Formal- izations of Commonsense Reasoning . Angelica Mucchi-Faina. 2005. Visible or inﬂuential? language reforms and gender (in) equality.  Social Science Information , 44(1):189–215. Mathias M¨ uller, Annette Rios, Elena Voita, and Rico Sennrich. 2018.  A large-scale test set for the evalu- ation of context-aware pronoun translation in neural machine translation .  CoRR , abs/1810.02268. Myle Ott, Sergey Edunov, David Grangier, and Michael Auli. 2018. Scaling neural machine trans- lation.  arXiv preprint arXiv:1806.00187 . Kishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu. 2002. Bleu: a method for automatic eval- uation of machine translation. In  ACL . Rachel Rudinger, Chandler May, and Benjamin Van Durme. 2017. Social bias in elicited natural lan- guage inferences. In  EthNLP@EACL . Rachel Rudinger, Jason Naradowsky, Brian Leonard, and Benjamin Van Durme. 2018. Gender bias in coreference resolution. In  NAACL-HLT . Kellie Webster, Marta Recasens, Vera Axelrod, and Ja- son Baldridge. 2018. Mind the gap: A balanced corpus of gendered ambiguous pronouns.  Transac- tions of the Association for Computational Linguis- tics , 6:605–617. Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Or- donez, and Kai-Wei Chang. 2017. Men also like shopping: Reducing gender bias ampliﬁcation using corpus-level constraints. In  EMNLP . Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Or- donez, and Kai-Wei Chang. 2018. Gender bias in coreference resolution: Evaluation and debiasing methods. In  NAACL-HLT . "}
