{"page": 0, "image_path": "doc_images/P18-2092_0.jpg", "ocr_text": "Exploiting Document Knowledge\nfor Aspect-level Sentiment Classification\n\nRuidan He‘, Wee Sun Lee', Hwee Tou Ng’, and Daniel Dahimeier*\n‘Department of Computer Science, National University of Singapore\nSAP Innovation Center Singapore\nif{ruidanhe, leews, nght }@comp -nus.edu.sg\nfa. dahlmeier@sap.com\n\nAbstract\n\nAttention-based long short-term memory\n(LSTM) networks have proven to be use-\nful in aspect-level sentiment classifica-\ntion. However, due to the difficulties\nin annotating aspect-level data, existing\npublic datasets for this task are all rela-\ntively small, which largely limits the ef-\nfectiveness of those neural models. In\nthis paper, we explore two approaches\nthat transfer knowledge from document-\nlevel data, which is much less expensive\nto obtain, to improve the performance of\naspect-level sentiment classification. We\ndemonstrate the effectiveness of our ap-\nproaches on 4 public datasets from Se-\nmEval 2014, 2015, and 2016, and we\nshow that attention-based LSTM benefits\nfrom document-level knowledge in multi-\nple ways.\n\n1 Introduction\n\nGiven a sentence and an opinion target (also called\nan aspect term) occurring in the sentence, aspect-\nlevel sentiment classification aims to determine\nthe sentiment polarity in the sentence towards the\nopinion target. An opinion target or target for short\nrefers to a word or a phrase describing an aspect of\nan entity. For example, in the sentence “This little\nplace has a cute interior decor but the prices are\nquite expensive’, the targets are interior decor and\nprices, and they are associated with positive and\nnegative sentiment respectively.\n\nA sentence may contain multiple sentiment-\ntarget pairs, thus one challenge is to separate\ndifferent opinion contexts for different targets.\nFor this purpose, state-of-the-art neural meth-\nods (Wang et al., 2016; Liu and Zhang, 2017; Chen\net al., 2017) adopt attention-based LSTM net-\nworks, where the LSTM aims to capture sequen-\ntial patterns and the attention mechanism aims\n\n579\n\nto emphasize target-specific contexts for encod-\ning sentence representations. Typically, LSTMs\nonly show their potential when trained on large\ndatasets. However, aspect-level training data re-\nquires the annotation of all opinion targets in a\nsentence, which is costly to obtain in practice. As\nsuch, existing public aspect-level datasets are all\nrelatively small. Insufficient training data limits\nthe effectiveness of neural models.\n\nDespite the lack of aspect-level labeled data,\nenormous document-level labeled data are eas-\nily accessible online such as Amazon reviews.\nThese reviews contain substantial linguistic pat-\nterns and come with sentiment labels naturally.\nIn this paper, we hypothesize that aspect-level\nsentiment classification can be improved by em-\nploying knowledge gained from document-level\nsentiment classification. Specifically, we ex-\nplore two transfer methods to incorporate this\nsort of knowledge — pretraining and multi-task\nlearning. In our experiments, we find that\nboth methods are helpful and combining them\nachieves significant improvements over attention-\nbased LSTM models trained only on aspect-level\ndata. We also illustrate by examples that ad-\nditional knowledge from document-level data is\nbeneficial in multiple ways. Our source code\ncan be obtained from https://github.com/\nruidan/Aspect-level-sentiment.\n\n2 Related Work\n\nVarious neural models (Dong et al., 2014; Nguyen\nand Shirai, 2015; Vo and Zhang, 2015; Tang et al.,\n2016a,b; Wang et al., 2016; Zhang et al., 2016;\nLiu and Zhang, 2017; Chen et al., 2017) have been\nproposed for aspect-level sentiment classification.\nThe main idea behind these works is to develop\nneural architectures that are able to learn continu-\nous features and capture the intricate relation be-\ntween a target and context words. However, to\nsufficiently train these models, substantial aspect-\n\nProceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 579-585\nMelbourne, Australia, July 15 - 20, 2018. ©2018 Association for Computational Linguistics\n", "vlm_text": "Exploiting Document Knowledge for Aspect-level Sentiment Classiﬁcation \nRuidan   $\\mathbf{H}\\mathbf{e}^{\\dag\\ddag}$  , Wee Sun Lee † , Hwee Tou  $\\mathbf{N}\\mathbf{g}^{\\dagger}$  ,  and  Daniel Dahlmeier ‡ † Department of Computer Science, National University of Singapore ‡ SAP Innovation Center Singapore \n† { ruidanhe,leews,nght } @comp.nus.edu.sg ‡ d.dahlmeier@sap.com \nAbstract \nAttention-based long short-term memory (LSTM) networks have proven to be use- ful in aspect-level sentiment classiﬁca- tion. However, due to the difﬁculties in annotating aspect-level data, existing public datasets for this task are all rela- tively small, which largely limits the ef- fectiveness of those neural models. In this paper, we explore two approaches that transfer knowledge from document- level data, which is much less expensive to obtain, to improve the performance of aspect-level sentiment classiﬁcation. We demonstrate the effectiveness of our ap- proaches on 4 public datasets from Se- mEval 2014, 2015, and 2016, and we show that attention-based LSTM beneﬁts from document-level knowledge in multi- ple ways. \n1 Introduction \nGiven a sentence and an opinion target (also called an aspect term) occurring in the sentence, aspect- level sentiment classiﬁcation aims to determine the sentiment polarity in the sentence towards the opinion target. An opinion target or target for short refers to a word or a phrase describing an aspect of an entity. For example, in the sentence “ This little place has a cute interior decor but the prices are quite expensive ”, the targets are  interior decor  and prices , and they are associated with positive and negative sentiment respectively. \nA sentence may contain multiple sentiment- target pairs, thus one challenge is to separate different opinion contexts for different targets. For this purpose, state-of-the-art neural meth- ods ( Wang et al. ,  2016 ;  Liu and Zhang ,  2017 ;  Chen et al. ,  2017 ) adopt attention-based LSTM net- works, where the LSTM aims to capture sequen- tial patterns and the attention mechanism aims to emphasize target-speciﬁc contexts for encod- ing sentence representations. Typically, LSTMs only show their potential when trained on large datasets. However, aspect-level training data re- quires the annotation of all opinion targets in a sentence, which is costly to obtain in practice. As such, existing public aspect-level datasets are all relatively small. Insufﬁcient training data limits the effectiveness of neural models. \n\nDespite the lack of aspect-level labeled data, enormous document-level labeled data are eas- ily accessible online such as Amazon reviews. These reviews contain substantial linguistic pat- terns and come with sentiment labels naturally. In this paper, we hypothesize that aspect-level sentiment classiﬁcation can be improved by em- ploying knowledge gained from document-level sentiment classiﬁcation. Speciﬁcally, we ex- plore two transfer methods to incorporate this sort of knowledge – pretraining and multi-task learning. In our experiments, we ﬁnd that both methods are helpful and combining them achieves signiﬁcant improvements over attention- based LSTM models trained only on aspect-level data. We also illustrate by examples that ad- ditional knowledge from document-level data is beneﬁcial in multiple ways. Our source code can be obtained from  https://github.com/ ruidan/Aspect-level-sentiment . \n2 Related Work \nVarious neural models ( Dong et al. ,  2014 ;  Nguyen and Shirai ,  2015 ;  Vo and Zhang ,  2015 ;  Tang et al. , 2016a , b ;  Wang et al. ,  2016 ;  Zhang et al. ,  2016 ; Liu and Zhang ,  2017 ;  Chen et al. ,  2017 ) have been proposed for aspect-level sentiment classiﬁcation. The main idea behind these works is to develop neural architectures that are able to learn continu- ous features and capture the intricate relation be- tween a target and context words. However, to sufﬁciently train these models, substantial aspect- level annotated data is required, which is expen- sive to obtain in practice. "}
{"page": 1, "image_path": "doc_images/P18-2092_1.jpg", "ocr_text": "level annotated data is required, which is expen-\nsive to obtain in practice.\n\nWe explore both pretraining and multi-task\nlearning for transferring knowledge from docu-\nment level to aspect level. Both methods are\nwidely studied in the literature. Pretraining is a\ncommon technique used in computer vision where\nlow-level neural layers can be usefully transferred\nto different tasks (Krizhevsky and Sutskever,\n2012; Zeiler and Fergus, 2014). In natural lan-\nguage processing (NLP), some efforts have been\ninitiated on pretraining LSTMs (Dai and Le, 2015;\nZoph et al., 2016; Ramachandran et al., 2017)\nfor sequence-to-sequence models in both super-\nvised and unsupervised settings, where promising\nresults have been obtained. On the other hand,\nmulti-task learning simultaneously trains on sam-\nples in multiple tasks with a combined objec-\ntive (Collobert and Weston, 2008; Luong et al.,\n2015a; Liu et al., 2016), which has improved\nmodel generalization ability in certain cases. In\nthe work of Mou et al. (2016), the authors investi-\ngated the transferability of neural models in NLP\napplications with extensive experiments, showing\nthat transferability largely depends on the seman-\ntic relatedness of the source and target tasks. For\nour problem, we hypothesize that aspect-level sen-\ntiment classification can be improved by employ-\ning knowledge gained from document-level senti-\nment classification, as these two tasks are highly\nrelated semantically.\n\n3 Models\n3.1 Attention-based LSTM\n\nWe first describe a conventional implementation\nof an attention-based LSTM model for this task.\nWe use it as a baseline model and extend it with\npretraining and multi-task learning approaches for\nincorporating document-level knowledge.\n\nThe inputs are a sentence s = (wy, Wa, ..., Wn)\nconsisting of n words, and an opinion target x =\n(1, £2, ..., Um) Occurring in the sentence consist-\ning of a subsequence of m words from s. Each\nword is associated with a continuous word embed-\nding e,, (Mikolov et al., 2013) from an embedding\nmatrix E € RY*¢, where V is the vocabulary size\nand d is the embedding dimension.\n\nLSTM is used to capture sequential informa-\ntion, and outputs a sequence of hidden vectors:\n\n{hy, ...,h,] = LSTM([ew,,---; Cw], stm) ()\n\nAn attention layer assigns a weight a; to each\nword in the sentence. The final target-specific rep-\nresentation of the sentence s is then given by:\n\nn\n\ni=1\nAnd a; is computed as follows:\n\nexp((;)\nyy exp(5;)\n8: = fecore(hi,t) = tanh(h? Wat) (4)\n\n(3)\n\nQa =\n\nm\n\n1\nt=— 5\nm yes (5)\n\nwhere t is the target representation computed as\nthe averaged word embedding of the target. fscore\nis a content-based function that captures the se-\nmantic association between a word and the target,\nfor which we adopt the formulation used in (Lu-\nong et al., 2015b; He et al., 2017) with parameter\nmatrix W, € R¢*4.\n\nThe sentence representation z is fed into an out-\nput layer to predict the probability distribution p\nover sentiment labels on the target:\n\np = softmax(W,z + b,) (6)\n\nWe refer to this baseline model as LSTM+ATT. It\nis trained via cross entropy minimization:\n\nJ=- > log pi(ci) (7)\nicD\nwhere D denotes the overall training corpus, c; de-\nnotes the true label for sample i, and p;(c;) de-\nnotes the probability of the true label.\n\n3.2. Transfer Approaches\n\nLSTM+ATT is used as our aspect-level\n\nmodel with parameter set Oaspect =\n{E, @1stm, Wa, Wo, bo}- We also build a\nstandard LSTM-based classifier based on\n\ndocument-level training examples. This network\nis the same as the LSTM+ATT apart from the\nlack of the attention layer. The training ob-\njective is also cross entropy minimization as\nshown in equation (7) and the parameter set is\ndoc = {E', Or gtms WS, bb}-\n\nPretraining (PRET): In this setting, we first train\non document-level examples. The last hidden vec-\ntor from the LSTM outputs is used as the doc-\nument representation. We initialize the relevant\n\n580\n", "vlm_text": "\nWe explore both pretraining and multi-task learning for transferring knowledge from docu- ment level to aspect level. Both methods are widely studied in the literature. Pretraining is a common technique used in computer vision where low-level neural layers can be usefully transferred to different tasks ( Krizhevsky and Sutskever , 2012 ;  Zeiler and Fergus ,  2014 ). In natural lan- guage processing (NLP), some efforts have been initiated on pretraining LSTMs ( Dai and Le ,  2015 ; Zoph et al. ,  2016 ;  Ramachandran et al. ,  2017 ) for sequence-to-sequence models in both super- vised and unsupervised settings, where promising results have been obtained. On the other hand, multi-task learning simultaneously trains on sam- ples in multiple tasks with a combined objec- tive ( Collobert and Weston ,  2008 ;  Luong et al. , 2015a ;  Liu et al. ,  2016 ), which has improved model generalization ability in certain cases. In the work of Mou et al. ( 2016 ), the authors investi- gated the transferability of neural models in NLP applications with extensive experiments, showing that transferability largely depends on the seman- tic relatedness of the source and target tasks. For our problem, we hypothesize that aspect-level sen- timent classiﬁcation can be improved by employ- ing knowledge gained from document-level senti- ment classiﬁcation, as these two tasks are highly related semantically. \n3 Models \n3.1 Attention-based LSTM \nWe ﬁrst describe a conventional implementation of an attention-based LSTM model for this task. We use it as a baseline model and extend it with pretraining and multi-task learning approaches for incorporating document-level knowledge. \nThe inputs are a sentence    $s=(w_{1},w_{2},...,w_{n})$  consisting of    $n$   words, and an opinion target    $x=$   $(x_{1},x_{2},...,x_{m})$   occurring in the sentence consist- ing of a subsequence of    $m$   words from    $s$  . Each word is associated with a continuous word embed- ding  $\\mathbf{e}_{w}$   ( Mikolov et al. ,  2013 ) from an embedding mat x    $\\mathbf{E}\\in\\mathbb{R}^{V\\times d}$  , where  $V$   is the vocabulary size and  d  is the embedding dimension. \nLSTM is used to capture sequential informa- tion, and outputs a sequence of hidden vectors: \n\n$$\n[\\mathbf{h}_{1},...,\\mathbf{h}_{n}]=\\mathrm{{LSTM}}([\\mathbf{e}_{w_{1}},...,\\mathbf{e}_{w_{n}}],\\theta_{l s t m})\n$$\n \nAn attention layer assigns a weight    $\\alpha_{i}$   to each word in the sentence. The ﬁnal target-speciﬁc rep- resentation of the sentence    $s$   is then given by: \n\n$$\n\\mathbf{z}=\\sum_{i=1}^{n}\\alpha_{i}\\mathbf{h}_{i}\n$$\n \nAnd    $\\alpha_{i}$   is computed as follows: \n\n$$\n\\begin{array}{r l}&{\\boldsymbol{\\alpha}_{i}=\\frac{\\exp(\\boldsymbol{\\beta}_{i})}{\\sum_{j=1}^{n}\\exp(\\boldsymbol{\\beta}_{j})}}\\\\ &{\\boldsymbol{\\beta}_{i}=f_{s c o r e}(\\mathbf{h}_{i},\\mathbf{t})=t a n h(\\mathbf{h}_{i}^{T}\\mathbf{W}_{a}\\mathbf{t})}\\\\ &{\\mathbf{t}=\\cfrac{1}{m}\\sum_{i=1}^{m}\\mathbf{e}_{x_{i}}}\\end{array}\n$$\n \nwhere    $\\mathbf{t}$   is the target representation computed as the averaged word embedding of the target.    $f_{s}$  score is a content-based function that captures the se- mantic association between a word and the target, for which we adopt the formulation used in ( Lu- ong et al. ,  2015b ;  He et al. ,  2017 ) with parameter matrix  $\\mathbf{W}_{a}\\in\\mathbb{R}^{d\\times d}$  . \nThe sentence representation    $\\mathbf{z}$   is fed into an out- put layer to predict the probability distribution    $\\mathbf{p}$  over sentiment labels on the target: \n\n$$\n\\mathbf{p}=s o f t m a x(\\mathbf{W}_{o}\\mathbf{z}+\\mathbf{b}_{o})\n$$\n \nWe refer to this baseline model as LSTM+ATT. It is trained via cross entropy minimization: \n\n$$\nJ=-\\sum_{i\\in D}\\log\\mathbf{p}_{i}(c_{i})\n$$\n \nwhere  $D$   denotes the overall training corpus,  $c_{i}$   de- notes the true label for sample    $i$  , and    $\\mathbf{p}_{i}(c_{i})$   de- notes the probability of the true label. \n3.2 Transfer Approaches \nLSTM  $^+$  ATT is used as our aspect-level model with parameter set  $\\begin{array}{r l r}{\\theta_{a s p e c t}\\ }&{{}}&{=}\\end{array}$   $\\{\\mathbf{E},\\theta_{l s t m},\\mathbf{W}_{a},\\mathbf{W}_{o},\\mathbf{b}_{o}\\}$  . We also build a standard LSTM-based classiﬁer based on document-level training examples. This network is the same as the   $\\mathrm{LSTM+ATT}$   apart from the lack of the attention layer. The training ob- jective is also cross entropy minimization as shown in equation ( 7 ) and the parameter set is  $\\theta_{d o c}=\\{\\mathbf{E}^{\\prime},\\theta_{l s t m}^{\\prime},\\mathbf{W}_{o}^{\\prime},\\mathbf{b}_{o}^{\\prime}\\}$  } . \nPretraining  (PRET): In this setting, we ﬁrst train on document-level examples. The last hidden vec- tor from the LSTM outputs is used as the doc- ument representation. We initialize the relevant "}
{"page": 2, "image_path": "doc_images/P18-2092_2.jpg", "ocr_text": "Neu\n637\n196\n464\n169\n50\n35\n88\n38\n\nPos\n2164\n728\n994\n341\n1178\n439\n1620\n597\n\nDataset Neg\n807\n196\n870\n128\n382\n328\n709\n190\n\nRestaurant 14-Train\nRestaurant 14-Test\nLaptop14-Train\nLaptop14-Test\n\nDiI\n\nD2\n\nRestaurant 15-Train\n\nD3\n\nD4 Restaurant 16-Test\n\nTable 1: Dataset description.\n\nparameters E, 6istm, Wo, bo of LSTM+ATT with\nthe pretrained weights, and train it on aspect-level\nexamples to fine tune those weights and learn Wa\nwhich is randomly initialized.\n\nMulti-task Learning (MULT): This approach si-\nmultaneously trains two tasks — document-level\nand aspect-level classification. In this setting, the\nembedding layer (E) and the LSTM layer (stm)\nare shared by both tasks, and a document is rep-\nresented as the mean vector over LSTM outputs.\nThe other parameters are task-specific. The over-\nall loss function is then given by:\n\nL=J+U (8)\n\nwhere U is the loss function of document-level\nclassification. \\ € (0,1) is a hyperparameter that\ncontrols the weight of U.\n\nCombined (PRET+MULT): In this setting, we\nfirst perform PRET on document-level exam-\nples. We use the pretrained weights for parame-\nter initialization for both aspect-level model and\ndocument-level model, and then perform MULT\nas discussed above.\n\n4 Experiments\n\n4.1 Datasets and Experimental Settings\n\nWe run experiments on four benchmark aspect-\nlevel datasets, taken from SemEval 2014 (Pontiki\net al., 2014), SemEval 2015 (Pontiki et al., 2015),\nand SemEval 2016 (Pontiki et al., 2016). Fol-\nlowing previous work (Tang et al., 2016b; Wang\net al., 2016), we remove samples with conflicting\npolarities in all datasets! . Statistics of the resulting\ndatasets are presented in Table 1.\n\nWe derived two document-level datasets from\nYelp2014 (Tang et al., 2015) and the Amazon\nElectronics dataset (McAuley et al., 2015) respec-\ntively. The original reviews were rated on a 5-\npoint scale. We consider 3-class classification and\n\n'We remove samples in the 2015/6 datasets if an opinion\ntarget is associated with different sentiment polarities.\n\nthus label reviews with rating < 3, > 3, and = 3 as\nnegative, positive, and neutral respectively. Each\nsampled dataset contains 30k instances with ex-\nactly balanced class labels. We pair up an aspect-\nlevel dataset and a document-level dataset when\nthey are from a similar domain — the Yelp dataset\nis used by D1, D3, and D4 for PRET and MULT,\nand the Electronics dataset is only used by D2.\n\nIn all experiments, 300-dimension GloVe vec-\ntors (Pennington et al., 2014) are used to initialize\nE and E’ when pretraining is not conducted for\nweight initialization. These vectors are also used\nfor initializing E’ in the pretraining phase. Val-\nues for hyperparameters are obtained from experi-\nments on development sets. We randomly sample\n20% of the original training data from the aspec\nlevel dataset as the development set and only use\nthe remaining 80% for training. For all experi\nments, the dimension of LSTM hidden vectors is\nset to 300, A is set to 0.1, and we use dropout with\nprobability 0.5 on sentence/document representa-\ntions before the output layer. We use RMSProp\nas the optimizer with the decay rate set to 0.9 and\nthe base learning rate set to 0.001. The mini-batch\nsize is set to 32.\n\n4.2 Model Comparison\n\nTable 2 shows the results of LSTM, LSTM+ATT,\nPRET, MULT, PRET+MULT, and four representa-\ntive prior works (Tang et al., 2016a,b; Wang et al.,\n2016; Chen et al., 2017). Significance tests are\nconducted for testing the robustness of methods\nunder random parameter initialization. Both accu-\nracy and macro-F1 are used for evaluation as label\ndistribution is unbalanced. The reported numbers\nare obtained as the average value over 5 runs with\nrandom initialization for each method.\n\nWe observe that PRET is very helpful, and con-\nsistently gives a 1-3% increase in accuracy over\nLSTM+ATT across all datasets. The improve-\nments in macro-Fl scores are even more, espe-\ncially on D3 and D4 where the labels are ex-\ntremely unbalanced. MULT gives similar perfor-\nmance as LSTM+ATT on D1 and D2, but im-\nprovements can be clearly observed for D3 and\nD4. The combination (PRET+MULT) overall\nyields better results.\n\nThere are two main reasons why the improve-\nments of macro-F1 scores are more significant on\nD3 and D4 than on D1: (1) D1 has much more\nneutral examples in the training set. A classifier\n", "vlm_text": "The table contains information about datasets labeled D1 to D4, indicating the number of positive (Pos), negative (Neg), and neutral (Neu) entries for both training and test sets. Here’s the breakdown:\n\n- **D1:**\n  - Restaurant14-Train: 2164 Pos, 807 Neg, 637 Neu\n  - Restaurant14-Test: 728 Pos, 196 Neg, 196 Neu\n\n- **D2:**\n  - Laptop14-Train: 994 Pos, 870 Neg, 464 Neu\n  - Laptop14-Test: 341 Pos, 128 Neg, 169 Neu\n\n- **D3:**\n  - Restaurant15-Train: 1178 Pos, 382 Neg, 50 Neu\n  - Restaurant15-Test: 439 Pos, 328 Neg, 35 Neu\n\n- **D4:**\n  - Restaurant16-Train: 1620 Pos, 709 Neg, 88 Neu\n  - Restaurant16-Test: 597 Pos, 190 Neg, 38 Neu\nparameters    $\\mathbf{E},\\theta_{l s t m},\\mathbf{W}_{o},\\mathbf{b}_{o}$   of  $\\mathrm{LSTM+ATT}$   with the pretrained weights, and train it on aspect-level examples to ﬁne tune those weights and learn    ${\\bf W}_{a}$  which is randomly initialized. \nMulti-task Learning  (MULT): This approach si- multaneously trains two tasks – document-level and aspect-level classiﬁcation. In this setting, the embedding layer   $\\mathbf{\\Psi}(\\mathbf{E})$   and the LSTM layer   $(\\theta_{l s t m})$  are shared by both tasks, and a document is rep- resented as the mean vector over LSTM outputs. The other parameters are task-speciﬁc. The over- all loss function is then given by: \n\n$$\nL=J+\\lambda U\n$$\n \nwhere    $U$   is the loss function of document-level classiﬁcation.    $\\lambda\\in(0,1)$   is a hyperparameter that controls the weight of  U . \nCombined  (PRET  $^+$  MULT): In this setting, we ﬁrst perform PRET on document-level exam- ples. We use the pretrained weights for parame- ter initialization for both aspect-level model and document-level model, and then perform MULT as discussed above. \n4 Experiments \n4.1 Datasets and Experimental Settings \nWe run experiments on four benchmark aspect- level datasets, taken from SemEval 2014 ( Pontiki et al. ,  2014 ), SemEval 2015 ( Pontiki et al. ,  2015 ), and SemEval 2016 ( Pontiki et al. ,  2016 ). Fol- lowing previous work ( Tang et al. ,  2016b ;  Wang et al. ,  2016 ), we remove samples with  conﬂicting polarities in all datasets 1 . Statistics of the resulting datasets are presented in Table  1 . \nWe derived two document-level datasets from Yelp2014 ( Tang et al. ,  2015 ) and the Amazon Electronics dataset ( McAuley et al. ,  2015 ) respec- tively. The original reviews were rated on a 5- point scale. We consider 3-class classiﬁcation and thus label reviews with rating  $<3,>3$  , and  $=3$   as negative, positive, and neutral respectively. Each sampled dataset contains   $30\\mathbf{k}$   instances with ex- actly balanced class labels. We pair up an aspect- level dataset and a document-level dataset when they are from a similar domain – the Yelp dataset is used by D1, D3, and D4 for PRET and MULT, and the Electronics dataset is only used by D2. \n\nIn all experiments, 300-dimension GloVe vec- tors ( Pennington et al. ,  2014 ) are used to initialize  $\\mathbf{E}$   and    $\\mathbf{E}^{\\prime}$    when pretraining is not conducted for weight initialization. These vectors are also used for initializing    $\\mathbf{E}^{\\prime}$    in the pretraining phase. Val- ues for hyperparameters are obtained from experi- ments on development sets. We randomly sample  $20\\%$   of the original training data from the aspect- level dataset as the development set and only use the remaining   $80\\%$   for training. For all experi- ments, the dimension of LSTM hidden vectors is set to 300,    $\\lambda$   is set to 0.1, and we use dropout with probability 0.5 on sentence/document representa- tions before the output layer. We use RMSProp as the optimizer with the decay rate set to 0.9 and the base learning rate set to 0.001. The mini-batch size is set to 32. \n4.2 Model Comparison \nTable  2  shows the results of LSTM, LSTM  $+$  ATT, PRET, MULT, PRET  $^+$  MULT, and four representa- tive prior works ( Tang et al. ,  2016a , b ;  Wang et al. , 2016 ;  Chen et al. ,  2017 ). Signiﬁcance tests are conducted for testing the robustness of methods under random parameter initialization. Both accu- racy and macro-F1 are used for evaluation as label distribution is unbalanced. The reported numbers are obtained as the average value over 5 runs with random initialization for each method. \nWe observe that PRET is very helpful, and con- sistently gives a   $1{-}3\\%$   increase in accuracy over LSTM  $+$  ATT across all datasets. The improve- ments in macro-F1 scores are even more, espe- cially on D3 and D4 where the labels are ex- tremely unbalanced. MULT gives similar perfor- mance as   $\\mathrm{LSTM+ATT}$   on D1 and D2, but im- provements can be clearly observed for D3 and D4. The combination (PRET  $^+$  MULT) overall yields better results. \nThere are two main reasons why the improve- ments of macro-F1 scores are more signiﬁcant on D3 and D4 than on D1: (1) D1 has much more neutral examples in the training set. A classiﬁer "}
{"page": 3, "image_path": "doc_images/P18-2092_3.jpg", "ocr_text": "Methods DI D2 D3 D4\nAcc. — Macro-Fl — Acc. — Macro-Fl Acc. Macro-F1 Acc. Macro-F1\n\nTang et al. (2016a) 75.37 64.51 68.25 65.96 76.39 58.70 82.16 54.21\nWang et al. (2016) 78.60 67.02 68.88 63.93 78.48 62.84 83.77 61.71\nTang et al. (2016b) 76.87 66.40 68.91 62.79 77.89 59.52 83.04 57.91\nChen et al. (2017) 78.48 68.54 72.08 68.43 79.98 60.57 83.88 62.14\nLSTM 75.23 64.21 66.79 64.02 75.28 54.10 81.95 58.11\nLSTM+ATT 76.83 66.48 68.07 64.82 77.38 60.52 82.73 59.12\nOurs: PRET 78.28 68.55 71.32 68.53 80.67 68.31 84.87 70.73\nOurs: MULT 7741 66.68 68.65 64.57 81.05 65.69 83.27 64.56\nOurs: PRET+MULT 79.11 69.73* TAS 67.46 81.30* 68.74\" 85.58\" 69.76\"\n\nTable 2: Average accuracies and Macro-F1 scores over 5 runs with random initialization. The best results\nare in bold. * indicates that PRET+MULT is significantly better than Tang et al. (2016a), Wang et al.\n(2016), Tang et al. (2016b), Chen et al. (2017), LSTM, and LSTM+ATT with p < 0.05 according to\n\none-tailed unpaired t-test.\n\nSettings Dl D2 D3 D4\nAcc. Macro-Fl — Acc. — Macro-Fl_ — Acc. — Macro-Fl_ — Acc. _ Macro-Fl\n\nLSTM only 78.09 67.85 71.04 66.80 78.95 65.30 83.85 67.11\nEmbeddings only T7AZ 67.19 69.12 65.06 80.13 67.04 84.12 70.11\nOutput layer only 76.88 66.81 69.63 66.07 78.30 64.49 82.55 62.83\nWithout LSTM 77.45 67.25 69.82 66.63 80.27 68.02 84.80 70.27\nWithout embeddings 77.97 67.96 70.59 67.16 79.08 65.56 83.94 68.79\nWithout output layer 78.36 68.06 71.10 67.87 80.82 67.68 84.71 70.48\n\nTable 3: PRET with different transferred layers. Averaged results over 5 runs are reported.\n\nwithout any external knowledge might still be able\nto learn some neutral-related features on D1 but it\nis very hard to learn from D3 and D4. (2) The\nnumbers of neutral examples in the test sets of\nD3 and D4 are very small. Thus, the precision\nand recall on neutral class will be largely affected\nby even a small prediction difference (e.g., with 5\nmore neutral examples correctly identified, recall\nis increased by more than 10% on both datasets).\nAs aresult, the macro-F1 scores on D3 and D4 are\naffected more.\n\n4.3 Ablation Tests\n\nTable 2 indicates that a large percentage of the per-\nformance gain comes from PRET. To better un-\nderstand the transfer effects of different layers —\nembedding layer (E), LSTM layer (0stm), and\noutput layer (W,, b,) — we conduct ablation tests\non PRET with different layers transfered from the\ndocument-level model to the aspect-level model.\nResults are presented in Table 3. “LSTM only”\ndenotes the setting where only the LSTM layer is\ntransferred, and “Without LSTM” denotes the set-\nting where only the embedding and output layers\nare transferred (excluding the LSTM layer). The\nkey observations are: (1) Transfer is helpful in\nall settings. Improvements over LSTM+ATT are\nobserved even when only one layer is transferred.\n(2) Overall, transfers of the LSTM and embedding\n\nlayer are more useful than the output layer. This\nis what we expect, since the output layer is nor-\nmally more task-specific. (3) Transfer of the em-\nbedding layer is more helpful on D3 and D4. One\npossible explanation is that the label distribution is\nextremely unbalanced on these two datasets. Sen-\ntiment information is not adequately captured by\nGloVe word embeddings. Therefore, with a small\nnumber of training examples in the negative and\nneutral classes, the embeddings trained by aspect-\nlevel classification still do not effectively capture\nthe true semantics of the relevant opinion words.\nTransfer of the embedding layer can greatly help\nin this case.\n\n4.4 Analysis\n\nTo show that aspect-level classification indeed\nbenefits from document-level knowledge, we\nconduct experiments to vary the percentage of\ndocument-level training examples from 0.0 to 1.0\nfor PRET+MULT. The changes of accuracies and\nmacro-F1 scores on the four datasets are shown in\nFigure 1. The improvements on accuracies with\nincreasing number of document examples are sta-\nble across all datasets. For macro-F1 scores, the\nimprovements on D1 and D2 are stable. We ob-\nserve sharp increases in the macro-F1 scores of\nD3 and D4 when changing the percentage from\n0 to 0.4. This may be related to their extremely\n\n582\n", "vlm_text": "The table presents performance metrics for various methods evaluated across four datasets: D1, D2, D3, and D4. For each dataset, accuracy (Acc.) and Macro-F1 scores are reported. The methods compared include:\n\n1. Tang et al. (2016a)\n2. Wang et al. (2016)\n3. Tang et al. (2016b)\n4. Chen et al. (2017)\n5. LSTM\n6. LSTM+ATT\n7. Ours: PRET\n8. Ours: MULT\n9. Ours: PRET+MULT\n\nFor each method and dataset, both accuracy and Macro-F1 scores are provided, with some scores marked with an asterisk (*) to possibly denote they are the best results in that particular column or highlight them for significance. The best results in terms of accuracy and Macro-F1 for each dataset are highlighted in bold. The \"Ours: PRET+MULT\" method frequently outperforms others, particularly in Macro-F1 scores across all datasets, as indicated by the asterisks.\nThe table presents the performance of different model settings on four datasets (D1, D2, D3, D4) in terms of accuracy (Acc.) and macro-F1 score (Macro-F1). The settings evaluated are:\n\n1. **LSTM only**: This setting uses only the LSTM component of the model.\n   - D1: 78.09% Acc., 67.85 Macro-F1\n   - D2: 71.04% Acc., 66.80 Macro-F1\n   - D3: 78.95% Acc., 65.30 Macro-F1\n   - D4: 83.85% Acc., 67.11 Macro-F1\n\n2. **Embeddings only**: This setting uses only the embeddings component.\n   - D1: 77.12% Acc., 67.19 Macro-F1\n   - D2: 69.12% Acc., 65.06 Macro-F1\n   - D3: 80.13% Acc., 67.04 Macro-F1\n   - D4: 84.12% Acc., 70.11 Macro-F1\n\n3. **Output layer only**: This setting uses only the output layer component.\n   - D1: 76.88% Acc., 66.81 Macro-F1\n   - D2: 69.63% Acc., 66.07 Macro-F1\n   - D3: 78.30% Acc., 64.49 Macro-F1\n   - D4: 82.55% Acc., 62.83 Macro-F1\n\n4. **Without LSTM**: This setting includes all components except the LSTM.\n   - D1: 77.45% Acc., 67.25 Macro-F1\n   - D2: 69.82% Acc., 66.63 Macro-F1\n   - D3: 80.27% Acc., 68.02 Macro-F1\n   - D4: 84.80% Acc., 70.27 Macro-F1\n\n5. **Without embeddings**: This setting includes all components except the embeddings.\n   - D1: 77.97% Acc., 67.96 Macro-F1\n   - D2: 70.59% Acc., 67.16 Macro-F1\n   - D3: 79.08% Acc., 65.56 Macro-F1\n   - D4: 83.94% Acc., 68.79 Macro-F1\n\n6. **Without output layer**: This setting includes all components except the output layer.\n   - D1: 78.36% Acc., 68.06 Macro-F1\n   - D2: 71.10% Acc., 67.87 Macro-F1\n   - D3: 80.\nwithout any external knowledge might still be able to learn some neutral-related features on D1 but it is very hard to learn from D3 and D4. (2) The numbers of neutral examples in the test sets of D3 and D4 are very small. Thus, the precision and recall on neutral class will be largely affected by even a small prediction difference (e.g., with 5 more neutral examples correctly identiﬁed, recall is increased by more than   $10\\%$   on both datasets). As a result, the macro-F1 scores on D3 and D4 are affected more. \n4.3 Ablation Tests \nTable  2  indicates that a large percentage of the per- formance gain comes from PRET. To better un- derstand the transfer effects of different layers – embedding layer   $\\mathbf{\\Psi}(\\mathbf{E})$  , LSTM layer   $(\\theta_{l s t m})$  , and output layer   $({\\bf W}_{o},{\\bf b}_{o})-\\mathrm{we}$   conduct ablation tests on PRET with different layers transfered from the document-level model to the aspect-level model. Results are presented in Table  3 . “LSTM only” denotes the setting where only the LSTM layer is transferred, and “Without LSTM” denotes the set- ting where only the embedding and output layers are transferred (excluding the LSTM layer). The key observations are: (1) Transfer is helpful in all settings. Improvements over LSTM  $^+$  ATT are observed even when only one layer is transferred. (2) Overall, transfers of the LSTM and embedding layer are more useful than the output layer. This is what we expect, since the output layer is nor- mally more task-speciﬁc. (3) Transfer of the em- bedding layer is more helpful on D3 and D4. One possible explanation is that the label distribution is extremely unbalanced on these two datasets. Sen- timent information is not adequately captured by GloVe word embeddings. Therefore, with a small number of training examples in the negative and neutral classes, the embeddings trained by aspect- level classiﬁcation still do not effectively capture the true semantics of the relevant opinion words. Transfer of the embedding layer can greatly help in this case. \n\n4.4 Analysis \nTo show that aspect-level classiﬁcation indeed beneﬁts from document-level knowledge, we conduct experiments to vary the percentage of document-level training examples from 0.0 to 1.0 for PRET  $+$  MULT. The changes of accuracies and macro-F1 scores on the four datasets are shown in Figure  1 . The improvements on accuracies with increasing number of document examples are sta- ble across all datasets. For macro-F1 scores, the improvements on D1 and D2 are stable. We ob- serve sharp increases in the macro-F1 scores of D3 and D4 when changing the percentage from 0 to 0.4. This may be related to their extremely "}
{"page": 4, "image_path": "doc_images/P18-2092_4.jpg", "ocr_text": "85]\n\n——oo\na\n\nee\n\nos 08\n\n20|\n\nAccuracy (%)\n\n0 02 0.4 1\n\nPercentage of dacument-level training examples\n\n68]\n\n66\n\n64]\n\nMacro-Fi (%)\n\n62\n\n60|\n\n0 02 04 06 08 1\n\nPercentage of dacument-level training examples\n\nFigure 1: Results of PRET+MULT vs. percentage\nof document-level training data.\n\nunbalanced label distribution. In such cases, with\nthe knowledge gained from a small number of bal-\nanced document-level examples, aspect-level pre-\ndictions on neutral examples can be significantly\nimproved.\n\nTo better understand in which conditions the\nproposed method is helpful, we analyze a sub-\nset of test examples that are correctly classi-\nfied by PRET+MULT but are misclassified by\nLSTM+ATT. We find that the benefits brought by\ndocument-level knowledge are typically shown in\nfour ways.\n\nFirst of all, to our surprise, LSTM+ATT made\nobvious mistakes on some instances with common\nopinion words. Below are two examples where the\ntarget is enclosed in [] with its true sentiment indi-\ncated in the subscript:\n\n1. “Iwas highly disappointed in the [food ]neg.”\n\n2. “This particular location certainly uses sub-\nstandard [meats] neg.”\n\nIn the above examples, LSTM+ATT does attend\nto the right opinion words, but makes the wrong\npredictions. One possible reason is that the word\nembeddings from GloVe without PRET do not\neffectively capture sentiment information, while\nthe aspect-level training samples are not sufficient\nto capture that for certain words. PRET+MULT\neliminates this kind of errors.\n\nAnother finding is that our method helps to\nbetter capture domain-specific opinion words due\nto additional knowledge from documents that are\n\n583\n\nfrom a similar domain:\n\n1. “The smaller [size]pos was a bonus because\nof space restrictions.”\n\n2. “The [price]pos is 200 dollars down.”\n\nLSTM+ATT attends on smaller correctly for the\nfirst example but makes the wrong prediction as\nsmaller can be negative in many cases. It does not\neven capture down in the second example.\n\nThirdly, we find that LSTM+ATT made a num-\nber of errors on sentences with negation words:\n\n1. I have experienced no problems, [works ]pos\nas anticipated.\n\n2. [Service]neg not the friendliest to our party!\n\nLSTMs typically only show their potential on\nlarge datasets. Without sufficient training exam-\nples, it may not be able to effectively capture\nvarious sequential patterns. Pretraining the net-\nwork on larger document-level corpus addresses\nthis problem.\n\nLastly, PRET+MULT makes fewer errors on\nrecognizing neutral instances. This can also be ob-\nserved from the macro-F1 scores in Table 2. The\nlack of training examples makes the prediction\nof neutral instances very difficult for all previous\nmethods. Knowledge from document-level exam-\nples with balanced labels compensates for this dis-\nadvantage.\n\n5 Conclusion\n\nThe effectiveness of existing aspect-level neural\nmodels is limited due to the difficulties in obtain-\ning training data in practice. Our work is the first\nattempt to incorporate knowledge from document-\nlevel corpus for training aspect-level sentiment\nclassifiers. We have demonstrated the effective-\nness of our proposed approaches and analyzed the\nmajor benefits brought by the knowledge transfer.\nThe proposed approaches can be potentially in-\ntegrated with other aspect-level neural models to\nfurther boost their performance.\n\nReferences\n\nPeng Chen, Zhongqian Sun, Lidong Bing, and Wei\nYang. 2017. Recurrent attention network on mem-\nory for aspect sentiment analysis. In Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP 2017).\n\nRonan Collobert and Jason Weston. 2008. A unified\narchitecture for natural language processing: Deep\nneural networks with multitask learning. In Jnter-\nnational Conference on Machine Learning (ICML\n\n2008).\n", "vlm_text": "The image contains two line graphs that illustrate the performance of a method called \"PRET $^+$ MULT\" based on the percentage of document-level training examples. The top graph shows accuracy percentages, while the bottom graph displays Macro-F1 percentages. Both graphs plot the metrics as a function of the percentage of document-level training examples.\n\nThe top graph indicates that as the percentage of document-level training examples increases, the accuracy of all four models (D1, D2, D3, D4) tends to improve. Model D4 consistently achieves the highest accuracy across all percentages.\n\nThe bottom graph reflects a similar trend for Macro-F1 scores, where increasing the percentage of document-level training examples generally leads to better performance across all models. Again, Model D4 typically achieves the best Macro-F1 scores.\n\nEach model (D1, D2, D3, D4) is represented using a different color and marker (D1: blue squares, D2: orange circles, D3: gray diamonds, D4: red crosses).\nunbalanced label distribution. In such cases, with the knowledge gained from a small number of bal- anced document-level examples, aspect-level pre- dictions on neutral examples can be signiﬁcantly improved. \nTo better understand in which conditions the proposed method is helpful, we analyze a sub- set of test examples that are correctly classi- ﬁed by PRET  $^+$  MULT but are misclassiﬁed by LSTM  $\\cdot+$  ATT. We ﬁnd that the beneﬁts brought by document-level knowledge are typically shown in four ways. \nFirst of all, to our surprise, LSTM+ATT made obvious mistakes on some instances with common opinion words. Below are two examples where the target is enclosed in [] with its true sentiment indi- cated in the subscript: \n1.  “I was highly disappointed in the [food] neg .” 2.  “This particular location certainly uses sub- standard [meats] neg .” \nIn the above examples, LSTM  $^+$  ATT does attend to the right opinion words, but makes the wrong predictions. One possible reason is that the word embeddings from GloVe without PRET do not effectively capture sentiment information, while the aspect-level training samples are not sufﬁcient to capture that for certain words. PRET  $^+$  MULT eliminates this kind of errors. \nAnother ﬁnding is that our method helps to better capture domain-speciﬁc opinion words due to additional knowledge from documents that are \n1.  “The smaller [size] pos  was a bonus because of space restrictions.” 2.  “The [price  $\\jmath_{p o s}$   is 200 dollars down.” LSTM  $^+$  ATT attends on  smaller  correctly for the ﬁrst example but makes the wrong prediction as smaller  can be negative in many cases. It does not even capture  down  in the second example. Thirdly, we ﬁnd that LSTM  $^+$  ATT made a num- ber of errors on sentences with negation words: 1.  I have experienced no problems, [works] pos as anticipated. 2.  [Service] neg  not the friendliest to our party! LSTMs typically only show their potential on \nlarge datasets. Without sufﬁcient training exam- ples, it may not be able to effectively capture various sequential patterns. Pretraining the net- work on larger document-level corpus addresses this problem. \nLastly, PRET  $^+$  MULT makes fewer errors on recognizing neutral instances. This can also be ob- served from the macro-F1 scores in Table  2 . The lack of training examples makes the prediction of neutral instances very difﬁcult for all previous methods. Knowledge from document-level exam- ples with balanced labels compensates for this dis- advantage. \n5 Conclusion \nThe effectiveness of existing aspect-level neural models is limited due to the difﬁculties in obtain- ing training data in practice. Our work is the ﬁrst attempt to incorporate knowledge from document- level corpus for training aspect-level sentiment classiﬁers. We have demonstrated the effective- ness of our proposed approaches and analyzed the major beneﬁts brought by the knowledge transfer. The proposed approaches can be potentially in- tegrated with other aspect-level neural models to further boost their performance. \nReferences \nPeng Chen, Zhongqian Sun, Lidong Bing, and Wei Yang. 2017. Recurrent attention network on mem- ory for aspect sentiment analysis. In  Conference on Empirical Methods in Natural Language Processing (EMNLP 2017) . Ronan Collobert and Jason Weston. 2008. A uniﬁed architecture for natural language processing: Deep neural networks with multitask learning. In  Inter- national Conference on Machine Learning (ICML 2008) . "}
{"page": 5, "image_path": "doc_images/P18-2092_5.jpg", "ocr_text": "Andrew M. Dai and Quoc V. Le. 2015. Semi-\nsupervised sequence learning. In Neural Informa-\ntion Processing Systems (NIPS 2015).\n\nLi Dong, Furu Wei, Chuanqi Tan, Duyu Tang, Ming\nZhou, and Ke Xu. 2014. Adaptive recursive neural\nnetwork for target-dependent Twitter sentiment clas-\nsification. In Annual Meeting of the Association for\nComputational Linguistics (ACL 2014).\n\nRuidan He, Wee Sun Lee, Hwee Tou Ng, and Daniel\nDahlmeier. 2017. An unsupervised neural attention\nmodel for aspect extraction. In Annual Meeting of\nthe Association for Computational Linguistics (ACL\n2017).\n\nAlex Krizhevsky and Ilya Sutskever. 2012. Imagenet\nclassification with deep convolutional neural net-\nworks. In Neural Information Processing Systems\n(NIPS 2012).\n\nJiangming Liu and Yue Zhang. 2017. Attention model-\ning for target sentiment. In Conference of the Euro-\npean Chapter of the Association for Computational\nLinguistics (EACL 2017).\n\nYang Liu, Sujian Li, Xiaodong Zhang, and Zhifang Sui.\n2016. Implicit discourse relation classification via\nmulti-task neural networks. In AAAI Conference on\nArtificial Intelligence (AAAI 2016).\n\nMinh-Tang Luong, Hieu Pham, and Christopher D.\nManning. 2015b. Effective approaches to attention-\nbased neural machine translation. In Annual Meet-\ning of the Association for Computational Linguistics\n(ACL 2015).\n\nMinh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol\nVinyals, and Lukasz Kaiser. 2015a. Multi-task se-\nquence to sequence learning. In International Con-\nference on Learning Representation (ICLR 2015).\n\nJulian J. McAuley, Christopher Targett, Qinfeng Shi,\nand Anton van den Hengel. 2015. Image-based rec-\nommendations on styles and substitutes. In The 38th\nInternational ACM SIGIR Conference on Research\nand Development in Information Retrieval.\n\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-\nrado, and Jeffrey Dean. 2013. Distributed repre-\nsentations of words and phrases and their composi-\ntionality. In Neural Information Processing Systems\n(NIPS 2013).\n\nLili Mou, Zhao Meng, Rui Yan, Ge Li, Yan Xu,\nLu Zhang, and Zhi Jin. 2016. How transferable\nare neural networks in NLP applications? In Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP 2016).\n\nThien Hai Nguyen and Kiyoaki Shirai. 2015.\nPhraseRNN: Phrase recursive neural network for\naspect-based sentiment analysis. In Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP 2015).\n\n584\n\nJeffrey Pennington, Richard Socher, and Christo-\npher D Manning. 2014. GloVe: Global vectors for\nword representation. In Conference on Empirical\nMethods in Natural Language Processing (EMNLP\n2014).\n\nMaria Pontiki, Dimitrios Galanis, Haris Papageorgiou,\nSuresh Manandhar, and Ion Androutsopoulos. 2015.\nSemEval-2015 task 12: Aspect based sentiment\nanalysis. In International Workshop on Semantic\nEvaluation (SemEval 2015).\n\nMaria Pontiki, Dimitrios Galanis, John Pavlopou-\nlos, Haris Papageorgiou, Ion Androutsopoulos, and\nSuresh Manandhar. 2014. SemEval-2014 task 4:\nAspect based sentiment analysis. In International\nWorkshop on Semantic Evaluation (SemEval 2014).\n\nMaria Pontiki, Dimitris Galanis, Haris Papageor-\ngiou, Ion Androutsopoulos, Suresh Manandhar, Mo-\nhammed AL-Smadi, Mahmoud Al-Ayyoub, Yanyan\nZhao, Bing Qin, Orphée De Clercq, Veronique\nHoste, Marianna Apidianaki, Xavier Tannier, Na-\ntalia Loukachevitch, Evgeniy Kotelnikov, Nuria Bel,\nSalud Maria Jiménez-Zafra, and Giilsen Eryigit.\n2016. SemEval-2016 task 5: Aspect based senti-\nment analysis. In International Workshop on Se-\nmantic Evaluation (SemEval 2016).\n\nPrajit Ramachandran, Peter J. Liu, and Quoc V. Le.\n2017. Unsupervised pretraining for sequence to\nsequence learning. In Conference on Empirical\nMethods in Natural Language Processing (EMNLP\n2017).\n\nDuyu Tang, Bing Qin, Xiaocheng Feng, and Ting Liu.\n2016a. Effective LSTMs for target-dependent senti-\nment classification. In International Conference on\nComputational Linguistics (COLING 2016).\n\nDuyu Tang, Bing Qin, and Ting Liu. 2015. Learn-\ning semantic representation of users and products for\ndocument level sentiment classification. In Annual\nMeeting of the Association for Computational Lin-\nguistics (ACL 2015).\n\nDuyu Tang, Bing Qin, and Ting Liu. 2016b. Aspect\nlevel sentiment classification with deep memory net-\nwork. In Conference on Empirical Methods in Nat-\nural Language Processing (EMNLP 2016).\n\nDuy-Tin Vo and Yue Zhang. 2015. Target-dependent\nTwitter sentiment classification with rich automatic\nfeatures. In International Joint Conference on Arti-\nficial Intelligence (IJCAI 2015).\n\nYequan Wang, Minlie Huang, Li Zhao, and Xiaoyan\nZhu. 2016. Attention-based LSTM for aspect-level\nsentiment classification. In Conference on Em-\npirical Methods in Natural Language Processing\n(EMNLP 2016).\n\nMatthew D. Zeiler and Rob Fergus. 2014. Visualizing\nand understanding convolutional networks. In Euro-\npean Conference on Computer Vision (ECCV 2014).\n", "vlm_text": "Andrew M. Dai and Quoc V. Le. 2015. Semi- supervised sequence learning. In  Neural Informa- tion Processing Systems (NIPS 2015) . Li Dong, Furu Wei, Chuanqi Tan, Duyu Tang, Ming Zhou, and Ke Xu. 2014. Adaptive recursive neural network for target-dependent Twitter sentiment clas- siﬁcation. In  Annual Meeting of the Association for Computational Linguistics (ACL 2014) . Ruidan He, Wee Sun Lee, Hwee Tou  $\\mathrm{Mg}$  , and Daniel Dahlmeier. 2017. An unsupervised neural attention model for aspect extraction. In  Annual Meeting of the Association for Computational Linguistics (ACL 2017) . Alex Krizhevsky and Ilya Sutskever. 2012. Imagenet classiﬁcation with deep convolutional neural net- works. In  Neural Information Processing Systems (NIPS 2012) . Jiangming Liu and Yue Zhang. 2017. Attention model- ing for target sentiment. In  Conference of the Euro- pean Chapter of the Association for Computational Linguistics (EACL 2017) . Yang Liu, Sujian Li, Xiaodong Zhang, and Zhifang Sui. 2016. Implicit discourse relation classiﬁcation via multi-task neural networks. In  AAAI Conference on Artiﬁcial Intelligence (AAAI 2016) . Minh-Tang Luong, Hieu Pham, and Christopher D. Manning. 2015b. Effective approaches to attention- based neural machine translation. In  Annual Meet- ing of the Association for Computational Linguistics (ACL 2015) . Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. 2015a. Multi-task se- quence to sequence learning. In  International Con- ference on Learning Representation (ICLR 2015) . Julian J. McAuley, Christopher Targett, Qinfeng Shi, and Anton van den Hengel. 2015. Image-based rec- ommendations on styles and substitutes. In  The 38th International ACM SIGIR Conference on Research and Development in Information Retrieval . Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor- rado, and Jeffrey Dean. 2013. Distributed repre- sentations of words and phrases and their composi- tionality. In  Neural Information Processing Systems (NIPS 2013) . Lili Mou, Zhao Meng, Rui Yan, Ge Li, Yan Xu, Lu Zhang, and Zhi Jin. 2016. How transferable are neural networks in NLP applications? In  Con- ference on Empirical Methods in Natural Language Processing (EMNLP 2016) . Thien Hai Nguyen and Kiyoaki Shirai. 2015. PhraseRNN: Phrase recursive neural network for aspect-based sentiment analysis. In  Conference on Empirical Methods in Natural Language Processing (EMNLP 2015) . \nJeffrey Pennington, Richard Socher, and Christo- pher D Manning. 2014. GloVe: Global vectors for word representation. In  Conference on Empirical Methods in Natural Language Processing (EMNLP 2014) . Maria Pontiki, Dimitrios Galanis, Haris Papageorgiou, Suresh Manandhar, and Ion Androutsopoulos. 2015. SemEval-2015 task 12: Aspect based sentiment analysis. In  International Workshop on Semantic Evaluation (SemEval 2015) . Maria Pontiki, Dimitrios Galanis, John Pavlopou- los, Haris Papageorgiou, Ion Androutsopoulos, and Suresh Manandhar. 2014. SemEval-2014 task 4: Aspect based sentiment analysis. In  International Workshop on Semantic Evaluation (SemEval 2014) . Maria Pontiki, Dimitris Galanis, Haris Papageor- giou, Ion Androutsopoulos, Suresh Manandhar, Mo- hammed AL-Smadi, Mahmoud Al-Ayyoub, Yanyan Zhao, Bing Qin, Orph´ ee De Clercq, Veronique Hoste, Marianna Apidianaki, Xavier Tannier, Na- talia Loukachevitch, Evgeniy Kotelnikov, N´ uria Bel, Salud Maria Jim´ enez-Zafra, and G¨ uls ¸en Eryi˘ git. 2016. SemEval-2016 task 5: Aspect based senti- ment analysis. In  International Workshop on Se- mantic Evaluation (SemEval 2016) . Prajit Ramachandran, Peter J. Liu, and Quoc V. Le. 2017. Unsupervised pretraining for sequence to sequence learning. In  Conference on Empirical Methods in Natural Language Processing (EMNLP 2017) . Duyu Tang, Bing Qin, Xiaocheng Feng, and Ting Liu. 2016a. Effective LSTMs for target-dependent senti- ment classiﬁcation. In  International Conference on Computational Linguistics (COLING 2016) . Duyu Tang, Bing Qin, and Ting Liu. 2015. Learn- ing semantic representation of users and products for document level sentiment classiﬁcation. In  Annual Meeting of the Association for Computational Lin- guistics (ACL 2015) . Duyu Tang, Bing Qin, and Ting Liu. 2016b. Aspect level sentiment classiﬁcation with deep memory net- work. In  Conference on Empirical Methods in Nat- ural Language Processing (EMNLP 2016) . Duy-Tin Vo and Yue Zhang. 2015. Target-dependent Twitter sentiment classiﬁcation with rich automatic features. In  International Joint Conference on Arti- ﬁcial Intelligence (IJCAI 2015) . Yequan Wang, Minlie Huang, Li Zhao, and Xiaoyan Zhu. 2016. Attention-based LSTM for aspect-level sentiment classiﬁcation. In  Conference on Em- pirical Methods in Natural Language Processing (EMNLP 2016) . Matthew D. Zeiler and Rob Fergus. 2014. Visualizing and understanding convolutional networks. In  Euro- pean Conference on Computer Vision (ECCV 2014) . "}
{"page": 6, "image_path": "doc_images/P18-2092_6.jpg", "ocr_text": "Meishan Zhang, Yue Zhang, and Duy-Tin Vo. 2016.\nGated neural networks for targeted sentiment anal-\nysis. In AAAI Conference on Artificial Intelligence\n(AAAI 2016).\n\nBarret Zoph, Deniz Yuret, Jonathan May, and Kevin\nKnight. 2016. Transfer learning for low-resource\nneural machine translation. In Conference on Em-\n\npirical Methods in Natural Language Processing\n(EMNLP 2016).\n\n585\n", "vlm_text": "Meishan Zhang, Yue Zhang, and Duy-Tin Vo. 2016. Gated neural networks for targeted sentiment anal- ysis. In  AAAI Conference on Artiﬁcial Intelligence (AAAI 2016) . Barret Zoph, Deniz Yuret, Jonathan May, and Kevin Knight. 2016. Transfer learning for low-resource neural machine translation. In  Conference on Em- pirical Methods in Natural Language Processing (EMNLP 2016) . "}
