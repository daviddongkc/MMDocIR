{"layout": 0, "type": "text", "text": "Multi-Task Identiﬁcation of Entities, Relations, and Coreference for Scientiﬁc Knowledge Graph Construction ", "text_level": 1, "page_idx": 0, "bbox": [100, 67, 497, 102], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 1, "type": "text", "text": "Yi Luan Luheng He Mari Ostendorf Hannaneh Hajishirzi University of Washington { luanyi, luheng, ostendor, hannaneh } @uw.edu ", "page_idx": 0, "bbox": [137.58200073242188, 128.531005859375, 465.9435729980469, 179.3809814453125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 2, "type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0, "bbox": [159, 223, 204, 236], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 3, "type": "text", "text": "We introduce a multi-task setup of identifying and classifying entities, relations, and coref- erence clusters in scientiﬁc articles. We cre- ate S CI ERC, a dataset that includes annota- tions for all three tasks and develop a uni- ﬁed framework called Scientiﬁc Information Extractor (S CI IE) for with shared span rep- resentations. The multi-task setup reduces cascading errors between tasks and leverages cross-sentence relations through coreference links. Experiments show that our multi-task model outperforms previous models in scien- tiﬁc information extraction without using any domain-speciﬁc features. We further show that the framework supports construction of a sci- entiﬁc knowledge graph, which we use to ana- lyze information in scientiﬁc literature. ", "page_idx": 0, "bbox": [89, 249.46954345703125, 274, 452.7574157714844], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 4, "type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0, "bbox": [71, 466, 155, 480], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 5, "type": "text", "text": "As scientiﬁc communities grow and evolve, new tasks, methods, and datasets are introduced and different methods are compared with each other. Despite advances in search engines, it is still hard to identify new technologies and their relationships with what existed before. To help researchers more quickly identify opportunities for new combina- tions of tasks, methods and data, it is important to design intelligent algorithms that can extract and organize scientiﬁc information from a large collec- tion of documents. ", "page_idx": 0, "bbox": [71, 489.75299072265625, 292, 638.3904418945312], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 6, "type": "text", "text": "Organizing scientiﬁc information into structured knowledge bases requires information extraction (IE) about scientiﬁc entities and their relationships. However, the challenges associated with scientiﬁc IE are greater than for a general domain. First, an- notation of scientiﬁc text requires domain expertise which makes annotation costly and limits resources. ", "page_idx": 0, "bbox": [71, 639.5180053710938, 292, 733.95947265625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 7, "type": "image", "page_idx": 0, "img_path": "layout_images/D18-1360_0.jpg", "img_caption": "Figure 1 : Example annotation: phrases that refer to the same scientiﬁc concept are annotated into the same coreference cluster, such as  MORphological PAser MORPA ,  it  and  MORPA  (marked as red). ", "bbox": [305, 221, 527, 451], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "f ‘Used-for: Used-for\nTo reduce [ambiguity ]otersr, the [MORphological PArser MORPA Method\nUsed-for ——|\nis provided with a [PCFG]Method...\n\n$——— Used-for ————,\n[It]Generie combines [context-free grammar |Method with...\nUsed-for\n\n[ —— Hyponym-of —————,\n[MORPA ] Method is a fully implemented [parser |Methoa developed for a [text-\nto-speech system] task.\n\ntext-to-speech\nsystem\n", "vlm_text": "The image contains a text annotation and a diagram. \n\nIn the text annotation (top part), different phrases are linked to show their relationships. These include:\n\n- \"ambiguity,\" \"MORphological PArser MORPA,\" \"PCFG,\" \"It,\" \"context-free grammar,\" \"MORPA,\" \"parser,\" and \"text-to-speech system\" with labels like \"Used-for\" and \"Hyponym-of\".\n\nThe diagram (bottom part) shows a representation of these relationships in a more visual format. The coreference cluster is marked in red and yellow, linking \"MORphological PArser MORPA,\" \"MORPA,\" and \"It.\" Other terms are connected with lines labeled \"Used-for\" or \"Hyponym-of.\""}
{"layout": 8, "type": "text", "text": "In addition, most relation extraction systems are de- signed for within-sentence relations. However, ex- tracting information from scientiﬁc articles requires extracting relations across sentences. Figure  1  il- lustrates this problem. The cross-sentence relations between some entities can only be connected by entities that refer to the same scientiﬁc concept, including generic terms (such as the pronoun    $i t$  , or phrases like  our method ) that are not informa- tive by themselves. With co-reference,  context-free grammar  can be connected to  MORPA  through the intermediate co-referred pronoun  it . Applying ex- isting IE systems to this data, without co-reference, will result in much lower relation coverage (and a sparse knowledge base). ", "page_idx": 0, "bbox": [306, 479.0119934082031, 527, 681.846435546875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 9, "type": "text", "text": "In this paper, we develop a uniﬁed learning model for extracting scientiﬁc entities, relations, and coreference resolution. This is different from previous work ( Luan et al. ,  2017b ;  Gupta and Man- ning ,  2011 ;  Tsai et al. ,  2013 ;  G abor et al. ,  2018 ) which often addresses these tasks as independent components of a pipeline. Our uniﬁed model is a multi-task setup that shares parameters across low-level tasks, making predictions by leveraging context across the document through coreference links. Speciﬁcally, we extend prior work for learn- ing span representations and coreference resolution ( Lee et al. ,  2017 ;  He et al. ,  2018 ). Different from a standard tagging system, our system enumerates all possible spans during decoding and can effectively detect overlapped spans. It avoids cascading errors between tasks by jointly modeling all spans and span-span relations. ", "page_idx": 0, "bbox": [306, 685.1400146484375, 527, 766.0314331054688], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 10, "type": "text", "text": "", "page_idx": 1, "bbox": [71, 63.68701934814453, 292, 225.87350463867188], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 11, "type": "text", "text": "To explore this problem, we create a dataset S CI - ERC for scientiﬁc information extraction, which includes annotations of scientiﬁc terms, relation categories and co-reference links. Our experiments show that the uniﬁed model is better at predict- ing span boundaries, and it outperforms previous state-of-the-art scientiﬁc IE systems on entity and relation extraction ( Luan et al. ,  2017b ;  Augenstein et al. ,  2017 ). In addition, we build a scientiﬁc knowledge graph integrating terms and relations extracted from each article. Human evaluation shows that propagating coreference can signiﬁ- cantly improve the quality of the automatic con- structed knowledge graph. ", "page_idx": 1, "bbox": [71, 227.14002990722656, 292, 416.4244689941406], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 12, "type": "text", "text": "In summary we make the following contribu- tions. We create a dataset for scientiﬁc information extraction by jointly annotating scientiﬁc entities, relations, and coreference links. Extending a previ- ous end-to-end coreference resolution system, we develop a multi-task learning framework that can detect scientiﬁc entities, relations, and coreference clusters without hand-engineered features. We use our uniﬁed framework to build a scientiﬁc knowl- edge graph from a large collection of documents and analyze information in scientiﬁc literature. ", "page_idx": 1, "bbox": [71, 417.69000244140625, 292, 566.3274536132812], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 13, "type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1, "bbox": [71, 579, 161, 592], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 14, "type": "text", "text": "There has been growing interest in research on au- tomatic methods for information extraction from scientiﬁc articles. Past research in scientiﬁc IE addressed analyzing citations ( Athar and Teufel , 2012b , a ;  Kas ,  2011 ;  Gabor et al. ,  2016 ;  Sim et al. , 2012 ;  Do et al. ,  2013 ;  Jaidka et al. ,  2014 ;  Abu- Jbara and Radev ,  2011 ), analyzing research com- munity ( Vogel and Jurafsky ,  2012 ;  Anderson et al. , 2012 ), and unsupervised methods for extracting sci- entiﬁc entities and relations ( Gupta and Manning , 2011 ;  Tsai et al. ,  2013 ;  G´ abor et al. ,  2016 ). ", "page_idx": 1, "bbox": [71, 602.9829711914062, 292, 751.6204223632812], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 15, "type": "text", "text": "More recently, two datasets in SemEval 2017 and 2018 have been introduced, which facilitate research on supervised and semi-supervised learn- ing for scientiﬁc information extraction. SemEval 17 ( Augenstein et al. ,  2017 ) includes 500 para- graphs from articles in the domains of computer science, physics, and material science. It includes three types of entities (called keyphrases): Tasks, Methods, and Materials and two relation types: hyponym-of and synonym-of. SemEval 18 ( G abor et al. ,  2018 ) is focused on predicting relations be- tween entities within a sentence. It consists of six relation types. Using these datasets, neural mod- els ( Ammar et al. ,  2017 ,  2018 ;  Luan et al. ,  2017b ; Augenstein and Søgaard ,  2017 ) are introduced for extracting scientiﬁc information. We extend these datasets by increasing relation coverage, adding cross-sentence coreference linking, and removing some annotation constraints. Different from most previous IE systems for scientiﬁc literature and gen- eral domains ( Miwa and Bansal ,  2016 ;  Xu et al. , 2016 ;  Peng et al. ,  2017 ;  Quirk and Poon ,  2017 ; Luan et al. ,  2018 ;  Adel and Sch utze ,  2017 ), which use preprocessed syntactic, discourse or corefer- ence features as input, our uniﬁed framework does not rely on any pipeline processing and is able to model overlapping spans. ", "page_idx": 1, "bbox": [82, 752.885986328125, 292, 766.0314331054688], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 16, "type": "text", "text": "", "page_idx": 1, "bbox": [306, 63.68701934814453, 527, 415.5624694824219], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 17, "type": "text", "text": "While  Singh et al.  ( 2013 ) show improvements by jointly modeling entities, relations, and coref- erence links, most recent neural models for these tasks focus on single tasks ( Clark and Manning , 2016 ;  Wiseman et al. ,  2016 ;  Lee et al. ,  2017 ;  Lam- ple et al. ,  2016 ;  Peng et al. ,  2017 ) or joint entity and relation extraction ( Katiyar and Cardie ,  2017 ; Zhang et al. ,  2017 ;  Adel and Sch utze ,  2017 ;  Zheng et al. ,  2017 ). Among those studies, many papers as- sume the entity boundaries are given, such as ( Clark and Manning ,  2016 ),  Adel and Sch utze  ( 2017 ) and Peng et al.  ( 2017 ). Our work relaxes this constraint and predicts entity boundaries by optimizing over all possible spans. Our model draws from recent end-to-end span-based models for coreference res- olution ( Lee et al. ,  2017 ,  2018 ) and semantic role labeling ( He et al. ,  2018 ) and extends them for the multi-task framework involving the three tasks of identiﬁcation of entity, relation and coreference. ", "page_idx": 1, "bbox": [306, 421.83599853515625, 527, 678.866455078125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 18, "type": "text", "text": "Neural multi-task learning has been applied to a range of NLP tasks. Most of these models share word-level representations ( Collobert and Weston , 2008 ;  Klerke et al. ,  2016 ;  Luan et al. ,  2016 ,  2017a ; Rei ,  2017 ), while  Peng et al.  ( 2017 ) uses high-order cross-task factors. Our model instead propagates cross-task information via span representations, which is related to  Swayamdipta et al.  ( 2017 ). ", "page_idx": 1, "bbox": [306, 685.1400146484375, 527, 766.0314331054688], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 19, "type": "text", "text": "", "page_idx": 2, "bbox": [71, 63.68701934814453, 292, 90.38247680664062], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 20, "type": "text", "text": "3 Dataset ", "text_level": 1, "page_idx": 2, "bbox": [71, 101, 129, 114], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 21, "type": "text", "text": "Our dataset (called S CI ERC) includes annotations for scientiﬁc entities, their relations, and corefer- ence clusters for 500 scientiﬁc abstracts. These ab- stracts are taken from 12 AI conference/workshop proceedings in four AI communities from the Se- mantic Scholar Corpus 2 . S CI ERC extends pre- vious datasets in scientiﬁc articles SemEval 2017 Task 10 (SemEval 17) ( Augenstein et al. ,  2017 ) and SemEval 2018 Task 7 (SemEval 18) ( G abor et al. , 2018 ) by extending entity types, relation types, rela- tion coverage, and adding cross-sentence relations using coreference links. Our dataset is publicly available at:  http://nlp.cs.washington. edu/sciIE/ . Table  1  shows the statistics of S CI - ERC. ", "page_idx": 2, "bbox": [71, 123.20203399658203, 292, 326.0364685058594], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 22, "type": "text", "text": "Annotation Scheme We deﬁne six types for an- notating scientiﬁc entities (Task, Method, Metric, Material, Other-ScientiﬁcTerm and Generic) and seven relation types (Compare, Part-of, Conjunc- tion, Evaluate-for, Feature-of, Used-for, Hyponym- Of). Directionality is taken into account except for the two symmetric relation types (Conjunction and Compare). Coreference links are annotated between identical scientiﬁc entities. A Generic en- tity is annotated only when the entity is involved in a relation or is coreferred with another entity. Annotation guidelines can be found in Appendix  A . Figure  1  shows an annotated example. ", "page_idx": 2, "bbox": [71, 334.1612548828125, 292, 510.29046630859375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 23, "type": "text", "text": "Following annotation guidelines from  Qasem- iZadeh and Schumann  ( 2016 ) and using the BRAT interface ( Stenetorp et al. ,  2012 ), our annotators perform a greedy annotation for spans and always prefer the longer span whenever ambiguity occurs. Nested spans are allowed when a subspan has a relation/coreference link with another term outside the span. ", "page_idx": 2, "bbox": [71, 510.9110107421875, 292, 618.9004516601562], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 24, "type": "text", "text": "Human Agreements One domain expert anno- tated all the documents in the dataset;   $12\\%$   of the data is dually annotated by 4 other domain experts to evaluate the user agreements. The kappa score for annotating entities is   $76.9\\%$  , relation extraction is   $67.8\\%$   and coreference is   $63.8\\%$  . ", "page_idx": 2, "bbox": [71, 627.0253295898438, 292, 708.3094482421875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 25, "type": "table", "page_idx": 2, "img_path": "layout_images/D18-1360_1.jpg", "table_footnote": "Table 1 : Dataset statistics for our dataset S CI ERC and two previous datasets on scientiﬁc information extraction. All datasets annotate 500 documents. ", "bbox": [306, 61, 527, 175], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "Statistics ScIERC SemEval 17 SemEval 18\n#Entities 8089 9946 7483\n#Relations 4716 672 1595\n#Relations/Doc 9.4 1.3 3:2\n#Coref links 2752 - -\n#Coref clusters 1023 - -\n\n", "vlm_text": "The table compares the following statistics across three datasets: SciERC, SemEval 17, and SemEval 18:\n\n1. **#Entities**: \n   - SciERC: 8089\n   - SemEval 17: 9946\n   - SemEval 18: 7483\n\n2. **#Relations**: \n   - SciERC: 4716\n   - SemEval 17: 672\n   - SemEval 18: 1595\n\n3. **#Relations/Doc**:\n   - SciERC: 9.4\n   - SemEval 17: 1.3\n   - SemEval 18: 3.2\n\n4. **#Coref links**: \n   - SciERC: 2752\n   - SemEval 17: -\n   - SemEval 18: -\n\n5. **#Coref clusters**: \n   - SciERC: 1023\n   - SemEval 17: -\n   - SemEval 18: -"}
{"layout": 26, "type": "text", "text": "Comparison with previous datasets S CI ERC is focused on annotating cross-sentence relations and has more relation coverage than SemEval 17 and SemEval 18, as shown in Table  1 . SemEval 17 is mostly designed for entity recognition and only covers two relation types. The task in SemEval 18 is to classify a relation between a pair of entities given entity boundaries, but only intra-sentence re- lations are annotated and each entity only appears in one relation, resulting in sparser relation cover- age than our dataset (3.2 vs. 9.4 relations per ab- stract). S CI ERC extends these datasets by adding more relation types and coreference clusters, which allows representing cross-sentence relations, and removing annotation constraints. Table  1  gives a comparison of statistics among the three datasets. In addition, S CI ERC aims at including broader coverage of general AI communities. ", "page_idx": 2, "bbox": [306, 189.5563201904297, 527, 433.4314880371094], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 27, "type": "text", "text": "4 Model ", "text_level": 1, "page_idx": 2, "bbox": [307, 444, 358, 457], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 28, "type": "text", "text": "We develop a uniﬁed framework (called S CI IE) to identify and classify scientiﬁc entities, relations, and coreference resolution across sentences. S CI IE is a multi-task learning setup that extends previous span-based models for coreference resolution ( Lee et al. ,  2017 ) and semantic role labeling ( He et al. , 2018 ). All three tasks of entity recognition, re- lation extraction, and coreference resolution are treated as multinomial classiﬁcation problems with shared span representations. S CI IE beneﬁts from expressive contextualized span representations as classiﬁer features. By sharing span representations, sentence-level tasks can beneﬁt from information propagated from coreference resolution across sen- tences, without increasing the complexity of infer- ence. Figure  2  shows a high-level overview of the S CI IE multi-task framework. ", "page_idx": 2, "bbox": [306, 467.1419982910156, 527, 697.074462890625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 29, "type": "text", "text": "4.1 Problem Deﬁnition ", "text_level": 1, "page_idx": 2, "bbox": [307, 708, 421, 720], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 30, "type": "text", "text": "The input is a document represented as a sequence of words    $D=\\{w_{1},.\\,.\\,.\\,,w_{n}\\}$  , from which we de- rive    $S~=~\\{s_{1},.\\,.\\,.\\,,s_{N}\\}$  , the set of all possible ", "page_idx": 2, "bbox": [306, 725.7869873046875, 527, 766.0314331054688], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 31, "type": "image", "page_idx": 3, "img_path": "layout_images/D18-1360_2.jpg", "img_caption": "Figure 2 : Overview of the multitask setup, where all three tasks are treated as classiﬁcation problems on top of shared span representations. Dotted arcs indicate the normalization space for each task. ", "bbox": [70, 63, 527, 277], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "*, Relation\n== NULL’ Extraction\n\nEntity\nRecognition\n\nHyponym-of\n\nCoreference\nResolution\n\nSpan\nRepresentations\n\n+Span Features\n\nBiLSTM outputs\n\nSentences the MORphological Parser MORPA is provided with a MORPA is a fully implemented parser developed for\n", "vlm_text": "The image is a diagram illustrating a multitask setup. It focuses on three tasks: Entity Recognition, Coreference Resolution, and Relation Extraction. Each task is treated as a classification problem using shared span representations. The diagram includes:\n\n- **Entity Recognition:** Identifies and classifies parts of the text as entities such as \"Task\" and \"Method.\"\n- **Coreference Resolution:** Connects references to the same entity, shown with arrows pointing back to the original mention.\n- **Relation Extraction:** Identifies relationships, like \"Hyponym-of\" and \"Used-for,\" between entities.\n\nBlue arrows demonstrate BiLSTM outputs connecting to sentences, indicating how sentence information is processed for these tasks. Dotted lines indicate normalization spaces specific to each task."}
{"layout": 32, "type": "text", "text": "within-sentence word sequence spans (up to a rea- sonable length) in the document. The output con- tains three structures: the entity types    $E$   for all spans  $S$  , the relations    $R$   f all pair of spans  $S\\times S$  , and the coreference links  C  for all spans in  S . The output structures are represented with a set of dis- crete random variables indexed by spans or pairs of spans. Speciﬁcally, the output structures are deﬁned as follows. ", "page_idx": 3, "bbox": [71, 298.0, 292, 419.5384826660156], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 33, "type": "text", "text": "Entity recognition  is to predict the best entity type for every candidate span. Let    $L_{\\mathrm{E}}$   represent the set of all possible entity types including the null-type    $\\epsilon$  The output structure    $E$   is a set of random variables indexed by spans:    $e_{i}\\in L_{\\mathrm{E}}$   for    $i=1,\\ldots,N$  . ", "page_idx": 3, "bbox": [71, 422.165283203125, 292, 491.5364685058594], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 34, "type": "text", "text": "Relation extraction  is to predict the best relation type given an ordered pair of spans    $(s_{i},s_{j})$  . Let    $L_{\\mathrm{R}}$  be the set of all possible relation types including the null-type  $\\epsilon$  . The output structure    $R$   is a set of random variables indexed over pairs of spans    $(i,j)$  the same sentence:    $r_{i j}\\,\\in\\,L_{\\mathrm{R}}$   for  $i,j=1,\\cdot\\cdot\\cdot,N$  . ", "page_idx": 3, "bbox": [71, 492.5262756347656, 292, 587.3594360351562], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 35, "type": "text", "text": "Coreference resolution  is to predict the best an- tecedent (including a special null antecedent) given a span, which is the same mention-ranking model used in  Lee et al.  ( 2017 ). The output structure  $C$  ndo eﬁned as:    $c_{i}\\in$   $\\{1,\\ldots,i-1,\\epsilon\\}$   for  $i=1,\\dots,N$  . ", "page_idx": 3, "bbox": [71, 589.9862670898438, 292, 671.2704467773438], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 36, "type": "text", "text": "4.2 Model Deﬁnition ", "text_level": 1, "page_idx": 3, "bbox": [72, 690, 175, 702], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 37, "type": "text", "text": "We formulate the multi-task learning setup as learning the conditional probability distribution  $P(E,R,C|D)$  . For efﬁcient training and inference, we decompose    $P(E,R,C|D)$   assuming spans are conditionally independent given    $D$  : ", "page_idx": 3, "bbox": [71, 712.2379760742188, 292, 766.0314331054688], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 38, "type": "text", "text": "", "page_idx": 3, "bbox": [306, 298.0, 462.5286865234375, 311.1454772949219], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 39, "type": "equation", "text": "\n$$\n\\begin{array}{l}{{P(E,R,C\\mid D)=P(E,R,C,S\\mid D)}}\\\\ {{{}=\\displaystyle\\prod_{i=1}^{N}P(e_{i}\\mid D)P(c_{i}\\mid D)\\displaystyle\\prod_{j=1}^{N}P(r_{i j}\\mid D),}}\\end{array}\n$$\n ", "text_format": "latex", "page_idx": 3, "bbox": [320, 318, 510, 376], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 40, "type": "text", "text": "where the conditional probabilities of each random variable are independently normalized: ", "page_idx": 3, "bbox": [306, 382.3869934082031, 526, 409.08148193359375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 41, "type": "equation", "text": "\n$$\n\\begin{array}{l}{P(e_{i}=e\\mid D)=\\frac{\\exp(\\Phi_{\\mathrm{E}}(e,s_{i}))}{\\sum_{e^{\\prime}\\in L_{\\mathrm{E}}}\\exp(\\Phi_{\\mathrm{E}}(e^{\\prime},s_{i}))}\\qquad(2)}\\\\ {P(r_{i j}=r\\mid D)=\\frac{\\exp(\\Phi_{\\mathrm{R}}(r,s_{i},s_{j}))}{\\sum_{r^{\\prime}\\in L_{\\mathrm{R}}}\\exp(\\Phi_{\\mathrm{R}}(r^{\\prime},s_{i},s_{j}))}}\\\\ {P(c_{i}=j\\mid D)=\\frac{\\exp(\\Phi_{\\mathrm{C}}(s_{i},s_{j}))}{\\sum_{j^{\\prime}\\in\\{1,\\ldots,i-1,\\epsilon\\}}\\exp(\\Phi_{\\mathrm{C}}(s_{i},s_{j^{\\prime}}))},}\\end{array}\n$$\n ", "text_format": "latex", "page_idx": 3, "bbox": [304, 415, 535, 512], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 42, "type": "text", "text": "where    $\\Phi_{\\mathrm{E}}$   denotes the unnormalized model score for an entity type  $e$   and a span    $s_{i}$  ,  $\\Phi_{\\mathrm{{R}}}$   denotes the score for a relation type    $r$   and span pairs    $s_{i},s_{j}$  , and    $\\Phi_{\\mathbf{C}}$   denotes the score for a binary coreference link between    $s_{i}$   and  $s_{j}$  . These    $\\Phi$   scores are further decomposed into span and pairwise span scores computed from feed-forward networks, as will be explained in Section  4.3 . ", "page_idx": 3, "bbox": [306, 517.5869750976562, 526, 625.576416015625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 43, "type": "text", "text": "For simplicity, we omit    $D$   from the    $\\Phi$   functions and    $S$   from the observation. ", "page_idx": 3, "bbox": [306, 625.97998046875, 526, 652.6744384765625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 44, "type": "text", "text": "Objective Given a set of all documents    $\\mathcal{D}$  , the model loss function is deﬁned as a weighted sum of the negative log-likelihood loss of all three tasks: ", "page_idx": 3, "bbox": [306, 659.7653198242188, 526, 700.4014282226562], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 45, "type": "equation", "text": "\n$$\n\\begin{array}{r l}{-}&{{}\\displaystyle\\sum_{(D,R^{*},E^{*},C^{*})\\in\\mathcal{D}}\\Big\\{\\lambda_{\\mathrm{E}}\\log P(E^{*}\\mid D)\\quad\\quad}\\\\ {+}&{{}\\lambda_{\\mathsf{R}}\\log P(R^{*}\\mid D)+\\lambda_{\\mathsf{C}}\\log P(C^{*}\\mid D)\\Big\\}}\\end{array}\n$$\n ", "text_format": "latex", "page_idx": 3, "bbox": [319, 706, 514, 765], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 46, "type": "text", "text": "where    $E^{*}$  ,  $R^{*}$  , and    $C^{*}$  are gold structures of the en- tity types, relations, and coreference, respectively. The task weights    $\\lambda_{\\mathrm{E}},\\,\\lambda_{\\mathrm{R}}$  , and    $\\lambda_{\\mathrm{C}}$   are introduced as hyper-parameters to control the importance of each task. ", "page_idx": 4, "bbox": [70, 63.68701934814453, 292, 131.02944946289062], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 47, "type": "text", "text": "For entity recognition and relation extraction,  $P(E^{*}\\mid D)$   and    $P(R^{*}\\mid D)$   are computed with the deﬁnition in Equation  ( 2 ) . For coreference resolution, we use the marginalized loss follow- ing  Lee et al.  ( 2017 ) since each mention can have multiple correct antecedents. Let    $C_{i}^{*}$    be the set of all correct antecedents for span    $i$  , we have:  $\\begin{array}{r}{\\log P(C^{*}\\mid D)=\\sum_{i=1..N}\\log\\sum_{c\\in C_{i}^{*}}P(c\\mid D)}\\end{array}$  . ∈ ", "page_idx": 4, "bbox": [70, 131.43299865722656, 292, 247.1944580078125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 48, "type": "text", "text": "4.3 Scoring Architecture ", "text_level": 1, "page_idx": 4, "bbox": [71, 248, 195, 261], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 49, "type": "text", "text": "We use feedforward neural networks (FFNNs) over shared span representations    $\\mathbf{g}$   to compute a set of span and pairwise span scores. For the span scores,  $\\phi_{e}(s_{i})$   measures how likely a span  $s_{i}$   has an entity type    $e$  , and    $\\phi_{\\mathrm{mr}}(s_{i})$   and  $\\phi_{\\mathrm{mc}}(s_{i})$   measure how likely a span    $s_{i}$   is a mention in a relation or a coreference link, respectively. The pairwise scores  $\\phi_{r}(s_{i},s_{j})$   and    $\\phi_{\\mathsf{c}}(s_{i},s_{j})$   measure how likely two spans are associated in a relation  $r$   or a coreference link, respectively. Let    $\\mathbf{g}_{i}$   be the ﬁxed-length vec- tor representation for span    $s_{i}$  . For different tasks, the span scores    $\\phi_{\\mathbf{X}}(s_{i})$   for    $\\mathbf{x}~\\in~\\{e,\\mathsf{m c},\\mathsf{m r}\\}$   and pairwise span scores    $\\phi_{\\mathbf{y}}(s_{i},s_{j})$   for    $\\mathtt{y}\\in\\{r,\\mathtt{c}\\}$   are computed as follows: ", "page_idx": 4, "bbox": [70, 265.6199951171875, 292, 454.90447998046875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 50, "type": "equation", "text": "\n$$\n\\begin{array}{r l}&{\\mathrel{\\phantom{=}}\\phi_{\\mathrm{x}}\\bigl(s_{i}\\bigr)=\\!\\mathbf{w}_{\\mathrm{x}}\\cdot\\mathrm{FFNN}_{\\mathrm{x}}\\bigl(\\mathbf{g}_{i}\\bigr)}\\\\ &{\\phi_{\\mathrm{y}}\\bigl(s_{i},s_{j}\\bigr)=\\!\\mathbf{w}_{\\mathrm{y}}\\cdot\\mathrm{FFNN}_{\\mathrm{y}}\\bigl([\\mathbf{g}_{i},\\mathbf{g}_{j},\\mathbf{g}_{i}\\odot\\mathbf{g}_{j}]\\bigr),}\\end{array}\n$$\n ", "text_format": "latex", "page_idx": 4, "bbox": [84, 460, 276, 495], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 51, "type": "text", "text": "where    $\\odot$  is element-wise multiplication, and  $\\{\\mathbf{w}_{\\mathrm{x}},\\mathbf{w}_{\\mathrm{y}}\\}$   are neural network parameters to be learned. ", "page_idx": 4, "bbox": [70, 500.9840087890625, 292, 541.2274169921875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 52, "type": "text", "text": "We use these scores to compute the different    $\\Phi$  : ", "page_idx": 4, "bbox": [82, 541.6310424804688, 292, 554.7764892578125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 53, "type": "equation", "text": "\n$$\n\\begin{array}{r l r}{\\Phi_{\\mathsf{E}}(e,s_{i})}&{=}&{\\phi_{e}(s_{i})\\qquad\\qquad\\qquad\\qquad\\quad(4)}\\\\ {\\Phi_{\\mathsf{R}}\\!\\left(r,s_{i},s_{j}\\right)}&{=}&{\\phi_{\\mathsf{m r}}(s_{i})+\\phi_{\\mathsf{m r}}(s_{j})+\\phi_{r}\\!\\left(s_{i},s_{j}\\right)}\\\\ {\\Phi_{\\mathsf{C}}\\!\\left(s_{i},s_{j}\\right)}&{=}&{\\phi_{\\mathsf{m c}}(s_{i})+\\phi_{\\mathsf{m c}}(s_{j})+\\phi_{\\mathsf{c}}(s_{i},s_{j})}\\end{array}\n$$\n ", "text_format": "latex", "page_idx": 4, "bbox": [69, 560, 293, 611], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 54, "type": "text", "text": "The scores in Equation  ( 4 )  are deﬁned for entity types, relations, and antecedents that are not the null-type    $\\epsilon$  . Scores involving the null label are set to a constant 0:    $\\begin{array}{r l r}{\\lefteqn{\\Phi_{\\mathrm{E}}(\\epsilon,s_{i})\\,=\\,\\Phi_{\\mathrm{R}}(\\epsilon,s_{i},s_{j})\\,=}}\\end{array}\n\n$   $\\Phi_{\\mathrm{{C}}}(s_{i},\\epsilon)=0$  . ", "page_idx": 4, "bbox": [70, 617.3939819335938, 292, 685], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 55, "type": "text", "text": "We use the same span representations    $\\mathbf{g}$   from\n\n ( Lee et al. ,  2017 ) and share them across the three tasks. We start by building bi-directional LSTMs ( Hochreiter and Schmidhuber ,  1997 ) from word, character and ELMo ( Peters et al. ,  2018 ) embed- dings. ", "page_idx": 4, "bbox": [70, 685.1400146484375, 292, 766.0314331054688], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 56, "type": "text", "text": "For a span    $s_{i}$  , its vector representation    $\\mathbf{g}_{i}$   is con- structed by concatenating    $s_{i}$  ’s left and right end points from the BiLSTM outputs, an attention- based soft “headword,” and embedded span width features. Hyperparameters and other implementa- tion details will be described in Section  6 . ", "page_idx": 4, "bbox": [306, 63.68701934814453, 527, 144.57846069335938], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 57, "type": "text", "text": "4.4 Inference and Pruning ", "text_level": 1, "page_idx": 4, "bbox": [306, 156, 439, 168], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 58, "type": "text", "text": "Following previous work, we use beam pruning to reduce the number of pairwise span factors from  $O(n^{4})$   to    $O(n^{2})\\,$   at both training and test time, where    $n$   is the number of words in the document. We deﬁne two separate beams:    $B_{\\mathrm{C}}$   to prune spans for the coreference resolution task, and    $B_{\\mathrm{R}}$   for rela- tion extraction. The spans in the beams are sorted by their span scores    $\\phi_{\\mathrm{m c}}$   and    $\\phi_{\\mathrm{mr}}$   respectively, and the sizes of the beams are limited by  $\\lambda_{\\mathbf{C}}n$   and    $\\lambda_{\\mathrm{R}}n$  . We also limit the maximum width of spans to a ﬁxed number    $W$  , which further reduces the num- ber of span factors to    $O(n)$  . ", "page_idx": 4, "bbox": [306, 174.2780303955078, 527, 336.4644775390625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 59, "type": "text", "text": "5 Knowledge Graph Construction ", "text_level": 1, "page_idx": 4, "bbox": [306, 349, 490, 362], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 60, "type": "text", "text": "We construct a scientiﬁc knowledge graph from a large corpus of scientiﬁc articles. The corpus includes all abstracts (  $110\\mathbf{k}$   in total) from 12 AI conference proceedings from the Semantic Scholar Corpus. Nodes in the knowledge graph correspond to scientiﬁc entities. Edges correspond to scientiﬁc relations between pairs of entities. The edges are typed according to the relation types deﬁned in Sec- tion  3 . Figure  4  shows a part of a knowledge graph created by our method. For example,  Statistical Machine Translation (SMT)  and  grammatical error correction  are nodes in the graph, and they are con- nected through a  Used-for  relation type. In order to construct the knowledge graph for the whole corpus, we ﬁrst apply the S CI IE model over sin- gle documents and then integrate the entities and relations across multiple documents (Figure  3 ). ", "page_idx": 4, "bbox": [306, 371.25, 527, 601.1824340820312], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 61, "type": "text", "text": "Extracting nodes (entities) The S CI IE model extracts entities, their relations, and coreference ", "page_idx": 4, "bbox": [306, 610.4533081054688, 527, 637.5404663085938], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 62, "type": "image", "page_idx": 4, "img_path": "layout_images/D18-1360_3.jpg", "img_caption": "Figure 3 : Knowledge graph construction process. ", "bbox": [306, 657, 526, 761], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "Abstract(1)\n\n—{ SciE\n\nAbstract(2)\n\nAbstract(m)\n\n+=\n\nDocument-level KGs\n\nScientific KG\n", "vlm_text": "The image illustrates a knowledge graph construction process. It consists of the following steps:\n\n1. **Extraction from Abstracts**: Multiple abstracts (labeled Abstract(1), Abstract(2), ..., Abstract(m)) are processed.\n2. **SciIE**: Each abstract goes through a step labeled \"SciIE,\" which likely stands for Scientific Information Extraction.\n3. **Document-level KGs**: This process creates document-level knowledge graphs (KGs) from the abstracts, represented by small networks of colored nodes and connecting lines.\n4. **Merging**: These document-level KGs are then merged to form a larger, integrated \"Scientific KG\" – a comprehensive knowledge graph."}
{"layout": 63, "type": "image", "page_idx": 5, "img_path": "layout_images/D18-1360_4.jpg", "img_caption": "Figure 4 : A part of an automatically constructed scientiﬁc knowledge graph with the most frequent neighbors of the scientiﬁc term  statistical machine translation (SMT)  on the graph. For simplicity we denote  Used-for (Reverse)  as  Uses ,  Evaluated-for (Reverse)  as  Evaluated-by , and replace common terms with their acronyms. The original graph and more examples are given Figure  10  in Appendix  B . ", "bbox": [69, 65, 293, 374], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "Tetrieval o\n\nAdaptation o\n\nSutpooep ©\n\n", "vlm_text": "The image is a part of a scientific knowledge graph centered around the term \"Statistical Machine Translation (SMT).\" It shows the most frequent neighbors of SMT in the graph, categorized by different relationships:\n\n1. **Compare**: \n   - NMT (Neural Machine Translation)\n   - MT (Machine Translation)\n\n2. **Evaluated-by**:\n   - WER (Word Error Rate)\n   - ROUGE\n   - METEOR\n   - BLEU\n   - Perplexity\n\n3. **Conjunction**:\n   - ASR (Automatic Speech Recognition)\n   - Classification\n\n4. **Used-for**:\n   - Retrieval\n   - Translation\n   - Search\n   - Paraphrasing\n   - Semantic Parsing\n   - Grammatical Error Correction\n\n5. **Uses**:\n   - Alignment\n   - Parser\n   - Adaptation\n   - Decoding\n   - RNN (Recurrent Neural Networks)\n   - NN (Neural Networks)\n   - Topic Model\n   - Word Segmentation\n   - Stochastic Local Search\n   - Domain Adaptation\n   - Log-linear Model\n   - Word Alignment\n   - WSD (Word Sense Disambiguation)\n   - Maximum Entropy\n   - Segmentation\n\nThis graph visually represents the connections and interactions of SMT with other terms and fields."}
{"layout": 64, "type": "text", "text": "clusters within one document. Phrases are heuris- tically normalized (described in Section  6 ) using entities and coreference links. In particular, we link all entities that belong to the same coreference cluster to replace generic terms with any other non- generic term in the cluster. Moreover, we replace all the entities in the cluster with the entity that has the longest string. Our qualitative analysis shows that there are fewer ambiguous phrases using coref- erence links (Figure  5 ). We calculate the frequency counts of all entities that appear in the whole cor- pus. We assign nodes in the knowledge graph by selecting the most frequent entities (with counts  $>\\,k)$   in the corpus, and merge in any remaining entities for which a frequent entity is a substring. ", "page_idx": 5, "bbox": [70, 395.99700927734375, 292, 598.8314208984375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 65, "type": "text", "text": "Assigning edges (relations) A pair of entities may appear in different contexts, resulting in differ- ent relation types between those entities (Figure  6 ). For every pair of entities in the graph, we calculate the frequency of different relation types across the whole corpus.We assign edges between entities by selecting the most frequent relation type. ", "page_idx": 5, "bbox": [70, 608.6973266601562, 292, 703.5304565429688], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 66, "type": "text", "text": "6 Experimental Setup ", "text_level": 1, "page_idx": 5, "bbox": [70, 716, 193, 730], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 67, "type": "text", "text": "We evaluate our uniﬁed framework S CI IE on S CI - ERC and SemEval 17. The knowledge graph for ", "page_idx": 5, "bbox": [70, 739.3369750976562, 292, 766.0314331054688], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 68, "type": "image", "page_idx": 5, "img_path": "layout_images/D18-1360_5.jpg", "bbox": [304, 62, 522, 148], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "action detection\npedestrian detection\nhuman detection\nface detection\nobject detection\n\ndetection\n\n@ With Coref. | Without Coref.\n\n1297\n1237\n\n", "vlm_text": "The image is a bar chart comparing performance figures for various detection tasks, measured with and without coreference (Coref). Here are the details:\n\n- **Detection**: \n  - Without Coref: 1297\n  - With Coref: 1237\n\n- **Object Detection**: \n  - Without Coref: 510\n  - With Coref: 585\n\n- **Face Detection**: \n  - Without Coref: 177\n  - With Coref: 124\n\n- **Human Detection**: \n  - Without Coref: 84\n  - With Coref: 90\n\n- **Pedestrian Detection**: \n  - Without Coref: 57\n  - With Coref: 90\n\n- **Action Detection**: \n  - Without Coref: 63\n  - With Coref: 87\n\nThe bars are color-coded: red represents \"Without Coref.\" and blue represents \"With Coref.\""}
{"layout": 69, "type": "text", "text": "Figure 5 : Frequency of detected entities with and without coreferece resolution: using coreference reduces the frequency of the generic phrase  detec- tion  while signiﬁcantly increasing the frequency of speciﬁc phrases. Linking entities through corefer- ence helps disambiguate phrases when generating the knowledge graph. ", "page_idx": 5, "bbox": [306, 159.0640411376953, 527, 253.50448608398438], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 70, "type": "image", "page_idx": 5, "img_path": "layout_images/D18-1360_6.jpg", "bbox": [305, 265, 521, 360], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "# Relation Triples\n\nese “aii or evers) woe eave Helo\n\n", "vlm_text": "The image consists of two bar charts comparing the number of relation triples for different categories.\n\n**Left Chart:**\n- Title: None\n- Categories: \"Conjunction,\" \"Used for,\" \"Used for (Reverse)\"\n- Data for \"MT-ASR\":\n  - Conjunction: 80\n  - Used for: 10\n  - Used for (Reverse): 4\n\n**Right Chart:**\n- Title: None\n- Categories: \"Hyponym of,\" \"Conjunction,\" \"Used for,\" \"Used for (Reverse)\"\n- Data for \"CRF-GM\":\n  - Hyponym of: 25\n  - Conjunction: 4\n  - Used for: 2\n  - Used for (Reverse): 2\n\nBoth charts are presented using light blue bars."}
{"layout": 71, "type": "text", "text": "Figure 6 : Frequency of relation types between pairs of entities: ( left ) automatic speech recognition (ASR) and machine translation (MT), ( right ) con- ditional random ﬁeld (CRF) and graphical model (GM). We use the most frequent relation between pairs of entities in the knowledge graph. ", "page_idx": 5, "bbox": [306, 367.9679870605469, 527, 448.8594665527344], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 72, "type": "text", "text": "scientiﬁc community analysis is built using the Se- mantic Scholar Corpus (110k abstracts in total). ", "page_idx": 5, "bbox": [306, 470.28399658203125, 527, 496.9784851074219], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 73, "type": "text", "text": "6.1 Baselines ", "text_level": 1, "page_idx": 5, "bbox": [306, 506, 376, 518], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 74, "type": "text", "text": "We compare our model with the following base- lines on S CI ERCdataset: ", "page_idx": 5, "bbox": [306, 523.6010131835938, 527, 550.2964477539062], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 75, "type": "text", "text": "•    ${\\bf L S T M+C R F}$   The state-of-the-art NER sys- tem ( Lample et al. ,  2016 ), which applies CRF on top of LSTM for named entity tagging, the approach has also been used in scientiﬁc term extraction ( Luan et al. ,  2017b ). ", "page_idx": 5, "bbox": [318, 558.80029296875, 527, 626.534423828125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 76, "type": "text", "text": "•  LSTM  $^+$  CRF  $+$  ELMo LSTM  $+$  CRF with ELM O  as an additional input feature. ", "page_idx": 5, "bbox": [318, 635.3223266601562, 527, 662.4104614257812], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 77, "type": "text", "text": "•  E2E Rel  State-of-the-art joint entity and re- lation extraction system ( Miwa and Bansal , 2016 ) that has also been used in scientiﬁc lit- erature ( Peters et al. ,  2017 ;  Augenstein et al. , 2017 ). This system uses syntactic features such as part-of-speech tagging and depen- dency parsing. ", "page_idx": 5, "bbox": [318, 671.1983032226562, 527, 766.0314331054688], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 78, "type": "text", "text": "•  E2E Rel(Pipeline)  Pipeline setting of E2E Rel. Extract entities ﬁrst and use entity results as input to relation extraction task. ", "page_idx": 6, "bbox": [82, 63.29429244995117, 292, 103.93148803710938], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 79, "type": "text", "text": "•  E2E Rel+ELMo  E2E Rel with ELM O  as an additional input feature. ", "page_idx": 6, "bbox": [82, 113.37327575683594, 292, 140.46047973632812], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 80, "type": "text", "text": "•  E2E Coref  State-of-the-art coreference sys- tem  Lee et al.  ( 2017 ) combined with ELM O . Our system S CI IE extends E2E Coref with multi-task learning. ", "page_idx": 6, "bbox": [82, 149.9022674560547, 292, 204.08847045898438], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 81, "type": "text", "text": "In the SemEval task, we compare our model S CI IE with the best reported system in the SemEval leaderboard ( Peters et al. ,  2017 ), which extends E2E Rel with several in-domain features such as gazetteers extracted from existing knowledge bases and model ensembles. We also compare with the state of the art on keyphrase extraction ( Luan et al. , 2017b ), which applies semi-supervised methods to a neural tagging model. ", "page_idx": 6, "bbox": [70, 213.92298889160156, 292, 335.46148681640625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 82, "type": "text", "text": "6.2 Implementation details ", "text_level": 1, "page_idx": 6, "bbox": [71, 346, 205, 358], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 83, "type": "text", "text": "Our system extends the implementation and hyper- parameters from  Lee et al.  ( 2017 ) with the follow- ing adjustments. We use a 1 layer BiLSTM with 200-dimensional hidden layers. All the FFNNs have 2 hidden layers of 150 dimensions each. We use 0.4 variational dropout ( Gal and Ghahramani , 2016 ) for the LSTMs, 0.4 dropout for the FFNNs, and 0.5 dropout for the input embeddings. We model spans up to 8 words. For beam pruning, we use    $\\lambda_{\\mathsf{C}}\\,=\\,0.3$   for coreference resolution and  $\\lambda_{\\mathrm{R}}=0.4$   for relation extraction. For constructing the knowledge graph, we use the following heuris- tics to normalize the entity phrases. We replace all acronyms with their corresponding full name and normalize all the plural terms with their singular counterparts. ", "page_idx": 6, "bbox": [70, 362.8110046386719, 292, 579.1944580078125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 84, "type": "text", "text": "7 Experimental Results ", "text_level": 1, "page_idx": 6, "bbox": [71, 589, 201, 603], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 85, "type": "text", "text": "We evaluate S CI IE on S CI ERC and SemEval 17 datasets. We provide qualitative results and human evaluation of the constructed knowledge graph. ", "page_idx": 6, "bbox": [70, 611.4180297851562, 292, 651.6614379882812], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 86, "type": "text", "text": "7.1 IE Results ", "text_level": 1, "page_idx": 6, "bbox": [71, 661, 145, 674], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 87, "type": "text", "text": "Results on SciERC Table  2  compares the result of our model with baselines on the three tasks: en- tity recognition (Table  2a ), relation extraction (Ta- ble  2b ), and coreference resolution (Table  2c ). As evidenced by the table, our uniﬁed multi-task setup ", "page_idx": 6, "bbox": [70, 678.6182861328125, 292, 746.3534545898438], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 88, "type": "table", "page_idx": 6, "img_path": "layout_images/D18-1360_7.jpg", "table_caption": "(c) Coreference resolution. ", "bbox": [305, 61, 527, 344], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "Dev Test\nModel P R_ Fl P R_ Fl\nLSTM+CRF 67.2 65.8 66.5 62.9 61.1 62.0\nLSTM+CRF+ELMo 68.1 66.3 67.2 63.8 63.2 63.5\nE2E Rel(Pipeline) 66.7 65.9 66.3 60.8 61.2 61.0\nE2E Rel 64.3 68.6 66.4 60.6 61.9 61.2\nE2E ReltELMo 67.5 66.3 66.9 63.5 63.9 63.7\nScilE 70.0 66.3 68.1 67.2 61.5 64.2\n(a) Entity recognition.\nDev Test\nModel iv R Fl P R Fl\nE2E Rel(Pipeline) 34.2 33.7 33.9 37.8 34.2 35.9\nE2E Rel 37.3 33.5 35.3 37.1 32.2 34.1\nE2ERel+ELMo 38.5 36.4 37.4 384 34.9 36.6\nSciE 45.4 34.9 39.5 47.6 33.5 39.3\n(b) Relation extraction.\nDev Test\n\nModel PR R Fl P R Fl\nE2E Coref 59.4 52.0 554 60.9 37.3 46.2\nSculE 615 548 580 52.0 449 48.2\n", "vlm_text": "The table presents the results of different models on two tasks: entity recognition and relation extraction. It shows precision (P), recall (R), and F1 scores for both development (Dev) and test sets.\n\n### (a) Entity recognition:\n- **Models Evaluated:**\n  - LSTM+CRF\n  - LSTM+CRF+ELMo\n  - E2E Rel(Pipeline)\n  - E2E Rel\n  - E2E Rel+ELMo\n  - SciIE\n\n- **Development Set Scores:**\n  - SciIE achieves the highest F1 score of 68.1.\n  \n- **Test Set Scores:**\n  - SciIE leads with an F1 score of 64.2.\n\n### (b) Relation extraction:\n- **Models Evaluated:**\n  - E2E Rel(Pipeline)\n  - E2E Rel\n  - E2E Rel+ELMo\n  - SciIE\n\n- **Development Set Scores:**\n  - SciIE reaches the highest F1 score of 39.5.\n\n- **Test Set Scores:**\n  - SciIE has the top F1 score of 39.3.\n\n### (c) Coreference Resolution (mentions \"E2E Coref\" in context):\n- **Models Evaluated:**\n  - E2E Coref\n  - SciIE\n\n- **Development Set Scores:**\n  - SciIE attains the highest F1 score of 58.0.\n  \n- **Test Set Scores:**\n  - SciIE achieves a maximum F1 score of 48.2.\n\nOverall, SciIE demonstrates superior performance across all tasks and datasets tested."}
{"layout": 89, "type": "text", "text": "Table 2 : Comparison with previous systems on the development and test set for our three tasks. For coreference resolution, we report the average P/R/F1 of MUC,  $\\mathbf{B}^{3}$  , and   $\\mathrm{CEAF}_{\\phi_{4}}$   scores. ", "page_idx": 6, "bbox": [305, 353.5559997558594, 527, 410.2334899902344], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 90, "type": "text", "text": "S CI IE outperforms all the baselines. For entity recognition, our model achieves   $1.3\\%$   and   $2.4\\%$  relative improvement over   $\\tt L S T M+C R F$   with and without ELM O , respectively. Moreover, it achieves  $1.8\\%$   and  $2.7\\%$   relative improvement over E2E Rel with and without ELM O , respectively. For rela- tion extraction, we observe more signiﬁcant im- provement with   $13.1\\%$   relative improvement over E2E Rel and  $7.4\\%$   improvement over E2E Rel with ELM O . For coreference resolution, S CI IE outper- forms E2E Coref with  $4.5\\%$   relative improvement. We still observe a large gap between human-level performance and a machine learning system. We invite the community to address this challenging task. ", "page_idx": 6, "bbox": [305, 430.8869934082031, 527, 633.721435546875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 91, "type": "text", "text": "Ablations We evaluate the effect of multi-task learning in each of the three tasks deﬁned in our dataset. Table  3  reports the results for individual tasks when additional tasks are included in the learning objective function. We observe that per- formance improves with each added task in the objective. For example, Entity recognition (65.7) beneﬁts from both coreference resolution (67.5) and relation extraction (66.8). Relation extrac- ", "page_idx": 6, "bbox": [305, 644.0993041992188, 527, 766.0314331054688], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 92, "type": "table", "page_idx": 7, "img_path": "layout_images/D18-1360_8.jpg", "bbox": [70, 60, 294, 141], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "Task Entity Rec. Relation Coref.\n\nMulti Task (SCIIE) 68.1 395 58.0\nSingle Task 65.7 37.9 55.3\n+Entity Rec. - 38.9 57.1\n+Relation 66.8 57.6\n\n+Coreference 67.5 39.5 -\n", "vlm_text": "The table displays results from a series of experiments related to \"Entity Recognition,\" \"Relation,\" and \"Coreference\" tasks. It compares a multitask approach (specifically called \"SciIE\") with several single-task approaches and combinations. Here are the key points:\n\n- **Multitask (SciIE)**: Achieves values of 68.1 for Entity Recognition, 39.5 for Relation, and 58.0 for Coreference.\n\n- **Single Task**: \n  - Entity Rec.: 65.7 for Entity Recognition, 37.9 for Relation, and 55.3 for Coreference.\n  - +Entity Rec.: - (no value for Entity Recognition), 38.9 for Relation, and 57.1 for Coreference.\n  - +Relation: 66.8 for Entity Recognition, - (no value for Relation), and 57.6 for Coreference.\n  - +Coreference: 67.5 for Entity Recognition, 39.5 for Relation, and - (no value for Coreference).\n\nEach row represents a task configuration, and the values likely represent performance metrics (such as accuracy, F1 score, etc.) for each task. The multitask approach in SciIE seems to perform better overall compared to most single-task configurations."}
{"layout": 93, "type": "text", "text": "Table 3 : Ablation study for multitask learning on S CI ERC development set. Each column shows results for the target task. ", "page_idx": 7, "bbox": [71, 149.58302307128906, 292, 189.82644653320312], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 94, "type": "text", "text": "tion (37.9) signiﬁcantly beneﬁts when multi-tasked with coreference resolution (  $7.1\\%$   relative improve- ment). Coreference resolution beneﬁts when multi- tasked with relation extraction, with   $4.9\\%$   relative improvement. ", "page_idx": 7, "bbox": [71, 213.3610382080078, 292, 280.7034606933594], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 95, "type": "text", "text": "Results on SemEval 17 Table  4  compares the results of our model with the state of the art on the SemEval 17 dataset for tasks of span identiﬁcation, keyphrase extraction and relation extraction as well as the overall score. Span identiﬁcation aims at identifying spans of entities. Keyphrase classiﬁ- cation and relation extraction has the same setting with the entity and relation extraction in S CI ERC. Our model outperforms all the previous models that use hand-designed features. We observe more signiﬁcant improvement in span identiﬁcation than keyphrase classiﬁcation. This conﬁrms the bene- ﬁt of our model in enumerating spans (rather than BIO tagging in state-of-the-art systems). More- over, we have competitive results compared to the previous state of the art in relation extraction. We observe less gain compared to the S CI ERC dataset mainly because there are no coference links, and the relation types are not comprehensive. ", "page_idx": 7, "bbox": [71, 291.0762634277344, 292, 548.50048828125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 96, "type": "text", "text": "7.2 Knowledge Graph Analysis ", "text_level": 1, "page_idx": 7, "bbox": [71, 561, 225, 574], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 97, "type": "text", "text": "We provide qualitative analysis and human evalua- tions on the constructed knowledge graph. ", "page_idx": 7, "bbox": [71, 579.9340209960938, 292, 606.62841796875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 98, "type": "text", "text": "Scientiﬁc trend analysis Figure  7  shows the his- torical trend analysis (from 1996 to 2016) of the most popular applications of the phrase  neural net- work , selected according to the statistics of the extracted relation triples with the ‘Used-for’ rela- tion type from speech, computer vision, and NLP conference papers. We observe that, before 2000, neural network  has been applied to a greater per- centage of speech applications compared to the NLP and computer vision papers. In NLP, neural networks ﬁrst gain popularity in language modeling ", "page_idx": 7, "bbox": [71, 617.0012817382812, 292, 766.0314331054688], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 99, "type": "image", "page_idx": 7, "img_path": "layout_images/D18-1360_9.jpg", "img_caption": "Figure 7 : Historical trend for top applications of the keyphrase  neural network  in NLP, speech, and CV conference papers we collected. y-axis indicates the ratio of papers that use  neural network  in the task to the number of papers that is about the task. ", "bbox": [306, 63, 527, 352], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "0.6\n\n0.4\n\n0.2\n\n0.6\n\n0.4\n\n0.2\n\n0.4\n\n0.2\n\n—s— Language Modeling\n—e— Machine Translation\n— POS Tagging\n\n1,995 2,000 2,005\n\n—s— Speech Recognition\n—e— Speech Synthesis\n—+— Speaker Recognition\n\n2,015\n\n1,995 2,000 2,005\n\n—s— Object Recognition\n—e— Object Detection\n—+— Image Segmentation\n\n2,015\n\n1995, 2000 2005\n\n2015\n\n", "vlm_text": "The image consists of three line graphs, each depicting the historical trend in the ratio of conference papers using neural networks in specific tasks within the fields of natural language processing (NLP), speech, and computer vision (CV) from 1995 to 2015. Each graph represents the proportion of papers using neural networks for different tasks compared to the total number of papers about those tasks.\n\n1. The top graph shows trends in three NLP tasks:\n   - Language Modeling (represented by a blue line with square markers)\n   - Machine Translation (represented by a red line with circular markers)\n   - POS Tagging (represented by a green line with diamond markers)\n\n2. The middle graph shows trends in three speech-related tasks:\n   - Speech Recognition (blue line with square markers)\n   - Speech Synthesis (red line with circular markers)\n   - Speaker Recognition (green line with diamond markers)\n\n3. The bottom graph shows trends in three CV tasks:\n   - Object Recognition (blue line with square markers)\n   - Object Detection (red line with circular markers)\n   - Image Segmentation (green line with diamond markers)\n\nIn all graphs, there is a noticeable increase in the ratio of papers using neural networks for these tasks starting around 2009 and continuing through 2015, with some tasks reaching a ratio of nearly 0.6 by 2015. This indicates the growing adoption of neural networks in these application areas during this period."}
{"layout": 100, "type": "image", "page_idx": 7, "img_path": "layout_images/D18-1360_10.jpg", "img_caption": "Figure 8 : Precision/pseudo-recall curves for human evaluation by varying cut-off thresholds. The AUC is 0.751 with coreference, and 0.695 without. ", "bbox": [306, 368, 527, 512], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "—e— With Coref.\n90 EE Without Coref.\ns\n£86\n84\n0 20 40 60 80 100\n\nPseudo-recall %\n", "vlm_text": "The image is a graph showing precision versus pseudo-recall curves for a human evaluation. The blue line represents results \"With Coreference,\" while the red line represents results \"Without Coreference.\" The precision percentage is plotted on the vertical axis, ranging from 84% to 92%, while the pseudo-recall percentage is on the horizontal axis, ranging from 0% to 100%.\n\nThe graph indicates that the Area Under the Curve (AUC) is 0.751 for the curve with coreference and 0.695 for the one without, suggesting better performance with coreference."}
{"layout": 101, "type": "text", "text": "and then extend to other tasks such as POS Tag- ging and Machine Translation. In computer vision, the application of neural networks gains popularity in  object recognition  earlier (around 2010) than the other two more complex tasks of  object detec- tion  and  image segmentation  (hardest and also the latest). ", "page_idx": 7, "bbox": [306, 536.7330932617188, 527, 631.1735229492188], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 102, "type": "text", "text": "Knowledge Graph Evaluation Figure  8  shows the human evaluation of the constructed knowl- edge graph, comparing the quality of automatically generated knowledge graphs with and without the coreference links. We randomly select 10 frequent scientiﬁc entities and extract all the relation triples that include one of the selected entities leading to  $1.5\\mathrm{k}$   relation triples from both systems. We ask four domain experts to annotate each of these ex- ", "page_idx": 7, "bbox": [306, 644.099365234375, 527, 766.031494140625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 103, "type": "table", "page_idx": 8, "img_path": "layout_images/D18-1360_11.jpg", "table_footnote": "Table 4 : Results for scientiﬁc keyphrase extraction and extraction on SemEval 2017 Task 10, comparing with previous best systems. ", "bbox": [70, 61, 528, 165], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "Span Indentification Keyphrase Extraction Relation Extraction Overall\nModel P R Fl P R Fl P R Fl P R Fl\n(Luan 2017) - - 56.9 - - 45.3 - - - - = -\nBest SemEval 55 54 55 44 43 44 36 23 28 44 41 43\nScuE 62.2 55.4 58.6 48.5 43.8 46.0 40.4 21.2 27.8 48.1 41.8 44.7\n", "vlm_text": "The table compares the performance of different models on tasks like Span Identification, Keyphrase Extraction, and Relation Extraction. Here's a breakdown:\n\n### Models:\n1. **Luan 2017**\n2. **Best SemEval**\n3. **SciIE**\n\n### Metrics:\n- **P**: Precision\n- **R**: Recall\n- **F1**: F1 Score\n\n### Results:\n- **Span Identification**\n  - Luan 2017: F1 = 56.9\n  - Best SemEval: P = 55, R = 54, F1 = 55\n  - SciIE: P = 62.2, R = 55.4, F1 = 58.6\n\n- **Keyphrase Extraction**\n  - Luan 2017: F1 = 45.3\n  - Best SemEval: P = 44, R = 43, F1 = 44\n  - SciIE: P = 48.5, R = 43.8, F1 = 46.0\n\n- **Relation Extraction**\n  - Luan 2017: F1 = 28\n  - Best SemEval: P = 36, R = 23, F1 = 28\n  - SciIE: P = 40.4, R = 21.2, F1 = 27.8\n\n- **Overall**\n  - Best SemEval: P = 44, R = 41, F1 = 43\n  - SciIE: P = 48.1, R = 41.8, F1 = 44.7\n\nThe SciIE model generally shows better performance in terms of precision, recall, and F1 scores across the tasks compared to the other models."}
{"layout": 104, "type": "text", "text": "tracted relations to deﬁne ground truth labels. Each domain expert is assigned 2 or 3 entities and all of the corresponding relations. Figure  8  shows preci- sion/recall curves for both systems. Since it is not feasible to compute the actual recall of the systems, we compute the pseudo-recall ( Zhang et al. ,  2015 ) based on the output of both systems. We observe that the knowledge graph curve with coreference linking is mostly above the curve without corefer- ence linking. The precision of both systems is high (above   $84\\%$   for both systems), but the system with coreference links has signiﬁcantly higher recall. ", "page_idx": 8, "bbox": [71, 187.90003967285156, 292, 350.08648681640625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 105, "type": "text", "text": "8 Conclusion ", "text_level": 1, "page_idx": 8, "bbox": [70, 362, 148, 375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 106, "type": "text", "text": "In this paper, we create a new dataset and develop a multi-task model for identifying entities, relations, and coreference clusters in scientiﬁc articles. By sharing span representations and leveraging cross- sentence information, our multi-task setup effec- tively improves performance across all tasks. More- over, we show that our multi-task model is better at predicting span boundaries and outperforms previ- ous state-of-the-art scientiﬁc IE systems on entity and relation extraction, without using any hand- engineered features or pipeline processing. Using our model, we are able to automatically organize the extracted information from a large collection of scientiﬁc articles into a knowledge graph. Our analysis shows the importance of coreference links in making a dense, useful graph. ", "page_idx": 8, "bbox": [71, 384.60400390625, 292, 600.9874267578125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 107, "type": "text", "text": "We still observe a large gap between the perfor- mance of our model and human performance, con- ﬁrming the challenges of scientiﬁc IE. Future work includes improving the performance using semi- supervised techniques and providing in-domain features. We also plan to extend our multi-task framework to information extraction tasks in other domains. ", "page_idx": 8, "bbox": [71, 601.8939819335938, 292, 709.8834228515625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 108, "type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 8, "bbox": [71, 721, 158, 734], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 109, "type": "text", "text": "This research was supported by the Ofﬁce of Naval Research under the MURI grant N00014-18-1- 2670, NSF (IIS 1616112, III 1703166), Allen Dis- tinguished Investigator Award, and gifts from Allen Institute for AI, Google, Amazon, and Bloomberg. We are grateful to Waleed Ammar and AI2 for sharing the Semantic Scholar Corpus. We also thank the anonymous reviewers, UW-NLP group and Shoou-I Yu for their helpful comments. ", "page_idx": 8, "bbox": [71, 739.3369750976562, 292, 766.0314331054688], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 110, "type": "text", "text": "", "page_idx": 8, "bbox": [306, 187.90003967285156, 527, 282.3404846191406], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 111, "type": "text", "text": "References ", "text_level": 1, "page_idx": 8, "bbox": [307, 306, 364, 318], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 112, "type": "text", "text": "Amjad Abu-Jbara and Dragomir Radev. 2011. Co- herent citation-based summarization of scientiﬁc pa- pers. In  Proc. Annual Meeting of the Association for Computational Linguistics: Human Language Tech- nologies . volume 1, pages 500–509. ", "page_idx": 8, "bbox": [306, 324.8116149902344, 527, 380.65155029296875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 113, "type": "text", "text": "Heike Adel and Hinrich Sch¨ utze. 2017. Global normal- ization of convolutional neural networks for joint en- tity and relation classiﬁcation. In  Proc. Conf. Empir- ical Methods Natural Language Process. (EMNLP) . pages 1723–1729. ", "page_idx": 8, "bbox": [306, 388.9476013183594, 527, 444.8375244140625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 114, "type": "text", "text": "Waleed Ammar, Dirk Groeneveld, Chandra Bhagavat- ula, Iz Beltagy, Miles Crawford, Doug Downey, Ja- son Dunkelberger, Ahmed Elgohary, Sergey Feld- man, Vu Ha, et al. 2018. Construction of the litera- ture graph in semantic scholar. In  Proc. Conf. North American Assoc. for Computational Linguistics: Hu- man Language Technologies (NAACL-HLT), (Indus- try Papers) . pages 84–91. ", "page_idx": 8, "bbox": [306, 453.1825866699219, 527, 541.8994750976562], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 115, "type": "text", "text": "Waleed Ammar, Matthew Peters, Chandra Bhagavat- ula, and Russell Power. 2017. The ai2 system at semeval-2017 task 10 (scienceie): semi-supervised end-to-end entity and relation extraction. In  Proc. Int. Workshop on Semantic Evaluation (SemEval) . pages 592–596. Ashton Anderson, Dan McFarland, and Dan Jurafsky. 2012. Towards a computational history of the ACL: 1980-2008. In  Proc. ACL Special Workshop on Re- discovering 50 Years of Discoveries . pages 13–21. Awais Athar and Simone Teufel. 2012a. Context- enhanced citation sentiment detection. In  Proc. Conf. North American Assoc. for Computational Lin- guistics: Human Language Technologies (NAACL- HLT) . pages 597–601. Awais Athar and Simone Teufel. 2012b. Detection of implicit citations for sentiment detection. In  Proc. ", "page_idx": 8, "bbox": [306, 550.2445068359375, 527, 765.7655029296875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 116, "type": "text", "text": "ACL Workshop on Detecting Structure in Scholarly Discourse . pages 18–26. ", "page_idx": 9, "bbox": [82, 64.56158447265625, 292, 87.52550506591797], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 117, "type": "text", "text": "Isabelle Augenstein, Mrinal Das, Sebastian Riedel, Lakshmi Vikraman, and Andrew McCallum. 2017. Semeval 2017 task 10: ScienceIE - extracting keyphrases and relations from scientiﬁc publications. In  Proc. Int. Workshop on Semantic Evaluation (Se- mEval) . Isabelle Augenstein and Anders Søgaard. 2017. Multi- task learning of keyphrase boundary classiﬁcation. In  Proc. Annu. Meeting Assoc. for Computational Linguistics (ACL) . pages 341–346. Kevin Clark and Christopher D. Manning. 2016. Improving coreference resolution by learning entity-level distributed representations. CoRR abs/1606.01323. Ronan Collobert and Jason Weston. 2008. A uniﬁed architecture for natural language processing: Deep neural networks with multitask learning. In  Proc. Int. Conf. Machine Learning (ICML) . pages 160– 167. Huy Hoang Nhat Do, Muthu Kumar Chandrasekaran, Philip S Cho, and Min Yen Kan. 2013. Extracting and matching authors and afﬁliations in scholarly documents. In  Proc. ACM/IEEE-CS Joint Confer- ence on Digital libraries . pages 219–228. Kata G´ abor, Davide Buscaldi, Anne-Kathrin Schu- mann, Behrang QasemiZadeh, Ha¨ ıfa Zargayouna, and Thierry Charnois. 2018. Semeval-2018 Task 7: Semantic relation extraction and classiﬁcation in sci- entiﬁc papers. In  Proc. Int. Workshop on Semantic Evaluation (SemEval) . Kata Gabor, Haifa Zargayouna, Davide Buscaldi, Is- abelle Tellier, and Thierry Charnois. 2016. Se- mantic annotation of the ACL anthology corpus for the automatic analysis of scientiﬁc literature. In Proc. Language Resources and Evaluation Confer- ence (LREC) . Kata G´ abor, Ha¨ ıfa Zargayouna, Isabelle Tellier, Davide Buscaldi, and Thierry Charnois. 2016. Unsuper- vised relation extraction in specialized corpora using sequence mining. In  International Symposium on In- telligent Data Analysis . Springer, pages 237–248. Yarin Gal and Zoubin Ghahramani. 2016. A theoret- ically grounded application of dropout in recurrent neural networks. In  Proc. Annu. Conf. Neural In- form. Process. Syst. (NIPS) . Sonal Gupta and Christopher D Manning. 2011. An- alyzing the dynamics of research by extracting key aspects of scientiﬁc papers. In  Proc. IJCNLP . pages 1–9. Luheng He, Kenton Lee, Omer Levy, and Luke Zettle- moyer. 2018. Jointly predicting predicates and argu- ments in neural semantic role labeling. In  ACL . ", "page_idx": 9, "bbox": [71, 96.33258056640625, 292, 765.76513671875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 118, "type": "text", "text": "Sepp Hochreiter and J¨ urgen Schmidhuber. 1997. Long short-term memory.  Neural computation  9(8):1735– 1780. Kokil Jaidka, Muthu Kumar Chandrasekaran, Beat- riz Fisas Elizalde, Rahul Jha, Christopher Jones, Min-Yen Kan, Ankur Khanna, Diego Molla-Aliod, Dragomir R Radev, Francesco Ronzano, et al. 2014. The computational linguistics summarization pilot task. In  Proc. Text Analysis Conference . Miray Kas. 2011. Structures and statistics of citation networks. Technical report, DTIC Document. Arzoo Katiyar and Claire Cardie. 2017. Going out on a limb: Joint extraction of entity mentions and relations without dependency trees. In  Proc. Annu. Meeting Assoc. for Computational Linguistics (ACL) . volume 1, pages 917–928. Sigrid Klerke, Yoav Goldberg, and Anders Søgaard. 2016. Improving sentence compression by learning to predict gaze. In  HLT-NAACL . Guillaume Lample, Miguel Ballesteros, Sandeep Sub- ramanian, Kazuya Kawakami, and Chris Dyer. 2016. Neural architectures for named entity recognition. In  Proc. Conf. North American Assoc. for Compu- tational Linguistics (NAACL) . Kenton Lee, Luheng He, Mike Lewis, and Luke S. Zettlemoyer. 2017. End-to-end neural coreference resolution. In  EMNLP . Kenton Lee, Luheng He, and Luke Zettlemoyer. 2018. Higher-order coreference resolution with coarse-to- ﬁne inference. In  NAACL . Yi Luan, Chris Brockett, Bill Dolan, Jianfeng Gao, and Michel Galley. 2017a. Multi-task learning for speaker-role adaptation in neural conversation mod- els. In  Proc. IJCNLP . Yi Luan, Yangfeng Ji, Hannaneh Hajishirzi, and Boyang Li. 2016. Multiplicative representations for unsupervised semantic role induction. In  Proc. Annu. Meeting Assoc. for Computational Linguistics (ACL) . page 118. Yi Luan, Mari Ostendorf, and Hannaneh Hajishirzi. 2017b. Scientiﬁc information extraction with semi- supervised neural tagging. In  Proc. Conf. Empirical Methods Natural Language Process. (EMNLP) . Yi Luan, Mari Ostendorf, and Hannaneh Hajishirzi. 2018. The uwnlp system at semeval-2018 task 7: Neural relation extraction model with selectively in- corporated concept embeddings. In  Proc. Int. Work- shop on Semantic Evaluation (SemEval) . pages 788– 792. Makoto Miwa and Mohit Bansal. 2016. End-to-end re- lation extraction using lstms on sequences and tree structures. In  Proc. Annu. Meeting Assoc. for Com- putational Linguistics (ACL) . pages 1105–1116. ", "page_idx": 9, "bbox": [307, 64.51123046875, 527, 765.7647705078125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 119, "type": "text", "text": "Nanyun Peng, Hoifung Poon, Chris Quirk, Kristina Toutanova, and Wen-tau Yih. 2017. Cross-sentence n-ary relation extraction with graph lstms. Trans. Assoc. for Computational Linguistics (TACL)  5:101– 115. Matthew Peters, Waleed Ammar, Chandra Bhagavat- ula, and Russell Power. 2017. Semi-supervised se- quence tagging with bidirectional language models. In  Proc. Annu. Meeting Assoc. for Computational Linguistics (ACL) . volume 1, pages 1756–1765. Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word repre- sentations. In  NAACL . Behrang QasemiZadeh and Anne-Kathrin Schumann. 2016. The ACL RD-TEC 2.0: A language resource for evaluating term extraction and entity recognition methods. In  LREC . Chris Quirk and Hoifung Poon. 2017. Distant su- pervision for relation extraction beyond the sen- tence boundary. In  Proc. European Chapter Assoc. for Computational Linguistics (EACL) . pages 1171– 1182. Marek Rei. 2017. Semi-supervised multitask learning for sequence labeling. In  Proc. Annu. Meeting As- soc. for Computational Linguistics (ACL) . Yanchuan Sim, Noah A Smith, and David A Smith. 2012. Discovering factions in the computational lin- guistics community. In  Proc. ACL Special Workshop on Rediscovering 50 Years of Discoveries . pages 22– 32. Sameer Singh, Sebastian Riedel, Brian Martin, Jiaping Zheng, and Andrew McCallum. 2013. Joint infer- ence of entities, relations, and coreference. In  Proc. of the 2013 workshop on Automated knowledge base construction . ACM, pages 1–6. Pontus Stenetorp, Sampo Pyysalo, Goran Topi´ c, Tomoko Ohta, Sophia Ananiadou, and Jun’ichi Tsu- jii. 2012. Brat: a web-based tool for nlp-assisted text annotation. In  Proc. European Chapter Assoc. for Computational Linguistics (EACL) . pages 102– 107. Swabha Swayamdipta, Sam Thomson, Chris Dyer, and Noah A. Smith. 2017. Frame-semantic parsing with softmax-margin segmental rnns and a syntactic scaf- fold.  CoRR  abs/1706.09528. Chen-Tse Tsai, Gourab Kundu, and Dan Roth. 2013. Concept-based analysis of scientiﬁc literature. In Proc. ACM Int. Conference on Information & Knowl- edge Management . ACM, pages 1733–1738. Adam Vogel and Dan Jurafsky. 2012. He said, she said: Gender in the ACL anthology. In  Proc. ACL Special Workshop on Rediscovering 50 Years of Discoveries . pages 33–41. ", "page_idx": 10, "bbox": [71, 64.56158447265625, 292, 765.7650756835938], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 120, "type": "text", "text": "Sam Wiseman, Alexander M. Rush, and Stuart M. Shieber. 2016. Learning global features for coref- erence resolution. In  HLT-NAACL . Yan Xu, Ran Jia, Lili Mou, Ge Li, Yunchuan Chen, Yangyang Lu, and Zhi Jin. 2016. Improved rela- tion classiﬁcation by deep recurrent neural networks with data augmentation. In  Proc. Int. Conf. Compu- tational Linguistics (COLING) . pages 1461–1470. Congle Zhang, Stephen Soderland, and Daniel S. Weld. 2015. Exploiting parallel news streams for unsuper- vised event extraction.  TACL  3:117–129. Meishan Zhang, Yue Zhang, and Guohong Fu. 2017. End-to-end neural relation extraction with global op- timization. In  Proc. Conf. Empirical Methods Natu- ral Language Process. (EMNLP) . pages 1730–1740. Suncong Zheng, Feng Wang, Hongyun Bao, Yuexing Hao, Peng Zhou, and Bo Xu. 2017. Joint extrac- tion of entities and relations based on a novel tag- ging scheme. In  Proc. Annu. Meeting Assoc. for Computational Linguistics (ACL) . volume 1, pages 1227–1236. ", "page_idx": 10, "bbox": [307, 64.5611572265625, 527, 331.60980224609375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 121, "type": "text", "text": "A Annotation Guideline ", "text_level": 1, "page_idx": 11, "bbox": [71, 64, 204, 76], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 122, "type": "text", "text": "A.1 Entity Category ", "text_level": 1, "page_idx": 11, "bbox": [72, 84, 174, 97], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 123, "type": "text", "text": "•  Task : Applications, problems to solve, sys- tems to construct. E.g. information extraction, machine reading system, image segmentation, etc. •  Method : Methods , models, systems to use, or tools, components of a system, frameworks. E.g. language model, CORENLP, POS parser, kernel method, etc. •  Evaluation Metric : Metrics, measures, or entities that can express quality of a sys- tem/method. E.g. F1, BLEU, Precision, Recall, ROC curve, mean reciprocal rank, mean-squared error, ro- bustness, time complexity, etc. •  Material : Data, datasets, resources, Corpus, Knowledge base. E.g. image data, speech data, stereo images, bilingual dictionary, paraphrased questions, CoNLL, Panntreebank, WordNet, Wikipedia, etc. •  Evaluation Metric : Metric measure or term that can express quality of a system/method. E.g. F1, BLEU, Precision, Recall, ROC curve, mean reciprocal rank, mean-squared error,robustness, compile time, time complex- ity... •  Generic : General terms or pronouns that may refer to a entity but are not themselves infor- mative, often used as connection words. E.g model, approach, prior knowledge, them, it... ", "page_idx": 11, "bbox": [83, 101.45732116699219, 292, 589.5304565429688], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 124, "type": "text", "text": "A.2 Relation Category ", "text_level": 1, "page_idx": 11, "bbox": [71, 599, 184, 611], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 125, "type": "text", "text": "Relation link can not go beyond sentence boundary. We deﬁne 4 asymmetric relation types ( Used-for , Feature-of ,  Hyponym-of ,  Part-of ), together with 2 symmetric relation types ( Compare ,  Conjunction ). B  always points to  A  for asymmetric relations ", "page_idx": 11, "bbox": [71, 616.1110229492188, 292, 683.4534301757812], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 126, "type": "text", "text": "•  Used-for :  B  is used for  A ,  B  models  A ,  A  is trained on  B ,  B  exploits  A ,  A  is based on  B . E.g. The  TISPER system  has been designed to enable many  text applications . ", "page_idx": 11, "bbox": [83, 692.165283203125, 292, 766.0314331054688], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 127, "type": "text", "text": "Our  method  models  user proﬁciency . Our  algorithms  exploits  local soothness . ", "page_idx": 11, "bbox": [350, 63.68701934814453, 527, 91.75350952148438], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 128, "type": "text", "text": "•  Feature-of :  B  belongs to  A ,  B  is a feature of A ,  B  is under  A  domain. E.g. prior knowledge  of the  model genre-speciﬁc regularities  of  discourse structure English text  in  science domain ", "page_idx": 11, "bbox": [318, 98.31230163574219, 527, 187.88449096679688], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 129, "type": "text", "text": "•  Hyponym-of :  B  is a hyponym of  A ,  B  is a type of  A . E.g. TUIT  is a  software library NLP applications  such as  machine trans- lation  and  language generation ", "page_idx": 11, "bbox": [318, 194.4423065185547, 527, 269.0934753417969], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 130, "type": "text", "text": "•  Part-of :  B  is a part of  A ... E.g. The  system  includes two models:  speech recognition  and  natural language under- standing We incorporate  NLU module  to the  sys- tem . ", "page_idx": 11, "bbox": [318, 275.6522521972656, 527, 363.85247802734375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 131, "type": "text", "text": "•  Compare : Symmetric relation (use blue to denote entity). Opposite of conjunction, com- pare two models/methods, or listing two op- posing entities. E.g. Unlike the  quantitative prior , the  qualita- tive prior  is often ignored... We compare our  system  with previous sequential tagging systems ... ", "page_idx": 11, "bbox": [318, 370.4112854003906, 527, 485.70947265625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 132, "type": "text", "text": "•  Conjunction : Symmetric relation (use blue to denote entity). Function as similar role or use/incorporate with. E.g. obtained from  human expert  or  knowl- edge base NLP applications such as  machine trans- lation  and  language generation ", "page_idx": 11, "bbox": [318, 492.2682800292969, 527, 594.0174560546875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 133, "type": "text", "text": "A.3 Coreference ", "text_level": 1, "page_idx": 11, "bbox": [306, 601, 393, 613], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 134, "type": "text", "text": "Two Entities that points to the same concept.  Anaphora and Cataphora : ", "page_idx": 11, "bbox": [306.93701171875, 618.4450073242188, 502.307861328125, 631.5904541015625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 135, "type": "text", "text": "", "page_idx": 11, "bbox": [324.1125793457031, 637.4642944335938, 453, 651.6570434570312], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 136, "type": "text", "text": "We introduce a  machine reading system ... The  system ... The  prior knowledge  include...Such knowledge  can be applied to... ", "page_idx": 11, "bbox": [350, 656.9500122070312, 527, 712.1144409179688], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 137, "type": "text", "text": "•  Coreferring noun phrase : ", "text_level": 1, "page_idx": 11, "bbox": [317, 721, 450, 734], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 138, "type": "text", "text": "We develop a  part-of-speech tagging sys- tem ...The  POS tagger ... ", "page_idx": 11, "bbox": [350, 739.3369750976562, 527, 766.0314331054688], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 139, "type": "text", "text": "A.4 Notes ", "text_level": 1, "page_idx": 12, "bbox": [72, 65, 125, 75], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 140, "type": "text", "text": "1.  Entity boundary annotation follows the ACL RD-TEC Annotation Guideline ( Qasem- iZadeh and Schumann ,  2016 ), with the exten- tion that spans can be embedded in longer spans, only if the shorter span is involved in a relation. ", "page_idx": 12, "bbox": [80, 81.16400909423828, 292, 162.05545043945312], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 141, "type": "text", "text": "2.  Do not include determinators (such as the, a), or adjective pronouns (such as this,its, these, such) to the span. If generic phrases are not involved in a relation, do not tag them. ", "page_idx": 12, "bbox": [80, 171.4250030517578, 292, 225.21847534179688], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 142, "type": "text", "text": "3. Do not tag relation if one entity is: ", "page_idx": 12, "bbox": [80, 234.58900451660156, 244.4114990234375, 247.73446655273438], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 143, "type": "text", "text": "•  Variable bound: We introduce a neural based approach.. Its  beneﬁt is... •  The word  which : We introduce a neural based approach, which  is a... ", "page_idx": 12, "bbox": [105, 254.6129913330078, 292, 339.4894714355469], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 144, "type": "text", "text": "4. Do not tag coreference if the entity is ", "page_idx": 12, "bbox": [80, 348.8599853515625, 256.4988098144531, 362.0054626464844], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 145, "type": "text", "text": "•  Generically-used Other-ScientiﬁcTerm: ...advantage gained from  local smooth- ness  which... We present algorithms ex- ploiting  local smoothness  in more aggres- sive ways... •  Same scientiﬁc term but refer to different examples: We use a  data structure , we also use an- other  data structure ... ", "page_idx": 12, "bbox": [105, 369.8809814453125, 292, 495.4054870605469], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 146, "type": "text", "text": "5. Do not label negative relations: ", "page_idx": 12, "bbox": [80, 504.7750244140625, 230.12059020996094, 517.9204711914062], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 147, "type": "text", "text": " $\\mathrm{X}$   is not used in   $\\mathrm{Y}$   or   $\\mathrm{X}$   is hard to be applied in Y ", "page_idx": 12, "bbox": [92, 522.8079833984375, 292, 549.50244140625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 148, "type": "text", "text": "B Annotation and Knowledge Graph Examples ", "text_level": 1, "page_idx": 12, "bbox": [71, 560, 269, 587], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 149, "type": "text", "text": "Here we take a screen shot of the BRAT interface for an ACL paper in Figure  9 . We also attach the original ﬁgure of Figure 3 in Figure  10 . More examples can be found in the project website 4 . ", "page_idx": 12, "bbox": [72, 595.1279907226562, 292, 648.92041015625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 150, "type": "image", "page_idx": 13, "img_path": "layout_images/D18-1360_12.jpg", "img_caption": "Figure 9 : Annotation example 1 from ACL ", "bbox": [75, 78, 525, 301], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "-COREF-\n\n(Generis V5FD-FOR USED FOR—nother=SelentificTermy*— 'YPONYM-OF —~\\other-SclentificTerm)\n\n1| Methods developed for spelling correction for languages like English (see the review by Kukich (Kukich, 1992)) are not\n\ne COREF:\n\nreadily applicable to agglutinative languages .\n|\n\n- -COREF- -COREF- >\nCOREF:\n\nUSED-FOR-\n\nUSED-FOR-\nasst USED FOR other: SclentificTerm) ”\\onneeserenuiietern————onuncrion——+\n\n2\\ This poster presents an approach to spelling correction in agglutinative languages that is based on two-level morphology anda\n|\n\n:\n\ndynamic-programming based search algorithm g\n\n=\n\n3 After an overview of our approach, we present results from experiments with spelling correction in Turkish\n\nCOREF: >\n\ntte\n— FoR ~ te SCETRIRETEA]\n\n-COREF-\n\n", "vlm_text": "The image is an annotated linguistic example from the ACL (Association for Computational Linguistics). It displays sentences with labels and connections indicating relationships between different parts of the text. Annotations include:\n\n- \"Generic,\" \"Task,\" and \"Other-ScientificTerm\" labels for different phrases.\n- \"USED-FOR,\" \"HYPONYM-OF,\" and \"CONJUNCTION\" indicating the relationships between terms.\n- \"COREF\" lines showing coreference, linking phrases referring to the same entity.\n\nThe sentences discuss methods for spelling correction, particularly in agglutinative languages, using approaches based on morphology and algorithms."}
{"layout": 151, "type": "image", "page_idx": 13, "img_path": "layout_images/D18-1360_13.jpg", "bbox": [97, 328, 502, 707], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "information Tetrieval 0.\n\nspeech synthesis ©\n\nParaphrasing o-\n\nf\n& &\n3 Fy § é\n) ¥ E £ g ¢\n% #\n4 & E fi PF ¢\na ° ° &\n% Ko) g, ° ° ¢\nom % % °\nSe, e é\na %\ney, &Q\ni ‘\n1m “ati ‘\nhe 0 6 A\nSlo.\n“ton °\nCom,\nang) We\nert prog ipo 5\n0 Uses\n\nise,\n\n\"ative\n\nvey Oday\nee.\n% ara,\n\niyi ty,\n\n° Ye Ong.\n%\n2 i :\n% %,\n\nuonepsuen O\n", "vlm_text": "This image is a mind map centered around \"statistical machine translation.\" It links to various concepts and categories, including:\n\n- **Evaluated by**: Includes BLEU, ROUGE, METEOR, word error rate, and perplexity.\n- **Uses**: Lists segmentation, maximum entropy, decoder, word sense disambiguation, word alignment, discriminative training, log-linear model, domain adaptation, stochastic local search, translation model, parser, n-gram language model, topic model, word segmentation, discriminative model, recurrent neural network, decoding, adaptation, and neural network.\n- **Used for**: Includes semantic parsing, retrieval, speech translation, grammatical error correction, hybrid system, search, paraphrasing, translation, and alignment.\n- **Conjunction**: Links to neural machine translation, machine translation, answer set programming, information retrieval, speech synthesis, classification, speech recognition, and automatic speech recognition.\n\nThe map shows relationships and connections between different terms and techniques related to statistical machine translation."}
{"layout": 152, "type": "text", "text": "Figure 10 : An example of our automatically generated knowledge graph centered on  statistical machine translation . This is the original ﬁgure of Figure  4 . ", "page_idx": 13, "bbox": [72, 720.5009765625, 525.5415649414062, 747.1954345703125], "page_size": [595.2760009765625, 841.8900146484375]}
