{"page": 0, "image_path": "doc_images/P19-1459_0.jpg", "ocr_text": "Probing Neural Network Comprehension of Natural Language Arguments\n\nTimothy Niven and Hung-Yu Kao\n\nIntelligent Knowledge Management Lab\nDepartment of Computer Science and Information Engineering\nNational Cheng Kung University\nTainan, Taiwan\ntim.niven.public@gmail.com, hykao@mail.ncku.edu.tw\n\nAbstract\n\nWe are surprised to find that BERT’s peak per-\nformance of 77% on the Argument Reasoning\nComprehension Task reaches just three points\nbelow the average untrained human baseline.\nHowever, we show that this result is entirely\naccounted for by exploitation of spurious sta-\ntistical cues in the dataset. We analyze the\nnature of these cues and demonstrate that a\nrange of models all exploit them. This anal-\nysis informs the construction of an adversarial\ndataset on which all models achieve random\naccuracy. Our adversarial dataset provides a\nmore robust assessment of argument compre-\nhension and should be adopted as the standard\nin future work.\n\n1 Introduction\n\nArgumentation mining is the task of determin-\ning argumentative structure in natural language\ntext - e.g., which text segments represent claims,\nand which comprise reasons that support or attack\nthose claims (Mochales and Moens, 2011; Lippi\nand Torroni, 2016). This is a challenging task for\nmachine learners, as it can be hard even for hu-\nmans to determine when two text segments stand\nin argumentative relation, as evidenced by studies\non argument annotation (Habernal et al., 2014).\nOne approach to this problem is to focus on\nwarrants (Toulmin, 1958) - a form of world\nknowledge that permit inferences. Consider a sim-\nple argument: ‘“(1) It is raining; therefore (2) you\nshould take an umbrella.”!_ The warrant “(3) it\nis bad to get wet” could license this inference.\nKnowing (3) facilitates drawing the inferential\nconnection between (1) and (2). However it would\nbe hard to find it stated anywhere since warrants\nare most often left implicit (Walton, 2005). Thus,\non this approach, machine learners must not only\nreason with warrants but also discover them.\n\n'This example adapted from Black and Hunter (2012)\n\nClaim Google is not a harmful monopoly\nReason People can choose not to use Google\nWarrant — Other search engines don’t redirect to Google\n\nAlternative All other search engines redirect to Google\n\nReason (and since) Warrant > Claim\nReason (but since) Alternative > — Claim\n\nFigure 1: An example of a data point from the ARCT\ntest set and how it should be read. The inference from\nRand A to —C is by design.\n\nThe Argument Reasoning Comprehension Task\n(ARCT) (Habernal et al., 2018a) defers the prob-\nlem of discovering warrants and focuses on in-\nference. An argument is provided, comprising a\nclaim C and reason R. This task is to pick the cor-\nrect warrant W over a distractor, called the alter-\nnative warrant A. The alternative is written such\nthat R \\ A — -=C. An alternative warrant for\nour earlier example could be “(4) it is good to get\nwet,” in which case we have (1) (A (4) — “(=2) you\nshouldn’t take an umbrella.” An example from the\ndataset is given in Figure 1.\n\nThe ARCT SemEval shared task (Habernal\net al., 2018b) verified the challenging nature of\nthis problem. Even supplying warrants, learners\nstill need to rely on further world knowledge. For\nexample, to correctly classify the data point in Fig-\nure | it is at least required to know how consumer\nchoice and web re-directs relate to the concept\nof monopoly, and that Google is a search engine.\nAll but one participating system in the shared task\ncould not exceed 60% accuracy (on binary classi-\nfication).\n\nIt is therefore surprising that BERT (Devlin\net al., 2018) achieves 77% test set accuracy with\nits best run (Table 1), only three points below the\naverage (untrained) human baseline. Without sup-\nplying the required world knowledge for this task\nit does not seem reasonable to expect it to perform\nso well. This motivates the question: what has\nBERT learned about argument comprehension?\n\n4658\n\nProceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4658-4664\nFlorence, Italy, July 28 - August 2, 2019. ©2019 Association for Computational Linguistics\n", "vlm_text": "Probing Neural Network Comprehension of Natural Language Arguments \nTimothy Niven  and  Hung-Yu Kao \nIntelligent Knowledge Management Lab Department of Computer Science and Information Engineering National Cheng Kung University Tainan, Taiwan tim.niven.public@gmail.com ,  hykao@mail.ncku.edu.tw \nAbstract \nWe are surprised to ﬁnd that BERT’s peak per- formance of   $77\\%$   on the Argument Reasoning Comprehension Task reaches just three points below the average untrained human baseline. However, we show that this result is entirely accounted for by exploitation of spurious sta- tistical cues in the dataset. We analyze the nature of these cues and demonstrate that a range of models all exploit them. This anal- ysis informs the construction of an adversarial dataset on which all models achieve random accuracy. Our adversarial dataset provides a more robust assessment of argument compre- hension and should be adopted as the standard in future work. \n1 Introduction \nArgumentation mining is the task of determin- ing argumentative structure in natural language text - e.g., which text segments represent claims, and which comprise reasons that support or attack those claims ( Mochales and Moens ,  2011 ;  Lippi and Torroni ,  2016 ). This is a challenging task for machine learners, as it can be hard even for hu- mans to determine when two text segments stand in argumentative relation, as evidenced by studies on argument annotation ( Habernal et al. ,  2014 ). \nOne approach to this problem is to focus on warrants  ( Toulmin ,  1958 ) - a form of world knowledge that permit inferences. Consider a sim- ple argument: “(1) It is raining; therefore (2) you should take an umbrella.” 1 The warrant “(3) it is bad to get wet” could license this inference. Knowing (3) facilitates drawing the inferential connection between (1) and (2). However it would be hard to ﬁnd it stated anywhere since warrants are most often left implicit ( Walton ,  2005 ). Thus, on this approach, machine learners must not only reason with warrants but also discover them. \nClaim Google is not a harmful monopoly Reason People can choose not to use Google Warrant Other search engines don’t redirect to Google Alternative  All other search engines redirect to Google \nReason  (and since)  Warrant  $\\rightarrow$  Reason  (but since)  Alternative  $\\rightarrow\\,\\neg\\,\\mathbf{C l a i m}$  \nFigure 1: An example of a data point from the ARCT test set and how it should be read. The inference from  $R$   and  $A$   to  $\\neg C$   is by design. \nThe Argument Reasoning Comprehension Task (ARCT) ( Habernal et al. ,  2018a ) defers the prob- lem of discovering warrants and focuses on in- ference. An argument is provided, comprising a claim    $C$   and reason    $R$  . This task is to pick the cor- rect warrant    $W$   over a distractor, called the  alter- native warrant    $A$  . The alternative is written such that    $R\\,\\wedge\\,A\\;\\to\\;\\neg C$  . An alternative warrant for our earlier example could be “(4) it is good to get wet,” in which case we have   $(1)\\land(4)\\rightarrow\\mathfrak{C}(\\lnot2)$   you shouldn’t take an umbrella.” An example from the dataset is given in Figure 1. \nThe ARCT SemEval shared task ( Habernal et al. ,  2018b ) veriﬁed the challenging nature of this problem. Even supplying warrants, learners still need to rely on further world knowledge. For example, to correctly classify the data point in Fig- ure 1 it is at least required to know how consumer choice and web re-directs relate to the concept of monopoly, and that Google is a search engine. All but one participating system in the shared task could not exceed  $60\\%$   accuracy (on binary classi- ﬁcation). \nIt is therefore surprising that BERT ( Devlin et al. ,  2018 ) achieves    $77\\%$   test set accuracy with its best run (Table 1), only three points below the average (untrained) human baseline. Without sup- plying the required world knowledge for this task it does not seem reasonable to expect it to perform so well. This motivates the question: what has BERT learned about argument comprehension? "}
{"page": 1, "image_path": "doc_images/P19-1459_1.jpg", "ocr_text": "Dev Test\n\nMean Mean Median — Max\nHuman (trained) 0.909 + 0.11\nHuman (untrained) 0.798 + 0.16\nBERT (Large) 0.701 + 0.05 | 0.671+0.09 0.712 0.770\nGIST (Choi and Lee, 2018) 0.716 + 0.01 | 0.711 + 0.01\nBERT (Base) 0.680 + 0.02 | 0.623+0.07 0.651 0.685\nWorld Knowledge (Botschen et al., 2018) | 0.674 + 0.0 0.568 + 0.03 0.610\nBoV 0.639 + 0.02 | 0.564+0.02 0.569 0.595\nBiLSTM 0.658 + 0.01 | 0.552+0.02 0.552 0.592\n\nTable 1: Baselines and BERT results. Our results come from 20 different random seeds (+ gives the standard\ndeviation). The mean for BERT Large is skewed by the 5/20 random seeds for which it failed to train, a problem\nnoted by Devlin et al. (2018). We therefore consider the median a better measure of BERT’s average performance.\nThe mean of the non-degenerate runs for BERT (Large) is 0.716 + 0.04.\n\n7 Softmax 7\n\neT\n\nle r Wo |e ro Wy\n\nFigure 2: General architecture of the models in our\nexperiments. Logits are independently calculated for\neach argument-warrant pair then concatenated and\npassed through softmax.\n\nTo investigate BERT’s decision making we\nlooked at data points it finds easy to classify over\nmultiple runs. Habernal et al. (2018b) performed\na similar analysis with the SemEval submissions,\nand consistent with their results we found that\nBERT exploits the presence of cue words in the\nwarrant, especially “not.” Through probing exper-\niments designed to isolate such effects, we demon-\nstrate in this work that BERT’s surprising perfor-\nmance can be entirely accounted for in terms of\nexploiting spurious statistical cues.\n\nHowever, we show that the major problem can\nbe eliminated in ARCT. Since RA A > —=C, we\ncan add a copy of each data point with the claim\nnegated and the label inverted. This means that\nthe distribution of statistical cues in the warrants\nwill be mirrored over both labels, eliminating the\nsignal. On this adversarial dataset all models per-\nform randomly, with BERT achieving a maximum\ntest set accuracy of 53%. The adversarial dataset\ntherefore provides a more robust evaluation of ar-\ngument comprehension and should be adopted as\nthe standard in future work on this dataset.\n\n2 Task Description and Baselines\n\nLet i = 1,...,n index each point in the dataset\nD, where |D| = n. The two candidate warrants\nin each case are randomly assigned a binary label\nj € {0, 1}, such that each has an equal probability\nof being correct. The inputs are the representations\n\nfor the claim c®, reason r, warrant zero wl),\n\nand warrant one wi! The label y is a binary\nindicator corresponding to the correct warrant.\nThe general architecture for all models is given\nin Figure 2. Shared parameters @ are learned to\nclassify each warrant independently with the ar-\n\ngument, yielding the logits:\n\nThese are then concatenated and passed through\nsoftmax to determine a probability distribution\nover the two warrants p = softmax((z)), 2).\nThe prediction is then g® = arg max, p.\n\nThe baselines are a bag of vectors (BoV),\nbidirectional LSTM (Hochreiter and Schmidhu-\nber, 1997) (BiLSTM), the SemEval winner GIST\n(Choi and Lee, 2018), the best model of Botschen\net al. (2018), and human performance (Table 1).\nFor all of our experiments we use grid search to se-\nlect hyperparameters, dropout regularization (Sri-\nvastava et al., 2014), and Adam (Kingma and Ba,\n2014) for optimization. We anneal the learning\nrate by 1/10 when validation accuracy drops. The\nfinal parameters come from the epoch with maxi-\nmum validation accuracy. The BoV and BiLSTM\ninputs are 300-dimensional GloVe embeddings\ntrained on 640B tokens (Pennington et al., 2014).\nCode to reproduce all experiments, and detailing\nall hyperparameters, is provided on GitHub.”\n\n7https://github.com/IKMLab/arct2. git\n\n4659\n", "vlm_text": "The table compares the performance of different models and humans on a development (Dev) and test set. The metrics provided for each are mean (with error), median, and maximum scores. Here's a breakdown:\n\n- **Human (trained)** and **Human (untrained)** are the top two rows, likely indicating benchmark human performance.\n- Various models follow, including **BERT (Large)**, **GIST**, **BERT (Base)**, **World Knowledge**, **BoV**, and **BiLSTM**.\n- The performance is measured in terms of mean scores for both the Dev and Test sets, with the test set also including median and max scores.\n- The **GIST** model shows the highest mean performance on the Dev set and is competitive on the Test set.\n- **BERT (Large)** has the best median and max scores on the Test set, indicating strong performance.\n\nThis table likely benchmarks the performance of AI models on a specific task, comparing them against human baselines.\nTable 1: Baselines and BERT results. Our results com m 20 different random seeds (  $\\pm$   gives the standard deviation). The mean for BERT Large is skewed by the  $5/20$   random seeds for which it failed to train, a problem noted by  Devlin et al.  ( 2018 ). We therefore consider the median a better measure of BERT’s average performance. The mean of the non-degenerate runs for BERT (Large) is    $0.716\\pm0.04$  . \nThe image depicts the general architecture of a model used in the experiments described in the accompanying caption. \n\n- At the bottom, there are two groups of elements labeled with the letters 'c', 'r', 'w0', and 'w1'. These represent different components in the model architecture: 'c' for claim, 'r' for reason, and 'w0' and 'w1' for two different warrants.\n- Each group has a bracket indicating that these elements are combined in some way to form independent argument-warrant pairs.\n- The pairs are processed by a function or mechanism represented by θ (theta), which outputs logits labeled as 'z0' and 'z1'.\n- These logits are then concatenated and passed through a Softmax layer at the top of the diagram. \n\nThis architecture allows the model to independently calculate logits for each argument-warrant pair before combining them for a final prediction.\nTo investigate BERT’s decision making we looked at data points it ﬁnds easy to classify over multiple runs.  Habernal et al.  ( 2018b ) performed a similar analysis with the SemEval submissions, and consistent with their results we found that BERT exploits the presence of cue words in the warrant, especially “not.” Through probing exper- iments designed to isolate such effects, we demon- strate in this work that BERT’s surprising perfor- mance can be entirely accounted for in terms of exploiting spurious statistical cues. \nHowever, we show that the major problem can be eliminated in ARCT. Since    $R\\wedge A\\to\\lnot C$  , we can add a copy of each data point with the claim negated and the label inverted. This means that the distribution of statistical cues in the warrants will be mirrored over both labels, eliminating the signal. On this adversarial dataset all models per- form randomly, with BERT achieving a maximum test set accuracy of    $53\\%$  . The adversarial dataset therefore provides a more robust evaluation of ar- gument comprehension and should be adopted as the standard in future work on this dataset. \n2 Task Description and Baselines \nLet    $i\\,=\\,1,\\dots,n$   index each point in the dataset  $\\mathcal{D}$  , where    $|\\mathcal{D}|\\,=\\,n$  . The two candidate warrants in each case are randomly assigned a binary label  $j\\in\\{0,1\\}$  , such that each has an equal probability of being correct. The inputs are the representations for the claim    $\\mathbf{c}^{(i)}$  , reason    $\\mathbf{r}^{(i)}$  , warrant zero    $\\mathbf{w}_{0}^{(i)}$  , and warrant one    $\\mathbf{w}_{1}^{(i)}$  . The label    $y^{(i)}$   is a binary indicator corresponding to the correct warrant. \nThe general architecture for all models is given in Figure 2. Shared parameters    $\\pmb{\\theta}$   are learned to classify each warrant independently with the ar- gument, yielding the logits: \n\n$$\nz_{j}^{(i)}=\\pmb\\theta[\\pmb{c}^{(i)};\\pmb{r}^{(i)};\\pmb{w}_{j}^{(i)}]\n$$\n \nThese are then concatenated and passed through softmax to determine a probability distribution over the two warrants    $\\mathbf{p}^{(i)}\\!\\stackrel{\\cdot}{=}\\!\\mathrm{softmax}\\!\\big([z_{0}^{(i)},z_{1}^{(i)}]\\big)$  . The prediction is then  $\\hat{y}^{(i)}=\\arg\\operatorname*{max}_{j}\\mathbf{p}^{(i)}$  . \nThe baselines are a bag of vectors   $\\mathrm{(BoV)}$  , bidirectional LSTM ( Hochreiter and Schmidhu- ber ,  1997 ) (BiLSTM), the SemEval winner GIST ( Choi and Lee ,  2018 ), the best model of  Botschen et al.  ( 2018 ), and human performance (Table 1). For all of our experiments we use grid search to se- lect hyperparameters, dropout regularization ( Sri- vastava et al. ,  2014 ), and Adam ( Kingma and Ba , 2014 ) for optimization. We anneal the learning rate by    $1/10$   when validation accuracy drops. The ﬁnal parameters come from the epoch with maxi- mum validation accuracy. The BoV and BiLSTM inputs are  300 -dimensional GloVe embeddings trained on  640 B tokens ( Pennington et al. ,  2014 ). Code to reproduce all experiments, and detailing all hyperparameters, is provided on GitHub. "}
{"page": 2, "image_path": "doc_images/P19-1459_2.jpg", "ocr_text": "BERT\n\n66_ 68 860 6\n\nClaim Reason\n\nTF\n\nWarrant\n\nFigure 3: Processing an argument-warrant pair with BERT. The reason (with word pieces of length a) and claim\n(length 6) together form the first utterance, and the warrant (length c) is the second. The final CLS vector is then\n\npassed to a linear layer to calculate the logit 2, )\n\n3 BERT\n\nOur BERT classifier is visualized in Figure 3. The\nclaim and reason are joined to form the first text\nsegment, which is paired with each warrant and in-\ndependently processed. The final layer CLS vector\nis passed to a linear layer to obtain the logits 2).\nThe whole architecture is fine-tuned. The learning\nrate is 2e—° and we allow a maximum of 20 train-\ning epochs, taking the parameters from the epoch\nwith the best validation set accuracy. We use the\nHugging Face PyTorch implementation.?\n\nDevlin et al. (2018) report that, on small\ndatasets, BERT sometimes fails to train, yield-\ning degenerate results. ARCT is very small with\n1, 210 training observations. In 5/20 runs we en-\ncountered this phenomenon, seeing close to ran-\ndom accuracies on validation and test sets. These\ncases occurred where training accuracy was also\nnot significantly above random (< 80%). Remov-\ning the degenerate runs, BERT’s mean is 71.6 +\n0.04., which would beat the previous state of the\nart - as would the median of 71.2%, which is a\nbetter average than the overall mean since it is not\nskewed by the degenerate cases. However, our\nmain finding is that these results are not mean-\ningful and should be discarded. In the following\nsections we focus on BERT’s peak performance\nof 77% to make this case.\n\n3https://github.com/huggingface/pytorch-pretrained-\nBERT\n\n4 Statistical Cues\n\nThe major source of spurious statistical cues in\nARCT comes from uneven distributions of lin-\nguistic artifacts over the warrants, and therefore\nover the labels. This section aims to demonstrate\nthe presence and nature of these cues. We only\nconsider unigrams and bigrams, although more so-\nphisticated cues may be present. To this end, we\naim to calculate how beneficial it is for a model\nto exploit a cue k, and how pervasive it is in the\ndataset (indicating the strength of the signal).\nFormally, let 1) be the set of tokens in the war-\nrant for data point 7 with label 7. We define a\ncue’s applicability aj, as the number of data points\nwhere it occurs with one label but not the other:\n\nn\no, =o [ai e 1H Ak ete \"]\ni=1\nThe productivity 7, of a cue is defined as the pro-\n\nportion of applicable data points for which it pre-\ndicts the correct answer:\n\nTht [ake TM AK ETI Ay = 3]\n\nak\n\nTk\n\nFinally, we define the coverage &, of a cue as\nthe proportion of applicable cases over the total\nnumber of data points: €, = a,/n. In these\nterms, the productivity of a cue measures the ben-\nefit of exploiting it, while coverage measures the\n\n4660\n", "vlm_text": "This image is a diagram illustrating the architecture of a BERT model used for processing input data. It shows how claims, reasons, and warrants are tokenized and input into the BERT model. The tokens are labeled and encoded, going through various layers marked by interconnected nodes representing the deep connections within BERT. The different sections are separated by special tokens like [CLS] and [SEP] to signify classification and separation tasks in natural language processing.\nFigure 3: Processing an argument-warrant pair with BERT. The reason (with word pieces of length    $a$  ) and claim (length  b ) together form the ﬁrst utterance, and the warrant (length  $c$  ) is the second. The ﬁnal CLS vector is then passed to a linear layer to calculate the logit  $z_{j}^{(i)}$  . \n3 BERT \nOur BERT classiﬁer is visualized in Figure 3. The claim and reason are joined to form the ﬁrst text segment, which is paired with each warrant and in- dependently processed. The ﬁnal layer CLS vector is passed to a linear layer to obtain the logits  $z_{j}^{(i)}$  . The whole architecture is ﬁne-tuned. The learning rate is    $2e^{-5}$    and we allow a maximum of  20  train- ing epochs, taking the parameters from the epoch with the best validation set accuracy. We use the Hugging Face PyTorch implementation. \nDevlin et al. ( 2018 ) report that, on small datasets, BERT sometimes fails to train, yield- ing degenerate results. ARCT is very small with 1 ,  210  training observations. In  5 / 20  runs we en- countered this phenomenon, seeing close to ran- dom accuracies on validation and test sets. These cases occurred where training accuracy was also not signiﬁcantly above random   $(<80\\%)$  ). Remov- ing the degenerate runs, BERT’s mean is    $71.6\\pm$  0 . 04 ., which would beat the previous state of the art - as would the median of    $71.2\\%$  , which is a better average than the overall mean since it is not skewed by the degenerate cases. However, our main ﬁnding is that these results are not mean- ingful and should be discarded. In the following sections we focus on BERT’s peak performance of  $77\\%$   to make this case. \n4 Statistical Cues \nThe major source of spurious statistical cues in ARCT comes from uneven distributions of lin- guistic artifacts over the warrants, and therefore over the labels. This section aims to demonstrate the presence and nature of these cues. We only consider unigrams and bigrams, although more so- phisticated cues may be present. To this end, we aim to calculate how beneﬁcial it is for a model to exploit a cue    $k$  , and how pervasive it is in the dataset (indicating the strength of the signal). \nFormally, let    $\\mathbb{T}_{j}^{(i)}$  be the set of tokens in the war- rant for data point    $i$   with label    $j$  . We deﬁne a cue’s  applicability  $\\alpha_{k}$   as the number of data points where it occurs with one label but not the other: \n\n$$\n\\alpha_{k}=\\sum_{i=1}^{n}\\mathbb{1}\\left[\\exists j,k\\in\\mathbb{T}_{j}^{(i)}\\land k\\notin\\mathbb{T}_{\\neg j}^{(i)}\\right]\n$$\n \nThe  productivity    $\\pi_{k}$   of a cue is deﬁned as the pro- portion of applicable data points for which it pre- dicts the correct answer: \n\n$$\n\\pi_{k}={\\frac{\\sum_{i=1}^{n}{\\mathbb{1}}\\left[\\exists j,k\\in\\mathbb{T}_{j}^{(i)}\\land k\\notin\\mathbb{T}_{\\neg j}^{(i)}\\land y_{i}=j\\right]}{\\alpha_{k}}}\n$$\n \nFinally, we deﬁne the  coverage    $\\xi_{k}$   of a cue as the proportion of applicable cases over the total number of data points:    $\\xi_{k}~=~\\alpha_{k}/n$  . In these terms, the productivity of a cue measures the ben- eﬁt of exploiting it, while coverage measures the "}
{"page": 3, "image_path": "doc_images/P19-1459_3.jpg", "ocr_text": "Productivity Coverage |\nTrain 0.65 0.66\nValidation 0.62 0.44\nTest 0.52 0.77\nAll 0.61 0.64 |\n\nTable 2: Productivity and coverage of using the pres-\nence of “not” in the warrant to predict the label in\nARCT. Across the whole dataset, if you pick the war-\nrant with “not” you will be right 61% of the time, which\ncovers 64% of all data points.\n\nstrength of the signal it provides. With m labels,\nif 7, > 1/m then the presence of a cue is going to\nbe useful for the task and a machine learner would\ndo well to make use of it.\n\nThe productivity and coverage of the strongest\nunigram cue we found (“not”) is given in Table\n2. It provides a particularly strong training sig-\nnal. While it is less productive in the test set, it\nis just one among many such cues. We found a\nrange of other unigrams, albeit with less overall\nproductivity, mostly being high frequency words\nsuch as “is,” “do,” and “are.” Bigrams that oc-\ncurred with not, such as “will not” and “cannot,”\nwere also found to be highly productive. These\nstatistics indicate the nature of the problem. In the\nnext section we demonstrate that our models are in\nfact exploiting these cues.\n\n5 Probing Experiments\n\nIf a model is exploiting distributional cues over the\nlabels, then if trained only on the warrants (W) it\nshould perform relatively well. The same can be\nsaid for removing either just the claim, leaving the\nreason and warrant (R, W), or removing the reason\n(C, W). The latter setups allow the models to addi-\ntionally consider cues in the reasons and claims,\nas well as cues holding over their combinations\nwith the warrants. Each of these setups breaks the\ntask since we no longer have an argument to match\nwith a warrant.\n\nExperimental results are given in Table 3. On\nwarrants alone (W) BERT achieves a maximum\n71% accuracy. That leaves only six percentage\npoints to account for its peak of 77%. We find\na gain of four percentage points for (R, W) over\n(W), and a gain of two for (C, W), accounting for\nthe missing six points. Based on this evidence our\nmajor finding is that the entirety of BERT’s perfor-\nmance can be accounted for in terms of exploiting\nspurious statistical cues.\n\nTest\n\nMean Median Max\nBERT 0.671 + 0.09 0.712 0.770\nBERT (W) 0.656 + 0.05 0.675 0.712\nBERT (R, W) 0.600 + 0.10 0.574 0.750\nBERT (C, W) 0.532 + 0.09 0.503 0.732\nBoV 0.564 + 0.02 0.569 0.595\nBoV (W) 0.567 + 0.02 0.572 0.606\nBoV (R, W) 0.554 + 0.02 0.557 0.579\nBoV (C, W) 0.545 + 0.02 0.544 0.589\nBiLSTM 0.552 + 0.02 0.552 0.592\nBiLSTM (W) 0.550 + 0.02 0.547 0.577\nBiLSTM (R, W) | 0.547 + 0.02 0.551 0.577\nBiLSTM (C, W) | 0.552 + 0.02 0.550 0.601\n\nTable 3: Results of probing experiments with BERT\nLarge, and the BoV and BiLSTM baselines. These re-\nsults indicate that BERT’s peak 77% performance can\nbe entirely accounted for by exploiting spurious cues.\nBy just considering warrants (W) we can get to 71%.\nAdding cues over reasons (R, W) and claims (C, W)\naccounts for the remaining six points.\n\n6 Adversarial Test Set\n\nThe major problem of statistical cues over labels\nin ARCT can be eliminated due the original de-\nsign of the dataset. Given that R \\ A > 7=C,\nwe can produce adversarial examples by negat-\ning the claim and inverting the label for each data\npoint (Figure 4). The adversarial examples are\nthen combined with the original data. This elim-\ninates the problem by mirroring the distributions\nof cues around both labels. The ARCT authors\nprovide a training set augmented in this way. The\nnegation of most claims in the validation and test\nsets already exist elsewhere in the dataset. The re-\nmaining claims were manually negated by a native\nEnglish speaker.\n\nWe tried two experimental setups. In the first,\nmodels trained and validated on the original data\nwere evaluated on the adversarial set. All results\nwere worse than random due to overfitting the cues\nin the original training set. In the second, mod\nels were trained from scratch on the adversarial\ntraining and validation sets, then evaluated on the\nadversarial test set. Results are given in Table 4.\nBERT’s peak performance has reduced to 53%,\nwith mean and median at 50%. We conclude from\nthese results that the adversarial dataset has suc\ncessfully eliminated the cues as expected, provid\ning a more robust evaluation of machine argument\ncomprehension. This result better apts with our\nintuitions about this task: with little to no under-\nstanding about the reality underlying these argu-\nments, good performance shouldn’t be feasible.\n\n4661\n", "vlm_text": "The table presents the values of two metrics, Productivity and Coverage, across three datasets: Train, Validation, and Test. Additionally, it provides the average values for these metrics across all datasets. The data in the table is as follows:\n\n- For the Train dataset: Productivity is 0.65 and Coverage is 0.66.\n- For the Validation dataset: Productivity is 0.62 and Coverage is 0.44.\n- For the Test dataset: Productivity is 0.52 and Coverage is 0.77.\n\nThe overall averages for all datasets combined are:\n- Productivity: 0.61\n- Coverage: 0.64\nTable 2: Productivity and coverage of using the pres- ence of “not” in the warrant to predict the label in ARCT. Across the whole dataset, if you pick the war- rant with “not” you will be right  $61\\%$   of the time, which covers    $64\\%$   of all data points. \nstrength of the signal it provides. With    $m$   labels, if    $\\pi_{k}>1/m$   then the presence of a cue is going to be useful for the task and a machine learner would do well to make use of it. \nThe productivity and coverage of the strongest unigram cue we found (“not”) is given in Table 2. It provides a particularly strong training sig- nal. While it is less productive in the test set, it is just one among many such cues. We found a range of other unigrams, albeit with less overall productivity, mostly being high frequency words such as “is,” “do,” and “are.” Bigrams that oc- curred with not, such as “will not” and “cannot,” were also found to be highly productive. These statistics indicate the nature of the problem. In the next section we demonstrate that our models are in fact exploiting these cues. \n5 Probing Experiments \nIf a model is exploiting distributional cues over the labels, then if trained only on the warrants (W) it should perform relatively well. The same can be said for removing either just the claim, leaving the reason and warrant (R, W), or removing the reason (C, W). The latter setups allow the models to addi- tionally consider cues in the reasons and claims, as well as cues holding over their combinations with the warrants. Each of these setups breaks the task since we no longer have an argument to match with a warrant. \nExperimental results are given in Table 3. On warrants alone (W) BERT achieves a maximum  $71\\%$   accuracy. That leaves only six percentage points to account for its peak of    $77\\%$  . We ﬁnd a gain of four percentage points for (R, W) over (W), and a gain of two for (C, W), accounting for the missing six points. Based on this evidence our major ﬁnding is that the entirety of BERT’s perfor- mance can be accounted for in terms of exploiting spurious statistical cues. \nThe table presents test performance metrics (Mean, Median, and Max) for different models and configurations. Here's a breakdown:\n\n- **BERT**\n  - Mean: 0.671 ± 0.09\n  - Median: 0.712\n  - Max: 0.770\n\n- **BERT (W)**\n  - Mean: 0.656 ± 0.05\n  - Median: 0.675\n  - Max: 0.712\n\n- **BERT (R, W)**\n  - Mean: 0.600 ± 0.10\n  - Median: 0.574\n  - Max: 0.750\n\n- **BERT (C, W)**\n  - Mean: 0.532 ± 0.09\n  - Median: 0.503\n  - Max: 0.732\n\n- **BoV**\n  - Mean: 0.564 ± 0.02\n  - Median: 0.569\n  - Max: 0.595\n\n- **BoV (W)**\n  - Mean: 0.567 ± 0.02\n  - Median: 0.572\n  - Max: 0.606\n\n- **BoV (R, W)**\n  - Mean: 0.554 ± 0.02\n  - Median: 0.557\n  - Max: 0.579\n\n- **BoV (C, W)**\n  - Mean: 0.545 ± 0.02\n  - Median: 0.544\n  - Max: 0.589\n\n- **BiLSTM**\n  - Mean: 0.552 ± 0.02\n  - Median: 0.552\n  - Max: 0.592\n\n- **BiLSTM (W)**\n  - Mean: 0.550 ± 0.02\n  - Median: 0.547\n  - Max: 0.577\n\n- **BiLSTM (R, W)**\n  - Mean: 0.547 ± 0.02\n  - Median: 0.551\n  - Max: 0.577\n\n- **BiLSTM (C, W)**\n  - Mean: 0.552 ± 0.02\n  - Median: 0.550\n  - Max: 0.601\n\nEach model's variation (W, R, C) likely indicates different settings or inputs used in the experiments.\nTable 3: Results of probing experiments with BERT Large, and the BoV and BiLSTM baselines. These re- sults indicate that BERT’s peak    $77\\%$   performance can be entirely accounted for by exploiting spurious cues. By just considering warrants (W) we can get to    $71\\%$  . Adding cues over reasons (R, W) and claims (C, W) accounts for the remaining six points. \n6 Adversarial Test Set \nThe major problem of statistical cues over labels in ARCT can be eliminated due the original de- sign of the dataset. Given that    $R\\,\\wedge\\,A\\;\\rightarrow\\;\\neg C$  , we can produce adversarial examples by negat- ing the claim and inverting the label for each data point (Figure 4). The adversarial examples are then combined with the original data. This elim- inates the problem by mirroring the distributions of cues around both labels. The ARCT authors provide a training set augmented in this way. The negation of most claims in the validation and test sets already exist elsewhere in the dataset. The re- maining claims were manually negated by a native English speaker. \nWe tried two experimental setups. In the ﬁrst, models trained and validated on the original data were evaluated on the adversarial set. All results were worse than random due to overﬁtting the cues in the original training set. In the second, mod- els were trained from scratch on the adversarial training and validation sets, then evaluated on the adversarial test set. Results are given in Table 4. BERT’s peak performance has reduced to    $53\\%$  , with mean and median at    $50\\%$  . We conclude from these results that the adversarial dataset has suc- cessfully eliminated the cues as expected, provid- ing a more robust evaluation of machine argument comprehension. This result better apts with our intuitions about this task: with little to no under- standing about the reality underlying these argu- ments, good performance shouldn’t be feasible. "}
{"page": 4, "image_path": "doc_images/P19-1459_4.jpg", "ocr_text": "Adversarial\n\nOriginal\nClaim Google is not a harmful monopoly\nReason People can choose not to use Google\nWarrant Other search engines do not redirect to Google\nAlternative | All other search engines redirect to Google\n\nGoogle is a harmful monopoly\nPeople can choose not to use Google\nAll other search engines redirect to Google\n\nOther search engines do not redirect to Google\n\nFigure 4: Original and adversarial data points. The claim is negated and the warrants are swapped. The assignment\nof labels to W and A are kept the same. By including both, the distribution of linguistic artifacts in the warrants\nare thereby mirrored around the labels, eliminating the major source of spurious statistical cues in ARCT.\n\n7 Related Work\n\nThe most successful previous work on ARCT\n(Choi and Lee, 2018; Zhao et al., 2018; Niven and\nKao, 2018) involved transfer learning from Natu-\nral Language Inference (NLI) datasets (Bowman\net al., 2015; Williams et al., 2017), and utilized\neffective NLI models such as ESIM (Chen et al.,\n2016) and InferSent (Conneau et al., 2017). More\nrecently, Botschen et al. (2018) added FrameNet\nknowledge with modest performance gains. These\nmodels should be evaluated on our adversarial\ndataset. In particular it will be interesting if\nBotschen et al.’s model stands out due to the in-\nclusion of some of the required world knowledge.\nThere is much recent work focusing on statis-\ntical cues in datasets in vision (Jo and Bengio,\n2017) and NLP (Sanchez et al., 2018; McCoy\net al., 2019; Gururangan et al., 2018; Glockner\net al., 2018; Poliak et al., 2018; Rajpurkar et al.,\n2018; Jia and Liang, 2017). Similar to our exper-\niment with warrants, Poliak et al. (2018) classi-\nfied NLI data based on the hypothesis only. A\nsimilar experiment to our probing task was per-\nformed by Niven and Kao (2018), but only with\nreasons and warrants. They found that indepen-\ndent warrant classification with shared parameters\nprovides some regularization against warrant-label\ncues (Niven and Kao, 2018). However, this does\nnot solve the problem since the presence of a cue\nis enough to increase the logits for either warrant.\nThe original ARCT data comes with a train-\ning set created in the same way as our adversarial\ndataset. Habernal et al. (2018a) reported experi-\nments using this training data that led to random\naccuracy. They suggested it could be that high\nsimilarity between the data points made the prob-\nlem too difficult for the simple models they imple-\nmented. Our work indicates the necessity of ap-\nplying this transformation to the entire dataset in\norder to obtain a more robust evaluation by elimi-\nnating spurious statistical cues over the labels.\n\nTest\nMean Median Max\nBERT 0.504 + 0.01 0.505 0.533\nBERT (W) 0.501 + 0.00 0.501 0.502\nBERT (R, W) | 0.500 + 0.00 0.500 0.502\nBERT (C, W) | 0.501 + 0.01 0.500 0.518\n\nTable 4: Results for BERT Large on the adversarial test\nset with adversarial training and validation sets.\n\n8 Conclusion\n\nARCT provides a fortuitous opportunity to see\nhow stark the problem of exploiting spurious\nstatistics can be. Due to our ability to eliminate the\nmajor source of these cues, we were able to show\nthat BERT’s maximum performance fell from jus\nthree points below the average untrained human\nbaseline to essentially random. To answer our\nquestion in the introduction: BERT has learned\nnothing about argument comprehension.\nHowever, our investigations confirmed thai\nBERT is indeed a very strong learner. Analysis\nof easy to classify data points showed reliance on\na lower proportion of the strongest cue word than\nthe BoV and BiLSTM - i.e. BERT has learned\nwhen to ignore the presence of “not” and focus on\ndifferent cues. This indicates an ability to exploi\nmuch more subtle joint distributional information.\nAs our learners get stronger, controlling for spu-\nrious statistics becomes more important in order\nto have confidence in their apparent performance.\nTaken with a growing body of previous work, our\nresults indicate the need for further research into\nthe extent of this problem in NLP more generally.\nThe adversarial dataset should be adopted as the\nstandard in future work on ARCT. We hope that\nproviding a more robust evaluation will help to\nspur more productive research on this problem.\n\nAcknowledgments\n\nWe would like to thank Ivan Habernal, and the re-\nviewers, for their helpful comments.\n\n4662\n", "vlm_text": "The table presents a comparison between \"Original\" and \"Adversarial\" viewpoints concerning whether Google is a harmful monopoly.\n\n1. **Claim**:\n   - Original: Google is not a harmful monopoly.\n   - Adversarial: Google is a harmful monopoly.\n\n2. **Reason**:\n   - Both viewpoints state that people can choose not to use Google.\n\n3. **Warrant**:\n   - Original: Other search engines do not redirect to Google.\n   - Adversarial: All other search engines redirect to Google.\n\n4. **Alternative**:\n   - Original: All other search engines redirect to Google.\n   - Adversarial: Other search engines do not redirect to Google.\n7 Related Work \nThe most successful previous work on ARCT ( Choi and Lee ,  2018 ;  Zhao et al. ,  2018 ;  Niven and Kao ,  2018 ) involved transfer learning from Natu- ral Language Inference (NLI) datasets ( Bowman et al. ,  2015 ;  Williams et al. ,  2017 ), and utilized effective NLI models such as ESIM ( Chen et al. , 2016 ) and InferSent ( Conneau et al. ,  2017 ). More recently,  Botschen et al.  ( 2018 ) added FrameNet knowledge with modest performance gains. These models should be evaluated on our adversarial dataset. In particular it will be interesting if Botschen et al. ’s model stands out due to the in- clusion of some of the required world knowledge. \nThere is much recent work focusing on statis- tical cues in datasets in vision ( Jo and Bengio , 2017 ) and NLP ( Sanchez et al. ,  2018 ;  McCoy et al. ,  2019 ;  Gururangan et al. ,  2018 ;  Glockner et al. ,  2018 ;  Poliak et al. ,  2018 ;  Rajpurkar et al. , 2018 ;  Jia and Liang ,  2017 ). Similar to our exper- iment with warrants,  Poliak et al.  ( 2018 ) classi- ﬁed NLI data based on the hypothesis only. A similar experiment to our probing task was per- formed by  Niven and Kao  ( 2018 ), but only with reasons and warrants. They found that indepen- dent warrant classiﬁcation with shared parameters provides some regularization against warrant-label cues ( Niven and Kao ,  2018 ). However, this does not solve the problem since the presence of a cue is enough to increase the logits for either warrant. \nThe original ARCT data comes with a  train- ing  set created in the same way as our adversarial dataset.  Habernal et al.  ( 2018a ) reported experi- ments using this training data that led to random accuracy. They suggested it could be that high similarity between the data points made the prob- lem too difﬁcult for the simple models they imple- mented. Our work indicates the necessity of ap- plying this transformation to the entire dataset in order to obtain a more robust evaluation by elimi- nating spurious statistical cues over the labels. \nThe table presents a comparison of the performance metrics for different BERT models, namely \"BERT\", \"BERT (W)\", \"BERT (R, W)\", and \"BERT (C, W)\", based on their test performance.\n\nThe metrics displayed in the table are:\n- Mean: The average performance of each model. \n  - BERT: 0.504 ± 0.01\n  - BERT (W): 0.501 ± 0.00\n  - BERT (R, W): 0.500 ± 0.00\n  - BERT (C, W): 0.501 ± 0.01\n\n- Median: The median performance score for each model during testing.\n  - BERT: 0.505\n  - BERT (W): 0.501\n  - BERT (R, W): 0.500\n  - BERT (C, W): 0.500\n\n- Max: The maximum performance score each model achieved during testing.\n  - BERT: 0.533\n  - BERT (W): 0.502\n  - BERT (R, W): 0.502\n  - BERT (C, W): 0.518\n\nThe values in bold indicate the highest performance among the metrics compared.\nTable 4: Results for BERT Large on the adversarial test set with adversarial training and validation sets. \n8 Conclusion \nARCT provides a fortuitous opportunity to see how stark the problem of exploiting spurious statistics can be. Due to our ability to eliminate the major source of these cues, we were able to show that BERT’s maximum performance fell from just three points below the average untrained human baseline to essentially random. To answer our question in the introduction: BERT has learned nothing about argument comprehension. \nHowever, our investigations conﬁrmed that BERT is indeed a very strong learner. Analysis of easy to classify data points showed reliance on a  lower  proportion of the strongest cue word than the BoV and BiLSTM - i.e. BERT has learned when to ignore the presence of “not” and focus on different cues. This indicates an ability to exploit much more subtle joint distributional information. As our learners get stronger, controlling for spu- rious statistics becomes more important in order to have conﬁdence in their apparent performance. Taken with a growing body of previous work, our results indicate the need for further research into the extent of this problem in NLP more generally. \nThe adversarial dataset should be adopted as the standard in future work on ARCT. We hope that providing a more robust evaluation will help to spur more productive research on this problem. \nAcknowledgments \nWe would like to thank Ivan Habernal, and the re- viewers, for their helpful comments. "}
{"page": 5, "image_path": "doc_images/P19-1459_5.jpg", "ocr_text": "References\n\nElizabeth Black and Anthony Hunter. 2012. A\nrelevance-theoretic framework for constructing and\ndeconstructing enthymemes. J. Log. Comput.,\n22:55-78.\n\nTeresa Botschen, Daniil Sorokin, and Iryna Gurevych.\n2018. Frame- and entity-based knowledge\nfor common-sense argumentative reasoning. In\nArgMining@ EMNLP.\n\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nCoRR, abs/1508.05326.\n\nQian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si Wei, and\nHui Jiang. 2016. Enhancing and combining sequen-\ntial and tree LSTM for natural language inference.\nCoRR, abs/1609.06038.\n\nHongSeok Choi and Hyunju Lee. 2018. Gist at\nsemeval-2018 task 12: A network transferring infer-\nence knowledge to argument reasoning comprehen-\nsion task. In Proceedings of The 12th International\nWorkshop on Semantic Evaluation, pages 773-777.\nAssociation for Computational Linguistics.\n\nAlexis Conneau, Douwe Kiela, Holger Schwenk,\nLoic Barrault, and Antoine Bordes. 2017.  Su-\npervised learning of universal sentence representa-\ntions from natural language inference data. CoRR,\nabs/1705.02364.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. CoRR, abs/1810.04805.\n\nMax Glockner, Vered Shwartz, and Yoav Goldberg.\n2018. Breaking NLI systems with sentences\nthat require simple lexical inferences. CoRR,\nabs/1805.02266.\n\nSuchin Gururangan, Swabha Swayamdipta, Omer\nLevy, Roy Schwartz, Samuel R. Bowman, and\nNoah A. Smith. 2018. Annotation artifacts in natu-\nral language inference data. CoRR, abs/1803.02324.\n\nIvan Habernal, Judith Eckle-Kohler, and Iryna\nGurevych. 2014. Argumentation mining on the web\nfrom information seeking perspective. In ArgNLP.\n\nIvan Habernal, Henning Wachsmuth, Iryna Gurevych,\nand Benno Stein. 2018a. The argument reasoning\ncomprehension task: Identification and reconstruc-\ntion of implicit warrants. In Proceedings of the 2018\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long Papers),\npages 1930-1940, New Orleans, Louisiana. Asso-\nciation for Computational Linguistics.\n\nIvan Habernal, Henning Wachsmuth, Iryna Gurevych,\nand Benno Stein. 2018b. Semeval-2018 task 12:\n\nThe argument reasoning comprehension task. In\nProceedings of The 12th International Workshop on\nSemantic Evaluation, pages 763-772. Association\nfor Computational Linguistics.\n\nSepp Hochreiter and Jiirgen Schmidhuber. 1997.\nLong short-term memory. Neural computation,\n9(8):1735-1780.\n\nRobin Jia and Percy Liang. 2017. Adversarial exam-\nples for evaluating reading comprehension systems.\nCoRR, abs/1707.07328.\n\nJason Jo and Yoshua Bengio. 2017. Measuring the ten-\ndency of cnns to learn surface statistical regularities.\nCoRR, abs/1711.11561.\n\nDiederik P. Kingma and Jimmy Ba. 2014. Adam:\nA method for stochastic optimization. CoRR,\nabs/1412.6980.\n\nMarco Lippi and Paolo Torroni. 2016. Argumentation\nmining: State of the art and emerging trends. ACM\nTrans. Internet Technol., 16(2):10:1-10:25.\n\nR. Thomas McCoy, Ellie Pavlick, and Tal Linzen.\n2019. Right for the wrong reasons: Diagnosing\nsyntactic heuristics in natural language inference.\nCoRR, abs/1902.01007.\n\nRaquel Mochales and Marie-Francine Moens. 2011.\nArgumentation mining. Artif. Intell. Law, 19(1):1-\n22.\n\nTimothy Niven and Hung-Yu Kao. 2018. NLITrans\nat SemEval-2018 task 12: Transfer of semantic\nknowledge for argument comprehension. In Pro-\nceedings of The 12th International Workshop on Se-\nmantic Evaluation, pages 1099-1103, New Orleans,\nLouisiana. Association for Computational Linguis-\ntics.\n\nJeffrey Pennington, Richard Socher, and Christo-\npher D. Manning. 2014. Glove: Global vectors for\nword representation. In Empirical Methods in Nat-\nural Language Processing (EMNLP), pages 1532—\n1543.\n\nAdam Poliak, Jason Naradowsky, Aparajita Haldar,\nRachel Rudinger, and Benjamin Van Durme. 2018.\nHypothesis only baselines in natural language infer-\nence. CoRR, abs/1805.01042.\n\nPranav Rajpurkar, Robin Jia, and Percy Liang. 2018.\nKnow what you don’t know: Unanswerable ques-\ntions for squad. CoRR, abs/1806.03822.\n\nIvan Sanchez, Jeff Mitchell, and Sebastian Riedel.\n2018. Behavior analysis of NLI models: Uncov-\nering the influence of three factors on robustness.\nIn Proceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long Papers), pages 1975-1985, New\nOrleans, Louisiana. Association for Computational\nLinguistics.\n\n4663\n", "vlm_text": "References \nElizabeth Black and Anthony Hunter. 2012. A relevance-theoretic framework for constructing and deconstructing enthymemes . J. Log. Comput. , 22:55–78. Teresa Botschen, Daniil Sorokin, and Iryna Gurevych. 2018. Frame- and entity-based knowledge for common-sense argumentative reasoning. In ArgMining@EMNLP . Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015.  A large anno- tated corpus for learning natural language inference . CoRR , abs/1508.05326. Qian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si Wei, and Hui Jiang. 2016.  Enhancing and combining sequen- tial and tree LSTM for natural language inference . CoRR , abs/1609.06038. HongSeok Choi and Hyunju Lee. 2018. Gist at semeval-2018 task 12: A network transferring infer- ence knowledge to argument reasoning comprehen- sion task . In  Proceedings of The 12th International Workshop on Semantic Evaluation , pages 773–777. Association for Computational Linguistics. Alexis Conneau, Douwe Kiela, Holger Schwenk, Lo¨ ıc Barrault, and Antoine Bordes. 2017. Su- pervised learning of universal sentence representa- tions from natural language inference data .  CoRR , abs/1705.02364. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. BERT: pre-training of deep bidirectional transformers for language under- standing .  CoRR , abs/1810.04805. Max Glockner, Vered Shwartz, and Yoav Goldberg. 2018. Breaking NLI systems with sentences that require simple lexical inferences . CoRR , abs/1805.02266. Suchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel R. Bowman, and Noah A. Smith. 2018.  Annotation artifacts in natu- ral language inference data .  CoRR , abs/1803.02324. Ivan Habernal, Judith Eckle-Kohler, and Iryna Gurevych. 2014. Argumentation mining on the web from information seeking perspective. In  ArgNLP . Ivan Habernal, Henning Wachsmuth, Iryna Gurevych, and Benno Stein. 2018a.  The argument reasoning comprehension task: Identiﬁcation and reconstruc- tion of implicit warrants . In  Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers) , pages 1930–1940, New Orleans, Louisiana. Asso- ciation for Computational Linguistics. Ivan Habernal, Henning Wachsmuth, Iryna Gurevych, and Benno Stein. 2018b. Semeval-2018 task 12: \nThe argument reasoning comprehension task . In Proceedings of The 12th International Workshop on Semantic Evaluation , pages 763–772. Association for Computational Linguistics. Sepp Hochreiter and J¨ urgen Schmidhuber. 1997. Long short-term memory. Neural computation , 9(8):1735–1780. Robin Jia and Percy Liang. 2017.  Adversarial exam- ples for evaluating reading comprehension systems . CoRR , abs/1707.07328. Jason Jo and Yoshua Bengio. 2017.  Measuring the ten- dency of cnns to learn surface statistical regularities . CoRR , abs/1711.11561. Diederik P. Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization . CoRR , abs/1412.6980.Marco Lippi and Paolo Torroni. 2016.  Argumentation mining: State of the art and emerging trends .  ACM Trans. Internet Technol. , 16(2):10:1–10:25. R. Thomas McCoy, Ellie Pavlick, and Tal Linzen. 2019. Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference . CoRR , abs/1902.01007. Raquel Mochales and Marie-Francine Moens. 2011. Argumentation mining .  Artif. Intell. Law , 19(1):1– 22. Timothy Niven and Hung-Yu Kao. 2018. NLITrans at SemEval-2018 task 12: Transfer of semantic knowledge for argument comprehension . In  Pro- ceedings of The 12th International Workshop on Se- mantic Evaluation , pages 1099–1103, New Orleans, Louisiana. Association for Computational Linguis- tics. Jeffrey Pennington, Richard Socher, and Christo- pher D. Manning. 2014.  Glove: Global vectors for word representation . In  Empirical Methods in Nat- ural Language Processing (EMNLP) , pages 1532– 1543. Adam Poliak, Jason Naradowsky, Aparajita Haldar, Rachel Rudinger, and Benjamin Van Durme. 2018. Hypothesis only baselines in natural language infer- ence .  CoRR , abs/1805.01042. Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018. Know what you don’t know: Unanswerable ques- tions for squad .  CoRR , abs/1806.03822. Ivan Sanchez, Jeff Mitchell, and Sebastian Riedel. 2018. Behavior analysis of NLI models: Uncov- ering the inﬂuence of three factors on robustness . In  Proceedings of the 2018 Conference of the North American Chapter of the Association for Computa- tional Linguistics: Human Language Technologies, Volume 1 (Long Papers) , pages 1975–1985, New Orleans, Louisiana. Association for Computational Linguistics. "}
{"page": 6, "image_path": "doc_images/P19-1459_6.jpg", "ocr_text": "Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,\nIlya Sutskever, and Ruslan Salakhutdinov. 2014.\nDropout: A simple way to prevent neural networks\nfrom overfitting. Journal of Machine Learning Re-\nsearch, 15:1929-1958.\n\nStephen E. Toulmin. 1958. The Uses of Argument.\nCambridge University Press.\n\nDouglas N. Walton. 2005. Informal logic: a hand-\nbook of critical argumentation. Cambridge Univer-\nsity Press.\n\nAdina Williams, Nikita Nangia, and Samuel R. Bow-\nman. 2017. A broad-coverage challenge corpus for\nsentence understanding through inference. CoRR,\nabs/1704.05426.\n\nMeigqian Zhao, Chunhua Liu, Lu Liu, Yan Zhao, and\nDong Yu. 2018. Blcu_nlp at semeval-2018 task 12:\nAn ensemble model for argument reasoning based\non hierarchical attention. In Proceedings of The\n12th International Workshop on Semantic Evalua-\ntion, pages 1104-1108. Association for Computa-\ntional Linguistics.\n\n4664\n", "vlm_text": "Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: A simple way to prevent neural networks from overﬁtting .  Journal of Machine Learning Re- search , 15:1929–1958. Stephen E. Toulmin. 1958. The Uses of Argument . Cambridge University Press. Douglas N. Walton. 2005. Informal logic: a hand- book of critical argumentation . Cambridge Univer- sity Press. Adina Williams, Nikita Nangia, and Samuel R. Bow- man. 2017.  A broad-coverage challenge corpus for sentence understanding through inference . CoRR , abs/1704.05426. Meiqian Zhao, Chunhua Liu, Lu Liu, Yan Zhao, and Dong Yu. 2018.  Blcu nlp at semeval-2018 task 12: An ensemble model for argument reasoning based on hierarchical attention . In  Proceedings of The 12th International Workshop on Semantic Evalua- tion , pages 1104–1108. Association for Computa- tional Linguistics. "}
