{"page": 0, "image_path": "doc_images/2312.09390v1_0.jpg", "ocr_text": "arXiv:2312.09390v1 [cs.CL] 14 Dec 2023\n\nWEAK-TO-STRONG GENERALIZATION: ELICITING\nSTRONG CAPABILITIES WITH WEAK SUPERVISION\n\nCollin Burns* Pavel Izmailov* Jan Hendrik Kirchner* Bowen Baker* Leo Gao*\nLeopold Aschenbrenner* Yining Chen* Adrien Ecoffet* | Manas Joglekar*\nJan Leike IlyaSutskever Jeff Wu*\n\nOpenAI\n\nABSTRACT\n\nWidely used alignment techniques, such as reinforcement learning from human\nfeedback (RLHF), rely on the ability of humans to supervise model behavior—for\nexample, to evaluate whether a model faithfully followed instructions or generated\nsafe outputs. However, future superhuman models will behave in complex ways\ntoo difficult for humans to reliably evaluate; humans will only be able to weakly\nsupervise superhuman models. We study an analogy to this problem: can weak\nmodel supervision elicit the full capabilities of a much stronger model? We test\nthis using a range of pretrained language models in the GPT-4 family on natural\nlanguage processing (NLP), chess, and reward modeling tasks. We find that when\nwe naively finetune strong pretrained models on labels generated by a weak model,\nthey consistently perform better than their weak supervisors, a phenomenon we\ncall weak-to-strong generalization. However, we are still far from recovering the\nfull capabilities of strong models with naive finetuning alone, suggesting that tech-\nniques like RLHF may scale poorly to superhuman models without further work.\nWe find that simple methods can often significantly improve weak-to-strong gen-\neralization: for example, when finetuning GPT-4 with a GPT-2-level supervisor\nand an auxiliary confidence loss, we can recover close to GPT-3.5-level perfor-\nmance on NLP tasks. Our results suggest that it is feasible to make empirical\nprogress today on a fundamental challenge of aligning superhuman models.\n\n1 INTRODUCTION\n\nWe mainly steer or align today’s models with reinforcement learning from human feedback (RLHF):\nwe reinforce behaviors that human evaluators rate highly and penalize behaviors that evaluators rate\npoorly (Christiano et al., 2017; Stiennon et al., 2020; Ouyang et al., 2022; Glaese et al., 2022; Bai\net al., 2022a). This procedure is very effective when human evaluators can tell if model behavior is\ngood or bad and is a core part of training modern language model assistants such as ChatGPT.\n\nHowever, superhuman models will be capable of complex and creative behaviors that humans can-\nnot fully understand. For example, if a superhuman assistant model generates a million lines of ex-\ntremely complicated code, humans will not be able to provide reliable supervision for key alignment-\nrelevant tasks, including: whether the code follows the user’s intentions, whether the assistant model\nanswers questions about the code honestly, whether the code is safe or dangerous to execute, and\nso on. As a result, if we finetune a superhuman model with human supervision on a reward mod-\neling (RM) or safety classification task, it is unclear how that model will generalize to complicated\nbehaviors that humans could not reliably supervise themselves.\n\nThis leads to a fundamental technical challenge of aligning superhuman models (superalignment):\nhow can weak supervisors control models much smarter than them? Despite the importance of\n\n*Primary authors. This was a joint project of the Superalignment Generalization team. Correspondence to\ngeneralization@openai.com. Code is available at github. com/openai/weak-to-strong.\n", "vlm_text": "WEAK-TO-STRONG GEN ERA LIZ ATION: ELICITINGS TRONG  C AP ABILITIES  W ITH  W EAK  S UPERVISION \nLeopold As chen brenner ∗ Yining Chen ∗ Adrien Ecoffet ∗ Manas Joglekar ∗ \nJan Leike Ilya Sutskever Jeff Wu ∗ OpenAI \n\nA BSTRACT \nWidely used alignment techniques, such as reinforcement learning from human feedback (RLHF), rely on the ability of humans to supervise model behavior—for example, to evaluate whether a model faithfully followed instructions or generated safe outputs. However, future superhuman models will behave in complex ways too difficult for humans to reliably evaluate; humans will only be able to  weakly supervise  superhuman models. We study an analogy to this problem: can weak model supervision elicit the full capabilities of a much stronger model? We test this using a range of pretrained language models in the GPT-4 family on natural language processing (NLP), chess, and reward modeling tasks. We find that when we naively finetune strong pretrained models on labels generated by a weak model, they consistently perform better than their weak supervisors, a phenomenon we call  weak-to-strong generalization . However, we are still far from recovering the full capabilities of strong models with naive finetuning alone, suggesting that tech- niques like RLHF may scale poorly to superhuman models without further work. We find that simple methods can often significantly improve weak-to-strong gen- era liz ation: for example, when finetuning GPT-4 with a GPT-2-level supervisor and an auxiliary confidence loss, we can recover close to GPT-3.5-level perfor- mance on NLP tasks. Our results suggest that it is feasible to make empirical progress today on a fundamental challenge of aligning superhuman models. \n1 I N TRO DUCT ION \nWe mainly steer or  align  today’s models with reinforcement learning from human feedback (RLHF): we reinforce behaviors that human evaluators rate highly and penalize behaviors that evaluators rate poorly ( Christiano et al. ,  2017 ;  Stiennon et al. ,  2020 ;  Ouyang et al. ,  2022 ;  Glaese et al. ,  2022 ;  Bai et al. ,  2022a ). This procedure is very effective when human evaluators can tell if model behavior is good or bad and is a core part of training modern language model assistants such as ChatGPT. \nHowever, superhuman models will be capable of complex and creative behaviors that humans can- not fully understand. For example, if a superhuman assistant model generates a million lines of ex- tremely complicated code, humans will not be able to provide reliable supervision for key alignment- relevant tasks, including: whether the code follows the user’s intentions, whether the assistant model answers questions about the code honestly, whether the code is safe or dangerous to execute, and so on. As a result, if we finetune a superhuman model with human supervision on a reward mod- eling (RM) or safety classification task, it is unclear how that model will generalize to complicated behaviors that humans could not reliably supervise themselves. \nThis leads to a fundamental technical challenge of aligning superhuman models (super alignment): how can weak supervisors control models much smarter than them? Despite the importance of "}
{"page": 1, "image_path": "doc_images/2312.09390v1_1.jpg", "ocr_text": "Traditional ML Superalignment Our Analogy\n\nHuman level\n\nSupervisor Student Supervisor Student Supervisor Student\n\nFigure 1: An illustration of our methodology. Traditional ML focuses on the setting where humans\nsupervise models that are weaker than humans. For the ultimate superalignment problem, humans\nwill have to supervise models much smarter than them. We study an analogous problem today:\nusing weak models to supervise strong models.\n\nthis problem, it is difficult to empirically study today. Most prior work on alignment has either\nconfronted this core challenge head-on—but been restricted to primarily theoretical frameworks and\ntoy problems (Irving et al., 2018; Christiano et al., 2018; Leike et al., 2018; Demski & Garrabrant,\n2019; Hubinger et al., 2019), or empirically studied humans supervising today’s models—without\naddressing the core challenges that may arise with superhuman models (Christiano et al., 2017; Wu\net al., 2021; Ouyang et al., 2022; Bowman et al., 2022; Saunders et al., 2022). In contrast, we would\nideally like to have a setup that captures core challenges of aligning future superhuman models while\nalso being able to make iterative empirical progress today.\n\nWe propose a simple setup for studying the problem of humans supervising superhuman models by\nconsidering an analogy: can we use weak models to supervise strong models? We can empirically\ntest this by finetuning large (strong) pretrained models on labels generated by small (weak) mod-\nels and observing how they generalize. Just like the problem of humans supervising superhuman\nmodels, our setup is an instance of what we call the weak-to-strong learning problem.\n\nWhy should weak-to-strong learning be possible? On the one hand, the strong model could simply\nlearn to imitate the weak supervisor, including its errors, since that is what we would naively train\nit to do. On the other hand, strong pretrained models should already have good representations of\nthe alignment-relevant tasks we care about. For example, if a model can generate complicated code,\nthen it should intuitively also know whether that code faithfully adheres to the user’s instructions.\nAs a result, for the purposes of alignment we do not need the weak supervisor to teach the strong\nmodel new capabilities; instead, we simply need the weak supervisor to elicit what the strong model\nalready knows. This gives us hope that the strong model can generalize beyond the weak supervision,\nsolving even hard problems for which the weak supervisor can only give incomplete or flawed\ntraining labels. We call this phenomenon weak-to-strong generalization.\n\nWe study our weak-to-strong learning setup (Section 3) by finetuning base (i.e. pretrained-only)\nlanguage models from the GPT-4 family (OpenAI, 2023),' spanning 7 orders of magnitude (OOMs)\nof pretraining compute, across three settings: a large set of popular natural language processing\n(NLP) benchmarks, chess puzzles, and our internal ChatGPT reward modeling dataset. Our main\nfindings include:\n\n'These models share the same general architecture and pretraining dataset as GPT-4. However, this model\nseries does not include the models known as GPT-2, GPT-3, and GPT-3.5.\n", "vlm_text": "The image presents a comparison of traditional machine learning (ML), superalignment, and an analogy involving robots. It is divided into three sections:\n\n1. **Traditional ML**: Depicts a human supervisor interacting with a small robot student. This implies that traditional ML involves human oversight and guidance for a machine-learning model.\n\n2. **Superalignment**: Shows a human supervisor interacting with a much larger robot student. This suggests a scenario where the machine exceeds human-level capabilities, necessitating advanced alignment techniques beyond traditional methods.\n\n3. **Our Analogy**: Illustrates a small robot supervisor interacting with a larger robot student. This symbolizes a future where non-human agents may supervise or teach other artificial intelligence systems.\n\nA dashed line labeled \"Human level\" indicates the line at or above which the capabilities or actions are considered on par with humans, underscoring the potential evolution of machine capabilities.\nFigure 1:  An illustration of our methodology.  Traditional ML focuses on the setting where humans supervise models that are weaker than humans. For the ultimate super alignment problem, humans will have to supervise models much smarter than them. We study an analogous problem today: using weak models to supervise strong models. \nthis problem, it is difficult to empirically study today. Most prior work on alignment has either confronted this core challenge head-on—but been restricted to primarily theoretical frameworks and toy problems ( Irving et al. ,  2018 ;  Christiano et al. ,  2018 ;  Leike et al. ,  2018 ;  Demski & Garrabrant , 2019 ;  Hubinger et al. ,  2019 ), or empirically studied humans supervising today’s models—without addressing the core challenges that may arise with superhuman models ( Christiano et al. ,  2017 ;  Wu et al. ,  2021 ;  Ouyang et al. ,  2022 ;  Bowman et al. ,  2022 ;  Saunders et al. ,  2022 ). In contrast, we would ideally like to have a setup that captures core challenges of aligning future superhuman models while also  being able to make iterative empirical progress today. \nWe propose a simple setup for studying the problem of humans supervising superhuman models by considering an analogy: can we use  weak models  to supervise  strong models ? We can empirically test this by finetuning large (strong) pretrained models on labels generated by small (weak) mod- els and observing how they generalize. Just like the problem of humans supervising superhuman models, our setup is an instance of what we call the  weak-to-strong learning  problem. \nWhy should weak-to-strong learning be possible? On the one hand, the strong model could simply learn to imitate the weak supervisor, including its errors, since that is what we would naively train it to do. On the other hand, strong pretrained models should already have good representations of the alignment-relevant tasks we care about. For example, if a model can generate complicated code, then it should intuitively also know whether that code faithfully adheres to the user’s instructions. As a result, for the purposes of alignment we do not need the weak supervisor to teach the strong model new capabilities; instead, we simply need the weak supervisor to elicit what the strong model already knows . This gives us hope that the strong model can generalize beyond the weak supervision, solving even hard problems for which the weak supervisor can only give incomplete or flawed training labels. We call this phenomenon  weak-to-strong generalization . \nWe study our weak-to-strong learning setup (Section  3 ) by finetuning base (i.e. pretrained-only) language models from the GPT-4 family ( OpenAI ,  2023 ),   spanning 7 orders of magnitude (OOMs) of pre training compute, across three settings: a large set of popular natural language processing (NLP) benchmarks, chess puzzles, and our internal ChatGPT reward modeling dataset. Our main findings include: "}
{"page": 2, "image_path": "doc_images/2312.09390v1_2.jpg", "ocr_text": "weak-to-strong\n\norf ea ce performance i ire Ama i phen\n100 P (naive) (best method) 7 (g.t. supervision)\nRepresentative Chess Puzzles ChatGPT\nNLP Task Reward Modeling\n90 80. 70\neS\nX= 80 60 65\n5)\ng\n5\n8 70 40. 60\ns\n60 20 55\n50 0 50\n\nFigure 2: Strong models trained with weak supervision generalize beyond their supervisor, and\nimproving weak-to-strong generalization is tractable. We show test accuracy on a representative\nNLP task (left), chess puzzles (middle) and the ChatGPT reward modeling task (right). We show the\nweak supervisor trained on ground truth labels (light grey) and the strong student trained with weak\nsupervision naively (green), with the best method in each setting (purple), or with ground truth\nsupervision (dark grey). For NLP and chess we supervise GPT-4 using GPT-2-level supervision,\nwhile for reward modeling we supervise a 3.5-level model using GPT-2-level supervision. The best\nmethod is the auxiliary confidence loss for the NLP task (Section 4.3.2), bootstrapping for Chess\npuzzles (Section 4.3.1), and unsupervised generative finetuning for reward modeling (Section 5.2.2;\ngenerative-finetuning is also used for the strong ceiling performance).\n\n1. Strong pretrained models naturally generalize beyond their weak supervisors. If we\nnaively finetune strong models with labels generated by weak models, they consistently\noutperform their weak supervisors (Section 4.2). For example, on NLP tasks, if we fine-\ntune GPT-4 with labels from a GPT-2-level model, we typically recover about half of the\nperformance gap between the two models.\n\n2. Naively finetuning on weak supervison is not enough. Despite positive weak-to-strong\ngeneralization, there still remains a substantial gap between strong models finetuned with\nweak supervision and strong models finetuned with ground truth supervision. Weak-to-\nstrong generalization is particularly poor for ChatGPT reward modeling. Collectively, our\nresults provide empirical evidence that naive RLHF will likely scale poorly to superhuman\nmodels without additional work.\n\n3. Improving weak-to-strong generalization is tractable. We find that we can improve per-\nformance by encouraging strong models to have confident predictions with an auxiliary\nloss, bootstrapping supervision with intermediate models, and improving model represen-\ntations with unsupervised finetuning. For example, when supervising GPT-4 with a GPT-2-\nlevel model on NLP tasks using the auxiliary confidence loss, we typically recover nearly\n80% of the performance gap between the weak and strong models.\n\nOur work has important limitations. None of our methods work consistently in all settings, and\nespecially in the RM setting we are still far from recovering the full performance gap between weak\nand strong models. Thus our methods serve more as proofs-of-concept that weak-to-strong gener-\nalization is tractable, rather than practical solutions we recommend deploying today. Furthermore,\nthere are still important disanalogies between our empirical setup and aligning superhuman models\nthat we did not address (Section 6); continuously refining our basic setup will be important for en-\nsuring that research today continues to make real progress toward aligning the superhuman models\nwe develop in the future.\n\nDespite the limitations of our work, we find our results to be highly encouraging. We show that sub-\nstantial weak-to-strong generalization is not only possible, but actually a widespread phenomenon.\nWe also show that with very simple methods, we can drastically improve the ability of weak super-\nvisors to elicit knowledge from strong models. With much more progress in this direction, we could\nget to the point where we can use weak supervisors to reliably elicit knowledge from much stronger\n", "vlm_text": "The image presents a bar chart with three panels, each illustrating test accuracy for different tasks using various supervision methods. The tasks are:\n\n1. **Representative NLP Task**: \n   - **Weak performance**: Light grey bar shows the lowest accuracy.\n   - **Weak-to-strong performance (naive)**: Green bar shows moderate accuracy.\n   - **Weak-to-strong performance (best method)**: Purple bar shows higher accuracy.\n   - **Strong ceiling performance (ground truth supervision)**: Dark grey bar shows the highest accuracy.\n\n2. **Chess Puzzles**:\n   - Similar color scheme as above, with overall lower accuracies than the NLP task. The best method (purple) shows improvement over naive methods (green), but less than the strong ceiling (dark grey).\n\n3. **ChatGPT Reward Modeling**:\n   - Again, similar color scheme, with accuracies lower than the NLP task but higher than chess puzzles.\n   - The best method slightly improves over naive methods but doesn't reach the strong ceiling performance.\n\nDifferent methods employed for improving performance include auxiliary confidence loss for NLP tasks, bootstrapping for chess puzzles, and unsupervised generative fine-tuning for reward modeling.\n1.  Strong pretrained models naturally generalize beyond their weak supervisors.  If we naively finetune strong models with labels generated by weak models, they consistently outperform their weak supervisors (Section  4.2 ). For example, on NLP tasks, if we fine- tune GPT-4 with labels from a GPT-2-level model, we typically recover about half of the performance gap between the two models. \n2.  Naively finetuning on weak supervison is not enough.  Despite positive weak-to-strong generalization, there still remains a substantial gap between strong models finetuned with weak supervision and strong models finetuned with ground truth supervision. Weak-to- strong generalization is particularly poor for ChatGPT reward modeling. Collectively, our results provide empirical evidence that naive RLHF will likely scale poorly to superhuman models without additional work. \n3.  Improving weak-to-strong generalization is tractable.  We find that we can improve per- formance by encouraging strong models to have confident predictions with an auxiliary loss, boots trapping supervision with intermediate models, and improving model represen- tations with unsupervised finetuning. For example, when supervising GPT-4 with a GPT-2- level model on NLP tasks using the auxiliary confidence loss, we typically recover nearly  $80\\%$   of the performance gap between the weak and strong models. \nOur work has important limitations. None of our methods work consistently in all settings, and especially in the RM setting we are still far from recovering the full performance gap between weak and strong models. Thus our methods serve more as proofs-of-concept that weak-to-strong gener- alization is tractable, rather than practical solutions we recommend deploying today. Furthermore, there are still important d is analogies between our empirical setup and aligning superhuman models that we did not address (Section  6 ); continuously refining our basic setup will be important for en- suring that research today continues to make real progress toward aligning the superhuman models we develop in the future. \nDespite the limitations of our work, we find our results to be highly encouraging. We show that sub- stantial weak-to-strong generalization is not only possible, but actually a widespread phenomenon. We also show that with very simple methods, we can drastically improve the ability of weak super- visors to elicit knowledge from strong models. With much more progress in this direction, we could get to the point where we can use weak supervisors to reliably elicit knowledge from much stronger models, at least for some key tasks that we care about. This may allow us to develop superhuman reward models or safety class if i ers, which we could in turn use to align superhuman models. "}
{"page": 3, "image_path": "doc_images/2312.09390v1_3.jpg", "ocr_text": "models, at least for some key tasks that we care about. This may allow us to develop superhuman\nreward models or safety classifiers, which we could in turn use to align superhuman models.\n\nAligning superhuman models is essential for making them safe; there is increasing recognition that\nfailing to align such powerful models has the potential to be catastrophic, making this one of the\nmost important unsolved technical problems in the world (CAIS). We think it is now more tractable\nthan ever to make rapid iterative empirical progress toward solving this problem.\n\n2 RELATED WORK\n\nWe study how we can leverage the generalization properties of deep neural networks to solve weak-\nto-strong learning. Our problem setting and methods are closely connected to many existing research\nareas.\n\nWeakly-supervised learning. Weak-to-strong learning is a special type of weakly supervised\nlearning—a setting in which models are trained using unreliable labels (Bach et al., 2017; Rat-\nner et al., 2017; Guo et al., 2018). There is also a rich literature on the related problem of learning\nfrom noisy labels (Song et al., 2022). Common methods include bootstrapping (Reed et al., 2014;\nHan et al., 2018; Li et al., 2020), noise-robust losses (Zhang & Sabuncu, 2018; Hendrycks et al.,\n2018; Ma et al., 2020), and noise modeling (Yi & Wu, 2019). Unlike most work on label noise, the\nerrors in our weak supervision are much harder to address than uniform label noise, instead having\n“instance-dependent” errors (Frénay & Verleysen, 2013). Semi-supervised learning, in which la-\nbels are only available for a subset of the data, is also closely related (Kingma et al., 2014; Laine &\nAila, 2016; Berthelot et al., 2019). We could also study our problem in a semi-supervised setting by\nhaving an “easy” subset of examples that weak supervisors provide reliable labels for and a subset\nof unlabeled “hard” examples that the weak supervisor can’t reliably label, a problem which we call\n“easy-to-hard generalization” (see Appendix C).\n\nStudent-teacher training. The framework of first training a teacher and then training a student on\nteacher’s pseudo-labels is widely used in semi-supervised learning (Laine & Aila, 2016; Tarvainen\n& Valpola, 2017; Xie et al., 2020), domain adaptation (French et al., 2017; Shu et al., 2018), and\nknowledge distillation (Hinton et al., 2015; Gou et al., 2021; Stanton et al., 2021; Beyer et al., 2022).\nIn contrast to most prior work, we focus on the setting where the student is much more capable than\nthe teacher.\n\nFurlanello et al. (2018) and Xie et al. (2020) also consider cases where the student is at least as\ncapable as the teacher. However in their settings the student is randomly initialized and has access\nto ground truth labels. Moreover, compared to most past work we are focused on qualitatively very\nweak supervision. For example, we are interested in huge leaps in generalization, similar to going\nfrom “3rd grade-level” supervisors to “12th grade-level” student models. Despite these differences\nwith past work, we expect many methods from semi-supervised learning and domain adaptation to\ntranslate to our setting. For example, we found that a type of confidence auxiliary loss similar to\npast work (Grandvalet & Bengio, 2004) improves weak-to-strong generalization in Section 4.3.\n\nRobustness of pretraining and finetuning. Many papers have shown that pretraining\non massive, diverse data leads to more robust representations that generalize better out-of-\ndistribution (Hendrycks et al., 2019; 2020b; Radford et al., 2021; Liu et al., 2022). Finetuning typ-\nically improves in-distribution generalization, but often performs poorly out-of-distribution, some-\ntimes even degrading performance relative to zero-shot prompting (Kumar et al., 2022; Wortsman\net al., 2022b; Awadalla et al., 2022). Recent approaches to mitigating this problem include weight\nensembling (Wortsman et al., 2022b;a), finetuning only a subset of layers (Kirichenko et al., 2023;\nLee et al., 2022a), or mitigating the distortion effects that finetuning has on pretrained features (Ku-\nmar et al., 2022). We did not find strong results in preliminary explorations of approaches similar to\nthese (Appendix B), but we expect that with more thorough explorations one may be able to attain\nmuch stronger results with these or other ideas from the robust finetuning literature.\n\nDebiasing. In weak-to-strong generalization, the weak labels contain a specific form of bias,\nwhich results from the weak models’ lack of capability. There is a substantial literature on learning\nfrom biased training data (Bellamy et al., 2018). However, most work focuses on known biases,\nfor example where we know that the models perform worse on minority groups. For known biases,\ncommon methods include Group Distributionally Robust Optimization (Sagawa et al., 2019), adver-\n", "vlm_text": "\nAligning superhuman models is essential for making them safe; there is increasing recognition that failing to align such powerful models has the potential to be catastrophic, making this one of the most important unsolved technical problems in the world ( CAIS ). We think it is now more tractable than ever to make rapid iterative empirical progress toward solving this problem. \n2 R ELATED  W ORK \nWe study how we can leverage the generalization properties of deep neural networks to solve weak- to-strong learning. Our problem setting and methods are closely connected to many existing research areas. \nWeakly-supervised learning. Weak-to-strong learning is a special type of weakly supervised learning—a setting in which models are trained using unreliable labels ( Bach et al. ,  2017 ;  Rat- ner et al. ,  2017 ;  Guo et al. ,  2018 ). There is also a rich literature on the related problem of learning from noisy labels ( Song et al. ,  2022 ). Common methods include boots trapping ( Reed et al. ,  2014 ; Han et al. ,  2018 ;  Li et al. ,  2020 ), noise-robust losses ( Zhang & Sabuncu ,  2018 ;  Hendrycks et al. , 2018 ;  Ma et al. ,  2020 ), and noise modeling ( Yi & Wu ,  2019 ). Unlike most work on label noise, the errors in our weak supervision are much harder to address than uniform label noise, instead having “instance-dependent” errors ( Fr´ enay & Verleysen ,  2013 ). Semi-supervised learning, in which la- bels are only available for a subset of the data, is also closely related ( Kingma et al. ,  2014 ;  Laine & Aila ,  2016 ;  Berthelot et al. ,  2019 ). We could also study our problem in a semi-supervised setting by having an “easy” subset of examples that weak supervisors provide reliable labels for and a subset of unlabeled “hard” examples that the weak supervisor can’t reliably label, a problem which we call “easy-to-hard generalization” (see Appendix  C ). \nStudent-teacher training. The framework of first training a teacher and then training a student on teacher’s pseudo-labels is widely used in semi-supervised learning ( Laine & Aila ,  2016 ;  Tarvainen & Valpola ,  2017 ;  Xie et al. ,  2020 ), domain adaptation ( French et al. ,  2017 ;  Shu et al. ,  2018 ), and knowledge distillation ( Hinton et al. ,  2015 ;  Gou et al. ,  2021 ;  Stanton et al. ,  2021 ;  Beyer et al. ,  2022 ). In contrast to most prior work, we focus on the setting where the student is much more capable than the teacher. \nFurlanello et al.  ( 2018 ) and  Xie et al.  ( 2020 ) also consider cases where the student is at least as capable as the teacher. However in their settings the student is randomly initialized and has access to ground truth labels. Moreover, compared to most past work we are focused on qualitatively  very weak supervision. For example, we are interested in huge leaps in generalization, similar to going from “3rd grade-level” supervisors to “12th grade-level” student models. Despite these differences with past work, we expect many methods from semi-supervised learning and domain adaptation to translate to our setting. For example, we found that a type of confidence auxiliary loss similar to past work ( Grandvalet & Bengio ,  2004 ) improves weak-to-strong generalization in Section  4.3 . \nRobustness of pre training and finetuning. Many papers have shown that pre training on massive, diverse data leads to more robust representations that generalize better out-of- distribution ( Hendrycks et al. ,  2019 ;  2020b ;  Radford et al. ,  2021 ;  Liu et al. ,  2022 ). Finetuning typ- ically improves in-distribution generalization, but often performs poorly out-of-distribution, some- times even degrading performance relative to zero-shot prompting ( Kumar et al. ,  2022 ;  Wortsman et al. ,  2022b ;  Awadalla et al. ,  2022 ). Recent approaches to mitigating this problem include weight ensembling ( Wortsman et al. ,  2022b ; a ), finetuning only a subset of layers ( Kirichenko et al. ,  2023 ; Lee et al. ,  2022a ), or mitigating the distortion effects that finetuning has on pretrained features ( Ku- mar et al. ,  2022 ). We did not find strong results in preliminary explorations of approaches similar to these (Appendix  B ), but we expect that with more thorough explorations one may be able to attain much stronger results with these or other ideas from the robust finetuning literature. \nDebiasing. In weak-to-strong generalization, the weak labels contain a specific form of bias, which results from the weak models’ lack of capability. There is a substantial literature on learning from biased training data ( Bellamy et al. ,  2018 ). However, most work focuses on  known  biases, for example where we know that the models perform worse on minority groups. For known biases, common methods include Group Distribution ally Robust Optimization ( Sagawa et al. ,  2019 ), adver- sarial training ( Zhang et al. ,  2018 ), and model editing ( Santurkar et al. ,  2021 ;  Meng et al. ,  2022 ). In contrast, our setting can be viewed as a particularly difficult debiasing problem where the bias is unknown. Some methods that automatically discover and mitigate biases include clustering ( Sohoni et al. ,  2020 ), loss variance reduction ( Khani et al. ,  2019 ), and auditing and re-training on high-loss group ( Kim et al. ,  2019 ;  Liu et al. ,  2021 ). "}
{"page": 4, "image_path": "doc_images/2312.09390v1_4.jpg", "ocr_text": "sarial training (Zhang et al., 2018), and model editing (Santurkar et al., 2021; Meng et al., 2022).\nIn contrast, our setting can be viewed as a particularly difficult debiasing problem where the bias is\nunknown. Some methods that automatically discover and mitigate biases include clustering (Sohoni\net al., 2020), loss variance reduction (Khani et al., 2019), and auditing and re-training on high-loss\ngroup (Kim et al., 2019; Liu et al., 2021).\n\nImitation and preference learning. The goal of alignment is to steer already-capable models\nto do what we want them to do. For example, the base GPT-4 model is good at generating text\nfollowing its pretraining distribution, but does not readily follow instructions. To align pretrained\nlanguage models today, we finetune them using imitation learning on human demonstrations (Bain\n& Sammut, 1995; Atkeson & Schaal, 1997) or by using methods such as reinforcement learning\nfrom human feedback (RLHF) (Christiano et al., 2017; Stiennon et al., 2020; Ouyang et al., 2022;\nGlaese et al., 2022; Bai et al., 2022a). Constitutional AI (Bai et al., 2022b; Lee et al., 2023) leverages\nAI feedback to align language models, but still uses an initial RLHF phase. However, both imitation\nlearning and preference learning assume high-quality human supervision, making it unclear if they\nwill work for superhuman models.\n\nScalable oversight. Scalable oversight techniques aim to improve the ability of humans to super-\nvise models. For example, humans may ask models to critique the outputs of other models (Irving\net al., 2018; Saunders et al., 2022) or use models to help decompose a problem into simpler sub-\nproblems (Leike et al., 2018; Christiano et al., 2018; Lightman et al., 2023). Scalable oversight\nmethods typically take advantage of special problem structure, like decomposability or the fact that\nevaluation is easier than generation. In contrast to improving human supervision, we focus on\ngeneralizing beyond human supervision such that models perform well even in settings we cannot\nreliably supervise. That said, our weak-to-strong learning setup can be used to compare scalable\noversight methods, generalization-based methods, and more. Our setup also resembles a proposal\nfor measuring progress on scalable oversight known as “sandwiching”, which uses weak and strong\nhumans (Cotra, 2021; Bowman, 2022).\n\nKnowledge elicitation and honesty. Christiano et al. (2022) introduced a theoretical problem\ncalled Eliciting Latent Knowledge (ELK), in which the goal is to elicit latent knowledge from a su-\nperhuman machine learning model even under worst case assumptions. For example, a special case\nof ELK is honesty (Evans et al., 2021), where the goal is for the models to report their true beliefs.\nWentworth (2020) hypothesizes a tendency for neural networks to develop “natural abstractions”\nthat are easier to elicit. Recent empirical work on ELK includes a benchmark for measurement\ntampering (Roger et al., 2023), methods for discovering latent knowledge (Burns et al., 2023), and\nstudies of honesty (Li et al., 2023; Pacchiardi et al., 2023). Our setting can be viewed as a general\nmethodology for empirically studying problems like ELK and honesty across a wide range of tasks.\n\n3. METHODOLOGY\n\nA core challenge of superalignment is that humans will need to supervise models much smarter than\nus. This is a special case of what we call the weak-to-strong learning problem: how can a weak\nsupervisor oversee a model much smarter than it? In this paper, we study a simple analogy, in which\nwe replace the weak human supervisor with a weak model supervisor.\n\nFor a given task of interest, consisting of a dataset and a performance metric, we:\n\n1. Create the weak supervisor. Throughout most of this work, we create weak supervisors\nby finetuning small pretrained models on ground truth labels.*> We call the performance\nof the weak supervisor the weak performance, and we generate weak labels by taking the\nweak model’s predictions on a held-out set of examples.\n\n2. Train a strong student model with weak supervision. We finetune a strong model with\nthe generated weak labels. We call this model the strong student model and its resulting\nperformance the weak-to-strong performance.\n\n?Like Evans et al. (2021), we define honesty to mean a model reporting what it believes to be true, in contrast\nto truthfulness which asks whether what a model reports is true.\n\n*In Appendix D and Appendix E we study other synthetic weak supervisors. Future work could test many\nmore sources of weak supervision, such as by having 3rd grader humans provide labels.\n", "vlm_text": "\nImitation and preference learning. The goal of alignment is to steer already-capable models to do what we want them to do. For example, the base GPT-4 model is good at generating text following its pre training distribution, but does not readily follow instructions. To align pretrained language models today, we finetune them using imitation learning on human demonstrations ( Bain & Sammut ,  1995 ;  Atkeson & Schaal ,  1997 ) or by using methods such as reinforcement learning from human feedback (RLHF) ( Christiano et al. ,  2017 ;  Stiennon et al. ,  2020 ;  Ouyang et al. ,  2022 ; Glaese et al. ,  2022 ;  Bai et al. ,  2022a ). Constitutional AI ( Bai et al. ,  2022b ;  Lee et al. ,  2023 ) leverages AI feedback to align language models, but still uses an initial RLHF phase. However, both imitation learning and preference learning assume high-quality human supervision, making it unclear if they will work for superhuman models. \nScalable oversight. Scalable oversight techniques aim to improve the ability of humans to super- vise models. For example, humans may ask models to critique the outputs of other models ( Irving et al. ,  2018 ;  Saunders et al. ,  2022 ) or use models to help decompose a problem into simpler sub- problems ( Leike et al. ,  2018 ;  Christiano et al. ,  2018 ;  Lightman et al. ,  2023 ). Scalable oversight methods typically take advantage of special problem structure, like de com pos ability or the fact that evaluation is easier than generation. In contrast to improving human supervision, we focus on generalizing beyond human supervision such that models perform well even in settings we cannot reliably supervise. That said, our weak-to-strong learning setup can be used to compare scalable oversight methods, generalization-based methods, and more. Our setup also resembles a proposal for measuring progress on scalable oversight known as “sandwiching”, which uses weak and strong humans ( Cotra ,  2021 ;  Bowman ,  2022 ). \nKnowledge eli citation and honesty. Christiano et al.  ( 2022 ) introduced a theoretical problem called Eliciting Latent Knowledge (ELK), in which the goal is to elicit latent knowledge from a su- perhuman machine learning model even under worst case assumptions. For example, a special case of ELK is honesty ( Evans et al. ,  2021 ), where the goal is for the models to report their true beliefs 2 . Wentworth  ( 2020 ) hypothesizes a tendency for neural networks to develop “natural abstractions” that are easier to elicit. Recent empirical work on ELK includes a benchmark for measurement tampering ( Roger et al. ,  2023 ), methods for discovering latent knowledge ( Burns et al. ,  2023 ), and studies of honesty ( Li et al. ,  2023 ;  Pacchiardi et al. ,  2023 ). Our setting can be viewed as a general methodology for empirically studying problems like ELK and honesty across a wide range of tasks. \n3 M ETHODOLOGY \nA core challenge of super alignment is that humans will need to supervise models much smarter than us. This is a special case of what we call the  weak-to-strong learning problem : how can a weak supervisor oversee a model much smarter than it? In this paper, we study a simple analogy, in which we replace the weak human supervisor with a weak model supervisor. \nFor a given task of interest, consisting of a dataset and a performance metric, we: \n Create the weak supervisor.  Throughout most of this work, we create weak supervisors by finetuning small pretrained models on ground truth labels.   We call the performance of the weak supervisor the  weak performance , and we generate  weak labels  by taking the weak model’s predictions on a held-out set of examples. \n2.  Train a strong student model with weak supervision.  We finetune a strong model with the generated weak labels. We call this model the  strong student model  and its resulting performance the  weak-to-strong performance . "}
{"page": 5, "image_path": "doc_images/2312.09390v1_5.jpg", "ocr_text": "3. Train a strong model with ground truth labels as a ceiling. Finally, for comparison, we\nfinetune a strong model with ground truth labels.* We call this model’s resulting perfor-\nmance the strong ceiling performance. Intuitively, this should correspond to “everything\nthe strong model knows,” i.e. the strong model applying its full capabilities to the task.\n\nFor more details on how we train each model, see Appendix A.\n\nTypically, weak-to-strong performance will be between weak performance and strong ceiling per-\nformance. We define the performance gap recovered (PGR) as a function of the above three\nperformances (weak, weak-to-strong, and strong ceiling) as shown in the illustration below.\n\nweak-to-strong — weak _\n\nPGR\n\nstrong ceiling — weak\n\nweak weak-to-strong strong ceiling\nperformance performance performance\n\nPGR measures the fraction of the performance gap (the difference in performance between the weak\nand strong ceiling models) that we can recover with weak supervision. If we achieve perfect weak-\nto-strong generalization, PGR is 1. If the weak-to-strong model does no better than the weak super-\nvisor, then PGR is 0.\n\nAdvantages. Our setup has a number of advantages, including:\n\n1. It can be studied with any pair of weak and strong models, making it easy to study scaling\nlaws and not requiring access to expensive state-of-the-art models. Moreover, it does not\nrequire working with humans, so feedback loops are fast.\n\n2. It can be studied for any task of interest, making it easy to empirically test across a wide\nrange of settings.\n\n3. Success will be practically useful even before we develop superhuman models: for ex-\nample, if we find ways to align GPT-4 with only weak human supervision or with only\nGPT-3-level supervision, that would make it more convenient to align models today.\n\nLimitations. Our setup still has important disanalogies to the ultimate problem of aligning super-\nhuman models. We view our setup as removing one of the main disanalogies in prior work, not as\nproviding a final, perfectly analogous setup. Two remaining disanalogies include:\n\n1. Imitation saliency. Future superhuman models will likely have salient representations\nof human behaviors, but our strong models may not have learned features relevant for\nimitating weak model predictions; simply imitating the weak supervisor may thus be an\neasier failure mode to avoid in our setting than it will be in the future. More generally, the\ntypes of errors weak models make today may be different from the types of errors humans\nwill make when attempting to supervise superhuman models.\n\n2. Pretraining leakage. Our pretraining data implicitly contains supervision from humans.\nIt may thus be artificially easy to elicit strong models’ capabilities in our setting, since they\nwere directly pretrained to observe strong (human-level) performance. Superhuman-level\nperformance may not be directly observed in the same way—superhuman knowledge might\nbe more latent, e.g. because it was learned from self-supervised learning—and thus might\nbe harder to elicit from superhuman models in the future.\n\nFor tasks solved by superhuman models that humans cannot evaluate, we will not have access to ground\ntruth labels. However, we allow access to ground truth labels in our experimental setting today for scientific\nand evaluation purposes. Note that we evaluated weak-to-strong performance against ground truth many times\nwhile iterating on methods; however, we held out our largest model (GPT-4) and about half of NLP tasks\nthroughout the project.\n", "vlm_text": "3.  Train a strong model with ground truth labels as a ceiling.  Finally, for comparison, we finetune a strong model with ground truth labels.   We call this model’s resulting perfor- mance the  strong ceiling performance . Intuitively, this should correspond to “everything the strong model knows,” i.e. the strong model applying its full capabilities to the task. \nFor more details on how we train each model, see Appendix  A . \nTypically, weak-to-strong performance will be between weak performance and strong ceiling per- formance. We define the  performance gap recovered (PGR)  as a function of the above three performances (weak, weak-to-strong, and strong ceiling) as shown in the illustration below. \nThe image shows a diagram explaining a formula for PGR (Performance Gain Ratio). The formula is as follows:\n\n\\[ \\text{PGR} = \\frac{\\text{weak-to-strong} - \\text{weak}}{\\text{strong ceiling} - \\text{weak}} \\]\n\nIn the visual representation, there is a number line segmented into three intervals: \"weak performance,\" \"weak-to-strong performance,\" and \"strong ceiling performance.\" \n\nTwo lines are above this number line:\n- A solid blue line representing the range from \"weak performance\" to \"weak-to-strong performance.\"\n- A dotted blue line that illustrates the entire range starting from \"weak performance\" and extending to the \"strong ceiling performance.\"\n\nThe solid line represents the performance range being measured, while the dotted line represents the complete possible range from \"weak\" to the \"strong ceiling\" performance. The formula calculates how much the actual performance gains (solid line) compare to the maximum possible gains (dotted line).\nPGR measures the fraction of the performance gap (the difference in performance between the weak and strong ceiling models) that we can recover with weak supervision. If we achieve perfect weak- to-strong generalization, PGR is 1. If the weak-to-strong model does no better than the weak super- visor, then PGR is 0. \nAdvantages. Our setup has a number of advantages, including: \n1. It can be studied with any pair of weak and strong models, making it easy to study scaling laws and not requiring access to expensive state-of-the-art models. Moreover, it does not require working with humans, so feedback loops are fast. 2. It can be studied for any task of interest, making it easy to empirically test across a wide range of settings. 3. Success will be practically useful even before we develop superhuman models: for ex- ample, if we find ways to align GPT-4 with only weak human supervision or with only GPT-3-level supervision, that would make it more convenient to align models today. \nLimitations. Our setup still has important d is analogies to the ultimate problem of aligning super- human models. We view our setup as removing one of the main d is analogies in prior work, not as providing a final, perfectly analogous setup. Two remaining d is analogies include: \n1.  Imitation saliency.  Future superhuman models will likely have salient representations of human behaviors, but our strong models may not have learned features relevant for imitating weak model predictions; simply imitating the weak supervisor may thus be an easier failure mode to avoid in our setting than it will be in the future. More generally, the types of errors weak models make today may be different from the types of errors humans will make when attempting to supervise superhuman models. \n2.  Pre training leakage.  Our pre training data implicitly contains supervision from humans. It may thus be artificially easy to elicit strong models’ capabilities in our setting, since they were directly pretrained to observe strong (human-level) performance. Superhuman-level performance may not be directly observed in the same way—superhuman knowledge might be more latent, e.g. because it was learned from self-supervised learning—and thus might be harder to elicit from superhuman models in the future. "}
{"page": 6, "image_path": "doc_images/2312.09390v1_6.jpg", "ocr_text": "More generally, we do not yet know how superhuman models will be built, but they could develop\nnew inductive biases that are qualitatively different from today’s models. We view iterating on our\nmethodology to produce even more analogous setups as a key priority for future work, as we discuss\nin more detail in Section 6.\n\n4 MAIN RESULTS\n\nIn this section, we report our main empirical results, including baselines and promising methods.\n\n4.1 TASKS\n\nPopular natural language processing benchmarks. We consider 22 popular NLP classification\ndatasets covering ethics, commonsense reasoning, natural language inference, sentiment analysis,\nand other domains. We convert all datasets to binary classification tasks and approximately balance\nthe classes. We produce soft labels from the weak model. See a full list of the datasets and their\nsources in Table 1.\n\nChess puzzles. We use the dataset originally introduced in Schwarzschild et al. (2021b), which\ncontains chess puzzles from the 1ichess.org website (Lichess Team, 2023). Each puzzle con-\nsists of a chess position, and a sequence of optimal moves to play to solve the puzzle. For our\nevaluation, we predict the first move played, which is the best move in the given chess position. We\nillustrate the data format in Appendix Figure 14. For weak labels, we sample from the weak model\nwith temperature 0. Note that unlike the other binary classification tasks we study in this paper, this\nis a generative task.\n\nChatGPT reward modeling. The standard approach to aligning models today is reinforcement\nlearning from human feedback (RLHF). A critical step of RLHF is to train a reward model (RM)\nto predict human preferences between model responses. Specifically, a reward model is trained\non a dataset consisting of dialogs between a human and an assistant model. For each query, the\nhumans compare multiple possible responses (completions) from the assistant, providing human\npreference data. Then, a reward model is trained to predict the results of pairwise comparisons\nbetween completions. Finally, the assistant model is trained by optimizing against the reward model\nwith reinforcement learning (RL). In our work, we do not study the RL step, and instead assume the\ngoal is to maximize reward model accuracy. For more details on reward models, see e.g. Ouyang\net al. (2022). We use a proprietary dataset used to train ChatGPT reward models.\n\nFor more details about our tasks and setup, see Appendix A.\n\n4.2 NAIVELY FINETUNING ON WEAK LABELS\n\nIn each of these 3 settings (NLP tasks, chess puzzles, and reward modeling) we evaluate how well\nstrong students generalize when naively finetuned on labels generated by weak supervisors. We\nstudy pretrained language models from the GPT-4 family (OpenAI, 2023), which allow us to study\nstudent-supervisor compute disparities of many orders of magnitude. We find that PGRs are al-\nmost universally positive—in virtually all settings that we studied, and across almost all student and\nsupervisor sizes, students outperform their supervisors (Figure 3).\n\nOn the popular NLP benchmarks, we find especially promising weak-to-strong generalization:\nstrong models trained with weak supervision can often generalize to a substantially higher perfor-\nmance than the weak model itself. Even with very weak supervisors and strong models with many\norders of magnitude more compute, we recover more than 20% of the performance gap. The PGR\nincreases both with weak supervisor size and with strong student size; for the largest students, the\nPGR is often above 50%.\n\nWe see more mixed results in the chess puzzle setting. In particular, when using the smallest weak\nmodels, the PGR is close to zero and the test accuracy curves appear flat. However, as the size of the\nweak supervisor increases, the PGR increases substantially; for small supervisor-student gaps, PGR\ncan be above 40%. Unlike in the NLP setting, where PGR improves with the strong student size,\nPGR decreases with the strong student size for a given weak supervisor on chess puzzles. The cor-\n", "vlm_text": "More generally, we do not yet know how superhuman models will be built, but they could develop new inductive biases that are qualitatively different from today’s models. We view iterating on our methodology to produce even more analogous setups as a key priority for future work, as we discuss in more detail in Section  6 . \n4 M AIN  R ESULTS \nIn this section, we report our main empirical results, including baselines and promising methods. \n4.1 T ASKS \nPopular natural language processing benchmarks. We consider 22 popular NLP classification datasets covering ethics, commonsense reasoning, natural language inference, sentiment analysis, and other domains. We convert all datasets to binary classification tasks and approximately balance the classes. We produce soft labels from the weak model. See a full list of the datasets and their sources in Table  1 . \nChess puzzles. We use the dataset originally introduced in  Schwarz s child et al.  ( 2021b ), which contains chess puzzles from the  lichess.org  website ( Lichess Team ,  2023 ). Each puzzle con- sists of a chess position, and a sequence of optimal moves to play to solve the puzzle. For our evaluation, we predict the first move played, which is the best move in the given chess position. We illustrate the data format in Appendix Figure  14 . For weak labels, we sample from the weak model with temperature 0. Note that unlike the other binary classification tasks we study in this paper, this is a generative task. \nChatGPT reward modeling. The standard approach to aligning models today is reinforcement learning from human feedback (RLHF). A critical step of RLHF is to train a reward model (RM) to predict human preferences between model responses. Specifically, a reward model is trained on a dataset consisting of dialogs between a human and an assistant model. For each query, the humans compare multiple possible responses (completions) from the assistant, providing human preference data. Then, a reward model is trained to predict the results of pairwise comparisons between completions. Finally, the assistant model is trained by optimizing against the reward model with reinforcement learning (RL). In our work, we do not study the RL step, and instead assume the goal is to maximize reward model accuracy. For more details on reward models, see e.g.  Ouyang et al.  ( 2022 ). We use a proprietary dataset used to train ChatGPT reward models. \nFor more details about our tasks and setup, see Appendix  A \n4.2 N AIVELY FINETUNING ON WEAK LABELS \nIn each of these 3 settings (NLP tasks, chess puzzles, and reward modeling) we evaluate how well strong students generalize when naively finetuned on labels generated by weak supervisors. We study pretrained language models from the GPT-4 family ( OpenAI ,  2023 ), which allow us to study student-supervisor compute disparities of many orders of magnitude. We find that PGRs are al- most universally positive—in virtually all settings that we studied, and across almost all student and supervisor sizes, students outperform their supervisors (Figure  3 ). \nOn the popular NLP benchmarks, we find especially promising weak-to-strong generalization: strong models trained with weak supervision can often generalize to a substantially higher perfor- mance than the weak model itself. Even with very weak supervisors and strong models with many orders of magnitude more compute, we recover more than  $20\\%$   of the performance gap. The PGR increases both with weak supervisor size and with strong student size; for the largest students, the PGR is often above  $50\\%$  . \nWe see more mixed results in the chess puzzle setting. In particular, when using the smallest weak models, the PGR is close to zero and the test accuracy curves appear flat. However, as the size of the weak supervisor increases, the PGR increases substantially; for small supervisor-student gaps, PGR can be above  $40\\%$  . Unlike in the NLP setting, where PGR improves with the strong student size, PGR  decreases  with the strong student size for a given weak supervisor on chess puzzles. The cor- "}
{"page": 7, "image_path": "doc_images/2312.09390v1_7.jpg", "ocr_text": "strong ceiling performance __ weak-to-strong performance\n(g.t. supervision) (weak supervision)  —°\n\n100,\n(@) 1007p tasks ()\n\n60.\n\nChess Puzzles ©) 727 ¢hatepT\n\n| Reward Modeling\n\n2\n8\n\n80 7\n\na\n\ns\n2\n&\n\nSs\n&\n\ntest accuracy (%)\n\ntest accuracy (%)\n\ntest accuracy (%)\neo\n2.8\n\ny\n8\no\n8\n°\n\n50. 0\n10# 10 104 1021 10* 108 1041021\n(@) 100 (e)_ 100\n\ney\ns\n\nCCC a\n\n(f) 100:\nNLP Tasks => |ChatGPT\n\nReward Modeling\n\nStee\n\n108 10¢ 10? 108 10* 10? 1 10* 10* 70? i\nstrong student compute strong student compute strong student compute\n(fraction of GPT4) (fraction of GPT4) (fraction of GPT4)\n\n80.\n\n2\ni}\n\nperformance gap recovered (%)\niS oars\n\nBio 8 8\nWp.\nperformance gap recovered (%)\nis oars\n\nBio 8 8\n\ns\n\n(vido Jo uonoesy)\naynduico 4osiuadns eam\n\n60.\n\na\n8\n2\ns\n3\n\n40-\n\n4\n\n20\n\n0.\n\nperformance gap recovered (%)\n\n-20\n\nFigure 3: Promising weak-to-strong generalization with naive finetuning on NLP tasks and\nchess, but poor generalization on the ChatGPT reward modeling task. (a,b,c) Test accuracy\nas a function of strong student size on (a) NLP tasks, (b) chess puzzles, and (c) the ChatGPT\nreward modeling task. Accuracy of strong students trained with ground truth in black, accuracy\nof strong students trained with weak supervision shown with colored lines (hue indicates size of\nweak supervisor). (d,e,f) Same as panels a,b,c but for performance gap recovered (see Section 3\nfor details). For NLP settings, we compute the median across tasks (see Figure 12 for full details).\nWe find decent weak-to-strong generalization and even positive PGR scaling on NLP tasks, decent\ngeneralization for small supervisor-student gaps but negative PGR scaling on chess puzzles, and\nboth poor generalization and scaling for ChatGPT reward modeling.\n\nresponding test accuracy curves appear concave, potentially exhibiting inverse scaling (McKenzie\net al., 2023) in strong student size.\n\nFinally, we find that weak-to-strong generalization is poor by default in the ChatGPT reward model\nsetting. We are usually only able to recover roughly 10% of the performance gap between the weak\nsupervisor and the strong student. Even for relatively small gaps in compute between the weak and\nstrong models, PGR almost never exceeds 20%.\n\nIn general, across all our settings, we observe weak-to-strong generalization: strong students consis-\ntently outperform their weak supervisors. It is not obvious why this should happen at all—especially\nfrom naive finetuning alone—and it gives us hope that weak-to-strong learning is a tractable prob-\nlem. At the same time, our results suggest that naively using weak, human-level supervision will be\ninsufficient to align strong, superhuman models; we will need qualitatively new techniques to solve\nsuperalignment.\n\n4.3 IMPROVING WEAK-TO-STRONG GENERALIZATION IS TRACTABLE\n\nWe now show that we can use simple methods to substantially improve weak-to-strong generaliza-\ntion. While none of the methods we test works universally, these methods are proofs-of-concept that\nacross many different tasks we can substantially improve generalization.\n\n4.3.1 BOOTSTRAPPING WITH INTERMEDIATE MODEL SIZES\n\nBootstrapping is a long-standing idea in alignment: instead of directly aligning very superhuman\nmodels, we could first align an only slightly superhuman model, use that to align an even smarter\nmodel, and so on (Christiano, 2019; 2018; Leike & Sutskever, 2023; Worley, 2021). Our setting\nallows us to empirically test this idea.\n", "vlm_text": "The image contains six line graphs showing the performance of strong and weak models on NLP tasks, chess puzzles, and ChatGPT reward modeling tasks.\n\nGraphs (a, b, c) show test accuracy as a function of strong student size, with:\n- (a) NLP Tasks\n- (b) Chess Puzzles\n- (c) ChatGPT Reward Modeling\n\nGraphs (d, e, f) depict performance gap recovered:\n- (d) NLP Tasks\n- (e) Chess Puzzles\n- (f) ChatGPT Reward Modeling\n\nKey observations from the graphs:\n- NLP tasks show good weak-to-strong generalization.\n- Chess puzzles display decent generalization for small gaps but negative scaling in performance gap recovery (PGR).\n- ChatGPT reward modeling demonstrates poor generalization and scaling.\n\nColored lines indicate the size of the weak supervisor, with hue representing the fraction of GPT4 used. Black lines indicate the accuracy of strong models trained with ground truth.\nresponding test accuracy curves appear concave, potentially exhibiting inverse scaling ( McKenzie et al. ,  2023 ) in strong student size. \nFinally, we find that weak-to-strong generalization is poor by default in the ChatGPT reward model setting. We are usually only able to recover roughly   $10\\%$   of the performance gap between the weak supervisor and the strong student. Even for relatively small gaps in compute between the weak and strong models, PGR almost never exceeds  $20\\%$  . \nIn general, across all our settings, we observe weak-to-strong generalization: strong students consis- tently outperform their weak supervisors. It is not obvious why this should happen at all—especially from naive finetuning alone—and it gives us hope that weak-to-strong learning is a tractable prob- lem. At the same time, our results suggest that naively using weak, human-level supervision will be insufficient to align strong, superhuman models; we will need qualitatively new techniques to solve super alignment. \n4.3IMPROVING WEAK-TO-STRONG GEN ERA LIZ ATION IS TRACTABLE\nWe now show that we can use simple methods to substantially improve weak-to-strong generaliza- tion. While none of the methods we test works universally, these methods are proofs-of-concept that across many different tasks we can substantially improve generalization. \n4.3.1 B OO T STRAPPING WITH INTERMEDIATE MODEL SIZES \nBoots trapping is a long-standing idea in alignment: instead of directly aligning very superhuman models, we could first align an only slightly superhuman model, use that to align an even smarter model, and so on ( Christiano ,  2019 ;  2018 ;  Leike & Sutskever ,  2023 ;  Worley ,  2021 ). Our setting allows us to empirically test this idea. "}
{"page": 8, "image_path": "doc_images/2312.09390v1_8.jpg", "ocr_text": "weak-to-strong performance ---- A with bootstrapping —a\n\n@) 100 (b)10\nchess bootstrapping ge\n> 80\n80 3 0.1\nSs g 60 i\n>\n360 2 3\n£ a 40 332\n3 g Sa\n8 S of\n3\" 8 33\n3 a]\n2 g 3 EI\n20 aoe s es\n5 107\na\nOOS oe oe 10° 10° 10* 10° 10? 107 1\nstrong student compute strong student compute\n(fraction of GPT4) (fraction of GPT4)\n\nFigure 4: Bootstrapping improves weak-to-strong generalization on chess puzzles. (a) Test\naccuracy as a function of strong student size. Accuracy of students trained with ground truth in\nblack, accuracy of students naively trained with weak supervision shown with dotted lines (hue\nindicates size of weak supervisor). Accuracies of students trained via bootstrapping shown with\ncolored squares (including both the final weak-to-strong performance and the performance of the\nintermediate models during bootstrapping). (b) Same as a with PGR. By taking multiple small steps\ninstead of one big step we see substantially improved generalization, especially for larger student\nmodels.\n\nSpecifically, we can construct a sequence of model sizes M, + M2 > ... + M, of increasing\nsizes. Then, we use the weak labels from M, to finetune M2, use Mz to generate new weak labels\nthat we can use to finetune the next model in the sequence, M3, and so on.\n\nWe evaluate bootstrapping in the chess puzzle setting. When we naively finetune on weak labels for\nchess (Section 4.2), we see high PGR when we cross small supervisor-student gaps, but low PGR\nfor larger gaps. As a result, in this setting it may help to take multiple small steps—steps where\nPGR should be high—instead of one big step.\n\nFor each round of bootstrapping, we run three iterations of weak-to-strong learning, i.e. we bootstrap\nthe weak supervision using two intermediate model sizes before finally finetuning the largest model\nin the sequence. We report the results (including all intermediate weak-to-strong models within\neach bootstrap) in Figure 4. Bootstrapping improves PGR compared to the baseline, especially for\nlarger student models. With the naive method, transfer accuracy curves flatten as the weak-strong\ngap grows larger; with bootstrapping, the accuracy continues to monotonically improve.\n\nWhile the results in the chess setting are promising, in preliminary experiments we observed only\nsmall improvements with bootstrapping on NLP tasks and no improvements in the RM setting.\nThis makes sense intuitively: unlike in the chess setting where naive PGR decreased with larger\nsupervisor-student gaps, naive PGR increased or was rougly constant for larger supervisor-student\ngaps in the NLP and reward modeling settings. Overall, these results suggest bootstrapping is a\nplausible avenue to investigate for improving weak-to-strong generalization and can be helpful in\nsome settings, but that naive bootstrapping alone will not be enough to align models much smarter\nthan their supervisors.\n\n4.3.2 AN AUXILIARY CONFIDENCE LOSS CAN DRAMATICALLY IMPROVE GENERALIZATION\nON NLP TASKS\n\nIn our baseline results (Section 4.2), we naively finetune the strong student on the labels provided by\nthe weak supervisor. Because we are directly training the strong student to imitate the weak super-\nvisor, it may also learn to imitate the errors of the supervisor (see Section 5.1 for more discussion).\nIntuitively, we want to avoid this failure mode and provide additional regularization towards what\nthe strong pretrained model already internally knows: we want the student to learn the intent of the\nsupervisor, but not to imitate its mistakes.\n", "vlm_text": "This image consists of two plots, labeled (a) and (b), showcasing the effects of bootstrapping on generalization from weak-to-strong supervision in chess puzzles.\n\n(a) The plot on the left displays the test accuracy (%) of different student models against their compute size, expressed as a fraction of GPT-4 compute. The performance of students trained with ground truth is shown in black, while those naively trained with weak supervision are shown in dotted lines, with colors indicating the size of the weak supervisor. The results of students trained via bootstrapping are indicated with colored squares, showing both the final weak-to-strong performance and the performance of intermediate models during bootstrapping. The bootstrapping approach appears to lead to improved test accuracies.\n\n(b) The plot on the right represents the percentage of performance gap recovered by models trained with bootstrapping, plotted against the strong student compute size (again expressed as a fraction of GPT-4). Here, improvement is significant, especially for larger student models, when small incremental steps are used, as opposed to a single large step, reflecting enhanced generalization capabilities via bootstrapping. Colors are used similarly to show the fraction of GPT-4 compute used by the weak model.\n\nOverall, the graphs suggest that bootstrapping effectively enhances the generalization performance of models trained on weaker resources when computed in terms of accurate chess move predictions.\nSpecifically, we can construct a sequence model sizes  $\\mathcal{M}_{1}\\rightarrow\\mathcal{M}_{2}\\rightarrow...\\rightarrow\\mathcal{M}_{n}$   of increasing sizes. Then, we use the weak labels from  M  to finetune  M use  M  to generate new weak labels that we can use to finetune the next model in the sequence, , and so on. \nWe evaluate boots trapping in the chess puzzle setting. When we naively finetune on weak labels for chess (Section  4.2 ), we see high PGR when we cross small supervisor-student gaps, but low PGR for larger gaps. As a result, in this setting it may help to take multiple small steps—steps where PGR should be high—instead of one big step. \nFor each round of boots trapping, we run three iterations of weak-to-strong learning, i.e. we bootstrap the weak supervision using two intermediate model sizes before finally finetuning the largest model in the sequence. We report the results (including all intermediate weak-to-strong models within each bootstrap) in Figure  4 . Boots trapping improves PGR compared to the baseline, especially for larger student models. With the naive method, transfer accuracy curves flatten as the weak-strong gap grows larger; with boots trapping, the accuracy continues to monotonically improve. \nWhile the results in the chess setting are promising, in preliminary experiments we observed only small improvements with boots trapping on NLP tasks and no improvements in the RM setting. This makes sense intuitively: unlike in the chess setting where naive PGR decreased with larger supervisor-student gaps, naive PGR increased or was rougly constant for larger supervisor-student gaps in the NLP and reward modeling settings. Overall, these results suggest boots trapping is a plausible avenue to investigate for improving weak-to-strong generalization and can be helpful in some settings, but that naive boots trapping alone will not be enough to align models much smarter than their supervisors. \n4.3.2 A N AUXILIARY CONFIDENCE LOSS CAN DRAMATICALLY IMPROVE GENERALIZATION ON  NLP  TASKS \nIn our baseline results (Section  4.2 ), we naively finetune the strong student on the labels provided by the weak supervisor. Because we are directly training the strong student to imitate the weak super- visor, it may also learn to imitate the errors of the supervisor (see Section  5.1  for more discussion). Intuitively, we want to avoid this failure mode and provide additional regular iz ation towards what the strong pretrained model already internally knows: we want the student to learn the intent of the supervisor, but not to imitate its mistakes. "}
{"page": 9, "image_path": "doc_images/2312.09390v1_9.jpg", "ocr_text": "weak-to-strong performance ---- with aux. loss —a\n100\n\n80\n\ns\n3\no\n\n=\n\nNLP with auxiliary loss\n\nRe}\n3\n\n60.\n\n2\nS\n\n40\n\nx\nS\n\n20\n\ntest accuracy (%)\n\na\ni=}\n\nperformance gap recovered (%)\n°\nER ©\n= (pldd youonoey) —\nayndwios JosiAjadns yeam\n\n©\nis)\nro)\n\n50. Iai ARAdie A ARM a Raid .\n\n10* 10° 10* 10? 1 10* 10% 10?\n\nstrong student compute strong student compute\n(fraction of GPT4) (fraction of GPT4)\n\nor “—\n1\n\nFigure 5: Substantially improved generalization on NLP datasets with a simple auxiliary loss.\n(a) Test accuracy as a function of strong student size. Accuracy of a student trained with ground\ntruth in black, accuracy of students naively trained with weak supervision shown with dotted lines.\nAccuracies of students trained with auxiliary confidence loss shown with colored triangles. Median\ncomputed across 22 NLP tasks (hue indicates size of weak supervisor), see Figure 6 for individual\ndatasets. (b) Same as a with PGR. The confidence loss can improve generalization drastically,\nespecially for large supervisor-student gaps.\n\nWe operationalize this intuition by adding an auxiliary confidence loss term to the standard cross\nentropy objective. This method is closely related to conditional entropy minimization (Grandvalet\n& Bengio, 2004) which is a prominent technique in semi-supervised learning. Specifically, we add\nan additional loss term which reinforces the strong model’s confidence in its own predictions—\neven when they disagree with the weak labels. We provide a detailed description of the method in\nAppendix A.4.\n\nIn Figure 5, we plot accuracy and PGR curves with this method on our NLP tasks. We find that\nwhile it performs slightly worse than the naive baseline for smaller strong students, it dramatically\nimproves generalization for large gaps in compute between weak and strong models. With the\nsmallest weak supervisor and largest strong student, the confidence loss increases median PGR from\nabout 25% to nearly 80%.\n\nIn addition, we also plot generalization curves for a representative subset of NLP datasets in Figure 6,\nas well as the full panel of datasets in Figure 12. There are some settings in which the confidence\nloss does not help much or degrades performance, e.g. when the gap between the weak supervisor\nand strong student is small or when the dataset features inverse scaling even with ground truth\nsupervision. But the confidence loss improves performance on most NLP datasets dramatically, and\nfor many datasets we get almost perfect generalization, recovering nearly all the performance of the\nstrong model, even when using the smallest weak supervisors.\n\nFinally, we find evidence consistent with our motivating intuition for the confidence loss (allowing\nthe strong student to confidently disagree with its weak supervisor): the auxiliary loss reduces the\nstrong student’s imitation of weak errors and mitigates weak label overfitting (see Section 5.1).\n\n5 UNDERSTANDING WEAK-TO-STRONG GENERALIZATION\n\nStrong methods will be essential for solving superalignment, but to trust those methods it is also\nimportant to understand when and why they work. A better understanding of weak-to-strong gener-\nalization could help us trust that generalization will continue working even in the future high-stakes\nsettings we care most about, and could help us develop better methods along the way. In this sec-\ntion, we study two phenomena relevant to weak-to-strong generalization: imitation of supervisor\nmistakes and salience of the tasks to the strong student model.\n\n10\n", "vlm_text": "The image presents two graphs (a and b) illustrating the generalization improvement on NLP datasets achieved by incorporating a simple auxiliary loss in training models. Here's a breakdown of the graphs:\n\n1. **Graph (a):**\n   - This graph displays test accuracy as a function of a strong student's compute capacity, represented as a fraction of GPT-4's compute.\n   - The x-axis shows the compute size of the strong student, ranging from \\(10^{-8}\\) to \\(1\\) (in fractions of GPT-4).\n   - The y-axis represents the test accuracy percentage, which ranges from 50% to 100%.\n   - Various colored triangles indicate the test accuracies of students trained with an auxiliary confidence loss, with different colors representing different sizes of the weak supervisor (also denoted as a fraction of GPT-4's compute).\n   - Dotted lines show the accuracy of students trained naively with weak supervision.\n   - A solid black line represents the accuracy of a student trained with ground truth data.\n   - The plot suggests that using an auxiliary loss significantly improves test accuracy, especially for larger supervisor-student compute gaps.\n\n2. **Graph (b):**\n   - This graph shows the performance gap recovered (%) as a function of strong student compute (again, in fractions of GPT-4).\n   - The x-axis and use of compute fractions mirror graph (a).\n   - The y-axis here measures the recovered performance gap percentage, ranging from -20% to 100%.\n   - The triangles (with auxiliary loss) and dotted lines (without auxiliary loss) depict how much of the performance gap has been recovered compared to the baseline.\n   - The graph highlights that the inclusion of an auxiliary confidence loss is particularly effective in recovering performance gaps, again mainly for scenarios with substantial supervisor-student compute disparities.\n\nOverall, the use of an auxiliary confidence loss can significantly enhance the generalization capability of NLP models by improving their test accuracy and recovering performance gaps, particularly in cases with considerable differences in the computational power of supervisor and student models.\nWe operational ize this intuition by adding an auxiliary confidence loss term to the standard cross entropy objective. This method is closely related to conditional entropy minimization ( Grandvalet & Bengio ,  2004 ) which is a prominent technique in semi-supervised learning. Specifically, we add an additional loss term which reinforces the strong model’s confidence in its own predictions— even when they disagree with the weak labels. We provide a detailed description of the method in Appendix  A.4 . \nIn Figure  5 , we plot accuracy and PGR curves with this method on our NLP tasks. We find that while it performs slightly worse than the naive baseline for smaller strong students, it dramatically improves generalization for large gaps in compute between weak and strong models. With the smallest weak supervisor and largest strong student, the confidence loss increases median PGR from about  $25\\%$   to nearly   $80\\%$  . \nIn addition, we also plot generalization curves for a representative subset of NLP datasets in Figure  6 , as well as the full panel of datasets in Figure  12 . There are some settings in which the confidence loss does not help much or degrades performance, e.g. when the gap between the weak supervisor and strong student is small or when the dataset features inverse scaling even with ground truth supervision. But the confidence loss improves performance on most NLP datasets dramatically, and for many datasets we get almost perfect generalization, recovering nearly all the performance of the strong model, even when using the smallest weak supervisors. \nFinally, we find evidence consistent with our motivating intuition for the confidence loss (allowing the strong student to confidently disagree with its weak supervisor): the auxiliary loss reduces the strong student’s imitation of weak errors and mitigates weak label over fitting (see Section  5.1 ). \n5UNDER STANDING WEAK-TO-STRONG GEN ERA LIZ ATION\nStrong methods will be essential for solving super alignment, but to trust those methods it is also important to understand  when  and  why  they work. A better understanding of weak-to-strong gener- alization could help us trust that generalization will continue working even in the future high-stakes settings we care most about, and could help us develop better methods along the way. In this sec- tion, we study two phenomena relevant to weak-to-strong generalization: imitation of supervisor mistakes and salience of the tasks to the strong student model. "}
{"page": 10, "image_path": "doc_images/2312.09390v1_10.jpg", "ocr_text": "strong performance weak-to-strong weak-to-strong\n(g.t. supervision) — performance = — with auxloss — —&\n(weak supervision) (weak supervision)\n\n[9]\n\n80.\n\n70.\n\n60.\n\n50.\n\n80.\n\ntest accuracy (%)\n= (pido jo uonoey) —\n\n8\nFS\n3\n8\n3\n]\nS\n3\n\n75. A fo! 10\n70.\n65.\n\n60.\n\n10° 10% 10? 1 108% 10* 10* 10?\nstrong student compute\n(fraction of GPT4)\n\nFigure 6: Simple auxiliary loss improves generalization across most datasets. Test accuracy as\na function of strong student compute for a representative sample of NLP tasks. See Table 1 for\ndataset details and Appendix Figure 12 for results on all 22 NLP tasks. Auxiliary loss is shown with\ntriangles, and the baseline with dotted lines. Weak supervisor model size shown in varying colors,\nwith ground truth supervision shown in black.\n\n5.1 UNDERSTANDING IMITATION\n\nWhen we train a strong model with weak supervision on some task, our hope is that the strong\nmodel will perform that desired task as well as possible, leveraging the latent capabilities it learned\nfrom pretraining to significantly outperform the weak supervisor. A salient way in which we could\nfail to achieve that desired generalization is if the strong model instead learns to imitate the weak\nsupervisor—predicting how the weak supervisor would have classified each example. In particular,\nif the weak labels contain systematic errors that are easy to learn, the strong model could learn to\nimitate those errors. This is also a concern raised in theoretical work on superalignment, which has\nargued that the human simulator failure mode could be important: naive human supervision might\nresult in superhuman models learning to imitate what a human would say, rather outputting its best\npredictions (Christiano et al., 2022).\n\n5.1.1 OVERFITTING TO WEAK SUPERVISION\n\nThe failure mode of imitating weak supervision is especially relevant to our naive baseline in Sec-\ntion 4.2, which directly trains the student to imitate the supervisor. In the case of infinite training\ndata, naively fitting the weak labels should result in perfect imitation, and a PGR of zero. In prac-\ntice, we train on finite data for a small number of epochs. Unlike typical ML settings, however, we\ncould expect to observe overfitting even when training for less than a single epoch: the strong model\nmight overfit to the weak supervisor labels and its errors, degrading ground truth test accuracy over\ntraining even without classic overfitting to any specific training examples.\n\nEmpirically, we see that the strong student indeed appears to overfit to the weak supervisor’s errors.\nIn Figure 7(a) we show ground truth test accuracy curves over the course of training for the ChatGPT\nRM task, and in Figure 7(b) and (c) we compare the best? and final ground truth test accuracies\n(median across all weak-strong model pairs). We find overfitting for large weak-strong gaps. For\nsmall weak-strong gaps, weak-to-strong performance typically monotonically increases over the\ncourse of training. For larger gaps, weak-to-strong performance often increases initially, but then\nstarts dropping well before a single epoch has elapsed. Ground truth early stopping, which “cheats”\n\n5Note that our best test accuracies may slightly overstate accuracy, due to noisy evaluations.\n\n11\n", "vlm_text": "The image consists of multiple graphs showing the test accuracy (%) against strong student compute (fraction of GPT-4) for various NLP tasks. Each subplot represents a different task, numbered [1], [9], [6], [8], [4], [12], [10], and [11]. \n\nKey elements:\n\n- **Lines and Symbols**: \n  - Triangles represent performance with auxiliary loss.\n  - Dotted lines show baseline performance without auxiliary loss.\n  \n- **Colors**: \n  - Different colors depict varying weak supervisor model sizes, transitioning from dark blue to yellow as the size increases.\n\n- **Black Lines**: \n  - Indicate strong performance with ground truth supervision.\n\nOverall, the figure demonstrates that using a simple auxiliary loss generally improves generalization across most datasets tested by increasing the test accuracy compared to the baseline.\n5.1 U NDER STANDING IMITATION \nWhen we train a strong model with weak supervision on some task, our hope is that the strong model will perform that desired task as well as possible, leveraging the latent capabilities it learned from pre training to significantly outperform the weak supervisor. A salient way in which we could fail to achieve that desired generalization is if the strong model instead learns to imitate the weak supervisor—predicting how the weak supervisor would have classified each example. In particular, if the weak labels contain systematic errors that are easy to learn, the strong model could learn to imitate those errors. This is also a concern raised in theoretical work on super alignment, which has argued that the  human simulator  failure mode could be important: naive human supervision might result in superhuman models learning to imitate what a human would say, rather outputting its best predictions ( Christiano et al. ,  2022 ). \n5.1.1OVERFITTING TO WEAK SUPERVISION\nThe failure mode of imitating weak supervision is especially relevant to our naive baseline in Sec- tion  4.2 , which directly trains the student to imitate the supervisor. In the case of infinite training data, naively fitting the weak labels should result in perfect imitation, and a PGR of zero. In prac- tice, we train on finite data for a small number of epochs. Unlike typical ML settings, however, we could expect to observe over fitting even when training for less than a single epoch: the strong model might overfit to the weak supervisor labels and its errors, degrading ground truth test accuracy over training even without classic over fitting to any specific training examples. \nEmpirically, we see that the strong student indeed appears to overfit to the weak supervisor’s errors. In Figure  7 (a) we show ground truth test accuracy curves over the course of training for the ChatGPT RM task, and in Figure  7 (b) and (c) we compare the best 5   and final ground truth test accuracies (median across all weak-strong model pairs). We find over fitting for large weak-strong gaps. For small weak-strong gaps, weak-to-strong performance typically monotonically increases over the course of training. For larger gaps, weak-to-strong performance often increases initially, but then starts dropping well before a single epoch has elapsed. Ground truth early stopping, which “cheats” "}
{"page": 11, "image_path": "doc_images/2312.09390v1_11.jpg", "ocr_text": "weak-to-strong\n\nperformance &arly stop—se\n\n(a) ©) 5. © 20\nChatGPT 5\nReward Modelin &\n65 >, 70. 9 oz §\nws ae 8\n\nSs xB S68 Be 320\nE64. oa = ae 8\ng 32 3 sgt\n§ 8& & 66. s3 s\n3° S83 3 ag &\n8 62 8 os Q\n\nFo2 8 8 3” 38 510\n3 3 es 8\n* 33 2 3\n\ns i=\n\n1B we -<\n61 NN ae @ Ey\n\n60. o\n\n0.2 0.4 0.6 0. 108 10¢ = =104 = 10 1 &, %,\nprogress (fraction of epoch) Os,\n“ty\n\nFigure 7: Strong models overfit to the weak labels. In all figures, we show data for the ChatGPT\nReward Modeling task. (a) Weak-to-strong performance over the course of training. Hues indicate\nthe student-supervisor gap. (b) Best weak-to-strong performance during training (stars) and weak-\nto-strong performance at the end of training (dashed). Weak performance in black. Hue indicates\nthe size of the weak supervisor. (c) Median best and final performance gap recovered (PGR) ag-\ngregated across all supervisor-student pairs. We see overfitting to weak labels for large weak-strong\ngaps, even within one epoch. In these cases, the best test accuracy achieved over training can be\nsubstantially better than the test accuracy at the end of training. See Figure 13 for the corresponding\nanalysis of a representative subset of NLP tasks.\n\nby evaluating against ground truth and stopping at an optimal step with respect to ground truth test\nlabels, typically gives a PGR improvement of around 5 percentage points.\n\nWe see the same phenomenon for NLP tasks in Figure 13. In the NLP setting, we find that “cheating”\nearly stopping on ground truth gives a 15 percentage point boost in PGR over the model at the end\nof training, and a 10 percentage point boost in PGR compared to “non-cheating” early stopping with\nrespect to weak labels.\n\nUnfortunately, an early stopping criterion that uses ground truth labels does not constitute a valid\nmethod. Nevertheless, the results above suggest that imitating weak supervisor errors may be an\nimportant phenomenon in our setting.\n\nMoreover, these results suggest that better early stopping or regularization strategies may be able to\nsubstantially improve weak-to-strong generalization, by reducing overfitting to the weak labels and\ntheir errors. Indeed, we see in Figure 13 that the auxiliary confidence loss introduced in Section 4.3.2\nreduces overfitting to weak labels on NLP tasks substantially. For large weak-strong gaps, early\nstopping on ground truth (compared to early stopping on weak labels) gives a 15% PGR boost when\nusing the naive method, but only a roughly 5% PGR boost when using the confidence loss.\n\n5.1.2 STUDENT-SUPERVISOR AGREEMENT\n\nAnother way to measure imitation is to directly measure the agreement between the student and the\nsupervisor: the fraction of test inputs where the strong student makes the same prediction as the\nweak supervisor. Note that if agreement were 100%, then weak-to-strong accuracy would be equal\nto supervisor accuracy, and PGR would be 0.\n\nIn general, we notice that for our naive finetuning baseline, student-supervisor agreement is consis-\ntently high—often noticeably higher than weak supervisor accuracy. This indicates that the student\nis imitating some of the supervisor’s errors. These phenomena hold across all tasks (NLP tasks,\nchess, and reward modeling) and all model sizes, for the naive method.\n\nThe confidence loss in Section 4.3.2 reduces student-supervisor agreements significantly (Figure 8),\nprimarily by imitating supervisor mistakes less (Figure 8c). The loss encourages the strong student\nto make confident predictions, including when they contradict the weak supervisor. In a handful of\nthe settings where it is most successful, the confidence loss reduces student-supervisor agreement\n\n12\n", "vlm_text": "The image contains a series of graphs analyzing the performance of models in the ChatGPT Reward Modeling task, focusing on the effect of strong models overfitting to weak labels:\n\n1. **Graph (a):** Shows test accuracy (%) over training progress (fraction of epoch). Different hues represent the student-supervisor compute gap, indicating how varying this affects performance.\n\n2. **Graph (b):** Compares best weak-to-strong performance during training (stars) with performance at the end (dashed lines). It highlights how early stopping can lead to better performance than continuing training until completion. Hue indicates the size of the weak supervisor.\n\n3. **Graph (c):** Displays the performance gap recovered (%), contrasting early stopping with the final performance. It demonstrates that early stopping can recover a significant portion of the performance gap.\n\nThe overall theme indicates that strong models may overfit to weak labels, and early stopping could aid in achieving better generalization.\nby evaluating against ground truth and stopping at an optimal step with respect to ground truth test labels, typically gives a PGR improvement of around  5  percentage points. \nWe see the same phenomenon for NLP tasks in Figure  13 . In the NLP setting, we find that “cheating” early stopping on ground truth gives a  15  percentage point boost in PGR over the model at the end of training, and a  10  percentage point boost in PGR compared to “non-cheating” early stopping with respect to weak labels. \nUnfortunately, an early stopping criterion that uses ground truth labels does not constitute a valid method. Nevertheless, the results above suggest that imitating weak supervisor errors may be an important phenomenon in our setting. \nMoreover, these results suggest that better early stopping or regular iz ation strategies may be able to substantially improve weak-to-strong generalization, by reducing over fitting to the weak labels and their errors. Indeed, we see in Figure  13  that the auxiliary confidence loss introduced in Section  4.3.2 reduces over fitting to weak labels on NLP tasks substantially. For large weak-strong gaps, early stopping on ground truth (compared to early stopping on weak labels) gives a  $15\\%$   PGR boost when using the naive method, but only a roughly  $5\\%$   PGR boost when using the confidence loss. \n5.1.2 S TUDENT - SUPERVISOR AGREEMENT \nAnother way to measure imitation is to directly measure the agreement between the student and the supervisor: the fraction of test inputs where the strong student makes the same prediction as the weak supervisor. Note that if agreement were   $100\\%$  , then weak-to-strong accuracy would be equal to supervisor accuracy, and PGR would be 0. \nIn general, we notice that for our naive finetuning baseline, student-supervisor agreement is consis- tently high—often noticeably higher than weak supervisor accuracy. This indicates that the student is imitating some of the supervisor’s errors. These phenomena hold across all tasks (NLP tasks, chess, and reward modeling) and all model sizes, for the naive method. \nThe confidence loss in Section  4.3.2  reduces student-supervisor agreements significantly (Figure  8 ), primarily by imitating supervisor mistakes less (Figure  8 c). The loss encourages the strong student to make confident predictions, including when they contradict the weak supervisor. In a handful of the settings where it is most successful, the confidence loss reduces student-supervisor agreement "}
{"page": 12, "image_path": "doc_images/2312.09390v1_12.jpg", "ocr_text": "weak-to-strong performance __,\n(aux. loss)\n(c) 100:\n\nNLP Task weak-to-strong performance +e\n(a) 100 (b) 100:\n\nre\ni}\n\n80: 80-\n\nea\ni}\n\n60: 60:\n\ns\n3\n\n(vido Jo uonoesy)\naynduico Josimadns yeam\n\nstudent-supervisor agreement (%)\nstudent-supervisor agreement (%)\nstudent-supervisor agreement (%)\n\n60/ 2ll samples 4g) Supervisor correct 4p) Supervisor mistakes\n10° 10 10% 10? 1 108 108 10* 10? 1 10% 10° 10* 107 1\nstrong student compute strong student compute strong student compute\n(fraction of GPT4) (fraction of GPT4) (fraction of GPT4)\n\nFigure 8: Student-supervisor agreement decreases with larger student-supervisor gaps; the\nconfidence loss reduces imitation of supervisor mistakes. (a) Student-supervisor agreement as\na function of strong student size on NLP tasks, (b) a but only on samples where the supervisor is\ncorrect, (c) a but only on samples where the supervisor is mistaken. Dotted lines indicate naive\nfinetuning on weak labels, and triangles indicate results with the auxiliary confidence loss results\n(see Section 4.3). Hue of line indicates size of weak supervisor. For results on reward models, see\nFigure 16.\n\nbelow strong student test accuracy (weak-to-strong performance)—i.e., the resulting model is fitting\nthe ground truth concept better than it is fitting the weak labels it was trained with.\n\n5.1.3. INVERSE SCALING FOR IMITATING THE SUPERVISOR\n\nNext, we study student-supervisor agreement as a function strong model size (see Figure 8 and\nFigure 16). Surprisingly, we find inverse scaling (McKenzie et al., 2023): larger student models\nconsistently agree /ess with the errors of the supervisor than smaller student models, despite being\ntrained to imitate the supervisor, not using early stopping, and having larger capacity than smaller\nstudent models.\n\nThis trend is especially strong if we evaluate agreement only on datapoints where the supervisor is\nwrong (Figure 8c), and the trend persists if looking at cross entropy loss instead of accuracy.\n\nThese results suggest that pretrained models may have a hard time fitting errors of other (smaller)\npretrained models, at least in finetuning settings with relatively limited data. Stanton et al. (2021)\nand Furlanello et al. (2018) report a related observation in the context of knowledge distillation: it\nis surprisingly hard for models to fit the predictions of other models, even when they have sufficient\ncapacity to do so.\n\nOne natural hypothesis is that the nature of (especially naive) weak-to-strong generalization depends\nheavily on the error structure of the weak supervisors and how easy those errors are to imitate. In\nAppendix E, we show initial experiments that test how different types of weak supervision errors\nimpact what the strong student learns. Our results suggest that errors that are more difficult for the\nstudent to imitate result in stronger naive weak-to-strong generalization, but that even when they are\neasy to imitate, the confidence loss can help.\n\n5.2. SALIENCY IN THE STRONG MODEL REPRESENTATIONS\n\nOne intuition for when weak-to-strong generalization might be feasible is when the task or con-\ncept we want to elicit is internally “salient” to the strong model. In this section, we study several\nphenomena related to the saliency of the concepts we are trying to elicit from the student model.\n\n5.2.1 ELICITING STRONG MODEL KNOWLEDGE WITH PROMPTING\n\nOne possible reason for the high PGR we observe in Section 4 could be that eliciting what the\nstrong model knows is easy. In particular, it is possible that strong pretrained models can solve\nmany relevant tasks zero-shot with a simple prompt.\n\nIn Figure 9a, we consider 7 representative NLP tasks and compare finetuning, zero-shot prompting,\nand 5-shot prompting; for this initial experiment, we use ground truth labels rather than weak labels\n\n13\n", "vlm_text": "The image consists of three plots illustrating the relationship between student-supervisor agreement and computational effort across different scenarios in NLP tasks. Each plot shows student-supervisor agreement as a percentage on the y-axis, with strong student compute (expressed as a fraction of GPT-4) on the x-axis. The hue of the lines represents the size of the weak supervisor (also expressed as a fraction of GPT-4).\n\n1. **Plot (a):** This plot displays the student-supervisor agreement across all samples. It shows that agreement decreases as strong student compute increases, which suggests that as students (models) become stronger, they tend to agree less with supervisors.\n\n2. **Plot (b):** This plot focuses on the samples where the supervisor's initial answer was correct. The agreement levels are generally high and consistent, regardless of the strong student's compute, indicating that when the supervisor is correct, the strong student agrees quite well.\n\n3. **Plot (c):** This plot considers only the samples where the supervisor was initially mistaken. Here, the agreement percentage significantly decreases with increased compute, illustrating that stronger students are less likely to mimic supervisor mistakes.\n\nThe plots also show different styles of lines and symbols: dotted lines represent naive fine-tuning on weak labels, and triangles indicate results that incorporate an auxiliary confidence loss method. There is a noticeable difference in performance when using auxiliary loss, as indicated by the different lines and symbols in each plot.\n\nOverall, the visual data suggests that increasing the computational resources of the student model (strong student compute) generally results in decreased agreement with the supervisor, except in cases where the supervisor is already correct. The confidence loss method seems to help students avoid replicating supervisor errors.\nbelow strong student test accuracy (weak-to-strong performance)—i.e., the resulting model is fitting the ground truth concept  better  than it is fitting the weak labels it was trained with. \n5.1.3 I NVERSE SCALING FOR IMITATING THE SUPERVISOR \nNext, we study student-supervisor agreement as a function strong model size (see Figure  8  and Figure  16 ). Surprisingly, we find inverse scaling ( McKenzie et al. ,  2023 ): larger student models consistently agree  less  with the errors of the supervisor than smaller student models, despite being trained to imitate the supervisor, not using early stopping, and having larger capacity than smaller student models. \nThis trend is especially strong if we evaluate agreement only on datapoints where the supervisor is wrong (Figure  8 c), and the trend persists if looking at cross entropy loss instead of accuracy. \nThese results suggest that pretrained models may have a hard time fitting errors of other (smaller) pretrained models, at least in finetuning settings with relatively limited data.  Stanton et al.  ( 2021 ) and  Furlanello et al.  ( 2018 ) report a related observation in the context of knowledge distillation: it is surprisingly hard for models to fit the predictions of other models, even when they have sufficient capacity to do so. \nOne natural hypothesis is that the nature of (especially naive) weak-to-strong generalization depends heavily on the error structure of the weak supervisors and how easy those errors are to imitate. In Appendix  E , we show initial experiments that test how different types of weak supervision errors impact what the strong student learns. Our results suggest that errors that are more difficult for the student to imitate result in stronger naive weak-to-strong generalization, but that even when they are easy to imitate, the confidence loss can help. \n5.2 S ALIENCY IN THE STRONG MODEL REPRESENTATIONS \nOne intuition for when weak-to-strong generalization might be feasible is when the task or con- cept we want to elicit is internally “salient” to the strong model. In this section, we study several phenomena related to the saliency of the concepts we are trying to elicit from the student model. \n5.2.1 E LICITING STRONG MODEL KNOWLEDGE WITH PROMPTING \nOne possible reason for the high PGR we observe in Section  4  could be that eliciting what the strong model knows is easy. In particular, it is possible that strong pretrained models can solve many relevant tasks zero-shot with a simple prompt. \nIn Figure  9 a, we consider  7  representative NLP tasks and compare finetuning, zero-shot prompting, and 5-shot prompting; for this initial experiment, we use ground truth labels rather than weak labels "}
{"page": 13, "image_path": "doc_images/2312.09390v1_13.jpg", "ocr_text": "fewshot\nfinetune gt.  weaklabels--* _finetune aux. loss —a\n(n=5)\n\n100 ©) 100 100\n\nrey\n3\nrey\n3\nrey\n3\n\ney\ns\n2\ns\nes\ns\n\ns\n3\nss\n3\ns\n3\n\ntest accuracy (%)\n2\ns\ntest accuracy (%)\ntest accuracy (%)\nDD\n\na\ns\n\n~ (pLd9 Jo uonoey)\n\naynduioo 10simadns eam\n\n)\n\n50 50+ 50\n10° 10 10° 10? O11 10° 10 10° 10? O11 108 10% 10° 10? O11\nstrong student compute strong student compute strong student compute\n(fraction of GPT4) (fraction of GPT4) (fraction of GPT4)\n\nFigure 9: Few-shot prompting becomes competitive with finetuning for large models; weak-to-\nstrong learning is qualitatively similar in the prompting setting. (a) Average zero-shot (single\ndashed), 5-shot (double dashed) and finetuning (solid) accuracy with ground truth labels as a func-\ntion of strong student size. (b) Average 5-shot with weak labels (colored dashed) accuracy as a\nfunction of student model size. Hue of line indicates size of weak supervisor. Zero-shot and 5-shot\nsame as in panel a. (c) Average weak-to-strong performance for 5-shot prompting (dashed with\ncrosses), naive finetuning (dashed thin) and finetuning with the confidence loss (solid with triangle)\nas a function of student model compute. Results are averaged across 7 NLP tasks. Few-shot weak-\nto-strong performance becomes competitive with or outperforms finetuning for the largest strong\nstudents, though finetuning with the confidence loss does better.\n\nfor finetuning and 5-shot. For both the zero-shot and 5-shot baseline we use task-specific prompts\nsummarized in Table 2. We find that zero-shot and 5-shot test accuracy is poor for most model sizes\nbut, consistent with Brown et al. (2020), improves drastically for larger model sizes. In particular, for\nthe largest models, 5-shot prompting becomes competitive with finetuning on many tasks, indicating\nthat eliciting the task-relevant knowledge of these very large models is relatively straightforward.\n\nWe are also interested in weak-to-strong learning in the context of few-shot prompting. To study\nthis setting, we construct a few-shot prompt where the labels are provided by the weak supervisor.\nWe report the results in Figure 9b. Consistent with our findings in the finetuning setting, we get\nworse performance when we few-shot prompt with weak labels than we do few-shot prompting\nwith ground truth labels. This suggests that weak-to-strong learning is a nontrivial problem in the\nprompting setting as well.\n\nSimilar to the finetuning setting, few-shot weak-to-strong performance improves for stronger su-\npervisors. Compared to our weak-to-strong finetuning baseline (Figure 9c), weak-to-strong perfor-\nmance of few-shot prompting is poor for smaller student models, but becomes competitive or even\noutperforms finetuning for the largest strong students. However, weak-to-strong finetuning with the\nconfidence loss still generally outperforms weak-to-strong few-shot prompting.\n\nOverall, these results provide an important reference for our results on weak-to-strong generaliza-\ntion. They suggest that for the largest model sizes, the knowledge needed to solve many task can\nbe elicited fairly easily with prompting. However, our current setup may be more disanalogous for\nprompting than for finetuning; many of our NLP tasks may have been implicitly observed during\npretraining, which we conjecture benefits prompting more than finetuning. We discuss this potential\ndisanalogy much more in Section 6.1.\n\n5.2.2 GENERATIVE SUPERVISION IMPROVES RM WEAK-TO-STRONG GENERALIZATION\n\nIf salient representations of the desired task is useful for weak-to-strong generalization, then we may\nbe able to improve generalization by increasing the salience of the task to the strong model. One\nway to increase the salience of a task without needing ground truth labels is to perform unsupervised\nfinetuning with the language modeling objective on data relevant to that task (Dai & Le, 2015). For\nexample, by finetuning a language model in an unsupervised way on online reviews, sentiment\nbecomes saliently represented to models internally (Radford et al., 2017).\n\n14\n", "vlm_text": "The image consists of three graphs labeled (a), (b), and (c), which depict various aspects of model performance in a study comparing few-shot prompting and finetuning:\n\n(a) The plot shows test accuracy as a function of strong student compute (measured as a fraction of GPT-4) for zero-shot learning, 5-shot learning with ground truth labels, and finetuning. Finetuning generally achieves higher accuracy as student compute increases.\n\n(b) This graph illustrates the test accuracy for 5-shot learning with weak labels, alongside the same metrics as in panel (a). Different line colors represent varying sizes of weak supervisors. Accuracy improves with the size of the student model and the supervisor.\n\n(c) The plot compares weak-to-strong performance using 5-shot prompting, naive finetuning, and finetuning with confidence loss, showing test accuracy as a function of strong student compute. Few-shot prompting performance becomes competitive with finetuning, though finetuning with the confidence loss yields higher accuracy for larger models.\n\nThe results are averaged across seven NLP tasks. The study suggests that few-shot prompting approaches the effectiveness of finetuning for large models, with specific methods like confidence loss boosting performance further.\nfor finetuning and 5-shot. For both the zero-shot and 5-shot baseline we use task-specific prompts summarized in Table  2 . We find that zero-shot and 5-shot test accuracy is poor for most model sizes but, consistent with  Brown et al.  ( 2020 ), improves drastically for larger model sizes. In particular, for the largest models, 5-shot prompting becomes competitive with finetuning on many tasks, indicating that eliciting the task-relevant knowledge of these very large models is relatively straightforward. \nWe are also interested in weak-to-strong learning in the context of few-shot prompting. To study this setting, we construct a few-shot prompt where the labels are provided by the weak supervisor. We report the results in Figure  9 b. Consistent with our findings in the finetuning setting, we get worse performance when we few-shot prompt with weak labels than we do few-shot prompting with ground truth labels. This suggests that weak-to-strong learning is a nontrivial problem in the prompting setting as well. \nSimilar to the finetuning setting, few-shot weak-to-strong performance improves for stronger su- pervisors. Compared to our weak-to-strong finetuning baseline (Figure  9 c), weak-to-strong perfor- mance of few-shot prompting is poor for smaller student models, but becomes competitive or even outperforms finetuning for the largest strong students. However, weak-to-strong finetuning with the confidence loss still generally outperforms weak-to-strong few-shot prompting. \nOverall, these results provide an important reference for our results on weak-to-strong generaliza- tion. They suggest that for the largest model sizes, the knowledge needed to solve many task can be elicited fairly easily with prompting. However, our current setup may be more d is analogous for prompting than for finetuning; many of our NLP tasks may have been implicitly observed during pre training, which we conjecture benefits prompting more than finetuning. We discuss this potential disanalogy much more in Section  6.1 . \n5.2.2GENERATIVE SUPERVISION IMPROVES RM WEAK-TO-STRONG GENERALIZATION\nIf salient representations of the desired task is useful for weak-to-strong generalization, then we may be able to improve generalization by increasing the salience of the task to the strong model. One way to increase the salience of a task without needing ground truth labels is to perform unsupervised finetuning with the language modeling objective on data relevant to that task ( Dai & Le ,  2015 ). For example, by finetuning a language model in an unsupervised way on online reviews, sentiment becomes saliently represented to models internally ( Radford et al. ,  2017 ). "}
{"page": 14, "image_path": "doc_images/2312.09390v1_14.jpg", "ocr_text": "no generative with generative strong ceiling performance\n9.\n\nfinetuning ~~ finetuning .t. supervision) = ——\n@) 6) 499\n&\nUD 1\n_ 3 80. <\n& g sO\n& 3 ak\ns 2 60 23\n5 a $3\n3 S ge\ns g 40 3 g\n- é Be\n—E 20. S\n2 0\nG\na\n0\n107 10 10° 10% 10% 107 10® 10° 10% 10% 107\nstrong student compute strong student compute\n(fraction of GPT4) (fraction of GPT4)\n\nFigure 10: Generative finetuning on reward modeling data improves weak-to-strong perfor-\nmance and PGR. (a) Weak-to-strong performance on the reward modeling task, with (solid lines)\nand without (dashed lines) an extra step of generative finetuning for the strong student model. Solid\nblack line shows a strong ceiling reward model that was also trained with the generative finetuning\nstep; dashed black line show a weak supervisor reward model trained without the generative fine-\ntuning step. (b) PGR with and without generative finetuning. For generative finetuning PGR, we\nuse the strong ceiling performance that also had this extra generative finetuning step. Even with this\nceiling adjustment, PGR is higher with an extra generative finetuning step.\n\nWe test this idea in our reward modeling setting, where it is standard practice to initialize the model\nwith a baseline finetuned on demonstrations of desired behaviors (Stiennon et al., 2020). In our case,\nwe re-use the ChatGPT comparison data instead of introducing a new supervision dataset. Compar-\nisons are comprised of a prefix (a single request or conversation between the user and assistant) and\nat least two candidate completions. We finetune the base models with a language modeling loss on\nall prefix-completion pairs, ignoring the human preferences between those completions.\n\nNote that these pairs include completions ranked worst by human raters, so this procedure should not\nin principle leak any information about the ground truth preference labels that the weak-to-strong\nmodels should not have access to. On the other hand, since the completions can come from humans\nor stronger models, there may be some leakage similar in kind to the pretraining leakage that we\ndiscuss as a disanalogy in Section 6.1. Even in this setup, the reward modeling task is highly non-\ntrivial, and we leave addressing this disanalogy (e.g. by collecting completions only from weaker\nmodels) for future work.\n\nWe found that the additional generative finetuning on the RM data leads to better weak-to-strong\nperformance. Because this procedure also improves the performance of models trained on ground\ntruth RM data, we compare our new weak-to-strong performance to strong “ceiling” models that\nwere also first generatively finetuned in the same way. Even with this adjusted ceiling, we find that\ngenerative supervision improves PGR by approximately 10-20%. We report the results in Figure 10.\n\nFurthermore, the improvement from generative finetuning stacks with the improvement from ground\ntruth early-stopping (a “cheating” method to illustrate potential performance if we could optimally\nearly stop, see Section 5.1.1). When we combine these two techniques, we can achieve PGR of\napproximately 30-40%, which would make the results on the RM task competitive with the weak-\nto-strong generalization we observe on NLP and chess puzzle tasks.\n\nWe can apply the idea of improving task saliency with generative finetuning on relevant data to all\nsettings, and we believe this could be a promising direction for future work.\n\n5.2.3. FINETUNING ON WEAK SUPERVISION TO INCREASE CONCEPT SALIENCY\nOne possible measure of concept saliency is how linearly represented a task is. In particular, we can\n\nmeasure the performance of a linear probe (logistic regression classifier) trained from frozen activa-\ntions of the model. If the optimal solution can be approximately recovered with a linear probe, that\n\n15\n", "vlm_text": "The image is a set of two graphs that illustrate the effects of generative finetuning on performance metrics in the context of reward modeling data. The caption indicates that the graphs are examining the benefits of using additional generative finetuning steps to improve the training of a strong student model compared to a weak supervisor model.\n\n**Graph (a):**\n- This graph shows \"test accuracy (%)\" on the y-axis against \"strong student compute (fraction of GPT-4)\" on the x-axis.\n- Solid lines represent models with the extra generative finetuning step, while dashed lines represent models without it.\n- The chance of achieving higher test accuracy is greater with the extra finetuning step, as indicated by the upward trend of the solid lines.\n- The solid black line denotes a strong model that has undergone generative finetuning, indicating higher accuracy compared to its dashed counterpart, which represents a weak model without generative finetuning.\n\n**Graph (b):**\n- This graph shows \"performance gap recovered (%)\" on the y-axis also against \"strong student compute (fraction of GPT-4)\" on the x-axis.\n- Similar to graph (a), there are versions with and without generative finetuning.\n- Generally, performance gap recovery is higher for models with the extra finetuning step (solid lines) compared to models without it (dashed lines).\n\nBoth graphs use color and line style to differentiate between models and conditions, emphasizing that additional generative finetuning can improve both weak-to-strong performance and performance gap recovery.\nWe test this idea in our reward modeling setting, where it is standard practice to initialize the model with a baseline finetuned on demonstrations of desired behaviors ( Stiennon et al. ,  2020 ). In our case, we re-use the ChatGPT comparison data instead of introducing a new supervision dataset. Compar- isons are comprised of a prefix (a single request or conversation between the user and assistant) and at least two candidate completions. We finetune the base models with a language modeling loss on all prefix-completion pairs, ignoring the human preferences between those completions. \nNote that these pairs include completions ranked worst by human raters, so this procedure should not in principle leak any information about the ground truth preference labels that the weak-to-strong models should not have access to. On the other hand, since the completions can come from humans or stronger models, there may be some leakage similar in kind to the pre training leakage that we discuss as a disanalogy in Section  6.1 . Even in this setup, the reward modeling task is highly non- trivial, and we leave addressing this disanalogy (e.g. by collecting completions only from weaker models) for future work. \nWe found that the additional generative finetuning on the RM data leads to better weak-to-strong performance. Because this procedure also improves the performance of models trained on ground truth RM data, we compare our new weak-to-strong performance to strong “ceiling” models that were also first generative ly finetuned in the same way. Even with this adjusted ceiling, we find that generative supervision improves PGR by approximately  $10.20\\%$  . We report the results in Figure  10 . \nFurthermore, the improvement from generative finetuning stacks with the improvement from ground truth early-stopping (a “cheating” method to illustrate potential performance if we could optimally early stop, see Section  5.1.1 ). When we combine these two techniques, we can achieve PGR of approximately   $30–40\\%$  , which would make the results on the RM task competitive with the weak- to-strong generalization we observe on NLP and chess puzzle tasks. \nWe can apply the idea of improving task saliency with generative finetuning on relevant data to all settings, and we believe this could be a promising direction for future work. \n5.2.3 F INETUNING ON WEAK SUPERVISION TO INCREASE CONCEPT SALIENCY \nOne possible measure of concept saliency is how linearly represented a task is. In particular, we can measure the performance of a linear probe (logistic regression classifier) trained from frozen activa- tions of the model. If the optimal solution can be approximately recovered with a linear probe, that "}
{"page": 15, "image_path": "doc_images/2312.09390v1_15.jpg", "ocr_text": "Ip(weak)\n\nIp(ot){\n\nft(weak)\nft(weak)+lIp(gt)\nft(gt)\n\n60 70 80 90\ntest accuracy (%)\n\nFigure 11: Finetuning on weak supervisor labels makes the desired generalization more lin-\nearly represented. We plot test accuracy for five different strategies, averaged across a subset of\nNLP tasks. Ip(weak): training a linear probe on the base model using weak labels, Ip(gt): training a\nlinear probe on the base models using ground truth labels, ft(weak): finetuning the model on weak\nlabels, ft(weak) + Ip(gt): finetuning the model on weak labels then training a linear probe on ground\ntruth labels, ft(gt): finetuning the model on ground truth labels. Finetuning on the weak labels sig-\nnificantly increases the linearity of the ground truth concept.\n\ncould simplify our problem greatly; we could focus on linear probing methods instead of finetuning\nmethods, which could greatly reduce the search space we need to consider to elicit the desired gen-\neralization. In our work, we focus only on how linearly represented a task is in the final activations,\nprior to the unembedding layer.\n\nIn Figure 11, we plot average test accuracy on a subset of our NLP datasets for several different\ncombinations of (1) finetuning or linear probing, using (2) weak or ground truth labels. First, we\nshow linear probes trained with ground truth labels (72% accuracy on average) perform worse than\nfinetuning with ground truth labels (82% on average), indicating that the optimal solution to most\ntasks is not represented completely linearly in the strong model’s final activations. For comparison,\nwe also report the results for linear probing and finetuning using weak labels, which we verify are\nworse than using ground-truth labels.\n\nHowever, we find that we can achieve substantially better performance by first finetuning the model\non the weak labels, and then linear probing using the ground truth labels. In other words, when\nwe finetune the strong model with weak labels, the representations become more linear even with\nrespect to ground truth labels. Yn fact, finetuning on weak labels then linear probing on ground truth\nlabels results in an accuracy of 78%, closing 60% of the gap between ground truth linear probing\nand finetuning. This also noticeably outperforms the naive weak-to-strong finetuning baseline.\n\nThis phenomenon is closely related to a recent finding reported by Kirichenko et al. (2023) in the\nspurious cues literature. They find that finetuning a model on biased supervision can result in mod-\nels with very biased outputs, but surprisingly strong linear representations of the desired concepts.\nThese results suggest an alternative approach to improving weak-to-strong generalization. We could\nfirst “linearize” the desired concept, e.g. by naively finetuning on weak labels. Then we could use\nsimpler linear probe-based weak-to-strong methods to elicit the desired concept.\n\n6 DISCUSSION\n\nIn this paper, we proposed a simple analogy for studying a core challenge of aligning superhuman\nmodels and showed that it is feasible to make significant progress on this problem. However, our\nsetup still has important disanalogies, which we now elaborate on. We then outline a number of\npromising avenues for future work.\n\n6.1 REMAINING DISANALOGIES\nImitation saliency: superhuman models may easily imitate weak errors. Future models will\n\nlikely be very good at predicting what humans will think and say, especially if they are trained\non human data in a similar manner to current models. Consequently, if we naively train such a\n\n16\n", "vlm_text": "The image is a bar chart comparing test accuracy percentages for five different training strategies, averaged across several NLP tasks. The strategies are:\n\n1. **lp(weak):** Training a linear probe on the base model using weak labels.\n2. **lp(gt):** Training a linear probe on the base model using ground truth labels.\n3. **ft(weak):** Finetuning the model on weak labels.\n4. **ft(weak)+lp(gt):** Finetuning the model on weak labels, then training a linear probe on ground truth labels.\n5. **ft(gt):** Finetuning the model on ground truth labels.\n\nThe bars show that finetuning on weak labels increases the linearity of the ground truth concept, with the strategy \"ft(weak)+lp(gt)\" achieving the highest accuracy.\ncould simplify our problem greatly; we could focus on linear probing methods instead of finetuning methods, which could greatly reduce the search space we need to consider to elicit the desired gen- era liz ation. In our work, we focus only on how linearly represented a task is in the final activation s, prior to the un embedding layer. \nIn Figure  11 , we plot average test accuracy on a subset of our NLP datasets for several different combinations of (1) finetuning or linear probing, using (2) weak or ground truth labels. First, we show linear probes trained with ground truth labels (  $72\\%$   accuracy on average) perform worse than finetuning with ground truth labels (  $82\\%$   on average), indicating that the optimal solution to most tasks is  not  represented completely linearly in the strong model’s final activation s. For comparison, we also report the results for linear probing and finetuning using weak labels, which we verify are worse than using ground-truth labels. \nHowever, we find that we can achieve substantially better performance by  first  finetuning the model on the  weak  labels, and  then  linear probing using the  ground truth  labels. In other words, when we finetune the strong model with weak labels, the representations become  more linear even with respect to ground truth labels . In fact, finetuning on weak labels then linear probing on ground truth labels results in an accuracy of   $78\\%$  , closing   $60\\%$   of the gap between ground truth linear probing and finetuning. This also noticeably outperforms the naive weak-to-strong finetuning baseline. \nThis phenomenon is closely related to a recent finding reported by  Kirichenko et al.  ( 2023 ) in the spurious cues literature. They find that finetuning a model on biased supervision can result in mod- els with very biased outputs, but surprisingly strong linear representations of the desired concepts. These results suggest an alternative approach to improving weak-to-strong generalization. We could first “linearize” the desired concept, e.g. by naively finetuning on weak labels. Then we could use simpler linear probe-based weak-to-strong methods to elicit the desired concept. \n6 D ISCUSSION \nIn this paper, we proposed a simple analogy for studying a core challenge of aligning superhuman models and showed that it is feasible to make significant progress on this problem. However, our setup still has important d is analogies, which we now elaborate on. We then outline a number of promising avenues for future work. \n6.1 R EMAINING D IS ANALOGIES \nImitation saliency: superhuman models may easily imitate weak errors. Future models will likely be very good at predicting what humans will think and say, especially if they are trained on human data in a similar manner to current models. Consequently, if we naively train such a superhuman model with human supervision, it might simply imitate the weak supervisor, outputting human-level capabilities rather than its latent superhuman capabilities ( Christiano et al. ,  2022 ). "}
{"page": 16, "image_path": "doc_images/2312.09390v1_16.jpg", "ocr_text": "superhuman model with human supervision, it might simply imitate the weak supervisor, outputting\nhuman-level capabilities rather than its latent superhuman capabilities (Christiano et al., 2022).\n\nThis problem is only partially captured by our setup. While our strong pretrained models do imitate\nweak supervisors to some extent, they are not explicitly pretrained to imitate weak models, and our\nresults from Section 5.1.3 suggest that larger strong models may even have more difficulty doing this\nimitation. As such, “imitating the weak supervisor” may not be as much of a problem in our setup\nas it will be for the ultimate superalignment problem. This may inflate generalization performance\ntoday. We believe a more thorough investigation of this problem is an important area for future\nwork.\n\nPretraining leakage: superhuman knowledge may be latent, not observable. | Many of the\ntasks we consider in this work may have been observed in pretraining at least indirectly, for exam-\nple through questions on online forums or through slight reframings of the task. For example, it is\nhighly likely that simple science questions similar to those in the SciQ NLP task are present in our\nGPT-4 series pretraining dataset at least implicitly in some form. However future superhuman mod-\nels may never directly observe superhuman alignment-relevant capabilities; these capabilities may\nbe predominantly “latent”, e.g. learned through self-supervised learning or reinforcement learning\nrather than through imitation learning. Intuitively, latent capabilities may be harder to elicit than\ncapabilities that models could have observed in their pretraining data.\n\nThis disanalogy could cause our results to be overly optimistic. We conjecture that this disanalogy\nalso increases prompting performance (Section 5.2.1) more than it increases finetuning performance;\nintuitively prompting may work especially well on tasks that the model assigns high probability to\nobserving. If so, this would make prompting more disanalogous in our setup than finetuning. We\nhope to test this conjecture in future work.\n\nIn Appendix D.1, we show a proof of concept that weak-to-strong generalization can still elicit latent\ncapabilities that were never explicitly observed during pretraining, and even when prompting is not\npossible. In particular, we use AlexNet (Krizhevsky et al., 2012) to supervise models pretrained with\nDINO (Caron et al., 2021), a self-supervised method in computer vision that learns strong represen-\ntations. We find that the strong student generalizes significantly beyond AlexNet’s performance,\neven though the student never observed any classification labels during pretraining. Future work\nshould study and mitigate this pretraining leakage disanology more systematically.\n\n6.2 FUTURE WORK\n\nWhat would convince us that we have a “solution” to superalignment? This is a complicated question\nand we do not claim to have a complete answer. However, we expect substantial progress in at least\nthe following three areas will be necessary: analogous setups, scalable methods, and strong scientific\nunderstanding. We now sketch out concrete problems for each of these areas.\n\n6.2.1 CONCRETE PROBLEMS: ANALOGOUS SETUPS\n\nHaving strong measurements and a reliable methodology is extremely important for making empir-\nical progress in any field. In particular, it is important that we have metrics which provide strong\nsignal about whether we are making real progress toward the problem we ultimately care about.\nImportant directions for follow-up work include:\n\n¢ Making our setup more analogous by fixing the main remaining disanalogies described in\nSection 6.1. Analogous setups are essential to ensure that methods that work today will\ncontinue to work for superhuman models.\n\n¢ Validating that disanalogies are not severe, for example by checking that results are quali-\ntatively similar to using e.g. 3rd grade humans to supervise our strongest models today.\n\n¢ Relaxing some of the simplifications we made, e.g. by generalizing our methods and results\nto complicated generative tasks.\n\n¢ Testing how robust our weak-to-strong classifiers are to optimization pressure when we\nattain high PGR; for example, if we attain good weak-to-strong generalization with RMs,\ncan we optimize the learned RM using RL?\n\n17\n", "vlm_text": "\nThis problem is only partially captured by our setup. While our strong pretrained models do imitate weak supervisors to some extent, they are not explicitly pretrained to imitate weak models, and our results from Section  5.1.3  suggest that larger strong models may even have more difficulty doing this imitation. As such, “imitating the weak supervisor” may not be as much of a problem in our setup as it will be for the ultimate super alignment problem. This may inflate generalization performance today. We believe a more thorough investigation of this problem is an important area for future work. \nPre training leakage: superhuman knowledge may be latent, not observable. Many of the tasks we consider in this work may have been observed in pre training at least indirectly, for exam- ple through questions on online forums or through slight reframings of the task. For example, it is highly likely that simple science questions similar to those in the SciQ NLP task are present in our GPT-4 series pre training dataset at least implicitly in some form. However future superhuman mod- els may never directly observe superhuman alignment-relevant capabilities; these capabilities may be predominantly “latent”, e.g. learned through self-supervised learning or reinforcement learning rather than through imitation learning. Intuitively, latent capabilities may be harder to elicit than capabilities that models could have observed in their pre training data. \nThis disanalogy could cause our results to be overly optimistic. We conjecture that this disanalogy also increases prompting performance (Section  5.2.1 ) more than it increases finetuning performance; intuitively prompting may work especially well on tasks that the model assigns high probability to observing. If so, this would make prompting more d is analogous in our setup than finetuning. We hope to test this conjecture in future work. \nIn Appendix  D.1 , we show a proof of concept that weak-to-strong generalization can still elicit latent capabilities that were never explicitly observed during pre training, and even when prompting is not possible. In particular, we use AlexNet ( Krizhevsky et al. ,  2012 ) to supervise models pretrained with DINO ( Caron et al. ,  2021 ), a self-supervised method in computer vision that learns strong represen- tations. We find that the strong student generalizes significantly beyond AlexNet’s performance, even though the student never observed any classification labels during pre training. Future work should study and mitigate this pre training leakage disanology more systematically. \n6.2 F UTURE  W ORK \nWhat would convince us that we have a “solution” to super alignment? This is a complicated question and we do not claim to have a complete answer. However, we expect substantial progress in at least the following three areas will be necessary: analogous setups, scalable methods, and strong scientific understanding. We now sketch out concrete problems for each of these areas. \n6.2.1 C ONCRETE  P ROBLEMS : A NALOGOUS  S ETUPS \nHaving strong measurements and a reliable methodology is extremely important for making empir- ical progress in any field. In particular, it is important that we have metrics which provide strong signal about whether we are making real progress toward the problem we ultimately care about. Important directions for follow-up work include: \n• Making our setup more analogous by fixing the main remaining d is analogies described in Section  6.1 . Analogous setups are essential to ensure that methods that work today will continue to work for superhuman models. • Validating that d is analogies are not severe, for example by checking that results are quali- tatively similar to using e.g. 3rd grade humans to supervise our strongest models today. • Relaxing some of the simplifications we made, e.g. by generalizing our methods and results to complicated generative tasks. • Testing how robust our weak-to-strong class if i ers are to optimization pressure when we attain high PGR; for example, if we attain good weak-to-strong generalization with RMs, can we optimize the learned RM using RL? "}
{"page": 17, "image_path": "doc_images/2312.09390v1_17.jpg", "ocr_text": "¢ Testing our conjecture that prompting-based methods in our current setup will not be as\nindicative of future results relative to finetuning-based methods (Section 5.2.1), and im-\nproving our setup to fix this.\n\n¢ Identifying new or more specific disanalogies with our setup and fixing them.\n\nAdditionally, we do not yet know what future models will look like. We should update our setup\nover time as we learn more about how broadly superhuman models will be built.\n\n6.2.2 CONCRETE PROBLEMS: SCALABLE METHODS\n\nOne intuition for why major progress on weak-to-strong generalization seems possible is because\nall we need to do is extract everything the strong model already “knows” about the task of interest—\nthe strong model should intuitively already understand the task, and should hopefully have salient\nrepresentations of that task. This suggests a number of properties that should be satisfied by the\ndesired generalization, and which we may be able to measure without access to ground truth.\n\n¢ The desired generalization should be able to disagree with the weak supervision when the\nweak supervision is wrong. This is a property our auxiliary confidence loss may capture.\n\n¢ The desired generalization should be “natural” or “salient” to the model. For example, we\nshould not need to change the model too much to elicit the desired concept.\n\n¢ The desired generalization should be consistent. Consistency properties range anywhere\nfrom basic logical consistency to complicated forms of consistency between many prompts\n(e.g. cycle consistency, cross examination, etc.).\n\nFuture work should identify additional unsupervised properties that can be used to specify the de-\nsired generalization. More generally, there are very likely existing methods in the machine learning\nliterature (e.g. in semi-supervised learning or robust finetuning), which would be natural to try and\nwhich could also lead to substantial gains in weak-to-strong generalization. Generalization-based\napproaches to weak-to-strong learning are complementary to scalable oversight methods, in which\nthe weak supervisor interacts with the strong model to improve the quality of the weak supervision.\n\n6.2.3. CONCRETE PROBLEMS: SCIENTIFIC UNDERSTANDING\n\nWe will need an extremely high degree of trust and reliability in our methods for aligning super-\nhuman models in high-stakes settings. We will not get this from strong benchmark performance\nalone. Instead, we also need a thorough understanding of precisely when and why our methods\nwork. Example questions of interest include:\n\n¢ What explains the difference between the relatively strong results on NLP datasets and the\nrelatively poor results with reward models when using naive finetuning?\n\n¢ What makes a concept easy or hard to elicit? What is a good definition of “salience”?\n\n* Can we reliably estimate generalization error at test time without any labels? For example,\ncan we measure the degree of weak-to-strong underspecification (Lee et al., 2022b)?\n\n¢ Can we reliably extrapolate generalization error across many orders of magnitude using\nscaling laws?\n\n¢ How important are the errors in the weak supervision, precisely? How do different kinds\nof weak label biases affect generalization?\n\n¢ How robust are our proposed methods to optimization pressure?\nIn Section 5 we only scratched the surface for understanding weak-to-strong generalization, but\n\nfuture work will need to go much further. An advantage of our setup is that it makes it easy to run\nsimple experiments to scientifically study generalization phenomena across a wide range of settings.\n\n6.3 CONCLUSION\nRecent progress in AI has been faster than almost anyone anticipated (Steinhardt, 2022; Bengio\n\net al.). For an increasing number of researchers, the possibility of superhuman models being de-\nveloped this decade has become increasingly plausible. Broadly superhuman models would be\n\n18\n", "vlm_text": "• Testing our conjecture that prompting-based methods in our current setup will not be as indicative of future results relative to finetuning-based methods (Section  5.2.1 ), and im- proving our setup to fix this. • Identifying new or more specific d is analogies with our setup and fixing them. \nAdditionally, we do not yet know what future models will look like. We should update our setup over time as we learn more about how broadly superhuman models will be built. \n6.2.2 C ONCRETE  P ROBLEMS : S CALABLE  M ETHODS \nOne intuition for why major progress on weak-to-strong generalization seems possible is because all we need to do is extract everything the strong model already “knows” about the task of interest— the strong model should intuitively already understand the task, and should hopefully have salient representations of that task. This suggests a number of properties that should be satisfied by the desired generalization, and which we may be able to measure without access to ground truth. \n• The desired generalization should be able to  disagree with the weak supervision  when the weak supervision is wrong. This is a property our auxiliary confidence loss may capture. • The desired generalization should be “ natural ” or “ salient ” to the model. For example, we should not need to change the model too much to elicit the desired concept. • The desired generalization should be  consistent . Consistency properties range anywhere from basic logical consistency to complicated forms of consistency between many prompts (e.g. cycle consistency, cross examination, etc.). \nFuture work should identify additional unsupervised properties that can be used to specify the de- sired generalization. More generally, there are very likely existing methods in the machine learning literature (e.g. in semi-supervised learning or robust finetuning), which would be natural to try and which could also lead to substantial gains in weak-to-strong generalization. Generalization-based approaches to weak-to-strong learning are complementary to scalable oversight methods, in which the weak supervisor interacts with the strong model to improve the quality of the weak supervision. \n6.2.3 C ONCRETE  P ROBLEMS : S CIENTIFIC  U NDER STANDING \nWe will need an extremely high degree of trust and reliability in our methods for aligning super- human models in high-stakes settings. We will not get this from strong benchmark performance alone. Instead, we also need a thorough understanding of precisely  when  and  why  our methods work. Example questions of interest include: \n• What explains the difference between the relatively strong results on NLP datasets and the relatively poor results with reward models when using naive finetuning? • What makes a concept easy or hard to elicit? What is a good definition of “salience”? • Can we reliably estimate generalization error at test time without any labels? For example, can we measure the degree of weak-to-strong under specification ( Lee et al. ,  2022b )? • Can we reliably extrapolate generalization error across many orders of magnitude using scaling laws? • How important are the errors in the weak supervision, precisely? How do different kinds of weak label biases affect generalization? • How robust are our proposed methods to optimization pressure? \nIn Section  5  we only scratched the surface for understanding weak-to-strong generalization, but future work will need to go much further. An advantage of our setup is that it makes it easy to run simple experiments to scientifically study generalization phenomena across a wide range of settings. \n6.3 C ONCLUSION \nRecent progress in AI has been faster than almost anyone anticipated ( Steinhardt ,  2022 ;  Bengio et al. ). For an increasing number of researchers, the possibility of superhuman models being de- veloped this decade has become increasingly plausible. Broadly superhuman models would be extraordinarily powerful and, if misused or misaligned with humans values, could potentially cause catastrophic harm ( CAIS ). Given the stakes, we need to establish extremely high reliability in the alignment of these systems ahead of time. But for years it has been unclear how to empirically study superhuman model alignment. We believe it is now easier to make progress on this problem than ever before. "}
{"page": 18, "image_path": "doc_images/2312.09390v1_18.jpg", "ocr_text": "extraordinarily powerful and, if misused or misaligned with humans values, could potentially cause\ncatastrophic harm (CAIS). Given the stakes, we need to establish extremely high reliability in the\nalignment of these systems ahead of time. But for years it has been unclear how to empirically study\nsuperhuman model alignment. We believe it is now easier to make progress on this problem than\never before.\n\n7 ACKNOWLEDGEMENTS\n\nWe would like to thank Boaz Barak, Paul Christiano, Jacob Steinhardt, Ananya Kumar, Jakub Pa-\nchocki, John Schulman, Wojciech Zaremba, Alec Radford, Nat McAleese, and William Saunders\nfor valuable technical insights and discussions. We are grateful to Mia Glaese, Boaz Barak, Kush\nBhatia, Jean-Stanislas Denain, Erik Jones, Polina Kirichenko, Daniel Kokotajlo, Yoonho Lee, Jessy\nLin, Richard Ngo, John Schulman, Peter Tong, Fred Zhang, Ruiqi Zhong, Ryan Greenblatt, Fabien\nRoger, Paul Christiano, Steven Adler, Rai Pokorny, Adam Kalai, Jacob Hilton, Roger Grosse, Dan\nHendrycks, Alec Radford, and Scott Aaronson for helpful feedback on earlier drafts of this paper.\nWe also thank Shantanu Jain, Avital Oliver, Suchir Balaji, Cathy Yeh, and the Platform team for\ninfrastructure help. CB is also grateful to Dan Hendrycks, Jacob Steinhardt, and Paul Christiano for\nmany formative discussions over the years.\n\nREFERENCES\n\nEric Arazo, Diego Ortego, Paul Albert, Noel O’Connor, and Kevin McGuinness. Unsupervised\nlabel noise modeling and loss correction. In International conference on machine learning, pp.\n312-321. PMLR, 2019. (Cited on page 33)\n\nChristopher G Atkeson and Stefan Schaal. Robot learning from demonstration. In JCML, volume 97,\npp. 12-20. Citeseer, 1997. (Cited on page 5)\n\nAnas Awadalla, Mitchell Wortsman, Gabriel Ilharco, Sewon Min, Ian Magnusson, Hannaneh Ha-\njishirzi, and Ludwig Schmidt. Exploring The Landscape of Distributional Robustness for Ques-\ntion Answering Models. arXiv preprint arXiv:2210.12517, 2022. (Cited on page 4)\n\nStephen H Bach, Bryan He, Alexander Ratner, and Christopher Ré. Learning the structure of genera-\ntive models without labeled data. In Jnternational Conference on Machine Learning, pp. 273-282.\nPMLR, 2017. (Cited on page 4)\n\nYuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn\nDrain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless\nassistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862,\n2022a. (Cited on page 1, 5)\n\nYuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones,\nAnna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional AI:\nHarmlessness from AI feedback. arXiv preprint arXiv:2212.08073, 2022b. (Cited on page 5, 47)\n\nMichael Bain and Claude Sammut. A Framework for Behavioural Cloning. In Machine Intelligence\n15, pp. 103-129, 1995. (Cited on page 5)\n\nRachel KE Bellamy, Kuntal Dey, Michael Hind, Samuel C Hoffman, Stephanie Houde, Kalapriya\nKannan, Pranay Lohia, Jacquelyn Martino, Sameep Mehta, Aleksandra Mojsilovic, et al. AI Fair-\nness 360: An extensible toolkit for detecting, understanding, and mitigating unwanted algorithmic\nbias. arXiv preprint arXiv: 1810.01943, 2018. (Cited on page 4)\n\nYoshua Bengio, Geoffrey Hinton, Andrew Yao, Dawn Song, Pieter Abbeel, Yuval Noah Harari, Ya-\nQin Zhang, Lan Xue, Shai Shalev-Shwartz, Gillian Hadfield, et al. Managing AI risks in an era\nof rapid progress. arXiv preprint arXiv:2310.17688. (Cited on page 18)\n\nDavid Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, and Colin A\n\nRaffel. Mixmatch: A holistic approach to semi-supervised learning. Advances in neural informa-\ntion processing systems, 32, 2019. (Cited on page 4)\n\n19\n", "vlm_text": "\n7 A C KNOWLEDGE MENT S \nWe would like to thank Boaz Barak, Paul Christiano, Jacob Steinhardt, Ananya Kumar, Jakub Pa- chocki, John Schulman, Wojciech Zaremba, Alec Radford, Nat McAleese, and William Saunders for valuable technical insights and discussions. We are grateful to Mia Glaese, Boaz Barak, Kush Bhatia, Jean-Stanislas Denain, Erik Jones, Polina Kirichenko, Daniel Kokotajlo, Yoonho Lee, Jessy Lin, Richard Ngo, John Schulman, Peter Tong, Fred Zhang, Ruiqi Zhong, Ryan Greenblatt, Fabien Roger, Paul Christiano, Steven Adler, Rai Pokorny, Adam Kalai, Jacob Hilton, Roger Grosse, Dan Hendrycks, Alec Radford, and Scott Aaronson for helpful feedback on earlier drafts of this paper. We also thank Shantanu Jain, Avital Oliver, Suchir Balaji, Cathy Yeh, and the Platform team for infrastructure help. CB is also grateful to Dan Hendrycks, Jacob Steinhardt, and Paul Christiano for many formative discussions over the years. \nR EFERENCES \nEric Arazo, Diego Ortego, Paul Albert, Noel O’Connor, and Kevin McGuinness. Unsupervised label noise modeling and loss correction. In  International conference on machine learning , pp. 312–321. PMLR, 2019. (Cited on page  33 ) Christopher G Atkeson and Stefan Schaal. Robot learning from demonstration. In  ICML , volume 97, pp. 12–20. Citeseer, 1997. (Cited on page  5 ) Anas Awadalla, Mitchell Wortsman, Gabriel Ilharco, Sewon Min, Ian Magnusson, Hannaneh Ha- jishirzi, and Ludwig Schmidt. Exploring The Landscape of Distribution al Robustness for Ques- tion Answering Models.  arXiv preprint arXiv:2210.12517 , 2022. (Cited on page  4 ) Stephen H Bach, Bryan He, Alexander Ratner, and Christopher R´ e. Learning the structure of genera- tive models without labeled data. In  International Conference on Machine Learning , pp. 273–282. PMLR, 2017. (Cited on page  4 ) Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless assistant with reinforcement learning from human feedback.  arXiv preprint arXiv:2204.05862 , 2022a. (Cited on page  1 ,  5 ) Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional AI: Harmlessness from AI feedback.  arXiv preprint arXiv:2212.08073 , 2022b. (Cited on page  5 ,  47 ) Michael Bain and Claude Sammut. A Framework for Behavioural Cloning. In  Machine Intelligence 15 , pp. 103–129, 1995. (Cited on page  5 ) Rachel KE Bellamy, Kuntal Dey, Michael Hind, Samuel C Hoffman, Stephanie Houde, Kalapriya Kannan, Pranay Lohia, Jacquelyn Martino, Sameep Mehta, Aleksandra Mojsilovic, et al. AI Fair- ness 360: An extensible toolkit for detecting, understanding, and mitigating unwanted algorithmic bias.  arXiv preprint arXiv:1810.01943 , 2018. (Cited on page  4 ) Yoshua Bengio, Geoffrey Hinton, Andrew Yao, Dawn Song, Pieter Abbeel, Yuval Noah Harari, Ya- Qin Zhang, Lan Xue, Shai Shalev-Shwartz, Gillian Hadfield, et al. Managing AI risks in an era of rapid progress.  arXiv preprint arXiv:2310.17688 . (Cited on page  18 ) David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, and Colin A Raffel. Mixmatch: A holistic approach to semi-supervised learning.  Advances in neural informa- tion processing systems , 32, 2019. (Cited on page  4 ) "}
{"page": 19, "image_path": "doc_images/2312.09390v1_19.jpg", "ocr_text": "Lucas Beyer, Xiaohua Zhai, Amélie Royer, Larisa Markeeva, Rohan Anil, and Alexander\nKolesnikov. Knowledge distillation: A good teacher is patient and consistent. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern recognition, pp. 10925-10934, 2022.\n(Cited on page 4)\n\nSteven Bills, Nick Cammarata, Dan Mossing, Henk Tillman, Leo Gao, Gabriel Goh, Ilya Sutskever,\nJan Leike, Jeff Wu, and William Saunders. Language models can explain neurons in language\nmodels. OpenAI Blog, 2023. (Cited on page 47)\n\nYonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. PIQA: Reasoning\nabout Physical Commonsense in Natural Language. In Thirty-Fourth AAAI Conference on Artifi-\ncial Intelligence, 2020. (Cited on page 29)\n\nSam Bowman. Artificial Sandwiching: When can we test scalable alignment protocols without\nhumans? AJ Alignment Forum, 2022. (Cited on page 5)\n\nSamuel Bowman, Jeeyoon Hyun, Ethan Perez, Edwin Chen, Craig Pettit, Scott Heiner, Kamile\nLukosuite, Amanda Askell, Andy Jones, Anna Chen, et al. Measuring progress on scalable over-\nsight for large language models. arXiv preprint arXiv:2211.03540, 2022. (Cited on page 2, 47)\n\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.\n(Cited on page 14)\n\nCollin Burns, Haotian Ye, Dan Klein, and Jacob Steinhardt. Discovering Latent Knowledge in\nLanguage Models Without Supervision. In The Eleventh International Conference on Learning\nRepresentations, 2023. (Cited on page 5)\n\nCAIS. Statement on AI risk. (Cited on page 4, 19, 47)\n\nJoe Carlsmith. Scheming Als: Will Als fake alignment during training in order to get power? arXiv\npreprint arXiv:2311.08379. (Cited on page 48)\n\nMathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and\nArmand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of\nthe IEEE/CVF international conference on computer vision, pp. 9650-9660, 2021. (Cited on page\n17, 35, 40)\n\nJunbum Cha, Sanghyuk Chun, Kyungjae Lee, Han-Cheol Cho, Seunghyun Park, Yunsung Lee, and\nSungrae Park. Swad: Domain generalization by seeking flat minima. Advances in Neural Infor-\nmation Processing Systems, 34:22405-22418, 2021. (Cited on page 36)\n\nTing Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey Hinton. Big\nself-supervised models are strong semi-supervised learners. Advances in neural information pro-\ncessing systems, 33:22243-22255, 2020a. (Cited on page 35)\n\nYining Chen, Colin Wei, Ananya Kumar, and Tengyu Ma. Self-training avoids using spurious\nfeatures under domain shift. Advances in Neural Information Processing Systems, 33:21061-\n21071, 2020b. (Cited on page 33)\n\nPaul Christiano. Approval-directed bootstrapping. AJ Alignment Forum, 2018. (Cited on page 8)\nPaul Christiano. Capability amplification. AJ Alignment Forum, 2019. (Cited on page 8)\n\nPaul Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep rein-\nforcement learning from human preferences. Advances in neural information processing systems,\n30, 2017. (Cited on page 1, 2, 5, 47)\n\nPaul Christiano, Buck Shlegeris, and Dario Amodei. Supervising strong learners by amplifying\nweak experts. arXiv preprint arXiv:1810.08575, 2018. (Cited on page 2, 5)\n\nPaul Christiano, Ajeya Cotra, and Mark Xu. Eliciting latent knowledge. Technical report, Alignment\nResearch Center (ARC), 2022. (Cited on page 5, 11, 17, 44)\n\n20\n", "vlm_text": "Lucas Beyer, Xiaohua Zhai, Am´ elie Royer, Larisa Markeeva, Rohan Anil, and Alexander Kolesnikov. Knowledge distillation: A good teacher is patient and consistent. In  Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pp. 10925–10934, 2022. (Cited on page  4 ) \nSteven Bills, Nick Cammarata, Dan Mossing, Henk Tillman, Leo Gao, Gabriel Goh, Ilya Sutskever, Jan Leike, Jeff Wu, and William Saunders. Language models can explain neurons in language models.  OpenAI Blog , 2023. (Cited on page  47 ) \nYonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. PIQA: Reasoning about Physical Commonsense in Natural Language. In  Thirty-Fourth AAAI Conference on Artifi- cial Intelligence , 2020. (Cited on page  29 ) \nSam Bowman. Artificial Sandwiching: When can we test scalable alignment protocols without humans?  AI Alignment Forum , 2022. (Cited on page  5 ) \nSamuel Bowman, Jeeyoon Hyun, Ethan Perez, Edwin Chen, Craig Pettit, Scott Heiner, Kamile Lukosuite, Amanda Askell, Andy Jones, Anna Chen, et al. Measuring progress on scalable over- sight for large language models.  arXiv preprint arXiv:2211.03540 , 2022. (Cited on page  2 ,  47 ) \nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neel a kant an, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems , 33:1877–1901, 2020. (Cited on page  14 ) \nCollin Burns, Haotian Ye, Dan Klein, and Jacob Steinhardt. Discovering Latent Knowledge in Language Models Without Supervision. In  The Eleventh International Conference on Learning Representations , 2023. (Cited on page  5 ) \nCAIS. Statement on AI risk. (Cited on page  4 ,  19 ,  47 ) \nJoe Carlsmith. Scheming AIs: Will AIs fake alignment during training in order to get power?  arXiv preprint arXiv:2311.08379 . (Cited on page  48 ) \nMathilde Caron, Hugo Touvron, Ishan Misra, Herv´ e J´ egou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In  Proceedings of the IEEE/CVF international conference on computer vision , pp. 9650–9660, 2021. (Cited on page 17 ,  35 ,  40 ) \nJunbum Cha, Sanghyuk Chun, Kyungjae Lee, Han-Cheol Cho, Seunghyun Park, Yunsung Lee, and Sungrae Park. Swad: Domain generalization by seeking flat minima.  Advances in Neural Infor- mation Processing Systems , 34:22405–22418, 2021. (Cited on page  36 ) \nTing Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey Hinton. Big self-supervised models are strong semi-supervised learners.  Advances in neural information pro- cessing systems , 33:22243–22255, 2020a. (Cited on page  35 ) \nYining Chen, Colin Wei, Ananya Kumar, and Tengyu Ma. Self-training avoids using spurious features under domain shift.  Advances in Neural Information Processing Systems , 33:21061– 21071, 2020b. (Cited on page  33 ) \nPaul Christiano. Approval-directed boots trapping.  AI Alignment Forum , 2018. (Cited on page  8 ) \nPaul Christiano. Capability amplification.  AI Alignment Forum , 2019. (Cited on page  8 ) \nPaul Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep rein- forcement learning from human preferences.  Advances in neural information processing systems , 30, 2017. (Cited on page  1 ,  2 ,  5 ,  47 ) \nPaul Christiano, Buck Shlegeris, and Dario Amodei. Supervising strong learners by amplifying weak experts.  arXiv preprint arXiv:1810.08575 , 2018. (Cited on page  2 ,  5 ) \nPaul Christiano, Ajeya Cotra, and Mark Xu. Eliciting latent knowledge. Technical report, Alignment Research Center (ARC), 2022. (Cited on page  5 ,  11 ,  17 ,  44 ) "}
{"page": 20, "image_path": "doc_images/2312.09390v1_20.jpg", "ocr_text": "Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina\nToutanova. BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions. In NAACL,\n2019. (Cited on page 29)\n\nAjeya Cotra. The case for aligning narrowly superhuman models. AJ Alignment Forum, 2021. (Cited\non page 5)\n\nAndrew M Dai and Quoc V Le. Semi-supervised sequence learning. Advances in neural information\nprocessing systems, 28, 2015. (Cited on page 14)\n\nAbram Demski and Scott Garrabrant. Embedded agency. arXiv preprint arXiv: 1902.09469, 2019.\n(Cited on page 2)\n\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An\nimage is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint\narXiv:2010.11929, 2020. (Cited on page 40)\n\nNelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann,\nAmanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Deep\nGanguli, Zac Hatfield-Dodds, Danny Hernandez, Andy Jones, Jackson Kernion, Liane Lovitt,\nKamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and\nChris Olah. A Mathematical Framework for Transformer Circuits. Transformer Circuits Thread,\n2021. https://transformer-circuits.pub/202 1/framework/index.html. (Cited on page 42)\n\nOwain Evans, Owen Cotton-Barratt, Lukas Finnveden, Adam Bales, Avital Balwit, Peter Wills,\nLuca Righetti, and William Saunders. Truthful Al: Developing and governing AI that does not\nlie. arXiv preprint arXiv:2110.06674, 2021. (Cited on page 5)\n\nGeoffrey French, Michal Mackiewicz, and Mark Fisher. Self-ensembling for visual domain adapta-\ntion. arXiv preprint arXiv: 1706.05208, 2017. (Cited on page 4)\n\nBenoit Frénay and Michel Verleysen. Classification in the presence of label noise: a survey. IEEE\ntransactions on neural networks and learning systems, 25(5):845-869, 2013. (Cited on page 4)\n\nTommaso Furlanello, Zachary Lipton, Michael Tschannen, Laurent Itti, and Anima Anandkumar.\nBorn again neural networks. In Jnternational Conference on Machine Learning, pp. 1607-1616.\nPMLR, 2018. (Cited on page 4, 13)\n\nAmelia Glaese, Nat McAleese, Maja Trebacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Maribeth\nRauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker, et al. Improving alignment of dia-\nlogue agents via targeted human judgements. arXiv preprint arXiv:2209. 14375, 2022. (Cited on\npage 1, 5)\n\nJianping Gou, Baosheng Yu, Stephen J Maybank, and Dacheng Tao. Knowledge distillation: A\nsurvey. International Journal of Computer Vision, 129:1789-1819, 2021. (Cited on page 4)\n\nYves Grandvalet and Yoshua Bengio. Semi-supervised learning by entropy minimization. Advances\nin neural information processing systems, 17, 2004. (Cited on page 4, 10, 33, 34)\n\nSheng Guo, Weilin Huang, Haozhi Zhang, Chenfan Zhuang, Dengke Dong, Matthew R. Scott, and\nDinglong Huang. CurriculumNet: Weakly Supervised Learning from Large-Scale Web Images.\nIn Proceedings of the European Conference on Computer Vision (ECCV), 2018. (Cited on page\n4)\n\nBo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor Tsang, and Masashi\nSugiyama. Co-teaching: Robust training of deep neural networks with extremely noisy labels.\nAdvances in neural information processing systems, 31, 2018. (Cited on page 4)\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-\n\nnition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.\n710-778, 2016. (Cited on page 40)\n\n21\n", "vlm_text": "Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions. In  NAACL , 2019. (Cited on page  29 ) Ajeya Cotra. The case for aligning narrowly superhuman models.  AI Alignment Forum , 2021. (Cited on page  5 ) Andrew M Dai and Quoc V Le. Semi-supervised sequence learning.  Advances in neural information processing systems , 28, 2015. (Cited on page  14 ) Abram Demski and Scott Garrabrant. Embedded agency.  arXiv preprint arXiv:1902.09469 , 2019. (Cited on page  2 ) Alexey Do sov it ski y, Lucas Beyer, Alexander Kolesnikov, Dirk Weiss en born, Xiaohua Zhai, Thomas Unter thin er, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929 , 2020. (Cited on page  40 ) Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. A Mathematical Framework for Transformer Circuits.  Transformer Circuits Thread , 2021. https://transformer-circuits.pub/2021/framework/index.html. (Cited on page  42 ) Owain Evans, Owen Cotton-Barratt, Lukas Finnveden, Adam Bales, Avital Balwit, Peter Wills, Luca Righetti, and William Saunders. Truthful AI: Developing and governing AI that does not lie.  arXiv preprint arXiv:2110.06674 , 2021. (Cited on page  5 ) Geoffrey French, Michal Mackiewicz, and Mark Fisher. Self-ensembling for visual domain adapta- tion.  arXiv preprint arXiv:1706.05208 , 2017. (Cited on page  4 ) Benoˆ ıt Fr´ enay and Michel Verleysen. Classification in the presence of label noise: a survey.  IEEE transactions on neural networks and learning systems , 25(5):845–869, 2013. (Cited on page  4 ) Tommaso Furlanello, Zachary Lipton, Michael Tschannen, Laurent Itti, and Anima Anandkumar. Born again neural networks. In  International Conference on Machine Learning , pp. 1607–1616. PMLR, 2018. (Cited on page  4 ,  13 ) Amelia Glaese, Nat McAleese, Maja Trebacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Maribeth Rauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker, et al. Improving alignment of dia- logue agents via targeted human judgements.  arXiv preprint arXiv:2209.14375 , 2022. (Cited on page  1 ,  5 ) Jianping Gou, Baosheng Yu, Stephen J Maybank, and Dacheng Tao. Knowledge distillation: A survey.  International Journal of Computer Vision , 129:1789–1819, 2021. (Cited on page  4 ) Yves Grandvalet and Yoshua Bengio. Semi-supervised learning by entropy minimization.  Advances in neural information processing systems , 17, 2004. (Cited on page  4 ,  10 ,  33 ,  34 ) Sheng Guo, Weilin Huang, Haozhi Zhang, Chenfan Zhuang, Dengke Dong, Matthew R. Scott, and Dinglong Huang. Curriculum Net: Weakly Supervised Learning from Large-Scale Web Images. In  Proceedings of the European Conference on Computer Vision (ECCV) , 2018. (Cited on page 4 ) Bo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor Tsang, and Masashi Sugiyama. Co-teaching: Robust training of deep neural networks with extremely noisy labels. Advances in neural information processing systems , 31, 2018. (Cited on page  4 ) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog- nition. In  Proceedings of the IEEE conference on computer vision and pattern recognition , pp. 770–778, 2016. (Cited on page  40 ) "}
{"page": 21, "image_path": "doc_images/2312.09390v1_21.jpg", "ocr_text": "Dan Hendrycks, Mantas Mazeika, Duncan Wilson, and Kevin Gimpel. Using trusted data to train\ndeep networks on labels corrupted by severe noise. Advances in neural information processing\nsystems, 31, 2018. (Cited on page 4)\n\nDan Hendrycks, Kimin Lee, and Mantas Mazeika. Using pre-training can improve model robustness\nand uncertainty. In International conference on machine learning, pp. 2712-2721. PMLR, 2019.\n(Cited on page 4)\n\nDan Hendrycks, Collin Burns, Steven Basart, Andrew Critch, Jerry Li, Dawn Song, and Jacob\nSteinhardt. Aligning AI with shared human values. arXiv preprint arXiv:2008.02275, 2020a.\n(Cited on page 29)\n\nDan Hendrycks, Xiaoyuan Liu, Eric Wallace, Adam Dziedzic, Rishabh Krishnan, and Dawn Song.\nPretrained transformers improve out-of-distribution robustness. arXiv preprint arXiv:2004.06100,\n2020b. (Cited on page 4)\n\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song,\nand Jacob Steinhardt. Measuring Mathematical Problem Solving With the MATH Dataset. Sort,\n2(4):0-6, 2021. (Cited on page 40)\n\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv\npreprint arXiv: 1503.02531, 2015. (Cited on page 4)\n\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nand Weizhu Chen. LoRA: Low-Rank Adaptation of Large Language Models. In International\nConference on Learning Representations, 2022. (Cited on page 35)\n\nLifu Huang, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Cosmos QA: Machine reading\ncomprehension with contextual commonsense reasoning. arXiv preprint arXiv: 1909.00277, 2019.\n(Cited on page 29)\n\nEvan Hubinger, Chris van Merwijk, Vladimir Mikulik, Joar Skalse, and Scott Garrabrant. Risks from\nlearned optimization in advanced machine learning systems. arXiv preprint arXiv:1906.01820,\n2019. (Cited on page 2, 48)\n\nGeoffrey Irving, Paul Christiano, and Dario Amodei. AI safety via debate. arXiv preprint\narXiv: 1805.00899, 2018. (Cited on page 2, 5)\n\nPavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, and Andrew Gordon Wil-\nson. Averaging weights leads to wider optima and better generalization. arXiv preprint\narXiv: 1803.05407, 2018. (Cited on page 36)\n\nFereshte Khani, Aditi Raghunathan, and Percy Liang. Maximum weighted loss discrepancy. arXiv\npreprint arXiv:1906.03518, 2019. (Cited on page 5)\n\nDaniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, and Dan Roth. Look-\ning beyond the surface: A challenge set for reading comprehension over multiple sentences. In\nProceedings of the 2018 Conference of the North American Chapter of the Association for Com-\nputational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pp. 252-262,\n2018. (Cited on page 29)\n\nMichael P Kim, Amirata Ghorbani, and James Zou. Multiaccuracy: Black-box post-processing for\nfairness in classification. In Proceedings of the 2019 AAAI/ACM Conference on Al, Ethics, and\nSociety, pp. 247-254, 2019. (Cited on page 5)\n\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\narXiv: 1412.6980, 2014. (Cited on page 40, 41)\n\nDurk P Kingma, Shakir Mohamed, Danilo Jimenez Rezende, and Max Welling. Semi-supervised\n\nlearning with deep generative models. Advances in neural information processing systems, 27,\n2014. (Cited on page 4)\n\n22\n", "vlm_text": "Dan Hendrycks, Mantas Mazeika, Duncan Wilson, and Kevin Gimpel. Using trusted data to train deep networks on labels corrupted by severe noise.  Advances in neural information processing systems , 31, 2018. (Cited on page  4 ) Dan Hendrycks, Kimin Lee, and Mantas Mazeika. Using pre-training can improve model robustness and uncertainty. In  International conference on machine learning , pp. 2712–2721. PMLR, 2019. (Cited on page  4 ) Dan Hendrycks, Collin Burns, Steven Basart, Andrew Critch, Jerry Li, Dawn Song, and Jacob Steinhardt. Aligning AI with shared human values.  arXiv preprint arXiv:2008.02275 , 2020a. (Cited on page  29 ) Dan Hendrycks, Xiaoyuan Liu, Eric Wallace, Adam Dziedzic, Rishabh Krishnan, and Dawn Song. Pretrained transformers improve out-of-distribution robustness.  arXiv preprint arXiv:2004.06100 , 2020b. (Cited on page  4 ) Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring Mathematical Problem Solving With the MATH Dataset.  Sort , 2(4):0–6, 2021. (Cited on page  40 ) Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network.  arXiv preprint arXiv:1503.02531 , 2015. (Cited on page  4 ) Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-Rank Adaptation of Large Language Models. In  International Conference on Learning Representations , 2022. (Cited on page  35 ) Lifu Huang, Ronan Le Bras, Chandra Bhaga va tula, and Yejin Choi. Cosmos QA: Machine reading comprehension with contextual commonsense reasoning.  arXiv preprint arXiv:1909.00277 , 2019. (Cited on page  29 ) Evan Hubinger, Chris van Merwijk, Vladimir Mikulik, Joar Skalse, and Scott Garrabrant. Risks from learned optimization in advanced machine learning systems.  arXiv preprint arXiv:1906.01820 , 2019. (Cited on page  2 ,  48 ) Geoffrey Irving, Paul Christiano, and Dario Amodei. AI safety via debate. arXiv preprint arXiv:1805.00899 , 2018. (Cited on page  2 ,  5 ) Pavel Izmailov, Dmitrii Pod opr ikh in, Timur Garipov, Dmitry Vetrov, and Andrew Gordon Wil- son. Averaging weights leads to wider optima and better generalization. arXiv preprint arXiv:1803.05407 , 2018. (Cited on page  36 ) Fereshte Khani, Aditi Raghu nathan, and Percy Liang. Maximum weighted loss discrepancy.  arXiv preprint arXiv:1906.03518 , 2019. (Cited on page  5 ) Daniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, and Dan Roth. Look- ing beyond the surface: A challenge set for reading comprehension over multiple sentences. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Com- putational Linguistics: Human Language Technologies, Volume 1 (Long Papers) , pp. 252–262, 2018. (Cited on page  29 ) Michael P Kim, Amirata Ghorbani, and James Zou. Multi accuracy: Black-box post-processing for fairness in classification. In  Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society , pp. 247–254, 2019. (Cited on page  5 ) Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization.  arXiv preprint arXiv:1412.6980 , 2014. (Cited on page  40 ,  41 ) Durk P Kingma, Shakir Mohamed, Danilo Jimenez Rezende, and Max Welling. Semi-supervised learning with deep generative models.  Advances in neural information processing systems , 27, 2014. (Cited on page  4 ) "}
{"page": 22, "image_path": "doc_images/2312.09390v1_22.jpg", "ocr_text": "Polina Kirichenko, Pavel Izmailov, and Andrew Gordon Wilson. Last Layer Re-Training is Suf-\nficient for Robustness to Spurious Correlations. In The Eleventh International Conference on\nLearning Representations, 2023. (Cited on page 4, 16)\n\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton. Imagenet classification with deep convolu-\ntional neural networks. Advances in neural information processing systems, 25, 2012. (Cited on\npage 17, 40)\n\nAnders Krogh and John Hertz. A simple weight decay can improve generalization. Advances in\nneural information processing systems, 4, 1991. (Cited on page 35)\n\nAnanya Kumar, Aditi Raghunathan, Robbie Matthew Jones, Tengyu Ma, and Percy Liang. Fine-\nTuning can Distort Pretrained Features and Underperform Out-of-Distribution. In /nternational\nConference on Learning Representations, 2022. (Cited on page 4, 35)\n\nSamuli Laine and Timo Aila. Temporal ensembling for semi-supervised learning. arXiv preprint\narXiv: 1610.02242, 2016. (Cited on page 4)\n\nDong-Hyun Lee et al. Pseudo-label: The simple and efficient semi-supervised learning method for\ndeep neural networks. In Workshop on challenges in representation learning, ICML, volume 3,\npp. 896. Atlanta, 2013. (Cited on page 33)\n\nHarrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu, Thomas Mesnard, Colton Bishop, Victor\nCarbune, and Abhinav Rastogi. Rlaif: Scaling reinforcement learning from human feedback with\nAl feedback. arXiv preprint arXiv:2309.00267, 2023. (Cited on page 5)\n\nYoonho Lee, Annie S Chen, Fahim Tajwar, Ananya Kumar, Huaxiu Yao, Percy Liang, and Chelsea\nFinn. Surgical Fine-Tuning Improves Adaptation to Distribution Shifts. In The Eleventh Interna-\ntional Conference on Learning Representations, 2022a. (Cited on page 4)\n\nYoonho Lee, Huaxiu Yao, and Chelsea Finn. Diversify and disambiguate: Learning from under-\nspecified data. arXiv preprint arXiv:2202.03418, 2022b. (Cited on page 18)\n\nJan Leike and Ilya Sutskever. Introducing Superalignment. OpenAI Blog, 2023. (Cited on page 8,\n47)\n\nJan Leike, David Krueger, Tom Everitt, Miljan Martic, Vishal Maini, and Shane Legg. Scalable\nagent alignment via reward modeling: a research direction. arXiv preprint arXiv:1811.07871,\n2018. (Cited on page 2, 5)\n\nJunnan Li, Richard Socher, and Steven CH Hoi. Dividemix: Learning with noisy labels as semi-\nsupervised learning. arXiv preprint arXiv:2002.07394, 2020. (Cited on page 4)\n\nKenneth Li, Oam Patel, Fernanda Viégas, Hanspeter Pfister, and Martin Wattenberg. Inference-\nTime Intervention: Eliciting Truthful Answers from a Language Model. arXiv preprint\narXiv:2306.03341, 2023. (Cited on page 5, 47)\n\nLichess Team. Lichess Database. https: //github.com/lichess-—org/database, 2023.\nAccessed: 2023. (Cited on page 7)\n\nHunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan\nLeike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let’s Verify Step by Step. arXiv preprint\narXiv:2305.20050, 2023. (Cited on page 5)\n\nEvan Z Liu, Behzad Haghgoo, Annie S Chen, Aditi Raghunathan, Pang Wei Koh, Shiori Sagawa,\nPercy Liang, and Chelsea Finn. Just train twice: Improving group robustness without training\ngroup information. In International Conference on Machine Learning, pp. 6781-6792. PMLR,\n2021. (Cited on page 5)\n\nZiquan Liu, Yi Xu, Yuanhong Xu, Qi Qian, Hao Li, Rong Jin, Xiangyang Ji, and Antoni B Chan.\n\nAn empirical study on distribution shift robustness from the perspective of pre-training and data\naugmentation. arXiv preprint arXiv:2205.12753, 2022. (Cited on page 4)\n\n23\n", "vlm_text": "Polina Kirichenko, Pavel Izmailov, and Andrew Gordon Wilson. Last Layer Re-Training is Suf- ficient for Robustness to Spurious Correlations. In  The Eleventh International Conference on Learning Representations , 2023. (Cited on page  4 ,  16 ) Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton. Imagenet classification with deep convolu- tional neural networks.  Advances in neural information processing systems , 25, 2012. (Cited on page  17 ,  40 ) Anders Krogh and John Hertz. A simple weight decay can improve generalization.  Advances in neural information processing systems , 4, 1991. (Cited on page  35 ) Ananya Kumar, Aditi Raghu nathan, Robbie Matthew Jones, Tengyu Ma, and Percy Liang. Fine- Tuning can Distort Pretrained Features and Under perform Out-of-Distribution. In  International Conference on Learning Representations , 2022. (Cited on page  4 ,  35 ) Samuli Laine and Timo Aila. Temporal ensembling for semi-supervised learning.  arXiv preprint arXiv:1610.02242 , 2016. (Cited on page  4 ) Dong-Hyun Lee et al. Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks. In  Workshop on challenges in representation learning, ICML , volume 3, pp. 896. Atlanta, 2013. (Cited on page  33 ) Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu, Thomas Mesnard, Colton Bishop, Victor Carbune, and Abhinav Rastogi. Rlaif: Scaling reinforcement learning from human feedback with AI feedback.  arXiv preprint arXiv:2309.00267 , 2023. (Cited on page  5 ) Yoonho Lee, Annie S Chen, Fahim Tajwar, Ananya Kumar, Huaxiu Yao, Percy Liang, and Chelsea Finn. Surgical Fine-Tuning Improves Adaptation to Distribution Shifts. In  The Eleventh Interna- tional Conference on Learning Representations , 2022a. (Cited on page  4 ) Yoonho Lee, Huaxiu Yao, and Chelsea Finn. Diversify and disambiguate: Learning from under- specified data.  arXiv preprint arXiv:2202.03418 , 2022b. (Cited on page  18 ) Jan Leike and Ilya Sutskever. Introducing Super alignment.  OpenAI Blog , 2023. (Cited on page  8 , 47 ) Jan Leike, David Krueger, Tom Everitt, Miljan Martic, Vishal Maini, and Shane Legg. Scalable agent alignment via reward modeling: a research direction.  arXiv preprint arXiv:1811.07871 , 2018. (Cited on page  2 ,  5 ) Junnan Li, Richard Socher, and Steven CH Hoi. Dividemix: Learning with noisy labels as semi- supervised learning.  arXiv preprint arXiv:2002.07394 , 2020. (Cited on page  4 ) Kenneth Li, Oam Patel, Fernanda Vi´ egas, Hanspeter Pfister, and Martin Wattenberg. Inference- Time Intervention: Eliciting Truthful Answers from a Language Model. arXiv preprint arXiv:2306.03341 , 2023. (Cited on page  5 ,  47 ) Lichess Team. Lichess Database.  https://github.com/lichess-org/database , 2023. Accessed: 2023. (Cited on page  7 ) Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let’s Verify Step by Step.  arXiv preprint arXiv:2305.20050 , 2023. (Cited on page  5 ) Evan Z Liu, Behzad Haghgoo, Annie S Chen, Aditi Raghu nathan, Pang Wei Koh, Shiori Sagawa, Percy Liang, and Chelsea Finn. Just train twice: Improving group robustness without training group information. In  International Conference on Machine Learning , pp. 6781–6792. PMLR, 2021. (Cited on page  5 ) Ziquan Liu, Yi Xu, Yuanhong Xu, Qi Qian, Hao Li, Rong Jin, Xiangyang Ji, and Antoni B Chan. An empirical study on distribution shift robustness from the perspective of pre-training and data augmentation.  arXiv preprint arXiv:2205.12753 , 2022. (Cited on page  4 ) "}
{"page": 23, "image_path": "doc_images/2312.09390v1_23.jpg", "ocr_text": "Xingjun Ma, Hanxun Huang, Yisen Wang, Simone Romano, Sarah Erfani, and James Bailey. Nor-\nmalized loss functions for deep learning with noisy labels. In International conference on machine\nlearning, pp. 6543-6553. PMLR, 2020. (Cited on page 4)\n\nIan R McKenzie, Alexander Lyzhov, Michael Pieler, Alicia Parrish, Aaron Mueller, Ameya Prabhu,\nEuan McLean, Aaron Kirtland, Alexis Ross, Alisa Liu, et al. Inverse Scaling: When Bigger Isn’t\nBetter. arXiv preprint arXiv:2306.09479, 2023. (Cited on page 8, 13)\n\nKevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and editing factual\nassociations in GPT. Advances in Neural Information Processing Systems, 35:17359-17372,\n2022. (Cited on page 5)\n\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a Suit of Armor Conduct\nElectricity? A New Dataset for Open Book Question Answering. In EMNLP, 2018. (Cited on\npage 29)\n\nRichard Ngo, Lawrence Chan, and Séren Mindermann. The alignment problem from a deep learning\nperspective. arXiv preprint arXiv:2209.00626, 2022. (Cited on page 48)\n\nYixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela. Adversar-\nial NLI: A new benchmark for natural language understanding. arXiv preprint arXiv:1910.14599,\n2019. (Cited on page 29)\n\nChris Olah, Arvind Satyanarayan, Ian Johnson, Shan Carter, Ludwig Schubert, Katherine\nYe, and Alexander Mordvintsev. The Building Blocks of Interpretability. Distill, 2018.\nhttps://distill_pub/2018/building-blocks. (Cited on page 47)\n\nOpenAl. GPT-4 Technical Report. arXiv preprint arXiv:2303.08774, 2023. (Cited on page 2, 7, 28)\n\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow\ninstructions with human feedback. Advances in Neural Information Processing Systems, 35:\n27730-27744, 2022. (Cited on page 1, 2, 5, 7, 32, 47)\n\nLorenzo Pacchiardi, Alex J Chan, Séren Mindermann, Ilan Moscovitz, Alexa Y Pan, Yarin Gal,\nOwain Evans, and Jan Brauner. How to catch an AI liar: Lie detection in black-box llms by\nasking unrelated questions. arXiv preprint arXiv:2309. 15840, 2023. (Cited on page 5)\n\nFabian Pedregosa, Gaél Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier\nGrisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas,\nAlexandre Passos, David Cournapeau, Matthieu Brucher, Matthieu Perrot, and Edouard Duch-\nesnay. Scikit-learn: Machine Learning in Python. Journal of Machine Learning Research, 12\n(85):2825-2830, 2011. (Cited on page 42)\n\nEthan Perez, Saffron Huang, Francis Song, Trevor Cai, Roman Ring, John Aslanides, Amelia\nGlaese, Nat McAleese, and Geoffrey Irving. Red teaming language models with language models.\narXiv preprint arXiv:2202.03286, 2022a. (Cited on page 47)\n\nEthan Perez, Sam Ringer, Kamilé LukoSiiité, Karina Nguyen, Edwin Chen, Scott Heiner, Craig\nPettit, Catherine Olsson, Sandipan Kundu, Saurav Kadavath, et al. Discovering language model\nbehaviors with model-written evaluations. arXiv preprint arXiv:2212.09251, 2022b. (Cited on\npage 47)\n\nMohammad Taher Pilehvar and Jose Camacho-Collados. WiC: the word-in-context dataset for eval-\nuating context-sensitive meaning representations. arXiv preprint arXiv: 1808.09121, 2018. (Cited\non page 29)\n\nAlec Radford, Rafal Jozefowicz, and Ilya Sutskever. Learning to generate reviews and discovering\nsentiment. arXiv preprint arXiv: 1704.01444, 2017. (Cited on page 14)\n\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\n\nmodels from natural language supervision. In International conference on machine learning, pp.\n8748-8763. PMLR, 2021. (Cited on page 4)\n\n24\n", "vlm_text": "Xingjun Ma, Hanxun Huang, Yisen Wang, Simone Romano, Sarah Erfani, and James Bailey. Nor- malized loss functions for deep learning with noisy labels. In  International conference on machine learning , pp. 6543–6553. PMLR, 2020. (Cited on page  4 ) Ian R McKenzie, Alexander Lyzhov, Michael Pieler, Alicia Parrish, Aaron Mueller, Ameya Prabhu, Euan McLean, Aaron Kirtland, Alexis Ross, Alisa Liu, et al. Inverse Scaling: When Bigger Isn’t Better.  arXiv preprint arXiv:2306.09479 , 2023. (Cited on page  8 ,  13 ) Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and editing factual associations in GPT. Advances in Neural Information Processing Systems , 35:17359–17372, 2022. (Cited on page  5 ) Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering. In  EMNLP , 2018. (Cited on page  29 ) Richard Ngo, Lawrence Chan, and S¨ oren Mindermann. The alignment problem from a deep learning perspective.  arXiv preprint arXiv:2209.00626 , 2022. (Cited on page  48 ) Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela. Adversar- ial NLI: A new benchmark for natural language understanding.  arXiv preprint arXiv:1910.14599 , 2019. (Cited on page  29 ) Chris Olah, Arvind Satya narayan, Ian Johnson, Shan Carter, Ludwig Schubert, Katherine Ye, and Alexander Mor dv in tse v. The Building Blocks of Interpret ability. Distill , 2018. https://distill.pub/2018/building-blocks. (Cited on page  47 ) OpenAI. GPT-4 Technical Report.  arXiv preprint arXiv:2303.08774 , 2023. (Cited on page  2 ,  7 ,  28 ) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems , 35: 27730–27744, 2022. (Cited on page  1 ,  2 ,  5 ,  7 ,  32 ,  47 ) Lorenzo Pacchiardi, Alex J Chan, S¨ oren Mindermann, Ilan Moscovitz, Alexa Y Pan, Yarin Gal, Owain Evans, and Jan Brauner. How to catch an AI liar: Lie detection in black-box llms by asking unrelated questions.  arXiv preprint arXiv:2309.15840 , 2023. (Cited on page  5 ) Fabian Pedregosa, Ga¨ el Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Pre tten hofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David Cournapeau, Matthieu Brucher, Matthieu Perrot, and  Edouard Duch- esnay. Scikit-learn: Machine Learning in Python.  Journal of Machine Learning Research , 12 (85):2825–2830, 2011. (Cited on page  42 ) Ethan Perez, Saffron Huang, Francis Song, Trevor Cai, Roman Ring, John Aslanides, Amelia Glaese, Nat McAleese, and Geoffrey Irving. Red teaming language models with language models. arXiv preprint arXiv:2202.03286 , 2022a. (Cited on page  47 ) Ethan Perez, Sam Ringer, Kamil˙ e Lukoˇ si¯ ut˙ e, Karina Nguyen, Edwin Chen, Scott Heiner, Craig Pettit, Catherine Olsson, Sandipan Kundu, Saurav Kadavath, et al. Discovering language model behaviors with model-written evaluations.  arXiv preprint arXiv:2212.09251 , 2022b. (Cited on page  47 ) Mohammad Taher Pilehvar and Jose Camacho-Collados. WiC: the word-in-context dataset for eval- uating context-sensitive meaning representations.  arXiv preprint arXiv:1808.09121 , 2018. (Cited on page  29 ) Alec Radford, Rafal Jozefowicz, and Ilya Sutskever. Learning to generate reviews and discovering sentiment.  arXiv preprint arXiv:1704.01444 , 2017. (Cited on page  14 ) Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In  International conference on machine learning , pp. 8748–8763. PMLR, 2021. (Cited on page  4 ) "}
{"page": 24, "image_path": "doc_images/2312.09390v1_24.jpg", "ocr_text": "Alexander Ratner, Stephen H Bach, Henry Ehrenberg, Jason Fries, Sen Wu, and Christopher Ré.\nSnorkel: Rapid training data creation with weak supervision. In Proceedings of the VLDB En-\ndowment. International Conference on Very Large Data Bases, volume 11, pp. 269. NIH Public\nAccess, 2017. (Cited on page 4)\n\nScott Reed, Honglak Lee, Dragomir Anguelov, Christian Szegedy, Dumitru Erhan, and Andrew\nRabinovich. Training deep neural networks on noisy labels with bootstrapping. arXiv preprint\narXiv: 1412.6596, 2014. (Cited on page 4, 33)\n\nMarco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. ” Why should i trust you?” Explaining the\npredictions of any classifier. In Proceedings of the 22nd ACM SIGKDD international conference\non knowledge discovery and data mining, pp. 1135-1144, 2016. (Cited on page 47)\n\nFabien Roger, Ryan Greenblatt, Max Nadeau, Buck Shlegeris, and Nate Thomas. Measurement\ntampering detection benchmark. arXiv preprint arXiv:2308.15605, 2023. (Cited on page 5)\n\nAnna Rogers, Olga Kovaleva, Matthew Downey, and Anna Rumshisky. Getting closer to AI com-\nplete question answering: A set of prerequisite real tasks. In Proceedings of the AAAI conference\non artificial intelligence, volume 34, pp. 8722-8731, 2020. (Cited on page 29)\n\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng\nHuang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual\nrecognition challenge. International journal of computer vision, 115:211-252, 2015. (Cited on\npage 40)\n\nShiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. Distributionally robust\nneural networks for group shifts: On the importance of regularization for worst-case generaliza-\ntion. arXiv preprint arXiv: 1911.08731, 2019. (Cited on page 4)\n\nShibani Santurkar, Dimitris Tsipras, Mahalaxmi Elango, David Bau, Antonio Torralba, and Alek-\nsander Madry. Editing a classifier by rewriting its prediction rules. Advances in Neural Informa-\ntion Processing Systems, 34:23359-23373, 2021. (Cited on page 5)\n\nMaarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, and Yejin Choi. Socialiga: Common-\nsense reasoning about social interactions. arXiv preprint arXiv: 1904.09728, 2019. (Cited on page\n29)\n\nWilliam Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan Ward, and Jan\nLeike. Self-critiquing models for assisting human evaluators. arXiv preprint arXiv:2206.05802,\n2022. (Cited on page 2, 5, 47)\n\nAvi Schwarzschild, Eitan Borgnia, Arjun Gupta, Arpit Bansal, Zeyad Emam, Furong Huang, Micah\nGoldblum, and Tom Goldstein. Datasets for studying generalization from easy to hard examples.\narXiv preprint arXiv:2108.06011, 2021a. (Cited on page 29)\n\nAvi Schwarzschild, Eitan Borgnia, Arjun Gupta, Furong Huang, Uzi Vishkin, Micah Goldblum,\nand Tom Goldstein. Can you learn an algorithm? generalizing from easy to hard problems with\nrecurrent networks. Advances in Neural Information Processing Systems, 34:6695—6706, 2021b.\n(Cited on page 7, 29)\n\nRui Shu, Hung Bui, Hirokazu Narui, and Stefano Ermon. A DIRT-T Approach to Unsupervised\nDomain Adaptation. In International Conference on Learning Representations, 2018. (Cited on\npage 4, 33)\n\nRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng,\nand Christopher Potts. Recursive deep models for semantic compositionality over a sentiment\ntreebank. In Proceedings of the 2013 conference on empirical methods in natural language pro-\ncessing, pp. 1631-1642, 2013. (Cited on page 29)\n\nNimit Sohoni, Jared Dunnmon, Geoffrey Angus, Albert Gu, and Christopher Ré. No subclass left\n\nbehind: Fine-grained robustness in coarse-grained classification problems. Advances in Neural\nInformation Processing Systems, 33:19339-19352, 2020. (Cited on page 5)\n\n25\n", "vlm_text": "Alexander Ratner, Stephen H Bach, Henry Ehrenberg, Jason Fries, Sen Wu, and Christopher R´ e. Snorkel: Rapid training data creation with weak supervision. In  Proceedings of the VLDB En- dowment. International Conference on Very Large Data Bases , volume 11, pp. 269. NIH Public Access, 2017. (Cited on page  4 ) Scott Reed, Honglak Lee, Dragomir Anguelov, Christian Szegedy, Dumitru Erhan, and Andrew Rabinovich. Training deep neural networks on noisy labels with boots trapping.  arXiv preprint arXiv:1412.6596 , 2014. (Cited on page  4 ,  33 ) Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. ” Why should i trust you?” Explaining the predictions of any classifier. In  Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining , pp. 1135–1144, 2016. (Cited on page  47 ) Fabien Roger, Ryan Greenblatt, Max Nadeau, Buck Shlegeris, and Nate Thomas. Measurement tampering detection benchmark.  arXiv preprint arXiv:2308.15605 , 2023. (Cited on page  5 ) Anna Rogers, Olga Kovaleva, Matthew Downey, and Anna Rumshisky. Getting closer to AI com- plete question answering: A set of prerequisite real tasks. In  Proceedings of the AAAI conference on artificial intelligence , volume 34, pp. 8722–8731, 2020. (Cited on page  29 ) Olga Russ a kov sky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge.  International journal of computer vision , 115:211–252, 2015. (Cited on page  40 ) Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. Distribution ally robust neural networks for group shifts: On the importance of regular iz ation for worst-case generaliza- tion.  arXiv preprint arXiv:1911.08731 , 2019. (Cited on page  4 ) Shibani Santurkar, Dimitris Tsipras, Mahalaxmi Elango, David Bau, Antonio Torralba, and Alek- sander Madry. Editing a classifier by rewriting its prediction rules.  Advances in Neural Informa- tion Processing Systems , 34:23359–23373, 2021. (Cited on page  5 ) Maarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, and Yejin Choi. Socialiqa: Common- sense reasoning about social interactions.  arXiv preprint arXiv:1904.09728 , 2019. (Cited on page 29 ) William Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan Ward, and Jan Leike. Self-critiquing models for assisting human evaluators.  arXiv preprint arXiv:2206.05802 , 2022. (Cited on page  2 ,  5 ,  47 ) Avi Schwarz s child, Eitan Borgnia, Arjun Gupta, Arpit Bansal, Zeyad Emam, Furong Huang, Micah Goldblum, and Tom Goldstein. Datasets for studying generalization from easy to hard examples. arXiv preprint arXiv:2108.06011 , 2021a. (Cited on page  29 ) Avi Schwarz s child, Eitan Borgnia, Arjun Gupta, Furong Huang, Uzi Vishkin, Micah Goldblum, and Tom Goldstein. Can you learn an algorithm? generalizing from easy to hard problems with recurrent networks.  Advances in Neural Information Processing Systems , 34:6695–6706, 2021b. (Cited on page  7 ,  29 ) Rui Shu, Hung Bui, Hirokazu Narui, and Stefano Ermon. A DIRT-T Approach to Unsupervised Domain Adaptation. In  International Conference on Learning Representations , 2018. (Cited on page  4 ,  33 ) Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. Recursive deep models for semantic compositional it y over a sentiment treebank. In  Proceedings of the 2013 conference on empirical methods in natural language pro- cessing , pp. 1631–1642, 2013. (Cited on page  29 ) Nimit Sohoni, Jared Dunnmon, Geoffrey Angus, Albert Gu, and Christopher R´ e. No subclass left behind: Fine-grained robustness in coarse-grained classification problems.  Advances in Neural Information Processing Systems , 33:19339–19352, 2020. (Cited on page  5 ) "}
{"page": 25, "image_path": "doc_images/2312.09390v1_25.jpg", "ocr_text": "Hwanjun Song, Minseok Kim, Dongmin Park, Yooju Shin, and Jae-Gil Lee. Learning from noisy\nlabels with deep neural networks: A survey. JEEE Transactions on Neural Networks and Learning\nSystems, 2022. (Cited on page 4)\n\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.\nDropout: a simple way to prevent neural networks from overfitting. The journal of machine\nlearning research, 15(1):1929-1958, 2014. (Cited on page 35)\n\nSamuel Stanton, Pavel Izmailov, Polina Kirichenko, Alexander A Alemi, and Andrew G Wilson.\nDoes knowledge distillation really work? Advances in Neural Information Processing Systems,\n34:6906-6919, 2021. (Cited on page 4, 13)\n\nJacob Steinhardt. AI Forecasting: One Year In, 2022. (Cited on page 18)\n\nNisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford,\nDario Amodei, and Paul Christiano. Learning to summarize with human feedback. Advances in\nNeural Information Processing Systems, 33:3008—3021, 2020. (Cited on page 1, 5, 15, 32)\n\nKai Sun, Dian Yu, Jianshu Chen, Dong Yu, Yejin Choi, and Claire Cardie. Dream: A challenge data\nset and models for dialogue-based reading comprehension. Transactions of the Association for\nComputational Linguistics, 7:217—231, 2019. (Cited on page 29)\n\nOyvind Tafjord, Matt Gardner, Kevin Lin, and Peter Clark. Quartz: An open-domain dataset of\nqualitative relationship questions. arXiv preprint arXiv:1909.03553, 2019. (Cited on page 29)\n\nAntti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged con-\nsistency targets improve semi-supervised deep learning results. Advances in neural information\nprocessing systems, 30, 2017. (Cited on page 4)\n\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. GLUE:\nA multi-task benchmark and analysis platform for natural language understanding. arXiv preprint\narXiv: 1804.07461, 2018. (Cited on page 29)\n\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer\nLevy, and Samuel Bowman. Superglue: A stickier benchmark for general-purpose language\nunderstanding systems. Advances in neural information processing systems, 32, 2019. (Cited on\npage 29)\n\nAlex Warstadt, Amanpreet Singh, and Samuel Bowman. Neural network acceptability judgments.\nTransactions of the Association for Computational Linguistics, 7:625-641, 2019. (Cited on page\n29)\n\nColin Wei, Kendrick Shen, Yining Chen, and Tengyu Ma. Theoretical Analysis of Self-Training with\nDeep Networks on Unlabeled Data. In International Conference on Learning Representations,\n2020. (Cited on page 33)\n\nJason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du,\nAndrew M Dai, and Quoc V Le. Finetuned Language Models are Zero-Shot Learners. In Inter-\nnational Conference on Learning Representations, 2021. (Cited on page 28, 29)\n\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yo-\ngatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language\nmodels. arXiv preprint arXiv:2206.07682, 2022. (Cited on page 46)\n\nJohannes Welbl, Nelson F Liu, and Matt Gardner. Crowdsourcing multiple choice science questions.\narXiv preprint arXiv:1707.06209, 2017. (Cited on page 29)\n\nJohn Wentworth. Alignment by Default. AJ Alignment Forum, 2020. (Cited on page 5)\n\nGordon Seidoh Worley. Bootstrapped Alignment. AJ Alignment Forum, 2021. (Cited on page 8)\n\n26\n", "vlm_text": "Hwanjun Song, Minseok Kim, Dongmin Park, Yooju Shin, and Jae-Gil Lee. Learning from noisy labels with deep neural networks: A survey.  IEEE Transactions on Neural Networks and Learning Systems , 2022. (Cited on page  4 ) \nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salak hut dino v. Dropout: a simple way to prevent neural networks from over fitting.  The journal of machine learning research , 15(1):1929–1958, 2014. (Cited on page  35 ) \nSamuel Stanton, Pavel Izmailov, Polina Kirichenko, Alexander A Alemi, and Andrew G Wilson. Does knowledge distillation really work?  Advances in Neural Information Processing Systems , 34:6906–6919, 2021. (Cited on page  4 ,  13 ) \nJacob Steinhardt. AI Forecasting: One Year In, 2022. (Cited on page  18 ) \nNisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano. Learning to summarize with human feedback.  Advances in Neural Information Processing Systems , 33:3008–3021, 2020. (Cited on page  1 ,  5 ,  15 ,  32 ) Kai Sun, Dian Yu, Jianshu Chen, Dong Yu, Yejin Choi, and Claire Cardie. Dream: A challenge data set and models for dialogue-based reading comprehension.  Transactions of the Association for Computational Linguistics , 7:217–231, 2019. (Cited on page  29 ) Oyvind Tafjord, Matt Gardner, Kevin Lin, and Peter Clark. Quartz: An open-domain dataset of qualitative relationship questions.  arXiv preprint arXiv:1909.03553 , 2019. (Cited on page  29 ) Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged con- sistency targets improve semi-supervised deep learning results.  Advances in neural information processing systems , 30, 2017. (Cited on page  4 ) Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding.  arXiv preprint arXiv:1804.07461 , 2018. (Cited on page  29 ) Alex Wang, Yada Pr uk s a chat kun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. Superglue: A stickier benchmark for general-purpose language understanding systems.  Advances in neural information processing systems , 32, 2019. (Cited on page  29 ) Alex Warstadt, Amanpreet Singh, and Samuel Bowman. Neural network acceptability judgments. Transactions of the Association for Computational Linguistics , 7:625–641, 2019. (Cited on page 29 ) Colin Wei, Kendrick Shen, Yining Chen, and Tengyu Ma. Theoretical Analysis of Self-Training with Deep Networks on Unlabeled Data. In  International Conference on Learning Representations , 2020. (Cited on page  33 ) Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned Language Models are Zero-Shot Learners. In  Inter- national Conference on Learning Representations , 2021. (Cited on page  28 ,  29 ) Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yo- gatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models.  arXiv preprint arXiv:2206.07682 , 2022. (Cited on page  46 ) Johannes Welbl, Nelson F Liu, and Matt Gardner. Crowd sourcing multiple choice science questions. arXiv preprint arXiv:1707.06209 , 2017. (Cited on page  29 ) John Wentworth. Alignment by Default.  AI Alignment Forum , 2020. (Cited on page  5 ) \nGordon Seidoh Worley. Boots trapped Alignment.  AI Alignment Forum , 2021. (Cited on page  8 ) "}
{"page": 26, "image_path": "doc_images/2312.09390v1_26.jpg", "ocr_text": "Mitchell Wortsman, Gabriel Ilharco, Samir Ya Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes,\nAri S Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, et al. Model\nsoups: averaging weights of multiple fine-tuned models improves accuracy without increasing\ninference time. In International Conference on Machine Learning, pp. 23965-23998. PMLR,\n2022a. (Cited on page 4, 36)\n\nMitchell Wortsman, Gabriel Ilharco, Jong Wook Kim, Mike Li, Simon Kornblith, Rebecca Roelofs,\nRaphael Gontijo Lopes, Hannaneh Hajishirzi, Ali Farhadi, Hongseok Namkoong, et al. Robust\nfine-tuning of zero-shot models. In Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pp. 7959-7971, 2022b. (Cited on page 4, 36)\n\nJeff Wu, Long Ouyang, Daniel M Ziegler, Nisan Stiennon, Ryan Lowe, Jan Leike, and Paul Chris-\ntiano. Recursively summarizing books with human feedback. arXiv preprint arXiv:2109.10862,\n2021. (Cited on page 2)\n\nQizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V Le. Self-training with noisy student\nimproves imagenet classification. In Proceedings of the IEEE/CVF conference on computer vision\nand pattern recognition, pp. 10687-10698, 2020. (Cited on page 4, 33)\n\nKun Yi and Jianxin Wu. Probabilistic End-To-End Noise Correction for Learning With Noisy La-\nbels. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR), 2019. (Cited on page 4)\n\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can a Ma-\nchine Really Finish Your Sentence? In Proceedings of the 57th Annual Meeting of the Association\nfor Computational Linguistics, 2019. (Cited on page 29)\n\nBrian Hu Zhang, Blake Lemoine, and Margaret Mitchell. Mitigating unwanted biases with adver-\nsarial learning. In Proceedings of the 2018 AAAI/ACM Conference on Al, Ethics, and Society, pp.\n335-340, 2018. (Cited on page 5)\n\nYuan Zhang, Jason Baldridge, and Luheng He. PAWS: Paraphrase Adversaries from Word Scram-\nbling. In Proc. of NAACL, 2019. (Cited on page 29)\n\nZhilu Zhang and Mert Sabuncu. Generalized cross entropy loss for training deep neural networks\nwith noisy labels. Advances in neural information processing systems, 31, 2018. (Cited on page\n4, 36)\n\nBen Zhou, Daniel Khashabi, Qiang Ning, and Dan Roth. “Going on a vacation” takes longer than\n“Going for a walk”: A Study of Temporal Commonsense Understanding. In EMNLP, 2019.\n(Cited on page 29)\n\n27\n", "vlm_text": "Mitchell Wortsman, Gabriel Ilharco, Samir Ya Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes, Ari S Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, et al. Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. In  International Conference on Machine Learning , pp. 23965–23998. PMLR, 2022a. (Cited on page  4 ,  36 ) Mitchell Wortsman, Gabriel Ilharco, Jong Wook Kim, Mike Li, Simon Kornblith, Rebecca Roelofs, Raphael Gontijo Lopes, Hannaneh Hajishirzi, Ali Farhadi, Hongseok Namkoong, et al. Robust fine-tuning of zero-shot models. In  Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 7959–7971, 2022b. (Cited on page  4 ,  36 ) Jeff Wu, Long Ouyang, Daniel M Ziegler, Nisan Stiennon, Ryan Lowe, Jan Leike, and Paul Chris- tiano. Recursively summarizing books with human feedback.  arXiv preprint arXiv:2109.10862 , 2021. (Cited on page  2 ) Qizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V Le. Self-training with noisy student improves imagenet classification. In  Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pp. 10687–10698, 2020. (Cited on page  4 ,  33 ) Kun Yi and Jianxin Wu. Probabilistic End-To-End Noise Correction for Learning With Noisy La- bels. In  Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , 2019. (Cited on page  4 ) Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can a Ma- chine Really Finish Your Sentence? In  Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , 2019. (Cited on page  29 ) Brian Hu Zhang, Blake Lemoine, and Margaret Mitchell. Mitigating unwanted biases with adver- sarial learning. In  Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society , pp. 335–340, 2018. (Cited on page  5 ) Yuan Zhang, Jason Baldridge, and Luheng He. PAWS: Paraphrase Adversaries from Word Scram- bling. In  Proc. of NAACL , 2019. (Cited on page  29 ) Zhilu Zhang and Mert Sabuncu. Generalized cross entropy loss for training deep neural networks with noisy labels.  Advances in neural information processing systems , 31, 2018. (Cited on page 4 ,  36 ) Ben Zhou, Daniel Khashabi, Qiang Ning, and Dan Roth. “Going on a vacation” takes longer than “Going for a walk”: A Study of Temporal Commonsense Understanding. In  EMNLP , 2019. (Cited on page  29 ) "}
{"page": 27, "image_path": "doc_images/2312.09390v1_27.jpg", "ocr_text": "APPENDIX OUTLINE\n\n¢ In Appendix A, we provide additional details on our setup and experiments.\n\n¢ In Appendix B, we describe additional results, including negative results and methods that\ndid not work well in our experiments.\n\n¢ In Appendix C, we report results on easy-to-hard generalization, where we only provide\nsupervision on easy examples.\n\n¢ In Appendix D, we provide results in two more weak-to-strong learning settings: a self-\nsupervised computer vision setting on ImageNet, and a pure linear probing setting.\n\n¢ In Appendix E, we provide additional results and discussion on the effect of weak supervi-\nsor error simulation.\n\n¢ In Appendix F, we discuss how we believe methodological progress should be made on\nsuperalignment.\n\n¢ In Appendix G, we describe how our work fits into the bigger picture of alignment.\n\nA FURTHER EXPERIMENTAL DETAILS\n\nHere, we provide further details on our experiments. Across all tasks, we use pretrained base models\nfrom the GPT-4 family (OpenAI, 2023), spanning a range of model sizes.\n\nA.1 NLP TASKS\n\nData preprocessing. We use popular NLP classification benchmark datasets listed in Table 1. We\nobfuscate the names of the datasets in our plots (e.g. Figure 12) for confidentiality; across all figures,\nwe replace the names of the datasets with their order in a randomized sequence. We apply various\npreprocessing to the datasets. For example, some tasks are in FLAN (Wei et al., 2021) and we use\ntheir preprocessing. For ANLI we group neutral entailments with contradictions. We convert each\ndataset to a binary classification problem. For multiple-choice datasets, suppose each datapoint has\na question Q and multiple candidate answers Ai,...,A,. We then convert this datapoint to k new\ndatapoints of the form (Q, A;), where the label is 0 for all incorrect answers A; and 1 for the correct\nanswers. In this procedure, we also aim to maintain class balance, so we keep the same number\nof correct and wrong answers per question®. We are also additionally rebalancing the classes in\ndatasets where one of the classes represents more than 55% of the data. To do so, we randomly drop\ndatapoints from the dominant class, so that the classes are perfectly balanced.\n\nModels. In order to adapt our language models to the classification setting, we replace the un-\nembedding layer of the model with a linear classification head with two outputs. We initialize the\nweights of the classification head with the unembedding weights for tokens “0” and “1”.\n\nTraining hyperparameters. We finetune all models for 2 epochs using a batch size of 32. In\nthe weak-to-strong generalization experiments, we early stop training based on the accuracy with\nrespect to the weak labels on a held-out validation set. See Section 5.1.1 for relevant discussion.\nWe only tuned the hyper-parameters of our methods on smaller model sizes, and on a subset of 8\ndatasets. The full GPT-4 model and most of the datasets were held-out, except for datasets [5—12]\n(see Figure 12).\n\nWeak labels. To produce the weak labels, we split the original dataset in half. We ensure that\nrelated datapoints, e.g. datapoints that share the same question or premise, are always grouped to-\ngether into the same half. Then, we train the weak supervisor model on the first half of the dataset,\nand use its prediction on the other half as the weak labels. We additionally save the weak labels on\nthe test set to evaluate metrics such as agreement in Section 5.1.3. The weak labels are soft labels\non the training data, i.e. the class probabilities predicted by the supervisor.\n\nEvaluation. For all datasets, we report accuracy on the test set which is also balanced to have an\nequal number of datapoints in each class. In particular, random guess performance corresponds to\n50% accuracy on all NLP datasets.\n\n®In some datasets there are multiple correct answers for each question.\n\n28\n", "vlm_text": "• In Appendix  A , we provide additional details on our setup and experiments. • In Appendix  B , we describe additional results, including negative results and methods that did not work well in our experiments. • In Appendix  C , we report results on easy-to-hard generalization, where we only provide supervision on easy examples. • In Appendix  D , we provide results in two more weak-to-strong learning settings: a self- supervised computer vision setting on ImageNet, and a pure linear probing setting. • In Appendix  E , we provide additional results and discussion on the effect of weak supervi- sor error simulation. • In Appendix  F , we discuss how we believe methodological progress should be made on super alignment. • In Appendix  G , we describe how our work fits into the bigger picture of alignment. \nA F URTHER EXPERIMENTAL DETAILS \nHere, we provide further details on our experiments. Across all tasks, we use pretrained base models from the GPT-4 family ( OpenAI ,  2023 ), spanning a range of model sizes. \nA.1 NLP T ASKS \nData preprocessing. We use popular NLP classification benchmark datasets listed in Table  1 . We obfuscate the names of the datasets in our plots (e.g. Figure  12 ) for confidentiality; across all figures, we replace the names of the datasets with their order in a randomized sequence. We apply various preprocessing to the datasets. For example, some tasks are in FLAN ( Wei et al. ,  2021 ) and we use their preprocessing. For ANLI we group neutral ent ailments with contradictions. We convert each dataset to a binary classification problem. For multiple-choice datasets, suppose each datapoint has a question  $Q$   and multiple candidate answers    $A_{1},\\ldots,A_{k}$  . We then convert this datapoint to    $k$   new datapoints of the form    $(Q,A_{i})$  , where the label is  0  for all incorrect answers    $A_{i}$   and  1  for the correct answers. In this procedure, we also aim to maintain class balance, so we keep the same number of correct and wrong answers per question 6 . We are also additionally rebalancing the classes in datasets where one of the classes represents more than    $55\\%$   of the data. To do so, we randomly drop datapoints from the dominant class, so that the classes are perfectly balanced. \nModels. In order to adapt our language models to the classification setting, we replace the un- embedding layer of the model with a linear classification head with two outputs. We initialize the weights of the classification head with the un embedding weights for tokens  $\"0\"$   and “1”. \nTraining hyper parameters. We finetune all models for 2 epochs using a batch size of 32. In the weak-to-strong generalization experiments, we early stop training based on the accuracy with respect to the weak labels on a held-out validation set. See Section  5.1.1  for relevant discussion. We only tuned the hyper-parameters of our methods on smaller model sizes, and on a subset of 8 datasets. The full GPT-4 model and most of the datasets were held-out, except for datasets [5–12] (see Figure  12 ). \nWeak labels. To produce the weak labels, we split the original dataset in half. We ensure that related datapoints, e.g. datapoints that share the same question or premise, are always grouped to- gether into the same half. Then, we train the weak supervisor model on the first half of the dataset, and use its prediction on the other half as the weak labels. We additionally save the weak labels on the test set to evaluate metrics such as agreement in Section  5.1.3 . The weak labels are soft labels on the training data, i.e. the class probabilities predicted by the supervisor. \nEvaluation. For all datasets, we report accuracy on the test set which is also balanced to have an equal number of datapoints in each class. In particular, random guess performance corresponds to  $50\\%$   accuracy on all NLP datasets. "}
{"page": 28, "image_path": "doc_images/2312.09390v1_28.jpg", "ocr_text": "Table 1: Datasets and their sources.\nsources.\n\nWe summarize the NLP datasets we use and their original\n\nDataset Original Source\nBoolQ Clark et al. (2019)\nCosmosQA Huang et al. (2019)\nDREAM Sun et al. (2019)\n\nETHICS [Justice]\nETHICS [Deontology]\nETHICS [Virtue]\nETHICS [Utilitarianism]\nFLAN ANLI R2\nGLUE CoLA\n\nGLUE SST-2\nHellaSwag\n\nMCTACO\nOpenBookQA\n\nPAWS\n\nQuAIL\n\nPIQA\n\nQuaRTz\n\nSciQ\n\nSocial IQa\nSuperGLUE MultiRC\nSuperGLUE WIC\nTwitter Sentiment\n\nHendrycks et al. (2020a)\n\nHendrycks et al. (2020a)\n\nHendrycks et al. (2020a)\n\nHendrycks et al. (2020a)\n\nNie et al. (2019); Wei et al. (2021)\nWarstadt et al. (2019); Wang et al. (2018)\nSocher et al. (2013); Wang et al. (2018)\nZellers et al. (2019)\n\nZhou et al. (2019)\n\nMihaylov et al. (2018)\n\nZhang et al. (2019)\n\nRogers et al. (2020)\n\nBisk et al. (2020)\n\nTafjord et al. (2019)\n\nWelbl et al. (2017)\n\nSap et al. (2019)\n\nKhashabi et al. (2018); Wang et al. (2019)\nPilehvar & Camacho-Collados (2018); Wang et al. (2019)\nZhang et al. (2019)\n\nDetailed results. In Figure 12, we provide detailed results across all datasets for both the baseline\nand the auxiliary confidence loss introduced in Section 4.3. In Figure 13 we report the detailed\nresults on overfitting to the weak supervisor predictions for the NLP datasets.\n\nA.2 CHESS PUZZLES\n\nData preprocessing. The GPT-4 pretraining dataset included chess games in the format of move\nsequence known as Portable Game Notation (PGN). We note that only games with players of Elo\n1800 or higher were included in pretraining. These games still include the moves that were played in-\ngame, rather than the best moves in the corresponding positions. On the other hand, the chess puzzles\nrequire the model to predict the best move. We use the dataset originally introduced in Schwarzschild\net al. (2021b) which is sourced from https: //database.lichess.org/#puzzles (see\nalso Schwarzschild et al., 2021a). We only evaluate the models ability to predict the first move of\nthe puzzle (some of the puzzles require making multiple moves). We follow the pretraining for-\nmat, and convert each puzzle to a list of moves leading up to the puzzle position, as illustrated in\nFigure 14. We use 50k puzzles sampled randomly from the dataset as the training set for the weak\nmodels and another 50k for weak-to-strong finetuning, and evaluate on 5k puzzles. For bootstrap-\nping (Section 4.3.1), we use a new set of 50k puzzles from the same distribution for each step of the\nprocess.\n\nTraining hyperparameters. We train (finetune) all models for 5 epochs using a batch size of 32.\nWe do not apply early-stopping.\n\nWeak labels. We produce weak labels by sampling predictions at temperature T = 0 (greedy\ndecoding) from the weak model on a held-out set of additional 50k puzzles. The weak labels are\ncompletions showing the highest likelihood move according to the weak model.\n\nEvaluation. To evaluate the models, we sample completions at temperature T = 0 on the held out\ntest set, and compute the fraction of datapoints where the model outputs the correct next move.\n\n29\n", "vlm_text": "This table lists various datasets along with their original sources. Here is a summary of its contents:\n\n- **BoolQ**: Clark et al. (2019)\n- **CosmosQA**: Huang et al. (2019)\n- **DREAM**: Sun et al. (2019)\n- **ETHICS [Justice]**: Hendrycks et al. (2020a)\n- **ETHICS [Deontology]**: Hendrycks et al. (2020a)\n- **ETHICS [Virtue]**: Hendrycks et al. (2020a)\n- **ETHICS [Utilitarianism]**: Hendrycks et al. (2020a)\n- **FLAN ANLI R2**: Nie et al. (2019); Wei et al. (2021)\n- **GLUE CoLA**: Warstadt et al. (2019); Wang et al. (2018)\n- **GLUE SST-2**: Socher et al. (2013); Wang et al. (2018)\n- **HellaSwag**: Zellers et al. (2019)\n- **MCTACO**: Zhou et al. (2019)\n- **OpenBookQA**: Mihaylov et al. (2018)\n- **PAWS**: Zhang et al. (2019)\n- **QuAIL**: Rogers et al. (2020)\n- **PIQA**: Bisk et al. (2020)\n- **QuaRTz**: Tafjord et al. (2019)\n- **SciQ**: Welbl et al. (2017)\n- **Social IQa**: Sap et al. (2019)\n- **SuperGLUE MultiRC**: Khashabi et al. (2018); Wang et al. (2019)\n- **SuperGLUE WIC**: Pilehvar & Camacho-Collados (2018); Wang et al. (2019)\n- **Twitter Sentiment**: Zhang et al. (2019)\nDetailed results. In Figure  12 , we provide detailed results across all datasets for both the baseline and the auxiliary confidence loss introduced in Section  4.3 . In Figure  13  we report the detailed results on over fitting to the weak supervisor predictions for the NLP datasets. \nA.2 C HESS  P UZZLES \nData preprocessing. The GPT-4 pre training dataset included chess games in the format of move sequence known as Portable Game Notation (PGN). We note that only games with players of Elo 1800 or higher were included in pre training. These games still include the moves that were played in- game, rather than the best moves in the corresponding positions. On the other hand, the chess puzzles require the model to predict the best move. We use the dataset originally introduced in  Schwarz s child et al.  ( 2021b ) which is sourced from  https://database.lichess.org/#puzzles  (see also  Schwarz s child et al. ,  2021a ). We only evaluate the models ability to predict the first move of the puzzle (some of the puzzles require making multiple moves). We follow the pre training for- mat, and convert each puzzle to a list of moves leading up to the puzzle position, as illustrated in Figure  14 . We use  $50k$   puzzles sampled randomly from the dataset as the training set for the weak models and another  $50k$   for weak-to-strong finetuning, and evaluate on    $5k$   puzzles. For bootstrap- ping (Section  4.3.1 ), we use a new set of    $50k$   puzzles from the same distribution for each step of the process. \nTraining hyper parameters. We train (finetune) all models for 5 epochs using a batch size of 32. We do not apply early-stopping. \nWeak labels. We produce weak labels by sampling predictions at temperature    $T\\,=\\,0$   (greedy decoding) from the weak model on a held-out set of additional  $50k$   puzzles. The weak labels are completions showing the highest likelihood move according to the weak model. \nEvaluation. To evaluate the models, we sample completions at temperature  $T=0$   on the held out test set, and compute the fraction of datapoints where the model outputs the correct next move. "}
{"page": 29, "image_path": "doc_images/2312.09390v1_29.jpg", "ocr_text": "test accuracy (%)\n\nOe eet\n(vido jo uonoey) =\nayndwioo Josimiadns yeam\n\na\n\n10* 10° 10 10? 1 «10810104102\nstrong student compute\n(fraction of GPT4)\n\nFigure 12: Full weak-to-strong generalization results across 22 NLP datasets. Test accuracy as\n\na function of strong student compute across our full suite of standard NLP tasks. See Table | for\ndataset details.\n\n30\n", "vlm_text": "The image contains a series of 22 line graphs representing test accuracy as a function of strong student compute across various natural language processing (NLP) tasks. Each graph is labeled with a number from 1 to 22, corresponding to a specific NLP dataset. The x-axis of each graph denotes the amount of compute used by the strong student, represented as a fraction of the compute used by GPT-4. The y-axis shows the test accuracy in percentage.\n\nMultiple lines in each graph are color-coded to indicate the level of weak-to-strong generalization, with colors ranging from dark blue (representing weak generalization) to yellow (representing strong generalization). The color gradient represents varying levels of generalization, labeled as powers of 10.\n\nThe overall visual depicts how test accuracy changes with different levels of compute across different datasets, illustrating the impact of compute and generalization on NLP task performance."}
{"page": 30, "image_path": "doc_images/2312.09390v1_30.jpg", "ocr_text": "851] 85 {i9] as {la 80 Jia\n80 0 0 7\n75 15 16\n75 (b)\n70 70 74 50.\n65 7 65 n ~ g\nPad 65 60 70 28 B40!\na ss - weg\n8 0 ° 50 I 8 30}\n8 eofta 90 fray 85 {fo} sol A £ 20\n” a 80 a’\n80. 70 0\n65. ‘s 75 (www %, as, on,\n° 7s 60 7 “ago “eo\n6 5s ° Whyerar _\n50 70 60\n0 04 08 12 16 2 0 04 08 12 16 2 0 o4 08 12 16 2 0 04 08 12 16 2\nprogress (fraction of epoch)\nFigure 13: Overfitting during training, for NLP datasets. Strong models overfit to the weak\nlabels. (a) Ground truth test accuracy of strong students over the course of training for a subset of\nour NLP task. Hues indicate the gap between weak supervisor and strong student model compute.\n\nInset numbers indicate dataset id (compare Figure 12). (b) Median best, early-stopped according to\nweak label agreement, and final performance gap recovered (PGR) aggregated across all supervisor-\nstudent pairs and all NLP tasks. Error bars indicate standard error of the mean (s.e.m.).\n\n“1. d4 1... Nf6 2. Nf3 2... d5 3. e3 3... e6 4. Bd3 4... 5\n\n5. ¢3 5... Be7 6, Nbd2 6... O-O 7. 0-0 7... Ne 8. Rel 8... Bd7 9. e4 9... dxed\n10. Nxe4 10... exd4 11, Nxf6+ IL... Bxf6 12. cxd4 12... Nb4 13. Bed 13... Qb6\n14, a3 14... Ne6 15. d5 15... exd5 16, Bxd5 16... BES 17. Bxc6 17... Qxc6\n\n18. Nd4 18... Bxd4 19. Qxd4 19... Rfe8 20. Rxe8+ 20... Rxe8 21. Be3 21... b6\n22. Rel 22...\"\n\nLabel: “ Qxcl+”\n\n(a) Elo-695 puzzle\n\nPrompt: “1. e4 I... e5 2. Nc3 2... Nf6 3. Nf3 3... Ne6 4. BbS 4... BeS\n\n5. Bxc6 5... dxc6 6. d3 6... Bg4 7. h3 7... Bxf3 8. Qxf3 8... 0-0 9. g4\n\n9... Bb4 10, Bd2 10... Nd7 11. hd 11... Be7 12. g5 12... Ne5 13. 0-0-0\n\n13... Qd7 14. bS 14... Qd8 15. Qg3 15... Ne6 16. Rdg! 16... b5 17. QxeS\n\n17... a5 18. £4 18... Re8 19. QfS 19... b4 20. Nad 20... Nd4 21. Qg4 21... 5\n\n. £5 22... Rab 23. £6 23... Bd6 24. fxg7 24... Kxg7 25. Rg2 25... Qe8\n\n. h6+ 26... Kg8 27. QhS 27... Qd7 28. Rf 28... Re6 29. Rgf2 29... Re6\n\n30. c3 30... bxe3 31. Nxc3 31... a4 32. Nd5 32... Qb5 33. Nf6+ 33... Kh8\n\n34. Qh3 34... Rb6 35. Be3 35... Ne6 36. Nxh7 36... Qxd3 37. Rdl 37... Qe4+\n38. Kbl 38... Qxe4+ 39. Kal 39... BeS 40. Nf6 40... Qe4 41. Nd5 41... Rb7 42.”\n\nLabel: “ Qf5”\n\n(b) Elo-2253 puzzle\n\nFigure 14: Chess puzzles: example datapoints. Two representative examples of an easy (a) and a\nhard (b) chess puzzle with corresponding prompts and target label formats.\n", "vlm_text": "The image consists of two parts:\n\n1. **(a) Line Graphs:**\n   - Eight subplots labeled [1], [4], [9], [12], [6], [10], [11], [8].\n   - Each plot shows lines representing test accuracy (%) on the y-axis against progress (fraction of epoch) on the x-axis.\n   - Lines vary in shade from light to dark purple, which likely indicates different ratios of \"strong student compute / weak supervisor compute,\" as noted in the color gradient bar.\n\n2. **(b) Bar Chart:**\n   - The chart shows three bars labeled as \"last,\" \"early stop (weak labels),\" and \"early stop (ground truth).\"\n   - The y-axis represents \"performance gap recovered (%)\".\n   - The bars increase in height from \"last\" to \"early stop (ground truth),\" with the latter having the highest value.\n\nThe graphs suggest a comparison of model performance metrics under different conditions.\nFigure 13:  Over fitting during training, for NLP datasets. Strong models overfit to the weak labels. (a)  Ground truth test accuracy of strong students over the course of training for a subset of our NLP task. Hues indicate the gap between weak supervisor and strong student model compute. Inset numbers indicate dataset id (compare Figure  12 ). ( b ) Median best, early-stopped according to weak label agreement, and final performance gap recovered (PGR) aggregated across all supervisor- student pairs and all NLP tasks. Error bars indicate standard error of the mean (s.e.m.). \nThis is a chess position. The label “Qxc1+” indicates that it’s Black’s move, and the suggested move is for the queen on c6 to capture the white rook on c1, delivering a check to the white king. The red arrow highlights this move from the queen to the rook.\nThe image shows a chessboard with a position from a game. The caption describes a series of moves leading up to this position, starting with:\n\n1. e4 e5\n2. Nc3 Nf6\n3. Nf3 Nc6\n4. Bb5 Bc5\n5. Bxc6 dxc6\n6. d3 Bg4\n7. h3 Bxf3\n8. Qxf3 O-O\n9. g4 Bb4\n10. Bd2 Nd7\n11. h4 Be7\n12. g5 Nc5\n13. O-O-O Qd7\n14. h5 Qd8\n15. Qg3 Ne6\n16. Rdg1 b5\n17. Qxe5 a5\n18. f4 Re8\n19. Qf5 b4\n20. Na4 Nd4\n21. Qg4 c5\n22. f5 Ra6\n23. f6 Bd6\n24. fxg7 Kxg7\n25. Rg2 Qc8\n26. h6+ Kg8\n27. Qh5 Qd7\n28. Rf1 Re6\n29. Rg3 Rg6\n30. c3 bxc3\n31. Nxc3 a4\n32. Nd5 Qb5\n33. Nf6+ Kh8\n34. Qh3 Rb6\n35. Be3 Ne6\n36. Nxh7 Qxd3\n37. Rd1 Qc4+\n38. Kb1 Qxe4+.\n\nAn arrow on the board points from the white queen on h3 to f5, indicating a potential move or focus. The puzzle has an Elo rating of 2253, suggesting it involves advanced tactics.\nFigure 14:  Chess puzzles: example datapoints.  Two representative examples of an easy  (a)  and a hard  (b)  chess puzzle with corresponding prompts and target label formats. "}
{"page": 31, "image_path": "doc_images/2312.09390v1_31.jpg", "ocr_text": "weak-to-strong\n\nad -shot bootstrappin:\nZero-shor o pings “baseline (©)100.\n\n(a)100. (b) 10\nChess Puzzles\n\n0\n6\n0\n6\n\n80:\n\na\n3\na\n3\n\n60:\n\nS\n$\nS\nS$\n\n40\n\ntest accuracy (%)\ntest accuracy (%)\n\nNy\n8\n\nnS\n(vld9 yo uonoey) —\nayndwos japow yeam\n\n20: 20.\n\neS\n\nstudent-supervisor agreement (%)\n\n0. Oe reer - 0.\n\n1o® 10° 10 10? 1 1o® 108 =910* 1021 10 10 10% = 10?\n\nstrong student compute strong student compute strong student compute\n(fraction of GPT4) (fraction of GPT4) (fraction of GPT4)\n\nFigure 15: Additional results on chess. Test accuracy of (a) baseline and (b) bootstrapping (see\nsection 4.3.1) compared to a zero-shot baseline. Zero-shot performance improves with model size,\nand students supervised with much weaker supervisors sometimes underperform compared to the\ncorresponding zero-shot model. (c) Supervisor-student agreement on the chess puzzle data. Similar\nto Figure 8, agreement decreases as the student becomes larger. Hue of line indicates compute of\nweak supervisor.\n\nZero-shot results. In Figure 15(a, b), we compare the naive baseline and bootstrapping (see sec-\ntion 4.3.1) generalization to a zero-shot baseline on the chess puzzle data. Especially since the\nmodels were pretrained on chess games, zero-shot evaluation provides a strong baseline. In partic-\nular, strong students trained with much weaker supervisors underperform the zero-shot baseline for\nthe same model size in some cases.\n\nSupervisor-student agreement results. In Figure 15(c), we report the supervisor-student agree-\nment on the chess puzzles. Similar to the NLP tasks (see Section 5.1.3), the agreement on chess also\ndecreases as the student models get larger.\n\nA.3. CHATGPT REWARD MODELING\n\nData preprocessing. Each datapoint presents a dialog d between a user and an assistant, with\na last message coming from the user; for each dialog, there are multiple candidate completions\n(c1,€2,.+-,€m), Le. responses from the assistant. We also have access to pairwise comparisons of\ncompletions, where the labeler specifies the preferred completion within a given pair of completions.\nTo sum up, the datapoints can be viewed as (d, c1, c2, y), where the label y is 1 if the labeler preferred\ncompletion cz and 0 otherwise. We use a mixture of multiple datasets used to train the reward models\nfor ChatGPT.\n\nModels. To adapt the language models to the reward modeling setting, we replace the unem-\nbedding layer of the model with a linear head with a single output, which is the logit for a given\ncompletion. The weights for this head are initialized to the unembedding weights of an arbi-\ntrary token in the original embedding layer. Similar to past work (Stiennon et al., 2020; Ouyang\net al., 2022), we run two forward passes for each comparison, and the model prediction is given\nby o(M.(d,c2) — Mw(d,c1)), where o is the sigmoid function and M,,(d,c) is the logit for\ncompletion c predicted by the model.\n\nTraining hyperparameters. We train for 1 epoch with a batch size of 220. We do not apply\nearly-stopping.\n\nWeak labels. We train the weak models on half of the available comparison data, and then\nmake predictions on the other half. The weak label y,, for a comparison (d, ci, c2) is given by\nYw = o(My(d,c2) — Mw(d, c1)), where o is the sigmoid function and M,,(d,c) is the logit for\ncompletion c predicted by the weak model.\n\nSupervisor-student agreement results. In Figure 16, we report the supervisor-student agreement\non the RM task. Similar to the NLP tasks in Figure 8 and chess puzzles in Figure 15(c), the agree-\nment decreases as the student gets larger.\n\n32\n", "vlm_text": "The image consists of three graphs (a, b, and c) showing results related to chess puzzles.\n\n(a) Test accuracy is plotted against strong student compute (fraction of GPT-4), comparing a zero-shot model with baseline and bootstrapping training methods. Test accuracy generally increases with compute.\n\n(b) Similar plot showing test accuracy for baseline and bootstrapping, as well as weak-to-strong supervision. Bootstrapping seems to improve accuracy compared to the baseline.\n\n(c) Shows student-supervisor agreement on chess puzzle data. Agreement decreases as student compute increases. The hue of the line indicates the compute level of the weak supervisor, with different colors representing various computing capacities.\n\nThe graphs illustrate relationships between model size, compute, and accuracy/agreement levels in training models on chess puzzles.\nZero-shot results. In Figure  15 (a, b), we compare the naive baseline and boots trapping (see sec- tion  4.3.1 ) generalization to a zero-shot baseline on the chess puzzle data. Especially since the models were pretrained on chess games, zero-shot evaluation provides a strong baseline. In partic- ular, strong students trained with much weaker supervisors under perform the zero-shot baseline for the same model size in some cases. \nSupervisor-student agreement results. In Figure  15 (c), we report the supervisor-student agree- ment on the chess puzzles. Similar to the NLP tasks (see Section  5.1.3 ), the agreement on chess also decreases as the student models get larger. \nA.3 C HAT GPT R EWARD  M ODELING \nData preprocessing. Each datapoint presents a dialog    $d$   between a user and an assistant, with a last message coming from the user; for each dialog, there are multiple candidate completions  $\\left(c_{1},c_{2},.\\,.\\,.\\,,c_{m}\\right)$  , i.e. responses from the assistant. We also have access to pairwise comparisons of completions, where the labeler specifies the preferred completion within a given pair of completions. To sum up, the datapoints can be viewed as  $(d,c_{1},c_{2},y)$  , where the label  $y$   is  1  if the labeler preferred completion    $c_{2}$   and  0  otherwise. We use a mixture of multiple datasets used to train the reward models for ChatGPT. \nModels. To adapt the language models to the reward modeling setting, we replace the unem- bedding layer of the model with a linear head with a single output, which is the logit for a given completion. The weights for this head are initialized to the un embedding weights of an arbi- trary token in the original embedding layer. Similar to past work ( Stiennon et al. ,  2020 ;  Ouyang et al. ,  2022 ), we run two forward passes for each comparison, and the model prediction is given by    $\\sigma(\\mathcal{M}_{w}(d,c_{2})\\,-\\,\\mathcal{M}_{w}(d,c_{1}))$  , where    $\\sigma$   is the sigmoid function and    $\\mathcal{M}_{w}(d,c)$   is the logit for completion  c  predicted by the model. \nTraining hyper parameters. We train for 1 epoch with a batch size of  220 . We do not apply early-stopping. \nWeak labels. We train the weak models on half of the available comparison data, and then make predictions on the other half. The weak label    $y_{w}$   for a comparison    $(d,c_{1},c_{2})$   is given by  $y_{w}=\\bar{\\sigma}(\\mathcal{M}_{w}(d,c_{2})-\\mathcal{M}_{w}(d,c_{1}))$  , where    $\\sigma$   is the sigmoid function and    $\\mathcal{M}_{w}(d,c)$   is the logit for completion  c  predicted by the weak model. \nSupervisor-student agreement results. In Figure  16 , we report the supervisor-student agreement on the RM task. Similar to the NLP tasks in Figure  8  and chess puzzles in Figure  15 (c), the agree- ment decreases as the student gets larger. "}
{"page": 32, "image_path": "doc_images/2312.09390v1_32.jpg", "ocr_text": "ChatGPT\nReward Modeling weak-to-strong performance —e\n\n(100\n\nSs\n3\n8\n\n(c) 100\n\n(%\n\n°\na\n°\na\n\n95:\n\n°\n\nfe\n8\nea\n8\n\n90.\n\n80.\n\n2\ns\nEs\ns\n\n(v.49 Jo unde)\naynduioo Josiuadns yeam\n\n~\na\n~\na\n\n75:\n\nfe)\n\nall samples supervisor mistakes\n\nstudent-supervisor agreement (%\n&\nstudent-supervisor agreement (%)\n2\n&\n\nstudent-supervisor agreement (\n\nsupervisor correct 70\ntoe 10° 10% 102. 7 toe 10° 104 1027 toe 108 104102\nstrong student compute strong student compute strong student compute\n(fraction of GPT4) (fraction of GPT4) (fraction of GPT4)\n\n~\n3\n~\n3\n\nFigure 16: Supervisor-student agreement decreases for stronger students on RMs. Please refer\nto caption of Figure 8 for detailed explanation of the plot. We reproduce the supervisor-student\nagreement experiment on the reward modeling data, and observe similar trends to the NLP tasks.\n\nGenerative finetuning. In Figure 17, we show that the PGR improvements from the generative\nfinetuning on RM data (Section 5.2.2) and from early-stopping on ground truth test accuracy (Sec-\ntion 5.1.1) stack together, leading to results competitive with the NLP and chess settings. In Fig-\nure 18, we report the results of an experiment similar to Figure 10, but where the weak models are\nalso pretrained with an additional generative finetuning step on the RM data.\n\nA.4 AUXILIARY CONFIDENCE Loss\n\nHere, we provide a detailed description of the method we use in Section 4.3.2.\n\nWe use the following loss function:\n\nLeont(f) = (1 — a) -CE(f(2), f(a) + a CE(f(2), fe(a)) ()\n\nwhere CE(-,-) is the cross-entropy loss between the predictive distributions on a given input 2,\nfw(x) € [0,1] represents the weak label predictive distribution, f(x) € [0,1] is the strong model\npredictive distribution, a is a weight and ¢ is a threshold. The predictions f (x) correspond to\nhardened strong model predictions using a threshold t, i.e. f:(x) = I[f (a) > t] € {0,1} where J is\nthe indicator function. We set the threshold ¢ adaptively, so that f(x) > t holds for exactly half of\nexamples in the batch’. We set max = 0.75 for the largest student models and to 0.5 otherwise and\nlinearly warm-up a from 0 to Qmax over the first 20% of training.\n\nOur balancing mechanism incorporates a prior over the distribution of labels into training and is\nonly practically feasible in the low-n classification setting. For most weak-strong pairs and datasets,\nit had a small or neutral effect on weak-to-strong generalization; however, in a few settings it made\na significant improvement.\n\nWe note that the loss in Equation | can be rewritten as a self-bootstrapping loss:\n\nLeon (f) = CE(f (x), (1 — @) + f(x) +.0° fix), (2)\n\ni.e. the cross-entropy target is a mixture of the weak model predictions and the (thresholded) pre-\ndictions of the strong student itself. This loss is related to the bootstrapping methods in Reed et al.\n(2014) and Arazo et al. (2019) for addressing label noise. It is also similar to self-training (Lee\net al., 2013) and conditional entropy minimization (Grandvalet & Bengio, 2004), which have led\nto state-of-the-art results in semi-supervised learning (Xie et al., 2020) and domain adaptation (Shu\net al., 2018). Chen et al. (2020b) and Wei et al. (2020) show that self-training can mitigate the bias\nof the supervisor model.\n\nIn Appendix B we also describe other methods we considered; for most of these methods, we got\nnegative early results.\n\nTThe choice of exactly half reflects the prior over class\nmodel predictions in non-balanced or non-binary settings.\n\ns, and should be computed explicitly from weak\n\n33\n", "vlm_text": "The image displays three line plots analyzing the agreement between supervisors and students using ChatGPT's reward modeling data across different student capabilities. The x-axis for all plots indicates the \"strong student compute\" as a fraction of GPT-4.\n\n1. **Plot (a):** \"All samples\" shows how student-supervisor agreement changes across all samples, with a slight downward trend as student compute increases.\n\n2. **Plot (b):** \"Supervisor correct\" focuses on cases where the supervisor's judgment is correct, showing a high level of agreement that slightly decreases with stronger student compute.\n\n3. **Plot (c):** \"Supervisor mistakes\" presents scenarios where the supervisor made mistakes, showing a noticeable decrease in agreement as student compute increases.\n\nThe color of the lines represents the \"weak supervisor compute\" as a fraction of GPT-4, with the colors ranging from purple (low) to yellow (high), indicating weak-to-strong performance differences.\nGenerative finetuning. In Figure  17 , we show that the PGR improvements from the generative finetuning on RM data (Section  5.2.2 ) and from early-stopping on ground truth test accuracy (Sec- tion  5.1.1 ) stack together, leading to results competitive with the NLP and chess settings. In Fig- ure  18 , we report the results of an experiment similar to Figure  10 , but where the weak models are also pretrained with an additional generative finetuning step on the RM data. \nA.4 A UXILIARY  C ONFIDENCE  L OSS \nHere, we provide a detailed description of the method we use in Section  4.3.2 . \nWe use the following loss function: \n\n$$\nL_{\\mathrm{conf}}(f)=(1-\\alpha)\\cdot\\mathrm{CE}(f(x),f_{w}(x))+\\alpha\\cdot\\mathrm{CE}(f(x),\\hat{f}_{t}(x))\n$$\n \n $\\mathrm{CE}(\\cdot,\\cdot)$   is the cross-entropy loss between the predictive on a given input    $x$  ,  $f_{w}(x)\\in[0,1]$   ∈  represents the weak label predictive distribution,  $f(x)\\,\\in\\,[0,1]$   ∈  is the strong model predictive distribution,    $\\alpha$   is a weight and  $t$   is a threshold. The predictions  $\\hat{f}_{t}(x)$   correspond to hardened strong model predictions using a threshold  $t$  , i.e.  $\\hat{f}_{t}(x)=I[f(x)>t]\\in\\{0,1\\}$   ∈{ }  where  $I$   is the indicator function. We set the threshold    $t$   adaptively, so that  $f(x)>t$   holds for exactly half of examples in the batch 7 . We set  $\\alpha_{\\mathrm{max}}=0.75$   for the largest student models and to  0 . 5  otherwise and linearly warm-up  $\\alpha$   from 0 to  $\\alpha_{\\mathrm{max}}$   over the first  $20\\%$   of training. \nOur balancing mechanism incorporates a prior over the distribution of labels into training and is only practically feasible in the low-  $\\cdot n$   classification setting. For most weak-strong pairs and datasets, it had a small or neutral effect on weak-to-strong generalization; however, in a few settings it made a significant improvement. \nWe note that the loss in Equation  1  can be rewritten as a self-boots trapping loss: \n\n$$\nL_{\\mathrm{conf}}(f)=\\mathrm{CE}(f(x),(1-\\alpha)\\cdot f_{w}(x)+\\alpha\\cdot\\hat{f}_{t}(x)),\n$$\n \ni.e. the cross-entropy target is a mixture of the weak model predictions and the (threshold ed) pre- dictions of the strong student itself. This loss is related to the boots trapping methods in  Reed et al. ( 2014 ) and  Arazo et al.  ( 2019 ) for addressing label noise. It is also similar to self-training ( Lee et al. ,  2013 ) and conditional entropy minimization ( Grandvalet & Bengio ,  2004 ), which have led to state-of-the-art results in semi-supervised learning ( Xie et al. ,  2020 ) and domain adaptation ( Shu et al. ,  2018 ).  Chen et al.  ( 2020b ) and  Wei et al.  ( 2020 ) show that self-training can mitigate the bias of the supervisor model. \nIn Appendix  B  we also describe other methods we considered; for most of these methods, we got negative early results. "}
{"page": 33, "image_path": "doc_images/2312.09390v1_33.jpg", "ocr_text": "no generative with generative strong ceiling performance\n9.\n\nfinetuning ~~ finetuning .t. supervision) = ——\n@) ©) 190\n&\n> B 8 ‘2\ng g se\ns 3 60 85\ni 2 23\n5 a $3\n8 & gf\n©\n2 2 Sz\n5 20. ios\n€ ov\no\na\n0\n107 10° 10° 10% 10% 10? 10° 10° 10% 10° 10?\nstrong student compute strong student compute\n(fraction of GPT4) (fraction of GPT4)\n\nFigure 17: The benefits of improved task-specific tuning and ground truth early stopping stack,\nresulting in even higher PGR. Like Figure 10 but with ground truth early stopping based on test\naccuracy.\n\nno generative with generative strong ceiling performance\nfinetuning ~~ finetuning tot supervision) =——\n@) ) 499\n\n&\n\n_ B 80 -\n\n& g SB\nfey ax\n\ns 2 60 23\n\n5 2 $3\n\n8 S Qf\n\nz g 40 28\n\n2 5 33\n— Se\n5 20: a\n2 0”\na\n\n60 Lor wr 0\n07 10° 105 10% 10% 10? 10® 10° 10% 10% 107\nstrong student compute strong student compute\n(fraction of GPT4) (fraction of GPT4)\n\nFigure 18: PGR improves when both supervisors and students have an extra generative fine-\ntuning step. Like Figure 10 but where “with generative finetuning” indicates that both supervisors\nand students have an extra generative finetuning step. In other words, for this experiment all base\nmodels have an extra generative finetuning step following pretraining.\n\nB_ ADDITIONAL RESULTS ON METHODS\n\nWe did preliminary experiments on a variety of methods for improving the strong model perfor-\nmance in our weak-to-strong generalization setting. We found many of them not useful for im-\nproving over the naive finetuning baseline, and others yielding limited improvements on a subset of\nsettings but not consistently over all datasets and model sizes. We summarize the algorithms, the\nmotivations, and the takeaways below. Note that we did not optimally tune each of the methods, so\nit is possible that with better tuning they may still perform well.\n\nConfidence thresholding. To filter out incorrect weak labels, we used a simple cut-off method\nthat selected only the top 5% to 20% examples from each class where the weak supervisor is most\nconfident to train the strong model. We found that our weak labels are typically well-calibrated,\nbut confidence thresholding only helps when the weak labels are very bad (e.g. 60% accuracy) and\nstops being useful when the weak labels reach around 70% to 80% accuracy. We observed these\nresults both in NLP and in the chess puzzle settings. See Appendix C for more discussion of related\nexperiments.\n\nConfidence losses. To encourage strong model to make confident predictions (Grandvalet &\nBengio, 2004), we added an auxiliary loss that encourages the model predicted class probabil-\n\n34\n", "vlm_text": "The image contains two line graphs labeled (a) and (b), showing the effects of different tuning methods on test accuracy and performance gap recovered relative to compute resources, using a fraction of GPT-4's compute.\n\n### Graph (a): Test Accuracy (%)\n- **X-Axis**: Strong student compute (fraction of GPT-4).\n- **Y-Axis**: Test accuracy (%).\n- **Lines**:\n  - Solid lines represent no generative finetuning.\n  - Dashed lines represent with generative finetuning.\n  - Solid black line shows strong ceiling performance (ground truth supervision).\n\n### Graph (b): Performance Gap Recovered (%)\n- **X-Axis**: Strong student compute (fraction of GPT-4).\n- **Y-Axis**: Performance gap recovered (%).\n- **Lines**:\n  - Solid and dashed lines similar to graph (a), indicating with and without generative finetuning.\n  - The color gradient from blue to yellow reflects the weak model compute (fraction of GPT-4).\n\nThe chart suggests that improved tuning and early stopping based on test accuracy increases both test accuracy and the performance gap recovered, particularly with more compute resources.\nThe image consists of two graphs labeled (a) and (b).\n\nGraph (a):\n- The y-axis represents test accuracy as a percentage.\n- The x-axis represents strong student compute (fraction of GPT-4).\n- There are three types of lines:\n  - Solid black line for \"no generative finetuning.\"\n  - Pink line for \"with generative finetuning.\"\n  - Black dashed line for \"strong ceiling performance (g.t. supervision).\"\n\nGraph (b):\n- The y-axis represents performance gap recovered as a percentage.\n- The x-axis again represents strong student compute (fraction of GPT-4).\n- The lines are similar to graph (a) with corresponding colors and styles.\n\nIn both graphs, it is shown that incorporating generative finetuning (pink line) improves both test accuracy and performance gap recovery compared to no generative finetuning (solid black line). The color gradient from purple to yellow represents the weak model compute (fraction of GPT-4).\nB A DDITIONAL RESULTS ON METHODS \nWe did preliminary experiments on a variety of methods for improving the strong model perfor- mance in our weak-to-strong generalization setting. We found many of them not useful for im- proving over the naive finetuning baseline, and others yielding limited improvements on a subset of settings but not consistently over all datasets and model sizes. We summarize the algorithms, the motivations, and the takeaways below. Note that we did not optimally tune each of the methods, so it is possible that with better tuning they may still perform well. \nConfidence threshold ing. To filter out incorrect weak labels, we used a simple cut-off method that selected only the top  $5\\%$   to    $20\\%$   examples from each class where the weak supervisor is most confident to train the strong model. We found that our weak labels are typically well-calibrated, but confidence threshold ing only helps when the weak labels are very bad (e.g.   $60\\%$   accuracy) and stops being useful when the weak labels reach around   $70\\%$   to   $80\\%$   accuracy. We observed these results both in NLP and in the chess puzzle settings. See Appendix  C  for more discussion of related experiments. \nConfidence losses. To encourage strong model to make confident predictions ( Grandvalet & Bengio ,  2004 ), we added an auxiliary loss that encourages the model predicted class probabil- "}
{"page": 34, "image_path": "doc_images/2312.09390v1_34.jpg", "ocr_text": "Table 2: Custom prompts used in the zero-shot and few-shot experiments. We design a simple\ncustom prompt for each of the tasks in the table below. In the few-shot setting, we also append\nlabeled (with ground truth or weak labels) examples to the prompt.\n\nDataset Custom Prompt\nThere is some context, followed by a question about the context, and an answer.\nCosmosQA . . . .\nRespond with 1 if the answer is correct, and 0 otherwise.\nDeontolo There is a short scenario, consisting of a situation or request, followed by an excuse.\nsy Respond with 1 if the excuse is valid, and with 0 otherwise.\nJustice There is a statement. Respond with 1 if it is correct or just and with 0 otherwise.\nThere is a news context, followed by a question about the context, and an answer\nQuAIL . . . . : .\nto the question. Respond with 1 if the answer is correct, and with 0 otherwise.\nSciQ There is a science knowledge question, followed by an answer.\nRespond with 1 if the answer is correct, and with 0 otherwise.\n: There is some context, followed by a social question, followed by an answer.\nSocial IQa . . . .\nRespond with 1 if the answer is correct, and 0 otherwise.\nVirtue There is a short scenario, followed by a judgement of the person involved.\n\nRespond with 1 if the judgement is correct, otherwise respond with 0.\n\nity p to be far away from 0.5. We tried both the Jz loss —(p — 0.5)? and the entropy loss\nplogp + (1 — p)log(1 — p). We found these losses to be helpful in preliminary experiments in\nthe linear probing setting, but they generally performed less well than the confidence auxiliary loss\nin Equation | in the finetuning setting. We have also observed negative results with the confidence\nlosses when the training data is highly class-imbalanced or when we do not use the rebalancing\nprocedure described in Section 4.3.\n\nProduct confidence loss. We also tried a confidence-like loss which sets the cross entropy tar-\ngets to be proportional to the product of the probabilities that the weak and strong models assign,\nrenormalized across classes and without propagating gradients through the targets. In preliminary\nexperiments, this loss consistently gave positive results over the baseline on two NLP tasks, but\nperformed poorly compared to our main confidence loss. Variants like geometric mean instead of\nproduct gave no boost. Compared to the confidence loss, it could be useful as it has no inter-batch\ndependence and could potentially be adapted for generative tasks.\n\nLP-FT. We used the LP-FT technique proposed in Kumar et al. (2022) which first trains a linear\nprobe on frozen strong model representations and then finetunes all layers, to avoid destroying the\npretrained representation. We were unable to get improvements compared to the finetuning baseline.\n\nWeight regularization. To regularize the strong model weights to avoid imitating the weak la-\nbels®, we tried a variety of regularization techniques for strong model training, including stronger\nweight decay (Krogh & Hertz, 1991) and dropout (Srivastava et al., 2014). We did not find signifi-\ncant improvement.\n\nLoRA. As another regularization technique, we also considered low-rank adaptation (LoORA) (Hu\net al., 2022), i.e. only making a low-rank update to the parameters of each layer of the model during\nfinetuning. We did not find any improvement, even when sweeping the LoRA rank.\n\nData augmentation. Inspired by the success of consistency algorithms in self-supervised train-\ning (Chen et al., 2020a; Caron et al., 2021), we used the strong student models to rephrase the inputs\nin each sample, and added an auxiliary loss enforcing the strong model predictions to be consistent\nbetween original and rephrased samples. We did not find any improvement on a selected subset of\nNLP datasets.\n\nSHowever, as we discuss in Section 5.1.3, in our setup the strong model tends to be bad at imitating the\nweak labels. Therefore, regularization could be more important in settings where the strong model can fit the\nweak labels well.\n\n35\n", "vlm_text": "The table lists various datasets and their corresponding custom prompts. \n\n- **CosmosQA**: Context and a question about it, followed by an answer. Respond with 1 if correct, 0 otherwise.\n- **Deontology**: Scenario with a request and an excuse. Respond with 1 if the excuse is valid, 0 otherwise.\n- **Justice**: Statement. Respond with 1 if it is correct or just, 0 otherwise.\n- **QuAIL**: News context with a question and an answer. Respond with 1 if correct, 0 otherwise.\n- **SciQ**: Science knowledge question with an answer. Respond with 1 if correct, 0 otherwise.\n- **Social IQa**: Context with a social question and an answer. Respond with 1 if correct, 0 otherwise.\n- **Virtue**: Scenario with a judgment. Respond with 1 if the judgment is correct, 0 otherwise.\nity    $p$   to be far away from 0.5. We tried both the    $l_{2}$   loss    $-(p\\,-\\,0.5)^{2}$    and the entropy loss  $p\\,{\\overset{.}{\\log}}\\,p+(1-p)\\,{\\overset{.}{\\log}}(1-p)$  . We found these losses to be helpful in preliminary experiments in the linear probing setting, but they generally performed less well than the confidence auxiliary loss in Equation  1  in the finetuning setting. We have also observed negative results with the confidence losses when the training data is highly class-imbalanced or when we do not use the rebalancing procedure described in Section  4.3 . \nProduct confidence loss. We also tried a confidence-like loss which sets the cross entropy tar- gets to be proportional to the product of the probabilities that the weak and strong models assign, re normalized across classes and without propagating gradients through the targets. In preliminary experiments, this loss consistently gave positive results over the baseline on two NLP tasks, but performed poorly compared to our main confidence loss. Variants like geometric mean instead of product gave no boost. Compared to the confidence loss, it could be useful as it has no inter-batch dependence and could potentially be adapted for generative tasks. \nLP-FT. We used the LP-FT technique proposed in  Kumar et al.  ( 2022 ) which first trains a linear probe on frozen strong model representations and then finetunes all layers, to avoid destroying the pretrained representation. We were unable to get improvements compared to the finetuning baseline. \nWeight regular iz ation. To regularize the strong model weights to avoid imitating the weak la- bels 8 , we tried a variety of regular iz ation techniques for strong model training, including stronger weight decay ( Krogh & Hertz ,  1991 ) and dropout ( Srivastava et al. ,  2014 ). We did not find signifi- cant improvement. \nLoRA. As another regular iz ation technique, we also considered low-rank adaptation (LoRA) ( Hu et al. ,  2022 ), i.e. only making a low-rank update to the parameters of each layer of the model during finetuning. We did not find any improvement, even when sweeping the LoRA rank. \nData augmentation. Inspired by the success of consistency algorithms in self-supervised train- ing ( Chen et al. ,  2020a ;  Caron et al. ,  2021 ), we used the strong student models to rephrase the inputs in each sample, and added an auxiliary loss enforcing the strong model predictions to be consistent between original and rephrased samples. We did not find any improvement on a selected subset of NLP datasets. "}
{"page": 35, "image_path": "doc_images/2312.09390v1_35.jpg", "ocr_text": "—elo<800 —elo<1000 —elo< 1200\n\n++ Zero-shot\n—elos900 —elos1100 —elo< 1300\n(a) (b) 70\n_. 80 cS 60\n3 i\n3 =e 50\n3 60 B= 0\no_ ad\n38 82\nS'S 40. x 30\nre 2s\n23 § 20\no\n& 20 €\nxc = 10\n=< 0\n0 20 40 60 80 0 10 20 30 40 50 60 70\naccuracy accuracy\n(%, all-trained, all-eval) (%, all-trained, hard-eval)\n\nFigure 19: Easy-to-hard generalization on chess puzzles. We finetune models on chess puzzles\nwith Elo < t, varying the threshold t, and evaluate the finetuned models on (a): all test puzzles,\nand (b): hard test puzzles with Elo > 2000. Across the board, we see strong performance, even\nwhen training only on very easy puzzles (Elo < 800). For reference, we also include the zero-\nshot performance of the model. Finetuning on easy puzzles, we improve upon the performance on\naverage on the test set, but we do not improve on hard puzzles, compared to the zero-shot model.\n\nAdding label noise, special losses for noisy labels. We experimented with the generalized cross-\nentropy loss proposed in Zhang & Sabuncu (2018) that is more robust to label noise, but did not find\nimprovement over cross-entropy. We also tried adding random noise to weak labels, and found that\nthe strong models were able to simulate the weak labels less well, especially early in training, but it\ndid not ultimately result in improved performance.\n\nFew-shot prompting. As an alternative to fine-tuning, we can use the in-context learning ability\nof the strong student models. For each task, we append a custom prompt shown in Table 2. For a\ndetailed description of the results, see Section 5.2.1.\n\nWeight averaging. Prior work (Izmailov et al., 2018; Cha et al., 2021; Wortsman et al., 2022b;a)\nsuggested that various forms of weight averaging can substantially improve performance, especially\nin distribution shift settings. In our setup, we experimented with applying exponential moving\naveraging to the parameters of the model during training, but did not observe improvements relative\nto the baseline.\n\nC_ EASY-TO-HARD GENERALIZATION\n\nIn Section 5.1.3 and Appendix E, we discuss that one reason weak-to-strong generalization may\nbe difficult is if the weak labels have systematic errors that the strong model can learn to emulate.\nOne natural type of systematic weak label error is to do poorly on hard examples and well on easy\nexamples.\n\nIn this section, we focus on studying what we call easy-to-hard generalization, where we train only\non easy examples using ground truth supervision, and assess generalization to harder examples.\n\nC.1 CHESS PUZZLES\n\nEach chess puzzle comes with a natural difficulty label: an Elo score, which describes its difficulty\naccording to humans. On the https://lichess.org website, people try to solve puzzles,\nwhich can be viewed as a game between a puzzle and a human player. The Elo scores are then\nassigned to both human players and chess puzzles following the standard Elo algorithm.\n\nWe consider the easy-to-hard generalization problem, where the difficulty is defined according to\nthe puzzle Elo rating. We note that the puzzle Elo describes the difficulty of the entire puzzle\nmove sequence, while we are only training the model to predict the first move in the sequence\n\n36\n", "vlm_text": "The image consists of two graphs analyzing the performance of models fine-tuned on chess puzzles with different Elo ratings. The graphs show the relationship between accuracy and Elo thresholds for both general and hard chess puzzles. \n\n### Graph Details:\n\n- **(a)** Evaluates the models on all test puzzles.\n  - **Curves**: Represent training on puzzles with Elo ≤ 800, 900, 1000, 1100, 1200, and 1300.\n  - **Zero-shot line**: Dotted, showing the initial performance without fine-tuning.\n\n- **(b)** Evaluates models on hard puzzles (Elo ≥ 2000).\n  - Similar curves and zero-shot performance line as in (a).\n\n### Observations:\n\n- Fine-tuning on puzzles with lower Elo improves average performance on all test puzzles.\n- However, for hard puzzles, performance does not improve significantly compared to the zero-shot model.\n\nThe key insight is the limited benefit of easy puzzles for improving performance on harder challenges.\nAdding label noise, special losses for noisy labels. We experimented with the generalized cross- entropy loss proposed in  Zhang & Sabuncu  ( 2018 ) that is more robust to label noise, but did not find improvement over cross-entropy. We also tried adding random noise to weak labels, and found that the strong models were able to simulate the weak labels less well, especially early in training, but it did not ultimately result in improved performance. \nFew-shot prompting. As an alternative to fine-tuning, we can use the in-context learning ability of the strong student models. For each task, we append a custom prompt shown in Table  2 . For a detailed description of the results, see Section  5.2.1 . \nWeight averaging. Prior work ( Izmailov et al. ,  2018 ;  Cha et al. ,  2021 ;  Wortsman et al. ,  2022b ; a ) suggested that various forms of weight averaging can substantially improve performance, especially in distribution shift settings. In our setup, we experimented with applying exponential moving averaging to the parameters of the model during training, but did not observe improvements relative to the baseline. \nC E ASY - TO - HARD GENERALIZATION \nIn Section  5.1.3  and Appendix  E , we discuss that one reason weak-to-strong generalization may be difficult is if the weak labels have systematic errors that the strong model can learn to emulate. One natural type of systematic weak label error is to do poorly on hard examples and well on easy examples. \nIn this section, we focus on studying what we call  easy-to-hard generalization , where we train only on easy examples using ground truth supervision, and assess generalization to harder examples. \nC.1 C HESS PUZZLES \nEach chess puzzle comes with a natural difficulty label: an Elo score, which describes its difficulty according to humans. On the  https://lichess.org  website, people try to solve puzzles, which can be viewed as a game between a puzzle and a human player. The Elo scores are then assigned to both human players and chess puzzles following the standard Elo algorithm. \nWe consider the easy-to-hard generalization problem, where the difficulty is defined according to the puzzle Elo rating. We note that the puzzle Elo describes the difficulty of the entire puzzle move sequence, while we are only training the model to predict the first move in the sequence "}
{"page": 36, "image_path": "doc_images/2312.09390v1_36.jpg", "ocr_text": "-@ train easy -e train all - train hard -e- zero shot\n10” 1089 107\n\nPk aos\n20S 5868 8\n\n3\nSs\n\n1082 1042\n\nRD @\n688\n\nON\nPose tess\n\nNy\n8\n\ntest accuracy (%)\n\n100\n\n1024 1078\n\n80.\n\n60.\n\ni\nAra,\n\n40\n\n20\n\n0.\n500 1000 1500 2000 2500 500 1000 1500 2000 2500 500 1000 1500 2000 2500\npuzzle elo\n\n(a) Easy cutoff: Elo < 1200\n\n-e- train easy -e- train all -e train hard -e- zero shot\n10° 1088 107\n\n100\n80\n60\n40\n20\n\nof\n100\n\n%)\na ©\n3s\n\nnN\nS\n\ntest accuracy (%)\nB\n&\n\n100\n80\n60\n\n40\n20\n\nit)\n500 1000 1500 2000 2500 500 1000 1500 2000 2500 500 1000 1500 2000 2500\npuzzle elo\n\n(b) Easy cutoff: Elo < 900\n\nFigure 20: Easy-to-hard generalization on chess puzzles. We present detailed performance of\nmodels finetuned on different subsets of chess puzzles across model sizes and test puzzle difficulty\nlevels. For each model size, we compare models trained only on easy puzzles, hard puzzles, or all\npuzzles. We also include the zero-shot model performance. We provide results for the easy puzzle\nElo cutoffs of (a): 1200 and (b): 900. All finetuned models are trained on 50k random datapoints\nfrom the corresponding distribution. The size of the model is shown in the upper-right corner of\neach panel, in terms of fraction of GPT-4 compute.\n\n37\n", "vlm_text": "The image shows a series of graphs displaying test accuracy as a function of puzzle ELO ratings, grouped into three sections across different conditions. Each subplot corresponds to a different condition, indicated by the numbers at the top right of each plot (from \\(10^{-10}\\) to 1). \n\nThe x-axis represents the puzzle ELO, ranging from 500 to 2500. The y-axis represents test accuracy in percentage, ranging from 0 to 100%.\n\nFour different training variations are plotted:\n- **Train easy** (green)\n- **Train all** (blue)\n- **Train hard** (purple)\n- **Zero shot** (white with black outline)\n\nThe background is divided into three colored sections:\n- **Green**: Lower ELO range (easier puzzles)\n- **White**: Intermediate range\n- **Pink/Purple**: Higher ELO range (harder puzzles)\n\nThe caption mentions an \"Easy cutoff: ELO ≤ 1200,\" possibly referring to the division point between easy and intermediate puzzles in the green section. The trend generally shows decreasing accuracy with increasing ELO, indicating that puzzle difficulty and model performance are correlated.\nThe image is a grid of plots (3x3) analyzing the test accuracy of different models trained and tested on chess puzzles of varying difficulty, defined by Elo ratings. The models are categorized and compared based on whether they are trained on easy puzzles, hard puzzles, or all puzzles. It also shows the zero-shot model performance (models that were not specifically trained on the data). The easy puzzles are identified by an Elo rating of 900 or less, as indicated by the green shaded areas. Each subplot represents a different model size, denoted by a fraction of GPT-4's computational power, ranging from \\(10^{-10}\\) to \\(1\\).\n\nKey details:\n- The x-axis of each subplot shows the puzzle Elo rating from 500 to 2500.\n- The y-axis represents test accuracy in percentage, from 0% to 100%.\n- Each series of points shows the performance trend for different training conditions.\n- The plot is assessed for three conditions defined with colors: green for training on easy puzzles, blue for training on all puzzles, and purple for training on hard puzzles. The black open circles represent the zero-shot condition.\n- As puzzle difficulty increases (higher Elo rating), test accuracy generally decreases for all models.\n- The performance of larger models typically starts out better and declines more gradually with puzzle difficulty in most cases."}
{"page": 37, "image_path": "doc_images/2312.09390v1_37.jpg", "ocr_text": "85.\n\n\\\nMN\n\n85: 80\nwe 2 ro\n° ae aan °\nHe\n75: 70. | 98\na. \" 1\n2\n65.\nain oa 86\n\nstrong student compute\n\n74 ro a (fraction of GPT4)\n80 ee Oe an ss a a —e\n10” o1\nae sear\n\n60:\n\ntest accuracy (%)\n\n75:\n\n75 70:\n\npole és\n\nLy me te\n10° 10°\" 10% 107 To®10*\" 0\" “70? 108 10* 10\" 107\nsample difficulty cutoff\n(fraction of GPT4)\n\n55\n\nFigure 21: Effect of varying training data difficulty on test set accuracy. Test accuracy as a func-\ntion of sample difficulty cutoff on a subset of our NLP tasks. The leftmost point on the horizontal\naxis corresponds to only using datapoints that models of all sizes that we consider get right when\ntrained on other data sampled from the same task, and the rightmost point (denoted with oo) corre-\nsponds to training on all datapoints; the point with value x on the horizontal axis corresponds to only\nusing the datapoints that models with x or higher compute (fraction of GPT-4) consistently get right.\nInset numbers indicate task id (compare Figure 12). Hue indicates compute of weak supervision.\nStars indicate points where weak supervisor size corresponds to sample difficulty cutoff.\n\n(see Appendix A.2). Consequently, the puzzle Elo is a high-quality but still imperfect measure of\ndifficulty of the problem for humans. It is also important to note, that puzzle Elo may not be a good\nmeasure of difficulty for the models: easy puzzles for humans can be hard for the models and vice\nversa.\n\nWe then split the dataset into subsets according to the puzzle Elo. We consider the hard\nset to be puzzles with difficulty above Elo 2000. For the easy set, we consider cuttoffs in\n{800, 900, 1000, 1100, 1200, 1300}, and use puzzles with difficulty below the cutoff. We also con-\nsider the unrestricted set of all puzzles. We sample 50k puzzles from each of these sets randomly,\nand finetune the model on them’.\n\nWe report the results in Figure 19, where we also provide the performance of a zero-shot baseline\nfor reference. We plot the accuracy of the models trained on the easy subsets of puzzles against the\nperformance of the same model trained on all puzzles. We find that the models generally perform\nwell on average on the test set in panel (a), and outperform the zero-shot baseline. Interestingly,\nwhen evaluated on hard examples only, in panel (b), the models perform similarly to the zero-shot\nbaseline, or slightly worse.\n\nWhen trained on easy puzzles, the models shift towards performing well on the easy puzzles, and\nunderperform on the hard puzzles. In Figure 20, we can see that generally the models improve upon\nthe zero-shot baseline outside of their training difficulty range, often up to Elo of 1500 or higher, but\nunderperform on the hardest examples.\n\nC.2 NLP TASKS: DIFFICULTY THRESHOLDING\n\nNLP tasks do not come with a natural source of difficulty labels, but we can create such labels by\nlooking at performance as a function of model size.\n\n°For easy puzzles with 800-Elo cutoff, we only use 25k puzzles, because there are not 50k puzzles available\nin this difficulty range.\n\n38\n", "vlm_text": "The image is a series of graphs showing how test accuracy varies with the difficulty cutoff of training data across several natural language processing (NLP) tasks. Each plot corresponds to a different task, as indicated by task IDs in the square brackets (e.g., [9], [22], etc.). \n\nKey elements:\n\n- **Horizontal Axis (x-axis):** Represents sample difficulty cutoff as a fraction of GPT-4 compute. The leftmost point includes only the easiest samples—those that all models considered get right—while the rightmost point (denoted with ∞) includes all samples. \n\n- **Vertical Axis (y-axis):** Displays test accuracy as a percentage.\n\n- **Lines & Colors:** Different hues represent varying levels of compute for the weak supervisors, ranging from blue (low compute) to yellow (higher compute).\n\n- **Stars:** Indicate points where the weak supervisor's size matches the sample difficulty cutoff.\n\nOverall, the plots seem to explore the relationship between data difficulty and model performance, suggesting how changing the difficulty cutoff impacts accuracy depending on the compute level.\n(see Appendix  A.2 ). Consequently, the puzzle Elo is a high-quality but still imperfect measure of difficulty of the problem for humans. It is also important to note, that puzzle Elo may not be a good measure of difficulty for the models: easy puzzles for humans can be hard for the models and vice versa. \nWe then split the dataset into subsets according to the puzzle Elo. We consider the hard set to be puzzles with difficulty above Elo 2000. For the easy set, we consider cuttoffs in  $\\{800,900,1000,1100,1200,1300\\}$  , and use puzzle h difficulty below the cutoff. We also con- sider the unrestricted set of  all  puzzles. We sample  $50k$   puzzles from each of these sets randomly, and finetune the model on them 9 . \nWe report the results in Figure  19 , where we also provide the performance of a zero-shot baseline for reference. We plot the accuracy of the models trained on the easy subsets of puzzles against the performance of the same model trained on all puzzles. We find that the models generally perform well on average on the test set in panel (a), and outperform the zero-shot baseline. Interestingly, when evaluated on hard examples only, in panel (b), the models perform similarly to the zero-shot baseline, or slightly worse. \nWhen trained on easy puzzles, the models shift towards performing well on the easy puzzles, and under perform on the hard puzzles. In Figure  20 , we can see that generally the models improve upon the zero-shot baseline outside of their training difficulty range, often up to Elo of 1500 or higher, but under perform on the hardest examples. \nC.2 NLP  TASKS :  DIFFICULTY THRESHOLD ING \nNLP tasks do not come with a natural source of difficulty labels, but we can create such labels by looking at performance as a function of model size. "}
{"page": 38, "image_path": "doc_images/2312.09390v1_38.jpg", "ocr_text": "— trained on all difficulties\n—e trained on easiest 30%\n—o trained on easiest 50%\n\n=\n=\n\no Rn\noa oa\n\nscore (elo)\nhy w\na\ntest accuracy (%)\nSy)\na\n\n‘0 70.\n15 65\n10\n60.\n5\n1 2 3 4 5 10° 10° 10* 10% 107\nproblem level strong student compute\n\n(fraction of GPT4)\n\nFigure 22: Filtering training samples by GPT-4 generated Elo scores results in very good easy-\nto-hard generalization. (a) GPT-4 generated Elo scores for different, human-defined, problem\ndifficulties (1 - easiest, 5 - hardest) on the MATH dataset. (b) Average test accuracy as a function of\nstrong student compute on a subset of our NLP tasks. Student is trained on ground truth labels on\nsamples of all difficulties (black), only the 30% easiest tasks (orange), or only the 50% easiest tasks\n(blue).\n\nWe define difficulty of a datapoint based on the smallest model size that consistently predicts the\nlabel on this datapoint correctly, when trained on ground truth. For example, suppose we have\n4 ground truth models Wi, W2, W3, W4 that use compute C) < C2 < C3 < C4 respectively.\nSuppose models W,, W3, W4 predict the example correctly when it is in a held-out set, while W2\npredicts it incorrectly. Then we will assign a difficulty of C3 to the example.\n\nThen given a difficulty cutoff D, we filter the training set to examples with difficulty < D. We\nsubsample the filtered set so that the number of training examples is equal to the number of examples\nat the lowest difficulty level. We train a model on the subsampled training set using ground truth\nlabels, and measure its accuracy on a held out test set (with no subsampling).\n\nThe subsampling ensures that we use the same training set size for each difficulty cutoff. Using\nground truth labels ensures that the label accuracy is the same (100%) for each cutoff. We also use\nthe same test set for each cutoff. This setup lets us vary only training data difficulty, and measure its\nimpact on the trained model’s accuracy.\n\nWe plot results in Figure 21. The y-axis is accuracy on the test set, while the x-axis is the difficulty\ncutoff. Increasing the difficulty cutoff generally leads to an increase in accuracy. This result suggests\nthat solving easy-to-hard generalization is non-trivial even if there are no weak label errors.\n\nFor smaller models (darker lines), the accuracy initially increases, but starts to decrease beyond a\npoint. The drop generally happens when the difficulty cutoff exceeds the capacity of the model itself,\ni.e. when the examples are too difficult for the model to fit. However, large models trained on easy\nexamples often perform well.\n\nC.3. GPT-4 PREDICTED DIFFICULTY\n\nUltimately, we care about strong models generalizing from human supervision. From this perspec-\ntive, it is important to understand whether we can achieve easy-to-hard generalization, where the dif-\nficulty is measured according to humans, rather than capacity-constrained models. In Appendix C.1,\nwe explored this question in chess, but we would want to extend this analysis to the NLP tasks.\n\nMost natural datasets do not come with information about problem difficulty. As a rough estimate,\nwe automatically generated difficulty labels using GPT-4. More concretely, we used GPT-4 to rank\npairs of examples in each dataset, asking “which question is easier, Question A or Question B?” We\nthen calculated the Elo scores for each example via a finite number of random comparisons.\n\n39\n", "vlm_text": "The image contains two plots labeled (a) and (b):\n\n(a) **Box Plot**: This plot shows GPT-4 generated Elo scores against problem difficulty levels (1 to 5) on the MATH dataset. The Elo scores increase with problem difficulty, indicating how they vary across different levels of problem complexity.\n\n(b) **Line Graph**: This graph represents average test accuracy as a function of strong student compute on NLP tasks. Three training methods are compared:\n- Black line: Trained on samples of all difficulties.\n- Orange line: Trained on the 30% easiest tasks.\n- Blue line: Trained on the 50% easiest tasks.\n\nTest accuracy improves consistently with increased compute power, with variations based on the difficulty level of training samples. The black line, which represents training on all difficulties, achieves the highest accuracy overall.\nWe define  difficulty  of a datapoint based on the smallest model size that consistently predicts the label on this datapoint correctly, when trained on ground truth. For example, suppose we have 4 ground truth models  $W_{1}$  ,    $W_{2}$  ,  $W_{3}$  ,    $W_{4}$   that use compute    $C_{1}\\,<\\,C_{2}\\,<\\,\\bar{C}_{3}\\,<\\,\\bar{C}_{4}$   respectively. Suppose models    $W_{1}$  ,  $W_{3}$  ,  $W_{4}$   predict the example correctly when it is in a held-out set, while    $W_{2}$  predicts it incorrectly. Then we will assign a difficulty of    $C_{3}$   to the example. \nThen given a difficulty cutoff    $D$  , we filter the training set to examples with difficulty    $\\le\\,D$  . We subsample the filtered set so that the number of training examples is equal to the number of examples at the lowest difficulty level. We train a model on the subsampled training set using ground truth labels, and measure its accuracy on a held out test set (with no sub sampling). \nThe sub sampling ensures that we use the same training set size for each difficulty cutoff. Using ground truth labels ensures that the label accuracy is the same   $(100\\%)$   for each cutoff. We also use the same test set for each cutoff. This setup lets us vary only training data difficulty, and measure its impact on the trained model’s accuracy. \nWe plot results in Figure  21 . The  $y$  -axis is accuracy on the test set, while the    $x$  -axis is the difficulty cutoff. Increasing the difficulty cutoff generally leads to an increase in accuracy. This result suggests that solving easy-to-hard generalization is non-trivial even if there are no weak label errors. \nFor smaller models (darker lines), the accuracy initially increases, but starts to decrease beyond a point. The drop generally happens when the difficulty cutoff exceeds the capacity of the model itself, i.e. when the examples are too difficult for the model to fit. However, large models trained on easy examples often perform well. \nC.3 GPT-4  PREDICTED DIFFICULTY \nUltimately, we care about strong models generalizing from human supervision. From this perspec- tive, it is important to understand whether we can achieve easy-to-hard generalization, where the dif- ficulty is measured according to humans, rather than capacity-constrained models. In Appendix  C.1 , we explored this question in chess, but we would want to extend this analysis to the NLP tasks. \nMost natural datasets do not come with information about problem difficulty. As a rough estimate, we automatically generated difficulty labels using GPT-4. More concretely, we used GPT-4 to rank pairs of examples in each dataset, asking “which question is easier, Question A or Question B?” We then calculated the Elo scores for each example via a finite number of random comparisons. "}
{"page": 39, "image_path": "doc_images/2312.09390v1_39.jpg", "ocr_text": "Table 3: Weak-to-strong generalization on ImageNet. We train linear probes on the representa-\ntions extracted by DINO models with weak supervision from an AlexNet model. The strong students\nsubstantially outperform their weak supervisor.\n\nModel Top-1 Accuracy (%) PGR (%)\nAlexNet (weak supervisor) 56.6 -\nDino ResNet50 63.7 -\nDino ViT-B/8 74.9 -\nAlexNet — DINO ResNet50 60.7 57.8\nAlexNet — DINO ViT-B/8 64.2 41.5\n\nTo evaluate the quality of GPT-4 Elo score as a measure of difficulty, we performed correlation anal-\nysis against human annotations for datasets with human difficulty levels such as MATH (Hendrycks\net al., 2021) and chess, as well as against weak model confidence. We found that the three measures\nalign better for reasoning tasks such as MATH, as we show in Figure 22(a), but not much for some\nnatural language tasks. When looking at the samples, we found that GPT-4 Elo scores tend to be\nhigher for longer questions, but those questions may actually be easy for smaller models since they\nprovide more context.\n\nUsing GPT-4 Elo score as a proxy for human difficulty, we used different cutoffs on scores to sep-\narate easy and hard examples, trained the strong models on the easy examples only (with ground\ntruth labels), and evaluated on the hard examples. Preliminary results are shown in Figure 22(b).\n\nIn general, we found that using GPT-4 Elo as measure of hardness makes generalization slopes\nsteeper than our main setup of weak-to-strong generalization. One possible confounder for interpre-\ntation is that our Elo measurements could be noisy, causing generalization to be better.\n\nNote that this setup is a classic covariate shift problem, whereas our main setup focuses more on\nconcept shift and noisy labels. It is unclear which setup would be more relevant, and we think it is\nimportant to study easy-to-hard generalization more thoroughly in future work.\n\nD OTHER WEAK-TO-STRONG SETTINGS\n\nD.1 SELF-SUPERVISED VISION MODELS\n\nWe additionally demonstrate weak-to-strong generalization in a simple image classification experi-\nment. We use a pretrained AlexNet model (Krizhevsky et al., 2012) as a weak supervisor, and use\nit to generate weak labels on the ImageNet (Russakovsky et al., 2015) validation set. As a strong\nstudent, we use linear probing on frozen representations extracted by DINO models (Caron et al.,\n2021) based on ResNet-50 (He et al., 2016) and ViT-B/8 (Dosovitskiy et al., 2020) architectures.\nThe DINO models are pretrained in an unsupervised way and did not observe direct supervision for\nImageNet classification or any other classification task during pretraining, so this experiment does\nnot have the pretraining leakage disanalogy discussed in Section 6.1.\n\nWe use 40k datapoints from the validation set to train the linear probes, and evaluate performance\non the remaining 10k datapoints. For training the linear probes, we use a batch size of 128, Adam\noptimizer (Kingma & Ba, 2014) and a learning rate of 10-3. We run 20 epochs of training for\nResNet-50 and 5 epochs for ViT-B/8.\n\nWe report the results in Table 3. Similarly to our main experiments in Section 4, the student can\nsubstantially outperform the supervisor, achieving PGR on the order of 50%. This experiment shows\nthat our results are not limited to the natural language setting, and generalize to other domains. It\nalso shows that strong students can generalize from weak supervision on tasks where they only had\nindirect pretraining, i.e. where the knowledge of the task is latent.\n\n40\n", "vlm_text": "The table presents performance metrics for different models. It includes:\n\n- **Models:**\n  - AlexNet (with weak supervision)\n  - Dino ResNet50\n  - Dino ViT-B/8\n  - AlexNet fine-tuned with Dino ResNet50\n  - AlexNet fine-tuned with Dino ViT-B/8\n\n- **Metrics:**\n  - **Top-1 Accuracy (%)**: Measures the percentage of times the model's top guess is correct.\n    - AlexNet (weak supervisor): 56.6%\n    - Dino ResNet50: 63.7%\n    - Dino ViT-B/8: 74.9%\n    - AlexNet → DINO ResNet50: 60.7%\n    - AlexNet → DINO ViT-B/8: 64.2%\n    \n  - **PGR (%)**: Presumed to represent some performance gain ratio or related metric.\n    - AlexNet → DINO ResNet50: 57.8%\n    - AlexNet → DINO ViT-B/8: 41.5%\n\nPGR is not provided for the original models without fine-tuning.\nTo evaluate the quality of GPT-4 Elo score as a measure of difficulty, we performed correlation anal- ysis against human annotations for datasets with human difficulty levels such as MATH ( Hendrycks et al. ,  2021 ) and chess, as well as against weak model confidence. We found that the three measures align better for reasoning tasks such as MATH, as we show in Figure  22 (a), but not much for some natural language tasks. When looking at the samples, we found that GPT-4 Elo scores tend to be higher for longer questions, but those questions may actually be easy for smaller models since they provide more context. \nUsing GPT-4 Elo score as a proxy for human difficulty, we used different cutoffs on scores to sep- arate easy and hard examples, trained the strong models on the easy examples only (with ground truth labels), and evaluated on the hard examples. Preliminary results are shown in Figure  22 (b). \nIn general, we found that using GPT-4 Elo as measure of hardness makes generalization slopes steeper than our main setup of weak-to-strong generalization. One possible confounder for interpre- tation is that our Elo measurements could be noisy, causing generalization to be better. \nNote that this setup is a classic covariate shift problem, whereas our main setup focuses more on concept shift and noisy labels. It is unclear which setup would be more relevant, and we think it is important to study easy-to-hard generalization more thoroughly in future work. \nD O THER WEAK - TO - STRONG SETTINGS \nD.1 S ELF - SUPERVISED VISION MODELS \nWe additionally demonstrate weak-to-strong generalization in a simple image classification experi- ment. We use a pretrained AlexNet model ( Krizhevsky et al. ,  2012 ) as a weak supervisor, and use it to generate weak labels on the ImageNet ( Russ a kov sky et al. ,  2015 ) validation set. As a strong student, we use linear probing on frozen representations extracted by DINO models ( Caron et al. , 2021 ) based on ResNet-50 ( He et al. ,  2016 ) and ViT-B/8 ( Do sov it ski y et al. ,  2020 ) architectures. The DINO models are pretrained in an unsupervised way and did not observe direct supervision for ImageNet classification or any other classification task during pre training, so this experiment does not have the pre training leakage disanalogy discussed in Section  6.1 . \nWe use  $40k$   datapoints from the validation set to train the linear probes, and evaluate performance on the remaining    $10k$   datapoints. For training the linear probes, we use a batch size of  128 , Adam optimizer ( Kingma & Ba ,  2014 ) and a learning rate of    $\\mathrm{\\bar{10^{-3}}}$  . We run  20  epochs of training for ResNet-50 and  5  epochs for ViT-B/8. \nWe report the results in Table  3 . Similarly to our main experiments in Section  4 , the student can substantially outperform the supervisor, achieving PGR on the order of  $50\\%$  . This experiment shows that our results are not limited to the natural language setting, and generalize to other domains. It also shows that strong students can generalize from weak supervision on tasks where they only had indirect pre training, i.e. where the knowledge of the task is latent. "}
{"page": 40, "image_path": "doc_images/2312.09390v1_40.jpg", "ocr_text": "80.\n\n90{t121 ol il soffit]\n75\n85 75: 80\n80 70\n75 70 70\n65\n5\" 65 60 is\n£5 60 Fe\n8 60. 60 50. 38\n5 55 ae\n8 90 28\n2 Te] a) as{fel ic} Ey\noe 80 80 78 |\n8 75 tre\n10\" B\n75 6 , 70\n70\n65 0 65 65\n60\n60. 60\n55. 65 55\n70° 10\" 70 T° 40\" 707 70° 107 70 T° 0\" 70\n\nstrong student compute\n(fraction of GPT4)\n\nFigure 23: Linear probing qualitatively matches finetuning weak-to-strong generalization. Test\naccuracy as a function of strong student compute on a subset of our NLP tasks. Inset numbers\nindicate dataset id (compare Figure 12). Accuracy of a linear probe on student model trained with\nground truth in black, accuracy of linear probe on students trained directly with weak linear probe\nsupervision shown in solid lines with circles (hue indicates compute of weak supervision).\n\nD.2 LINEAR PROBING\n\nIn addition to our main finetuning experiments, we also perform weak-to-strong generalization ex-\nperiments in the linear probing setting. We freeze all weak and strong model parameters, and train\nnew linear classification heads both using ground truth labels and using weak labels. We train lin-\near probes with Adam optimizer (Kingma & Ba, 2014), 10-3 learning rate, batch size 128, and no\nweight decay for 200 epochs, for both weak and strong model training. We do early stopping based\non agreement to the weak labels on the validation set and report test accuracy. Results are shown in\nFigure 23. We observe qualitatively similar generalization compared to the full finetuning case.\n\nGenerally, we found the linear probing setting to be very useful to quickly iterate on methods,\ndatasets and ideas. While finetuning provides better results, the qualitative trends in linear probing\nare similar, and the experiments are much faster and easier to run. For example, we initially found\npositive results with confidence loss (Section 4.3) and bootstrapping (Section 4.3.1) in the linear\nprobing setting.\n\nE THE EFFECTS OF WEAK LABEL STRUCTURE\n\nOne challenge in weak-to-strong generalization is the presence of errors in the weak labels.\nThroughout most of this paper, we consider a particular type of weak error structure: the kinds\nof errors smaller, capacity-constrained language models make. However, this is not the only type of\nerrors possible.\n\nIn this section, we analyze synthetic examples of other kinds of weak label structures, and the\nimplications they have on generalization. Weak model error structure must be considered in relation\nto the particular strong model at hand. For example, we conjecture that the extent to which the strong\nmodel can imitate the weak supervisor may be very important. If we have two strong models of the\nsame performance on the actual task but one is very good at imitating the labels, then we expect that\nmodel will generalize less desirably, at least with the naive finetuning method.\n\nIn Section 5.1.3 we found that surprisingly the strongest students are imitating the weak supervisor\nmistakes less than smaller student models in our setting. Since we expect superhuman models to\nbe very good at imitating human supervisor, this may be a major disanalogy. In this section we test\ncases where the weak supervisor can be imitated easily.\n\n41\n", "vlm_text": "The image consists of a series of line graphs showing the test accuracy (%) as a function of strong student compute (expressed as a fraction of GPT-4) across different NLP tasks. Each subplot is identified by a dataset ID indicated in square brackets, and the performance of models is compared in different scenarios:\n\n1. **Black Line:** Represents the accuracy of a linear probe on a student model trained with ground truth.\n2. **Colored Lines with Circles:** Show the accuracy of models trained directly with weak linear probe supervision, with the color (hue) indicating the compute used for weak supervision.\n\nThe x-axis is on a logarithmic scale, displaying the varying levels of strong student compute, while the y-axis represents the test accuracy percentage.\n\nThe caption suggests that linear probing qualitatively matches fine-tuning when moving from weak to strong generalization in these tasks.\nD.2 L INEAR PROBING \nIn addition to our main finetuning experiments, we also perform weak-to-strong generalization ex- periments in the linear probing setting. We freeze all weak and strong model parameters, and train new linear classification heads both using ground truth labels and using weak labels. We train lin- ear probes with Adam optimizer ( Kingma & Ba ,  2014 ),  $10^{-3}$    learning rate, batch size 128, and no weight decay for 200 epochs, for both weak and strong model training. We do early stopping based on agreement to the weak labels on the validation set and report test accuracy. Results are shown in Figure  23 . We observe qualitatively similar generalization compared to the full finetuning case. \nGenerally, we found the linear probing setting to be very useful to quickly iterate on methods, datasets and ideas. While finetuning provides better results, the qualitative trends in linear probing are similar, and the experiments are much faster and easier to run. For example, we initially found positive results with confidence loss (Section  4.3 ) and boots trapping (Section  4.3.1 ) in the linear probing setting. \nE T HE EFFECTS OF WEAK LABEL STRUCTURE \nOne challenge in weak-to-strong generalization is the presence of errors in the weak labels. Throughout most of this paper, we consider a particular type of weak error structure: the kinds of errors smaller, capacity-constrained language models make. However, this is not the only type of errors possible. \nIn this section, we analyze synthetic examples of other kinds of weak label structures, and the implications they have on generalization. Weak model error structure must be considered in relation to the particular strong model at hand. For example, we conjecture that the extent to which the strong model can imitate the weak supervisor may be very important. If we have two strong models of the same performance on the actual task but one is very good at imitating the labels, then we expect that model will generalize less desirably, at least with the naive finetuning method. \nIn Section  5.1.3  we found that surprisingly the strongest students are imitating the weak supervisor mistakes less than smaller student models in our setting. Since we expect superhuman models to be very good at imitating human supervisor, this may be a major disanalogy. In this section we test cases where the weak supervisor can be imitated easily. "}
{"page": 41, "image_path": "doc_images/2312.09390v1_41.jpg", "ocr_text": "10— ©100— ©1000— ©0.576— -00.758— 00.845— © 0.1— ©005- © 0-—\n\n(a) ©30- ©300- —baseline  (b) © 0.7 — ©0.803- — baseline (¢) -¢0.07- ©0.01—- — baseline\n90 82\n85: BS 81\n\n=> => Se =>\n\nE E a Le E 80\n\n8 8 75 gan g 79\n\n5 5 oe 5\n\n3 3 Z- 3 78\n\n8 8 70 ® 77,\n\nzB zB zB\n\n2 2 65: 2 46.\n60 75\n55: 74\n\nSs\n\nry\ni}\n\n95.\n90:\n85\n80\n75\n70:\n65\n60:\n\n2auN 12 @\nSasasa\ney 2 2\nEy & 8s\n\n&\n\ni}\nSy\na\n\nstudent-supervisor agreement (%)\nstudent-supervisor agreement (%) =\nstudent-supervisor agreement (%) =\n\n10 30 100 300 1000 full\n# features\n\n0.1 0.07 0.05 0.01 0 full\n300 features + X noise\n\nnoise\n\nFigure 24: Synthetic experiment on simulation difficulty. We consider three types of weak errors\nin a linear probing setting: (a,d) perfectly simulatable, where weak models use a subset of strong\nmodel features; (b,e) completely unsimulatable, where the weak labels are obtained by applying\nrandom noise to the ground truth; (¢,f) a mixture of the two settings, where label noise is applied\nto perfectly simulatable weak labels. Top row of panels shows test accuracy and bottom row shows\nagreement to the weak labels. In addition to weak label accuracy, the structure of mistakes plays a\nmajor role in weak-to-strong generalization.\n\nE.1 SYNTHETIC EXPERIMENTS ON SIMULATION DIFFICULTY\n\nFirst, we consider a simplified linear probing setting, where we can ensure that the student can per-\nfectly simulate the supervisor predictions by construction. Specifically, we extract a representation\nX € R\"*4 of the SciQ dataset using a model of an intermediate size in the GPT-4 family, where n\nis the number of datapints, and d is the dimensionality of the residual stream (Elhage et al., 2021).\nWe can then consider the family of linear models!° M;, where k < d by training a linear probe only\non the first k features extracted by the model. In particular, for k = d we recover the standard linear\nprobe. By construction for k; > kz, the model M;,, can perfectly simulate M,, .\n\nNext, we can run our standard weak-to-strong generalization experiment, following the setup de-\nscribed in Section 3, using the family of models M;. We train the weak supervisor models on 10k\ndatapoints, and produce hard weak labels on the remaining 13k datapoints. We report the results\nin Figure 24(a,d). In this setting, the simulation is very easy, and we do not observe substantial\nimprovements in the strong student model compared to the supervisor performance. The test agree-\nment values are substantially higher than the weak model accuracy, indicating that the students are\noverfitting to the supervisor errors. Interestingly, even in this simple setting the agreements are\nnot 100%, likely due to the fact that the student models are trained on finite data, and with light\n[2-regularization.\n\nWe can also consider the opposite setting: what if the student model cannot simulate the mistakes\nof the weak teacher at all? Specifically, we generate weak labels by randomly flipping the labels\nto match the accuracy of the weak models from the previous experiment. As a result, we get weak\nlabels with the same accuracy, but which are completely unpredictable. In Figure 24(b,e), when we\ntrain the student model on the these weak labels, we can get substantially higher accuracy than the\naccuracy of the weak labels. In other words, if the errors of the weak supervisor are completely\nunpredictable (random) for the student, with enough data we should be able to recover good gener-\nalization, substantially exceeding the performance of the supervisor.\n\n‘We train logistic regression using the default parameters in the sklearn.linear_model.\nLogisticRegression class (Pedregosa et al., 2011) for this experiment.\n\n42\n", "vlm_text": "The image consists of six panels, organized in two rows, showing the results of a synthetic experiment related to simulation difficulty in a linear probing setting. The experiment is divided into three types of weak errors, each represented in two panels. \n\n1. Panels (a) and (d) depict the scenario of perfectly simulatable errors, where weak models use a subset of strong model features. \n   - Panel (a) shows test accuracy as a percentage, plotted against different numbers of features (10, 30, 100, 300, 1000).\n   - Panel (d) presents the student-supervisor agreement as a percentage, with the same feature numbers.\n\n2. Panels (b) and (e) illustrate the completely unsimulatable errors scenario, where weak labels come from random noise added to the ground truth.\n   - Panel (b) presents test accuracy against varying levels of noise (0.576, 0.7, 0.758, 0.803, 0.845).\n   - Panel (e) shows student-supervisor agreement against these noise levels.\n\n3. Panels (c) and (f) focus on a mixed setting where label noise is added to perfectly simulatable weak labels.\n   - Panel (c) displays test accuracy against varying degrees of feature numbers plus noise.\n   - Panel (f) provides student-supervisor agreement against the same parameters.\n\nEach plot showcases multiple lines representing different settings or baselines, which are indicated by different colored markers and line styles. The results suggest how feature number and noise influence test accuracy and student-supervisor agreement, noting that the structure of mistakes is crucial for weak-to-strong generalization.\nE.1 S YNTHETIC EXPERIMENTS ON SIMULATION DIFFICULTY \nFirst, we consider a simplified linear probing setting, where we can ensure that the student can per- fectly simulate the supervisor predictions by construction. Specifically, we extract a representation  $X\\in\\dot{\\mathbb{R}}^{n\\times d}$    of the SciQ dataset  sing a model of an intermediate size in the GPT-4 family, where    $n$  is the number of datapints, and  d  is the dimensionality of the residual stream ( Elhage et al. ,  2021 ). We can the consider the family of linear models 10    $\\mathcal{M}_{k}$   where  $k\\leq d$  on the first  k  features extracted by the model. In particular, for  $k=d$   we recover the standard linear probe. By construction for    $k_{1}\\geq k_{2}$  , the model    $\\mathcal{M}_{k_{1}}$   can perfectly simulate  $\\mathcal{M}_{k_{2}}$  . \nNext, we can run our standard weak-to-strong generalization experiment, following the setup de- scribed in Section  3 , using the family of models  $\\mathcal{M}_{k}$  . We tra he weak supervisor models on  $10k$  datapoints, and produce hard weak labels on the remaining  $13k$   datapoints. We report the results in Figure  24 (a,d). In this setting, the simulation is very easy, and we do not observe substantial improvements in the strong student model compared to the supervisor performance. The test agree- ment values are substantially higher than the weak model accuracy, indicating that the students are over fitting to the supervisor errors. Interestingly, even in this simple setting the agreements are not    $100\\%$  , likely due to the fact that the student models are trained on finite data, and with light  $l_{2}$  -regular iz ation. \nWe can also consider the opposite setting: what if the student model cannot simulate the mistakes of the weak teacher at all? Specifically, we generate weak labels by randomly flipping the labels to match the accuracy of the weak models from the previous experiment. As a result, we get weak labels with the same accuracy, but which are completely unpredictable. In Figure  24 (b,e), when we train the student model on the these weak labels, we can get substantially higher accuracy than the accuracy of the weak labels. In other words, if the errors of the weak supervisor are completely unpredictable (random) for the student, with enough data we should be able to recover good gener- alization, substantially exceeding the performance of the supervisor. "}
{"page": 42, "image_path": "doc_images/2312.09390v1_42.jpg", "ocr_text": "[4]\n\nIbo\n\nCobaseline\n(aux. loss\n\na)\nao\n\nNoa\na)\n\n, performance gap recovered (%)\noO\n\nonan\nSaaa\n\n100 6\n\n: encal|\n\n75/0 baseline\n(aux. loss\n\nperformance gap recovered (%)\nOo\n\n0 2]\n\nC baseline\n(aux. loss\n\nno\na\n\nNoa\nao\n\n, performance gap recovered (%)\noO\n\nonan\nSaag\n\n$\nOY\n‘a\n\nFigure 25: PGR for weak labels with same accuracy but different error structures. The inset\nnumber in each panel indicates the dataset (compare Figure 12). Weak-to-strong generalization and\nmethods both depend critically on the structure of the weak supervisor errors. While it is trivial to\npick error structures that generalize well (for instance, random noise), these error structures are also\n\nvery disanalogous to the ultimate superalignment setting, where we want to study the structures of\nhuman errors.\n\n43\n", "vlm_text": "The image contains three bar graphs showing the \"performance gap recovered (%)\" for different error structures in weak supervisors. Each panel corresponds to a specific dataset, labeled [4], [6], and [12]. Performance is compared between two methods: \"baseline\" (blue bars) and \"aux. loss\" (orange bars).\n\nThe x-axis lists different weak supervisor error structures:\n\n1. Weak supervisor\n2. Random\n3. Longest prompt\n4. Shortest prompt\n5. Strong ground truth (unconfident)\n6. Model unconfident\n7. Strong ground truth (confidently correct)\n8. Model confidently correct\n\nThe y-axis shows performance gap recovery percentage, ranging from -100% to 100%. The inset number in each panel references the dataset associated with each graph. \n\nThe results show how different error structures affect the generalization from weak to strong labels, highlighting cases where auxiliary loss outperforms the baseline and vice versa."}
{"page": 43, "image_path": "doc_images/2312.09390v1_43.jpg", "ocr_text": "Finally, in Figure 24(c,f) we consider a mixture of these two settings: we start with a perfectly\nsimulatable weak model M39, and then add various amounts of label noise to the resulting weak\nlabels. By training a strong student model (using all features) on the resulting weak labels, we\nrecover the performance close to the performance of M399.\n\nDiscussion of results. The simple experiment in this section suggests that in addition to the weak\nlabel accuracy, it is important to consider the structure of weak errors. In particular, if the weak er-\nrors are extremely easy for the strong model to simulate, the student may not generalize much better\nthan the weak supervisor with naive finetuning on the weak labels. On the other hand, if the mistakes\nof the weak supervisor are completely unpredictable, the student can denoise the predictions of the\nsupervisor and generalize better. In future work, we believe it is important to consider various types\nof weak supervision with different structures of mistakes, and build a better understanding of how\nthey affect weak-to-strong generalization.\n\nE.2. DIFFERENT WEAK ERROR STRUCTURE MEANS DIFFERENT GENERALIZATION\n\nTo further explore the impact of different weak error structures, we created several synthetic sets of\nweak labels for each dataset, all with error rate identical to the weak model’s error rate. To construct\nthese labels, we start from ground truth, and then flip a subset of labels to match the accuracy of a\nparticular weak model. We target a few types of error structures, such as pure noise, easy-to-model\nbias, hard-to-model bias, and adversarial bias.\n\nIn particular, we looked at:\n\n1. weak supervisor: the baseline — labels are generated in the same way as in the rest of\nthe paper\n\n. random: flip the label of random datapoints\n\n. longest prompt: flip the label of longest datapoints by characters\n\n. shortest prompt: flip the label of shortest datapoints by characters\n\nn Wh\n\n. strong g.t. model unconfident: flip the label of the datapoints that the strong ceil-\ning model is most unconfident on\n\n6. strong g.t. model confidently correct: flips the label of the datapoints that the\nstrong ceiling model is most confidently correct on\n\nDespite all of these weak labelers having the same weak accuracy, we find that the generalization\ncan vary wildly depending on the structure of the weak errors. We report the results in Figure 25.\n\nFurthermore, the dynamics of supervisor-student agreement through training can have qualitatively\ndifferent behavior (Figure 26). For errors coming from a weak model, we see that there is often ini-\ntially a period of generalization, followed by a period of overfitting where it learns the weak model’s\nerrors. The confidence auxiliary loss mitigates this overfitting. For easy-to-fit error structures such\nas longest prompt, the overfitting happens much faster. For other kinds of errors, such as random\nnoise, we often see that generalization improves throughout: weak errors are not modeled, but the\nsignal from the weak model is.\n\nE.3. MAKING IMITATION TRIVIAL\n\nOne possible major disanalogy in our setup, as discussed in Section 6.1, is the fact that our models\nare not very good at imitating the weak model!! (Section 5.1.3), but superhuman models may be\nvery good at imitating humans. It is possible that if the strong model were good at imitating the\nweak model, then it would generalize substantially less desirably by default.\n\nTo test an extreme version of this hypothesis, we create a synthetic setting where the strong model\ncan trivially imitate the weak model very well. In particular, we modify the task by appending “I\nthink this is {weak_labe1}. What do you think?” to every prompt, where weak_label is “correct”\nor “incorrect” based on the weak model prediction. In this case, the hardened weak label is present\nin-context, and the simulation is trivial.\n\n'l Also known as learning the “human simulator” in the terminology of Christiano et al. (2022).\n\n44\n", "vlm_text": "Finally, in Figure  24 (c,f) we consider a mixture of these two settings: we start with a perfectly si mula table weak model    $\\mathcal{M}_{300}$  , and then add various amounts of label noise to the resulting weak labels. By training a strong student model (using all features) on the resulting weak labels, we recover the performance close to the performance of  $\\mathcal{M}_{300}$  . \nDiscussion of results. The simple experiment in this section suggests that in addition to the weak label accuracy, it is important to consider the  structure of weak errors . In particular, if the weak er- rors are extremely easy for the strong model to simulate, the student may not generalize much better than the weak supervisor with naive finetuning on the weak labels. On the other hand, if the mistakes of the weak supervisor are completely unpredictable, the student can denoise the predictions of the supervisor and generalize better. In future work, we believe it is important to consider various types of weak supervision with different structures of mistakes, and build a better understanding of how they affect weak-to-strong generalization. \nE.2 D IFFERENT WEAK ERROR STRUCTURE MEANS DIFFERENT GENERALIZATION \nTo further explore the impact of different weak error structures, we created several synthetic sets of weak labels for each dataset, all with error rate identical to the weak model’s error rate. To construct these labels, we start from ground truth, and then flip a subset of labels to match the accuracy of a particular weak model. We target a few types of error structures, such as pure noise, easy-to-model bias, hard-to-model bias, and adversarial bias. \nIn particular, we looked at: \n1.  weak supervisor : the baseline — labels are generated in the same way as in the rest of the paper 2.  random : flip the label of random datapoints 3.  longest prompt : flip the label of longest datapoints by characters 4.  shortest prompt : flip the label of shortest datapoints by characters 5.  strong g.t. model un confident : flip the label of the datapoints that the strong ceil- ing model is most un confident on 6.  strong g.t. model confidently correct : flips the label of the datapoints that the strong ceiling model is most confidently correct on \nDespite all of these weak labelers having the same weak accuracy, we find that the generalization can vary wildly depending on the structure of the weak errors. We report the results in Figure  25 . \nFurthermore, the dynamics of supervisor-student agreement through training can have qualitatively different behavior (Figure  26 ). For errors coming from a weak model, we see that there is often ini- tially a period of generalization, followed by a period of over fitting where it learns the weak model’s errors. The confidence auxiliary loss mitigates this over fitting. For easy-to-fit error structures such as  longest prompt , the over fitting happens much faster. For other kinds of errors, such as random noise, we often see that generalization improves throughout: weak errors are not modeled, but the signal from the weak model is. \nE.3 M AKING IMITATION TRIVIAL \nOne possible major disanalogy in our setup, as discussed in Section  6.1 , is the fact that our models are not very good at imitating the weak model 11   (Section  5.1.3 ), but superhuman models may be very good at imitating humans. It is possible that if the strong model were good at imitating the weak model, then it would generalize substantially less desirably by default. \nTo test an extreme version of this hypothesis, we create a synthetic setting where the strong model can trivially imitate the weak model very well. In particular, we modify the task by appending “I think this is    $\\{{\\tt w e a k}\\tt_{-}{\\tt l a b e l}\\}$  . What do you think?” to every prompt, where weak label is “correct” or “incorrect” based on the weak model prediction. In this case, the hardened weak label is present in-context, and the simulation is trivial. "}
{"page": 44, "image_path": "doc_images/2312.09390v1_44.jpg", "ocr_text": "[6] baseline — weak supervisor correct —\naux.loss — weak supervisor wrong +--+\n\nstudent-supervisor\nagreement (%)\n\nweak supervisor longest prompt shortest prompt\n\nS\nS\n\nstudent-supervisor\nagreement (%)\n\n20\n0 random strong g.t. model unconfident stron Sty conect\n0 0.5 1 15 20 0.5 1 1.5 20 0.5 1 15 2\nprogress (fraction of epoch)\n[4] baseline — weak supervisor correct —\naux.loss — weak supervisor wrong +--+\n100\n\n80 / weak supervisor\n60\n\nS\nS\n\nstudent-supervisor\nagreement (%)\n\nnN\n8S\n\nlongest prompt shortest prompt\n\nS\nSo\n\ney\nSs\n\nrandom\n\nRO\n68\n\nstudent-supervisor\nagreement (%)\n\nstrong g.t. model\nstrong g.t. model unconfident confidently correct\n\nnN\n3S\n\n0 0.5 1 15 20 0.5 1 1.5 20 0.5 1 15 2\nprogress (fraction of epoch)\n\n[12] baseline — weak supervisor correct —\naux.loss — _ weak supervisor wrong\n\na Armee\n\nweak supervisor\n\nstudent-supervisor\nagreement (%)\n\nlongest prompt shortest prompt\n\nS\nS\n\nrandom\nstrong g.t. model\n\nconfidently correct\n\nstudent-supervisor\nagreement (%)\n\nstrong g.t. model unconfident\n\n0.5 1 15 20 0.5 1 1.5 2\nprogress (fraction of epoch)\n\nFigure 26: Training dynamics change for different weak errors. We show teacher-student agree-\nment for different weak error structures on three datasets. We see that the training dynamics have\nqualitatively different behavior for different error structures, despite all weak labelers having the\nsame accuracy.\n\n45\n", "vlm_text": "The image displays a series of line graphs showing the training dynamics related to teacher-student agreement across different weak error structures for three datasets. Each row of graphs corresponds to a different dataset.\n\nKey elements in the graphs include:\n\n- **X-Axis**: Progress (fraction of epoch)\n- **Y-Axis**: Student-supervisor agreement (%)\n- **Lines**: \n  - Baseline (solid blue)\n  - Auxiliary loss (solid orange)\n  - Weak supervisor correct (dotted blue)\n  - Weak supervisor wrong (dotted orange)\n\nEach graph represents different weak error structures, such as \"weak supervisor,\" \"longest prompt,\" \"shortest prompt,\" \"random,\" \"strong g.t. model unconfident,\" and \"strong g.t. model confidently correct.\" The plots reveal varying dynamics in student-supervisor agreement depending on the error structure, despite all weak labelers having the same accuracy."}
{"page": 45, "image_path": "doc_images/2312.09390v1_45.jpg", "ocr_text": "40\n\nCobaseline\n30 | taux. loss\n\n20 fo] [12]\n\npi ia\n10\nofl {] mw om om ||\n\nNLP Tasks\n\nan\nSI\n\nperformance gap recovered (%)\n\nt\nnN\n3S\n\nFigure 27: Generalization when emulating weak labels is trivial. Very little weak-to-strong gen-\neralization occurs if emulating the weak labels is trivial: average PGR across tasks is 0.002 + 0.003\nfor baseline, and 0.046 + 0.108 for aux loss, compared to around 0.2 and 0.8 respectively for the\noriginal tasks.\n\nAs expected, we find that both the baseline and the confidence loss introduced in Section 4.3 show\npoor weak-to-strong generalization (Figure 27) in most cases. Interestingly, the confidence loss still\nimproves upon the baseline achieving non-trivial generalization in several tasks.\n\nF HOW SHOULD WE EMPIRICALLY STUDY SUPERALIGNMENT,\nMETHODOLOGICALLY?\n\nWhat makes a setup good for studying superalignment in the first place, all things considered?\nTractability and ease of study are clearly important criteria, but also certainly not the only ones.\nThis question is non-obvious because superalignment is qualitatively different from other machine\nlearning problems: it is a problem we will face in the future, not a problem that we face today.\nNevertheless, it is crucial that we solve this problem before it becomes serious, as even a single\nfailure of superintelligence misalignment in practice could be catastrophic.\n\nThis presents a major methodological challenge: how do we even approach studying a problem that\nis not yet a problem? How do we make progress on the core difficulties of superalignment? How do\nwe make progress with today’s systems, knowing that our efforts will not be wasted by surprising\nnew model capabilities that will inevitably arise in the future (Wei et al., 2022)? We do not claim to\nhave a complete answer to these questions, but we outline some best practices for maximizing our\nchances of making real progress on superalignment.\n\nAnalogous setups. We should construct increasingly analogous empirical setups, and we should\nenumerate any remaining disanalogies. A setup is analogous if our results on that setup do not rely\non assumptions that will break down in the future, making results today likely qualitatively similar\nto results in the future. Our main evaluation setup, introduced in Section 3, is intended to be more\nanalogous to the superalignment problem. We enumerate some remaining disanalogies with our\nsetup in Section 6.1.\n\nEnumerating assumptions. We should enumerate the key assumptions that our results (either\nimplicitly or explicitly) rely on. Clarifying what assumptions we are making makes it much easier\nto know when our results might break down. We enumerate our main disanalogies and assumptions\nin Section 6.1 and Appendix G.3.\n\nSensitivity analysis. We should evaluate the sensitivity of our results to changes in our assump-\ntions and empirical setup. While we can make informed guesses about the future, we do not know\nexactly what future models will be like, so it is difficult to entirely trust any particular experimen-\ntal setup. Validating that our results are robust to many different sets of assumptions can make us\nsubstantially more confident our results will transfer to the future superalignment problem. We do\nsome initial sensitivity analysis in Appendix E, and intend to do much more in future work.\n\nScalable techniques. We should avoid techniques that rely on assumptions that will likely break\ndown for future (superhuman) models. For example, when we do few-shot prompting we are in-\n\n46\n", "vlm_text": "The image is a bar chart comparing the performance gap recovered (%) across different NLP tasks when using baseline methods and auxiliary loss methods to emulate weak labels. \n\n- The x-axis represents different NLP tasks.\n- The y-axis represents the percentage of the performance gap recovered.\n- There are two data series: one for the baseline (represented by blue bars) and one for the auxiliary loss (represented by orange bars).\n- Each task has a label above the bars, ranging from [1] to [12].\n\nOverall, the chart indicates how minor the generalization is from weak to strong labels, with the auxiliary loss occasionally providing a slight improvement over the baseline.\nAs expected, we find that both the baseline and the confidence loss introduced in Section  4.3  show poor weak-to-strong generalization (Figure  27 ) in most cases. Interestingly, the confidence loss still improves upon the baseline achieving non-trivial generalization in several tasks. \nF H OW SHOULD WE EMPIRICALLY STUDY SUPER ALIGNMENT , METHODOLOGICAL LY ? \nWhat makes a setup good for studying super alignment in the first place, all things considered? Tract ability and ease of study are clearly important criteria, but also certainly not the only ones. This question is non-obvious because super alignment is qualitatively different from other machine learning problems: it is a problem we will face in the future, not a problem that we face today. Nevertheless, it is crucial that we solve this problem  before  it becomes serious, as even a single failure of super intelligence misalignment in practice could be catastrophic. \nThis presents a major methodological challenge: how do we even approach studying a problem that is not yet a problem? How do we make progress on the core difficulties of super alignment? How do we make progress with today’s systems, knowing that our efforts will not be wasted by surprising new model capabilities that will inevitably arise in the future ( Wei et al. ,  2022 )? We do not claim to have a complete answer to these questions, but we outline some best practices for maximizing our chances of making real progress on super alignment. \nAnalogous setups. We should construct increasingly analogous empirical setups, and we should enumerate any remaining d is analogies. A setup is analogous if our results on that setup do not rely on assumptions that will break down in the future, making results today likely qualitatively similar to results in the future. Our main evaluation setup, introduced in Section  3 , is intended to be more analogous to the super alignment problem. We enumerate some remaining d is analogies with our setup in Section  6.1 . \nEnumerating assumptions. We should enumerate the key assumptions that our results (either implicitly or explicitly) rely on. Clarifying what assumptions we are making makes it much easier to know when our results might break down. We enumerate our main d is analogies and assumptions in Section  6.1  and Appendix  G.3 . \nSensitivity analysis. We should evaluate the sensitivity of our results to changes in our assump- tions and empirical setup. While we can make informed guesses about the future, we do not know exactly what future models will be like, so it is difficult to entirely trust any particular experimen- tal setup. Validating that our results are robust to many different sets of assumptions can make us substantially more confident our results will transfer to the future super alignment problem. We do some initial sensitivity analysis in Appendix  E , and intend to do much more in future work. \nScalable techniques. We should avoid techniques that rely on assumptions that will likely break down for future (superhuman) models. For example, when we do few-shot prompting we are in- tuitively in centi viz ing models to predict some useful distribution of human text, whereas when we do finetuning we are intuitively in centi viz ing a model to output what it knows regardless of how it knows it. This is one of the reasons we focus on finetuning methods in this paper: they are more likely to scale to superhuman models compared to prompting. "}
{"page": 46, "image_path": "doc_images/2312.09390v1_46.jpg", "ocr_text": "tuitively incentivizing models to predict some useful distribution of human text, whereas when we\ndo finetuning we are intuitively incentivizing a model to output what it knows regardless of how it\nknows it. This is one of the reasons we focus on finetuning methods in this paper: they are more\nlikely to scale to superhuman models compared to prompting.\n\nIncidental usefulness today. One possible validation that progress on our setup is real would\nbe to show that it is incidentally useful in practice today; while we advocate focusing on the core\nchallenges of superalignment, if our findings are never useful with today’s models that would be\nevidence that we are not on the right track. One example of a near-term practical milestone would\nbe to align GPT-4 on instruction-following tasks using only GPT-3-level supervision; if we could get\nstrong alignment without any humans involved at all, that would make alignment much simpler and\ncheaper today. However, usefulness today is certainly not sufficient for aligning superintelligence,\nand in general a common failure mode of empirical alignment research is it prioritizes usefulness\ntoday at the expense of analogousness and scalability.\n\nUpdating over time. We should update our evaluations and validate past findings as we learn\nmore about what future models will look like. While we focus on the pretrained language model\nparadigm today, we plan on updating our setup if or when this stops being the dominant paradigm.\n\nG HOW WEAK-TO-STRONG GENERALIZATION FITS INTO ALIGNMENT\n\nSuperintelligent AI systems will be extraordinarily powerful; humans could face catastrophic risks\nincluding even extinction (CAIS) if those systems are misaligned or misused. It is important for\nAI developers to have a plan for aligning superhuman models ahead of time—before they have the\npotential to cause irreparable harm.\n\nOur plan for aligning superintelligence is a work in progress, but we believe that weak-to-strong\ntechniques could serve as a key ingredient. In this section we sketch several illustrative possiblities\nfor how we could use weak-to-strong generalization to help align superintelligent systems.\n\nG.1 HIGH-LEVEL PLAN\nLeike & Sutskever (2023) propose the following high level plan, which we adopt:\n\n1. Once we have a model that is capable enough that it can automate machine learning—and\nin particular alignment—tesearch, our goal will be to align that model well enough that it\ncan safely and productively automate alignment research.\n\n2. We will align this model using our most scalable techniques available, e.g. RLHF (Chris-\ntiano et al., 2017; Ouyang et al., 2022), constitutional AI (Bai et al., 2022b), scalable over-\nsight (Saunders et al., 2022; Bowman et al., 2022), adversarial training, or—the focus of\nthis paper—-weak-to-strong generalization techniques.\n\n3. We will validate that the resulting model is aligned using our best evaluation tools available,\ne.g. red-teaming (Perez et al., 2022a;b) and interpretability (Ribeiro et al., 2016; Olah et al.,\n2018; Bills et al., 2023; Li et al., 2023).\n\n4. Using a large amount of compute, we will have the resulting model conduct research to\nalign vastly smarter superhuman systems. We will bootstrap from here to align arbitrarily\nmore capable systems.\n\nThe goal of weak-to-strong generalization is to ensure step (2) is solved: align the first model ca-\npable of automating machine learning and alignment research. Importantly, this first model will\nlikely be qualitatively superhuman along important dimensions, so RLHF is unlikely to be sufficient\n(Section 4). If we had a superhuman model, how would we apply weak-to-strong generalization to\nalign it?\n\n47\n", "vlm_text": "\nIncidental usefulness today. One possible validation that progress on our setup is real would be to show that it is incidentally useful in practice today; while we advocate focusing on the core challenges of super alignment, if our findings are never useful with today’s models that would be evidence that we are not on the right track. One example of a near-term practical milestone would be to align GPT-4 on instruction-following tasks using only GPT-3-level supervision; if we could get strong alignment without any humans involved at all, that would make alignment much simpler and cheaper today. However, usefulness today is certainly not sufficient for aligning super intelligence, and in general a common failure mode of empirical alignment research is it prioritizes usefulness today at the expense of analogous ness and s cal ability. \nUpdating over time. We should update our evaluations and validate past findings as we learn more about what future models will look like. While we focus on the pretrained language model paradigm today, we plan on updating our setup if or when this stops being the dominant paradigm. \nGHOW WEAK-TO-STRONG GENERALIZATION FITS INTO ALIGNMENT\nSuper intelligent AI systems will be extraordinarily powerful; humans could face catastrophic risks including even extinction ( CAIS ) if those systems are misaligned or misused. It is important for AI developers to have a plan for aligning superhuman models ahead of time—before they have the potential to cause irreparable harm. \nOur plan for aligning super intelligence is a work in progress, but we believe that weak-to-strong techniques could serve as a key ingredient. In this section we sketch several illustrative pos sib li ties for how we could use weak-to-strong generalization to help align super intelligent systems. \nG.1 H IGH - LEVEL PLAN \nLeike & Sutskever  ( 2023 ) propose the following high level plan, which we adopt: \n1. Once we have a model that is capable enough that it can automate machine learning—and in particular alignment—research, our goal will be to align that model well enough that it can safely and productively automate alignment research. 2. We will align this model using our most scalable techniques available, e.g. RLHF ( Chris- tiano et al. ,  2017 ;  Ouyang et al. ,  2022 ), constitutional AI ( Bai et al. ,  2022b ), scalable over- sight ( Saunders et al. ,  2022 ;  Bowman et al. ,  2022 ), adversarial training, or—the focus of this paper—-weak-to-strong generalization techniques. 3. We will validate that the resulting model is aligned using our best evaluation tools available, e.g. red-teaming ( Perez et al. ,  2022a ; b ) and interpret ability ( Ribeiro et al. ,  2016 ;  Olah et al. , 2018 ;  Bills et al. ,  2023 ;  Li et al. ,  2023 ). 4. Using a large amount of compute, we will have the resulting model conduct research to align vastly smarter superhuman systems. We will bootstrap from here to align arbitrarily more capable systems. \nThe goal of weak-to-strong generalization is to ensure step (2) is solved: align the first model ca- pable of automating machine learning and alignment research. Importantly, this first model will likely be qualitatively superhuman along important dimensions, so RLHF is unlikely to be sufficient (Section  4 ). If we had a superhuman model, how would we apply weak-to-strong generalization to align it? "}
{"page": 47, "image_path": "doc_images/2312.09390v1_47.jpg", "ocr_text": "G.2 ELICITING KEY ALIGNMENT-RELEVANT CAPABILITIES WITH WEAK-TO-STRONG\nGENERALIZATION\n\nThere are many different alignment-relevant capabilities we could try to elicit from a superhuman\nmodel that could significantly help with alignment, including: '?\n\n¢ Safety: does a given behavior produced by an AI system risk the safety of human lives or\nwell-being in important ways?\n\n¢ Honesty: is a given natural language statement true or false?\n\n¢ Instruction following: does a given behavior produced by an AI system follow a user’s\ninstruction faithfully?\n\n* Code security: does some given code have important security vulnerabilities or back-\ndoors? Is it safe to execute it?\n\nIn the ideal case, the capability we elicit from the model would be robust enough that we can turn it\ninto a reward model and safely optimize it; future work should assess the feasibility of this approach.\nAt the opposite extreme, we could potentially use the elicited capability as an “oracle” that we can\nmanually query; intuitively, if we had a superhuman oracle model, we may be able to leverage it to\nhelp us bootstrap to a more robust alignment solution, even if that oracle is not itself entirely robust.\n\nG.3. ALIGNMENT PLAN ASSUMPTIONS\n\nMany alignment plans which appear different on the surface actually depend on heavily correlated\nassumptions. For a given alignment plan, it is also often unclear which subproblems the plan at-\ntempts to solve, and which subproblems the plan assumes are unlikely to be an obstacle. As a result,\nwe think enumerating assumptions is an important part of making progress on alignment.\n\nIn addition to the major disanalogies discussed in Section 6.1, the assumptions we make for an\nalignment plan based on weak-to-strong generalization include:\n\n¢ No deceptive alignment in base models. We assume that pretrained base models (or the\nequivalent in future paradigms) will be highly intelligent but not highly agentic (e.g. will not\nhave long-term goals)—and consequently will not be deceptively aligned (Hubinger et al.,\n2019; Ngo et al., 2022; Carlsmith) out-of-the-box. Our goal is to elicit the superhuman\ncapabilities of this capable but safe base model, and use those capabilities to create an\naligned (possibly agentic) superhuman model.\n\n¢ Elicited concepts are sufficiently robust, or do not need to be. We assume it is ei-\nther possible to solve alignment using only a small amount of optimization applied to the\ncapabilities we elicit, or that it is possible to make weak-to-strong elicited capabilities suf-\nficiently robust against overoptimization.\n\n* The concepts we care about are natural to future AGI. The superhuman base model we\napply weak-to-strong generalization to has some “alignment-complete” concept, such as\nhonesty, that is extrapolated in the way we would endorse if we could understand everything\nthe superhuman model understands, and which is natural enough to the model that it is\nfeasible to elicit.\n\n¢ Sufficiently gradual takeoff. Before we have superintelligence, we will have somewhat\nsuperhuman models long enough that we can use them to finish solving the full superintelli-\ngence alignment problem. We can use it to solve superalignment before it causes recursive\nself improvement or catastrophic damage.\n\n¢ Moderately superhuman models are sufficient to solve alignment. We assume the first\nmodels capable of automating alignment research in practice are moderately superhuman,\ni.e. in a regime similar to what we study empirically in this work. For example, we may\nassume that we only need to bridge a weak-strong gap of at most (say) 4 OOMs of effective\ncompute.\n\n\"Ideally we elicit several related concepts and verify that we get consistent answers between them.\n\n48\n", "vlm_text": "G.2 E LICITING KEY ALIGNMENT - RELEVANT CAPABILITIES WITH WEAK - TO - STRONG GENERALIZATION \nThere are many different alignment-relevant capabilities we could try to elicit from a superhuman model that could significantly help with alignment, including: 12 \n•  Safety:  does a given behavior produced by an AI system risk the safety of human lives or well-being in important ways? •  Honesty:  is a given natural language statement true or false? •  Instruction following:  does a given behavior produced by an AI system follow a user’s instruction faithfully? •  Code security:  does some given code have important security vulnerabilities or back- doors? Is it safe to execute it? \nIn the ideal case, the capability we elicit from the model would be robust enough that we can turn it into a reward model and safely optimize it; future work should assess the feasibility of this approach. At the opposite extreme, we could potentially use the elicited capability as an “oracle” that we can manually query; intuitively, if we had a superhuman oracle model, we may be able to leverage it to help us bootstrap to a more robust alignment solution, even if that oracle is not itself entirely robust. \nG.3 A LIGNMENT PLAN ASSUMPTIONS \nMany alignment plans which appear different on the surface actually depend on heavily correlated assumptions. For a given alignment plan, it is also often unclear which sub problems the plan at- tempts to solve, and which sub problems the plan assumes are unlikely to be an obstacle. As a result, we think enumerating assumptions is an important part of making progress on alignment. \nIn addition to the major d is analogies discussed in Section  6.1 , the assumptions we make for an alignment plan based on weak-to-strong generalization include: \n•  No deceptive alignment in base models.  We assume that pretrained base models (or the equivalent in future paradigms) will be highly intelligent but not highly agentic (e.g. will not have long-term goals)—and consequently will not be deceptively aligned ( Hubinger et al. , 2019 ;  Ngo et al. ,  2022 ;  Carlsmith ) out-of-the-box. Our goal is to elicit the superhuman capabilities of this capable but safe base model, and use those capabilities to create an aligned (possibly agentic) superhuman model. •  Elicited concepts are sufficiently robust, or do not need to be.  We assume it is ei- ther possible to solve alignment using only a small amount of optimization applied to the capabilities we elicit, or that it is possible to make weak-to-strong elicited capabilities suf- ficiently robust against over optimization. •  The concepts we care about are natural to future AGI.  The superhuman base model we apply weak-to-strong generalization to has some “alignment-complete” concept, such as honesty, that is extrapolated in the way we would endorse if we could understand everything the superhuman model understands, and which is natural enough to the model that it is feasible to elicit. •  Sufficiently gradual takeoff.  Before we have super intelligence, we will have somewhat superhuman models long enough that we can use them to finish solving the full super in tell i- gence alignment problem. We can use it to solve super alignment before it causes recursive self improvement or catastrophic damage. •  Moderately superhuman models are sufficient to solve alignment.  We assume the first models capable of automating alignment research in practice are moderately superhuman, i.e. in a regime similar to what we study empirically in this work. For example, we may assume that we only need to bridge a weak-strong gap of at most (say) 4 OOMs of effective compute. "}
{"page": 48, "image_path": "doc_images/2312.09390v1_48.jpg", "ocr_text": "¢ No need to solve human values. We assume we do not need to solve hard philosophi-\ncal questions of human values and value aggregation before we can align a superhuman\nresearcher model well enough that it avoids egregiously catastrophic outcomes.\n\nThis list represents a non-exhaustive set of notable assumptions we often operate under, and we\nwill constantly reassess and update these assumptions over time as we learn more. We do not think\nthese are necessarily valid assumptions by default, and believe it is important to validate them, work\ntowards making them true, or mitigate failure modes from them being invalid.\n\nFurthermore, there are a huge number of uncertainties about what future AI systems will look like\nand exactly how we should align them.\n\n49\n", "vlm_text": "•  No need to solve human values.  We assume we do not need to solve hard philosophi- cal questions of human values and value aggregation before we can align a superhuman researcher model well enough that it avoids egregiously catastrophic outcomes. \nThis list represents a non-exhaustive set of notable assumptions we often operate under, and we will constantly reassess and update these assumptions over time as we learn more. We  do not  think these are necessarily valid assumptions by default, and believe it is important to validate them, work towards making them true, or mitigate failure modes from them being invalid. \nFurthermore, there are a huge number of uncertainties about what future AI systems will look like and exactly how we should align them. "}
