{"page": 0, "image_path": "doc_images/D19-1539_0.jpg", "ocr_text": "Cloze-driven Pretraining of Self-attention Networks\n\nAlexei Baevski, Sergey Edunov’, Yinhan Liu*, Luke Zettlemoyer, Michael Auli\nFacebook AI Research\nMenlo Park, CA and Seattle, WA\n\nAbstract\n\nWe present a new approach for pretraining a\nbi-directional transformer model that provides\nsignificant performance gains across a vari-\nety of language understanding problems. Our\nmodel solves a cloze-style word reconstruction\ntask, where each word is ablated and must be\npredicted given the rest of the text. Experi-\nments demonstrate large performance gains on\nGLUE and new state of the art results on NER\nas well as constituency parsing benchmarks,\nconsistent with BERT. We also present a de-\ntailed analysis of a number of factors that con-\ntribute to effective pretraining, including data\ndomain and size, model capacity, and varia-\ntions on the cloze objective.\n\n1 Introduction\n\nLanguage model pretraining has recently been\nshown to provide significant performance gains\nfor a range of challenging language understand-\ning problems (Dai and Le, 2015; Peters et al.,\n2018; Radford et al., 2018). However, existing\nwork has either used unidirectional (left-to-right)\nlanguage models (LMs) (Radford et al., 2018) or\nbi-directional (both left-to-right and right-to-left)\nLMs (BiLMs) where each direction is trained with\nan independent loss function (Peters et al., 2018).\nIn this paper, we show that even larger perfor-\nmance gains are possible by jointly pretraining\nboth directions of a large language-model-inspired\nself-attention cloze model.\n\nOur bi-directional transformer architecture pre-\ndicts every token in the training data (Figure 1).\nWe achieve this by introducing a cloze-style train-\ning objective where the model must predict the\ncenter word given left-to-right and right-to-left\ncontext representations. Our model separately\ncomputes both forward and backward states with\n\n*Equal contribution.\n\n<s> a b c <s>\n\nFigure 1: Illustration of the model. Block; is a standard\ntransformer decoder block. Green blocks operate left to\nright by masking future time-steps and blue blocks op-\nerate right to left. At the top, states are combined with\na standard multi-head self-attention module whose out-\nput is fed to a classifier that predicts the center token.\n\na masked self-attention architecture, that closely\nresembles a language model. At the top of the net-\nwork, the forward and backward states are com-\nbined to jointly predict the center word. This ap-\nproach allows us to consider both contexts when\npredicting words and to incur loss for every word\nin the training set, if the model does not assign it\nhigh likelihood.\n\nExperiments on the GLUE (Wang et al., 2018)\nbenchmark show strong gains over the state of the\nart for each task, including a 9.1 point gain on RTE\nover Radford et al. (2018). These improvements\nare consistent with, if slightly behind, BERT (De-\nvlin et al., 2018), which we will discuss in more\ndetail in the next section. We also show that it\nis possible to stack task-specific architectures for\nNER and constituency parsing on top of our pre-\ntrained representations, and achieve new state-of-\nthe-art performance levels for both tasks. We also\npresent extensive experimental analysis to better\n\n5360\n\nProceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natural Language Processing, pages 5360-5369,\nHong Kong, China, November 3—7, 2019. ©2019 Association for Computational Linguistics\n", "vlm_text": "Cloze-driven Pretraining of Self-attention Networks \nAlexei Baevski, Sergey Edunov ∗ , Yinhan Liu ∗ , Luke Zettlemoyer, Michael Auli Facebook AI Research Menlo Park, CA and Seattle, WA \nAbstract \nWe present a new approach for pretraining a bi-directional transformer model that provides signiﬁcant performance gains across a vari- ety of language understanding problems. Our model solves a cloze-style word reconstruction task, where each word is ablated and must be predicted given the rest of the text. Experi- ments demonstrate large performance gains on GLUE and new state of the art results on NER as well as constituency parsing benchmarks, consistent with BERT. We also present a de- tailed analysis of a number of factors that con- tribute to effective pretraining, including data domain and size, model capacity, and varia- tions on the cloze objective. \n1 Introduction \nLanguage model pretraining has recently been shown to provide signiﬁcant performance gains for a range of challenging language understand- ing problems ( Dai and Le ,  2015 ;  Peters et al. , 2018 ;  Radford et al. ,  2018 ). However, existing work has either used unidirectional (left-to-right) language models (LMs) ( Radford et al. ,  2018 ) or bi-directional (both left-to-right and right-to-left) LMs (BiLMs) where each direction is trained with an independent loss function ( Peters et al. ,  2018 ). In this paper, we show that even larger perfor- mance gains are possible by jointly pretraining both directions of a large language-model-inspired self-attention cloze model. \nOur bi-directional transformer architecture pre- dicts  every  token in the training data (Figure  1 ). We achieve this by introducing a cloze-style train- ing objective where the model must predict the center word given left-to-right and right-to-left context representations. Our model separately computes both forward and backward states with \nThe image is an illustration of a model architecture featuring a series of transformer decoder blocks. The model consists of two parallel sequences of blocks: green blocks on the left, which move left to right, masking future time-steps, and blue blocks on the right, which operate right to left. These are standard transformer decoder blocks labeled as `Block_1` to `Block_N`. \n\nAt the top of the diagram, there's a gray block labeled `comb`, representing a combination mechanism. This combines the outputs from the green and blue sequences using a standard multi-head self-attention module. The result from this combination is further processed by a classifier to predict the center token. The arrows below the green and blue `Block_1` indicate input starting with a special token `<s>`, typically used as a start of sequence or similar in NLP models.\na masked self-attention architecture, that closely resembles a language model. At the top of the net- work, the forward and backward states are com- bined to jointly predict the center word. This ap- proach allows us to consider both contexts when predicting words and to incur loss for every word in the training set, if the model does not assign it high likelihood. \nExperiments on the GLUE ( Wang et al. ,  2018 ) benchmark show strong gains over the state of the art for each task, including a 9.1 point gain on RTE over  Radford et al.  ( 2018 ). These improvements are consistent with, if slightly behind, BERT ( De- vlin et al. ,  2018 ), which we will discuss in more detail in the next section. We also show that it is possible to stack task-speciﬁc architectures for NER and constituency parsing on top of our pre- trained representations, and achieve new state-of- the-art performance levels for both tasks. We also present extensive experimental analysis to better understand these results, showing that (1) hav- ing multiple sentences in each training example is crucial for many tasks; (2) pre-training contin- ues to improve performance with up to 18B tokens and would likely continue to improve with more data; and ﬁnally (3) our novel cloze-driven train- ing regime is more effective than predicting left and right tokens separately. "}
{"page": 1, "image_path": "doc_images/D19-1539_1.jpg", "ocr_text": "understand these results, showing that (1) hav-\ning multiple sentences in each training example\nis crucial for many tasks; (2) pre-training contin-\nues to improve performance with up to 18B tokens\nand would likely continue to improve with more\ndata; and finally (3) our novel cloze-driven train-\ning regime is more effective than predicting left\nand right tokens separately.\n\n2 Related work\n\nThere has been much recent work on learning\nsentence-specific representations for language un-\nderstanding tasks. McCann et al. (2017) learn con-\ntextualized word representations from a sequence\nto sequence translation task and uses the represen-\ntations from the encoder network to improve a va-\nriety of language understanding tasks. Subsequent\nwork focused on language modeling pretraining\nwhich has been shown to be more effective and\nwhich does not require bilingual data (Zhang and\nBowman, 2018).\n\nOur work was inspired by ELMo (Peters et al.,\n2018) and the generative pretraining (GPT) ap-\nproach of Radford et al. (2018). ELMo introduces\nlanguage models to pretrain word representations\nfor downstream tasks including a novel mecha-\nnism to learn a combination of different layers\nin the language model that is most beneficial to\nthe current task. GPT relies on a left to right\nlanguage model and an added projection layer\nfor each downstream task without a task-specific\nmodel. Our approach mostly follows GPT, though\nwe show that our model also works well with an\nELMo module on NER and constituency parsing.\n\nThe BERT model (Devlin et al., 2018) is a\ntransformer encoder model that captures left and\nright context. There is significant overlap between\ntheir work and ours but there are also significant\ndifferences: our model is a bi-directional trans-\nformer language model that predicts every single\ntoken in a sequence. Our model has two uni-\ndirectional components encoding either the left or\nright context and both are combined to predict cen-\nter words. BERT is also a transformer encoder that\nhas access to the entire input but this choice re-\nquires a special training regime. In particular, they\nmulti-task between predicting a subset of masked\ninput tokens, similar to a denoising autoencoder,\nand a next sentence prediction task. In compar-\nison, we optimize a single loss function that re-\nquires the model to predict each token of an in-\n\nput sentence given all surrounding tokens. We use\nall tokens as training targets and therefore extract\nlearning signal from every single token in the sen-\ntence and not just a subset. Melamud et al. (2016)\nfollow a similar approach to ours by predicting\nthe center word but their architecture is based on\nLSTMs and we include the center word when we\nactually fine-tune on downstream tasks.\n\nBERT tailors pretraining to capture dependen-\n\ncies between sentences via a next sentence predic-\nion task as well as by constructing training exam-\nples of sentence-pairs with input markers that dis-\ninguish between tokens of the two sentences. Our\nmodel is trained similarly to a classical language\nmodel since we do not adapt the training exam-\nples to resemble the end task data and we do not\nsolve a denoising task during training.\nFinally, BERT as well as Radford et al. (2018)\nconsider only a single data source to pretrain\nheir models, either BooksCorpus (Radford et al.,\n2018), or BooksCorpus and additional Wikipedia\ndata (Devlin et al., 2018), whereas our study ab-\nlates the effect of various amounts of training data\nas well as different data sources.\n\n3 Two tower model\n\nOur cloze model represents a probability distribu-\ntion p(t;|t1,...,ti-1,ti¢1,---,tn) for a sentence\nwith n tokens t;,...,t). There are two self-\nattentional towers each consisting of N stacked\nblocks: the forward tower operates left-to-right\nand the backward tower operates in the opposite\ndirection. To predict a token, we combine the\nrepresentations of the two towers, as described in\nmore detail below, taking care that neither repre-\nsentation contains information about the current\ntarget token.\n\nThe forward tower computes the representation\nF! for token i at layer | based on the forward rep-\nresentations of the previous layer FE} via self-\nattention; the backward tower computes represen-\ntation B! based on information from the opposite\ndirection Bo. When examples of uneven length\nare batched, one of the towers may not have any\ncontext at the beginning. We deal with this issue\nby adding an extra zero state over which the self-\nattention mechanism can attend.\n\nWe pretrain on individual examples as they oc-\ncur in the training corpora (§5.1). For News Crawl\nthis is individual sentences while on Wikipedia,\nBookcorpus, and Common Crawl examples are\n\n5361\n", "vlm_text": "\n2 Related work \nThere has been much recent work on learning sentence-speciﬁc representations for language un- derstanding tasks.  McCann et al.  ( 2017 ) learn con- textualized word representations from a sequence to sequence translation task and uses the represen- tations from the encoder network to improve a va- riety of language understanding tasks. Subsequent work focused on language modeling pretraining which has been shown to be more effective and which does not require bilingual data ( Zhang and Bowman ,  2018 ). \nOur work was inspired by ELMo ( Peters et al. , 2018 ) and the generative pretraining (GPT) ap- proach of  Radford et al.  ( 2018 ). ELMo introduces language models to pretrain word representations for downstream tasks including a novel mecha- nism to learn a combination of different layers in the language model that is most beneﬁcial to the current task. GPT relies on a left to right language model and an added projection layer for each downstream task without a task-speciﬁc model. Our approach mostly follows GPT, though we show that our model also works well with an ELMo module on NER and constituency parsing. \nThe BERT model ( Devlin et al. ,  2018 ) is a transformer encoder model that captures left and right context. There is signiﬁcant overlap between their work and ours but there are also signiﬁcant differences: our model is a bi-directional trans- former language model that predicts every single token in a sequence. Our model has two uni- directional components encoding either the left or right context and both are combined to predict cen- ter words. BERT is also a transformer encoder that has access to the entire input but this choice re- quires a special training regime. In particular, they multi-task between predicting a subset of masked input tokens, similar to a denoising autoencoder, and a next sentence prediction task. In compar- ison, we optimize a single loss function that re- quires the model to predict each token of an in- put sentence given all surrounding tokens. We use all tokens as training targets and therefore extract learning signal from every single token in the sen- tence and not just a subset.  Melamud et al.  ( 2016 ) follow a similar approach to ours by predicting the center word but their architecture is based on LSTMs and we include the center word when we actually ﬁne-tune on downstream tasks. \n\nBERT tailors pretraining to capture dependen- cies between sentences via a next sentence predic- tion task as well as by constructing training exam- ples of sentence-pairs with input markers that dis- tinguish between tokens of the two sentences. Our model is trained similarly to a classical language model since we do not adapt the training exam- ples to resemble the end task data and we do not solve a denoising task during training. \nFinally, BERT as well as  Radford et al.  ( 2018 ) consider only a single data source to pretrain their models, either BooksCorpus ( Radford et al. , 2018 ), or BooksCorpus and additional Wikipedia data ( Devlin et al. ,  2018 ), whereas our study ab- lates the effect of various amounts of training data as well as different data sources. \n3 Two tower model \nOur cloze model represents a probability distribu- tion    $p(t_{i}|t_{1},.\\,.\\,.\\,,t_{i-1},t_{i+1},.\\,.\\,.\\,,t_{n})$   for a sentence with    $n$   tokens    $t_{1},\\dots,t_{n}$  . There are two self- attentional towers each consisting of    $N$   stacked blocks: the  forward  tower operates left-to-right and the  backward  tower operates in the opposite direction. To predict a token, we combine the representations of the two towers, as described in more detail below, taking care that neither repre- sentation contains information about the current target token. \nThe forward tower computes the representation  $F_{i}^{l}$    for token    $i$   at layer  $l$   based on the forward rep- resentations of the previous layer    $F_{\\leq i}^{l-1}$  via self- ≤ attention; the backward tower computes represen- tation    $B_{i}^{l}$    based on information from the opposite direction    $B_{\\geq i}^{l-1}$  . When examples of uneven length ≥ are batched, one of the towers may not have any context at the beginning. We deal with this issue by adding an extra zero state over which the self- attention mechanism can attend. \nWe pretrain on individual examples as they oc- cur in the training corpora ( § 5.1 ). For News Crawl this is individual sentences while on Wikipedia, Bookcorpus, and Common Crawl examples are paragraph length. Sentences are prepended and appended with sample boundary markers    $<s>$  . "}
{"page": 2, "image_path": "doc_images/D19-1539_2.jpg", "ocr_text": "paragraph length. Sentences are prepended and\nappended with sample boundary markers < s >.\n\n3.1. Block structure\n\nThe structure of the blocks follows most of the\narchitectural choices described in Vaswani et al.\n(2017). Each block consists of two sub-blocks:\nthe first is a multi-head self-attention module with\nHT = 16 heads for which we mask out any sub-\nsequent time-steps, depending on if we are deal-\ning with the forward or backward tower. The sec-\nond sub-block is a feed-forward module (FFN)\nof the form ReLU(W,X + b,)W2 + bo where\nW, « R&S, W, © R/*°. Different to Vaswani\net al. (2017) we apply layer normalization before\nthe self-attention and FFN blocks instead of af-\nter, as we find it leads to more effective training.\nSub-blocks are surrounded by a residual connec-\ntion (He et al., 2015). Position is encoded via\nfixed sinusoidal position embeddings and we use\na character CNN encoding of the input tokens for\nword-based models (Kim et al., 2016). Input em-\nbeddings are shared between the two towers.\n\n3.2. Combination of representations\n\nThe forward and backward representations com-\nputed by the two towers are combined to pre-\ndict the ablated word. To combine them we use\na self-attention module which is followed by an\nFEN block (§3.1). The output of the FFN block\nf is projected by W into V classes represent-\ning the types in the vocabulary: W? f to which\na softmax is applied. When the model predicts\ntoken i, the input to the attention module are\nforward states Fi’... F/, and backward states\nBE. ,--- B;, where n is the length of the sequence\nand L is the number of layers. We implement this\nby masking BE, and F£,. The attention query\nfor token i is a combination of F/, and BY.\nFor the base model we sum the two representa-\ntions and for the larger models they are concate-\nnated. Keys and values are based on the forward\nand backward states fed to the attention module.\nIn summary, this module has access to information\nabout the entire input surrounding the current tar-\nget token. During training, we predict every token\nin this way. The output of this module is fed to an\noutput classifier which predicts the center token.\nWe use an adaptive softmax for the output classi-\nfier (Grave et al., 2017) for the word based models\nand regular softmax for the BPE based models.\n\nEmbedding\n\nEmbedding ofb Embedding\n\nofa ofc\n\nFigure 2: Illustration of fine-tuning for a downstream\ntask. For classification problems, output of the first\nand last token is fed to a task-specific classifier. Mask-\ning for the final combination layer (comb) is removed\nwhich results in representations based on all forward\nand backward states (cf. Figure 1). The red dot-dashed\narrows show connections that are masked during train-\ning, but unmasked for fine-tuning.\n\nWhile all states that contain information about\nthe current target word are masked in the final self-\nattention block during training, we found it bene-\nficial to disable this masking when fine tuning the\npretrained model for downstream tasks. This is es-\npecially true for tasks that label each token, such\nas NER, as this allows the model to access the full\ncontext including the token itself.\n\n4 Fine-tuning\n\nWe use the following approach to fine-tune the\npretrained two tower model to specific down-\nstream tasks (Figure 2).\n\nClassification and regression tasks. For sin-\ngle sentence classification tasks, we consider the\nlanguage model outputs for the boundary tokens\n< s > which we add before the start and end\nof each sentence. The language model outputs\nare the representations f just before the final soft-\nmax layer (§3.2). The outputs are of dimension\nd = 1024 and we concatenate them to project to\nthe number of classes C' in the downstream task\nwith W, € RCx24 (Radford et al., 2018); we add a\nbias term b € R© and initialize all weights as well\nas the bias to zero. The output of the projection\nis softmax-normalized and the model is optimized\nwith cross-entropy for classification tasks. Re-\n\n5362\n", "vlm_text": "\n3.1 Block structure \nThe structure of the blocks follows most of the architectural choices described in  Vaswani et al. ( 2017 ). Each block consists of two sub-blocks: the ﬁrst is a multi-head self-attention module with  $H\\,=\\,16$   heads for which we mask out any sub- sequent time-steps, depending on if we are deal- ing with the forward or backward tower. The sec- ond sub-block is a feed-forward module (FFN) of the form    $R e L U(W_{1}X\\,+\\,b_{1})W_{2}\\,+\\,b_{2}$   where  $W_{1}\\in\\mathbb{R}^{e\\times f}$  ,    $W_{1}\\in\\mathbb{R}^{f\\times e}$  . Different to  Vaswani et al.  ( 2017 ) we apply layer normalization before the self-attention and FFN blocks instead of af- ter, as we ﬁnd it leads to more effective training. Sub-blocks are surrounded by a residual connec- tion ( He et al. ,  2015 ). Position is encoded via ﬁxed sinusoidal position embeddings and we use a character CNN encoding of the input tokens for word-based models ( Kim et al. ,  2016 ). Input em- beddings are shared between the two towers. \n3.2 Combination of representations \nThe forward and backward representations com- puted by the two towers are combined to pre- dict the ablated word. To combine them we use a self-attention module which is followed by an FFN block ( § 3.1 ).  e out t of the FFN block  $f$   is projected by  W  into  V  $V$   classes represent- ing the types in the vocabulary:    $\\mathbf{W}^{T}f$   to which a softmax is applied. When the model predicts token    $i$  , the input to the attention module are forward states    $F_{1}^{L}\\,.\\,.\\,F_{i-1}^{L}$    and backward states −  $B_{i+1}^{L}\\cdot\\cdot\\cdot B_{n}^{:}$    where    $n$   is the length of the sequence and  $L$   is the number of layers. We implement this by masking    $B_{\\leq i}^{L}$    and    $F_{\\geq i}^{\\dot{L}}$  . The attention query ≤ ≥ for token    $i$   is a combination of    $F_{i-1}^{L}$    and    $B_{i+1}^{L}$  . − For the base model we sum the two representa- tions and for the larger models they are concate- nated. Keys and values are based on the forward and backward states fed to the attention module. In summary, this module has access to information about the entire input surrounding the current tar- get token. During training, we predict every token in this way. The output of this module is fed to an output classiﬁer which predicts the center token. We use an adaptive softmax for the output classi- ﬁer ( Grave et al. ,  2017 ) for the word based models and regular softmax for the BPE based models. \nThe image is an illustration related to fine-tuning a model for a downstream classification task. It depicts a particular method of processing input sequences (a, b, c) to generate embeddings for classification.\n\nKey components of the illustration:\n1. **Input Tokens (a, b, c):** At the bottom of the diagram, three tokens (a, b, c) are provided as inputs to the model.\n2. **Processing Units:** Above each input token, there are units represented by rectangles with arrows inside, indicating some form of forward and backward state processing.\n3. **Combination Layer (comb):** Three combination (comb) layers are shown, one for each token embedding. These layers gather information from the processed states of each input token.\n4. **Masked and Unmasked Connections:**\n   - The red dot-dashed arrows indicate connections that are masked during the training phase but become unmasked during the fine-tuning process to incorporate more complete information.\n   - The dashed arrows also illustrate the paths in which information flows to generate the embeddings.\n5. **Embedding Outputs:** The final embeddings for tokens a, b, and c are produced after processing through the combination layers, capturing both forward and backward states.\n\nOverall, the illustration visualizes how information is aggregated and transformed to generate meaningful embeddings used for classifying based on sequence input, demonstrating specific adjustments during the fine-tuning phase.\nWhile all states that contain information about the current target word are masked in the ﬁnal self- attention block during training, we found it bene- ﬁcial to disable this masking when ﬁne tuning the pretrained model for downstream tasks. This is es- pecially true for tasks that label each token, such as NER, as this allows the model to access the full context including the token itself. \n4 Fine-tuning \nWe use the following approach to ﬁne-tune the pretrained two tower model to speciﬁc down- stream tasks (Figure  2 ). \nClassiﬁcation and regression tasks. For sin- gle sentence classiﬁcation tasks, we consider the language model outputs for the boundary tokens  $<\\textit{s}>$   which we add before the start and end of each sentence. The language model outputs are the representations  $f$   just before the ﬁnal soft- max layer ( § 3.2 ). The outputs are of dimension  $d=1024$   and we concatenate them to project to the number of classes    $C$   in the downstream task with    $W_{1}\\in\\mathbb{R}^{C\\times2d}$    ( Radford et al. ,  2018 ); we add a bias term  b  $b\\in\\mathbb{R}^{C}$   ∈   and initialize all weights as well as the bias to zero. The output of the projection is softmax-normalized and the model is optimized with cross-entropy for classiﬁcation tasks. Re- gression tasks such as the Semantic Textual Sim- ilarity benchmark (STS-B; Cer et al., 2017) use  $C\\,=\\,1$   and are trained with mean squared error. For tasks involving sentence-pairs, we concatenate them and add a new separator token    $<\\,s e p>$   be- tween them. We add the output of this token to the ﬁnal projection    $W_{2}\\in\\mathbb{R}^{C\\times3d}$  . "}
{"page": 3, "image_path": "doc_images/D19-1539_3.jpg", "ocr_text": "gression tasks such as the Semantic Textual Sim-\nilarity benchmark (STS-B; Cer et al., 2017) use\nC = 1 and are trained with mean squared error.\nFor tasks involving sentence-pairs, we concatenate\nthem and add a new separator token < sep > be-\ntween them. We add the output of this token to the\nfinal projection W € RO*3¢,\n\nStructured prediction tasks. For named entity\nrecognition and parsing we use task-specific archi-\ntectures which we fine-tune together with the lan-\nguage model but with different learning rate. The\narchitectures are detailed in the respective results\nsections. The input to the architectures are the\noutput representations of the pretrained language\nmodel.\n\nNo Masking. For fine-tuning, we found it bene-\nficial to remove masking of the current token in\nthe final layer that pools the output of the two\ntowers. This is different than in the actual pre-\ntraining. It is important to have access to informa-\ntion about the token to be classified for token level\nclassification tasks such as NER but we also found\nthis to perform better for sentence classification\ntasks. In practice, we completely disable masking\nin the combination layer so that it operates over\nall forward and backward states. However, dis-\nabling masking below the combination layer does\nnot perform well.\n\nOptimization. During fine-tuning we use larger\nlearning rates for the new parameters, that is W1,\nWo, b or the task-specific architecture, compared\nto the pretrained model. For GLUE tasks, we do\nso by simply scaling the output of the language\nmodel before the W; and W, projections by a\nfactor of 16. For structured prediction tasks, we\nexplicitly use different learning rates for the pre-\ntrained model and the task-specific parameters.\nWe fine tune with the Adam optimizer (Kingma\nand Ba, 2015). For GLUE tasks, we disable\ndropout in the language model and add 0.1 dropout\nbetween language model output and the final out-\nput projection; for structured prediction tasks, we\nuse 0.3 at all levels (within the pretrained model,\nwithin the task-specific architecture, and on the\nweights connecting them). In all settings, we use a\nbatch size of 16 examples. We use a cosine sched-\nule to linearly warm up the learning rate from le-\n07 to the target value over the first 10% of train-\ning steps, and then anneal the learning rate to le-\n06, following the cosine curve for the remaining\n\nsteps. For GLUE tasks, we tuned the learning rate\n‘or each task and chose the best value over three\nsettings: le-04, 5e-05 and 3e-05. For structured\nprediction tasks, we tuned on the pairs of learning\nrate, see the results section for details. For GLUE\nasks, we train three seeds for each learning rate\nvalue for three epochs and choose the model af-\ner each epoch that performs best on the validation\nset. For structured prediction tasks, we train for up\n0 25 epochs and stop if the validation loss does\nnot improve over the previous epoch.\n\n5 Experimental setup\n\n5.1 Datasets for pretraining\n\nWe train the two tower model on several datasets.\n\nCommon Crawl. We consider various subsets\nof Common Crawl which is web data. We fol-\nlow the same pre-processing as Grave et al. (2018)\nwhich is based on the May 2017 Common Crawl\ndump. This setup add 20 copies of English\nWikipedia resulting in about 14% of the final\ndataset to be Wikipedia. We subsample up to 18B\ntokens. All experiments use Common Crawl sub-\nsampled to 9B tokens, except §6.4.\n\nNews Crawl. We use up to 4.5B words of En-\nglish news web data distributed as part of WMT\n2018 (Bojar et al., 2018).\n\nBooksCorpus + Wikipedia. This is similar to\nthe training data used by BERT which comprises\nthe BooksCorpus (Zhu et al., 2015) of about 800M\nwords plus English Wikipedia data of 2.5B words.\n\n5.2. Pretraining hyper-parameters\n\nWe adapt the transformer implementation avail-\nable in the fairseq toolkit to our two tower archi-\ntecture (Ott et al., 2019). For hyper-parameter and\noptimization choices we mostly follow Baevski\nand Auli (2018). Our experiments consider three\nmodel sizes shown in Table 1: There are two CNN\ninput models in a base and large configuration as\nwell as a Byte-Pair-Encoding based model (BPE;\nSennrich et al., 2016). The CNN models have un-\nconstrained input vocabulary, and an output vo-\ncabulary limited to 1M most common types for\nthe large model, and 700K most common types\nfor the base model. CNN models use an adap-\ntive softmax in the output: the head band contains\nthe 60K most frequent types with dimensionality\n\n5363\n", "vlm_text": "\nStructured prediction tasks. For named entity recognition and parsing we use task-speciﬁc archi- tectures which we ﬁne-tune together with the lan- guage model but with different learning rate. The architectures are detailed in the respective results sections. The input to the architectures are the output representations of the pretrained language model. \nNo Masking. For ﬁne-tuning, we found it bene- ﬁcial to remove masking of the current token in the ﬁnal layer that pools the output of the two towers. This is different than in the actual pre- training. It is important to have access to informa- tion about the token to be classiﬁed for token level classiﬁcation tasks such as NER but we also found this to perform better for sentence classiﬁcation tasks. In practice, we completely disable masking in the combination layer so that it operates over all forward and backward states. However, dis- abling masking below the combination layer does not perform well. \nOptimization. During ﬁne-tuning we use larger learning rates for the new parameters, that is    $W_{1}$  ,  $W_{2}$  ,    $b$   or the task-speciﬁc architecture, compared to the pretrained model. For GLUE tasks, we do so by simply scaling the output of the language model before the    $W_{1}$   and    $W_{2}$   projections by a factor of 16. For structured prediction tasks, we explicitly use different learning rates for the pre- trained model and the task-speciﬁc parameters. \nWe ﬁne tune with the Adam optimizer ( Kingma and Ba ,  2015 ). For GLUE tasks, we disable dropout in the language model and add 0.1 dropout between language model output and the ﬁnal out- put projection; for structured prediction tasks, we use 0.3 at all levels (within the pretrained model, within the task-speciﬁc architecture, and on the weights connecting them). In all settings, we use a batch size of 16 examples. We use a cosine sched- ule to linearly warm up the learning rate from 1e- 07 to the target value over the ﬁrst   $10\\%$   of train- ing steps, and then anneal the learning rate to 1e- 06, following the cosine curve for the remaining steps. For GLUE tasks, we tuned the learning rate for each task and chose the best value over three settings: 1e-04, 5e-05 and 3e-05. For structured prediction tasks, we tuned on the pairs of learning rate, see the results section for details. For GLUE tasks, we train three seeds for each learning rate value for three epochs and choose the model af- ter each epoch that performs best on the validation set. For structured prediction tasks, we train for up to 25 epochs and stop if the validation loss does not improve over the previous epoch. \n\n5 Experimental setup \n5.1 Datasets for pretraining \nWe train the two tower model on several datasets. \nCommon Crawl. We consider various subsets of Common Crawl which is web data. We fol- low the same pre-processing as  Grave et al.  ( 2018 ) which is based on the May 2017 Common Crawl dump. This setup add 20 copies of English Wikipedia resulting in about   $14\\%$   of the ﬁnal dataset to be Wikipedia. We subsample up to 18B tokens. All experiments use Common Crawl sub- sampled to 9B tokens, except  $\\S6.4$  . \nNews Crawl. We use up to 4.5B words of En- glish news web data distributed as part of WMT 2018 ( Bojar et al. ,  2018 ). \nBooksCorpus  $^+$   Wikipedia. This is similar to the training data used by BERT which comprises the BooksCorpus ( Zhu et al. ,  2015 ) of about 800M words plus English Wikipedia data of 2.5B words. \n5.2 Pretraining hyper-parameters \nWe adapt the transformer implementation avail- able in the fairseq toolkit to our two tower archi- tecture ( Ott et al. ,  2019 ). For hyper-parameter and optimization choices we mostly follow  Baevski and Auli  ( 2018 ). Our experiments consider three model sizes shown in Table  1 : There are two CNN input models in a base and large conﬁguration as well as a Byte-Pair-Encoding based model (BPE; Sennrich et al., 2016). The CNN models have un- constrained input vocabulary, and an output vo- cabulary limited to 1M most common types for the large model, and 700K most common types for the base model. CNN models use an adap- tive softmax in the output: the head band contains the 60K most frequent types with dimensionality "}
{"page": 4, "image_path": "doc_images/D19-1539_4.jpg", "ocr_text": "FFN  AttnHeads Query formation Train time\nModel Parameters Updates Blocks Dim (final layer) (final layer) (days)\nCNN Base 177M 600K 6 4096 12 Sum 6\nCNN Large 330M 1M 12 4096 32 Concat 10\nBPE Large 370M 1M 12 4096 32 Concat 4.5\n\nTable 1: Hyper-parameters for our models. Parameter count excludes the (adaptive) softmax layer. Train time as\nmeasured on 128 Volta GPUs for the CNN models and 64 Volta GPUs for the BPE model.\n\n1024, followed by a 160K band with dimension-\nality 256. The remaining types have dimensional-\nity 64; there are 480K types for the small model\nand 780K for the large model. The BPE model\nuses a vocabulary of 55K types and we share input\nand output embeddings in a flat softmax with di-\nmension 1024 (Inan et al., 2016; Press and Wolf,\n2017). The BPE vocabulary was constructed by\napplying 30K merge operations over the training\ndata, then applying the BPE code to the training\ndata and retaining all types occurring at least three\ntimes.\n\nEvery setup uses model dimensionaltiy d =\n1024 with H = 16 attention heads for all but the\nfinal attention layer. Model based on character in-\nputs use character embedding size 128 and we ap-\nply six filters of size 1x128, 2x256, 3x384, 4x512,\n5x512, 6x512 followed by a single highway layer.\nThe models are trained with model and attention\ndropout rate of 0.1 and ReLU dropout rate of 0.05.\n\nDifferent to Vaswani et al. (2017) we use Nes-\nterov’s accelerated gradient method (Sutskever\net al., 2013) with a momentum of 0.99 and we\nrenormalize gradients if their norm exceeds 0.1\n(Pascanu et al., 2013). The learning rate is lin-\nearly warmed up from 10~ to 1 for 16K steps and\nthen annealed using a cosine learning rate sched-\nule with a single phase to 0.0001 (Loshchilov and\nHutter, 2016).\n\nWe run experiments on DGX-1 machines with\n8 NVIDIA V100 GPUs and machines are inter-\nconnected by Infiniband. We also use the NCCL2\nlibrary and the torch.distributed package for inter-\nGPU communication. We train models with 16-\nbit floating point precision, following Ott et al.\n(2018). The BPE model trains much faster than\nthe character CNN models (Table 1).\n\n6 Results\n6.1 GLUE\n\nFirst, we conduct experiments on the general\nlanguage understanding evaluation benchmark\n(GLUE; Wang et al., 2018) and present a short\noverview of the tasks. More information can be\nfound in Wang et al. (2018). There are two single-\nsentence classification tasks: First, the Corpus of\nLinguistic Acceptability (CoLA; Warstadt et al.,\n2018) is a binary task to judge sentence grammat-\nicality; evaluation is in terms of the Matthews cor-\nrelation coefficient (mec). Second, the Stanford\nSentiment Treebank (SST-2; Socher et al., 2013)\nrequires to judge if movie reviews have positive or\nnegative sentiment; evaluation is in terms of accu-\nracy (acc).\n\nThere are three tasks assessing sentence sim-\nilarity: The Microsoft Research Paragraph Cor-\npus (MRPC; Dolan and Brockett, 2015) and the\nQuora Question Pairs benchmark (QQP); we eval-\nuate in terms of Fl. The Semantic Textual Similar-\nity Benchmark (STS-B; Cer et al., 2017) requires\npredicting a similarity score between | and 5 for a\nsentence pair; we report the Spearman correlation\ncoefficient (scc).\n\nFinally, there are four natural language infer-\nence tasks: the Multi-Genre Natural Language In-\nference (MNLI; Williams et al., 2018), the Stan-\nford Question Answering Dataset (QNLI; Ra-\njpurkar et al., 2016), the Recognizing Textual En-\ntailment (RTE; Dagan et al., 2006, Bar Haim et al.,\n2006, Ciampiccolo et al., 2007 Bentivogli et al.,\n2009). We exclude the Winograd NLI task from\nour results similar to Radford et al. (2018); De-\nvlin et al. (2018) and report accuracy. For MNLI\nwe report both matched (m) and mismatched (mm)\naccuracy on test.\n\nWe also report an average over the GLUE met-\nrics. This figure is not comparable to the aver-\nage on the official GLUE leaderboard since we ex-\nclude Winograd and do not report MRPC accuracy\n\n5364\n", "vlm_text": "The table presents a comparison of three different models based on several characteristics. Here are the details:\n\n- **Model**: The name or type of the model.\n  - CNN Base\n  - CNN Large\n  - BPE Large\n\n- **Parameters**: The number of parameters in each model.\n  - CNN Base: 177 million (177M)\n  - CNN Large: 330 million (330M)\n  - BPE Large: 370 million (370M)\n\n- **Updates**: The number of updates or iterations used for training the model.\n  - CNN Base: 600,000 (600K)\n  - CNN Large: 1 million (1M)\n  - BPE Large: 1 million (1M)\n\n- **Blocks**: The number of blocks in the model architecture.\n  - CNN Base: 6\n  - CNN Large: 12\n  - BPE Large: 12\n\n- **FFN Dim**: The dimension of the feed-forward network.\n  - All models have an FFN dimension of 4096.\n\n- **Attn Heads (final layer)**: The number of attention heads in the final layer of the model.\n  - CNN Base: 12\n  - CNN Large: 32\n  - BPE Large: 32\n\n- **Query formation (final layer)**: The method used for query formation in the final layer.\n  - CNN Base: Sum\n  - CNN Large: Concat\n  - BPE Large: Concat\n\n- **Train time (days)**: The time taken to train each model (in days).\n  - CNN Base: 6 days\n  - CNN Large: 10 days\n  - BPE Large: 4.5 days\n1024, followed by a 160K band with dimension- ality 256. The remaining types have dimensional- ity 64; there are 480K types for the small model and 780K for the large model. The BPE model uses a vocabulary of 55K types and we share input and output embeddings in a ﬂat softmax with di- mension 1024 ( Inan et al. ,  2016 ;  Press and Wolf , 2017 ). The BPE vocabulary was constructed by applying 30K merge operations over the training data, then applying the BPE code to the training data and retaining all types occurring at least three times. \nEvery setup uses model dimensionaltiy    $d\\ =$  1024  with  $H=16$   attention heads for all but the ﬁnal attention layer. Model based on character in- puts use character embedding size 128 and we ap- ply six ﬁlters of size   $1{\\mathrm{x}}128,\\,2{\\mathrm{x}}256,\\,3{\\mathrm{x}}384,\\,4{\\mathrm{x}}512,$  , 5x512, 6x512 followed by a single highway layer. The models are trained with model and attention dropout rate of 0.1 and ReLU dropout rate of 0.05. \nDifferent to  Vaswani et al.  ( 2017 ) we use Nes- terov’s accelerated gradient method ( Sutskever et al. ,  2013 ) with a momentum of  0 . 99  and we renormalize gradients if their norm exceeds  0 . 1 ( Pascanu et al. ,  2013 ). The learning rate is lin- early warmed up from  $10^{-7}$    to  1  for 16K steps and then annealed using a cosine learning rate sched- ule with a single phase to 0.0001 ( Loshchilov and Hutter ,  2016 ). \nWe run experiments on DGX-1 machines with 8 NVIDIA V100 GPUs and machines are inter- connected by Inﬁniband. We also use the NCCL2 library and the torch.distributed package for inter- GPU communication. We train models with 16- bit ﬂoating point precision, following  Ott et al. ( 2018 ). The BPE model trains much faster than the character CNN models (Table  1 ). \n6 Results \n6.1 GLUE \nFirst, we conduct experiments on the general language understanding evaluation benchmark (GLUE; Wang et al., 2018) and present a short overview of the tasks. More information can be found in  Wang et al.  ( 2018 ). There are two single- sentence classiﬁcation tasks: First, the Corpus of Linguistic Acceptability (CoLA; Warstadt et al., 2018) is a binary task to judge sentence grammat- icality; evaluation is in terms of the Matthews cor- relation coefﬁcient (mcc). Second, the Stanford Sentiment Treebank (SST-2; Socher et al., 2013) requires to judge if movie reviews have positive or negative sentiment; evaluation is in terms of accu- racy (acc). \nThere are three tasks assessing sentence sim- ilarity: The Microsoft Research Paragraph Cor- pus (MRPC; Dolan and Brockett, 2015) and the Quora Question Pairs benchmark (QQP); we eval- uate in terms of F1. The Semantic Textual Similar- ity Benchmark (STS-B; Cer et al., 2017) requires predicting a similarity score between 1 and 5 for a sentence pair; we report the Spearman correlation coefﬁcient (scc). \nFinally, there are four natural language infer- ence tasks: the Multi-Genre Natural Language In- ference (MNLI; Williams et al., 2018), the Stan- ford Question Answering Dataset (QNLI; Ra- jpurkar et al., 2016), the Recognizing Textual En- tailment (RTE; Dagan et al., 2006, Bar Haim et al., 2006, Ciampiccolo et al., 2007 Bentivogli et al., 2009). We exclude the Winograd NLI task from our results similar to  Radford et al.  ( 2018 );  De- vlin et al.  ( 2018 ) and report accuracy. For MNLI we report both matched (m) and mismatched (mm) accuracy on test. \nWe also report an average over the GLUE met- rics. This ﬁgure is not comparable to the aver- age on the ofﬁcial GLUE leaderboard since we ex- clude Winograd and do not report MRPC accuracy "}
{"page": 5, "image_path": "doc_images/D19-1539_5.jpg", "ocr_text": "CoLA  SST-2. MRPC\n\nSTS-B\n\nQQP MNLI-(m/mm) QNLI RTE\n\n(mcc) (acc) (Fl) (scc) (Fl) (acc) (acc) (acc) Avg\nOpenAI GPT 454 91.3 82.3 80.0 70.3 82.1/81.4 88.1 56.0 75.2\nCNN Base 53.1 93.6 81.3 82.2 70.5 82.5/82.2 89.5 64.6 77.7\nCNN Large 52.8 946 83.7 834 71.7 84.3/83.8 89.8 63.7 78.6\nBPE Large 51.8 940 83.0 842 70.6 82.9/82.2 89.3 65.1 78.1\nGPT onSTILTs 47.2 93.1 87.7 848 70.1 80.7/80.6 87.2 69.1 77.8\nBERT pasr 52.1 935 889 85.8 71.2 84.6/83.4 90.1 66.4 79.6\nBERT; Arce 60.5 949 893 865 72.1 86.7/85.9 91.1 70.1 81.9\n\nTable 2: Test results as per the GLUE evaluation server. The average column does not include the WNLI test set.\nmec = Matthews correlation, acc = Accuracy, scc = Spearman correlation.\n\nSTS-B Pearson correlation as well as QQP accu-\nracy.\n\nTable 2 shows results for three configurations\nof our approach (cf. Table 1). The BPE model\nhas more parameters than the CNN model but\ndoes not perform better in aggregate, however, it is\nfaster to train. All our models outperform the uni-\ndirectional transformer (OpenAI GPT) of Radford\net al. (2018), however, our model is about 50%\nlarger than their model. We also show results for\nSTILTs (Phang et al., 2018) and BERT (Devlin\net al., 2018). Our CNN base model performs as\nwell as STILTs in aggregate, however, on some\ntasks involving sentence-pairs, STILTs performs\nmuch better (MRPC, RTE); there is a similar trend\nfor BERT.\n\nSTILTs adds another fine-tuning step on an-\nother downstream task which is similar to the fi-\nnal task. The technique is equally applicable to\nour approach. Training examples for our model\nare Common Crawl paragraphs of arbitrary length.\nWe expect that tailoring training examples for lan-\nguage model pretraining to the end tasks to signif-\nicantly improve performance. For example, BERT\ntrains on exactly two sentences while as we train\non entire paragraphs.\n\n6.2 Structured Prediction\n\nWe also evaluated performance on two structured\npredictions tasks, NER and constituency parsing.\nFor both problems, we stacked task-specific archi-\ntectures from recent work on top of our pretrained\ntwo tower models. We evaluate two ways of stack-\ning: (1) ELMo-style, where the pretrained mod-\nels are not fine-tuned but are linearly combined at\ndifferent depths, and (2) with fine-tuning, where\nwe set different learning rates for the task-specific\n\nModel dev F1_ test F1\nELMogase 95.7 92.2\nCNN Large + ELMo 96.4 93.2\nCNN Large + fine-tune 96.9 93.5\nBERT gase 96.4 92.4\nBERT, ARGE 96.6 92.8\n\nTable 3: CoNLL-2003 Named Entity Recognition re-\nsults. Test result was evaluated on parameter set with\nthe best dev FI.\n\nModel devF1 test F1\nELMogase 95.2 95.1\nCNN Large + ELMo 95.1 95.2\nCNN Large + fine-tune 95.5 95.6\n\nTable 4: Penn Treebank Constituency Parsing results.\nTest result was evaluated on parameter set with the best\ndev Fl.\n\nlayers but otherwise update all of the parameters\nduring the task-specific training.\n\n6.2.1 Named Entity Recognition\n\nWe evaluated span-level Fl performance on the\nCoNLL 2003 Named Entity Recognition (NER)\ntask, where spans of text must be segmented and\nlabeled as Person, Organization, Location, or Mis-\ncellaneous. We adopted the NER architecture in\nPeters et al. (2018), a biLSTM-CRF, with two mi-\nnor modifications: (1) instead of two layers of biL-\nSTM, we only used one, and (2) a linear projection\nlayer was added between the token embedding and\nbiLSTM layer. We did grid search on the pairs of\nlearning rate, and found that projection-biLSTM-\n\n5365\n", "vlm_text": "This table presents the performance of various models on a set of NLP tasks. Each column represents a specific task or a metric, while each row corresponds to a model. Here's a breakdown of the table:\n\n**Columns:**\n1. **Model Name**: Lists the different models being evaluated.\n2. **CoLA (mcc)**: Matthews correlation coefficient for the CoLA task.\n3. **SST-2 (acc)**: Accuracy for the SST-2 task.\n4. **MRPC (F1)**: F1 score for the MRPC task.\n5. **STS-B (scc)**: Spearman correlation coefficient for the STS-B task.\n6. **QQP (F1)**: F1 score for the QQP task.\n7. **MNLI-(m/mm) (acc)**: Accuracy for the MNLI-matched and MNLI-mismatched tasks.\n8. **QNLI (acc)**: Accuracy for the QNLI task.\n9. **RTE (acc)**: Accuracy for the RTE task.\n10. **Avg**: Average performance across all tasks.\n\n**Rows:**\n1. **OpenAI GPT**: A model with varied performance across tasks, with a high on SST-2 (91.3) and a low on RTE (56.0).\n2. **CNN Base**: Performs slightly better than OpenAI GPT, notably on QNLI and RTE.\n3. **CNN Large**: Generally performs better than CNN Base, especially on SST-2 and MRPC.\n4. **BPE Large**: Slightly improves on SST-2 and QNLI compared to CNN Base.\n5. **GPT on STILTs**: Shows improvements over OpenAI GPT, especially on RTE.\n6. **BERT_BASE**: Higher scores across most tasks compared to earlier models, especially on MRPC and RTE.\n7. **BERT_LARGE**: Best performance among all models, notably high on CoLA, QNLI, and RTE.\n\n**Overall:**\n- BERT models, particularly BERT_LARGE, demonstrate superior performance across multiple tasks.\n- The table highlights differences in effectiveness between general models and those fine-tuned or larger versions like CNN Large and BERT_LARGE.\nSTS-B Pearson correlation as well as QQP accu- racy. \nTable  2  shows results for three conﬁgurations of our approach (cf. Table  1 ). The BPE model has more parameters than the CNN model but does not perform better in aggregate, however, it is faster to train. All our models outperform the uni- directional transformer (OpenAI GPT) of  Radford et al.  ( 2018 ), however, our model is about   $50\\%$  larger than their model. We also show results for STILTs ( Phang et al. ,  2018 ) and BERT ( Devlin et al. ,  2018 ). Our CNN base model performs as well as STILTs in aggregate, however, on some tasks involving sentence-pairs, STILTs performs much better (MRPC, RTE); there is a similar trend for BERT. \nSTILTs adds another ﬁne-tuning step on an- other downstream task which is similar to the ﬁ- nal task. The technique is equally applicable to our approach. Training examples for our model are Common Crawl paragraphs of arbitrary length. We expect that tailoring training examples for lan- guage model pretraining to the end tasks to signif- icantly improve performance. For example, BERT trains on exactly two sentences while as we train on entire paragraphs. \n6.2 Structured Prediction \nWe also evaluated performance on two structured predictions tasks, NER and constituency parsing. For both problems, we stacked task-speciﬁc archi- tectures from recent work on top of our pretrained two tower models. We evaluate two ways of stack- ing: (1) ELMo-style, where the pretrained mod- els are not ﬁne-tuned but are linearly combined at different depths, and (2) with ﬁne-tuning, where we set different learning rates for the task-speciﬁc \nThe table presents a comparison of different models based on their performance using the F1 score on development and test datasets. Here are the details:\n\n- **ELMo\\(_{BASE}\\)**: \n  - Development F1 Score: 95.7\n  - Test F1 Score: 92.2\n  \n- **CNN Large + ELMo**: \n  - Development F1 Score: 96.4\n  - Test F1 Score: 93.2\n  \n- **CNN Large + fine-tune**: \n  - Development F1 Score: 96.9\n  - Test F1 Score: 93.5\n  \n- **BERT\\(_{BASE}\\)**: \n  - Development F1 Score: 96.4\n  - Test F1 Score: 92.4\n  \n- **BERT\\(_{LARGE}\\)**: \n  - Development F1 Score: 96.6\n  - Test F1 Score: 92.8\n\nThe table indicates that \"CNN Large + fine-tune\" achieved the highest F1 scores on both the development (96.9) and test (93.5) datasets among the models compared.\nThe table presents the performance of different models on a development (dev) and test dataset, using the F1 score as the evaluation metric. Here's a breakdown of the content in the table:\n\n- **ELMoBASE**:\n  - Dev F1 Score: 95.2\n  - Test F1 Score: 95.1\n\n- **CNN Large + ELMo**:\n  - Dev F1 Score: 95.1\n  - Test F1 Score: 95.2\n\n- **CNN Large + fine-tune**:\n  - Dev F1 Score: 95.5\n  - Test F1 Score: 95.6\n\nThe table suggests that among the three models listed, \"CNN Large + fine-tune\" has the highest F1 scores on both the development and test datasets.\nlayers but otherwise update all of the parameters during the task-speciﬁc training. \n6.2.1 Named Entity Recognition \nWe evaluated span-level F1 performance on the CoNLL 2003 Named Entity Recognition (NER) task, where spans of text must be segmented and labeled as Person, Organization, Location, or Mis- cellaneous. We adopted the NER architecture in Peters et al.  ( 2018 ), a biLSTM-CRF, with two mi- nor modiﬁcations: (1) instead of two layers of biL- STM, we only used one, and (2) a linear projection layer was added between the token embedding and biLSTM layer. We did grid search on the pairs of learning rate, and found that projection-biLSTM- "}
{"page": 6, "image_path": "doc_images/D19-1539_6.jpg", "ocr_text": "CoLA  SST-2. MRPC\n\nSTS-B\n\nMNLI-m\n\nQQP QNLI RTE\n\n(mcc) (acc) (Fl) (scc) (Fl) (acc) (acc) (acc) Avg\ncloze 55.1 92.9 88.3 88.3 87.2 82.3 86.5 66.4 80.9\nbilm 50.0 924 866 87.1 86.1 81.7 840 66.4 79.3\ncloze+bilm 52.6 93.2 88.9 87.9 87.2 82.1 86.1 65.5 80.4\n\nTable 5: Different loss functions on the development sets of GLUE (cf. Table 2). Results are based on the CNN\n\nbase model (Table 1)\n\nCRF with 1E-03 and pretrained language model\nwith 1E-05 gave us the best result.\n\nTable 3 shows the results, with comparison\nto previous published ELMog 4gz results (Peters\net al., 2018) and the BERT models. Both of our\nstacking methods outperform the previous state of\nthe art, but fine tuning gives the biggest gain.\n\n6.2.2 Constituency Parsing\n\nWe also report parseval F1 for Penn Treebank con-\nstituency parsing. We adopted the current state-of-\nthe-art architecture (Kitaev and Klein, 2018). We\nagain used grid search for learning rates and num-\nber of layers in parsing encoder, and used 8E-04\nfor language model finetuning, 8E-03 for the pars-\ning model parameters, and two layers for encoder.\n\nTable 4 shows the results. Here, fine tuning is\nrequired to achieve gains over the previous state\nof the art, which used ELMo embeddings.\n\n6.3 Objective functions for pretraining\n\nThe two-tower model is trained to predict the cur-\nrent token given representations of the entire left\nand right context (cloze). Next we compare this\nchoice to two alternatives: First, Peters et al.\n(2018) train two language models operating left-\nto-right and right-to-left to predict the next word\nfor each respective direction. We change the two-\ntower model to predict the next word using the in-\ndividual towers only and remove the combination\nmodule on top of the two towers (bilm); however,\nwe continue to jointly train the two towers.\nSecond, we combine the cloze loss with the\nbilm loss to obtain a triplet loss which trains the\nmodel to predict the current word given both left\nand right context, as well as just right or left con-\ntext. The latter is much harder than the cloze loss\nsince less context is available and therefore gradi-\nents for the bilm loss are much larger: the cloze\nmodel achieves perplexity of about 4 while as for\nthe bilm it is 27-30, depending on the direction.\nThis results in the bilm loss dominating the triplet\n\n81.5\n\n—e— Average GLUE score\n\n81\n\n80.5\n\nAvg. GLUE score\n\n80\n\n562M 1.1B 2.25B 45B 9B 18B\n\nTrain data tokens\n\nFigure 3: Average GLUE score with different amounts\nof Common Craw] data for pretraining.\n\nloss and we found that scaling the bilm term by a\nfactor of 0.15 results in better performance.\n\nTable 5 shows that the cloze loss performs sig-\nnificantly better than the bilm loss and that com-\nbining the two loss types does not improve over\nthe cloze loss by itself. We conjecture that in-\ndividual left and right context prediction tasks\nare too different from center word prediction and\nthat their learning signals are not complementary\nenough.\n\n6.4 Domain and amount of training data\n\nNext we investigate how much pretraining benefits\nfrom larger training corpora and how the domain\nof the data influences end-task performance.\n\nFigure 3 shows that more training data can sig-\nnificantly increase accuracy. We train all models\nwith the exact same hyper-parameter settings on\nCommon Crawl data using the CNN base archi-\ntecture for 600K updates. We train on up to 18B\nCommon Crawl tokens and the results suggest that\nmore training data is likely to further increase per-\nformance.\n\nTable 6 shows a breakdown into individual\n\n5366\n", "vlm_text": "The table presents performance metrics for different models on various Natural Language Processing (NLP) tasks. Here is the breakdown of the table contents:\n\n- Columns represent different NLP tasks and their corresponding performance metrics:\n  - CoLA (mcc): Matthews correlation coefficient for the CoLA task.\n  - SST-2 (acc): Accuracy for the SST-2 task.\n  - MRPC (F1): F1 Score for the MRPC task.\n  - STS-B (scc): Spearman correlation coefficient for the STS-B task.\n  - QQP (F1): F1 Score for the QQP task.\n  - MNLI-m (acc): Accuracy for the MNLI-matched task.\n  - QNLI (acc): Accuracy for the QNLI task.\n  - RTE (acc): Accuracy for the RTE task.\n  - Avg: Average performance across all tasks.\n\n- Rows represent different modeling approaches:\n  - cloze: Performance using the \"cloze\" modeling approach.\n  - bilm: Performance using the \"bilm\" modeling approach.\n  - cloze + bilm: Performance using a combination of \"cloze\" and \"bilm\" modeling approaches.\n\n- Performance values are numerical scores indicating the efficacy of the model on the respective task for each modeling approach. The \"Avg\" column provides an average score across all the tasks for each approach.\n\nFor instance, the \"cloze\" approach achieves a score of 55.1 on CoLA and an average score of 80.9 across all tasks.\nCRF with 1E-03 and pretrained language model with 1E-05 gave us the best result. \nTable  3  shows the results, with comparison to previous published   $\\mathrm{ELLMo}_{B A S E}$   results ( Peters et al. ,  2018 ) and the BERT models. Both of our stacking methods outperform the previous state of the art, but ﬁne tuning gives the biggest gain. \n6.2.2 Constituency Parsing \nWe also report parseval F1 for Penn Treebank con- stituency parsing. We adopted the current state-of- the-art architecture ( Kitaev and Klein ,  2018 ). We again used grid search for learning rates and num- ber of layers in parsing encoder, and used 8E-04 for language model ﬁnetuning, 8E-03 for the pars- ing model parameters, and two layers for encoder. \nTable  4  shows the results. Here, ﬁne tuning is required to achieve gains over the previous state of the art, which used ELMo embeddings. \n6.3 Objective functions for pretraining \nThe two-tower model is trained to predict the cur- rent token given representations of the entire left and right context (cloze). Next we compare this choice to two alternatives: First,  Peters et al. ( 2018 ) train two language models operating left- to-right and right-to-left to predict the next word for each respective direction. We change the two- tower model to predict the next word using the in- dividual towers only and remove the combination module on top of the two towers (bilm); however, we continue to jointly train the two towers. \nSecond, we combine the cloze loss with the bilm loss to obtain a triplet loss which trains the model to predict the current word given both left and right context, as well as just right or left con- text. The latter is much harder than the cloze loss since less context is available and therefore gradi- ents for the bilm loss are much larger: the cloze model achieves perplexity of about 4 while as for the bilm it is 27-30, depending on the direction. This results in the bilm loss dominating the triplet \nThe image is a line graph depicting the relationship between the amount of Common Crawl data (measured in tokens) used for pretraining and the average GLUE score achieved. The x-axis represents the number of train data tokens in millions or billions (562M, 1.1B, 2.25B, 4.5B, 9B, 18B), while the y-axis represents the average GLUE score (ranging approximately from 80 to 81.5). As the number of train data tokens increases, the average GLUE score also increases, indicating a positive correlation between the amount of pretraining data and performance on the GLUE benchmark. The trend in the graph shows an upward progression, suggesting that more pretraining data typically leads to better performance.\nloss and we found that scaling the bilm term by a factor of  0 . 15  results in better performance. \nTable  5  shows that the cloze loss performs sig- niﬁcantly better than the bilm loss and that com- bining the two loss types does not improve over the cloze loss by itself. We conjecture that in- dividual left and right context prediction tasks are too different from center word prediction and that their learning signals are not complementary enough. \n6.4 Domain and amount of training data \nNext we investigate how much pretraining beneﬁts from larger training corpora and how the domain of the data inﬂuences end-task performance. \nFigure  3  shows that more training data can sig- niﬁcantly increase accuracy. We train all models with the exact same hyper-parameter settings on Common Crawl data using the CNN base archi- tecture for 600K updates. We train on up to 18B Common Crawl tokens and the results suggest that more training data is likely to further increase per- formance. \nTable  6  shows a breakdown into individual "}
{"page": 7, "image_path": "doc_images/D19-1539_7.jpg", "ocr_text": "traindata CoLA SST-2 MRPC_ STS-B) QQP MNLI-m QNLI RTE Avg\n\n(M tok) (mcc) (acc) (Fl) (scc) (Fl) (acc) (acc) (acc)\n562. 52.5 92.9 88.2 88.3 87.1 81.7. 85.7 63.3 79.9\n1125. 55.5 93.1 86.1 88.4 87.1 81.9 85.7 65.2 80.4\ncerawl 2250 55.4 924 87.7 884 87.2 82.2 86.2 66.9 80.8\n4500 56.6 93.0 87.3 886 87.0 82.0 86.2 65.7 80.8\n9000 55.1 92.9 88.3 88.3 87.2 82.3 86.5 66.4 80.9\n18000 56.3 93.1 88.0 88.8 87.2 82.3 86.3 684 81.3\n562 50.9 92.8 814 78.2 84.9 79.1 82.0 55.7 75.6\nnews 1125 514 93.0 83.0 82.3 85.2 79.7 82.8 53.9 76.4\ncrawl 2250 54.8 92.9 83.5 82.8 85.4 80.4 82.4 548 77.1\n4500 53.9 93.6 83.8 83.1 85.5 80.4 83.6 54.2 77.3\nBWiki - sent 3300 53.5 916 864 86.2 86.9 82.3 86.9 63.8 79.7\nBWiki - blck 3300 50.6 91.9 864 87.1 868 81.9 86.2 60.4 78.9\n\nTable 6: Effect of different domains and amount of data for pretraining on the on the development sets of GLUE\n(cf. Table 2). Results are based on the CNN base model (Table 1).\n\nGLUE tasks. For pretraining on Common Crawl,\nCoLA and RTE benefit most from additional train-\ning data. The same table also shows results for\nNews Crawl which contains newswire data. This\ndata generally performs less well than Common\nCrawl, even on MRPC which is newswire. A\nlikely reason is that News Crawl examples are in-\ndividual sentences of 23 words on average which\ncompares to several sentences or 50 words on av-\nerage for Common Crawl. Mutli-sentence training\nexamples are more effective for end-tasks based\non sentence pairs, e.g., there is a 14 point accu-\nracy gap on RTE between News Crawl and Com-\nmon Crawl with 4.5B tokens. More News Crawl\ndata is most beneficial for CoLA and STS-B.\n\nWe also experiment with BooksCorpus (Zhu\net al., 2015) as well as English Wikipedia, similar\nto Devlin et al. (2018). Examples in BooksCorpus\nare a mix of individual sentences and paragraphs;\nexamples are on average 36 tokens. Wikipedia ex-\namples are longer paragraphs of 66 words on av-\nerage. To reduce the effect of training on exam-\nples of different lengths, we adopted the following\nstrategy: we concatenate all training examples into\na single string and then crop blocks of 512 consec-\nutive tokens from this string. We train on a batch\nof these blocks (B Wiki - blck). It turns out that this\nstrategy did not work better compared to our exist-\ning strategy of simply using the data as is (B Wiki -\nsent). BooksCorpus and Wikipedia performs very\nwell on QNLI and MNLI but less well on other\ntasks.\n\nIn summary, more data for pretraining improves\nperformance, keeping everything else equal. Also\npretraining on corpora that retains paragraph\nstructure performs better than individual sen-\ntences.\n\n7 Conclusion\n\nWe presented a pretraining architecture based on a\nbi-directional transformer model that predicts ev-\nery token in the training data. The model is trained\nwith a cloze-style objective and predicts the center\nword given all left and right context.\n\nResults on the GLUE benchmark show large\ngains over Radford et al. (2018) for each task,\nwhile experiments with model stacking set new\nstate of the art performance levels for parsing and\nnamed entity recognition. We also did extensive\nexperimental analysis to better understand these\nresults, showing that (1) having multiple sentences\nin each training example is crucial for many tasks;\n(2) pre-training continues to improve performance\nup to 18B tokens and would likely continue to im-\nprove with more data; and finally (3) our novel\ncloze-driven training regime is more effective than\npredicting left and right tokens separately.\n\nIn future work, we will investigate variations\nof our architecture. In particular, we had initial\nsuccess sharing the parameters of the two towers\nwhich allows training much deeper models with-\nout increasing the parameter count.\n\n5367\n", "vlm_text": "The table presents performance metrics of language models trained on different datasets and with varying amounts of training data. The datasets used are labeled as \"ccrawl,\" \"news crawl,\" \"BWiki - sent,\" and \"BWiki - blck.\" For each dataset and size, several evaluation metrics are listed: \n\n- **train data (M tok):** Indicates the amount of training data used, measured in millions of tokens.\n- **CoLA (mcc):** The Matthews correlation coefficient for the CoLA dataset.\n- **SST-2 (acc):** Accuracy on the SST-2 dataset.\n- **MRPC (F1):** F1-score on the MRPC dataset.\n- **STS-B (scc):** Spearman's rank correlation coefficient on the STS-B dataset.\n- **QQP (F1):** F1-score on the QQP dataset.\n- **MNLI-m (acc):** Accuracy on the MNLI-matched dataset.\n- **QNLI (acc):** Accuracy on the QNLI dataset.\n- **RTE (acc):** Accuracy on the RTE dataset.\n- **Avg:** Average performance score across all the listed tasks.\n\nDifferent models have been trained on increasing amounts of data, ranging from 562 million to 18,000 million tokens for \"ccrawl\" and up to 4,500 million tokens for \"news crawl\". Two configurations of the BWiki dataset are provided, both using 3,300 million tokens. The performance of these models is then evaluated based on the various tasks listed, with an average score provided to summarize overall performance across tasks.\nGLUE tasks. For pretraining on Common Crawl, CoLA and RTE beneﬁt most from additional train- ing data. The same table also shows results for News Crawl which contains newswire data. This data generally performs less well than Common Crawl, even on MRPC which is newswire. A likely reason is that News Crawl examples are  in- dividual sentences  of 23 words on average which compares to several sentences or 50 words on av- erage for Common Crawl. Mutli-sentence training examples are more effective for end-tasks based on sentence pairs, e.g., there is a 14 point accu- racy gap on RTE between News Crawl and Com- mon Crawl with 4.5B tokens. More News Crawl data is most beneﬁcial for CoLA and STS-B. \nWe also experiment with BooksCorpus ( Zhu et al. ,  2015 ) as well as English Wikipedia, similar to  Devlin et al.  ( 2018 ). Examples in BooksCorpus are a mix of individual sentences and paragraphs; examples are on average 36 tokens. Wikipedia ex- amples are longer paragraphs of 66 words on av- erage. To reduce the effect of training on exam- ples of different lengths, we adopted the following strategy: we concatenate all training examples into a single string and then crop blocks of  512  consec- utive tokens from this string. We train on a batch of these blocks (BWiki - blck). It turns out that this strategy did not work better compared to our exist- ing strategy of simply using the data as is (BWiki - sent). BooksCorpus and Wikipedia performs very well on QNLI and MNLI but less well on other tasks. \nIn summary, more data for pretraining improves performance, keeping everything else equal. Also pretraining on corpora that retains paragraph structure performs better than individual sen- tences. \n7 Conclusion \nWe presented a pretraining architecture based on a bi-directional transformer model that predicts ev- ery token in the training data. The model is trained with a cloze-style objective and predicts the center word given all left and right context. \nResults on the GLUE benchmark show large gains over  Radford et al.  ( 2018 ) for each task, while experiments with model stacking set new state of the art performance levels for parsing and named entity recognition. We also did extensive experimental analysis to better understand these results, showing that (1) having multiple sentences in each training example is crucial for many tasks; (2) pre-training continues to improve performance up to 18B tokens and would likely continue to im- prove with more data; and ﬁnally (3) our novel cloze-driven training regime is more effective than predicting left and right tokens separately. \nIn future work, we will investigate variations of our architecture. In particular, we had initial success sharing the parameters of the two towers which allows training much deeper models with- out increasing the parameter count. "}
{"page": 8, "image_path": "doc_images/D19-1539_8.jpg", "ocr_text": "References\n\nAlexei Baevski and Michael Auli. 2018. Adaptive in-\nput representations for neural language modeling.\narXiv, abs/1809.10853.\n\nLuisa Bentivogli, Peter Clark, Ido Dagan, and Danilo\nGiampiccolo. 2009. The fifth pascal recognizing\ntextual entailment challenge. In Proc. of TAC.\n\nOndyej Bojar, Christian Federmann, Mark Fishel,\nYvette Graham, Barry Haddow, Matthias Huck,\nPhilipp Koehn, and Christof Monz. 2018. Find-\nings of the 2018 conference on machine translation\n(WMT18). In Proc. of WMT.\n\nDaniel M. Cer, Mona T. Diab, Eneko Agirre, Ifigo\nLopez-Gazpio, and Lucia Specia. 2018. Semeval-\n2017 task 1: Semantic textual similarity - multilin-\ngual and cross-lingual focused evaluation. In Proc.\nof SemEval.\n\nIdo Dagan, Oren Glickman, and Bernardo Magnini.\n2006. The pascal recognizing textual entailment\nchallenge. Machine learning challenges, evaluat-\ning predictive uncertainty, visual object classifica-\ntion, and recognizing textual entailment, pages 177—\n\n190.\n\nAndrew M. _ Dai\nSemi-supervised\nabs/1511.01432.\n\nand Quoc V. Le.\nsequence learning.\n\n2015.\narXiv,\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. CoRR, abs/1810.04805.\n\nWilliam B. Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nIn Proc. of IWP.\n\nDanilo Giampiccolo, Bernardo Magnini, Ido Dagan,\nand Bill Dolan. 2007. The pascal recognizing\ntextual entailment challenge. Proc. of the ACL-\nPASCAL workshop on textual entailment and para-\nphrasing.\n\nEdouard Grave, Piotr Bojanowski, Prakhar Gupta, Ar-\nmand Joulin, and Tomas Mikolov. 2018. Learning\nword vectors for 157 languages. In Proc. of LREC.\n\nEdouard Grave, Armand Joulin, Moustapha Cissé,\nDavid Grangier, and Hervé Jégou. 2017. Efficient\nsoftmax approximation for gpus. In Proc. of ICML.\n\nRoy Bar Haim, Ido Dagan, Bill Dolan, Lisa Ferro,\nDanilo Giampiccolo, Bernardo Magnini, and Idan\nSzpektor. 2006. The pascal recognising textual en-\ntailment challenge.\n\nKaiming He, Xiangyu Zhang, Shaoging Ren, and Jian\nSun. 2015. Deep Residual Learning for Image\nRecognition. In Proc. of CVPR.\n\nHakan Inan, Khashayar Khosravi, and Richard Socher.\n2016. Tying word vectors and word classifiers:\nA loss framework for language modeling. arXiv,\nabs/1611.01462.\n\nYoon Kim, Yacine Jernite, David Sontag, and Alexan-\nder M Rush. 2016. Character-aware neural language\nmodels. In AAAI, pages 2741-2749.\n\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nMethod for Stochastic Optimization. In Proc. of\nICLR.\n\nNikita Kitaev and Dan Klein. 2018. Constituency pars-\ning with a self-attentive encoder. In Proc. of ACL.\n\nSGDR:\narXiv,\n\nIlya Loshchilov and Frank Hutter. 2016.\nstochastic gradient descent with restarts.\nabs/1608.03983.\n\nBryan McCann, James Bradbury, Caiming Xiong, and\nRichard Socher. 2017. Learned in translation: Con-\ntextualized word vectors. In Proc. of NIPS.\n\nOren Melamud, Jacob Goldberger, and Ido Dagan.\n2016. context2vec: Learning generic context em-\nbedding with bidirectional Istm. In Proc. of CoNLL.\n\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela\nFan, Sam Gross, Nathan Ng, David Grangier, and\nMichael Auli. 2019.  fairseq: A fast, extensible\ntoolkit for sequence modeling. In Proc. of NAACL\nSystem Demonstrations.\n\nMyle Ott, Sergey Edunov, David Grangier, and\nMichael Auli. 2018. Scaling neural machine trans-\nlation. In Proc. of WMT.\n\nRazvan Pascanu, Tomas Mikolov, and Yoshua Bengio.\n2013. On the difficulty of training recurrent neural\nnetworks. In Proc. of ICML.\n\nMatthew E Peters, Mark Neumann, Mohit lyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. In Proc. of ACL.\n\nJason Phang, Thibault Fevry, and Samuel R. Bowman.\n2018. Sentence encoders on stilts: Supplementary\ntraining on intermediate labeled-data tasks. arXiv,\nabs/1811.01088.\n\nOfir Press and Lior Wolf. 2017. Using the output em-\nbedding to improve language models. In Proc. of\nEACL.\n\nAlec Radford, Karthik Narasimhan, Tim Sali-\nmans, and Ilya Sutskever. 2018. Improving\nlanguage understanding by generative pre-training.\nhttps://s3-us-west-2.amazonaws.\ncom/openai-assets/research-covers/\nlanguage-unsupervised/language_\nunderstanding_paper.pdf.\n\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev,\nand Percy Liang. 2016. Squad: 100, 000+ ques-\ntions for machine comprehension of text. arXiv,\nabs/1606.05250.\n\n5368\n", "vlm_text": "References \nput representations for neural language modeling. arXiv , abs/1809.10853. Luisa Bentivogli, Peter Clark, Ido Dagan, and Danilo Giampiccolo. 2009. The ﬁfth pascal recognizing textual entailment challenge. In  Proc. of TAC . Ondˇ rej Bojar, Christian Federmann, Mark Fishel, Yvette Graham, Barry Haddow, Matthias Huck, Philipp Koehn, and Christof Monz. 2018. Find- ings of the 2018 conference on machine translation (WMT18). In  Proc. of WMT . Daniel M. Cer, Mona T. Diab, Eneko Agirre, I˜ nigo Lopez-Gazpio, and Lucia Specia. 2018. Semeval- 2017 task 1: Semantic textual similarity - multilin- gual and cross-lingual focused evaluation. In  Proc. of SemEval . Ido Dagan, Oren Glickman, and Bernardo Magnini. 2006. The pascal recognizing textual entailment challenge. Machine learning challenges, evaluat- ing predictive uncertainty, visual object classiﬁca- tion, and recognizing textual entailment , pages 177– 190. Andrew M. Dai and Quoc V. Le. 2015. Semi-supervised sequence learning. arXiv , abs/1511.01432. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understand- ing.  CoRR , abs/1810.04805. William B. Dolan and Chris Brockett. 2005. Automati- cally constructing a corpus of sentential paraphrases. In  Proc. of IWP . Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Bill Dolan. 2007. The pascal recognizing textual entailment challenge. Proc. of the ACL- PASCAL workshop on textual entailment and para- phrasing . Edouard Grave, Piotr Bojanowski, Prakhar Gupta, Ar- mand Joulin, and Tomas Mikolov. 2018. Learning word vectors for 157 languages. In  Proc. of LREC . Edouard Grave, Armand Joulin, Moustapha Ciss´ e, David Grangier, and Herv´ e J´ egou. 2017. Efﬁcient softmax approximation for gpus. In  Proc. of ICML . Roy Bar Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Giampiccolo, Bernardo Magnini, and Idan Szpektor. 2006. The pascal recognising textual en- tailment challenge. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015. Deep Residual Learning for Image Recognition. In  Proc. of CVPR . \nHakan Inan, Khashayar Khosravi, and Richard Socher. 2016. Tying word vectors and word classiﬁers: A loss framework for language modeling. arXiv , abs/1611.01462. Yoon Kim, Yacine Jernite, David Sontag, and Alexan- der M Rush. 2016. Character-aware neural language models. In  AAAI , pages 2741–2749. Diederik P. Kingma and Jimmy Ba. 2015. Adam: A Method for Stochastic Optimization. In  Proc. of ICLR . Nikita Kitaev and Dan Klein. 2018. Constituency pars- ing with a self-attentive encoder. In  Proc. of ACL . Ilya Loshchilov and Frank Hutter. 2016. SGDR: stochastic gradient descent with restarts. arXiv , abs/1608.03983. Bryan McCann, James Bradbury, Caiming Xiong, and Richard Socher. 2017. Learned in translation: Con- textualized word vectors. In  Proc. of NIPS . Oren Melamud, Jacob Goldberger, and Ido Dagan. 2016. context2vec: Learning generic context em- bedding with bidirectional lstm. In  Proc. of CoNLL . Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli. 2019. fairseq: A fast, extensible toolkit for sequence modeling. In  Proc. of NAACL System Demonstrations . Myle Ott, Sergey Edunov, David Grangier, and Michael Auli. 2018. Scaling neural machine trans- lation. In  Proc. of WMT . Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. 2013. On the difﬁculty of training recurrent neural networks. In  Proc. of ICML . Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word rep- resentations. In  Proc. of ACL . Jason Phang, Thibault Fevry, and Samuel R. Bowman. 2018. Sentence encoders on stilts: Supplementary training on intermediate labeled-data tasks.  arXiv , abs/1811.01088. Oﬁr Press and Lior Wolf. 2017. Using the output em- bedding to improve language models. In  Proc. of EACL . Alec Radford, Karthik Narasimhan, Tim Sali- mans, and Ilya Sutskever. 2018. Improving language understanding by generative pre-training. https://s3-us-west-2.amazonaws. com/openai-assets/research-covers/ language-unsupervised/language_ understanding paper.pdf . Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,   ${000+}$   ques- tions for machine comprehension of text. arXiv , abs/1606.05250. "}
{"page": 9, "image_path": "doc_images/D19-1539_9.jpg", "ocr_text": "Rico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words with\nsubword units. In Proc. of ACL.\n\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D. Manning, Andrew Ng, and\nChristopher Potts. 2013. Recursive deep models\nfor semantic compositionality over a sentiment tree-\nbank. In Proc. of EMNLP.\n\nIlya Sutskever, James Martens, George E. Dahl, and\nGeoffrey E. Hinton. 2013. On the importance of ini-\ntialization and momentum in deep learning. In Proc.\nof ICML.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and [Illia Polosukhin. 2017. Attention Is All\nYou Need. In Proc. of NIPS.\n\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2018.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. arXiv,\nabs/1804.07461.\n\nAlex Warstadt, Amanpreet Singh, and Sam Bow-\nman. 2018. Corpus of linguistic acceptability.\nhttps://nyu-mll.github.io/CoLA.\n\nAdina Williams, Nikita Nangia, and Samuel R Bow-\nman. 2018. A broad-coverage challenge corpus for\nsentence understanding through inference. In Proc.\nof NAACL.\n\nKelly W. Zhang and Samuel R. Bowman. 2018. Lan-\nguage modeling teaches you more syntax than trans-\nlation does: Lessons learned through auxiliary task\nanalysis. arXiv, abs/1809.10040.\n\nYukun Zhu, Ryan Kiros, Richard S. Zemel, Ruslan\nSalakhutdinov, Raquel Urtasun, Antonio Torralba,\nand Sanja Fidler. 2015. Aligning books and movies:\nTowards story-like visual explanations by watching\nmovies and reading books. arXiv, abs/1506.06724.\n\n5369\n", "vlm_text": "Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural machine translation of rare words with \nsubword units. In  Proc. of ACL . Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew  $\\mathrm{Mg}$  , and Christopher Potts. 2013. Recursive deep models for semantic compositional it y over a sentiment tree- bank. In  Proc. of EMNLP . Ilya Sutskever, James Martens, George E. Dahl, and Geoffrey E. Hinton. 2013. On the importance of ini- tialization and momentum in deep learning. In  Proc. of ICML . Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention Is All You Need. In  Proc. of NIPS . Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. 2018. GLUE: A multi-task benchmark and analysis plat- form for natural language understanding. arXiv , abs/1804.07461. Alex Warstadt, Amanpreet Singh, and Sam Bow- man. 2018. Corpus of linguistic acceptability. https://nyu-mll.github.io/CoLA. Adina Williams, Nikita Nangia, and Samuel R Bow- man. 2018. A broad-coverage challenge corpus for sentence understanding through inference. In  Proc. of NAACL . Kelly W. Zhang and Samuel R. Bowman. 2018. Lan- guage modeling teaches you more syntax than trans- lation does: Lessons learned through auxiliary task analysis.  arXiv , abs/1809.10040. Yukun Zhu, Ryan Kiros, Richard S. Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books.  arXiv , abs/1506.06724. "}
