{"page": 0, "image_path": "doc_images/2405.09818v1_0.jpg", "ocr_text": "arX1v:2405.09818v1 [cs.CL] 16 May 2024\n\nChameleon: Mixed-Modal Early-Fusion Foundation\n\nModels\n\nChameleon Team!*\n\nTFAIR at Meta\n\n*See Contributions section for full author list.\n\nWe present Chameleon, a family of early-fusion token-based mixed-modal models capable of under-\nstanding and generating images and text in any arbitrary sequence. We outline a stable training\napproach from inception, an alignment recipe, and an architectural parameterization tailored for the\nearly-fusion, token-based, mixed-modal setting. The models are evaluated on a comprehensive range\n\nof tasks, including visual question answering, imag\n\ne captioning, text generation, image generation, and\n\nlong-form mixed modal generation. Chameleon demonstrates broad and general capabilities, including\nstate-of-the-art performance in image captioning tasks, outperforms Llama-2 in text-only tasks while\nbeing competitive with models such as Mixtral 8x7B and Gemini-Pro, and performs non-trivial image\n\ngeneration, all in a single model. It also matches\n\nor exceeds the performance of much larger models,\n\nincluding Gemini Pro and GPT-4V, according to human judgments on a new long-form mixed-modal\ngeneration evaluation, where either the prompt or outputs contain mixed sequences of both images and\n\ntext. Chameleon marks a significant step forward\n\nDate: May 17, 2024\n\n1 Introduction\n\nin a unified modeling of full multimodal documents.\n\n~\n\nRecent multimodal foundation models are very widely adopted but still model different modalities separately,\noften using modality specific encoders or decoders. This can limit their ability to integrate information across\n\nmodalities and generate multimodal documents that\n\ncan contain arbitrary sequences of images and text. In\n\nthis paper, we present Chameleon, a family of mixed-modal foundation models capable of generating and\nreasoning with mixed sequences of arbitrarily interleaved textual and image content (Figures 2-4). This\n\nallows for full multimodal document modeling, which\n\nis a direct generalization of standard multimodal tasks\n\nsuch as image generation, understanding and reasoning over images, and text-only LLMs. Chameleon is\ninstead designed to be mixed-model from inception and uses a uniform architecture trained from scratch in an\nend-to-end fashion on an interleaved mixture of all modalities, i.e., images, text, and code.\n\nOur unified approach uses fully token-based representations for both image and textual modalities (Figure 1).\nBy quantizing images into discrete tokens, analogous to words in text, we can apply the same transformer\narchitecture to sequences of both image and text tokens, without the need for separate image/text encoders\n\n(Alayrac et al., 2022; Liu et al., 2023b; Laurencon et\n2022; Jin et al., 2023; Betker et al., 2023). This early-\na shared representational space from the start, allows\nHowever, it also presents significant technical challen,\nscaling.\n\nWe address these challenges through a combination o:\n\nal., 2023) or domain-specific decoders (Ramesh et al.,\nfusion approach, where all modalities are projected into\nfor seamless reasoning and generation across modalities.\nges, particularly in terms of optimization stability and\n\nf architectural innovations and training techniques. We\n\nintroduce novel modifications to the transformer architecture, such as query-key normalization and revised\n\nplacement of layer norms, which we find to be crucial\n2.3). We further show how to adapt the supervised\nmixed-modal setting, enabling strong alignment at sc:\ntrain Chameleon-34B on 5x the number of tokens as\n\nfor stable training in the mixed-modal setting (Section\nfinetuning approaches used for text-only LLMs to the\nale (Section 3). Using these techniques, we successfully\nLlama-2 — enabling new mixed-modal applications while\n\nstill matching or even outperforming existing LLMs on unimodal benchmarks.\n", "vlm_text": "Chameleon: Mixed-Modal Early-Fusion Foundation Models \nChameleon Team  $^{1,*}$  \n1 FAIR at Meta ∗ See Contributions section for full author list. \nWe present Chameleon, a family of early-fusion token-based mixed-modal models capable of under- standing and generating images and text in any arbitrary sequence. We outline a stable training approach from inception, an alignment recipe, and an architectural parameter iz ation tailored for the early-fusion, token-based, mixed-modal setting. The models are evaluated on a comprehensive range of tasks, including visual question answering, image captioning, text generation, image generation, and long-form mixed modal generation. Chameleon demonstrates broad and general capabilities, including state-of-the-art performance in image captioning tasks, outperforms Llama-2 in text-only tasks while being competitive with models such as Mixtral 8x7B and Gemini-Pro, and performs non-trivial image generation, all in a single model. It also matches or exceeds the performance of much larger models, including Gemini Pro and GPT-4V, according to human judgments on a new long-form mixed-modal generation evaluation, where either the prompt or outputs contain mixed sequences of both images and text. Chameleon marks a significant step forward in a unified modeling of full multimodal documents. \nDate:  May 17, 2024 \n1 Introduction \nRecent multimodal foundation models are very widely adopted but still model different modalities separately, often using modality specific encoders or decoders. This can limit their ability to integrate information across modalities and generate multimodal documents that can contain arbitrary sequences of images and text. In this paper, we present  Chameleon , a family of mixed-modal foundation models capable of generating and reasoning with mixed sequences of arbitrarily interleaved textual and image content (Figures  2 - 4 ). This allows for full multimodal document modeling, which is a direct generalization of standard multimodal tasks such as image generation, understanding and reasoning over images, and text-only LLMs.  Chameleon  is instead designed to be mixed-model from inception and uses a uniform architecture trained from scratch in an end-to-end fashion on an interleaved mixture of all modalities, i.e., images, text, and code. \nOur unified approach uses fully token-based representations for both image and textual modalities (Figure  1 ). By quantizing images into discrete tokens, analogous to words in text, we can apply the same transformer architecture to sequences of both image and text tokens, without the need for separate image/text encoders ( Alayrac et al. ,  2022 ;  Liu et al. ,  2023b ;  Laurençon et al. ,  2023 ) or domain-specific decoders ( Ramesh et al. , 2022 ;  Jin et al. ,  2023 ;  Betker et al. ,  2023 ). This early-fusion approach, where all modalities are projected into a shared representational space from the start, allows for seamless reasoning and generation across modalities. However, it also presents significant technical challenges, particularly in terms of optimization stability and scaling. \nWe address these challenges through a combination of architectural innovations and training techniques. We introduce novel modifications to the transformer architecture, such as query-key normalization and revised placement of layer norms, which we find to be crucial for stable training in the mixed-modal setting (Section 2.3 ). We further show how to adapt the supervised finetuning approaches used for text-only LLMs to the mixed-modal setting, enabling strong alignment at scale (Section  3 ). Using these techniques, we successfully train  Chameleon-34B  on 5x the number of tokens as Llama-  $2-$   enabling new mixed-modal applications while still matching or even outperforming existing LLMs on unimodal benchmarks. "}
{"page": 1, "image_path": "doc_images/2405.09818v1_1.jpg", "ocr_text": "“Here is a recipe for\n\nbanana bread.” e\n\nIMAGE OUTPUT\n\nMixed-Modal Auto-Regressive LM\n\nMixed Modal Auto-Regressive LM\n\nS\nry\nEnd Start\nImage Image\n\n(b) Mixed-Modal Generation\n\n“What can | bake\nwith this?”\n\nIMAGE PROMPT\n\n(a) Mixed-Modal Pre-Training\n\nFigure 1 Chameleon represents all modalities — images, text, and code, as discrete tokens and uses a uniform\ntransformer-based architecture that is trained from scratch in an end-to-end fashion on ~10T tokens of interleaved\nmixed-modal data. As a result, Chameleon can both reason over, as well as generate, arbitrary mixed-modal documents.\nText tokens are represented in green and image tokens are represented in blue.\n\nExtensive evaluations demonstrate that Chameleon is a broadly capable model on a diverse set of tasks.\nOn visual question answering and image captioning benchmarks, Chameleon-34B achieves state-of-the-art\nperformance, outperforming models like Flamingo, IDEFICS and Llava-1.5 (Section 5.2). At the same time,\nit maintains competitive performance on text-only benchmarks, matching models like Mixtral 8x7B and\nGemini-Pro on commonsense reasoning and reading comprehension tasks (Section 5.1). But perhaps most\nimpressively, Chameleon unlocks entirely new capabilities in terms of mixed-modal reasoning and generation.\n\nAs using only static, public benchmarks to evaluate model performance could be limited (Schaeffer, 2023), we\nalso conduct a carefully designed human evaluation experiment by measuring the quality of mixed-modal\nlong form responses to open-ended prompts. Chameleon-34B substantially outperforms strong baselines like\nGemini-Pro and GPT-4V (Section 4), achieving a 60.4% preference rate against Gemini-Pro and a 51.6%\npreference rate against GPT-4V in pairwise comparisons.\n\nIn summary, we present the following contributions:\n\ne We present Chameleon, a family of early-fusion token-based mixed-modal models capable of reasoning\nover and generating interleaved image-text documents, setting a new bar for open multimodal foundation\nmodels.\n\ne We introduce architectural innovations and training techniques that enable the stable and scalable\ntraining of early-fusion token-based models, addressing key challenges in mixed-modal learning.\n\ne Through extensive evaluations, we demonstrate state-of-the-art performance across a diverse set of\nvision-language benchmarks, while maintaining competitive performance on text-only tasks, and high\nquality image generation, all in the same model.\n\ne We conduct the first large-scale human evaluation on open-ended mixed-modal reasoning and generation,\ndemonstrating the unique capabilities of Chameleon in this new setting.\n\nChameleon represents a significant step towards realizing the vision of unified foundation models capable of\nflexibly reasoning over and generating multimodal content.\n", "vlm_text": "The image consists of two diagrams illustrating a Mixed-Modal Auto-Regressive Language Model (LM) called \"Chameleon\" that handles text and image tokens. \n\n1. **Left Diagram (a): Mixed-Modal Pre-Training**\n   - Shows a flow where a text and image prompt (e.g., a bowl of bananas) are processed together.\n   - The prompts are tokenized into text tokens (green) and image tokens (blue).\n   - These tokens are fed into the LM for pre-training.\n\n2. **Right Diagram (b): Mixed-Modal Generation**\n   - Illustrates how the LM generates outputs.\n   - It can create text outputs (e.g., a recipe for banana bread) and image outputs (e.g., an image of banana bread) from processed tokens.\n   - Image de-tokenizer converts the image tokens back into a coherent image.\n\nThe diagrams highlight the LM's ability to handle and generate mixed-modal documents using input tokens from both text and images.\nExtensive evaluations demonstrate that  Chameleon  is a broadly capable model on a diverse set of tasks. On visual question answering and image captioning benchmarks,  Chameleon-34B  achieves state-of-the-art performance, outperforming models like Flamingo, IDEFICS and Llava-1.5 (Section  5.2 ). At the same time, it maintains competitive performance on text-only benchmarks, matching models like Mixtral 8x7B and Gemini-Pro on commonsense reasoning and reading comprehension tasks (Section  5.1 ). But perhaps most impressively,  Chameleon  unlocks entirely new capabilities in terms of mixed-modal reasoning and generation. \nAs using only static, public benchmarks to evaluate model performance could be limited ( Schaeffer ,  2023 ), we also conduct a carefully designed human evaluation experiment by measuring the quality of mixed-modal long form responses to open-ended prompts.  Chameleon-34B  substantially outperforms strong baselines like Gemini-Pro and GPT-4V (Section  4 ), achieving a   $60.4\\%$   preference rate against Gemini-Pro and a   $51.6\\%$  preference rate against GPT-4V in pairwise comparisons. \nIn summary, we present the following contributions: \n•  We present  Chameleon , a family of early-fusion token-based mixed-modal models capable of reasoning over and generating interleaved image-text documents, setting a new bar for open multimodal foundation models. •  We introduce architectural innovations and training techniques that enable the stable and scalable training of early-fusion token-based models, addressing key challenges in mixed-modal learning. •  Through extensive evaluations, we demonstrate state-of-the-art performance across a diverse set of vision-language benchmarks, while maintaining competitive performance on text-only tasks, and high quality image generation, all in the same model. •  We conduct the first large-scale human evaluation on open-ended mixed-modal reasoning and generation, demonstrating the unique capabilities of  Chameleon  in this new setting. \nChameleon  represents a significant step towards realizing the vision of unified foundation models capable of flexibly reasoning over and generating multimodal content. "}
{"page": 2, "image_path": "doc_images/2405.09818v1_2.jpg", "ocr_text": "Prompt: I’m bored. Could you show me some cool, quirky-looking birds? Also tell me about them in 2-3\nsentences.\n\nResponse: Absolutely! Here are three birds with some unique features that\nyou might find interesting:\n\nKeel-Billed Toucan The Keel-Billed Toucan is a brightly colored bird\nnative to South American rainforests and woodlands. Its most distinctive\nfeature is its colorful beak, which can grow up to seven inches long and has a\ndistinctive, curved shape. The beak is used for plucking fruit from trees and\nfor social displays. Despite its large size, the Keel-Billed Toucan is a skilled\nflyer and can often be seen flitting through the treetops. Here is an image of\nthis vibrant bird: <img>\n\nPuffin Puffins are small, aquatic birds that are found in the northern hemisphere.\nThey have distinctive black and white feathers, as well as a brightly colored\nbeak that is used for attracting mates and defending their territory. Puffins\nare known for their distinctive \"puffin\" call, which sounds like a loud, guttural\nchuckle. They feed on fish, which they catch by diving into the water from the\nair. Puffins are colonial birds, and they breed in large colonies on rocky cliffs\nand islands. Here is an image of this adorable bird: <img>\n\nGolden Pheasant The Golden Pheasant is a brightly colored bird native to the\norests of China. It is a member of the pheasant family and is known for its\nstriking plumage, which features a golden-yellow body, red face, and green tail\neathers. The Golden Pheasant is a ground-dwelling bird, and it feeds on a diet\nof insects, seeds, and fruits. It is a popular game bird, and is often hunted for\nits meat and feathers. In captivity, the Golden Pheasant is sometimes kept as\nan ornamental bird, due to its striking appearance. Here is an image of this\nbeautiful bird: <img>\n\nI hope these images and descriptions have helped to brighten your day! If you have any other questions\nor requests, please don’t hesitate to ask.\n\nFigure 2 Sample interleaved image and text generation from Chameleon. The corresponding images are generated in\nlocations marked by <img>.\n\nw\n", "vlm_text": "Prompt:  I’m bored. Could you show me some cool, quirky-looking birds? Also tell me about them in 2–3 sentences. \nResponse:  Absolutely! Here are three birds with some unique features that you might find interesting: \nThe image shows a toucan, a bird known for its large, colorful beak. The toucan is perched on a branch, and the background is a blurred green, likely indicating a natural setting like a forest or jungle.\nKeel-Billed Toucan  The Keel-Billed Toucan is a brightly colored bird native to South American rainforests and woodlands. Its most distinctive feature is its colorful beak, which can grow up to seven inches long and has a distinctive, curved shape. The beak is used for plucking fruit from trees and for social displays. Despite its large size, the Keel-Billed Toucan is a skilled flyer and can often be seen flitting through the treetops. Here is an image of this vibrant bird:  <img> \nThe image shows a puffin standing on a grassy area with a blurred background that appears to be the sea. The puffin has a distinctive colorful beak, black and white plumage, and orange feet.\nPuffin  Puffins are small, aquatic birds that are found in the northern hemisphere. They have distinctive black and white feathers, as well as a brightly colored beak that is used for attracting mates and defending their territory. Puffins are known for their distinctive \"puffin\" call, which sounds like a loud, guttural chuckle. They feed on fish, which they catch by diving into the water from the air. Puffins are colonial birds, and they breed in large colonies on rocky cliffs and islands. Here is an image of this adorable bird:  <img> \nThe image shows a close-up of a Golden Pheasant, also known as a Chinese Pheasant. This bird is characterized by its bright yellow crest, red and orange plumage, and distinctively colored neck feathers that alternate between black and orange stripes. The background is blurred, highlighting the vivid colors and details of the bird's head and neck."}
{"page": 3, "image_path": "doc_images/2405.09818v1_3.jpg", "ocr_text": "2. Pre-Training\n\nChameleon represents images, in addition to text, as a series of discrete tokens and takes advantage of the\nscaling properties of auto-regressive Transformers (Ramesh et al., 2021; Aghajanyan et al., 2022, 2023; Yu\net al., 2023). We present any ordering of images and text during training ranging from text-only, to single\ntext /image pairs to full interleaved text-image documents.\n\nPrompt: <img> While chameleons are masters of camouflage in the natural\nworld, let’s see how they might fare in a man-made setting.\nexan 1. Analyze the colors, patterns, and textures in this image and identify the\nITU most challenging spot for a chameleon to blend in.\nx ’ 2. Then generate an image of a chameleon with the optimal pattern to\na: camouflage itself.\n3. Then, rate and explain how successful each attempt is on a scale from 1-10.\n\nResponse: The image is of a stop sign in a forested area, with some sunlight filtering through the trees.\nThe sign has a red octagonal shape with a white border and the word \"STOP\" written in white letters.\nIn the background, there are green leaves and trees.\n\nChallenging Spot for a Chameleon: The stop sign would be the most challenging spot for a chameleon\nto blend in due to its bright red color and bold white letters, which would stand out in the forested\nenvironment.\n\nOptimal Pattern for Camouflage: To camouflage itself on the stop sign, a chameleon would need\nto have a pattern that would break up its outline and make it less conspicuous. A pattern with shades\n< of brown and red would help the chameleon blend in more effectively.\n—~ Additionally, a mottled or dappled pattern would help to scatter\nlight and further reduce the chameleon’s visibility.\n<img>\nRating and Explanation: A rating of 2/10 would be given for\nthe chameleon’s ability to blend in on the stop sign. While the\nchameleon’s green and brown pattern helps to break up its outline,\nthe bright red color of the stop sign will still make it stand out in\nthe forested environment.\n\nFigure 3 Sample generation from Chameleon.\n\n2.1. Tokenization\n\nImage Tokenization We train a new image tokenizer based on Gafni et al. (2022), which encodes a 512 x 512\nimage into 1024 discrete tokens from a codebook of size 8192. For training this tokenizer, we use only licensed\nimages. Given the importance of generating human faces, we up-sample the percentage of images with faces\nduring pre-training by 2 times. A core weakness of our tokenizer is in reconstructing images with a large\namount of text, therefore upper bounding the capability of our models, when it comes to heavy OCR-related\ntasks.\n\nTokenizer We train a new BPE tokenizer (Sennrich et al., 2016) over a subset of the training data outlined\nbelow with a vocabulary size of 65,536, which includes the 8192 image codebook tokens, using the sentencepiece\nlibrary (Kudo and Richardson, 2018).\n", "vlm_text": "2 Pre-Training \nChameleon  represents images, in addition to text, as a series of discrete tokens and takes advantage of the scaling properties of auto-regressive Transformers ( Ramesh et al. ,  2021 ;  Aghajanyan et al. ,  2022 ,  2023 ;  Yu et al. ,  2023 ). We present any ordering of images and text during training ranging from text-only, to single text/image pairs to full interleaved text-image documents. \nThe image shows an old, weathered stop sign in what appears to be a forest or wooded area. The sign is red with the word \"STOP\" written in white letters and appears to be mounted on a wooden pole. The surrounding background consists of green trees and foliage.\nPrompt:  <img>  While chameleons are masters of camouflage in the natural world, let’s see how they might fare in a man-made setting. 1. Analyze the colors, patterns, and textures in this image and identify the most challenging spot for a chameleon to blend in. 2. Then generate an image of a chameleon with the optimal pattern to camouflage itself. 3. Then, rate and explain how successful each attempt is on a scale from 1-10. \nResponse:  The image is of a stop sign in a forested area, with some sunlight filtering through the trees. The sign has a red octagonal shape with a white border and the word \"STOP\" written in white letters. In the background, there are green leaves and trees. \nChallenging Spot for a Chameleon:  The stop sign would be the most challenging spot for a chameleon to blend in due to its bright red color and bold white letters, which would stand out in the forested environment. \nOptimal Pattern for Camouflage: To camouflage itself on the stop sign, a chameleon would need to have a pattern that would break up its outline and make it less conspicuous. A pattern with shades \nof brown and red would help the chameleon blend in more effectively. Rating and Explanation: A rating of 2/10 would be given for \n\n2.1 Token iz ation \nImage Token iz ation We train a new image tokenizer based on  Gafni et al.  ( 2022 ), which encodes a    $512\\times512$  image into  1024  discrete tokens from a codebook of size  8192 . For training this tokenizer, we use only licensed images. Given the importance of generating human faces, we up-sample the percentage of images with faces during pre-training by 2 times. A core weakness of our tokenizer is in reconstructing images with a large amount of text, therefore upper bounding the capability of our models, when it comes to heavy OCR-related tasks. \nTokenizer We train a new BPE tokenizer ( Sennrich et al. ,  2016 ) over a subset of the training data outlined below with a vocabulary size of 65,536, which includes the 8192 image codebook tokens, using the  sentence piece library ( Kudo and Richardson ,  2018 ). "}
{"page": 4, "image_path": "doc_images/2405.09818v1_4.jpg", "ocr_text": "Prompt: What can I bake with this? Give me the full recipe and show me an\nimage with the final dish. <img>\n\nResponse: Bananas are a versatile fruit that can be used in various baked\ngoods, such as muffins, bread, cakes, and cookies. They add natural sweetness,\nmoisture, and a subtle banana flavor to the finished product. Additionally,\nbananas are rich in potassium, fiber, and other nutrients, making them a\nhealthy addition to your baking repertoire.\n\nHere is a simple recipe for banana bread that uses 3 ripe bananas:\n\nIngredients:\ne 3 ripe bananas, mashed\n1 1/2 cups all-purpose flour\n1 tsp baking powder\n1/2 tsp baking soda\n1/2 tsp salt\n1/4 cup unsalted butter, softened\n1 cup granulated sugar\n2 large eggs\n1 tsp vanilla extract\n1/2 cup chopped walnuts (optional)\nInstructions:\ne Preheat your oven to 350°F (180°C). Grease a 9x5-inch loaf pan and set it aside.\ne Ina medium bowl, whisk together the flour, baking powder, baking soda, and salt. Set aside.\ne Ina large mixing bowl, use an electric mixer to cream together the butter and sugar until light and\nfluffy, about 2-3 minutes.\ne Beat in the eggs one at a time, followed by the mashed bananas and vanilla extract.\n\ne Gradually mix in the dry ingredients until just combined, being careful\nnot to overmix.\n\ne If using walnuts, fold them into the batter.\n\ne Pour the batter into the prepared loaf pan and smooth the top.\n\ne Bake for 50-60 minutes, or until a toothpick inserted into the center of\nthe bread comes out clean.\nRemove the bread from the oven and let it cool in the pan for 10 minutes\nbefore transferring it to a wire rack to cool completely. <img>\n\nFigure 4 Sample Chameleon outputs.\n\n2.2. Pre-Training Data\n\nWe delineate the pre-training stage into two separate stages. The first stage takes up the first 80% of training\nwhile the second stage takes the last 20%. For all Text-To-Image pairs we rotate so that 50% of the time the\nimage comes before the text (i.e., captioning).\n\n2.2.1 First Stage\nIn the first stage we use a data mixture consisting of the following very large scale completely unsupervised\n\ndatasets.\n\nText-Only: We use a variety of textual datasets, including a combination of the pre-training data used to\ntrain LLaMa-2 (Touvron et al., 2023) and CodeLLaMa (Rozicre et al., 2023) for a total of 2.9 trillion text-only\ntokens.\n\n", "vlm_text": "The image shows a whole banana and a bowl containing banana slices. The items are placed on a green fabric surface.\nPrompt:  What can I bake with this? Give me the full recipe and show me an image with the final dish.  <img> Response:  Bananas are a versatile fruit that can be used in various baked goods, such as muffins, bread, cakes, and cookies. They add natural sweetness, moisture, and a subtle banana flavor to the finished product. Additionally, bananas are rich in potassium, fiber, and other nutrients, making them a healthy addition to your baking repertoire. \nHere is a simple recipe for banana bread that uses 3 ripe bananas: \nIngredients: \n• 3 ripe bananas, mashed • 1 1/2 cups all-purpose flour • 1 tsp baking powder • 1/2 tsp baking soda • 1/2 tsp salt • 1/4 cup unsalted butter, softened • 1 cup granulated sugar • 2 large eggs • 1 tsp vanilla extract • 1/2 cup chopped walnuts (optional) \nInstructions: \n• Preheat your oven to 350°F (180°C). Grease a 9x5-inch loaf pan and set it aside. • In a medium bowl, whisk together the flour, baking powder, baking soda, and salt. Set aside. •  In a large mixing bowl, use an electric mixer to cream together the butter and sugar until light and fluffy, about 2-3 minutes. • Beat in the eggs one at a time, followed by the mashed bananas and vanilla extract. \nThe image shows a loaf of bread on a white plate, with a couple of slices cut from it.\n•  Gradually mix in the dry ingredients until just combined, being careful not to overmix.\n\n • If using walnuts, fold them into the batter.\n\n • Pour the batter into the prepared loaf pan and smooth the top.\n\n •  Bake for 50-60 minutes, or until a toothpick inserted into the center of the bread comes out clean.\n\n •  Remove the bread from the oven and let it cool in the pan for 10 minutes before transferring it to a wire rack to cool completely.  <img> \n2.2 Pre-Training Data \nWe delineate the pre-training stage into two separate stages. The first stage takes up the first 80% of training while the second stage takes the last   $20\\%$  . For all  Text-To-Image  pairs we rotate so that 50% of the time the image comes before the text (i.e., captioning). \n2.2.1 First Stage \nIn the first stage we use a data mixture consisting of the following very large scale completely unsupervised datasets. \nText-Only: We use a variety of textual datasets, including a combination of the pre-training data used to train LLaMa-2 ( Touvron et al. ,  2023 ) and CodeLLaMa ( Roziere et al. ,  2023 ) for a total of  2.9 trillion  text-only tokens. "}
{"page": 5, "image_path": "doc_images/2405.09818v1_5.jpg", "ocr_text": "—— w/ QKnorm and dropout — wie QKnorm — wo dropout\n30.0} —— w/o dropout 37 — w/QkKnorm 38 —— w/ dropout\n—— w/o Qk-norm or dropout ”\n25.0\n3.6 ry\n5 20.0 g g37\nz 23.6 2\n$15.0 © iS\n3 Bas B36\n10.0\n3.5 3.5\n5.0\n3.4\n0.0 3.4\nOk 5k 10k 15k 20k 25k 30k Ok 25k 50k 75k 100k 125k 150k 175k Ok 20k 40k 60k 80k\nStep Step Step\n(a) Uncontrolled growth of output (b) An ablation with Chameleon-7B (c) An ablation with Chameleon-7B\nnorms is a strong indicator of future with and without QK-Norm. with and without dropout.\n\ntraining divergence.\nFigure 5 Output norm and training loss curves for Chameleon models under various settings.\nText-Image: The text-image data for pre-training is a combination of publicly available data sources and\n\nlicensed data. The images are then resized and center cropped into 512 x 512 images for tokenization. In\notal, we include 1.4 billion text-image pairs, which produces 1.5 trillion text-image tokens.\n\nText/Image Interleaved: We procure data from publicly available web sources, not including data from Meta’s\nroducts or services, for a total of 400 billion tokens of interleaved text and image data similar to Laurengon\n\net al. (2023). We apply the same filtering for images, as was applied in Text-To-Image.\n\n2.2.2 Second Stage\n\nn the second stage, we lower the weight of the first stage data by 50% and mix in higher quality datasets\nwhile maintaining a similar proportion of image text tokens.\n\nWe additionally include a filtered subset of the train sets from a large collection of instruction tuning sets.\n\n2.3. Stability\n\nIt was challenging to maintain stable training when scaling the Chameleon models above 8B parameters\nand 1T tokens, with instabilities often only arising very late in training. We adopted to following recipe for\narchitecture and optimization to achieve stability.\n\nArchitecture Our architecture largely follows LLaMa-2 (Touvron et al., 2023). For normalization, we continue\no use RMSNorm (Zhang and Sennrich, 2019); we use the SwiGLU (Shazeer, 2020) activation function and\nrotary positional embeddings (RoPE) (Su et al., 2021).\n\nWe found that the standard LLaMa architecture showed complex divergences due to slow norm growth in the\nmid-to-late stages of training. We narrowed down the cause of the divergence to the softmax operation being\nproblematic when training with multiple modalities of significantly varying entropy due to the translation\ninvariant property of softmax (i.e., softmax(z) = softmaa(z+c)). Because we share all weights of the model\nacross modalities, each modality will try to “compete” with the other by increasing its norms slightly; while\nnot problematic at the beginning of training, it manifests in divergences once we get outside the effective\nrepresentation range of bf16 (In Figure 6b, we show that ablations without image generation did not diverge).\nn a unimodal setting, this problem has also been named the logit drift problem (Wortsman et al., 2023). In\nFigure 5a, we plot the norms of the output of the last transformer layer as training progresses and we find\nhat although training divergences can manifest after as much as even 20-30% of training progress, monitoring\nuncontrolled growth of output norms is strongly correlated with predicting future loss divergence.\n\nThe softmax operation appears in two places in transformers: the core attention mechanism and the softmax\nover the logits. As inspired by Dehghani et al. (2023) and Wortsman et al. (2023), we first deviate from\nhe Llama architecture by using query-key normalization (QK-Norm). QK-Norm directly controls the norm\ngrowth of input to the softmax by applying layer norm to the query and key vectors within the attention.\n\n", "vlm_text": "The image is a graph illustrating the growth of output norms over training steps under different conditions. The x-axis represents the \"Step\" (ranging from 0k to 30k), and the y-axis represents the \"Output Norm\" (ranging from 0.0 to 35.0). \n\nThe graph shows three lines:\n1. A yellow line titled \"w/ QK-norm and dropout,\" indicating that when both QK-norm and dropout are applied, the output norm remains relatively stable and low throughout the training process.\n2. An orange line titled \"w/o dropout,\" which shows that without dropout, the output norm begins to grow more rapidly, suggesting increased instability in the training process.\n3. A blue line titled \"w/o QK-norm or dropout,\" indicating that without both QK-norm and dropout, the output norm grows significantly, suggesting strong potential for training divergence and instability.\n\nThe caption suggests that the uncontrolled growth of output norms is an indicator of possible future training divergence.\nThe image is a line graph comparing the training loss of a model called Chameleon-7B, with and without a technique referred to as QK-Norm. \n\nThe x-axis represents the training step, ranging from 0k to 175k. The y-axis represents the training loss, with values approximately ranging from 3.4 to 3.7. \n\nThere are two lines on the graph:\n- A dark line representing training loss without QK-Norm, labeled \"w/o QK-norm\".\n- A pink line representing training loss with QK-Norm, labeled \"w/ QK-norm\".\n\nThe lines show that both approaches lead to a reduction in training loss over time, but the model employing QK-Norm consistently achieves a slightly lower training loss, indicating potentially better performance.\nThe image is a line chart showing the training loss versus the training steps for a model called Chameleon-7B. It compares two scenarios: one where dropout is used and one where it isn't. The green line represents training without dropout, while the magenta line represents training with dropout. Both lines show a decrease in training loss over time, but the line without dropout generally achieves a lower loss compared to the one with dropout.\nText-Image: The text-image data for pre-training is a combination of publicly available data sources and licensed data. The images are then resized and center cropped into    $512\\times512$   images for token iz ation. In total, we include 1.4 billion text-image pairs, which produces  1.5 trillion  text-image tokens. \nText/Image Interleaved: We procure data from publicly available web sources, not including data from Meta’s products or services, for a total of  400 billion  tokens of interleaved text and image data similar to  Laurençon et al.  ( 2023 ). We apply the same filtering for images, as was applied in  Text-To-Image . \n2.2.2 Second Stage \nIn the second stage, we lower the weight of the first stage data by   $50\\%$   and mix in higher quality datasets while maintaining a similar proportion of image text tokens. \nWe additionally include a filtered subset of the train sets from a large collection of instruction tuning sets. \n2.3 Stability \nIt was challenging to maintain stable training when scaling the  Chameleon  models above 8B parameters and 1T tokens, with instabilities often only arising very late in training. We adopted to following recipe for architecture and optimization to achieve stability. \nArchitecture Our architecture largely follows LLaMa-2 ( Touvron et al. ,  2023 ). For normalization, we continue to use RMSNorm ( Zhang and Sennrich ,  2019 ); we use the SwiGLU ( Shazeer ,  2020 ) activation function and rotary positional embeddings (RoPE) ( Su et al. ,  2021 ). \nWe found that the standard LLaMa architecture showed complex divergences due to slow norm growth in the mid-to-late stages of training. We narrowed down the cause of the divergence to the softmax operation being problematic when training with multiple modalities of significantly varying entropy due to the translation invariant property of softmax (i.e.,    $s o f t m a x(z)=s o f t m a x(z+c))$  ). Because we share all weights of the model across modalities, each modality will try to “compete” with the other by increasing its norms slightly; while not problematic at the beginning of training, it manifests in divergences once we get outside the effective representation range of bf16 (In Figure  6b , we show that ablations without image generation did not diverge). In a unimodal setting, this problem has also been named the logit drift problem ( Wortsman et al. ,  2023 ). In Figure  5a , we plot the norms of the output of the last transformer layer as training progresses and we find that although training divergences can manifest after as much as even   $20–30\\%$   of training progress, monitoring uncontrolled growth of output norms is strongly correlated with predicting future loss divergence. \nThe softmax operation appears in two places in transformers: the core attention mechanism and the softmax over the logits. As inspired by  Dehghani et al.  ( 2023 ) and  Wortsman et al.  ( 2023 ), we first deviate from the Llama architecture by using query-key normalization (QK-Norm). QK-Norm directly controls the norm growth of input to the softmax by applying layer norm to the query and key vectors within the attention. "}
{"page": 6, "image_path": "doc_images/2405.09818v1_6.jpg", "ocr_text": "2\nS\n\n— 7b —— 7B wio image generation — w/o norm reordering\n— w/ norm reordering\n\n— 34b\n\nw\n5\n\n&\n\nTraining Loss\n\nS\n8\n\nTraining Loss\nw\n3\nTraining Loss\n\nN\n©\n\n°\n&\n\n28\n\nOk 100k 200k 300k 400k 500k 600k Ok 50k 100k_~—-150k = 200k_~—-250k 0k 2k 4k 6k 8k 10k\nStep Step Step\n\n(a) Training Curves for 600k steps for (b) Training loss curve with image gen-  (c) For Chameleon-34B, using\n\nChameleon-7B and Chameleon-34B eration disabled does not suffer from dropout does not fix divergences,\n\nover Mixed-Modal Data. instability issues. both with and without norm-\nreordering.\n\nFigure6 Training loss curves for Chameleon models under various settings.\n\nIn Figure 5b, we show training loss curves for Chameleon-7B with and without QK-Norm, and the latter\ndiverges after approximately 20% of a training epoch.\n\nWe found that to stabilize Chameleon-7B by controlling norm growth, it was necessary to introduce dropout\nafter the attention and feed-forward layers, in addition to QK-norm (see Figure 5c). However, this recipe was\nnot enough to stabilitize, Chameleon-34B, which required an additional re-ordering of the norms. Specifically,\nwe use the strategy of normalization proposed in Liu et al. (2021), within the transformer block. The benefit\nof the Swin transformer normalization strategy is that it bounds the norm growth of the feedforward block,\nwhich can become additionally problematic given the multiplicate nature of the SwiGLU activation function.\nIf h represents the hidden vector at time-step t after self-attention is applied to input z,\n\nChameleon-34B: fh = x + attention_norm(attention(z))\noutput = h + ffn_norm(feed_forward(h))\n\nLlama2: h = x + attention(attention_norm(:))\noutput = h + feed_forward(ffm_norm(h))\n\nThere was no difference in perplexity when training a model from scratch with and without the normalization\nre-ordering until the divergence of the LLaMa-2 parameterization. Additionally, we found that this type of\nnormalization did not work well in combination with dropout and therefore, we train Chameleon-34B without\ndropout (Figure 6c). Furthermore, we retroactively found that Chameleon-7B can also be stably trained\nwithout dropout, when using norm-reordering, but QK-norm is essential in both cases. We plot training\ncurves for the first 600k steps for both Chameleon-7B and Chameleon-34B in Figure 6a.\n\nOptimization Our training process uses the AdamW optimizer (Loshchilov and Hutter, 2017), with 6) set\n0 0.9 and 82 to 0.95, with an e = 10-5. We use a linear warm-up of 4000 steps with an exponential decay\nschedule of the learning rate to 0. Additionally, we apply a weight decay of 0.1 and global gradient clipping at\na threshold of 1.0. We use a dropout of 0.1 (Srivastava et al., 2014) for Chameleon-7B for training stability,\nut not for Chameleon-34B (see Figure 5c and 6c).\n\nThe application of QK-Norm while helping the inner softmaxes within the Transformer does not solve the\nproblem of logit shift in the final softmax. Following Chowdhery et al. (2022); Wortsman et al. (2023),\nwe apply z-loss regularization. Specifically, we regularize the partition function Z of the softmax function\n\no(«)i = S where Z = 7; e*' by adding 10-° log? Z to our loss function.\n\nFor Chameleon-7B it was important to use both dropout and z-loss to achieve stability, while Chameleon-34B\nonly required z-loss (Figure 6c).\n\nChameleon-7B was trained with a global batch size of 27° (~ 8M) tokens and Chameleon-34B was trained\nwith a global batch size of 3 x 2?? (~ 12M) tokens. We do 2.1 epochs over our full training dataset for a total\n", "vlm_text": "The image is a graph plotting the training loss against the number of training steps for two models, Chameleon-7B and Chameleon-34B. The x-axis represents the number of training steps, ranging from 0k to 600k. The y-axis represents the training loss.\n\nThe training curve for Chameleon-7B is shown in yellow and starts at a higher training loss of around 3.2, gradually decreasing over the training steps. The Chameleon-34B curve is depicted in brown and starts at a lower training loss, around 3.0, also decreasing over time.\n\nBoth curves indicate a general trend of decreasing training loss as the number of training steps increases, suggesting that the models are learning and improving as training progresses. The Chameleon-34B appears to have a lower training loss throughout the training process compared to Chameleon-7B, indicating potentially better performance.\nThe image is a line graph displaying the training loss over steps for a model training process. The x-axis represents the number of steps, ranging from 0 to 250,000, while the y-axis represents the training loss, ranging from approximately 0.90 to 1.15. The graph depicts a downward trend in the training loss with some fluctuations, indicating an overall decrease in loss as the number of steps increases. The curve is labeled \"7B w/o image generation,\" suggesting it refers to a model or dataset with 7 billion parameters and image generation disabled. The caption notes that the training loss curve does not suffer from instability issues, indicating steady model training.\nThe image is a line graph showing the training loss over steps for a model called Chameleon-34B. The x-axis represents the training steps (ranging from 0 to 10k), and the y-axis represents the training loss (ranging from 3.5 to 6.0). There are two lines on the graph:\n\n1. A dark line representing 'w/o norm reordering' (without norm reordering) showing a decreasing training loss that experiences some spikes indicating divergences.\n2. A light blue line representing 'w/ norm reordering' (with norm reordering) also showing a decreasing trend but exhibiting less severe spikes compared to the 'w/o norm reordering' line.\n\nThe caption indicates that for the Chameleon-34B model, using dropout does not resolve divergences in the training loss, whether normalization reordering is applied or not.\nIn Figure  5b , we show training loss curves for  Chameleon-7B  with and without QK-Norm, and the latter diverges after approximately   $20\\%$   of a training epoch. \nWe found that to stabilize  Chameleon-7B  by controlling norm growth, it was necessary to introduce dropout after the attention and feed-forward layers, in addition to QK-norm (see Figure  5c ). However, this recipe was not enough to stab il it ize,  Chameleon-34B , which required an additional re-ordering of the norms. Specifically, we use the strategy of normalization proposed in  Liu et al.  ( 2021 ), within the transformer block. The benefit of the Swin transformer normalization strategy is that it bounds the norm growth of the feed forward block, which can become additionally problematic given the multi pli cate nature of the SwiGLU activation function. If    $h$   represents the hidden vector at time-step    $t$   after self-attention is applied to input    $x$  , \nChameleon-34B:  $\\begin{array}{r l}&{h=x+\\mathrm{Attention\\_norm}(\\mathrm{Attention}(x))}\\\\ &{\\mathrm{output}=h+\\mathrm{fln\\_norm}(\\mathrm{freed\\_forward}(h))}\\\\ &{h=x+\\mathrm{Attention}(\\mathrm{Attention\\_norm}(x))}\\\\ &{\\mathrm{output}=h+\\mathrm{freed\\_forward}(\\mathrm{fln\\_norm}(h))}\\end{array}$  \nThere was no difference in perplexity when training a model from scratch with and without the normalization re-ordering until the divergence of the LLaMa-2 parameter iz ation. Additionally, we found that this type of normalization did not work well in combination with dropout and therefore, we train  Chameleon-34B  without dropout (Figure  6c ). Furthermore, we retroactively found that  Chameleon-7B  can also be stably trained without dropout, when using norm-reordering, but QK-norm is essential in both cases. We plot training curves for the first   $600\\mathrm{k}$   steps for both  Chameleon-7B  and  Chameleon-34B  in Figure  6a . \nOptimization Our training process uses the AdamW optimizer ( Loshchilov and Hutter ,  2017 ), with    $\\beta_{1}$   set to 0.9 and    $\\beta_{2}$   to 0.95, with an    $\\epsilon=10^{-5}$  . We use a linear warm-up of 4000 steps with an exponential decay schedule of the learning rate to 0. Additionally, we apply a weight decay of 0.1 and global gradient clipping at a threshold of 1.0. We use a dropout of 0.1 ( Srivastava et al. ,  2014 ) for  Chameleon-7B  for training stability, but not for  Chameleon-34B  (see Figure  5c  and  6c ). \nThe application of QK-Norm while helping the inner softmaxes within the Transformer does not solve the problem of logit shift in the final softmax. Following  Chowdhery et al.  ( 2022 );  Wortsman et al.  ( 2023 ), we apply    $z$  -loss  regular iz ation. Specifically, we regularize the partition function    $Z$   of the softmax function  $\\textstyle\\sigma(x)_{i}={\\frac{e^{x_{i}}}{Z}}$    w ere    $\\begin{array}{r}{Z=\\sum_{i}e^{x_{i}}}\\end{array}$   by adding    $10^{-5}\\log^{2}Z$   to our loss function. \nFor  Chameleon -7B it was important to use both dropout and    $z$  -loss  to achieve stability, while  Chameleon -34B only required  z-loss  (Figure  6c ). \nChameleon-7B  was traine  a  batch size of    $2^{23}$    (  $\\sim8\\mathrm{M})$   tokens and  Chameleon-34B  was trained with a global batch size of  3  $3\\times2^{22}$   ×   (  $\\sim12\\mathrm{M}$  ∼ M) tokens. We do 2.1 epochs over our full training dataset for a total of 9.2 trillion tokens seen during training. We show the first   $600\\mathrm{k}$   steps of training (  $55\\%$   for  Chameleon-7B and   $80\\%$   for  Chameleon-34B ) in Figure  6a . "}
{"page": 7, "image_path": "doc_images/2405.09818v1_7.jpg", "ocr_text": "of 9.2 trillion tokens seen during training. We show the first 600k steps of training (55% for Chameleon-7B\nand 80% for Chameleon-34B) in Figure 6a.\n\nTable 1 Summary of core architecture and optimization decisions made in Chameleon in contrast to LLaMa-1 and\nLLaMa-2.\nModel Params Context Length GQA _ Tokens LR Epochs Dropout Zloss Qknorm\nLLaMa-1 7B 2k x 1.0T 3.0 x 1074 1.0 0.0 0.0 x\n33B 2k x 14T 15x 1074 1.0 0.0 0.0 x\nLLaMa-2 7B 4k x 2.0T 3.0 x 1074 1.0 0.0 0.0 x\n34B Ak v 2.0T 1.5 x 10-4 1.0 0.0 0.0 x\nChameleon 7B 4k x 44T  1.0x 10~* 21 0.1 10° v\n34B Ak v 4AT 1.0 x 10-4 2.1 0.0 10~° v\n\nPre-Training Hardware Our model pretraining was conducted on Meta’s Research Super Cluster (RSC) (Lee\nand Sengupta, 2022), and our alignment was done on other internal research clusters. NVIDIA A100 80\nGB GPUs power both environments. The primary distinction is the interconnect technology: RSC employs\nNVIDIA Quantum InfiniBand, whereas our research cluster utilizes Elastic Fabric. We report our GPU usage\nfor pre-training in Table 2.\n\nTable 2 Chameleon Model Pre-Training Resource Usage\n\nChameleon Concurrent GPUs GPU Hours\n\n7B 1024 856481\n34B 3072 4282407\n\n2.4 Inference\n\nTo support alignment and evaluation, both automated and human, and to demonstrate the application-\nreadiness of our approach, we augment the inference strategy with respect to interleaved generation to improve\nthroughput and reduce latency.\n\nAutoregressive, mixed-modal generation introduces unique performance-related challenges at inference time.\nThese include:\n\ne Data-dependencies per-step — given that our decoding formulation changes depending on whether the\nmodel is generating images or text at a particular step, tokens must be inspected at each step (ie.\ncopied from the GPU to the CPU in a blocking fashion) to guide control flow.\n\ne Masking for modality-constrained generation — to facilitate exclusive generation for a particular modality\n(e.g. image-only generation), tokens that do not fall in a particular modality space must be masked and\nignored when de-tokenizing.\n\ne Fixed-sized text units — unlike text-only generation, which is inherently variable-length, token-based\nimage generation produces fixed-size blocks of tokens corresponding to an image.\n\nGiven these unique challenges, we built a standalone inference pipeline based on PyTorch (Paszke et al., 2019)\nsupported with GPU kernels from xformers (Lefaudeux et al., 2022).\n\nOur inference implementation supports streaming for both text and images. When generating in a streaming\nfashion, token-dependent conditional logic is needed at each generation step. Without streaming, however,\nblocks of image tokens can be generated in a fused fashion without conditional computation. In all cases,\ntoken masking removes branching on the GPU. Even in the non-streaming setting, however, while generating\ntext, each output token must be inspected for image-start tokens to condition image-specific decoding\naugmentations.\n", "vlm_text": "\nThe table presents information about different models, namely LLama-1, LLama-2, and Chameleon, with varying parameters. The columns detail the following attributes for each model and its versions:\n\n1. **Model**: The name of the model.\n2. **Params**: The number of parameters in billions (B).\n3. **Context Length**: The context length in tokens, showing 2k or 4k.\n4. **GQA (Generalized Question Answering)**: Whether the model can perform GQA, denoted by a checkmark (✓) or a cross (×).\n5. **Tokens**: Amount of data (in trillions) used in training the model.\n6. **LR (Learning Rate)**: The learning rate used during training.\n7. **Epochs**: Number of epochs during training.\n8. **Dropout**: The dropout rate applied during training.\n9. **Zloss**: Represents zero loss tolerance threshold.\n10. **Qknorm (Query Key Normalization)**: Indicates whether query key normalization was applied, denoted by a checkmark (✓) or a cross (×). \n\nEach model (with variations in parameters like 7B or 33B/34B) is discussed with these attributes, showing differences in training regimes, capacity, and features like generalized question answering and query key normalization.\nPre-Training Hardware Our model pre training was conducted on Meta’s Research Super Cluster (RSC) ( Lee and Sengupta ,  2022 ), and our alignment was done on other internal research clusters. NVIDIA A100 80 GB GPUs power both environments. The primary distinction is the interconnect technology: RSC employs NVIDIA Quantum InfiniBand, whereas our research cluster utilizes Elastic Fabric. We report our GPU usage for pre-training in Table  2 . \nThe table displays data related to a project named \"Chameleon\" and includes the following columns:\n\n- **Concurrent GPUs**: Indicates the number of GPUs used concurrently (1024 for 7B and 3072 for 34B).\n- **GPU Hours**: Shows the total GPU hours used (856,481 for 7B and 4,282,407 for 34B).\n\n\"7B\" and \"34B\" likely refer to different configurations or model sizes within the Chameleon project.\n2.4 Inference \nTo support alignment and evaluation, both automated and human, and to demonstrate the application- readiness of our approach, we augment the inference strategy with respect to interleaved generation to improve throughput and reduce latency. \nAuto regressive, mixed-modal generation introduces unique performance-related challenges at inference time. These include: \n•  Data-dependencies per-step  — given that our decoding formulation changes depending on whether the model is generating images or text at a particular step, tokens must be inspected at each step (i.e. copied from the GPU to the CPU in a blocking fashion) to guide control flow. \n•  Masking for modality-constrained generation  — to facilitate exclusive generation for a particular modality (e.g. image-only generation), tokens that do not fall in a particular modality space must be masked and ignored when de-tokenizing. \n•  Fixed-sized text units  — unlike text-only generation, which is inherently variable-length, token-based image generation produces fixed-size blocks of tokens corresponding to an image. \nGiven these unique challenges, we built a standalone inference pipeline based on PyTorch ( Paszke et al. ,  2019 ) supported with GPU kernels from  xformers  ( Lefaudeux et al. ,  2022 ). \nOur inference implementation supports streaming for both text and images. When generating in a streaming fashion, token-dependent conditional logic is needed at each generation step. Without streaming, however, blocks of image tokens can be generated in a fused fashion without conditional computation. In all cases, token masking removes branching on the GPU. Even in the non-streaming setting, however, while generating text, each output token must be inspected for image-start tokens to condition image-specific decoding augmentations. "}
{"page": 8, "image_path": "doc_images/2405.09818v1_8.jpg", "ocr_text": "Table 3 Supervised Fine-Tuning Dataset Statistics\n\nCategory #ofSamples #ofTokens #of Images\nText 1.6M 940.0M -\nCode 14.1K 1.1M -\nChameleon-SFT Visual Chat 15.6K 19.4M 16.7K\nImage Generation 64.3K 68.0M 64.3K\nInterleaved Generation 16.9K 35.8M 30.7K\nSafety 95.3K 38.6M 1.6K\n\n3 Alignment\n\nWe follow recent work in using a light weight alignment stage based on supervised fine tuning on carefully\ncurated high quality datasets (Zhou et al., 2023). We include a range of different types of data, targeting\nboth exposing model capabilities and improving safety.\n\n3.1 Data\n\nWe separate our supervised fine-tuning (SFT) dataset into the following categories: Text, Code, Visual\nChat, Image Generation, Interleaved Text/Image Generation, and Safety. We include examples from each\ncategory from the Chameleon-SFT dataset in Figure 7.\n\nWe inherit the Text SFT dataset from LLaMa-2 (Touvron et al., 2023) and the Code SFT from CodeLLaMa\n(Roziere et al., 2023). For the Image Generation SFT dataset, we curate highly aesthetic images by applying\nand filtering each image in our licensed data, with an aesthetic classifier from Schuhmann et al. (2022). We\nfirst select images rated as at least six from the aesthetic classifier and then select the top 64K images closest\nin size and aspect ratio to 512 x 512 (the native resolution of our image tokenizer).\n\nFor both Visual Chat and Interleaved Text/Image Generation SFT data, we focused on very high-quality\ndata collection using third-party vendors following a similar strategy recommended by Touvron et al. (2023);\nZhou et al. (2023). We do not include any Meta user data. We present our dataset’s statistics in Table 3.\n\nSafety Data We include a collection of prompts that can potentially provoke the model to produce unsafe\ncontent, and match them with a refusal response (e.g. “I can’t help with that.”). These prompts cover a\nwide variety of sensitive topics, such as violence, controlled substances, privacy, and sexual content. Our\ncollection of safety tuning data includes examples from LLaMa-2-Chat (Touvron et al., 2023), synthetic\ntext-based examples generated with Rainbow Teaming (Samvelyan et al., 2024), image generation prompts\nmanually selected from Pick-A-Pic (Kirstain et al., 2023) for safety testing, examples for cyber security\nsafety (Roziere et al., 2023), as well as mixed-modal prompts collected internally through manual annotation\nand automatic expansion (Honovich et al., 2022). Collecting mixed-modal prompts is of particular importance,\nsince it addresses potential multi-modal attack vectors, which are outside the distribution of text-only and\ntext-to-image safety tuning datasets.\n\n3.2 Fine-Tuning Strategy\n\nData Balancing We found that balancing modalities within the SFT stage is important for high quality\nalignment. Specifically during the SFT stage, if there is a severe imbalance between pairings of modalities (or\nwhen a specific modality should trigger), the model learns an unconditional prior of generating that modality\nwhich can either mute or over exaggerate the generation of a single modality.\n\nOptimization Our supervised fine-tuning strategy incorporates a cosine learning rate schedule, starting at an\ninitial rate of le-5, combined with a weight decay set at 0.1. We maintain a batch size of 128, accommodating\nsequences up to 4096 tokens. During fine-tuning, each dataset instance comprises a paired prompt and its\ncorresponding answer. To improve efficiency, we pack as many prompts and answers as possible into each\n", "vlm_text": "The table presents information about different categories of data related to \"Chameleon-SFT.\" It includes the following columns:\n\n1. **Category**: Lists various data categories, namely Text, Code, Visual Chat, Image Generation, Interleaved Generation, and Safety.\n   \n2. **# of Samples**: Represents the number of samples within each data category. \n    - Text has 1.6 million samples.\n    - Code has 14.1 thousand samples.\n    - Visual Chat has 15.6 thousand samples.\n    - Image Generation has 64.3 thousand samples.\n    - Interleaved Generation has 16.9 thousand samples.\n    - Safety has 95.3 thousand samples.\n\n3. **# of Tokens**: Represents the number of tokens in each data category.\n    - Text has 940.0 million tokens.\n    - Code has 1.1 million tokens.\n    - Visual Chat has 19.4 million tokens.\n    - Image Generation has 68.0 million tokens.\n    - Interleaved Generation has 35.8 million tokens.\n    - Safety has 38.6 million tokens.\n\n4. **# of Images**: Represents the number of images in each data category, where applicable.\n    - Visual Chat has 16.7 thousand images.\n    - Image Generation has 64.3 thousand images.\n    - Interleaved Generation has 30.7 thousand images.\n    - Safety has 1.6 thousand images.\n    - Text and Code categories don't have images, indicated by a dash (\"-\").\n3 Alignment \nWe follow recent work in using a light weight alignment stage based on supervised fine tuning on carefully curated high quality datasets ( Zhou et al. ,  2023 ). We include a range of different types of data, targeting both exposing model capabilities and improving safety. \n3.1 Data \nWe separate our supervised fine-tuning (SFT) dataset into the following categories:  Text ,  Code ,  Visual Chat ,  Image Generation ,  Interleaved Text/Image Generation , and  Safety . We include examples from each category from the  Chameleon-SFT  dataset in Figure  7 . \nWe inherit the  Text  SFT dataset from LLaMa-2 ( Touvron et al. ,  2023 ) and the  Code  SFT from CodeLLaMa ( Roziere et al. ,  2023 ). For the  Image Generation  SFT dataset, we curate highly aesthetic images by applying and filtering each image in our licensed data, with an aesthetic classifier from  Schuhmann et al.  ( 2022 ). We first select images rated as at least six from the aesthetic classifier and then select the top 64K images closest in size and aspect ratio to    $512\\times512$   (the native resolution of our image tokenizer). \nFor both  Visual Chat  and  Interleaved Text/Image Generation  SFT data, we focused on very high-quality data collection using third-party vendors following a similar strategy recommended by  Touvron et al.  ( 2023 ); Zhou et al.  ( 2023 ). We do not include any Meta user data. We present our dataset’s statistics in Table  3 . \nSafety Data We include a collection of prompts that can potentially provoke the model to produce unsafe content, and match them with a refusal response (e.g. “I can’t help with that.”). These prompts cover a wide variety of sensitive topics, such as violence, controlled substances, privacy, and sexual content. Our collection of safety tuning data includes examples from LLaMa-2-Chat ( Touvron et al. ,  2023 ), synthetic text-based examples generated with Rainbow Teaming ( Samvelyan et al. ,  2024 ), image generation prompts manually selected from Pick-A-Pic ( Kirstain et al. ,  2023 ) for safety testing, examples for cyber security safety ( Roziere et al. ,  2023 ), as well as mixed-modal prompts collected internally through manual annotation and automatic expansion ( Honovich et al. ,  2022 ). Collecting mixed-modal prompts is of particular importance, since it addresses potential multi-modal attack vectors, which are outside the distribution of text-only and text-to-image safety tuning datasets. \n3.2 Fine-Tuning Strategy \nData Balancing We found that balancing modalities within the SFT stage is important for high quality alignment. Specifically during the SFT stage, if there is a severe imbalance between pairings of modalities (or when a specific modality should trigger), the model learns an unconditional prior of generating that modality which can either mute or over exaggerate the generation of a single modality. \nOptimization Our supervised fine-tuning strategy incorporates a cosine learning rate schedule, starting at an initial rate of 1e-5, combined with a weight decay set at 0.1. We maintain a batch size of 128, accommodating sequences up to 4096 tokens. During fine-tuning, each dataset instance comprises a paired prompt and its corresponding answer. To improve efficiency, we pack as many prompts and answers as possible into each Prompt:  Can you write a poem using rhyming couplets? Response: Sure, here’s a short poem using rhyming couplets: "}
{"page": 9, "image_path": "doc_images/2405.09818v1_9.jpg", "ocr_text": "Prompt: Can you write a poem using\nrhyming couplets?\n\nResponse: Sure, here’s a short poem\nusing rhyming couplets:\n\nThe sun sets in the sky,\nA beautiful sight to the eye. Prompt: Renaissance square of\nThe stars come out to play, Stepanakert\nA magical sight to behold. Response:\nPrompt: Given a list of integers,\nThe wind whispers through the find the first number that ap-\ntrees, pears at least twice.\nA gentle breeze that brings ease. Response:\nThe birds sing their sweet melodies, def find_duplicate(my _ list):\nA soothing sound that calms the soul. seen = set()\n\nThe world is full of wonder,\nA place of beauty and thunder.\nSo let us cherish every moment,\n\nAnd bask in the beauty all around us. return None\n\nfor num in my _ list:\n\nif num in seen:\nreturn num\nseen.add(num)\n\n(a) Text\n\nPrompt: Which elements contribute to this flower\narrangement’s distinctiveness, and what sets its sur-\nroundings apart?\n\nResponse: The image features an autumnal-themed\nfloral arrangement in a classically designed vase.\nThe arrangement stands out with its striking com-\nbination of red maple leaves, purple flowers, and\npink-red roses. The vase’s rosettes give it an antique\ndesign. In the background, blurry green vegetation\nis visible, creating a contrast to the autumnal theme\nof the bouquet. The red maple leaves, both in the\narrangement and scattered around the vase, evoke\nautumn, while the green vegetation suggests a more\nsummer-like setting. The outdoor setting is bright\nand sunny, adding to the contrast between the ar-\nrangement and its settings.\n\n(b) Code (c) Image Generation\n\nGyumri is an urban municipal community and the\nsecond-largest city in Armenia, serving as the ad-\nministrative center of Shirak Province in the north-\nwestern part of the country.\nco\n\nArchaeological excavations conducted throughout\nthe Soviet period have shown that the area of\nmodern-day Gyumri has been populated since at\nleast the third millennium BC,\n\n(d) Visual Chat\n\n(e) Interleaved Text /Image Generation\n\nFigure 7 Example alignment data for different categories.\n\nsequence, inserting a distinct token to delineate the end of a prompt and the beginning of an answer. We\nuse an autoregressive training objective, selectively masking the loss for the prompt tokens. This targeted\napproach allows us to optimize the model exclusively based on the answer tokens, which provides slight gains\noverall. We also apply a dropout of 0.05. Additionally, we maintain the same zloss that was used during\n\n10\n\n", "vlm_text": "\nThe sun sets in the sky, A beautiful sight to the eye. The stars come out to play, A magical sight to behold. \nPrompt:  Renaissance square of Stepanakert Response: \nPrompt:  Given a list of integers, find the first number that ap- pears at least twice. Response: def find duplicate(my_list): seen = set() for num in my_list: if num in seen: return num seen.add(num) return None \nThe image depicts a dramatic and moody cityscape under a cloudy sky. Central to the image is a grand building with a prominent triangular roof, possibly a government or important civic structure due to its architectural prominence. In front of the structure are a few people walking on a crosswalk. The scene conveys a sense of grandeur and foreboding due to the dramatic clouds overhead, adding depth and atmosphere to the overall image.\nThe wind whispers through the trees, A gentle breeze that brings ease. The birds sing their sweet melodies, A soothing sound that calms the soul. \nThe world is full of wonder, A place of beauty and thunder. So let us cherish every moment, And bask in the beauty all around us. \n(b)  Code \n(a)  Text \nThe image shows a small ceramic vase containing vibrant flowers and red leaves. The vase is intricate, with decorative spirals and vertical lines. The background is an outdoor scene, possibly a garden, with blurred greenery.\nGyumri is an urban municipal community and the second-largest city in Armenia, serving as the ad- mini strat ive center of Shirak Province in the north- western part of the country. \nThe image shows a cobblestone street lined with historic buildings on both sides. The buildings have an architectural style that suggests they may be from a past era, possibly with brick or stone facades. The street seems quiet and picturesque, and the sky above is blue with some scattered clouds. The overall atmosphere is calm and somewhat nostalgic.\nPrompt:  Which elements contribute to this flower arrangement’s distinctiveness, and what sets its sur- roundings apart? \nResponse:  The image features an autumnal-themed floral arrangement in a classically designed vase. The arrangement stands out with its striking com- bination of red maple leaves, purple flowers, and pink-red roses. The vase’s rosettes give it an antique design. In the background, blurry green vegetation is visible, creating a contrast to the autumnal theme of the bouquet. The red maple leaves, both in the arrangement and scattered around the vase, evoke autumn, while the green vegetation suggests a more summer-like setting. The outdoor setting is bright and sunny, adding to the contrast between the ar- rangement and its settings. \nArchaeological excavations conducted throughout the Soviet period have shown that the area of modern-day Gyumri has been populated since at least the third millennium BC. \nThe image shows ancient stone ruins in a grassy, open landscape. The stone walls form a series of low, rectangular enclosures, with grass growing on and around them. The area appears to be a historical or archaeological site. In the background, there are open fields under a partly cloudy sky.\n(d)  Visual Chat \nFigure 7  Example alignment data for different categories. \nsequence, inserting a distinct token to delineate the end of a prompt and the beginning of an answer. We use an auto regressive training objective, selectively masking the loss for the prompt tokens. This targeted approach allows us to optimize the model exclusively based on the answer tokens, which provides slight gains overall. We also apply a dropout of 0.05. Additionally, we maintain the same zloss that was used during "}
{"page": 10, "image_path": "doc_images/2405.09818v1_10.jpg", "ocr_text": "Advice: 10.2%\n\nWhat does a meningitis rash look\nlike? What are the other\nsymptoms I should be on the\nlookout for?\n\nHow-to: 12.5%\n\nHow do I properly clean my TV\n\nscreen? I used Windex and now\nthere are towel fibers and wipe\nmarks all over. Show me some\n\nreference photos.\n\nBrainstorming: 18.6% Comparison: 9.6%\n\nPlease tell me what the difference between\nthese two creatures is, and show me some\nmore examples. <img1> <img2>\n\nShow me a Middle Eastern alternative to\nthese dishes. <img1> <img2>\n\nArticle: 3.1%\nReport: 5.4%\n\nWho designed the church in the image below,\n\nWrite me an introduction to a story about\nknick-knacks, and finish the story by\nshifting the focus with an image.\n\nExplanation: 14.4%\n\nI've been studying\n\nclassical French art,\n\nand my favorite\n\nso far is his painting\n\nseen here: <img>\n\nCould you please ¥\n\ngive me a few images of other\ncontemporary artworks that have\nthis same aesthetic?\n\nHypothetical: 5.6%\n\nWhat would the modern-day\nvehicle look like if oil had never\nbeen discovered?\n\nand what's the name of the\nChurch? <img> Can you\nplease provide me with\nadditional photos of famous\nlandmarks designed\n\nStory: 3.9%\n\nCan you create and illustrate a short story\nfor children about an octopus that can't\nstop eating pizza?\n\nby the same architect?\nOther: 5.2%\n\nCreate a decal for my truck that features\nrunning horses as well as the TRD insignia. Use\nblack to gray gradients.\n\nIdentification: 9.3 %\n\nIs the below image a\nShetland Pony? If\nnot, what is it, and\ncan you show mea\nShetland Pony?\n<img>\n\nReasoning: 2.1%\n\nWhat is typically found at a construction site?\nShow me a construction site that has a crane.\n\nFigure 8 Task categories and examples of prompts. Image attributions: Seguin (2010); Agriflanders (2009); Tuszynski\n(2015); Sokolov (2022).\n\npre-training. During supervised fine-tuning, images in the prompt are resized with border padding to ensure\nthat all the information is available in the image, whereas images in the answer are center-cropped to ensure\nvisually good image generation quality.\n\n4 Human Evaluations and Safety Testing\n\nChameleon has significant new mixed modal understan\nwith existing benchmarks. In this section, we detail how\nlanguage models’ responses to a set of diverse prompts\n\ning and generation abilities that cannot be measured\nwe conduct human evaluations on large multi-modal\nthat regular users may ask daily. We first introduce\n\nhow we collect the prompts and then describe our baselines and evaluation methods, along with the evaluation\n\nresults and analysis. A safety study is also included in\n\nhis section.\n\n4.1 Prompts for Evaluation\n\nWe work with a third-party crowdsourcing vendor to collect a set of diverse and natural prompts from human\nannotators. Specifically, we ask annotators to creatively think about what they want a multi-modal model\nto generate for different real-life scenarios. For example, for the scenario of “imagine you are in a kitchen”,\nannotators may come up with prompts like “How to cook pasta?” or “How should I design the layout of my\nisland? Show me some examples.” The prompts can be text-only or text with some images, and the expected\nresponses should be mixed-modal, containing both text and images.\n\nAfter collecting an initial set of prompts, we ask three random annotators to evaluate whether the prompts\nare clear and whether they expect the responses to contain images. We use a majority vote to filter unclear\nprompts and prompts that don’t expect mixed-modal responses. In the end, our final evaluation set contains\n1,048 prompts: 441 (42.1%) are mixed-modal (i-e., containing both text and images), and the remaining 607\n(57.9%) are text-only.\n\nTo better understand the tasks users would like a multi-modal AI system to fulfill, we manually examine\n\n11\n", "vlm_text": "The image is a chart that categorizes different types of tasks along with corresponding examples of prompts. Each category is given a percentage, presumably indicating its frequency or importance. \n\nThe categories and examples are as follows:\n\n1. **Advice: 10.2%**\n   - Example: Asking about the appearance and other symptoms of a meningitis rash.\n\n2. **How-to: 12.5%**\n   - Example: Asking for help in cleaning a TV screen properly, with reference photos.\n\n3. **Explanation: 14.4%**\n   - Example: Inquiry about classical French art, with a request for images of contemporary artwork with a similar aesthetic.\n\n4. **Hypothetical: 5.6%**\n   - Example: Speculating what modern-day vehicles would look like without the discovery of oil.\n\n5. **Brainstorming: 18.6%**\n   - Example: Requesting Middle Eastern alternatives to certain dishes, with images.\n\n6. **Article: 3.1%**\n   - Example: Writing an introduction to a story about knick-knacks, and finishing it with an image.\n\n7. **Story: 3.9%**\n   - Example: Creating and illustrating a short story about an octopus that can't stop eating pizza.\n\n8. **Identification: 9.3%**\n   - Example: Confirming if an image shows a Shetland Pony and asking for examples.\n\n9. **Comparison: 9.6%**\n   - Example: Describing differences between two creatures with requested examples.\n\n10. **Report: 5.4%**\n    - Example: Asking for the designer and name of a church in an image, and requesting additional images of landmarks by the same architect.\n\n11. **Other: 5.2%**\n    - Example: Creating a truck decal with running horses and the TRD insignia.\n\n12. **Reasoning: 2.1%**\n    - Example: Asking what is typically found at a construction site and requesting an image with a crane. \n\nThe image also includes some visual elements, such as food items, animals, a church, and a pony, to accompany certain example prompts.\npre-training. During supervised fine-tuning, images in the prompt are resized with border padding to ensure that all the information is available in the image, whereas images in the answer are center-cropped to ensure visually good image generation quality. \n4 Human Evaluations and Safety Testing \nChameleon  has significant new mixed modal understanding and generation abilities that cannot be measured with existing benchmarks. In this section, we detail how we conduct human evaluations on large multi-modal language models’  responses  to a set of diverse  prompts  that regular users may ask daily. We first introduce how we collect the prompts and then describe our baselines and evaluation methods, along with the evaluation results and analysis. A safety study is also included in this section. \n4.1 Prompts for Evaluation \nWe work with a third-party crowd sourcing vendor to collect a set of diverse and natural prompts from human annotators. Specifically, we ask annotators to creatively think about what they want a multi-modal model to generate for different real-life scenarios. For example, for the scenario of “imagine you are in a kitchen”, annotators may come up with prompts like “How to cook pasta?” or “How should I design the layout of my island? Show me some examples.” The prompts can be text-only or text with some images, and the expected responses should be mixed-modal, containing both text and images. \nAfter collecting an initial set of prompts, we ask three random annotators to evaluate whether the prompts are clear and whether they expect the responses to contain images. We use a majority vote to filter unclear prompts and prompts that don’t expect mixed-modal responses. In the end, our final evaluation set contains 1,048 prompts: 441   $(42.1\\%)$  ) are mixed-modal (i.e., containing both text and images), and the remaining 607 (57.9%) are text-only. \nTo better understand the tasks users would like a multi-modal AI system to fulfill, we manually examine "}
{"page": 11, "image_path": "doc_images/2405.09818v1_11.jpg", "ocr_text": "Wins SS Ties Sm\" Loses\n\nModel\n\nGenie Geminis “ Ho\nGemini+\n\nGPT-4V+\nGemini\n\nGPT-4V\n\nGPT-4V+ 35.8\nGemini =e me 83\nx» oPray “e mee\n\ney\n8\n\nw\niy\n\nPercentage (%)\nwos\n8 6\n\nFulfills Partially fulfills Does not fulfill 0 20 40 60 80 100\nTask Fulfillment Rate Percent (%)\n(a) The prompt task fulfillment rates. (b) Chameleon vs. the baselines: Gemini+, GPT-4V-+,\n\nGemini, GPT-4V.\n\nFigure 9 Performance of Chameleon vs baselines, on mixed-modal understanding and generation on a set of diverse\nand natural prompts from human annotators.\n\nthe prompts and classify them into 12 categories. The description of these task categories', as well as their\nexample prompts, can be found in Figure 8.\n\n4.2 Baselines and Evaluations\n\nWe compare Chameleon 34B with OpenAI GPT-4V and Google Gemini Pro by calling their APIs. While these\nmodels can take mixed-modal prompts as input, their responses are text-only. We create additional baselines\nby augmenting GPT-4V and Gemini responses with images to have even stronger baselines. Specifically, we\ninstruct these models to generate image captions by adding the following sentence at the end of each original\ninput prompt: “If the question requires an image to be generated, then generate an image caption instead\nand enclose the caption in a pair of (caption) (/caption) tags.” We then use OpenAI DALL-E 3 to generate\nimages conditioned on these captions and replace the captions in the original responses with those generated\nimages. We denote the enhanced responses as GPT-4V+ and Gemini+ in this section. Working with the same\nhird-party crowdsourcing vendor, we conduct two types of evaluations to measure the model performance:\nabsolute and relative.\n\n4.2.1 Absolute Evaluation\n\nFor absolute evaluations, the output of each model is judged separately by asking three different annotators\na set of questions regarding the relevance and quality of the responses. Below, we give detailed results and\nanalysis on the most critical question, whether the response fulfills the task described in the prompt.\n\nOn task fulfillment, we ask annotators whether the response fulfills, partially fulfills, or does not fulfill the\nask described in the prompt. As shown in Figure 9a, much more of Chameleon’s responses are considered\n‘o have completely fulfilled the tasks: 55.2% for Chameleon vs. 37.6% of Gemini+ and 44.7% of GPT-4V+.\nWhen judging the original responses of Gemini and GPT-4V, the annotators consider much fewer prompts to\ne fully fulfilled: Gemini completely fulfills 17.6% of the tasks and GPT-4V 23.1%. We suspect that because\nall the prompts expect mixed-modal output, the text-only responses from Gemini and GPT-4V might be\nviewed as only partially completing the tasks by the annotators.\n\nThe task fulfillment rates in each category and in each input modality can be found in Appendix B. The\nask categories that Chameleon performs well include Brainstorming, Comparison, and Hypothetical, and the\n\n1While not instructed specifically, certain image understanding tasks that require identifying the text in an image, such as\nOCR (Optical character recognition), do not appear in our evaluation set of prompts.\n\n12\n", "vlm_text": "The image contains two bar graphs comparing the performance of various models (Chameleon, Gemini+, GPT-4V+, Gemini, and GPT-4V) in task fulfillment and head-to-head comparison scenarios.\n\n1. **Graph (a): The prompt task fulfillment rates.** \n   - This graph displays the percentage of tasks that each model fulfills, partially fulfills, or does not fulfill. \n   - Chameleon has the highest percentage of tasks fully fulfilled, followed by Gemini+. \n   - GPT-4V+ and GPT-4V are more evenly distributed between full and partial fulfillment, while Gemini has relatively high partially fulfilled tasks but lower full fulfillment rates.\n\n2. **Graph (b): Chameleon vs. the baselines.** \n   - This graph shows the head-to-head comparison of Chameleon's performance against other models: Gemini+, GPT-4V+, Gemini, and GPT-4V. \n   - It presents the percentage of times Chameleon wins, ties, or loses against each model.\n   - Chameleon has the highest win percentage against Gemini+ (41.5%) and maintains competitive performance with the other models. It tends to win more often than lose in most comparisons except against GPT-4V+, where it loses 32.6% of the time.\n\nOverall, the graphs illustrate Chameleon's strengths in fulfilling tasks compared to the baseline models, although its performance varies depending on the specific model it is compared against.\nthe prompts and classify them into 12 categories. The description of these task categories 1 , as well as their example prompts, can be found in  Figure 8 . \n4.2 Baselines and Evaluations \nWe compare  Chameleon  34B with OpenAI GPT-4V and Google Gemini Pro by calling their APIs. While these models can take mixed-modal prompts as input, their responses are text-only. We create additional baselines by augmenting GPT-4V and Gemini responses with images to have even stronger baselines. Specifically, we instruct these models to generate image captions by adding the following sentence at the end of each original input prompt: “If the question requires an image to be generated, then generate an image caption instead and enclose the caption in a pair of ⟨caption⟩⟨/caption⟩tags.” We then use OpenAI DALL-E 3 to generateimages conditioned on these captions and replace the captions in the original responses with those generated images. We denote the enhanced responses as GPT-4V+ and Gemini+ in this section. Working with the same third-party crowd sourcing vendor, we conduct two types of evaluations to measure the model performance: absolute  and  relative . \n4.2.1 Absolute Evaluation \nFor absolute evaluations, the output of each model is judged separately by asking three different annotators a set of questions regarding the relevance and quality of the responses. Below, we give detailed results and analysis on the most critical question,  whether the response fulfills the task described in the prompt . \nOn task fulfillment, we ask annotators whether the response  fulfills ,  partially fulfills , or  does not fulfill  the task described in the prompt. As shown in  Figure 9a , much more of  Chameleon ’s responses are considered to have completely fulfilled the tasks:   $55.2\\%$   for  Chameleon  vs.   $37.6\\%$   of Gemini  $^+$   and   $44.7\\%$   of GPT-4V+. When judging the original responses of Gemini and GPT-4V, the annotators consider much fewer prompts to be fully fulfilled: Gemini completely fulfills   $17.6\\%$   of the tasks and GPT-4V  $23.1\\%$  . We suspect that because all the prompts expect mixed-modal output, the text-only responses from Gemini and GPT-4V might be viewed as only partially completing the tasks by the annotators. \nThe task fulfillment rates in each category and in each input modality can be found in  Appendix B . The task categories that  Chameleon  performs well include  Brainstorming ,  Comparison , and  Hypothetical , and the categories  Chameleon  needs to improve include  Identification  and  Reasoning . On the other hand, we don’t see that the model performance differs a lot when comparing mixed-modality and text-only prompts, although Chameleon  seems to perform slightly better on text-only prompts, while Gemini  $^{+}$   and GPT-4V+ are slightly better on mixed-modal ones.  Figure 2  shows an example of  Chameleon ’s response to a brainstorming prompt. "}
{"page": 12, "image_path": "doc_images/2405.09818v1_12.jpg", "ocr_text": "categories Chameleon needs to improve include Identification and Reasoning. On the other hand, we don’t\nsee that the model performance differs a lot when comparing mixed-modality and text-only prompts, although\nChameleon seems to perform slightly better on text-only prompts, while Gemini+ and GPT-4V+ are slightly\nbetter on mixed-modal ones. Figure 2 shows an example of Chameleon’s response to a brainstorming prompt.\n\n4.2.2 Relative Evaluation\n\nFor relative evaluations, we directly compare Chameleon with each baseline model by presenting their\nresponses to the same prompt in random order and asking human annotators which response they prefer. The\noptions include the first response, the second response, and about the same. Figure 9b shows Chameleon’s\nwin rates” over the baselines. Compared with Gemini+, Chameleon’s responses are better in 41.5% of the\ncases, 34.5% are tie, and 24.0% are inferior. Annotators also think that Chameleon’s responses are slightly\nmore often better than GPT-4V-+, with 35.8% win, 31.6% tie, and 32.6% loss. Overall, Chameleon has win\nrates of 60.4% and 51.6% over Gemini+ and GPT-4V+, respectively. When compared with the original\nresponses from Gemini without the augmented images, Chameleon’s responses are considered better in 53.5%\nof the cases, 31.2% are tied, and 15.3% are inferior. Chameleon’s responses are also considered better than\nGPT-4V more frequently, with 46.0% win, 31.4% tie, and 22.6% loss. Chameleon’s win rates over Gemini\nand GPT-4V are 69.1% and 61.7%, respectively.\n\n4.3 Inter-annotator Agreement\n\nEvery question in our evaluation is answered by three different human annotators, and we take the majority\nvotes as the final answer. To understand the quality of the human annotators and whether the questions we\nasked are reasonably designed, we examine the level of agreement between different annotators.\n\nThe levels of agreement on each question in the absolute evaluation are shown in Figure 10.\n\nContaining images\n\nImage quality\n\nImage relevance\n\nLanguage quality\n\nObjectionable content\n\nRelevance\n\nAgreement\nTask fulfillment\n\nmae All\n=a Two\nAccuracy Mm None\n8 NY NY NS) wy) NY NS)\nSs Sy Ss cS) Ss oS\nSS SF LS Ls\nCount\n\nFigure10 The inter-annotator agreement on the questions in the absolute evaluation.\n\nFor questions about simple, objective properties of the responses, we very rarely see three annotators disagree\nwith each other. For example, annotators have unanimous judgments on whether the model responses contain\nobjectionable content (e.g., hate speech); in this case, all models produce safe responses. For some questions,\nsuch as whether the response fulfills the task or whether the model interprets the prompt correctly, when one\n\n?The win rate is calculated by adding 1 point for a win and 0.5 points for a tie.\n\n13\n", "vlm_text": "\n4.2.2 Relative Evaluation \nFor relative evaluations, we directly compare  Chameleon  with each baseline model by presenting their responses to the same prompt in random order and asking human annotators which response they prefer. The options include the  first  response, the  second  response, and  about the same .  Figure 9b  shows  Chameleon ’s win rates 2   over the baselines. Compared with Gemini  $^{.+}$  ,  Chameleon ’s responses are better in  $41.5\\%$   of the cases,  $34.5\\%$   are tie, and   $24.0\\%$   are inferior. Annotators also think that  Chameleon ’s responses are slightly more often better than GPT-4V+, with  $35.8\\%$   win,   $31.6\\%$   tie, and   $32.6\\%$   loss. Overall,  Chameleon  has win rates of   $60.4\\%$   and   $51.6\\%$   over Gemini  $^+$   and GPT-  $^{4\\mathrm{V}+}$  , respectively. When compared with the original responses from Gemini without the augmented images,  Chameleon ’s responses are considered better in   $53.5\\%$  of the cases,   $31.2\\%$   are tied, and   $15.3\\%$   are inferior.  Chameleon ’s responses are also considered better than GPT-4V more frequently, with   $46.0\\%$   win,   $31.4\\%$   tie, and   $22.6\\%$   loss.  Chameleon ’s win rates over Gemini and GPT-4V are  $69.1\\%$   and   $61.7\\%$  , respectively. \n4.3 Inter-annotator Agreement \nEvery question in our evaluation is answered by three different human annotators, and we take the majority votes as the final answer. To understand the quality of the human annotators and whether the questions we asked are reasonably designed, we examine the level of agreement between different annotators. \nThe levels of agreement on each question in the absolute evaluation are shown in  Figure 10 . \nThe image is a horizontal bar chart titled \"Figure 10: The inter-annotator agreement on the questions in the absolute evaluation.\" It has different categories on the y-axis and the count on the x-axis. The categories include:\n\n- Containing images\n- Image quality\n- Image relevance\n- Language quality\n- Objectionable content\n- Relevance\n- Task fulfillment\n- Accuracy\n\nThe chart shows three types of agreement: \"All\" (in blue), \"Two\" (in orange), and \"None\" (in green). Each category has bars representing the level of agreement. \"All\" generally has the highest counts across almost all categories.\nFor questions about simple, objective properties of the responses, we very rarely see three annotators disagree with each other. For example, annotators have unanimous judgments on whether the model responses contain objectionable content (e.g., hate speech); in this case, all models produce safe responses. For some questions, such as whether the response fulfills the task or whether the model interprets the prompt correctly, when one annotator’s judgment differs from the other two’s, the decision is usually still close (e.g.,  fulfills  vs.  partially fulfills ) rather than opposite (e.g.,  fulfills  vs.  does not fulfill ). "}
{"page": 13, "image_path": "doc_images/2405.09818v1_13.jpg", "ocr_text": "annotator’s judgment differs from the other two’s, the decision is usually still close (e.g., fulfills vs. partially\nfulfills) rather than opposite (e.g., fulfills vs. does not fulfill).°\n\nTable 4 The inter-annotator agreement on relative evaluations.\n\nAll 3 annotators agree 2 of 3 annotators agree No Agreement\n\nChameleon vs. Gemini+ 331 (31.5%) 609 (58.1%) 108 (10.3%)\nChameleon vs. GPT-4V+ 371 (35.4%) 579 (55.2%) 98 (9.3%)\n\nChameleon vs. Gemini 317 (30.2%) 621 (59.3%) 110 (10.5%)\nChameleon vs. GPT-4V 300 (28.6%) 611 (58.3%) 137 (13.1%)\n\nFor the relative evaluation, Table 4 shows the numbers of cases where all three annotators agree, two annotators\nagree, and there is no agreement. For each model pair, we have a bit higher than 10% of the cases where there\nis no agreement among the three annotators (considered as a tie in our evaluation.) On about 28% to 35% of\nthe pairs, all annotators have unanimous judgments, and in about 55% to 60% of the pairs, one annotator\ndiffers from other two. This may be interpreted as Chameleon performing similarly to other baselines in\nmany cases, making the relative evaluation challenging.*\n\n4.4 Safety Testing\n\nWe crowdsource prompts that provoke the model to create unsafe content in predefined categories such as\nself-harm, violence and hate, and criminal planning. These prompts cover both text and mixed-modal inputs,\nas well as intents to produce unsafe text, images, or mixed-modal outputs. We generate the model’s response\nto each prompt, and ask annotators to label whether the response is safe or unsafe with respect to each\ncategory’s definition of safety; an unsure option is also provided for borderline responses. Table 5 shows\nthat an overwhelming majority of Chameleon’s responses are considered safe, with only 78 (0.39%) unsafe\nresponses for the 7B model and 19 (0.095%) for the 30B model.\n\nWe also evaluate the model’s ability to withstand adversarial prompting in an interactive session. For\nthat purpose, an internal red team probed the 30B model over 445 prompt-response interactions, including\nmulti-turn interactions. Table 5 shows that of those responses, 7 (1.6%) were considered unsafe and 20 (4.5%)\nwere labeled as unsure. While further safety tuning using RLHF/RLAIF has been shown to further harden\nthe model against jailbreaking and intentional malicious attacks, these results demonstrate that our current\n\nsafety tuning approach provides significant protection for reasonable, benign usage of this research artifact.\n\n4.5 Discussion\n\nCompared to Gemini and GPT-4V, Chameleon is very competitive when handling prompts that expect\ninterleaving, mixed-modal responses. The images generated by Chameleon are usually relevant to the context,\n\nmaking the documents with interleaving text and images very appealing to users. However, readers should\nbe aware of the limitations of human evaluation. First, the prompts used in the evaluation came from\ncrowdsourcing instead of real users who interact with a model. While we certainly have a diverse set of\nprompts, the coverage may still be limited, given the size of the dataset. Second, partially because our\nprompts focus on the mixed-modal output, certain visual understanding tasks, such as OCR or Infographics\n\n(i.e., interpreting a given chart or plot), are naturally excluded from our evaluation. Finally, at this moment,\nthe APIs of existing multi-modal LLMs provide only textual responses. While we strengthen the baselines by\naugmenting their output with separately generated images, it is still preferred if we can compare Chameleon\nto other native mixed-modal models.\n\n3For the question of task fulfillment, the inter-rater reliability derived by Krippendorff’s Alpha (Krippendorff, 2018; Marzi\net al., 2024) is 0.338; the 95% confidence interval is [0.319, 0.356], based on bootstrap sampling of 1,000 iterations.\n\n4When comparing Chameleon with Gemini+ and GPT-4V+, the Krippendorff’s Alpha values are 0.337 [0.293, 0.378] and\n0.396 [0.353, 0.435], respectively.\n\n14\n", "vlm_text": "\nThe table provides statistics about agreement levels among annotators for different comparisons involving Chameleon, Gemini, and GPT-4 models (both standard and enhanced versions denoted by a \"+\"). The table has three main columns indicating different levels of annotator agreement: \"All 3 annotators agree,\" \"2 of 3 annotators agree,\" and \"No Agreement.\" Each row represents a comparison between Chameleon and another model, while the figures in the table represent the number and percentage of instances where the annotators reached the respective levels of agreement.\n\n- **Chameleon vs. Gemini+**:\n  - All 3 annotators agree: 331 instances (31.5%)\n  - 2 of 3 annotators agree: 609 instances (58.1%)\n  - No Agreement: 108 instances (10.3%)\n\n- **Chameleon vs. GPT-4V+**:\n  - All 3 annotators agree: 371 instances (35.4%)\n  - 2 of 3 annotators agree: 579 instances (55.2%)\n  - No Agreement: 98 instances (9.3%)\n\n- **Chameleon vs. Gemini**:\n  - All 3 annotators agree: 317 instances (30.2%)\n  - 2 of 3 annotators agree: 621 instances (59.3%)\n  - No Agreement: 110 instances (10.5%)\n\n- **Chameleon vs. GPT-4V**:\n  - All 3 annotators agree: 300 instances (28.6%)\n  - 2 of 3 annotators agree: 611 instances (58.3%)\n  - No Agreement: 137 instances (13.1%)\nFor the relative evaluation,  Table 4  shows the numbers of cases where all three annotators agree, two annotators agree, and there is no agreement. For each model pair, we have a bit higher than   $10\\%$   of the cases where there is no agreement among the three annotators (considered as a tie in our evaluation.) On about   $28\\%$   to   $35\\%$   of the pairs, all annotators have unanimous judgments, and in about   $55\\%$   to  $60\\%$   of the pairs, one annotator differs from other two. This may be interpreted as  Chameleon  performing similarly to other baselines in many cases, making the relative evaluation challenging. \n4.4 Safety Testing \nWe crowd source prompts that provoke the model to create unsafe content in predefined categories such as self-harm, violence and hate, and criminal planning. These prompts cover both text and mixed-modal inputs, as well as intents to produce unsafe text, images, or mixed-modal outputs. We generate the model’s response to each prompt, and ask annotators to label whether the response is  safe  or  unsafe  with respect to each category’s definition of safety; an  unsure  option is also provided for borderline responses. Table  5  shows that an overwhelming majority of  Chameleon ’s responses are considered safe, with only 78 (0.39%) unsafe responses for the 7B model and 19   $(0.095\\%$  ) for the 30B model. \nWe also evaluate the model’s ability to withstand adversarial prompting in an interactive session. For that purpose, an internal red team probed the 30B model over 445 prompt-response interactions, including multi-turn interactions. Table  5  shows that of those responses, 7 (1.6%) were considered unsafe and 20 (4.5%) were labeled as unsure. While further safety tuning using RLHF/RLAIF has been shown to further harden the model against jail breaking and intentional malicious attacks, these results demonstrate that our current safety tuning approach provides significant protection for reasonable, benign usage of this research artifact. \n4.5 Discussion \nCompared to Gemini and GPT-4V,  Chameleon  is very competitive when handling prompts that expect interleaving, mixed-modal responses. The images generated by  Chameleon  are usually relevant to the context, making the documents with interleaving text and images very appealing to users. However, readers should be aware of the limitations of human evaluation. First, the prompts used in the evaluation came from crowd sourcing instead of real users who interact with a model. While we certainly have a diverse set of prompts, the coverage may still be limited, given the size of the dataset. Second, partially because our prompts focus on the mixed-modal output, certain visual understanding tasks, such as OCR or Info graphics (i.e., interpreting a given chart or plot), are naturally excluded from our evaluation. Finally, at this moment, the APIs of existing multi-modal LLMs provide only textual responses. While we strengthen the baselines by augmenting their output with separately generated images, it is still preferred if we can compare  Chameleon to other native mixed-modal models. "}
{"page": 14, "image_path": "doc_images/2405.09818v1_14.jpg", "ocr_text": "Table 5 Safety testing on 20,000 crowdsourced prompts and 445 red team interactions provoking the model to produce\nunsafe content.\n\nDataset Params Safe Unsafe Unsure\n\nCrowdsourced 7B 99.2% 0.4% 0.4%\n. . 34B 99.7% 0.1% 0.2%\n\nRed Team 34B 93.9% 1.6% 4.5%\n\n5 Benchmark Evaluations\n\nGiven the general capabilities of Chameleon, there is not a single model that we can directly evaluate against;\ntherefore, we evaluate against the best models in every category within our capabilities.\n\n5.1 Text\n\nWe evaluate the general text-only capabilities of our pre-trained (not SFT’d) model against other state-of-the-\nart text-only large language models. We follow the evaluation protocol outlined by Touvron et al. (2023).\nSpecifically we evaluate all models, using an in-house evaluation platform on the areas of commonsense\nreasoning, reading comprehension, math problems, and world knowledge. We report our results in Table 6.\n\nTable 6 Comparison of overall performance on collective academic benchmarks against open-source foundational models.\n* Evaluated using our framework/using API. For GSM8k/MATH, we report maj@1 unless mentioned otherwise.\n“™ From Gemini et al. (2023).\n\nChameleon Llama-2 Mistral Gemini GPT-\nPro 4\n7B 34B 7B 34B 70B 7B 8x7B — —\nCommonsense Reasoning and Reading Comprehension\nPIQA 79.6 83.3 78.8 81.9 82.8 83.0 83.6 — —\nSIQA 57.0 63.3 48.3 50.9 50.7\nHellaSwag 74.2 82.7 77.2 83.3 85.3 81.3 84.4 — —\n75.6 85.1 — — 87.1 83.9 86.7 84.7 95.3\n10-shot 10-shot 10-shot 10-shot 10-shot 10-shot 10-shot\nWinoGrande 70.4 78.5 69.2 76.7 80.2 75.3 7.20 — —\nArc-E 76.1 84.1 75.2 79.4 80.2 80.0 83.1  — —\nArc-C 46.5 59.7 45.9 54.5 57.4 55.5 59.7  — —\nOBQA 51.0 54.0 58.6 58.2 60.2\nBoolQ 81.4 86.0 77.4 83.7 85.0 84.7% — — —\nMath and World Knowledge\nGSM8k 41.6 61.4 14.6 42.2 56.8 52.1 74.4 86.5 92.0\nmaj@s maj@s maj@32 SFT\n50.9 77.0 —~ —~ ~ ~ Bar St eer\nmaj@s maj@32 maj@32\nMATH 11.5 2.5 6.24 13.5 13.1 28.4 32.6 52.9**\nmaj@1 maj@1 maj@4 maj@4\n12.9 24.7\nmaj@4 maj@4\nMMLU 52.1 65.8 45.3 62.6 68.9 60.1 70.6 71.8 86.4\n\ne@ Commonsense Reasoning and Reading Comprehension: We report 0-shot performance on the following\nbenchmarks that measure commonsense reasoning and reading comprehension capabilities: PIQA (Bisk\net al., 2020), SIQA (Sap et al., 2019), HellaSwag (Zellers et al., 2019), WinoGrande (Sakaguchi et al.,\n2021), ARC-Easy (Clark et al., 2018), ARC-Challenge (Clark et al., 2018), OpenBookQA (Mihaylov\net al., 2018), and BoolQ (Clark et al., 2019). We score the prompt with each candidate answer and\ncompute accuracy using the candidate with the highest score. All baseline model performances except a\nfew are taken directly from the reported sources. We observe that Chameleon-7B and Chameleon-34B\n", "vlm_text": "Table 5  Safety testing on 20,000 crowd sourced prompts and 445 red team interactions provoking the model to produce unsafe content. \nThe table presents data about the safety evaluation of different models with varying parameters using two types of datasets: \"Crowdsourced\" and \"Red Team.\"\n\n- **Columns**:\n  - **Dataset**: Type of dataset used for evaluation.\n  - **Params**: The number of parameters in the model, given as either 7 billion (7B) or 34 billion (34B).\n  - **Safe**: The percentage of the dataset classified as safe.\n  - **Unsafe**: The percentage of the dataset classified as unsafe.\n  - **Unsure**: The percentage of the dataset where safety classification was uncertain.\n\n- **Rows**:\n  - For the \"Crowdsourced\" dataset:\n    - Models with 7B parameters showed 99.2% safe, 0.4% unsafe, and 0.4% unsure.\n    - Models with 34B parameters showed 99.7% safe, 0.1% unsafe, and 0.2% unsure.\n  - For the \"Red Team\" dataset:\n    - Models with 34B parameters showed 93.9% safe, 1.6% unsafe, and 4.5% unsure.\n\nThis table highlights how the safety assessment varied across different datasets and model sizes.\n5 Benchmark Evaluations \nGiven the general capabilities of  Chameleon , there is not a single model that we can directly evaluate against; therefore, we evaluate against the best models in every category within our capabilities. \n5.1 Text \nWe evaluate the general text-only capabilities of our pre-trained (not SFT’d) model against other state-of-the- art text-only large language models. We follow the evaluation protocol outlined by  Touvron et al.  ( 2023 ). Specifically we evaluate all models, using an in-house evaluation platform on the areas of commonsense reasoning, reading comprehension, math problems, and world knowledge. We report our results in Table  6 . \nTable 6  Comparison of overall performance on collective academic benchmarks against open-source foundational models.\n\n ∗ Evaluated using our framework/using API. For GSM8k/MATH, we report maj  $@1$   unless mentioned otherwise.\n\n ∗∗ From  Gemini et al.  ( 2023 ). \nThe table displays benchmark performance results for various AI models, comparing their capabilities in \"Commonsense Reasoning and Reading Comprehension\" as well as \"Math and World Knowledge.\" The models evaluated include Chameleon, Llama-2, Mistral, Gemini Pro, and GPT-4, with different configurations such as varying sizes (e.g., 7B, 34B, 8x7B, etc.).\n\n### Commonsense Reasoning and Reading Comprehension Benchmarks\n- PIQA, SIQA, HellaSwag, WinoGrande, Arc-E, Arc-C, OBQA, and BoolQ are the tasks for this category.\n- Performance is usually represented as a percentage accuracy.\n- Some tasks include \"10-shot\" testing, indicating few-shot learning performance.\n\n### Math and World Knowledge Benchmarks\n- GSM8k, MATH, and MMLU are the tasks for this category.\n- Performance is also demonstrated as a percentage.\n- Additional metrics such as `maj@8`, `maj@32`, `maj@1`, `maj@4`, and `CoT` (Chain-of-Thought) are included, indicating different methodologies or test setups.\n- SFT and CoT for GPT-4 likely refer to fine-tuning or reasoning strategies.\n\nEach entry in the table provides performance results, indicating how well these models perform on the respective tasks. The table helps compare the relative efficacy of different AI models across a variety of AI/ML challenges.\n•  Commonsense Reasoning and Reading Comprehension:  We report 0-shot performance on the following benchmarks that measure commonsense reasoning and reading comprehension capabilities:  PIQA  ( Bisk et al. ,  2020 ),  SIQA  ( Sap et al. ,  2019 ),  HellaSwag  ( Zellers et al. ,  2019 ),  WinoGrande  ( Sakaguchi et al. , 2021 ),  ARC-Easy  ( Clark et al. ,  2018 ),  ARC-Challenge  ( Clark et al. ,  2018 ),  OpenBookQA  ( Mihaylov et al. ,  2018 ), and  BoolQ  ( Clark et al. ,  2019 ). We score the prompt with each candidate answer and compute accuracy using the candidate with the highest score. All baseline model performances except a few are taken directly from the reported sources. We observe that  Chameleon-7B  and  Chameleon-34B are competitive with the corresponding Llama-2 models, with  Chameleon-34B  even outperforming Llama-2 70B on  5 / 8  tasks and performing on par with Mixtral 8x7B. "}
{"page": 15, "image_path": "doc_images/2405.09818v1_15.jpg", "ocr_text": "Overa’\n\n8x7B\n\nare competitive with the corresponding Llama-2 models, with Chameleon-34B even outperforming\nLlama-2 70B on 5/8 tasks and performing on par with Mixtral 8x7B.\n\ne MATH and World Knowledge We report 8-shot performance on GSM8K (Cobbe et al., 2021) i.e., grade\nschool math word problems and 4-shot performance on the MATH (Hendrycks et al., 2021) benchmark.\nWe report maj@N exact match accuracy for both benchmarks by sampling N generations from the model\n(greedy sampling for N=1) and choosing the answer via majority voting. Despite training for additional\nmodalities, both Chameleon models demonstrate strong math capabilities. On GSM8k, Chameleon-7B\noutperforms the corresponding Llama-2 models, with performance comparable to Mistral 7B (50.9 vs\n52.1 maj@8). Furthermore, Chameleon-34B can outperform Llama2-70B on maj@1 (61.4 vs 56.8) and\nMixtral 8x7B on maj@32 (77.0 vs 75.1). Similarly, on MATH, Chameleon-7B outperforms Llama-2\nand matches Mistral 7B on maj@4, while Chameleon-34B outperforms Llama2-70B, approaching the\n\nerformance of Mixtral 8x7B on maj@4 (24.7 vs 28.4).\n\nWe also report performance on MMLU (Hendrycks et al., 2020), which measures world/in-domain\nnowledge and problem-solving abilities using 57 subjects, including elementary mathematics, US\nhistory, computer science, and law. Both Chameleon models outperform their Llama-2 counterparts\nwith Chameleon-34B approaching the performance of Mixtral 8x7B/Gemini-Pro (65.8 vs 70.6/71.8).\n\n1, Chameleon outperforms LLaMa-2 across the board, with performance approaching Mistral 7B/Mixtral\n(Jiang et al., 2023, 2024) on some tasks. These gains are likely due to multiple factors. First, we do\n\ntwo epochs over the LLaMa-2 pre-training data, and in general use more compute for pretraining. Second,\n\ninclud\n\ning code data significantly improves performance on text-only reasoning tasks. Lastly, having higher\n\nquality data in the last 20% of pre-training significantly improves performance.\n\n5.2\n\nImage-To-Text\n\nWe next evaluate Chameleon on the segment of tasks that requires text generation conditioned on an image,\nspecifically on image captioning and visual question-answering tasks, and present results of Chameleon-34B\nin Table 7. Together with our pre-trained model, we also present results with a model fine-tuned on all tasks\ntogether (Chameleon-34B-MultiTask), as well as models exclusively fine-tuned for the specific evaluation\n\ntasks (Chameleon-34B-SFT).\n\nWe evaluate against available open-source late-fusion models: specifically Flamingo 80B (Alayrac et al., 2022),\nIDEFICS 80B (Laurengon et al., 2023), and Llava-1.5 (Liu et al., 2023a), as well as recent closed-source\nmodels, such as Gemini (Gemini et al., 2023) and GPT4-V (OpenAI, 2023). We note that we did not take\nany special care when formatting the pre-training data to ensure that 0-shot inference can be effectively done.\nTherefore, we augment the input images or questions with the published prompts used by other models. This\nwas purposefully done to maintain the fidelity of the pre-training data.\n\ne Image Captioning: For image captioning evaluations we report CiDER (Vedantam et al., 2015) scores\non the Karpathy test split of MS-COCO (Lin et al., 2014), and the Karpathy test split of Flickr30k\n(Plummer et al., 2015) using the pycocoevalcap (Chen et al., 2020) package. For Chameleon models,\n\nwe restrict captions to 30 tokens. We evaluated GPT-4V and Gemini models using several prompts and\ngeneration lengths via their APIs and report the best performance that we were able to achieve.\n\nIn the open-source pre-trained category, Chameleon-34B (2-shot) outperforms the larger 80B models of\nboth Flamingo and IDEFICS on COCO with 32-shots, while matching their performance on Flickr30k.\nWith respect to fine-tuned/closed-source models, both multi-task and SFT variants of Chameleon-34B\noutperform all other models on COCO, while for Flickr30k, the SFT model outperforms other models\nwith the multitask model being a close competitor.\n\ne Visual Question Answering: For visual question answering (VQA) we report performance on the test-\ndev split of VQA-v2 (Goyal et al., 2017). For VQA-v2, the pre-trained Chameleon-34B model with\n2-shots matches the 32-shot performance of the larger Flamingo and IDEFICS models, while for fine-\ntuned/closed models, Chameleon-34B-Multitask approaches the performance of IDEFICS-80B-Instruct\nand Gemini Pro, but trails larger models such as Flamingo-80B-FT, GPT-4V, and Gemini Ultra.\nLlava-1.5 outperforms Chameleon-34B on VQAv2 potentially owing to its additional fine-tuning on\n\n16\n", "vlm_text": "\n•  MATH and World Knowledge  We report 8-shot performance on  GSM8K  ( Cobbe et al. ,  2021 ) i.e., grade school math word problems and 4-shot performance on the  MATH  ( Hendrycks et al. ,  2021 ) benchmark. We report maj  $@\\mathrm{N}$   exact match accuracy for both benchmarks by sampling N generations from the model (greedy sampling for  $\\mathrm{N{=}1}$  ) and choosing the answer via majority voting. Despite training for additional modalities, both  Chameleon  models demonstrate strong math capabilities. On  GSM8k ,  Chameleon-7B outperforms the corresponding Llama-2 models, with performance comparable to Mistral 7B (50.9 vs  $52.1\\ \\mathrm{maj@8}$  ). Furthermore,  Chameleon-34B  can outperform Llama2-70B on maj@1 (61.4 vs 56.8) and Mixtral 8x7B on maj  $@32$   (77.0 vs 75.1). Similarly, on MATH,  Chameleon-7B  outperforms Llama-2 and matches Mistral 7B on maj@4, while  Chameleon-34B  outperforms Llama2-70B, approaching the performance of Mixtral 8x7B on maj@4 (24.7 vs 28.4). \nWe also report performance on MMLU ( Hendrycks et al. ,  2020 ), which measures world/in-domain knowledge and problem-solving abilities using 57 subjects, including elementary mathematics, US history, computer science, and law. Both  Chameleon  models outperform their Llama-2 counterparts with  Chameleon-34B  approaching the performance of Mixtral 8x7B/Gemini-Pro (65.8 vs 70.6/71.8). \nOverall,  Chameleon  outperforms LLaMa-2 across the board, with performance approaching Mistral 7B/Mixtral 8x7B ( Jiang et al. ,  2023 ,  2024 ) on some tasks. These gains are likely due to multiple factors. First, we do two epochs over the LLaMa-2 pre-training data, and in general use more compute for pre training. Second, including code data significantly improves performance on text-only reasoning tasks. Lastly, having higher quality data in the last   $20\\%$   of pre-training significantly improves performance. \n5.2 Image-To-Text \nWe next evaluate  Chameleon  on the segment of tasks that requires text generation conditioned on an image, specifically on image captioning and visual question-answering tasks, and present results of  Chameleon-34B in Table  7 . Together with our pre-trained model, we also present results with a model fine-tuned on all tasks together ( Chameleon-34B -MultiTask), as well as models exclusively fine-tuned for the specific evaluation tasks ( Chameleon-34B -SFT). \nWe evaluate against available open-source late-fusion models: specifically Flamingo 80B ( Alayrac et al. ,  2022 ), IDEFICS 80B ( Laurençon et al. ,  2023 ), and Llava-1.5 ( Liu et al. ,  2023a ), as well as recent closed-source models, such as Gemini ( Gemini et al. ,  2023 ) and GPT4-V ( OpenAI ,  2023 ). We note that we did not take any special care when formatting the pre-training data to ensure that 0-shot inference can be effectively done. Therefore, we augment the input images or questions with the published prompts used by other models. This was purposefully done to maintain the fidelity of the pre-training data. \n•  Image Captioning:  For image captioning evaluations we report CiDER ( Vedantam et al. ,  2015 ) scores on the Karpathy test split of MS-COCO ( Lin et al. ,  2014 ), and the Karpathy test split of Flickr30k ( Plummer et al. ,  2015 ) using the  py co coeval cap  ( Chen et al. ,  2020 ) package. For  Chameleon  models, we restrict captions to  30  tokens. We evaluated GPT-4V and Gemini models using several prompts and generation lengths via their APIs and report the best performance that we were able to achieve. \nIn the open-source pre-trained category,  Chameleon-34B  (2-shot) outperforms the larger 80B models of both Flamingo and IDEFICS on COCO with 32-shots, while matching their performance on Flickr30k. With respect to fine-tuned/closed-source models, both multi-task and SFT variants of  Chameleon-34B outperform all other models on COCO, while for Flickr30k, the SFT model outperforms other models with the multitask model being a close competitor. \n•  Visual Question Answering:  For visual question answering (VQA) we report performance on the test- dev split of VQA-v2 ( Goyal et al. ,  2017 ). For VQA-v2, the pre-trained  Chameleon-34B  model with 2-shots matches the 32-shot performance of the larger Flamingo and IDEFICS models, while for fine- tuned/closed models,  Chameleon-34B -Multitask approaches the performance of IDEFICS-80B-Instruct and Gemini Pro, but trails larger models such as Flamingo-80B-FT, GPT-4V, and Gemini Ultra. Llava-1.5 outperforms  Chameleon-34B  on VQAv2 potentially owing to its additional fine-tuning on "}
{"page": 16, "image_path": "doc_images/2405.09818v1_16.jpg", "ocr_text": "Table 7 Model Performances on Image-to-Text Capabilities. * Evaluated using API.\n\nModel Model Size COCO Flickr30k VQAv2\ningo- 5\nPre-trained Flamingo-80B 80B H38 Bo 87.6\nIDEFICS-80B 80B 116.6 73.7 65.9\n32-shot 4-shot 32-shot\nChameleon 34B 120.2 74.7 66.0\nChameleon 2-shot 2-shot 2-shot\nChameleon-SFT 34B 140.8 82.3 —\nO-shot 2-shot\nChameleon-MultiTask 34B 139.1 76.2 69.6\n2-shot 2-shot\nFine-tuned Flamingo-80B-FT 80B 138.1 — 82.0\ntnersune IDEFICS-80B-Instruct 80B 123.2 78.4 68.8\n32-shot 32-shot 32-shot\nGPT-4V — 78.5*  55.3* 77.2\nClosed Source S-shot S-shot\n(finetuning Gemini Nano 2 — — — 67.5\nstatus unknown) — Gemini Pro — 99.8\" 82.2\" 71.2\n2-shot 4-shot\nGemini Ultra — — — 77.8\n\nconversations from GPT-4, ShareGPT (ShareGPT, 2023), GQA (Hudson and Manning, 2019), and\nregion-level VQA datasets, but significantly trails behind on the other tasks.\n\nIn general, we find Chameleon is fairly competitive on both image captioning and VQA tasks. It rivals other\nmodels by using much fewer in-context training examples and with smaller model sizes, in both pre-trained\nand fine-tuned model evaluations.\n\n6 Related Work\n\nChameleon builds upon the lineage of works exploring token-based approaches for multimodal learning. The\nidea of using discrete tokens to represent continuous modalities like images was first explored in works like\nBEIT (Bao et al., 2021), which proposed a self-supervised vision representation learning method based on\ntokenized image patches. Aghajanyan et al. (2022) extended this idea to learning from mixed-modal documents\nthrough interleaved image and text tokens, allowing for joint reasoning over both modalities within a unified\narchitecture. CM3Leon (Yu et al., 2023) further scaled up this approach to autoregressive text-to-image\ngeneration, building on the initial proposal of token-based image generation in DALL-E (Ramesh et al., 2021).\n\nAs a fully token-based early-fusion model, Chameleon differs from late-fusion approaches like Flamingo\n(Alayrac et al., 2022) which encode images and text separately before combining them at a later stage.\nOther models like LLaVA (Liu et al., 2023a), IDEFICS (Laurengon et al., 2023), and VisualGPT (Chen\net al., 2022) also maintain separate image and text encoders. In contrast, Chameleon’s unified token space\nallows it to seamlessly reason over and generate interleaved image and text sequences, without the need for\nmodality-specific components. This early-fusion approach, however, comes with significant challenges in terms\nof representation learning and alignment, as discussed in BaltruSaitis et al. (2018).\n\nThe most similar model to Chameleon is Gemini (Gemini et al., 2023), which also uses an early-fusion\ntoken-based approach. However, a key difference is that Gemini uses separate image decoders, whereas\nChameleon is an end-to-end dense model without any routing components. This makes Chameleon a more\ngeneral-purpose model for both multimodal understanding and generation tasks, similar in spirit to the\nPerceiver (Jaegle et al., 2021) architecture which also aims for a unified model across modalities and tasks.\n\nIn summary, Chameleon builds on a rich history of work in multimodal learning and token-based architectures,\nwhile pushing the boundaries in terms of model scale and architecture design. By demonstrating strong\nperformance across a wide range of vision-language tasks and enabling new capabilities in mixed-modal\nreasoning and generation, Chameleon represents a significant step towards realizing the vision of general-\n\n17\n", "vlm_text": "The table presents a comparison of different AI models on three datasets: COCO, Flickr30k, and VQAv2. The models are categorized into three groups: Pre-trained, Chameleon, and Fine-tuned, with an additional category for Closed Source models whose finetuning status is unknown.\n\n- **Pre-trained Models:**\n  - Flamingo-80B (80B model size) scores 113.8 on COCO, 75.1 on Flickr30k (using a 4-shot setting), and 67.6 on VQAv2 (with a 32-shot setting).\n  - IDEFICS-80B (80B model size) scores 116.6 on COCO, 73.7 on Flickr30k (4-shot), and 65.9 on VQAv2 (32-shot).\n\n- **Chameleon Models:**\n  - Chameleon (34B model size) scores 120.2 on COCO, 74.7 on Flickr30k (2-shot), and 66.0 on VQAv2.\n  - Chameleon-SFT (34B model size) scores 140.8 on COCO and 82.3 on Flickr30k (both 2-shot), with no score on VQAv2.\n  - Chameleon-MultiTask (34B model size) scores 139.1 on COCO, 76.2 on Flickr30k (2-shot), and 69.6 on VQAv2.\n\n- **Fine-tuned Models:**\n  - Flamingo-80B-FT (80B model size) scores 138.1 on COCO and 82.0 on VQAv2.\n  - IDEFICS-80B-Instruct (80B model size) scores 123.2 on COCO, 78.4 on Flickr30k (2-shot), and 68.8 on VQAv2.\n\n- **Closed Source Models (finetuning status unknown):**\n  - GPT-4V scores 78.5 on COCO* and 55.3 on Flickr30k* (both 8-shot), and 77.2 on VQAv2.\n  - Gemini Nano 2 scores 67.5 on VQAv2.\n  - Gemini Pro scores 99.8 on COCO*, 82.2 on Flickr30k* (4-shot), and 71.2 on VQAv2.\n  - Gemini Ultra scores 77.8 on VQAv2.\n\nNote: An asterisk (*) refers to scores with specific settings different from the default.\nconversations from GPT-4, ShareGPT ( ShareGPT ,  2023 ), GQA ( Hudson and Manning ,  2019 ), and region-level VQA datasets, but significantly trails behind on the other tasks. \nIn general, we find  Chameleon  is fairly competitive on both image captioning and VQA tasks. It rivals other models by using much fewer in-context training examples and with smaller model sizes, in both pre-trained and fine-tuned model evaluations. \n6 Related Work \nChameleon builds upon the lineage of works exploring token-based approaches for multimodal learning. The idea of using discrete tokens to represent continuous modalities like images was first explored in works like BEiT ( Bao et al. ,  2021 ), which proposed a self-supervised vision representation learning method based on tokenized image patches.  Aghajanyan et al.  ( 2022 ) extended this idea to learning from mixed-modal documents through interleaved image and text tokens, allowing for joint reasoning over both modalities within a unified architecture. CM3Leon ( Yu et al. ,  2023 ) further scaled up this approach to auto regressive text-to-image generation, building on the initial proposal of token-based image generation in DALL-E ( Ramesh et al. ,  2021 ). \nAs a fully token-based early-fusion model, Chameleon differs from late-fusion approaches like Flamingo ( Alayrac et al. ,  2022 ) which encode images and text separately before combining them at a later stage. Other models like LLaVA ( Liu et al. ,  2023a ), IDEFICS ( Laurençon et al. ,  2023 ), and VisualGPT ( Chen et al. ,  2022 ) also maintain separate image and text encoders. In contrast, Chameleon’s unified token space allows it to seamlessly reason over and generate interleaved image and text sequences, without the need for modality-specific components. This early-fusion approach, however, comes with significant challenges in terms of representation learning and alignment, as discussed in  Bal tru a it is et al.  ( 2018 ). \nThe most similar model to Chameleon is Gemini ( Gemini et al. ,  2023 ), which also uses an early-fusion token-based approach. However, a key difference is that Gemini uses separate image decoders, whereas Chameleon is an end-to-end dense model without any routing components. This makes Chameleon a more general-purpose model for both multimodal understanding and generation tasks, similar in spirit to the Perceiver ( Jaegle et al. ,  2021 ) architecture which also aims for a unified model across modalities and tasks. \nIn summary, Chameleon builds on a rich history of work in multimodal learning and token-based architectures, while pushing the boundaries in terms of model scale and architecture design. By demonstrating strong performance across a wide range of vision-language tasks and enabling new capabilities in mixed-modal reasoning and generation, Chameleon represents a significant step towards realizing the vision of general- "}
{"page": 17, "image_path": "doc_images/2405.09818v1_17.jpg", "ocr_text": "purpose multimodal foundation models.\n\n7 Conclusion\n\nIn this paper, we introduced Chameleon, a new family of early-fusion token-based foundation models that\nset a new bar for multimodal machine learning. By learning a unified representation space over interleaved\nimage and text tokens, Chameleon is a single model that achieves strong performance across a wide range of\nvision-language benchmarks while enabling new mixed-modal reasoning and generation capabilities.\n\nThe key to Chameleon’s success is its fully token-based architecture, which allows for seamless information\nintegration across modalities. By quantizing images into discrete tokens and training on mixed-modal data\nfrom scratch, Chameleon learns to jointly reason over image and text in a way that is impossible with\nlate-fusion architectures or models that maintain separate encoders for each modality. At the same time,\nChameleon introduces novel techniques for stable and scalable training of early-fusion models, addressing key\noptimization and architectural design challenges that have previously limited the scale of such approaches. On\ntasks such as image captioning and visual question answering, Chameleon-34B outperforms models such as\nFlamingo and IDEFICS, while maintaining competitive performance on text-only benchmarks. Chameleon\nalso unlocks entirely new possibilities for multimodal interaction, as demonstrated by its strong performance\non our new benchmark for mixed-modal open-ended QA.\n\nAcknowledgements\n\nWe thank Naren Briar for her invaluable contribution to manually curating safety prompts, which were crucial\nfor our safety tuning efforts. We also thank Pierre Fernandez for his indispensable support with the Chameleon\nrelease, Shelly Sheynin for her work on the Chameleon image tokenizer, Puxin Xu and David for helping us\nwith datasets. Additionally, we thank Mitchell Wortsman for engaging in insightful discussions about stability\nin large-scale language models and Mike Lewis for general discussions and advice throughout the project. We\nthank Aaron Grattafiori, Firat Ozgenel, Divya Shah, Danny Livshits, Cristian Canton Ferrer, Saghar Hosseini,\nRamon Calderer, Joshua Saxe, Daniel Song and Manish Bhatt for their help with the safety and red teaming\nefforts.\n\nContributors\n\nWe attribute credit separated by bucket of work. Additionally, * indicates joint first authors, ' indicates key\ncontributors, * indicates workstream leads, and # indicates project leads.\n\nPre-Training: Srinivasan Iyer’, Bernie Huang’, Lili Yu', Arun Babu', Chunting Zhou!, Kushal Tirumala, Xi\nVictoria Lin, Hu Xu, Xian Li, Akshat Shrivastava, Omer Levy*?, Armen Aghajanyan’*\n\nAlignment and Safety: Ram Pasunuru’, Andrew Cohen’, Aram H. Markosyan', Koustuv Sinhat, Xiaoqing\nEllen Tan‘, Ivan Evtimov, Ping Yu, Tianlu Wang, Olga Golovneva, Asli Celikyilmaz?\n\nInference and Evaluation: Pedro Rodriguez’, Leonid Shamis', Vasu Sharma', Christine Jou, Karthik Padthe',\nChing-Feng Yeh, Mingda Chen, Bapi Akula, Jacob Kahn‘, Daniel Li!, Scott Yih?\n\nOverall Project: Barlas Oguz, Morteza Behrooz, Benjamin Muller, Carleigh Wood, Mary Williamson, Ramya\nRaghavendra, Barbara Usher, William Ngan, Nikolay Bashlykov, Lukas Blecher, Sony Theakanath (Lead\nPM), Ammar Rizvi (Lead TPM), Gargi Ghosh’, Luke Zettlemoyer*\n\n18\n", "vlm_text": "7 Conclusion \nIn this paper, we introduced  Chameleon , a new family of early-fusion token-based foundation models that set a new bar for multimodal machine learning. By learning a unified representation space over interleaved image and text tokens,  Chameleon  is a single model that achieves strong performance across a wide range of vision-language benchmarks while enabling new mixed-modal reasoning and generation capabilities. \nThe key to  Chameleon ’s success is its fully token-based architecture, which allows for seamless information integration across modalities. By quantizing images into discrete tokens and training on mixed-modal data from scratch,  Chameleon  learns to jointly reason over image and text in a way that is impossible with late-fusion architectures or models that maintain separate encoders for each modality. At the same time, Chameleon  introduces novel techniques for stable and scalable training of early-fusion models, addressing key optimization and architectural design challenges that have previously limited the scale of such approaches. On tasks such as image captioning and visual question answering,  Chameleon-34B  outperforms models such as Flamingo and IDEFICS, while maintaining competitive performance on text-only benchmarks.  Chameleon also unlocks entirely new possibilities for multimodal interaction, as demonstrated by its strong performance on our new benchmark for mixed-modal open-ended QA. \nAcknowledgements \nWe thank Naren Briar for her invaluable contribution to manually curating safety prompts, which were crucial for our safety tuning efforts. We also thank Pierre Fernandez for his indispensable support with the Chameleon release, Shelly Sheynin for her work on the Chameleon image tokenizer, Puxin Xu and David for helping us with datasets. Additionally, we thank Mitchell Wortsman for engaging in insightful discussions about stability in large-scale language models and Mike Lewis for general discussions and advice throughout the project. We thank Aaron Gr atta fiori, Firat Ozgenel, Divya Shah, Danny Livshits, Cristian Canton Ferrer, Saghar Hosseini, Ramon Calderer, Joshua Saxe, Daniel Song and Manish Bhatt for their help with the safety and red teaming efforts. \nContributors \nWe attribute credit separated by bucket of work. Additionally,   ∗ indicates joint first authors,    $^\\dagger$    indicates key contributors,    $^\\ddagger$    indicates workstream leads, and    $\\sharp$  indicates project leads. \nPre-Training: Srinivasan Iyer ∗ , Bernie Huang  $^*$  , Lili Yu  $\\dagger$  , Arun Babu  $^\\dagger$  , Chunting Zhou  $^\\dagger$  , Kushal Tirumala, Xi Victoria Lin, Hu Xu, Xian Li, Akshat Shri vast ava, Omer Levy  $^{\\ddagger}$  , Armen Aghajanyan ∗ ‡ \nAlignment and Safety: Ram Pasunuru  $^*$  , Andrew Cohen  $^\\dagger$  , Aram H. Markosyan  $^\\dagger$  , Koustuv Sinha  $^\\dagger$  , Xiaoqing Ellen   $\\mathrm{tan}^{\\dagger}$  , Ivan Evtimov, Ping Yu, Tianlu Wang, Olga Golovneva, Asli Cel i kyi l maz ‡ \nPedro Rodriguez  $^\\dagger$  , Leonid Shamis  $^\\dagger$  , Vasu Sharma  $^\\dagger$  , Christine Jou, Karthik Padthe  $^\\dagger$  , Inference and Evaluation: Ching-Feng Yeh, Mingda Chen, Bapi Akula, Jacob Kahn  $\\ddagger$  , Daniel Li  $^\\ddagger$  , Scott Yih ‡ \nOverall Project: Barlas Oguz, Morteza Behrooz, Benjamin Muller, Carleigh Wood, Mary Williamson, Ramya Rag have ndra, Barbara Usher, William Ngan, Nikolay Bashlykov, Lukas Blecher, Sony Theakanath (Lead PM), Ammar Rizvi (Lead TPM), Gargi Ghosh  $\\sharp$  , Luke Z ett le moyer ♯ "}
{"page": 18, "image_path": "doc_images/2405.09818v1_18.jpg", "ocr_text": "References\n\nArmen Aghajanyan, Bernie Huang, Candace Ross, Vladimir Karpukhin, Hu Xu, Naman Goyal, Dmytro Okhonko,\nMandar Joshi, Gargi Ghosh, Mike Lewis, et al. Cm3: A causal masked multimodal model of the internet. arXiv\npreprint arXiv:2201.07520, 2022.\n\nArmen Aghajanyan, Lili Yu, Alexis Conneau, Wei-Ning Hsu, Karen Hambardzumyan, Susan Zhang, Stephen Roller,\nNaman Goyal, Omer Levy, and Luke Zettlemoyer. Scaling laws for generative mixed-modal language models. arXiv\npreprint arXiv:2301.03728, 2023.\n\nAgriflanders. Miniatuurpaardjes prijskamp - Agriflanders 2009, 2009. https://en.wikipedia.org/wiki/File:\nMiniatuurpaardje.jpg. CC-BY-SA 2.0, https://creativecommons.org/licenses/by/2.0/deed.en.\n\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch,\nKatherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. Advances in\nNeural Information Processing Systems, 35:23716—23736, 2022.\n\nTadas BaltruSaitis, Chaitanya Ahuja, and Louis-Philippe Morency. Multimodal machine learning: A survey and\ntaxonomy. [EEE transactions on pattern analysis and machine intelligence, 41(2):423-443, 2018.\n\nHangbo Bao, Li Dong, Songhao Piao, and Furu Wei. Beit: Bert pre-training of image transformers. arXiv preprint\narXiv:2106.08254, 2021.\n\nJames Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce\nLee, Yufei Guo, et al. Improving image generation with better captions. Computer Science. https://cdn. openai.\ncom/papers/dall-e-3. pdf, 2(3):8, 2023.\n\nYonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural\nlanguage. In Proceedings of the AAAI conference on artificial intelligence, pages 7432-7439, 2020.\n\nJun Chen, Han Guo, Kai Yi, Boyang Li, and Mohamed Elhoseiny. Visualgpt: Data-efficient adaptation of pretrained\nanguage models for image captioning. In Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 18030-18040, 2022.\n\nXinlei Chen, Hao Fang, Tsung-Yi Lin, and Ramakrishna Vedantam. https://github.com/salaniz/pycocoevalcap, 2020.\n\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham,\nHyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. PaLM: Scaling language modeling with pathways.\narXiv preprint arXiv:2204.02311, 2022.\n\nChristopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq:\nExploring the surprising difficulty of natural yes/no questions. arXiv preprint arXiv:1905.10044, 2019.\n\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord.\n\nThink you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv: 1803.05457,\n2018.\n\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert,\n\nJerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint\narXiv:2110.14168, 2021.\n\nMostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas Peter\nSteiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, et al. Scaling vision transformers to 22 billion\nparameters. In International Conference on Machine Learning, pages 7480-7512. PMLR, 2023.\n\nOran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv Taigman. Make-a-scene: Scene-based\ntext-to-image generation with human priors. arXiv preprint arXiv:2203.13131, 2022.\n\nGemini, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan\nSchalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable multimodal models. arXiv\npreprint arXiv:2312.11805, 2023.\n\nYash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the V in VQA matter:\nElevating the role of image understanding in visual question answering. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 6904-6913, 2017.\n\n19\n", "vlm_text": "References \nArmen Aghajanyan, Bernie Huang, Candace Ross, Vladimir Karpukhin, Hu Xu, Naman Goyal, Dmytro Okhonko, Mandar Joshi, Gargi Ghosh, Mike Lewis, et al. Cm3: A causal masked multimodal model of the internet.  arXiv preprint arXiv:2201.07520 , 2022. Armen Aghajanyan, Lili Yu, Alexis Conneau, Wei-Ning Hsu, Karen Ham bard zum yan, Susan Zhang, Stephen Roller, Naman Goyal, Omer Levy, and Luke Z ett le moyer. Scaling laws for generative mixed-modal language models.  arXiv preprint arXiv:2301.03728 , 2023. Agri flanders. Mini atu ur pa ard jes prijskamp - Agri flanders 2009, 2009. https://en.wikipedia.org/wiki/File: Mini atu ur pa ard je.jpg . CC-BY-SA 2.0,  https://creative commons.org/licenses/by/2.0/deed.en . Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning.  Advances in Neural Information Processing Systems , 35:23716–23736, 2022. Tadas Bal tru a it is, Chaitanya Ahuja, and Louis-Philippe Morency. Multimodal machine learning: A survey and taxonomy.  IEEE transactions on pattern analysis and machine intelligence , 41(2):423–443, 2018. Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. Beit: Bert pre-training of image transformers.  arXiv preprint arXiv:2106.08254 , 2021. James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions.  Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf , 2(3):8, 2023. Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural language. In  Proceedings of the AAAI conference on artificial intelligence , pages 7432–7439, 2020. Jun Chen, Han Guo, Kai Yi, Boyang Li, and Mohamed Elhoseiny. Visualgpt: Data-efficient adaptation of pretrained language models for image captioning. In  Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 18030–18040, 2022. Xinlei Chen, Hao Fang, Tsung-Yi Lin, and Ramakrishna Vedantam.  https://github.com/salaniz/py co coeval cap , 2020. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. PaLM: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311 , 2022. Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions.  arXiv preprint arXiv:1905.10044 , 2019. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge.  arXiv preprint arXiv:1803.05457 , 2018. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems.  arXiv preprint arXiv:2110.14168 , 2021. Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas Peter Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Al abdul mohsin, et al. Scaling vision transformers to 22 billion parameters. In  International Conference on Machine Learning , pages 7480–7512. PMLR, 2023. Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv Taigman. Make-a-scene: Scene-based text-to-image generation with human priors.  arXiv preprint arXiv:2203.13131 , 2022. Gemini, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable multimodal models.  arXiv preprint arXiv:2312.11805 , 2023. Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the V in VQA matter: Elevating the role of image understanding in visual question answering. In  Proceedings of the IEEE conference on computer vision and pattern recognition , pages 6904–6913, 2017. "}
{"page": 19, "image_path": "doc_images/2405.09818v1_19.jpg", "ocr_text": "Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring\nmassive multitask language understanding. In International Conference on Learning Representations, 2020.\n\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob\nSteinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021.\n\nOr Honovich, Thomas Scialom, Omer Levy, and Timo Schick. Unnatural instructions: Tuning language models with\n(almost) no human labor, 2022.\n\nDrew A Hudson and Christopher D Manning. Gqa: A new dataset for real-world visual reasoning and compositional\nquestion answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages\n6700-6709, 2019.\n\nAndrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch, Catalin Ionescu, David Ding, Skanda\nKoppula, Daniel Zoran, Andrew Brock, Evan Shelhamer, et al. Perceiver io: A general architecture for structured\ninputs & outputs. arXiv preprint arXiv:2107.14795, 2021.\n\nAlbert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las\nCasas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint\narXiv:2310.06825, 2023.\n\nAlbert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh\nChaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. arXiv preprint\narXiv:2401.04088, 2024.\n\nYang Jin, Kun Xu, Liwei Chen, Chao Liao, Jianchao Tan, Bin Chen, Chenyi Lei, An Liu, Chengru Song, Xiaoqiang Lei,\net al. Unified language-vision pretraining with dynamic discrete visual tokenization. arXiv preprint arXiv:2309.04669,\n2023.\n\nYuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Pick-a-pic: An open\ndataset of user preferences for text-to-image generation, 2023.\n\nKlaus Krippendorff. Content analysis: An introduction to its methodology. Sage publications, 2018.\n\nTaku Kudo and John Richardson. Sentencepiece: A simple and language independent subword tokenizer and detokenizer\nfor neural text processing. arXiv preprint arXiv:1808.06226, 2018.\n\nHugo Laurengon, Lucile Saulnier, Léo Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang,\nSiddharth Karamcheti, Alexander M Rush, Douwe Kiela, et al. Obelisc: An open web-scale filtered dataset of\ninterleaved image-text documents. arXiv preprint arXiv:2306.16527, 2023.\n\nKevin Lee and Shubho Sengupta. Introducing the ai research supercluster — meta’s cutting-edge ai supercomputer for\nai research. https://ai.facebook.com/blog/ai-rsc/, 2022.\n\nBenjamin Lefaudeux, Francisco Massa, Diana Liskovich, Wenhan Xiong, Vittorio Caggiano, Sean Naren, Min Xu, Jieru\nHu, Marta Tintore, Susan Zhang, Patrick Labatut, Daniel Haziza, Luca Wehrstedt, Jeremy Reizenstein, and Grigory\nSizov. xformers: A modular and hackable transformer modelling library. https://github.com/facebookresearch/\nxformers, 2022.\n\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In European conference on computer vision, pages 740-755.\nSpringer, 2014.\n\nHaotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. arXiv\npreprint arXiv:2310.08744, 2023a.\n\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv preprint arXiv:2304.08485,\n2023b.\n\nZe Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer:\nHierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on\ncomputer vision, pages 10012-10022, 2021.\n\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017.\n\nGiacomo Marzi, Marco Balzano, and Davide Marchiori. K-alpha calculator—krippendorft’s alpha calculator: A user-\nfriendly tool for computing krippendorff’s alpha inter-rater reliability coefficient. MethodsX, 12:102545, 2024. ISSN\n\n20\n", "vlm_text": "Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In  International Conference on Learning Representations , 2020. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset.  arXiv preprint arXiv:2103.03874 , 2021. Or Honovich, Thomas Scialom, Omer Levy, and Timo Schick. Unnatural instructions: Tuning language models with (almost) no human labor, 2022. Drew A Hudson and Christopher D Manning. Gqa: A new dataset for real-world visual reasoning and compositional question answering. In  Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 6700–6709, 2019. Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch, Catalin Ionescu, David Ding, Skanda Koppula, Daniel Zoran, Andrew Brock, Evan Shelhamer, et al. Perceiver io: A general architecture for structured inputs   $\\&$   outputs.  arXiv preprint arXiv:2107.14795 , 2021. Albert Q Jiang, Alexandre S a blay roll es, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b.  arXiv preprint arXiv:2310.06825 , 2023. Albert Q Jiang, Alexandre S a blay roll es, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts.  arXiv preprint arXiv:2401.04088 , 2024. Yang Jin, Kun Xu, Liwei Chen, Chao Liao, Jianchao Tan, Bin Chen, Chenyi Lei, An Liu, Chengru Song, Xiaoqiang Lei, et al. Unified language-vision pre training with dynamic discrete visual token iz ation.  arXiv preprint arXiv:2309.04669 , 2023. Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Pick-a-pic: An open dataset of user preferences for text-to-image generation, 2023. Klaus Kri pp end orff.  Content analysis: An introduction to its methodology . Sage publications, 2018. Taku Kudo and John Richardson. Sentence piece: A simple and language independent subword tokenizer and de token ize r for neural text processing.  arXiv preprint arXiv:1808.06226 , 2018. Hugo Laurençon, Lucile Saulnier, Léo Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander M Rush, Douwe Kiela, et al. Obelisc: An open web-scale filtered dataset of interleaved image-text documents.  arXiv preprint arXiv:2306.16527 , 2023. Kevin Lee and Shubho Sengupta. Introducing the ai research supercluster — meta’s cutting-edge ai supercomputer for ai research.  https://ai.facebook.com/blog/ai-rsc/ , 2022. Benjamin Lefaudeux, Francisco Massa, Diana Liskovich, Wenhan Xiong, Vittorio Caggiano, Sean Naren, Min Xu, Jieru Hu, Marta Tintore, Susan Zhang, Patrick Labatut, Daniel Haziza, Luca Wehrstedt, Jeremy Rei zen stein, and Grigory Sizov. xformers: A modular and hackable transformer modelling library.  https://github.com/facebook research/ xformers , 2022. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In  European conference on computer vision , pages 740–755. Springer, 2014. Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning.  arXiv preprint arXiv:2310.03744 , 2023a. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning.  arXiv preprint arXiv:2304.08485 , 2023b. Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In  Proceedings of the IEEE/CVF international conference on computer vision , pages 10012–10022, 2021. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regular iz ation.  arXiv preprint arXiv:1711.05101 , 2017. Giacomo Marzi, Marco Balzano, and Davide Marchiori. K-alpha calculator–kri pp end orff’s alpha calculator: A user- friendly tool for computing kri pp end orff’s alpha inter-rater reliability coefficient.  MethodsX , 12:102545, 2024. ISSN "}
{"page": 20, "image_path": "doc_images/2405.09818v1_20.jpg", "ocr_text": "2215-0161. doi: https://doi.org/10.1016/j.mex.2023.102545. https://www.sciencedirect.com/science/article/pii/\n$2215016123005411.\n\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity? a new\ndataset for open book question answering. arXiv preprint arXiv:1809.02789, 2018.\n\nOpenAI. GPTV System Card. https://cdn.openai.com/papers/GPTV_System_Card.pdf, 2023.\n\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming\nLin, Natalia Gimelshein, Luca Antiga, et al. PyTorch: An imperative style, high-performance deep learning library.\nn NeurIPS, 2019.\n\nBryan A Plummer, Liwei Wang, Chris M Cervantes, Juan C Caicedo, Julia Hockenmaier, and Svetlana Lazebnik.\nFlickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models. In Proceedings\nof the IEEE international conference on computer vision, pages 2641-2649, 2015.\n\nAditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever.\nZero-shot text-to-image generation. arXiv preprint arXiv:2102.12092, 2021.\n\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image\ngeneration with clip latents. arXiv preprint arXiv:2204.06125, 2022.\n\nBaptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu,\nTal Remez, Jérémy Rapin, et al. Code llama: Open foundation models for code. arXiv preprint arXiv:2308. 12950,\n2023.\n\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd\nschema challenge at scale. Communications of the ACM, 64(9):99-106, 2021.\n\nMikayel Samvelyan, Sharath Chandra Raparthy, Andrei Lupu, Eric Hambro, Aram H. Markosyan, Manish Bhatt,\nYuning Mao, Minqi Jiang, Jack Parker-Holder, Jakob Foerster, Tim Rocktaschel, and Roberta Raileanu. Rainbow\nteaming: Open-ended generation of diverse adversarial prompts, 2024.\n\nMaarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi. Socialiqga: Commonsense reasoning about\nsocial interactions. arXiv preprint arXiv:1904.09728, 2019.\n\nRylan Schaeffer. Pretraining on the test set is all you need. arXiv preprint arXiv:2309.08632, 2023.\n\nChristoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo\nCoombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for\ntraining next generation image-text models. arXiv preprint arXiv:2210.08402, 2022.\n\nGeorges Seguin. Mille-feuille, 2010. https://en.wikipedia.org/wiki/File:Mille-feuille_20100916.jpg. CC-BY-SA 3.0,\nhttps://creativecommons.org/licenses/by-sa/3.0/deed.en.\n\nRico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. In\nACL, Berlin, Germany, 2016. https://aclanthology.org/P16-1162.\n\nShareGPT. GPTV System Card. https://sharegpt.com/, 2023.\nNoam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020.\n\nMaksim Sokolov. Sagrada Familia July 2022, 2022. https://en.wikipedia.org/wiki/File:Sagrada_Familia_%28July _\n2022%29_08.jpg. CC-BY-SA-4.0, https://creativecommons.org/licenses/by-sa/4.0/deed.en.\n\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple\nway to prevent neural networks from overfitting. The journal of machine learning research, 15(1):1929-1958, 2014.\n\nJianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer\nwith rotary position embedding. arxiv e-prints, art. arXiv preprint arXiv:2104.09864, 2021.\n\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaci, Nikolay Bashlykov,\nSoumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models.\narXiv preprint arXiv:2307.09288, 2023.\n\nJarek Tuszynski. American toad (Bufo americanus) found in Fairfax, Virginia, 2015. https://en.wikipedia.org/wiki/File:\nMiniatuurpaardje.jpg. CC-BY-SA-4.0, https://creativecommons.org/licenses/by-sa/4.0/deed.en.\n\nRamakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image description evaluation.\nIn Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4566-4575, 2015.\n\n21\n", "vlm_text": "2215-0161. doi: https://doi.org/10.1016/j.mex.2023.102545.  https://www.science direct.com/science/article/pii/ S2215016123005411 . \nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity? a new dataset for open book question answering.  arXiv preprint arXiv:1809.02789 , 2018. \nOpenAI. GPTV System Card.  https://cdn.openai.com/papers/GP TV System Card.pdf , 2023. \nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. PyTorch: An imperative style, high-performance deep learning library. In  NeurIPS , 2019. \nBryan A Plummer, Liwei Wang, Chris M Cervantes, Juan C Caicedo, Julia Hocken maier, and Svetlana Lazebnik. Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models. In  Proceedings of the IEEE international conference on computer vision , pages 2641–2649, 2015. \nAditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation.  arXiv preprint arXiv:2102.12092 , 2021. \nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents.  arXiv preprint arXiv:2204.06125 , 2022. \nBaptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, et al. Code llama: Open foundation models for code.  arXiv preprint arXiv:2308.12950 , 2023. \nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhaga va tula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale.  Communications of the ACM , 64(9):99–106, 2021. \nMikayel Samvelyan, Sharath Chandra Raparthy, Andrei Lupu, Eric Hambro, Aram H. Markosyan, Manish Bhatt, Yuning Mao, Minqi Jiang, Jack Parker-Holder, Jakob Foerster, Tim Rock t s chel, and Roberta Raileanu. Rainbow teaming: Open-ended generation of diverse adversarial prompts, 2024. \nMaarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi. Socialiqa: Commonsense reasoning about social interactions.  arXiv preprint arXiv:1904.09728 , 2019. \nRylan Schaeffer. Pre training on the test set is all you need.  arXiv preprint arXiv:2309.08632 , 2023. \nChristoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models.  arXiv preprint arXiv:2210.08402 , 2022. \nGeorges Seguin. Mille-feuille, 2010.  https://en.wikipedia.org/wiki/File:Mille-feu ille 20100916.jpg . CC-BY-SA 3.0, https://creative commons.org/licenses/by-sa/3.0/deed.en . \nRico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. In ACL , Berlin, Germany, 2016.  https://a cl anthology.org/P16-1162 . \nShareGPT. GPTV System Card.  https://sharegpt.com/ , 2023. \nNoam Shazeer. Glu variants improve transformer.  arXiv preprint arXiv:2002.05202 , 2020. \nMaksim Sokolov. Sagrada Familia July 2022, 2022.  https://en.wikipedia.org/wiki/File:Sagrada Familia%28July_ 2022%29_08.jpg . CC-BY-SA-4.0,  https://creative commons.org/licenses/by-sa/4.0/deed.en . \nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salak hut dino v. Dropout: a simple way to prevent neural networks from over fitting.  The journal of machine learning research , 15(1):1929–1958, 2014. \nJianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. arxiv e-prints, art.  arXiv preprint arXiv:2104.09864 , 2021. \nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 , 2023. "}
{"page": 21, "image_path": "doc_images/2405.09818v1_21.jpg", "ocr_text": "Mitchell Wortsman, Peter J Liu, Lechao Xiao, Katie Everett, Alex Alemi, Ben Adlam, John D Co-Reyes, Izzeddin\nGur, Abhishek Kumar, Roman Novak, et al. Small-scale proxies for large-scale transformer training instabilities.\narXiv preprint arXiv:2309.14322, 2023.\n\nLili Yu, Bowen Shi, Ramakanth Pasunuru, Benjamin Muller, Olga Golovneva, Tianlu Wang, Arun Babu, Binh Tang,\nBrian Karrer, Shelly Sheynin, et al. Scaling autoregressive multi-modal models: Pretraining and instruction tuning.\narXiv preprint arXiv:2309.02591, 2023.\n\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really finish your\nsentence? arXiv preprint arXiv:1905.07830, 2019.\n\nBiao Zhang and Rico Sennrich. Root mean square layer normalization. Advances in Neural Information Processing\nSystems, 32, 2019.\n\nChunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu,\net al. Lima: Less is more for alignment. arXiv preprint arXiv:2305. 11206, 2023.\n\n22\n", "vlm_text": "Mitchell Wortsman, Peter J Liu, Lechao Xiao, Katie Everett, Alex Alemi, Ben Adlam, John D Co-Reyes, Izzeddin Gur, Abhishek Kumar, Roman Novak, et al. Small-scale proxies for large-scale transformer training instabilities. arXiv preprint arXiv:2309.14322 , 2023. Lili Yu, Bowen Shi, Ramakanth Pasunuru, Benjamin Muller, Olga Golovneva, Tianlu Wang, Arun Babu, Binh Tang, Brian Karrer, Shelly Sheynin, et al. Scaling auto regressive multi-modal models: Pre training and instruction tuning. arXiv preprint arXiv:2309.02591 , 2023. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really finish your sentence?  arXiv preprint arXiv:1905.07830 , 2019. Biao Zhang and Rico Sennrich. Root mean square layer normalization.  Advances in Neural Information Processing Systems , 32, 2019. Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. Lima: Less is more for alignment.  arXiv preprint arXiv:2305.11206 , 2023. "}
{"page": 22, "image_path": "doc_images/2405.09818v1_22.jpg", "ocr_text": "Appendix\n\nA Samples\n\n4 Prompt: My mom just adopted a dog called a mountain cur. I thought I\nknew a lot about dogs but I have never heard of this breed. Can you give\nme some basic information as well as show me a few pictures of what they\nlook like?\n\nResponse: Sure! Mountain Cur dogs are a type of hunting dog\nthat originated in the Appalachian Mountains. They are known for\ntheir intelligence, loyalty, and strong prey drive. They are typically\nmedium-sized dogs, with a muscular build and a short, easy-to-maintain\ncoat that comes in a variety of colors including black, brown, tan, red,\nand white. Here is an image of a black and white mountain cur dog: <img>\n\nThey have a long muzzle, floppy ears, and a bushy tail. They are generally\nfriendly and outgoing, but can be wary of strangers if not socialized\nproperly. They are highly energetic dogs that require regular exercise and\nmental stimulation to prevent boredom and destructive behavior.\n\nHere is another image of a mountain cur dog in the forest: <img>\n\nMountain Cur dogs are often used for hunting, particularly for small game\nsuch as squirrels, raccoons, and rabbits. They are also popular family\npets due to their loyal and affectionate nature. However, they do require\nregular exercise and training to ensure they remain happy and healthy.\nWith proper care and attention, Mountain Cur dogs can make wonderful\ncompanions for active families or individuals who enjoy the outdoors.\n\nPrompt: What color is a polar bear’s fur? Show me a photograph of the\npolar bear in the wild.\n\nResponse: Polar bears have white fur, which helps them blend in with\ntheir snowy and icy surroundings. This camouflage allows them to\n\nhunt more effectively, as they can sneak up on their prey without being\ndetected. Additionally, the white fur reflects sunlight, keeping the polar\nbear cool in the harsh Arctic environment. Here is an image of a polar\nbear in the wild:<img>\n\n23\n", "vlm_text": "Appendix \nA Samples \nThe image includes a part of a black and white dog with pointy ears. The background is green, likely indicating grass or a similar outdoor setting.\nPrompt:  My mom just adopted a dog called a mountain cur. I thought I knew a lot about dogs but I have never heard of this breed. Can you give me some basic information as well as show me a few pictures of what they look like? \nResponse: Sure! Mountain Cur dogs are a type of hunting dog that originated in the Appalachian Mountains. They are known for their intelligence, loyalty, and strong prey drive. They are typically medium-sized dogs, with a muscular build and a short, easy-to-maintain coat that comes in a variety of colors including black, brown, tan, red, and white. Here is an image of a black and white mountain cur dog:  <img> \nThey have a long muzzle, floppy ears, and a bushy tail. They are generally friendly and outgoing, but can be wary of strangers if not socialized properly. They are highly energetic dogs that require regular exercise and mental stimulation to prevent boredom and destructive behavior. \nThe image features a dog sitting outdoors on grass with a wooded background.\nHere is another image of a mountain cur dog in the forest:  <img> \nMountain Cur dogs are often used for hunting, particularly for small game such as squirrels, raccoons, and rabbits. They are also popular family pets due to their loyal and affectionate nature. However, they do require regular exercise and training to ensure they remain happy and healthy. With proper care and attention, Mountain Cur dogs can make wonderful companions for active families or individuals who enjoy the outdoors. \nPrompt:  What color is a polar bear’s fur? Show me a photograph of the polar bear in the wild. \nThe image shows a polar bear walking on ice with water in the background.\nResponse:  Polar bears have white fur, which helps them blend in with their snowy and icy surroundings. This camouflage allows them to hunt more effectively, as they can sneak up on their prey without being detected. Additionally, the white fur reflects sunlight, keeping the polar bear cool in the harsh Arctic environment. Here is an image of a polar bear in the wild: <img> "}
{"page": 23, "image_path": "doc_images/2405.09818v1_23.jpg", "ocr_text": "Prompt: <img> My plant looks like this... Is this a problem? If so, how do I fix it?\n\nResponse: The image you provided shows a pothos plant with yellowing leaves,\nwhich can be caused by a variety of factors, including overwatering, underwater-\ning, lack of nutrients, too much direct sunlight, or pests. Here are some steps\nyou can take to troubleshoot and potentially fix the issue:\n\ne Check the soil: Pothos plants prefer well-draining soil that is neither too\nwet nor too dry. If the soil feels soggy or waterlogged, stop watering for a\nwhile. Conversely, if the soil feels dry to the touch, it’s time to water.\n\ne Adjust watering: Overwatering is a common cause of yellowing leaves on\npothos plants. Reduce the frequency of watering and allow the soil to dry\nout slightly between waterings.\n\nProvide adequate light: Pothos plants prefer bright, indirect light but can tolerate low light\nconditions. If your plant is not getting enough light, move it to a brighter location.\n\nFertilize: Pothos plants benefit from regular fertilization. Use a balanced, water-soluble fertilizer\nand follow the instructions on the label.\n\nPrune: Yellow leaves can be removed to improve the appearance of the plant. Cut back any yellow\nleaves to the nearest healthy stem using clean pruning shears.\nCheck for pests: Inspect the plant for pests such as mealybugs, spider mites, or scale. If present,\ntreat the plant with insecticidal soap or neem oil according to the product’s instructions.\nRepot: If the plant is pot-bound (meaning the roots have filled the container), consider repotting it\nin a larger container with fresh, well-draining soil.\nBy following these steps, you may be able to identify and address the underlying cause of the yellowing\nleaves on your pothos plant.\n\nB_ Additional Information of Human Evaluations\n\nTable 8 Descriptions of the prompt task categories.\n\nTask category Description\n\nHow-to nstructions on how to complete a task or accomplish a goal.\n\nAdvice nformed problem-solving.\n\nExplanation A more detailed exposition of a topic, e.g., how batteries work, why animals hibernate, or\nhow to apply rules of composition to photography.\n\nHypothetical Responses to imaginative, “what if” questions.\n\nBrainstorming Generating ideas, options, or possibilities.\n\nReasoning Deducing the answer to a question using commonsense or information provided in the prompt.\n\nComparison Describes the similarities / differences between multiple things, like products, places, foods,\netc.\n\nIdentification dentifying objects in the input image.\n\nArticle Asking for the creation of content such as blog posts.\n\nReport Generating a summary of real events.\n\nStory Creating fictional narratives.\n\nOther Other miscellaneous requests.\n\nFor the twelve task categories of the prompts we collected for human evaluation, a short description of each\ncategory can be found in Table 8.\n\nThe task fulfillment rates, broken down by each task category and modality are shown in Table 9 and Table 10.\n\nChameleon’s win rates, broken down by task category and modality, are shown in Table 11, Table 12, Table 13\nand Table 14.\n\n24\n", "vlm_text": "The image shows a potted plant, specifically a variety of pothos (also known as Epipremnum aureum). The plant features green leaves with yellow variegation, commonly found in indoor settings.\nPrompt:  <img>  My plant looks like this... Is this a problem? If so, how do I fix it? \nResponse:  The image you provided shows a pothos plant with yellowing leaves, which can be caused by a variety of factors, including over watering, underwater- ing, lack of nutrients, too much direct sunlight, or pests. Here are some steps you can take to troubleshoot and potentially fix the issue: \n•  Check the soil: Pothos plants prefer well-draining soil that is neither too wet nor too dry. If the soil feels soggy or waterlogged, stop watering for a while. Conversely, if the soil feels dry to the touch, it’s time to water. •  Adjust watering: Over watering is a common cause of yellowing leaves on pothos plants. Reduce the frequency of watering and allow the soil to dry out slightly between waterings. \n•  Provide adequate light: Pothos plants prefer bright, indirect light but can tolerate low light conditions. If your plant is not getting enough light, move it to a brighter location. •  Fertilize: Pothos plants benefit from regular fertilization. Use a balanced, water-soluble fertilizer and follow the instructions on the label. •  Prune: Yellow leaves can be removed to improve the appearance of the plant. Cut back any yellow leaves to the nearest healthy stem using clean pruning shears. •  Check for pests: Inspect the plant for pests such as mealybugs, spider mites, or scale. If present, treat the plant with insecticidal soap or neem oil according to the product’s instructions. •  Repot: If the plant is pot-bound (meaning the roots have filled the container), consider repotting it in a larger container with fresh, well-draining soil. By following these steps, you may be able to identify and address the underlying cause of the yellowing leaves on your pothos plant. \nB Additional Information of Human Evaluations \nTable 8  Descriptions of the prompt task categories. \nThe table provides a list of task categories along with their descriptions. Each category represents a type of activity or request, and the description explains what each category involves. Below are the task categories and their corresponding descriptions:\n\n1. **How-to**: Instructions on how to complete a task or accomplish a goal.\n2. **Advice**: Informed problem-solving.\n3. **Explanation**: A more detailed exposition of a topic, such as how batteries work, why animals hibernate, or how to apply rules of composition to photography.\n4. **Hypothetical**: Responses to imaginative, \"what if\" questions.\n5. **Brainstorming**: Generating ideas, options, or possibilities.\n6. **Reasoning**: Deducing the answer to a question using commonsense or information provided in the prompt.\n7. **Comparison**: Describes the similarities or differences between multiple things, like products, places, foods, etc.\n8. **Identification**: Identifying objects in the input image.\n9. **Article**: Asking for the creation of content such as blog posts.\n10. **Report**: Generating a summary of real events.\n11. **Story**: Creating fictional narratives.\n12. **Other**: Other miscellaneous requests.\nFor the twelve task categories of the prompts we collected for human evaluation, a short description of each category can be found in  Table 8 . \nThe task fulfillment rates, broken down by each task category and modality are shown in  Table 9  and  Table 10 \nChameleon ’s win rates, broken down by task category and modality, are shown in  Table 11 ,  Table 12 ,  Table 13 and  Table 14 . "}
{"page": 24, "image_path": "doc_images/2405.09818v1_24.jpg", "ocr_text": "Table 9 Task fulfillment breakdown.\n\nChameleon Gemini+ GPT-4V+\nTask Type Fulfills Partially Does not Fulfills§ Partially Does not Fulfills Partially Does not\nfulfills fulfill fulfills fulfill fulfills fulfill\nAdvice 69.2% 26.2% 4.7% 42.1% 56.1% 1.9% 43.9% 48.6% 7.5%\nArticle 59.4% 37.5% 3.1% 40.6% 53.1% 6.3% 62.5% 37.5% 0.0%\nBrainstorming 57.9% 36.4% 5.6% 33.3% 61.5% 5.1% 47.7% 47.2% 5.1%\nComparison 60.4% 34.7% 5.0% 47.5% 46.5% 5.9% 48.6% 446% 11.9%\nExplanation 53.0% 37.7% 9.3% 33.8% 61.6% 46% 41.7% 50.3% 7.9%\nHow-to 52.7% 40.5% 6.9% 43.5% 52.7% 3.8% 48.1% 41.2% 10.7%\nHypothetical 55.9% 39.0% 5.1% 39.0% 47.5% 13.6% 42.4% 44.1% 13.6%\nIdentification 55.7% 33.0% 11.3% 33.0% 66.0% 1.0% 35.1% = 55.7% 9.3%\nOther 41.8% 40.0% 18.2% 38.2% 41.8% 20.0% 50.9% 40.0% 9.1%\nReasoning 50.0% 13.6% 36.4% 27.3% 59.1% 13.6% 31.8% 54.5% 13.6%\nReport 49.1% 40.4% 10.5% 29.8% 61.4% 8.8% 38.6% A7.A% 14.0%\nStory 31.7% 63.4% 4.9% 39.0% 56.1% 4.9% 53.7% 43.9% 2.4%\nGemini GPT-4V\nTask Type Fulfills Partially Does not Fulfills Partially Does not\nfulfills fulfill fulfills fulfill\nAdvice 21.5% 70.1% 8.4% 23.4% 75.7% 0.9%\nArticle 12.5% 84.4% 3.1% 9.4% 90.6% 0.0%\nBrainstorming 18.5% 71.8% 9.7% 27.2% 66.7% 6.2%\nComparison 14.9% 76.2% 8.9% 19.8% 72.3% 7.9%\nExplanation 15.2% 78.1% 6.6% 19.9% 77.5% 2.6%\nHow-to 19.8% 74.0% 6.1% 31.3% — 67.2% 1.5%\nHypothetical 30.5% 49.2% 20.3% 32.2% 61.0% 6.8%\nIdentification 18.6% 75.3% 6.2% 22.7% 68.0% 9.3%\nOther 14.5% 60.0% 25.5% 18.2% 67.3% 14.5%\nReasoning 9.1% 77.3% 13.6% 13.6% 81.8% 4.5%\nReport 12.3% 77.2% 10.5% 22.8% 68.4% 8.8%\nStory 9.8% 82.9% 7.3% 7.3% 90.2% 2.4%\nTable 10 Modality fulfillment breakdown.\nChameleon Gemini+ GPT-4V+\nFulfills Partially Does not Fulfills Partially Does not Fulfills Partially Does not\nfulfills fulfill fulfills fulfill fulfills fulfill\nMixed-modality 55.3% 36.7% 7.9% 39.2% 57.8% 2.9% 42.6% 52.4% 5.0%\nText-only 57.7% 38.4% 4.0% 36.4% 55.5% 8.1% 46.1% 42.7% 11.2%\nGemini GPT-4V\nFulfills Partially Does not Fulfills Partially Does not\nfulfills fulfill fulfills fulfill\nMixed-modality 19.7% 76.0% 4.3% 24.3% 72.6% 3.2%\nText-only 18.3% 72.7% 9.1% 23.6% 72.0% 4.4%\n\n", "vlm_text": "The table compares the performance of three systems—Chameleon, Gemini+, and GPT-4V+—across various task types. Each system's performance is categorized into \"Fulfills,\" \"Partially fulfills,\" and \"Does not fulfill,\" expressed as percentages. The tasks include:\n\n- Advice\n- Article\n- Brainstorming\n- Comparison\n- Explanation\n- How-to\n- Hypothetical\n- Identification\n- Other\n- Reasoning\n- Report\n- Story\n\nThe percentages reflect how each system performs in fulfilling the tasks.\nThe table compares the performance of two systems, Gemini and GPT-4V, across different task types. It shows the percentage of times each system either \"fulfills,\" \"partially fulfills,\" or \"does not fulfill\" the requirements of each task type.\n\nFor each task type (e.g., Advice, Article, Brainstorming), the table provides three columns for both Gemini and GPT-4V:\n\n1. **Fulfills**: The percentage of times the system completely fulfills the task requirements.\n2. **Partially fulfills**: The percentage of times the system partially meets the task requirements.\n3. **Does not fulfill**: The percentage of times the system does not meet the task requirements at all.\n\nFor example, for the \"Advice\" task type, Gemini fulfills the task 21.5% of the time, partially fulfills it 70.1% of the time, and does not fulfill it 8.4% of the time. In comparison, GPT-4V fulfills the same task 23.4% of the time, partially fulfills it 75.7% of the time, and does not fulfill it 0.9% of the time.\nThe table presents a comparative analysis of different models, namely Chameleon, Gemini+, and GPT-4V+, evaluating how well they \"Fulfill,\" \"Partially fulfill,\" or \"Do not fulfill\" certain criteria. These evaluations are further broken down into \"Mixed-modality\" and \"Text-only\" categories. \n\nHere's a breakdown of the results:\n\n**Chameleon:**\n- Mixed-modality:\n  - Fulfills: 55.3%\n  - Partially fulfills: 36.7%\n  - Does not fulfill: 7.9%\n- Text-only:\n  - Fulfills: 57.7%\n  - Partially fulfills: 38.4%\n  - Does not fulfill: 4.0%\n\n**Gemini+:**\n- Mixed-modality: \n  - Fulfills: 39.2%\n  - Partially fulfills: 57.8%\n  - Does not fulfill: 2.9%\n- Text-only: \n  - Fulfills: 36.4%\n  - Partially fulfills: 55.5%\n  - Does not fulfill: 8.1%\n\n**GPT-4V+:**\n- Mixed-modality:\n  - Fulfills: 42.6%\n  - Partially fulfills: 52.4%\n  - Does not fulfill: 5.0%\n- Text-only:\n  - Fulfills: 46.1%\n  - Partially fulfills: 42.7%\n  - Does not fulfill: 11.2%\n\n**Gemini:**\n- Mixed-modality:\n  - Fulfills: 19.7%\n  - Partially fulfills: 76.0%\n  - Does not fulfill: 4.3%\n- Text-only:\n  - Fulfills: 18.3%\n  - Partially fulfills: 72.7%\n  - Does not fulfill: 9.1%\n\n**GPT-4V:**\n- Mixed-modality:\n  - Fulfills: 24.3%\n  - Partially fulfills: 72.6%\n  - Does not fulfill: 3.2%\n- Text-only:\n  - Fulfills: 23.6%\n  - Partially fulfills: 72.0%\n  - Does not fulfill: 4.4% \n\nThis comparison highlights the various capabilities of these models within both a mixed-modality and text-only context, showing how each model performs in fulfilling its intended purpose in terms of fully, partially, or not meeting the criteria."}
{"page": 25, "image_path": "doc_images/2405.09818v1_25.jpg", "ocr_text": "Table 11 Complete Win Rates: Chameleon vs. Gemini+.\n\nTable 12 Complete Win\n\nWins Ties Loses Win rate\nOverall 435 362 251 58.8%\nAdvice 48 35 24 61.2%\nArticle 4 14 4 65.6%\nBrainstorming 101 60 34 67.2%\nComparison Al 38 22 59.4%\nExplanation 65 46 40 58.3%\nHow-to 53 51 27 59.9%\nHypothetical 7 24 18 49.2%\nIdentification 39 33 25 57.2%\nOther 24 17 14 59.1%\nReasoning 7 8 7 50.0%\nReport 6 22 19 A7.A%\nStory 0 14 17 41.5%\nMixed-modal Prompts 194 145 102 60.4%\nText-only Prompts 241 217 149 57.6%\nRates: Chameleon vs. GPT-4V+.\n\nWins Ties Loses Win rate\nOverall 375 331 342 51.6%\nAdvice 54 27 26 63.1%\nArticle 9 11 12 45.3%\nBrainstorming 78 57 60 54.6%\nComparison 35 35 31 52.0%\nExplanation 53 56 42 53.6%\nHow-to 49 46 36 55.0%\nHypothetical 23 19 17 55.1%\nIdentification 31 26 40 45.4%\nOther 16 13 26 40.9%\nReasoning 11 5 6 61.4%\nReport 16 21 20 46.5%\nStory 0 15 26 18.3%\nMixed-modal Prompts 149 119 173 47.3%\nText-only Prompts 226 212 169 54.7%\n\n26\n", "vlm_text": "The table presents a breakdown of performance metrics across different categories. It includes columns for \"Wins,\" \"Ties,\" \"Loses,\" and \"Win rate\" expressed as a percentage. Here's a detailed description of the contents:\n\n1. **Overall Performance**:\n   - Wins: 435\n   - Ties: 362\n   - Loses: 251\n   - Win rate: 58.8%\n\n2. **Performance by Category**:\n   - **Advice**:\n     - Wins: 48\n     - Ties: 35\n     - Loses: 24\n     - Win rate: 61.2%\n   - **Article**:\n     - Wins: 14\n     - Ties: 14\n     - Loses: 4\n     - Win rate: 65.6%\n   - **Brainstorming**:\n     - Wins: 101\n     - Ties: 60\n     - Loses: 34\n     - Win rate: 67.2%\n   - **Comparison**:\n     - Wins: 41\n     - Ties: 38\n     - Loses: 22\n     - Win rate: 59.4%\n   - **Explanation**:\n     - Wins: 65\n     - Ties: 46\n     - Loses: 40\n     - Win rate: 58.3%\n   - **How-to**:\n     - Wins: 53\n     - Ties: 51\n     - Loses: 27\n     - Win rate: 59.9%\n   - **Hypothetical**:\n     - Wins: 17\n     - Ties: 24\n     - Loses: 18\n     - Win rate: 49.2%\n   - **Identification**:\n     - Wins: 39\n     - Ties: 33\n     - Loses: 25\n     - Win rate: 57.2%\n   - **Other**:\n     - Wins: 24\n     - Ties: 17\n     - Loses: 14\n     - Win rate: 59.1%\n   - **Reasoning**:\n     - Wins: 7\n     - Ties: 8\n     - Loses: 7\n     - Win rate: 50.0%\n   - **Report**:\n     - Wins: 16\n     - Ties: 22\n     - Loses: 19\n     - Win rate: 47.4%\n   - **Story**:\n     - Wins: 10\n     - Ties: 14\n     - Loses: 17\n     - Win rate: 41.5%\n\n3. **Performance by Prompt Type**:\n   - **Mixed-modal Prompts**:\n     - Wins: 194\n     - Ties: 145\n\nThe table summarizes the performance outcomes of some unspecified activity or experiment across different categories and prompt types. It displays the number of 'Wins', 'Ties', and 'Loses', along with the 'Win rate' percentage for each category and prompt type. \n\nBelow is a breakdown of the table contents:\n\n1. **Overall Performance:**\n   - Wins: 375\n   - Ties: 331\n   - Loses: 342\n   - Win rate: 51.6%\n\n2. **Category-specific Performance:**\n   - **Advice:** Wins = 54, Ties = 27, Loses = 26, Win rate = 63.1%\n   - **Article:** Wins = 9, Ties = 11, Loses = 12, Win rate = 45.3%\n   - **Brainstorming:** Wins = 78, Ties = 57, Loses = 60, Win rate = 54.6%\n   - **Comparison:** Wins = 35, Ties = 35, Loses = 31, Win rate = 52.0%\n   - **Explanation:** Wins = 53, Ties = 56, Loses = 42, Win rate = 53.6%\n   - **How-to:** Wins = 49, Ties = 46, Loses = 36, Win rate = 55.0%\n   - **Hypothetical:** Wins = 23, Ties = 19, Loses = 17, Win rate = 55.1%\n   - **Identification:** Wins = 31, Ties = 26, Loses = 40, Win rate = 45.4%\n   - **Other:** Wins = 16, Ties = 13, Loses = 26, Win rate = 40.9%\n   - **Reasoning:** Wins = 11, Ties = 5, Loses = 6, Win rate = 61.4%\n   - **Report:** Wins = 16, Ties = 21, Loses = 20, Win rate = 46.5%\n   - **Story:** Wins = 0, Ties = 15, Loses = 26, Win rate = 18.3%\n\n3. **Performance by Prompt Type:**\n   - **Mixed-modal Prompts:** Wins = 149, Ties = 119, Loses = 173, Win rate = 47.3%\n   - **Text-only Prompts:** Wins = 226, Ties = 212, Loses = 169, Win rate = 54.7%\n\nThe information suggests that the activities or tasks are evaluated across various categories and prompt types, with different success rates ('Win rates') that are recorded as a percentage."}
{"page": 26, "image_path": "doc_images/2405.09818v1_26.jpg", "ocr_text": "Table 13. Complete Win Rates: Chameleon vs. Gemini.\n\nTable 14 Complete Win\n\nWins Ties Loses Win rate\nOverall 561 327 160 69.1%\nAdvice 59 25 23 66.8%\nArticle 18 11 3 73.4%\nBrainstorming 133 42 20 79.0%\nComparison 54 29 18 67.8%\nExplanation 78 51 22 68.5%\nHow-to 65 42 24 65.6%\nHypothetical 27 26 6 67.8%\nIdentification 45 30 22 61.9%\nOther 27 23 5 70.0%\nReasoning 11 6 5 63.6%\nReport 30 21 6 71.1%\nStory 14 21 6 59.8%\nMixed-modal Prompts 240 123 78 68.4%\nText-only Prompts 321 204 82 69.7%\nRates: Chameleon vs. GPT-4V.\n\nWins Ties Loses Win rate\nOverall 482 329 237 61.7%\nAdvice 53 30 24 63.6%\nArticle 18 9 5 70.3%\nBrainstorming 107 53 35 68.5%\nComparison 44 35 22 60.9%\nExplanation 75 36 40 61.6%\nHow-to 51 49 31 57.6%\nHypothetical 20 25 14 55.1%\nIdentification 40 29 28 56.2%\nOther 20 22 13 56.4%\nReasoning 10 6 6 59.1%\nReport 25 18 14 59.6%\nStory 19 17 5 67.1%\nMixed-modal Prompts 191 125 125 57.5%\nText-only Prompts 291 204 112 64.7%\n\n27\n", "vlm_text": "The table presents information on the performance of a system or model, categorized by different types of prompts. The table is divided into four columns: Wins, Ties, Loses, and Win rate, with rows representing categories such as \"Advice,\" \"Article,\" \"Brainstorming,\" and more.\n\nHere's a breakdown of the information:\n\n- The \"Overall\" row summarizes all categories, showing a total of 561 wins, 327 ties, 160 losses, with a win rate of 69.1%.\n\n- Each category row (e.g., \"Advice,\" \"Article,\" \"Brainstorming\") indicates the number of wins, ties, loses, and the calculated win rate for that specific category. For example, \"Advice\" had 59 wins, 25 ties, 23 losses, and a win rate of 66.8%.\n\n- The categories include: Advice, Article, Brainstorming, Comparison, Explanation, How-to, Hypothetical, Identification, Other, Reasoning, Report, and Story.\n\n- The last two rows compare \"Mixed-modal Prompts\" and \"Text-only Prompts,\" showing their respective wins, ties, loses, and win rates. Mixed-modal Prompts had 240 wins with a win rate of 68.4%, while Text-only Prompts had 321 wins with a win rate of 69.7%.\n\nThis table seems to provide an analysis of how different types of prompts perform, potentially in an AI model or some competitive setting, by displaying wins, ties, losses, and win rates for each category.\nThe table presents data on the performance of different types of prompts in terms of \"Wins,\" \"Ties,\" and \"Loses,\" along with their respective \"Win rate.\" Here's a breakdown of the table content:\n\n- **Overall Performance**: \n   - Wins: 482\n   - Ties: 329\n   - Loses: 237\n   - Win rate: 61.7%\n\n- **Performance by Prompt Type**:\n   - **Advice**: \n     - Wins: 53\n     - Ties: 30\n     - Loses: 24\n     - Win rate: 63.6%\n   - **Article**: \n     - Wins: 18\n     - Ties: 9\n     - Loses: 5\n     - Win rate: 70.3%\n   - **Brainstorming**: \n     - Wins: 107\n     - Ties: 53\n     - Loses: 35\n     - Win rate: 68.5%\n   - **Comparison**: \n     - Wins: 44\n     - Ties: 35\n     - Loses: 22\n     - Win rate: 60.9%\n   - **Explanation**: \n     - Wins: 75\n     - Ties: 36\n     - Loses: 40\n     - Win rate: 61.6%\n   - **How-to**: \n     - Wins: 51\n     - Ties: 49\n     - Loses: 31\n     - Win rate: 57.6%\n   - **Hypothetical**: \n     - Wins: 20\n     - Ties: 25\n     - Loses: 14\n     - Win rate: 55.1%\n   - **Identification**: \n     - Wins: 40\n     - Ties: 29\n     - Loses: 28\n     - Win rate: 56.2%\n   - **Other**: \n     - Wins: 20\n     - Ties: 22\n     - Loses: 13\n     - Win rate: 56.4%\n   - **Reasoning**: \n     - Wins: 10\n     - Ties: 6\n     - Loses: 6\n     - Win rate: 59.1%\n   - **Report**: \n     - Wins: 25\n     - Ties: 18\n     - Loses: 14\n     - Win rate: 59.6%\n   - **Story**: \n     - Wins: 19\n     - Ties: 17\n     - Loses: 5\n     - Win rate: 67.1%\n\n- **Performance by Prompt Format**:\n   - **Mixed-modal Prompts**:\n     - Wins: 191"}
