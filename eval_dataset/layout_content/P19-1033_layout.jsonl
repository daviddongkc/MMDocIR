{"layout": 0, "type": "text", "text": "Neural News Recommendation with Long- and Short-term User Representations ", "text_level": 1, "page_idx": 0, "bbox": [116, 69, 480, 101], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 1, "type": "text", "text": "Mingxiao  $\\mathbf{A}\\mathbf{n}^{1,*}$  , Fangzhao   $\\mathbf{W}\\mathbf{u}^{2}$  , Chuhan   $\\mathbf{W}\\mathbf{u}^{3}$  , Kun Zhang 1 , Zheng Liu 2 , Xing Xie 2 ", "text_level": 1, "page_idx": 0, "bbox": [89, 114, 511, 127.75], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 2, "type": "text", "text": "1 University of Science and Technology of China, Hefei 230026, China 2 Microsoft Research Asia, Beijing 100080, China 3 Department of Electronic Engineering, Tsinghua University, Beijing 100084, China { anmx,zhkun } @mail.ustc.edu.cn ,  wufangzhao@gmail.com wuch15@mails.tsinghua.edu.cn ,  { zhengliu,xingx } @microsoft.com ", "page_idx": 0, "bbox": [90.45298767089844, 128.30560302734375, 510.0797424316406, 207.2969970703125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 3, "type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0, "bbox": [159, 223, 204, 235], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 4, "type": "text", "text": "Personalized news recommendation is impor- tant to help users ﬁnd their interested news and improve reading experience. A key problem in news recommendation is learning accurate user representations to capture their interests. Users usually have both long-term preferences and short-term interests. However, existing news recommendation methods usually learn single representations of users, which may be insufﬁcient. In this paper, we propose a neu- ral news recommendation approach which can learn both long- and short-term user represen- tations. The core of our approach is a news encoder and a user encoder. In the news en- coder, we learn representations of news from their titles and topic categories, and use atten- tion network to select important words. In the user encoder, we propose to learn long-term user representations from the embeddings of their IDs. In addition, we propose to learn short-term user representations from their re- cently browsed news via GRU network. Be- sides, we propose two methods to combine long-term and short-term user representations. The ﬁrst one is using the long-term user repre- sentation to initialize the hidden state of the GRU network in short-term user representa- tion. The second one is concatenating both long- and short-term user representations as a uniﬁed user vector. Extensive experiments on a real-world dataset show our approach can effectively improve the performance of neural news recommendation. ", "page_idx": 0, "bbox": [89, 247.1995849609375, 273, 641.7703247070312], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 5, "type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0, "bbox": [71, 653, 156, 667], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 6, "type": "text", "text": "Online news platforms such as MSN News 1   and Google News 2   which aggregate news from various sources and distribute them to users have gained ", "page_idx": 0, "bbox": [71, 671.2858276367188, 290, 715.48828125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 7, "type": "image", "page_idx": 0, "img_path": "layout_images/P19-1033_0.jpg", "img_caption": "Figure 1: An illustrative example of long-term and short-term interests in news reading. ", "bbox": [305, 221, 527, 318], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "2017 NBA Championship\n\nCelebration From Warriors the 2019 Oscar\n\nRami Malek Wins |\n\nua tisd\nti fj\n\nBohemian Rhapsody Is Highest:\nGrossing Musician Biopic Ever\n\nOklahoma City Thunder vs.\nGolden State Warriors\n\n", "vlm_text": "The image depicts a timeline that illustrates a person's news reading interests over time, highlighting both long-term and short-term interests. The timeline is anchored by different points in time (denoted as \\( t_1, t_i, t_{i+1}, t_j \\)) with corresponding news events associated with each point:\n\n1. At \\( t_1 \\), the person has an interest in an event about the \"2017 NBA Championship Celebration From Warriors.\"\n2. At a later time \\( t_i \\), the person reads about \"Bohemian Rhapsody Is Highest-Grossing Musician Biopic Ever.\"\n3. At \\( t_{i+1} \\), the interest is in \"Rami Malek Wins the 2019 Oscar.\"\n4. Finally, at \\( t_j \\), the focus is again on a sports-related event: \"Oklahoma City Thunder vs. Golden State Warriors.\"\n\nThe timeline suggests that the individual has sustained long-term interests in both basketball (specifically related to the Golden State Warriors) and movies/entertainment, particularly related to \"Bohemian Rhapsody\" and Rami Malek."}
{"layout": 8, "type": "text", "text": "huge popularity and attracted hundreds of millions of users ( Das et al. ,  2007 ;  Wang et al. ,  2018 ). However, massive news are generated everyday, making it impossible for users to read through all news ( Lian et al. ,  2018 ). Thus, personalized news recommendation is very important for online news platforms to help users ﬁnd their interested contents and alleviate information overload ( Lavie et al. ,  2010 ;  Zheng et al. ,  2018 ). ", "page_idx": 0, "bbox": [307, 344.25299072265625, 525, 465.7925109863281], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 9, "type": "text", "text": "Learning accurate user representations is criti- cal for news recommendation ( Okura et al. ,  2017 ). Existing news recommendation methods usually learn a single representation for each user ( Okura et al. ,  2017 ;  Lian et al. ,  2018 ;  Wu et al. ,  2019 ). For example,  Okura et al.  ( 2017 ) proposed to learn representations of news using denoising autoen- coder and learn representations of users from their browsed news using GRU network ( Cho et al. , 2014 ). However, it is very difﬁcult for RNN net- works such as GRU to capture the entire informa- tion of very long news browsing history.  Wang et al.  ( 2018 ) proposed to learn the representa- tions of news using knowledge-aware convolu- tional neural network (CNN), and learn the repre- sentations of users from their browsed news based on the similarities between the candidate news and the browsed news. However, this method needs to store the entire browsing history of each user in the online news recommendation stage, which may bring huge challenge to the storage and may cause heavy latency. ", "page_idx": 0, "bbox": [307, 468.35302734375, 525, 766.031494140625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 10, "type": "text", "text": "Our work is motivated by the observation that the interests of online users in news are very di- verse. Some user interests may last for a long time and are consistent for the same user ( Li et al. , 2014 ). For example, as shown in Fig.  1 , if a user is a fan of “Golden State Warriors”, this user may tend to read many basketball news about this NBA team for several years. We call this kind of user preferences as long-term interest. In addition, many user interests may evolve with time and may be triggered by speciﬁc contexts or temporal de- mands. For example, in Fig.  1 , the browsing of the news on movie “Bohemian Rhapsody” causes the user reading several related news such as “Rami Malek Wins the 2019 Oscar” since “Rami Malek” is an important actor in this movie, although this user may never read news about “Rami Malek” be- fore. We call this kind of user interests as short- term interest. Thus, both long-term and short- term user interests are important for personalized news recommendation, and distinguishing long- term user interests from short-term ones may help learn more accurate user representations. ", "page_idx": 1, "bbox": [72, 63.68701934814453, 290, 374.9156188964844], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 11, "type": "text", "text": "In this paper, we propose a neural news rec- ommendation approach with both long- and short- term user representations (LSTUR). Our approach contains two major components, i.e., a news en- coder and a user encoder. The news encoder is used to learn representations of news articles from their titles and topic categories. We apply attention mechanism to the news encoder to learn informa- tive news representations by selecting important words. The user encoder consists of two modules, i.e., a long-term user representation (LTUR) mod- ule and a short-term user representation (STUR) module. In STUR, we use a GRU network to learn short-term representations of users from their re- cently browsing news. In LTUR, we learn the long-term representations of users from the em- beddings of their IDs. In addition, we propose two methods to combine the short-term and long- term user representations. The ﬁrst one is using the long-term user representations to initialize the hidden state of GRU network in the STUR model. The second one is concatenating the long-tern and short-term user representations as a uniﬁed user vector. We conducted extensive experiments on a real-world dataset. The experimental results show our approach can effectively improve the perfor- mance of news recommendation and consistently outperform many baseline methods. ", "page_idx": 1, "bbox": [72, 387.05712890625, 290, 766.0316162109375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 12, "type": "text", "text": "2 Related Works ", "text_level": 1, "page_idx": 1, "bbox": [306, 64, 402, 76], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 13, "type": "text", "text": "Personalized news recommendation is an impor- tant task in natural language processing ﬁeld and has wide applications ( Zheng et al. ,  2018 ). It is critical for news recommendation methods to learn accurate news and user representations ( Wang et al. ,  2018 ). Many conventional news recommen- dation methods rely on manual feature engineer- ing to build news and user representations ( Phelan et al. ,  2009 ;  Liu et al. ,  2010 ;  Li et al. ,  2010 ;  Son et al. ,  2013 ;  Li et al. ,  2014 ;  Bansal et al. ,  2015 ; Lian et al. ,  2018 ). For example, Liu et al. ( 2010 ) proposed to use the topic categories and interests features predicted by a Bayesian model to repre- sent news, and use the click distribution features of news categories to represent users. Li et al. ( 2014 ) used a Latent Dirichlet Allocation (LDA) ( Blei et al. ,  2003 ) model to generate topic distribution features as the news representations. They repre- sented a session by using the topic distribution of browsed news in this session, and the representa- tions of users were built from their session repre- sentations weighted by the time. However, these methods heavily rely on manual feature engineer- ing, which needs massive domain knowledge to craft. In addition, the contexts and orders of words in news are not incorporated, which are important for understanding the semantic meanings of news and learning representations of news and users. ", "page_idx": 1, "bbox": [307, 88.72718048095703, 525, 467.7008361816406], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 14, "type": "text", "text": "In recent years, several deep learning meth- ods were proposed for personalized news rec- ommendation ( Wang et al. ,  2018 ;  Okura et al. , 2017 ;  Zheng et al. ,  2018 ). For example, Okura et al. ( 2017 ) proposed to learn representations of news from news bodies using denoising autoen- coder, and learn representations of users from the representations of their browsed news using a GRU network. Wang et al. ( 2018 ) proposed to learn representations of news from their titles via a knowledge-aware CNN network, and learn rep- resentations of users from the representations of their browsed news articles weighted by their sim- ilarities with the candidate news. Wu et al. ( 2019 ) proposed to learn news and user representations with personalized word- and news-level attention networks, which exploits the embedding of user ID to generate the query vector for the attentions. However, these methods usually learn a single rep- resentation vector for each user, and cannot dis- tinguish the long-term preferences and short-term interests of users in reading news. Thus, the user representations learned in these methods may be insufﬁcient for news recommendation. Different from these methods, our approach can learn both long-term and short-term user representations in a uniﬁed framework to capture the diverse inter- ests of users for personalized neural new commen- dation. Extensive experiments on the real-world dataset validate the effectiveness of our approach and the advantage over many baseline methods. ", "page_idx": 1, "bbox": [307, 468.35235595703125, 525, 766.03076171875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 15, "type": "text", "text": "", "page_idx": 2, "bbox": [71, 63.68701934814453, 291, 185.22653198242188], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 16, "type": "text", "text": "3 Our Approach ", "text_level": 1, "page_idx": 2, "bbox": [71, 196, 166, 208], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 17, "type": "text", "text": "In this section, we present our neural news rec- ommendation approach with long- and short-term user representations (LSTUR). Our approach con- tains two major components, i.e., a news encoder to learn representations of news and a user encoder to learn representations of users. Next, we intro- duce each component in detail. ", "page_idx": 2, "bbox": [71, 216.79811096191406, 291, 311.2386169433594], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 18, "type": "text", "text": "3.1 News Encoder ", "text_level": 1, "page_idx": 2, "bbox": [71, 321, 164, 332], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 19, "type": "text", "text": "The  news encoder  is used to learn representations of news from their titles, topic and subtopic cat- egories. The architecture of the  news encoder  in our approach is illustrated in Fig.  2 . There are two sub-modules in the  news encoder , i.e., a title en- coder and a topic encoder. ", "page_idx": 2, "bbox": [71, 337.9901428222656, 291, 418.88165283203125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 20, "type": "text", "text": "The title encoder is used to learn news repre- sentations from titles. There are three layers in the title encoder. The ﬁrst layer is word embedding, which is used to convert a news title from a word sequence into a sequence of dense semantic vec- tors. Denote the word sequence in a news title    $t$  as    $t=[w_{1},w_{2},.\\,.\\,.\\,,w_{N}]$  , where    $N$   is the length of this title. It is transformed into    $[{\\pmb w}_{1},{\\pmb w}_{2},.\\,.\\,.\\,,{\\pmb w}_{N}]$  via a word embedding matrix. ", "page_idx": 2, "bbox": [71, 419.2911682128906, 291, 540.8306884765625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 21, "type": "text", "text": "The second layer in title encoder is a convolu- tional neural network (CNN) ( LeCun et al. ,  2015 ). Local contexts are very useful for understanding the semantic meaning of news titles. For exam- ple, in the news title “Next season of super bowl games”, the local contexts of “bowl” such as “su- per” and “games” are very important for inferring that it belongs to a sports event name. Thus, we apply a CNN network to learn contextual word representations by capturing the local context in- formation. Denote the contextual representation of    $w_{i}$   as  $c_{i}$  , which is computed as follows: ", "page_idx": 2, "bbox": [71, 541.2412109375, 291, 705.0636596679688], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 22, "type": "equation", "text": "\n$$\n\\pmb{c}_{i}=\\mathrm{ReLU}(\\pmb{C}\\times\\pmb{w}_{[i-M:i+M]}+\\pmb{b}),\n$$\n ", "text_format": "latex", "page_idx": 2, "bbox": [99, 713, 261, 730], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 23, "type": "text", "text": "where    $\\pmb{w}_{[i-M:i+M]}$   is the concatenation of the em- beddings of words between position    $\\textit{i}-\\textit{M}$   and ", "page_idx": 2, "bbox": [71, 739.3372192382812, 291, 766.0316772460938], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 24, "type": "image", "page_idx": 2, "img_path": "layout_images/P19-1033_1.jpg", "img_caption": "Figure 2: The framework of the news encoder. ", "bbox": [308, 65, 523, 209], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "Wy\n\nPadding\nPadding\n", "vlm_text": "The image illustrates a conceptual framework for a news encoder, often used in natural language processing tasks such as news article categorization or summarization. Here's a breakdown of the elements in the framework:\n\n1. **News Input**: The process begins with a news article, which is broken down into three components: News Title, News Subtopic, and News Topic.\n\n2. **Word Embedding**: The words in the News Title are represented visually as boxes labeled from \\( w_1 \\) to \\( w_N \\). These words are transformed into vectors using a word embedding layer, depicted in green. Padding is used to ensure uniform input size across different data instances.\n\n3. **Contextual Representation**: The embedded word vectors are processed to generate contextual representations \\( c_1 \\) through \\( c_N \\) (shown in green and blue), through mechanisms that likely involve attention or contextual transformation.\n\n4. **Attention Mechanism**: The framework utilizes an attention mechanism, depicted by the dotted arrows and coefficients \\( a_1, a_2, \\ldots, a_N \\). This mechanism calculates attention scores for each word to form a weighted context vector \\( v \\).\n\n5. **Title Encoding**: The context vector \\( v \\) is then used to produce the encoded title vector \\( e_t \\).\n\n6. **Subtopic and Topic Encoding**: Separate embedding layers are used to convert the News Subtopic and News Topic into their respective vector embeddings \\( e_{sv} \\) and \\( e_{t} \\).\n\n7. **Final Encoding**: The encoded subtopic \\( e_{sv} \\), topic \\( e_v \\), and title \\( e_t \\) vectors are combined, likely through vector addition or concatenation (represented by the ⊕ symbol), to form the final news encoding vector \\( e \\).\n\nThis framework leverages hierarchical embeddings and attention to efficiently synthesize multiple layers of news content information into a unified encoding."}
{"layout": 25, "type": "text", "text": " $i+M$  .    $C$   and    $^{b}$   are the parameters of the convo- lutional ﬁlters in CNN, and    $M$  is the window size. ", "page_idx": 2, "bbox": [306, 230, 525, 256.9354553222656], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 26, "type": "text", "text": "The third layer is an attention network ( Bah- danau et al. ,  2015 ). Different words in the same news title may have different informativeness for representing news. For instance, in the news ti- tle “The best NBA moments in   $2018\"$  , the word “NBA” is very informative for representing this news since it is an important indication of sports news, while the word   $^{\\ast2018^{\\ast}}$   is less informative. Thus, we employ a word-level attention network to select important words in news titles to learn more informative news representations. The at- tention weight    $\\alpha_{i}$   of the    $i$  -th word is formulated as follows: ", "page_idx": 2, "bbox": [306, 257.3389892578125, 525, 433.0745544433594], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 27, "type": "equation", "text": "\n$$\n\\begin{array}{l}{a_{i}=\\operatorname{tanh}({\\pmb v}\\times{\\pmb c}_{i}+{\\pmb v}_{b}),}\\\\ {\\alpha_{i}=\\displaystyle\\frac{\\exp(a_{i})}{\\sum_{j=1}^{N}\\exp(a_{j})},}\\end{array}\n$$\n ", "text_format": "latex", "page_idx": 2, "bbox": [359, 438, 473, 490], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 28, "type": "text", "text": "where    $\\mathbfit{v}$   and    $v_{b}$   are the trainable parameters. The ﬁnal representation of a news title    $t$   is the summation of its contextual word representations weighted by their attention weights as follows: ", "page_idx": 2, "bbox": [306, 496.85498046875, 525, 550.6484375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 29, "type": "equation", "text": "\n$$\ne_{t}=\\sum_{i=1}^{N}\\alpha_{i}\\pmb{c}_{i}.\n$$\n ", "text_format": "latex", "page_idx": 2, "bbox": [382, 558, 450, 596], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 30, "type": "text", "text": "The topic encoder module is used to learn news representations from its topics and subtopics. On many online news platforms such as MSN news, news articles are usually labeled with a topic cate- gory (e.g., “Sports”) and a subtopic category (e.g., “Football NFL”) to help target user interests. The topic and subtopic categories of news are also in- formative for learning representations of news and users. They can reveal the general and detailed topics of the news, and reﬂect the preferences of users. For example, if a user browsed many news articles with the “Sports” topic category, then we ", "page_idx": 2, "bbox": [306, 603.8450317382812, 525, 766.0314331054688], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 31, "type": "image", "page_idx": 3, "img_path": "layout_images/P19-1033_2.jpg", "img_caption": "Figure 3: The two frameworks of our LSTUR approach. ", "bbox": [78, 59, 512, 226], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "Candidate\nNews\n\n(a) LSTUR-ini.\n\ney\n\nCx\n\nCandidate\nNews\n\n(b) LSTUR-con.\n", "vlm_text": "The image depicts two frameworks of the \"LSTUR\" (Long- and Short-term User Representation) approach for personalized news recommendation systems. It compares two architectures, LSTUR-ini and LSTUR-con, used for processing user click history and candidate news to generate personalized news recommendations.\n\n1. **LSTUR-ini (left side of the image):**\n   - The framework starts with a user's click history, represented as a timeline.\n   - \"User Embedding\" is obtained from the user's past click history and initialized into the model.\n   - The Gru cells (Gated Recurrent Unit) sequentially process encoded news information (from the \"News Encoder\") of the clicked news articles (`c1`, `c2`, ..., `ck`).\n   - The candidate news (`cx`) is also processed by a similar \"News Encoder\" to obtain its vector representation (`ex`).\n   - A dot product operation computes the \"Score\" indicating the relevance of the candidate news to the user's interests.\n\n2. **LSTUR-con (right side of the image):**\n   - Similar to LSTUR-ini, this model also begins with the user's click history.\n   - Instead of initialization, the approach concatenates the user-level embedding (`us`) derived from click history, with a fixed user embedding (`ul`), forming a combined user vector (`u`).\n   - The combined vector (`u`) is processed in a similar manner using GRU cells alongside news encoders to handle click history (`c1`, `c2`, ..., `ck`).\n   - The \"Dot Product\" between the user's combined representation and the candidate news encoding (`ex`) generates a \"Score\" for the news recommendation fit.\n\nBoth frameworks aim to output a \"Score\" based on the comparison between user representation (derived from historical click data) and candidate news, facilitating personalized recommendations."}
{"layout": 32, "type": "text", "text": "can infer this user is probably interested in sports, and it may be effective to recommend candidate news in the “Sports” topic category to this user. To incorporate the topic and subtopic information into news representation, we propose to learn the representations of topics and subtopics from the embeddings of their IDs, as shown in Fig.  2 . De- note  $e_{v}$   and    $e_{s v}$   as the representations of topic and subtopic. The ﬁnal representation of a news arti- cle is the concatenation of the representations of its title, topic and subtopic, i.e.,    $\\boldsymbol{e}=[e_{t},e_{v},e_{s v}]$  . ", "page_idx": 3, "bbox": [71, 247.3880157470703, 290, 396.0255432128906], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 33, "type": "text", "text": "3.2 User Encoder ", "text_level": 1, "page_idx": 3, "bbox": [72, 408, 160, 420], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 34, "type": "text", "text": "The  user encoder  is used to learn representations of users from the history of their browsed news. It contains two modules, i.e., a short-term user repre- sentation model (STUR) to capture user’s tempo- ral interests, and a long-term user representation model (LTUR) to capture user’s consistent prefer- ences. Next, we introduce them in detail. ", "page_idx": 3, "bbox": [71, 427.08905029296875, 290, 521.529541015625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 35, "type": "text", "text": "3.2.1 Short-Term User Representation ", "text_level": 1, "page_idx": 3, "bbox": [71, 532, 258, 545], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 36, "type": "text", "text": "Online users may have dynamic short-term inter- ests in reading news articles, which may be inﬂu- enced by speciﬁc contexts or temporal information demands. For example, if a user just reads a news article about “Mission: Impossible 6 – Fallout”, and she may want to know more about the actor “Tom Cruise” in this movie and click news arti- cles related to “Tom Cruise”, although she is not his fan and may never read his news before. We propose to learn the short-term representations of users from their recent browsing history to capture their temporal interests, and use gated recurrent networks (GRU) ( Cho et al. ,  2014 ) network to cap- ture the sequential news reading patterns ( Okura et al. ,  2017 ). Denote news browsing sequence from a user sorted by timestamp in ascending or- der as    ${\\mathcal{C}}=\\{c_{1},c_{2},.\\,.\\,.\\,,c_{k}\\}$  , where  $k$   is the length of this sequence. We apply the news encoder to obtain the representations of these browsed arti- cles, denoted as    $\\{e_{1},e_{2},.\\,.\\,.\\,,e_{k}\\}$  . The short-term user representation is computed as follows: ", "page_idx": 3, "bbox": [71, 549.6481323242188, 290, 766.031494140625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 37, "type": "text", "text": "", "page_idx": 3, "bbox": [307, 247, 525, 314.7306213378906], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 38, "type": "equation", "text": "\n$$\n\\begin{array}{r l}&{\\boldsymbol{r}_{t}=\\sigma\\big(\\boldsymbol{W}_{r}[\\boldsymbol{h}_{t-1},\\boldsymbol{e}_{t}]\\big),}\\\\ &{\\boldsymbol{z}_{t}=\\sigma\\big(\\boldsymbol{W}_{z}[\\boldsymbol{h}_{t-1},\\boldsymbol{e}_{t}]\\big),}\\\\ &{\\tilde{\\boldsymbol{h}}_{t}=\\operatorname{tanh}\\bigl(\\boldsymbol{W}_{\\tilde{h}}[\\boldsymbol{r}_{t}\\odot\\boldsymbol{h}_{t-1},\\boldsymbol{e}_{t}]\\bigr),}\\\\ &{\\boldsymbol{h}_{t}=\\boldsymbol{z}_{t}\\odot\\boldsymbol{h}_{t}+({\\bf1}-\\boldsymbol{z}_{t})\\odot\\tilde{\\boldsymbol{h}}_{t},}\\end{array}\n$$\n ", "text_format": "latex", "page_idx": 3, "bbox": [342, 323, 488, 395], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 39, "type": "text", "text": "where    $\\sigma$   is the sigmoid function,    $\\odot$  is the item- wise product,    $W_{r},\\,W_{z}$   and    $W_{\\tilde{h}}$    are the param- eters of the GRU network. The short-term user representation is the last hidden state of the GRU network, i.e.,    ${\\pmb u}_{s}={\\pmb h}_{k}$  . ", "page_idx": 3, "bbox": [307, 404.12310791015625, 525, 471.4656066894531], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 40, "type": "text", "text": "3.2.2 Long-Term User Representations ", "text_level": 1, "page_idx": 3, "bbox": [307, 481, 496, 494], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 41, "type": "text", "text": "Besides the temporal interests, online users may also have long-term interests in reading news. For example, a basketball fan may tend to browse many sports news related to NBA in several years. Thus, we propose to learn long-term representa- tions of users to capture their consistent prefer- ences. In our approach the long-term user repre- sentations are learned from the embeddings of the user IDs, which are randomly initialized and ﬁne- tuned during model training. Denote  $u$   as the ID of a user and    $W_{u}$   as the look-up table for long-term user representation, the long-term user representa- tion of this user is    $\\pmb{u}_{l}=\\pmb{W}_{u}[u]$  . ", "page_idx": 3, "bbox": [307, 497.2091064453125, 525, 673], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 42, "type": "text", "text": "3.2.3 Long- and Short-Term User Representation ", "text_level": 1, "page_idx": 3, "bbox": [306, 681, 470, 708], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 43, "type": "text", "text": "In this section, we introduce two methods to com- bine the long-term and short-term user presenta- tions for uniﬁed user representation, which are shown in Fig.  3 . ", "page_idx": 3, "bbox": [307, 712.2380981445312, 525, 766.0315551757812], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 44, "type": "text", "text": "The ﬁrst method is using the long-term user representation to initialize the hidden state of the GRU network in the short-term user representation model, as shown in Fig.  3a . We denote this method as LSTUR-ini. We use the last hidden state of the GRU network as the ﬁnal user representation. The second method is concatenating the long-term user representation with the short-term user represen- tation as the ﬁnal user representation, as shown in Fig.  3b . We denote this method as LSTUR-con. ", "page_idx": 4, "bbox": [71, 63.68701934814453, 290, 198.77554321289062], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 45, "type": "text", "text": "3.3 Model Training ", "text_level": 1, "page_idx": 4, "bbox": [72, 211, 170, 224], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 46, "type": "text", "text": "For online news recommendation services where user and news representations can be computed in advance, the scoring function should be as simple as possible to reduce latency. Motivated by ( Okura et al. ,  2017 ), we use the simple dot production to compute the news click probability score. Denote the representation of a user    $u$   as  $\\mathbfcal{U}$   and the repre- sentation of a candidate news article    $e_{x}$   as    $e_{x}$  , the probability score    $s(u,c_{x})$   of this user clicking this news is computed as  $\\boldsymbol{s}(u,c_{x})=\\mathbf{u}^{\\top}\\boldsymbol{e}_{x}$  . ", "page_idx": 4, "bbox": [71, 230.8550567626953, 290, 366], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 47, "type": "text", "text": "Motivated by ( Huang et al. ,  2013 ) and ( Zhai et al. ,  2016 ), we propose to use the negative sam- pling technique for model training. For each news browsed by a user (regarded as a positive sam- ple), we randomly sample    $K$   news articles from the same impression which are not clicked by this user as negative samples. Our model will jointly predict the click probability scores of the positive news and the    $K$   negative news. In this way, the news click prediction problem is reformulated as a pseudo    $K+1$  -way classiﬁcation task. We mini- mize the summation of the negative log-likelihood of all positive samples during training, which can be formulated as follows: ", "page_idx": 4, "bbox": [71, 367.33209228515625, 290, 556.6166381835938], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 48, "type": "equation", "text": "\n$$\n-\\sum_{i=1}^{P}\\log\\frac{\\exp(s(u,c_{i}^{p}))}{\\exp(s(u,c_{i}^{p}))+\\sum_{k=1}^{K}\\exp(s(u,c_{i,k}^{n}))},\n$$\n ", "text_format": "latex", "page_idx": 4, "bbox": [69, 565, 290, 604], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 49, "type": "text", "text": "where    $P$   is the number of positive training sam- ples, and    $c_{i,k}^{n}$    is the    $k$  -th negative sample in the same session with the  $i$  -th positive sample. ", "page_idx": 4, "bbox": [71, 629.958984375, 290, 670.202392578125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 50, "type": "text", "text": "Since not all users can be incorporated in news recommendation model training (e.g., the new coming users), it is not appropriate to assume all users have long-term representations in our mod- els in the prediction stage. In order to handle this problem, in the model training stage, we randomly mask the long-term representations of users with a certain probability    $p$  . When we mask the long- term representations, all the dimensions are set to zero. Thus, the long-term user representation in our LSTUR approach can be reformulated as: ", "page_idx": 4, "bbox": [71, 671.5910034179688, 290, 766.0313720703125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 51, "type": "text", "text": "", "page_idx": 4, "bbox": [307, 63.68695831298828, 525, 117.48043823242188], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 52, "type": "equation", "text": "\n$$\n{\\mathbfit{u}}_{l}=M\\cdot W_{u}[u],M\\sim B(1,1-p),\n$$\n ", "text_format": "latex", "page_idx": 4, "bbox": [334, 128, 498, 143], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 53, "type": "text", "text": "where  $B$   is Bernoulli distribution, and    $M$   is a ran- dom variable that subject to    $B(1,1{-}p)$  . We ﬁnd in experiments that this trick for model training can improve the performance of our approach. ", "page_idx": 4, "bbox": [307, 154.62599182128906, 525, 208.41848754882812], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 54, "type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 4, "bbox": [307, 219, 391, 233], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 55, "type": "text", "text": "4.1 Dataset and Experimental Settings ", "text_level": 1, "page_idx": 4, "bbox": [308, 242, 494, 254], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 56, "type": "text", "text": "Since there is no off-the-shelf dataset for news rec- ommendation, we built one by ourselves through collecting logs from MSN News 3   in four weeks from December 23rd, 2018 to January 19th, 2019. We used the logs in the ﬁrst three weeks for model training, and those in the last week for test. We also randomly sampled   $10\\%$   of logs from the train- ing set as the validation data. For each sample, we collected the browsing history in last 7 days to learn short-term user representations. The detailed dataset statistics are summarized in Table  1 . ", "page_idx": 4, "bbox": [307, 259.113037109375, 525, 407.7505798339844], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 57, "type": "table", "page_idx": 4, "img_path": "layout_images/P19-1033_3.jpg", "table_caption": "Table 1: Statistics of the dataset in our experiments. ", "bbox": [308, 417, 525, 481], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "#of users | 25,000 | # of users in training set 22,938\n#of news | 38,501 | Avg. # of words per title 9.98\n\n# of imprs | 393,191 # of positive samples 492,185\nNP ratio? 18.74 # of negative samples 9,224,537\n\n", "vlm_text": "The table presents various statistics related to a dataset, presumably for a machine learning or data analysis context:\n\n- **# of users**: There are 25,000 users in total.\n- **# of news**: The dataset includes 38,501 news articles.\n- **# of imprs (impressions)**: There are 393,191 impressions recorded in the dataset.\n- **NP ratio**: The ratio of negative to positive samples is 18.74.\n- **# of users in training set**: There are 22,938 users in the training set.\n- **Avg. # of words per title**: On average, each title contains 9.98 words.\n- **# of positive samples**: There are 492,185 positive samples in the dataset.\n- **# of negative samples**: The dataset contains 9,224,537 negative samples."}
{"layout": 58, "type": "text", "text": "In our experiments, we used the pretrained GloVe embedding 5   ( Pennington et al. ,  2014 ) as the initialization of word embeddings. The word em- bedding dimension is 200. The number of ﬁlters in CNN network is 300, and the window size of the ﬁlters in CNN network is set to 3. We applied dropout ( Srivastava et al. ,  2014 ) to each layer in our approach to mitigate overﬁtting. The dropout rate is 0.2. The default value of long-term user rep- resentation masking probability    $p$   for model train- ing is 0.5. We used Adam ( Kingma and Ba ,  2014 ) to optimize the model, and the learning rate was 0 . 01 . The batch size is set to 400. The number of negative samples for each positive sample is 4. These hyper-parameters were all selected ac- cording to the results on validation set. We used impression-based ranking metrics to evaluate the performance, including area under the ROC curve (AUC), mean reciprocal rank (MRR), and nor- malized discounted cumulative gain (nDCG). We repeated each experiment for 10 times indepen- dently, and reported the average results with 0.95 conﬁdence probability. ", "page_idx": 4, "bbox": [307, 497.87298583984375, 525, 714.2564697265625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 59, "type": "text", "text": "", "page_idx": 5, "bbox": [72, 63.68701934814453, 290, 158.12753295898438], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 60, "type": "text", "text": "4.2 Performance Evaluation ", "text_level": 1, "page_idx": 5, "bbox": [71, 167, 212, 179], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 61, "type": "text", "text": "We evaluate the performance of our approach by comparing it with several baseline methods, in- cluding: ", "page_idx": 5, "bbox": [72, 184.3190460205078, 290, 224.56253051757812], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 62, "type": "text", "text": "•  LibFM  ( Rendle ,  2012 ), a state-of-the-art ma- trix factorization method which is widely used in recommendation. In our experiments, the user features are the concatenation of TF-IDF features extracted from the browsed news titles, and the normalized count features from the topics and subtopics of the browsed news. The features for news consists of TF- IDF features from its title, and one-hot vec- tors of its topic and subtopic. The input to LibFM  is the concatenation of user features and features of candidate news. •  DeepFM  ( Guo et al. ,  2017 ), a widely used method that combines factorization machines and deep neural networks. We use the same features as  LibFM . •  Wide & Deep  ( Cheng et al. ,  2016 ), another deep learning based recommendation method that combines a wide channel and a deep channel. Again, the same features with LibFM  are used for both channels. •  DSSM  ( Huang et al. ,  2013 ), deep structured semantic model. The inputs are hashed words via character trigram, where all the browsed news titles are merged as query document. •  CNN  ( Kim ,  2014 ), using CNN with max pooling to learn news representations from the titles of browsed news by keeping the most salient features. •  DKN  ( Wang et al. ,  2018 ), a deep news rec- ommendation model which contains CNN and candidate-aware attention on the news browsing histories. •  GRU  ( Okura et al. ,  2017 ), learning news rep- resentations by a denoising autoencoder and user representations by a GRU network. ", "page_idx": 5, "bbox": [85, 231.2660675048828, 290, 766.0316162109375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 63, "type": "text", "text": "The results of comparing different methods are summarized in Table  2 . ", "page_idx": 5, "bbox": [307, 63.68720245361328, 525, 90.38168334960938], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 64, "type": "text", "text": "We have obtained observations from Table  2 . First, the news recommendation methods (e.g. CNN ,  DKN  and  LSTUR ) which use neural net- works to learn news and user representations can signiﬁcantly outperform the methods using man- ual feature engineering (e.g. LibFM ,  DeepFM , Wide & Deep , and  DSSM ). This is probably be- cause handcrafted features are usually not optimal, and neural networks can capture both global and local semantic contexts in news, which are useful for learning more accurate news and user repre- sentations for news recommendation. ", "page_idx": 5, "bbox": [307, 92.26819610595703, 525, 254.45474243164062], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 65, "type": "text", "text": "Second, our LSTUR approach outperforms all baseline methods compared here, including deep learning models such as  CNN ,  GRU  and  DKN . Our LSTUR approach can capture both the long-term preferences and short-term interests to capture the complex and diverse user interests in news read- ing, while the baseline methods only learn a single representation for each user, which is insufﬁcient. In addition, our LSTUR approach uses attention mechanism in the news encoder to select impor- tant words, which can help learn more informative news representations. ", "page_idx": 5, "bbox": [307, 256.34027099609375, 525, 418.5278015136719], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 66, "type": "text", "text": "Third, our proposed two methods to learn long- and short-term user representations, i.e., LSTUR- ini and LSTUR-con, can achieve comparable per- formance and both outperform baseline methods, which validate the effectiveness of these meth- ods. In addtion, the performance of LSTUR-con is more stable than LSTUR-ini, which indicates that using the concatenation of both short-term and long-term user representations is capable of retaining all the information. We also conducted experiments to explore the performance of com- bining both LSTUR-con and LSTUR-ini in the same model, but the performance improvement is very limited, implying that each of them can fully capture the long- and short-term user interests for news recommendation. ", "page_idx": 5, "bbox": [307, 420.413330078125, 525, 636.796875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 67, "type": "text", "text": "4.3 Effectiveness of Long- and Short-Term User Representation ", "text_level": 1, "page_idx": 5, "bbox": [307, 651, 513, 677], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 68, "type": "text", "text": "In this section, we conducted several experiments to explore the effectiveness of our approach in learning both long-term and short-term user rep- resentations. We compare the performance of our LSTUR  methods with the long-term user represen- tation model LTUR and the short-term user rep- ", "page_idx": 5, "bbox": [307, 685.140380859375, 525, 766.0308227539062], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 69, "type": "table", "page_idx": 6, "img_path": "layout_images/P19-1033_4.jpg", "bbox": [123, 61, 474, 208], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "Methods AUC MRR nDCG@5 | nDCG@10\nLibFM | 56.52 £ 1.31 | 25.53+0.81 | 26.66 + 1.04 | 34.72 + 0.95\nDeepFM | 58.13 + 1.69 | 27.01 + 0.20 | 28.37 + 0.57 | 36.78 + 0.62\nWide & Deep | 58.07 + 0.55 | 27.07 + 0.37 | 28.51 £0.45 | 36.93 £0.43\nDSSM 58.43 + 0.58 | 27.25 + 0.49 | 28.31 + 0.60 | 36.91 + 0.54\nCNN 61.13 £0.77 | 29.44 + 0.73 | 31.44 + 0.87 | 39.51 £0.74\nDKN 61.25 + 0.78 | 29.47 + 0.64 | 31.54+0.79 | 39.59 + 0.67\nGRU 62.69 + 0.16 | 30.24 + 0.13 | 32.5640.17 | 40.55 £0.13\nLSTUR-con | 63.47 +0.10 | 30.94+0.14 | 33.43+0.13 | 41.34+40.13\nLSTUR-ini | 63.56 + 0.42 | 30.98 + 0.32 | 33.45 + 0.39 | 41.37 + 0.36\n\n", "vlm_text": "The table compares the performance of various methods using four different evaluation metrics: AUC (Area Under the Curve), MRR (Mean Reciprocal Rank), nDCG@5 (Normalized Discounted Cumulative Gain at 5), and nDCG@10 (Normalized Discounted Cumulative Gain at 10). Each method has a corresponding value for each metric, expressed as a mean ± standard deviation. The methods listed are:\n\n1. LibFM\n2. DeepFM\n3. Wide & Deep\n4. DSSM\n5. CNN\n6. DKN\n7. GRU\n8. LSTUR-con\n9. LSTUR-ini\n\nLSTUR-ini has the highest values in all four metrics, suggesting it performs better than the other methods according to these evaluation criteria."}
{"layout": 70, "type": "image", "page_idx": 6, "img_path": "layout_images/P19-1033_5.jpg", "img_caption": "Figure 4: The effectiveness of incorporating long-tern Figure 5: The comparisons of different methods in user representations (LTUR) and short-term user rep- learning short-term user representations from recently resentations (STUR). browsed news articles. ", "bbox": [29, 240, 544, 401], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "0.640\n0.635\n0.630\n0.625\n0.620\n0.615\n0.610\n0.605\n0.600\n\nLTUR\n\n0.420\n0.415\n0.410\n0.405\n0.400\n0.395\n0.390\n\n0.638\n0.636\n0.634\n0.632\n0.630\n0.628\n0.626\n\nAverage\nAttention\nGg LSTM\n=\n\n1\n\nnDCG@10\n\n0.418\n0.416\n0.414\n0.412\n0.410\n0.408\n0.406\n", "vlm_text": "The image consists of two bar charts comparing different methods for representing user data in the context of a study on user representations. \n\nThe chart on the left assesses the effectiveness of incorporating long-term and short-term user representations: \n- It evaluates the methods labeled LTUR, STUR, LSTUR-con, and LSTUR-ini using two metrics: AUC and nDCG@10. \n- Each method's performance is represented by bars of different colors: yellow (LTUR), light green (STUR), medium green (LSTUR-con), and dark green (LSTUR-ini).\n- According to the bar chart, the LSTUR-ini method shows the highest performance in both metrics, followed by LSTUR-con, STUR, and LTUR.\n\nThe chart on the right compares different methods for learning short-term user representations from recently browsed news articles:\n- The methods compared include Average, Attention, LSTM, and GRU.\n- The evaluation is again based on AUC and nDCG@10 metrics, with each method represented by different colored bars: yellow (Average), light green (Attention), medium green (LSTM), and dark green (GRU).\n- The GRU method (dark green bar) outperforms others in both AUC and nDCG@10, followed by LSTM, Attention, and then Average.\n\nOverall, both charts are focused on evaluating the effectiveness of various methods for generating user representations using AUC and nDCG@10 as performance metrics."}
{"layout": 71, "type": "text", "text": "resentation model STUR. The results are summa- rized in Fig.  4 . ", "page_idx": 6, "bbox": [71, 422.0149841308594, 290, 448.70947265625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 72, "type": "text", "text": "From the results we ﬁnd both LTUR and STUR are useful for news recommendation, and the STUR model can outperform the LTUR model. According to the statistics in Table  1 , the long- term representations of many users in test data are unavailable, which leads to relative weak per- formance of LTUR on these users. In addition, combining STUR and LTUR using our two long- and short-term user representation methods, i.e., LSTUR-ini and LSTUR-con, can effectively im- prove the performance. This result validates that incorporating both long-term and short-term user representations is useful to capture the diverse user interests more accurately and is beneﬁcial for news recommendation. ", "page_idx": 6, "bbox": [71, 450.11798095703125, 290, 652.9514770507812], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 73, "type": "text", "text": "4.4 Effectiveness of News Encoders in STUR ", "text_level": 1, "page_idx": 6, "bbox": [71, 666, 288, 678], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 74, "type": "text", "text": "In our STUR model, GRU is used to learn short- term user representations from the recent browsing news. We explore the effectiveness of GRU in en- coding news by replacing it with several other en- coders, including: 1) Average: using the average of all the news representations in recent browsing history; 2) Attention: the summation of news rep- resentations weighted by their attention weights; 3) LSTM ( Hochreiter and Schmidhuber ,  1997 ), re- placing GRU with LSTM. The results are summa- rized in Fig.  5 . ", "page_idx": 6, "bbox": [71, 685.1400756835938, 290, 766.031494140625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 75, "type": "text", "text": "", "page_idx": 6, "bbox": [307, 422.0150146484375, 525, 489.3575134277344], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 76, "type": "text", "text": "According to Fig.  5 , the sequence-based en- coders (e.g., GRU, LSTM) outperform the Aver- age and Attention based encoders. This is proba- bly because the sequence-based encoders can cap- ture the sequential new reading patterns to learn short-term representations of users, which is difﬁ- cult for Average and Attention based encoders. In addition, GRU achieves better performance than LSTM. This may be because GRU contains fewer parameters and has lower risk of overﬁtting . Thus, we select GRU as the news encoder in STUR. ", "page_idx": 6, "bbox": [307, 490.7650451660156, 525, 639.4025268554688], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 77, "type": "text", "text": "4.5 Effectiveness of News Title Encoders ", "text_level": 1, "page_idx": 6, "bbox": [307, 652, 504, 664], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 78, "type": "text", "text": "In this section, we conduct experiments to com- pare different news title encoders. In our ap- proach, the news encoder is a combination of CNN network and an attention network (denoted as   $\\mathbf{CNN+Aut}$  ). We compare it with several vari- ants, i.e., CNN, LSTM, and LSTM with attention  $(\\mathrm{LSTM+Aut})$  , to validate the effectiveness of our ", "page_idx": 6, "bbox": [307, 671.5911254882812, 525, 766.031494140625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 79, "type": "image", "page_idx": 7, "img_path": "layout_images/P19-1033_6.jpg", "img_caption": "Figure 6: The comparisons of different methods in learning news title representations and the effectiveness of attention machenism in selecting important words. ", "bbox": [70, 62, 528, 243], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "0.640\n0.635\n0.630\n0.625\n0.620\n0.615\n\nLSTM\nLSTM+Att\nG3: CNN\n— rAtt\nLSTUR-ini LSTUR-con\n(a) AUC\n\n0.420\n0.415\n0.410\n0.405\n0.400\n0.395\n\nLSTM\nLSTM+Att\n\nGa CNN\n\nGm CNN\n(ames)\n\nLSTUR-ini\n(b) nDCG@ 10\n\nLSTUR-con\n\n", "vlm_text": "The image is composed of two bar charts comparing different methods for learning news title representations. Here's a breakdown:\n\n1. **Methods Compared**:\n   - LSTM\n   - LSTM with Attention (LSTM+Att)\n   - CNN\n   - CNN with Attention (CNN+Att)\n\n2. **Metrics Used**:\n   - (a) AUC (Area Under the Curve)\n   - (b) nDCG@10 (Normalized Discounted Cumulative Gain)\n\n3. **Comparison**:\n   - **LSTUR-ini** and **LSTUR-con** are the two conditions or settings being compared across both metrics.\n   - For each condition, the performance of the methods is shown with bars of different colors, representing the performance differences and the impact of using the attention mechanism.\n\nThe charts illustrate that the methods incorporating attention generally perform better in both metrics."}
{"layout": 80, "type": "image", "page_idx": 7, "img_path": "layout_images/P19-1033_7.jpg", "img_caption": "Figure 7: The effectiveness of incorporating news topic and subtopic information for news recommendation. ", "bbox": [81, 255, 517, 424], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "0.640 0.420\n\n[1 None [7 None\n0.638 3 +Topic 0.418 3) +Topic\n0.636 (3) _+Subtopic 0.416 G3 +Subtopic\n\nam: GB +Both\n\nLSTUR-ini LSTUR-con LSTUR-ini LSTUR-con\n\n(a) AUC (b) nDCG@10\n", "vlm_text": "The image consists of two bar charts showing the effectiveness of incorporating news topic and subtopic information into news recommendation systems. There are two conditions being compared: \"LSTUR-ini\" and \"LSTUR-con.\"\n\n1. The left chart, labeled \"(a) AUC,\" compares the Area Under the Curve (AUC) metric for four cases: \n\n   - Yellow bar: None (no additional topic or subtopic information)\n   - Light green bar: +Topic (with topic information)\n   - Dark green bar: +Subtopic (with subtopic information)\n   - Teal bar: +Both (with both topic and subtopic information)\n\n   For both LSTUR-ini and LSTUR-con, incorporating topic and/or subtopic information improves the AUC, with the combination of both (\"+Both\") resulting in the highest AUC values.\n\n2. The right chart, labeled \"(b) nDCG@10,\" compares the normalized Discounted Cumulative Gain at rank 10 (nDCG@10) for the same four cases. Similar to the AUC chart, the incorporation of topic and/or subtopic information improves the nDCG@10 scores, with \"+Both\" again showing the highest values for both LSTUR-ini and LSTUR-con.\n\nIn summary, adding topic and subtopic information enhances the performance metrics (AUC and nDCG@10) for news recommendation, with the greatest enhancement observed when both pieces of information are incorporated."}
{"layout": 81, "type": "text", "text": "approach. The results are summarized in Fig.  6 . ", "page_idx": 7, "bbox": [71, 444.91400146484375, 280.49468994140625, 458.0594787597656], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 82, "type": "text", "text": "According to Fig.  6 , using attention mechanism in both encoders based on CNN and LSTM can achieve better performance. This is probably be- cause the attention network can select important words, which can learn more informative news representations. In addition, encoders using CNN outperform those using LSTM. This may be be- cause local contexts in news titles are more impor- tant for learning news representations. ", "page_idx": 7, "bbox": [71, 458.4729919433594, 290, 580.012451171875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 83, "type": "text", "text": "4.5.1 Effectiveness of News Topic ", "text_level": 1, "page_idx": 7, "bbox": [71, 588, 234, 601], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 84, "type": "text", "text": "In this section, we conduct experiments to vali- date the effectiveness of incorporating topic and subtopic of news in the news encoder. We com- pare the performance of our approach with its vari- ants without topic and/or subtopics. The results are shown in Fig.  7 . ", "page_idx": 7, "bbox": [71, 603.8350219726562, 290, 684.7264404296875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 85, "type": "text", "text": "According to Fig.  7 , incorporating either top- ics or subtopics can effectively improve the per- formance of our approach. In addition, the news encoder with subtopics outperforms the news en- coder with topics. This is probably because subtopics can provide more ﬁne-grained topic in- formation which is more helpful for news rec- ommendation. Thus, the model with subtopics can achieve better news recommendation perfor- mance. Moreover, combining topics and subtopics can further improve the performance of our ap- proach. These results validate the effectiveness of our approach in exploiting topic information for news recommendation. ", "page_idx": 7, "bbox": [71, 685.1400146484375, 290, 766.031494140625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 86, "type": "text", "text": "", "page_idx": 7, "bbox": [307, 444.9140319824219, 525, 552.904541015625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 87, "type": "text", "text": "4.5.2 Inﬂuence of Masking Probability ", "text_level": 1, "page_idx": 7, "bbox": [306, 561, 494, 574], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 88, "type": "text", "text": "In this section, we explore the inﬂuence of the probability    $p$   in Eq. ( 6 ) for randomly masking long-term user representation in model training. We vary the value of  $p$   from 0.0 to 0.9 with a step of 0.1 for both LSTUR-ini and LSTUR-con. The results are summarized in Fig.  8 . ", "page_idx": 7, "bbox": [307, 576.7350463867188, 525, 657.62646484375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 89, "type": "text", "text": "According to Fig.  8 , the results of LSTUR-ini and LSTUR-con have similar patterns. The perfor- mance of both methods improves when    $p$   increases from 0. When  $p$   is too small, the model will tend to overﬁt on the LTUR, since LTUR has many pa- rameters. Thus, the performance is not optimal. However, when  $p$   is too large, the performance of both methods starts to decline. This may be be- ", "page_idx": 7, "bbox": [307, 658.041015625, 525, 766.031494140625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 90, "type": "image", "page_idx": 8, "img_path": "layout_images/P19-1033_8.jpg", "img_caption": "Figure 8: The inﬂuence of mask probability  $p$   on the performance of our approach. ", "bbox": [80, 61, 520, 298], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "-e— AUC —®— MRR —e— nDCG@5 —— nDCG@10\n\nAUC —a— MRR —e— nDCG@5 —— nDCG@ 10\n\n64.0 |\n63.0 |\n\n41.5\n40.5\n\n34.0 |\n33.0 |\n\n32.0\n31.0\n30.0\n\n64.0\n63.0\n\n41.5\n40.5\n\n0.0 0.3 0.6\nMask probability p\n\n(a) LSTUR-ini.\n\n34.0\n33.0\n32.0\n31.0\n30.0\n\n0.0 0.3 0.6 0.9\nMask probability p\n\n(b) LSTUR-con.\n\n", "vlm_text": "The image consists of two line charts comparing the influence of mask probability \\( p \\) on the performance of two approaches: LSTUR-ini (on the left) and LSTUR-con (on the right). \n\nEach chart includes four metrics:\n\n- **AUC**: Area under the curve, shown with green circles.\n- **MRR**: Mean reciprocal rank, shown with orange squares.\n- **nDCG@5**: Normalized discounted cumulative gain at rank 5, shown with blue circles.\n- **nDCG@10**: Normalized discounted cumulative gain at rank 10, shown with red stars.\n\nThe x-axis represents the mask probability \\( p \\) ranging from 0.0 to 0.9. The y-axis represents percentage values for each metric. Both charts show how these metrics change as the mask probability increases."}
{"layout": 91, "type": "text", "text": "2019 CES Highlights  $:$   Innovations in Enviro-Sensing for Robocars California dries off after storm batter state for days 15 Recipes Inspired By Vintage Movies Texas State Rep . Dennis Bonnen Elected As House Speaker Should You Buy American Express Stock After Earnings ? How Meghan Markle Has Changed Prince Harry Considerably ", "page_idx": 8, "bbox": [78, 323.985107421875, 284, 379.04718017578125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 92, "type": "text", "text": "Figure 9: Visualization of the word-level attentions. ", "page_idx": 8, "bbox": [78, 389.28656005859375, 284, 401.29150390625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 93, "type": "text", "text": "cause the useful information in LTUR cannot be effectively incorporated. Thus, the performance is also not optimal. A moderate choice on    $p$   (e.g., 0.5) is most appropriate for both LSTUR-ini and LSTUR-con methods, which can properly balance the learning of LTUR and STUR. ", "page_idx": 8, "bbox": [71, 429.9059753417969, 290, 510.7974853515625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 94, "type": "text", "text": "5 Visualization of Attention Weights ", "text_level": 1, "page_idx": 8, "bbox": [71, 533, 266, 547], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 95, "type": "text", "text": "In this section, we visually explore the effective- ness of the word-level attention network in the news encoder. The attention weights in several example news titles are shown in Fig.  9 . From the results, we ﬁnd our approach can effectively recognize important words to learn more infor- mative news representations. For example, the words “Enviro-Sensing” and “Robocars” in the ﬁrst news title are assigned high attention weights because these words are indications of news on technologies, while the words “2019” and “for” are assigned low attention weights by our ap- proach since they are less informative. These re- sults validate the effectiveness of the attention net- work in the news encoder. ", "page_idx": 8, "bbox": [71, 563.197998046875, 290, 766.0313720703125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 96, "type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 8, "bbox": [306, 319, 383, 333], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 97, "type": "text", "text": "In this paper, we propose a neural news recom- mendation approach which can learn both long- and short-term user representations. The core of our model is a news encoder and a user encoder. In the news encoder, we learn representations of news from their titles and topic categories, and use an attention network to highlight important words for informative representation learning. In the user encoder, we propose to learn long-term represen- tations of users from the embeddings of their IDs. In addition, we learn short-term representations of users from their recently browsed news via a GRU network. Besides, we propose two methods to fuse long- and short-term user representations, i.e., us- ing long-term user representation to initialize the hidden state of the GRU network in short-term user representation, or concatenating both long- and short-term user representations as a uniﬁed user vector. Extensive experiments on a real-world dataset collected from MSN news show our ap- proach can effecitively improve the performance of news recommendation. ", "page_idx": 8, "bbox": [307, 341.2719421386719, 525, 638.9505004882812], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 98, "type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 8, "bbox": [307, 650, 401, 663], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 99, "type": "text", "text": "The authors would like to thank Microsoft News for providing technical support and data in the ex- periments, and Jiun-Hung Chen (Microsoft News) and Ying Qiao (Microsoft News) for their support and discussions. We also want to thank Jianqiang Huang for his help in the experiments. ", "page_idx": 8, "bbox": [307, 671.591064453125, 525, 752.4824829101562], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 100, "type": "text", "text": "References ", "text_level": 1, "page_idx": 9, "bbox": [71, 64, 128, 75], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 101, "type": "text", "text": "Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben- gio. 2015. Neural machine translation by jointly learning to align and translate . In  ICLR . Trapit Bansal, Mrinal Das, and Chiranjib Bhat- tacharyya. 2015. Content driven user proﬁling for comment-worthy recommendations of news and blog articles . In  RecSys , pages 195–202. David M Blei, Andrew   $\\mathrm{~Y~Ng~}$  , and Michael I Jordan. 2003.  Latent dirichlet allocation .  Journal of Ma- chine Learning Research , 3(Jan):993–1022. Heng-Tze Cheng, Mustafa Ispir, Rohan Anil, Zakaria Haque, Lichan Hong, Vihan Jain, Xiaobing Liu, Hemal Shah, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen An- derson, Greg Corrado, and Wei Chai. 2016.  Wide & deep learning for recommender systems . In  DLRS , pages 7–10. Kyunghyun Cho, Bart van Merrienboer, Caglar Gul- cehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using RNN encoder–decoder for statistical machine translation . In  EMNLP , pages 1724–1734. Abhinandan S. Das, Mayur Datar, Ashutosh Garg, and Shyam Rajaram. 2007.  Google news person- alization: scalable online collaborative ﬁltering . In WWW , pages 271–280. Huifeng Guo, Ruiming TANG, Yunming Ye, Zhen- guo Li, and Xiuqiang He. 2017. DeepFM: A factorization-machine based neural network for CTR prediction . In  IJCAI , pages 1725–1731. Sepp Hochreiter and J¨ urgen Schmidhuber. 1997. Long short-term memory . Neural Computation , 9(8):1735–1780. Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Acero, and Larry Heck. 2013.  Learning deep structured semantic models for web search using clickthrough data . In  CIKM , pages 2333–2338. Yoon Kim. 2014.  Convolutional neural networks for sentence classiﬁcation . In  EMNLP , pages 1746– 1751. Diederik P Kingma and Jimmy Ba. 2014.  Adam: A method for stochastic optimization .  arXiv preprint arXiv:1412.6980.Talia Lavie, Michal Sela, Ilit Oppenheim, Ohad Inbar, and Joachim Meyer. 2010.  User attitudes towards news content personalization .  International Journal of Human-Computer Studies , 68(8):483–495. Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. 2015.  Deep learning .  Nature , 521(7553):436–444. ", "page_idx": 9, "bbox": [72, 83.5025634765625, 290, 765.7651977539062], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 102, "type": "text", "text": "Lei Li, Li Zheng, Fan Yang, and Tao Li. 2014.  Model- ing and broadening temporal user interest in person- alized news recommendation .  Expert Systems with Applications , 41(7):3168–3177. Lihong Li, Wei Chu, John Langford, and Robert E. Schapire. 2010. A contextual-bandit approach to personalized news article recommendation . In WWW , pages 661–670. Jianxun Lian, Fuzheng Zhang, Xing Xie, and Guangzhong Sun. 2018.  Towards better represen- tation learning for personalized news recommenda- tion: a multi-channel deep fusion approach . In  IJ- CAI , pages 3805–3811. Jiahui Liu, Peter Dolan, and Elin Rønby Pedersen. 2010.  Personalized news recommendation based on click behavior . In  IUI , pages 31–40. Shumpei Okura, Yukihiro Tagami, Shingo Ono, and Akira Tajima. 2017. Embedding-based news rec- ommendation for millions of users . In  KDD , pages 1933–1942. Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. Glove: Global vectors for word representation . In  EMNLP , pages 1532–1543. Owen Phelan, Kevin McCarthy, and Barry Smyth. 2009.  Using twitter to recommend real-time topical news . In  RecSys , pages 385–388. Steffen Rendle. 2012. Factorization machines with libFM . ACM Transactions on Intelligent Systems and Technology , 3(3):1–22. Jeong-Woo Son, A-Yeong Kim, and Seong-Bae Park. 2013.  A location-based news article recommenda- tion with explicit localized semantic analysis . In  SI- GIR , pages 293–302. Nitish Srivastava, Geoffrey E. Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi- nov. 2014.  Dropout: a simple way to prevent neural networks from overﬁtting . Journal of Machine Learning Research , 15(1):1929–1958. Hongwei Wang, Fuzheng Zhang, Xing Xie, and Minyi Guo. 2018.  DKN: Deep knowledge-aware network for news recommendation . In  WWW , pages 1835– 1844. Chuhan Wu, Fangzhao Wu, Mingxiao An, Jianqiang Huang, Yongfeng Huang, and Xing Xie. 2019. NPA: Neural news recommendation with personal- ized attention. In  KDD . Shuangfei Zhai, Keng hao Chang, Ruofei Zhang, and Zhongfei Mark Zhang. 2016.  Deepintent: Learning attentions for online advertising with recurrent neu- ral networks . In  KDD , pages 1295–1304. Guanjie Zheng, Fuzheng Zhang, Zihan Zheng, Yang Xiang, Nicholas Jing Yuan, Xing Xie, and Zhen- hui Li. 2018. DRN: A deep reinforcement learn- ing framework for news recommendation . In  WWW , pages 167–176. ", "page_idx": 9, "bbox": [307, 64.561279296875, 525, 759.103759765625], "page_size": [595.2760009765625, 841.8900146484375]}
