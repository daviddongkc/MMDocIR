{"page": 0, "image_path": "doc_images/P19-1041_0.jpg", "ocr_text": "Disentangled Representation Learning for\nNon-Parallel Text Style Transfer\n\nVineet John, Lili Mou, Hareesh Bahuleyan, Olga Vechtomova\nUniversity of Waterloo\n{vineet. john, hpallika, ovechtom}@uwaterloo.ca\ndoublepower.mou@gmail.com\n\nAbstract\n\nThis paper tackles the problem of disentan-\ngling the latent representations of style and\ncontent in language models. We propose a\nsimple yet effective approach, which incorpo-\nrates auxiliary multi-task and adversarial ob-\njectives, for style prediction and bag-of-words\nprediction, respectively. We show, both qual-\nitatively and quantitatively, that the style and\ncontent are indeed disentangled in the latent\nspace. This disentangled latent representation\nlearning can be applied to style transfer on\nnon-parallel corpora. We achieve high perfor-\nmance in terms of transfer accuracy, content\npreservation, and language fluency, in compar-\nison to various previous approaches.!\n\n1 Introduction\n\nThe neural network has been a successful learning\nmachine during the past decade due to its highly\nexpressive modeling capability, which is a conse-\nquence of multiple layers of non-linear transfor-\nmations of input features. Such transformations,\nhowever, make intermediate features “latent,” in\nthe sense that they do not have explicit meaning\nand are not interpretable. Therefore, neural net-\nworks are usually treated as black-box machinery.\n\nDisentangling the latent space of neural net-\nworks has become an increasingly important re-\nsearch topic. In the image domain, for example,\nChen et al. (2016) use adversarial and information\nmaximization objectives to produce interpretable\nlatent representations that can be tweaked to ad-\njust writing style for handwritten digits, as well as\nlighting and orientation for face models. However,\nthis problem is less explored in natural language\nprocessing.\n\n‘Our code and all model output are — avail-\nable at https://sites.google.com/view/\ndisentangle4transfer.\n\n424\n\nIn this paper, we address the problem of dis-\nentangling the latent space of neural networks for\next generation. Our model is built on an autoen-\ncoder that encodes a sentence to the latent space\n(vector representation) by learning to reconstruct\nhe sentence itself. We would like the latent space\no be disentangled with respect to different fea-\nures, namely, style and content in our task.\n\nTo accomplish this, we propose a simple yet ef-\n‘ective approach that combines multi-task and ad-\nversarial objectives. We artificially divide the la-\nent representation into two parts: the style space\nand content space, where we consider the senti-\nment of a sentence as its style. We design a sys-\nematic set of auxiliary losses, enforcing the sepa-\nration of style and content latent spaces. In partic-\nular, the multi-task loss operates on a latent space\no ensure that the space does contain the infor-\nmation we wish to encode. The adversarial loss,\non the contrary, minimizes the predictability of in-\n‘ormation that should not be contained in a given\nlatent space. In early work, researchers typically\nwork with the style space (Shen et al., 2017; Fu\net al., 2018), but simply ignore the content space,\nas it is hard to formalize what “content” actually\nrefers to. Cycle consistency of back-translation\ndefines content implicitly (Xu et al., 2018), but\nrequires reinforcement learning over the discrete\nsentence space, which could be extremely difficult\nto train.\n\nIn our paper, we propose to approximate the\ncontent information by bag-of-words (BoW) fea-\ntures, where we focus on style-neutral, non-\nstopwords. Along with traditional style-oriented\nauxiliary losses, our BoW multi-task loss and\nBoW adversarial loss enable better disentangle-\nment of the style and content spaces.\n\nThe learned disentangled latent space can be di-\nrectly used for text style transfer, which aims to\ntransform a given sentence to a new sentence with\n\nProceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 424-434\nFlorence, Italy, July 28 - August 2, 2019. ©2019 Association for Computational Linguistics\n", "vlm_text": "Disentangled Representation Learning for Non-Parallel Text Style Transfer \nVineet John, Lili Mou, Hareesh Bahuleyan, Olga Vechtomova University of Waterloo \n{ vineet.john,hpallika,ovechtom } @uwaterloo.ca doublepower.mou@gmail.com \nAbstract \nThis paper tackles the problem of disentan- gling the latent representations of style and content in language models. We propose a simple yet effective approach, which incorpo- rates auxiliary multi-task and adversarial ob- jectives, for style prediction and bag-of-words prediction, respectively. We show, both qual- itatively and quantitatively, that the style and content are indeed disentangled in the latent space. This disentangled latent representation learning can be applied to style transfer on non-parallel corpora. We achieve high perfor- mance in terms of transfer accuracy, content preservation, and language ﬂuency, in compar- ison to various previous approaches. \n1 Introduction \nThe neural network has been a successful learning machine during the past decade due to its highly expressive modeling capability, which is a conse- quence of multiple layers of non-linear transfor- mations of input features. Such transformations, however, make intermediate features “latent,” in the sense that they do not have explicit meaning and are not interpretable. Therefore, neural net- works are usually treated as black-box machinery. \nDisentangling the latent space of neural net- works has become an increasingly important re- search topic. In the image domain, for example, Chen et al.  ( 2016 ) use adversarial and information maximization objectives to produce interpretable latent representations that can be tweaked to ad- just writing style for handwritten digits, as well as lighting and orientation for face models. However, this problem is less explored in natural language processing. \nIn this paper, we address the problem of dis- entangling the latent space of neural networks for text generation. Our model is built on an autoen- coder that encodes a sentence to the latent space (vector representation) by learning to reconstruct the sentence itself. We would like the latent space to be disentangled with respect to different fea- tures, namely,  style  and  content  in our task. \nTo accomplish this, we propose a simple yet ef- fective approach that combines multi-task and ad- versarial objectives. We artiﬁcially divide the la- tent representation into two parts: the style space and content space, where we consider the senti- ment of a sentence as its style. We design a sys- tematic set of auxiliary losses, enforcing the sepa- ration of style and content latent spaces. In partic- ular, the multi-task loss operates on a latent space to ensure that the space does contain the infor- mation we wish to encode. The adversarial loss, on the contrary, minimizes the predictability of in- formation that should not be contained in a given latent space. In early work, researchers typically work with the style space ( Shen et al. ,  2017 ;  Fu et al. ,  2018 ), but simply ignore the content space, as it is hard to formalize what “content” actually refers to. Cycle consistency of back-translation deﬁnes content implicitly ( Xu et al. ,  2018 ), but requires reinforcement learning over the discrete sentence space, which could be extremely difﬁcult to train. \nIn our paper, we propose to approximate the content information by bag-of-words (BoW) fea- tures, where we focus on style-neutral, non- stopwords. Along with traditional style-oriented auxiliary losses, our BoW multi-task loss and BoW adversarial loss enable better disentangle- ment of the style and content spaces. \nThe learned disentangled latent space can be di- rectly used for text style transfer, which aims to transform a given sentence to a new sentence with the same content but a different style. We follow the setting where the model is trained on a non- parallel but style-labeled corpus ( Hu et al. ,  2017 ; Shen et al. ,  2017 ); thus, we call it  non-parallel text style transfer . With our disentangled latent space, we simply use the autoencoder to encode the con- tent vector of a sentence, but ignore its encoded style vector. We then infer from the training data an empirical embedding of the style that we would like to transfer to. The encoded content vector and the empirically-inferred style vector are concate- nated and fed to the decoder. This grafting tech- nique enables us to obtain a new sentence similar in content to the input sentence, but with a differ- ent style. "}
{"page": 1, "image_path": "doc_images/P19-1041_1.jpg", "ocr_text": "the same content but a different style. We follow\nthe setting where the model is trained on a non-\nparallel but style-labeled corpus (Hu et al., 2017;\nShen et al., 2017); thus, we call it non-parallel text\nstyle transfer. With our disentangled latent space,\nwe simply use the autoencoder to encode the con-\ntent vector of a sentence, but ignore its encoded\nstyle vector. We then infer from the training data\nan empirical embedding of the style that we would\nlike to transfer to. The encoded content vector and\nthe empirically-inferred style vector are concate-\nnated and fed to the decoder. This grafting tech-\nnique enables us to obtain a new sentence similar\nin content to the input sentence, but with a differ-\nent style.\n\nWe conducted experiments on two benchmark\ndatasets. Both qualitative and quantitative results\nshow that the style and content spaces are indeed\ndisentangled well. In the style-transfer evaluation,\nwe achieve high performance in style-transfer ac-\ncuracy, content preservation, as well as language\nfluency, compared with previous results. Ablation\ntests also show that all our auxiliary losses can be\ncombined well, each playing its own role in disen-\ntangling the latent space.\n\n2 Related Work\n\nDisentangling neural networks’ latent space has\nbeen explored in computer vision in recent years,\nand researchers have successfully disentangled\nthe features (such as rotation and color) of im-\nages (Chen et al., 2016; Higgins et al., 2017). In\nthese approaches, the disentanglement is purely\nunsupervised, as no style labels are needed. Un-\nfortunately, we have not observed disentangled\nfeatures by applying these approaches in text rep-\nresentations, and thus we require style labels in our\napproach.\n\nStyle-transfer has also been explored in com-\nputer vision. For example, Gatys et al. (2016)\nshow that the artistic style of an image can be cap-\ntured well by certain statistics.\n\nIn NLP, the definition of “style” itself is vague,\nand as a convenient starting point, researchers of-\nten treat sentiment as a salient style attribute. Hu\net al. (2017) propose to control the sentiment by\nusing discriminators to reconstruct sentiment and\ncontent from generated sentences. However, there\nis no evidence that the latent space would be disen-\ntangled by simply reconstructing a sentence. Shen\net al. (2017) use a pair of adversarial discrimina-\n\n425\n\ntors to align the recurrent hidden decoder states\nof original and style-transferred sentences, for a\ngiven style. Fu et al. (2018) propose two ap-\nproaches: training style-specific embeddings and\ntraining separate style-specific decoders. Their\nstyle embeddings are similar to an earlier study by\nstudy by Ficler and Goldberg (2017). Their multi-\ndecoder approach is used by Nogueira dos Santos\net al. (2018), and is extended to private-shared net-\nworks for styled generation (Zhang et al., 2018).\nZhao et al. (2018) also extend the multi-decoder\napproach and use a Wasserstein-distance penalty\nto align content representations of sentences with\ndifferent styles. Tsvetkov et al. (2018) use a\nmachine-translation preprocessing step to strip au-\nthor style from documents, and then use a multi-\ndecoder model to convert the result into a sentence\nwith a specific style.\n\nRecently, cycle consistency of back-translation\nis applied to ensure content preservation (Xu et al.,\n2018; Logeswaran et al., 2018). These methods re-\nquire reinforcement learning and are usually diffi-\ncult to train.\n\nLi et al. (2018) propose a hybrid retrieval and\ngeneration method that transfers the style by re-\ntrieving and incrementally editing a sentence sim-\nilar to the source sentence.\n\nRao and Tetreault (2018) treat the formality of\nwriting as a style, and create a parallel corpus for\nstyle transfer with sequence-to-sequence models.\nThis is beyond the scope of our paper, as we focus\non non-parallel text style transfer.\n\nStyle transfer generation is also related to non-\nparallel machine translation, where researchers\napply similar techniques of adversarial alignment,\nback translation, etc. (Lample et al., 2018a,b; Con-\nneau et al., 2018).\n\nOur paper differs from previous work in that we\naccomplish style transfer with a disentangled la-\ntent space, for which we propose a systematic set\nof auxiliary losses.\n\n3 Approach\n\nFigure | shows the overall framework of our ap-\nproach. We will first present an autoencoder as our\nbase model. Then we design the auxiliary losses\nfor style and content disentanglement. Finally, we\nintroduce our approach to style-transfer text gen-\neration.\n", "vlm_text": "\nWe conducted experiments on two benchmark datasets. Both qualitative and quantitative results show that the style and content spaces are indeed disentangled well. In the style-transfer evaluation, we achieve high performance in style-transfer ac- curacy, content preservation, as well as language ﬂuency, compared with previous results. Ablation tests also show that all our auxiliary losses can be combined well, each playing its own role in disen- tangling the latent space. \n2 Related Work \nDisentangling neural networks’ latent space has been explored in computer vision in recent years, and researchers have successfully disentangled the features (such as rotation and color) of im- ages ( Chen et al. ,  2016 ;  Higgins et al. ,  2017 ). In these approaches, the disentanglement is purely unsupervised, as no style labels are needed. Un- fortunately, we have not observed disentangled features by applying these approaches in text rep- resentations, and thus we require style labels in our approach. \nStyle-transfer has also been explored in com- puter vision. For example,  Gatys et al.  ( 2016 ) show that the artistic style of an image can be cap- tured well by certain statistics. \nIn NLP, the deﬁnition of “style” itself is vague, and as a convenient starting point, researchers of- ten treat sentiment as a salient style attribute.  Hu et al.  ( 2017 ) propose to control the sentiment by using discriminators to reconstruct sentiment and content from generated sentences. However, there is no evidence that the latent space would be disen- tangled by simply reconstructing a sentence.  Shen et al.  ( 2017 ) use a pair of adversarial discrimina- tors to align the recurrent hidden decoder states of original and style-transferred sentences, for a given style. Fu et al.  ( 2018 ) propose two ap- proaches: training style-speciﬁc embeddings and training separate style-speciﬁc decoders. Their style embeddings are similar to an earlier study by study by  Ficler and Goldberg  ( 2017 ). Their multi- decoder approach is used by  Nogueira dos Santos et al.  ( 2018 ), and is extended to private-shared net- works for styled generation ( Zhang et al. ,  2018 ). Zhao et al.  ( 2018 ) also extend the multi-decoder approach and use a Wasserstein-distance penalty to align content representations of sentences with different styles. Tsvetkov et al.  ( 2018 ) use a machine-translation preprocessing step to strip au- thor style from documents, and then use a multi- decoder model to convert the result into a sentence with a speciﬁc style. \n\nRecently, cycle consistency of back-translation is applied to ensure content preservation ( Xu et al. , 2018 ;  Logeswaran et al. ,  2018 ). These methods re- quire reinforcement learning and are usually difﬁ- cult to train. \nLi et al.  ( 2018 ) propose a hybrid retrieval and generation method that transfers the style by re- trieving and incrementally editing a sentence sim- ilar to the source sentence. \nRao and Tetreault  ( 2018 ) treat the formality of writing as a style, and create a parallel corpus for style transfer with sequence-to-sequence models. This is beyond the scope of our paper, as we focus on non-parallel text style transfer. \nStyle transfer generation is also related to non- parallel machine translation, where researchers apply similar techniques of adversarial alignment, back translation, etc. ( Lample et al. ,  2018a , b ;  Con- neau et al. ,  2018 ). \nOur paper differs from previous work in that we accomplish style transfer with a disentangled la- tent space, for which we propose a systematic set of auxiliary losses. \n3 Approach \nFigure  1  shows the overall framework of our ap- proach. We will ﬁrst present an autoencoder as our base model. Then we design the auxiliary losses for style and content disentanglement. Finally, we introduce our approach to style-transfer text gen- eration. "}
{"page": 2, "image_path": "doc_images/P19-1041_2.jpg", "ocr_text": "(a) Training Phase Jinui(s) Jais(c)\n\nthe s the\nGRU. GRU)\nproduct product } J...\n\n—>| |—\nis great RNN c RNN is great\n\nfx\n\nTmut(c) Jais(s)\n\n(b) Inference Phase 38\n\nthe S| 1s the\nbook ean [| a> ean book\nis good ic pic is boring\n*\n\nx\n\nFigure 1: Overview of our approach.\n\n3.1 Autoencoder\n\nAn autoencoder encodes an input to a latent vector\nspace, from which it reconstructs the input itself.\n\nLet x = (a1, %2,:-++X,) be an input sequence\nwith n words. Our encoder uses a recurrent neural\nnetwork (RNN) with gated recurrent units (GRUs,\nCho et al., 2014); it reads x word-by-word, and\nperforms a linear transformation of the final hid-\nden state to obtain a hidden vector representation\nh.\n\nThen, a decoder RNN generates a sentence\nword-by-word, which ideally should be x itself.\nSuppose at a time step t the decoder RNN predicts\nthe word x; with probability p(a;|h, 21 --- x41),\nthe autoencoder is trained with a sequence-\naggregated cross-entropy loss, given by\n\nn\nJaz(9e, Op) = — S- log p(a4|h, 11 - ++ @4-1)\nt=1\n(1)\n\nwhere @_ and @p are the parameters of the en-\ncoder and decoder, respectively. For brevity, we\nonly present the loss for a single data point (ie.,\na sentence) throughout the paper. Total loss sums\nover all data points, and is implemented with mini-\nbatches. Both the encoder and decoder are deter-\nministic functions in the this model (Rumelhart\net al., 1986), and thus, we call it a deterministic\nautoencoder (DAE).\n\nVariational Autoencoder. Alternatively, we\nmay use a variational autoencoder (VAE, Kingma\nand Welling, 2013), which imposes a probabilistic\ndistribution on the latent vector. The decoder re-\nconstructs data based on the sampled latent vector\nfrom its posterior, and the Kullback—Leibler (KL,\n1951) divergence is penalized for regularization.\n\nFormally, the VAE loss is\n\nJaz (Ge, 9D) = — Ege (njxy log p(x|h)]\n+ Aa KL (ge (Alx)\\|p(h)) (2)\n\nwhere Ay is the hyperparameter balancing the\nreconstruction loss and the KL term. p(h)\nis the prior, typically the standard normal\nN (0,1). qz(h|x) is the posterior in the form\nN (pw, diag o?), where js and o are predicted by\nthe encoder.\n\nCompared with DAE, the reconstruction of\nVAE is based on the samples of the posterior,\nwhich populates encoded representations into a\nneighbourhood close to its prior and thus smooths\nhe latent space. Bowman et al. (2016) show that\nVAE enables more fluent sentence generation from\na latent space than DAE.\n\nThe autoencoding loss serves as our primary\ntraining objective for sentence generation. For dis-\nentangled representation learning, we hope that h\ncan be separated into two spaces s and ce, repre-\nsenting style and content, respectively, i.e., h =\ns;c], where [-;-] denotes concatenation. This is\naccomplished by a systematic design of auxiliary\nlosses described below, and shown in Figure la.\n\n3.2 Style-Oriented Losses\n\nWe first design auxiliary losses that ensure the\nstyle information is contained in the style space s.\nThis involves (1) a multi-task loss that ensures s is\ndiscriminative for the style, and (2) an adversarial\nloss that ensures c is not.\n\nMulti-Task Loss for Style. In the dataset, each\nsentence is labeled with its style, particularly, bi-\nnary sentiment of positive or negative, following\nmost previous work (Hu et al., 2017; Shen et al.,\n2017; Fu et al., 2018; Zhao et al., 2018).\n\nWe build a two-way softmax classifier (equiva-\nlent to logistic regression) on the style space s to\npredict the style label, given by\n\nys = softmax(Wruts) + Omurs)) (3)\n\nwhere Omus) = [Wmuts); Bmut(s)] are the param-\neters of the style classifier in the setting of multi-\ntask learning, and y, is the output of softmax layer.\nThe classifier is trained with cross-entropy loss\nagainst the ground-truth distribution ¢,(-) by\n\nTrut(s)(O8; mus) = — S t,(1) log ys(1) (4)\nlélabels\n\n426\n", "vlm_text": "The image depicts a schematic of a machine learning approach involving GRU (Gated Recurrent Unit) RNNs (Recurrent Neural Networks), which is divided into two phases: Training and Inference.\n\n**(a) Training Phase**:\n- The left part of the image shows a GRU RNN processing a phrase, \"the product is great\", and generating two elements, `s` (content) and `c` (context).\n- The elements `s` and `c` are processed and fed into another GRU RNN to reconstruct the input phrase \"the product is great\".\n- The loss functions `J_rec`, `J_mul(s)`, `J_mul(c)`, `J_dis(s)`, and `J_dis(c)` are depicted with red arrows to denote the optimization objectives. These likely represent different objectives involved in training, such as reconstruction (`J_rec`) and disentangling or regularizing (`J_mul` and `J_dis`).\n\n**(b) Inference Phase**:\n- The bottom part of the image illustrates the inference process, where a GRU RNN processes a new input, \"the book is good\", to produce `s*`, `c*`, and then a synthesized vector `ŝ`.\n- This synthesized vector `ŝ` is subsequently fed into another GRU RNN to produce the output, \"the book is boring\", implying some form of content manipulation or transformation based on the learned representations during training.\n\nTogether, these phases represent a system designed to generate, modify, or reconstruct text sequences using learned content (`s`) and context (`c`) representations, with the potential application in tasks like text generation, style transfer, or context-aware modifications in natural language processing (NLP).\n3.1 Autoencoder \nAn autoencoder encodes an input to a latent vector space, from which it reconstructs the input itself. \nLet  ${\\bf x_{\\alpha}}={\\bf\\alpha}(x_{1},x_{2},\\cdot\\cdot\\cdot x_{n})$   be an input sequence with  $n$   words. Our encoder uses a recurrent neural network (RNN) with gated recurrent units (GRUs, Cho et al. ,  2014 ); it reads  x  word-by-word, and performs a linear transformation of the ﬁnal hid- den state to obtain a hidden vector representation  $h$  . \nThen, a decoder RNN generates a sentence word-by-word, which ideally should be    $_\\mathrm{x}$   itself. Suppose at a time step    $t$   the decoder RNN predicts the word  $x_{t}$   with probability  $p(x_{t}|h,x_{1}\\cdot\\cdot\\cdot x_{t-1})$  , the autoencoder is trained with a sequence- aggregated cross-entropy loss, given by \n\n$$\nJ_{\\mathrm{AE}}({\\pmb\\theta}_{\\mathrm{E}},{\\pmb\\theta}_{\\mathrm{D}})=-\\sum_{t=1}^{n}\\log p(x_{t}|{\\pmb h},x_{1}\\cdot\\cdot\\cdot x_{t-1})\n$$\n \nwhere    $\\theta_{\\mathrm{E}}$   and    $\\theta_{\\mathrm{D}}$   are the parameters of the en- coder and decoder, respectively. For brevity, we only present the loss for a single data point (i.e., a sentence) throughout the paper. Total loss sums over all data points, and is implemented with mini- batches. Both the encoder and decoder are deter- ministic functions in the this model ( Rumelhart et al. ,  1986 ), and thus, we call it a  deterministic autoencoder (DAE).\nVariational Autoencoder. Alternatively, we may use a variational autoencoder (VAE,  Kingma and Welling ,  2013 ), which imposes a probabilistic distribution on the latent vector. The decoder re- constructs data based on the sampled latent vector from its posterior, and the Kullback–Leibler (KL, 1951 ) divergence is penalized for regularization. \nFormally, the VAE loss is \n\n$$\n\\begin{array}{r}{J_{\\mathrm{AE}}(\\pmb{\\theta}_{\\mathrm{E}},\\pmb{\\theta}_{\\mathrm{D}})=-\\,\\mathbb{E}_{q_{E}(\\pmb{h}|\\mathrm{x})}[\\log p(\\mathrm{x}|\\pmb{h})]\\quad\\quad}\\\\ {+\\,\\lambda_{\\mathrm{k}!}\\,\\mathrm{KL}(q_{E}(\\pmb{h}|\\mathrm{x})\\|p(\\pmb{h}))}\\end{array}\n$$\n \nwhere    $\\lambda_{\\mathrm{bl}}$   is the hyperparameter balancing the reconstruction loss and the KL term.  $p(h)$  is the prior, typically the standard normal\n\n  $\\mathcal{N}(\\mathbf{0},\\operatorname{I})$  .  $q_{E}(h|\\mathrm{x})$   is the posterior in the form\n\n  ${\\mathcal{N}}(\\pmb{\\mu},\\mathrm{diag}\\,\\pmb{\\sigma}^{2})$  , where    $\\pmb{\\mu}$   and    $\\pmb{\\sigma}$   are predicted by the encoder. \nCompared with DAE, the reconstruction of VAE is based on the samples of the posterior, which populates encoded representations into a neighbourhood close to its prior and thus smooths the latent space.  Bowman et al.  ( 2016 ) show that VAE enables more ﬂuent sentence generation from a latent space than DAE. \nThe autoencoding loss serves as our primary training objective for sentence generation. For dis- entangled representation learning, we hope that    $h$  can be separated into two spaces    $\\mathbf{\\nabla}_{\\mathbf{S}}$   and    $_c$  , repre- senting style and content, respectively, i.e.,    $h\\ =$   $[s;c]$  , where    $[\\cdot;\\cdot]$   denotes concatenation. This is accomplished by a systematic design of auxiliary losses described below, and shown in Figure  1 a. \n3.2 Style-Oriented Losses \nWe ﬁrst design auxiliary losses that ensure the style information is contained in the style space    $\\mathbf{\\nabla}_{s}$  . This involves (1) a multi-task loss that ensures    $\\mathbf{\\nabla}_{s}$   is discriminative for the style, and (2) an adversarial loss that ensures  $_c$   is not. \nMulti-Task Loss for Style.  In the dataset, each sentence is labeled with its style, particularly, bi- nary sentiment of positive or negative, following most previous work ( Hu et al. ,  2017 ;  Shen et al. , 2017 ;  Fu et al. ,  2018 ;  Zhao et al. ,  2018 ). \nWe build a two-way softmax classiﬁer (equiva- lent to logistic regression) on the style space    $\\mathbf{\\nabla}_{s}$   to predict the style label, given by \n\n$$\n\\pmb{y}_{s}=\\mathrm{softmax}(W_{\\mathrm{mul(s)}}\\pmb{s}+\\pmb{b}_{\\mathrm{mul(s)}})\n$$\n \nwhere    $\\theta_{\\mathrm{multi(s)}}\\;=\\;\\left[W_{\\mathrm{multi(s)}};b_{\\mathrm{multi(s)}}\\right]$   are the param- eters of the style classiﬁer in the setting of multi- task learning, and  $\\pmb{y}_{s}$   is the output of softmax layer. \nThe classiﬁer is trained with cross-entropy loss against the ground-truth distribution  $t_{s}(\\cdot)$   by \n\n$$\nJ_{\\mathrm{mul(s)}}(\\pmb{\\theta}_{\\mathrm{E}};\\pmb{\\theta}_{\\mathrm{mul(s)}})=-\\sum_{l\\in\\mathrm{labels}}t_{s}(l)\\log y_{s}(l)\n$$\n "}
{"page": 3, "image_path": "doc_images/P19-1041_3.jpg", "ocr_text": "In fact, we train the style classifier at the same\ntime as the autoencoding loss. Thus, this could\nbe viewed as multi-task learning, incentivizing the\nentire model to not only decode the sentence, but\nalso predict its sentiment from the style vector s.\nWe denote it by “mul(s).” The idea of multi-task\ntraining is not new and has been used in previous\nwork for sentence representation learning (Jernite\net al., 2017) and sentiment analysis (Balikas et al.,\n2017), among others.\n\nAdversarial Loss for Style. The multi-task\nloss only ensures that the style space contains style\ninformation. However, the content space might\nalso contain style information, which is undesir-\nable for disentanglement.\n\nWe thus apply an adversarial loss to discour-\nage the content space containing style information.\nWe first train a separate classifier, called an adver-\nsary, that deliberately discriminates the style label\nbased on the content vector c. Then, the encoder\nis trained to encode a content space from which its\nadversary cannot predict the style.\n\nConcretely, the adversarial discriminator and its\ntraining objective have a similar form as Eqns. (3)\nand (4), but with different input and parameters,\ngiven by\n\nYs = softmax(Wais(s)€ + bais(s)) (5)\nTais(s) (Oais(s)) = — > te(1) log ys(1) (6)\n\nlélabels\nwhere Ogis(s) = [Waiscsy; Bais(s)] are the parameters\nof the adversary.\n\nIt should be emphasized that, when we train the\nadversary, the gradient is not propagated back to\nthe autoencoder, i.e., the vector c is treated as shal-\nlow features. Therefore, we view Jgis(s) aS a func-\ntion of Ogis(s) only, whereas Jmuys) is a function of\nboth 6g and Anus).\n\nHaving trained an adversary, we would like the\nautoencoder to be tuned in such an ad hoc fashion\nthat c is not discriminative for style. In existing\nliterature, there could be different approaches, for\nexample, maximizing the adversary’s loss (Shen\net al., 2017; Zhao et al., 2018) or penalizing the\nentropy of the adversary’s prediction (Fu et al.,\n2018). In our work, we adopt the latter, as it\ncan be easily extended to multi-category classifi-\ncation, used in Subsection 3.3. Formally, the style-\noriented adversarial objective is to maximize\n\nTaayis)(Oz) = H(ys|e; Paisis)) (7)\n\n427\n\nwhere ys is the predicted distribution over the\nstyle labels and H(p) = — Drictabers Pi log pi is\nthe entropy of the adversary. Here, Jaqyis) is max-\nimized with respect to the encoder Op and we fix\nAgis(s)- The objective attains maximum value when\nys is uniform.\n\nWhile adversarial loss has been explored in pre-\nvious style-transfer studies (Shen et al., 2017; Fu\net al., 2018), it has not been combined with the\nmulti-task loss. As shown in our experiments, a\nsimple combination of these two losses is promis-\ningly effective, achieving better style transfer per-\nformance than a variety of previous methods.\n\n3.3. Content-Oriented Losses\n\nThe above style-oriented losses only regularize\nstyle information, but they do not impose any con-\nstraint on how the content information should be\nencoded.\n\nIn practice, the style space is usually smaller\nthan content space. But it is unrealistic to expect\nthat the content would not flow into the style space\nsimply because of its limited capacity. Therefore,\nwe need to design content-oriented losses to reg-\nularize the content information. In most previous\nwork, however, the treatment of content is miss-\ning (Hu et al., 2017; Fu et al., 2018).\n\nInspired by the above combination of multi-task\nand adversarial losses, we apply the same idea to\nthe content space. However, it is usually hard to\ndefine what “content” actually refers to.\n\nTo this end, we propose to approximate the con-\ntent information by bag-of-words (BoW) features.\nThe BoW feature of a sentence is a vector, each\nelement indicating the probability of a word’s oc-\ncurrence. For a sentence x with NV words, the word\nws’s BoW probability is t.(w.) = Ee Mwiswe}\nwhere I{-} is an indicator function. Here, we only\nconsider content words, excluding stopwords and\nsentiment words (Hu and Liu, 2004),? since we\nfocus on “content” information. It should be men-\ntioned that the removal of stopwords and senti-\nment words is not essential, but results in better\nperformance. We analyze the effect of using dif-\nferent vocabularies in Appendix B.\n\nMulti-Task Loss for Content. Similar to the\nstyle-oriented loss, the multi-task loss for content,\ndenoted as “‘mul(c),” ensures that the content space\n\n’\n\n?The list of sentiment words is available at\nhttps://www.cs.uic.edu/~liub/FBS/\nsentiment-analysis.html#lexicon\n", "vlm_text": "In fact, we train the style classiﬁer at the same time as the autoencoding loss. Thus, this could be viewed as  multi-task  learning, incentivizing the entire model to not only decode the sentence, but also predict its sentiment from the style vector    $\\mathbf{\\nabla}_{\\mathbf{S}}$  . We denote it by “mul(s).” The idea of multi-task training is not new and has been used in previous work for sentence representation learning ( Jernite et al. ,  2017 ) and sentiment analysis ( Balikas et al. , 2017 ), among others. \nAdversarial Loss for Style. The multi-task loss only ensures that the style space contains style information. However, the content space might also contain style information, which is undesir- able for disentanglement. \nWe thus apply an adversarial loss to discour- age the content space containing style information. We ﬁrst train a separate classiﬁer, called an  adver- sary , that deliberately discriminates the style label based on the content vector    $_c$  . Then, the encoder is trained to encode a content space from which its adversary cannot predict the style. \nConcretely, the adversarial discriminator and its training objective have a similar form as Eqns. ( 3 ) and ( 4 ), but with different input and parameters, given by \n\n$$\n\\begin{array}{r}{\\pmb{y}_{s}=\\mathrm{softmax}(\\pmb{W}_{\\mathrm{dis(s)}}\\pmb{c}+\\pmb{b}_{\\mathrm{dis(s)}})}\\\\ {J_{\\mathrm{dis(s)}}(\\pmb{\\theta}_{\\mathrm{dis(s)}})=-\\sum_{l\\in\\mathrm{labels}}t_{c}(l)\\log y_{s}(l)}\\end{array}\n$$\n \nwhere  $\\theta_{\\mathrm{{dis(s)}}}=[W_{\\mathrm{{dis(s)}}};b_{\\mathrm{{dis(s)}}}]$   are the parameters of the adversary. \nIt should be emphasized that, when we train the adversary, the gradient is not propagated back to the autoencoder, i.e., the vector    $^c$   is treated as shal- low features. Therefore, we view    $J_{\\mathrm{{dis(s)}}}$   as a func- tion of    $\\theta_{\\mathrm{{dis(s)}}}$   only, whereas    $J_{\\mathrm{multi}(\\mathrm{s})}$   is a function of both    $\\theta_{\\mathrm{E}}$   and    $\\theta_{\\mathrm{multi}(\\mathrm{s})}$  . \nHaving trained an adversary, we would like the autoencoder to be tuned in such an  ad hoc  fashion that    $_c$   is not discriminative for style. In existing literature, there could be different approaches, for example, maximizing the adversary’s loss ( Shen et al. ,  2017 ;  Zhao et al. ,  2018 ) or penalizing the entropy of the adversary’s prediction (  $\\mathrm{^{Ru}}$   et al. , 2018 ). In our work, we adopt the latter, as it can be easily extended to multi-category classiﬁ- cation, used in Subsection  3.3 . Formally, the style- oriented adversarial objective is to maximize \n\n$$\nJ_{\\mathrm{adv}(\\mathrm{s})}(\\pmb{\\theta}_{\\mathrm{E}})=\\mathcal{H}(\\pmb{y}_{s}|\\pmb{c};\\pmb{\\theta}_{\\mathrm{dis}(\\mathrm{s})})\n$$\n \nwhere    $\\pmb{y}_{s}$   is the predicted distribution over the style labels and    $\\begin{array}{r}{\\mathcal{H}(\\pmb{p})\\ =\\ -\\sum_{i\\in\\mathrm{labels}}p_{i}\\log p_{i}}\\end{array}$  ∈  is the entropy of the adversary. Here,    $J_{\\mathrm{adv(s)}}$   is max- imized with respect to the encoder    $\\theta_{\\mathrm{E}}$   and we ﬁx\n\n  $\\theta_{\\mathrm{{dis(s)}}}$  . The objective attains maximum value when\n\n  $\\pmb{y}_{s}$   is uniform. \nWhile adversarial loss has been explored in pre- vious style-transfer studies ( Shen et al. ,  2017 ;  Fu et al. ,  2018 ), it has not been combined with the multi-task loss. As shown in our experiments, a simple combination of these two losses is promis- ingly effective, achieving better style transfer per- formance than a variety of previous methods. \n3.3 Content-Oriented Losses \nThe above style-oriented losses only regularize style information, but they do not impose any con- straint on how the content information should be encoded. \nIn practice, the style space is usually smaller than content space. But it is unrealistic to expect that the content would not ﬂow into the style space simply because of its limited capacity. Therefore, we need to design content-oriented losses to reg- ularize the content information. In most previous work, however, the treatment of content is miss- ing ( Hu et al. ,  2017 ;  Fu et al. ,  2018 ). \nInspired by the above combination of multi-task and adversarial losses, we apply the same idea to the content space. However, it is usually hard to deﬁne what “content” actually refers to. \nTo this end, we propose to approximate the con- tent information by bag-of-words (BoW) features. The BoW feature of a sentence is a vector, each element indicating the probability of a word’s oc- currence. For a sentence  x  with    $N$   words, the word  $w_{*}$  ’s BoW probability is    $\\begin{array}{r}{t_{c}(w_{*})=\\frac{\\sum_{i=1}^{N}\\mathbb{I}\\{w_{i}=w_{*}\\}}{N}}\\end{array}$  P , where  $\\mathbb{I}\\{\\cdot\\}$   is an indicator function. Here, we only consider content words, excluding stopwords and sentiment words ( Hu and Liu ,  2004 ),   since we focus on “content” information. It should be men- tioned that the removal of stopwords and senti- ment words is not essential, but results in better performance. We analyze the effect of using dif- ferent vocabularies in Appendix  B . \nMulti-Task Loss for Content.  Similar to the style-oriented loss, the multi-task loss for content, denoted as “mul(c),” ensures that the content space  $_c$   contains content information, i.e., BoW features. We introduce a softmax classiﬁer over the BoW vocabulary "}
{"page": 4, "image_path": "doc_images/P19-1041_4.jpg", "ocr_text": "c contains content information, i.e., BoW features.\nWe introduce a softmax classifier over the BoW\nvocabulary\n\nYe = softmax(Wiuc)€ + Bmutc)) (8)\n\nwhere Omui(c)=[Wmulic); Bmul(c)] are the classifier’s\nparameters; y, is the predicted BoW distribution.\n\nThe training objective is a cross-entropy loss\nagainst the ground-truth distribution t,(-):\n\nImut(c) (98; Amu) = S- t.(w) log Yo(w)\nwe€vocab\n(9)\n\nwhere the optimization is performed with both en-\ncoder parameters Og and the multi-task classifier\nOma). Notice that, although the target distribu-\ntion is not one-hot for BoW, the cross-entropy loss\nin Eqn. (9) has the same form as (4).\n\nIt is also interesting that, at first glance, the\nmulti-task loss for content appears to be redun-\ndant to the autoencoding loss, when in fact, it is\nnot. The autoencoding loss only requires that the\nmodel could reconstruct the sentence based on the\ncombined content and style spaces, but does not\nensure their separation. The multi-task loss fo-\ncuses on content words and is applied to the con-\ntent space only.\n\nAdversarial Loss for Content. To ensure that\nthe style space does not contain content informa-\ntion, we design our final auxiliary loss, the BoW\nadversarial loss for content, denoted as “adv(c).”\n\nWe build a content adversary, a softmax classi-\nfier on the style space predicting BoW features\n\nYo = softmax(Waise | s + baisic)) (10)\nais(c) (Baise) = — > te(w) log ye(w) (11)\n\nw€vocab\nwhere Ogis(c) = [Wais(cy; bais(cy] are the classifier’s\nparameters for BoW prediction.\nThe adversarial loss for the model is to maxi-\nmize the entropy of the discriminator\n\nJaay(c) (OE) = H(yels: Dais(c))\n\nAgain, Jgisc) is trained with respect to the dis-\ncriminator’s parameters Ogisic), Whereas Jaqy(c) is\ntrained with respect to Og, similar to the adversar-\nial loss for style.\n\nOur BoW-based, content-oriented losses are\nnovel in the style-transfer literature. While they\ndo not directly work with “style,” they regularize\nthe content information, so that the style and con-\ntent can be better disentangled.\n\n(12)\n\n1 foreach mini-batch do\n\n2 minimize Tais(s) (Bais(s)) w.r.t. Oais(s)3\n\n3 minimize Tais(c) (Oais(cy) w.r.t. Baise);\n\n4 minimize Joy; w.r.t. Og, Op, Omul(s); Omul(c)3\ns end\n\nAlgorithm 1: Training process.\n\n3.4 Training Process\n\nThe overall loss Joy, for our model comprises sev-\neral terms: the autoencoder’s reconstruction ob-\njective, the multi-task and adversarial objectives,\nfor style and content, respectively, given by\n\nJove = Taz(9g, Op) (13)\n+Amut(s)Jmul(s) (Og, Omul(s) ) — Aadv(s) Jadv(s) (Og)\n\n+Xmul(c)J mute) (OE; Omuc)) _ Aadv(c) Jadv(c) (OE)\n\nwhere As are the hyperparameters that balance the\nautoencoding loss and these auxiliary losses.\n\nTo put it all together, the model training in-\nvolves an alternation of optimizing the adversaries\nby Jgis(s) and Jgis(cy, and the model itself by Joyr,\nshown in Algorithm 1.\n\n3.5 Generating Style-Transferred Sentences\n\nA direct application of our disentangled latent\nspace is style-transfer sentence generation, i.e., we\ncan synthesize a sentence with generally the same\nmeaning but a different style in the inference stage.\nLet x* be an input sentence with s* and c* be-\ning the encoded style and content vectors, respec-\ntively. If we would like to transfer its content to a\ndifferent style, we compute an empirical estimate\nof the target style’s vector s of the training set, us-\ning\nDietarget style Si\n\noo 14\n# target style samples (14)\n\ns=\nThe inferred target style $ is concatenated with the\nencoded content c* for decoding style-transferred\nsentences, as shown in Figure 1b.\n\n4 Experiments\n\n4.1 Datasets\n\nWe conducted experiments on two datasets, Yelp\nand Amazon reviews. Both comprise sentences la-\nbeled by binary sentiment (positive or negative).\nThey are used to train latent space disentangle-\nment as well as to evaluate sentiment transfer.\nYelp Service Reviews. We used the Yelp re-\nview dataset, following previous work (Shen et al.,\n\n428\n", "vlm_text": "\n\n$$\n\\pmb{y}_{c}=\\mathrm{softmax}(W_{\\mathrm{mul(c)}}\\pmb{c}+\\pmb{b}_{\\mathrm{mul(c)}})\n$$\n \nwhere  $\\theta_{\\mathrm{mul(c)}}{=}[W_{\\mathrm{mul(c)}};b_{\\mathrm{mul(c)}}]$   are the classiﬁer’s parameters;  $\\pmb{y}_{c}$   is the predicted BoW distribution. \nThe training objective is a cross-entropy loss against the ground-truth distribution    $t_{c}(\\cdot)$  : \n\n$$\nJ_{\\mathrm{mul(c)}}(\\pmb{\\theta}_{\\mathrm{E}};\\pmb{\\theta}_{\\mathrm{mul(c)}})=-\\sum_{w\\in\\mathrm{volab}}t_{c}(w)\\log y_{c}(w)\n$$\n \nwhere the optimization is performed with both en- coder parameters    $\\pmb{\\theta}_{\\mathrm{E}}$   and the multi-task classiﬁer  $\\theta_{\\mathrm{multi(c)}}$  . Notice that, although the target distribu- tion is not one-hot for BoW, the cross-entropy loss in Eqn. ( 9 ) has the same form as ( 4 ). \nIt is also interesting that, at ﬁrst glance, the multi-task loss for content appears to be redun- dant to the autoencoding loss, when in fact, it is not. The autoencoding loss only requires that the model could reconstruct the sentence based on the combined content and style spaces, but does not ensure their separation. The multi-task loss fo- cuses on content words and is applied to the con- tent space only. \nAdversarial Loss for Content.  To ensure that the style space does not contain content informa- tion, we design our ﬁnal auxiliary loss, the BoW adversarial loss for content, denoted as “adv(c).” \nWe build a content adversary, a softmax classi- ﬁer on the style space predicting BoW features \n\n$$\n\\begin{array}{r l}&{\\pmb{y}_{c}=\\mathrm{softmax}(W_{\\mathrm{dis(c)}}^{\\top}\\pmb{s}+\\pmb{b}_{\\mathrm{dis(c)}})}\\\\ &{\\quad J_{\\mathrm{dis(c)}}(\\pmb{\\theta}_{\\mathrm{dis(c)}})=-\\displaystyle\\sum_{w\\in\\mathrm{vacab}}t_{c}(w)\\log{y_{c}(w)}}\\end{array}\n$$\n \nwhere  $\\theta_{\\mathrm{dis(c)}}=[W_{\\mathrm{dis(c)}};b_{\\mathrm{dis(c)}}]$   are the classiﬁer’s parameters for BoW prediction. \nThe adversarial loss for the model is to maxi- mize the entropy of the discriminator \n\n$$\nJ_{\\mathrm{adv}(\\mathrm{c})}(\\pmb{\\theta}_{\\mathrm{E}})=\\mathcal{H}(\\pmb{y}_{c}|\\pmb{s};\\pmb{\\theta}_{\\mathrm{dis}(\\mathrm{c})})\n$$\n \nAgain,    $J_{\\mathrm{{dis(c)}}}$   is trained with respect to the dis- criminator’s parameters    $\\theta_{\\mathrm{{dis(c)}}}$  , whereas    $J_{\\mathrm{adv}(\\mathrm{c})}$   is trained with respect to    $\\theta_{\\mathrm{E}}$  , similar to the adversar- ial loss for style. \nOur BoW-based, content-oriented losses are novel in the style-transfer literature. While they do not directly work with “style,” they regularize the content information, so that the style and con- tent can be better disentangled. \n1  foreach  mini-batch  do \n2 minimize    $J_{\\mathrm{{dis(s)}}}\\big(\\pmb{\\theta}_{\\mathrm{{dis(s)}}}\\big)$   w.r.t.    $\\theta_{\\mathrm{{dis(s)}}}$  ; 3 minimize    $J_{\\mathrm{{dis(c)}}}(\\pmb{\\theta}_{\\mathrm{{dis(c)}}})$   w.r.t.    $\\theta_{\\mathrm{{dis(c)}}}$  ; 4 minimize    $\\boldsymbol{J}_{\\mathrm{over}}$   w.r.t.    $\\theta_{\\mathrm{E}},\\theta_{\\mathrm{D}},\\theta_{\\mathrm{mul(s)}},\\theta_{\\mathrm{mul(c)}}.$  ; 5  end \n3.4 Training Process \nThe overall loss    $J_{\\mathrm{ovr}}$   for our model comprises sev- eral terms: the autoencoder’s reconstruction ob- jective, the multi-task and adversarial objectives, for style and content, respectively, given by \n\n$$\n\\begin{array}{r l r}&{J_{\\mathrm{ov}}=J_{\\mathrm{AE}}(\\pmb{\\theta}_{\\mathrm{E}},\\pmb{\\theta}_{\\mathrm{D}})}&{(13)}\\\\ &{}&{+\\lambda_{\\mathrm{mul(s)}}J_{\\mathrm{mul(s)}}\\big(\\pmb{\\theta}_{\\mathrm{E}},\\pmb{\\theta}_{\\mathrm{mul(s)}}\\big)-\\lambda_{\\mathrm{adv(s)}}J_{\\mathrm{adv(s)}}\\big(\\pmb{\\theta}_{\\mathrm{E}}\\big)}\\\\ &{}&{+\\lambda_{\\mathrm{mul(c)}}J_{\\mathrm{mul(c)}}\\big(\\pmb{\\theta}_{\\mathrm{E}},\\pmb{\\theta}_{\\mathrm{mul(c)}}\\big)-\\lambda_{\\mathrm{adv(c)}}J_{\\mathrm{adv(c)}}\\big(\\pmb{\\theta}_{\\mathrm{E}}\\big)}\\end{array}\n$$\n \nwhere  $\\lambda\\mathbf{s}$   are the hyperparameters that balance the autoencoding loss and these auxiliary losses. \nTo put it all together, the model training in- volves an alternation of optimizing the adversaries by    $J_{\\mathrm{{dis(s)}}}$   and    $J_{\\mathrm{{dis(c)}}}$  , and the model itself by    $\\boldsymbol{J}_{\\mathrm{over}}$  , shown in Algorithm  1 . \n3.5 Generating Style-Transferred Sentences \nA direct application of our disentangled latent space is style-transfer sentence generation, i.e., we can synthesize a sentence with generally the same meaning but a different style in the inference stage. \nLet  $\\mathrm{{x}^{*}}$  be an input sentence with    $s^{*}$  and    $c^{*}$  be- ing the encoded style and content vectors, respec- tively. If we would like to transfer its content to a different style, we compute an empirical estimate of the target style’s vector  $\\hat{\\pmb{s}}$   of the training set, us- ing \n\n$$\n\\hat{\\pmb{s}}=\\frac{\\sum_{i\\in\\mathrm{target\\;cycle}}s_{i}}{\\#\\;\\mathrm{target\\;cycle}}\n$$\n \nThe inferred target style  $\\hat{\\pmb{s}}$   is concatenated with the encoded content    $c^{*}$  for decoding style-transferred sentences, as shown in Figure  1 b. \n4 Experiments \n4.1 Datasets \nWe conducted experiments on two datasets, Yelp and Amazon reviews. Both comprise sentences la- beled by binary sentiment (positive or negative). They are used to train latent space disentangle- ment as well as to evaluate sentiment transfer. \nYelp Service Reviews.  We used the Yelp re- view dataset, following previous work ( Shen et al. , 2017 ;  Zhao et al. ,  2018 ). It contains 444101, 63483, and 126670 labeled reviews for train, vali- dation, and test, respectively. We set the maximum length of a sentence to 15 words and the vocabu- lary size to  ${\\sim}9200$  , following  Shen et al.  ( 2017 ). "}
{"page": 5, "image_path": "doc_images/P19-1041_5.jpg", "ocr_text": "2017; Zhao et al., 2018).> It contains 444101,\n63483, and 126670 labeled reviews for train, vali-\ndation, and test, respectively. We set the maximum\nlength of a sentence to 15 words and the vocabu-\nlary size to ~9200, following Shen et al. (2017).\n\nAmazon Product Reviews. We further eval-\nuate our model with an Amazon review dataset,\nfollowing some other previous papers (Fu et al.,\n2018).* It contains 555142, 2000, and 2000 la-\nbeled reviews for train, validation, and test, re-\nspectively. The maximum length of a sentence is\nset to 20 words and the vocabulary size is ~58k,\nas in Fu et al. (2018).\n\n4.2 Experimental Settings\n\nOur RNN has a hidden state of 256 dimensions,\nlinearly transformed to a style space of 8 dimen-\nsions and a content space of 128 dimensions. They\nwere chosen empirically, and we found them ro-\nbust to model performance. For the decoder, we\nfed the latent vector h = [s, c] to the hidden state\nat each step.\n\nWe used the Adam optimizer (Kingma and Ba,\n2014) for the autoencoder and the RMSProp op-\ntimizer (Tieleman and Hinton, 2012) for the dis-\ncriminators, following stability tricks in adversar-\nial training (Arjovsky et al., 2017). Each optimizer\nhas an initial learning rate of 10~3. Our model is\ntrained for 20 epochs, by which time it has con-\nverged. The word embedding layer was initial-\nized by word2vec (Mikolov et al., 2013) trained\non respective training sets. Both the autoencoder\nand the discriminators are trained once per mini-\nbatch with Xmul(s) = 10, Xmul(c) = 3, Nadv(s) =1,\nand Aadvic) = 0.03. These hyperparameters were\ntuned by a log-scale grid search within two orders\nof magnitude around the default value 1; we chose\nthe values yielding the best validation results.\n\nFor the VAE model, the KL penalty is weighted\nby Axys) and Axe) for style and content, respec-\ntively. We set both to 0.03, tuned by the same\nmethod of log-scale grid search. During training,\nwe also used the sigmoid KL annealing schedule,\nfollowing Bahuleyan et al. (2018).\n\n4.3, Exp. I: Disentangling Latent Space\n\nFirst, we analyze how the style (sentiment) and\ncontent of the latent space are disentangled. We\n\n3The Yelp dataset is available at https://github.\ncom/shentianxiao/language-style-transfer\n‘The Amazon dataset is available at https://\ngithub.com/fuzhenxin/text_style_transfer\n\n429\n\nYelp Amazon\nLatent Space DAE | VAE || DAE | VAE\nNone (majority guess) 0.60 0.51\nContent space (c) 0.66 | 0.70 || 0.67 | 0.69\nStyle space (s) 0.97 | 0.97 || 0.82 | 0.81\nComplete space ((s;¢]) |} 0.97 | 0.97 || 0.82 | 0.81\n\nTable 1: Classification accuracy on latent spaces.\n\nStyle Space\n\nContent Space\n\n(a) DAE bas -\n\n*\n\n(b) VAE\n\nSea\n\niS\n\nFigure 2: t-SNE plots of the disentangled style and con-\ntent spaces on Yelp (with all auxiliary losses).\n\ntrain separate logistic regression sentiment clas-\nsifiers on different latent spaces, and report their\nclassification accuracy in Table 1.\n\nWe see the 128-dimensional content vector c is\nnot particularly discriminative for style. Its accu-\nracy is slightly better than majority guess. How-\never, the 8-dimensional style vector s, despite its\nlow dimensionality, achieves substantially higher\nstyle classification accuracy. When combining\ncontent and style vectors, we observe no further\nimprovement. These results verify the effective-\nness of our disentangling approach, as the style\nspace contains style information, whereas the con-\ntent space does not.\n\nWe show t-SNE plots (van der Maaten and Hin-\nton, 2008) for both DAE and VAE in Figure 2.\nAs seen, sentences with different styles are no-\nticeably separated in a clean manner in the style\nspace (LHS), but are indistinguishable in the con-\ntent space (RHS). It is also evident that the latent\nspace learned by VAE is considerably smoother\nand more continuous than the one learned by DAE.\n\n4.4 Exp. II: Non-Parallel Text Style Transfer\n\nIn this experiment, we apply the disentangled la-\ntent space to sentiment-transfer text generation.\nMetrics. We evaluate competing models based\non (1) style transfer accuracy, (2) content preser-\nvation, and (3) quality of generated language. The\nevaluation of sentence generation has proven to be\ndifficult in contemporary literature, so we adopt a\nfew automatic metrics and use human judgment as\n", "vlm_text": "\nAmazon Product Reviews.  We further eval- uate our model with an Amazon review dataset, following some other previous papers ( Fu et al. , 2018 ).   It contains 555142, 2000, and 2000 la- beled reviews for train, validation, and test, re- spectively. The maximum length of a sentence is set to 20 words and the vocabulary size is    ${\\sim}58\\mathrm{k\\Omega}$  , as in  Fu et al.  ( 2018 ). \n4.2 Experimental Settings \nOur RNN has a hidden state of 256 dimensions, linearly transformed to a style space of 8 dimen- sions and a content space of 128 dimensions. They were chosen empirically, and we found them ro- bust to model performance. For the decoder, we fed the latent vector    $\\pmb{h}=[\\pmb{s},\\pmb{c}]$   to the hidden state at each step. \nWe used the Adam optimizer ( Kingma and Ba , 2014 ) for the autoencoder and the RMSProp op- timizer ( Tieleman and Hinton ,  2012 ) for the dis- criminators, following stability tricks in adversar- ial training ( Arjovsky et al. ,  2017 ). Each optimizer has an initial learning rate of    $10^{-3}$  . Our model is trained for 20 epochs, by which time it has con- verged. The word embedding layer was initial- ized by word2vec ( Mikolov et al. ,  2013 ) trained on respective training sets. Both the autoencoder and the discriminators are trained once per mini- batch with    $\\lambda_{\\mathrm{multi(s)}}=10$  ,    $\\lambda_{\\mathrm{mul(c)}}=3$  ,    $\\lambda_{\\mathrm{adv(s)}}=1$  , and    $\\lambda_{\\mathrm{adv(c)}}\\,=\\,0.03$  . These hyperparameters were tuned by a log-scale grid search within two orders of magnitude around the default value  1 ; we chose the values yielding the best validation results. \nFor the VAE model, the KL penalty is weighted by    $\\lambda_{\\mathrm{k l(s)}}$   and    $\\lambda_{\\mathrm{kl(c)}}$   for style and content, respec- tively. We set both to  0 . 03 , tuned by the same method of log-scale grid search. During training, we also used the  sigmoid  KL annealing schedule, following  Bahuleyan et al.  ( 2018 ). \n4.3 Exp. I: Disentangling Latent Space \nFirst, we analyze how the style (sentiment) and content of the latent space are disentangled. We \nThe table presents the performance of models on two datasets, Yelp and Amazon, using different types of latent spaces. It compares the performance of Denoising Autoencoder (DAE) and Variational Autoencoder (VAE) models based on their ability to utilize various latent spaces. The performance metrics likely represent accuracy or a related metric.\n\nHere's a breakdown of the table:\n\n- **None (majority guess):**\n  - Yelp: 0.60\n  - Amazon: 0.51\n  - This represents the baseline performance using majority class guessing without leveraging latent space.\n\n- **Content Space (c):**\n  - Yelp: \n    - DAE: 0.66\n    - VAE: 0.70\n  - Amazon: \n    - DAE: 0.67\n    - VAE: 0.69\n  - This evaluates the models' performance when only the content space is utilized.\n\n- **Style Space (s):**\n  - Yelp: \n    - DAE: 0.97\n    - VAE: 0.97\n  - Amazon:\n    - DAE: 0.82\n    - VAE: 0.81\n  - This evaluates the models' performance when only the style space is utilized.\n\n- **Complete Space ([s; c]):**\n  - Yelp:\n    - DAE: 0.97\n    - VAE: 0.97\n  - Amazon:\n    - DAE: 0.82\n    - VAE: 0.81\n  - This assesses the models' performance when both content and style spaces are used.\n\nThe results indicate that utilizing style space (and complete space, which includes style space) yields the highest performance on both datasets, particularly for the Yelp dataset.\nThe image shows t-SNE plots that visualize the disentangled style and content spaces for Yelp data. There are two rows corresponding to different models: DAE (Denoising Autoencoder) and VAE (Variational Autoencoder). Each row contains two plots, one for style space and one for content space.\n\n- In the style space plots (left column), data points are represented using crosses and are color-coded as either 'neg' (negative, in blue) or 'pos' (positive, in red). For DAE, the points are separated into distinct clusters for each class, while in VAE, the points are also noticeably clustered but less distinctly separated compared to DAE.\n\n- In the content space plots (right column), both models appear to have overlapping clusters of 'neg' and 'pos' points, suggesting less separation in this space. The content space is more uniformly distributed than the style space in both models.\n\nOverall, the figure is demonstrating the ability of DAE and VAE to separate the data into different style and content representations, as visualized by t-SNE plots.\ntrain separate logistic regression sentiment clas- siﬁers on different latent spaces, and report their classiﬁcation accuracy in Table  1 . \nWe see the 128-dimensional content vector    $_c$   is not particularly discriminative for style. Its accu- racy is slightly better than majority guess. How- ever, the 8-dimensional style vector  s , despite its low dimensionality, achieves substantially higher style classiﬁcation accuracy. When combining content and style vectors, we observe no further improvement. These results verify the effective- ness of our disentangling approach, as the style space contains style information, whereas the con- tent space does not. \nWe show t-SNE plots ( van der Maaten and Hin- ton ,  2008 ) for both DAE and VAE in Figure  2 . As seen, sentences with different styles are no- ticeably separated in a clean manner in the style space (LHS), but are indistinguishable in the con- tent space (RHS). It is also evident that the latent space learned by VAE is considerably smoother and more continuous than the one learned by DAE. \n4.4 Exp. II: Non-Parallel Text Style Transfer \nIn this experiment, we apply the disentangled la- tent space to sentiment-transfer text generation. \nMetrics.  We evaluate competing models based on (1) style transfer accuracy, (2) content preser- vation, and (3) quality of generated language. The evaluation of sentence generation has proven to be difﬁcult in contemporary literature, so we adopt a few automatic metrics and use human judgment as "}
{"page": 6, "image_path": "doc_images/P19-1041_6.jpg", "ocr_text": "well.\n\nStyle-Transfer Accuracy (STA): We follow most\nprevious work (Hu et al., 2017; Shen et al., 2017;\nFu et al., 2018) and train a separate convolutional\nneural network (CNN) to predict the sentiment of\na sentence (Kim, 2014), which is then used to ap-\nproximate the style transfer accuracy. In other\nwords, we report the CNN classifier’s accuracy\non the style-transferred sentences, considering the\ntarget style to be the ground-truth. While the style\nclassifier itself may not be perfect, it achieves a\nreasonable sentiment accuracy on the validation\nsets (97% for Yelp; 82% for Amazon). Thus,\nit provides a quantitative way of evaluating the\nstrength of style transfer.\n\nCosine Similarity (CS): We followed Fu et al.\n(2018) and computed the cosine measure be-\ntween source and generated sentence embeddings,\nwhich are the concatenation of min, max, and\nmean of word embeddings (sentiment words re-\nmoved). This provides a rough estimation of con-\ntent preservation.\n\nWord Overlap (WO): We find that cosine simi-\nlarity, although correlated to human judgment, is\nnot a sensitive measure. Instead, we propose a\nsimple and effective measure that counts the un-\nigram word overlap rate of the original sentence x\nand the style-transferred sentence y, computed by\ncount(xNy) Here, we exclude both stopwords and\n\ncount(xUy)\nsentiment words.\n\nPerplexity (PPL): We use a trigram Kneser—\nNey (KN, Kneser and Ney, 1995) language model\nas a quantitative and automated metric to evaluate\nthe fluency of a sentence. It estimates the empiri-\ncal distribution of trigrams in a corpus, and com-\nputes the perplexity of a test sentence. We trained\nthe language model on the respective datasets, and\nreport PPL on the generated sentences. A smaller\nPPL indicates more fluent sentences.\n\nGeometric Mean (GM): We use the geometric\nmean of STA, WO, and 1/PPL—teflecting trans-\nfer strength, content preservation, and fluency,\nrespectively—to obtain an aggregated score con-\nsidering all aspects. Notice that a smaller PPL\nis desired; thus, we use 1/PPL when computing\nGM. Also, cosine similarity (CS) is not included,\nbecause it is insensitive yet repetitive with word\noverlap (WO). Here, we adopt the geometric mean\nso that the scale of each metric does not influence\nthe judgment.\n\nManual Evaluation: Despite the above auto-\n\n430\n\nmatic metrics, we also conduct human evaluations\no further confirm the performance of our model.\nThis was done on the Yelp dataset only, due to\nhe amount of manual effort involved. We asked\n6 human annotators to rate each sentence on a\n1-5 Likert scale (Stent et al., 2005) in terms of\ntransfer strength (TS), content preservation (CP),\nand language quality (LQ). This evaluation was\nconducted in a strictly blind fashion: samples ob-\nained from all evaluated models were randomly\nshuffled, so that the annotator was unaware of\nwhich model generated a particular sentence. The\ninter-rater agreement—as measured by Krippen-\ndorff’s alpha (Klaus, 2004) for our Likert scale\nratings—is 0.74, 0.68, and 0.72 for these three as-\npects, respectively. According to Klaus (2004),\nhis is an acceptable inter-rater agreement. We\nalso computed the geometric mean (GM) to ob-\nain an aggregated score.\n\nOverall performance. We compare our ap-\nproach with previous state-of-the-art work in Ta-\nble 2. For competing methods, we quote re-\nsults from existing papers whenever possible. In\nsome studies, the authors have released their style-\ntransferred sentences, and we tested them with\nour metrics. A caveat is that this may involve a\ndifferent data split, providing a rough (but unbi-\nased) comparison. For others, we re-evaluated the\nmodel using publicly available code. We sought\ncomparison with Hu et al. (2017), but unfortu-\nnately could not find publicly available code. In-\nstead we sought performance comparisons of their\nmodel in subsequent work, and found that, accord-\ning to the human evaluation in Shen et al. (2017),\nHu et al. (2017) is comparable but slightly worse\nhan Shen et al. (2017). The latter is compared\nwith our model in terms of both automatic metrics\nand human evaluation.\n\nWe see in Table 2 a clear trade-off between style\ntransfer and content preservation, as they are con-\ntradictory goals. Especially, a few models have a\ntransfer accuracy lower than 50%. They are shown\nin gray, and not the focus of the comparison, be-\ncause the system cannot achieve the goal of style\ntransfer most of the time.\n\nOur method achieves high style-transfer accu-\nracy (STA) in both experiments. On the Yelp\ndataset, it outperforms previous methods by more\nhan 7%, whereas on Amazon, VAE is 1% lower\nhan Tsvetkov et al. (2018), ranking second.\n\nOur approach achieves high content preserva-\n", "vlm_text": "Style-Transfer Accuracy  ( STA ): We follow most previous work ( Hu et al. ,  2017 ;  Shen et al. ,  2017 ; Fu et al. ,  2018 ) and train a separate convolutional neural network (CNN) to predict the sentiment of a sentence ( Kim ,  2014 ), which is then used to ap- proximate the style transfer accuracy. In other words, we report the CNN classiﬁer’s accuracy on the style-transferred sentences, considering the target style to be the ground-truth. While the style classiﬁer itself may not be perfect, it achieves a reasonable sentiment accuracy on the validation sets  $(97\\%$  for Yelp;  $82\\%$  for Amazon).Thus,it provides a quantitative way of evaluating the strength of style transfer. \nCosine Similarity  ( CS ): We followed  Fu et al. ( 2018 ) and computed the cosine measure be- tween source and generated sentence embeddings, which are the concatenation of  min ,  max , and mean  of word embeddings (sentiment words re- moved). This provides a rough estimation of con- tent preservation. \nWord Overlap  ( WO ): We ﬁnd that cosine simi- larity, although correlated to human judgment, is not a sensitive measure. Instead, we propose a simple and effective measure that counts the un- igram word overlap rate of the original sentence  x and the style-transferred sentence  y , computed by  $\\frac{c o u n t(\\mathbf{x}\\cap\\mathbf{y})}{c o u n t(\\mathbf{x}\\cup\\mathbf{y})}$  . Here, we exclude both stopwords and ∪ sentiment words. \nPerplexity  ( PPL ): We use a trigram Kneser– Ney (KN,  Kneser and Ney ,  1995 ) language model as a quantitative and automated metric to evaluate the ﬂuency of a sentence. It estimates the empiri- cal distribution of trigrams in a corpus, and com- putes the perplexity of a test sentence. We trained the language model on the respective datasets, and report PPL on the generated sentences. A smaller PPL indicates more ﬂuent sentences. \nGeometric Mean  ( GM ): We use the geometric mean of STA, WO, and 1/PPL—reﬂecting trans- fer strength, content preservation, and ﬂuency, respectively—to obtain an aggregated score con- sidering all aspects. Notice that a smaller PPL is desired; thus, we use 1/PPL when computing GM. Also, cosine similarity (CS) is not included, because it is insensitive yet repetitive with word overlap (WO). Here, we adopt the geometric mean so that the scale of each metric does not inﬂuence the judgment. \nManual Evaluation:  Despite the above auto- matic metrics, we also conduct human evaluations to further conﬁrm the performance of our model. This was done on the Yelp dataset only, due to the amount of manual effort involved. We asked 6 human annotators to rate each sentence on a 1–5 Likert scale ( Stent et al. ,  2005 ) in terms of transfer strength ( TS ), content preservation ( CP ), and language quality ( LQ ). This evaluation was conducted in a strictly blind fashion: samples ob- tained from all evaluated models were randomly shufﬂed, so that the annotator was unaware of which model generated a particular sentence. The inter-rater agreement—as measured by Krippen- dorff’s alpha ( Klaus ,  2004 ) for our Likert scale ratings—is 0.74, 0.68, and 0.72 for these three as- pects, respectively. According to  Klaus  ( 2004 ), this is an acceptable inter-rater agreement. We also computed the geometric mean ( GM ) to ob- tain an aggregated score. \n\nOverall performance. We compare our ap- proach with previous state-of-the-art work in Ta- ble  2 . For competing methods, we quote re- sults from existing papers whenever possible. In some studies, the authors have released their style- transferred sentences, and we tested them with our metrics. A caveat is that this may involve a different data split, providing a rough (but unbi- ased) comparison. For others, we re-evaluated the model using publicly available code. We sought comparison with  Hu et al.  ( 2017 ), but unfortu- nately could not ﬁnd publicly available code. In- stead we sought performance comparisons of their model in subsequent work, and found that, accord- ing to the human evaluation in  Shen et al.  ( 2017 ), Hu et al.  ( 2017 ) is comparable but slightly worse than  Shen et al.  ( 2017 ). The latter is compared with our model in terms of both automatic metrics and human evaluation. \nWe see in Table  2  a clear trade-off between style transfer and content preservation, as they are con- tradictory goals. Especially, a few models have a transfer accuracy lower than  $50\\%$  . They are shown in gray, and not the focus of the comparison, be- cause the system cannot achieve the goal of style transfer most of the time. \nOur method achieves high style-transfer accu- racy (STA) in both experiments. On the Yelp dataset, it outperforms previous methods by more than   $7\\%$  , whereas on Amazon, VAE is   $1\\%$   lower than  Tsvetkov et al.  ( 2018 ), ranking second. \nOur approach achieves high content preserva- "}
{"page": 7, "image_path": "doc_images/P19-1041_7.jpg", "ocr_text": "Model Yelp Dataset Amazon Dataset\n\nSTAT | CST | WOT | PPL! | GM? | STAT | CST | WOT | PPL! | GMT\nStyle-Embedding (Fu et al., 2018) 0.18 | 0.96 | 0.6 124 | 0.10 }} 0.40! | 0.93' | 0.36 32 0.1\nCross-Alignment (Shen et al., 2017) 0.787 | 0.89 | 0.21 93 0.12 0.61 0.89 | 0.02 202 0.04\nMulti-Decoder (Zhao et al., 2018) 0.827 | 0.88 | 0.27 85 0.14 0.55 0.93 | 0.17 75 0.11\nDel-Ret-Gen (Li et al., 2018) ? 0.86 | 0.94 | 0.52 70 0.19 0.43 0.98 0.80 65 0.1\nBackTranslate (Tsvetkov et al., 2018) || 0.85 | 0.83 | 0.08 206 0.07 0.83 0.82 | 0.02 115 0.05\nCycle-RL (Xu et al., 2018) 0.80 | 0.92 | 0.43 470 0.09 0.72 0.91 0.22 332 0.08\nOurs (DAE) 0.88 | 0.92 | 0.55 52 0.21 0.72 0.92 | 0.35 73 0.15\nOurs (VAE) 0.93 | 0.90 | 0.47 32 0.24 0.82 0.90 | 0.20 63 0.14\n\nTable 2: Performance of text style transfer. STA:\n\nStyle transfer accuracy. CS: Cosine similarity. WO: Word\n\noverlap rate. PPL: Perplexity. GM: Geometric mean. The larger’ (or lower\"), the better. ‘Quoted from previous\npapers (with the same data split). Involving custom data splits, providing a rough (but unbiased) comparison.\nOthers are based on our replication, and we use published code whenever possible. We achieve 0.809 and 0.835\ntransfer accuracy on the Yelp dataset, close to the results in Shen et al. (2017) and Zhao et al. (2018), respectively,\nshowing that our replication is fair. Gray numbers show that a method fails to transfer style most of the time.\n\nModel TS | CP | LQ || GM\n\nFu et al. (2018) 1.67 | 3.84 | 3.66 [| 2.86\n\nShen et al. (2017) || 3.63 | 3.07 | 3.08 || 3.25\n\nZhao et al. (2018) |} 3.55 | 3.09 | 3.77 || 3.46\n\nOurs (DAE) 3.67 | 3.64 | 4.19 || 3.83\n\nOurs (VAE) 4.32 | 3.73 | 4.48 || 4.16\nTable 3: Manual evaluation on the Yelp dataset.\n\ntion as well. Among all the methods that can\nachieve more than 50% transfer accuracy, DAE\nhas the highest word overlap (WO) on Yelp; VAE\nis also high, although slightly lower than Li et al.\n(2018). On Amazon, the phenomenon is similar.\nDAE is the best; VAE is 2% lower in WO (al-\nthough 10% better in transfer accuracy), compared\nwith Xu et al. (2018).\n\nFor language fluency, VAE yields the best PPL\nin both datasets. It is also noted that, the cycle\nreinforcement learning (Cycle-RL) approach does\nnot generate fluent sentences (Xu et al., 2018).\nThey have unusually high PPL scores, but after\nreading the samples provided by the authors (via\npersonal email correspondence) we are assured\nthat the sentences obtained by Cycle-RL are less\nfluent.\n\nWhen we consider all the above aspects, our\napproach (either DAE or VAE) has the highest\ngeometric meaning (GM), showing that we have\nachieved good balance on transfer strength, con-\ntent preservation, as well as language fluency.\n\nTable 3 presents the results of human evalua-\ntion on selected methods.° Again, we see that the\nstyle embedding model (Fu et al., 2018) is ineffec-\ntive as it has a very low transfer strength, and that\nour method outperforms other baselines in all as-\n\n5Selection was based on the time of availability.\n\n431\n\nObjectives STA CS | WO | PPL || GM\nTae 0.11 | 094 | 047] 40 | 0.11\nTas Tene 077 | 091 | 033 | 41 || 0.18\nTas Jeans) 0.78 | 0.89 | 0.23 | 35 || 0.17\nTate Junie ane [| 0.91 | 087 [0.17 | 23] 0.19\nJae; Sours Juve |} 9,93 | 0,90 | 0.47] 32 |) 0.24\nTrnurieys Faavioy\n\nTable 4: Ablation tests on Yelp. In all variants, we fol-\nlow the same protocol of style transfer by substituting\nan empirical estimate of the target style vector.\n\npects. The results are consistent with Table 2. This\nalso implies that the automatic metrics we used are\nreasonable, and could be extrapolated to different\nmodels; it also shows consistent evidence of the\neffectiveness of our approach.\n\nAblation Test. We conducted ablation tests\non the Yelp dataset, and show results in Table 4.\nWith Jap only, we cannot achieve reasonable style\ntransfer accuracy by substituting an empirically\nestimated style vector of the target style. This is\nbecause the style and content spaces would not be\ndisentangled spontaneously with the autoencoding\nloss alone. With either Jmuys) Or Jadvs), the model\nachieves reasonable transfer accuracy and cosine\nsimilarity. Combining them together improves the\ntransfer accuracy to 90%, outperforming previous\nmethods by a margin of 5% (Table 2). This shows\nthat the multi-task loss and the adversarial loss\nwork in different ways. Our insight of combining\nthe two auxiliary losses is a simple yet effective\nway of disentangling latent space.\n\nOn the other hand, Jus) and Jaays) only reg-\nularize the style information, leading to gradual\ndrop of content preserving scores. Then, we\nuse another insight of introducing content-oriented\nauxiliary losses, Jmmucy and Jagy(c), based on BoW\n", "vlm_text": "The table compares various models based on their performance on the Yelp and Amazon Datasets. It lists models with metrics including STA, CS, WO, PPL, and GM, with some differences between the two datasets:\n\n- **Models**: Various models like Style-Embedding, Cross-Alignment, Multi-Decoder, Del-Ret-Gen, BackTranslate, Cycle-RL, and two variants of the author's model (DAE and VAE).\n\n- **Metrics**:\n  - **STA↑**: Style Transfer Accuracy (higher is better)\n  - **CS↑**: Content Similarity (higher is better)\n  - **WO↑**: Word Overlap (higher is better)\n  - **PPL↓**: Perplexity (lower is better)\n  - **GM↑**: Geometric Mean (higher is better)\n\nThe metrics are presented for both the Yelp and Amazon datasets, being highlighted for certain models based on performance. Bold values indicate the best performance in the respective columns.\nTable 2: Performance of text style transfer.  STA:  Style transfer accuracy.  CS:  Cosine similarity.  WO:  Word overlap rate.  PPL:  Perplexity.  GM:  Geometric mean. The larger ↑ (or lower ↓ ), the better.   † Quoted from previous papers (with the same data split). ‡ Involving custom data splits, providing a rough (but unbiased) comparison. Others are based on our replication, and we use published code whenever possible. We achieve 0.809 and 0.835 transfer accuracy on the Yelp dataset, close to the results in  Shen et al.  ( 2017 ) and  Zhao et al.  ( 2018 ), respectively, showing that our replication is fair. Gray numbers show that a method fails to transfer style most of the time. \nThe table presents the performance of different models evaluated based on four criteria: TS, CP, LQ, and GM. Here's a breakdown of the table content:\n\n1. **Model Names:**\n   - Fu et al. (2018)\n   - Shen et al. (2017)\n   - Zhao et al. (2018)\n   - Ours (DAE)\n   - Ours (VAE)\n\n2. **Performance Metrics:**\n   - TS\n   - CP\n   - LQ\n   - GM\n\n3. **Scores for Each Model:**\n   - **Fu et al. (2018):**\n     - TS: 1.67\n     - CP: 3.84\n     - LQ: 3.66\n     - GM: 2.86\n   - **Shen et al. (2017):**\n     - TS: 3.63\n     - CP: 3.07\n     - LQ: 3.08\n     - GM: 3.25\n   - **Zhao et al. (2018):**\n     - TS: 3.55\n     - CP: 3.09\n     - LQ: 3.77\n     - GM: 3.46\n   - **Ours (DAE):**\n     - TS: 3.67\n     - CP: 3.64\n     - LQ: 4.19\n     - GM: 3.83\n   - **Ours (VAE):**\n     - TS: 4.32\n     - CP: 3.73\n     - LQ: 4.48\n     - GM: 4.16\n\nBased on the scores, the model \"Ours (VAE)\" achieved the highest scores across all metrics compared to the other models.\ntion as well. Among all the methods that can achieve more than   $50\\%$   transfer accuracy, DAE has the highest word overlap (WO) on Yelp; VAE is also high, although slightly lower than  Li et al. ( 2018 ). On Amazon, the phenomenon is similar. DAE is the best; VAE is  $2\\%$   lower in WO (al- though   $10\\%$   better in transfer accuracy), compared with  $\\mathrm{Xu}$   et al.  ( 2018 ). \nFor language ﬂuency, VAE yields the best PPL in both datasets. It is also noted that, the cycle reinforcement learning (Cycle-RL) approach does not generate ﬂuent sentences ( Xu et al. ,  2018 ). They have unusually high PPL scores, but after reading the samples provided by the authors (via personal email correspondence) we are assured that the sentences obtained by Cycle-RL are less ﬂuent. \nWhen we consider all the above aspects, our approach (either DAE or VAE) has the highest geometric meaning (GM), showing that we have achieved good balance on transfer strength, con- tent preservation, as well as language ﬂuency. \nTable  3  presents the results of human evalua- tion on selected methods. Again, we see that the style embedding model ( Fu et al. ,  2018 ) is ineffec- tive as it has a very low transfer strength, and that our method outperforms other baselines in all as- \nThe table presents several performance metrics under different objectives for a given task or experiment. Here's a breakdown of the content in the table:\n\n- **Column Headers:**\n  - **Objectives:** Lists the different combinations of objectives used in the experiments.\n  - **STA:** Provides scores related to this specific measure or evaluation criterion.\n  - **CS:** Provides scores for this evaluation criterion.\n  - **WO:** Shows scores for this specific evaluation measure.\n  - **PPL:** Represents Perplexity scores, typically used to evaluate language models.\n  - **GM:** Represents some cumulative or overall metric, possibly a geometric mean.\n\n- **Objectives and their corresponding scores:**\n  1. **\\( J_{AE} \\):**  \n     - STA: 0.11\n     - CS: 0.94 (in bold, indicating a notable or best score)\n     - WO: 0.47\n     - PPL: 40\n     - GM: 0.11\n\n  2. **\\( J_{AE}, J_{mul(s)} \\):**  \n     - STA: 0.77\n     - CS: 0.91\n     - WO: 0.33\n     - PPL: 41\n     - GM: 0.18\n\n  3. **\\( J_{AE}, J_{adv(s)} \\):**  \n     - STA: 0.78\n     - CS: 0.89\n     - WO: 0.23\n     - PPL: 35\n     - GM: 0.17\n\n  4. **\\( J_{AE}, J_{mul(s)}, J_{adv(s)} \\):**  \n     - STA: 0.91\n     - CS: 0.87\n     - WO: 0.17\n     - PPL: 23 (in bold, indicating a notable or best score)\n     - GM: 0.19\n\n  5. **\\( J_{AE}, J_{mul(s)}, J_{adv(s)}, J_{mul(c)}, J_{adv(c)} \\):**  \n     - STA: 0.93 (in bold, indicating a notable or best score)\n     - CS: 0.90\n     - WO: 0.47 (in bold, indicating a notable or best score)\n     - PPL: 32\n     - GM: 0.24 (in bold, indicating a notable or best score)\n\nThe bolded values might indicate the best performance across respective columns, providing insight into which objective combinations are most effective based on each metric.\nTable 4: Ablation tests on Yelp. In all variants, we fol- low the same protocol of style transfer by substituting an empirical estimate of the target style vector. \npects. The results are consistent with Table  2 . This also implies that the automatic metrics we used are reasonable, and could be extrapolated to different models; it also shows consistent evidence of the effectiveness of our approach. \nAblation Test. We conducted ablation tests on the Yelp dataset, and show results in Table  4 . With    $J_{\\mathrm{AE}}$   only, we cannot achieve reasonable style transfer accuracy by substituting an empirically estimated style vector of the target style. This is because the style and content spaces would not be disentangled spontaneously with the autoencoding loss alone. With either  ${\\cal J}_{\\mathrm{multi}}(\\mathrm{s})$   or    $J_{\\mathrm{adv(s)}}$  , the model achieves reasonable transfer accuracy and cosine similarity. Combining them together improves the transfer accuracy to  $90\\%$  , outperforming previous methods by a margin of  $5\\%$   (Table  2 ). This shows that the multi-task loss and the adversarial loss work in different ways. Our insight of combining the two auxiliary losses is a simple yet effective way of disentangling latent space. \nOn the other hand,    ${\\cal J}_{\\mathrm{multi}(\\mathrm{s})}$   and    $J_{\\mathrm{adv(s)}}$   only reg- ularize the style information, leading to gradual drop of content preserving scores. Then, we use another insight of introducing content-oriented auxiliary losses,    ${\\cal J}_{\\mathrm{mul(c)}}$   and    $J_{\\mathrm{adv}(\\mathrm{c})}$  , based on BoW features, which regularize the content information in the same way as style. By incorporating all these auxiliary losses, we achieve high transfer ac- curacy, high content preservation, as well as high language ﬂuency. "}
{"page": 8, "image_path": "doc_images/P19-1041_8.jpg", "ocr_text": "features, which regularize the content information\nin the same way as style. By incorporating all\nthese auxiliary losses, we achieve high transfer ac-\ncuracy, high content preservation, as well as high\nlanguage fluency.\n\n5 Conclusion and Future Work\n\nIn this paper, we propose an effective approach\nfor disentangling style and content latent spaces.\nWe systematically combine multi-task and adver-\nsarial objectives to separate content and style from\neach other, where we also propose to approximate\ncontent information with bag-of-words features of\nstyle-neutral, non-stopword vocabulary.\n\nBoth qualitative and quantitative experiments\nshow that the latent space is indeed separated into\nstyle and content parts. The disentangled space\ncan be directly applied to text style-transfer tasks.\nOur method achieves high style-transfer strength,\nhigh content-preservation scores, as well as high\nlanguage fluency, compared with previous work.\n\nOur approach can be naturally extended to non-\ncategorical styles, because our style feature is en-\ncoded from the input sentence. Non-categorical\nstyles cannot be easily handled by fixed style\nembeddings or style-specific decoders (Fu et al.,\n2018). Bao et al. (2019) have successfully shown\nthat the syntax and semantics of a sentence can be\ndisentangled from each other.\n\nAcknowledgments\n\nWe thank all reviewers for insightful comments.\nThis work was supported in part by the NSERC\ngrant RGPIN-261439-2013 and an Amazon Re-\nsearch Award. We would also like to acknowl-\nedge NVIDIA Corporation for the donated Titan\nXp GPU.\n\nReferences\n\nMartin Arjovsky, Soumith Chintala, and Leon Bottou.\n2017. Wasserstein generative adversarial networks.\nIn JCML, pages 214-223.\n\nHareesh Bahuleyan, Lili Mou, Olga Vechtomova, and\nPascal Poupart. 2018. Variational attention for\nsequence-to-sequence models. In COLING, pages\n1672-1682.\n\nGeorgios Balikas, Simon Moura, and Massih-Reza\nAmini. 2017. Multitask learning for fine-grained\ntwitter sentiment analysis. In SJGIR, pages 1005—\n1008.\n\n432\n\nYu Bao, Hao Zhou, Shujian Huang, Lei Li, Lili Mou,\nOlga Vechtomova, XIN-YU DAL, and Jiajun CHEN.\n2019. Generating sentences from disentangled syn-\ntactic and semantic spaces. In ACL.\n\nSamuel R. Bowman, Luke Vilnis, Oriol Vinyals, An-\ndrew M. Dai, Rafal Jozefowicz, and Samy Ben-\ngio. 2016. Generating sentences from a continuous\nspace. In CoNLL, pages 10-21.\n\nXi Chen, Yan Duan, Rein Houthooft, John Schulman,\nIlya Sutskever, and Pieter Abbeel. 2016. Infogan:\nInterpretable representation learning by information\nmaximizing generative adversarial nets. In NIPS,\npages 2172-2180.\n\nKyunghyun Cho, Bart van Merrienboer, Caglar Gul-\ncehre, Dzmitry Bahdanau, Fethi Bougares, Holger\nSchwenk, and Yoshua Bengio. 2014. Learning\nphrase representations using RNN encoder-decoder\nfor statistical machine translation. In EMNLP, pages\n1724-1734.\n\nAlexis Conneau, Guillaume Lample, Marc’ Aurelio\nRanzato, Ludovic Denoyer, and Hervé Jégou. 2018.\nWord translation without parallel data. In JCLR.\n\nJessica Ficler and Yoav Goldberg. 2017. Controlling\nlinguistic style aspects in neural language genera-\ntion. In Proc. Workshop on Stylistic Variation, pages\n94-104.\n\nZhenxin Fu, Xiaoye Tan, Nanyun Peng, Dongyan\nZhao, and Rui Yan. 2018. Style transfer in text: Ex-\nploration and evaluation. In AAAI, pages 663-670.\n\nLeon A. Gatys, Alexander S. Ecker, and Matthias\nBethge. 2016. Image style transfer using convolu-\ntional neural networks. In CVPR, pages 2414-2423.\n\nIrina Higgins, Loic Matthey, Arka Pal, Christopher\nBurgess, Xavier Glorot, Matthew Botvinick, Shakir\nMohamed, and Alexander Lerchner. 2017. Beta-\nVAE: Learning basic visual concepts with a con-\nstrained variational framework. In JCLR.\n\nMinging Hu and Bing Liu. 2004. Mining and summa-\nrizing customer reviews. In KDD, pages 168-177.\n\nZhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan\nSalakhutdinov, and Eric P. Xing. 2017. Toward con-\ntrolled generation of text. In ICML, pages 1587-\n1596.\n\nYacine Jernite, Samuel R. Bowman, and David Son-\ntag. 2017. Discourse-based objectives for fast un-\nsupervised sentence representation learning. arXiv,\nabs/1705.00557.\n\nYoon Kim. 2014. Convolutional neural networks for\nsentence classification. In EMNLP, pages 1746—\n1751.\n\nDiederik P. Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv prerpint\narXiv: 1412.6980.\n", "vlm_text": "\n5 Conclusion and Future Work \nIn this paper, we propose an effective approach for disentangling style and content latent spaces. We systematically combine multi-task and adver- sarial objectives to separate content and style from each other, where we also propose to approximate content information with bag-of-words features of style-neutral, non-stopword vocabulary. \nBoth qualitative and quantitative experiments show that the latent space is indeed separated into style and content parts. The disentangled space can be directly applied to text style-transfer tasks. Our method achieves high style-transfer strength, high content-preservation scores, as well as high language ﬂuency, compared with previous work. \nOur approach can be naturally extended to non- categorical styles, because our style feature is en- coded from the input sentence. Non-categorical styles cannot be easily handled by ﬁxed style embeddings or style-speciﬁc decoders ( Fu et al. , 2018 ).  Bao et al.  ( 2019 ) have successfully shown that the syntax and semantics of a sentence can be disentangled from each other. \nAcknowledgments \nWe thank all reviewers for insightful comments. This work was supported in part by the NSERC grant RGPIN-261439-2013 and an Amazon Re- search Award. We would also like to acknowl- edge NVIDIA Corporation for the donated Titan Xp GPU. \nReferences \n2017.  Wasserstein generative adversarial networks . In  ICML , pages 214–223. Hareesh Bahuleyan, Lili Mou, Olga Vechtomova, and Pascal Poupart. 2018. Variational attention for sequence-to-sequence models . In  COLING , pages 1672–1682. Georgios Balikas, Simon Moura, and Massih-Reza Amini. 2017. Multitask learning for ﬁne-grained twitter sentiment analysis . In  SIGIR , pages 1005– 1008. \nYu Bao, Hao Zhou, Shujian Huang, Lei Li, Lili Mou, Olga Vechtomova, XIN-YU DAI, and Jiajun CHEN. 2019. Generating sentences from disentangled syn- tactic and semantic spaces. In  ACL . Samuel R. Bowman, Luke Vilnis, Oriol Vinyals, An- drew M. Dai, Rafal Jozefowicz, and Samy Ben- gio. 2016.  Generating sentences from a continuous space . In  CoNLL , pages 10–21. Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. 2016.  Infogan: Interpretable representation learning by information maximizing generative adversarial nets . In  NIPS , pages 2172–2180. Kyunghyun Cho, Bart van Merrienboer, Caglar Gul- cehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using RNN encoder-decoder for statistical machine translation . In  EMNLP , pages 1724–1734. Alexis Conneau, Guillaume Lample, Marc’Aurelio Ranzato, Ludovic Denoyer, and Herv´ e J´ egou. 2018. Word translation without parallel data . In  ICLR . Jessica Ficler and Yoav Goldberg. 2017.  Controlling linguistic style aspects in neural language genera- tion . In  Proc. Workshop on Stylistic Variation , pages 94–104. Zhenxin Fu, Xiaoye Tan, Nanyun Peng, Dongyan Zhao, and Rui Yan. 2018.  Style transfer in text: Ex- ploration and evaluation . In  AAAI , pages 663–670. Leon A. Gatys, Alexander S. Ecker, and Matthias Bethge. 2016.  Image style transfer using convolu- tional neural networks . In  CVPR , pages 2414–2423. Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed, and Alexander Lerchner. 2017. Beta- VAE: Learning basic visual concepts with a con- strained variational framework . In  ICLR . Minqing Hu and Bing Liu. 2004.  Mining and summa- rizing customer reviews . In  KDD , pages 168–177. Zhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan Salakhutdinov, and Eric P. Xing. 2017.  Toward con- trolled generation of text . In  ICML , pages 1587– 1596. Yacine Jernite, Samuel R. Bowman, and David Son- tag. 2017.  Discourse-based objectives for fast un- supervised sentence representation learning .  arXiv , abs/1705.00557. Yoon Kim. 2014.  Convolutional neural networks for sentence classiﬁcation . In  EMNLP , pages 1746– 1751. Diederik P. Kingma and Jimmy Ba. 2014.  Adam: A method for stochastic optimization .  arXiv prerpint arXiv:1412.6980."}
{"page": 9, "image_path": "doc_images/P19-1041_9.jpg", "ocr_text": "Diederik P. Kingma and Max Welling. 2013. Auto-\nencoding variational Bayes. arXiv preprint\narXiv:1312.6114.\n\nKrippendorff Klaus. 2004. Content Analysis: An Intro-\nduction to Its Methodology. Sage Publications.\n\nReinhard Kneser and Hermann Ney. 1995. Improved\nbacking-off for m-gram language modeling. In\nICASSP, pages 181-184.\n\nSolomon Kullback and Richard A Leibler. 1951. On\ninformation and sufficiency. The Annals of Mathe-\nmatical Statistics, 22(1):79-86.\n\nGuillaume Lample, Alexis Conneau, Ludovic De-\nnoyer, and Marc’ Aurelio Ranzato. 2018a. Unsu-\npervised machine translation using monolingual cor-\npora only. In JCLR.\n\nGuillaume Lample, Myle Ott, Alexis Conneau, Lu-\ndovic Denoyer, and Marc’ Aurelio Ranzato. 2018b.\nPhrase-based & neural unsupervised machine trans-\nlation. In EMNLP, pages 5039-5049.\n\nJuncen Li, Robin Jia, He He, and Percy Liang. 2018.\nDelete, retrieve, generate: A simple approach to sen-\ntiment and style transfer. In NAACL-HLT, pages\n1865-1874.\n\nLajanugen Logeswaran, Honglak Lee, and Samy Ben-\ngio. 2018. Content preserving text generation with\nattribute controls. In N/PS, pages 5108-5118.\n\nLaurens van der Maaten and Geoffrey Hinton. 2008.\nVisualizing data using t-SNE. JMLR, 9:2579-2605.\n\nTomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S.\nCorrado, and Jeffrey Dean. 2013. Distributed repre-\nsentations of words and phrases and their composi-\ntionality. In NJPS, pages 3111-3119.\n\nSudha Rao and Joel R. Tetreault. 2018. Dear sir or\nmadam, may I introduce the GYAFC dataset: Cor-\npus, benchmarks and metrics for formality style\ntransfer. In NAACL-HLT, pages 129-140.\n\nD. E. Rumelhart, G. E. Hinton, and R. J. Williams.\n1986. Learning internal representations by error\npropagation. In Parallel Distributed Processing:\nExplorations in the Microstructure of Cognition,\nvolume 1, pages 318-362.\n\nCicero Nogueira dos Santos, Igor Melnyk, and Inkit\nPadhi. 2018. Fighting offensive language on social\nmedia with unsupervised text style transfer. In ACL\n(Short Papers), pages 189-194.\n\nTianxiao Shen, Tao Lei, Regina Barzilay, and Tommi S.\nJaakkola. 2017. Style transfer from non-parallel text\nby cross-alignment. In NIPS, pages 6833-6844.\n\nAmanda Stent, Matthew Marge, and Mohit Singhai.\n2005. Evaluating evaluation methods for generation\nin the presence of variation. In C/CLing, pages 341-\n351.\n\n433\n\nTijmen Tieleman and Geoffrey Hinton. 2012. Lecture\n6.5-rmsprop: Divide the gradient by a running av-\nerage of its recent magnitude. COURSERA: Neural\nNetworks for Machine Learning.\n\nYulia Tsvetkov, Alan W. Black, Ruslan Salakhutdi-\nnov, and Shrimai Prabhumoye. 2018. Style transfer\nthrough back-translation. In ACL, pages 866-876.\n\nJingjing Xu, Xu Sun, Qi Zeng, Xiaodong Zhang, Xu-\nancheng Ren, Houfeng Wang, and Wenjie Li. 2018.\nUnpaired sentiment-to-sentiment translation: A cy-\ncled reinforcement learning approach. In ACL,\npages 979-988.\n\nYe Zhang, Nan Ding, and Radu Soricut. 2018.\nSHAPED: Shared-private encoder-decoder for text\nstyle adaptation. In NAACL-HLT, pages 1528-1538.\n\nJunbo Jake Zhao, Yoon Kim, Kelly Zhang, Alexan-\nder M. Rush, and Yann LeCun. 2018. Adversarially\nregularized autoencoders. In JCML, pages 5897—\n5906.\n\nA Qualitative Examples\n\nTable 5 provides several examples of our style-\ntransfer model. Results show that we can success-\nfully transfer the sentiment while preserving the\ncontent of a sentence.\n\nB_ Effect of the BoW Vocabulary\n\nTable 6 demonstrates the effect of choosing dif-\nferent BoW vocabulary for the auxiliary content\nlosses. As seen, we are able to achieve reasonable\nperformance with any of these vocabularies, but\nusing a vocabulary that excludes sentiment words\nand stopwords performs the best.\n", "vlm_text": "Diederik P. Kingma and Max Welling. 2013. Auto- encoding variational Bayes . arXiv preprint arXiv:1312.6114 . Krippendorff Klaus. 2004.  Content Analysis: An Intro- duction to Its Methodology . Sage Publications. Reinhard Kneser and Hermann Ney. 1995.  Improved backing-off for m-gram language modeling . In ICASSP , pages 181–184. Solomon Kullback and Richard A Leibler. 1951.  On information and sufﬁciency .  The Annals of Mathe- matical Statistics, 22(1):79–86.Guillaume Lample, Alexis Conneau, Ludovic De- noyer, and Marc’Aurelio Ranzato. 2018a. Unsu- pervised machine translation using monolingual cor- pora only . In  ICLR . Guillaume Lample, Myle Ott, Alexis Conneau, Lu- dovic Denoyer, and Marc’Aurelio Ranzato. 2018b. Phrase-based & neural unsupervised machine trans- lation . In  EMNLP , pages 5039–5049. Juncen Li, Robin Jia, He He, and Percy Liang. 2018. Delete, retrieve, generate: A simple approach to sen- timent and style transfer . In  NAACL-HLT , pages 1865–1874. Lajanugen Logeswaran, Honglak Lee, and Samy Ben- gio. 2018.  Content preserving text generation with attribute controls . In  NIPS , pages 5108–5118. Laurens van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-SNE .  JMLR , 9:2579–2605. Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Corrado, and Jeffrey Dean. 2013.  Distributed repre- sentations of words and phrases and their composi- tionality . In  NIPS , pages 3111–3119. Sudha Rao and Joel R. Tetreault. 2018. Dear sir or madam, may I introduce the GYAFC dataset: Cor- pus, benchmarks and metrics for formality style transfer . In  NAACL-HLT , pages 129–140. D. E. Rumelhart, G. E. Hinton, and R. J. Williams. 1986. Learning internal representations by error propagation . In  Parallel Distributed Processing: Explorations in the Microstructure of Cognition , volume 1, pages 318–362.Cicero Nogueira dos Santos, Igor Melnyk, and Inkit Padhi. 2018.  Fighting offensive language on social media with unsupervised text style transfer . In  ACL (Short Papers) , pages 189–194. Tianxiao Shen, Tao Lei, Regina Barzilay, and Tommi S. Jaakkola. 2017.  Style transfer from non-parallel text by cross-alignment . In  NIPS , pages 6833–6844. Amanda Stent, Matthew Marge, and Mohit Singhai. 2005.  Evaluating evaluation methods for generation in the presence of variation . In  CICLing , pages 341– 351. \nTijmen Tieleman and Geoffrey Hinton. 2012. Lecture 6.5-rmsprop: Divide the gradient by a running av- erage of its recent magnitude.  COURSERA: Neural Networks for Machine Learning . Yulia Tsvetkov, Alan W. Black, Ruslan Salakhutdi- nov, and Shrimai Prabhumoye. 2018.  Style transfer through back-translation . In  ACL , pages 866–876. Jingjing Xu, Xu Sun, Qi Zeng, Xiaodong Zhang, Xu- ancheng Ren, Houfeng Wang, and Wenjie Li. 2018. Unpaired sentiment-to-sentiment translation: A cy- cled reinforcement learning approach . In  ACL , pages 979–988. Ye Zhang, Nan Ding, and Radu Soricut. 2018. SHAPED: Shared-private encoder-decoder for text style adaptation . In  NAACL-HLT , pages 1528–1538. Junbo Jake Zhao, Yoon Kim, Kelly Zhang, Alexan- der M. Rush, and Yann LeCun. 2018.  Adversarially regularized autoencoders . In  ICML , pages 5897– 5906. \nA Qualitative Examples \nTable  5  provides several examples of our style- transfer model. Results show that we can success- fully transfer the sentiment while preserving the content of a sentence. \nB Effect of the BoW Vocabulary \nTable  6  demonstrates the effect of choosing dif- ferent BoW vocabulary for the auxiliary content losses. As seen, we are able to achieve reasonable performance with any of these vocabularies, but using a vocabulary that excludes sentiment words and stopwords performs the best. "}
{"page": 10, "image_path": "doc_images/P19-1041_10.jpg", "ocr_text": "Original (Positive)\n\nDAE Transferred (Negative)\n\nVAE Transferred (Negative)\n\nthe food is excellent and the\n\nhe food was a bit bad but the\n\nthe food was bland and i am not\n\nservice is exceptional staff was exceptional thrilled with this\n\nthe waitresses are friendly and |) the guys are rude and helpful the waitresses are rude and are\nhelpful lazy\n\nthe restaurant itself is romantic |) the restaurant itself is awkward | the restaurant itself was dirty\nand quiet and quite crowded\n\ngreat deal horrible deal no deal\n\nboth times i have eaten the\nlunch buffet and it was out-\nstanding\n\nheir burgers were decent but\nhe eggs were not the consis-\nency\n\nboth times i have eaten here the\nfood was mediocre at best\n\nOriginal (Negative)\n\nDAE Transferred (Positive)\n\nVAE Transferred (Positive)\n\nhe desserts were very bland\n\nhe desserts were very good\n\nhe desserts were very good\n\nit was a bed of lettuce and\nspinach with some _ italian\nmeats and cheeses\n\nit was a beautiful setting and\njust had a large variety of ger-\nman flavors\n\nit was a huge assortment of fla-\nvors and italian food\n\nhe people behind the counter\nwere not friendly whatsoever\n\nhe best selection behind the\nregister and service presenta-\ntion\n\nhe people behind the counter is\nfriendly caring\n\nhe interior is old and generally\nfalling apart\n\nhe decor is old and now per-\nfectly\n\nhe interior is old and noble\n\nhey are clueless\n\nhey are stoked\n\nhey are genuinely profession-\nals\n\nTable 5: Examples of style transferred sentence generation.\n\nBoW Vocabulary STA CS WO | PPL || GM\n\nFull corpus vocabulary 0.822 | 0.896 | 0.344 | 30 || 0.21\n\nVocabulary without sentiment words 0.872 | 0.901 | 0.359 | 30 || 0.22\n\nVocabulary without stopwords 0.836 | 0.894 | 0.429 | 33 0.22\n\nVocabulary without stopwords and sentiment words |) 0.934 | 0.904 | 0.473 | 32 || 0.24\nTable 6: Analysis of the BoW vocabulary.\n\n434\n\n", "vlm_text": "The table presents transformations of restaurant reviews using two different methods: DAE Transferred and VAE Transferred. It's divided into two sections:\n\n1. **Original (Positive)**: Positive reviews and their transformations to negative sentiment.\n   - DAE Transferred (Negative)\n   - VAE Transferred (Negative)\n\n2. **Original (Negative)**: Negative reviews and their transformations to positive sentiment.\n   - DAE Transferred (Positive)\n   - VAE Transferred (Positive)\n\nEach row compares the original sentiment with how it's changed by the two transfer methods. For instance, a positive review like \"the food is excellent and the service is exceptional\" gets shifted to neutral or negative tones by these methods.\nThe table presents data related to different configurations of Bag of Words (BoW) vocabulary and their corresponding metrics. Here's a breakdown of the table content:\n\n- Column Headers:\n  - **BoW Vocabulary**: Lists the different vocabulary configurations.\n  - **STA**: Numeric metric value for each vocabulary configuration.\n  - **CS**: Another numeric metric value for each configuration.\n  - **WO**: Another numeric metric value for each configuration.\n  - **PPL**: This metric value is presented in a bold font for some configurations.\n  - **GM**: Another metric value for each configuration.\n\n- Rows:\n  - **Full corpus vocabulary**: \n    - STA: 0.822\n    - CS: 0.896\n    - WO: 0.344\n    - PPL: **30**\n    - GM: 0.21\n\n  - **Vocabulary without sentiment words**: \n    - STA: 0.872\n    - CS: 0.901\n    - WO: 0.359\n    - PPL: **30**\n    - GM: 0.22\n\n  - **Vocabulary without stopwords**: \n    - STA: 0.836\n    - CS: 0.894\n    - WO: 0.429\n    - PPL: 33\n    - GM: 0.22\n\n  - **Vocabulary without stopwords and sentiment words**: \n    - STA: **0.934**\n    - CS: **0.904**\n    - WO: **0.473**\n    - PPL: 32\n    - GM: **0.24**\n\nThe table suggests that different BoW vocabulary configurations have different effects on the metrics STA, CS, WO, PPL, and GM. Notably, removing both stopwords and sentiment words seems to yield the best results for STA, CS, WO, and GM metrics."}
