{"page": 0, "image_path": "doc_images/N19-1421_0.jpg", "ocr_text": "COMMONSENSEQA: A Question Answering Challenge Targeting\nCommonsense Knowledge\n\n1,2\n\nAlon Talmor Jonathan Herzig*\n\nth-2\n\nNicholas Lourie? —‘ Jonathan Beran’\n\nSchool of Computer Science, Tel-Aviv University\n?Allen Institute for Artificial Intelligence\n{alontalmor@mail, jonathan.herzig@cs, joberant@cs}.tau.ac.il,\nnicholasl@allenai.org\n\nAbstract\n\nWhen answering a question, people often draw\nupon their rich world knowledge in addi-\ntion to the particular context. Recent work\nhas focused primarily on answering questions\ngiven some relevant document or context,\nand required very little general background.\nTo investigate question answering with prior\nknowledge, we present COMMONSENSEQA:\na challenging new dataset for commonsense\nquestion answering. To capture common sense\nbeyond associations, we extract from CON-\nCEPTNET (Speer et al., 2017) multiple target\nconcepts that have the same semantic relation\nto a single source concept. Crowd-workers\nare asked to author multiple-choice questions\nthat mention the source concept and discrim-\ninate in turn between each of the target con-\ncepts. This encourages workers to create ques-\ntions with complex semantics that often re-\nquire prior knowledge. We create 12,247 ques-\ntions through this procedure and demonstrate\nthe difficulty of our task with a large number\nof strong baselines. Our best baseline is based\non BERT-large (Devlin et al., 2018) and ob-\ntains 56% accuracy, well below human perfor-\nmance, which is 89%.\n\n1 Introduction\n\nWhen humans answer questions, they capitalize\non their common sense and background knowl-\nedge about spatial relations, causes and effects,\nscientific facts and social conventions. For in-\nstance, given the question “Where was Simon\nwhen he heard the lawn mower?”, one can infer\nthat the lawn mower is close to Simon, and that\nit is probably outdoors and situated at street level.\nThis type of knowledge seems trivial for humans,\nbut is still out of the reach of current natural lan-\nguage understanding (NLU) systems.\n\n* The authors contributed equally\n\na) Sample ConceptNet for specific subgraphs\n\ncanyon\n\nb) Crowd source corresponding natural language questions\nand two additional distractors\n\nWhere on a river can you hold a cup upright to catch water on a sunny day?\nv waterfall, X bridge, X valley, X pebble, X mountain\n\nWhere can | stand on a river to see water falling without getting wet?\nX waterfall, V bridge, X valley, X stream, X bottom\n\n1m crossing the river, my feet are wet but my body is dry, where am 1?\nX waterfall, X bridge, Vv valley, X bank, X island\n\nFigure 1: (a) A source concept (‘river’) and three tar-\nget concepts (dashed) are sampled from CONCEPT-\nNET (b) Crowd-workers generate three questions, each\nhaving one of the target concepts for its answer (W),\nwhile the other two targets are not (X). Then, for each\nquestion, workers choose an additional distractor from\nCONCEPTNET (in italics), and author one themselves\n(in bold).\n\nWork on Question Answering (QA) has mostly\nfocused on answering factoid questions, where the\nanswer can be found in a given context with lit-\ntle need for commonsense knowledge (Hermann\net al., 2015; Rajpurkar et al., 2016; Nguyen et al.,\n2016; Joshi et al., 2017). Small benchmarks such\nas the Winograd Scheme Challenge (Levesque,\n2011) and COPA (Roemmele et al., 2011), tar-\ngeted common sense more directly, but have been\ndifficult to collect at scale.\n\nRecently, efforts have been invested in devel-\noping large-scale datasets for commonsense rea-\nsoning. In SWAG (Zellers et al., 2018b), given\na textual description of an event, a probable sub-\nsequent event needs to be inferred. However, it\n\n4149\n\nProceedings of NAACL-HLT 2019, pages 4149-4158\nMinneapolis, Minnesota, June 2 - June 7, 2019. ©2019 Association for Computational Linguistics\n", "vlm_text": "C OMMONSENSE QA: A Question Answering Challenge Targeting Commonsense Knowledge \nAlon Talmor ∗ , , Jonathan Herzig ∗ , Nicholas Lourie 2 Jonathan Berant 1 , \n1 School of Computer Science, Tel-Aviv University 2 Allen Institute for Artiﬁcial Intelligence { alontalmor@mail,jonathan.herzig@cs,joberant@cs } .tau.ac.il , nicholasl@allenai.org \nAbstract \nWhen answering a question, people often draw upon their rich world knowledge in addi- tion to the particular context. Recent work has focused primarily on answering questions given some relevant document or context, and required very little general background. To investigate question answering with prior knowledge, we present C OMMONSENSE QA: a challenging new dataset for commonsense question answering. To capture common sense beyond associations, we extract from C ON - CEPT N ET  ( Speer et al. ,  2017 ) multiple target concepts that have the same semantic relation to a single source concept. Crowd-workers are asked to author multiple-choice questions that mention the source concept and discrim- inate in turn between each of the target con- cepts. This encourages workers to create ques- tions with complex semantics that often re- quire prior knowledge. We create 12,247 ques- tions through this procedure and demonstrate the difﬁculty of our task with a large number of strong baselines. Our best baseline is based on BERT-large ( Devlin et al. ,  2018 ) and ob- tains   $56\\%$   accuracy, well below human perfor- mance, which is  $89\\%$  . \n1 Introduction \nWhen humans answer questions, they capitalize on their common sense and background knowl- edge about spatial relations, causes and effects, scientiﬁc facts and social conventions. For in- stance, given the question  “Where was Simon when he heard the lawn mower?” , one can infer that the lawn mower is close to Simon, and that it is probably outdoors and situated at street level. This type of knowledge seems trivial for humans, but is still out of the reach of current natural lan- guage understanding (NLU) systems. \nThe image is a diagram representing a sample of a ConceptNet that highlights specific subgraphs showing the concept \"river\" at the center. The concept \"river\" is connected to other concepts by \"AtLocation\" relationships. On the left, the concepts connected to \"river\" are \"pebble,\" \"stream,\" \"bank,\" and \"canyon.\" These are marked with red boxes, indicating they are typically found or related to a river's natural environment. On the right, the concepts \"waterfall,\" \"bridge,\" and \"valley\" are connected to \"river\" and are marked with blue dashed boxes, which may suggest their potential locations relative to a river or human-made structures associated with it.\nb) Crowd source corresponding natural language questions  and two additional distractors Where on a  river  can you hold a cup upright to catch water on a sunny day?  $\\checkmark$   waterfall ,    ✘ bridge ,    ✘ valley , ✘ pebble , ✘ mountain Where can I stand on a  river  to see water falling without getting wet?   $\\pmb{\\chi}$  waterfall ,    $\\checkmark$  bridge ,    ✘ valley , ✘ stream , ✘ bottom I’m crossing the  river , my feet are wet but my body is dry, where am I?  ✘ waterfall ,    ✘ bridge ,    ✔ valley , ✘ bank , ✘ island \nFigure 1: (a) A source concept (‘river’) and three tar- get concepts (dashed) are sampled from C ONCEPT - N ET  (b) Crowd-workers generate three questions, each having one of the target concepts for its answer   $(\\checkmark)$  , while the other two targets are not   $({\\pmb X})$  . Then, for each question, workers choose an additional distractor from C ONCEPT N ET  (in italics), and author one themselves (in bold). \nWork on Question Answering (QA) has mostly focused on answering factoid questions, where the answer can be found in a given context with lit- tle need for commonsense knowledge ( Hermann et al. ,  2015 ;  Rajpurkar et al. ,  2016 ;  Nguyen et al. , 2016 ;  Joshi et al. ,  2017 ). Small benchmarks such as the Winograd Scheme Challenge ( Levesque , 2011 ) and COPA ( Roemmele et al. ,  2011 ), tar- geted common sense more directly, but have been difﬁcult to collect at scale. \nRecently, efforts have been invested in devel- oping large-scale datasets for commonsense rea- soning. In SWAG ( Zellers et al. ,  2018b ), given a textual description of an event, a probable sub- sequent event needs to be inferred. However, it has been quickly realized that models trained on large amounts of unlabeled data ( Devlin et al. , 2018 ) capture well this type of information and performance on SWAG is already at human level. VCR ( Zellers et al. ,  2018a ) is another very re- cent attempt that focuses on the visual aspects of common sense. Such new attempts highlight the breadth of commonsense phenomena, and make it evident that research on common sense has only scratched the surface. Thus, there is need for datasets and models that will further our under- standing of what is captured by current NLU mod- els, and what are the main lacunae. "}
{"page": 1, "image_path": "doc_images/N19-1421_1.jpg", "ocr_text": "has been quickly realized that models trained on\nlarge amounts of unlabeled data (Devlin et al.,\n2018) capture well this type of information and\nperformance on SWAG is already at human level.\nVCR (Zellers et al., 2018a) is another very re-\ncent attempt that focuses on the visual aspects of\ncommon sense. Such new attempts highlight the\nbreadth of commonsense phenomena, and make it\nevident that research on common sense has only\nscratched the surface. Thus, there is need for\ndatasets and models that will further our under-\nstanding of what is captured by current NLU mod-\nels, and what are the main lacunae.\n\nIn this work, we present COMMONSENSEQA,\na new dataset focusing on commonsense ques-\ntion answering, based on knowledge encoded in\nCONCEPTNET (Speer et al., 2017). We propose a\nmethod for generating commonsense questions at\nscale by asking crowd workers to author questions\nthat describe the relation between concepts from\nCONCEPTNET (Figure 1). A crowd worker ob-\nserves a source concept (‘River’ in Figure 1) and\nthree target concepts (‘Waterfall’, ‘Bridge’, ‘Val-\nley’) that are all related by the same CONCEPT-\nNET relation (AtLocation). The worker then\nauthors three questions, one per target concept,\nsuch that only that particular target concept is the\nanswer, while the other two distractor concepts are\nnot. This primes the workers to add commonsense\nknowledge to the question, that separates the tar-\nget concept from the distractors. Finally, for each\nquestion, the worker chooses one additional dis-\ntractor from CONCEPTNET, and authors another\ndistractor manually. Thus, in total, five candidate\nanswers accompany each question.\n\nBecause questions are generated freely by\nworkers, they often require background knowl-\nedge that is trivial to humans but is seldom explic-\nitly reported on the web due to reporting bias (Gor-\ndon and Van Durme, 2013). Thus, questions in\nCOMMONSENSEQA have a different nature com-\npared to prior QA benchmarks, where questions\nare authored given an input text.\n\nUsing our method, we collected 12,247 com-\nmonsense questions. We present an analysis that\nillustrates the uniqueness of the gathered ques-\ntions compared to prior work, and the types of\ncommonsense skills that are required for tackling\nit. We extensively evaluate models on COMMON-\nSENSEQA, experimenting with pre-trained mod-\nels, fine-tuned models, and reading comprehen-\n\nsion (RC) models that utilize web snippets ex-\ntracted from Google search on top of the ques-\ntion itself. We find that fine-tuning BERT-LARGE\n(Devlin et al., 2018) on COMMONSENSEQA ob-\ntains the best performance, reaching an accuracy\nof 55.9%. This is substantially lower than human\nperformance, which is 88.9%.\n\nTo summarize, our contributions are:\n\n1. A new QA dataset centered around common\nsense, containing 12,247 examples.\n\n2. A new method for generating commonsense\nquestions at scale from CONCEPTNET.\n\n3. An empirical evaluation of state-of-the-art\nNLU models on COMMONSENSEQA, show-\ning that humans substantially outperform cur-\nrent models.\n\nThe dataset can be downloaded from www.\ntau-nlp.org/commonsenseqa. The code\nfor all our baselines is available at github.\ncom/ jonathanherzig/commonsensegqa.\n\n2 Related Work\n\nMachine common sense, or the knowledge of and\nability to reason about an open ended world, has\nlong been acknowledged as a critical component\nfor natural language understanding. Early work\nsought programs that could reason about an envi-\nronment in natural language (McCarthy, 1959), or\nleverage a world-model for deeper language un-\nderstanding (Winograd, 1972). Many common-\nsense representations and inference procedures\nhave been explored (McCarthy and Hayes, 1969;\nKowalski and Sergot, 1986) and large-scale com-\nmonsense knowledge-bases have been developed\n(Lenat, 1995; Speer et al., 2017). However, evalu-\nating the degree of common sense possessed by a\nmachine remains difficult.\n\nOne important benchmark, the Winograd\nSchema Challenge (Levesque, 2011), asks mod-\nels to correctly solve paired instances of coref-\nerence resolution. While the Winograd Schema\nChallenge remains a tough dataset, the difficulty\nof generating examples has led to only a small\navailable collection of 150 examples. The Choice\nof Plausible Alternatives (COPA) is a similarly im-\nportant but small dataset consisting of 500 devel-\nopment and 500 test questions (Roemmele et al.,\n2011). Each question asks which of two alterna-\ntives best reflects a cause or effect relation to the\npremise. For both datasets, scalability is an issue\nwhen evaluating modern modeling approaches.\n\n4150\n", "vlm_text": "\nIn this work, we present C OMMONSENSE QA, a new dataset focusing on commonsense ques- tion answering, based on knowledge encoded in C ONCEPT N ET  ( Speer et al. ,  2017 ). We propose a method for generating commonsense questions at scale by asking crowd workers to author questions that describe the relation between concepts from C ONCEPT N ET  (Figure  1 ). A crowd worker ob- serves a source concept ( ‘River’  in Figure  1 ) and three target concepts ( ‘Waterfall’ ,  ‘Bridge’ ,  ‘Val- ley’ ) that are all related by the same C ONCEPT - N ET  relation ( AtLocation ). The worker then authors three questions, one per target concept, such that only that particular target concept is the answer, while the other two distractor concepts are not. This primes the workers to add commonsense knowledge to the question, that separates the tar- get concept from the distractors. Finally, for each question, the worker chooses one additional dis- tractor from C ONCEPT N ET , and authors another distractor manually. Thus, in total, ﬁve candidate answers accompany each question. \nBecause questions are generated freely by workers, they often require background knowl- edge that is trivial to humans but is seldom explic- itly reported on the web due to reporting bias ( Gor- don and Van Durme ,  2013 ). Thus, questions in C OMMONSENSE QA have a different nature com- pared to prior QA benchmarks, where questions are authored given an input text. \nUsing our method, we collected 12,247 com- monsense questions. We present an analysis that illustrates the uniqueness of the gathered ques- tions compared to prior work, and the types of commonsense skills that are required for tackling it. We extensively evaluate models on C OMMON - SENSE QA, experimenting with pre-trained mod- els, ﬁne-tuned models, and reading comprehen- sion (RC) models that utilize web snippets ex- tracted from Google search on top of the ques- tion itself. We ﬁnd that ﬁne-tuning BERT- LARGE ( Devlin et al. ,  2018 ) on C OMMONSENSE QA ob- tains the best performance, reaching an accuracy of   $55.9\\%$  . This is substantially lower than human performance, which is   $88.9\\%$  . \n\nTo summarize, our contributions are: 1. A new QA dataset centered around common sense, containing 12,247 examples. 2. A new method for generating commonsense questions at scale from C ONCEPT N ET . 3. An empirical evaluation of state-of-the-art NLU models on C OMMONSENSE QA, show- ing that humans substantially outperform cur- rent models. \n2 Related Work \nMachine common sense, or the knowledge of and ability to reason about an open ended world, has long been acknowledged as a critical component for natural language understanding. Early work sought programs that could reason about an envi- ronment in natural language ( McCarthy ,  1959 ), or leverage a world-model for deeper language un- derstanding ( Winograd ,  1972 ). Many common- sense representations and inference procedures have been explored ( McCarthy and Hayes ,  1969 ; Kowalski and Sergot ,  1986 ) and large-scale com- monsense knowledge-bases have been developed ( Lenat ,  1995 ;  Speer et al. ,  2017 ). However, evalu- ating the degree of common sense possessed by a machine remains difﬁcult. \nOne important benchmark, the Winograd Schema Challenge ( Levesque ,  2011 ), asks mod- els to correctly solve paired instances of coref- erence resolution. While the Winograd Schema Challenge remains a tough dataset, the difﬁculty of generating examples has led to only a small available collection of 150 examples. The Choice of Plausible Alternatives (COPA) is a similarly im- portant but small dataset consisting of 500 devel- opment and 500 test questions ( Roemmele et al. , 2011 ). Each question asks which of two alterna- tives best reﬂects a cause or effect relation to the premise. For both datasets, scalability is an issue when evaluating modern modeling approaches. "}
{"page": 2, "image_path": "doc_images/N19-1421_2.jpg", "ocr_text": "With the recent adoption of crowdsourcing, sev-\neral larger datasets have emerged, focusing on pre-\ndicting relations between situations or events in\nnatural language. JHU Ordinal Commonsense In-\nference requests a label from 1-5 for the plau-\nsibility that one situation entails another (Zhang\net al., 2017). The Story Cloze Test (also referred to\nas ROC Stories) pits ground-truth endings to sto-\nries against implausible false ones (Mostafazadeh\net al., 2016). Interpolating these approaches, Sit-\nuations with Adversarial Generations (SWAG),\nasks models to choose the correct description of\nwhat happens next after an initial event (Zellers\net al., 2018b). LM-based techniques achieve very\nhigh performance on the Story Cloze Test and\nSWAG by fine-tuning a pre-trained LM on the tar-\nget task (Radford et al., 2018; Devlin et al., 2018).\n\nInvestigations of commonsense datasets, and of\nnatural language datasets more generally, have re-\nvealed the difficulty in creating benchmarks that\nmeasure the understanding of a program rather\nthan its ability to take advantage of distributional\nbiases, and to model the annotation process (Gu-\nrurangan et al., 2018; Poliak et al., 2018). Annota-\ntion artifacts in the Story Cloze Test, for example,\nallow models to achieve high performance while\nonly looking at the proposed endings and ignor-\ning the stories (Schwartz et al., 2017; Cai et al.,\n2017). Thus, the development of benchmarks for\ncommon sense remains a difficult challenge.\n\nResearchers have also investigated question an-\nswering that utilizes common sense. Science ques-\ntions often require common sense, and have re-\ncently received attention (Clark et al., 2018; Mi-\nhaylov et al., 2018; Ostermann et al., 2018); how-\never, they also need specialized scientific knowl-\nedge. In contrast to these efforts, our work stud-\nies common sense without requiring additional\ninformation. SQUABU created a small hand-\ncurated test of common sense and science ques-\ntions (Davis, 2016), which are difficult for current\ntechniques to solve. In this work, we create simi-\nlarly well-crafted questions but at a larger scale.\n\n3 Dataset Generation\n\nOur goal is to develop a method for generating\nquestions that can be easily answered by humans\nwithout context, and require commonsense knowl-\nedge. We generate multiple-choice questions in a\nprocess that comprises the following steps.\n\n1. We extract subgraphs from CONCEPTNET,\n\nEB Crowaworkers author questions BR Crowaworkers add distractors\n\nDust in house? (attic, yard, street) Dust in house? (attic, yard, street, bed, desert)\n\nFind glass outside? (bar, fork, car) Find glass outside? (bar, fork, car, sand, wine)\n\nMakes you happy? (laugh, sad, fall) Makes you happy? (laugh, sad, fall, blue, feel)\n\nExtract subgraphs from ConceptNet 2 _Crowdworkers filter questions by quality\n— as\n\nSust_J{_attic_j{_yard_J{_street Dust in house? (attic, yard, ..) > 1.0\nSs 2\n\nMea RaCe Find glass outside? (bar, fork, ..) 30.2 X\n\nMakes you happy? (laugh, sad, ...) > 0.8\n\noe\nTaugh )(_sad_)(_fall\n\nt\n\nFilter edges from ConceptNet with rules\n\nCrappy\n\nQ collect relevant snippets via search engine\n\nDust in house? (attic, yard, ..)\n\nMakes you happy? (laugh, sad, ...)\n\n@ G\n\nFigure 2: COMMONSENSEQA generation process.\nThe input is CONCEPTNET knowledge base, and the\noutput is a set of multiple-choice questions with corre-\nsponding relevant context (snippets).\n\neach with one source concept and three tar-\nget concepts.\n\n2. We ask crowdsourcing workers to author\nthree questions per subgraph (one per target\nconcept), to add two additional distractors per\nquestion, and to verify questions’ quality.\n\n3. We add textual context to each question by\nquerying a search engine and retrieving web\nsnippets.\n\nThe entire data generation process is summarized\nin Figure 2. We now elaborate on each of the steps:\n\nExtraction from CONCEPTNET CONCEPT-\nNET is a graph knowledge-base G C C x R x C,\nwhere the nodes C represent natural language con-\ncepts, and edges R represent commonsense re-\nlations. Triplets (ci,7r,c2) carry commonsense\nknowledge such as ‘(gambler, CapableOf, lose\nmoney)’. _CONCEPTNET contains 32 million\ntriplets. To select a subset of triplets for crowd-\nsourcing we take the following steps:\n\n1. We filter triplets with general relations (e.g.,\nRelatedTo) or relations that are already\nwell-explored in NLP (e.g., IsA). In total we\nuse 22 relations.\n\n2. We filter triplets where one of the concepts is\nmore than four words or not in English.\n\n3. We filter triplets where the edit distance be-\ntween c; and c2 is too low.\n\nThis results in a set of 236,208 triplets (q,r, a),\nwhere we call the first concept the question con-\ncept and the second concept the answer concept.\n\nWe aim to generate questions that contain the\n\n4151\n", "vlm_text": "With the recent adoption of crowdsourcing, sev- eral larger datasets have emerged, focusing on pre- dicting relations between situations or events in natural language. JHU Ordinal Commonsense In- ference requests a label from 1-5 for the plau- sibility that one situation entails another ( Zhang et al. ,  2017 ). The Story Cloze Test (also referred to as ROC Stories) pits ground-truth endings to sto- ries against implausible false ones ( Mostafazadeh et al. ,  2016 ). Interpolating these approaches, Sit- uations with Adversarial Generations (SWAG), asks models to choose the correct description of what happens next after an initial event ( Zellers et al. ,  2018b ). LM-based techniques achieve very high performance on the Story Cloze Test and SWAG by ﬁne-tuning a pre-trained LM on the tar- get task ( Radford et al. ,  2018 ;  Devlin et al. ,  2018 ). \nInvestigations of commonsense datasets, and of natural language datasets more generally, have re- vealed the difﬁculty in creating benchmarks that measure the understanding of a program rather than its ability to take advantage of distributional biases, and to model the annotation process ( Gu- rurangan et al. ,  2018 ;  Poliak et al. ,  2018 ). Annota- tion artifacts in the Story Cloze Test, for example, allow models to achieve high performance while only looking at the proposed endings and ignor- ing the stories ( Schwartz et al. ,  2017 ;  Cai et al. , 2017 ). Thus, the development of benchmarks for common sense remains a difﬁcult challenge. \nResearchers have also investigated question an- swering that utilizes common sense. Science ques- tions often require common sense, and have re- cently received attention ( Clark et al. ,  2018 ;  Mi- haylov et al. ,  2018 ;  Ostermann et al. ,  2018 ); how- ever, they also need specialized scientiﬁc knowl- edge. In contrast to these efforts, our work stud- ies common sense without requiring additional information. SQUABU created a small hand- curated test of common sense and science ques- tions ( Davis ,  2016 ), which are difﬁcult for current techniques to solve. In this work, we create simi- larly well-crafted questions but at a larger scale. \n3 Dataset Generation \nOur goal is to develop a method for generating questions that can be easily answered by humans without context, and require commonsense knowl- edge. We generate multiple-choice questions in a process that comprises the following steps. \n1. We extract subgraphs from C ONCEPT N ET , \nThe image illustrates the process of generating questions for Commonsense QA using the ConceptNet knowledge base. \n\n1. **Crowdworkers Author Questions**: \n   - Create questions like \"Dust in house?\" with options such as (attic, yard, street).\n\n2. **Extract Subgraphs from ConceptNet**: \n   - Concepts like \"dust\", \"attic\", \"yard\", and \"street\" are mapped.\n\n3. **Filter Edges with Rules**: \n   - Irrelevant connections are removed.\n\n4. **Crowdworkers Add Distractors**: \n   - Include additional options such as \"bed, desert\" for the question on dust.\n\n5. **Crowdworkers Filter Questions by Quality**: \n   - Determine the relevance and quality of questions with scores.\n\n6. **Collect Relevant Snippets via Search Engine**: \n   - Find contextual information for high-quality questions. \n\nThe image flow begins with question creation and ends with gathering relevant context.\neach with one source concept and three tar- get concepts. 2. We ask crowdsourcing workers to author three questions per subgraph (one per target concept), to add two additional distractors per question, and to verify questions’ quality. 3. We add textual context to each question by querying a search engine and retrieving web snippets. \nThe entire data generation process is summarized in Figure  2 . We now elaborate on each of the steps: \nExtraction from C ONCEPT N ET C ONCEPT - N ET  is a graph k owledge-base    $G\\subseteq\\mathcal{C}\\times\\mathcal{R}\\times\\mathcal{C}$  , where the nodes  C present natural language con- cepts, and edges  R  represent commonsense re- lations. Triplets    $\\left(c_{1},r,c_{2}\\right)$   carry commonsense knowledge such as ‘( gambler ,  CapableOf ,  lose money )’. C ONCEPT N ET  contains 32 million triplets. To select a subset of triplets for crowd- sourcing we take the following steps: \n1. We ﬁlter triplets with general relations (e.g., RelatedTo ) or relations that are already well-explored in NLP (e.g.,  IsA ). In total we use 22 relations. 2. We ﬁlter triplets where one of the concepts is more than four words or not in English. 3. We ﬁlter triplets where the edit distance be- tween    $c_{1}$   and    $c_{2}$   is too low. This results in a set of 236,208 triplets    $(q,r,a)$  , where we call the ﬁrst concept the  question con- cept  and the second concept the  answer concept . We aim to generate questions that contain the question concept and where the answer is the an- swer concept. To create multiple-choice questions we need to choose  distractors  for each question. Sampling distractors at random from C ONCEPT - N ET  is a bad solution, as such distractors are easy to eliminate using simple surface clues. "}
{"page": 3, "image_path": "doc_images/N19-1421_3.jpg", "ocr_text": "question concept and where the answer is the an-\nswer concept. To create multiple-choice questions\nwe need to choose distractors for each question.\nSampling distractors at random from CONCEPT-\nNET is a bad solution, as such distractors are easy\nto eliminate using simple surface clues.\n\nTo remedy this, we propose to create ques-\nfor each question concept q and\nrelation + we group three different triplets\n{(q.7, a1), (4,7, 42), (q,7r,a3)} (see Figure 1).\nThis generates three answer concepts that are se-\nmantically similar and have a similar relation to\nthe question concept g. This primes crowd work-\ners to formulate questions that require background\nknowledge about the concepts in order to answer\nthe question.\n\nThe above procedure generates approximately\n130,000 triplets (43,000 question sets), for which\nwe can potentially generate questions.\n\ntion sets:\n\nCrowdsourcing questions We used Amazon\nMechanical Turk (AMT) workers to generate and\nvalidate commonsense questions.\n\nAMT workers saw, for every question set, the\nquestion concept and three answer concepts. They\nwere asked to formulate three questions, where\nall questions contain the question concept. Each\nquestion should have as an answer one of the an-\nswer concepts, but not the other two. To discour-\nage workers from providing simple surface clues\nfor the answer, they were instructed to avoid us-\ning words that have a strong relation to the answer\nconcept, for example, not to use the word ‘open’\nwhen the answer is ‘door’.\n\nFormulating questions for our task is non-\ntrivial. Thus, we only accept annotators for which\nat least 75% of the questions they formulate pass\nthe verification process described below.\n\nAdding additional distractors To make the\ntask more difficult, we ask crowd-workers to add\ntwo additional incorrect answers to each formu-\nlated question. One distractor is selected from a\nset of answer concepts with the same relation to\nthe question concept in CONCEPTNET (Figure 1,\nin red). The second distractor is formulated man-\nually by the workers themselves (Figure 1, in pur-\nple). Workers were encouraged to formulate a dis-\ntractor that would seem plausible or related to the\nquestion but easy for humans to dismiss as incor-\nrect. In total, each formulated question is accom-\npanied with five candidate answers, including one\n\nMeasurement Value\n# CONCEPTNET distinct question nodes 2,254\n# CONCEPTNET distinct answer nodes 12,094\n# CONCEPTNET distinct nodes 12,107\n\n# CONCEPTNET distinct relation lables 22\n\naverage question length (tokens) 13.41\nlong questions (more than 20 tokens) 10.3%\naverage answer length (tokens) 1.5\n# answers with more than | token 44%\n# of distinct words in questions 14,754\n# of distinct words in answers 4,911\n\nTable 1: Key statistics for COMMONSENSEQA\n\ncorrect answer and four distractors.\n\nVerifying questions quality We train a disjoint\ngroup of workers to verify the generated questions.\nVerifiers annotate a question as unanswerable, or\nchoose the right answer. Each question is veri-\nfied by 2 workers, and only questions verified by at\nleast one worker that answered correctly are used.\nThis processes filters out 15% of the questions.\n\nAdding textual context To examine whether\nweb text is useful for answering commonsense\nquestions, we add textual information to each\nquestion in the following way: We issue a web\nquery to Google search for every question and\ncandidate answer, concatenating the answer to the\nquestion, e.g., ‘What does a parent tell their child\nto do after they’ve played with a lot of toys? +\n“clean room”’. We take the first 100 result snip-\npets for each of the five answer candidates, yield-\ning a context of 500 snippets per question. Using\nthis context, we can investigate the performance\nof reading comprehension (RC) models on COM-\nMONSENSEQA.\n\nOverall, we generated 12,247 final examples,\nfrom a total of 16,242 that were formulated. The\ntotal cost per question is $0.33. Table 1 describes\nthe key statistics of COMMONSENSEQA.\n\n4 Dataset Analysis\n\nCONCEPTNET concepts and relations Com-\nMONSENSEQA builds on CONCEPTNET, which\ncontains concepts such as dog, house, or row\nboat, connected by relations such as Causes,\nCapableOf, or Antonym. The top-5 ques-\ntion concepts in COMMONSENSEQA are ‘Person’\n(3.1%), ‘People’ (2.0%), ‘Human’ (0.7%), ‘Water’\n(0.5%) and ‘Cat’ (0.5%). In addition, we present\nthe main relations along with the percentage of\nquestions generated from them in Table 2. It’s\n\n4152\n", "vlm_text": "\nTo remedy this, we propose to create  ques- tion sets : for each question concept    $q$   and relation    $r$  we group three different triplets  $\\{(q,r,a_{1}),(q,r,a_{2}),(q,r,a_{3})\\}$   (see Figure  1 ). This generates three answer concepts that are se- mantically similar and have a similar relation to the question concept    $q$  . This primes crowd work- ers to formulate questions that require background knowledge about the concepts in order to answer the question. \nThe above procedure generates approximately 130,000 triplets (43,000 question sets), for which we can potentially generate questions. \nCrowdsourcing questions We used Amazon Mechanical Turk (AMT) workers to generate and validate commonsense questions. \nAMT workers saw, for every question set, the question concept and three answer concepts. They were asked to formulate three questions, where all questions contain the question concept. Each question should have as an answer one of the an- swer concepts, but not the other two. To discour- age workers from providing simple surface clues for the answer, they were instructed to avoid us- ing words that have a strong relation to the answer concept, for example, not to use the word  ‘open’ when the answer is  ‘door’ . \nFormulating questions for our task is non- trivial. Thus, we only accept annotators for which at least   $75\\%$   of the questions they formulate pass the veriﬁcation process described below. \nAdding additional distractors To make the task more difﬁcult, we ask crowd-workers to add two additional incorrect answers to each formu- lated question. One distractor is selected from a set of answer concepts with the same relation to the question concept in C ONCEPT N ET  (Figure  1 , in red). The second distractor is formulated man- ually by the workers themselves (Figure  1 , in pur- ple). Workers were encouraged to formulate a dis- tractor that would seem plausible or related to the question but easy for humans to dismiss as incor- rect. In total, each formulated question is accom- panied with ﬁve candidate answers, including one \nThe table contains various measurements related to ConceptNet. Here's a summary:\n\n- **# CONCEPTNET distinct question nodes:** 2,254\n- **# CONCEPTNET distinct answer nodes:** 12,094\n- **# CONCEPTNET distinct nodes:** 12,107\n- **# CONCEPTNET distinct relation labels:** 22\n- **Average question length (tokens):** 13.41\n- **Long questions (more than 20 tokens):** 10.3%\n- **Average answer length (tokens):** 1.5\n- **# answers with more than 1 token:** 44%\n- **# of distinct words in questions:** 14,754\n- **# of distinct words in answers:** 4,911\ncorrect answer and four distractors. \nVerifying questions quality We train a disjoint group of workers to verify the generated questions. Veriﬁers annotate a question as unanswerable, or choose the right answer. Each question is veri- ﬁed by 2 workers, and only questions veriﬁed by at least one worker that answered correctly are used. This processes ﬁlters out   $15\\%$   of the questions. \nAdding textual context To examine whether web text is useful for answering commonsense questions, we add textual information to each question in the following way: We issue a web query to Google search for every question and candidate answer, concatenating the answer to the question, e.g.,  ‘What does a parent tell their child to do after they’ve played with a lot of toys?   $^+$  “clean room”’ . We take the ﬁrst 100 result snip- pets for each of the ﬁve answer candidates, yield- ing a context of 500 snippets per question. Using this context, we can investigate the performance of reading comprehension (RC) models on C OM - MONSENSE QA. \nOverall, we generated 12,247 ﬁnal examples, from a total of 16,242 that were formulated. The total cost per question is  $\\S0.33$  . Table  1  describes the key statistics of C OMMONSENSE QA. \n4 Dataset Analysis \nC ONCEPT N ET  concepts and relations C OM - MONSENSE QA builds on C ONCEPT N ET , which contains  concepts  such as  dog ,  house , or  row boat , connected by  relations  such as  Causes , CapableOf , or  Antonym . The top-5 ques- tion concepts in C OMMONSENSE QA are  ‘Person’\n\n  $(3.1\\%)$  ,  ‘People’  $(2.0\\%)$  ,  ‘Human’    $(0.7\\%)$  ,  ‘Water’\n\n  $(0.5\\%)$   and  ‘Cat’    $(0.5\\%)$  . In addition, we present the main relations along with the percentage of questions generated from them in Table  2 . It’s "}
{"page": 4, "image_path": "doc_images/N19-1421_4.jpg", "ocr_text": "Relation Formulated question example %\nAtLocation Where would I not want a fox? A. hen house, B. england, C. mountains, D. ... 473\nCauses What is the hopeful result of going to see a play? A. being entertained, B. meet, C. sit, D. ... 173\nCapableof Why would a person put flowers in a room with dirty gym socks? A. smell good, B. many colors, C. continue to grow , D. ... 9.4\nAntonym Someone who had a very bad flight might be given a trip in this to make up for it? A. first class, B. reputable, C. propitious , D. ... 8.5\nHasSubevent How does a person begin to attract another person for reproducing? A. kiss, B. genetic mutation, C. have sex , D. ... 3.6\nHasPrerequisite | If/ am tilting a drink toward my face, what should I do before the liquid spills over? A. open mouth, B. eat first, C. use glass ,D.... | 3.3\nCausesDesire What do parents encourage kids to do when they experience boredom? A. read book, B. sleep, C. travel , D. ... 24\nDesires What do all humans want to experience in their own home? A. fee! comfortable, B. work hard, C. fall in love , D. ... 17\nPartof What would someone wear to protect themselves from a cannon? A. body armor, B. tank, C. hat , D. ... 1.6\nHasProperty What is a reason to pay your television bill? A. legal, B. obsolete, C. entertaining , D. ... 12\n\nTable 2: Top CONCEPTNET relations in COMMONSENSEQA, along with their frequency in the data and an exam-\n\nple question. The first answer (A) is the correct answer\nQ. Where are Rosebushes typically found outside of large buildings? Category Definition %e\n\n: Spatial Concept A appears near Concept B a\nCHa parts_,s*\"44 Spatial _>), Is member of_) Cause & Effect | Concept A causes Concept B 23\n2\nBuilding Courtyard Flowers Rosebushes Has parts Concept A contains Concept B as one of its parts | 23\nIs member of _ | Concept A belongs to the larger class of Concept B | 17\nPurpose Concept A is the purpose of Concept B 18\nQ. Where would you get a Balalaika if you do not have one? Social Ibis a social convention that Concept A 15\nCe smember of) Spatial ty _Purpose_>) correlates with Concept B\nteat Activity Concept A is an activity performed in the context 8\nBalalaika Instrument Music store Get instruments of Concept B\nDefinition Concept A is a definition of Concept B 6\nQ. I want to use string to keep something from moving, how should I do it? Preconditions whence must hold true in order for Concept B to | 3\noO Spatial O Activity pa” \"* Cause & effect\nSomething String Tie around Keep from moving Table 3: Skills and their frequency in the sampled data.\n\nFigure 3: Examples of manually-annotated questions,\nwith the required skills needed to arrive at the answers\n(red circles). Skills are labeled edges, and concepts are\nnodes.\n\nworth noting that since question formulators were\nnot shown the CONCEPTNET relation, they often\nasked questions that probe other relationships be-\ntween the concepts. For example, the question\n“What do audiences clap for?” was generated\nfrom the AtLocation relation, but focuses on\nsocial conventions instead.\n\nQuestion formulation Question formulators\nwere instructed to create questions with high\nlanguage variation. 122 formulators contributed\nto question generation. However, 10 workers\nformulated more than 85% of the questions.\n\nWe analyzed the distribution of first and second\nwords in the formulated questions along with ex-\nample questions. Figure 4 presents the breakdown.\nInterestingly, only 44% of the first words are WH-\nwords. In about 5% of the questions, formulators\nused first names to create a context story, and in\n7% they used the word “if” to present a hypothet-\nical question. This suggests high variability in the\nquestion language.\n\nCommonsense Skills To analyze the types of\ncommonsense knowledge needed to correctly an-\n\nAs each example can be annotated with multiple skills,\nthe total frequency does not sum to 100%.\n\nswer questions in COMMONSENSEQA, we ran-\ndomly sampled 100 examples from the develop-\nment set and performed the following analysis.\nFor each question, we explicitly annotated the\ntypes of commonsense skills that a human uses\nto answer the question. We allow multiple com-\nmonsense skills per questions, with an average of\n1.75 skills per question. Figure 3 provides three\nexample annotations. Each annotation contains a\nnode for the answer concept, and other nodes for\nconcepts that appear in the question or latent con-\ncepts. Labeled edges describe the commonsense\nskill that relates the two nodes. We defined com-\nmonsense skills based on the analysis of LoBue\nand Yates (2011), with slight modifications to ac-\ncommodate the phenomena in our data. Table 3\npresents the skill categories we used, their defini-\ntion and their frequency in the analyzed examples.\n\n5 Baseline Models\n\nOur goal is to collect a dataset of commonsense\nquestions that are easy for humans, but hard for\ncurrent NLU models. To evaluate this, we experi-\nment with multiple baselines. Table 4 summarizes\nthe various baseline types and characterizes them\nbased on (a) whether training is done on COM-\nMONSENSEQA or the model is fully pre-trained,\n\n4153\n", "vlm_text": "The table consists of three columns:\n\n1. **Relation**: Types of semantic relationships, such as \"AtLocation,\" \"Causes,\" \"CapableOf,\" etc.\n2. **Formulated question example**: Example questions related to each relation, with multiple-choice answers (A, B, C, D).\n3. **%**: The percentage associated with each relation, indicating the frequency or relevance, with numbers like 47.3, 17.3, 9.4, etc.\nThe image contains a diagram illustrating the relationship between questions and the reasoning skills required to answer them. Each diagram is based on a single question and shows a network of nodes and edges. The nodes represent different concepts, and the edges represent the skills necessary to connect these concepts to formulate an answer. These skills are labeled on the edges and are highlighted with red circles for emphasis.\n\n- The first question asks, \"Where are Rosebushes typically found outside of large buildings?\" The concepts involved are \"Building,\" \"Courtyard,\" \"Flowers,\" and \"Rosebushes,\" connected by skills labeled as \"Has parts,\" \"Spatial,\" and \"Is member of.\"\n\n- The second question is, \"Where would you get a Balalaika if you do not have one?\" The concepts \"Balalaika,\" \"Instrument,\" \"Music store,\" and \"Get instruments\" are linked by skills labeled \"Is member of,\" \"Spatial,\" and \"Purpose.\"\n\n- The third question states, \"I want to use string to keep something from moving, how should I do it?\" The related concepts are \"Something,\" \"String,\" \"Tie around,\" and \"Keep from moving,\" connected by \"Spatial,\" \"Activity,\" and \"Cause & effect\" skills.\n\nOverall, the diagram demonstrates how various logical reasoning skills connect different ideas to provide answers to complex questions.\nworth noting that since question formulators were not shown the C ONCEPT N ET  relation, they often asked questions that probe other relationships be- tween the concepts. For example, the question “What do  audiences  clap for?”  was generated from the  AtLocation  relation, but focuses on social conventions instead. \nQuestion formulation Question formulators were instructed to create questions with high language variation. 122 formulators contributed to question generation. However, 10 workers formulated more than   $85\\%$   of the questions. \nWe analyzed the distribution of ﬁrst and second words in the formulated questions along with ex- ample questions. Figure  4  presents the breakdown. Interestingly, only  $44\\%$   of the ﬁrst words are WH- words. In about  $5\\%$   of the questions, formulators used ﬁrst names to create a context story, and in  $7\\%$   they used the word    $\"i f\"$   to present a hypothet- ical question. This suggests high variability in the question language. \nCommonsense Skills To analyze the types of commonsense knowledge needed to correctly an- \nThe image is a table titled \"Table 3: Skills and their frequency in the sampled data.\" It lists categories, their definitions, and the corresponding percentages:\n\n- **Spatial**: Concept A appears near Concept B (41%)\n- **Cause & Effect**: Concept A causes Concept B (23%)\n- **Has parts**: Concept A contains Concept B as one of its parts (23%)\n- **Is member of**: Concept A belongs to the larger class of Concept B (17%)\n- **Purpose**: Concept A is the purpose of Concept B (18%)\n- **Social**: It is a social convention that Concept A correlates with Concept B (15%)\n- **Activity**: Concept A is an activity performed in the context of Concept B (8%)\n- **Definition**: Concept A is a definition of Concept B (6%)\n- **Preconditions**: Concept A must hold true in order for Concept B to take place (3%)\nswer questions in C OMMONSENSE QA, we ran- domly sampled 100 examples from the develop- ment set and performed the following analysis. \nFor each question, we explicitly annotated the types of commonsense skills that a human uses to answer the question. We allow multiple com- monsense skills per questions, with an average of 1.75 skills per question. Figure  3  provides three example annotations. Each annotation contains a node for the answer concept, and other nodes for concepts that appear in the question or latent con- cepts. Labeled edges describe the commonsense skill that relates the two nodes. We deﬁned com- monsense skills based on the analysis of  LoBue and Yates  ( 2011 ), with slight modiﬁcations to ac- commodate the phenomena in our data. Table  3 presents the skill categories we used, their deﬁni- tion and their frequency in the analyzed examples. \n5 Baseline Models \nOur goal is to collect a dataset of commonsense questions that are easy for humans, but hard for current NLU models. To evaluate this, we experi- ment with multiple baselines. Table  4  summarizes the various baseline types and characterizes them based on (a) whether training is done on C OM - MONSENSE QA or the model is fully pre-trained, "}
{"page": 5, "image_path": "doc_images/N19-1421_5.jpg", "ocr_text": "EP\na\nQ\nQ\n=i\n\na\n=\n\n=\ng\n3\n2\nci\n8\n8\noO\n®\n8\n38\nEa\n5\na\n9\n2\nEa\nZ\no|\n\nhad what>\n\nObably most happy to\nf snow on what?\n\nthey do?\n\ndairy what should\n\nIf a person needs food from a\n\nFigure 4: Distribution of the first and second words in questions. The inner part displays words and their frequency\n\nand the outer part provides example questions.\n\nModel\nVECSIM\nLMIB\nQABILINEAR\nQACOMPARE\nESIM\n\nGPT\n\nBERT\nBIDAF++\n\nTraining Context\n\nx\n\nS\\N NN Ax\nQ&w wR KK\n\nTable 4: Baseline models along with their character-\nistics. Training states whether the model was trained\non COMMONSENSEQA, or was only trained a differ-\nent dataset. Context states whether the model uses extra\ncontext as input.\n\nand (b) whether context (web snippets) is used.\nWe now elaborate on the different baselines.\n\na VECSIM A model that chooses the answer with\nhighest cosine similarity to the question, where the\nquestion and answers are represented by an aver-\nage of pre-trained word embeddings.\n\nb LM1B Inspired by Trinh and Le (2018), we\nemploy a large language model (LM) from Joze-\nfowicz et al. (2016), which was pre-trained on\nthe One Billion Words Benchmark (Chelba et al.,\n2013). We use this model in two variations. In\nthe first (LM1B-CONCAT), we simply concate-\nnate each answer to the question. In the second\n(LM 1B-REP), we first cluster questions according\nto their first two words. Then, we recognize five\nhigh-frequency prefixes that cover 35% of the de-\nvelopment set (e.g., “what is”). We rephrase ques-\ntions that fit into one of these prefixes as a declar-\native sentence that contains the answer. E.g., we\n\n4\n\nrephrase “What is usually next to a door?” and the\ncandidate answer “wall” to “Wall is usually next\nto a door”. For questions that do not start with\nthe above prefixes, we concatenate the answer as\nin LM1B-concat. In both variations we return\nthe answer with highest LM probability.\n\n¢ QABILINEAR This model, propsed by Yu et al.\n(2014) for QA, scores an answer a; with a bilinear\nmodel: qWa; , where the question g and answers\na; are the average pre-trained word embeddings\nand W is a learned parameter matrix. A softmax\nlayer over the candidate answers is used to train\nthe model with cross-entropy loss.\n\nd QACOMPARE This model is similar to an NLI\nmodel from Liu et al. (2016). The model repre-\nsents the interaction between the question g and a\ncandidate answer a; as: h = relu([q; ai; q@ai; q—\na;]W + b;), where ’;’ denotes concatenation and\n© is element-wise product. Then, the model pre-\ndicts an answer score using a feed forward layer:\nhW, + by. Average pre-trained embeddings and\nsoftmax are used to train the model.\n\ne ESIM We use ESIM, a strong NLI model\n(Chen et al., 2016). Similar to Zellers et al.\n(2018b), we change the output layer size to the\nnumber of candidate answers, and apply softmax\nto train with cross-entropy loss.\n\nf BIDAF++ A state-of-the-art RC model, that\nuses the retrieved Google web snippets (Section 3)\nas context. We augment BIDAF (Seo et al., 2016)\nwith a self-attention layer and ELMo representa-\n\n154\n", "vlm_text": "This image is a visual representation of the distribution of the first and second words in questions. It is a semicircular diagram or chart with different sections, each representing the frequency of certain first words in questions and example questions using those words. \n\nThe central section labeled \"Other\" occupies the largest segment, accounting for 37% of the distribution. The other sections are surrounded by lighter areas with example questions, labeled with percentages indicating their frequency:\n\n- \"The\" makes up 13% of the cases, with example questions like \"The tourist was probably most happy to capture pictures of snow on what?\"\n- \"If\" accounts for 7%, e.g., \"If a person needs food from a dairy, what should they do?\"\n- \"What\" appears in 21% of the questions, e.g., \"What could bringing suit do to a rivalry?\"\n- \"Where\" is used in 18%, e.g., \"Where on a river can a human hold a cup upright to catch water on a sunny, clear day?\"\n- Other sections include smaller percentages like \"What would\" at 5%, \"James\" and \"John\" both at 2%, \"Why\" at 2%, and \"A\" at 3% with additional subcategories such as \"A Person\" at 0.5%.\n\nThe example questions provide context for each word's usage in forming queries, such as \"John buys a 5 pound block of salt. Where is he likely to keep it?\" under the \"Where\" category.\nThis table lists different models and indicates whether they involve \"Training\" and \"Context.\" The models included are:\n\n- VecSim\n- LM1B\n- QABilinear\n- QACompare\n- ESIM\n- GPT\n- BERT\n- BIDAF++\n\nCheckmarks (✓) indicate the presence of a characteristic (Training or Context), while crosses (✗) indicate its absence.\nTable 4: Baseline models along with their character- istics.  Training  states whether the model was trained on C OMMONSENSE QA, or was only trained a differ- ent dataset.  Context  states whether the model uses extra context as input. \nand (b) whether context (web snippets) is used. We now elaborate on the different baselines. \na V EC S IM  A model that chooses the answer with highest cosine similarity to the question, where the question and answers are represented by an aver- age of pre-trained word embeddings. \nb LM1B  Inspired by  Trinh and Le  ( 2018 ), we employ a large language model (LM) from  Joze- fowicz et al.  ( 2016 ), which was pre-trained on the One Billion Words Benchmark ( Chelba et al. , 2013 ). We use this model in two variations. In the ﬁrst (LM1B- CONCAT ), we simply concate- nate each answer to the question. In the second (LM1B- REP ), we ﬁrst cluster questions according to their ﬁrst two words. Then, we recognize ﬁve high-frequency preﬁxes that cover   $35\\%$   of the de- velopment set (e.g.,  “what is” ). We rephrase ques- tions that ﬁt into one of these preﬁxes as a declar- ative sentence that contains the answer. E.g., we rephrase  “What is usually next to a door?”  and the candidate answer  “wall”  to  “Wall is usually next to a door” . For questions that do not start with the above preﬁxes, we concatenate the answer as in LM1B- CONCAT . In both variations we return the answer with highest LM probability. \n\nc QAB ILINEAR  This model, propsed by  Yu et al. ( 2014 ) for QA, scores an answer    $a_{i}$   with a bilinear model:    $q W a_{i}^{\\top}$  , where the question    $q$   and answers  $a_{i}$   are the average pre-trained word embeddings and    $W$   is a learned parameter matrix. A softmax layer over the candidate answers is used to train the model with cross-entropy loss. \nd QAC OMPARE  This model is similar to an NLI model from  Liu et al.  ( 2016 ). The model repre- sents the interaction between the question    $q$   and a candidate answer  $a_{i}$   as:    $h={\\mathrm{rel}}([q;a_{i};q\\odot a_{i};q-$   $a_{i}]W_{1}+b_{1})$  , where ’ ; ’ denotes concatenation and  $\\odot$  is element-wise product. Then, the model pre- dicts an answer score using a feed forward layer:  $h W_{2}+b_{2}$  . Average pre-trained embeddings and softmax are used to train the model. \ne ESIM  We use ESIM, a strong NLI model\n\n ( Chen et al. ,  2016 ). Similar to  Zellers et al.\n\n ( 2018b ), we change the output layer size to the number of candidate answers, and apply softmax to train with cross-entropy loss. \nf   $\\bf B I D A F++$  A state-of-the-art RC model, that uses the retrieved Google web snippets (Section  3 ) as context. We augment B I DAF ( Seo et al. ,  2016 ) with a self-attention layer and ELMo representa- tions ( Peters et al. ,  2018 ;  Huang et al. ,  2018 ). To adapt to the multiple-choice setting, we choose the answer with highest model probability. "}
{"page": 6, "image_path": "doc_images/N19-1421_6.jpg", "ocr_text": "tions (Peters et al., 2018; Huang et al., 2018). To\nadapt to the multiple-choice setting, we choose the\nanswer with highest model probability.\n\ng GENERATIVE PRE-TRAINED — TRANS-\nFORMER (GPT) Radford et al. (2018) proposed\na method for adapting pre-trained LMs to perform\na wide range of tasks. We applied their model to\nCOMMONSENSEQA by encoding each question\nand its candidate answers as a series of delimiter-\nseparated sequences. For example, the question\n“Tf you needed a lamp to do your work, where\nwould you put it?”, and the candidate answer\n“bedroom” would become “[start] If... ?\n{sep] [end]”. The hidden repre-\nsentations over each [end] token are converted\nto logits by a linear transformation and passed\nthrough a softmax to produce final probabilities\nfor the answers. We used the same pre-trained LM\nand hyper-parameters for fine-tuning as Radford\net al. (2018) on ROC Stories, except with a batch\nsize of 10.\n\nh BERT Similarly to the GPT, BERT fine-tunes\na language model and currently holds state-of-the-\nart across a broad range of tasks (Devlin et al.,\n2018). BERT uses a masked language mod-\neling objective, which predicts missing words\nmasked from unlabeled text. To apply BERT to\nCOMMONSENSEQA, we linearize each question-\nanswer pair into a delimiter-separated sequence\n(i.e., “[CLS] If... ? [SEP] bedroom [SEP]”)\nthen fine-tune the pre-trained weights from un-\ncased BERT-LARGE.! Similarly to the GPT, the\nhidden representations over each [CLS] token are\nrun through a softmax layer to create the predic-\ntions. We used the same hyper-parameters as De-\nvlin et al. (2018) for SWAG.\n\nbedroom\n\n6 Experiments\n\nExperimental Setup We split the data into a\ntraining/development/test set with an 80/10/10\nsplit. We perform two types of splits: (a) ran-\ndom split — where questions are split uniformly\nat random, and (b) question concept split — where\neach of the three sets have disjoint question con-\ncepts. We empirically find (see below) that a ran-\ndom split is harder for models that learn from\nCOMMONSENSEQA, because the same question\nconcept appears in the training set and develop-\nment/test set with different answer concepts, and\n\n'The original weights and code released by Google may\nbe found here: https://github.com/google-research/bert\n\nnetworks that memorize might fail in such a sce-\nnario. Since the random split is harder, we con-\nsider it the primary split of COMMONSENSEQA.\n\nWe evaluate all models on the test set using ac-\ncuracy (proportion of examples for which predic-\nion is correct), and tune hyper-parameters for all\ntrained models on the development set. To under-\nstand the difficulty of the task, we add a SANITY\nmode, where we replace the hard distractors (that\nshare a relation with the question concept and one\normulated by a worker) with random CONCEPT-\nNET distractors. We expect a reasonable baseline\n0 perform much better in this mode.\n\nFor pre-trained word embeddings we consider\n300d GloVe embeddings (Pennington et al., 2014)\nand 300d Numberbatch CONCEPTNET node em-\nbeddings (Speer et al., 2017), which are kept fixed\nat training time. We also combine ESIM with\n1024d ELMo contextual representations, which\nare also fixed during training.\n\nHuman Evaluation To test human accuracy, we\ncreated a separate task for which we did not use a\nqualification test, nor used AMT master workers.\nWe sampled 100 random questions and for each\nquestion gathered answers from five workers that\nwere not involved in question generation. Humans\nobtain 88.9% accuracy, taking a majority vote for\neach question.\n\nResults Table 5 presents test set results for all\nmodels and setups.\n\nThe best baselines are BERT-LARGE and GPT\nwith an accuracy of 55.9% and 45.5%, respec-\ntively, on the random split (63.6% and 55.5%, re-\nspectively, on the question concept split). This is\nwell below human accuracy, demonstrating that\nthe benchmark is much easier for humans. Nev-\nertheless, this result is much higher than random\n(20%), showing the ability of language models to\nstore large amounts of information related to com-\nmonsense knowledge.\n\nThe top part of Table 5 describes untrained\nmodels. We observe that performance is higher\nthan random, but still quite low. The middle part\ndescribes models that were trained on COMMON-\nSENSEQA, where BERT-LARGE obtains best per-\nformance, as mentioned above. ESIM models\nfollow BERT-LARGE and GPT, and obtain much\nlower performance. We note that ELMo represen-\ntations did not improve performance compared to\nGloVe embeddings, possibly because we were un-\n\n4155\n", "vlm_text": "\ng G ENERATIVE P RE - TRAINED T RANS - FORMER  (GPT)  Radford et al.  ( 2018 ) proposed a method for adapting pre-trained LMs to perform a wide range of tasks. We applied their model to C OMMONSENSE QA by encoding each question and its candidate answers as a series of delimiter- separated sequences. For example, the question “If you needed a lamp to do your work, where would you put it?” , and the candidate answer\n\n “bedroom”  would become “ [start]  If ... ?\n\n [sep]  bedroom  [end] ”. The hidden repre- sentations over each  [end]  token are converted to logits by a linear transformation and passed through a softmax to produce ﬁnal probabilities for the answers. We used the same pre-trained LM and hyper-parameters for ﬁne-tuning as  Radford et al.  ( 2018 ) on ROC Stories, except with a batch size of 10. \nh BERT  Similarly to the GPT, BERT ﬁne-tunes a language model and currently holds state-of-the- art across a broad range of tasks ( Devlin et al. , 2018 ). BERT uses a masked language mod- eling objective, which predicts missing words masked from unlabeled text. To apply BERT to C OMMONSENSE QA, we linearize each question- answer pair into a delimiter-separated sequence (i.e., “ [CLS]  If ... ?  [SEP]  bedroom  [SEP] ”) then ﬁne-tune the pre-trained weights from un- cased BERT- LARGE .   Similarly to the GPT, the hidden representations over each  [CLS]  token are run through a softmax layer to create the predic- tions. We used the same hyper-parameters as  De- vlin et al.  ( 2018 ) for SWAG. \n6 Experiments \nExperimental Setup We split the data into a training/development/test set with an 80/10/10 split. We perform two types of splits: (a)  ran- dom split  – where questions are split uniformly at random, and (b)  question concept split  – where each of the three sets have disjoint question con- cepts. We empirically ﬁnd (see below) that a ran- dom split is harder for models that learn from C OMMONSENSE QA, because the same question concept appears in the training set and develop- ment/test set with different answer concepts, and networks that memorize might fail in such a sce- nario. Since the random split is harder, we con- sider it the primary split of C OMMONSENSE QA. \n\nWe evaluate all models on the test set using ac- curacy (proportion of examples for which predic- tion is correct), and tune hyper-parameters for all trained models on the development set. To under- stand the difﬁculty of the task, we add a SANITY mode, where we replace the hard distractors (that share a relation with the question concept and one formulated by a worker) with random C ONCEPT - N ET  distractors. We expect a reasonable baseline to perform much better in this mode. \nFor pre-trained word embeddings we consider 300d GloVe embeddings ( Pennington et al. ,  2014 ) and 300d Numberbatch C ONCEPT N ET  node em- beddings ( Speer et al. ,  2017 ), which are kept ﬁxed at training time. We also combine ESIM with 1024d ELMo contextual representations, which are also ﬁxed during training. \nHuman Evaluation To test human accuracy, we created a separate task for which we did not use a qualiﬁcation test, nor used AMT master workers. We sampled 100 random questions and for each question gathered answers from ﬁve workers that were not involved in question generation. Humans obtain  $88.9\\%$   accuracy, taking a majority vote for each question. \nResults Table  5  presents test set results for all models and setups. \nThe best baselines are BERT- LARGE  and GPT with an accuracy of   $55.9\\%$   and   $45.5\\%$  , respec- tively, on the random split   $(63.6\\%$   and   $55.5\\%$  , re- spectively, on the question concept split). This is well below human accuracy, demonstrating that the benchmark is much easier for humans. Nev- ertheless, this result is much higher than random  $(20\\%)$  , showing the ability of language models to store large amounts of information related to com- monsense knowledge. \nThe top part of Table  5  describes untrained models. We observe that performance is higher than random, but still quite low. The middle part describes models that were trained on C OMMON - SENSE QA, where BERT- LARGE  obtains best per- formance, as mentioned above. ESIM models follow BERT- LARGE  and GPT, and obtain much lower performance. We note that ELMo represen- tations did not improve performance compared to GloVe embeddings, possibly because we were un- "}
{"page": 7, "image_path": "doc_images/N19-1421_7.jpg", "ocr_text": "Random split Question concept split\n\nModel Accuracy SANITY | Accuracy SANITY\n\nVECSIM+NUMBERBATCH 29.1 54.0 30.3 54.9\n\nLM1B-REP 26.1 39.6 26.0 39.1\n\nLMI1B-cONCAT 25.3 37.4 25.3 35.2\n\nVECSIM+GLOVE 22.3 26.8 20.8 27.1\n\nBERT-LARGE 55.9 92.3 63.6 93.2\n\nGPT 45.5 87.2 55.5 88.9\n\nESIM+ELMo 34.1 76.9 37.9 718\n\nESIM+GLOVE 32.8 79.1 40.4 78.2\n\nQABILINEAR+GLOVE 31.5 74.8 34.2 71.8\n\nESIM+NUMBERBATCH 30.1 74.6 31.2 75.1\n\nQABILINEAR+NUMBERBATCH 28.8 73.3 32.0 71.6\n\nQACOMPARE+GLOVE 25.7 69.2 34.1 71.3\n\nQACOMPARE+NUMBERBATCH 20.4 60.6 25.2 66.8\n\nBIDAF++ 32.0 71.0 38.4 72.0\n\nHUMAN 88.9\n\nTable 5: Test set accuracy for all models.\nCategory Formulated question example Correct answer | Distractor Accuracy | %\nSurface If someone laughs after surprising them they have a good sense of what? | humor laughter 717 35%\nclues How might a automobile get off a freeway? exit ramp driveway\nNegation/ | Where would you store a pillow case that is not in use? drawer bedroom 42.8 7%\nAntonym Where might the stapler be if I cannot find it? desk drawer desktop\nFactoid How many hours are in a day? twenty four week 38.4 13%\nknowledge | What geographic area is a lizard likely to be? west texas ball stopped\nBad Where is a well used toy car likely to be found? child’s room ‘own home 35.4 31%\ngranularity | Where may you be if you’re buying pork chops at a corner shop? town\nConjunction | What can you use to store a book while traveling? library of congress | 23.8 23%\nOna hot day what can you do to enjoy something cool and sweet? fresh cake\n\nTable 6: BERT-LARGE baseline analysis. For each category we provide two examples, the correct answer, one\ndistractor, model accuracy and frequency in the dataset. The predicted answer is in bold.\n\nable to improve performance by back-propagating\ninto the representations themselves (as we do in\nBERT-LARGE and GPT). The bottom part shows\nresults for BIDAF++ that uses web snippets as\ncontext. We observe that using snippets does not\nlead to high performance, hinting that they do not\ncarry a lot of useful information.\n\nPerformance on the random split is five points\nlower than the question concept split on average\nacross all trained models. We hypothesize that\nthis is because having questions in the develop-\nment/test set that share a question concept with the\ntraining set, but have a different answer, creates\ndifficulty for networks that memorize the relation\nbetween a question concept and an answer.\n\nLastly, all SANITY models that were trained\non COMMONSENSEQA achieve very high perfor-\nmance (92% for BERT-LARGE), showing that se-\nlecting difficult distractors is crucial.\n\nBaseline analysis To understand the perfor-\nmance of BERT-LARGE, we analyzed 100 ex-\namples from the development set (Table 6). We\nlabeled examples with categories (possibly more\nthan one per example) and then computed the av-\n\nerage accuracy of the model for each category.\n\nWe found that the model does well (77.7% ac-\ncuracy) on examples where surface clues hint to\nthe correct answer. Examples that involve nega-\ntion or understanding antonyms have lower accu-\nracy (42.8%), similarly to examples that require\nfactoid knowledge (38.4%). Accuracy is partic-\nularly low in questions where the correct answer\nhas finer granularity compared to one of the dis-\ntractors (35.4%), and in cases where the correct\nanswer needs to meet a conjunction of conditions,\nand the distractor meets only one of them (23.8%).\n\nLearning Curves To extrapolate how current\nmodels might perform with more data, we evalu-\nated BERT-large on the development set, training\nwith varying amounts of data. The resulting learn-\ning curves are plotted in figure 5. For each training\nset size, hyper-parameters were identical to sec-\ntion 5, except the number of epochs was varied to\nkeep the number of mini-batches during training\nconstant. To deal with learning instabilities, each\ndata point is the best of 3 runs. We observe that\nthe accuracy of BERT-LARGE is expected to be\nroughly 75% assuming 100k examples, still sub-\n\n4156\n", "vlm_text": "The table presents a comparison of various models in terms of their Accuracy and SANITY scores across two different evaluation splits: \"Random split\" and \"Question concept split.\" The models are listed in the \"Model\" column, and the corresponding Accuracy and SANITY scores for each evaluation split are provided in adjacent columns. \n\nHere are the key findings from the table:\n\n1. **Random split:**\n   - BERT-LARGE achieved the highest Accuracy (55.9) and SANITY (92.3) among the models.\n   - Human performance is significantly higher with an Accuracy of 88.9.\n\n2. **Question concept split:**\n   - Again, BERT-LARGE leads with the highest Accuracy (63.6) and SANITY (93.2).\n   - Human performance is not listed for this split.\n\nModels such as VEC_SIM+Numberbatch, LM1B variants, ESIM, QABilinear, and others show varying levels of performance but are generally outperformed by BERT-LARGE. The presence of Human performance in the \"Random split\" provides a benchmark for the models' abilities.\nThis table summarizes categories of questions with examples, correct answers, distractors, and accuracy rates. Here’s a breakdown:\n\n- **Category**: The type of logical or linguistic challenge in the question.\n- **Formulated question example**: Sample questions for each category.\n- **Correct answer**: The accurate response for each question.\n- **Distractor**: Incorrect options provided as potential answers.\n- **Accuracy**: The percentage of correct responses for each question type.\n- **%**: Percentage of questions in each category.\n\nThe table highlights different reasoning challenges and their associated performance metrics.\nable to improve performance by back-propagating into the representations themselves (as we do in BERT- LARGE  and GPT). The bottom part shows results for   $\\mathrm{BIDAF++}$   that uses web snippets as context. We observe that using snippets does not lead to high performance, hinting that they do not carry a lot of useful information. \nPerformance on the random split is ﬁve points lower than the question concept split on average across all trained models. We hypothesize that this is because having questions in the develop- ment/test set that share a question concept with the training set, but have a different answer, creates difﬁculty for networks that memorize the relation between a question concept and an answer. \nLastly, all SANITY models that were trained on C OMMONSENSE QA achieve very high perfor- mance   $92\\%$   for BERT- LARGE ), showing that se- lecting difﬁcult distractors is crucial. \nBaseline analysis To understand the perfor- mance of BERT- LARGE , we analyzed 100 ex- amples from the development set (Table  6 ). We labeled examples with categories (possibly more than one per example) and then computed the av- erage accuracy of the model for each category. \n\nWe found that the model does well (  $77.7\\%$   ac- curacy) on examples where surface clues hint to the correct answer. Examples that involve nega- tion or understanding antonyms have lower accu- racy   $(42.8\\%)$  , similarly to examples that require factoid knowledge   $(38.4\\%)$  . Accuracy is partic- ularly low in questions where the correct answer has ﬁner granularity compared to one of the dis- tractors   $(35.4\\%)$  , and in cases where the correct answer needs to meet a conjunction of conditions, and the distractor meets only one of them   $(23.8\\%)$  . \nLearning Curves To extrapolate how current models might perform with more data, we evalu- ated BERT-large on the development set, training with varying amounts of data. The resulting learn- ing curves are plotted in ﬁgure  5 . For each training set size, hyper-parameters were identical to sec- tion  5 , except the number of epochs was varied to keep the number of mini-batches during training constant. To deal with learning instabilities, each data point is the best of 3 runs. We observe that the accuracy of BERT- LARGE  is expected to be roughly  $75\\%$   assuming   $100\\mathbf{k}$   examples, still sub- "}
{"page": 8, "image_path": "doc_images/N19-1421_8.jpg", "ocr_text": "1.0\n\n0.9 foo ccc cecseesssesssessntsseesssessnensesssnsnsvesersesreseneesessnessneesnessreseeey\n\n°\n\ndev accuracy\n\n@ question concept\n@ random\nse human performance\n\n10? 107 1o* 10°\n# instances\n\nFigure 5: Development accuracy for BERT-LARGE\ntrained with varying amounts of data.\n\nstantially lower than human performance.\n\n7 Conclusion\n\nWe present COMMONSENSEQA, a new QA\ndataset that contains 12,247 examples and aims to\ntest commonsense knowledge. We describe a pro-\ncess for generating difficult questions at scale us-\ning CONCEPTNET, perform a detailed analysis of\nthe dataset, which elucidates the unique properties\nof our dataset, and extensively evaluate on a strong\nsuite of baselines. We find that the best model is\na pre-trained LM tuned for our task and obtains\n55.9% accuracy, dozens of points lower than hu-\nman accuracy. We hope that this dataset facili-\ntates future work in incorporating commonsense\nknowledge into NLU systems.\n\nAcknowledgments\n\nWe thank the anonymous reviewers for their con-\nstructive feedback. This work was completed in\npartial fulfillment for the PhD degree of Jonathan\nHerzig, which was also supported by a Google\nPhD fellowship. This research was partially sup-\nported by The Israel Science Foundation grant\n942/16, The Blavatnik Computer Science Re-\nsearch Fund and The Yandex Initiative for Ma-\nchine Learning.\n\nReferences\n\nZheng Cai, Lifu Tu, and Kevin Gimpel. 2017. Pay at-\ntention to the ending: Strong neural baselines for the\nroc story cloze task. In ACL.\n\nC. Chelba, T. Mikolov, M. Schuster, Q. Ge, T. Brants,\nP. Koehn, and T. Robinson. 2013. One billion word\n\nbenchmark for measuring progress in statistical lan-\nguage modeling. arXiv preprint arXiv: 1312.3005.\n\nQian Chen, Xiaodan Zhu, Zhenhua Ling, Si Wei,\nHui Jiang, and Diana Inkpen. 2016. Enhanced\nIstm for natural language inference. arXiv preprint\narXiv: 1609.06038.\n\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,\nAshish Sabharwal, Carissa Schoenick, and Oyvind\nTafjord. 2018. Think you have solved question an-\nswering? try arc, the ai2 reasoning challenge.\n\nEmest Davis. 2016. How to write science questions\nthat are easy for people and hard for computers. AJ\nmagazine, 37(1):13-22.\n\nJ. Devlin, M. Chang, K. Lee, and K. Toutanova. 2018.\nBert: Pre-training of deep bidirectional transformers\nfor language understanding. arXiv.\n\nJonathan Gordon and Benjamin Van Durme. 2013. Re-\nporting bias and knowledge acquisition. In Proceed-\nings of the 2013 Workshop on Automated Knowledge\nBase Construction, AKBC ’13, pages 25-30, New\nYork, NY, USA. ACM.\n\nSuchin Gururangan, Swabha Swayamdipta, Omer\nLevy, Roy Schwartz, Samuel R Bowman, and\nNoah A Smith. 2018. Annotation artifacts in\nnatural language inference data. arXiv preprint\narXiv: 1803.02324.\n\nKarl Moritz Hermann, Tomas Kocisky, Edward\nGrefenstette, Lasse Espeholt, Will Kay, Mustafa Su-\nleyman, and Phil Blunsom. 2015. Teaching ma-\nchines to read and comprehend. In Advances in Neu-\nral Information Processing Systems, pages 1693-\n1701.\n\nHsin-Yuan Huang, Eunsol Choi, and Wen-tau Yih.\n2018. Flowqa: Grasping flow in history for con-\nversational machine comprehension. arXiv preprint\narXiv:1810.06683.\n\nM. Joshi, E. Choi, D. Weld, and L. Zettlemoyer. 2017.\nTriviaQA: A large scale distantly supervised chal-\nlenge dataset for reading comprehension. In Associ-\nation for Computational Linguistics (ACL).\n\nRafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam\nShazeer, and Yonghui Wu. 2016. Exploring\nthe limits of language modeling. arXiv preprint\narXiv: 1602.02410.\n\nR Kowalski and M Sergot. 1986. A logic-based calcu-\nlus of events. New Gen. Comput., 4(1):67-95.\n\nDouglas B. Lenat. 1995. Cyc: A large-scale invest-\nment in knowledge infrastructure. Commun. ACM,\n38:32-38.\n\nHector J. Levesque. 2011. The winograd schema chal-\nlenge. In AAAI Spring Symposium: Logical Formal-\nizations of Commonsense Reasoning.\n\n4157\n", "vlm_text": "The image is a line chart comparing development accuracy against the number of instances on a logarithmic scale. \n\n- The x-axis represents the number of instances, ranging from \\(10^2\\) to \\(10^5\\).\n- The y-axis represents development accuracy, ranging from 0.2 to 1.0.\n\nThere are two sets of data points with trend lines:\n\n1. **Question Concept (blue)**\n   - Blue circles with a dashed trend line.\n   \n2. **Random (orange)**\n   - Orange circles with a dashed trend line.\n\nThere is also a horizontal dotted line representing \"human performance\" at a higher accuracy level.\n\nOverall, the chart illustrates that both \"question concept\" and \"random\" performances improve as the number of instances increase, though neither reach human performance.\nFigure 5: Development accuracy for BERT- LARGE trained with varying amounts of data. \nstantially lower than human performance. \n7 Conclusion \nWe present C OMMONSENSE QA, a new QA dataset that contains 12,247 examples and aims to test commonsense knowledge. We describe a pro- cess for generating difﬁcult questions at scale us- ing C ONCEPT N ET , perform a detailed analysis of the dataset, which elucidates the unique properties of our dataset, and extensively evaluate on a strong suite of baselines. We ﬁnd that the best model is a pre-trained LM tuned for our task and obtains  $55.9\\%$   accuracy, dozens of points lower than hu- man accuracy. We hope that this dataset facili- tates future work in incorporating commonsense knowledge into NLU systems. \nAcknowledgments \nWe thank the anonymous reviewers for their con- structive feedback. This work was completed in partial fulﬁllment for the PhD degree of Jonathan Herzig, which was also supported by a Google PhD fellowship. This research was partially sup- ported by The Israel Science Foundation grant 942/16, The Blavatnik Computer Science Re- search Fund and The Yandex Initiative for Ma- chine Learning. \nReferences \nZheng Cai, Lifu Tu, and Kevin Gimpel. 2017. Pay at- tention to the ending: Strong neural baselines for the roc story cloze task. In  ACL . \nC. Chelba, T. Mikolov, M. Schuster, Q. Ge, T. Brants, P. Koehn, and T. Robinson. 2013. One billion word \nbenchmark for measuring progress in statistical lan- guage modeling.  arXiv preprint arXiv:1312.3005 . \nQian Chen, Xiaodan Zhu, Zhenhua Ling, Si Wei, Hui Jiang, and Diana Inkpen. 2016. Enhanced lstm for natural language inference.  arXiv preprint arXiv:1609.06038 . \nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have solved question an- swering? try arc, the ai2 reasoning challenge. \nErnest Davis. 2016. How to write science questions that are easy for people and hard for computers.  AI magazine , 37(1):13–22. \nJ. Devlin, M. Chang, K. Lee, and K. Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding.  arXiv . \nJonathan Gordon and Benjamin Van Durme. 2013.  Re- porting bias and knowledge acquisition . In  Proceed- ings of the 2013 Workshop on Automated Knowledge Base Construction , AKBC ’13, pages 25–30, New York, NY, USA. ACM. \nSuchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel R Bowman, and Noah A Smith. 2018. Annotation artifacts in natural language inference data. arXiv preprint arXiv:1803.02324 . \nKarl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Su- leyman, and Phil Blunsom. 2015. Teaching ma- chines to read and comprehend. In  Advances in Neu- ral Information Processing Systems , pages 1693– 1701. \nHsin-Yuan Huang, Eunsol Choi, and Wen-tau Yih. 2018. Flowqa: Grasping ﬂow in history for con- versational machine comprehension.  arXiv preprint arXiv:1810.06683 . \nM. Joshi, E. Choi, D. Weld, and L. Zettlemoyer. 2017. TriviaQA: A large scale distantly supervised chal- lenge dataset for reading comprehension. In  Associ- ation for Computational Linguistics (ACL) . \nRafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. 2016. Exploring the limits of language modeling. arXiv preprint arXiv:1602.02410 . \nR Kowalski and M Sergot. 1986.  A logic-based calcu- lus of events .  New Gen. Comput. , 4(1):67–95. \nDouglas B. Lenat. 1995. Cyc: A large-scale invest- ment in knowledge infrastructure.  Commun. ACM , 38:32–38. \nHector J. Levesque. 2011. The winograd schema chal- lenge. In  AAAI Spring Symposium: Logical Formal- izations of Commonsense Reasoning . "}
{"page": 9, "image_path": "doc_images/N19-1421_9.jpg", "ocr_text": "Yang Liu, Chengjie Sun, Lei Lin, and Xiaolong Wang.\n2016. Learning natural language inference using\nbidirectional Istm model and inner-attention. arXiv\npreprint arXiv: 1605.09090.\n\nPeter LoBue and Alexander Yates. 2011. Types of\ncommon-sense knowledge needed for recognizing\ntextual entailment. In Proceedings of the 49th An-\nnual Meeting of the Association for Computational\nLinguistics: Human Language Technologies: short\npapers-Volume 2, pages 329-334. Association for\nComputational Linguistics.\n\nJ. McCarthy. 1959. Programs with common sense. In\nProceedings of the Teddington Conference on the\nMechanization of Thought Processes.\n\nJohn McCarthy and Patrick J. Hayes. 1969. Some\nphilosophical problems from the standpoint of ar-\ntificial intelligence. In B. Meltzer and D. Michie,\neditors, Machine Intelligence 4, pages 463-502. Ed-\ninburgh University Press. Reprinted in McC90.\n\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish\nSabharwal. 2018. Can a suit of armor conduct elec-\ntricity? a new dataset for open book question an-\nswering.\n\nN. Mostafazadeh, N. Chambers, X. He, D. Parikh,\nD. Batra, L. Vanderwende, P. Kohli, and J. Allen.\n2016. A corpus and cloze evaluation for deeper\nunderstanding of commonsense stories. In North\nAmerican Association for Computational Linguis-\ntics (NAACL).\n\nT. Nguyen, M. Rosenberg, X. Song, J. Gao, S. Tiwary,\nR. Majumder, and L. Deng. 2016. MS MARCO:\nA human generated machine reading comprehension\ndataset. In Workshop on Cognitive Computing at\nNIPS.\n\nSimon Ostermann, Ashutosh Modi, Michael Roth, Ste-\nfan Thater, and Manfred Pinkal. 2018. Mcscript: A\nnovel dataset for assessing machine comprehension\nusing script knowledge. CoRR, abs/1803.05223.\n\nJ. Pennington, R. Socher, and C. D. Manning. 2014.\nGlove: Global vectors for word representation. In\nEmpirical Methods in Natural Language Processing\n(EMNLP).\n\nMatthew E. Peters, Mark Neumann, Mohit lyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. In Proc. of NAACL.\n\nAdam Poliak, Jason Naradowsky, Aparajita Haldar,\nRachel Rudinger, and Benjamin Van Durme. 2018.\nHypothesis only baselines in natural language infer-\nence. In Proc. of *SEM.\n\nA. Radford, K. Narasimhan, T. Salimans, and\nI. Sutskever. 2018. Improving language understand-\ning by generative pre-training. Technical Report,\nOpenAl.\n\nP. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang. 2016.\nSquad: 100,000+ questions for machine comprehen-\nsion of text. In Empirical Methods in Natural Lan-\nguage Processing (EMNLP).\n\nM. Roemmele, C. Bejan, and A. Gordon. 2011. Choice\nof plausible alternatives: An evaluation of common-\nsense causal reasoning. In AAAI Spring Symposium\non Logical Formalizations of Commonsense Rea-\nsoning.\n\nRoy Schwartz, Maarten Sap, Ioannis Konstas, Leila\nZilles, Yejin Choi, and Noah A. Smith. 2017. The\neffect of different writing tasks on linguistic style:\nA case study of the roc story cloze task. In CoNLL.\n\nM. Seo, A. Kembhavi, A. Farhadi, and H. Hajishirzi.\n2016. Bidirectional attention flow for machine com-\nprehension. arXiv.\n\nRobert Speer, Joshua Chin, and Catherine Havasi.\n2017. Conceptnet 5.5: An open multilingual graph\nof general knowledge. In AAAI, pages 4444-4451.\n\nO. Tange. 2011.\nline power tool.\n36(1):42-47.\n\nGnu parallel - the command-\nslogin: The USENIX Magazine,\n\nTrieu H Trinh and Quoc V Le. 2018. A simple\nmethod for commonsense reasoning. arXiv preprint\narXiv: 1806.02847.\n\nT. Winograd. 1972. Understanding Natural Language.\nAcademic Press.\n\nLei Yu, Karl Moritz Hermann, Phil Blunsom, and\nStephen Pulman. 2014. Deep learning for answer\nsentence selection. arXiv preprint arXiv: 1412.1632.\n\nRowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin\nChoi. 2018a. | From recognition to cognition:\nVisual commonsense reasoning. arXiv preprint\narXiv:1811.10830.\n\nRowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin\nChoi. 2018b. Swag: A large-scale adversarial\ndataset for grounded commonsense inference. arXiv\npreprint arXiv: 1808.05326.\n\nSheng Zhang, Rachel Rudinger, Kevin Duh, and Ben-\njamin Van Durme. 2017. Ordinal common-sense in-\nference. TACL, 5:379-395.\n\n4158\n", "vlm_text": "Yang Liu, Chengjie Sun, Lei Lin, and Xiaolong Wang. 2016. Learning natural language inference using bidirectional lstm model and inner-attention.  arXiv preprint arXiv:1605.09090 . Peter LoBue and Alexander Yates. 2011. Types of common-sense knowledge needed for recognizing textual entailment. In  Proceedings of the 49th An- nual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers-Volume 2 , pages 329–334. Association for Computational Linguistics. J. McCarthy. 1959. Programs with common sense. In Proceedings of the Teddington Conference on the Mechanization of Thought Processes . John McCarthy and Patrick J. Hayes. 1969. Some philosophical problems from the standpoint of ar- tiﬁcial intelligence. In B. Meltzer and D. Michie, editors,  Machine Intelligence 4 , pages 463–502. Ed- inburgh University Press. Reprinted in McC90. Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. 2018. Can a suit of armor conduct elec- tricity? a new dataset for open book question an- swering. N. Mostafazadeh, N. Chambers, X. He, D. Parikh, D. Batra, L. Vanderwende, P. Kohli, and J. Allen. 2016. A corpus and cloze evaluation for deeper understanding of commonsense stories. In  North American Association for Computational Linguis- tics (NAACL) . T. Nguyen, M. Rosenberg, X. Song, J. Gao, S. Tiwary, R. Majumder, and L. Deng. 2016. MS MARCO: A human generated machine reading comprehension dataset. In  Workshop on Cognitive Computing at NIPS . Simon Ostermann, Ashutosh Modi, Michael Roth, Ste- fan Thater, and Manfred Pinkal. 2018. Mcscript: A novel dataset for assessing machine comprehension using script knowledge.  CoRR , abs/1803.05223. J. Pennington, R. Socher, and C. D. Manning. 2014. Glove: Global vectors for word representation. In Empirical Methods in Natural Language Processing (EMNLP) . Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word rep- resentations. In  Proc. of NAACL . Adam Poliak, Jason Naradowsky, Aparajita Haldar, Rachel Rudinger, and Benjamin Van Durme. 2018. Hypothesis only baselines in natural language infer- ence. In  Proc. of   $^{*}\\!S E M$  . A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever. 2018. Improving language understand- ing by generative pre-training. Technical Report, OpenAI . \nP. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang. 2016. Squad:   $100{,}000{+}$   questions for machine comprehen- sion of text. In  Empirical Methods in Natural Lan- guage Processing (EMNLP) . M. Roemmele, C. Bejan, and A. Gordon. 2011. Choice of plausible alternatives: An evaluation of common- sense causal reasoning. In  AAAI Spring Symposium on Logical Formalizations of Commonsense Rea- soning . Roy Schwartz, Maarten Sap, Ioannis Konstas, Leila Zilles, Yejin Choi, and Noah A. Smith. 2017. The effect of different writing tasks on linguistic style: A case study of the roc story cloze task. In  CoNLL . M. Seo, A. Kembhavi, A. Farhadi, and H. Hajishirzi. 2016. Bidirectional attention ﬂow for machine com- prehension.  arXiv . Robert Speer, Joshua Chin, and Catherine Havasi. 2017. Conceptnet 5.5: An open multilingual graph of general knowledge. In  AAAI , pages 4444–4451. O. Tange. 2011. Gnu parallel - the command- line power tool . ;login: The USENIX Magazine , 36(1):42–47. Trieu H Trinh and Quoc V Le. 2018. A simple method for commonsense reasoning.  arXiv preprint arXiv:1806.02847 . T. Winograd. 1972.  Understanding Natural Language . Academic Press.Lei Yu, Karl Moritz Hermann, Phil Blunsom, and Stephen Pulman. 2014. Deep learning for answer sentence selection.  arXiv preprint arXiv:1412.1632 . Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2018a. From recognition to cognition: Visual commonsense reasoning. arXiv preprint arXiv:1811.10830 . Rowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin Choi. 2018b. Swag: A large-scale adversarial dataset for grounded commonsense inference.  arXiv preprint arXiv:1808.05326 . Sheng Zhang, Rachel Rudinger, Kevin Duh, and Ben- jamin Van Durme. 2017. Ordinal common-sense in- ference.  TACL , 5:379–395. "}
