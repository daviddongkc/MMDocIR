{"page": 0, "image_path": "doc_images/N18-1008_0.jpg", "ocr_text": "Tied Multitask Learning for Neural Speech Translation\n\nAntonios Anastasopoulos and David Chiang\nDepartment of Computer Science and Engineeering\nUniversity of Notre Dame\n{aanastas,dchiang}@nd. edu\n\nAbstract\n\nWe explore multitask models for neural trans-\nlation of speech, augmenting them in order\nto reflect two intuitive notions. First, we in-\ntroduce a model where the second task de-\ncoder receives information from the decoder\nof the first task, since higher-level intermediate\nrepresentations should provide useful infor-\nmation. Second, we apply regularization that\nencourages transitivity and invertibility. We\nshow that the application of these notions on\njointly trained models improves performance\non the tasks of low-resource speech transcrip-\ntion and translation. It also leads to better per-\nformance when using attention information for\nword discovery over unsegmented input.\n\n1 Introduction\n\nRecent efforts in endangered language documen-\ntation focus on collecting spoken language re-\nsources, accompanied by spoken translations in a\nhigh resource language to make the resource in-\nterpretable (Bird et al., 2014a). For example, the\nBULB project (Adda et al., 2016) used the LIG-\nAikuma mobile app (Bird et al., 2014b; Blachon\net al., 2016) to collect parallel speech corpora be-\ntween three Bantu languages and French. Since\nit’s common for speakers of endangered languages\nto speak one or more additional languages, collec-\ntion of such a resource is a realistic goal.\n\nSpeech can be interpreted either by transcrip-\ntion in the original language or translation to an-\nother language. Since the size of the data is ex-\ntremely small, multitask models that jointly train\na model for both tasks can take advantage of\nboth signals. Our contribution lies in improv-\ning the sequence-to-sequence multitask learning\nparadigm, by drawing on two intuitive notions:\nthat higher-level representations are more useful\nthan lower-level representations, and that transla-\ntion should be both transitive and invertible.\n\n82\n\nHigher-level intermediate representations, such\nas transcriptions, should in principle carry infor-\nmation useful for an end task like speech transla-\nion. A typical multitask setup (Weiss et al., 2017)\nshares information at the level of encoded frames,\nbut intuitively, a human translating speech must\nwork from a higher level of representation, at least\nat the level of phonemes if not syntax or semantics.\nThus, we present a novel architecture for tied mul-\nitask learning with sequence-to-sequence models,\nin which the decoder of the second task receives\ninformation not only from the encoder, but also\nrom the decoder of the first task.\n\nIn addition, transitivity and invertibility are two\n\nproperties that should hold when mapping be-\nween levels of representation or across languages.\nWe demonstrate how these two notions can be im-\nplemented through regularization of the attention\nmatrices, and how they lead to further improved\nperformance.\nWe evaluate our models in three experiment\nsettings: low-resource speech transcription and\ntranslation, word discovery on unsegmented in-\nput, and high-resource text translation. Our high-\nresource experiments are performed on English,\nFrench, and German. Our low-resource speech ex-\nperiments cover a wider range of linguistic diver-\nsity: Spanish-English, Mboshi-French, and Ainu-\nEnglish.\n\nIn the speech transcription and translation tasks,\nour proposed model leads to improved perfor-\nmance against all baselines as well as previous\nmultitask architectures. We observe improvements\nof up to 5% character error rate in the transcrip-\ntion task, and up to 2.8% character-level BLEU in\nthe translation task. However, we didn’t observe\nsimilar improvements in the text translation exper-\niments. Finally, on the word discovery task, we im-\nprove upon previous work by about 3% F-score on\nboth tokens and types.\n\nProceedings of NAACL-HLT 2018, pages 82-91\nNew Orleans, Louisiana, June 1 - 6, 2018. ©2018 Association for Computational Linguistics\n", "vlm_text": "Tied Multitask Learning for Neural Speech Translation \nAntonios Anastasopoulos  and  David Chiang Department of Computer Science and Engineeering University of Notre Dame { aanastas,dchiang } @nd.edu \nAbstract \nWe explore multitask models for neural trans- lation of speech, augmenting them in order to reﬂect two intuitive notions. First, we in- troduce a model where the second task de- coder receives information from the decoder of the ﬁrst task, since higher-level intermediate representations should provide useful infor- mation. Second, we apply regularization that encourages  transitivity  and  invertibility . We show that the application of these notions on jointly trained models improves performance on the tasks of low-resource speech transcrip- tion and translation. It also leads to better per- formance when using attention information for word discovery over unsegmented input. \n1 Introduction \nRecent e ﬀ orts in endangered language documen- tation focus on collecting spoken language re- sources, accompanied by spoken translations in a high resource language to make the resource in- terpretable ( Bird et al. ,  2014a ). For example, the BULB project ( Adda et al. ,  2016 ) used the LIG- Aikuma mobile app ( Bird et al. ,  2014b ;  Blachon et al. ,  2016 ) to collect parallel speech corpora be- tween three Bantu languages and French. Since it’s common for speakers of endangered languages to speak one or more additional languages, collec- tion of such a resource is a realistic goal. \nSpeech can be interpreted either by transcrip- tion in the original language or translation to an- other language. Since the size of the data is ex- tremely small, multitask models that jointly train a model for both tasks can take advantage of both signals. Our contribution lies in improv- ing the sequence-to-sequence multitask learning paradigm, by drawing on two intuitive notions: that higher-level representations are more useful than lower-level representations, and that transla- tion should be both transitive and invertible. \nHigher-level intermediate representations , such as transcriptions, should in principle carry infor- mation useful for an end task like speech transla- tion. A typical multitask setup ( Weiss et al. ,  2017 ) shares information at the level of encoded frames, but intuitively, a human translating speech must work from a higher level of representation, at least at the level of phonemes if not syntax or semantics. Thus, we present a novel architecture for  tied  mul- titask learning with sequence-to-sequence models, in which the decoder of the second task receives information not only from the encoder, but also from the decoder of the ﬁrst task. \nIn addition,  transitivity  and  invertibility  are two properties that should hold when mapping be- tween levels of representation or across languages. We demonstrate how these two notions can be im- plemented through regularization of the attention matrices, and how they lead to further improved performance. \nWe evaluate our models in three experiment settings: low-resource speech transcription and translation, word discovery on unsegmented in- put, and high-resource text translation. Our high- resource experiments are performed on English, French, and German. Our low-resource speech ex- periments cover a wider range of linguistic diver- sity: Spanish-English, Mboshi-French, and Ainu- English. \nIn the speech transcription and translation tasks, our proposed model leads to improved perfor- mance against all baselines as well as previous multitask architectures. We observe improvements of up to  $5\\%$   character error rate in the transcrip- tion task, and up to  $2.8\\%$   character-level BLEU in the translation task. However, we didn’t observe similar improvements in the text translation exper- iments. Finally, on the word discovery task, we im- prove upon previous work by about  $3\\%$   F-score on both tokens and types. "}
{"page": 1, "image_path": "doc_images/N18-1008_1.jpg", "ocr_text": "2 Model\n\nOur models are based on a sequence-to-sequence\nmodel with attention (Bahdanau et al., 2015). In\ngeneral, this type of model is composed of three\nparts: a recurrent encoder, the attention, and a re-\ncurrent decoder (see Figure la).!\n\nThe encoder transforms an input sequence of\nwords or feature frames x1, ..., Xv into a sequence\nof input states hy,..., hy:\n\nhy, = enc(hy-, Xp).\n\nThe attention transforms the input states into a se-\nquence of context vectors via a matrix of attention\n\nweights:\nCm = » Onan.\nn\n\nFinally, the decoder computes a sequence of out-\nput states from which a probability distribution\nover output words can be computed.\n\nSin\nPm)\n\nIn a standard encoder-decoder multitask model\n(Figure 1b) (Dong et al., 2015; Weiss et al., 2017),\nwe jointly model two output sequences using a\nshared encoder, but separate attentions and de-\n\ndec(S—1, ms ¥m-1)\n\nsoftmax(s,).\n\ncoders:\n1 1\nCn = » Ann\nn\n1 Il 1 yl\nSin = dec (Si,-1> Cm Yn)\nP(y},) = softmax(s},)\nand\n2 2\nCn = » nn\nn\n2 272 2 2\nSin = dec (Si.—1> Cm Yin)\nP(y?,) = softmax(s?,).\n\nWe can also arrange the decoders in a cascade\n(Figure 1c), in which the second decoder attends\nonly to the output states of the first decoder:\n\n2 12 gl\nCn = » Gm! Sm!\n\nm\n\n2\n‘m\n\nPCy)\n\n'For simplicity, we have assumed only a single layer for\nboth the encoder and decoder. It is possible to use multiple\nstacked RNNs; typically, the output of the encoder and de-\ncoder (c,, and P(y,,), respectively) would be computed from\nthe top layer only.\n\n272 2 2\nS dec (Sint Cn» Yn)\n\n2\nsoftmax(s;,).\n\n83\n\nTu et al. (2017) use exactly this architecture to\ntrain on bitext by setting the second output se-\nquence to be equal to the input sequence (y? = Xj).\n\nIn our proposed triangle model (Figure 1d), the\nfirst decoder is as above, but the second decoder\nhas two attentions, one for the input states of the\nencoder and one for the output states of the first\ndecoder:\n\n2 12 gl 2\n\nCn = [Din Qnam Sm min Ginn\n2 272 2 2\n\nSin = dec (Sin Ge Yin-v)\n\nPly?) = softmax(s?,).\n\nNote that the context vectors resulting from the\ntwo attentions are concatenated, not added.\n\n3 Learning and Inference\n\nFor compactness, we will write X for the matrix\nwhose rows are the x,, and similarly H, C, and\nso on. We also write A for the matrix of attention\nweights: [A]ji; = aij.\n\nLet 6 be the parameters of our model, which we\ntrain on sentence triples (X, Y!,Y’).\n\n3.1\n\nDefine the score of a sentence triple to be a log-\nlinear interpolation of the two decoders’ probabil-\nities:\n\nMaximum likelihood estimation\n\nscore(Y!, Y | X; 6) = Alog P(Y! | X;4) +\n(1 — A) log P(Y? | X,$!; 6)\n\nwhere A is a parameter that controls the impor-\ntance of each sub-task. In all our experiments, we\nset A to 0.5. We then train the model to maximize\n\nL£(0) = » score(Y!, Y | X; 0),\n\nwhere the summation is over all sentence triples in\nthe training data.\n\n3.2 Regularization\n\nWe can optionally add a regularization term to the\nobjective function, in order to encourage our atten-\ntion mechanisms to conform to two intuitive prin-\nciples of machine translation: transitivity and in-\nvertibility.\n\nTransitivity attention regularizer To a first ap-\nproximation, the translation relation should be\ntransitive (Wang et al., 2006; Levinboim and Chi-\nang, 2015): If source word x; aligns to target word\n", "vlm_text": "Our models are based on a sequence-to-sequence model with attention ( Bahdanau et al. ,  2015 ). In general, this type of model is composed of three parts: a recurrent encoder, the attention, and a re- current decoder (see Figure  1 a). \nThe encoder transforms an input sequence of words or feature frames    $\\mathbf{x}_{1},\\ldots,\\mathbf{x}_{N}$   into a sequence of  input states  ${\\bf h}_{1},\\dots,{\\bf h}_{N}$  : \n\n$$\n\\mathbf h_{n}=\\mathsf{e n c}(\\mathbf h_{n-1},\\mathbf x_{n}).\n$$\n \nThe attention transforms the input states into a se- quence of  context vectors  via a matrix of  attention weights : \n\n$$\n\\mathbf{c}_{m}=\\sum_{n}\\alpha_{m n}\\mathbf{h}_{n}.\n$$\n \nFinally, the decoder computes a sequence of  out- put states  from which a probability distribution over output words can be computed. \n\n$$\n\\begin{array}{c}{\\mathbf{s}_{m}=\\operatorname*{det}(\\mathbf{s}_{m-1},\\mathbf{c}_{m},\\mathbf{y}_{m-1})}\\\\ {P(\\mathbf{y}_{m})=\\mathrm{softmax}(\\mathbf{s}_{m}).}\\end{array}\n$$\n \nIn a standard encoder-decoder  multitask  model (Figure  1 b) ( Dong et al. ,  2015 ;  Weiss et al. ,  2017 ), we jointly model two output sequences using a shared encoder, but separate attentions and de- coders: \n\n$$\n\\begin{array}{l}{\\displaystyle\\mathbf{c}_{m}^{1}=\\sum_{n}\\alpha_{m n}^{1}\\mathbf{h}_{n}}\\\\ {\\displaystyle\\quad\\mathbf{s}_{m}^{1}=\\operatorname*{det}^{1}(\\mathbf{s}_{m-1}^{1},\\mathbf{c}_{m}^{1},\\mathbf{y}_{m-1}^{1})}\\\\ {\\displaystyle P(\\mathbf{y}_{m}^{1})=\\operatorname{softmax}(\\mathbf{s}_{m}^{1})}\\end{array}\n$$\n \nand \n\n$$\n\\begin{array}{l l}{{\\displaystyle{\\bf c}_{m}^{2}=\\sum_{n}\\alpha_{m n}^{2}{\\bf h}_{n}}}\\\\ {{\\displaystyle~~{\\bf s}_{m}^{2}=\\mathrm{dec}^{2}({\\bf s}_{m-1}^{2},{\\bf c}_{m}^{2},{\\bf y}_{m-1}^{2})}}\\\\ {{\\displaystyle P({\\bf y}_{m}^{2})=\\mathrm{softmax}({\\bf s}_{m}^{2})}.}\\end{array}\n$$\n \nWe can also arrange the decoders in a  cascade (Figure  1 c), in which the second decoder attends only to the output states of the ﬁrst decoder: \n\n$$\n\\begin{array}{c}{\\displaystyle\\mathbf{c}_{m}^{2}=\\sum_{m^{\\prime}}\\alpha_{m m^{\\prime}}^{12}\\mathbf{s}_{m^{\\prime}}^{1}}\\\\ {\\displaystyle\\mathbf{s}_{m}^{2}=\\mathrm{dec}^{2}(\\mathbf{s}_{m-1}^{2},\\mathbf{c}_{m}^{2},\\mathbf{y}_{m-1}^{2})}\\\\ {\\displaystyle P(\\mathbf{y}_{m}^{2})=\\mathrm{softmax}(\\mathbf{s}_{m}^{2}).}\\end{array}\n$$\n \nTu et al.  ( 2017 ) use exactly this architecture to train on bitext by setting the second output se- quence to be equal to the input sequence   $(\\mathbf{y}_{i}^{2}=\\mathbf{x}_{i})$  ). \nIn our proposed  triangle  model (Figure  1 d), the ﬁrst decoder is as above, but the second decoder has two attentions, one for the input states of the encoder and one for the output states of the ﬁrst decoder: \n\n$$\n\\begin{array}{r}{\\begin{array}{c}{\\mathbf{c}_{m}^{2}=\\left[\\sum_{m^{\\prime}}\\alpha_{m m^{\\prime}}^{12}\\mathbf{s}_{m^{\\prime}}^{1}\\quad\\sum_{n}\\alpha_{m n}^{2}\\mathbf{h}_{n}\\right]}\\\\ {\\mathbf{s}_{m}^{2}=\\operatorname*{det}\\mathbf{c}^{2}(\\mathbf{s}_{m-1}^{2},\\mathbf{c}_{m}^{2},\\mathbf{y}_{m-1}^{2})}\\\\ {P(\\mathbf{y}_{m}^{2})=\\mathrm{softmax}(\\mathbf{s}_{m}^{2}).}\\end{array}}\\end{array}\n$$\n \nNote that the context vectors resulting from the two attentions are concatenated, not added. \n3 Learning and Inference \nFor compactness, we will write    $\\mathbf{X}$   for the matrix whose rows are the    ${\\bf x}_{n}$  , and similarly    $\\mathbf{H},\\,\\mathbf{C}$  , and so on. We also write  A  for the matrix of attention weights:   $[\\mathbf{A}]_{i j}=\\alpha_{i j}$  . \nLet  $\\theta$   be the parameters of our model, which we train on sentence triples   $({\\bf X},{\\bf Y}^{1},{\\bf Y}^{2})$  . \n3.1 Maximum likelihood estimation \nDeﬁne the score of a sentence triple to be a log- linear interpolation of the two decoders’ probabil- ities: \n\n$$\n\\begin{array}{r}{\\begin{array}{l}{\\mathrm{score}(\\mathbf{Y}^{1},\\mathbf{Y}^{2}\\mid\\mathbf{X};\\theta)=\\lambda\\log P(\\mathbf{Y}^{1}\\mid\\mathbf{X};\\theta)\\;+}\\\\ {(1-\\lambda)\\log P(\\mathbf{Y}^{2}\\mid\\mathbf{X},\\mathbf{S}^{1};\\theta)}\\end{array}}\\end{array}\n$$\n \nwhere    $\\lambda$   is a parameter that controls the impor- tance of each sub-task. In all our experiments, we set    $\\lambda$   to 0 . 5. We then train the model to maximize \n\n$$\n\\mathcal{L}(\\boldsymbol{\\theta})=\\sum\\mathrm{score}(\\mathbf{Y}^{1},\\mathbf{Y}^{2}\\mid\\mathbf{X};\\boldsymbol{\\theta}),\n$$\n \nwhere the summation is over all sentence triples in the training data. \n3.2 Regularization \nWe can optionally add a regularization term to the objective function, in order to encourage our atten- tion mechanisms to conform to two intuitive prin- ciples of machine translation:  transitivity  and  in- vertibility . \nTransitivity  attention regularizer To a ﬁrst ap- proximation, the translation relation should be transitive ( Wang et al. ,  2006 ;  Levinboim and Chi- ang ,  2015 ): If source word  $\\mathbf{X}_{i}$   aligns to target word "}
{"page": 2, "image_path": "doc_images/N18-1008_2.jpg", "ocr_text": "PU} Yip) PU} Yip)\nT softmax T softmax\nee Sok,\nT decoder T decoder\nP(yi-+-Yu) POL Yyyi) PUT =*Yap) PUT \"Yin) Geen PUT *Yin) Goo\nT softmax T softmax T softmax Poottmax —fttention Psonmax 7 attentions\nSi---Su sist ee sf-st, sf--st,\nT decoder T decoder T decoder T decoder T decoder\nCee chee, oe, chee, clenel,\nTattention attention —_—“attention. attention ——_ attention ~~ _\nhy ++ hy hy -+-hy hy ---hy hy ---hy\nTencoder Tencoder Tencoder T encoder\nXp Xy Xp oe Xy Xp 00° Xy Xp Xy\n\n(a) single-task (b) multitask\n\n(c) cascade (d) triangle\n\nFigure 1: Variations on the standard attentional model. In the standard single-task model, the decoder attends to the\nencoder’s states. In a typical multitask setup, two decoders attend to the encoder’s states. In the cascade (Tu et al.,\n2017), the second decoder attends to the first decoder’s states. In our proposed triangle model, the second decoder\nattends to both the encoder’s states and the first decoder’s states. Note that for clarity’s sake there are dependencies\n\nnot shown.\n\nyj and y; aligns to target word Yz then x; should\nalso probably align to Yy- To encourage the model\nto preserve this relationship, we add the following\ntransitivity regularizer to the loss function of the\ntriangle models with a small weight Atrans = 0.2:\n\nLoans = Score(¥!, ¥) = Ayans JAA! = A?\n\nInvertibility attention regularizer The transla-\ntion relation also ought to be roughly invertible\n(Levinboim et al., 2015): if, in the reconstruc-\ntion version of the cascade model, source word\nx; aligns to target word yy} then it stands to rea-\nson that y; is likely to align to x;. So, whereas Tu\net al. (2017) let the attentions of the translator and\nthe reconstructor be unrelated, we try adding the\nfollowing invertibility regularizer to encourage the\nattentions to each be the inverse of the other, again\nwith a weight Ainy = 0.2:\n\nLiny = score(Y!, ¥?) — ainy ATA! — ff.\n3.3. Decoding\n\nSince we have two decoders, we now need to em-\nploy a two-phase beam search, following Tu et al.\n(2017):\n\n1. The first decoder produces, through standard\nbeam search, a set of triples each consist-\ning of a candidate transcription Y!, a score\nP(¥!), and a hidden state sequence Ss.\n\n2. For each transcription candidate from the first\ndecoder, the second decoder now produces\n\n84\n\nCorpus Speakers Segments Hours\nAinu-English 1 2,668 2.5\nMboshi-French 3 5,131 44\nSpanish-English 240 17,394 20\n\nTable 1: Statistics on our speech datasets.\n\nthrough beam search a set of candidate trans-\nlations Y, each with a score P(Y?).\n\n3. We then output the combination that yields\nthe highest total score(Y!, Y?).\n\n3.4 Implementation\n\nAll our models are implemented in DyNet (Neubig\net al., 2017).2 We use a dropout of 0.2, and train\nusing Adam with initial learning rate of 0.0002 for\na maximum of 500 epochs. For testing, we select\nthe model with the best performance on dev. At\ninference time, we use a beam size of 4 for each\ndecoder (due to GPU memory constraints), and\nthe beam scores include length normalization (Wu\net al., 2016) with a weight of 0.8, which Nguyen\nand Chiang (2017) found to work well for low-\nresource NMT.\n\n4 Speech Transcription and Translation\n\nWe focus on speech transcription and translation\nof endangered languages, using three different cor-\n\n?Our code is available at: https://bitbucket.org/\nantonis/dynet-multitask-models.\n", "vlm_text": "The image provides a comparison of different variations on the standard attentional model used in sequence-to-sequence tasks. Here's a breakdown of each model shown in the image:\n\n1. **(a) Single-task**: \n   - This is the standard model where a single decoder attends to the states of an encoder. \n   - The process follows this order: input sequence (`x_1 ... x_N`) is encoded into hidden states (`h_1 ... h_N`), an attention mechanism produces context vectors (`c_1 ... c_M`) which inform the decoder states (`s_1 ... s_M`), and a softmax function is applied for output prediction (`P(y_1 ... y_M)`).\n\n2. **(b) Multitask**:\n   - In this setup, the model possesses two decoders that both attend to the states of the same encoder. \n   - Similarly, inputs are encoded into hidden states, and each decoder has its own attention mechanism, context vectors, and predictions.\n\n3. **(c) Cascade** (referenced from Tu et al., 2017):\n   - Here, the second decoder does not directly attend to the encoder states. Instead, it attends to the states of the first decoder.\n   - This approach creates a sequential setup where the output of the first decoder informs the second decoder, resulting in a cascading attention.\n\n4. **(d) Triangle**:\n   - The innovative aspect of the triangle model is that the second decoder attends to both the encoder’s states and the first decoder’s states.\n   - This dual-attention setup might allow for more nuanced and context-aware predictions, as the second decoder is directly influenced by both prior stages.\n\nEach model variation utilizes an attention mechanism to align and focus on different parts of input sequences, enhancing the performance of tasks like machine translation or summarization.\n $\\mathbf{y}_{j}^{1}$    and  $\\mathbf{y}_{j}^{1}$    aligns to target word  $\\mathbf{y}_{k}^{2}$  , then  $\\mathbf{X}_{i}$   should also probably align to  $\\mathbf{y}_{k}^{2}$  . To encourage the model to preserve this relationship, we add the following transitivity  regularizer to the loss function of the triangle  models with a small weight  $\\lambda_{\\mathrm{trans}}=0.2$  : \n\n$$\n\\mathcal{L}_{\\mathrm{trans}}=\\mathrm{score}(\\mathbf{Y}^{1},\\mathbf{Y}^{2})-\\lambda_{\\mathrm{trans}}\\left\\|\\mathbf{A}^{12}\\mathbf{A}^{1}-\\mathbf{A}^{2}\\right\\|_{2}^{2}\\!.\n$$\n \nInvertibility  attention regularizer The transla- tion relation also ought to be roughly invertible ( Levinboim et al. ,  2015 ): if, in the  reconstruc- tion  version of the  cascade  model, source word  $\\mathbf{X}_{i}$   aligns to target word    $\\mathbf{y}_{j}^{1}$  , then it stands to rea- son that  $\\mathbf{y}_{j}$   is likely to align to    $\\mathbf{X}_{i}$  . So, whereas  Tu et al.  ( 2017 ) let the attentions of the translator and the reconstructor be unrelated, we try adding the following  invertibility  regularizer to encourage the attentions to each be the inverse of the other, again with a weight  $\\lambda_{\\mathrm{inv}}=0.2$  : \n\n$$\n\\mathcal{L}_{\\mathrm{inv}}=\\mathrm{score}(\\mathbf{Y}^{1},\\mathbf{Y}^{2})-\\lambda_{\\mathrm{inv}}\\left\\|\\mathbf{A}^{1}\\mathbf{A}^{12}-\\mathbf{I}\\right\\|_{2}^{2}.\n$$\n \n3.3 Decoding \nSince we have two decoders, we now need to em- ploy a two-phase beam search, following  Tu et al. ( 2017 ): \n1. The  ﬁrst decoder  produces, through standard beam search, a set of triples each consist- ing of a candidate transcription  $\\hat{\\mathbf{Y}}^{1}$  , a score  $P(\\hat{\\mathbf{Y}}^{1})$  ), and a hidden state sequence  $\\hat{\\mathbf{S}}$  . 2. For each transcription candidate from the  ﬁrst decoder , the  second decoder  now produces \nThe table lists information about three different language corpora: Ainu-English, Mboshi-French, and Spanish-English. Each row provides details on the number of speakers, segments, and duration in hours for each corpus. Specifically:\n\n- **Ainu-English**: This corpus has 1 speaker, 2,668 segments, and comprises 2.5 hours.\n- **Mboshi-French**: This corpus features 3 speakers, 5,131 segments, and spans 4.4 hours.\n- **Spanish-English**: This corpus includes 240 speakers, 17,394 segments, and totals 20 hours.\nthrough beam search a set of candidate trans- lations  $\\hat{\\mathbf{Y}}^{2}$  , each with a score    $P(\\hat{\\mathbf{Y}}^{2})$  ). 3. We then output the combination that yields the highest total score  $(\\mathbf{Y}^{1},\\mathbf{Y}^{2})$  . \n3.4 Implementation \nAll our models are implemented in DyNet ( Neubig et al. ,  2017 ).   We use a dropout of 0.2, and train using Adam with initial learning rate of 0 . 0002 for a maximum of 500 epochs. For testing, we select the model with the best performance on dev. At inference time, we use a beam size of 4 for each decoder (due to GPU memory constraints), and the beam scores include length normalization ( Wu et al. ,  2016 ) with a weight of 0.8, which  Nguyen and Chiang  ( 2017 ) found to work well for low- resource NMT. \n4 Speech Transcription and Translation \nWe focus on speech transcription and translation of endangered languages, using three di ﬀ erent cor- "}
{"page": 3, "image_path": "doc_images/N18-1008_3.jpg", "ocr_text": "Model Search Mboshi_ French | Ainu English | Spanish English\nASR MT ASR MT CER BLEU | CER’ BLEU CER BLEU\n(1) | auto text 1-best — 1-bes' 42.3 21.4 44.0 16.4 63.2 24.2\n(2) | gold text — 1-bes' 0.0 31.2 0.0 19.3 0.0 51.3\n(3) single-task 1-best — 20.8 — 12.0 —_ 21.6\n(4) multitask 4-best 1-bes 36.9 21.0 40.1 18.3 57.4 26.0\n(5) cascade 4-best 1-bes 39.7 24.3 42.1 19.8 58.1 26.8\n(6) triangle 4-best 1-bes 32.3 24.1 39.9 19.2 58.9 28.6\n(7) | triangle+Lyrans | 4-best — 1-bes' 33.0 24.7 | 43.3 20.2 59.3 28.6\n(8) triangle l-best 1-bes 31.8 19.7 | 38.9 19.8 58.4 28.8\n(9) | triangle+Lirans | 1-best — 1-bes' 32.1 20.9 | 43.0 20.3 59.1 28.5\nTable 2: The multitask models outperform the baseline single-task model and the pivot approach (auto/text) on all\nlanguage pairs tested. The triangle model also outperforms the simple multitask models on both tasks in almost all\ncases. The best results for each dataset and task are highlighted.\n\npora on three different language directions: Span-\nish (es) to English (en), Ainu (ai) to English, and\nMboshi (mb) to French (fr).\n\n4.1 Data\n\nSpanish is, of course, not an endangered language,\nbut the availability of the CALLHOME Spanish\nSpeech dataset (LDC2014T23) with English trans-\nlations (Post et al., 2013) makes it a convenient\nlanguage to work with, as has been done in almost\nall previous work in this area. It consists of tele-\nphone conversations between relatives (about 20\ntotal hours of audio) with more than 240 speak-\ners. We use the original train-dev-test split, with\nthe training set comprised of 80 conversations and\ndev and test of 20 conversations each.\n\nHokkaido Ainu is the sole surviving member of\nthe Ainu language family and is generally consid-\nered a language isolate. As of 2007, only ten native\nspeakers were alive. The Glossed Audio Corpus\nof Ainu Folklore provides 10 narratives with au-\ndio (about 2.5 hours of audio) and translations in\nJapanese and English.? Since there does not exist\na standard train-dev-test split, we employ a cross\nvalidation scheme for evaluation purposes. In each\nfold, one of the 10 narratives becomes the test set,\nwith the previous one (mod 10) becoming the dev\nset, and the remaining 8 narratives becoming the\ntraining set. The models for each of the 10 folds\nare trained and tested separately. On average, for\neach fold, we train on about 2000 utterances; the\ndev and test sets consist of about 270 utterances.\n\nshttp://ainucorpus .ninjal.ac.jp/corpus/en/\n\n85\n\nWe report results on the concatenation of all folds.\nThe Ainu text is split into characters, except for the\nequals (=) and underscore (_) characters, which are\nused as phonological or structural markers and are\nhus merged with the following character.*\nMboshi (Bantu C25 in the Guthrie classifica-\nion) is a language spoken in Congo-Brazzaville,\nwithout standard orthography. We use a corpus\n(Godard et al., 2017) of 5517 parallel utterances\n(about 4.4 hours of audio) collected from three na-\nive speakers. The corpus provides non-standard\ngrapheme transcriptions (close to the language\nphonology) produced by linguists, as well as\nFrench translations. We sampled 100 segments\nrom the training set to be our dev set, and used\nhe original dev set (514 sentences) as our test set.\n\n4.2 Implementation\n\nWe employ a 3-layer speech encoding scheme\nsimilar to that of Duong et al. (2016). The first\nbidirectional layer receives the audio sequence in\nhe form of 39-dimensional Perceptual Linear Pre-\ndictive (PLP) features (Hermansky, 1990) com-\nputed over overlapping 25ms-wide windows ev-\nery 10ms. The second and third layers consist of\nLSTMs with hidden state sizes of 128 and 512 re-\nspectively. Each layer encodes every second out-\nput of the previous layer. Thus, the sequence is\ndownsampled by a factor of 4, decreasing the com-\nputation load for the attention mechanism and the\ndecoders. In the speech experiments, the decoders\n\n4The data preprocessing scripts are released with the rest\nof our code.\n", "vlm_text": "The table presents a comparison of different models evaluated on a set of metrics for various languages. Here's a breakdown of the table's content:\n\n1. **Columns:**\n   - \"Model\": Lists different models being compared.\n     - \"ASR\" and \"MT\": Indicate the components related to Automatic Speech Recognition (ASR) and Machine Translation (MT) within the model.\n   - \"Search\": Specifies the search strategies used for ASR and MT: \"1-best\" or \"4-best\".\n   - Performance metrics for different language pairs:\n     - \"Mboshi CER\" (Character Error Rate)\n     - \"French BLEU\" (Bilingual Evaluation Understudy Score)\n     - \"Ainu CER\"\n     - \"English BLEU\"\n     - \"Spanish CER\"\n     - \"English BLEU\"\n\n2. **Rows:**\n   - Model types include variations like \"auto\", \"gold\", \"single-task\", \"multitask\", \"cascade\", \"triangle\", and \"triangle+L_trans\".\n   - Each row provides the CER or BLEU scores achieved by these models for different language tasks. Scores in bold indicate presumably the best performance for a specific metric across the models.\n\n3. **Performance:**\n   - CER measures the error rate in transcriptions.\n   - BLEU measures the quality of translated text.\n\nThe table aims to show how different model configurations and search strategies impact performance for speech recognition and translation across several languages.\npora on three di ﬀ erent language directions: Span- ish (es) to English (en), Ainu (ai) to English, and Mboshi (mb) to French (fr). \n4.1 Data \nSpanish is, of course, not an endangered language, but the availability of the CALLHOME Spanish Speech dataset (LDC2014T23) with English trans- lations ( Post et al. ,  2013 ) makes it a convenient language to work with, as has been done in almost all previous work in this area. It consists of tele- phone conversations between relatives (about 20 total hours of audio) with more than 240 speak- ers. We use the original train-dev-test split, with the training set comprised of 80 conversations and dev and test of 20 conversations each. \nHokkaido Ainu is the sole surviving member of the Ainu language family and is generally consid- ered a language isolate. As of 2007, only ten native speakers were alive. The Glossed Audio Corpus of Ainu Folklore provides 10 narratives with au- dio (about 2.5 hours of audio) and translations in Japanese and English.   Since there does not exist a standard train-dev-test split, we employ a cross validation scheme for evaluation purposes. In each fold, one of the 10 narratives becomes the test set, with the previous one (mod 10) becoming the dev set, and the remaining 8 narratives becoming the training set. The models for each of the 10 folds are trained and tested separately. On average, for each fold, we train on about 2000 utterances; the dev and test sets consist of about 270 utterances. \nWe report results on the concatenation of all folds. The Ainu text is split into characters, except for the equals  $(=)$   and underscore ( ) characters, which are used as phonological or structural markers and are thus merged with the following character. \nMboshi (Bantu C25 in the Guthrie classiﬁca- tion) is a language spoken in Congo-Brazzaville, without standard orthography. We use a corpus\n\n ( Godard et al. ,  2017 ) of 5517 parallel utterances\n\n (about 4.4 hours of audio) collected from three na- tive speakers. The corpus provides non-standard grapheme transcriptions (close to the language phonology) produced by linguists, as well as French translations. We sampled 100 segments from the training set to be our dev set, and used the original dev set (514 sentences) as our test set. \n4.2 Implementation \nWe employ a 3-layer speech encoding scheme similar to that of  Duong et al.  ( 2016 ). The ﬁrst bidirectional layer receives the audio sequence in the form of 39-dimensional Perceptual Linear Pre- dictive (PLP) features ( Hermansky ,  1990 ) com- puted over overlapping 25ms-wide windows ev- ery   $10\\mathrm{ms}$  . The second and third layers consist of LSTMs with hidden state sizes of 128 and 512 re- spectively. Each layer encodes every second out- put of the previous layer. Thus, the sequence is downsampled by a factor of 4, decreasing the com- putation load for the attention mechanism and the decoders. In the speech experiments, the decoders output the sequences at the grapheme level, so the output embedding size is set to 64. "}
{"page": 4, "image_path": "doc_images/N18-1008_4.jpg", "ocr_text": "output the sequences at the grapheme level, so the\noutput embedding size is set to 64.\n\nWe found that this simpler speech encoder\nworks well for our extremely small datasets. Ap-\nplying our models to larger datasets with many\nmore speakers would most likely require a more\nsophisticated speech encoder, such as the one used\nby Weiss et al. (2017).\n\n4.3 Results\n\nIn Table 2, we present results on three small\ndatasets that demonstrate the efficacy of our mod-\nels. We compare our proposed models against\nthree baselines and one “skyline.” The first base-\nline is a traditional pivot approach (line 1), where\nthe ASR output, a sequence of characters, is the\ninput to a character-based NMT system (trained\non gold transcriptions). The “skyline” model (line\n2) is the same NMT system, but tested on gold\ntranscriptions instead of ASR output. The second\nbaseline is translation directly from source speech\nto target text (line 3). The last baseline is the stan-\ndard multitask model (line 4), which is similar to\nthe model of Weiss et al. (2017).\n\nThe cascade model (line 5) outperforms the\nbaselines on the translation task, while only falling\nbehind the multitask model in the transcription\ntask. On all three datasets, the triangle model\n(lines 6, 7) outperforms all baselines, including\nthe standard multitask model. On Ainu-English,\nwe even obtain translations that are comparable to\nthe “skyline” model, which is tested on gold Ainu\ntranscriptions.\n\nComparing the performance of all models\nacross the three datasets, there are two notable\ntrends that verify common intuitions regarding the\nspeech transcription and translation tasks. First, an\nincrease in the number of speakers hurts the per-\nformance of the speech transcription tasks. The\ncharacter error rates for Ainu are smaller than the\nCER in Mboshi, which in turn are smaller than the\nCER in CALLHOME. Second, the character-level\nBLEU scores increase as the amount of training\ndata increases, with our smallest dataset (Ainu)\nhaving the lowest BLEU scores, and the largest\ndataset (CALLHOME) having the highest BLEU\nscores. This is expected, as more training data\nmeans that the translation decoder learns a more\ninformed character-level language model for the\ntarget language.\n\nNote that Weiss et al. (2017) report much higher\n\n86\n\nBLEU scores on CALLHOME: our model un-\nderperforms theirs by almost 9 word-level BLEU\npoints. However, their model has significantly\nmore parameters and is trained on 10 times\nmore data than ours. Such an amount of data\nwould never be available in our endangered lan-\nguages scenario. When calculated on the word-\nlevel, all our models’ BLEU scores are between 3\nand 7 points for the extremely low resource\ndatasets (Mboshi-French and Ainu-English), and\nbetween 7 and 10 for CALLHOME. Clearly, the\nsize of the training data in our experiments is not\nenough for producing high quality speech transla-\ntions, but we plan to investigate the performance\nof our proposed models on larger datasets as part\nof our future work.\n\nTo evaluate the effect of using the combined\nscore from both decoders at decoding time, we\nevaluated the triangle models using only the 1-best\noutput from the speech model (lines 8, 9). One\nwould expect that this would favor speech at\nhe expense of translation. In transcription accu-\nracy, we indeed observed improvements across the\nboard. In translation accuracy, we observed a sur-\nprisingly large drop on Mboshi-French, but sur-\nprisingly little effect on the other language pairs\n— in fact, BLEU scores tended to go up slightly,\nbut not significantly.\n\nFinally, Figure 2 visualizes the attention ma-\ntrices for one utterance from the baseline multi-\nask model and our proposed triangle model. It\nis clear that our intuition was correct: the transla-\nion decoder receives most of its context from the\ntranscription decoder, as indicated by the higher\nattention weights of A!*. Ideally, the area under\nhe red squares (gold alignments) would account\n‘or 100% of the attention mass of A!*. In our tri-\nangle model, the total mass under the red squares\nis 34%, whereas the multitask model’s correct at-\nentions amount to only 21% of the attention mass.\n\n5 Word Discovery\n\nAlthough the above results show that our model\ngives large performance improvements, in abso-\nlute terms, its performance on such low-resource\ntasks leaves a lot of room for future improvement.\nA possible more realistic application of our meth-\nods is word discovery, that is, finding word bound-\naries in unsegmented phonetic transcriptions.\nAfter training an attentional encoder-decoder\nmodel between Mboshi unsegmented phonetic se-\n", "vlm_text": "\nWe found that this simpler speech encoder works well for our extremely small datasets. Ap- plying our models to larger datasets with many more speakers would most likely require a more sophisticated speech encoder, such as the one used by  Weiss et al.  ( 2017 ). \n4.3 Results \nIn Table  2 , we present results on three small datasets that demonstrate the e ﬃ cacy of our mod- els. We compare our proposed models against three baselines and one “skyline.” The ﬁrst base- line is a traditional pivot approach (line 1), where the ASR output, a sequence of characters, is the input to a character-based NMT system (trained on gold transcriptions). The “skyline” model (line 2) is the same NMT system, but tested on gold transcriptions instead of ASR output. The second baseline is translation directly from source speech to target text (line 3). The last baseline is the stan- dard  multitask  model (line 4), which is similar to the model of  Weiss et al.  ( 2017 ). \nThe  cascade  model (line 5) outperforms the baselines on the translation task, while only falling behind the  multitask  model in the transcription task. On all three datasets, the  triangle  model (lines 6, 7) outperforms all baselines, including the standard  multitask  model. On Ainu-English, we even obtain translations that are comparable to the “skyline” model, which is tested on gold Ainu transcriptions. \nComparing the performance of all models across the three datasets, there are two notable trends that verify common intuitions regarding the speech transcription and translation tasks. First, an increase in the number of speakers hurts the per- formance of the speech transcription tasks. The character error rates for Ainu are smaller than the CER in Mboshi, which in turn are smaller than the CER in CALLHOME. Second, the character-level BLEU scores increase as the amount of training data increases, with our smallest dataset (Ainu) having the lowest BLEU scores, and the largest dataset (CALLHOME) having the highest BLEU scores. This is expected, as more training data means that the translation decoder learns a more informed character-level language model for the target language. \nNote that  Weiss et al.  ( 2017 ) report much higher BLEU scores on CALLHOME: our model un- derperforms theirs by almost 9  word-level  BLEU points. However, their model has signiﬁcantly more parameters and is trained on 10 times more data than ours. Such an amount of data would never be available in our endangered lan- guages scenario. When calculated on the word- level, all our models’ BLEU scores are between 3 and 7 points for the extremely low resource datasets (Mboshi-French and Ainu-English), and between 7 and 10 for CALLHOME. Clearly, the size of the training data in our experiments is not enough for producing high quality speech transla- tions, but we plan to investigate the performance of our proposed models on larger datasets as part of our future work. \n\nTo evaluate the e ﬀ ect of using the combined score from both decoders at decoding time, we evaluated the  triangle  models using only the 1-best output from the speech model (lines 8, 9). One would expect that this would favor speech at the expense of translation. In transcription accu- racy, we indeed observed improvements across the board. In translation accuracy, we observed a sur- prisingly large drop on Mboshi-French, but sur- prisingly little e ﬀ ect on the other language pairs – in fact, BLEU scores tended to go up slightly, but not signiﬁcantly. \nFinally, Figure  2  visualizes the attention ma- trices for one utterance from the baseline multi- task model and our proposed  triangle  model. It is clear that our intuition was correct: the transla- tion decoder receives most of its context from the transcription decoder, as indicated by the higher attention weights of    $\\mathbf{A}^{12}$  . Ideally, the area under the red squares (gold alignments) would account for   $100\\%$   of the attention mass of    $\\mathbf{A}^{12}$  . In our tri- angle model, the total mass under the red squares is  $34\\%$  , whereas the multitask model’s correct at- tentions amount to only  $21\\%$   of the attention mass. \n5 Word Discovery \nAlthough the above results show that our model gives large performance improvements, in abso- lute terms, its performance on such low-resource tasks leaves a lot of room for future improvement. A possible more realistic application of our meth- ods is word discovery, that is, ﬁnding word bound- aries in unsegmented phonetic transcriptions. \nAfter training an attentional encoder-decoder model between Mboshi unsegmented phonetic se- "}
{"page": 5, "image_path": "doc_images/N18-1008_5.jpg", "ocr_text": "Img aum sm x ow ye 6u\n=\n\nLip gaum sm yxow je bu\n\nAZ\n\nd\n\na4Agl el e assaia sins aw af\n[= ©\na4Ael el e assaia sins aw af\n\n|\n\n(a) multitask\n\nAZ\n\nee\n\n(b) triangle + transitivity\n\nFigure 2: Attentions in an Mboshi-French sentence, extracted from two of our models. The red squares denote gold\nalignments. The second decoder of the triangle model receives most of its context from the first decoder through\nA’? instead of the source. The A? matrix of the triangle model is more informed (34% correct attention mass) than\n\nthe multitask one (21% correct), due to the transitivity regularizer.\n\nquences and French word sequences, the atten-\ntion weights can be thought of as soft alignments,\nwhich allow us to project the French word bound-\naries onto Mboshi. Although we could in princi-\nple perform word discovery directly on speech, we\nleave this for future work, and only explore single-\ntask and reconstruction models.\n\n5.1 Data\n\nWe use the same Mboshi-French corpus as in Sec-\ntion 4, but with the original training set of 4617\nutterances and the dev set of 514 utterances. Our\nparallel data consist of the unsegmented phonetic\nMboshi transcriptions, along with the word-level\nFrench translations.\n\n5.2 Implementation\n\nWe first replicate the model of Boito et al. (2017),\nwith a single-layer bidirectional encoder and sin-\ngle layer decoder, using an embedding and hidden\nsize of 12 for the base model, and an embedding\nand hidden state size of 64 for the reverse model.\nIn our own models, we set the embedding size to\n32 for Mboshi characters, 64 for French words,\nand the hidden state size to 64. We smooth the at-\n\ntention weights A using the method of Duong et al.\n(2016) with a temperature T = 10 for the softmax\ncomputation of the attention mechanism.\nFollowing Boito et al. (2017), we train mod-\nels both on the base Mboshi-to-French direction,\nas well as the reverse (French-to-Mboshi) direc-\ntion, with and without this smoothing operation.\nWe further smooth the computed soft alignments\nof all models so that dan = (Gmn—1 + Gmn+Gmn+1)/3\nas a post-processing step. From the single-task\nmodels we extract the A! attention matrices. We\nalso train reconstruction models on both direc-\ntions, with and without the invertibility regularizer,\nextracting both A! and A!? matrices. The two ma-\ntrices are then combined so that A = A! + (A!?)?,\n\n5.3 Results\n\nEvaluation is done both at the token and the\ntype level, by computing precision, recall, and F-\nscore over the discovered segmentation, with the\nbest results shown in Table 3. We reimplemented\nthe base (Mboshi-French) and reverse (French-\nMboshi) models from Boito et al. (2017), and the\nperformance of the base model was comparable\nto the one reported. However, we were unable to\n", "vlm_text": "This image visually represents the attention mechanisms in machine learning models dealing with a translation task between the Mboshi and French languages. It is divided into sections showing two models: (a) multitask and (b) triangle with transitivity regularizer. Each section further includes matrices labeled \\( \\mathbf{A}^1 \\) and \\( \\mathbf{A}^2 \\), representing attention weights at different stages of processing, and \\( \\mathbf{A}^{12} \\) for the triangle model.\n\n1. **\\( \\mathbf{A}^1 \\) Matrices**: These matrices depict the initial attention layer for both models, showing alignments between input source tokens (Mboshi) and initial processing steps.\n2. **\\( \\mathbf{A}^2 \\) Matrices**: These matrices depict the attention layer related to the decoder or next stage where the French sentences align with either the Mboshi inputs or previous processing outputs.\n3. **\\( \\mathbf{A}^{12} \\) Matrix** (Right of the triangle model): This matrix illustrates how much attention in the second decoder of the triangle model is derived from the first decoder instead of directly from the source. It shows connections between the outputs of the first layer of decoders and the subsequent decoder.\n4. **Red Squares**: These denote the 'gold' alignments which are the correct or target alignments for the translation task.\n5. **Comparison**: The attention distribution effectiveness between the multitask and triangle models, with triangle + transitivity showing a more \"informed\" attention aligning more accurately by a percentage, presumably improving translation or understanding.\n\nThe waveform below each section relates to the speech or verbal input being translated, likely representing the acoustic features tied into the multitask learning model.\nquences and French word sequences, the atten- tion weights can be thought of as soft alignments, which allow us to project the French word bound- aries onto Mboshi. Although we could in princi- ple perform word discovery directly on speech, we leave this for future work, and only explore single- task and reconstruction models. \n5.1 Data \nWe use the same Mboshi-French corpus as in Sec- tion  4 , but with the original training set of 4617 utterances and the dev set of 514 utterances. Our parallel data consist of the unsegmented phonetic Mboshi transcriptions, along with the word-level French translations. \n5.2 Implementation \nWe ﬁrst replicate the model of  Boito et al.  ( 2017 ), with a single-layer bidirectional encoder and sin- gle layer decoder, using an embedding and hidden size of 12 for the base model, and an embedding and hidden state size of 64 for the reverse model. In our own models, we set the embedding size to 32 for Mboshi characters, 64 for French words, and the hidden state size to 64. We smooth the at- tention weights  A  using the method of  Duong et al. ( 2016 ) with a temperature    $T=10$   for the softmax computation of the attention mechanism. \n\nFollowing  Boito et al.  ( 2017 ), we train mod- els both on the  base  Mboshi-to-French direction, as well as the  reverse  (French-to-Mboshi) direc- tion, with and without this smoothing operation. We further smooth the computed soft alignments of all models so that    $a_{m n}=(a_{m n-1}\\!+\\!a_{m n}\\!+\\!a_{m n+1})/3$  as a post-processing step. From the  single-task models we extract the    $\\mathbf{A}^{1}$    attention matrices. We also train  reconstruction  models on both direc- tions, with and without the  invertibility  regularizer, extracting both  $\\mathbf{A}^{1}$    and  $\\mathbf{A}^{12}$    matrices. The two ma- trices are then combined so that  $\\mathbf{A}=\\mathbf{A}^{1}+(\\mathbf{A}^{12})^{T}$  . \n5.3 Results \nEvaluation is done both at the token and the type level, by computing precision, recall, and F- score over the discovered segmentation, with the best results shown in Table  3 . We reimplemented the base (Mboshi-French) and reverse (French- Mboshi) models from  Boito et al.  ( 2017 ), and the performance of the base model was comparable to the one reported. However, we were unable to "}
{"page": 6, "image_path": "doc_images/N18-1008_6.jpg", "ocr_text": ". : Tokens Types\nModel (with smoothing) Precision Recall F-score | Precision Recall F-score\nBoito et al. 2017 base 5.85 6.82 6.30 6.76 15.00 9.32\n(reported) reverse 21.44 16.49 18.64 27.23 15.02 19.36\nBoito et al. 2017 base 6.87 6.33 6.59 6.17 13.02 8.37\n(reimplementation) reverse 7.58 8.16 7.86 9.22 11.97 10.42\nour sinele-task base 7.99 7.57 7.78 7.59 16.41 10.38\n§ \" reverse 11.31 = 11.82 11.56 9.29 14.75 11.40\nreconstruction + 0.2Liny 8.93 9.78 9.33 8.66 15.48 11.02\nreconstruction + 0.5.Liny 742 10.00 8.52 10.46 16.36 12.76\nTable 3: The reconstruction model with the invertibility regularizer produces more informed attentions that result\nin better word discovery for Mboshi with an Mboshi-French model. Scores reported by previous work are in italics\n\nand best scores from our experiments are in bold.\n\nreproduce the significant gains that were reported\nwhen using the reverse model (italicized in Ta-\nble 3). Also, our version of both the base and re-\nverse singletask models performed better than our\nreimplementation of the baseline.\n\nFurthermore, we found that we were able to\nobtain even better performance at the type level\nby combining the attention matrices of a recon-\nstruction model trained with the invertibility reg-\nularizer. Boito et al. (2017) reported that combin-\ning the attention matrices of a base and a reverse\nmodel significantly reduced performance, but they\ntrained the two models separately. In contrast, we\nobtain the base (A!) and the reverse attention ma-\ntrices (A!?) from a model that trains them jointly,\nwhile also tying them together through the invert-\nibility regularizer. Using the regularizer is key to\nthe improvements; in fact, we did not observe any\nimprovements when we trained the reconstruction\nmodels without the regularizer.\n\n6 Negative Results: High-Resource Text\nTranslation\n\n6.1 Data\n\nFor evaluating our models on text translation, we\nuse the Europarl corpus which provides parallel\nsentences across several European languages. We\nextracted 1,450,890 three-way parallel sentences\non English, French, and German. The concatena-\ntion of the newstest 2011-2013 sets (8,017 sen-\ntences) is our dev set, and our test set is the con-\ncatenation of the newstest 2014 and 2015 sets\n(6,003 sentences). We test all architectures on the\nsix possible translation directions between English\n\n88\n\n(en), French (fr) and German (de). All the se-\nquences are represented by subword units with\nbyte-pair encoding (BPE) (Sennrich et al., 2016)\ntrained on each language with 32000 operations.\n\n6.2. Experimental Setup\n\nOn all experiments, the encoder and the decoder(s)\nhave 2 layers of LSTM units with hidden state size\nand attention size of 1024, and embedding size\nof 1024. For this high resource scenario, we only\ntrain for a maximum of 40 epochs.\n\n6.3 Results\n\nThe accuracy of all the models on all six lan-\nguage pair directions is shown in Table 4. In all\ncases, the best models are the baseline single-task\nor simple multitask models. There are some in-\nstances, such as English-German, where the re-\nconstruction or the triangle models are not statis-\nically significantly different from the best model.\nThe reason for this, we believe, is that in the case\nof text translation between so linguistically close\nlanguages, the lower level representations (the out-\nput of the encoder) provide as much information\nas the higher level ones, without the search errors\nhat are introduced during inference.\n\nA notable outcome of this experiment is that we\ndo not observe the significant improvements with\nhe reconstruction models that Tu et al. (2017) ob-\nserved. A few possible differences between our\nexperiment and theirs are: our models are BPE-\nbased, theirs are word-based; we use Adam for\noptimization, they use Adadelta; our model has\nslightly fewer parameters than theirs; we test on\nless typologically different language pairs than\n\n", "vlm_text": "The table presents an evaluation of different models with smoothing in terms of their precision, recall, and F-score for both tokens and types. The models compared are from Boito et al. 2017 (both reported and reimplemented versions) and a single-task model from the authors of the table. Additionally, the table evaluates models that use reconstruction with two different levels of an inverse loss function (denoted as \\( \\mathcal{L}_{inv} \\)).\n\n1. **Columns:**\n   - The table is divided into two main sections: Tokens and Types.\n   - Each section evaluates three metrics: Precision, Recall, and F-score.\n\n2. **Rows:**\n   - The first two rows present the results from Boito et al. 2017. The \"base\" and \"reverse\" configurations are compared, with the reverse model generally performing better.\n   - The next two rows show reimplementation results of Boito et al. 2017, indicating similar trends but with slight differences in metrics.\n   - \"Our single-task\" row displays the performance of the authors' own model, where the reverse configuration significantly improves metrics in both tokens and types compared to the base version.\n   - The last two rows assess models with reconstruction and varying levels of inverse loss. Performance varies, with an inverse loss of 0.5 achieving the highest F-scores in both tokens and types categories.\n\nThe table demonstrates comparative model performance, emphasizing the impact of different configurations and additional loss functions on the evaluation metrics.\nreproduce the signiﬁcant gains that were reported when using the reverse model ( italicized  in Ta- ble  3 ). Also, our version of both the base and re- verse singletask models performed better than our re implementation of the baseline. \nFurthermore, we found that we were able to obtain even better performance at the type level by combining the attention matrices of a recon- struction model trained with the  invertibility  reg- ularizer.  Boito et al.  ( 2017 ) reported that combin- ing the attention matrices of a base and a reverse model signiﬁcantly reduced performance, but they trained the two models separately. In contrast, we obtain the base   $(\\mathbf{A}^{1})$   and the reverse attention ma- trices   $(\\mathbf{A}^{12})$   from a model that trains them jointly, while also tying them together through the  invert- ibility  regularizer. Using the regularizer is key to the improvements; in fact, we did not observe any improvements when we trained the reconstruction models without the regularizer. \n6 Negative Results: High-Resource Text Translation \n6.1 Data \nFor evaluating our models on text translation, we use the Europarl corpus which provides parallel sentences across several European languages. We extracted 1,450,890 three-way parallel sentences on English, French, and German. The concatena- tion of the newstest 2011–2013 sets (8,017 sen- tences) is our dev set, and our test set is the con- catenation of the newstest 2014 and 2015 sets (6,003 sentences). We test all architectures on the six possible translation directions between English (en), French (fr) and German (de). All the se- quences are represented by subword units with byte-pair encoding (BPE) ( Sennrich et al. ,  2016 ) trained on each language with 32000 operations. \n\n6.2 Experimental Setup \nOn all experiments, the encoder and the decoder(s) have 2 layers of LSTM units with hidden state size and attention size of 1024, and embedding size of 1024. For this high resource scenario, we only train for a maximum of 40 epochs. \n6.3 Results \nThe accuracy of all the models on all six lan- guage pair directions is shown in Table  4 . In all cases, the best models are the baseline single-task or simple multitask models. There are some in- stances, such as English-German, where the  re- construction  or the  triangle  models are not statis- tically signiﬁcantly di ﬀ erent from the best model. The reason for this, we believe, is that in the case of text translation between so linguistically close languages, the lower level representations (the out- put of the encoder) provide as much information as the higher level ones, without the search errors that are introduced during inference. \nA notable outcome of this experiment is that we do not observe the signiﬁcant improvements with the  reconstruction  models that  Tu et al.  ( 2017 ) ob- served. A few possible di ﬀ erences between our experiment and theirs are: our models are BPE- based, theirs are word-based; we use Adam for optimization, they use Adadelta; our model has slightly fewer parameters than theirs; we test on less typologically di ﬀ erent language pairs than "}
{"page": 7, "image_path": "doc_images/N18-1008_7.jpg", "ocr_text": "sot\n\nModel en>fr en-de fren frode deen de-fr\nsingletask 20.92 12.69 20.96 11.24 16.10 15.29\nmultitask s > x,t 20.54 12.79 20.01 11.18 16.31 15.07\ncascade s > x > t 15.93 11.31 16.58 7.60 13.46 13.24\ncascade s > t > x 20.34 = 12.27 19.17 11.09 15.24 14.78\nreconstruction 20.19 12.44 20.63 10.88 15.66 13.44\nreconstruction +Liny 20.72 12.64 20.11 1046 15.43 12.64\ntriangle » as, t 20.39 12.70 17.93 10.17 14.94 14.07\ntriangle » as, tt+Lerans | 20.52 12.64 18.34 1042 15.22 14.37\ntriangle » 2s, x 20.38 12.40 1850 10.22 15.62 14.77\ntriangle » 2s, X+Lrans | 20.64 12.42 19.20 10.21 15.87 14.89\n\nTable 4: BLEU scores for each model and translation direction s — tf. In the multitask, cascade, and triangle\nmodels, x stands for the third language, other than s and ¢. In each column, the best results are highlighted. The\nnon-highlighted results are statistically significantly worse than the single-task baseline.\n\nEnglish-Chinese.\n\nHowever, we also observe that in most cases\nour proposed regularizers lead to increased perfor-\nmance. The invertibility regularizer aids the recon-\nstruction models in achiev slightly higher BLEU\nscores in 3 out of the 6 cases. The transitivity reg-\nularizer is even more effective: in 9 out the 12\nsource-target language combinations, the triangle\nmodels achieve higher performance when trained\nusing the regularizer. Some of them are statistical\nsignificant improvements, as in the case of French\nto English where English is the intermediate target\nlanguage and German is the final target.\n\n7 Related Work\n\nThe speech translation problem has been tradi-\ntionally approached by using the output of an\nASR system as input to a MT system. For ex-\nample, Ney (1999) and Matusov et al. (2005)\nuse ASR output lattices as input to translation\nmodels, integrating speech recognition uncertainty\ninto the translation model. Recent work has fo-\ncused more on modelling speech translation with-\nout explicit access to transcriptions. Duong et al.\n(2016) introduced a sequence-to-sequence model\nfor speech translation without transcriptions but\nonly evaluated on alignment, while Anastasopou-\nlos et al. (2016) presented an unsupervised align-\nment method for speech-to-translation alignment.\nBansal et al. (2017) used an unsupervised term\ndiscovery system (Jansen et al., 2010) to clus-\nter recurring audio segments into pseudowords\n\n89\n\nand translate speech using a bag-of-words model.\nBérard et al. (2016) translated synthesized speech\ndata using a model similar to the Listen Attend\nand Spell model (Chan et al., 2016). A larger-scale\nstudy (Bérard et al., 2018) used an end-to-end neu-\nral system system for translating audio books be-\ntween French and English. On a different line of\nwork, Boito et al. (2017) used the attentions of a\nsequence-to-sequence model for word discovery.\n\nMultitask learning (Caruana, 1998) has found\nextensive use across several machine learning and\nNLP fields. For example, Luong et al. (2016) and\nEriguchi et al. (2017) jointly learn to parse and\ntranslate; Kim et al. (2017) combine CTC- and\nattention-based models using multitask models for\nspeech transcription; Dong et al. (2015) use mul-\ntitask learning for multiple language translation.\nToshniwal et al. (2017) apply multitask learning\nto neural speech recognition in a less traditional\nfashion: the lower-level outputs of the speech en-\ncoder are used for fine-grained auxiliary tasks such\nas predicting HMM states or phonemes, while the\nfinal output of the encoder is passed to a character-\nlevel decoder.\n\nOur work is most similar to the work of Weiss\net al. (2017). They used sequence-to-sequence\nmodels to transcribe Spanish speech and trans-\nlate it in English, by jointly training the two tasks\nin a multitask scenario where the decoders share\nthe encoder. In contrast to our work, they use a\nlarge corpus for training the model on roughly 163\nhours of data, using the Spanish Fisher and CALL-\n", "vlm_text": "The table presents the results of different models used for translating between several language pairs, specifically English (en), French (fr), and German (de). The table is organized with rows representing various model types and columns representing translation tasks or directions. Each cell contains a number, which likely represents a performance metric like BLEU score, indicating the quality of the translation output for that language pair and model.\n\nHere's a breakdown of what appears in the table:\n\n- **Column Headers**: \n  - `s → t`: Indicates source language `s` to target language `t`.\n  - Language pairs: `en→fr`, `en→de`, `fr→en`, `fr→de`, `de→en`, `de→fr`.\n\n- **Row Labels (Models)**:\n  - `singletask`\n  - `multitask s → x, t`\n  - `cascade s → x → t`\n  - `cascade s → t → x`\n  - `reconstruction`\n  - `reconstruction + L_{inv}`\n  - `triangle s → x → t`\n  - `triangle s → x → t + L_{trans}`\n  - `triangle s → t → x`\n  - `triangle s → t → x + L_{trans}`\n\n- **Performance Scores**: \n  - Each model's performance across different language pairs is indicated by the numerical values in each cell.\n  \n- **Bolded Values**:\n  - Certain values are bolded, perhaps indicating the best performance for that language pair among the models compared.\n\nFrom these details, it's inferred that the table aims to compare translation quality across different modeling approaches for specific language pairs.\nEnglish-Chinese. \nHowever, we also observe that in most cases our proposed regularizers lead to increased perfor- mance. The  invertibility  regularizer aids the  recon- struction  models in achiev slightly higher BLEU scores in 3 out of the 6 cases. The  transitivity  reg- ularizer is even more e ﬀ ective: in 9 out the 12 source-target language combinations, the  triangle models achieve higher performance when trained using the regularizer. Some of them are statistical signiﬁcant improvements, as in the case of French to English where English is the intermediate target language and German is the ﬁnal target. \n7 Related Work \nThe speech translation problem has been tradi- tionally approached by using the output of an ASR system as input to a MT system. For ex- ample, Ney (1999) and Matusov et al. (2005)use ASR output lattices as input to translation models, integrating speech recognition uncertainty into the translation model. Recent work has fo- cused more on modelling speech translation with- out explicit access to transcriptions.  Duong et al. ( 2016 ) introduced a sequence-to-sequence model for speech translation without transcriptions but only evaluated on alignment, while  Anastasopou- los et al.  ( 2016 ) presented an unsupervised align- ment method for speech-to-translation alignment. Bansal et al.  ( 2017 ) used an unsupervised term discovery system ( Jansen et al. ,  2010 ) to clus- ter recurring audio segments into pseudowords and translate speech using a bag-of-words model. B´ erard et al.  ( 2016 ) translated synthesized speech data using a model similar to the Listen Attend and Spell model ( Chan et al. ,  2016 ). A larger-scale study ( B´ erard et al. ,  2018 ) used an end-to-end neu- ral system system for translating audio books be- tween French and English. On a di ﬀ erent line of work,  Boito et al.  ( 2017 ) used the attentions of a sequence-to-sequence model for word discovery. \n\nMultitask learning ( Caruana ,  1998 ) has found extensive use across several machine learning and NLP ﬁelds. For example,  Luong et al.  ( 2016 ) and Eriguchi et al.  ( 2017 ) jointly learn to parse and translate;  Kim et al.  ( 2017 ) combine CTC- and attention-based models using multitask models for speech transcription;  Dong et al.  ( 2015 ) use mul- titask learning for multiple language translation. Toshniwal et al.  ( 2017 ) apply multitask learning to neural speech recognition in a less traditional fashion: the lower-level outputs of the speech en- coder are used for ﬁne-grained auxiliary tasks such as predicting HMM states or phonemes, while the ﬁnal output of the encoder is passed to a character- level decoder. \nOur work is most similar to the work of  Weiss et al.  ( 2017 ). They used sequence-to-sequence models to transcribe Spanish speech and trans- late it in English, by jointly training the two tasks in a multitask scenario where the decoders share the encoder. In contrast to our work, they use a large corpus for training the model on roughly 163 hours of data, using the Spanish Fisher and CALL- HOME conversational speech corpora. The pa- rameter number of their model is signiﬁcantly larger than ours, as they use 8 encoder layers, and 4 layers for each decoder. This allows their model to adequately learn from such a large amount of data and deal well with speaker variation. How- ever, training such a large model on endangered language datasets would be infeasible. "}
{"page": 8, "image_path": "doc_images/N18-1008_8.jpg", "ocr_text": "HOME conversational speech corpora. The pa-\nrameter number of their model is significantly\nlarger than ours, as they use 8 encoder layers, and\n4 layers for each decoder. This allows their model\nto adequately learn from such a large amount of\ndata and deal well with speaker variation. How-\never, training such a large model on endangered\nlanguage datasets would be infeasible.\n\nOur model also bears similarities to the archi-\ntecture of the model proposed by Tu et al. (2017).\nThey report significant gains in Chinese-English\ntranslation by adding an additional reconstruction\ndecoder that attends on the last states of the trans-\nlation decoder, mainly inspired by auto-encoders.\n\n8 Conclusion\n\nWe presented a novel architecture for multitask\nlearning that provides the second task with higher-\nlevel representations produced from the first task\ndecoder. Our model outperforms both the single-\ntask models as well as traditional multitask ar-\nchitectures. Evaluating on extremely low-resource\nsettings, our model improves on both speech tran-\nscription and translation. By augmenting our mod-\nels with regularizers that implement transitivity\nand invertibility, we obtain further improvements\non all low-resource tasks.\n\nThese results will hopefully lead to new tools\nfor endangered language documentation. Projects\nlike BULB aim to collect about 100 hours of audio\nwith translations, but it may be impractical to tran-\nscribe this much audio for many languages. For\nfuture work, we aim to extend these methods to\nsettings where we don’t necessarily have sentence\ntriples, but where some audio is only transcribed\nand some audio is only translated.\n\nAcknowledgements This work was generously\nsupported by NSF Award 1464553. We are grate-\nful to the anonymous reviewers for their useful\ncomments.\n\nReferences\n\nGilles Adda, Sebastian Stiiker, Martine Adda-Decker,\nOdette Ambouroue, Laurent Besacier, David Bla-\nchon, Héléne Bonneau-Maynard, Pierre Godard, Fa-\ntima Hamlaoui, Dmitry Idiatov, et al. 2016. Break-\ning the unwritten language barrier: The BULB\nproject. Procedia Computer Science, 81:8-14.\n\nAntonios Anastasopoulos, David Chiang, and Long\nDuong. 2016. An unsupervised probability model\n\n90\n\nfor speech-to-translation alignment of low-resource\nlanguages. In Proc. EMNLP.\n\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2015. Neural machine translation by jointly\nlearning to align and translate. In Proc. ICLR.\n\nSameer Bansal, Herman Kamper, Adam Lopez, and\nSharon Goldwater. 2017. Towards speech-to-text\ntranslation without speech recognition. In Proc.\nEACL.\n\nAlexandre Bérard, Laurent Besacier, Ali Can Ko-\ncabiyikoglu, and Olivier Pietquin. 2018. End-to-\nend automatic speech translation of audiobooks.\narXiv: 1802.04200.\n\nAlexandre Bérard, Olivier Pietquin, Christophe Servan,\nand Laurent Besacier. 2016. Listen and translate:\nA proof of concept for end-to-end speech-to-text\ntranslation. In Proc. NIPS Workshop on End-to-end\nLearning for Speech and Audio Processing.\n\nSteven Bird, Lauren Gawne, Katie Gelbart, and Isaac\nMcAlister. 2014a. Collecting bilingual audio in re-\nmote indigenous communities. In Proc. COLING.\n\nSteven Bird, Florian R. Hanke, Oliver Adams, and Hae-\njoong Lee. 2014b. Aikuma: A mobile app for col-\nlaborative language documentation. In Proc. of the\n2014 Workshop on the Use of Computational Meth-\nods in the Study of Endangered Languages.\n\nDavid Blachon, Elodie Gauthier, Laurent Besacier,\nGuy-Noél Kouarata, Martine Adda-Decker, and An-\nnie Rialland. 2016. Parallel speech collection for\nunder-resourced language studies using the LIG-\nAikuma mobile device app. In Proc. SLTU (Spoken\nLanguage Technologies for Under-Resourced Lan-\nguages), volume 81.\n\nMarcely Zanon Boito, Alexandre Bérard, Aline Villav-\nicencio, and Laurent Besacier. 2017. Unwritten lan-\nguages demand attention too! word discovery with\nencoder-decoder models. arXiv:1709.05631.\n\nRich Caruana. 1998. Multitask learning. In Learning\nto learn, pages 95-133. Springer.\n\nWilliam Chan, Navdeep Jaitly, Quoc Le, and Oriol\nVinyals. 2016. Listen, attend and spell: A neural\nnetwork for large vocabulary conversational speech\nrecognition. In Proc. ICASSP, pages 4960-4964.\nIEEE.\n\nDaxiang Dong, Hua Wu, Wei He, Dianhai Yu, and\nHaifeng Wang. 2015. Multi-task learning for mul-\ntiple language translation. In Proc. ACL-IJCNLP.\n\nLong Duong, Antonios Anastasopoulos, David Chiang,\nSteven Bird, and Trevor Cohn. 2016. An attentional\nmodel for speech translation without transcription.\nIn Proc. NAACL HLT.\n\nAkiko Eriguchi, Yoshimasa Tsuruoka, and Kyunghyun\nCho. 2017. Learning to parse and translate improves\nneural machine translation. In Proc. ACL.\n", "vlm_text": "\nOur model also bears similarities to the archi- tecture of the model proposed by  Tu et al.  ( 2017 ). They report signiﬁcant gains in Chinese-English translation by adding an additional  reconstruction decoder that attends on the last states of the  trans- lation  decoder, mainly inspired by auto-encoders. \n8 Conclusion \nWe presented a novel architecture for multitask learning that provides the second task with higher- level representations produced from the ﬁrst task decoder. Our model outperforms both the single- task models as well as traditional multitask ar- chitectures. Evaluating on extremely low-resource settings, our model improves on both speech tran- scription and translation. By augmenting our mod- els with regularizers that implement transitivity and invertibility, we obtain further improvements on all low-resource tasks. \nThese results will hopefully lead to new tools for endangered language documentation. Projects like BULB aim to collect about 100 hours of audio with translations, but it may be impractical to tran- scribe this much audio for many languages. For future work, we aim to extend these methods to settings where we don’t necessarily have sentence triples, but where some audio is only transcribed and some audio is only translated. \nAcknowledgements This work was generously supported by NSF Award 1464553. We are grate- ful to the anonymous reviewers for their useful comments. \nReferences \nGilles Adda, Sebastian St¨ uker, Martine Adda-Decker, Odette Ambouroue, Laurent Besacier, David Bla- chon, H´ el\\` ene Bonneau-Maynard, Pierre Godard, Fa- tima Hamlaoui, Dmitry Idiatov, et al. 2016.  Break- ing the unwritten language barrier: The BULB project .  Procedia Computer Science , 81:8–14. \nAntonios Anastasopoulos, David Chiang, and Long Duong. 2016.  An unsupervised probability model \nfor speech-to-translation alignment of low-resource languages . In  Proc. EMNLP . Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben- gio. 2015. Neural machine translation by jointly learning to align and translate . In  Proc. ICLR . Sameer Bansal, Herman Kamper, Adam Lopez, and Sharon Goldwater. 2017. Towards speech-to-text translation without speech recognition . In  Proc. EACL . Alexandre B´ erard, Laurent Besacier, Ali Can Ko- cabiyikoglu, and Olivier Pietquin. 2018. End-to- end automatic speech translation of audiobooks . arXiv:1802.04200. Alexandre B´ erard, Olivier Pietquin, Christophe Servan, and Laurent Besacier. 2016. Listen and translate: A proof of concept for end-to-end speech-to-text translation . In  Proc. NIPS Workshop on End-to-end Learning for Speech and Audio Processing . Steven Bird, Lauren Gawne, Katie Gelbart, and Isaac McAlister. 2014a.  Collecting bilingual audio in re- mote indigenous communities . In  Proc. COLING . Steven Bird, Florian R. Hanke, Oliver Adams, and Hae- joong Lee. 2014b.  Aikuma: A mobile app for col- laborative language documentation . In  Proc. of the 2014 Workshop on the Use of Computational Meth- ods in the Study of Endangered Languages . David Blachon, Elodie Gauthier, Laurent Besacier, Guy-No¨ el Kouarata, Martine Adda-Decker, and An- nie Rialland. 2016. Parallel speech collection for under-resourced language studies using the LIG- Aikuma mobile device app . In  Proc. SLTU (Spoken Language Technologies for Under-Resourced Lan- guages) , volume 81. Marcely Zanon Boito, Alexandre B´ erard, Aline Villav- icencio, and Laurent Besacier. 2017.  Unwritten lan- guages demand attention too! word discovery with encoder-decoder models . arXiv:1709.05631. Rich Caruana. 1998. Multitask learning. In  Learning to learn , pages 95–133. Springer. William Chan, Navdeep Jaitly, Quoc Le, and Oriol Vinyals. 2016. Listen, attend and spell: A neural network for large vocabulary conversational speech recognition. In  Proc. ICASSP , pages 4960–4964. IEEE. Daxiang Dong, Hua Wu, Wei He, Dianhai Yu, and Haifeng Wang. 2015.  Multi-task learning for mul- tiple language translation . In  Proc. ACL-IJCNLP . Long Duong, Antonios Anastasopoulos, David Chiang, Steven Bird, and Trevor Cohn. 2016.  An attentional model for speech translation without transcription . In  Proc. NAACL HLT . Akiko Eriguchi, Yoshimasa Tsuruoka, and Kyunghyun Cho. 2017.  Learning to parse and translate improves neural machine translation . In  Proc. ACL . "}
{"page": 9, "image_path": "doc_images/N18-1008_9.jpg", "ocr_text": ". Godard, G. Adda, M. Adda-Decker, J. Benjumea,\nL. Besacier, J. Cooper-Leavitt, G-N. Kouarata,\nL. Lamel, H. Maynard, M. Mueller, et al. 2017. A\nvery low resource language speech corpus for com-\nputational language documentation experiments.\narXiv:1710.03501.\n\nHynek Hermansky. 1990. Perceptual linear predictive\n(PLP) analysis of speech. J. Acoustical Society of\nAmerica, 87(4):1738-1752.\n\nAren Jansen, Kenneth Church, and Hynek Hermansky.\n2010. Towards spoken term discovery at scale with\nzero resources. In Proc. INTERSPEECH.\n\nSuyoun Kim, Takaaki Hori, and Shinji Watanabe. 2017.\nJoint CTC-attention based end-to-end speech recog-\nnition using multi-task learning. In Proc. ICASSP.\n\nTomer Levinboim and David Chiang. 2015. Multi-task\nword alignment triangulation for low-resource lan-\nguages. In Proc. NAACL HLT.\n\nTomer Levinboim, Ashish Vaswani, and David Chiang.\n2015. Model invertibility regularization: Sequence\nalignment with or without parallel data. In Proc.\nNAACL HLT.\n\nMinh-Thang Luong, Quoc V Le, Ilya Sutskever, Oriol\nVinyals, and Lukasz Kaiser. 2016. Multi-task se-\nquence to sequence learning. In Proc. ICLR.\n\nEvgeny Matusov, Stephan Kanthak, and Hermann Ney.\n2005. On the integration of speech recognition and\nstatistical machine translation. In Ninth European\nConference on Speech Communication and Technol-\nogy.\n\nGraham Neubig, Chris Dyer, Yoav Goldberg, Austin\nMatthews, Waleed Ammar, Antonios Anastasopou-\nlos, Miguel Ballesteros, David Chiang, Daniel\nClothiaux, Trevor Cohn, et al. 2017. DyNet: The\ndynamic neural network toolkit. arXiv:1701.03980.\n\nHermann Ney. 1999. Speech translation: Coupling of\nrecognition and translation. In Proc. ICASSP, vol-\nume |.\n\nToan Q. Nguyen and David Chiang. 2017. Transfer\nlearning across low-resource related languages for\nneural machine translation. In Proc. IJCNLP.\n\nMatt Post, Gaurav Kumar, Adam Lopez, Damianos\nKarakos, Chris Callison-Burch, and Sanjeev Khu-\ndanpur. 2013. Improved speech-to-text transla-\ntion with the Fisher and Callhome Spanish-English\nspeech translation corpus. In Proc. IWSLT.\n\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words with\nsubword units. In Proc. ACL.\n\nShubham Toshniwal, Hao Tang, Liang Lu, and Karen\nLivescu. 2017. Multitask learning with low-level\nauxiliary tasks for encoder-decoder based speech\nrecognition. In Proc. Interspeech.\n\n91\n\nZhaopeng Tu, Yang Liu, Lifeng Shang, Xiaohua Liu,\nand Hang Li. 2017. Neural machine translation with\nreconstruction. In Proc. AAAI.\n\nHaifeng Wang, Hua Wu, and Zhanyi Liu. 2006. Word\nalignment for languages with scarce resources using\nbilingual corpora of other language pairs. In Proc.\nCOLING/ACL, pages 874-881.\n\nRon J. Weiss, Jan Chorowski, Navdeep Jaitly, Yonghui\nWu, and Zhifeng Chen. 2017. Sequence-to-\nsequence models can directly transcribe foreign\nspeech. In Proc. INTERSPEECH.\n\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V.\nLe, Mohammad Norouzi, Wolfgang Macherey,\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus\nMacherey, et al. 2016. Google’s neural machine\ntranslation system: Bridging the gap between human\nand machine translation. arXiv:1609.08144.\n", "vlm_text": "P. Godard, G. Adda, M. Adda-Decker, J. Benjumea, L. Besacier, J. Cooper-Leavitt, G-N. Kouarata, L. Lamel, H. Maynard, M. Mueller, et al. 2017.  A very low resource language speech corpus for com- putational language documentation experiments . arXiv:1710.03501. Hynek Hermansky. 1990. Perceptual linear predictive (PLP) analysis of speech.  J. Acoustical Society of America , 87(4):1738–1752. Aren Jansen, Kenneth Church, and Hynek Hermansky. 2010.  Towards spoken term discovery at scale with zero resources . In  Proc. INTERSPEECH . Suyoun Kim, Takaaki Hori, and Shinji Watanabe. 2017. Joint CTC-attention based end-to-end speech recog- nition using multi-task learning . In  Proc. ICASSP . Tomer Levinboim and David Chiang. 2015.  Multi-task word alignment triangulation for low-resource lan- guages . In  Proc. NAACL HLT . Tomer Levinboim, Ashish Vaswani, and David Chiang. 2015.  Model invertibility regularization: Sequence alignment with or without parallel data . In  Proc. NAACL HLT . Minh-Thang Luong, Quoc V Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. 2016. Multi-task se- quence to sequence learning . In  Proc. ICLR . Evgeny Matusov, Stephan Kanthak, and Hermann Ney. 2005. On the integration of speech recognition and statistical machine translation. In  Ninth European Conference on Speech Communication and Technol- ogy . Graham Neubig, Chris Dyer, Yoav Goldberg, Austin Matthews, Waleed Ammar, Antonios Anastasopou- los, Miguel Ballesteros, David Chiang, Daniel Clothiaux, Trevor Cohn, et al. 2017.  DyNet: The dynamic neural network toolkit . arXiv:1701.03980. Hermann Ney. 1999. Speech translation: Coupling of recognition and translation. In  Proc. ICASSP , vol- ume 1. Toan Q. Nguyen and David Chiang. 2017. Transfer learning across low-resource related languages for neural machine translation . In  Proc. IJCNLP . Matt Post, Gaurav Kumar, Adam Lopez, Damianos Karakos, Chris Callison-Burch, and Sanjeev Khu- danpur. 2013. Improved speech-to-text transla- tion with the Fisher and Callhome Spanish-English speech translation corpus . In  Proc. IWSLT . Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016.  Neural machine translation of rare words with subword units . In  Proc. ACL . Shubham Toshniwal, Hao Tang, Liang Lu, and Karen Livescu. 2017. Multitask learning with low-level auxiliary tasks for encoder-decoder based speech recognition . In  Proc. Interspeech . \nZhaopeng Tu, Yang Liu, Lifeng Shang, Xiaohua Liu, and Hang Li. 2017.  Neural machine translation with reconstruction . In  Proc. AAAI . Haifeng Wang, Hua Wu, and Zhanyi Liu. 2006.  Word alignment for languages with scarce resources using bilingual corpora of other language pairs . In  Proc. COLING / ACL , pages 874–881. Ron J. Weiss, Jan Chorowski, Navdeep Jaitly, Yonghui Wu, and Zhifeng Chen. 2017. Sequence-to- sequence models can directly transcribe foreign speech . In  Proc. INTERSPEECH . Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. 2016. Google’s neural machine translation system: Bridging the gap between human and machine translation . arXiv:1609.08144. "}
