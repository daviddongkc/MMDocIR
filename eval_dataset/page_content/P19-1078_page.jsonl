{"page": 0, "image_path": "doc_images/P19-1078_0.jpg", "ocr_text": "Transferable Multi-Domain State Generator for Task-Oriented\nDialogue Systems\n\nChien-Sheng Wu; Andrea Madotto', Ehsan Hosseini-Asl*, Caiming Xiong’,\nRichard Socher and Pascale Fung*\niThe Hong Kong University of Science and Technology\nSalesforce Research\njason.wu@connect.ust.hk\n\nAbstract\n\nOver-dependence on domain ontology and\nlack of knowledge sharing across domains are\ntwo practical and yet less studied problems of\ndialogue state tracking. Existing approaches\ngenerally fall short in tracking unknown slot\nvalues during inference and often have diffi-\nculties in adapting to new domains. In this\npaper, we propose a TRAnsferable Dialogue\nstatE generator (TRADE) that generates di-\nalogue states from utterances using a copy\nmechanism, facilitating knowledge transfer\nwhen predicting (domain, slot, value) triplets\nnot encountered during training. Our model is\ncomposed of an utterance encoder, a slot gate,\nand a state generator, which are shared across\ndomains. Empirical results demonstrate that\nTRADE achieves state-of-the-art joint goal ac-\ncuracy of 48.62% for the five domains of Mul-\ntiWOZ, a human-human dialogue dataset. In\naddition, we show its transferring ability by\nsimulating zero-shot and few-shot dialogue\nstate tracking for unseen domains. TRADE\nachieves 60.58% joint goal accuracy in one of\nthe zero-shot domains, and is able to adapt\nto few-shot cases without forgetting already\ntrained domains.\n\n1 Introduction\n\nDialogue state tracking (DST) is a core component\nin task-oriented dialogue systems, such as restau-\nrant reservation or ticket booking. The goal of\nDST is to extract user goals/intentions expressed\nduring conversation and to encode them as a com-\npact set of dialogue states, i.e., a set of slots and\ntheir corresponding values. For example, as shown\nin Fig. 1, (slot, value) pairs such as (price, cheap)\nand (area, centre) are extracted from the conver-\nsation. Accurate DST performance is crucial for\n\n“Work partially done while the first author was an intern\nat Salesforce Research.\n\n808\n\nDialogue History\n[— Usr: Lam looking for a cheap restaurant in the centre of the city.~ =~ =~ ~~~ ,\nre is a cheap chinese restaurant called Dojo Noodle Bar.\nes please , for 8 people at 18:30 on Thursday.\n\nUsr: Lam also looking for some entertainment close to the restaurant. - = - - +\nSys: Is there any type of attraction you would like me to search?\n\nUsr: Why do not you try an architectural attraction.\n\nSys: All Saints Church looks good , would you like to head there? - - - = 1\n\n1\n: '\nUsr: L also need to book a taxi between the restaurant and the chureh. ~~~ -|\nhat time would you like the taxi from Dojo Noodle Bar?\n\nUsr: 20:30, please.\n\nMulti-Domain Dialogue State Tracking\n\nRestaurant: (price, cheap), (area, centre), (people, 8), (time, )\n(18:30), (day, Thursday), (name, Dojo Noodle Bar)\n\n{ Auraction: (type, architecture), (area, centre) jy -\n{ Taxi: (leaveAt, 20:30), (destination, All Saints Church), }\n(departure, Dojo Noodle Bar)\nHotel: }( Train.\n\nFigure 1: An example of multi-domain dialogue state\ntracking in a conversation. The solid arrows on the left\nare the single-turn mapping, and the dot arrows on the\nright are multi-turn mapping. The state tracker needs to\ntrack slot values mentioned by the user for all the slots\nin all the domains.\n\nappropriate dialogue management, where user in-\ntention determines the next system action and/or\nthe content to query from the databases.\nTraditionally, state tracking approaches are\nbased on the assumption that ontology is defined\nin advance, where all slots and their values are\nknown. Having a predefined ontology can sim-\nplify DST into a classification problem and im-\nprove performance (Henderson et al., 2014b;\nMrkSi¢ et al., 2017; Zhong et al., 2018). However,\nthere are two major drawbacks to this approach:\n1) A full ontology is hard to obtain in advance (Xu\nand Hu, 2018). In the industry, databases are usu-\nally exposed through an external API only, which\nis owned and maintained by others. It is not feasi-\nble to gain access to enumerate all the possible val-\nues for each slot. 2) Even if a full ontology exists,\nthe number of possible slot values could be large\nand variable. For example, a restaurant name or\na train departure time can contain a large number\n\nProceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 808-819\nFlorence, Italy, July 28 - August 2, 2019. ©2019 Association for Computational Linguistics\n", "vlm_text": "Transferable Multi-Domain State Generator for Task-Oriented Dialogue Systems \nChien-Sheng   $\\mathbf{W}\\mathbf{u}^{\\dagger}$  , Andrea Madotto † , Ehsan Hosseini-Asl ‡ , Caiming Xiong ‡ , Richard Socher ‡   and Pascale Fung † \n† The Hong Kong University of Science and Technology ‡ Salesforce Research jason.wu@connect.ust.hk \nAbstract \nOver-dependence on domain ontology and lack of knowledge sharing across domains are two practical and yet less studied problems of dialogue state tracking. Existing approaches generally fall short in tracking unknown slot values during inference and often have difﬁ- culties in adapting to new domains. In this paper, we propose a  TRA nsferable  D ialogue stat E  generator ( TRADE ) that generates di- alogue states from utterances using a copy mechanism, facilitating knowledge transfer when predicting  (domain, slot, value)  triplets not encountered during training. Our model is composed of an utterance encoder, a slot gate, and a state generator, which are shared across domains. Empirical results demonstrate that TRADE achieves state-of-the-art joint goal ac- curacy of  $48.62\\%$   for the ﬁve domains of Mul- tiWOZ, a human-human dialogue dataset. In addition, we show its transferring ability by simulating zero-shot and few-shot dialogue state tracking for unseen domains. TRADE achieves  $60.58\\%$   joint goal accuracy in one of the zero-shot domains, and is able to adapt to few-shot cases without forgetting already trained domains. \n1 Introduction \nDialogue state tracking (DST) is a core component in task-oriented dialogue systems, such as restau- rant reservation or ticket booking. The goal of DST is to extract user goals/intentions expressed during conversation and to encode them as a com- pact set of dialogue states, i.e., a set of slots and their corresponding values. For example, as shown in Fig.  1 ,  (slot, value)  pairs such as  (price, cheap) and  (area, centre)  are extracted from the conver- sation. Accurate DST performance is crucial for \nThe image illustrates a dialogue between a user and a system, involving multiple domains like restaurant reservations, attractions, and taxi bookings. It shows a dialogue history where the user is looking for a cheap restaurant, entertainment, and a taxi service. The system provides suggestions and confirmations for each request. Key details are captured in a Multi-Domain Dialogue State Tracking box, listing extracted information from the dialogue: \n\n- **Restaurant**: Details include price, location, number of people, time, day, and restaurant name. \n- **Attraction**: Includes type and area. \n- **Taxi**: Contains leave time, destination, and departure point.\n\nThe dialogue is aimed at booking a restaurant, suggesting an attraction, and arranging a taxi, all while maintaining context across these domains. The arrows in the image indicate links between parts of the dialogue and corresponding entries in the state tracking, illustrating how information is extracted and organized. Hotel and train domains are mentioned but not utilized in this example.\nFigure 1: An example of multi-domain dialogue state tracking in a conversation. The solid arrows on the left are the single-turn mapping, and the dot arrows on the right are multi-turn mapping. The state tracker needs to track slot values mentioned by the user for all the slots in all the domains. \nappropriate dialogue management, where user in- tention determines the next system action and/or the content to query from the databases. \nTraditionally, state tracking approaches are based on the assumption that ontology is deﬁned in advance, where all slots and their values are known. Having a predeﬁned ontology can sim- plify DST into a classiﬁcation problem and im- prove performance ( Henderson et al. ,  2014b ; Mrkˇ si´ c et al. ,  2017 ;  Zhong et al. ,  2018 ). However, there are two major drawbacks to this approach: 1) A full ontology is hard to obtain in advance ( Xu and Hu ,  2018 ). In the industry, databases are usu- ally exposed through an external API only, which is owned and maintained by others. It is not feasi- ble to gain access to enumerate all the possible val- ues for each slot. 2) Even if a full ontology exists, the number of possible slot values could be large and variable. For example, a restaurant name or a train departure time can contain a large number of possible values. Therefore, many of the previ- ous works that are based on neural classiﬁcation models may not be applicable in real scenario. "}
{"page": 1, "image_path": "doc_images/P19-1078_1.jpg", "ocr_text": "of possible values. Therefore, many of the previ-\nous works that are based on neural classification\nmodels may not be applicable in real scenario.\n\nBudzianowski et al. (2018) recently intro-\nduced a multi-domain dialogue dataset (Multi-\nWOZ), which adds new challenges in DST due\nto its mixed-domain conversations. As shown in\nFig. 1, a user can start a conversation by asking\nto reserve a restaurant, then requests information\nregarding an attraction nearby, and finally asks to\nbook a taxi. In this case, the DST model has to de-\ntermine the corresponding domain, slot and value\nat each turn of dialogue, which contains a large\nnumber of combinations in the ontology, i.e., 30\n(domain, slot) pairs and over 4,500 possible slot\nvalues in total. Another challenge in the multi-\ndomain setting comes from the need to perform\nmulti-turn mapping. Single-turn mapping refers to\nthe scenario where the (domain, slot, value) triplet\ncan be inferred from a single turn, while in multi-\nturn mapping, it should be inferred from multiple\nturns which happen in different domains. For in-\nstance, the (area, centre) pair from the attraction\ndomain in Fig. | can be predicted from the area in-\nformation in the restaurant domain, which is men-\ntioned in the preceding turns.\n\nTo tackle these challenges, we emphasize that\nDST models should share tracking knowledge\nacross domains. There are many slots among\ndifferent domains that share all or some of their\nvalues. For example, the area slot can exist in\nmany domains, e.g., restaurant, attraction, and\ntaxi. Moreover, the name slot in the restaurant do-\nmain can share the same value with the departure\nslot in the taxi domain. Additionally, to enable\nthe DST model to track slots in unseen domains,\ntransferring knowledge across multiple domains is\nimperative. We expect DST models can learn to\ntrack some slots in zero-shot domains by learning\nto track the same slots in other domains.\n\nIn this paper, we propose a transferable dialogue\nstate generator (TRADE) for multi-domain task-\noriented dialogue state tracking. The simplicity of\nour approach and the boost of the performance is\nthe main advantage of TRADE. Contributions in\nthis work are summarized as !:\n\ne To overcome the multi-turn mapping problem,\nTRADE leverages its context-enhanced slot\ngate and copy mechanism to properly track slot\n\n'The code is released at\n\njasonwu0731/trade-dst\n\ngithub.com/\n\n809\n\nvalues mentioned anywhere in dialogue history.\n\nBy sharing its parameters across domains,\nand without requiring a predefined ontology,\nTRADE can share knowledge between domains\nto track unseen slot values, achieving state-of-\nthe-art performance on multi-domain DST.\n\nTRADE enables zero-shot DST by leveraging\nthe domains it has already seen during train-\ning. If a few training samples from unseen do-\nmains are available, TRADE can adapt to new\nfew-shot domains without forgetting the previ-\nous domains.\n\n2 TRADE Model\n\nThe proposed model in Fig. 2 comprises three\ncomponents: an utterance encoder, a slot gate,\nand a state generator. Instead of predicting the\nprobability of every predefined ontology term,\nour model directly generates slot values. Simi-\nlar to Johnson et al. (2017) for multilingual neu-\nral machine translation, we share all the model\nparameters, and the state generator starts with a\ndifferent start-of-sentence token for each (domain,\nslot) pair.\n\nThe utterance encoder encodes dialogue utter-\nances into a sequence of fixed-length vectors. To\ndetermine whether any of the (domain, slot) pairs\nare mentioned, the context-enhanced slot gate is\nused with the state generator. The state gener-\nator decodes multiple output tokens for all (do-\nmain, slot) pairs independently to predict their cor-\nresponding values. The context-enhanced slot gate\npredicts whether each of the pairs is actually trig-\ngered by the dialogue via a three-way classifier.\n\nLet us define X = {(U;,Ri),...,(Ur, Rr)}\nas the set of user utterance and system re-\nsponse pairs in T turns of dialogue, and B\n{B,,...,Br} as the dialogue states for each\nturn. Each B; is a tuple (domain:D,, slot: Si,\nvalue:¥\"\"), where D = {D,,..., Dy} are the\nN different domains, and S = {$),..., Sx} are\nthe M different slots. Assume that there are J pos-\nsible (domain, slot) pairs, and yyave is the true\nword sequence for j-th (domain , slot) pair.\n\n2.1 Utterance Encoder\n\nNote that the utterance encoder can be any exist-\ning encoding model. We use bi-directional gated\nrecurrent units (GRU) (Chung et al., 2014) to\n", "vlm_text": "\nBudzianowski et al.  ( 2018 ) recently intro- duced a multi-domain dialogue dataset (Multi- WOZ), which adds new challenges in DST due to its mixed-domain conversations. As shown in Fig.  1 , a user can start a conversation by asking to reserve a restaurant, then requests information regarding an attraction nearby, and ﬁnally asks to book a taxi. In this case, the DST model has to de- termine the corresponding domain, slot and value at each turn of dialogue, which contains a large number of combinations in the ontology, i.e., 30 (domain, slot) pairs and over 4,500 possible slot values in total. Another challenge in the multi- domain setting comes from the need to perform multi-turn mapping. Single-turn mapping refers to the scenario where the  (domain, slot, value)  triplet can be inferred from a single turn, while in multi- turn mapping, it should be inferred from multiple turns which happen in different domains. For in- stance, the  (area, centre)  pair from the  attraction domain in Fig.  1  can be predicted from the  area  in- formation in the  restaurant  domain, which is men- tioned in the preceding turns. \nTo tackle these challenges, we emphasize that DST models should share tracking knowledge across domains. There are many slots among different domains that share all or some of their values. For example, the  area  slot can exist in many domains, e.g.,  restaurant ,  attraction , and taxi . Moreover, the  name  slot in the  restaurant  do- main can share the same value with the  departure slot in the  taxi  domain. Additionally, to enable the DST model to track slots in unseen domains, transferring knowledge across multiple domains is imperative. We expect DST models can learn to track some slots in zero-shot domains by learning to track the same slots in other domains. \nIn this paper, we propose a transferable dialogue state generator (TRADE) for multi-domain task- oriented dialogue state tracking. The simplicity of our approach and the boost of the performance is the main advantage of TRADE. Contributions in this work are summarized as   1 : \n•  To overcome the multi-turn mapping problem, TRADE leverages its context-enhanced slot gate and copy mechanism to properly track slot values mentioned anywhere in dialogue history.\n\n \n\n•  By sharing its parameters across domains, and without requiring a predeﬁned ontology, TRADE can share knowledge between domains to track unseen slot values, achieving state-of- the-art performance on multi-domain DST.\n\n •  TRADE enables zero-shot DST by leveraging the domains it has already seen during train- ing. If a few training samples from unseen do- mains are available, TRADE can adapt to new few-shot domains without forgetting the previ- ous domains. \n2 TRADE Model \nThe proposed model in Fig.  2  comprises three components: an utterance encoder, a slot gate, and a state generator. Instead of predicting the probability of every predeﬁned ontology term, our model directly generates slot values. Simi- lar to  Johnson et al.  ( 2017 ) for multilingual neu- ral machine translation, we share all the model parameters, and the state generator starts with a different start-of-sentence token for each  (domain, slot)  pair. \nThe utterance encoder encodes dialogue utter- ances into a sequence of ﬁxed-length vectors. To determine whether any of the  (domain, slot)  pairs are mentioned, the context-enhanced slot gate is used with the state generator. The state gener- ator decodes multiple output tokens for all  (do- main, slot)  pairs independently to predict their cor- responding values. The context-enhanced slot gate predicts whether each of the pairs is actually trig- gered by the dialogue via a three-way classiﬁer. \nLet us deﬁne    $X\\,=\\,\\{(U_{1},R_{1}),.\\,.\\,.\\,,(U_{T},R_{T})\\}$  as the set of user utterance and system re- sponse pairs in    $T$   turns of dialogue, and    $B\\;=\\;$   $\\{B_{1},.\\,.\\,.\\,,B_{T}\\}$  turn. Each  $B_{t}$   is a tuple (domain:  $D_{n}$  , slot:  $S_{m}$  , value:  $Y_{j}^{\\mathrm{value}})$  ), where    $D\\,=\\,\\{D_{1},.\\,.\\,.\\,,D_{N}\\}$   are the  $N$    erent domains, and    ${\\cal S}=\\{S_{1},.\\,.\\,.\\,,S_{M}\\}$   are the  $M$   different slots. Assume that there are    $J$   pos- sible  (domain, slot)  pairs, and    $Y_{j}^{\\mathrm{value}}$  is the true word sequence for  $j$  -th  (domain ,slot)  pair. \n2.1 Utterance Encoder \nNote that the utterance encoder can be any exist- ing encoding model. We use bi-directional gated recurrent units (GRU) ( Chung et al. ,  2014 ) to "}
{"page": 2, "image_path": "doc_images/P19-1078_2.jpg", "ocr_text": "“()\n\nState ~\n\n. PTR / Ashley \\\n\nContext Vector DONTCARE ' i Generator :\n\nC50 ' t final '\n\nJ NONE ' 1 pa A sail pill i '\n\n@i 5?\n\nx(1 = p8)| [x fo\") '\n\n; '\n\nHotel? |\n\n_ Sooees ees es ene (os Dunes nips i\n\n: (a) Utterance |_ : ;\n' Encoder : r\n\nTT . Ashley.”\n\nEx: hotel Ex: name\nsp,\n= {1,...,7\nUtterances Domains { } Slots\nvee Hotel, Train, Price, Area, Day,\nBot: Which area are you looking for the hotel? ‘Attraction. Departure me\n\nUser: There is one at east town called Ashley Hot\n\nel.\n\nRestaurant, Taxi LeaveAt, food, etc.\n\nFigure 2: The architecture of the proposed TRADE model, which includes (a) an utterance encoder, (b) a state\ngenerator, and (c) a slot gate, all of which are shared among domains. The state generator will decode J times\nindependently for all the possible (domain, slot) pairs. At the first decoding step, state generator will take the j-th\n(domain, slot) embeddings as input to generate its corresponding slot values and slot gate. The slot gate predicts\nwhether the j-th (domain, slot) pair is triggered by the dialogue.\n\nencode the dialogue history. The input to the\nutterance encoder is denoted as history X;\n(U1, Rit, -» Ur, Ri] € RtlXdeme, which is\nthe concatenation of all words in the dialogue his-\ntory. / is the number of selected dialogue turns\nand dem» indicates the embedding size. The en-\ncoded dialogue history is represented as H;\n[nye ARE] € RIlxdnaa, where drag is the\nhidden size. As mentioned in Section 1, due to the\nmulti-turn mapping problem, the model should in-\nfer the states across a sequence of turns. There-\nfore, we use the recent dialogue history of length /\nas the utterance encoder input, rather than the cur-\nrent utterance only.\n\n2.2 State Generator\n\nTo generate slot values using text from the in-\nput source, a copy mechanism is required. There\nare three common ways to perform copying, i.e.,\nindex-based copy (Vinyals et al., 2015), hard-\ngated copy (Gulcehre et al., 2016; Madotto et al.,\n2018; Wu et al., 2019) and soft-gated copy (See\net al., 2017; McCann et al., 2018). The index-\nbased mechanism is not suitable for DST task be-\ncause the exact word(s) of the true slot value are\nnot always found in the utterance. The hard-gate\ncopy mechanism usually needs additional supervi-\n\n810\n\nsion on the gating function. As such, we employ\nsoft-gated pointer-generator copying to combine a\ndistribution over the vocabulary and a distribution\nover the dialogue history into a single output dis-\ntribution.\n\nWe use a GRU as the decoder of the state gen-\nerator to predict the value for each (domain, slot)\npair, as shown in Fig. 2. The state generator de-\ncodes J pairs independently. We simply supply\nthe summed embedding of the domain and slot as\nthe first input to the decoder. At decoding step\nk for the j-th (domain, slot) pair, the generator\nGRU takes a word embedding w,, as its input\nand returns a hidden state nse. The state gener-\nator first maps the hidden state nse\ncabulary space Pres using the trainable embed-\nding E € R'V/*¢naa, where |V| is the vocabulary\nsize. At the same time, the nse is used to com-\nPees\n\ninto the vo-\n\npute the history attention over the encoded\n\ndialogue history H;:\n\nPxpetb = Softmax(E - (hdge)') € RM,\n\npases = Softmax(H; - (nse) \") e RMA,\n\nqd)\n\nThe final output distribution Piel is the weighted-\n", "vlm_text": "The image is a diagram illustrating a dialogue system architecture. It shows different components involved in processing user inputs and generating responses. \n\n- **Utterance Encoder (a)**: Encodes input utterances and produces a context vector.\n- **Context Vector**: Used to determine relevant slots and information, highlighted by the connections to the Slot Gate and State Generator.\n- **Slot Gate (c)**: Includes labels like PTR, DONTCARE, and NONE, used to manage slot assignments.\n- **State Generator (b)**: Combines encoded information to generate and predict states, such as turning user input into a domain-specific response.\n  \nThe diagram includes flow arrows and components connected to form a pipeline from input utterance to output generation, with examples provided for domains (Hotel, Train, etc.) and slots (Price, Area, etc.). The dialogue example at the bottom involves a bot and user interaction about finding a hotel.\nFigure 2: The architecture of the proposed TRADE model, which includes (a) an utterance encoder, (b) a state generator, and (c) a slot gate, all of which are shared among domains. The state generator will decode    $J$   times independently for all the possible  (domain, slot)  pairs. At the ﬁrst decoding step, state generator will take the    $j$  -th (domain, slot)  embeddings as input to generate its corresponding slot values and slot gate. The slot gate predicts whether the    $j$  -th  (domain, slot)  pair is triggered by the dialogue. \nencode the dialogue history. The input to the utterance encoder is denoted as  history    $X_{t}~=$   $[U_{t-l},R_{t-l},.\\,.\\,.\\,,U_{t},R_{t}]\\ \\in\\ \\mathbb{R}^{|X_{t}|\\times d_{e m b}}$  , which is the concatenation of all words in the dialogue his- tory.    $l$   is the number of selected dialogue turns and    $d_{e m b}$   indicates the embedding size. The en- coded dialogue history is represented as    $H_{t}~=$   $[h_{1}^{\\mathrm{enc}},\\dots,h_{|X_{t}|}^{\\mathrm{enc}}]\\,\\in\\,\\mathbb{R}^{|\\dot{X}_{t}|\\times\\bar{d}_{h d d}}$  , where    $d_{h d d}$   is the | | hidden size. As mentioned in Section  1 , due to the multi-turn mapping problem, the model should in- fer the states across a sequence of turns. There- fore, we use the recent dialogue history of length  $l$  as the utterance encoder input, rather than the cur- rent utterance only. \n2.2 State Generator \nTo generate slot values using text from the in- put source, a copy mechanism is required. There are three common ways to perform copying, i.e., index-based copy ( Vinyals et al. ,  2015 ), hard- gated copy ( Gulcehre et al. ,  2016 ;  Madotto et al. , 2018 ;  Wu et al. ,  2019 ) and soft-gated copy ( See et al. ,  2017 ;  McCann et al. ,  2018 ). The index- based mechanism is not suitable for DST task be- cause the exact word(s) of the true slot value are not always found in the utterance. The hard-gate copy mechanism usually needs additional supervi- sion on the gating function. As such, we employ soft-gated pointer-generator copying to combine a distribution over the vocabulary and a distribution over the dialogue history into a single output dis- tribution. \n\nWe use a GRU as the decoder of the state gen- erator to predict the value for each  (domain, slot) pair, as shown in Fig.  2 . The state generator de- codes    $J$   pairs independently. We simply supply the summed embedding of the domain and slot as the ﬁrst input to the decoder. At decoding step  $k$   for the    $j$  -th  (domain, slot)  pair, the generator GRU takes a word embedding    $w_{j k}$   as its input and returns a hidden state    $h_{j k}^{\\mathrm{dec}}$  . The state gener- ator ﬁrst maps the hidden state    $h_{j k}^{\\mathrm{dec}}$    into the vo- cabulary space    $P_{j k}^{\\mathrm{vocal}}$  using the trainable embed- ding  $E\\in\\mathbb{R}^{|V|\\times\\tilde{d}_{h d d}}$  , where    $|V|$   is the vocabulary size. At the same time, the    $h_{j k}^{\\mathrm{dec}}$    is used to com- pute the history attention    $P_{j k}^{\\mathrm{hfsory}}$  over the encoded dialogue history    $H_{t}$  : \n\n$$\n\\begin{array}{r}{P_{j k}^{\\mathrm{vcoab}}=\\mathrm{Softmax}(E\\cdot(h_{j k}^{\\mathrm{dec}})^{\\top})\\in\\mathbb{R}^{|V|},}\\\\ {P_{j k}^{\\mathrm{hitsor y}}=\\mathrm{Softmax}(H_{t}\\cdot(h_{j k}^{\\mathrm{dec}})^{\\top})\\in\\mathbb{R}^{|X_{t}|}.}\\end{array}\n$$\n \nThe ﬁnal output distribution    $P_{j k}^{\\mathrm{final}}$  is the weighted- sum of two distributions, "}
{"page": 3, "image_path": "doc_images/P19-1078_3.jpg", "ocr_text": "sum of two distributions,\n\nPiet = _ re x Pipe\n\n+ (at) x pipe eri,\n\ngen\n\nPk\n\nThe scalar p® ik is trainable to combine the two dis-\ntributions, which i is computed by\n\nSigmoid(W - iar wye Cjr]) € RI,\n\ngen __\nPir =\n\n(3)\n\nwhere Wj is a trainable matrix and cj, is the con-\ntext vector. Note that due to Eq (2), our model\nis able to generate words even if they are not pre-\ndefined in the vocabulary.\n\n2.3 Slot Gate\n\nUnlike single-domain DST problems, where only\na few slots that need to be tracked, e.g., four slots\nin WOZ (Wen et al., 2017), and eight slots in\nDSTC2 (Henderson et al., 2014a), there are a large\nnumber of (domain, slot) pairs in multi-domain\nDST problems. Therefore, the ability to predict\nthe domain and slot at current turn t becomes more\nchallenging.\n\nOur context-enhanced slot gate G' is a simple\nthree-way classifier that maps a context vector\ntaken from the encoder hidden states H; to a prob-\nability distribution over ptr, none, and dontcare\nclasses. For each (domain, slot) pair, if the slot\ngate predicts none or dontcare, we ignore the val-\nues generated by the decoder and fill the pair as\n“not-mentioned” or “does not care”. Otherwise,\nwe take the generated words from our state gener-\nator as its value. With a linear layer parameterized\nby W, € R3*4h2, the slot gate for the j-th (do-\nmain, slot) pair is defined as\n\nGj = Softmax(W, - (cjo)') ER’, 4)\nwhere cjg is the context vector computed in Eq (3)\nusing the first decoder hidden state.\n\n2.4 Optimization\n\nDuring training, we optimize for both the slot gate\nand the state generator. For the former, the cross-\nentropy loss L, is computed between the predicted\nslot gate G; and the true one-hot label ye\n\n>\n\n— log(G; - ( Cae ).\n\n(5)\n\ntM\n\nj=1\n\n811\n\nFor the latter, another cross-entropy loss L,, be-\ntween Pye and the true words yjaeel is used. We\ndefine L,, as\n\nI¥5|\nvalue\n\nJ\nL,= > Piel (ys )T).\n\nj=l k=\n\n— log(P. (6)\n\n1\n\nL, is the sum of losses from all the (domain, slot)\npairs and their decoding time steps. We optimize\nthe weighted-sum of these two loss functions us-\ning hyper-parameters a and (3,\n\nL=al,+ Bly. (7)\n\n3 Unseen Domain DST\n\nIn this section, we focus on the ability of TRADE\nto generalize to an unseen domain by consider-\ning zero-shot transferring and few-shot domain ex-\npanding. In the zero-shot setting, we assume we\nhave no training data in the new domain, while in\nthe few-shot case, we assume just 1% of the origi-\nnal training data in the unseen domain is available\n(around 20 to 30 dialogues). One of the motiva-\ntions to perform unseen domain DST is because\ncollecting a large-scale task-oriented dataset for\na new domain is expensive and time-consuming\n(Budzianowski et al., 2018), and there are a large\namount of domains in realistic scenarios.\n\n3.1 Zero-shot DST\n\nIdeally, based on the slots already learned, a DST\nmodel is able to directly track those slots that\nare present in a new domain. For example, if\nthe model is able to track the departure slot in\nthe train domain, then that ability may transfer to\nthe taxi domain, which uses similar slots. Note\nthat generative DST models take the dialogue con-\ntext/history X, the domain D, and the slot S' as\ninput and then generate the corresponding val-\nues YY\", Let (X, Deources Ssources Yan) be the\nset of samples seen during the training phase and\n(X, Daarget, Starget; Yuuuer) the samples which the\nmodel was not trained to track. A zero-shot DST\nmodel should be able to generate the correct values\nof Yiuee given the context X, domain Drarget, and\nslot Starget, Without using any training samples.\nThe same context X may appear in both source\nand target domains but the pairs (Drarget; Starget)\nare unseen. This setting is extremely challeng-\ning if no slot in Starget appears in Ssource, since the\nmodel has never been trained to track such a slot.\n", "vlm_text": "\n\n$$\n\\begin{array}{r l}&{P_{j k}^{\\mathrm{final}}=p_{j k}^{\\mathrm{gen}}\\times P_{j k}^{\\mathrm{vocal}}}\\\\ &{\\qquad\\qquad+\\left(1-p_{j k}^{\\mathrm{gen}}\\right)\\times P_{j k}^{\\mathrm{hfstopy}}\\in\\mathbb{R}^{|V|}.}\\end{array}\n$$\n \nThe scalar    $p_{j k}^{\\mathsf{g e n}}$    is trainable to combine the two dis- tributions, which is computed by \n\n$$\n\\begin{array}{r}{\\begin{array}{r l}&{p_{j k}^{\\mathrm{gen}}=\\mathrm{Sigmoid}\\big(W_{1}\\cdot[h_{j k}^{\\mathrm{dec}};w_{j k};c_{j k}]\\big)\\in\\mathbb{R}^{1},}\\\\ &{\\qquad c_{j k}=P_{j k}^{\\mathrm{hstoy}}\\cdot H_{t}\\in\\mathbb{R}^{d_{h d d}}}\\end{array}}\\end{array}\n$$\n \nwhere  $W_{1}$   is a trainable matrix and    $c_{j k}$   is the con- text vector. Note that due to   $\\operatorname{Eq}$   ( 2 ), our model is able to generate words even if they are not pre- deﬁned in the vocabulary. \n2.3 Slot Gate \nUnlike single-domain DST problems, where only a few slots that need to be tracked, e.g., four slots in WOZ ( Wen et al. ,  2017 ), and eight slots in DSTC2 ( Henderson et al. ,  2014a ), there are a large number of  (domain, slot)  pairs in multi-domain DST problems. Therefore, the ability to predict the domain and slot at current turn  $t$   becomes more challenging. \nOur context-enhanced slot gate    $G$   is a simple three-way classiﬁer that maps a context vector taken from the encoder hidden states  $H_{t}$   to a prob- ability distribution over  ptr ,  none , and  dontcare classes. For each  (domain, slot)  pair, if the slot gate predicts  none  or  dontcare , we ignore the val- ues generated by the decoder and ﬁll the pair as “not-mentioned” or “does not care”. Otherwise, we take the generated words from our state gener- ator as its value. With a linear layer parameterized by    $W_{g}\\,\\in\\,\\mathbb{R}^{3\\times d_{h d d}}$  , the slot gate for the    $j$  -th  (do- main, slot)  pair is deﬁned as \n\n$$\nG_{j}=\\operatorname{Softmax}(W_{g}\\cdot(c_{j0})^{\\top})\\in\\mathbb{R}^{3},\n$$\n \nwhere    $c_{j0}$   is the context vector computed in Eq ( 3 ) using the ﬁrst decoder hidden state. \n2.4 Optimization \nDuring training, we optimize for both the slot gate and the state generator. For the former, the cross- entropy loss  $L_{g}$   is computed between the predicted slot gate    $G_{j}$   and the true one-hot label    $y_{j}^{\\mathrm{gate}}$  , \n\n$$\nL_{g}=\\sum_{j=1}^{J}-\\log\\!\\big(G_{j}\\cdot(y_{j}^{\\mathrm{gate}})^{\\top}\\big).\n$$\n \nFor the latter, another cross-entropy loss    $L_{v}$   be- tween    $P_{j k}^{\\mathrm{final}}$  and the true words    $Y_{j}^{\\mathrm{label}}$  is used. We deﬁne  $\\bar{L_{v}}$   as \n\n$$\nL_{v}=\\sum_{j=1}^{J}\\sum_{k=1}^{|Y_{j}|}-\\log(P_{j k}^{\\mathrm{final}}\\cdot(y_{j k}^{\\mathrm{value}})^{\\top}).\n$$\n \n $L_{v}$   is the sum of losses from all the  (domain, slot) pairs and their decoding time steps. We optimize the weighted-sum of these two loss functions us- ing hyper-parameters    $\\alpha$   and    $\\beta$  , \n\n$$\n\\begin{array}{r}{L=\\alpha L_{g}+\\beta L_{v}.}\\end{array}\n$$\n \n3 Unseen Domain DST \nIn this section, we focus on the ability of TRADE to generalize to an unseen domain by consider- ing zero-shot transferring and few-shot domain ex- panding. In the zero-shot setting, we assume we have no training data in the new domain, while in the few-shot case, we assume just   $1\\%$   of the origi- nal training data in the unseen domain is available (around 20 to 30 dialogues). One of the motiva- tions to perform unseen domain DST is because collecting a large-scale task-oriented dataset for a new domain is expensive and time-consuming ( Budzianowski et al. ,  2018 ), and there are a large amount of domains in realistic scenarios. \n3.1 Zero-shot DST \nIdeally, based on the slots already learned, a DST model is able to directly track those slots that are present in a new domain. For example, if the model is able to track the  departure  slot in the  train  domain, then that ability may transfer to the  taxi  domain, which uses similar slots. Note that generative DST models take the dialogue con- text/history    $X$  , the domain    $D$  , and the slot    $S$   as input and then generate the corresponding val- ues    $Y^{\\mathrm{value}}$  . Let    $(X,D_{\\mathrm{source}},S_{\\mathrm{source}},Y_{\\mathrm{source}}^{\\mathrm{value}})$   be the set of samples seen during the training phase and  $(X,D_{\\mathrm{target}},S_{\\mathrm{target}},Y_{\\mathrm{target}}^{\\mathrm{value}})$   the samples which the model was not trained to track. A zero-shot DST model should be able to generate the correct values of    $Y_{\\mathrm{target}}^{\\mathrm{value}}$    given the context    $X$  , domain    $D_{\\mathrm{target}}$  , and slot    $S_{\\mathrm{target}}$  , without using any training samples. The same context    $X$   may appear in both source and target domains but the pairs   $(D_{\\mathrm{target}},S_{\\mathrm{target}})$  are unseen. This setting is extremely challeng- ing if no slot in  $S_{\\mathrm{target}}$   appears in    $S_{\\mathrm{source}}$  , since the model has never been trained to track such a slot. "}
{"page": 4, "image_path": "doc_images/P19-1078_4.jpg", "ocr_text": "3.2. Expanding DST for Few-shot Domain\n\nIn this section, we assume that only a small\nnumber of samples from the new domain\n(X, Dyarget; Stargets Varner ) are available, and the\npurpose is to evaluate the ability of our DST model\nto transfer its learned knowledge to the new do-\nmain without forgetting previously learned do-\nThere are two advantages to perform-\ning few-shot domain expansion: 1) being able to\nquickly adapt to new domains and obtain decent\nperformance with only a small amount of training\ndata; 2) not requiring retraining with all the data\nfrom previously learned domains, since the data\nmay no longer be available and retraining is often\nvery time-consuming.\n\nmains.\n\nFirstly, we consider a straightforward naive\nbaseline, ie., fine-tuning with no constraints.\nThen, we employ two specific continual learn-\ning techniques: elastic weight consolidation\n(EWC) (Kirkpatrick et al., 2017) and gradient\nepisodic memory (GEM) (Lopez-Paz et al., 2017)\nto fine-tune our model. We define Og as the\nmodel’s parameters trained in the source domain,\nand © indicates the current optimized parameters\naccording to the target domain data.\n\nEWC uses the diagonal of the Fisher informa-\ntion matrix F as a regularizer for adapting to the\ntarget domain data. This matrix is approximated\nusing samples from the source domain. The EWC\nloss is defined as\n\naN\nLewe(®) = L(O) 4 dV Fh Osi)\", (8)\n\nwhere X is a hyper-parameter. Different from\nEWC, GEM keeps a small number of samples Kr\nfrom the source domains, and, while the model\nlearns the new target domain, a constraint is ap-\nplied on the gradient to prevent the loss on the\nstored samples from increasing. The training pro-\ncess is defined as:\n\nMinimizeg L(O) (9)\nSubject to L(O, K) < L(Os,K),\nwhere L(O, kK‘) is the loss value of the A stored\nsamples. Lopez-Paz et al. (2017) show how to\nsolve the optimization problem in Eq (9) with\nquadratic programming if the loss of the stored\nsamples increases.\n\n812\n\nHotel Train Attraction | Restaurant Taxi\nprice,\n‘ype. food,\nparking, | destination, :\nprice,\nstay, | departure, destination,\narea, area,\nday, day, departure,\nSlots name, name,\npeople, | arrive by, : arrive by,\ntype time,\narea, leave at, da leave by\nstars, people conte\ninternet, peop\nname\nTrain 3381 3103 2117 3813 1654\nValid 416 484 401 438 207\nTest 394 494 395 437 195\n\nTable 1: The dataset information of MultiWOZ. In to-\ntal, there are 30 (domain, slot) pairs from the selected\nfive domains. The numbers in the last three rows indi-\ncate the number of dialogues for train, validation and\ntest sets.\n\n4 Experiments\n\n4.1 Dataset\n\nMulti-domain Wizard-of-Oz (Budzianowski et al.,\n2018) (MultiWOZ) is the largest existing human-\nhuman conversational corpus spanning over\nseven domains, containing 8438 multi-turn dia-\nlogues, with each dialogue averaging 13.68 turns.\nDifferent from existing standard datasets like\nWOZ (Wen et al., 2017) and DSTC2 (Henderson\net al., 2014a), which contain less than 10 slots and\nonly a few hundred values, MultiWOZ has 30 (do-\nmain, slot) pairs and over 4,500 possible values.\nWe use the DST labels from the original training,\nvalidation and testing dataset. Only five domains\n(restaurant, hotel, attraction, taxi, train) are used\nin our experiment because the other two domains\n(hospital, police) have very few dialogues (10%\ncompared to others) and only appear in the train-\ning set. The slots in each domain and the corre-\nsponding data size are reported in Table 1.\n\n4.2. Training Details\n\nMulti-domain Joint Training The model is\ntrained end-to-end using the Adam optimizer\n(Kingma and Ba, 2015) with a batch size of 32.\nThe learning rate annealing is in the range of\n0.001, 0.0001} with a dropout ratio of 0.2. Both\na and 6 in Eq (7) are set to one. All the em-\nbeddings are initialized by concatenating Glove\nembeddings (Pennington et al., 2014) and charac-\ner embeddings (Hashimoto et al., 2016), where\nhe dimension is 400 for each vocabulary word.\nA greedy search decoding strategy is used for\nour state generator since the generated slot val-\nues are usually short in length. In addition, to in-\n\n", "vlm_text": "3.2 Expanding DST for Few-shot Domain \nIn this section, we assume that only a small number of samples from the new domain  $(X,D_{\\mathrm{target}},S_{\\mathrm{target}},Y_{\\mathrm{target}}^{\\mathrm{value}})$   are available, and the purpose is to evaluate the ability of our DST model to transfer its learned knowledge to the new do- main without forgetting previously learned do- mains. There are two advantages to perform- ing few-shot domain expansion: 1) being able to quickly adapt to new domains and obtain decent performance with only a small amount of training data; 2) not requiring retraining with all the data from previously learned domains, since the data may no longer be available and retraining is often very time-consuming. \nFirstly, we consider a straightforward naive baseline, i.e., ﬁne-tuning with no constraints. Then, we employ two speciﬁc continual learn- ing techniques: elastic weight consolidation (EWC) ( Kirkpatrick et al. ,  2017 ) and gradient episodic memory (GEM) ( Lopez-Paz et al. ,  2017 ) to ﬁne-tune our model. We deﬁne    $\\Theta_{S}$   as the model’s parameters trained in the source domain, and    $\\Theta$   indicates the current optimized parameters according to the target domain data. \nEWC uses the diagonal of the Fisher informa- tion matrix    $F$   as a regularizer for adapting to the target domain data. This matrix is approximated using samples from the source domain. The EWC loss is deﬁned as \n\n$$\nL_{e w c}(\\Theta)=L(\\Theta)+\\sum_{i}\\frac{\\lambda}{2}F_{i}(\\Theta_{i}-\\Theta_{S,i})^{2},\n$$\n \nwhere    $\\lambda$   is a hyper-parameter. Different from EWC, GEM keeps a small number of samples    $K$  from the source domains, and, while the model learns the new target domain, a constraint is ap- plied on the gradient to prevent the loss on the stored samples from increasing. The training pro- cess is deﬁned as: \n\n$$\n\\begin{array}{r l}&{\\mathrm{Minimize}_{\\Theta}\\,L(\\Theta)}\\\\ &{\\mathrm{Subject\\,to}\\,\\,L(\\Theta,K)\\leq L(\\Theta_{S},K),}\\end{array}\n$$\n \nwhere    $L(\\Theta,K)$   is the loss value of the    $K$   stored samples. Lopez-Paz et al.  ( 2017 ) show how to solve the optimization problem in Eq ( 9 ) with quadratic programming if the loss of the stored samples increases. \nThe table presents a breakdown of different \"Slots\" and data distribution for five categories: Hotel, Train, Attraction, Restaurant, and Taxi. Each category lists relevant slots and the number of instances available in \"Train,\" \"Valid,\" and \"Test\" datasets.\n\nFor each category:\n- **Hotel:** Slots include price, type, parking, stay, day, people, area, stars, internet, name. Instances: Train (3381), Valid (416), Test (394).\n- **Train:** Slots include destination, departure, day, arrive by, leave at, people. Instances: Train (3103), Valid (484), Test (494).\n- **Attraction:** Slots include area, name, type. Instances: Train (2717), Valid (401), Test (395).\n- **Restaurant:** Slots include food, price, area, name, time, day, people. Instances: Train (3813), Valid (438), Test (437).\n- **Taxi:** Slots include destination, departure, arrive by, leave by. Instances: Train (1654), Valid (207), Test (195).\n\nThe table does not have any caption text and provides a structured overview of dataset partitioning and slot information relevant to each category.\nTable 1: The dataset information of MultiWOZ. In to- tal, there are 30  (domain, slot)  pairs from the selected ﬁve domains. The numbers in the last three rows indi- cate the number of dialogues for train, validation and test sets. \n4 Experiments \n4.1 Dataset \nMulti-domain Wizard-of-Oz ( Budzianowski et al. , 2018 ) (MultiWOZ) is the largest existing human- human conversational corpus spanning over seven domains, containing 8438 multi-turn dia- logues, with each dialogue averaging 13.68 turns. Different from existing standard datasets like WOZ ( Wen et al. ,  2017 ) and DSTC2 ( Henderson et al. ,  2014a ), which contain less than 10 slots and only a few hundred values, MultiWOZ has 30  (do- main, slot)  pairs and over 4,500 possible values. We use the DST labels from the original training, validation and testing dataset. Only ﬁve domains ( restaurant ,  hotel ,  attraction ,  taxi ,  train ) are used in our experiment because the other two domains ( hospital ,  police ) have very few dialogues (  $10\\%$  compared to others) and only appear in the train- ing set. The slots in each domain and the corre- sponding data size are reported in Table  1 . \n4.2 Training Details \nMulti-domain Joint Training The model is trained end-to-end using the Adam optimizer ( Kingma and Ba ,  2015 ) with a batch size of 32. The learning rate annealing is in the range of\n\n [0 . 001 ,  0 . 0001]  with a dropout ratio of 0.2. Both\n\n  $\\alpha$   and    $\\beta$   in Eq ( 7 ) are set to one. All the em- beddings are initialized by concatenating Glove embeddings ( Pennington et al. ,  2014 ) and charac- ter embeddings ( Hashimoto et al. ,  2016 ), where the dimension is 400 for each vocabulary word. A greedy search decoding strategy is used for our state generator since the generated slot val- ues are usually short in length. In addition, to in- crease model generalization and simulate an out- of-vocabulary setting, a word dropout is utilized with the utterance encoder by randomly masking a small amount of input tokens, similar to  Bow- man et al.  ( 2016 ). "}
{"page": 5, "image_path": "doc_images/P19-1078_5.jpg", "ocr_text": "crease model generalization and simulate an out-\nof-vocabulary setting, a word dropout is utilized\nwith the utterance encoder by randomly masking\na small amount of input tokens, similar to Bow-\nman et al. (2016).\n\nDomain Expanding — For training, we follow the\nsame procedure as in the joint training section, and\nwe run a small grid search for all the methods us-\ning the validation set. For EWC, we set different\nvalues of \\ for all the domains, and the optimal\nvalue is selected using the validation set. Finally,\nin GEM, we set the memory sizes K to 1% of the\nsource domains.\n\n4.3 Results\n\nTwo evaluation metrics, joint goal accuracy and\nslot accuracy, are used to evaluate the perfor-\nmance on multi-domain DST. The joint goal accu-\nracy compares the predicted dialogue states to the\nground truth B; at each dialogue turn t, and the\noutput is considered correct if and only if all the\npredicted values exactly match the ground truth\nvalues in B;. The slot accuracy, on the other hand,\nindividually compares each (domain, slot, value)\ntriplet to its ground truth label.\n\nMulti-domain Training We make a comparison\nwith the following existing models: MDBT (Ra-\nmadan et al., 2018), GLAD (Zhong et al.,\n2018), GCE (Nouri and Hosseini-Asl, 2018), and\nSpanPtr (Xu and Hu, 2018), and we briefly de-\nscribe these baselines models below:\n\nMDBT 7”: Multiple bi-LSTMs are used to en-\ncode system and user utterances. The seman-\ntic similarity between utterances and every pre-\ndefined ontology term is computed separately.\nEach ontology term is triggered if the predicted\nscore is greater than a threshold.\n\nGLAD *: This model uses self-attentive RNNs\nto learn a global tracker that shares parameters\namong slots and a local tracker that tracks each\nslot. The model takes previous system actions\nand the current user utterance as input, and com-\nputes semantic similarity with predefined ontol-\nogy terms.\n\nGCE: This is the current state-of-the-art model\non the single-domain WOZ dataset (Wen et al.,\n2github.com/osmanio2/\n\nmulti-domain-belief-tracking\n3github.com/salesforce/glad\n\n813\n\n. MultiWOZ\n\nMultiwOZ (Only Restaurant)\nJoint Slot | Joint Slot\nMDBT | 15.57 89.53 | 17.98 54.99\nGLAD | 35.57 95.44 | 53.23 96.54\nGCE | 36.27 98.42 | 60.93 95.85\nSpanPtr | 30.28 93.85 | 49.12 87.89\nTRADE | 48.62 96.92 | 65.35 93.28\n\nTable 2: The multi-domain DST evaluation on Mul-\ntiWOZ and its single restaurant domain. TRADE\nhas the highest joint accuracy, which surpasses current\nstate-of-the-art GCE model.\n\n2017). It is a simplified and speed up version of\nGLAD without slot-specific RNNs.\n\ne SpanPtr: Most related to our work, this is\nthe first model that applies pointer networks\n(Vinyals et al., 2015) to the single-domain DST\nproblem, which generates both start and end\npointers to perform index-based copying.\n\nTo have a fair comparison, we modify the orig-\ninal implementation of the MDBT and GLAD\nmodels by: 1) adding name, destination, and de-\nparture slots for evaluation if they were discarded\nor replaced by placeholders; and 2) removing the\nhand-crafted rules of tracking the booking slots\nsuch as stay and people slots if there are any; and\n3) creating a full ontology for their model to cover\nall (domain, slot, value) pairs that were not in the\noriginal ontology generated by the data provider.\n\nAs shown in Table 2, TRADE achieves the\nhighest performance, 48.62% on joint goal accu-\nracy and 96.92% on slot accuracy, on MultiWOZ.\nFor comparison with the performance on single-\ndomain, the results on the restaurant domain of\nMultiWOZ are reported as well. The performance\ndifference between SpanPtr and our model mainly\ncomes from the limitation of index-based copying.\nFor examples, if the true label for the price range\nslot is cheap, the relevant user utterance describ-\ning the restaurant may actually be, for example,\neconomical, inexpensive, or cheaply. Note that the\nMDBT, GLAD, and GCE models each need a pre-\ndefined domain ontology to perform binary clas-\nsification for each ontology term, which hinders\ntheir DST tracking performance, as mentioned in\nSection 1.\n\nWe visualize the cosine similarity matrix for all\npossible slot embeddings in Fig. 3. Most of the\n", "vlm_text": "\nDomain Expanding For training, we follow the same procedure as in the joint training section, and we run a small grid search for all the methods us- ing the validation set. For EWC, we set different values of    $\\lambda$   for all the domains, and the optimal value is selected using the validation set. Finally, in GEM, we set the memory sizes    $K$   to  $1\\%$   of the source domains. \n4.3 Results \nTwo evaluation metrics, joint goal accuracy and slot accuracy, are used to evaluate the perfor- mance on multi-domain DST. The joint goal accu- racy compares the predicted dialogue states to the ground truth    $B_{t}$   at each dialogue turn    $t$  , and the output is considered correct if and only if all the predicted values exactly match the ground truth values in    $B_{t}$  . The slot accuracy, on the other hand, individually compares each (domain, slot, value) triplet to its ground truth label. \nMulti-domain Training We make a comparison with the following existing models: MDBT ( Ra- madan et al. ,  2018 ), GLAD ( Zhong et al. , 2018 ), GCE ( Nouri and Hosseini-Asl ,  2018 ), and SpanPtr ( Xu and Hu ,  2018 ), and we brieﬂy de- scribe these baselines models below:\n\n \n•  MDBT   2 : Multiple bi-LSTMs are used to en- code system and user utterances. The seman- tic similarity between utterances and every pre- deﬁned ontology term is computed separately. Each ontology term is triggered if the predicted score is greater than a threshold.\n\n \n•  GLAD   3 : This model uses self-attentive RNNs to learn a global tracker that shares parameters among slots and a local tracker that tracks each slot. The model takes previous system actions and the current user utterance as input, and com- putes semantic similarity with predeﬁned ontol- ogy terms.\n\n \n•  GCE: This is the current state-of-the-art model on the single-domain WOZ dataset ( Wen et al. , \nThe table presents the performance of different models on the MultiWOZ dataset, both for the full dataset and for a subset focusing only on restaurant-related dialogues. The columns represent the evaluation metrics (\"Joint\" and \"Slot\") for each model on two variations of the MultiWOZ dataset. Here is the breakdown of the table:\n\n**Columns:**\n1. **MultiWOZ (Full Dataset)**\n   - **Joint:** Indicates the joint performance score of each model on the entire MultiWOZ dataset.\n   - **Slot:** Indicates the slot accuracy score of each model on the full dataset.\n\n2. **MultiWOZ (Only Restaurant)**\n   - **Joint:** Indicates the joint performance score of each model specifically on the restaurant-related subset of the MultiWOZ dataset.\n   - **Slot:** Indicates the slot accuracy score of each model specifically on the restaurant subset.\n\n**Rows (Models):**\n1. **MDBT**\n   - Joint: 15.57 (full dataset), 17.98 (restaurant)\n   - Slot: 89.53 (full dataset), 54.99 (restaurant)\n\n2. **GLAD**\n   - Joint: 35.57 (full dataset), 53.23 (restaurant)\n   - Slot: 95.44 (full dataset), 96.54 (restaurant)\n\n3. **GCE**\n   - Joint: 36.27 (full dataset), 60.93 (restaurant)\n   - Slot: 98.42 (full dataset), 95.85 (restaurant)\n\n4. **SpanPtr**\n   - Joint: 30.28 (full dataset), 49.12 (restaurant)\n   - Slot: 93.85 (full dataset), 87.89 (restaurant)\n\n5. **TRADE**\n   - Joint: 48.62 (full dataset), 65.35 (restaurant)\n   - Slot: 96.92 (full dataset), 93.28 (restaurant)\n\nThe TRADE model achieves the highest joint scores for both the full MultiWOZ dataset and the restaurant subset, indicating its superior performance in understanding and predicting dialogue states across these tasks.\nTable 2: The multi-domain DST evaluation on Mul- tiWOZ and its single  restaurant  domain. TRADE has the highest joint accuracy, which surpasses current state-of-the-art GCE model. \n2017 ). It is a simpliﬁed and speed up version of GLAD without slot-speciﬁc RNNs. \n•  SpanPtr: Most related to our work, this is the ﬁrst model that applies pointer networks ( Vinyals et al. ,  2015 ) to the single-domain DST problem, which generates both start and end pointers to perform index-based copying. \nTo have a fair comparison, we modify the orig- inal implementation of the MDBT and GLAD models by: 1) adding  name ,  destination , and  de- parture  slots for evaluation if they were discarded or replaced by placeholders; and 2) removing the hand-crafted rules of tracking the booking slots such as  stay  and  people  slots if there are any; and 3) creating a full ontology for their model to cover all  (domain, slot, value)  pairs that were not in the original ontology generated by the data provider. \nAs shown in Table  2 , TRADE achieves the highest performance,   $48.62\\%$   on joint goal accu- racy and   $96.92\\%$   on slot accuracy, on MultiWOZ. For comparison with the performance on single- domain, the results on the  restaurant  domain of MultiWOZ are reported as well. The performance difference between SpanPtr and our model mainly comes from the limitation of index-based copying. For examples, if the true label for the price range slot is  cheap , the relevant user utterance describ- ing the restaurant may actually be, for example, economical ,  inexpensive , or  cheaply . Note that the MDBT, GLAD, and GCE models each need a pre- deﬁned domain ontology to perform binary clas- siﬁcation for each ontology term, which hinders their DST tracking performance, as mentioned in Section  1 . \nWe visualize the cosine similarity matrix for all possible slot embeddings in Fig.  3 . Most of the "}
{"page": 6, "image_path": "doc_images/P19-1078_6.jpg", "ocr_text": "Joint Slot | Joint Slot | Joint Slot Joint Slot Joint Slot\n\nEvaluation on 4 Domains | Except Hotel | Except Train | Except Attraction | Except Restaurant | Except Taxi\nBase Model (BM) 58.98 96.75 | 55.26 96.76 | 55.02 97.03 | 54.69 96.64 | 49.87 96.77\n\ntraining on 4 domains\nFine-tuning BM Naive | 36.08 93.48 | 23.25 90.32 | 40.05 95.54 32.85 91.69 46.10 96.34\non 1% new domain EWC | 40.82 94.16 | 28.02 91.49 | 45.37 84.94 34.45 92.53 46.88 96.44\nGEM | 53.54 96.27 | 50.69 96.42 | 50.51 96.66 45.91 95.58 46.43 96.45\nEvaluation on New Domain Hotel Train Attraction Restaurant Taxi\n\nTraining 1% New Domain 19.53 77.33 | 44.24 85.66 | 35.88 68.60 32.72 82.39 60.38 72.82\nFine-tuning BM Naive | 19.13 75.22 | 59.83 90.63 | 29.39 60.73 42.42 86.82 63.81 79.81\non 1% new domain EWC | 19.35 76.25 | 58.10 90.33 | 32.28 62.43 40.93 85.80 63.61 79.65\nGEM | 19.73 77.92 | 54.31 89.55 | 34.73 64.37 39.24 86.05 63.16 79.27\n\nTable 3: We run domain expansion experiments by excluding one domain and fine-tuning on that domain. The\nfirst row is the base model trained on the four domains. The second row is the results on the four domains after\nfine-tuning on 1% new domain data using three different strategies. One can find out that GEM outperforms Naive\nand EWC fine-tuning in terms of catastrophic forgetting on the four domains. Then, we evaluate the results on new\ndomain for two cases: training from scratch and fine-tuning from the base model. Results show that fine-tuning\nfrom the base model usually achieves better results on the new domain compared to training from scratch.\n\nFigure 3: Embeddings cosine similarity visualization.\nThe rows and columns are all the possible slots in Mul-\ntiWOZ. Slots that share similar values or have corre-\nlated values learn similar embeddings. For example\ndestination vs. departure (which share similar values)\nor price range vs. stars exhibit high correlation.\n\nslot embeddings are not close to each other, which\nis expected because the model only depends on\nthese features as start-of-sentence embeddings to\ndistinguish different slots. Note that some slots\nare relatively close because either the values they\ntrack may share similar semantic meanings or the\nslots are correlated. For example, destination\nand departure track names of cities, while people\nand stay track numbers. On the other hand, price\nrange and star in hotel domain are correlated be-\ncause high-star hotels are usually expensive.\n\nZero-shot We run zero-shot experiments by ex-\ncluding one domain from the training set. As\n\n814\n\nTrained Single | Zero-Shot\n\nJoint Slot Joint — Slot\nHotel | 55.52 92.66 | 13.70 65.32\nTrain | 77.71 95.30 | 22.37 49.31\nAttraction | 71.64 88.97 | 19.87 55.53\nRestaurant | 65.35 93.28 | 11.52 53.43\nTaxi | 76.13 89.53 | 60.58 73.92\n\nTable 4: Zero-shot experiments on an unseen domain.\nIn taxi domain, our model achieves 60.58% joint goal\naccuracy without training on any samples from taxi do-\nmain. Trained Single column is the results achieved by\ntraining on 100% single-domain data as a reference.\n\nshown in Table 4, the taxi domain achieves the\nhighest zero-shot performance, 60.58% on joint\ngoal accuracy, which is close to the result achieved\nby training on all the taxi domain data (76.13%).\nAlthough performances on the other zero-shot\ndomains are not especially promising, they still\nachieve around 50 to 65% slot accuracy without\nusing any in-domain samples. The reason why the\nzero-shot performance on the taxi domain is high\nis because all four slots share similar values with\nthe corresponding slots in the train domain.\n\nDomain Expanding _ In this setting, the TRADE\nmodel is pre-trained on four domains and a held-\nout domain is reserved for domain expansion to\nperform fine-tuning. After fine-tuning on the new\ndomain, we evaluate the performance of TRADE\non 1) the four pre-trained domains and 2) the new\ndomain. We experiment with different fine-tuning\nstrategies. The base model row in Table 3 indi-\ncates the results evaluated on the four domains us-\n", "vlm_text": "The table presents results from an evaluation of a Base Model (BM) trained on four domains and its fine-tuning on a new domain. The evaluations are separated into two main parts:\n\n1. **Evaluation on 4 Domains**: \n   - This section compares different methods in scenarios excluding one specific domain each time: Hotel, Train, Attraction, Restaurant, and Taxi.\n   - The \"Joint\" and \"Slot\" columns show the performance scores for each approach.\n     - \"Joint\" likely refers to a combined metric while \"Slot\" targets individual slot accuracy.\n   - Methods compared include: \n     - Base Model (BM) with results based on the four-domain training.\n     - Fine-tuning BM on 1% of new domain data with sub-strategies: Naive, EWC (Elastic Weight Consolidation), and GEM (Gradient Episodic Memory).\n   - The highest values are usually bolded, indicating the top-performing method for each comparison.\n\n2. **Evaluation on New Domain**:\n   - This part assesses performance when training with only 1% of new domain data.\n   - Again, it compares the \"Joint\" and \"Slot\" metrics for different domains: Hotel, Train, Attraction, Restaurant, and Taxi.\n   - Strategies compared include training only 1% of the new domain and fine-tuning BM on 1% of the new domain data using Naive, EWC, and GEM methods.\n   - The highest values are noted in bold for clarity.\n\nThe table highlights how well the different approaches retain performance across the existing and new domains, focusing on domain adaptation and continual learning techniques.\nTable 3: We run domain expansion experiments by excluding one domain and ﬁne-tuning on that domain. The ﬁrst row is the base model trained on the four domains. The second row is the results on the four domains after ﬁne-tuning on  $1\\%$   new domain data using three different strategies. One can ﬁnd out that GEM outperforms Naive and EWC ﬁne-tuning in terms of catastrophic forgetting on the four domains. Then, we evaluate the results on new domain for two cases: training from scratch and ﬁne-tuning from the base model. Results show that ﬁne-tuning from the base model usually achieves better results on the new domain compared to training from scratch. \nThe image is a heatmap visualizing the cosine similarity between embeddings for different slots in the MultiWOZ dataset. The slots, such as \"parking,\" \"internet,\" \"food,\" etc., are listed along both the rows and columns. Each cell in the heatmap represents the cosine similarity between pairs of slot embeddings. A darker color indicates a higher similarity, suggesting that the slots have learned similar embeddings likely due to sharing similar or correlated values. For instance, \"destination\" and \"departure\" or \"price range\" and \"stars\" show high correlation, suggesting that these pairs typically have related or shared attributes in the context of the dataset.\nslot embeddings are not close to each other, which is expected because the model only depends on these features as start-of-sentence embeddings to distinguish different slots. Note that some slots are relatively close because either the values they track may share similar semantic meanings or the slots are correlated. For example,  destination and  departure  track names of cities, while  people and  stay  track numbers. On the other hand,  price range  and  star  in hotel domain are correlated be- cause high-star hotels are usually expensive. \nZero-shot We run zero-shot experiments by ex- cluding one domain from the training set. As \nThe table presents data comparing \"Trained Single\" and \"Zero-Shot\" performance across five categories: Hotel, Train, Attraction, Restaurant, and Taxi. Each category is evaluated in two metrics: \"Joint\" and \"Slot.\"\n\n1. **Hotel:**\n   - Trained Single: Joint (55.52), Slot (92.66)\n   - Zero-Shot: Joint (13.70), Slot (65.32)\n\n2. **Train:**\n   - Trained Single: Joint (77.71), Slot (95.30)\n   - Zero-Shot: Joint (22.37), Slot (49.31)\n\n3. **Attraction:**\n   - Trained Single: Joint (71.64), Slot (88.97)\n   - Zero-Shot: Joint (19.87), Slot (55.53)\n\n4. **Restaurant:**\n   - Trained Single: Joint (65.35), Slot (93.28)\n   - Zero-Shot: Joint (11.52), Slot (53.43)\n\n5. **Taxi:**\n   - Trained Single: Joint (76.13), Slot (89.53)\n   - Zero-Shot: Joint (60.58), Slot (73.92)\n\nThe \"Trained Single\" method consistently performs better than the \"Zero-Shot\" approach in both Joint and Slot metrics for all categories. The bold value (60.58 in Taxi Zero-Shot Joint) likely highlights a specific point of interest or significance within the table, perhaps indicating the best performance in that column or an unexpected result.\nshown in Table  4 , the  taxi  domain achieves the highest zero-shot performance,   $60.58\\%$   on joint goal accuracy, which is close to the result achieved by training on all the  taxi  domain data   $(76.13\\%)$  . Although performances on the other zero-shot domains are not especially promising, they still achieve around 50 to   $65\\%$   slot accuracy without using any in-domain samples. The reason why the zero-shot performance on the  taxi  domain is high is because all four slots share similar values with the corresponding slots in the  train  domain. \nDomain Expanding In this setting, the TRADE model is pre-trained on four domains and a  held- out  domain is reserved for domain expansion to perform ﬁne-tuning. After ﬁne-tuning on the new domain, we evaluate the performance of TRADE on 1) the four pre-trained domains and 2) the new domain. We experiment with different ﬁne-tuning strategies. The  base model  row in Table  3  indi- cates the results evaluated on the four domains us- ing their in-domain training data, and the  Train- ing   $I\\%$   New Domain  row indicates the results achieved by training from scratch using   $1\\%$   of the new domain data. In general, GEM outper- forms naive and EWC ﬁne-tuning in terms of over- coming catastrophic forgetting. We also ﬁnd that pre-training followed by ﬁne-tuning outperforms training from scratch on the single domain. "}
{"page": 7, "image_path": "doc_images/P19-1078_7.jpg", "ocr_text": "ing their in-domain training data, and the Train-\ning 1% New Domain row indicates the results\nachieved by training from scratch using 1% of\nthe new domain data. In general, GEM outper-\nforms naive and EWC fine-tuning in terms of over-\ncoming catastrophic forgetting. We also find that\npre-training followed by fine-tuning outperforms\ntraining from scratch on the single domain.\n\nFine-tuning TRADE with GEM maintains\nhigher performance on the original four domains.\nTake the hotel domain as an example, the per-\nformance on the four domains after fine-tuning\nwith GEM only drops from 58.98% to 53.54%\n(-5.44%) on joint accuracy, whereas naive fine-\ntuning deteriorates the tracking ability, dropping\njoint goal accuracy to 36.08% (-22.9%).\n\nExpanding TRADE from four domains to a new\ndomain achieves better performance than training\nfrom scratch on the new domain. This observa-\ntion underscores the advantages of transfer learn-\ning with the proposed TRADE model. For ex-\nample, our TRADE model achieves 59.83% joint\naccuracy after fine-tuning using only 1% of Train\ndomain data, outperforming the training Train do-\nmain from scratch, which achieves 44.24% using\nthe same amount of new-domain data.\n\nFinally, when considering hotel and attraction\nas new domain, fine-tuning with GEM outper-\nforms the naive fine-tuning approach on the new\ndomain. To elaborate, GEM obtains 34.73% joint\naccuracy on the attraction domain, but naive fine-\ntuning on that domain can only achieve 29.39%.\nThis implies that in some cases learning to keep\nthe tracking ability (learned parameters) of the\nlearned domains helps to achieve better perfor-\nmance for the new domain.\n\n5 Error Analysis\n\nAn error analysis of multi-domain training is\nshown in Fig. 4. Not surprisingly, name slots in\nthe restaurant, attraction, and hotel domains have\nthe highest error rates, 8.50%, 8.17%, and 7.86%,\nrespectively. It is because this slot usually has a\nlarge number of possible values that is hard to rec-\nognize. On the other hand, number-related slots\nsuch as arrive_by, people, and stay usually have\nthe lowest error rates. We also find that the type\nslot of hotel domain has a high error rate, even if\nit is an easy task with only two possible values in\nthe ontology. The reason is that labels of the (ho-\ntel, type) pair are sometimes missing in the dataset,\n\n815\n\nSlot Error Rate\n\nrestaurant-name\n\nhotel-type\ntaxi-departure\ntaxi-destination\n\ntrain-leaveat\n-type\nange\nange\nKing\n\nx ai\n\nhotel-interi\n\nrea\nnet\nhotel-area\ntrain-book people\ntrain-day\n\ntrain-arriveby\ntrain-departure\nrestaurant-food\n\nrestaurant-book day\nhotel-stars\n\nhotel-book people\ntaxi-leaveat\n\nhotel-book day\nhotel-book stay\nrestaurant-book people\n\naxi-arriveby\n\n0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08\n\nFigure 4: Slots error rate on test set of multi-domain\ntraining. The name slot in restaurant domain has the\nhighest error rate, 8.50%, and the arrive_by slot in taxi\ndomain has the lowest error rate, 1.33%\n\nparking book tie:\noe book people\n\nbook people bok day\npricerange\n\npricerange\n‘ype\n\nbook day\n\nfeos\n\nbook stay\n\n(a) Hotel\n\n(b) Restaurant\n\nFigure 5: Zero-shot DST error analysis on (a) hotel and\n(b) restaurant domains. The x-axis represents the num-\nber of each slot which has correct non-empty values.\nIn hotel domain, the knowledge to track people, area,\nprice_range, and day slots are successfully transferred\nfrom other domains seen in training.\n\nwhich makes our prediction incorrect even if it is\nsupposed to be predicted.\n\nIn Fig. 5, the zero-shot analysis of two se-\nlected domains, hotel and restaurant, which con-\ntain more slots to be tracked, are shown. To bet-\nter understand the behavior of knowledge trans-\nferring, here we only consider labels that are not\nempty, i.e., we ignore data that is labeled as “none”\nbecause predicting “none” is relatively easier for\nthe model. In both hotel and restaurant domains,\nknowledge about people, area, price_range, and\nday slots are successfully transferred from the\nother four domains. For unseen slots that only ap-\npear in one domain, it is very hard for our model\nto track correctly. For example, parking, stars and\ninternet slots are only appeared in hotel domain,\nand the food slot is unique to the restaurant do-\nmain.\n", "vlm_text": "\nFine-tuning TRADE with GEM maintains higher performance on the original four domains. Take the  hotel  domain as an example, the per- formance on the four domains after ﬁne-tuning with GEM only drops from   $58.98\\%$   to   $53.54\\%$   $(-5.44\\%)$   on joint accuracy, whereas naive ﬁne- tuning deteriorates the tracking ability, dropping joint goal accuracy to   $36.08\\%$     $(-22.9\\%)$  . \nExpanding TRADE from four domains to a new domain achieves better performance than training from scratch on the new domain. This observa- tion underscores the advantages of transfer learn- ing with the proposed TRADE model. For ex- ample, our TRADE model achieves   $59.83\\%$   joint accuracy after ﬁne-tuning using only   $1\\%$   of  Train domain data, outperforming the training  Train  do- main from scratch, which achieves   $44.24\\%$   using the same amount of new-domain data. \nFinally, when considering  hotel  and  attraction as new domain, ﬁne-tuning with GEM outper- forms the naive ﬁne-tuning approach on the new domain. To elaborate, GEM obtains  $34.73\\%$   joint accuracy on the  attraction  domain, but naive ﬁne- tuning on that domain can only achieve   $29.39\\%$  . This implies that in some cases learning to keep the tracking ability (learned parameters) of the learned domains helps to achieve better perfor- mance for the new domain. \n5 Error Analysis \nAn error analysis of multi-domain training is shown in Fig.  4 . Not surprisingly,  name  slots in the  restaurant ,  attraction , and  hotel  domains have the highest error rates,   $8.50\\%$  ,  $8.17\\%$  , and   $7.86\\%$  , respectively. It is because this slot usually has a large number of possible values that is hard to rec- ognize. On the other hand, number-related slots such as  arrive by ,  people , and  stay  usually have the lowest error rates. We also ﬁnd that the  type slot of  hotel  domain has a high error rate, even if it is an easy task with only two possible values in the ontology. The reason is that labels of the  (ho- tel, type)  pair are sometimes missing in the dataset, \nThe image is a bar chart showing the error rates of different slots on a test set from multi-domain training, specifically in areas like restaurant, attraction, hotel, taxi, and train domains. The slots are listed vertically, and each has a corresponding horizontal bar indicating its error rate. The slot for \"restaurant-name\" has the highest error rate at 8.50%, while the \"taxi-arriveby\" slot records the lowest error rate at 1.33%. Other slots like \"attraction-name,\" \"hotel-name,\" and \"hotel-type\" also have relatively high error rates compared to the rest.\nThe image contains two bar charts, labeled as Figure 5, showing zero-shot dialogue state tracking (DST) error analysis in two domains: Hotel (a) and Restaurant (b). \n\n- In the Hotel domain chart (a), the X-axis represents the number of slots with correct non-empty values for various criteria, including parking, stars, name, book people, area, price range, type, book day, book stay, and internet. The chart indicates that knowledge to track the slots for people, area, price range, and day from other domains seen during training has been successfully transferred.\n\n- In the Restaurant domain chart (b), the criteria include book time, book people, book day, price range, food, name, and area. Similar to the hotel domain, it shows how accurately the values for each slot are tracked.\n\nBoth charts provide a visual representation of how effectively the DST model handles zero-shot slot tracking across these domains, with higher bars indicating more successful tracking for the respective slots.\nwhich makes our prediction incorrect even if it is supposed to be predicted. \nIn Fig.  5 , the zero-shot analysis of two se- lected domains,  hotel  and  restaurant , which con- tain more slots to be tracked, are shown. To bet- ter understand the behavior of knowledge trans- ferring, here we only consider labels that are not empty, i.e., we ignore data that is labeled as “none” because predicting “none” is relatively easier for the model. In both  hotel  and  restaurant  domains, knowledge about  people, area, price range , and day  slots are successfully transferred from the other four domains. For unseen slots that only ap- pear in one domain, it is very hard for our model to track correctly. For example,  parking, stars  and internet  slots are only appeared in  hotel  domain, and the  food  slot is unique to the  restaurant  do- main. "}
{"page": 8, "image_path": "doc_images/P19-1078_8.jpg", "ocr_text": "6 Related Work\n\nDialogue State Tracking Traditional dialogue\nstate tracking models combine semantics extracted\nby language understanding modules to estimate\nthe current dialogue states (Williams and Young,\n2007; Thomson and Young, 2010; Wang and\nLemon, 2013; Williams, 2014), or to jointly learn\nspeech understanding (Henderson et al., 2014b;\nZilka and Jurcicek, 2015; Wen et al., 2017). One\ndrawback is that they rely on hand-crafted fea-\ntures and complex domain-specific lexicons (be-\nsides the ontology), and are difficult to extend and\nscale to new domains.\n\nMrkSié et al. (2017) use distributional repre-\nsentation learning to leverage semantic informa-\ntion from word embeddings to and resolve lex-\nical/morphological ambiguity. However, param-\neters are not shared across slots. On the other\nhand, Nouri and Hosseini-Asl (2018) utilizes\nglobal modules to share parameters between slots,\nand Zhong et al. (2018) uses slot-specific local\nmodules to learn slot features, which has proved\nto successfully improve tracking of rare slot val-\nues. Lei et al. (2018) use a Seq2Seq model to gen-\nerate belief spans and the delexicalized response\nat the same time. Ren et al. (2018) propose\nStateNet that generates a dialogue history repre-\nsentation and compares the distances between this\nrepresentation and value vectors in the candidate\nset. Xu and Hu (2018) use the index-based pointer\nnetwork for different slots, and show the ability\nto point to unknown values. However, many of\nthem require a predefined domain ontology, and\nthe models were only evaluated on single-domain\nsetting (DSTC2).\n\nFor multi-domain DST, Rastogi et al. (2017)\npropose a multi-domain approach using two-layer\nbi-GRU. Although it does not need an ad-hoc state\nupdate mechanism, it relies on delexicalization to\nextract the features. Ramadan et al. (2018) pro-\npose a model to jointly track domain and the di-\nalogue states using multiple bi-LSTM. They uti-\nlize semantic similarity between utterances and\nthe ontology terms and allow the information to\nbe shared across domains. For a more general\noverview, readers may refer to the neural dialogue\nreview paper from Gao et al. (2018).\n\nZero/Few-Shot and Continual Learning Dif-\nferent components of dialogue systems have pre-\nviously been used for zero-shot application, e.g.,\n\n816\n\nintention classifiers (Chen et al., 2016), slot-\nfilling (Bapna et al., 2017), and dialogue pol-\nicy (GaSi¢é and Young, 2014). For language\ngeneration, Johnson et al. (2017) propose sin-\ngle encoder-decoder models for zero-shot machine\ntranslation, and Zhao and Eskenazi (2018) pro-\npose cross-domain zero-shot dialogue generation\nusing action matching. Moreover, few-shot learn-\ning in natural language applications has been ap-\nplied in semantic parsing (Huang et al., 2018), ma-\nchine translation (Gu et al., 2018), and text clas-\nsification (Yu et al., 2018) with meta-learning ap-\nproaches (Schmidhuber, 1987; Finn et al., 2017).\nThese tasks usually have multiple tasks to per-\nform fast adaptation, instead in our case the num-\nber of existing domains are limited. Lastly, sev-\neral approaches have been proposed for contin-\nual learning in the machine learning commu-\nnity (Kirkpatrick et al., 2017; Lopez-Paz et al.,\n2017; Rusu et al., 2016; Fernando et al., 2017;\nLee et al., 2017), especially in image recognition\ntasks (Aljundi et al., 2017; Rannen et al., 2017).\nThe applications within NLP has been compara-\ntively limited, e.g., Shu et al. (2016, 2017b) for\nopinion mining, Shu et al. (2017a) for document\nclassification, and Lee (2017) for hybrid code net-\nworks (Williams et al., 2017).\n\n7 Conclusion\n\nWe introduce a transferable dialogue state gen-\nerator for multi-domain dialogue state tracking,\nwhich learns to track states without any predefined\ndomain ontology. TRADE shares all of its param-\neters across multiple domains and achieves state-\nof-the-art joint goal accuracy and slot accuracy on\nthe MultiWOZ dataset for five different domains.\nMoreover, domain sharing enables TRADE to per-\nform zero-shot DST for unseen domains and to\nquickly adapt to few-shot domains without forget-\nting the learned ones. In future work, transferring\nknowledge from other resources can be applied to\nfurther improve zero-shot performance, and col-\nlecting a dataset with a large number of domains is\nable to facilitate the application and study of meta-\nlearning techniques within multi-domain DST.\n\nAcknowledgments\n\nThis work is partially funded by MRP/055/18 of\nthe Innovation Technology Commission, of the\nHong Kong University of Science and Technology\n(HKUST).\n", "vlm_text": "6 Related Work \nDialogue State Tracking Traditional dialogue state tracking models combine semantics extracted by language understanding modules to estimate the current dialogue states ( Williams and Young , 2007 ;  Thomson and Young ,  2010 ;  Wang and Lemon ,  2013 ;  Williams ,  2014 ), or to jointly learn speech understanding ( Henderson et al. ,  2014b ; Zilka and Jurcicek ,  2015 ;  Wen et al. ,  2017 ). One drawback is that they rely on hand-crafted fea- tures and complex domain-speciﬁc lexicons (be- sides the ontology), and are difﬁcult to extend and scale to new domains. \nMrkˇ si´ c et al.  ( 2017 ) use distributional repre- sentation learning to leverage semantic informa- tion from word embeddings to and resolve lex- ical/morphological ambiguity. However, param- eters are not shared across slots. On the other hand, Nouri and Hosseini-Asl  ( 2018 ) utilizes global modules to share parameters between slots, and Zhong et al. (2018) uses slot-speciﬁc localmodules to learn slot features, which has proved to successfully improve tracking of rare slot val- ues.  Lei et al.  ( 2018 ) use a Seq2Seq model to gen- erate belief spans and the delexicalized response at the same time. Ren et al.  ( 2018 ) propose StateNet that generates a dialogue history repre- sentation and compares the distances between this representation and value vectors in the candidate set.  Xu and Hu  ( 2018 ) use the index-based pointer network for different slots, and show the ability to point to unknown values. However, many of them require a predeﬁned domain ontology, and the models were only evaluated on single-domain setting (DSTC2). \nFor multi-domain DST,  Rastogi et al.  ( 2017 ) propose a multi-domain approach using two-layer bi-GRU. Although it does not need an ad-hoc state update mechanism, it relies on de lexical iz ation to extract the features. Ramadan et al.  ( 2018 ) pro- pose a model to jointly track domain and the di- alogue states using multiple bi-LSTM. They uti- lize semantic similarity between utterances and the ontology terms and allow the information to be shared across domains. For a more general overview, readers may refer to the neural dialogue review paper from  Gao et al.  ( 2018 ). \nZero/Few-Shot and Continual Learning Dif- ferent components of dialogue systems have pre- viously been used for zero-shot application, e.g., intention classiﬁers ( Chen et al. ,  2016 ), slot- ﬁlling ( Bapna et al. ,  2017 ), and dialogue pol- icy ( Gaˇ si´ c and Young ,  2014 ). For language generation,  Johnson et al.  ( 2017 ) propose sin- gle encoder-decoder models for zero-shot machine translation, and  Zhao and Eskenazi  ( 2018 ) pro- pose cross-domain zero-shot dialogue generation using action matching. Moreover, few-shot learn- ing in natural language applications has been ap- plied in semantic parsing ( Huang et al. ,  2018 ), ma- chine translation ( Gu et al. ,  2018 ), and text clas- siﬁcation ( Yu et al. ,  2018 ) with meta-learning ap- proaches ( Schmidhuber ,  1987 ;  Finn et al. ,  2017 ). These tasks usually have multiple tasks to per- form fast adaptation, instead in our case the num- ber of existing domains are limited. Lastly, sev- eral approaches have been proposed for contin- ual learning in the machine learning commu- nity ( Kirkpatrick et al. ,  2017 ;  Lopez-Paz et al. , 2017 ;  Rusu et al. ,  2016 ;  Fernando et al. ,  2017 ; Lee et al. ,  2017 ), especially in image recognition tasks ( Aljundi et al. ,  2017 ;  Rannen et al. ,  2017 ). The applications within NLP has been compara- tively limited, e.g.,  Shu et al.  ( 2016 ,  2017b ) for opinion mining,  Shu et al.  ( 2017a ) for document classiﬁcation, and  Lee  ( 2017 ) for hybrid code net- works ( Williams et al. ,  2017 ). \n\n7 Conclusion \nWe introduce a transferable dialogue state gen- erator for multi-domain dialogue state tracking, which learns to track states without any predeﬁned domain ontology. TRADE shares all of its param- eters across multiple domains and achieves state- of-the-art joint goal accuracy and slot accuracy on the MultiWOZ dataset for ﬁve different domains. Moreover, domain sharing enables TRADE to per- form zero-shot DST for unseen domains and to quickly adapt to few-shot domains without forget- ting the learned ones. In future work, transferring knowledge from other resources can be applied to further improve zero-shot performance, and col- lecting a dataset with a large number of domains is able to facilitate the application and study of meta- learning techniques within multi-domain DST. \nAcknowledgments \nThis work is partially funded by MRP/055/18 of the Innovation Technology Commission, of the Hong Kong University of Science and Technology (HKUST). "}
{"page": 9, "image_path": "doc_images/P19-1078_9.jpg", "ocr_text": "References\n\nRahaf Aljundi, Punarjay Chakravarty, and Tinne Tuyte-\nlaars. 2017. Expert gate: Lifelong learning with\na network of experts. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recog-\nnition, pages 3366-3375.\n\nAnkur Bapna, Gokhan Tur, Dilek Hakkani-Tur, and\nLarry Heck. 2017. Towards zero-shot frame se-\n\nmantic parsing for domain scaling. arXiv preprint\narXiv:1707.02363.\n\nSamuel R. Bowman, Luke Vilnis, Oriol Vinyals, An-\ndrew Dai, Rafal Jozefowicz, and Samy Bengio.\n2016. Generating sentences from a continuous\nspace. In Proceedings of The 20th SIGNLL Confer-\nence on Computational Natural Language Learning,\npages 10-21. Association for Computational Lin-\nguistics.\n\nPawet Budzianowski, Tsung-Hsien Wen, Bo-Hsiang\nTseng, Ifigo Casanueva, Stefan Ultes, Osman Ra-\nmadan, and Milica Gasic. 2018. Multiwoz-a large-\nscale multi-domain wizard-of-oz dataset for task-\noriented dialogue modelling. In Proceedings of the\n2018 Conference on Empirical Methods in Natural\nLanguage Processing, pages 5016-5026.\n\nYun-Nung Chen, Dilek Hakkani-Tiir, and Xiaodong\nHe. 2016. Zero-shot learning of intent embeddings\nfor expansion by convolutional deep structured se-\nmantic models. In 20/6 IEEE International Con-\nference on Acoustics, Speech and Signal Processing\n(ICASSP), pages 6045-6049. IEEE.\n\nJunyoung Chung, Caglar Gulcehre, KyungHyun Cho,\nand Yoshua Bengio. 2014. Empirical evaluation of\ngated recurrent neural networks on sequence model-\ning. arXiv preprint arXiv: 1412.3555.\n\nChrisantha Fernando, Dylan Banarse, Charles Blun-\ndell, Yori Zwols, David Ha, Andrei A Rusu, Alexan-\nder Pritzel, and Daan Wierstra. 2017. Pathnet: Evo-\nlution channels gradient descent in super neural net-\nworks. arXiv preprint arXiv: 1701.08734.\n\nChelsea Finn, Pieter Abbeel, and Sergey Levine. 2017.\nModel-agnostic meta-learning for fast adaptation of\ndeep networks. In Proceedings of the 34th Interna-\ntional Conference on Machine Learning-Volume 70,\npages 1126-1135. JMLR. org.\n\nJianfeng Gao, Michel Galley, and Lihong Li. 2018.\nNeural approaches to conversational ai. In The\n41st International ACM SIGIR Conference on Re-\nsearch & Development in Information Retrieval,\npages 1371-1374. ACM.\n\nMilica GaSi¢ and Steve Young. 2014. Gaussian pro-\ncesses for pomdp-based dialogue manager optimiza-\ntion. IEEE/ACM Transactions on Audio, Speech,\nand Language Processing, 22(1):28-40.\n\n817\n\nJiatao Gu, Yong Wang, Yun Chen, Victor O. K. Li,\nand Kyunghyun Cho. 2018. Meta-learning for low-\nresource neural machine translation. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, pages 3622-3631.\nAssociation for Computational Linguistics.\n\nCaglar Gulcehre, Sungjin Ahn, Ramesh Nallap-\nati, Bowen Zhou, and Yoshua Bengio. 2016.\nPointing the unknown words. arXiv preprint\narXiv: 1603.08148.\n\nKazuma Hashimoto, Caiming Xiong, Yoshimasa Tsu-\nruoka, and Richard Socher. 2016. A joint many-task\nmodel: Growing a neural network for multiple nlp\ntasks. arXiv preprint arXiv: 1611.01587.\n\nMatthew Henderson, Blaise Thomson, and Jason D\nWilliams. 2014a. The second dialog state tracking\nchallenge. In Proceedings of the 15th Annual Meet-\ning of the Special Interest Group on Discourse and\nDialogue (SIGDIAL), pages 263-272.\n\nMatthew Henderson, Blaise Thomson, and Steve\nYoung. 2014b. Word-based dialog state tracking\nwith recurrent neural networks. In Proceedings\nof the 15th Annual Meeting of the Special Inter-\nest Group on Discourse and Dialogue (SIGDIAL),\npages 292-299.\n\nPo-Sen Huang, Chenglong Wang, Rishabh Singh,\nWen-tau Yih, and Xiaodong He. 2018. Natural\nlanguage to structured query generation via meta-\nlearning. In Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 2 (Short Papers), pages 732-738.\nAssociation for Computational Linguistics.\n\nMelvin Johnson, Mike Schuster, Quoc V. Le, Maxim\nKrikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat,\nFernanda Viégas, Martin Wattenberg, Greg Corrado,\nMacduff Hughes, and Jeffrey Dean. 2017. Google’s\nmultilingual neural machine translation system: En-\nabling zero-shot translation. Transactions of the As-\nsociation for Computational Linguistics, 5:339-351.\n\nDiederik P Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. International\nConference on Learning Representations.\n\nJames Kirkpatrick, Razvan Pascanu, Neil Rabinowitz,\nJoel Veness, Guillaume Desjardins, Andrei A Rusu,\nKieran Milan, John Quan, Tiago Ramalho, Ag-\nnieszka Grabska-Barwinska, et al. 2017. Overcom-\ning catastrophic forgetting in neural networks. Pro-\nceedings of the national academy of sciences, page\n201611835.\n\nSang-Woo Lee, Jin-Hwa Kim, Jaehyun Jun, Jung-\nWoo Ha, and Byoung-Tak Zhang. 2017. Overcom-\ning catastrophic forgetting by incremental moment\nmatching. In Advances in Neural Information Pro-\ncessing Systems, pages 4652-4662.\n", "vlm_text": "References \nRahaf Aljundi, Punarjay Chakravarty, and Tinne Tuyte- laars. 2017. Expert gate: Lifelong learning with a network of experts. In  Proceedings of the IEEE Conference on Computer Vision and Pattern Recog- nition , pages 3366–3375. \nAnkur Bapna, Gokhan Tur, Dilek Hakkani-Tur, and Larry Heck. 2017. Towards zero-shot frame se- mantic parsing for domain scaling.  arXiv preprint arXiv:1707.02363 . \nSamuel R. Bowman, Luke Vilnis, Oriol Vinyals, An- drew Dai, Rafal Jozefowicz, and Samy Bengio. 2016. Generating sentences from a continuous space . In  Proceedings of The 20th SIGNLL Confer- ence on Computational Natural Language Learning , pages 10–21. Association for Computational Lin- guistics. \nPaweł Budzianowski, Tsung-Hsien Wen, Bo-Hsiang Tseng, I˜ nigo Casanueva, Stefan Ultes, Osman Ra- madan, and Milica Gasic. 2018. Multiwoz-a large- scale multi-domain wizard-of-oz dataset for task- oriented dialogue modelling. In  Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing , pages 5016–5026. \nYun-Nung Chen, Dilek Hakkani-T¨ ur, and Xiaodong He. 2016. Zero-shot learning of intent embeddings for expansion by convolutional deep structured se- mantic models. In  2016 IEEE International Con- ference on Acoustics, Speech and Signal Processing (ICASSP) , pages 6045–6049. IEEE. \nJunyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. 2014. Empirical evaluation of gated recurrent neural networks on sequence model- ing.  arXiv preprint arXiv:1412.3555 . \nChrisantha Fernando, Dylan Banarse, Charles Blun- dell, Yori Zwols, David Ha, Andrei A Rusu, Alexan- der Pritzel, and Daan Wierstra. 2017. Pathnet: Evo- lution channels gradient descent in super neural net- works.  arXiv preprint arXiv:1701.08734 . \nChelsea Finn, Pieter Abbeel, and Sergey Levine. 2017. Model-agnostic meta-learning for fast adaptation of deep networks. In  Proceedings of the 34th Interna- tional Conference on Machine Learning-Volume 70 , pages 1126–1135. JMLR. org. \nJianfeng Gao, Michel Galley, and Lihong Li. 2018. Neural approaches to conversational ai. In  The 41st International ACM SIGIR Conference on Re- search & Development in Information Retrieval , pages 1371–1374. ACM. \nMilica Gaˇ si´ c and Steve Young. 2014. Gaussian pro- cesses for pomdp-based dialogue manager optimiza- tion. IEEE/ACM Transactions on Audio, Speech, and Language Processing , 22(1):28–40. \nJiatao Gu, Yong Wang, Yun Chen, Victor O. K. Li, and Kyunghyun Cho. 2018.  Meta-learning for low- resource neural machine translation . In  Proceed- ings of the 2018 Conference on Empirical Methods in Natural Language Processing , pages 3622–3631. Association for Computational Linguistics. \nCaglar Gulcehre, Sungjin Ahn, Ramesh Nallap- ati, Bowen Zhou, and Yoshua Bengio. 2016. Pointing the unknown words. arXiv preprint arXiv:1603.08148 . \nKazuma Hashimoto, Caiming Xiong, Yoshimasa Tsu- ruoka, and Richard Socher. 2016. A joint many-task model: Growing a neural network for multiple nlp tasks.  arXiv preprint arXiv:1611.01587 . \nMatthew Henderson, Blaise Thomson, and Jason D Williams. 2014a. The second dialog state tracking challenge. In  Proceedings of the 15th Annual Meet- ing of the Special Interest Group on Discourse and Dialogue (SIGDIAL) , pages 263–272. \nMatthew Henderson, Blaise Thomson, and Steve Young. 2014b. Word-based dialog state tracking with recurrent neural networks. In  Proceedings of the 15th Annual Meeting of the Special Inter- est Group on Discourse and Dialogue (SIGDIAL) , pages 292–299. \nPo-Sen Huang, Chenglong Wang, Rishabh Singh, Wen-tau Yih, and Xiaodong He. 2018. Natural language to structured query generation via meta- learning . In  Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech- nologies, Volume 2 (Short Papers) , pages 732–738. Association for Computational Linguistics. \nMelvin Johnson, Mike Schuster, Quoc V. Le, Maxim Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat, Fernanda Vi´ egas, Martin Wattenberg, Greg Corrado, Macduff Hughes, and Jeffrey Dean. 2017.  Google’s multilingual neural machine translation system: En- abling zero-shot translation .  Transactions of the As- sociation for Computational Linguistics , 5:339–351. \nDiederik P Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. International Conference on Learning Representations . \nJames Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Ag- nieszka Grabska-Barwinska, et al. 2017. Overcom- ing catastrophic forgetting in neural networks.  Pro- ceedings of the national academy of sciences , page 201611835. \nSang-Woo Lee, Jin-Hwa Kim, Jaehyun Jun, Jung- Woo Ha, and Byoung-Tak Zhang. 2017. Overcom- ing catastrophic forgetting by incremental moment matching. In  Advances in Neural Information Pro- cessing Systems , pages 4652–4662. "}
{"page": 10, "image_path": "doc_images/P19-1078_10.jpg", "ocr_text": "Sungjin Lee. 2017. Toward continual learn-\ning for conversational agents. arXiv preprint\narXiv:1712.09943.\n\nWenqiang Lei, Xisen Jin, Min-Yen Kan, Zhaochun\nRen, Xiangnan He, and Dawei Yin. 2018. Sequic-\nity: Simplifying task-oriented dialogue systems with\nsingle sequence-to-sequence architectures. In Pro-\nceedings of the 56th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers), volume 1, pages 1437-1447.\n\nDavid Lopez-Paz et al. 2017. Gradient episodic mem-\nory for continual learning. In Advances in Neural\nInformation Processing Systems, pages 6467-6476.\n\nAndrea Madotto, Chien-Sheng Wu, and Pascale Fung.\n2018. Mem2seq: Effectively incorporating knowl-\nedge bases into end-to-end task-oriented dialog sys-\ntems. In Proceedings of the 56th Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume I: Long Papers), volume 1, pages 1468-1478.\n\nBryan McCann, Nitish Shirish Keskar, Caiming Xiong,\nand Richard Socher. 2018. The natural language de-\ncathlon: Multitask learning as question answering.\narXiv preprint arXiv: 1806.08730.\n\nNikola Mrksié, Diarmuid O Séaghdha, Tsung-Hsien\nWen, Blaise Thomson, and Steve Young. 2017.\nNeural belief tracker: Data-driven dialogue state\ntracking. In Proceedings of the 55th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 1777-1788, Van-\ncouver, Canada. Association for Computational Lin-\nguistics.\n\nElnaz Nouri and Ehsan Hosseini-Asl. 2018. — To-\nward scalable neural dialogue state tracking model.\nIn Advances in neural information processing sys-\ntems (NeurIPS), 2nd Conversational AI workshop.\nhttps://arxiv.org/abs/1812.00899.\n\nJeffrey Pennington, Richard Socher, and Christopher\nManning. 2014. Glove: Global vectors for word\nrepresentation. In Proceedings of the 2014 confer-\nence on empirical methods in natural language pro-\ncessing (EMNLP), pages 1532-1543.\n\nOsman Ramadan, Pawel Budzianowski, and Milica\nGasic. 2018. Large-scale multi-domain belief track-\ning with knowledge sharing. In Proceedings of the\n56th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 2: Short Papers), pages\n432-437. Association for Computational Linguis-\ntics.\n\nAmal Rannen, Rahaf Aljundi, Matthew B Blaschko,\nand Tinne Tuytelaars. 2017. Encoder based lifelong\nlearning. In Proceedings of the IEEE International\nConference on Computer Vision, pages 1320-1328.\n\nAbhinav Rastogi, Dilek Hakkani-Tiir, and Larry Heck.\n2017. Scalable multi-domain dialogue state track-\ning. In 2017 IEEE Automatic Speech Recognition\nand Understanding Workshop (ASRU), pages 561—\n568. IEEE.\n\n818\n\nLiliang Ren, Kaige Xie, Lu Chen, and Kai Yu. 2018.\nTowards universal dialogue state tracking. In Pro-\nceedings of the 2018 Conference on Empirical Meth-\nods in Natural Language Processing, pages 2780—\n2786.\n\nAndrei A Rusu, Neil C Rabinowitz, Guillaume Des-\njardins, Hubert Soyer, James Kirkpatrick, Koray\nKavukcuoglu, Razvan Pascanu, and Raia Hadsell.\n2016. Progressive neural networks. arXiv preprint\narXiv: 1606.04671.\n\nJurgen Schmidhuber. 1987. Evolutionary principles in\nself-referential learning. on learning now to learn:\nThe meta-meta-meta...-hook. Diploma thesis, Tech-\nnische Universitat Munchen, Germany, 14 May.\n\nAbigail See, Peter J Liu, and Christopher D Manning.\n2017. Get to the point: Summarization with pointer-\ngenerator networks. In Proceedings of the 55th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), volume 1,\npages 1073-1083.\n\nLei Shu, Bing Liu, Hu Xu, and Annice Kim. 2016.\nLifelong-rl: Lifelong relaxation labeling for sepa-\nrating entities and aspects in opinion targets. In Pro-\nceedings of the Conference on Empirical Methods in\nNatural Language Processing. Conference on Em-\npirical Methods in Natural Language Processing,\nvolume 2016, page 225. NIH Public Access.\n\nLei Shu, Hu Xu, and Bing Liu. 2017a. Doc: Deep\nopen classification of text documents. In Proceed-\nings of the 2017 Conference on Empirical Methods\nin Natural Language Processing, pages 2911-2916.\nAssociation for Computational Linguistics.\n\nLei Shu, Hu Xu, and Bing Liu. 2017b. Lifelong learn-\ning crf for supervised aspect extraction. In Pro-\nceedings of the 55th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 2: Short\nPapers), pages 148-154. Association for Computa-\ntional Linguistics.\n\nBlaise Thomson and Steve Young. 2010. Bayesian\nupdate of dialogue state: A pomdp framework for\nspoken dialogue systems. Computer Speech & Lan-\nguage, 24(4):562-588.\n\nOriol Vinyals, Meire Fortunato, and Navdeep Jaitly.\n2015. Pointer networks. In Advances in Neural In-\nformation Processing Systems, pages 2692-2700.\n\nZhuoran Wang and Oliver Lemon. 2013. A simple\nand generic belief tracking mechanism for the dia-\nlog state tracking challenge: On the believability of\nobserved information. In Proceedings of the SIG-\nDIAL 2013 Conference, pages 423-432.\n\nTsung-Hsien Wen, David Vandyke, Nikola Mrk3i¢,\nMilica Gasic, Lina M. Rojas Barahona, Pei-Hao Su,\nStefan Ultes, and Steve Young. 2017. A network-\nbased end-to-end trainable task-oriented dialogue\nsystem. In Proceedings of the 15th Conference of\n", "vlm_text": "Sungjin Lee. 2017. Toward continual learn- ing for conversational agents. arXiv preprint \narXiv:1712.09943 . Wenqiang Lei, Xisen Jin, Min-Yen Kan, Zhaochun Ren, Xiangnan He, and Dawei Yin. 2018. Sequic- ity: Simplifying task-oriented dialogue systems with single sequence-to-sequence architectures. In  Pro- ceedings of the 56th Annual Meeting of the Associa- tion for Computational Linguistics (Volume 1: Long Papers) , volume 1, pages 1437–1447. David Lopez-Paz et al. 2017. Gradient episodic mem- ory for continual learning. In  Advances in Neural Information Processing Systems , pages 6467–6476. Andrea Madotto, Chien-Sheng Wu, and Pascale Fung. 2018. Mem2seq: Effectively incorporating knowl- edge bases into end-to-end task-oriented dialog sys- tems. In  Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Vol- ume 1: Long Papers) , volume 1, pages 1468–1478. Bryan McCann, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. 2018. The natural language de- cathlon: Multitask learning as question answering. arXiv preprint arXiv:1806.08730 . Nikola Mrkˇ si´ c, Diarmuid  O S´ eaghdha, Tsung-Hsien Wen, Blaise Thomson, and Steve Young. 2017. Neural belief tracker: Data-driven dialogue state tracking . In  Proceedings of the 55th Annual Meet- ing of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 1777–1788, Van- couver, Canada. Association for Computational Lin- guistics. Elnaz Nouri and Ehsan Hosseini-Asl. 2018. To- ward scalable neural dialogue state tracking model. In  Advances in neural information processing sys- tems (NeurIPS), 2nd Conversational AI workshop . https://arxiv.org/abs/1812.00899 . Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. Glove: Global vectors for word representation. In  Proceedings of the 2014 confer- ence on empirical methods in natural language pro- cessing (EMNLP) , pages 1532–1543. Osman Ramadan, Paweł Budzianowski, and Milica Gasic. 2018.  Large-scale multi-domain belief track- ing with knowledge sharing . In  Proceedings of the 56th Annual Meeting of the Association for Compu- tational Linguistics (Volume 2: Short Papers) , pages 432–437. Association for Computational Linguis- tics. Amal Rannen, Rahaf Aljundi, Matthew B Blaschko, and Tinne Tuytelaars. 2017. Encoder based lifelong learning. In  Proceedings of the IEEE International Conference on Computer Vision , pages 1320–1328. Abhinav Rastogi, Dilek Hakkani-T¨ ur, and Larry Heck. 2017. Scalable multi-domain dialogue state track- ing. In  2017 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU) , pages 561– 568. IEEE. \nLiliang Ren, Kaige Xie, Lu Chen, and Kai Yu. 2018. Towards universal dialogue state tracking. In  Pro- ceedings of the 2018 Conference on Empirical Meth- ods in Natural Language Processing , pages 2780– 2786. Andrei A Rusu, Neil C Rabinowitz, Guillaume Des- jardins, Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. 2016. Progressive neural networks.  arXiv preprint arXiv:1606.04671 . Jurgen Schmidhuber. 1987.  Evolutionary principles in self-referential learning. on learning now to learn: The meta-meta-meta...-hook . Diploma thesis, Tech- nische Universitat Munchen, Germany, 14 May. Abigail See, Peter J Liu, and Christopher D Manning. 2017. Get to the point: Summarization with pointer- generator networks. In  Proceedings of the 55th An- nual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , volume 1, pages 1073–1083. Lei Shu, Bing Liu, Hu Xu, and Annice Kim. 2016. Lifelong-rl: Lifelong relaxation labeling for sepa- rating entities and aspects in opinion targets. In  Pro- ceedings of the Conference on Empirical Methods in Natural Language Processing. Conference on Em- pirical Methods in Natural Language Processing , volume 2016, page 225. NIH Public Access. Lei Shu, Hu Xu, and Bing Liu. 2017a. Doc: Deep open classiﬁcation of text documents . In  Proceed- ings of the 2017 Conference on Empirical Methods in Natural Language Processing , pages 2911–2916. Association for Computational Linguistics. Lei Shu, Hu Xu, and Bing Liu. 2017b.  Lifelong learn- ing crf for supervised aspect extraction . In  Pro- ceedings of the 55th Annual Meeting of the Associa- tion for Computational Linguistics (Volume 2: Short Papers) , pages 148–154. Association for Computa- tional Linguistics. Blaise Thomson and Steve Young. 2010. Bayesian update of dialogue state: A pomdp framework for spoken dialogue systems.  Computer Speech & Lan- guage , 24(4):562–588. Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. 2015. Pointer networks. In  Advances in Neural In- formation Processing Systems , pages 2692–2700. Zhuoran Wang and Oliver Lemon. 2013. A simple and generic belief tracking mechanism for the dia- log state tracking challenge: On the believability of observed information. In  Proceedings of the SIG- DIAL 2013 Conference , pages 423–432. Tsung-Hsien Wen, David Vandyke, Nikola Mrkˇ si´ c, Milica Gasic, Lina M. Rojas Barahona, Pei-Hao Su, Stefan Ultes, and Steve Young. 2017.  A network- based end-to-end trainable task-oriented dialogue system . In  Proceedings of the 15th Conference of "}
{"page": 11, "image_path": "doc_images/P19-1078_11.jpg", "ocr_text": "the European Chapter of the Association for Compu-\ntational Linguistics: Volume 1, Long Papers, pages\n438-449. Association for Computational Linguis-\ntics.\n\nJason D Williams. 2014. Web-style ranking and slu\ncombination for dialog state tracking. In Proceed-\nings of the 15th Annual Meeting of the Special Inter-\nest Group on Discourse and Dialogue (SIGDIAL),\npages 282-291.\n\nJason D Williams, Kavosh Asadi, and Geoffrey Zweig.\n2017. Hybrid code networks: practical and efficient\nend-to-end dialog control with supervised and rein-\nforcement learning. In Proceedings of the 55th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 665—\n677. Association for Computational Linguistics.\n\nJason D Williams and Steve Young. 2007. Partially\nobservable markov decision processes for spoken\ndialog systems. Computer Speech & Language,\n21(2):393-422.\n\nChien-Sheng Wu, Richard Socher, and Caiming Xiong.\n2019. Global-to-local memory pointer networks for\ntask-oriented dialogue. In Proceedings of the 7th\nInternational Conference on Learning Representa-\ntions.\n\nPuyang Xu and Qi Hu. 2018. An end-to-end approach\nfor handling unknown slot values in dialogue state\ntracking. In Proceedings of the 56th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 1448-1457. Asso-\nciation for Computational Linguistics.\n\nMo Yu, Xiaoxiao Guo, Jinfeng Yi, Shiyu Chang, Saloni\nPotdar, Yu Cheng, Gerald Tesauro, Haoyu Wang,\nand Bowen Zhou. 2018. Diverse few-shot text clas-\nsification with multiple metrics. In Proceedings of\nthe 2018 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long Pa-\npers), pages 1206-1215. Association for Computa-\ntional Linguistics.\n\nTiancheng Zhao and Maxine Eskenazi. 2018. Zero-\nshot dialog generation with cross-domain latent ac-\ntions. In Proceedings of the 19th Annual SIGdial\nMeeting on Discourse and Dialogue, pages 1-10.\n\nVictor Zhong, Caiming Xiong, and Richard Socher.\n2018. Global-locally self-attentive encoder for dia-\nlogue state tracking. In Proceedings of the 56th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 1458—\n1467. Association for Computational Linguistics.\n\nLukas Zilka and Filip Jurcicek. 2015. Incremental\nIstm-based dialog state tracker. In 2015 Ieee Work-\nshop on Automatic Speech Recognition and Under-\nstanding (Asru), pages 757-762. IEEE.\n\n819\n", "vlm_text": "the European Chapter of the Association for Compu- tational Linguistics: Volume 1, Long Papers , pages 438–449. Association for Computational Linguis- tics. \nJason D Williams. 2014. Web-style ranking and slu combination for dialog state tracking. In  Proceed- ings of the 15th Annual Meeting of the Special Inter- est Group on Discourse and Dialogue (SIGDIAL) , pages 282–291. \nJason D Williams, Kavosh Asadi, and Geoffrey Zweig. 2017.  Hybrid code networks: practical and efﬁcient end-to-end dialog control with supervised and rein- forcement learning . In  Proceedings of the 55th An- nual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 665– 677. Association for Computational Linguistics. \nJason D Williams and Steve Young. 2007. Partially observable markov decision processes for spoken dialog systems. Computer Speech & Language , 21(2):393–422. \nChien-Sheng Wu, Richard Socher, and Caiming Xiong. 2019. Global-to-local memory pointer networks for task-oriented dialogue. In  Proceedings of the 7th International Conference on Learning Representa- tions . \nPuyang Xu and Qi Hu. 2018.  An end-to-end approach for handling unknown slot values in dialogue state tracking . In  Proceedings of the 56th Annual Meet- ing of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 1448–1457. Asso- ciation for Computational Linguistics. \nMo Yu, Xiaoxiao Guo, Jinfeng Yi, Shiyu Chang, Saloni Potdar, Yu Cheng, Gerald Tesauro, Haoyu Wang, and Bowen Zhou. 2018.  Diverse few-shot text clas- siﬁcation with multiple metrics . In  Proceedings of the 2018 Conference of the North American Chap- ter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Pa- pers) , pages 1206–1215. Association for Computa- tional Linguistics. \nTiancheng Zhao and Maxine Eskenazi. 2018. Zero- shot dialog generation with cross-domain latent ac- tions. In  Proceedings of the 19th Annual SIGdial Meeting on Discourse and Dialogue , pages 1–10. \nVictor Zhong, Caiming Xiong, and Richard Socher. 2018.  Global-locally self-attentive encoder for dia- logue state tracking . In  Proceedings of the 56th An- nual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 1458– 1467. Association for Computational Linguistics. \nLukas Zilka and Filip Jurcicek. 2015. Incremental lstm-based dialog state tracker. In  2015 Ieee Work- shop on Automatic Speech Recognition and Under- standing (Asru) , pages 757–762. IEEE. "}
