{"layout": 0, "type": "text", "text": "Making Your First Choice: To Address Cold Start Problem in Vision Active Learning ", "text_level": 1, "page_idx": 0, "bbox": [137, 96, 476, 137], "page_size": [612.0, 792.0]}
{"layout": 1, "type": "text", "text": "Liangyu Chen 1 Yutong Bai 2 Siyu Huang 3 Yongyi Lu 2 Bihan Wen 1 Alan L. Yuille 2 Zongwei Zhou ∗ 1 Nanyang Technological University 2 Johns Hopkins University 3 Harvard University ", "page_idx": 0, "bbox": [130.85797119140625, 178.42208862304688, 481.1425476074219, 214.84141540527344], "page_size": [612.0, 792.0]}
{"layout": 2, "type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0, "bbox": [283, 242, 328, 255], "page_size": [612.0, 792.0]}
{"layout": 3, "type": "text", "text": "Active learning promises to improve annotation efﬁciency by iterative ly selecting the most important data to be annotated ﬁrst. However, we uncover a striking contradiction to this promise: active learning fails to select data as efﬁciently as random selection at the ﬁrst few choices. We identify this as the cold start problem in vision active learning, caused by a biased and outlier initial query. This paper seeks to address the cold start problem by exploiting the three advantages of contrastive learning: (1) no annotation is required; (2) label diversity is ensured by pseudo-labels to mitigate bias; (3) typical data is determined by contrastive features to reduce outliers. Experiments are conducted on CIFAR-10-LT and three medical imaging datasets ( i.e . Colon Pathology, Abdominal CT, and Blood Cell Microscope). Our initial query not only sign i cant ly outperforms existing active querying strategies but also surpasses random selection by a large margin. We foresee our solution to the cold start problem as a simple yet strong baseline to choose the initial query for vision active learning. ", "page_idx": 0, "bbox": [143, 267.70050048828125, 468, 421.5245056152344], "page_size": [612.0, 792.0]}
{"layout": 4, "type": "text", "text": "Code is available: https://github.com/c-liangyu/CSVAL ", "page_idx": 0, "bbox": [143, 422.46856689453125, 364.77667236328125, 434.4735107421875], "page_size": [612.0, 792.0]}
{"layout": 5, "type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0, "bbox": [108, 453, 191, 467], "page_size": [612.0, 792.0]}
{"layout": 6, "type": "text", "text": "“ The secret of getting ahead is getting started. ", "text_level": 1, "page_idx": 0, "bbox": [213, 480, 400, 490], "page_size": [612.0, 792.0]}
{"layout": 7, "type": "text", "text": "— Mark Twain ", "page_idx": 0, "bbox": [442.91900634765625, 500.63958740234375, 504, 512.64453125], "page_size": [612.0, 792.0]}
{"layout": 8, "type": "text", "text": "The cold start problem was initially found in recommend er systems [ 56 ,  39 ,  9 ,  23 ] when algorithms had not gathered sufﬁcient information about users with no purchase history. It also occurred in many other ﬁelds, such as natural language processing [ 55 ,  33 ] and computer vision [ 5 ,  11 ,  38 ] during the active learning procedure 1 . Active learning promises to improve annotation efﬁciency by iterative ly selecting the most important data to annotate. However, we uncover a striking contradiction to this promise: Active learning fails to select data as effectively as random selection at the ﬁrst choice. We identify this as the cold start problem in vision active learning and illustrate the problem using three medical imaging applications (Figure 1a–c) as well as a natural imaging application (Figure 1d). Cold start is a crucial topic [ 54 ,  30 ] because a performant initial query can lead to noticeably improved subsequent cycle performance in the active learning procedure, evidenced in   $\\S3.3$  . There is a lack of studies that systematically illustrate the cold start problem, investigate its causes, and provide practical solutions to address it. To this end, we ask:  What causes the cold start problem and how can we select the initial query when there is no labeled data available? ", "page_idx": 0, "bbox": [107, 522.08056640625, 504, 664.8848876953125], "page_size": [612.0, 792.0]}
{"layout": 9, "type": "image", "page_idx": 1, "img_path": "layout_images/2210.02442v1_0.jpg", "img_caption": "Figure 1:  Cold start problem in vision active learning.  Most existing active querying strategies ( e.g . BALD, Consistency, etc.) are outperformed by random selection in selecting initial queries, since random selection is i.i.d. to the entire dataset. However, some classes are not selected by active querying strategies due to selection bias, so their results are not presented in the low budget regime. ", "bbox": [106, 72, 505, 315], "page_size": [612.0, 792.0], "ocr_text": "AUC\n\nAUC\n\nY BALD (Kirsch et al., 2019)\n} Margin (Balcan ef al., 2007)\n\nMi VAAL (Sinha et al., 2019)\n\n1.0 *\nr taHey\nt '\n09 i :\ni\n0.8) *\n10\" TO: TO: 10\" To\nNumber of images\n(a) PathMNIST\n1.0) PELL peepee\nyan ‘ 114i al\nota thi\nog ttt\nt\noT 10: TO: TO\n\nNumber of images\n(c) BloodMNIST\n\nAUC\n\nAUC\n\nA Consistency (Gao et al, 2020)\n\n44 Coreset (Sener ef al, 2017) @ Random\nEntropy (Wang et al., 2014)\n1.0\nLe al\noa watt Mi E\nF ar are) ¥\nut ‘ ad\n0.8) yo. ra\n10’ 10> 10\"\nNumber of images\n(b) OrganAMNIST\n1.0\n' i\na ea\n0.8} a |\noe aii tb aneteh\n\nTO\nNumber of images\n\n(d) CIFAR-10\n\n10\"\n", "vlm_text": "The image consists of four plots demonstrating the performance of various active learning query strategies in terms of AUC (Area Under the Curve) against the number of images used for training, across different datasets. \n\n1. **Top-left plot (a) PathMNIST**: The performance of different strategies on the PathMNIST dataset is shown. The random selection strategy, marked by grey circles, appears to outperform or match active learning strategies such as BALD, Consistency, Margin, VAAL, Coreset, and Entropy at lower budgets of images (fewer training images).\n\n2. **Top-right plot (b) OrganAMNIST**: The graph represents AUC versus the number of images for the OrganAMNIST dataset, showing a similar trend where random selection performs comparably or better than other strategies at lower images.\n\n3. **Bottom-left plot (c) BloodMNIST**: This plot relates to the BloodMNIST dataset and again shows random selection doing well initially compared to the other active learning strategies, which are represented by different red symbols.\n\n4. **Bottom-right plot (d) CIFAR-10**: For CIFAR-10, while random selection starts out strong, the active learning methods start to catch up or outperform as the number of images increases beyond the lower budget scenario.\n\nAll plots illustrate the cold start problem in active learning for vision tasks, emphasizing that random sampling can initially be more effective than certain active querying strategies because it is representative of the entire dataset. The legend and annotations specify the different active learning strategies being compared: BALD, Consistency, Margin, VAAL, Coreset, and Entropy. The caption discusses how many active strategies are outperformed by random selection initially due to selection bias, where some classes may not be adequately sampled by active learning strategies."}
{"layout": 10, "type": "text", "text": "Random selection is generally considered a baseline to start the active learning because the randomly sampled query is independent and identically distributed (i.i.d.) to the entire data distribution. As is known, maintaining a similar distribution between training and test data is beneﬁcial, particularly when using limited training data [ 25 ]. Therefore, a large body of existing work selects the initial query randomly [ 10 ,  61 ,  55 ,  62 ,  18 ,  17 ,  42 ,  24 ,  22 ,  60 ], highlighting that active querying compromises accuracy and diversity compared to random sampling at the beginning of active learning [ 36 ,  63 ,  44 , 11, 20, 59]. Why? We attribute the causes of the cold start problem to the following two aspects: ", "page_idx": 1, "bbox": [107, 338.28656005859375, 505, 415.7464904785156], "page_size": [612.0, 792.0]}
{"layout": 11, "type": "text", "text": "(i)  Biased query : Active learning tends to select data that is biased to speciﬁc classes. Empirically, Figure 2 reveals that the class distribution in the selected query is highly unbalanced. These active querying strategies ( e.g . Entropy, Margin, VAAL, etc.) can barely outperform random sampling at the beginning because some classes are simply not selected for training. It is because data of the minority classes occurs much less frequently than those of the majority classes. Moreover, datasets in practice are often highly unbalanced, particularly in medical images [ 32 ,  58 ]. This can escalate the biased sampling. We hypothesize that the  label diversity  of a query is an important criterion to determine the importance of the annotation. To evaluate this hypothesis theoretically, we explore the upper bound performance by enforcing a uniform distribution using ground truth (Table 1) To evaluate this hypothesis practically, we pursue the label diversity by exploiting the pseudo-labels generated by    $K$  -means clustering (Table 2). The label diversity can reduce the redundancy in the selection of majority classes, and increase the diversity by including data of minority classes. ", "page_idx": 1, "bbox": [107, 420.1295471191406, 505, 552.134521484375], "page_size": [612.0, 792.0]}
{"layout": 12, "type": "text", "text": "(ii)  Outlier query : Many active querying strategies were proposed to select typical data and eliminate outliers, but they heavily rely on a trained classiﬁer to produce predictions or features. For example, to calculate the value of Entropy, a trained classiﬁer is required to predict logits of the data. However, there is no such classiﬁer at the start of active learning, at which point no labeled data is available for training. To express informative features for reliable predictions, we consider contrastive learning, which can be trained using unlabeled data only. Contrastive learning encourages models to discriminate between data augmented from the same image and data from different images [ 15 ,  13 ]. Such a learning process is called instance discrimination. We hypothesize that instance discrimination can act as an alternative to select typical data and eliminate outliers. Spec i call y, the data that is hard to discriminate from others could be considered as typical data. With the help of Dataset Maps [ 48 ,  $26]^{2}$  , we evaluate this hypothesis and propose a novel active querying strategy that can effectively select  typical data  ( hard-to-contrast  data in our deﬁnition, see   $\\S2.2)$   and reduce outliers. ", "page_idx": 1, "bbox": [107, 556.5185546875, 505, 688.5234985351562], "page_size": [612.0, 792.0]}
{"layout": 13, "type": "text", "text": "Systematic ablation experiments and qualitative visualization s in  $\\S3$   conﬁrm that (i) the level of label diversity and (ii) the inclusion of typical data are two explicit criteria for determining the annotation importance. Naturally, contrastive learning is expected to approximate these two criteria: pseudo- labels in clustering implicitly enforce label diversity in the query; instance discrimination determines typical data. Extensive results show that our initial query not only sign i cant ly outperforms existing active querying strategies, but also surpasses random selection by a large margin on three medical imaging datasets ( i.e . Colon Pathology, Abdominal CT, and Blood Cell Microscope) and two natural imaging datasets ( i.e . CIFAR-10 and CIFAR-10-LT). Our active querying strategy eliminates the need for manual annotation to ensure the label diversity within initial queries, and more importantly, starts the active learning procedure with the typical data. ", "page_idx": 2, "bbox": [107, 72.757568359375, 505, 182.94346618652344], "page_size": [612.0, 792.0]}
{"layout": 14, "type": "text", "text": "To the best of our knowledge, we are among the ﬁrst to indicate and address the cold start problem in the ﬁeld of medical image analysis (and perhaps, computer vision), making three contributions: (1) illustrating the cold start problem in vision active learning, (2) investigating the underlying causes with rigorous empirical analysis and visualization, and (3) determining effective initial queries for the active learning procedure. Our solution to the cold start problem can be used as a strong yet simple baseline to select the initial query for image class i cation and other vision tasks. ", "page_idx": 2, "bbox": [107, 187.32757568359375, 505, 253.8774871826172], "page_size": [612.0, 792.0]}
{"layout": 15, "type": "text", "text": "Related work.  When the cold start problem was ﬁrst observed in recommend er systems, there were several solutions to remedy the in suf cie nt information due to the lack of user history [ 63 ,  23 ]. In natural language processing (NLP), Yuan  et al . [ 55 ] were among the ﬁrst to address the cold start problem by pre-training models using self-supervision. They attributed the cold start problem to model instability and data scarcity. Vision active learning has shown higher performance than random selection [ 61 ,  47 ,  18 ,  2 ,  43 ,  34 ,  62 ], but there is limited study discussing how to select the initial query when facing the entire unlabeled dataset. A few studies somewhat indicated the existence of the cold start problem: Lang  et al . [ 30 ] explored the effectiveness of the  $K$  -center algorithm [ 16 ] to select the initial queries. Similarly, Pourahmadi  et al . [ 38 ] showed that a simple  $K$  -means clustering algorithm worked fairly well at the beginning of active learning, as it was capable of covering diverse classes and selecting a similar number of data per class. Most recently, a series of studies [ 20 ,  54 ,  46 ,  37 ] continued to propose new strategies for selecting the initial query from the entire unlabeled data and highlighted that typical data (deﬁned in varying ways) could sign i cant ly improve the learning efﬁciency of active learning at a low budget. In addition to the existing publications, our study justiﬁes the two causes of the cold start problem, systematically presents the existence of the problem in six dominant strategies, and produces a comprehensive guideline of initial query selection. ", "page_idx": 2, "bbox": [107, 257.90289306640625, 505, 433.9024963378906], "page_size": [612.0, 792.0]}
{"layout": 16, "type": "text", "text": "2 Method ", "text_level": 1, "page_idx": 2, "bbox": [107, 450, 168, 463], "page_size": [612.0, 792.0]}
{"layout": 17, "type": "text", "text": "In this section, we analyze in-depth the cause of cold start problem in two perspectives, biased query as the inter-class query and outlier query as the intra-class factor. We provide a complementary method to select the initial query based on both criteria.   $\\S2.1$   illustrates that label diversity is a favourable selection criterion, and discusses how we obtain label diversity via simple contrastive learning and  $K$  -means algorithms.   $\\S2.2$   describes an unsupervised method to sample atypical (hard-to-contrast) queries from Dataset Maps. ", "page_idx": 2, "bbox": [107, 475.7615661621094, 505, 542.3125], "page_size": [612.0, 792.0]}
{"layout": 18, "type": "text", "text": "2.1 Inter-class Criterion: Enforcing Label Diversity to Mitigate Bias ", "text_level": 1, "page_idx": 2, "bbox": [107, 556, 404, 568], "page_size": [612.0, 792.0]}
{"layout": 19, "type": "text", "text": " $K$  -means clustering.  The selected query should cover data of diverse classes, and ideally, select similar number of data from each class. However, this requires the availability of ground truth, which are inaccessible according to the nature of active learning. Therefore, we exploit pseudo-labels generated by a simple    $K$  -means clustering algorithm and select an equal number of data from each cluster to form the initial query to facilitate label diversity. Without knowledge about the exact number of ground-truth classes, over-clustering is suggested in recent works [ 51 ,  57 ] to increase performances on the datasets with higher intra-class variance. Concretely, given 9, 11, 8 classes in the ground truth, we set    $K$   (the number of clusters) to 30 in our experiments. ", "page_idx": 2, "bbox": [107, 576.4449462890625, 505, 665.1715087890625], "page_size": [612.0, 792.0]}
{"layout": 20, "type": "text", "text": "Contrastive features.    $K$  -means clustering requires features of each data point. Li  et al . [ 31 ] suggested that for the purpose of clustering, contrastive methods ( e.g . MoCo, SimCLR, BYOL) are ", "page_idx": 2, "bbox": [107, 669, 505, 692.469482421875], "page_size": [612.0, 792.0]}
{"layout": 21, "type": "image", "page_idx": 3, "img_path": "layout_images/2210.02442v1_1.jpg", "img_caption": "Figure 2:  Label diversity of querying criteria.  Random, the leftmost strategy, denotes the class distribution of randomly queried samples, which can also reﬂect the approximate class distribution of the entire dataset. As seen, even with a relatively larger initial query budget (40,498 images,  $45\\%$  of the dataset), most active querying strategies are biased towards certain classes in the PathMNIST dataset. For example, VAAL prefers selecting data in the muscle class, but largely ignores data in the mucus and mucosa classes. On the contrary, our querying strategy selects more data from minority classes (e.g., mucus and mucosa) while retaining the class distribution of major classes. Similar observations in Organ AM NIST and BloodMNIST are shown in Appendix Figure 7. The higher the entropy is, the more balanced the class distribution is. ", "bbox": [106, 71, 506, 293], "page_size": [612.0, 792.0], "ocr_text": "Random Consistency VAAL Margin Entropy Coreset BALD Ours\n\nadipose — | —\nbackground | — i a —\ndebris | ee ———\nymphocytes | = — — ——\nmucus | UJ J = —\nmuscle | as a\nmucosa | I = |_| —\nstroma | — —\nepithelium a’ = LL] |_| a\nEntropy 3.154 3.116 2.800 2.858 2.852 3.006 3.094 3.122\n", "vlm_text": "This image is a bar graph depicting the label diversity of different querying strategies on the PathMNIST dataset. Each querying strategy (Random, Consistency, VAAL, Margin, Entropy, Coreset, BALD, and Ours) has a corresponding column showing the class distribution they select for various categories, such as adipose, background, debris, lymphocytes, mucus, muscle, mucosa, stroma, and epithelium.\n\nKey points from the image:\n\n- **Random** strategy shows a more uniform distribution across all classes.\n- **VAAL** has a preference for selecting data in the muscle class, ignoring mucus and mucosa.\n- The **Ours** strategy selects more from minority classes like mucus and mucosa while maintaining the distribution of major classes.\n- Each strategy's entropy is indicated at the bottom, with higher entropy reflecting a more balanced distribution. The values range, with Random having the highest entropy (3.154) and VAAL having the lowest (2.800)."}
{"layout": 22, "type": "text", "text": "more suitable than generative methods ( e.g . color iz ation, reconstruction) because the contrastive feature matrix can be naturally regarded as cluster representations. Therefore, we use MoCo v2 [ 15 ]— a popular self-supervised contrastive method—to extract image features. ", "page_idx": 3, "bbox": [107, 305.758544921875, 505, 339.5815124511719], "page_size": [612.0, 792.0]}
{"layout": 23, "type": "text", "text": " $K$  -means and MoCo v2 are certainly not the only choices for clustering and feature extraction. We employ these two well-received methods for simplicity and efﬁcacy in addressing the cold start problem. Figure 2 shows our querying strategy can yield better label diversity than other six dominant active querying strategies; similar observations are made in Organ AM NIST and BloodMNIST (Figure 7) as well as CIFAR-10 and CIFAR-10-LT (Figure 10). ", "page_idx": 3, "bbox": [107, 343.9655456542969, 505, 399.60650634765625], "page_size": [612.0, 792.0]}
{"layout": 24, "type": "text", "text": "2.2 Intra-class Criterion: Querying Hard-to-Contrast Data to Avoid Outliers ", "text_level": 1, "page_idx": 3, "bbox": [107, 412, 441, 424], "page_size": [612.0, 792.0]}
{"layout": 25, "type": "text", "text": "Dataset map.  Given  $K$   clusters generated from Criterion #1, we now determine which data points ought to be selected from each cluster. Intuitively, a data point can better represent a cluster distribution if it is harder to contrast itself with other data points in this cluster—we consider them typical data. To ﬁnd these typical data, we modify the original Dataset   $\\mathrm{{\\bf~M}a p}^{3}$    by replacing the ground truth term with a pseudo-label term. This modi cation is made because ground truths are unknown in the active learning setting but pseudo-labels are readily accessible from Criterion #1. For a visual comparison, Figure 3b and Figure 3c present the Data Maps based on ground truths and pseudo-labels, respectively. Form y, the modiﬁed Data Map can be formulated a ollows. Let  $\\grave{\\mathcal{D}}=\\{\\pmb{x}_{m}\\}_{m=1}^{M}$  denote a dataset of  M  unlabeled images. Considering a minibatch of  N  images, for each image  ${\\pmb x}_{n}$  , its two augmented views form a positive pair, denoted as  $\\tilde{\\mathbf{x}}_{i}$   and  $\\tilde{\\pmb{x}}_{j}$  . The contrastive prediction task i on pairs of augmented images derived from the minibatch generate    $2N$   images, in which a true label  $y_{n}^{*}$    for an anchor augmentation is associated with its counterpart of the positive pair. We treat the other  $2(N-1)$   augmented images within a minibatch as negative pairs. We deﬁne the probability of positive pair in the instance discrimination task as: ", "page_idx": 3, "bbox": [107, 432.7379150390625, 505, 586.9204711914062], "page_size": [612.0, 792.0]}
{"layout": 26, "type": "equation", "text": "\n$$\np_{i,j}=\\frac{\\exp(\\sin(z_{i},z_{j}))/\\tau}{\\sum_{n=1}^{2N}\\mathbb{1}_{[n\\neq i]}\\exp(\\sin(z_{i},z_{n}))/\\tau},\n$$\n ", "text_format": "latex", "page_idx": 3, "bbox": [220, 592, 390, 622], "page_size": [612.0, 792.0]}
{"layout": 27, "type": "equation", "text": "\n$$\np_{\\theta^{(e)}}(y_{n}^{*}|x_{n})=\\frac{1}{2}[p_{2n-1,2n}+p_{2n,2n-1}],\n$$\n ", "text_format": "latex", "page_idx": 3, "bbox": [222, 635, 388, 658], "page_size": [612.0, 792.0]}
{"layout": 28, "type": "text", "text": "where  $\\sin(\\pmb{u},)=\\pmb{u}^{\\top}\\pmb{v}/||\\pmb{u}||\\|\\pmb{v}\\|$  is the cosine similarit etween    $\\mathbfcal{U}$   and  $\\boldsymbol{v};z_{2n-1}$   and  $z_{2n}$   denote the projection head output of a positive pair for the input  ${\\pmb x}_{n}$   in a batch;    $\\mathbb{1}_{[n\\neq i]}\\in\\{0,1\\}$   is an indicator ", "page_idx": 3, "bbox": [107, 667, 505, 692.6124877929688], "page_size": [612.0, 792.0]}
{"layout": 29, "type": "image", "page_idx": 4, "img_path": "layout_images/2210.02442v1_2.jpg", "img_caption": "Figure 3:  Active querying based on Dataset Maps.  (a) Dataset overview. (b) Easy- and hard-to- learn data can be selected from the maps based on ground truths [ 26 ]. This querying strategy has two limitations: it requires manual annotations and the data are stratiﬁed by classes in the 2D space, leading to a poor label diversity in the selected queries. (c) Easy- and hard-to-contrast data can be selected from the maps based on pseudo-labels. This querying strategy is label-free and the selected hard-to-contrast data represent the most common patterns in the entire dataset, as presented in (a). These data are more suitable for training, and thus alleviate the cold start problem. ", "bbox": [105, 67, 507, 333], "page_size": [612.0, 792.0], "ocr_text": "@ basophil e\n© eosinophil 8\n© erythroblast e\n\n2\ni)\n\nS\na\n\n2°\nFS\n\nconfidence\n\n0.2\n\ner “OY 0.2 0.4\n\nHard-to-learn variability\n(b) Data Map by ground truth\n\nee\noe 4\n\nOverall distribution\n\n(a)\n\nlymphocyte ig\nmonocyte © platelet\nneutrophil\n\n1.0\n\n2 S\na ©\n\nconfidence\n°\nBR\n\n0.2\n\n0.0\n\nvariability\n\nHard-to-contrast\n\n(c) Data Map by pseudo-labels\n", "vlm_text": "This image presents an analysis of a dataset related to blood cells, as indicated by the visual content and the caption text. The image is divided into three main parts:\n\n1. **(a) Overall distribution**: This section shows a visual overview of a dataset, comprising numerous small images of blood cells arranged in a grid. Each cell type is presumably represented by various colors in the second part of the image. The colors relate to different blood cell classes: basophil, eosinophil, erythroblast, lymphocyte, monocyte, neutrophil, and others like Ig and platelet.\n\n2. **(b) Data Map by ground truth**: This part presents a scatter plot categorized by confidence and variability. Each point is color-coded according to the type of blood cell it represents, corresponding to the legend at the top. The plots also highlight segments characterized as \"Easy-to-learn\" and \"Hard-to-learn\" with small cutout images of blood cells representing these categories.\n\n3. **(c) Data Map by pseudo-labels**: Similar to the section (b), this scatter plot visualizes confidence against variability, but here the categorization is based on pseudo-labels instead of ground truths. The plot distinguishes between \"Easy-to-contrast\" and \"Hard-to-contrast\" data points, which are indicated with accompanying sample images of blood cells.\n\nThe analysis utilizes an active querying approach to improve data selection for machine learning training, focusing on distinguishing between data that are easier or harder to learn or contrast. The use of pseudo-labels (as shown in section c) is noted to alleviate the cold start problem by identifying common patterns in the dataset."}
{"layout": 30, "type": "text", "text": "function evaluati to  1  iff  $n\\neq i$   and  $\\tau$   denotes a t rature p ameter.    $\\theta^{(e)}$    denotes the parameters at the end of the  $e^{\\mathrm{\\tilde{th}}}$    epoch. We deﬁne conﬁdence  $\\left(\\hat{\\mu}_{m}\\right)$   across  E  epochs as: ", "page_idx": 4, "bbox": [107, 348.69757080078125, 505, 375.22650146484375], "page_size": [612.0, 792.0]}
{"layout": 31, "type": "equation", "text": "\n$$\n\\hat{\\mu}_{m}=\\frac{1}{E}\\sum_{e=1}^{E}p_{\\theta^{(e)}}(y_{m}^{*}\\vert x_{m}).\n$$\n ", "text_format": "latex", "page_idx": 4, "bbox": [248, 379, 363, 413], "page_size": [612.0, 792.0]}
{"layout": 32, "type": "text", "text": "The conﬁdence    $(\\hat{\\mu}_{m})$   is the Y-axis of the Dataset Maps (see Figure 3b-c). ", "page_idx": 4, "bbox": [107, 416, 399.8734130859375, 428.4554748535156], "page_size": [612.0, 792.0]}
{"layout": 33, "type": "text", "text": "Hard-to-contrast data.  We consider the data with a low conﬁdence value (Equation 3) as “hard-to- contrast” because they are seldom predicted correctly in the instance discrimination task. Apparently, if the model cannot distinguish a data point with others, this data point is expected to carry typical characteristics that are shared across the dataset [ 40 ]. Visually, hard-to-contrast data gather in the bottom region of the Dataset Maps and “easy-to-contrast” data gather in the top region. As expected, hard-to-learn data are more typical, possessing the most common visual patterns as the entire dataset; whereas easy-to-learn data appear like outliers [ 54 ,  26 ], which may not follow the majority data distribution (examples in Figure 3a and Figure 3c). Additionally, we also plot the original Dataset Map [ 12 ,  48 ] in Figure 3b, which grouped data into hard-to-learn and easy-to-learn 4 . Although the results in   $\\S3.2$   show equally compelling performance achieved by both easy-to-learn [ 48 ] and hard-to-contrast data (ours), the latter do not require any manual annotation, and therefore are more practical and suitable for vision active learning. ", "page_idx": 4, "bbox": [107, 432.4798889160156, 505, 564.843505859375], "page_size": [612.0, 792.0]}
{"layout": 34, "type": "text", "text": "In summary, to meet the both criteria, our proposed active querying strategy includes three steps: (i) extracting features by self-supervised contrastive learning, (ii) assigning clusters by  $K$  -means algorithm for label diversity, and (iii) selecting hard-to-contrast data from dataset maps. ", "page_idx": 4, "bbox": [107, 569.2275390625, 505, 603.0504760742188], "page_size": [612.0, 792.0]}
{"layout": 35, "type": "text", "text": "3 Experimental Results ", "text_level": 1, "page_idx": 4, "bbox": [106, 617, 237, 631], "page_size": [612.0, 792.0]}
{"layout": 36, "type": "text", "text": "Datasets   $\\&$   metrics.  Active querying strategies have a selection bias that is particularly harmful in long-tail distributions. Therefore, unlike most existing works [ 38 ,  54 ], which tested on highly balanced annotated datasets, we deliberately examine our method and other baselines on long- tail datasets to simulate real-world scenarios. Three medical datasets of different modalities Table 1:  Diversity is a signiﬁcant add-on to most querying strategies.  AUC scores of different querying strategies are compared on three medical imaging datasets. In either low budget ( i.e .   $0.5\\%$  or   $1\\%$   of MedMNIST datasets) or high budget ( i.e .   $10\\%$   or  $20\\%$   of CIFAR-10-LT) regimes, both random and active querying strategies beneﬁt from enforcing the label diversity of the selected data. The cells are highlighted in blue when adding diversity performs no worse than the original querying strategies. Coreset [ 41 ] works very well as its original form because this querying strategy has implicitly considered the label diversity (also veriﬁed in Table 2) by formulating a  $K$  -center problem, which selects    $K$   data points to represent the entire dataset. Some results are missing (marked as “-”) because the querying strategy fails to sample at least one data point for each class. Results of more sampling ratios are presented in Appendix Figures 6, 9. ", "page_idx": 4, "bbox": [107, 641.5599365234375, 505, 686.6505126953125], "page_size": [612.0, 792.0]}
{"layout": 37, "type": "text", "text": "", "page_idx": 5, "bbox": [107, 70.07189178466797, 505, 180.6174774169922], "page_size": [612.0, 792.0]}
{"layout": 38, "type": "table", "page_idx": 5, "img_path": "layout_images/2210.02442v1_3.jpg", "bbox": [107, 185, 507, 332], "page_size": [612.0, 792.0], "ocr_text": "PathMNIST OrganAMNIST BloodMNIST CIFAR-10-LT\n0.5% 1% 0.5% 1% 0.5% 1% 10% 20%\nUnit. (499) (899) (172) (345) (59) (119) (1420) (2841)\nRendon 7 96.840.6 97.6406 | 91.1409 933404 | 947407 965404 | O16E11 93.1406\n96.4413 97.640.9 | 90.7411 93.140.7 | 93.2415 95.8407 | 62.0+46.1 Z\nConsistency” 9640.1 979E0.1 | 923405 928410 | 929409 959405 | OI4EL1 934402\ni 96.2+0.0 _97.6+0.0 | 91.0+0.3  94.0+0.6 | 87.9402 _95.5+0.5 | 67.1417.1 88.6403\nVAAL 7 92.740.5 93.0406 | 70.641.9 84.640.5 | 898413 93440.9 | 926402 93.7404\nMasih F 9790.2 96.0404 | 818412 858414 | 89.7419 94.7407 | 917E09 93.2402\n8 91.042.3  96.0-40.3 z 85.9+40.7 - : 81.9+40.8 86.3403\nhue 7 93.2416 95.240.2 | 791423 86.7408 | 859405 S18E10 | 920412 919413\nPY : 87.5+0.1 : 2 : E 65.6415.6  86.4+0.2\nCoreset 7 95.0422 948425 | 856404 8990.5 | 885406 S41E11 | D1SH04 93.6402\n‘ 95.640.7 _97.540.2 | 83.8+0.6  88.5+0.4 | 87.3416 94.0412 | 65.9415.9  86.9+0.1\na 4 9580.2 97.0401 | 87.2403 89.2403 | 899408 927407 | 928E01 908424\n92.0423 95.3+1.0 = 5 83.342.2 93.5413 | 6494149  84.7+0.6\n\n", "vlm_text": "The table presents the results of various active learning strategies applied to different datasets with varying labeled data percentages. The datasets are PathMNIST, OrganAMNIST, BloodMNIST, and CIFAR-10-LT, each with different percentages of labeled data (e.g., 0.5%, 1%, 10%, and 20%). \n\nThe column \"Unif.\" indicates whether a uniform distribution was used in the selection strategy (✓ for yes, ✗ for no). Active learning strategies listed are: Random, Consistency, VAAL, Margin, Entropy, Coreset, and BALD.\n\nEach cell in the table provides the accuracy results (mean ± standard deviation) for each strategy on each dataset and labeled data percentage. Some cells are highlighted in blue, indicating either better performance or a unique characteristic/benchmark result according to the table's context. The numbers in parentheses next to the percentage of data indicate the number of samples used for training (e.g., 499 for 0.5% of PathMNIST).\n\nOverall, the table displays and compares the effectiveness of each strategy for different datasets and conditions, offering insights into their relative performance."}
{"layout": 39, "type": "text", "text": "Table 2:  Class coverage of selected data.  Compared with random selection (i.i.d. to entire data distribution), most active querying strategies contain selection bias to speciﬁc classes, so the class coverage in their selections might be poor, particularly using low budgets. As seen, using  $0.002\\%$   or even smaller proportion of MedMNIST datasets, the class coverage of active querying strategies is much lower than random selection. By integrating  $K$  -means clustering with contrastive features, our querying strategy is capable of covering   $100\\%$   classes in most scenarios using low budgets   $(\\leq\\!0.002\\%$  of MedMNIST). We also found that our querying strategy covers the most of the classes in the CIFAR-10-LT dataset, which is designated ly more imbalanced. ", "page_idx": 5, "bbox": [107, 357.8229064941406, 505, 446.5505065917969], "page_size": [612.0, 792.0]}
{"layout": 40, "type": "table", "page_idx": 5, "img_path": "layout_images/2210.02442v1_4.jpg", "bbox": [108, 452, 509, 548], "page_size": [612.0, 792.0], "ocr_text": "PathMNIST OrganAMNIST BloodMNIST CIFAR-10-LT\n\n0.00015% — 0.00030% 0.001% 0.002% 0.001% 0.002% 0.2% 0.3%\n(13) (26) (34) (69) (dl) (23) (24) (37)\n\nRandom 0.79+0.11 — 0.950.07 | 0.914£0.08 — 0.98+0.04 | 0.70+0.13 _0.94+0.08 | 0.58+0.10  0.6640.12\nConsistency 0.78 0.88 0.82 0.91 0.75 0.88 0.50 0.70\nVAAL 0.11 0.11 0.18 0.18 0.13 0.13 0.30 0.30\nMargin 0.67 0.78 0.73 0.82 0.63 0.75 0.60 0.70\nEntropy 0.33 0.33 0.45 0.73 0.63 0.40 0.70\nCoreset 0.66 0.78 0.91 1.00 0.88 0.60 0.70\nBALD 0.33 0.44 0.64 0.64 0.88 0.60 0.70\nOurs 0.78 1.00 1.00 1.00 1.00 0.70 0.80\n\n", "vlm_text": "The table presents performance comparisons across different datasets and methods. It includes four datasets: PathMNIST, OrganAMNIST, BloodMNIST, and CIFAR-10-LT. Each dataset is associated with two sampling rates, represented as percentages with the exact number of samples in parentheses. The methods compared are grouped vertically as \"Random,\" \"Consistency,\" \"VAAL,\" \"Margin,\" \"Entropy,\" \"Coreset,\" \"BALD,\" and \"Ours.\"\n\nFor each combination of dataset and sampling rate, the table provides the results of these methods in terms of their performance metric, which seems to be accuracy or a similar measure. The \"Random\" method includes an average and standard deviation displayed in a \"mean ± standard deviation\" format.\n\n- For PathMNIST and the smallest sampling rate (0.00015%), \"Random\" performs at 0.79 ± 0.11, while \"Ours,\" \"Consistency,\" \"Margin,\" and \"Coreset\" all achieved 0.78.\n- At a sampling rate of 0.00030% for PathMNIST, \"Ours,\" \"Random,\" and several other methods reach a performance of 1.00.\n- In OrganAMNIST, \"Random\" scores 0.91 ± 0.08 and 0.98 ± 0.04 for the two sampling rates, with \"Ours\" reaching a perfect score of 1.00 in both cases.\n- BloodMNIST shows \"Random\" at 0.70 ± 0.13 and 0.94 ± 0.08, with \"Ours\" again achieving 1.00 for both sampling rates.\n- For CIFAR-10-LT, \"Random\" has performance metrics of 0.58 ± 0.10 and 0.66 ± 0.12. The \"Ours\" method performs at 0.70 and 0.80, respectively.\n\nThe \"Ours\" method consistently reaches 1.00 accuracy or the highest performance across all medical image datasets and performs better than or equal to other methods in the CIFAR-10-LT dataset."}
{"layout": 41, "type": "text", "text": "in MedMNIST [ 53 ] are used: PathMNIST (colorectal cancer tissue his to pathological images), BloodMNIST (microscopic peripheral blood cell images), Organ AM NIST (axial view abdominal CT images of multiple organs). Organ AM NIST is augmented following Azizi  et al . [ 3 ], while the others following Chen  et al . [ 15 ]. Area Under the ROC Curve (AUC) and Accuracy are used as the evaluation metrics. All results were based on at least three independent runs, and particularly, 100 independent runs for random selection. UMAP [35] is used to analyze feature clustering results. ", "page_idx": 5, "bbox": [107, 585.3155517578125, 505, 651.865478515625], "page_size": [612.0, 792.0]}
{"layout": 42, "type": "text", "text": "Baselines & implementations.  We benchmark a total of seven querying strategies: (1) random selection, (2) Max-Entropy [ 52 ], (3) Margin [ 4 ], (4) Consistency [ 18 ], (5) BALD [ 28 ], (6) VAAL [ 45 ], and (7) Coreset [ 41 ]. For contrastive learning, we trained 200 epochs with MoCo v2, following its default hyper parameter settings. We set  $\\tau$   to 0.05 in equation 2. To reproduce the large batch size and iteration numbers in [ 13 ], we apply repeated augmentation [ 21 ,  49 ,  50 ] (detailed in Table 5). More baseline and implementation details can be found in Appendix A. ", "page_idx": 5, "bbox": [107, 655.8909301757812, 505, 722.7994995117188], "page_size": [612.0, 792.0]}
{"layout": 43, "type": "image", "page_idx": 6, "img_path": "layout_images/2210.02442v1_5.jpg", "img_caption": "Figure 4:  Quantitative comparison of map-based querying strategies.  Random selection (dot- lines) can be treated as a highly competitive baseline in cold start because it outperforms six popular active querying strategies as shown in Figure 1. In comparison with random selection and three other querying strategies, hard-to-contrast performs the best. Although easy-to-learn and hard-to-learn sometimes performs similarly to hard-to-contrast, their selection processes require ground truths [ 26 ], which are not available in the setting of active learning. ", "bbox": [106, 70, 506, 266], "page_size": [612.0, 792.0], "ocr_text": "AUC\n\n89 images 179 images\n(a) PathMNIST\n\nOD Easy-to-learn\n1.9\n\n0.9)\n\n0.8)\n\nAUC\n\n0.7|\n\n0.6|\n\nos!\n\n34images 69 images\n(b) OrganAMNIST\n\nllimages 23 images\n(c) BloodMNIST\n\nBB Hard-to-learn Bi Easy-to-contrast WB Hard-to-contrast\n\n1.9 1.0\n0.9| 0.9\n0.8 0.8\n\nv v\n\n5 5\n\n< <\n0.7 0.7\n0.6| 0.6\n0.5 0.5\n\n2481 images 3721 images\n(d) CIFAR-10-LT\n", "vlm_text": "The image is a bar chart comparing map-based querying strategies across four datasets: PathMNIST, OrganAMNIST, BloodMNIST, and CIFAR-10-LT. It evaluates the performance using the AUC (Area Under the Curve) metric.\n\n1. **PathMNIST (89 and 179 images):**\n   - Strategies compared: Easy-to-learn, Hard-to-learn, Easy-to-contrast, Hard-to-contrast\n   - Hard-to-contrast performs best, followed by Easy-to-learn.\n\n2. **OrganAMNIST (34 and 69 images):**\n   - Hard-to-contrast shows the highest AUC, followed closely by Easy-to-learn and Easy-to-contrast.\n\n3. **BloodMNIST (11 and 23 images):**\n   - Hard-to-contrast is the top performer, with Easy-to-learn performing nearly as well.\n\n4. **CIFAR-10-LT (2481 and 3721 images):**\n   - Hard-to-contrast leads slightly, with Easy-to-contrast and Easy-to-learn also showing strong performance.\n\nOverall, the “hard-to-contrast” strategy tends to outperform others across different datasets. The caption suggests that while “easy-to-learn” and “hard-to-learn” strategies sometimes perform similarly to “hard-to-contrast,” they require ground truths, which are not always available in active learning scenarios."}
{"layout": 44, "type": "text", "text": "3.1 Contrastive Features Enable Label Diversity to Mitigate Bias ", "text_level": 1, "page_idx": 6, "bbox": [107, 288, 390, 299], "page_size": [612.0, 792.0]}
{"layout": 45, "type": "text", "text": "Label coverage   $\\&$   diversity.  Most active querying strategies have selection bias towards speciﬁc classes, thus the class coverage in their selections might be poor (see Table 2), particularly at low budgets. By simply enforcing label diversity to these querying strategies can sign i cant ly improve the performance (see Table 1), which suggests that the label diversity is one of the causes that existing active querying strategies perform poorer than random selection. ", "page_idx": 6, "bbox": [107, 308, 505, 364.0845031738281], "page_size": [612.0, 792.0]}
{"layout": 46, "type": "text", "text": "Our proposed active querying strategy, however, is capable of covering   $100\\%$   classes in most low budget scenarios (  ${\\leq}0.002\\%$   of full dataset) by integraing    $K$  -means clustering with contrastive features. ", "page_idx": 6, "bbox": [107, 368.46856689453125, 505, 402.29150390625], "page_size": [612.0, 792.0]}
{"layout": 47, "type": "text", "text": "3.2 Pseudo-labels Query Hard-to-Contrast Data and Avoid Outliers ", "text_level": 1, "page_idx": 6, "bbox": [107, 416, 402, 427], "page_size": [612.0, 792.0]}
{"layout": 48, "type": "text", "text": "Hard-to-contrast data are practical for cold start problem.  Figure 4 presents the quantitative comparison of four map-based querying strategies, wherein easy- or hard-to-learn are selected by the maps based on ground truths, easy- or hard-to-contrast are selected by the maps based on pseudo- labels. Note that easy- or hard-to-learn are enforced with label diversity, due to their class-stratiﬁed distributions in the projected 2D space (illustrated in Figure 3). Results suggest that  selecting easy-to-learn or hard-to-contrast data contribute to the optimal models . In any case, easy- or hard-to- learn data can not be selected without knowing ground truths, so these querying strategies are not practical for active learning procedure. Selecting hard-to-contrast, on the other hand, is a label-free strategy and yields the highest performance amongst existing active querying strategies (reviewed in Figure 1). More importantly, hard-to-contrast querying strategy sign i cant ly outperforms random sele b  $1.8\\%$   $(94.14\\%{\\pm}1.0\\%$   $92.27\\%{\\pm}2.2\\%)$  ),   $2.6\\%$   (  $84.35\\%{\\pm}0.7\\%$   vs.   $81.75\\%\\pm2.1\\%)$  ), and 5.2% (88.51%  $88.51\\%{\\pm}1.5\\%$  ±  $83.36\\%{\\pm}3.5\\%)$  ± 3.5%) on PathMNIST, Organ AM NIST, and BloodMNIST, respectively, by querying 0.1% of entire dataset. Similarly on CIFAR-10-LT, hard-to-contrast perf lection by   $21.2\\%$   $87.35\\%{\\pm}0.0\\%$   vs.  $66.12\\%{\\pm}0.9\\%)$   and  $24.1\\%$   $(90.59\\%{\\pm}0.1\\%$  ± 0.1% vs. 66.53%  $66.53\\%{\\pm}0.5\\%)$  ± 0.5%) by querying 20% and 30% of entire dataset respectively. Note that easy- or hard-to-learn are not enforced with label diversity, for a more informative comparison. ", "page_idx": 6, "bbox": [107, 436.0079345703125, 505, 612.0075073242188], "page_size": [612.0, 792.0]}
{"layout": 49, "type": "text", "text": "3.3 On the Importance of Selecting Superior Initial Query ", "text_level": 1, "page_idx": 6, "bbox": [107, 625, 361, 637], "page_size": [612.0, 792.0]}
{"layout": 50, "type": "text", "text": "A good start foresees improved active learning.  We stress the importance of the cold start problem in vision active learning by conducting correlation analysis. Starting with 20 labeled images as the initial query, the training set is increased by 10 more images in each active learning cycle. Figure 14a presents the performance along the active learning (each point in the curve accounts for 5 independent trials). The initial query is selected by a total of 9 different strategies 5 , and subsequent queries are ", "page_idx": 6, "bbox": [107, 645.723876953125, 505, 701.7235107421875], "page_size": [612.0, 792.0]}
{"layout": 51, "type": "image", "page_idx": 7, "img_path": "layout_images/2210.02442v1_6.jpg", "img_caption": "(b) Fine-tuning from self-supervised pre-training ", "bbox": [111, 74, 501, 254], "page_size": [612.0, 792.0], "ocr_text": "Random Entropy Margin BALD Coreset\n1005 1005 1005 1005 1007\nv4 904 904 904 904\nes Be ne ef ao\n3 GB\n2 704 704 70+ 70+ 704\n60+ 604 604 604 604\n0 20 30 40 50 60 10 2 30 40 50 60 10 20 30 40 50 60 10 20 30 40 50 60 20 30 40 50 60\n(a) Training from scratch\n1007 1007 1007 1007 1007\n904 so 904 so 904\nsof og al eee wl 3a oi] eee ”\n3\n2 704 704 704 704 704\n604 60 «04 60 604\n80-4] 80-4 8 tt 4 80 1 4\n10 2 30 40 50 60 10 20 30 40 50 60 10 20 30 40 50 60 10 2 30 40 50 60 10 20 30 40 50 60\n\n# of Labeled Images\n\n# of Labeled Images\n\n# of Labeled Images\n\n# of Labeled Images\n\n# of Labeled Images\n\nte eotet ee\n\nHard-to-Contras\nEasy-to-Contras\nEasy-to-Leam\nConsistency\nEntropy\n\nMargin\n\nBALD\n\nVAAL\n\nCoreset\n", "vlm_text": "The image consists of ten graphs that compare different methods in terms of Area Under the Curve (AUC) percentage with varying numbers of labeled images. It is divided into two rows:\n\n1. **Top Row** - Captioned as \"(a) Training from scratch,\" it includes graphs for different selection strategies such as Random, Entropy, Margin, BALD, and Coreset.\n\n2. **Bottom Row** - Captioned as \"(b) Fine-tuning from self-supervised pre-training,\" it shows the same strategies as the top row but differs in the pre-training approach.\n\nEach graph shows AUC (%) on the y-axis and the number of labeled images on the x-axis, comparing several methods like Hard-to-Contrast, Easy-to-Contrast, and others. Each method is represented by different line styles and symbols. The red line (Hard-to-Contrast) generally appears to outperform the other methods across the graphs."}
{"layout": 52, "type": "text", "text": "Figure 5:  On the importance of selecting a superior initial query.  Hard-to-contrast data (red lines) outperform other initial queries in every cycle of active learning on Organ aM NIST. We ﬁnd that the performance of the initial cycle (20 images) and the last cycle (50 images) are strongly correlated. ", "page_idx": 7, "bbox": [107, 263.96990966796875, 505, 298.1524963378906], "page_size": [612.0, 792.0]}
{"layout": 53, "type": "text", "text": "selected by 5 different strategies.  $\\operatorname{succ}_{n}$   denotes the AUC score achieved by the model that is trained by    $n$   labeled images. The Pearson correlation coefﬁcient between   $\\mathrm{AUC_{20}}$   (starting) and   $\\mathrm{AUC_{50}}$  (ending) shows strong positive correlation   $r=0.79$  , 0.80, 0.91, 0.67, 0.92 for random selection, Entropy, Margin, BALD, and Coreset, respectively). This result is statistically signiﬁcant   $\\mathscr{p}<\n\n$  0.05). Hard-to-contrast data (our proposal) consistently outperforms the others on Organ AM NIST\n\n (Figure 5), BloodMNIST (Figure 13), and PathMNIST (Figure 14), and steadily improves the model performances within the next active learning cycles. ", "page_idx": 7, "bbox": [107, 327.72454833984375, 505, 405.1835021972656], "page_size": [612.0, 792.0]}
{"layout": 54, "type": "text", "text": "The initial query is consequential regardless of model initialization.  A pre-trained model can improve the performance of each active learning cycle for both random and active selection [ 55 ], but the cold start problem remains (evidenced in Figure 14b). This suggests that the model instability and data scarcity are two independent issues to be addressed for the cold start problem. Our “hard-to- contrast” data selection criterion only exploits contrastive learning (an improved model), but also determines the typical data to be annotated ﬁrst (a better query). As a result, when ﬁne-tuning from MoCo v2, the Pearson correlation coefﬁcient between  $\\mathrm{AUC_{20}}$   and   $\\mathrm{AUC_{50}}$   remains high (  $r=0.92$  , 0.81, 0.70, 0.82, 0.85 for random selection, Entropy, Margin, BALD, and Coreset, respectively) and statistically signiﬁcant   $(p<0.05)$  . ", "page_idx": 7, "bbox": [107, 409.20892333984375, 505, 509], "page_size": [612.0, 792.0]}
{"layout": 55, "type": "text", "text": "4 Conclusion ", "text_level": 1, "page_idx": 7, "bbox": [107, 533, 184, 546], "page_size": [612.0, 792.0]}
{"layout": 56, "type": "text", "text": "This paper systematically examines the causes of the cold start problem in vision active learning and offers a practical and effective solution to address this problem. Analytical results indicate that (1) the level of label diversity and (2) the inclusion of hard-to-contrast data are two explicit criteria to determine the annotation importance. To this end, we devise a novel active querying strategy that can enforce label diversity and determine hard-to-contrast data. The results of three medical imaging and two natural imaging datasets show that our initial query not only sign i cant ly outperforms existing active querying strategies but also surpasses random selection by a large margin. This ﬁnding is signiﬁcant because it is the ﬁrst few choices that deﬁne the efﬁcacy and efﬁciency of the subsequent learning procedure. We foresee our solution to the cold start problem as a simple, yet strong, baseline to sample the initial query for active learning in image class i cation. ", "page_idx": 7, "bbox": [107, 563.49658203125, 505, 673.6835327148438], "page_size": [612.0, 792.0]}
{"layout": 57, "type": "text", "text": "Limitation.  This study provides an empirical benchmark of initial queries in active learning, while more theoretical analyses can be provided. Yehuda  et al . [ 54 ] also found that the choice of active learning strategies depends on the initial query budget. A challenge is to articulate the quantity of determining active learning strategies, which we leave for future work. ", "page_idx": 7, "bbox": [107, 677.7089233398438, 505, 722.7994995117188], "page_size": [612.0, 792.0]}
{"layout": 58, "type": "text", "text": "Potential societal impacts.  Real-world data often exhibit long-tailed distributions, rather than the ideal uniform distributions over each class. We improve active learning by enforcing label diversity and hard-to-contrast data. However, we only extensively test our strategies on academic datasets. In many other real-world domains such as robotics and autonomous driving, the data may impose additional constraints on annotation accessibility or learning dynamics, e.g., being fair or private. We focus on standard accuracy and AUC as our evaluation metrics while ignoring other ethical issues in imbalanced data, especially in underrepresented minority classes. ", "page_idx": 8, "bbox": [107, 72.39891815185547, 506, 150.2164764404297], "page_size": [612.0, 792.0]}
{"layout": 59, "type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 8, "bbox": [107, 166, 209, 179], "page_size": [612.0, 792.0]}
{"layout": 60, "type": "text", "text": "This work was supported by the Lustgarten Foundation for Pancreatic Cancer Research. The authors want to thank Mingfei Gao for the discussion of initial query quantity and suggestions on the implementation of consistency-based active learning framework. The authors also want to thank Guy Hacohen, Yuanhan Zhang, Akshay L. Chandra, Jingkang Yang, Hao Cheng, Rongkai Zhang, and Junfei Xiao, for their feedback and constructive suggestions at several stages of the project. Computational resources were provided by Machine Learning and Data Analytics Laboratory, Nanyang Technological University. The authors thank the administrator Sung Kheng Yeo for his technical support.\n\n ", "page_idx": 8, "bbox": [107, 190.64154052734375, 506, 279.009521484375], "page_size": [612.0, 792.0]}
{"layout": 61, "type": "text", "text": "References ", "text_level": 1, "page_idx": 8, "bbox": [107, 293, 165, 307], "page_size": [612.0, 792.0]}
{"layout": 62, "type": "text", "text": "[1]  Andrea Acevedo, Anna Merino, Santiago Alférez, Ángel Molina, Laura Boldú, and José Rodellar. A dataset of microscopic peripheral blood cell images for development of automatic recognition systems. Data in Brief , 30, 2020.\n\n [2]  Sharat Agarwal, Himanshu Arora, Saket Anand, and Chetan Arora. Contextual diversity for active learning. ArXiv , abs/2008.05723, 2020.\n\n [3]  Shekoofeh Azizi, Basil Mustafa, Fiona Ryan, Zachary Beaver, Jan Freyberg, Jonathan Deaton, Aaron Loh, Alan Kart hikes a lingam, Simon Kornblith, Ting Chen, et al. Big self-supervised models advance medical image class i cation.  arXiv preprint arXiv:2101.05224 , 2021.\n\n [4]  Maria-Florina Balcan, Andrei Broder, and Tong Zhang. Margin based active learning. In  International Conference on Computational Learning Theory , pages 35–50. Springer, 2007.\n\n [5]  Javad Zolfaghari Bengar, Joost van de Weijer, Bartlomiej Twardowski, and Bogdan Raducanu. Reducing label effort: Self-supervised meets active learning. In  Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 1631–1639, 2021.\n\n [6]  Yoshua Bengio, Jérôme Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In  ICML ’09 , 2009.\n\n [7]  Maxim Berman, Hervé Jégou, Andrea Vedaldi, Iasonas Kokkinos, and Matthijs Douze. Multigrain: a uniﬁed image embedding for classes and instances.  ArXiv , abs/1902.05509, 2019.\n\n [8]  Patrick Bilic, Patrick Ferdinand Christ, Eugene Vorontsov, Grzegorz Chlebus, Hao Chen, Qi Dou, Chi-Wing Fu, Xiao Han, Pheng-Ann Heng, Jürgen Hesser, et al. The liver tumor segmentation benchmark (lits). arXiv preprint arXiv:1901.04056 , 2019.\n\n [9]  Jesús Bobadilla, Fernando Ortega, Antonio Hernando, and Jesús Bernal. A collaborative ﬁltering approach to mitigate the new user cold start problem.  Knowledge-based systems , 26:225–238, 2012.\n\n [10]  Alexander Borisov, Eugene Tuv, and George Runger. Active batch learning with stochastic query by forest. In  JMLR: Workshop and Conference Proceedings (2010) . Citeseer, 2010.\n\n [11]  Akshay L Chandra, Sai Vikas Desai, Chaitanya Deva gupta pu, and Vineeth N Bala subramania n. On initial pools for deep active learning. In  NeurIPS 2020 Workshop on Pre-registration in Machine Learning , pages 14–32. PMLR, 2021.\n\n [12]  Haw-Shiuan Chang, Erik Learned-Miller, and Andrew McCallum. Active bias: Training more accurate neural networks by emphasizing high variance samples.  Advances in Neural Information Processing Systems , 30, 2017.\n\n [13]  Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations.  arXiv preprint arXiv:2002.05709 , 2020.\n\n [14]  Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Moco demo: Cifar-10. https: //colab.research.google.com/github/facebook research/moco/blob/colab-notebook/ colab/mo co ci far 10 demo.ipynb . Accessed: 2022-05-26. ", "page_idx": 8, "bbox": [107, 314.51605224609375, 506, 723.2189331054688], "page_size": [612.0, 792.0]}
{"layout": 63, "type": "text", "text": "[15]  Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum contrastive learning.  arXiv preprint arXiv:2003.04297 , 2020.\n\n [16]  Reza Zanjirani Farahani and Masoud Hekmatfar.  Facility location: concepts, models, algorithms and case studies . Springer Science & Business Media, 2009.\n\n [17]  Yarin Gal, Riashat Islam, and Zoubin Ghahramani. Deep bayesian active learning with image data. In International Conference on Machine Learning , pages 1183–1192. PMLR, 2017.\n\n [18]  Mingfei Gao, Zizhao Zhang, Guo Yu, Sercan Ö Arık, Larry S Davis, and Tomas Pﬁster. Consistency-based semi-supervised active learning: Towards minimizing labeling cost. In  European Conference on Computer Vision , pages 510–526. Springer, 2020.\n\n [19]  Priya Goyal, Dhruv Mahajan, Abhinav Gupta, and Ishan Misra. Scaling and benchmarking self-supervised visual representation learning. In  Proceedings of the IEEE International Conference on Computer Vision , pages 6391–6400, 2019.\n\n [20]  Guy Hacohen, Avihu Dekel, and Daphna Weinshall. Active learning on a budget: Opposite strategies suit high and low budgets.  ArXiv , abs/2202.02794, 2022.\n\n [21]  Elad Hoffer, Tal Ben-Nun, Itay Hubara, Niv Giladi, Torsten Hoeﬂer, and Daniel Soudry. Augment your batch: Improving generalization through instance repetition.  2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pages 8126–8135, 2020.\n\n [22] Alex Holub, Pietro Perona, and Michael C Burl. Entropy-based active learning for object recognition. In 2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops , pages 1–8. IEEE, 2008.\n\n [23]  Neil Houlsby, José Miguel Hernández-Lobato, and Zoubin Ghahramani. Cold-start active learning with robust ordinal matrix factorization. In  International conference on machine learning , pages 766–774. PMLR, 2014.\n\n [24]  Siyu Huang, Tianyang Wang, Haoyi Xiong, Jun Huan, and Dejing Dou. Semi-supervised active learning with temporal output discrepancy. In  Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 3447–3456, 2021.\n\n [25]  Shruti Jadon. Covid-19 detection from scarce chest x-ray image data using few-shot deep learning approach. In  Medical Imaging 2021: Imaging Informatics for Healthcare, Research, and Applications , volume 11601, page 116010X. International Society for Optics and Photonics, 2021.\n\n [26]  Siddharth Karamcheti, Ranjay Krishna, Li Fei-Fei, and Christopher D Manning. Mind your outliers! investigating the negative impact of outliers on active learning for visual question answering.  arXiv preprint arXiv:2107.02331 , 2021.\n\n [27]  Jakob Nikolas Kather, Johannes Krisam, Pornpimol Char oen tong, Tom Luedde, Esther Herpel, Cleo-Aron Weis, Timo Gaiser, Alexander Marx, Nektarios A. Valous, Dyke Ferber, Lina Jansen, Constantin o Carlos Reyes-Aldasoro, Inka Zörnig, Dirk Jäger, Hermann Brenner, Jenny Chang-Claude, Michael Hoff meister, and Niels Halama. Predicting survival from colorectal cancer histology slides using deep learning: A retrospective multi center study.  PLoS Medicine , 16, 2019.\n\n [28]  Andreas Kirsch, Joost Van Amersfoort, and Yarin Gal. Batchbald: Efﬁcient and diverse batch acquisition for deep bayesian active learning.  Advances in neural information processing systems , 32, 2019.\n\n [29] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.\n\n [30]  Adrian Lang, Christoph Mayer, and Radu Timofte. Best practices in pool-based active learning for image class i cation. 2021.\n\n [31]  Yunfan Li, Peng Hu, Zitao Liu, Dezhong Peng, Joey Tianyi Zhou, and Xi Peng. Contrastive clustering. In 2021 AAAI Conference on Artiﬁcial Intelligence (AAAI) , 2021.\n\n [32]  Geert Litjens, Thijs Kooi, Babak Ehteshami Bejnordi, Arnaud Arindra Adiyoso Setio, Francesco Ciompi, Mohsen Ghafoorian, Jeroen Awm Van Der Laak, Bram Van Ginneken, and Clara I Sánchez. A survey on deep learning in medical image analysis.  Medical image analysis , 42:60–88, 2017.\n\n [33]  Katerina Margatina, Loic Barrault, and Nikolaos Aletras. Bayesian active learning with pretrained language models.  arXiv preprint arXiv:2104.08320 , 2021.\n\n [34]  Christoph Mayer and Radu Timofte. Adversarial sampling for active learning. In  Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision , pages 3071–3079, 2020.\n\n [35]  Leland McInnes, John Healy, and James Melville. Umap: Uniform manifold approximation and projection for dimension reduction.  arXiv preprint arXiv:1802.03426 , 2018.\n\n [36]  Sudhanshu Mittal, Maxim Tatar chen ko, Özgün Çiçek, and Thomas Brox. Parting with illusions about deep active learning.  arXiv preprint arXiv:1912.05361 , 2019. ", "page_idx": 9, "bbox": [107, 73.67805480957031, 505, 722.51953125], "page_size": [612.0, 792.0]}
{"layout": 64, "type": "text", "text": "[37]  Vishwesh Nath, Dong Yang, Holger R Roth, and Daguang Xu. Warm start active learning with proxy labels and selection via semi-supervised ﬁne-tuning. In  International Conference on Medical Image Computing and Computer-Assisted Intervention , pages 297–308. Springer, 2022.\n\n [38]  Kossar Pourahmadi, Parsa Noor a line j ad, and Hamed Pirsiavash. A simple baseline for low-budget active learning.  arXiv preprint arXiv:2110.12033 , 2021.\n\n [39]  Tian Qiu, Guang Chen, Zi-Ke Zhang, and Tao Zhou. An item-oriented recommendation algorithm on cold-start problem.  EPL (Euro physics Letters) , 95(5):58003, 2011.\n\n [40]  Joshua Robinson, Ching-Yao Chuang, Suvrit Sra, and Stefanie Jegelka. Contrastive learning with hard negative samples.  arXiv preprint arXiv:2010.04592 , 2020.\n\n [41]  Ozan Sener and Silvio Savarese. Active learning for convolutional neural networks: A core-set approach. arXiv preprint arXiv:1708.00489 , 2017.\n\n [42] Burr Settles. Active learning literature survey. 2009.\n\n [43]  Changjian Shui, Fan Zhou, Christian Gagné, and Boyu Wang. Deep active learning: Uniﬁed and principled method for query and training. In  International Conference on Artiﬁcial Intelligence and Statistics , pages 1308–1318. PMLR, 2020.\n\n [44]  Oriane Siméoni, Mateusz Budnik, Yannis Avrithis, and Guillaume Gravier. Rethinking deep active learning: Using unlabeled data at model training. In  2020 25th International Conference on Pattern Recognition (ICPR) , pages 1220–1227. IEEE, 2021.\n\n [45]  Samarth Sinha, Sayna Ebrahimi, and Trevor Darrell. Variation al adversarial active learning. In  Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 5972–5981, 2019.\n\n [46]  Ben Sorscher, Robert Geirhos, Shashank Shekhar, Surya Ganguli, and Ari S Morcos. Beyond neural scaling laws: beating power law scaling via data pruning.  arXiv preprint arXiv:2206.14486 , 2022.\n\n [47]  Jamshid Sourati, Ali Gholipour, Jennifer G Dy, Xavier Tomas-Fernandez, Sila Kurugol, and Simon K Warﬁeld. Intelligent labeling based on ﬁsher information for medical image segmentation using deep learning.  IEEE transactions on medical imaging , 38(11):2642–2653, 2019.\n\n [48]  Swabha S way am dip ta, Roy Schwartz, Nicholas Lourie, Yizhong Wang, Hannaneh Hajishirzi, Noah A Smith, and Yejin Choi. Dataset cartography: Mapping and diagnosing datasets with training dynamics. arXiv preprint arXiv:2009.10795 , 2020.\n\n [49]  Zhan Tong, Yibing Song, Jue Wang, and Limin Wang. Videomae: Masked auto encoders are data-efﬁcient learners for self-supervised video pre-training.  arXiv preprint arXiv:2203.12602 , 2022.\n\n [50]  Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre S a blay roll es, and Herv’e J’egou. Training data-efﬁcient image transformers & distillation through attention. In  ICML , 2021.\n\n [51]  Wouter Van Gansbeke, Simon Vanden he nde, Stamatios Georgoulis, Marc Proesmans, and Luc Van Gool. Scan: Learning to classify images without labels. In  European Conference on Computer Vision , pages 268–285. Springer, 2020.\n\n [52]  Dan Wang and Yi Shang. A new active labeling method for deep learning. In  2014 International joint conference on neural networks (IJCNN) , pages 112–119. IEEE, 2014.\n\n [53]  Jiancheng Yang, Rui Shi, Donglai Wei, Zequan Liu, Lin Zhao, Bilian Ke, Hanspeter Pﬁster, and Bingbing Ni. Medmnist v2: A large-scale lightweight benchmark for 2d and 3d biomedical image class i cation. arXiv preprint arXiv:2110.14795 , 2021.\n\n [54]  Ofer Yehuda, Avihu Dekel, Guy Hacohen, and Daphna Weinshall. Active learning through a covering lens. arXiv preprint arXiv:2205.11320 , 2022.\n\n [55]  Michelle Yuan, Hsuan-Tien Lin, and Jordan Boyd-Graber. Cold-start active learning through self-supervised language modeling.  arXiv preprint arXiv:2010.09535 , 2020.\n\n [56]  Zi-Ke Zhang, Chuang Liu, Yi-Cheng Zhang, and Tao Zhou. Solving the cold-start problem in recommend er systems with social tags.  EPL (Euro physics Letters) , 92(2):28002, 2010.\n\n [57] Evgenii Z helton oz hsk ii, Chaim Baskin, Alex M Bronstein, and Avi Mendelson. Self-supervised learning for large-scale unsupervised image clustering.  arXiv preprint arXiv:2008.10312 , 2020.\n\n [58]  S Kevin Zhou, Hayit Greenspan, Christos Davatzikos, James S Duncan, Bram van Ginneken, Anant Madabhushi, Jerry L Prince, Daniel Rueckert, and Ronald M Summers. A review of deep learning in medical imaging: Imaging traits, technology trends, case studies with progress highlights, and future promises.  Proceedings of the IEEE , 2021.\n\n [59]  Zongwei Zhou.  Towards Annotation-Efﬁcient Deep Learning for Computer-Aided Diagnosis . PhD thesis, Arizona State University, 2021.\n\n [60]  Zongwei Zhou, Jae Shin, Ruibin Feng, R Todd Hurst, Christopher B Kendall, and Jianming Liang. Integrating active learning and transfer learning for carotid intima-media thickness video interpretation. Journal of digital imaging , 32(2):290–299, 2019. ", "page_idx": 10, "bbox": [107, 73.67805480957031, 505, 722.51953125], "page_size": [612.0, 792.0]}
{"layout": 65, "type": "text", "text": "[61]  Zongwei Zhou, Jae Shin, Lei Zhang, Suryakanth Gurudu, Michael Gotway, and Jianming Liang. Fine- tuning convolutional neural networks for biomedical image analysis: actively and increment ally. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 7340–7349, 2017.\n\n [62]  Zongwei Zhou, Jae Y Shin, Suryakanth R Gurudu, Michael B Gotway, and Jianming Liang. Active, continual ﬁne tuning of convolutional neural networks for reducing annotation efforts.  Medical Image Analysis , page 101997, 2021.\n\n [63]  Yu Zhu, Jinghao Lin, Shibi He, Beidou Wang, Ziyu Guan, Haifeng Liu, and Deng Cai. Addressing the item cold-start problem by attribute-driven active learning.  IEEE Transactions on Knowledge and Data Engineering , 32(4):631–644, 2019. ", "page_idx": 11, "bbox": [108, 73.67805480957031, 505, 182.11654663085938], "page_size": [612.0, 792.0]}
{"layout": 66, "type": "text", "text": "A Implementation Con gu rations ", "text_level": 1, "page_idx": 12, "bbox": [106, 72, 290, 84], "page_size": [612.0, 792.0]}
{"layout": 67, "type": "text", "text": "A.1 Data Split ", "text_level": 1, "page_idx": 12, "bbox": [107, 96, 176, 108], "page_size": [612.0, 792.0]}
{"layout": 68, "type": "text", "text": "PathMNIST with nine categories has 107,180 colorectal cancer tissue his to pathological images extracted from Kather  et al . [ 27 ], with 89,996/10,004/7,180 images for training/validation/testing. BloodMNIST contains 17,092 microscopic peripheral blood cell images extracted from Acevedo  et al . [ 1 ] with eight categories, where 11,959/1,712/3,421 images for training/validation/testing. Organ AM NIST consists of the axial view abdominal CT images based on Bilic  et al . [ 8 ], with 34,581/6,491/17,778 images of 11 categories for training/validation/testing. CIFAR-10-LT (  $(\\rho{=}100)$  ) consists of a subset of CIFAR-10 [29], with 12,406/10,000 images for training/testing. ", "page_idx": 12, "bbox": [108, 116.37457275390625, 505, 193.83445739746094], "page_size": [612.0, 792.0]}
{"layout": 69, "type": "text", "text": "A.2 Training Recipe for Contrastive Learning ", "text_level": 1, "page_idx": 12, "bbox": [107, 207, 311, 219], "page_size": [612.0, 792.0]}
{"layout": 70, "type": "text", "text": "Pseudocode for Our Proposed Strategy.  The algorithm 1 provides the pseudocode for our proposed hard-to-contrast initial query strategy, as elaborated in   $\\S2$  . ", "page_idx": 12, "bbox": [108, 226.64488220214844, 505, 249.91746520996094], "page_size": [612.0, 792.0]}
{"layout": 71, "type": "text", "text": "Algorithm 1:  Active querying hard-to-contrast data ", "text_level": 1, "page_idx": 12, "bbox": [108, 273, 315, 284], "page_size": [612.0, 792.0]}
{"layout": 72, "type": "text", "text": "input:  $\\mathbf{\\dot{\\mathcal{D}}}=\\{\\pmb{x}_{m}\\}_{m=1}^{M}$    { labeled dataset    $\\mathcal{D}$   contain  $M$   images} annotation budget  B ; the number of clusters  $K$  ; batch size  $N$  ; the number of epochs  $E$   $\\tau$  cture of encoder    $f$  , projection head    $g$  ; augmentation    $\\mathcal{T}$   $\\theta^{(e)},e\\in[1,E]$   ∈  {model parameters at epoch  $e$   during contrastive learning} output: ed query    $\\mathcal{Q}$   $\\boldsymbol{\\mathcal{Q}}=\\boldsymbol{\\mathcal{Q}}$  Q for  epoch    $e\\in\\{1,\\dots,E\\}$   do for  samp  $\\{\\pmb{x}_{n}\\}_{n=1}^{N}$    do for all  $n\\in\\{1,\\dots,N\\}$   ∈{ }  do draw two augmentation functions  $t\\!\\sim\\!\\tau$  ,    $t^{\\prime}\\!\\sim\\!\\tau$  # the ﬁrst augmentation  $\\begin{array}{r l}&{\\tilde{\\mathbf{x}}_{2n-1}=t(\\bar{\\mathbf{x}}_{n})}\\\\ &{\\mathbf{h}_{2n-1}=f(\\tilde{\\mathbf{x}}_{2n-1})}\\\\ &{z_{2n-1}=g(\\mathbf{h}_{2n-1})}\\end{array}$  # representation # projection # the second augmentation  $\\begin{array}{r}{\\tilde{\\mathbf{x}}_{2n}=t^{\\prime}(\\pmb{x}_{n})\\quad}\\\\ {\\pmb{h}_{2n}=f(\\tilde{\\pmb{x}}_{2n})\\quad}\\\\ {z_{2n}=g(\\pmb{h}_{2n})\\quad}\\end{array}$  # representation # projection end for for all    $i\\in\\{1,.\\,.\\,.\\,,2N\\}$   and    $j\\in\\{1,\\dots,2N\\}$   do  $s_{i,j}=z_{i}^{\\top}z_{j}/(\\|z_{i}\\|\\|z_{j}\\|)$  ∥ ∥∥ ∥ # pairwise similarity  $\\begin{array}{r}{p_{i,j}=\\frac{\\exp(s_{i,j})/\\tau}{\\sum_{n=1}^{2N}\\mathbb{1}_{[n\\neq i]}\\exp\\left(s_{i,n}\\right)/\\tau}}\\end{array}$  # predicted probability of contrastive pre-text task end for  $\\begin{array}{r}{p_{\\theta^{(e)}}(y_{n}^{*}|x_{n})=\\frac{1}{2}[p_{2n-1,2n}+p_{2n,2n-1}]}\\end{array}$  end for end for for  unlabeled images    $\\{\\pmb{x}_{m}\\}_{m=1}^{M}$    do  $\\begin{array}{r}{\\hat{\\mu}_{m}=\\frac{1}{E}\\sum_{e=1}^{E}p_{\\theta^{(e)}}\\bar{(y_{m}^{*}|x_{m})}}\\end{array}$  Assign  $\\pmb{x}_{m}$   to one of the clusters computed by  $K{\\mathrm{-mean}}(h,K)$  end for for all    $k\\in\\{1,\\ldots,K\\}$   do sort images in the cluster  $K$   based on  $\\hat{\\mu}$   in an ascending order query labels for top  $B/K$   samples, yielding  $Q_{k}$   ${\\bar{\\mathcal{Q}}}={\\dot{\\bar{\\mathcal{Q}}}}\\cup{\\mathcal{Q}}_{k}$  end for return    $\\mathcal{Q}$  ", "page_idx": 12, "bbox": [117, 286.5519104003906, 480, 720.8250732421875], "page_size": [612.0, 792.0]}
{"layout": 73, "type": "table", "page_idx": 13, "table_caption": "Table 3: Contrastive learning settings on MedMNIST and CIFAR-10-LT. (a) MedMNIST pre-training (b) CIFAR-10-LT pre-training ", "table_footnote": "", "bbox": [116, 78, 464, 188.75], "page_size": [612.0, 792.0]}
{"layout": 74, "type": "table", "page_idx": 13, "table_caption": "Table 4:  Data augmentations. ", "bbox": [242, 189.25, 500, 309], "page_size": [612.0, 792.0]}
{"layout": 75, "type": "table", "page_idx": 13, "img_path": "layout_images/2210.02442v1_7.jpg", "table_caption": "(a) Augmentations for RGB images ", "table_footnote": "Gaussian blur  $\\sigma_{m i n}{=}0.1$  ,  $\\sigma_{m a x}{=}2.0$  ,  $\\mathrm{{p}{=}0.5}$  ", "bbox": [124, 316, 295, 394], "page_size": [612.0, 792.0], "ocr_text": "augmentation value\n\nhflip\ncrop [0.08, 1]\ncolor jitter [0.4, 0.4, 0.4, 0.1], p=0.8\n\ngray scale\n", "vlm_text": "The table shows data augmentation techniques along with their respective values:\n\n1. **hflip** - No specific value provided.\n2. **crop** - Range is \\([0.08, 1]\\).\n3. **color jitter** - Values are \\([0.4, 0.4, 0.4, 0.1]\\) with a probability \\(p=0.8\\).\n4. **gray scale** - No specific value provided.\n5. **Gaussian blur** - Values are \\(0.1, \\quad 0.2, \\quad p=0.5\\) (Note: Gaussian blur is partly cut off).\n\nEach row corresponds to a different augmentation method and its parameters."}
{"layout": 76, "type": "table", "page_idx": 13, "img_path": "layout_images/2210.02442v1_8.jpg", "table_caption": "(b) Augmentations for Organ AM NIST ", "bbox": [319, 315, 493, 384], "page_size": [612.0, 792.0], "ocr_text": "augmentation value\n\nhflip\ncrop [0.08, 1]\ncolor jitter [0.4, 0.4, 0.4, 0.1], p=0.8\n\nrotation degrees=45\n", "vlm_text": "The table lists various data augmentations along with their corresponding values:\n\n- **hflip**: No specific value provided.\n- **crop**: Range [0.08, 1].\n- **color jitter**: Values [0.4, 0.4, 0.4, 0.1], probability p = 0.8.\n- **rotation**: Degrees = 45."}
{"layout": 77, "type": "text", "text": "Pre-training Settings.  Our settings mostly follow [ 15 ,  14 ]. Table 3a summarizes our contrastive pre-training settings on MedMNIST, following [ 15 ]. Table 3a shows the corresponding pre-training settings on CIFAR-10-LT, following the ofﬁcial MoCo demo on CIFAR-10 [ 14 ]. The contrastive learning model is pre-trained on 2 NVIDIA RTX3090 GPUs with 24GB memory each. The total number of model parameters is 55.93 million, among which 27.97 million requires gradient back propagation. ", "page_idx": 13, "bbox": [107, 432.1799011230469, 505, 499.0885009765625], "page_size": [612.0, 792.0]}
{"layout": 78, "type": "text", "text": "Dataset Augmentation.  We apply the same augmentation as in MoCo v2 [ 15 ] on all the images of RGB modalities to reproduce the optimal augmentation pipeline proposed by the authors, including PathMNIST, BloodMNIST, CIFAR-10-LT. Because Organ AM NIST is a grey scale CT image dataset, we apply the augmentation in [ 3 ] designed for radiological images, replacing random gray scale and Gaussian blur with random rotation. Table 4 shows the details of data augmentation. ", "page_idx": 13, "bbox": [107, 503.1138916015625, 505, 559.1134643554688], "page_size": [612.0, 792.0]}
{"layout": 79, "type": "text", "text": "Repeated Augmentation.  Our MoCo v2 pre-training is so fast in computation that data loading becomes a new bottleneck that dominates running time in our setup. We perform repeated augmentation on MedMNIST datasets at the level of dataset, also to enlarge augmentation space and improve generalization. [ 21 ] proposed repeated augmentation in a growing batch mode to improve generalization and convergence speed by reducing variances. This approach provokes a challenge in computing resources. Recent works [ 21 ,  50 ,  7 ] proved that ﬁxed batch mode also boosts generalization and optimization by increasing muti pli city of augmentations as well as parameter updates and decreasing the number of unique samples per batch, which holds the batch size ﬁxed. Because the original contrastive learning works [ 13 ,  15 ] were implemented on ImageNet dataset, we attempt to simulate the quantity of ImageNet per epoch to achieve optimal performances. The details are shown in Table 5. ", "page_idx": 13, "bbox": [107, 563.137939453125, 505, 684.5924682617188], "page_size": [612.0, 792.0]}
{"layout": 80, "type": "text", "text": "We only applied repeated augmentation on MedMNIST, but not CIFAR-10-LT. This is because we follow all the settings of the ofﬁcial CIFAR-10 demo [ 14 ] in which repeated augmentation is not employed. ", "page_idx": 13, "bbox": [107, 688.9765625, 505, 722.7994995117188], "page_size": [612.0, 792.0]}
{"layout": 81, "type": "table", "page_idx": 14, "img_path": "layout_images/2210.02442v1_9.jpg", "table_caption": "Table 5:  Repeated augmentation.  For a faster model convergence, we apply repeated augmenta- tion [21, 49, 50] on MedMNIST by reproducing the large batch size and iteration numbers. ", "bbox": [106, 70, 506, 163], "page_size": [612.0, 792.0], "ocr_text": "# training\n\nrepeated times\n\n# samples per epoch\n\nImageNet 1,281,167 1 1,281,167\nPathMNIST 89,996 14 1,259,944\nOrganAMNIST 34,581 37 1,279,497\nBloodMNIST 11,959 105 1,255,695\nCIFAR-10-LT(p=100) 12,406 i 12,406\n\n", "vlm_text": "The table presents data on several datasets, including ImageNet, PathMNIST, OrganAMNIST, BloodMNIST, and CIFAR-10-LT (ρ=100). It contains three columns, each with specific details:\n\n1. **# training**: The number of training samples in each dataset.\n   - ImageNet: 1,281,167 samples\n   - PathMNIST: 89,996 samples\n   - OrganAMNIST: 34,581 samples\n   - BloodMNIST: 11,959 samples\n   - CIFAR-10-LT (ρ=100): 12,406 samples\n\n2. **repeated times**: The number of times the dataset is repeated to match a specific number of samples per epoch.\n   - ImageNet: 1\n   - PathMNIST: 14\n   - OrganAMNIST: 37\n   - BloodMNIST: 105\n   - CIFAR-10-LT (ρ=100): 1\n\n3. **# samples per epoch**: The total number of samples considered per epoch after applying the repetition.\n   - ImageNet: 1,281,167 samples\n   - PathMNIST: 1,259,944 samples\n   - OrganAMNIST: 1,279,497 samples\n   - BloodMNIST: 1,255,695 samples\n   - CIFAR-10-LT (ρ=100): 12,406 samples\n\nThe table is used to show how training samples are adjusted by repeating them to match a particular number of samples per epoch, providing a sense of uniformity across different datasets during training phases."}
{"layout": 82, "type": "text", "text": "A.3 Training Recipe for MedMNIST and CIFAR-10 ", "text_level": 1, "page_idx": 14, "bbox": [107, 183, 337, 195], "page_size": [612.0, 792.0]}
{"layout": 83, "type": "text", "text": "Benchmark Settings.  We evaluate the initial queries by the performance of model trained on the selected initial query, and present the results in Table 1, 7 and Figure 4. The benchmark experiments are performed on NVIDIA RTX 1080 GPUs, with the following settings in Table 6. ", "page_idx": 14, "bbox": [108, 203.0588836669922, 504, 237.24046325683594], "page_size": [612.0, 792.0]}
{"layout": 84, "type": "text", "text": "Cold Start Settings for Existing Active Querying Criteria.  To compare the cold start performance of active querying criteria with random selection ( Figure 1), we trained a model with the test set and applied existing active querying criteria. ", "page_idx": 14, "bbox": [108, 241.26487731933594, 504, 275.447509765625], "page_size": [612.0, 792.0]}
{"layout": 85, "type": "table", "page_idx": 14, "img_path": "layout_images/2210.02442v1_10.jpg", "table_caption": "Table 6:  Benchmark settings.  We apply the same settings for training MedMNIST, CIFAR-10, and CIFAR-10-LT. ", "bbox": [106, 292, 506, 435], "page_size": [612.0, 792.0], "ocr_text": "cong\n\nbackbone\n\noptimizer\n\nlearning rate\n\nlearning rate schedule\nearly stopping patience\nmax epochs\n\naugmentation\n\nbatch size\n\nvalue\nInception-ResNet-v2\nSGD\n0.1\nreduce learning rate on plateau, factor=0.5, patience=8\n50\n10000\nflip, p=0.5\nrotation, p=0.5, in 90, 180, or 270 degrees\nreverse color, p=0.1\nfade color, p=0.1, 80% random noises + 20% original image\n128\n", "vlm_text": "The table contains hyperparameters and configurations for a machine learning model:\n\n- **backbone**: Inception-ResNet-v2\n- **optimizer**: SGD\n- **learning rate**: 0.1\n- **learning rate schedule**: Reduce learning rate on plateau, factor=0.5, patience=8\n- **early stopping patience**: 50\n- **max epochs**: 10000\n- **augmentation**:\n  - Flip, p=0.5\n  - Rotation, p=0.5, in 90, 180, or 270 degrees\n  - Reverse color, p=0.1\n  - Fade color, p=0.1, 80% random noises + 20% original image\n- **batch size**: 128"}
{"layout": 86, "type": "text", "text": "B Additional Results on MedMNIST ", "text_level": 1, "page_idx": 15, "bbox": [107, 71, 303, 85], "page_size": [612.0, 792.0]}
{"layout": 87, "type": "text", "text": "B.1 Label Diversity is a Signiﬁcant Add-on to Most Querying Strategies ", "text_level": 1, "page_idx": 15, "bbox": [107, 99, 419, 112], "page_size": [612.0, 792.0]}
{"layout": 88, "type": "text", "text": "As we present in Table 1, label diversity is an important underlying criterion in designing active querying criteria. We plot the full results on all three MedMNIST datasets in Figure 6. Most existing active querying strategies became more performant and robust in the presence of label diversity. ", "page_idx": 15, "bbox": [107, 122.28857421875, 504, 156.1114959716797], "page_size": [612.0, 792.0]}
{"layout": 89, "type": "image", "page_idx": 15, "img_path": "layout_images/2210.02442v1_11.jpg", "img_caption": "Figure 6: [Extended from Table 1]  Label diversity yields more performant and robust active querying strategies.  The experiments are conducted on three datasets in MedMNIST. The red and gray dots denote AUC scores of different active querying strategies with and without label diversity, respectively. Most existing active querying strategies became more performant and robust in the presence of label diversity,  e.g . BALD, Margin, VAAL, and Uncertainty in particular. Some gray dots are not plotted in the low budget regime because there are classes absent in the queries due to the selection bias. ", "bbox": [106, 170, 506, 707], "page_size": [612.0, 792.0], "ocr_text": "BALD Consistency Coreset Margin VAAL Entropy\n(Kirsch et al., 2017) (Gao et al., 2020) (Sener et al., 2017) | (Balcan et a/., 2007) —_ (Sinha et al., 2019) (Wang et al., 2014)\n1.0 wen 19 yo 1.9 are 1 a7 1.9 we 19 oo\nwt” Ls # ‘ end\nri if f # if \"\ni tl wee\n0.9| 0.9 j 0.9 4 0.9| 0.9 0.9 i i\n* | f\nof oa 0.8) 0.8) 0.8} om\n|\n| |\n107 To\" To\" To\" To\" To\" 107 10\" 10? T0* To? TO\n# of images # of images # of images # of images # of images # of images\n(a) PathMNIST\n1.0 1.0 1.0) 1.0 1.0\n37 on od\na’ e fod\ned a *\ntf + :\n0.9} 0.9} + 0.9) 0.9} 0.9}\nay : :\n° tt\n2 t\n< f\n' } if\n0.8] 0.8} | 0.8) t 0.8) 0.8}\n1 i\ni\nTo? 107 107 T T ei\n# of images # of images # of images # of images # of images # of images\n(b) OrganAMNIST\n1.0) aon 1 prem 10 ar 10 —\ni t | we a ae\nuw 4 ¢\n0.9) é 0.9} 4 0.9) if 0.9 j i\ni) f s it ot\n'\nz || 1 Hl '\n{ i f\n3} + 0.8 os} ¢ 0.8} + 0.8 0.8 ;\ni\n10\" TO\" TO\" To\" TO\" TO\"\n# of images # of images # of images # of images # of images # of images\n\n(c) BloodMNIST\n", "vlm_text": "The image shows a series of graphs demonstrating the impact of label diversity on the performance of different active querying strategies across three datasets within MedMNIST: PathMNIST, OrganMNIST, and BloodMNIST. Each graph plots the AUC (Area Under the Curve) against the number of images, comparing the performance of various active querying strategies with (red dots) and without (gray dots) label diversity.\n\nThe strategies analyzed include:\n\n1. BALD (Bayesian Active Learning by Disagreement)\n2. Consistency\n3. Coreset\n4. Margin\n5. VAAL (Variational Adversarial Active Learning)\n6. Entropy\n\nThe graphs indicate that most active querying strategies, such as BALD, Margin, VAAL, and Uncertainty, show improved performance and robustness in the presence of label diversity. The red dots generally lie above the gray dots, suggesting that incorporating label diversity results in higher AUC scores. Some gray dots are missing in low budget regimes due to selection bias causing class absence in queries."}
{"layout": 90, "type": "text", "text": "B.2 Contrastive Features Enable Label Diversity to Mitigate Bias ", "text_level": 1, "page_idx": 16, "bbox": [106, 72, 392, 84], "page_size": [612.0, 792.0]}
{"layout": 91, "type": "text", "text": "Our proposed active querying strategy is capable of covering the majority of classes in most low budget scenarios by integrating K-means clustering and contrastive features, including the tail classes ( e.g . femur-left, basophil). Compared to the existing active querying criteria, we achieve the best class coverage of selected query among at all budgets presented in Table 2. ", "page_idx": 16, "bbox": [107, 92.7325439453125, 504, 137.46446228027344], "page_size": [612.0, 792.0]}
{"layout": 92, "type": "image", "page_idx": 16, "img_path": "layout_images/2210.02442v1_12.jpg", "bbox": [111, 147, 507, 424], "page_size": [612.0, 792.0], "ocr_text": "bladder\nfemur-left\nfemur-right\nheart\nkidney-left\nkidney-right\nliver\nlung-left\nlung-right\npancreas\nspleen\n\nbasophil\neosinophil\nerythroblast\nig\nlymphocyte\nmonocyte\nneutrophil\nplatelet\n\nRandom Consistency\n\nVAAL\n\nMargin Entropy\n\nCoreset\n\nBALD\n\nOurs\n\n(a) OrganAMNIST\n\nRandom Consistency VAAL Margin Entropy Coreset BALD Ours\nLd | —\n_ I\n= (a) oo\n— | _\na —| |\n(cm LL |\n| a —|\n— l\n\n(b) BloodMNIST\n\n", "vlm_text": "The image contains two sets of bar charts comparing different methods based on their performance for two datasets: OrganAMNIST and BloodMNIST. \n\n- The first set (labeled \"a\") represents OrganAMNIST with categories such as bladder, femur-left, heart, etc.\n- The second set (labeled \"b\") represents BloodMNIST with categories like basophil, eosinophil, lymphocyte, etc.\n\nEach set compares several methods like Random, Consistency, VAAL, Margin, Entropy, Coreset, BALD, and Ours, showing quantitative results for each category."}
{"layout": 93, "type": "text", "text": "Figure 7: [Continued from Figure 2]  Our querying strategy yields better label diversity.  Random on the leftmost denotes the class distribution of randomly queried samples, which can also reﬂect the approximate class distribution of the entire dataset. As seen, even with a relatively larger initial query budget (691 images,  $2\\%$   of Organ AM NIST, and 2,391 images,   $20\\%$   of BloodMNIST), most active querying strategies are biased towards certain classes. For example in Organ AM NIST, VAAL prefers selecting data in the femur-right and platelet class, but largely ignores data in the lung, liver and monocyte classes. On the contrary, our querying strategy not only selects more data from minority classes (e.g., femur-left and basophil) while retaining the class distribution of major classes. ", "page_idx": 16, "bbox": [107, 433.0749206542969, 504, 521.802490234375], "page_size": [612.0, 792.0]}
{"layout": 94, "type": "image", "page_idx": 17, "img_path": "layout_images/2210.02442v1_13.jpg", "img_caption": "Figure 8:  Visualization of  $K$  -means clustering and our active selection.  UMAP [ 35 ] is used to visualize the feature clustering. Colors indicate the ground truth. Contrastive features clustered by the    $K$  -means algorithm present a fairly clear separation in the 2D space, which helps enforce the label diversity without the need of ground truth. The crosses denote the selected easy- (top) and hard-to-contrast (bottom) data. Overall, hard-to-contrast data have a greater spread within each cluster than easy-to-contrast ones. In addition, we ﬁnd that easy-to-contrast tends to select outlier classes that do not belong to the majority class in a cluster (see red arrows). This behavior will invalidate the purpose of clustering and inevitably jeopardize the label diversity. ", "bbox": [105, 70, 506, 438], "page_size": [612.0, 792.0], "ocr_text": "Easy-to-contrast\n\nHard-to-contrast\n\n(a) PathMNIST\n\n(b) OrganAMNIST (c) BloodMNIST\n\n", "vlm_text": "The image shows a visualization of $K$-means clustering applied to features from three datasets: PathMNIST, OrganAMNIST, and BloodMNIST. The visualization uses UMAP to project features into a 2D space, with different colors representing different ground truth classes. The top row represents \"easy-to-contrast\" data, while the bottom row represents \"hard-to-contrast\" data.\n\nCrosses mark data points selected either as easy-to-contrast or hard-to-contrast within each cluster. In the easy-to-contrast visualizations, red arrows point to outlier classes that lie within clusters dominated by different classes. Such selections are considered outliers and could negatively impact the intended label diversity in clustering.\n\nOverall, the hard-to-contrast data appear to be more widely spread within each cluster compared to the easy-to-contrast data. This suggests differences in data distribution and selection within these clustering contexts."}
{"layout": 95, "type": "text", "text": "Selected Query Visualization.  To ease the analysis, we project the image features (extracted by a trained MoCo v2 encoder) onto a 2D space by UMAP [ 35 ]. The assigned pseudo labels have large overlap with ground truths, suggesting that the features from MoCo v2 are quite disc rim i native for each class. Overall, Figure 8 shows that hard-to-contrast queries have a greater spread within each cluster than easy-to-contrast ones. Both strategies can cover   $100\\%$   classes. Nevertheless, we notice that easy-to-contrast selects  local outliers  in clusters: samples that do not belong to the majority class in a cluster. Such behavior will invalidate the purpose of clustering, which is to query uniformly by separating classes. Additionally, it possibly exposes the risk of introducing out-of-distribution data to the query, which undermines active learning [26]. ", "page_idx": 17, "bbox": [108, 458.5339050292969, 504, 558.1704711914062], "page_size": [612.0, 792.0]}
{"layout": 96, "type": "text", "text": "C Experiments on CIFAR-10 and CIFAR-10-LT ", "text_level": 1, "page_idx": 18, "bbox": [106, 71, 363, 84], "page_size": [612.0, 792.0]}
{"layout": 97, "type": "text", "text": "C.1 Label Diversity is a Signiﬁcant Add-on to Most Querying Strategies ", "text_level": 1, "page_idx": 18, "bbox": [106, 95, 421, 108], "page_size": [612.0, 792.0]}
{"layout": 98, "type": "text", "text": "As illustrated in Table 7 and Figure 9, label diversity is an important underlying criterion in designing active querying criteria on CIFAR-10-LT, an extremely imbalanced dataset. We compare the results of CIFAR-10-LT with MedMNIST datasets Figure 6. CIFAR-10-LT is more imbalanced than MedMNIST, and the performance gain and robustness improvement of label diversity CIFAR-10-LT is sign i cant ly larger than MedMNIST. Most of the active querying strategies fail to query all the classes even at relatively larger initial query budgets. ", "page_idx": 18, "bbox": [107, 115.8455810546875, 505, 182.39549255371094], "page_size": [612.0, 792.0]}
{"layout": 99, "type": "text", "text": "Table 7:  Diversity is a signiﬁcant add-on to most querying strategies.  AUC scores of different querying strategies are compared on CIFAR-10 and CIFAR-10-LT. In the low budget regime ( e.g .   $10\\%$  and  $20\\%$   of the entire dataset), active querying strategies beneﬁt from enforcing the label diversity of the selected data. The cells are highlighted in blue when adding diversity performs no worse than the original querying strategies. Some results are missing (marked as “-”) because the querying strategy fails to sample at least one data point for each class. Results of more sampling ratios are presented in Appendix Figure 9. ", "page_idx": 18, "bbox": [107, 191.78086853027344, 505, 269.5985107421875], "page_size": [612.0, 792.0]}
{"layout": 100, "type": "table", "page_idx": 18, "img_path": "layout_images/2210.02442v1_14.jpg", "bbox": [108, 276, 507, 436], "page_size": [612.0, 792.0], "ocr_text": "CIFAR-10-LT\n\n1% 5% 10% 20% 30% 40%\nUnif. (142) (710) (1420) (2841) (4261) (5682)\n\nConsistency v 718.0222 90.0+0.1 Oi Seta 93.4+0.2 9322202 94.640.2\n- - 67.1417.1 88.60.3 90.4+0.6 90.7+0.2\nVAAL v 80.9+1.0 90.3+0.5 92.6+0.2 93.7+0.4 93.9+0.8 —\nMargin v 81.241.8 88.7+0.7 ORTEOD O20 94.5+0.1 94.7+0.4\n- - 81.9+0.8 86.30.3 87.4+0.2 88.1+0.1\nEntropy v 78.1+1.4 89.6+0.5 92.041.2 OO TIES 94.0+0.6 94.0+0.7\n- 79.041.2 65.6415.6 86.4+0.2 88.5+0.2 89.5+0.7\nCoreset v 80.8+1.0 SO eS) 91.5+0.4 93.6+0.2 93.4+0.7 94.8+0.1\n- - 65.9415.9  86.9+0.1 88.2+0.1 90.30.2\nBALD v 83.30.6 90.8+0.3 92.8+0.1 90.8+2.4 94.0+0.8 94.740.4\n- 76.8423 64.9414.9  84.740.6 88.0+0.5 88.9+0.1\n\n", "vlm_text": "This table appears to show performance metrics (possibly accuracy or a similar measure) on the CIFAR-10-LT dataset for different methods and data percentages. Here's a summary of its structure:\n\n- The methods tested are Consistency, VAAL, Margin, Entropy, Coreset, and BALD.\n- Each method is tested with and without uniform data sampling (Unif.).\n- The dataset percentages range from 1% (142 instances) to 40% (5682 instances).\n- The performance results are shown with a mean ± standard deviation format for each percentage increase.\n\nThe background shading on some cells could suggest a highlight of results or particular conditions."}
{"layout": 101, "type": "text", "text": "C.2 Contrastive Features Enable Label Diversity to Mitigate Bias ", "text_level": 1, "page_idx": 18, "bbox": [107, 454, 393, 467], "page_size": [612.0, 792.0]}
{"layout": 102, "type": "text", "text": "Our proposed active querying strategy is capable of covering the majority of classes in most low budget scenarios by integrating K-means clustering and contrastive features, including the tail classes (horse, ship, and truck). Compared to the existing active querying criteria, we achieve the best class coverage of selected query among at all budgets presented in Table 2. As depicted in Figure 9, our querying strategy has a more similar distribution to the overall distribution of dataset and successfully covers all the classes, with the highest proportion of minor classes (ship and truch) among random selection and all active querying methods. ", "page_idx": 18, "bbox": [107, 475.32257080078125, 505, 552.781494140625], "page_size": [612.0, 792.0]}
{"layout": 103, "type": "image", "page_idx": 19, "img_path": "layout_images/2210.02442v1_15.jpg", "bbox": [111, 212, 503, 507], "page_size": [612.0, 792.0], "ocr_text": "(Kirsch et al., 2017)\n\nBALD\n\nConsistency Coreset Margin VAAL Entropy\n(Gao et al., 2020) (Sener et a/., 2017) | (Balcan et al., 2007) (Sinha et al., 2019) (Wang et al., 2014)\n\n1.9 eum 9 om 19 wae 10 a 1.9\n\" # af?\nif 4\n} i\n0.9} 0.9) + 0.9) 0.9) if 0.9)\n< ct {\n]\n| ! ¢\n0.8) mat 0.8) 0.8) | 0.8)\n|\nTO 10\" TO: TO* TO: T0* 107 TO 10’ T0* TO: 10*\n# of images # of images # of images # of images # of images # of images\n(a) CIFAR-10\n1.0 1.0 1.0 1.0)\nos} 0.8 0.3} st 0.8\n« * ‘\n<= * s\n\n0.6) 0.6)\" 0.6* 0.6\" 0.6\" 0.6\n\nTo? 10> 10* To? 10> 10* Tor 10> 10* To? 10> 10* 107 107 10* To? 10> 10*\n\n# of images # of images # of images # of images # of images # of images\n", "vlm_text": "The image consists of two sets of charts, each containing five subplots. Each subplot represents the performance of a different method for a specific dataset. The datasets and methods are as follows:\n\n1. **Dataset: CIFAR-10**\n   - Methods: \n     - BALD (Kirsch et al., 2017)\n     - Consistency (Gao et al., 2020)\n     - Coreset (Sener et al., 2017)\n     - Margin (Balcan et al., 2007)\n     - VAAL (Sinha et al., 2019)\n     - Entropy (Wang et al., 2014)\n\n2. **Dataset: SVHN**\n   - Same methods as listed above.\n\nFor each method, the x-axis represents the number of images, while the y-axis represents the Area Under the Curve (AUC). Red circles and lines represent performance metrics along with error bars, which are likely indicating variability or confidence intervals in the AUC measurements.\n\nThese plots are examining how well each method performs in terms of AUC as the number of images in the dataset is increased, in a semi-logarithmic scale (logarithmic x-axis and linear y-axis)."}
{"layout": 104, "type": "text", "text": "Figure 9: Diversity yields more performant and robust active querying strategies. The experiments are conducted on CIFAR-10-LT. The red and gray dots denote AUC scores of different active querying strategies with and without label diversity, respectively. Observations are consistent with those in medical applications (see Figure 6): Most existing active querying strategies became more performant and robust in the presence of label diversity. ", "page_idx": 19, "bbox": [107, 521.6668701171875, 504, 577.66650390625], "page_size": [612.0, 792.0]}
{"layout": 105, "type": "image", "page_idx": 20, "img_path": "layout_images/2210.02442v1_16.jpg", "bbox": [112, 214, 505, 497], "page_size": [612.0, 792.0], "ocr_text": "Random Consistency VAAL Margin Entropy Coreset BALD\n\nairplane\nautomobile\ncat\n\nbird\n\ndeer\n\ndog\n\nfrog\n\nhorse\n\nship\n\ntruck\n\n(a) CIFAR-10\n\nRandom Consistency VAAL Margin Entropy Coreset BALD Ours\n\nairplane\nautomobile\ncat\n\nbird\n\ndeer\n\n(b) CIFAR-10-LT\n", "vlm_text": "The image contains two bar charts comparing different methods across various categories. Each chart shows the performance of different strategies (Random, Consistency, VAAL, Margin, Entropy, Coreset, BALD, and Ours) related to the CIFAR-10 and CIFAR-10-LT datasets.\n\n- **Top Chart (a) CIFAR-10**: Displaying performance measures on categories like airplane, automobile, cat, etc.\n- **Bottom Chart (b) CIFAR-10-LT**: Showing performance on the same categories for a long-tail version of the dataset.\n\nEach method appears to have varying levels of performance across these categories."}
{"layout": 106, "type": "text", "text": "Figure 10:  Our querying strategy yields better label diversity.  Random on the leftmost denotes the class distribution of randomly queried samples, which can also reﬂect the approximate class distribution of the entire dataset. As seen, even with a relatively larger initial query budget (5,000 images,  $10\\%$   of CIFAR-10, and 1420 images,   $10\\%$   of CIFAR-10-LT), most active querying strategies are biased towards certain classes. Our querying strategy, on the contrary, is capable of selecting more data from the minority classes such as horse, ship, and truck. ", "page_idx": 20, "bbox": [108, 505.09490966796875, 504, 572.0045166015625], "page_size": [612.0, 792.0]}
{"layout": 107, "type": "image", "page_idx": 21, "img_path": "layout_images/2210.02442v1_17.jpg", "img_caption": "Figure 11:  Active querying based on Dataset Maps.  (a,d) PathMNIST and Organ AM NIST dataset overview. (b,e) Easy- and hard-to-learn data can be selected from the maps based on ground truths [ 26 ]. This querying strategy has two limitations: (1) requiring manual annotations and (2) data are stratiﬁed by classes in the 2D space, leading to a poor label diversity in the selected queries. (c,f) Easy- and hard-to-contrast data can be selected from the maps based on pseudo labels. This querying strategy is label-free and the selected “hard-to-contrast” data represent the most common patterns in the entire dataset. These data are more suitable for training and thus alleviate the cold start problem. ", "bbox": [104, 175, 507, 613], "page_size": [612.0, 792.0], "ocr_text": "PathMNIST\n\nOrganAMNIST\n\n> adipose © debris @ smooth muscle\n\n© background lymphocytes colorectal adenocarcinoma epithelium\n© cancer-associated stroma » mucus © normal colon mucosa\n\n1.0\n0.8\n\ng S\n\nL0.6 L0.6\n\nvo v\n\nmS} Res\n\n‘= =\n\n50. 50.4\n\n& &\n\n: o - ‘\n= 0.2 0.2 a\na J\na a a\nHard-to-learn variability variability Hard-to-contrast\n(b) Data Map by ground truth (c) Data Map by pseudo-labels\nbladder @ — kidney-left @ — lung-right heart\n©  femur-left kidney-right pancreas © lung-left\n© femur-right © liver @ spleen\n\n1.0\n\nS\n&\n\ng Oo\n20.6 20.6\nOo vo\n3 3\n< i\n50.4 50.4\n8 8\n0.2 0.2\nOOK 0.2 0.4 02 04\n\n: i s Hard-todeam variability variability Hard-to-contrast\n(d) Overall distribution (e) Data Map by ground truth (f) Data Map by pseudo-labels\n", "vlm_text": "The image appears to display an analysis of two medical imaging datasets, PathMNIST and OrganAMNIST, focusing on an active querying approach using Dataset Maps to select data for training machine learning models. Here's a breakdown of the components shown:\n\n1. **PathMNIST Overview**:\n   - (a) Overall distribution: A visual representation of the PathMNIST dataset, showing a grid of histology images that depict various tissue types.\n   - (b) Data Map by ground truth: Shows a scatter plot of the images based on their confidence and variability, where data are stratified by different classes (e.g., adipose, smooth muscle, etc.). Easy-to-learn and hard-to-learn samples are marked, requiring manual annotations.\n   - (c) Data Map by pseudo-labels: Similar scatter plot as in (b), but based on pseudo-labels instead of ground truth. Easy-to-contrast and hard-to-contrast samples are identified, which does not require manual labels and helps mitigate the cold start problem.\n\n2. **OrganAMNIST Overview**:\n   - (d) Overall distribution: Displays a grid of grayscale images from the OrganAMNIST dataset, representing different organ scans.\n   - (e) Data Map by ground truth: A scatter plot showing data categorized by various organs using ground truth. It highlights easy-to-learn and hard-to-learn samples, again requiring manual annotations.\n   - (f) Data Map by pseudo-labels: Shows a scatter plot similar to (e) but uses pseudo-labels for stratification. Easy-to-contrast and hard-to-contrast samples are highlighted to enhance training.\n\nOverall, the image depicts a strategy for selecting training data from medical imaging datasets by comparing methods that rely on ground truth versus those using pseudo-labels, highlighting their relative advantages and challenges."}
{"layout": 108, "type": "image", "page_idx": 22, "img_path": "layout_images/2210.02442v1_18.jpg", "img_caption": "Figure 12:  Active querying based on Dataset Maps.  (a) CIFAR-10-LT dataset overview. (b) Easy- and hard-to-learn data can be selected from the maps based on ground truths [ 26 ]. This querying strategy has two limitations: (1) requiring manual annotations and (2) data are stratiﬁed by classes in the 2D space, leading to a poor label diversity in the selected queries. (c) Easy- and hard-to-contrast data can be selected from the maps based on pseudo labels. This querying strategy is label-free and the selected “hard-to-contrast” data represent the most common patterns in the entire dataset. These data are more suitable for training and thus alleviate the cold start problem. ", "bbox": [106, 262, 506, 527], "page_size": [612.0, 792.0], "ocr_text": "airplane © deer @ ship cat\n\n®@ automobile dog truck\n© bird ® frog © horse\n1.0\n= 0.8 0.8\n8 g\n20.6 20.6\nEasy-to-learn 3 3 Easy-to-contrast\n= = d\n6 0.4 6 0.4 r\nis) & y\n0.2 aie 0.2\nTHeF 02 0.4 040 0.45 0.50 ta\nHard-to-learn variability variability Hard-to-contrast\n\n(a) Overall distribution (b) Data Map by ground truth (c) Data Map by pseudo-labels\n", "vlm_text": "This image illustrates different querying strategies based on Dataset Maps for the CIFAR-10-LT dataset. It consists of three parts:\n\n1. **(a) Overall distribution**: This section shows a collage of images from the CIFAR-10-LT dataset, representing a general overview of the data distribution, with various classes such as airplanes, automobiles, birds, cats, etc., indicated by different colored dots.\n\n2. **(b) Data Map by ground truth**: This plot represents data points in a 2D space defined by 'confidence' and 'variability'. The data points are colored according to their class. Easy-to-learn and hard-to-learn data points are highlighted. Easy-to-learn data are those with high confidence and low variability, while hard-to-learn data have low confidence and high variability. Four example images are shown for each category. This method requires manual annotations and may lead to poor label diversity in selected queries.\n\n3. **(c) Data Map by pseudo-labels**: Similar to (b), this plot is generated using pseudo labels instead of ground truth. The plot helps identify easy-to-contrast and hard-to-contrast data, representing the most common patterns in the dataset. Easy-to-contrast data are located towards higher confidence and lower variability, while hard-to-contrast data are in areas of medium confidence and higher variability. Four example images are provided for each category. This strategy is label-free and better suited for training, helping alleviate the cold start problem by offering a more diverse selection of data points based on the model's understanding."}
{"layout": 109, "type": "image", "page_idx": 23, "img_path": "layout_images/2210.02442v1_19.jpg", "img_caption": "(b) Fine-tuning from self-supervised pre-training ", "bbox": [110, 108, 502, 290], "page_size": [612.0, 792.0], "ocr_text": "Random Entropy Margin BALD Coreset\n1007 1007 1007 1007 1007\n904 904 so 04 oo\n= 804 804 204 80+ 20+\nRy\n2 704 70-4 704 704 704\n“* Hard-to-Contrast\nsoy sail 60 oy oo -® Easy-to-Contras!\n50 T T T T 1 50-F T T T T 1 ™—1—T ™— 50 T T T T 1 50: T T T 1 ~# Easy-to-Leam\n40 20 2% 40 50 69 10 20 2% 40 50 6D 20 30 40 0 60 19 20 30 40 50 Gd 10 20 90 40 80 69 4 Consistency\n(a) Training from scratch ae Entropy\n-© Margin\n1007 1005 1005 1005 1007 -@ BALD\n~& VAAL\n904 Ss 904 904 904 904\n> Coreset\nso 20 204 804\n704 704 704 704\n60+ 60 604 60+\n5008 80\n10 20 30 40 50 60 10 20 30 40 50 60 10 2 30 40 50 60 10 20 30 40 50 60 10 20 30 40 50 60\n\n# of Labeled Images\n\n# of Labeled Images\n\n# of Labeled Images\n\n# of Labeled Images\n\n# of Labeled Images\n", "vlm_text": "The image is a set of graphs comparing the performance of different methods in terms of AUC (Area Under the Curve) percentage against the number of labeled images used in training. There are two main sections: \"Training from scratch\" (a) on the left and \"Fine-tuning from self-supervised pre-training\" (b) on the right. Each section contains multiple subplots for different selection strategies: Random, Entropy, Margin, BALD, and Coreset.\n\nFor each subplot:\n- The x-axis represents the number of labeled images, ranging from 10 to 60.\n- The y-axis represents the AUC percentage ranging from 50% to 100%.\n\nVarious lines in the graphs correspond to different selection strategies, such as:\n- Hard-to-Contrast (marked in red)\n- Easy-to-Contrast (marked in black squares)\n- Easy-to-Learn (marked in black triangles)\n- Consistency (marked in black diamonds)\n- Entropy (marked in black circles)\n- Margin (marked in black inverted triangles)\n- BALD (marked in black squares)\n- VAAL (marked in black diamonds)\n- Coreset (marked in black circles)\n\nThe lines indicate the relative performance of these strategies across different numbers of labeled data. The graph aims to demonstrate how various strategies perform with increasing labeled data, comparing the effectiveness of training from scratch to fine-tuning from self-supervised pre-trained models."}
{"layout": 110, "type": "text", "text": "Figure 13:  Performance of each active learning querying strategies with different initial query strategies on BloodMNIST.  Hard-to-contrast initial query strategy (red lines) outperforms other initial query strategies in every cycle of active learning. With each active learning querying strategy, the performance of the initial cycle (20 labeled images) and the last cycle (50 labeled images) are strongly correlated. ", "page_idx": 23, "bbox": [108, 299.66192626953125, 505, 355.6625061035156], "page_size": [612.0, 792.0]}
{"layout": 111, "type": "image", "page_idx": 23, "img_path": "layout_images/2210.02442v1_20.jpg", "img_caption": "(b) Fine-tuning from self-supervised pre-training ", "bbox": [110, 435, 502, 618], "page_size": [612.0, 792.0], "ocr_text": "Random Entropy Margin BALD Coreset\n\n1005 1005 1005 1005 1005\n904 904 904 904 204\n804 Fe 804 a 804 ae ag oe | 804 seal 805 S53\nno 70 am no 704 eo\n“* Hard-to-Contrast\n604 60+ 604 604 oo\n“® Easy-to-Contras\n50 4 ee —1—1—1_«50+ ss 50-+—¥ —1—1  Easy-to-Leam\n10 20 30 40 50 60 20 30 40 50 60 10 20 30 40 50 Gd 10 20 30 40 50 60 10 20 30 40 50 60 4. Consistency\n(a) Training from scratch > Entropy\n~@ Margin\n1005 41007 1007 41007 1005 -& BALD\nVA\n905) Weg 90 904 9054 v\n“Set ees ns ~* Coreset\nZ v0 804 of a 20 804\ng\n2 704 704 704 704 704\n604 60+ 604 «04 60+\n50 r — 50 I 0 ———_ 50. ———_ 50 ———1.\n\n10 20 30 40 50 60 10 20 30 40 50 60 10 20 30 40 50 60 10 20 30 40 50 60 10 20 30 40 50 60\n# of Labeled Images # of Labeled Images # of Labeled Images # of Labeled Images # of Labeled Images\n", "vlm_text": "The image consists of a series of graphs comparing the performance of different strategies for active learning, as shown by AUC (Area Under the Curve) percentages, with respect to the number of labeled images used. The comparison is between two scenarios: (a) \"Training from scratch\" and (b) \"Fine-tuning from self-supervised pre-training.\"\n\nEach graph represents a different active learning strategy: Random, Entropy, Margin, BALD, and Coreset. Within each graph, different lines indicate performance of various approaches, such as Hard-to-Contrast, Easy-to-Contrast, Easy-to-Learn, Consistency, Entropy, Margin, BALD, VAAL, and Coreset.\n\n- The x-axis represents the number of labeled images (ranging from 10 to 60).\n- The y-axis represents the AUC percentage, ranging from 50% to 100%.\n\nThe most prominent line across all graphs, as indicated in red, is the \"Hard-to-Contrast\" method, which consistently achieves higher AUC percentages compared to other methods plotted in black. The image illustrates the effect of pre-training on active learning methods' performance across different quantities of labeled data."}
{"layout": 112, "type": "text", "text": "Figure 14:  Performance of each active learning querying strategies with different initial query strategies on PathMNIST.  Hard-to-contrast initial query strategy (red lines) outperforms other initial query strategies in every cycle of active learning. With each active learning querying strategy, the performance of the initial cycle (20 labeled images) and the last cycle (50 labeled images) are strongly correlated. ", "page_idx": 23, "bbox": [108, 627.4598999023438, 505, 683.4595336914062], "page_size": [612.0, 792.0]}
