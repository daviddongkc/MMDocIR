{"page": 0, "image_path": "doc_images/2020.acl-main.408_0.jpg", "ocr_text": "ERASER ©: A Benchmark to Evaluate Rationalized NLP Models\n\nJay DeYoung*”, Sarthak Jain*”, Nazneen Fatema Rajani*®, Eric Lehman”,\n\nCaiming Xiong®, Richard Socher®, and Byron C. Wallace”\n\n*Equal contribution.\n\n\"Khoury College of Computer Sciences, Northeastern University\n®Salesforce Research, Palo Alto, CA, 94301\n\nAbstract\n\nState-of-the-art models in NLP are now pre-\ndominantly based on deep neural networks\nthat are opaque in terms of how they come\nto make predictions. This limitation has\nincreased interest in designing more inter-\npretable deep models for NLP that reveal the\n‘reasoning’ behind model outputs. But work\nin this direction has been conducted on dif-\nferent datasets and tasks with correspondingly\nunique aims and metrics; this makes it difficult\nto track progress. We propose the Evaluating\nRationales And Simple English Reasoning\n(ERASER ©) benchmark to advance research\non interpretable models in NLP. This bench-\nmark comprises multiple datasets and tasks for\nwhich human annotations of “rationales” (sup-\nporting evidence) have been collected. We pro-\npose several metrics that aim to capture how\nwell the rationales provided by models align\nwith human rationales, and also how faithful\nthese rationales are (i.e., the degree to which\nprovided rationales influenced the correspond-\ning predictions). Our hope is that releasing this\nbenchmark facilitates progress on designing\nmore interpretable NLP systems. The bench-\nmark, code, and documentation are available\nat https://www.eraserbenchmark.com/\n\n1 Introduction\n\nInterest has recently grown in designing NLP sys-\ntems that can reveal why models make specific\npredictions. But work in this direction has been\nconducted on different datasets and using different\nmetrics to quantify performance; this has made it\ndifficult to compare methods and track progress.\nWe aim to address this issue by releasing a stan-\ndardized benchmark of datasets — repurposed and\naugmented from pre-existing corpora, spanning a\nrange of NLP tasks — and associated metrics for\nmeasuring different properties of rationales. We re-\nfer to this as the Evaluating Rationales And Simple\nEnglish Reasoning (ERASER ©) benchmark.\n\nMovie Reviews\n\nIn this movie, ... Plots to take over the world.\n‘The soundtrack is run-of-the-mill,\n\n(@) Positive (b) Negative\ne-SNLI\n\nH Aman in an orange vest\nPAmanis\n\n(a) Entailment (b) Contradiction (c) Neutral\n\nCommonsense Explanations (CoS-E)\n\nWhere do you find the 2\n\n(a) Compost pile (b) Flowers (c) Forest (d) Field (e) Ground\n\nEvidence Inference\n\nArticle Patients for this trial were recruited ...\n\nPrompt With respect to breathlessness, what is the reported\ndifference between patients receiving placebo and those\nreceiving furosemide?\n\n{@) Sig. decreased (b) No sig. difference (c) Sig. increased\n\nFigure 1: Examples of instances, labels, and rationales\nillustrative of four (out of seven) datasets included in\nERASER. The ‘erased’ snippets are rationales.\n\nIn curating and releasing ERASER we take in-\nspiration from the stickiness of the GLUE (Wang\net al., 2019b) and SuperGLUE (Wang et al., 2019a)\nbenchmarks for evaluating progress in natural lan-\nguage understanding tasks, which have driven rapid\nprogress on models for general language repre-\nsentation learning. We believe the still somewhat\nnascent subfield of interpretable NLP stands to ben-\nefit similarly from an analogous collection of stan-\ndardized datasets and tasks; we hope these will\naid the design of standardized metrics to measure\ndifferent properties of ‘interpretability’, and we\npropose a set of such metrics as a starting point.\n\nInterpretability is a broad topic with many possi-\nble realizations (Doshi- Velez and Kim, 2017; Lip-\nton, 2016). In ERASER we focus specifically on\nrationales, i.e., snippets that support outputs. All\ndatasets in ERASER include such rationales, ex-\nplicitly marked by human annotators. By definition,\nrationales should be sufficient to make predictions,\n\n4443\n\nProceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4443-4458\nJuly 5 - 10, 2020. ©2020 Association for Computational Linguistics\n", "vlm_text": "ERASER  $\\circledcirc$  : A Benchmark to Evaluate Rationalized NLP Models \nJay DeYoung⋆Ψ, Sarthak  $\\mathbf{J}\\mathbf{a}\\mathbf{i}\\mathbf{n}^{\\star\\Psi}$ , Nazneen Fatema Rajani⋆Φ, Eric LehmanΨ,Caiming Xiong Φ ,  Richard Socher Φ , and  Byron C. Wallace Ψ \n⋆ Equal contribution. Ψ Khoury College of Computer Sciences, Northeastern University Φ Salesforce Research, Palo Alto, CA, 94301 \nAbstract \nState-of-the-art models in NLP are now pre- dominantly based on deep neural networks that are opaque in terms of how they come to make predictions. This limitation has increased interest in designing more inter- pretable deep models for NLP that reveal the ‘reasoning’ behind model outputs. But work in this direction has been conducted on dif- ferent datasets and tasks with correspondingly unique aims and metrics; this makes it difﬁcult to track progress. We propose the  E valuating R ationales  A nd  S imple  E nglish  R easoning ( ERASER  $\\circledcirc$  ) benchmark to advance research on interpretable models in NLP. This bench- mark comprises multiple datasets and tasks for which human annotations of “rationales” (sup- porting evidence) have been collected. We pro- pose several metrics that aim to capture how well the rationales provided by models align with human rationales, and also how  faithful these rationales are (i.e., the degree to which provided rationales inﬂuenced the correspond- ing predictions). Our hope is that releasing this benchmark facilitates progress on designing more interpretable NLP systems. The bench- mark, code, and documentation are available at  https://www.eraserbenchmark.com/ \n1 Introduction \nInterest has recently grown in designing NLP sys- tems that can reveal  why  models make speciﬁc predictions. But work in this direction has been conducted on different datasets and using different metrics to quantify performance; this has made it difﬁcult to compare methods and track progress. We aim to address this issue by releasing a stan- dardized benchmark of datasets — repurposed and augmented from pre-existing corpora, spanning a range of NLP tasks — and associated metrics for measuring different properties of rationales. We re- fer to this as the  E valuating  R ationales  A nd  S imple E nglish  R easoning ( ERASER  $\\circledcirc$  ) benchmark. \nThe image displays examples from four different datasets from the ERASER benchmark, each designed to evaluate interpretability in natural language processing. \n\n1. **Movie Reviews**: The instance is a movie review with parts of the text highlighted as rationales. The options for labeling are (a) Positive and (b) Negative, with the review leaning towards a positive sentiment as inferred from the non-erased snippet.\n\n2. **e-SNLI**: This involves a premise and a hypothesis, with certain parts erased as rationales. The task is to determine if the relationship is (a) Entailment, (b) Contradiction, or (c) Neutral. The non-erased portion suggests an \"Entailment\" relationship.\n\n3. **Commonsense Explanations (CoS-E)**: The task is to answer a question using commonsense reasoning. The rationale is the erased part of the sentence. The question asks, \"Where do you find the most amount of leaves?\" with options (a) Compost pile, (b) Flowers, (c) Forest, (d) Field, and (e) Ground. \"Forest,\" which is not erased, is the most likely answer.\n\n4. **Evidence Inference**: This task involves analyzing a medical statement and answering a question based on the evidence provided. The erased text is part of the rationale. The question asks about the effect of furosemide on breathlessness compared to a placebo, with options (a) Sig. decreased, (b) No sig. difference, and (c) Sig. increased. \"No sig. difference\" appears to be the suggested answer.\n\nIn each example, the 'erased' snippets represent the rationales pivotal for making the judgment, demonstrating how interpretability can be integrated into dataset annotations.\nIn curating and releasing ERASER we take in- spiration from the stickiness of the GLUE ( Wang et al. ,  2019b ) and SuperGLUE ( Wang et al. ,  2019a ) benchmarks for evaluating progress in natural lan- guage understanding tasks, which have driven rapid progress on models for general language repre- sentation learning. We believe the still somewhat nascent subﬁeld of interpretable NLP stands to ben- eﬁt similarly from an analogous collection of stan- dardized datasets and tasks; we hope these will aid the design of standardized metrics to measure different properties of ‘interpret ability’, and we propose a set of such metrics as a starting point. \nInterpret ability is a broad topic with many possi- ble realizations ( Doshi-Velez and Kim ,  2017 ;  Lip- ton ,  2016 ). In ERASER we focus speciﬁcally on rationales , i.e., snippets that support outputs. All datasets in ERASER include such rationales, ex- plicitly marked by human annotators. By deﬁnition, rationales should be  sufﬁcient  to make predictions, but they may not be  comprehensive . Therefore, for some datasets, we have also collected comprehen- sive rationales (in which  all  evidence supporting an output has been marked) on test instances. "}
{"page": 1, "image_path": "doc_images/2020.acl-main.408_1.jpg", "ocr_text": "but they may not be comprehensive. Therefore, for\nsome datasets, we have also collected comprehen-\nsive rationales (in which all evidence supporting\nan output has been marked) on test instances.\n\nThe ‘quality’ of extracted rationales will depend\non their intended use. Therefore, we propose an\ninitial set of metrics to evaluate rationales that\nare meant to measure different varieties of ‘inter-\npretability’. Broadly, this includes measures of\nagreement with human-provided rationales, and as-\nsessments of faithfulness. The latter aim to capture\nthe extent to which rationales provided by a model\nin fact informed its predictions. We believe these\nprovide a reasonable start, but view the problem of\ndesigning metrics for evaluating rationales — espe-\ncially for measuring faithfulness — as a topic for\nfurther research that ERASER can facilitate. And\nwhile we will provide a ‘leaderboard’, this is better\nviewed as a ‘results board’; we do not privilege\nany one metric. Instead, ERASER permits compar-\nison between models that provide rationales with\nrespect to different criteria of interest.\n\nWe implement baseline models and report their\nperformance across the corpora in ERASER. We\nfind that no single ‘off-the-shelf’ architecture is\nreadily adaptable to datasets with very different\ninstance lengths and associated rationale snippets\n(Section 3). This highlights a need for new models\nthat can consume potentially lengthy inputs and\nadaptively provide rationales at a task-appropriate\nlevel of granularity. ERASER provides a resource\nto develop such models.\n\nIn sum, we introduce the ERASER benchmark\n(www.eraserbenchmark.com), a unified set of di-\nverse NLP datasets (these are repurposed and aug-\nmented from existing corpora,! including senti-\nment analysis, Natural Language Inference, and\nQA tasks, among others) in a standardized for-\nmat featuring human rationales for decisions, along\nwith starter code and tools, baseline models, and\nstandardized (initial) metrics for rationales.\n\n2 Related Work\n\nInterpretability in NLP is a large, fast-growing\narea; we do not attempt to provide a comprehensive\noverview here. Instead we focus on directions par-\nticularly relevant to ERASER, i.e., prior work on\nmodels that provide rationales for their predictions.\n\nLearning to explain. In ERASER we assume that\n\n'We ask users of the benchmark to cite all original papers,\nand provide a BibTeX entry for doing so on the website.\n\nrationales (marked by humans) are provided during\ntraining. However, such direct supervision will not\nalways be available, motivating work on methods\nthat can explain (or “rationalize”) model predic-\ntions using only instance-level supervision.\n\nIn the context of modern neural models for text\nclassification, one might use variants of attention\n(Bahdanau et al., 2015) to extract rationales. At-\ntention mechanisms learn to assign soft weights to\n(usually contextualized) token representations, and\nso one can extract highly weighted tokens as ratio-\nnales. However, attention weights do not in gen-\neral provide faithful explanations for predictions\n(Jain and Wallace, 2019; Serrano and Smith, 2019;\nWiegreffe and Pinter, 2019; Zhong et al., 2019;\nPruthi et al., 2020; Brunner et al., 2020; Moradi\net al., 2019; Vashishth et al., 2019). This likely\nowes to encoders entangling inputs, complicating\nthe interpretation of attention weights on inputs\nover contextualized representations of the same.”\n\nBy contrast, hard attention mechanisms dis-\ncretely extract snippets from the input to pass to the\nclassifier, by construction providing faithful expla-\nnations. Recent work has proposed hard attention\nmechanisms as a means of providing explanations.\nLei et al. (2016) proposed instantiating two models\nwith their own parameters; one to extract rationales,\nand one that consumes these to make a prediction.\nThey trained these models jointly via REINFORCE\n(Williams, 1992) style optimization.\n\nRecently, Jain et al. (2020) proposed a variant\nof this two-model setup that uses heuristic feature\nscores to derive pseudo-labels on tokens compris-\ning rationales; one model can then be used to per-\nform hard extraction in this way, while a second\n(independent) model can make predictions on the\nbasis of these. Elsewhere, Chang et al. (2019)\nintroduced the notion of classwise rationales that\nexplains support for different output classes using\na game theoretic framework. Finally, other recent\nwork has proposed using a differentiable binary\nmask over inputs, which also avoids recourse to\nREINFORCE (Bastings et al., 2019).\n\nPost-hoc explanation. Another strand of inter-\npretability work considers post-hoc explanation\nmethods, which seek to explain why a model made\na specific prediction for a given input. Commonly\n\n*Interestingly, Zhong et al. (2019) find that attention some-\ntimes provides plausible but not faithful rationales. Elsewhere,\nPruthi et al. (2020) show that one can easily learn to deceive\nvia attention weights. These findings highlight that one should\nbe mindful of the criteria one wants rationales to fulfill.\n\n4444\n", "vlm_text": "\nThe ‘quality’ of extracted rationales will depend on their intended use. Therefore, we propose an initial set of metrics to evaluate rationales that are meant to measure different varieties of ‘inter- pretability’. Broadly, this includes measures of agreement with human-provided rationales, and as- sessments of  faithfulness . The latter aim to capture the extent to which rationales provided by a model in fact informed its predictions. We believe these provide a reasonable start, but view the problem of designing metrics for evaluating rationales — espe- cially for measuring faithfulness — as a topic for further research that ERASER can facilitate. And while we will provide a ‘leaderboard’, this is better viewed as a ‘results board’; we do not privilege any one metric. Instead, ERASER permits compar- ison between models that provide rationales with respect to different criteria of interest. \nWe implement baseline models and report their performance across the corpora in ERASER. We ﬁnd that no single ‘off-the-shelf’ architecture is readily adaptable to datasets with very different instance lengths and associated rationale snippets (Section  3 ). This highlights a need for new models that can consume potentially lengthy inputs and adaptively provide rationales at a task-appropriate level of granularity. ERASER provides a resource to develop such models. \nIn sum, we introduce the ERASER benchmark ( www.eraserbenchmark.com ), a uniﬁed set of di- verse NLP datasets (these are repurposed and aug- mented from existing corpora,   including senti- ment analysis, Natural Language Inference, and QA tasks, among others) in a standardized for- mat featuring human rationales for decisions, along with starter code and tools, baseline models, and standardized (initial) metrics for rationales. \n2 Related Work \nInterpret ability in NLP is a large, fast-growing area; we do not attempt to provide a comprehensive overview here. Instead we focus on directions par- ticularly relevant to ERASER, i.e., prior work on models that provide rationales for their predictions. \nLearning to explain . In ERASER we assume that rationales (marked by humans) are provided during training. However, such direct supervision will not always be available, motivating work on methods that can explain (or “rationalize”) model predic- tions using only instance-level supervision. \n\nIn the context of modern neural models for text classiﬁcation, one might use variants of  attention ( Bahdanau et al. ,  2015 ) to extract rationales. At- tention mechanisms learn to assign soft weights to (usually contextualized) token representations, and so one can extract highly weighted tokens as ratio- nales. However, attention weights do not in gen- eral provide faithful explanations for predictions ( Jain and Wallace ,  2019 ;  Serrano and Smith ,  2019 ; Wiegreffe and Pinter ,  2019 ;  Zhong et al. ,  2019 ; Pruthi et al. ,  2020 ;  Brunner et al. ,  2020 ;  Moradi et al. ,  2019 ;  Vashishth et al. ,  2019 ). This likely owes to encoders entangling inputs, complicating the interpretation of attention weights on  inputs over  contextualized representations  of the same. \nBy contrast,  hard  attention mechanisms dis- cretely extract snippets from the input to pass to the classiﬁer, by construction providing faithful expla- nations. Recent work has proposed hard attention mechanisms as a means of providing explanations. Lei et al.  ( 2016 ) proposed instantiating two models with their own parameters; one to extract rationales, and one that consumes these to make a prediction. They trained these models jointly via REINFORCE ( Williams ,  1992 ) style optimization. \nRecently,  Jain et al.  ( 2020 ) proposed a variant of this two-model setup that uses heuristic feature scores to derive pseudo-labels on tokens compris- ing rationales; one model can then be used to per- form hard extraction in this way, while a second (independent) model can make predictions on the basis of these. Elsewhere,  Chang et al.  ( 2019 ) introduced the notion of classwise rationales that explains support for different output classes using a game theoretic framework. Finally, other recent work has proposed using a differentiable binary mask over inputs, which also avoids recourse to REINFORCE ( Bastings et al. ,  2019 ). \nPost-hoc explanation . Another strand of inter- pretability work considers  post-hoc  explanation methods, which seek to explain why a model made a speciﬁc prediction for a given input. Commonly these take the form of token-level importance scores. Gradient-based explanations are a standard example ( Sundararajan et al. ,  2017 ;  Smilkov et al. , 2017 ). These enjoy a clear semantics (describing how perturbing inputs locally affects outputs), but may nonetheless exhibit counter intuitive behaviors ( Feng et al. ,  2018 ). "}
{"page": 2, "image_path": "doc_images/2020.acl-main.408_2.jpg", "ocr_text": "these take the form of token-level importance\nscores. Gradient-based explanations are a standard\nexample (Sundararajan et al., 2017; Smilkov et al.,\n2017). These enjoy a clear semantics (describing\nhow perturbing inputs locally affects outputs), but\nmay nonetheless exhibit counterintuitive behaviors\n(Feng et al., 2018).\n\nGradients of course assume model differentia-\nbility. Other methods do not require any model\nproperties. Examples include LIME (Ribeiro et al.,\n2016) and Alvarez-Melis and Jaakkola (2017);\nthese methods approximate model behavior lo-\ncally by having it repeatedly make predictions over\nperturbed inputs and fitting a simple, explainable\nmodel over the outputs.\n\nAcquiring rationales. Aside from interpretability\nconsiderations, collecting rationales from annota-\ntors may afford greater efficiency in terms of model\nperformance realized given a fixed amount of anno-\ntator effort (Zaidan and Eisner, 2008). In particular,\nrecent work by McDonnell et al. (2017, 2016) has\nobserved that at least for some tasks, asking anno-\ntators to provide rationales justifying their catego-\nrizations does not impose much additional effort.\nCombining rationale annotation with active learn-\ning (Settles, 2012) is another promising direction\n(Wallace et al., 2010; Sharma et al., 2015).\n\nLearning from rationales. Work on learning from\nrationales marked by annotators for text classifica-\ntion dates back over a decade (Zaidan et al., 2007).\nEarlier efforts proposed extending standard dis-\ncriminative models like Support Vector Machines\n(SVMs) with regularization terms that penalized\nparameter estimates which disagreed with provided\nrationales (Zaidan et al., 2007; Small et al., 2011).\nOther efforts have attempted to specify generative\nmodels of rationales (Zaidan and Eisner, 2008).\nMore recent work has aimed to exploit ratio-\nnales in training neural text classifiers. Zhang et al.\n(2016) proposed a rationale-augmented Convolu-\ntional Neural Network (CNN) for text classifica-\ntion, explicitly trained to identify sentences support-\ning categorizations. Strout et al. (2019) showed that\nproviding this model with rationales during train-\ning yields predicted rationales that are preferred\nby humans (compared to rationales produced with-\nout explicit supervision). Other work has proposed\n‘pipeline’ approaches in which independent mod-\nels are trained to perform rationale extraction and\nclassification on the basis of these, respectively\n(Lehman et al., 2019; Chen et al., 2019), assuming\n\nName Size (train/dev/test) Tokens Comp?\nEvidence Inference 7958 / 972/959 4761 °\nBoolQ 6363 / 1491 / 2817 3583 °\nMovie Reviews 1600 / 200 / 200 774 °\nFEVER 97957 / 6122/6111 327 v\nMultiRC 24029 / 3214 / 4848 303 v\nCoS-E 8733 / 1092 / 1092 28 v\ne-SNLI 911938 / 16449 / 16429 16 v\n\nTable 1: Overview of datasets in the ERASER bench-\nmark. Tokens is the average number of tokens in each\ndocument. Comprehensive rationales mean that all sup-\nporting evidence is marked; V denotes cases where this\nis (more or less) true by default; o, ¢ are datasets for\nwhich we have collected comprehensive rationales for\neither a subset or all of the test datasets, respectively.\nAdditional information can be found in Appendix A.\n\nexplicit training data is available for the former.\nRajani et al. (2019) fine-tuned a Transformer-\nbased language model (Radford et al., 2018) on\nfree-text rationales provided by humans, with an\nobjective of generating open-ended explanations to\nimprove performance on downstream tasks.\n\nEvaluating rationales. Work on evaluating ratio-\nnales has often compared these to human judg-\nments (Strout et al., 2019; Doshi-Velez and Kim,\n2017), or elicited other human evaluations of ex-\nplanations (Ribeiro et al., 2016; Lundberg and Lee,\n2017; Nguyen, 2018). There has also been work on\nvisual evaluations of saliency maps (Li et al., 2016;\nDing et al., 2017; Sundararajan et al., 2017).\n\nMeasuring agreement between extracted and\nhuman rationales (or collecting subjective assess-\nments of them) assesses the plausibility of ratio-\nnales, but such approaches do not establish whether\nthe model actually relied on these particular ratio-\nnales to make a prediction. We refer to rationales\nthat correspond to the inputs most relied upon to\ncome to a disposition as faithful.\n\nMost automatic evaluations of faithfulness mea-\nsure the impact of perturbing or erasing words or\ntokens identified as important on model output (Ar-\nras et al., 2017; Montavon et al., 2017; Serrano and\nSmith, 2019; Samek et al., 2016; Jain and Wallace,\n2019). We build upon these methods in Section\n4. Finally, we note that a recent article urges the\ncommunity to evaluate faithfulness on a continuous\nscale of acceptability, rather than viewing this as a\nbinary proposition (Jacovi and Goldberg, 2020).\n\n3 Datasets in ERASER\n\nFor all datasets in ERASER we distribute both ref-\nerence labels and rationales marked by humans\nas supporting these in a standardized format. We\n\n4445\n", "vlm_text": "\nGradients of course assume model differentia- bility. Other methods do not require any model properties. Examples include LIME ( Ribeiro et al. , 2016 ) and  Alvarez-Melis and Jaakkola  ( 2017 ); these methods approximate model behavior lo- cally by having it repeatedly make predictions over perturbed inputs and ﬁtting a simple, explainable model over the outputs. \nAcquiring rationales . Aside from interpret ability considerations, collecting rationales from annota- tors may afford greater efﬁciency in terms of model performance realized given a ﬁxed amount of anno- tator effort ( Zaidan and Eisner ,  2008 ). In particular, recent work by  McDonnell et al.  ( 2017 ,  2016 ) has observed that at least for some tasks, asking anno- tators to provide rationales justifying their catego- rizations does not impose much additional effort. Combining rationale annotation with  active learn- ing  ( Settles ,  2012 ) is another promising direction ( Wallace et al. ,  2010 ;  Sharma et al. ,  2015 ). \nLearning from rationales . Work on learning from rationales marked by annotators for text classiﬁca- tion dates back over a decade ( Zaidan et al. ,  2007 ). Earlier efforts proposed extending standard dis- criminative models like Support Vector Machines (SVMs) with regularization terms that penalized parameter estimates which disagreed with provided rationales ( Zaidan et al. ,  2007 ;  Small et al. ,  2011 ). Other efforts have attempted to specify  generative models of rationales ( Zaidan and Eisner ,  2008 ). \nMore recent work has aimed to exploit ratio- nales in training neural text classiﬁers.  Zhang et al. ( 2016 ) proposed a  rationale-augmented  Convolu- tional Neural Network (CNN) for text classiﬁca- tion, explicitly trained to identify sentences support- ing categorizations.  Strout et al.  ( 2019 ) showed that providing this model with rationales during train- ing yields predicted rationales that are preferred by humans (compared to rationales produced with- out explicit supervision). Other work has proposed ‘pipeline’ approaches in which independent mod- els are trained to perform rationale extraction and classiﬁcation on the basis of these, respectively ( Lehman et al. ,  2019 ;  Chen et al. ,  2019 ), assuming \nThe table lists different datasets along with their respective sizes, token counts, and whether they are marked as complete. The columns are:\n\n- **Name**: The name of the dataset.\n- **Size (train/dev/test)**: Indicates the number of samples in the training, development, and test sets.\n- **Tokens**: The number of tokens in the dataset.\n- **Comp?**: Indicates whether the dataset is marked as complete with a symbol (✓ or ◆). \n\nHere's the information for each dataset:\n\n1. **Evidence Inference**: \n   - Size: 7958 / 972 / 959\n   - Tokens: 4761\n   - Comp?: ◇\n   \n2. **BoolQ**:\n   - Size: 6363 / 1491 / 2817\n   - Tokens: 3583\n   - Comp?: ◇\n\n3. **Movie Reviews**:\n   - Size: 1600 / 200 / 200\n   - Tokens: 774\n   - Comp?: ◆\n\n4. **FEVER**:\n   - Size: 97957 / 6122 / 6111\n   - Tokens: 327\n   - Comp?: ✓\n\n5. **MultiRC**:\n   - Size: 24029 / 3214 / 4848\n   - Tokens: 303\n   - Comp?: ✓\n\n6. **CoS-E**:\n   - Size: 8733 / 1092 / 1092\n   - Tokens: 28\n   - Comp?: ✓\n\n7. **e-SNLI**:\n   - Size: 911938 / 16449 / 16429\n   - Tokens: 16\n   - Comp?: ✓\nTable 1: Overview of datasets in the ERASER bench- mark.  Tokens  is the average number of tokens in each document. Comprehensive rationales mean that all sup- porting evidence is marked;  ! denotes cases where this is (more or less) true by default;    $\\diamond,\\bullet$  are datasets for which we have collected comprehensive rationales for either a subset or all of the test datasets, respectively. Additional information can be found in Appendix  A . \nexplicit training data is available for the former. Rajani et al.  ( 2019 ) ﬁne-tuned a Transformer- based language model ( Radford et al. ,  2018 ) on free-text rationales provided by humans, with an objective of generating open-ended explanations to improve performance on downstream tasks. \nEvaluating rationales . Work on evaluating ratio- nales has often compared these to human judg- ments ( Strout et al. ,  2019 ;  Doshi-Velez and Kim , 2017 ), or elicited other human evaluations of ex- planations ( Ribeiro et al. ,  2016 ;  Lundberg and Lee , 2017 ;  Nguyen ,  2018 ). There has also been work on visual evaluations of saliency maps ( Li et al. ,  2016 ; Ding et al. ,  2017 ;  Sundararajan et al. ,  2017 ). \nMeasuring agreement between extracted and human rationales (or collecting subjective assess- ments of them) assesses the plausibility of ratio- nales, but such approaches do not establish whether the model actually relied on these particular ratio- nales to make a prediction. We refer to rationales that correspond to the inputs most relied upon to come to a disposition as  faithful . \nMost automatic evaluations of faithfulness mea- sure the impact of perturbing or erasing words or tokens identiﬁed as important on model output ( Ar- ras et al. ,  2017 ;  Montavon et al. ,  2017 ;  Serrano and Smith ,  2019 ;  Samek et al. ,  2016 ;  Jain and Wallace , 2019 ). We build upon these methods in Section 4 . Finally, we note that a recent article urges the community to evaluate faithfulness on a continuous scale of acceptability, rather than viewing this as a binary proposition ( Jacovi and Goldberg ,  2020 ). \n3 Datasets in ERASER \nFor all datasets in ERASER we distribute both ref- erence labels and rationales marked by humans as supporting these in a standardized format. We delineate train, validation, and test splits for all corpora (see Appendix  A  for processing details). We ensure that these splits comprise disjoint sets of source documents to avoid contamination.   We have made the decision to distribute the test sets publicly,   in part because we do not view the ‘cor- rect’ metrics to use as settled. We plan to acquire additional human annotations on held-out portions of some of the included corpora so as to offer hid- den test set evaluation opportunities in the future. "}
{"page": 3, "image_path": "doc_images/2020.acl-main.408_3.jpg", "ocr_text": "delineate train, validation, and test splits for all\ncorpora (see Appendix A for processing details).\nWe ensure that these splits comprise disjoint sets\nof source documents to avoid contamination.? We\nhave made the decision to distribute the test sets\npublicly,’ in part because we do not view the ‘cor-\nrect’ metrics to use as settled. We plan to acquire\nadditional human annotations on held-out portions\nof some of the included corpora so as to offer hid-\nden test set evaluation opportunities in the future.\n\nEvidence inference (Lehman et al., 2019). A\ndataset of full-text articles describing randomized\ncontrolled trials (RCTs). The task is to infer\nwhether a given intervention is reported to either\nsignificantly increase, significantly decrease, or\nhave no significant effect on a specified outcome, as\ncompared to a comparator of interest. Rationales\nhave been marked as supporting these inferences.\nAs the original annotations are not necessarily ex-\nhaustive, we collected exhaustive rationale annota-\ntions on a subset of the validation and test data.>\n\nBoolQ (Clark et al., 2019). This corpus consists\nof passages selected from Wikipedia, and yes/no\nquestions generated from these passages. As the\noriginal Wikipedia article versions used were not\nmaintained, we have made a best-effort attempt to\nrecover these, and then find within them the pas-\nsages answering the corresponding questions. For\npublic release, we acquired comprehensive annota-\ntions on a subset of documents in our test set.\n\nMovie Reviews (Zaidan and Eisner, 2008). In-\ncludes positive/negative sentiment labels on movie\nreviews. Original rationale annotations were not\nnecessarily comprehensive; we thus collected com-\nprehensive rationales on the final two folds of the\noriginal dataset (Pang and Lee, 2004).° In contrast\nto most other datasets, the rationale annotations\nhere are span level as opposed to sentence level.\n\nFEVER (Thorne et al., 2018). Short for Fact Ex-\ntraction and VERification; entails verifying claims\nfrom textual sources. Specifically, each claim is to\nbe classified as supported, refuted or not enough\ninformation with reference to a collection of source\n\n3Except for BoolQ, wherein source documents in the orig-\ninal train and validation set were not disjoint and we preserve\nthis structure in our dataset. Questions, of course, are disjoint.\n\n‘Consequently, for datasets that have been part of previ-\nous benchmarks with other aims (namely, GLUE/superGLUE)\nbut which we have re-purposed for work on rationales in\nERASER, e.g., BoolQ (Clark et al., 2019), we have carved out\nfor release test sets from the original validation sets.\n\n> Annotation details are in Appendix B.\n\ntexts. We take a subset of this dataset, including\nonly supported and refuted claims.\n\nMultiRC (Khashabi et al., 2018). A reading com-\nprehension dataset composed of questions with\nmultiple correct answers that by construction de-\npend on information from multiple sentences. Here\neach rationale is associated with a question, while\nanswers are independent of one another. We con-\nvert each rationale/question/answer triplet into an\ninstance within our dataset. Each answer candidate\nthen has a label of True or False.\n\nCommonsense Explanations (CoS-E) (Rajani\net al., 2019). This corpus comprises multiple-\nchoice questions and answers from (Talmor et al.,\n2019) along with supporting rationales. The ratio-\nnales in this case come in the form both of high-\nlighted (extracted) supporting snippets and free-\ntext, open-ended descriptions of reasoning. Given\nour focus on extractive rationales, ERASER in-\ncludes only the former for now. Following Talmor\net al. (2019), we repartition the training and valida-\ntion sets to provide a canonical test split.\n\ne-SNLI (Camburu et al., 2018). This dataset aug-\nments the SNLI corpus (Bowman et al., 2015) with\nrationales marked in the premise and/or hypothesis\n(and natural language explanations, which we do\nnot use). For entailment pairs, annotators were re-\nquired to highlight at least one word in the premise.\nFor contradiction pairs, annotators had to highlight\nat least one word in both the premise and the hy-\npothesis; for neutral pairs, they were only allowed\nto highlight words in the hypothesis.\n\nHuman Agreement We report human agreement\nover extracted rationales for multiple annotators\nand documents in Table 2. All datasets have a high\nCohen «& (Cohen, 1960); with substantial or better\nagreement.\n\n4 Metrics\n\nIn ERASER models are evaluated both for their\npredictive performance and with respect to the ra-\ntionales that they extract. For the former, we rely\non the established metrics for the respective tasks.\nHere we describe the metrics we propose to eval-\nuate the quality of extracted rationales. We do\nnot claim that these are necessarily the best met-\nrics for evaluating rationales, however. Indeed, we\nhope the release of ERASER will spur additional\nresearch into how best to measure the quality of\nmodel explanations in the context of NLP.\n\n4446\n", "vlm_text": "\nEvidence inference  ( Lehman et al. ,  2019 ). A dataset of full-text articles describing randomized controlled trials (RCTs). The task is to infer whether a given  intervention  is reported to either signiﬁcantly increase ,  signiﬁcantly decrease , or have  no signiﬁcant effect  on a speciﬁed  outcome , as compared to a  comparator  of interest. Rationales have been marked as supporting these inferences. As the original annotations are not necessarily ex- haustive, we collected exhaustive rationale annota- tions on a subset of the validation and test data. \nBoolQ  ( Clark et al. ,  2019 ). This corpus consists of passages selected from Wikipedia, and yes/no questions generated from these passages. As the original Wikipedia article versions used were not maintained, we have made a best-effort attempt to recover these, and then ﬁnd within them the pas- sages answering the corresponding questions. For public release, we acquired comprehensive annota- tions on a subset of documents in our test set. \nMovie Reviews  ( Zaidan and Eisner ,  2008 ). In- cludes positive/negative sentiment labels on movie reviews. Original rationale annotations were not necessarily comprehensive; we thus collected com- prehensive rationales on the ﬁnal two folds of the original dataset ( Pang and Lee ,  2004 ).   In contrast to most other datasets, the rationale annotations here are  span level  as opposed to sentence level. \nFEVER  ( Thorne et al. ,  2018 ). Short for Fact Ex- traction and VERiﬁcation; entails verifying claims from textual sources. Speciﬁcally, each claim is to be classiﬁed as  supported ,  refuted  or  not enough information  with reference to a collection of source texts. We take a subset of this dataset, including only  supported  and  refuted  claims. \n\nMultiRC  ( Khashabi et al. ,  2018 ). A reading com- prehension dataset composed of questions with multiple correct answers that by construction de- pend on information from multiple sentences. Here each rationale is associated with a question, while answers are independent of one another. We con- vert each rationale/question/answer triplet into an instance within our dataset. Each answer candidate then has a label of  True  or  False . \nCommonsense Explanations (CoS-E)  ( Rajani et al. ,  2019 ). This corpus comprises multiple- choice questions and answers from ( Talmor et al. , 2019 ) along with supporting rationales. The ratio- nales in this case come in the form both of high- lighted (extracted) supporting snippets and free- text, open-ended descriptions of reasoning. Given our focus on extractive rationales, ERASER in- cludes only the former for now. Following  Talmor et al.  ( 2019 ), we repartition the training and valida- tion sets to provide a canonical test split. \ne-SNLI  ( Camburu et al. ,  2018 ). This dataset aug- ments the SNLI corpus ( Bowman et al. ,  2015 ) with rationales marked in the premise and/or hypothesis (and natural language explanations, which we do not use). For entailment pairs, annotators were re- quired to highlight at least one word in the premise. For contradiction pairs, annotators had to highlight at least one word in both the premise and the hy- pothesis; for neutral pairs, they were only allowed to highlight words in the hypothesis. \nHuman Agreement  We report human agreement over extracted rationales for multiple annotators and documents in Table  2 . All datasets have a high Cohen    $\\kappa$   ( Cohen ,  1960 ); with substantial or better agreement. \n4 Metrics \nIn ERASER models are evaluated both for their predictive performance and with respect to the ra- tionales that they extract. For the former, we rely on the established metrics for the respective tasks. Here we describe the metrics we propose to eval- uate the quality of extracted rationales. We do not claim that these are necessarily the best met- rics for evaluating rationales, however. Indeed, we hope the release of ERASER will spur additional research into how best to measure the quality of model explanations in the context of NLP. "}
{"page": 4, "image_path": "doc_images/2020.acl-main.408_4.jpg", "ocr_text": "Dataset Cohen & Fl P R #Annotators/doc _ #Documents\nEvidence Inference - - - - -\nBoolQ 0.618 +0.194 0.617 + 0.227 0.647 + 0.260 0.726 + 0.217 3 199\nMovie Reviews 0.712 + 0.135 0.799 + 0.138 0.693 + 0.153 0.989 + 0.102 2 96\nFEVER 0.854 +0.196 0.871 + 0.197 0.931 + 0.205 0.855 + 0.198 2 24\nMultiRC 0.728 + 0.268 0.749 + 0.265 0.695 + 0.284 0.910 + 0.259 2 99\nCoS-E 0.619 + 0.308 0.654 + 0.317 0.626 + 0.319 0.792 + 0.371 2 100\ne-SNLI 0.743 + 0.162 0.799 + 0.130 0.812 + 0.154 0.853 + 0.124 3 9807\n\nTable 2: Human agreement with respect to rationales. For Movie Reviews and BoolQ we calculate the mean\nagreement of individual annotators with the majority vote per token, over the two-three annotators we hired via\nUpwork and Amazon Turk, respectively. The e-SNLI dataset already comprised three annotators; for this we\ncalculate mean agreement between individuals and the majority. For CoS-E, MultiRC, and FEVER, members of\nour team annotated a subset to use a comparison to the (majority of, where appropriate) existing rationales. We\ncollected comprehensive rationales for Evidence Inference from Medical Doctors; as they have a high amount of\nexpertise, we would expect agreement to be high, but have not collected redundant comprehensive annotations.\n\n4.1 Agreement with human rationales\n\nThe simplest means of evaluating extracted ratio-\nnales is to measure how well they agree with those\nmarked by humans. We consider two classes of\nmetrics, appropriate for models that perform dis-\ncrete and ‘soft’ selection, respectively.\n\nFor the discrete case, measuring exact matches\nbetween predicted and reference rationales is likely\ntoo harsh.© We thus consider more relaxed mea-\nsures. These include Intersection-Over-Union\n(IOU), borrowed from computer vision (Evering-\nham et al., 2010), which permits credit assignment\nfor partial matches. We define IOU on a token level:\nfor two spans, it is the size of the overlap of the\ntokens they cover divided by the size of their union.\nWe count a prediction as a match if it overlaps with\nany of the ground truth rationales by more than\nsome threshold (here, 0.5). We use these partial\nmatches to calculate an Fl score. We also measure\ntoken-level precision and recall, and use these to\nderive token-level F1 scores.\n\nMetrics for continuous or soft token scoring\nmodels consider token rankings, rewarding models\nfor assigning higher scores to marked tokens. In\nparticular, we take the Area Under the Precision-\nRecall curve (AUPRC) constructed by sweeping a\nthreshold over token scores. We define additional\nmetrics for soft scoring models below.\n\nIn general, the rationales we have for tasks are\nsufficient to make judgments, but not necessarily\ncomprehensive. However, for some datasets we\nhave explicitly collected comprehensive rationales\nfor at least a subset of the test set. Therefore, on\nthese datasets recall evaluates comprehensiveness\ndirectly (it does so only noisily on other datasets).\n\n°Consider that an extra token destroys the match but not\nusually the meaning\n\nWe highlight which corpora contain comprehensive\nrationales in the test set in Table 3.\n\n4.2 Measuring faithfulness\n\nAs discussed above, a model may provide ratio-\nnales that are plausible (agreeable to humans) but\nthat it did not rely on for its output. In many set-\ntings one may want rationales that actually explain\nmodel predictions, i.e., rationales extracted for an\ninstance in this case ought to have meaningfully in-\nfluenced its prediction for the same. We call these\nfaithful rationales. How best to measure rationale\nfaithfulness is an open question. In this first version\nof ERASER we propose simple metrics motivated\nby prior work (Zaidan et al., 2007; Yu et al., 2019).\nIn particular, following Yu et al. (2019) we define\nmetrics intended to measure the comprehensiveness\n(were all features needed to make a prediction se-\nlected?) and sufficiency (do the extracted rationales\ncontain enough signal to come to a disposition?) of\nrationales, respectively.\n\nComprehensiveness. To calculate rationale\ncomprehensiveness we create contrast exam-\nples (Zaidan et al., 2007): We construct a con-\ntrast example for x;, £;, which is x; with the pre-\ndicted rationales r; removed. Assuming a classifi-\ncation setting, let m(x;); be the original prediction\nprovided by a model m for the predicted class j.\nThen we consider the predicted probability from\nthe model for the same class once the supporting\nrationales are stripped. Intuitively, the model ought\nto be less confident in its prediction once rationales\nare removed from x;. We can measure this as:\n\ncomprehensiveness = m(2;); —m(ai\\ri); (1)\n\nA high score here implies that the rationales were\nindeed influential in the prediction, while a low\nscore suggests that they were not. A negative value\n\n4447\n", "vlm_text": "The table provides metrics for different datasets in the context of some evaluation, possibly related to annotation or inference tasks. Here's a breakdown of the columns and what they represent:\n\n- **Dataset**: The name of the dataset being evaluated.\n- **Cohen κ**: Cohen's kappa, a statistical measure of inter-annotator agreement.\n- **F1**: F1 score, the harmonic mean of precision and recall, indicating the balance between the two.\n- **P**: Precision, the fraction of relevant instances among the retrieved instances.\n- **R**: Recall, the fraction of relevant instances that have been retrieved over the total amount of relevant instances.\n- **#Annotators/doc**: The number of annotators per document.\n- **#Documents**: The number of documents in the dataset.\n\nEach row corresponds to a dataset and provides the values for these metrics, except for the \"Evidence Inference\" dataset, where the values are not provided. The numbers are presented with a ± symbol, indicating an average value with some deviation.\nTable 2: Human agreement with respect to rationales. For Movie Reviews and BoolQ we calculate the mean agreement of individual annotators with the majority vote per token, over the two-three annotators we hired via Upwork and Amazon Turk, respectively. The e-SNLI dataset already comprised three annotators; for this we calculate mean agreement between individuals and the majority. For CoS-E, MultiRC, and FEVER, members of our team annotated a subset to use a comparison to the (majority of, where appropriate) existing rationales. We collected comprehensive rationales for Evidence Inference from Medical Doctors; as they have a high amount of expertise, we would expect agreement to be high, but have not collected redundant comprehensive annotations. \n4.1 Agreement with human rationales \nThe simplest means of evaluating extracted ratio- nales is to measure how well they agree with those marked by humans. We consider two classes of metrics, appropriate for models that perform dis- crete and ‘soft’ selection, respectively. \nFor the discrete case, measuring exact matches between predicted and reference rationales is likely too harsh.   We thus consider more relaxed mea- sures. These include Intersection-Over-Union (IOU), borrowed from computer vision ( Evering- ham et al. ,  2010 ), which permits credit assignment for partial matches. We deﬁne IOU on a token level: for two spans, it is the size of the overlap of the tokens they cover divided by the size of their union. We count a prediction as a match if it overlaps with any of the ground truth rationales by more than some threshold (here, 0.5). We use these partial matches to calculate an F1 score. We also measure token -level precision and recall, and use these to derive token-level F1 scores. \nMetrics for continuous or soft token scoring models consider token rankings, rewarding models for assigning higher scores to marked tokens. In particular, we take the Area Under the Precision- Recall curve (AUPRC) constructed by sweeping a threshold over token scores. We deﬁne additional metrics for soft scoring models below. \nIn general, the rationales we have for tasks are sufﬁcient  to make judgments, but not necessarily comprehensive . However, for some datasets we have explicitly collected comprehensive rationales for at least a subset of the test set. Therefore, on these datasets  recall  evaluates comprehensiveness directly (it does so only noisily on other datasets). \nWe highlight which corpora contain comprehensive rationales in the test set in Table  3 . \n4.2 Measuring faithfulness \nAs discussed above, a model may provide ratio- nales that are plausible (agreeable to humans) but that it did not rely on for its output. In many set- tings one may want rationales that actually  explain model predictions, i.e., rationales extracted for an instance in this case ought to have meaningfully in- ﬂuenced its prediction for the same. We call these faithful rationales. How best to measure rationale faithfulness is an open question. In this ﬁrst version of ERASER we propose simple metrics motivated by prior work ( Zaidan et al. ,  2007 ;  Yu et al. ,  2019 ). In particular, following  Yu et al.  ( 2019 ) we deﬁne metrics intended to measure the  comprehensiveness (were all features needed to make a prediction se- lected?) and  sufﬁciency  (do the extracted rationales contain enough signal to come to a disposition?) of rationales, respectively. \nComprehensiveness . To calculate rationale comprehensiveness we create  contrast  exam- ples ( Zaidan et al. ,  2007 ): We construct a con- trast example for    $x_{i},\\,\\tilde{x}_{i}$  , which is    $x_{i}$   with the pre- dicted rationales  $r_{i}$   removed. Assuming a classiﬁ- cation setting, let    $m(x_{i})_{j}$   be the original predictio provided by a model  m  for the predicted class  j . Then we consider the predicted probability from the model for the same class once the supporting rationales are stripped. Intuitively, the model ought to be less conﬁdent in its prediction once rationales are removed from    $x_{i}$  . We can measure this as: \n\n$$\n{\\mathrm{compress}}=m(x_{i})_{j}-m(x_{i}\\backslash r_{i})_{j}\n$$\n \nA high score here implies that the rationales were indeed inﬂuential in the prediction, while a low score suggests that they were not. A negative value "}
{"page": 5, "image_path": "doc_images/2020.acl-main.408_5.jpg", "ocr_text": "Comprehensiveness\n\n[| S = Suffiency\nanE=8 Bnate |\nSteee Settee SEettee\nfey nT Qo na O° 7a\na a! 52323 52328\n324% 3 ey 2 3 EY 2 3\n% 3 a 3 3 * 4 3 3 * 4\ni) eo eo\no ic oe\n{°) 99 99\nWY a R&S Coe\nWhere do you find the most amount of leafs?\n\n‘Where do you find the\n\nmost amount of leafs?\n\nvi\n\n2; r%\n\nFigure 2: Illustration of faithfulness scoring metrics, comprehensiveness and sufficiency, on the Commonsense\nExplanations (CoS-E) dataset. For the former, erasing the tokens comprising the provided rationale (7;) ought to\n\ndecrease model confidence in the output ‘Forest’. For the latter, the model should be able to come to a similar\ndisposition regarding ‘Forest’ using only the rationales r;.\n\nhere means that the model became more confident\nin its prediction after the rationales were removed;\nthis would seem counter-intuitive if the rationales\nwere indeed the reason for its prediction.\nSufficiency. This captures the degree to which\nthe snippets within the extracted rationales are ade-\nquate for a model to make a prediction.\n\nsufficiency = m(a;); — m(ri); (2)\n\nThese metrics are illustrated in Figure 2.\n\nAs defined, the above measures have assumed\ndiscrete rationales r;. We would also like to eval-\nuate the faithfulness of continuous importance\nscores assigned to tokens by models. Here we\nadopt a simple approach for this. We convert soft\nscores over features s; provided by a model into\ndiscrete rationales r; by taking the top—kg values,\n\nwhere ky is a threshold for dataset d. We set kg to\n\nthe average rationale length provided by humans\nfor dataset d (see Table 4). Intuitively, this says:\nHow much does the model prediction change if we\nremove a number of tokens equal to what humans\nuse (on average for this dataset) in order of the\n\nimportance scores assigned to these by the model.\nOnce we have discretized the soft scores into ra-\ntionales in this way, we compute the faithfulness\nscores as per Equations | and 2.\n\nThis approach is conceptually simple. It is also\ncomputationally cheap to evaluate, in contrast to\nmeasures that require per-token measurements, e.g.,\nimportance score correlations with ‘leave-one-out’\nscores (Jain and Wallace, 2019), or counting how\nmany ‘important’ tokens need to be erased before\n\na prediction flips (Serrano and Smith, 2019). How-\never, the necessity of discretizing continuous scores\nforces us to pick a particular threshold k.\n\nWe can also consider the behavior of these mea-\nsures as a function of k, inspired by the measure-\nments proposed in Samek et al. (2016) in the con-\ntext of evaluating saliency maps for image classi-\nfication. They suggested ranking pixel regions by\nimportance and then measuring the change in out-\n\nput as they are removed in rank order. Our datasets\ncomprise documents and rationales with quite dif-\nferent lengths; to make this measure comparable\nacross datasets, we construct bins designating the\nnumber of tokens to be deleted. Denoting the to-\nkens up to and including bin & for instance i by riz,\n\nwe define an aggregate comprehensiveness mea-\nsure:\n\n1 8\nTESIO>} m(wi)j —m(axi\\riz)s) (3)\n\nThis is defined for sufficiency analogously. Here\n\nwe group tokens into k = 5 bins by grouping them\ninto the top 1%, 5%, 10%, 20% and 50% of to-\nkens, with respect to the corresponding importance\nscore. We refer to these metrics as “Area Over the\nPerturbation Curve” (AOPC).”\n\nThese AOPC sufficiency and comprehensiveness\nmeasures score a particular token ordering under\na model. As a point of reference, we also report\nthese when random scores are assigned to tokens.\n\n7Our AOPC metrics are similar in concept to ROAR\n(Hooker et al., 2019) except that we re-use an existing model\nas opposed to retraining for each fraction.\n\n4448\n", "vlm_text": "The image is an illustration of two faithfulness scoring metrics, comprehensiveness and sufficiency, as applied to the Commonsense Explanations (CoS-E) dataset. It consists of three parts:\n\n1. **Left Section**: The input text, \\( x_i \\), is: \"Where do you find the most amount of leafs?\" It goes through a model which results in a probability distribution over five options: (a) Compost pile, (b) Flowers, (c) Forest, (d) Field, and (e) Ground. The probability for \"Forest\" is the highest, indicated by a prominent red bar in the chart, showing the model's confidence in this prediction.\n\n2. **Middle Section**: The modified input, \\( \\tilde{x}_{i} \\), is \"Where do you find the most amount of?\" The rationale is erased (the phrase \"most amount of leafs\" is greyed out). The resulting probability distribution shows the model's reduced confidence in the \"Forest\" prediction, which illustrates the comprehensiveness metric: when the rationale is removed, the confidence for the chosen option should decrease.\n\n3. **Right Section**: The rationale, \\( r_{i} \\), is just \"Where do you find the most amount of leafs?\" (only the rationale is present). The model evaluates this input and arrives at a high confidence level for \"Forest,\" even without the full context provided earlier. This demonstrates the sufficiency metric: using just the rationale should allow the model to maintain similar confidence in its prediction.\nhere means that the model became  more  conﬁdent in its prediction after the rationales were removed; this would seem counter-intuitive if the rationales were indeed the reason for its prediction. \nSufﬁciency . This captures the degree to which the snippets within the extracted rationales are ade- quate for a model to make a prediction. \n\n$$\n\\mathrm{sufficiently}=m({x}_{i})_{j}-m({r}_{i})_{j}\n$$\n \nThese metrics are illustrated in Figure  2 . \nAs deﬁned, the above measures have assumed discrete rationales  $r_{i}$  . We would also like to eval- uate the faithfulness of continuous importance scores assigned to tokens by models. Here we adopt a simple approach for this. We convert soft scores over features    $s_{i}$   provided by a model into discre ationales    $r_{i}$   by taking the op  $-k_{d}$   v es, where  $k_{d}$   is a threshold for dataset  d . We set  $k_{d}$   to the average rationale length provided by humans for dataset    $d$   (see Table  4 ). Intuitively, this says: How much does the model prediction change if we remove a number of tokens equal to what humans use (on average for this dataset) in order of the importance scores assigned to these by the model. Once we have discretized the soft scores into ra- tionales in this way, we compute the faithfulness scores as per Equations  1  and  2 . \nThis approach is conceptually simple. It is also computationally cheap to evaluate, in contrast to measures that require per-token measurements, e.g., importance score correlations with ‘leave-one-out’ scores ( Jain and Wallace ,  2019 ), or counting how many ‘important’ tokens need to be erased before a prediction ﬂips ( Serrano and Smith ,  2019 ). How- ever, the necessity of discretizing continuous scores forces us to pick a particular threshold    $k$  . \n\nWe can also consider the behavior of these mea- sures as a function of    $k$  , inspired by the measure- ments proposed in  Samek et al.  ( 2016 ) in the con- text of evaluating saliency maps for image classi- ﬁcation. They suggested ranking pixel regions by importance and then measuring the change in out- put as they are removed in rank order. Our datasets comprise documents and rationales with quite dif- ferent lengths; to make this measure comparable across datasets, we construct bins designating the number of tokens to be deleted. Denoting the to- kens up to and including bin  $k$   for instance    $i$   by  $r_{i k}$  , we deﬁne an aggregate comprehensiveness mea- sure: \n\n$$\n\\frac{1}{|\\mathcal{B}|+1}\\big(\\sum_{k=0}^{|\\mathcal{B}|}m({x}_{i})_{j}-m({x}_{i}\\backslash{r}_{i k})_{j}\\big)\n$$\n \nThis is deﬁned for sufﬁciency analogously. Here we group tok    $k=5$  y gro  them into the top 1%, 5%, 10%, 20% and 50% of to- kens, with respect to the corresponding importance score. We refer to these metrics as “Area Over the Perturbation Curve” (AOPC). \nThese AOPC sufﬁciency and comprehensiveness measures score a particular token ordering under a model. As a point of reference, we also report these when  random  scores are assigned to tokens. "}
{"page": 6, "image_path": "doc_images/2020.acl-main.408_6.jpg", "ocr_text": "5 Baseline Models\n\nOur focus in this work is primarily on the ERASER\nbenchmark itself, rather than on any particular\nmodel(s). But to establish a starting point for future\nwork, we evaluate several baseline models across\nthe corpora in ERASER.® We broadly classify these\ninto models that assign ‘soft’ (continuous) scores\nto tokens, and those that perform a ‘hard’ (discrete)\nselection over inputs. We additionally consider\nmodels specifically designed to select individual\ntokens (and very short sequences) as rationales, as\ncompared to longer snippets. All of our implemen-\ntations are in PyTorch (Paszke et al., 2019) and are\navailable in the ERASER repository.”\n\nAll datasets in ERASER comprise inputs, ratio-\nnales, and labels. But they differ considerably in\ndocument and rationale lengths (Table A). This mo-\ntivated use of different models for datasets, appro-\npriate to their sizes and rationale granularities. We\nhope that this benchmark motivates design of mod-\nels that provide rationales that can flexibly adapt to\nvarying input lengths and expected rationale gran-\nularities. Indeed, only with such models can we\nperform comparisons across all datasets.\n\n5.1 Hard selection\n\nModels that perform hard selection may be viewed\nas comprising two independent modules: an en-\ncoder which is responsible for extracting snippets\nof inputs, and a decoder that makes a prediction\nbased only on the text provided by the encoder. We\nconsider two variants of such models.\n\nLei et al. (2016). In this model, an encoder in-\nduces a binary mask over inputs x, z. The decoder\naccepts the tokens in x unmasked by z to make a\nprediction y. These modules are trained jointly via\nREINFORCE (Williams, 1992) style estimation,\nminimizing the loss over expected binary vectors\nz yielded from the encoder. One of the advantages\nof this approach is that it need not have access to\nmarked rationales; it can learn to rationalize on the\nbasis of instance labels alone. However, given that\nwe do have rationales in the training data, we exper-\niment with a variant in which we train the encoder\nexplicitly using rationale-level annotations.\n\nIn our implementation of Lei et al. (2016), we\ndrop in two independent BERT (Devlin et al., 2019)\nor GloVe (Pennington et al., 2014) base modules\n\nSThis is not intended to be comprehensive.\n°nttps://github.com/jayded/\neraserbenchmark\n\nwith bidirectional LSTMs (Hochreiter and Schmid-\nhuber, 1997) on top to induce contextualized rep-\nresentations of tokens for the encoder and decoder,\nrespectively. The encoder generates a scalar (de-\nnoting the probability of selecting that token) for\neach LSTM hidden state using a feedfoward layer\nand sigmoid. In the variant using human rationales\nduring training, we minimize cross entropy loss\nover rationale predictions. The final loss is then\na composite of classification loss, regularizers on\nrationales (Lei et al., 2016), and loss over rationale\npredictions, when available.\n\nPipeline models. These are simple models in\nwhich we first train the encoder to extract ratio-\nnales, and then train the decoder to perform pre-\ndiction using only rationales. No parameters are\nshared between the two models.\n\nHere we first consider a simple pipeline that first\nsegments inputs into sentences. It passes these,\none at a time, through a Gated Recurrent Unit\n(GRU) (Cho et al., 2014), to yield hidden represen-\ntations that we compose via an attentive decoding\nlayer (Bahdanau et al., 2015). This aggregate rep-\nresentation is then passed to a classification module\nwhich predicts whether the corresponding sentence\nis a rationale (or not). A second model, using effec-\ntively the same architecture but parameterized inde-\npendently, consumes the outputs (rationales) from\nthe first to make predictions. This simple model is\ndescribed at length in prior work (Lehman et al.,\n2019). We further consider a ‘BERT-to-BERT’\npipeline, where we replace each stage with a BERT\nmodule for prediction (Devlin et al., 2019).\n\nIn pipeline models, we train each stage indepen-\ndently. The rationale identification stage is trained\nusing approximate sentence boundaries from our\nsource annotations, with randomly sampled neg-\native examples at each epoch. The classification\nstage uses the same positive rationales as the iden-\ntification stage, a type of teacher forcing (Williams\nand Zipser, 1989) (details in Appendix C).\n\n5.2. Soft selection\n\nWe consider a model that passes tokens through\nBERT (Devlin et al., 2019) to induce contextual-\nized representations that are then passed to a bi-\ndirectional LSTM (Hochreiter and Schmidhuber,\n1997). The hidden representations from the LSTM\nare collapsed into a single vector using additive\nattention (Bahdanau et al., 2015). The LSTM layer\nallows us to bypass the 512 word limit imposed by\n\n4449\n", "vlm_text": "5 Baseline Models \nOur focus in this work is primarily on the ERASER benchmark itself, rather than on any particular model(s). But to establish a starting point for future work, we evaluate several baseline models across the corpora in ERASER.   We broadly classify these into models that assign ‘soft’ (continuous) scores to tokens, and those that perform a ‘hard’ (discrete) selection over inputs. We additionally consider models speciﬁcally designed to select individual tokens (and very short sequences) as rationales, as compared to longer snippets. All of our implemen- tations are in PyTorch ( Paszke et al. ,  2019 ) and are available in the ERASER repository. \nAll datasets in ERASER comprise inputs, ratio- nales, and labels. But they differ considerably in document and rationale lengths (Table  A ). This mo- tivated use of different models for datasets, appro- priate to their sizes and rationale granularities. We hope that this benchmark motivates design of mod- els that provide rationales that can ﬂexibly adapt to varying input lengths and expected rationale gran- ularities. Indeed, only with such models can we perform comparisons across all datasets. \n5.1 Hard selection \nModels that perform  hard  selection may be viewed as comprising two independent modules: an  en- coder  which is responsible for extracting snippets of inputs, and a  decoder  that makes a prediction based only on the text provided by the encoder. We consider two variants of such models. \nLei et al.  ( 2016 ) . In this model, an encoder in- duces a binary mask over inputs  $x,z.$  accepts the tokens in  $x$   unmasked by  $z$   to make a prediction  $\\hat{y}$  . These modules are trained jointly via REINFORCE ( Williams ,  1992 ) style estimation, minimizing the loss over expected binary vectors  $z$   yielded from the encoder. One of the advantages of this approach is that it need not have access to marked rationales; it can learn to rationalize on the basis of instance labels alone. However, given that we do have rationales in the training data, we exper- iment with a variant in which we train the encoder explicitly using rationale-level annotations. \nIn our implementation of  Lei et al.  ( 2016 ), we drop in two independent BERT ( Devlin et al. ,  2019 ) or GloVe ( Pennington et al. ,  2014 ) base modules with bidirectional LSTMs ( Hochreiter and Schmid- huber ,  1997 ) on top to induce contextualized rep- resentations of tokens for the encoder and decoder, respectively. The encoder generates a scalar (de- noting the probability of selecting that token) for each LSTM hidden state using a feedfoward layer and sigmoid. In the variant using human rationales during training, we minimize cross entropy loss over rationale predictions. The ﬁnal loss is then a composite of classiﬁcation loss, regularizers on rationales ( Lei et al. ,  2016 ), and loss over rationale predictions, when available. \n\nPipeline models . These are simple models in which we ﬁrst train the encoder to extract ratio- nales, and then train the decoder to perform pre- diction using only rationales. No parameters are shared between the two models. \nHere we ﬁrst consider a simple pipeline that ﬁrst segments inputs into sentences. It passes these, one at a time, through a Gated Recurrent Unit (GRU) ( Cho et al. ,  2014 ), to yield hidden represen- tations that we compose via an attentive decoding layer ( Bahdanau et al. ,  2015 ). This aggregate rep- resentation is then passed to a classiﬁcation module which predicts whether the corresponding sentence is a rationale (or not). A second model, using effec- tively the same architecture but parameterized inde- pendently, consumes the outputs (rationales) from the ﬁrst to make predictions. This simple model is described at length in prior work ( Lehman et al. , 2019 ). We further consider a ‘BERT-to-BERT’ pipeline, where we replace each stage with a BERT module for prediction ( Devlin et al. ,  2019 ). \nIn pipeline models, we train each stage indepen- dently. The rationale identiﬁcation stage is trained using approximate sentence boundaries from our source annotations, with randomly sampled neg- ative examples at each epoch. The classiﬁcation stage uses the same positive rationales as the iden- tiﬁcation stage, a type of  teacher forcing  ( Williams and Zipser ,  1989 ) (details in Appendix  C ). \n5.2 Soft selection \nWe consider a model that passes tokens through BERT ( Devlin et al. ,  2019 ) to induce contextual- ized representations that are then passed to a bi- directional LSTM ( Hochreiter and Schmidhuber , 1997 ). The hidden representations from the LSTM are collapsed into a single vector using additive attention ( Bahdanau et al. ,  2015 ). The LSTM layer allows us to bypass the 512 word limit imposed by "}
{"page": 7, "image_path": "doc_images/2020.acl-main.408_7.jpg", "ocr_text": "Perf. IOUFI Token FI Perf. AUPRC Comp.t — Suff. |\nEvidence Inference Evidence Inference\nLei et al. (2016) 0.461 0.000 0.000 GloVe +LSTM- Attention 0.429 0.506 -0.002  -0.023\nLei et al. (2016)(u) —-0.461-~——0.000 0.000 GloVe +LSTM-Gradient 0.429 0.016 0.046 -0.138\nLehman et al.(2019) 0.471 0.119 0.123 GloVe + LSTM - Lime 0.429 0.014 0.006 -0.128\nBert-To-Bert 0.708 0.455 0.468 GloVe +LSTM-Random 0.429 0.014 -0.001 — -0.026\nBoolQ BoolQ\nLei et al. (2016) 0.381 0.000 0.000 GloVe +LSTM- Attention 0.471 0.525 0.010 0.022\nLei et al. (2016)(u) —-0.380-——0.000 0.000 GloVe +LSTM- Gradient 0.471 0.072 0.024 0.031\nLehman et al.(2019) 0.411 0.050 0.127 GloVe + LSTM - Lime 0.471 0.073 0.028 -0.154\nBert-To-Bert 0.544 0,052 0.134 GloVe +LSTM-Random 0.471 0.074 0.000 0.005\nMovie Reviews Movies\nLei et al. (2016) 0.914 0.124 0.285 BERT+LSTM - Attention 0.970 0.417 0.129 0.097\nLei et al. (2016)(u) 0.920 0.012 0.322 BERT+LSTM -Gradient 0.970 0.385 0.142 0.112\nLehman et al. (2019) 0.750 0.063 0.139 BERT+LSTM - Lime 0.970 0.280 0.187 0.093\nBert-To-Bert 0.860 0.075 0.145 BERT+LSTM - Random 0.970 0.259 0.058 0.330\nFEVER FEVER\nLei et al. (2016) 0.719 0.218 0.234 BERT+LSTM - Attention 0.870 0.235 0.037 0.122\nLei et al. (2016)(u) 0.718 ~—:0.000 0.000 BERT+LSTM -Gradient 0.870 0.232 0.059 0.136\nLehman et al. (2019) 0.691 0.540 0.523 BERT+LSTM - Lime 0.870 0.291 0.212 0.014\nBert-To-Bert 0.877 0.835 0.812 BERT+LSTM - Random 0.870 0.244 0.034 0.122\nMultiRC MultiRC\nLei et al. (2016) 0.655 0.271 0.456 BERT+LSTM - Attention 0.655 0.244 0.036 0.052\nLei et al. (2016)(u) 0.648 -0.000'_——0.000° BERT+LSTM -Gradient 0.655 0.224 0.077 0.064\nLehman et al.(2019) 0.614 0.136 0.140 BERT+LSTM - Lime 0.655 0.208 0.213 -0.079\nBert-To-Bert 0.633 (0.416 0.412 BERT+LSTM - Random 0.655 0.186 0.029 0.081\nCoS-E CoS-E\nLei et al. (2016) 0477 0.255 0.331 BERT+LSTM - Attention 0.487 0.606 0.080 0.217\nLei et al. (2016)(u) 0.476 ~—-0.000'_—0.000° BERT+LSTM -Gradient 0.487 0.585 0.124 0.226\nBert-To-Bert 0.344 0.389 0.519 BERT+LSTM - Lime 0.487 0.544 0.223 0.143\nBERT+LSTM - Random 0.487 0.594 0.072 0.224\ne-SNLI\nLei et al. (2016) 0.917 0.693 0.692 e-SNLI\nLei et al. (2016)(u) 0.903 0.261 0.379 BERT+LSTM - Attention 0.960 0.395 0.105 0.583\nBert-To-Bert 0.733 0.704 0.701 BERT+LSTM -Gradient 0.960 0.416 0.180 0.472\nBERT+LSTM - Lime 0.960 0.513 0.437 0.389\nBERT+LSTM - Random 0.960 0.357 0.081 0.487\n\nTable 3: Performance of models that perform hard ra-\ntionale selection. All models are supervised at the ratio-\nnale level except for those marked with (u), which learn\nonly from instance-level supervision; * denotes cases in\nwhich rationale training degenerated due to the REIN-\nFORCE style training. Perf. is accuracy (CoS-E) or\nmacro-averaged F1 (others). Bert-To-Bert for CoS-E\nand e-SNLI uses a token classification objective. Bert-\nTo-Bert CoS-E uses the highest scoring answer.\n\nBERT; when we exceed this, we effectively start\nencoding a ‘new’ sequence (setting the positional\nindex to 0) via BERT. The hope is that the LSTM\nlearns to compensate for this. Evidence Inference\nand BoolQ comprise very long (>1000 token) in-\nputs; we were unable to run BERT over these. We\ninstead resorted to swapping GloVe 300d embed-\ndings (Pennington et al., 2014) in place of BERT\nrepresentations for tokens. spans.\n\nTo soft score features we consider: Simple gra-\ndients, attention induced over contextualized repre-\nsentations, and LIME (Ribeiro et al., 2016).\n\nTable 4: Metrics for ‘soft’ scoring models. Perf. is ac-\ncuracy (CoS-E) or F1 (others). Comprehensiveness and\nsufficiency are in terms of AOPC (Eq. 3). ‘Random’\nassigns random scores to tokens to induce orderings;\nthese are averages over 10 runs.\n\n6 Evaluation\n\nHere we present initial results for the baseline mod-\nels discussed in Section 5, with respect to the met-\nrics proposed in Section 4. We present results in\ntwo parts, reflecting the two classes of rationales\ndiscussed above: ‘Hard’ approaches that perform\ndiscrete selection of snippets, and ‘soft’ methods\nthat assign continuous importance scores to tokens.\n\nIn Table 3 we evaluate models that perform dis-\ncrete selection of rationales. We view these as in-\nherently faithful, because by construction we know\nwhich snippets the decoder used to make a pre-\ndiction.!° Therefore, for these methods we report\nonly metrics that measure agreement with human\nannotations.\n\n‘This assumes independent encoders and decoders.\n\n4450\n", "vlm_text": "The table presents performance evaluation metrics for different NLP approaches across several datasets. The table is divided into sections based on the dataset used: Evidence Inference, BoolQ, Movie Reviews, FEVER, MultiRC, CoS-E, and e-SNLI. Each section compares different methods including \"Lei et al. (2016)\", \"Lehman et al. (2019)\", and \"Bert-To-Bert\".\n\nFor each method, three metrics are reported:\n- Perf. (Performance)\n- IOU F1 (Intersection Over Union F1 Score)\n- Token F1 (F1 Score at the Token level)\n\nThe values in the table represent the effectiveness of each approach on the respective dataset. Notably, the \"Bert-To-Bert\" method generally seems to perform well across different datasets in terms of the metrics provided.\nTable 3: Performance of models that perform hard ra- tionale selection. All models are supervised at the ratio- nale level except for those marked with (u), which learn only from instance-level supervision;   †   denotes cases in which rationale training degenerated due to the REIN- FORCE style training. Perf. is accuracy (CoS-E) or macro-averaged F1 (others). Bert-To-Bert for CoS-E and e-SNLI uses a token classiﬁcation objective. Bert- To-Bert CoS-E uses the highest scoring answer. \nBERT; when we exceed this, we effectively start encoding a ‘new’ sequence (setting the positional index to 0) via BERT. The hope is that the LSTM learns to compensate for this. Evidence Inference and BoolQ comprise very long   $(>\\!1000$   token) in- puts; we were unable to run BERT over these. We instead resorted to swapping GloVe 300d embed- dings ( Pennington et al. ,  2014 ) in place of BERT representations for tokens. spans. \nTo soft score features we consider: Simple gra- dients, attention induced over contextualized repre- sentations, and LIME ( Ribeiro et al. ,  2016 ). \nThe table presents the performance metrics of several model variants across different datasets. The models use either GloVe + LSTM or BERT + LSTM as the base architecture with different explanation methods: Attention, Gradient, Lime, and Random.\n\nFor each dataset, four metrics are reported:\n1. **Perf.**: Model performance, which could represent accuracy or another relevant metric.\n2. **AUPRC**: Area Under the Precision-Recall Curve, assessing the model's precision-recall tradeoff.\n3. **Comp. ↑ (Comprehensiveness)**: Measures the improvement in model prediction by using the explanation. Higher values imply more comprehensive explanations.\n4. **Suff. ↓ (Sufficiency)**: Evaluates how sufficient the explanation is for the prediction. Lower values indicate more sufficient explanations.\n\nThe datasets evaluated are:\n- Evidence Inference\n- BoolQ\n- Movies\n- FEVER\n- MultiRC\n- CoS-E\n- e-SNLI\n\nEach dataset section contains the metrics for each combination of model and explanation method. The values illustrate how different explanation methods affect the performance and interpretability of the models across the datasets.\nTable 4: Metrics for ‘soft’ scoring models. Perf. is ac- curacy (CoS-E) or F1 (others). Comprehensiveness and sufﬁciency are in terms of AOPC (Eq.  3 ). ‘Random’ assigns random scores to tokens to induce orderings; these are averages over 10 runs. \n6 Evaluation \nHere we present initial results for the baseline mod- els discussed in Section  5 , with respect to the met- rics proposed in Section  4 . We present results in two parts, reﬂecting the two classes of rationales discussed above: ‘Hard’ approaches that perform discrete selection of snippets, and ‘soft’ methods that assign continuous importance scores to tokens. \nIn Table  3  we evaluate models that perform dis- crete selection of rationales. We view these as in- herently faithful, because by construction we know which snippets the decoder used to make a pre- diction.   Therefore, for these methods we report only metrics that measure agreement with human annotations. "}
{"page": 8, "image_path": "doc_images/2020.acl-main.408_8.jpg", "ocr_text": "Due to computational constraints, we were un-\nable to run our BERT-based implementation of Lei\net al. (2016) over larger corpora. Conversely, the\nsimple pipeline of Lehman et al. (2019) assumes\na setting in which rationale are sentences, and so\nis not appropriate for datasets in which rationales\ntend to comprise only very short spans. Again, in\nour view this highlights the need for models that\ncan rationalize at varying levels of granularity, de-\npending on what is appropriate.\n\nWe observe that for the “rationalizing” model\nof Lei et al. (2016), exploiting rationale-level super-\nvision often (though not always) improves agree-\nment with human-provided rationales, as in prior\nwork (Zhang et al., 2016; Strout et al., 2019). In-\nterestingly, this does not seem strongly correlated\nwith predictive performance.\n\nLei et al. (2016) outperforms the simple pipeline\nmodel when using a BERT encoder. Further, Lei\net al. (2016) outperforms the ‘BERT-to-BERT’\npipeline on the comparable datasets for the final\nprediction tasks. This may be an artifact of the\namount of text each model can select: ‘BERT-to-\nBERT” is limited to sentences, while Lei et al.\n(2016) can select any subset of the text. Designing\nextraction models that learn to adaptively select\ncontiguous rationales of appropriate length for a\ngiven task seems a potentially promising direction.\n\nIn Table 4 we report metrics for models that\nassign continuous importance scores to individ-\nual tokens. For these models we again measure\ndownstream (task) performance (macro F1 or ac-\ncuracy). Here the models are actually the same,\nand so downstream performance is equivalent. To\nassess the quality of token scores with respect to\nhuman annotations, we report the Area Under the\nPrecision Recall Curve (AUPRC).\n\nThese scoring functions assign only soft scores\nto inputs (and may still use all inputs to come to\na particular prediction), so we report the metrics\nintended to measure faithfulness defined above:\ncomprehensiveness and sufficiency, averaged over\n‘bins’ of tokens ordered by importance scores. To\nprovide a point of reference for these metrics —\nwhich depend on the underlying model — we re-\nport results when rationales are randomly selected\n(averaged over 10 runs).\n\nBoth simple gradient and LIME-based scoring\nyield more comprehensive rationales than attention\nweights, consistent with prior work (Jain and Wal-\nlace, 2019; Serrano and Smith, 2019). Attention\n\nfares better in terms of AUPRC — suggesting bet-\nter agreement with human rationales — which is\nalso in line with prior findings that it may provide\nplausible, but not faithful, explanation (Zhong et al.,\n2019). Interestingly, LIME does particularly well\nacross these tasks in terms of faithfulness.\n\nFrom the ‘Random’ results that we conclude\nmodels with overall poor performance on their fi-\nnal tasks tend to have an overall poor ordering, with\nmarginal differences in comprehensiveness and suf-\nficiency between them. For models that with high\nsufficiency scores: Movies, FEVER, CoS-E, and e-\nSNLI, we find that random removal is particularly\ndamaging to performance, indicating poor absolute\nranking; whereas those with high comprehensive-\nness are sensitive to rationale length.\n\n7 Conclusions and Future Directions\n\nWe have introduced a new publicly available re-\nsource: the Evaluating Rationales And Simple En-\nglish Reasoning (ERASER) benchmark. This com-\nprises seven datasets, all of which include both\ninstance level labels and corresponding supporting\nsnippets (‘rationales’) marked by human annotators.\nWe have augmented many of these datasets with\nadditional annotations, and converted them into a\nstandard format comprising inputs, rationales, and\noutputs. ERASER is intended to facilitate progress\non explainable models for NLP.\n\nWe proposed several metrics intended to mea-\nsure the quality of rationales extracted by models,\nboth in terms of agreement with human annota-\ntions, and in terms of ‘faithfulness’. We believe\nthese metrics provide reasonable means of compar-\nison of specific aspects of interpretability, but we\nview the problem of measuring faithfulness, in par-\nticular, a topic ripe for additional research (which\nERASER can facilitate).\n\nOur hope is that ERASER enables future work\non designing more interpretable NLP models, and\ncomparing their relative strengths across a vari-\nety of tasks, datasets, and desired criteria. It also\nserves as an ideal starting point for several future\ndirections such as better evaluation metrics for in-\nterpretability, causal analysis of NLP models and\ndatasets of rationales in other languages.\n\n8 Acknowledgements\n\nWe thank the anonymous ACL reviewers.\n\nThis work was supported in part by the NSF (CA-\nREER award 1750978), and by the Army Research\nOffice (W911NF1810328).\n\n4451\n", "vlm_text": "Due to computational constraints, we were un- able to run our BERT-based implementation of  Lei et al.  ( 2016 ) over larger corpora. Conversely, the simple pipeline of  Lehman et al.  ( 2019 ) assumes a setting in which rationale are sentences, and so is not appropriate for datasets in which rationales tend to comprise only very short spans. Again, in our view this highlights the need for models that can rationalize at varying levels of granularity, de- pending on what is appropriate. \nWe observe that for the “rationalizing” model of  Lei et al.  ( 2016 ), exploiting rationale-level super- vision often (though not always) improves agree- ment with human-provided rationales, as in prior work ( Zhang et al. ,  2016 ;  Strout et al. ,  2019 ). In- terestingly, this does not seem strongly correlated with predictive performance. \nLei et al.  ( 2016 ) outperforms the simple pipeline model when using a BERT encoder. Further,  Lei et al.  ( 2016 ) outperforms the ‘BERT-to-BERT’ pipeline on the comparable datasets for the ﬁnal prediction tasks. This may be an artifact of the amount of text each model can select: ‘BERT-to- BERT’ is limited to sentences, while  Lei et al. ( 2016 ) can select any subset of the text. Designing extraction models that learn to adaptively select contiguous rationales of appropriate length for a given task seems a potentially promising direction. \nIn Table  4  we report metrics for models that assign continuous importance scores to individ- ual tokens. For these models we again measure downstream (task) performance (macro F1 or ac- curacy). Here the models are actually the same, and so downstream performance is equivalent. To assess the quality of token scores with respect to human annotations, we report the Area Under the Precision Recall Curve (AUPRC). \nThese scoring functions assign only soft scores to inputs (and may still use all inputs to come to a particular prediction), so we report the metrics intended to measure faithfulness deﬁned above: comprehensiveness and sufﬁciency, averaged over ‘bins’ of tokens ordered by importance scores. To provide a point of reference for these metrics — which depend on the underlying model — we re- port results when rationales are randomly selected (averaged over 10 runs). \nBoth simple gradient and LIME-based scoring yield more comprehensive rationales than attention weights, consistent with prior work ( Jain and Wal- lace ,  2019 ;  Serrano and Smith ,  2019 ). Attention fares better in terms of AUPRC — suggesting bet- ter agreement with human rationales — which is also in line with prior ﬁndings that it may provide plausible, but not faithful, explanation ( Zhong et al. , 2019 ). Interestingly, LIME does particularly well across these tasks in terms of faithfulness. \n\nFrom the ‘Random’ results that we conclude models with overall poor performance on their ﬁ- nal tasks tend to have an overall poor ordering, with marginal differences in comprehensiveness and suf- ﬁciency between them. For models that with high sufﬁciency scores: Movies, FEVER, CoS-E, and e- SNLI, we ﬁnd that random removal is particularly damaging to performance, indicating poor absolute ranking; whereas those with high comprehensive- ness are sensitive to rationale length. \n7 Conclusions and Future Directions \nWe have introduced a new publicly available re- source: the Evaluating Rationales And Simple En- glish Reasoning (ERASER) benchmark. This com- prises seven datasets, all of which include both instance level labels and corresponding supporting snippets (‘rationales’) marked by human annotators. We have augmented many of these datasets with additional annotations, and converted them into a standard format comprising inputs, rationales, and outputs. ERASER is intended to facilitate progress on explainable models for NLP. \nWe proposed several metrics intended to mea- sure the quality of rationales extracted by models, both in terms of agreement with human annota- tions, and in terms of ‘faithfulness’. We believe these metrics provide reasonable means of compar- ison of speciﬁc aspects of interpret ability, but we view the problem of measuring faithfulness, in par- ticular, a topic ripe for additional research (which ERASER can facilitate). \nOur hope is that ERASER enables future work on designing more interpretable NLP models, and comparing their relative strengths across a vari- ety of tasks, datasets, and desired criteria. It also serves as an ideal starting point for several future directions such as better evaluation metrics for in- terpretability, causal analysis of NLP models and datasets of rationales in other languages. \n8 Acknowledgements \nWe thank the anonymous ACL reviewers. \nThis work was supported in part by the NSF (CA- REER award 1750978), and by the Army Research Ofﬁce (W911NF1810328). "}
{"page": 9, "image_path": "doc_images/2020.acl-main.408_9.jpg", "ocr_text": "References\n\nDavid Alvarez-Melis and Tommi Jaakkola. 2017. A\ncausal framework for explaining the predictions of\nblack-box sequence-to-sequence models. In Pro-\nceedings of the 2017 Conference on Empirical Meth-\nods in Natural Language Processing, pages 412-\n421.\n\nLeila Arras, Franziska Horn, Grégoire Montavon,\nKlaus-Robert Miiller, and Wojciech Samek. 2017.\nwhat is relevant in a text document?”: An inter-\npretable machine learning approach. In PloS one.\n\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2015. Neural machine translation by jointly\nlearning to align and translate. In 3rd Inter-\nnational Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference Track Proceedings.\n\nJoost Bastings, Wilker Aziz, and Ivan Titov. 2019. In-\nterpretable neural predictions with differentiable bi-\nnary variables. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 2963-2977, Florence, Italy. Associa-\ntion for Computational Linguistics.\n\nIz Beltagy, Kyle Lo, and Arman Cohan. 2019. Scib-\nert: Pretrained language model for scientific text. In\nEMNLP.\n\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nIn Proceedings of the 2015 Conference on Empirical\nMethods in Natural Language Processing (EMNLP).\nAssociation for Computational Linguistics.\n\nGino Brunner, Yang Liu, Damian Pascual, Oliver\nRichter, Massimiliano Ciaramita, and Roger Watten-\nhofer. 2020. On identifiability in transformers. In\nInternational Conference on Learning Representa-\ntions.\n\nOana-Maria Camburu, Tim Rocktéischel, Thomas\nLukasiewicz, and Phil Blunsom. 2018. e-snli: Nat-\nural language inference with natural language expla-\nnations. In Advances in Neural Information Process-\ning Systems, pages 9539-9549.\n\nShiyu Chang, Yang Zhang, Mo Yu, and Tommi\nJaakkola. 2019. A game theoretic approach to class-\nwise selective rationalization. In Advances in Neu-\nral Information Processing Systems, pages 10055—\n10065.\n\nSihao Chen, Daniel Khashabi, Wenpeng Yin, Chris\nCallison-Burch, and Dan Roth. 2019. Seeing things\nfrom a different angle: Discovering diverse perspec-\ntives about claims. In Proceedings of the Conference\nof the North American Chapter of the Association\nfor Computational Linguistics (NAACL), pages 542—\n557, Minneapolis, Minnesota.\n\nKyunghyun Cho, Bart van Merriénboer, Caglar Gul-\ncehre, Dzmitry Bahdanau, Fethi Bougares, Holger\nSchwenk, and Yoshua Bengio. 2014. Learning\nphrase representations using RNN encoder—decoder\nfor statistical machine translation. In Proceedings of\nthe 2014 Conference on Empirical Methods in Nat-\nural Language Processing (EMNLP), pages 1724—\n1734, Doha, Qatar. Association for Computational\nLinguistics.\n\nChristopher Clark, Kenton Lee, Ming-Wei Chang,\nTom Kwiatkowski, Michael Collins, and Kristina\nToutanova. 2019. Boolq: Exploring the surprising\ndifficulty of natural yes/no questions. In NAACL.\n\nJacob Cohen. 1960. A coefficient of agreement for\nnominal scales. Educational and Psychological\nMeasurement, 20(1):37-46.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume I (Long and Short Papers),\npages 4171-4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\n\nYanzhuo Ding, Yang Liu, Huanbo Luan, and Maosong\nSun. 2017. Visualizing and understanding neural\nmachine translation. In Proceedings of the 55th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), Vancouver,\nCanada. Association for Computational Linguistics.\n\nFinale Doshi-Velez and Been Kim. 2017. Towards a\nrigorous science of interpretable machine learning.\narXiv preprint arXiv:1702.08608.\n\nMark Everingham, Luc Van Gool, Christopher K. I.\nWilliams, John Winn, and Andrew Zisserman. 2010.\nThe pascal visual object classes (voc) challenge. n-\nternational Journal of Computer Vision, 88(2):303-\n338.\n\nShi Feng, Eric Wallace, Alvin Grissom, Mohit lyyer,\nPedro Rodriguez, and Jordan L. Boyd-Graber. 2018.\nPathologies of neural models make interpretation\ndifficult. In EMNLP.\n\nMatt Gardner, Joel Grus, Mark Neumann, Oyvind\nTafjord, Pradeep Dasigi, Nelson F. Liu, Matthew Pe-\nters, Michael Schmitz, and Luke Zettlemoyer. 2018.\nAllenNLP: A deep semantic natural language pro-\ncessing platform. In Proceedings of Workshop for\nNLP Open Source Software (NLP-OSS), pages 1—\n6, Melbourne, Australia. Association for Computa-\ntional Linguistics.\n\nSepp Hochreiter and Jiirgen Schmidhuber. 1997.\nLong short-term memory. Neural computation,\n9(8):1735-1780.\n\n4452\n", "vlm_text": "References \nDavid Alvarez-Melis and Tommi Jaakkola. 2017. A causal framework for explaining the predictions of black-box sequence-to-sequence models. In  Pro- ceedings of the 2017 Conference on Empirical Meth- ods in Natural Language Processing , pages 412– 421. \nLeila Arras, Franziska Horn, Gr´ egoire Montavon, Klaus-Robert M¨ uller, and Wojciech Samek. 2017. ”what is relevant in a text document?”: An inter- pretable machine learning approach. In  PloS one . \nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben- gio. 2015. Neural machine translation by jointly learning to align and translate . In  3rd Inter- national Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings . \nJoost Bastings, Wilker Aziz, and Ivan Titov. 2019.  In- terpretable neural predictions with differentiable bi- nary variables . In  Proceedings of the 57th Annual Meeting of the Association for Computational Lin- guistics , pages 2963–2977, Florence, Italy. Associa- tion for Computational Linguistics. \nIz Beltagy, Kyle Lo, and Arman Cohan. 2019.  Scib- ert: Pretrained language model for scientiﬁc text . In EMNLP . \nSamuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A large anno- tated corpus for learning natural language inference. In  Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP) Association for Computational Linguistics. \nGino Brunner, Yang Liu, Damian Pascual, Oliver Richter, Massimiliano Ciaramita, and Roger Watten- hofer. 2020.  On identiﬁability in transformers . In International Conference on Learning Representa- tions . \nOana-Maria Camburu, Tim Rockt¨ aschel, Thomas Lukasiewicz, and Phil Blunsom. 2018. e-snli: Nat- ural language inference with natural language expla- nations. In  Advances in Neural Information Process- ing Systems , pages 9539–9549. \nShiyu Chang, Yang Zhang, Mo Yu, and Tommi Jaakkola. 2019. A game theoretic approach to class- wise selective rationalization. In  Advances in Neu- ral Information Processing Systems , pages 10055– 10065. \nSihao Chen, Daniel Khashabi, Wenpeng Yin, Chris Callison-Burch, and Dan Roth. 2019.  Seeing things from a different angle: Discovering diverse perspec- tives about claims . In  Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics (NAACL) , pages 542– 557, Minneapolis, Minnesota. \nKyunghyun Cho, Bart van Merri¨ enboer, Caglar Gul- cehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using RNN encoder–decoder for statistical machine translation . In  Proceedings of the 2014 Conference on Empirical Methods in Nat- ural Language Processing (EMNLP) , pages 1724– 1734, Doha, Qatar. Association for Computational Linguistics. \nChristopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. 2019. Boolq: Exploring the surprising difﬁculty of natural yes/no questions. In  NAACL . \nJacob Cohen. 1960. A coefﬁcient of agreement for nominal scales . Educational and Psychological Measurement , 20(1):37–46. \nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language under- standing . In  Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) , pages 4171–4186, Minneapolis, Minnesota. Associ- ation for Computational Linguistics. \nYanzhuo Ding, Yang Liu, Huanbo Luan, and Maosong Sun. 2017. Visualizing and understanding neural machine translation . In  Proceedings of the 55th An- nual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , Vancouver, Canada. Association for Computational Linguistics. \nFinale Doshi-Velez and Been Kim. 2017. Towards a rigorous science of interpretable machine learning. arXiv preprint arXiv:1702.08608 . \nMark Everingham, Luc Van Gool, Christopher K. I. Williams, John Winn, and Andrew Zisserman. 2010. The pascal visual object classes (voc) challenge .  In- ternational Journal of Computer Vision , 88(2):303– 338. \nShi Feng, Eric Wallace, Alvin Grissom, Mohit Iyyer, Pedro Rodriguez, and Jordan L. Boyd-Graber. 2018. Pathologies of neural models make interpretation difﬁcult. In  EMNLP . \nMatt Gardner, Joel Grus, Mark Neumann, Oyvind Tafjord, Pradeep Dasigi, Nelson F. Liu, Matthew Pe- ters, Michael Schmitz, and Luke Zettlemoyer. 2018. AllenNLP: A deep semantic natural language pro- cessing platform . In  Proceedings of Workshop for NLP Open Source Software (NLP-OSS) , pages 1– 6, Melbourne, Australia. Association for Computa- tional Linguistics. \nSepp Hochreiter and J¨ urgen Schmidhuber. 1997. Long short-term memory. Neural computation , 9(8):1735–1780. "}
{"page": 10, "image_path": "doc_images/2020.acl-main.408_10.jpg", "ocr_text": "Sara Hooker, Dumitru Erhan, Pieter-Jan Kindermans,\nand Been Kim. 2019. A benchmark for interpretabil-\nity methods in deep neural networks. In H. Wal-\nlach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc,\nE. Fox, and R. Garnett, editors, Advances in Neu-\nral Information Processing Systems 32, pages 9737—\n9748. Curran Associates, Inc.\n\nAlon Jacovi and Yoav Goldberg. 2020. Towards faith-\nfully interpretable nlp systems: How should we\ndefine and evaluate faithfulness? arXiv preprint\narXiv:2004.03685.\n\nSarthak Jain and Byron C. Wallace. 2019. Attention is\nnot Explanation. In Proceedings of the 2019 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long and Short Pa-\npers), pages 3543-3556, Minneapolis, Minnesota.\nAssociation for Computational Linguistics.\n\nSarthak Jain, Sarah Wiegreffe, Yuval Pinter, and By-\nron C. Wallace. 2020. Learning to Faithfully Ratio-\nnalize by Construction. In Proceedings of the Con-\nference of the Association for Computational Lin-\nguistics (ACL).\n\nDaniel Khashabi, Snigdha Chaturvedi, Michael Roth,\nShyam Upadhyay, and Dan Roth. 2018. Looking\nBeyond the Surface: A Challenge Set for Reading\nComprehension over Multiple Sentences. In Proc.\nof the Annual Conference of the North American\nChapter of the Association for Computational Lin-\nguistics (NAACL).\n\nDiederik Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. International\nConference on Learning Representations.\n\nEric Lehman, Jay DeYoung, Regina Barzilay, and By-\nron C Wallace. 2019. Inferring which medical treat-\nments work from reports of clinical trials. In Pro-\nceedings of the North American Chapter of the As-\nsociation for Computational Linguistics (NAACL),\npages 3705-3717.\n\nTao Lei, Regina Barzilay, and Tommi Jaakkola. 2016.\nRationalizing neural predictions. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 107-117.\n\nJiwei Li, Xinlei Chen, Eduard Hovy, and Dan Jurafsky.\n2016. Visualizing and understanding neural models\nin NLP. In Proceedings of the 2016 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 681-691, San Diego, California. As-\nsociation for Computational Linguistics.\n\nZachary C Lipton. 2016. The mythos of model inter-\npretability. arXiv preprint arXiv: 1606.03490.\n\nScott M Lundberg and Su-In Lee. 2017. A unified\napproach to interpreting model predictions. In Ad-\n\nvances in Neural Information Processing Systems,\npages 4765-4774.\n\nTyler McDonnell, Mucahid Kutlu, Tamer Elsayed, and\nMatthew Lease. 2017. The many benefits of anno-\ntator rationales for relevance judgments. In JJCAI,\npages 4909-4913.\n\nTyler McDonnell, Matthew Lease, Mucahid Kutlu, and\nTamer Elsayed. 2016. Why is that relevant? col-\nlecting annotator rationales for relevance judgments.\nIn Fourth AAAI Conference on Human Computation\nand Crowdsourcing.\n\nGrégoire Montavon, Sebastian Lapuschkin, Alexander\nBinder, Wojciech Samek, and Klaus-Robert Miiller.\n2017. Explaining nonlinear classification decisions\nwith deep taylor decomposition. Pattern Recogni-\ntion, 65:211-222.\n\nPooya Moradi, Nishant Kambhatla, and Anoop Sarkar.\n2019. Interrogating the explanatory power of atten-\ntion in neural machine translation. In Proceedings of\nthe 3rd Workshop on Neural Generation and Trans-\nlation, pages 221-230, Hong Kong. Association for\nComputational Linguistics.\n\nMark Neumann, Daniel King, Iz Beltagy, and Waleed\nAmmar. 2019. Scispacy: Fast and robust models\nfor biomedical natural language processing. CoRR,\nabs/1902.07669.\n\nDong Nguyen. 2018. Comparing automatic and human\nevaluation of local explanations for text classifica-\ntion. In Proceedings of the 2018 Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics: Human Language Technolo-\ngies, Volume I (Long Papers), pages 1069-1078.\n\nBo Pang and Lillian Lee. 2004. A sentimental edu-\ncation: Sentiment analysis using subjectivity sum-\nmarization based on minimum cuts. In Proceed-\nings of the 42nd Annual Meeting of the Association\nfor Computational Linguistics (ACL-04), pages 271—\n278, Barcelona, Spain.\n\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, et al. 2019. Pytorch: An imperative style,\nhigh-performance deep learning library. In Ad-\nvances in Neural Information Processing Systems,\npages 8024-8035.\n\nDavid J Pearce. 2005. An improved algorithm for find-\ning the strongly connected components of a directed\ngraph. Technical report, Victoria University, NZ.\n\nJeffrey Pennington, Richard Socher, and Christopher\nManning. 2014. Glove: Global vectors for word rep-\nresentation. In Proceedings of the 2014 Conference\non Empirical Methods in Natural Language Process-\ning (EMNLP), pages 1532-1543, Doha, Qatar. Asso-\nciation for Computational Linguistics.\n\nDanish Pruthi, Mansi Gupta, Bhuwan Dhingra, Gra-\nham Neubig, and Zachary C. Lipton. 2020. Learn-\ning to deceive with attention-based explanations. In\nAnnual Conference of the Association for Computa-\ntional Linguistics (ACL).\n\n4453\n", "vlm_text": "Sara Hooker, Dumitru Erhan, Pieter-Jan Kindermans, and Been Kim. 2019.  A benchmark for interpretabil- ity methods in deep neural networks . In H. Wal- lach, H. Larochelle, A. Beygelzimer, F. d'Alch´ e-Buc, E. Fox, and R. Garnett, editors,  Advances in Neu- ral Information Processing Systems 32 , pages 9737– 9748. Curran Associates, Inc. Alon Jacovi and Yoav Goldberg. 2020. Towards faith- fully interpretable nlp systems: How should we deﬁne and evaluate faithfulness? arXiv preprint arXiv:2004.03685 . Sarthak Jain and Byron C. Wallace. 2019.  Attention is not Explanation . In  Proceedings of the 2019 Con- ference of the North American Chapter of the Asso- ciation for Computational Linguistics: Human Lan- guage Technologies, Volume 1 (Long and Short Pa- pers) , pages 3543–3556, Minneapolis, Minnesota. Association for Computational Linguistics. Sarthak Jain, Sarah Wiegreffe, Yuval Pinter, and By- ron C. Wallace. 2020. Learning to Faithfully Ratio- nalize by Construction. In  Proceedings of the Con- ference of the Association for Computational Lin- guistics (ACL) . Daniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, and Dan Roth. 2018. Looking Beyond the Surface: A Challenge Set for Reading Comprehension over Multiple Sentences . In  Proc. of the Annual Conference of the North American Chapter of the Association for Computational Lin- guistics (NAACL) . Diederik Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. International Conference on Learning Representations . Eric Lehman, Jay DeYoung, Regina Barzilay, and By- ron C Wallace. 2019. Inferring which medical treat- ments work from reports of clinical trials. In  Pro- ceedings of the North American Chapter of the As- sociation for Computational Linguistics (NAACL) , pages 3705–3717. Tao Lei, Regina Barzilay, and Tommi Jaakkola. 2016. Rationalizing neural predictions. In  Proceedings of the 2016 Conference on Empirical Methods in Natu- ral Language Processing , pages 107–117. Jiwei Li, Xinlei Chen, Eduard Hovy, and Dan Jurafsky. 2016.  Visualizing and understanding neural models in NLP . In  Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech- nologies , pages 681–691, San Diego, California. As- sociation for Computational Linguistics. Zachary C Lipton. 2016. The mythos of model inter- pretability.  arXiv preprint arXiv:1606.03490 . Scott M Lundberg and Su-In Lee. 2017. A uniﬁed approach to interpreting model predictions. In  Ad- vances in Neural Information Processing Systems , pages 4765–4774. \nTyler McDonnell, Mucahid Kutlu, Tamer Elsayed, and Matthew Lease. 2017. The many beneﬁts of anno- tator rationales for relevance judgments. In  IJCAI , pages 4909–4913. Tyler McDonnell, Matthew Lease, Mucahid Kutlu, and Tamer Elsayed. 2016. Why is that relevant? col- lecting annotator rationales for relevance judgments. In  Fourth AAAI Conference on Human Computation and Crowdsourcing . Gr´ egoire Montavon, Sebastian Lapuschkin, Alexander Binder, Wojciech Samek, and Klaus-Robert M¨ uller. 2017. Explaining nonlinear classiﬁcation decisions with deep taylor decomposition. Pattern Recogni- tion , 65:211–222. Pooya Moradi, Nishant Kambhatla, and Anoop Sarkar. 2019.  Interrogating the explanatory power of atten- tion in neural machine translation . In  Proceedings of the 3rd Workshop on Neural Generation and Trans- lation , pages 221–230, Hong Kong. Association for Computational Linguistics. Mark Neumann, Daniel King, Iz Beltagy, and Waleed Ammar. 2019. Scispacy: Fast and robust models for biomedical natural language processing .  CoRR , abs/1902.07669. Dong Nguyen. 2018. Comparing automatic and human evaluation of local explanations for text classiﬁca- tion. In  Proceedings of the 2018 Conference of the North American Chapter of the Association for Com- putational Linguistics: Human Language Technolo- gies, Volume 1 (Long Papers) , pages 1069–1078. Bo Pang and Lillian Lee. 2004. A sentimental edu- cation: Sentiment analysis using subjectivity sum- marization based on minimum cuts . In  Proceed- ings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL-04) , pages 271– 278, Barcelona, Spain. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. 2019. Pytorch: An imperative style, high-performance deep learning library. In  Ad- vances in Neural Information Processing Systems , pages 8024–8035. David J Pearce. 2005. An improved algorithm for ﬁnd- ing the strongly connected components of a directed graph. Technical report, Victoria University, NZ. Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014.  Glove: Global vectors for word rep- resentation . In  Proceedings of the 2014 Conference on Empirical Methods in Natural Language Process- ing (EMNLP) , pages 1532–1543, Doha, Qatar. Asso- ciation for Computational Linguistics. Danish Pruthi, Mansi Gupta, Bhuwan Dhingra, Gra- ham Neubig, and Zachary C. Lipton. 2020.  Learn- ing to deceive with attention-based explanations . In Annual Conference of the Association for Computa- tional Linguistics (ACL) . "}
{"page": 11, "image_path": "doc_images/2020.acl-main.408_11.jpg", "ocr_text": "Sampo Pyysalo, F Ginter, Hans Moen, T Salakoski, and\nSophia Ananiadou. 2013. Distributional semantics\nresources for biomedical text processing. Proceed-\nings of Languages in Biology and Medicine.\n\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training.\n\nNazneen Fatema Rajani, Bryan McCann, Caiming\nXiong, and Richard Socher. 2019. Explain yourself!\nleveraging language models for commonsense rea-\nsoning. Proceedings of the Association for Compu-\ntational Linguistics (ACL).\n\nMarco Ribeiro, Sameer Singh, and Carlos Guestrin.\n2016. why should i trust you?: Explaining the pre-\ndictions of any classifier. In Proceedings of the 2016\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Demon-\nstrations, pages 97-101.\n\nWojciech Samek, Alexander Binder, Grégoire Mon-\ntavon, Sebastian Lapuschkin, and Klaus-Robert\nMiiller. 2016. Evaluating the visualization of what\na deep neural network has learned. JEEE trans-\nactions on neural networks and learning systems,\n\n28(11):2660-2673.\n\nTal Schuster, Darsh J Shah, Yun Jie Serene Yeo, Daniel\nFilizzola, Enrico Santus, and Regina Barzilay. 2019.\nTowards debiasing fact verification models. In Pro-\nceedings of the 2019 Conference on Empirical Meth-\nods in Natural Language Processing (EMNLP). As-\nsociation for Computational Linguistics.\n\nSofia Serrano and Noah A. Smith. 2019. Is attention\ninterpretable? In Proceedings of the 57th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 2931-2951, Florence, Italy. Associa-\ntion for Computational Linguistics.\n\nBurr Settles. 2012. Active learning. Synthesis Lec-\ntures on Artificial Intelligence and Machine Learn-\ning, 6(1):1-114.\n\nManali Sharma, Di Zhuang, and Mustafa Bilgic. 2015.\nActive learning with rationales for text classification.\nIn Proceedings of the 2015 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 441-451.\n\nKevin Small, Byron C Wallace, Carla E Brodley, and\nThomas A Trikalinos. 2011. The constrained weight\nspace svm: learning with ranked features. In Pro-\nceedings of the International Conference on Inter-\nnational Conference on Machine Learning (ICML),\npages 865-872.\n\nD. Smilkov, N. Thorat, B. Kim, F. Viégas, and M. Wat-\ntenberg. 2017. SmoothGrad: removing noise by\nadding noise. [CML workshop on visualization for\ndeep learning.\n\nRobyn Speer. 2019. ftfy. Zenodo. Version 5.5.\n\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,\nIlya Sutskever, and Ruslan Salakhutdinov. 2014.\nDropout: A simple way to prevent neural networks\nfrom overfitting. Journal of Machine Learning Re-\nsearch, 15:1929-1958.\n\nJulia Strout, Ye Zhang, and Raymond Mooney. 2019.\nDo human rationales improve machine explana-\ntions? In Proceedings of the 2019 ACL Workshop\nBlackboxNLP: Analyzing and Interpreting Neural\nNetworks for NLP, pages 56-62, Florence, Italy. As-\nsociation for Computational Linguistics.\n\nMukund Sundararajan, Ankur Taly, and Qiqi Yan. 2017.\nAxiomatic attribution for deep networks. In Pro-\nceedings of the 34th International Conference on\nMachine Learning-Volume 70, pages 3319-3328.\nJMLR. org.\n\nAlon Talmor, Jonathan Herzig, Nicholas Lourie, and\nJonathan Berant. 2019. CommonsenseQA: A ques-\ntion answering challenge targeting commonsense\nknowledge. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume I (Long and Short Papers),\npages 4149-4158, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\n\nJames Thorne, Andreas Vilachos, Christos\nChristodoulopoulos, and Arpit Mittal. 2018.\nFEVER: a Large-scale Dataset for Fact Extraction\nand VERification. In Proceedings of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics (NAACL), pages 809-819.\n\nShikhar Vashishth, Shyam Upadhyay, Gaurav Singh\nTomar, and Manaal Faruqui. 2019. Attention in-\n\nterpretability across nlp tasks. arXiv _ preprint\narXiv:1909.11218.\n\nByron C Wallace, Kevin Small, Carla E Brodley, and\nThomas A Trikalinos. 2010. Active learning for\nbiomedical citation screening. In Proceedings of\nthe 16th ACM SIGKDD international conference on\nKnowledge discovery and data mining, pages 173-\n182. ACM.\n\nAlex Wang, Yada Pruksachatkun, Nikita Nangia,\nAmanpreet Singh, Julian Michael, Felix Hill, Omer\nLevy, and Samuel Bowman. 2019a. Superglue: A\nstickier benchmark for general-purpose language un-\nderstanding systems. In H. Wallach, H. Larochelle,\nA. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Gar-\nnett, editors, Advances in Neural Information Pro-\ncessing Systems 32, pages 3266-3280. Curran Asso-\nciates, Inc.\n\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2019b.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In Inter-\nnational Conference on Learning Representations.\n\n4454\n", "vlm_text": "Sampo Pyysalo, F Ginter, Hans Moen, T Salakoski, and Sophia Ananiadou. 2013. Distributional semantics resources for biomedical text processing.  Proceed- ings of Languages in Biology and Medicine . Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language under- standing by generative pre-training . Nazneen Fatema Rajani, Bryan McCann, Caiming Xiong, and Richard Socher. 2019. Explain yourself! leveraging language models for commonsense rea- soning.  Proceedings of the Association for Compu- tational Linguistics (ACL) . Marco Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. why should i trust you?: Explaining the pre- dictions of any classiﬁer. In  Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Demon- strations , pages 97–101. Wojciech Samek, Alexander Binder, Gr´ egoire Mon- tavon, Sebastian Lapuschkin, and Klaus-Robert M¨ uller. 2016. Evaluating the visualization of what a deep neural network has learned. IEEE trans- actions on neural networks and learning systems , 28(11):2660–2673. Tal Schuster, Darsh J Shah, Yun Jie Serene Yeo, Daniel Filizzola, Enrico Santus, and Regina Barzilay. 2019. Towards debiasing fact veriﬁcation models . In  Pro- ceedings of the 2019 Conference on Empirical Meth- ods in Natural Language Processing (EMNLP) . As- sociation for Computational Linguistics. Soﬁa Serrano and Noah A. Smith. 2019.  Is attention interpretable? In  Proceedings of the 57th Annual Meeting of the Association for Computational Lin- guistics , pages 2931–2951, Florence, Italy. Associa- tion for Computational Linguistics. Burr Settles. 2012. Active learning. Synthesis Lec- tures on Artiﬁcial Intelligence and Machine Learn- ing , 6(1):1–114. Manali Sharma, Di Zhuang, and Mustafa Bilgic. 2015. Active learning with rationales for text classiﬁcation. In  Proceedings of the 2015 Conference of the North American Chapter of the Association for Computa- tional Linguistics: Human Language Technologies , pages 441–451. Kevin Small, Byron C Wallace, Carla E Brodley, and Thomas A Trikalinos. 2011. The constrained weight space svm: learning with ranked features. In  Pro- ceedings of the International Conference on Inter- national Conference on Machine Learning (ICML) , pages 865–872. D. Smilkov, N. Thorat, B. Kim, F. Vi´ egas, and M. Wat- tenberg. 2017. SmoothGrad: removing noise by adding noise .  ICML workshop on visualization for deep learning . \nRobyn Speer. 2019.  ftfy . Zenodo. Version 5.5. \nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: A simple way to prevent neural networks from overﬁtting .  Journal of Machine Learning Re- search , 15:1929–1958. \nJulia Strout, Ye Zhang, and Raymond Mooney. 2019. Do human rationales improve machine explana- tions? In  Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP , pages 56–62, Florence, Italy. As- sociation for Computational Linguistics. \nMukund Sundararajan, Ankur Taly, and Qiqi Yan. 2017. Axiomatic attribution for deep networks. In  Pro- ceedings of the 34th International Conference on Machine Learning-Volume 70 , pages 3319–3328. JMLR. org. \nAlon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2019.  CommonsenseQA: A ques- tion answering challenge targeting commonsense knowledge . In  Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) , pages 4149–4158, Minneapolis, Minnesota. Associ- ation for Computational Linguistics. \nJames Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. 2018. FEVER: a Large-scale Dataset for Fact Extraction and VERiﬁcation. In  Proceedings of the North American Chapter of the Association for Computa- tional Linguistics (NAACL) , pages 809–819. \nShikhar Vashishth, Shyam Upadhyay, Gaurav Singh Tomar, and Manaal Faruqui. 2019. Attention in- terpretability across nlp tasks. arXiv preprint arXiv:1909.11218 . \nByron C Wallace, Kevin Small, Carla E Brodley, and Thomas A Trikalinos. 2010. Active learning for biomedical citation screening. In  Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining , pages 173– 182. ACM. \nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2019a.  Superglue: A stickier benchmark for general-purpose language un- derstanding systems . In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch´ e-Buc, E. Fox, and R. Gar- nett, editors,  Advances in Neural Information Pro- cessing Systems 32 , pages 3266–3280. Curran Asso- ciates, Inc. \nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. 2019b. GLUE: A multi-task benchmark and analysis plat- form for natural language understanding . In  Inter- national Conference on Learning Representations . "}
{"page": 12, "image_path": "doc_images/2020.acl-main.408_12.jpg", "ocr_text": "Sarah Wiegreffe and Yuval Pinter. 2019. Attention is\nnot not explanation. In Proceedings of the 2019 Con-\nference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 11-20, Hong Kong, China. Associ-\nation for Computational Linguistics.\n\nRonald J Williams. 1992. Simple statistical gradient-\nfollowing algorithms for connectionist reinforce-\nment learning. Machine learning, 8(3-4):229-256.\n\nRonald J Williams and David Zipser. 1989. A learn-\ning algorithm for continually running fully recurrent\nneural networks. Neural computation, 1(2):270-\n280.\n\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R’emi Louf, Morgan Funtow-\nicz, and Jamie Brew. 2019. Huggingface’s trans-\nformers: State-of-the-art natural language process-\ning. ArXiv, abs/1910.03771.\n\nMo Yu, Shiyu Chang, Yang Zhang, and Tommi\nJaakkola. 2019. Rethinking cooperative rationaliza-\ntion: Introspective extraction and complement con-\ntrol. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n4094-4103, Hong Kong, China. Association for\nComputational Linguistics.\n\nOmar Zaidan, Jason Eisner, and Christine Piatko.\n2007. Using annotator rationales to improve ma-\nchine learning for text categorization. In Proceed-\nings of the conference of the North American chap-\nter of the Association for Computational Linguistics\n(NAACL), pages 260-267.\n\nOmar F Zaidan and Jason Eisner. 2008. Modeling an-\nnotators: A generative approach to learning from an-\nnotator rationales. In Proceedings of the Conference\non Empirical Methods in Natural Language Process-\ning (EMNLP), pages 31-40.\n\nYe Zhang, Iain Marshall, and Byron C Wallace. 2016.\nRationale-augmented convolutional neural networks\nfor text classification. In Proceedings of the Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), volume 2016, page 795. NIH\nPublic Access.\n\nRuiqi Zhong, Steven Shao, and Kathleen McKeown.\n2019. Fine-grained sentiment analysis with faithful\nattention. arXiv preprint arXiv: 1908.06870.\n\nAppendix\nA Dataset Preprocessing\n\nWe describe what, if any, additional processing we\nperform on a per-dataset basis. All datasets were\nconverted to a unified format.\n\nMultiRC (Khashabi et al., 2018) We perform min-\nimal processing. We use the validation set as the\nesting set for public release.\n\nEvidence Inference (Lehman et al., 2019) We per-\norm minimal processing. As not all of the pro-\nvided evidence spans come with offsets, we delete\nany prompts that had no grounded evidence spans.\n\nMovie reviews (Zaidan and Eisner, 2008) We per-\nform minimal processing. We use the ninth fold as\nhe validation set, and collect annotations on the\nenth fold for comprehensive evaluation.\n\nFEVER (Thorne et al., 2018) We perform substan-\nial processing for FEVER - we delete the ’Not\nEnough Info” claim class, delete any claims with\nsupport in more than one document, and reparti-\nion the validation set into a validation and a test\nset for this benchmark (using the test set would\ncompromise the information retrieval portion of\nhe original FEVER task). We ensure that there\nis no document overlap between train, validation,\nand test sets (we use Pearce (2005) to ensure this,\nas conceptually a claim may be supported by facts\nin more than one document). We ensure that the\nvalidation set contains the documents used to cre-\nate the FEVER symmetric dataset (Schuster et al.,\n2019) (unfortunately, the documents used to create\nthe validation and test sets overlap so we cannot\nprovide this partitioning). Additionally, we clean\nup some encoding errors in the dataset via Speer\n(2019).\n\nBoolQ (Clark et al., 2019) The BoolQ dataset re-\nquired substantial processing. The original dataset\ndid not retain source Wikipedia articles or col-\nlection dates. In order to identify the source\nparagraphs, we download the 12/20/18 Wikipedia\narchive, and use FuzzyWuzzy https://github.\ncom/seatgeek/fuzzywuzzy to identify the source\nparagraph span that best matches the original re-\nlease. If the Levenshtein distance ratio does not\nreach a score of at least 90, the corresponding in-\nstance is removed. For public release, we use the\nofficial validation set for testing, and repartition\ntrain into a training and validation set.\n\ne-SNLI (Camburu et al., 2018) We perform mini-\nmal processing. We separate the premise and hy-\npothesis statements into separate documents.\n\nCommonsense Explanations (CoS-E) (Rajani\net al., 2019) We perform minimal processing, pri-\nmarily deletion of any questions without a rationale\n\n4455\n", "vlm_text": "Sarah Wiegreffe and Yuval Pinter. 2019.  Attention is not not explanation . In  Proceedings of the 2019 Con- ference on Empirical Methods in Natural Language Processing and the 9th International Joint Confer- ence on Natural Language Processing (EMNLP- IJCNLP) , pages 11–20, Hong Kong, China. Associ- ation for Computational Linguistics. \nRonald J Williams. 1992. Simple statistical gradient- following algorithms for connectionist reinforce- ment learning.  Machine learning , 8(3-4):229–256. \nRonald J Williams and David Zipser. 1989. A learn- ing algorithm for continually running fully recurrent neural networks. Neural computation , 1(2):270– 280. \nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pier- ric Cistac, Tim Rault, R’emi Louf, Morgan Funtow- icz, and Jamie Brew. 2019. Huggingface’s trans- formers: State-of-the-art natural language process- ing.  ArXiv , abs/1910.03771. \nMo Yu, Shiyu Chang, Yang Zhang, and Tommi Jaakkola. 2019.  Rethinking cooperative rationaliza- tion: Introspective extraction and complement con- trol . In  Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu- ral Language Processing (EMNLP-IJCNLP) , pages 4094–4103, Hong Kong, China. Association for Computational Linguistics. \nOmar Zaidan, Jason Eisner, and Christine Piatko. 2007. Using annotator rationales to improve ma- chine learning for text categorization. In  Proceed- ings of the conference of the North American chap- ter of the Association for Computational Linguistics (NAACL) , pages 260–267. \nOmar F Zaidan and Jason Eisner. 2008. Modeling an- notators: A generative approach to learning from an- notator rationales. In  Proceedings of the Conference on Empirical Methods in Natural Language Process- ing (EMNLP) , pages 31–40. \nYe Zhang, Iain Marshall, and Byron C Wallace. 2016. Rationale-augmented convolutional neural networks for text classiﬁcation. In  Proceedings of the Con- ference on Empirical Methods in Natural Language Processing (EMNLP) , volume 2016, page 795. NIH Public Access. \nRuiqi Zhong, Steven Shao, and Kathleen McKeown. 2019. Fine-grained sentiment analysis with faithful attention.  arXiv preprint arXiv:1908.06870 . \nAppendix \nA Dataset Preprocessing \nWe describe what, if any, additional processing we perform on a per-dataset basis. All datasets were converted to a uniﬁed format. \nMultiRC  ( Khashabi et al. ,  2018 ) We perform min- imal processing. We use the validation set as the testing set for public release. \nEvidence Inference  ( Lehman et al. ,  2019 ) We per- form minimal processing. As not all of the pro- vided evidence spans come with offsets, we delete any prompts that had no grounded evidence spans. \nMovie reviews  ( Zaidan and Eisner ,  2008 ) We per- form minimal processing. We use the ninth fold as the validation set, and collect annotations on the tenth fold for comprehensive evaluation. \nFEVER  ( Thorne et al. ,  2018 ) We perform substan- tial processing for FEVER - we delete the ”Not Enough Info” claim class, delete any claims with support in more than one document, and reparti- tion the validation set into a validation and a test set for this benchmark (using the test set would compromise the information retrieval portion of the original FEVER task). We ensure that there is no document overlap between train, validation, and test sets (we use  Pearce  ( 2005 ) to ensure this, as conceptually a claim may be supported by facts in more than one document). We ensure that the validation set contains the documents used to cre- ate the FEVER symmetric dataset ( Schuster et al. , 2019 ) (unfortunately, the documents used to create the validation and test sets overlap so we cannot provide this partitioning). Additionally, we clean up some encoding errors in the dataset via  Speer ( 2019 ). \nBoolQ  ( Clark et al. ,  2019 ) The BoolQ dataset re- quired substantial processing. The original dataset did not retain source Wikipedia articles or col- lection dates. In order to identify the source paragraphs, we download the 12/20/18 Wikipedia archive, and use FuzzyWuzzy  https://github. com/seatgeek/fuzzywuzzy  to identify the source paragraph span that best matches the original re- lease. If the Levenshtein distance ratio does not reach a score of at least 90, the corresponding in- stance is removed. For public release, we use the ofﬁcial validation set for testing, and repartition train into a training and validation set. \ne-SNLI  ( Camburu et al. ,  2018 ) We perform mini- mal processing. We separate the premise and hy- pothesis statements into separate documents. \nCommonsense Explanations (CoS-E)  ( Rajani et al. ,  2019 ) We perform minimal processing, pri- marily deletion of any questions without a rationale "}
{"page": 13, "image_path": "doc_images/2020.acl-main.408_13.jpg", "ocr_text": "Dataset Documents Instances Rationale % Evidence Statements Evidence Lengths\n\nMultiRC\n\nTrain 400 24029 17.4 56298 21.5\nVal 56 3214 18.5 7498 22.8\nTest 83 4848 - - -\nEvidence Inference\n\nTrain 1924 7958 1.34 10371 39.3\nVa 247 972 1.38 1294 40.3\nTest 240 959 - - -\nExhaustive Evidence Inference\n\nVal 81 101 447 504.0 35.2\nTest 106 152 - - -\nMovie Reviews\n\nTrain 1599 1600 9.35 13878 77\nVal 150 150 745 1143.0 6.6\nTest 200 200 - - -\nExhaustive Movie Reviews\n\nVal 50 50 19.10 592.0 12.8\nFEVER\n\nTrain 2915 97957 20.0 146856 31.3\nVal 570 6122 21.6 8672 28.2\nTest 614 6111 - - -\nBoolQ\n\nTrain 4518 6363 6.64 6363.0 110.2\nVal 1092 1491 713 1491.0 106.5\nTest 2294 2817 - - -\ne-SNLI\n\nTrain 911938 549309 27.3 1199035.0 1.8\nVal 16328 9823 25.6 23639.0 1.6\nTest 16299 9807 - - -\nCoS-E\n\nTrain 8733 8733 26.6 8733 74\nVal 1092 1092 27.1 1092 7.6\n\nTest 1092 1092 - - -\n\nTable 5: Detailed breakdowns for each dataset - the number of documents, instances, evidence statements, and\nlengths. Additionally we include the percentage of each relevant document that is considered a rationale. For test\nsets, counts are for all instances including documents with non comprehensive rationales.\n\nDataset Labels Instances Documents Sentences Tokens\nEvidence Inference 3 9889 2411 156.0 4760.6\nBoolQ 2 10661 7026 175.3 3582.5\nMovie Reviews 2 2000 1999 36.8 774.1\nFEVER 2 110190 4099 12.1 326.5\nMultiRC 2 32091 539 14.9 302.5\nCoS-E 5 10917 10917 1.0 27.6\ne-SNLI 3 568939 944565 17 16.0\n\nTable 6: General dataset statistics: number of labels, instances, unique documents, and average numbers of sen-\ntences and tokens in documents, across the publicly released train/validation/test splits in ERASER. For CoS-E\nand e-SNLI, the sentence counts are not meaningful as the partitioning of question/sentence/answer formatting is\nan arbitrary choice in this framework.\n\n4456\n", "vlm_text": "This table lists various datasets used for natural language processing tasks, providing details on their size and characteristics across training, validation, and test splits. Here's a breakdown of each column in the table:\n\n1. **Dataset**: The name of the dataset along with the corresponding data split (Train, Val, Test).\n\n2. **Documents**: The number of documents in each dataset split.\n\n3. **Instances**: The number of instances or examples in each dataset split.\n\n4. **Rationale %**: The percentage of instances that include rationales—explanations or supporting information for the instances.\n\n5. **Evidence Statements**: The number of evidence statements included in each dataset split, where applicable.\n\n6. **Evidence Lengths**: The average length of the evidence provided in statements over each dataset split.\n\nThe datasets listed include MultiRC, Evidence Inference, Exhaustive Evidence Inference, Movie Reviews, Exhaustive Movie Reviews, FEVER, BoolQ, e-SNLI, and CoS-E. Each dataset may serve different purposes such as reasoning, inference, or sentiment analysis, and the table lays out how comprehensive (in terms of evidence and rationales) and sizable each dataset is.\nTable 5: Detailed breakdowns for each dataset - the number of documents, instances, evidence statements, and lengths. Additionally we include the percentage of each relevant document that is considered a rationale. For test sets, counts are for all instances including documents with non comprehensive rationales. \nThe table provides details about several datasets used for computational tasks. It includes the following columns:\n\n1. **Dataset**: Names of the datasets.\n   - Evidence Inference\n   - BoolQ\n   - Movie Reviews\n   - FEVER\n   - MultiRC\n   - CoS-E\n   - e-SNLI\n\n2. **Labels**: The number of labels or classes in each dataset.\n   - Evidence Inference: 3\n   - BoolQ: 2\n   - Movie Reviews: 2\n   - FEVER: 2\n   - MultiRC: 2\n   - CoS-E: 5\n   - e-SNLI: 3\n\n3. **Instances**: The total number of instances or examples in each dataset.\n   - Evidence Inference: 9,889\n   - BoolQ: 10,661\n   - Movie Reviews: 2,000\n   - FEVER: 110,190\n   - MultiRC: 32,091\n   - CoS-E: 10,917\n   - e-SNLI: 568,939\n\n4. **Documents**: The total number of documents in each dataset.\n   - Evidence Inference: 2,411\n   - BoolQ: 7,026\n   - Movie Reviews: 1,999\n   - FEVER: 4,099\n   - MultiRC: 539\n   - CoS-E: 10,917\n   - e-SNLI: 944,565\n\n5. **Sentences**: The average number of sentences in each document or instance.\n   - Evidence Inference: 156.0\n   - BoolQ: 175.3\n   - Movie Reviews: 36.8\n   - FEVER: 12.1\n   - MultiRC: 14.9\n   - CoS-E: 1.0\n   - e-SNLI: 1.7\n\n6. **Tokens**: The average number of tokens per instance.\n   - Evidence Inference: 4,760.6\n   - BoolQ: 3,582.5\n   - Movie Reviews: 774.1\n   - FEVER: 326.5\n   - MultiRC: 302.5\n   - CoS-E: 27.6\n   - e-SNLI: 16.0\n\nThe table summarizes key characteristics of each dataset, including the complexity (labels), size (instances, documents), and textual content (sentences, tokens).\nTable 6: General dataset statistics: number of labels, instances, unique documents, and average numbers of sen- tences and tokens in documents, across the publicly released train/validation/test splits in ERASER. For CoS-E and e-SNLI, the sentence counts are not meaningful as the partitioning of question/sentence/answer formatting is an arbitrary choice in this framework. "}
{"page": 14, "image_path": "doc_images/2020.acl-main.408_14.jpg", "ocr_text": "or questions with rationales that were not possi-\nble to automatically map back to the underlying\ntext. As recommended by the authors of Talmor\net al. (2019) we repartition the train and validation\nsets into a train, validation, and test set for this\nbenchmark. We encode the entire question and an-\nswers as a prompt and convert the problem into a\nfive-class prediction. We also convert the “Sanity”\ndatasets for user convenience.\n\nAll datasets in ERASER were tokenized using\nspaCy!! library (with SciSpacy (Neumann et al.,\n2019) for Evidence Inference). In addition, we also\nsplit all datasets except e-SNLI and CoS-E into\nsentences using the same library.\n\nB_ Annotation details\n\nWe collected comprehensive rationales for a subset\nof some test sets to accurately evaluate model recall\nof rationales.\n\n1. Movies. We used the Upwork Platform!” to\nhire two fluent english speakers to annotate\neach of the 200 documents in our test set.\nWorkers were paid at rate of USD 8.5 per hour\nand on average, it took them 5 min to anno-\ntate a document. Each annotator was asked to\nannotate a set of 6 documents and compared\nagainst in-house annotations (by authors).\n\n2. Evidence Inference. We again used Upwork\nto hire 4 medical professionals fluent in en-\nglish and having passed a pilot of 3 documents.\n125 documents were annotated (only once by\none of the annotators, which we felt was ap-\npropriate given their high-level of expertise)\nwith an average cost of USD 13 per document.\nAverage time spent of single document was\n31 min.\n\n3. BoolQ. We used Amazon Mechanical Turk\n(MTurk) to collect reference comprehensive\nrationales from randomly selected 199 docu-\nments from our test set (ranging in 800 to 1500\ntokens in length). Only workers from AU, NZ,\nCA, US, GB with more than 10K approved\nHITs and an approval rate of greater than 98%\nwere eligible. For every document, 3 annota-\ntions were collected and workers were paid\nUSD 1.50 per HIT. The average work time\n(obtained through MTurk interface) was 21\nmin. We did not anticipate the task taking so\n\n\"https://spacy.io/\nPhttp: //www.upwork.com\n\nlong (on average); the effective low pay rate\nwas unintended.\n\nC_ Hyperparameter and training details\nC.1 (Lei et al., 2016) models\n\nFor these models, we set the sparsity rate at 0.01\nand we set the contiguity loss weight to 2 times\nsparsity rate (following the original paper). We\nused bert-base-uncased (Wolf et al., 2019) as to-\nken embedder (for all datasets except BoolQ, Ev-\nidence Inference and FEVER) and Bidirectional\nLSTM with 128 dimensional hidden state in each\ndirection. A dropout (Srivastava et al., 2014) rate\nof 0.2 was used before feeding the hidden repre-\nsentations to attention layer in decoder and linear\nlayer in encoder. One layer MLP with 128 dimen-\nsional hidden state and ReLU activation was used\nto compute the decoder output distribution.\n\nFor three datasets mentioned above, we use\nGloVe embeddings (nt tp: //nlp.stanford.edu/\ndata/glove.840B.300d.zip).\n\nA learning rate of 2e-5 with Adam (Kingma and\nBa, 2014) optimizer was used for all models and we\nonly fine-tuned top two layers of BERT encoder.\nTh models were trained for 20 epochs and early\nstopping with patience of 5 epochs was used. The\nbest model was selected on validation set using the\nfinal task performance metric.\n\nThe input for the above model was encoded\nin form of [CLS] document [SEP] query\n[SEP].\n\nThis model was implemented using the\nAllenNLP library (Gardner et al., 2018).\n\nC.2. BERT-LSTM/GloVe-LSTM\n\nThis model is essentially the same as the decoder in\nprevious section. The BERT-LSTM uses the same\nhyperparameters, and GloVe-LSTM is trained with\na learning rate of le-2.\n\nC.3 Lehman et al. (2019) models\n\nWith the exception of the Evidence Inference\ndataset, these models were trained using the GLoVe\n(Pennington et al., 2014) 200 dimension word vec-\ntors, and Evidence Inference using the (Pyysalo\net al., 2013) PubMed word vectors. We use Adam\n(Kingma and Ba, 2014) with a learning rate of\nle-3, Dropout (Srivastava et al., 2014) of 0.05 at\neach layer (embedding, GRU, attention layer) of\nthe model, for 50 epochs with a patience of 10. We\nmonitor validation loss, and keep the best model\non the validation set.\n\n4457\n", "vlm_text": "or questions with rationales that were not possi- ble to automatically map back to the underlying text. As recommended by the authors of  Talmor et al.  ( 2019 ) we repartition the train and validation sets into a train, validation, and test set for this benchmark. We encode the entire question and an- swers as a prompt and convert the problem into a ﬁve-class prediction. We also convert the “Sanity” datasets for user convenience. \nAll datasets in ERASER were tokenized using spaCy 11   library (with SciSpacy ( Neumann et al. , 2019 ) for Evidence Inference). In addition, we also split all datasets except e-SNLI and CoS-E into sentences using the same library. \nB Annotation details \nWe collected  comprehensive  rationales for a subset of some test sets to accurately evaluate model recall of rationales. \n1.  Movies . We used the Upwork Platform 12   to hire two ﬂuent english speakers to annotate each of the 200 documents in our test set. Workers were paid at rate of USD 8.5 per hour and on average, it took them   $5~\\mathrm{min}$   to anno- tate a document. Each annotator was asked to annotate a set of 6 documents and compared against in-house annotations (by authors). \n2.  Evidence Inference . We again used Upwork to hire 4 medical professionals ﬂuent in en- glish and having passed a pilot of 3 documents. 125 documents were annotated (only once by one of the annotators, which we felt was ap- propriate given their high-level of expertise) with an average cost of USD 13 per document. Average time spent of single document was  $31\\;\\mathrm{{min}}$  . \n3.  BoolQ . We used Amazon Mechanical Turk (MTurk) to collect reference comprehensive rationales from randomly selected 199 docu- ments from our test set (ranging in 800 to 1500 tokens in length). Only workers from AU, NZ, CA, US, GB with more than 10K approved HITs and an approval rate of greater than  $98\\%$  were eligible. For every document, 3 annota- tions were collected and workers were paid USD 1.50 per HIT. The average work time (obtained through MTurk interface) was 21 min. We did not anticipate the task taking so \nlong (on average); the effective low pay rate was unintended. \nC Hyperparameter and training details \nC.1 ( Lei et al. ,  2016 ) models \nFor these models, we set the sparsity rate at 0.01 and we set the contiguity loss weight to 2 times sparsity rate (following the original paper). We used bert-base-uncased ( Wolf et al. ,  2019 ) as to- ken embedder (for all datasets except BoolQ, Ev- idence Inference and FEVER) and Bidirectional LSTM with 128 dimensional hidden state in each direction. A dropout ( Srivastava et al. ,  2014 ) rate of 0.2 was used before feeding the hidden repre- sentations to attention layer in decoder and linear layer in encoder. One layer MLP with 128 dimen- sional hidden state and ReLU activation was used to compute the decoder output distribution. \nFor three datasets mentioned above, we use GloVe embeddings ( http://nlp.stanford.edu/ data/glove.840B.300d.zip ). \nA learning rate of 2e-5 with Adam ( Kingma and Ba ,  2014 ) optimizer was used for all models and we only ﬁne-tuned top two layers of BERT encoder. Th models were trained for 20 epochs and early stopping with patience of 5 epochs was used. The best model was selected on validation set using the ﬁnal task performance metric. \nThe input for the above model was encoded in form of  [CLS] document [SEP] query [SEP] . \nThis model was implemented using the AllenNLP  library ( Gardner et al. ,  2018 ). \nC.2 BERT-LSTM/GloVe-LSTM \nThis model is essentially the same as the decoder in previous section. The BERT-LSTM uses the same hyperparameters, and GloVe-LSTM is trained with a learning rate of 1e-2. \nC.3 Lehman et al.  ( 2019 ) models \nWith the exception of the Evidence Inference dataset, these models were trained using the GLoVe ( Pennington et al. ,  2014 ) 200 dimension word vec- tors, and Evidence Inference using the ( Pyysalo et al. ,  2013 ) PubMed word vectors. We use Adam ( Kingma and Ba ,  2014 ) with a learning rate of 1e-3, Dropout ( Srivastava et al. ,  2014 ) of 0.05 at each layer (embedding, GRU, attention layer) of the model, for 50 epochs with a patience of 10. We monitor validation loss, and keep the best model on the validation set. "}
{"page": 15, "image_path": "doc_images/2020.acl-main.408_15.jpg", "ocr_text": "C.4 BERT-to-BERT model\n\nWe primarily used the ‘bert-base-uncased‘ model\nfor both components of the identification and clas-\nsification pipeline, with the sole exception being\nEvidence Inference with SciBERT (Beltagy et al.,\n2019). We trained with the standard BERT parame-\nters of a learning rate of le-5, Adam (Kingma and\nBa, 2014), for 10 epochs. We monitor validation\nloss, and keep the best model on the validation set.\n\n4458\n", "vlm_text": "C.4 BERT-to-BERT model \nWe primarily used the ‘bert-base-uncased‘ model for both components of the identiﬁcation and clas- siﬁcation pipeline, with the sole exception being Evidence Inference with SciBERT ( Beltagy et al. , 2019 ). We trained with the standard BERT parame- ters of a learning rate of 1e-5, Adam ( Kingma and Ba ,  2014 ), for 10 epochs. We monitor validation loss, and keep the best model on the validation set. "}
