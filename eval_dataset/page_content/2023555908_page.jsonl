{"page": 0, "image_path": "page_images/2023555908_0.jpg", "ocr_text": "LAW\nLIBRARY\n\nLIBRARY OF CONGRESS\n\nSafety and Security of Artificial\nIntelligence Systems\n\nAustralia * Canada * European Union * New Zealand\nUnited Kingdom\n\nJuly 2023\n\nLL File No. 2023-022302\nLRA-D-PUB-002615\n\nThe Law Library of Congress, Global Legal Research Directorate\n(202) 707-5080 * law@loc.gov * http://www.law.gov\n", "vlm_text": "Safety and Security of Artificial  Intelligence Systems   \nAustralia • Canada • European Union • New Zealand  United Kingdom  \nJuly 2023  \nLL File No. 2023-022302 LRA-D-PUB-002615  "}
{"page": 1, "image_path": "page_images/2023555908_1.jpg", "ocr_text": "This report is provided for reference purposes only.\nIt does not constitute legal advice and does not represent the official\nopinion of the United States Government. The information provided\n\nreflects research undertaken as of the date of writing.\nIt has not been updated.\n\n", "vlm_text": "This report is provided for reference purposes only.  It does not constitute legal advice and does not represent the official  opinion of the United States Government. The information provided  reflects research undertaken as of the date of writing.   It has not been updated. "}
{"page": 2, "image_path": "page_images/2023555908_2.jpg", "ocr_text": "Contents\n\nComparative SUMIMALY .......ccccssesecsessesceseeseessssssscseesessesusseseeseesesseseeseessescessseseesssnssusseeseesseesseeseessenseneanesees 1\n\nFigure 1: Types of Legislation Related to AI by Jurisdiction ......s..sessessssssssesessssssssesestsstestsseeseeneensseeseeses 3\nAUStralia oes ceceecsesscseseesessessssueseeseesessesseseessesssussucsessesussueseeseeseeseseseessessesssusseesesnssusseeseeseeesseeseessessaneanesees 4\nCama da..esceccecceccsseeseessessseseesessesnssneseeseessesesesecseesssscsussessesussussussecseesssseseeseessesssucseesesnesussusseeseeseseesecseeneeneenens 25\n\nEuropean Union....\n\nNew Zealand ......cccccceccsssssseseseseeeseseseseeseceseseeeceeseseeeseeseseneceeseseeeseeseseneceeseseeeseeseseeeeeeseaeeeeeseseeeeeeaeseeeseeaeaees 52\nUnited Kingdom «00... cccccescescesessesseeseesseeseesseseesssscsnssessessesussesseeseessssssceseessesssusseesesnesussusseeseesesseseeseeneenseness 66\nTable:\n\nTable of Primary SOULCES .......c.ccsessessesseseeseeseessssssneseesesnssusssseeseesesscseeseessescsueseesessesussueseeseesesseseeseeseeneanens 89\n", "vlm_text": "Contents  \nComparative Summary ............................................................................................................................. 1  Figure 1: Types of Legislation Related to AI by Jurisdiction  ................................................................. 3  Australia ...................................................................................................................................................... 4  Canada  ....................................................................................................................................................... 25  European Union ....................................................................................................................................... 35  New Zealand ............................................................................................................................................ 52  United Kingdom ...................................................................................................................................... 66  \nTable of Primary Sources ........................................................................................................................ 89  "}
{"page": 3, "image_path": "page_images/2023555908_3.jpg", "ocr_text": "Comparative Summary\n\nJenny Gesley\nForeign Law Specialist\n\nThis report surveys the safety and security of artificial intelligence systems (AI systems) in five\nselected jurisdictions, namely Australia, Canada, New Zealand, the United Kingdom (UK), and\nthe European Union (EU).\n\nThe use of AI has increased exponentially and is permeating every aspect of our lives, from\npersonal to professional. While it can be used in many positive ways to solve global challenges,\nthere are also security risks to be considered, such as fundamental rights infringements, personal\ndata security, and harmful uses. The European Union Agency for Cybersecurity (ENISA) has\nidentified three dimensions to the relationship between cybersecurity and AI. First, there is the\ncybersecurity of Al, meaning a lack of robustness and the vulnerabilities of AI models and\nalgorithms. Second, AI can also support cybersecurity when it is used as a tool or means to create\nadvanced cybersecurity, such as by developing more effective security controls and by facilitating\nthe efforts of law enforcement and other public authorities to respond to cybercrime. Lastly, there\nis the malicious use of AI, meaning when AI is used in a harmful, malicious, or adversarial way\nto create more sophisticated types of attacks.! This report focuses on the first dimension.\n\nIn order to ensure that AI systems are used to benefit society, jurisdictions around the world are\nlooking into ways to regulate AI. Whereas the EU intends to adopt its legislative proposal for a\nspecific Artificial Intelligence Act (draft AI Act) by the end of 2023 and the Canadian government\nintroduced an Artificial Intelligence and Data Act (AIDA) in June 2022, other surveyed\njurisdictions have not yet advanced similar legislation. Both Australia and New Zealand do not\ncurrently have laws or proposed specific laws related to Al. However, a May 2023 discussion\npaper published by the Australian government seeks public feedback on possible policy and\nregulatory responses to AI, including the adoption of a risk-based approach similar to the EU\ndraft AI Act. Likewise, the UK has not passed AI-specific legislation and does not intend to do so\ncurrently, but might at a later stage “enhance regulatory powers, ensure regulatory coordination,\nor create new institutional architecture.” Currently, there are 18 legal frameworks containing over\n50 pieces of legislation that touch upon AI in the UK. Figure 1 below depicts types of Al-related\nlegislation by jurisdiction.\n\nThere is no universal definition of an “AI system.” The European Parliament, in its amendments\nto the draft AI Act, proposes to align its definition with that of the OECD. An AI system is\naccordingly defined as “a machine-based system that is designed to operate with varying levels\nof autonomy and that can, for explicit or implicit objectives, generate outputs such as predictions,\nrecommendations, or decisions, that influence physical or virtual environments.” Canada’s\nproposed AIDA defines an AI system in a similar way with a few minor variations. The UK\nacknowledged the EU’s definition, but criticized it for not capturing the full application of Al and\nits regulatory implications. It stated that “no single definition is going to be suitable for every\n\n1 ENISA, Cybersecurity of AI and Standardisation 10 (Mar. 14, 2023), para. 2.2, https:/ / perma.cc/TL52-PFMG.\n", "vlm_text": "Comparative Summary  \nJenny Gesley  Foreign Law Specialist \nThis report surveys the safety and security of artificial intelligence systems (AI systems) in five  selected jurisdictions, namely  Australia ,  Canada, New Zealand , the  United Kingdom (UK) , and  the  European Union (EU) .   \nThe use of AI has increased exponentially and is permeating every aspect of our lives, from  personal to professional. While it can be used in many positive ways to solve global challenges,  there are also security risks to be considered, such as fundamental rights infringements, personal  data security, and harmful uses. The European Union Agency for Cybersecurity (ENISA) has  identified three dimensions to the relationship between cybersecurity and AI. First, there is the  cybersecurity of AI, meaning a lack of robustness and the vulnerabilities of AI models and  algorithms. Second, AI can also support cybersecurity when it is used as a tool or means to create  advanced cybersecurity, such as by developing more effective security controls and by facilitating  the efforts of law enforcement and other public authorities to respond to cybercrime. Lastly, there  is the malicious use of AI, meaning when AI is used in a harmful, malicious, or adversarial way  to create more sophisticated types of attacks.  This report focuses on the first dimension.  \nIn order to ensure that AI systems are used to benefit society, jurisdictions around the world are  looking into ways to regulate AI. Whereas the  EU  intends to adopt its legislative proposal for a  specific Artificial Intelligence Act (draft AI Act) by the end of 2023 and the  Canadian  government  introduced an Artificial Intelligence and Data Act (AIDA) in June 2022, other surveyed  jurisdictions have not yet advanced similar legislation. Both  Australia  and  New Zealand  do not  currently have laws or proposed specific laws related to AI. However, a May 2023 discussion  paper published by the  Australian  government seeks public feedback on possible policy and  regulatory responses to AI, including the adoption of a risk-based approach similar to the EU  draft AI Act.   Likewise, the  UK  has not passed AI-specific legislation and does not intend to do so  currently, but might at a later stage “enhance regulatory powers, ensure regulatory coordination,  or create new institutional architecture.” Currently, there are 18 legal frameworks containing over  50 pieces of legislation that touch upon AI in the  UK . Figure 1 below depicts types of AI-related  legislation by jurisdiction.  \nThere is no universal definition of an “AI system.” The  European Parliament , in its amendments  to the draft AI Act, proposes to align its definition with that of the OECD. An AI system is  accordingly defined as “a machine-based system that is designed to operate with varying levels  of autonomy and that can, for explicit or implicit objectives, generate outputs such as predictions,  recommendations, or decisions, that influence physical or virtual environments.”  Canada’s  proposed AIDA defines an AI system in a similar way with a few minor variations. The  UK acknowledged the EU’s definition, but criticized it for not capturing the full application of AI and  its regulatory implications. It stated that “no single definition is going to be suitable for every  scenario.”  Australia  and  New Zealand  do not have a legal definition of AI, but various public  and private sector bodies have provided definitions in documents that discuss AI.  "}
{"page": 4, "image_path": "page_images/2023555908_4.jpg", "ocr_text": "scenario.” Australia and New Zealand do not have a legal definition of AI, but various public\nand private sector bodies have provided definitions in documents that discuss AI.\n\nThe proposed EU draft AI Act contains specific security requirements for AI systems that would\nqualify as high-risk, in particular registration in an EU Database for Stand-Alone High-Risk AI\nSystems and compliance with detailed mandatory requirements with regard to risk management\nsystems; the quality of data sets used; technical documentation; record keeping; transparency and\nprovision of information to users; human oversight; appropriate levels of accuracy, robustness,\nand cybersecurity; quality management systems; and ex-ante conformity assessment. The\nproposed bill in Canada contains similar security requirements, with the exception of provisions\non quality management systems and appropriate levels of robustness. They appear to reflect\nthose in the US National Institute of Standards and Technology’s AI Risk Management\nFramework 1.0. In addition, there is a Directive on Automated Decision-Making that requires all\nautomated decisions by federal institutions to be subject to an algorithmic impact assessment.\nAustralia and New Zealand have several technology-neutral general statutes and guidance\ndocuments that address aspects of cybersecurity that are not specific to Al systems. In the UK, an\nimpact assessment found that there are “key gaps in the UK’s current legal frameworks relate[d]\nto individual rights, safety standards specific to AI, transparency, human involvement,\naccountability, and rights to redress.”\n\nWith regard to security of personal data, the EU’s General Data Protection Regulation (GDPR)\nmakes security of personal data a prerequisite for the processing of personal data and requires\ncontrollers to apply the principles of security by design and by default. The UK incorporated the\nEU GDPR into its national law through the Data Protection Act 2018 (DPA). The DPA, together\nwith other EU legislation, was incorporated into a new body of domestic law after the UK left the\nEU. In Canada, in addition to the federal Personal Information Protection and Electronic\nDocuments Act, the proposed bill would, among other things, require covered people to\n“establish measures with respect to the manner in which data is anonymized” and “establish\nmeasures with respect to the management of anonymized data.” As mentioned, Australia’s\nPrivacy Act 1988 (Cth) and New Zealand's Privacy Act 2020 are technology-neutral and apply to\nAl systems. Guidance by the Office of the Australian Information Commissioner regarding data\nanalytics recommends, among other things, using de-identified data wherever possible, taking a\nprivacy-by-design approach, and protecting information in line with risk assessments. In New\nZealand, the Privacy Commissioner expects entities implementing generative AI tools to, for\nexample, conduct privacy impact assessments, be transparent, and ensure human review.\n\nWith regard to AI security policy across the supply chain, no concrete Al-specific measures have\nbeen implemented by the surveyed jurisdictions. However, the EU’s ENISA noted that the AI-\nrelated supply chain issue is one of the challenges for cybersecurity. Likewise, the UK\ngovernment stated that “Al supply chains can be complex and opaque, making effective\ngovernance of AI and supply chain risk management difficult.” It added, however, that it is too\nsoon to introduce new measures to regulate the Al supply chain.\n", "vlm_text": "\nThe proposed  EU  draft AI Act contains specific security requirements for AI systems that would  qualify as high-risk, in particular registration in an EU Database for Stand-Alone High-Risk AI  Systems and compliance with detailed mandatory requirements with regard to risk management  systems; the quality of data sets used; technical documentation; record keeping; transparency and  provision of information to users; human oversight; appropriate levels of accuracy, robustness,  and cybersecurity; quality management systems; and ex-ante conformity assessment. The  proposed bill in  Canada  contains similar security requirements, with the exception of provisions  on quality management systems and appropriate levels of robustness. They appear to reflect  those in the US National Institute of Standards and Technology’s AI Risk Management  Framework 1.0. In addition, there is a Directive on Automated Decision-Making that requires all  automated decisions by federal institutions to be subject to an algorithmic impact assessment.  Australia  and  New Zealand  have several technology-neutral general statutes and guidance  documents that address aspects of cybersecurity that are not specific to AI systems. In the  UK , an  impact assessment found that there are “key gaps in the UK’s current legal frameworks relate[d]  to individual rights, safety standards specific to AI, transparency, human involvement,  accountability, and rights to redress.”  \nWith regard to security of personal data, the  EU’s  General Data Protection Regulation (GDPR)  makes security of personal data a prerequisite for the processing of personal data and requires  controllers to apply the principles of security by design and by default. The  UK  incorporated the  EU GDPR into its national law through the Data Protection Act 2018 (DPA). The DPA, together  with other EU legislation, was incorporated into a new body of domestic law after the UK left the  EU. In  Canada,  in addition to the federal Personal Information Protection and Electronic  Documents Act, the proposed bill would, among other things, require covered people to  “establish measures with respect to the manner in which data is anonymized” and “establish  measures with respect to the management of anonymized data.” As mentioned,  Australia’s Privacy Act 1988 (Cth) and  New Zealand’s  Privacy Act 2020 are technology-neutral and apply to  AI systems. Guidance by the Office of the  Australian  Information Commissioner regarding data  analytics recommends, among other things, using de-identified data wherever possible, taking a  privacy-by-design approach, and protecting information in line with risk assessments. In  New  Zealand , the Privacy Commissioner expects entities implementing generative AI tools to, for  example, conduct privacy impact assessments, be transparent, and ensure human review.   \nWith regard to AI security policy across the supply chain, no concrete AI-specific measures have  been implemented by the surveyed jurisdictions. However, the  EU’s ENISA  noted that the AI- related supply chain issue is one of the challenges for cybersecurity. Likewise, the  UK government stated that “AI supply chains can be complex and opaque, making effective  governance of AI and supply chain risk management difficult.” It added, however, that it is too  soon to introduce new measures to regulate the AI supply chain.  "}
{"page": 5, "image_path": "page_images/2023555908_5.jpg", "ocr_text": "Safety and Security of Artificial Intelligence Systems: Comparative Summary\n\nFigure 1: Types of Legislation Related to AI\nby Jurisdiction\n\nSpecific\n(Pro\n\nGeneral Legislation\nApplied to AI\n\nOF CONGRESS\n\nThe Law Library of Congress 3\n", "vlm_text": "The image is a Venn diagram illustrating the approach different regions take towards AI legislation:\n\n1. **Specific AI Legislation (Proposed)** - Red circle\n   - Includes: Canada, European Union\n\n2. **General Legislation Mentioning AI** - Orange circle\n   - Includes: United Kingdom, European Union\n\n3. **General Legislation Applied to AI** - Blue circle\n   - Includes: Australia, New Zealand, Canada\n\nThe European Union is at the intersection of all three circles, indicating it has proposed specific AI legislation, mentions AI in general legislation, and applies general legislation to AI."}
{"page": 6, "image_path": "page_images/2023555908_6.jpg", "ocr_text": "SUMMARY\n\nAustralia\n\nKelly Buchanan\nChief, Foreign, Comparative, and\nInternational Law Division II\n\nAustralia does not currently have specific laws related to artificial intelligence (Al).\nHowever, a discussion paper published in May 2023 seeks public feedback on possible\npolicy and regulatory responses to AI, including the adoption of a risk-based approach\nsimilar to that developed in the European Union and Canada. In addition, various work\nhas been undertaken in the public and private sectors related to AI regulation and\ndevelopment, including the publication of an AI ethics framework in 2019 and the\nestablishment of a national AI center and a Responsible AI Network to provide\nguidance to Australian businesses, as well as discussion and guidance regarding\ngeneral and sector-specific laws and their application to AI. Existing laws relevant to\nthe regulation of AI include, for example, privacy legislation, telecommunications law,\nlegislation related to the protection of critical infrastructure, consumer protection law,\nand anti-discrimination legislation. A recent report on reforming the federal privacy\nstatute made recommendations on the regulation of automated decision-making under\nthat law.\n\nIn terms of cybersecurity of AI, there are various laws and government documents that\nmay be relevant. The government is considering the development of new cybersecurity\nlegislation that would harmonize the existing “patchwork” of laws, policies, and\nframeworks, as well as possible amendments to critical infrastructure legislation that\nwould bring customer data and “systems” into the definition of critical assets.\n\nRelevant government agencies have engaged with international partners on\ncybersecurity guidance relevant to AI and on the development of Al-related standards.\n\nI. Introduction\n\nAustralia does not currently have specific legislation or provisions that regulate artificial\nintelligence (AI). Both the current and previous government have engaged in discussions\nregarding the AI policy and regulatory framework.? There has been a focus in recent years on\ndeveloping a self-regulatory, principles-based approach, including the development of ethics\nprinciples and the establishment of a Responsible AI Network to guide AI adoption among\n\n1See Lachlan Mee, Regulating Artificial Intelligence in Australia, LinkedIn (Oct. 7, 2022), https:/ / perma.cc/58LR-\nEPBN; Kim O’Connell et al., Developments in the Regulation of Artificial Intelligence, KWM (Apr. 19, 2023),\nhttps:/ / perma.cc/J6D4-2968.\n\n2 See generally Dudley Kneller, A Change in Government May Put the Brakes on Australia’s Support of AI and\nADM? Or Perhaps Not?, Gadens (July 18, 2022), https:/ / perma.cc/7CBZ-28DT.\n", "vlm_text": "Australia  \nKelly Buchanan  Chief, Foreign, Comparative, and  International Law Division II   \nSUMMARY \nAustralia does not currently have specific laws related to artificial intelligence (AI).  However, a discussion paper published in May 2023 seeks public feedback on possible  policy and regulatory responses to AI, including the adoption of a risk-based approach  similar to that developed in the European Union and Canada. In addition, various work  has been undertaken in the public and private sectors related to AI regulation and  development, including the publication of an AI ethics framework in 2019 and the  establishment of a national AI center and a Responsible AI Network to provide  guidance to Australian businesses, as well as discussion and guidance regarding  general and sector-specific laws and their application to AI. Existing laws relevant to  the regulation of AI include, for example, privacy legislation, telecommunications law,  legislation related to the protection of critical infrastructure, consumer protection law,  and anti-discrimination legislation. A recent report on reforming the federal privacy  statute made recommendations on the regulation of automated decision-making under  that law.  \nIn terms of cybersecurity of AI, there are various laws and government documents that  may be relevant. The government is considering the development of new cybersecurity  legislation that would harmonize the existing “patchwork” of laws, policies, and  frameworks, as well as possible amendments to critical infrastructure legislation that  would bring customer data and “systems” into the definition of critical assets.  \nRelevant government agencies have engaged with international partners on  cybersecurity guidance relevant to AI and on the development of AI-related standards.  \nI.  Introduction  \nAustralia does not currently have specific legislation or provisions that regulate artificial  intelligence (AI).  Both the current and previous government have engaged in discussions  regarding the AI policy and regulatory framework.  There has been a focus in recent years on  developing a self-regulatory, principles-based approach, including the development of ethics  principles and the establishment of a Responsible AI Network to guide AI adoption among  Australian businesses. However, on May 31, 2023, the government released a discussion paper, Safe and Responsible AI in Australia ,  that   "}
{"page": 7, "image_path": "page_images/2023555908_7.jpg", "ocr_text": "Australian businesses. However, on May 31, 2023, the government released a discussion paper,\nSafe and Responsible AI in Australia, that\n\nfocuses on governance mechanisms to ensure AI is developed and used safely and\nresponsibly in Australia. These mechanisms can include regulations, standards, tools,\nframeworks, principles and business practices.*\n\nThe government is seeking feedback on the options presented in the paper through a submission\nprocess, closing on July 26, 2023, to inform “consideration across government on any appropriate\nregulatory and policy responses” to AI.\n\nIn addition, on June 1, 2023, the National Science and Technology Council published a research\nreport on generative AI, which was commissioned by the government. The report included\n“examples of strategies that have been put in place internationally by other advanced economies\nsince the launch of models like ChatGPT to address the potential opportunities and impacts of\nartificial intelligence (AI).”7\n\nPreviously, in March 2022, the Department of the Prime Minister and Cabinet's Digital\nTechnology Taskforce published a paper for consultation, Positioning Australia as a Leader in\nDigital Economy Regulation (Automated Decision Making and AI Regulation): Issues Paper.’ The\nsubmission process closed in May 2022, but the new government, elected that same month, does\nnot appear to have taken further action related to the paper.\n\nIn 2021, the government released Australia’s Digital Economy Strategy, which included a vision\nfor Australia to be a top 10 digital economy by 2030.19 Also in 2021, the government published its\nAI Action Plan, which “set out a vision for Australia to be a global leader in developing and\nadopting trusted, secure and responsible AI.”\" Both of these documents were published under\nthe previous government and have been removed from current departmental websites.\n\n3 Department of Industry, Science and Resources, Safe and Responsible AI in Australia: Discussion Paper (June\n2023), https:/ / perma.cc/GW2C-NC75; Press Release, Ed Husic, Safe and Responsible AI (June 1, 2023),\nhttps:/ / perma.cc/ BT7T-QW9U,; Jake Evans, Artificial Intelligence Technologies Could be Classified by Risk, As\nGovernment Consults on Al Regulation, ABC News (May 31, 2023), https:/ / perma.cc/64XT-U8NX.\n\n+ Supporting Responsible AI: Discussion Paper, Department of Industry, Science and Resources,\nhttps:/ / perma.cc/F4KF-SSMA.\n\n51d.\n\n© Rapid Response Information Report: Generative AI, Australia’s Chief Scientist (June 1, 2023),\nhttps:/ / perma.cc/ N4BZ-23DX.\n\n71d.\n\n8 Australian Government, Positioning Australia as a Leader in Digital Economy Regulation - Automated Decision\nMaking and AI Regulation: Issues Paper (Mar. 2022), https:/ /perma.cc/QB3A-7KE3.\n\n° Positioning Australia as a Leader in Digital Economy Regulation (Automated Decision Making and AI Regulation):\nIssues Paper, Department of Industry, Science and Resources, https: / / perma.cc/6U66-8WUH.\n\n10 Australian Government, Digital Economy Strategy 2030 (2021), https:/ / perma.cc/4A8U-CBWF.\n\n1 See Australia’s Artificial Intelligence Action Plan, Department of Industry, Sciences and Resources (archived\npage), https:/ / perma.cc/BQ4J-FHEW.\n", "vlm_text": "\nfocuses on governance mechanisms to ensure AI is developed and used safely and  responsibly in Australia. These mechanisms can include regulations, standards, tools,  frameworks, principles and business practices.    \nThe government is seeking feedback on the options presented in the paper through a submission  process, closing on July 26, 2023, to inform “consideration across government on any appropriate  regulatory and policy responses” to AI.    \nIn addition, on June 1, 2023, the National Science and Technology Council published a research  report on generative AI, which was commissioned by the government.  The report included  “examples of strategies that have been put in place internationally by other advanced economies  since the launch of models like ChatGPT to address the potential opportunities and impacts of  artificial intelligence (AI).” 7   \nPreviously, in March 2022, the Department of the Prime Minister and Cabinet’s Digital  Technology Taskforce published a paper for consultation,  Positioning Australia as a Leader in  Digital Economy Regulation (Automated Decision Making and AI Regulation): Issues Paper .  The  submission process closed in May 2022,  but the new government, elected that same month, does  not appear to have taken further action related to the paper.   \nIn 2021, the government released Australia’s  Digital Economy Strategy , which included a vision  for Australia to be a top 10 digital economy by 2030.  Also in 2021, the government published its  AI Action Plan , which “set out a vision for Australia to be a global leader in developing and  adopting trusted, secure and responsible AI.” 11  Both of these documents were published under  the previous government and have been removed from current departmental websites.   "}
{"page": 8, "image_path": "page_images/2023555908_8.jpg", "ocr_text": "In recent years, several policy and investment initiatives have commenced in relation to AI,\nincluding:\ne Publication of Australia’s “AI Ethics Framework” in 201912\n\n¢ Development of a “List of Critical Technologies in the National Interest,”13 which includes Al\ntechnologies!4\n\ne Becoming a founding member of the Global Partnership on Artificial Intelligence!5\n\ne¢ Government funding for programs and grants to support businesses to “integrate quantum\nand artificial intelligence technologies into their operations” !¢\n\ne The “Next Generation AI and Emerging Technologies Graduates” national scholarship\nprogram!”\n\nThe 2023 discussion paper provides an overview of current federal government initiatives\nrelevant to the “development, application or deployment of AI.”18\n\nState and territory governments are also considering the implications of Al for their own policy\nand regulatory frameworks. However, such work is not covered in this report.\n\nII. Overview of the Legal and Policy Framework\nA. Relevant Laws and Possible AI Legislation\n\nCurrent Commonwealth (i.e. federal) laws relevant to AI, including cybersecurity’? of AI, include:\n\n? Australia’s Artificial Intelligence Ethics Framework, Department of Industry, Science and Resources (Nov. 7,\n2019), https:/ / perma.cc/Z7UP-7R5L.\n\n13 List of Critical Technologies in the National Interest, Department of Industry, Science and Resources (May 19,\n2023), https:/ / perma.cc/B758-QPXY.\n\n¥4 List of Critical Technologies in the National Interest: AI Technologies, Department of Industry, Science and\nResources, https:/ / perma.cc/3WAS-MD5S.\n\n5 The Global Partnership on Artificial Intelligence Launches, Department of Industry, Science and Resources (June\n16, 2020), https: / / perma.cc/2KVE-SKPN.\n\n16 Investments to Grow Australia’s Critical Technologies Industries, Department of Industry, Science and Resources\n(May 12, 2023), https: / / perma.cc/ V863-6PJ5. Information on funding initiatives under the previous\ngovernment is available at Funding Available for Al and Digital Capability Centres, Department of Industry,\nScience and Resources (Mar. 31, 2022), https:/ / perma.cc/RQU2-F9Q2.\n\n1” Next Generation AI and Emerging Technologies Graduates Program, CSIRO, https:/ /perma.cc/ WF3A-VEMY.\n18 Safe and Responsible AI in Australia: Discussion Paper, supra note 3, at 15 & attachment A.\n19 See Dennis Miralis et al., Australia, in Cybersecurity 2023 (ICLG, 2023), https:/ / perma.cc/38UH-B72B.\n", "vlm_text": "In recent years, several policy and investment initiatives have commenced in relation to AI,  including: \n\n \n•   Publication of Australia’s “AI Ethics Framework” in 2019 12    \n\n •   Development of a “List of Critical Technologies in the National Interest,” 13  which includes AI  technologies 14  \n\n •   Becoming a founding member of the Global Partnership on Artificial Intelligence 15  \n\n •   Government funding for programs and grants to support businesses to “integrate quantum  and artificial intelligence technologies into their operations” 16  \n\n •   The “Next Generation AI and Emerging Technologies Graduates” national scholarship  program 17    \nThe 2023 discussion paper provides an overview of current federal government initiatives  relevant to the “development, application or deployment of AI.” 18   \nState and territory governments are also considering the implications of AI for their own policy  and regulatory frameworks. However, such work is not covered in this report.   \nII.  Overview of the Legal and Policy Framework  \nA.  Relevant Laws and Possible AI Legislation  \nCurrent Commonwealth (i.e. federal) laws relevant to AI, including cybersecurity 19  of AI, include:  "}
{"page": 9, "image_path": "page_images/2023555908_9.jpg", "ocr_text": "e Privacy Act 1988 (Cth)?\n\ne Security of Critical Infrastructure Act 2018 (Cth)?! (GOCI Act)\n\ne Telecommunications Act 1997 (Cth)\n\ne Telecommunications (Interception and Access) Act 1979 (Cth)?3\ne Criminal Code Act 1995 (Cth)\n\n¢ Corporations Act 2001 (Cth)?\n\nOther general laws that could be relevant include consumer protection legislation, copyright\nlegislation, online safety legislation, anti-discrimination legislation, administrative law, and\ncommon law related to tort and contract. In addition, sector-specific regulations that may apply\ninclude those related to therapeutic goods, food, motor vehicles, airline safety, and financial\nservices.2” The 2023 Safe and Responsible AI in Australia discussion paper states that “[t]hese are\nareas where the government has deemed specific sector-specific laws are necessary,” which “need\nto be well designed to avoid duplicating economy-wide regulations while filling in any gaps\nappropriate to AI.”28\n\nThe discussion paper notes that “the process of applying or adjusting” existing regulatory\nframeworks is already under way.””? This includes the implementation of the Online Safety Act\n2021 (Cth), which contains provisions on cyberbullying, image-based abuse, and the removal of\nillegal and harmful online content; the publication of guidance on software as a medical device;\na determination by the Office of the Australian Information Commissioner (OAIC) regarding\nClearview Al's gathering and use of biometric information for a facial recognition tool; new laws\nto provide regulators with powers to combat online misinformation and disinformation, which\n\n20 Privacy Act 1988 (Cth), https:/ / perma.cc/TP5W-Z238.\n21 Security of Critical Infrastructure Act 2018 (Cth), https:/ / perma.cc/QX3Y-QZHL.\n\n»2 Telecommunications Act 1997 (Cth), https:/ / perma.cc/P85S-MMKS8 (vol 1), https:/ / perma.cc/ ETSH-TFCF\n(vol 2).\n\n23 Telecommunications (Interception and Access) Act 1979 (Cth), https:/ / perma.cc/42DG-GN2G.\n\n24 Criminal Code Act 1995 (Cth), https:/ / perma.cc/9YWA-732B. Part 10.7 of the code contains computer\noffenses.\n\n25 Corporations Act 2001 (Cth) s 180, https:/ / perma.cc/5S7W-CTXW. Commentators explain that “[a] failure\nby acompany to prevent, mitigate, manage or respond to [a cybersecurity] Incident may result in breaches of\nprovisions of the Corporations Act 2001 (Cth). The Corporations Act 2001 (Cth) imposes duties on directors to\nexercise powers and duties with the care and diligence that a reasonable person would. A director who ignores\nthe real possibility of an Incident may be liable for failing to exercise their duties with care and diligence.”\nMiralis, supra note 19.\n\n26 Safe and Responsible Al in Australia: Discussion Paper, supra note 3, at 10.\n271d.\n8 Id.\n291d.\n", "vlm_text": "•   Privacy Act 1988 (Cth) 20  \n\n •   Security of Critical Infrastructure Act 2018 (Cth) 21  (SOCI Act) \n\n •   Telecommunications Act 1997 (Cth) 22  \n\n •   Telecommunications (Interception and Access) Act 1979 (Cth) 23  \n\n •   Criminal Code Act 1995 (Cth) 24  \n\n •   Corporations Act 2001 (Cth) 25   \nOther general laws that could be relevant include consumer protection legislation, copyright  legislation, online safety legislation, anti-discrimination legislation, administrative law, and  common law related to tort and contract.  In addition, sector-specific regulations that may apply  include those related to therapeutic goods, food, motor vehicles, airline safety, and financial  services.  The 2023  Safe and Responsible AI in Australia  discussion paper states that “[t]hese are  areas where the government has deemed specific sector-specific laws are necessary,” which “need  to be well designed to avoid duplicating economy-wide regulations while filling in any gaps  appropriate to AI.” 28   \nThe discussion paper notes that “the process of applying or adjusting” existing regulatory  frameworks is already under way.” 29  This includes the implementation of the Online Safety Act  2021 (Cth), which contains provisions on cyberbullying, image-based abuse, and the removal of  illegal and harmful online content; the publication of guidance on software as a medical device;  a determination by the Office of the Australian Information Commissioner (OAIC) regarding  Clearview AI’s gathering and use of biometric information for a facial recognition tool; new laws  to provide regulators with powers to combat online misinformation and disinformation, which  were announced in January 2023; and the review of the Privacy Act (further discussed below,  Part IV.D).   "}
{"page": 10, "image_path": "page_images/2023555908_10.jpg", "ocr_text": "were announced in January 2023; and the review of the Privacy Act (further discussed below,\nPart IV.D).30\n\nThrough the consultation process related to the discussion paper the government is seeking to\n“identify potential gaps in the existing domestic governance landscape and any possible\nadditional AI governance mechanisms to support the development and adoption of AI.”3! The\npaper specifically seeks feedback on a possible risk management approach for AI, “which builds\non the EU’s proposed AI Act and Canada’s directive.”°? It notes that “[t]here is a developing\ninternational direction towards a risk-based approach for governance of AI,”53 and that there is a\nneed to “ensure there are appropriate safeguards, especially for high-risk applications of AI\nand ADM.”34\n\nAccording to some legal commentators, writing prior to the release of the discussion paper, “[t]he\nexpectation is that a dedicated AI law will be introduced in Australia which will at least address\nthe concerns raised by the [Australian Human Rights Commission (HRC)] and other government\nand industry body reports.”5 Furthermore, “[i]t may well be that the regulation of Al in Australia\nwill be modelled off the EU’s AI Act and will adopt a similar risk-based approach which\nprescribes certain requirements based on the degree of risk the relevant AI system presents, and\nthe industry in which the AI system is deployed.” The same commentators suggest that\n“{fluture AI laws in Australia will likely include robust and prescriptive requirements with\nrespect to transparency, and the degree to which decisions made by AI systems can be explained\nwhich are integral to this evaluative process.”3”\n\nA recent survey of the Australian public found that “Australians expect AI to be regulated and\nwant an independent regulator to monitor the technology as it booms into mainstream society.”°8\nThe Australian Information Industry Association noted the difficulty in regulating AI but said\nthere was a need for “guidelines” and “guardrails,” that the absence of a specific policy on AI\nwas a “major obstacle” to the sector, and that while existing laws could be used in cases where\nAI causes harm, those laws “do not ensure AI technologies are designed or used safely.”9\n\n30 Td. at 11.\n\n31 Id. at 4.\n\n32 Id. at 31-32 & attachment C.\n33 Id. at 16.\n\n34 Id. at 26.\n\n35 Kit Lee & Philip Catania, Australia, in Comparative Guides: Artificial Intelligence, Global Legal Post (May\n2023), https:/ / perma.cc/ W7CS-LEZS.\n\n36 Td.\n\n37 Td.\n\n38 Jake Evans, Two-Thirds of Australians Say Not Enough Being Done to Protect from Unsafe Al, as Minister Called on\nto Act, ABC News (Mar. 28, 2023), https:/ / perma.cc/ NF79-FQ73.\n\n39 Td.\n\n", "vlm_text": "\nThrough the consultation process related to the discussion paper the government is seeking to  “identify potential gaps in the existing domestic governance landscape and any possible  additional AI governance mechanisms to support the development and adoption of AI.  $^{\\prime\\prime}31$   The  paper specifically seeks feedback on a possible risk management approach for AI, “which builds  on the EU’s proposed AI Act and Canada’s directive.” 32  It notes that “[t]here is a developing  international direction towards a risk-based approach for governance of  $\\mathrm{Al},^{\\prime\\prime}{}^{33}$   and that there is a  need to “ensure there are appropriate safeguards, especially for high-risk applications of AI  and ADM.” 34   \nAccording to some legal commentators, writing prior to the release of the discussion paper, “[t]he  expectation is that a dedicated AI law will be introduced in Australia which will at least address  the concerns raised by the [Australian Human Rights Commission (HRC)] and other government  and industry body reports.” 35  Furthermore, “[i]t may well be that the regulation of AI in Australia  will be modelled off the EU’s AI Act and will adopt a similar risk-based approach which  prescribes certain requirements based on the degree of risk the relevant AI system presents, and  the industry in which the AI system is deployed.” 36  The same commentators suggest that  “[f]uture AI laws in Australia will likely include robust and prescriptive requirements with  respect to transparency, and the degree to which decisions made by AI systems can be explained  which are integral to this evaluative process.” 37   \nA recent survey of the Australian public found that “Australians expect AI to be regulated and  want an independent regulator to monitor the technology as it booms into mainstream society.  $^{\\prime\\prime}38$    The Australian Information Industry Association noted the difficulty in regulating AI but said  there was a need for “guidelines” and “guardrails,” that the absence of a specific policy on AI  was a “major obstacle” to the sector, and that while existing laws could be used in cases where  AI causes harm, those laws “do not ensure AI technologies are designed or used safely.” 39   "}
{"page": 11, "image_path": "page_images/2023555908_11.jpg", "ocr_text": "B. Policies and Guidance of Relevant Agencies\nThere are several government agencies involved in promoting and regulating AI in Australia.\n\nThe Department of Industry, Science and Resources administers AI investment programs and\nprovides advice to the government on relevant policies and legislation. It led the development of\nthe 2023 discussion paper.\n\nThe National Artificial Intelligence Centre, coordinated by CSIRO (Australia’s national science\nagency),40 supports the Responsible AI Network“! and seeks to coordinate “ Australia’s expertise\nand capabilities for a strong, collaborative and focused AI ecosystem that benefits all\nAustralians.”#2 In March 2023, the center published Australia’s AI Ecosystem Momentum Report.8\nThe center is coordinated by the Data61 Group, which is the “data and digital specialist arm” of\nCSIRO.“ Data61 and the then-named Department of Industry, Innovation and Science developed\nthe Artificial Intelligence Roadmap in 2019.\n\nThe Responsible AI Network seeks to provide clear guidance for industry on best practices, with\n“six actionable pillars”: Law, Standards, Principles, Governance, Leadership, and Technology.\nCSIRO states that “[w]lorldwide, Standards and regulatory changes are coming, which will\nrequire major upskilling and change for organisations to adapt to this new regulatory\nlandscape.” 4¢\n\nThe HRC has been active in advocating for the regulation of AI using a human rights approach.!”\nIt published its Human Rights and Technology Final Report in 2021, which contained several\nrecommendations for regulating AI, including the establishment of an AI Safety Commissioner.*®\n\n40 National Artificial Intelligence Centre, CSIRO, https:/ / perma.cc/7P6C-FK8L; The National Artificial Intelligence\nCentre is Launched, Department of Industry, Science and Resources (Dec. 14, 2021), https:/ / perma.cc/6WEP-\nX4BU.\n\n41 National AI Centre’s Responsible AI Network, CSIRO, https:/ /perma.cc/QL9Q-EDTN.\n#2 National Artificial Intelligence Centre, supra note 40.\n\n* National Artificial Intelligence Centre, Australia’s AI Ecosystem Momentum (Mar. 2023),\nhttps:/ / perma.cc/9IQAU-QBLV.\n\n#4 Data61 Business Unit, CSIRO, https:/ / perma.cc/27EX-394U. See also Artificial Intelligence, CSIRO,\nhttps:/ / perma.cc/86FS-CYN5.\n\n4 Artificial Intelligence Roadmap, CSIRO, https:/ / perma.cc/9LLZ-SDZQ; CSIRO Data61, Artificial Intelligence:\nSolving Problems, Growing the Economy and Improving Our Quality of Life (2019), https:// perma.cc/XB7W-J5F5.\n\n46 National AI Centre’s Responsible AI Network, supra note 41.\n\n47 See, e.g., Australian Human Rights Commission, Human Rights in the Digital Age: Additional Material\nSubmitted to the UN Global Digital Compact 12 - 17 (Submission to the United Nations’ Office of the Secretary-\nGeneral’s Envoy on Technology, Apr. 30, 2023), https:/ / perma.cc/6VUC-Y9BT.\n\n48 Australian Human Rights Commission, Human Rights and Technology: Final Report (2021),\nhttps:/ / perma.cc/TPD5-LDNW.\n", "vlm_text": "B.  Policies and Guidance of Relevant Agencies  \nThere are several government agencies involved in promoting and regulating AI in Australia.  \nThe Department of Industry, Science and Resources administers AI investment programs and  provides advice to the government on relevant policies and legislation. It led the development of  the 2023 discussion paper.  \nThe National Artificial Intelligence Centre, coordinated by CSIRO (Australia’s national science  agency),  supports the Responsible AI Network 41  and seeks to coordinate “Australia’s expertise  and capabilities for a strong, collaborative and focused AI ecosystem that benefits all  Australians.” 42  In March 2023, the center published  Australia’s AI Ecosystem Momentum Report .   The center is coordinated by the Data61 Group, which is the “data and digital specialist arm” of  CSIRO.   Data61 and the then-named Department of Industry, Innovation and Science developed  the  Artificial Intelligence Roadmap  in 2019.   \nThe Responsible AI Network seeks to provide clear guidance for industry on best practices, with  “six actionable pillars”: Law, Standards, Principles, Governance, Leadership, and Technology.  CSIRO states that “[w]orldwide, Standards and regulatory changes are coming, which will  require major upskilling and change for organisations to adapt to this new regulatory  landscape.” 46   \nThe HRC has been active in advocating for the regulation of AI using a human rights approach. It published its  Human Rights and Technology Final Report  in 2021, which contained several  recommendations for regulating AI, including the establishment of an AI Safety Commissioner. "}
{"page": 12, "image_path": "page_images/2023555908_12.jpg", "ocr_text": "It previously published the Artificial Intelligence: Governance and Leadership Whitepaper in 201949\nand a technical paper titled Using Artificial Intelligence to Make Decisions: Addressing the Problem of\nAlgorithmic Bias in 2020.50\n\nOther agencies have examined and provided guidance on the use of AI in different\nregulatory contexts:\n\ne The Australian Securities and Investments Commission (ASIC) has published “cyber\nresilience good practices”>! and a regulatory guide on the provision of automated financial\nproduct advice to retail clients using algorithms and technology.>? ASIC’s 2022-23 priorities\nfor the supervision of market intermediaries includes “undertaking a thematic review of\nartificial intelligence/ machine learning (AI/ML) practices and associated risks and controls\namong market intermediaries and buy-side firms, including the implementation of AI/ML\nguidance issued by the International Organization of Securities Commissions (IOSCO).”53\n\ne In 2020, the Australian Competition and Consumer Commission (ACCC) commenced its\nDigital Platform Services Inquiry, which is expected to be completed in 2025.%4 It has\npublished several consultation papers and interim reports. Most recently, in March 2023, it\npublished an issues paper on the “expanding ecosystems of digital platform providers in\nAustralia,” which will be the focus of its seventh interim report to be published in September\n2023.55 The issues paper discusses the impact of large providers of digital platform services\nexpanding into different sectors and technologies, including AI (although the focus of the\npaper is on smart home devices and cloud storage and computing), including increased risk\nof potential consumer and competition issues arising from this.°°\n\n4 Australian Human Rights Commission, Artificial Intelligence: Governance and Leadership Whitepaper (2019),\nhttps:/ / perma.cc/ YR7W-7EMB.\n\n50 Australian Human Rights Commission, Using Artificial Intelligence to Make Decisions: Addressing the Problem of\nAlgorithmic Bias Technical Paper (2020), https:/ / perma.cc/8CW8-2XUT.\n\n51 Cyber Resilience Best Practices, Australian Securities and Exchange Commission, https:/ / perma.cc/B2W3-\nNCD2.\n\n5? Australian Securities and Exchange Commission, Providing Digital Financial Product Advice to Retail Clients\n(Regulatory Guide 255, Aug. 2016), https:/ / perma.cc/5P6T-3WPT.\n\n533 ASIC’s Priorities for the Supervision of Market Intermediaries in 2022-23, Australian Securities and Exchange\nCommission, https:/ / perma.cc/ MSY8-W8GU.\n\n54 Digital Platform Services Inquiry 2020-25, Australian Competition and Consumer Commission,\nhttps:/ / perma.cc/5685-6RV9.\n\n55 Id. See also Justin Hendry, ACCC to Probe Big Tech’s Growing ‘Web’, InnovationAus.com (Mar. 8, 2023),\nhttps:/ / perma.cc/R4B9-3QRQ.\n\n56 Australian Competition and Consumer Commission, Digital Platform Services Inquiry - September 2023 Report\non the Expanding Ecosystems of Digital Platform Service Providers: Issues Paper (Mar. 2023),\nhttps:/ / perma.cc/UZJ5-YC4B.\n", "vlm_text": "It previously published the  Artificial Intelligence: Governance and Leadership Whitepaper  in 2019 49   and a technical paper titled  Using Artificial Intelligence to Make Decisions: Addressing the Problem of  Algorithmic Bias  in 2020.    \nOther agencies have examined and provided guidance on the use of AI in different  regulatory contexts: \n\n \n•   The Australian Securities and Investments Commission (ASIC) has published “cyber  resilience good practices” 51  and a regulatory guide on the provision of automated financial  product advice to retail clients using algorithms and technology.  ASIC’s 2022-23 priorities  for the supervision of market intermediaries includes “undertaking a thematic review of  artificial intelligence/machine learning (AI/ML) practices and associated risks and controls  among market intermediaries and buy-side firms, including the implementation of AI/ML  \nguidance issued by the International Organization of Securities Commissions (IOSCO).”  \n\n •   In 2020, the Australian Competition and Consumer Commission (ACCC) commenced its  Digital Platform Services Inquiry, which is expected to be completed in 2025.  It has  published several consultation papers and interim reports. Most recently, in March 2023, it  published an issues paper on the “expanding ecosystems of digital platform providers in  Australia,” which will be the focus of its seventh interim report to be published in September  2023.  The issues paper discusses the impact of large providers of digital platform services  expanding into different sectors and technologies, including AI (although the focus of the  paper is on smart home devices and cloud storage and computing), including increased risk  of potential consumer and competition issues arising from this.    "}
{"page": 13, "image_path": "page_images/2023555908_13.jpg", "ocr_text": "The National Transport Commission published a paper on The Regulatory Framework for\nAutomated Vehicles in Australia in February 2022, which presented “proposals on the end-to-\nend regulatory framework for the commercial deployment of automated vehicles.”5”\n\nStandards Australia published a report titled An Artificial Intelligence Standards Roadmap:\nMaking Australia’s Voice Heard,>* which set out recommendations related to ensuring Australia\ncan effectively influence AI standards development globally and that Al-related standards\nare developed in a way that takes into account diversity and inclusion, ensures fairness, and\nbuilds social trust.5?\n\nThe Office of the Australian Information Commissioner (OAIC), among other relevant\nactivities, has made various submissions as part of consultation processes related to the\nregulation of AI. These include submissions on the HRC’s 2019 whitepaper (expressing the\nview that “there is scope within the existing regulatory framework, with appropriate\nadjustments, to increase accountability in the use of AI and related technology and to ensure\neffective oversight”®) and on the HRC’s 2020 human rights and technology discussion paper\n(stating that it considers “data protection [to be] a central pillar of this regulatory approach\nwhich provides a framework for addressing many issues highlighted in the discussion\npaper”®!), as well as on the discussion paper on the AI ethics framework (in which it suggested\nthat “further consideration should be given to the suitability of adopting some EU GDPR\nrights in the Australian context where gaps are identified in relation to emerging and existing\ntechnologies, including AI’®).\n\nThe Therapeutic Goods Administration (TGA) has published guidance on the regulation of\nsoftware-based medical devices, including artificial intelligence text-based products, stating\nthat these “may be subject to medical device regulations for software and need approval by\nthe TGA.”® It has also published medical device cyber security guidance for industry, which\nis intended for “manufacturers that develop software for use in or as standalone medical\ndevices, such as in Software as a Medical Device (SaMD); this includes devices that\n\n5? National Transport Commission, The Regulatory Framework for Automated Vehicles in Australia 2 (Feb. 2022),\nhttps:/ / perma.cc/ B3AE-BHH2.\n\n58 Standards Australia, An Artificial Intelligence Standards Roadmap: Making Australia’s Voice Heard (Mar. 2020),\nhttps:/ / perma.cc/ ULQ7-T456.\n\n59 Standards Australia Sets Priorities for Artificial Intelligence, Standards Australia (Mar. 12, 2020),\nhttps:/ / perma.cc/ WA8L-9A58.\n\n6 Office of the Australian Information Commissioner (OAIC), Artificial Intelligence: Governance and Leadership\nWhite Paper - Submission to the Australian Human Rights Commission (June 19, 2019), https:/ / perma.cc/G99Z-\nJSGY.\n\n6 OAIC, Human Rights and Technology Discussion Paper - Submission to the Australian Human Rights Commission\n(July 6, 2020), https:/ / perma.cc/ VKC2-2XL8.\n\n® OAIC, Artificial Intelligence: Australia’s Ethics Framework - Submission to the Department of Industry, Innovation\nand Science and Data 61 (June 24, 2019), https:/ / perma.cc/ VR2B-K466.\n\n® Regulation of Software Based Medical Devices, Therapeutic Goods Administration (last updated June 2, 2023),\nhttps:/ / perma.cc/ L9D3-XYH6.\n", "vlm_text": "•   The National Transport Commission published a paper on  The Regulatory Framework for  Automated Vehicles in Australia  in February 2022, which presented “proposals on the end-to- end regulatory framework for the commercial deployment of automated vehicles.” 57   \n\n •   Standards Australia published a report titled  An Artificial Intelligence Standards Roadmap:  Making Australia’s Voice Heard ,  which set out recommendations related to ensuring Australia  can effectively influence AI standards development globally and that AI-related standards  are developed in a way that takes into account diversity and inclusion, ensures fairness, and  builds social trust.  \n\n •   The Office of the Australian Information Commissioner (OAIC), among other relevant  activities, has made various submissions as part of consultation processes related to the  regulation of AI. These include submissions on the HRC’s 2019 whitepaper (expressing the  view that “there is scope within the existing regulatory framework, with appropriate  adjustments, to increase accountability in the use of AI and related technology and to ensure  effective oversight” 60 ) and on the HRC’s 2020 human rights and technology discussion paper  (stating that it considers “data protection [to be] a central pillar of this regulatory approach  which provides a framework for addressing many issues highlighted in the discussion  paper” 61 ), as well as on the discussion paper on the AI ethics framework (in which it suggested  that “further consideration should be given to the suitability of adopting some EU GDPR  rights in the Australian context where gaps are identified in relation to emerging and existing  technologies, including  $\\mathrm{AI}^{\\prime\\prime62}$  ). \n\n •   The Therapeutic Goods Administration (TGA) has published guidance on the regulation of  software-based medical devices, including artificial intelligence text-based products, stating  that these “may be subject to medical device regulations for software and need approval by  the TGA.  ${\\mathrm{\\Sigma}}^{\\prime\\prime}63$   It has also published medical device cyber security guidance for industry, which  is intended for “manufacturers that develop software for use in or as standalone medical  devices, such as in Software as a Medical Device (SaMD); this includes devices that  incorporate artificial intelligence in their design  ${\\mathrm{}}^{\\prime\\prime}64$   The TGA in addition has published cyber  security information for users of medical devices.  \n\n "}
{"page": 14, "image_path": "page_images/2023555908_14.jpg", "ocr_text": "incorporate artificial intelligence in their design.”* The TGA in addition has published cyber\nsecurity information for users of medical devices.®\n\ne The Office of the Commonwealth Ombudsmen has published the Automated Decision-Making:\nBetter Practice Guide,6* which provides guidance to government agencies.\n\ne The Australian Communications and Media Authority (ACMA) published an occasional\npaper on artificial intelligence in communications and the media in 2020 which examined\n\n> the implementation of ethical principles in communications and media markets\n> potential risks to consumers in interacting with automated customer service agents\n\n> the challenge of misinformation risks associated with online “filter bubbles” and\ncontent personalisation, including to diversity in individuals’ news consumption\n\n> how AI may be used in unsolicited communications and by scammers\n\n> developments in technical standardisation\n\n> how AI could change the spectrum environment”\nIII. Definition of AI Systems\n\nThere is no definition of AI or AI systems in Australia’s legislation. Most recently, the Safe and\nResponsible AI in Australia paper states that AI\n\nrefers to an engineered system that generates predictive outputs such as content, forecasts,\nrecommendations or decisions for a given set of human-defined objectives or parameters\nwithout explicit programming. AI systems are designed to operate with varying levels of\nautomation.®\n\nIV. Cybersecurity of AI\n\nAs stated above, there are no specific rules or requirements related to AI in Australian legislation,\nincluding with respect to cybersecurity. Entities in both the public and private sectors have\nobligations placed on them regarding data protection, risk management, and incident reporting.\nThere are also principles, guidance documents, and strategies developed by government agencies\nthat are relevant to ensuring the protection of AI data and systems. As also noted above,\nauthorities that regulate particular sectors or entities, such as ASIC and the TGA, have issued\ntheir own guidance on cybersecurity or cyber resilience.\n\n64 Medical Device Cyber Security Guidance for Industry, Therapeutic Goods Administration (Apr. 8, 2021; last\nupdated Nov. 24, 2022), https:/ / perma.cc/ R6VS-RBY2.\n\n6 Medical Device Cyber Security Information for Users, Therapeutic Goods Administration (Apr. 8, 2021; last\nupdated Nov. 24, 2022), https:/ / perma.cc/TYF7-6HKN.\n\n6 Commonwealth Ombudsman, Automated Decision-Making: Better Practice Guide (2020),\nhttps:/ / perma.cc/X7QM-2JE4.\n\n6? Australian Communications and Media Authority, Artificial Intelligence in Communications and Media:\nOccasional Paper 2 (July 2020), https:/ / perma.cc/S8DR-GEH6.\n\n6 Safe and Responsible AI in Australia: Discussion Paper, supra note 3, at 5.\n", "vlm_text": "\n•   The Office of the Commonwealth Ombudsmen has published the  Automated Decision-Making:  Better Practice Guide ,  which provides guidance to government agencies. \n\n \n•   The Australian Communications and Media Authority (ACMA) published an occasional  paper on artificial intelligence in communications and the media in 2020 which examined   \n $>$  the implementation of ethical principles in communications and media markets     $>$  potential risks to consumers in interacting with automated customer service agents     $>$  the challenge of misinformation risks associated with online ”filter bubbles” and  content personalisation, including to diversity in individuals’ news consumption     $>$  how AI may be used in unsolicited communications and by scammers     $>$  developments in technical standardisation     $>$  how AI could change the spectrum environment 67   \nIII.  Definition of AI Systems  \nThere is no definition of AI or AI systems in Australia’s legislation. Most recently, the  Safe and  Responsible AI in Australia  paper states that AI   \nrefers to an engineered system that generates predictive outputs such as content, forecasts,  recommendations or decisions for a given set of human-defined objectives or parameters  without explicit programming. AI systems are designed to operate with varying levels of  automation.   \nIV.  Cybersecurity of AI  \nAs stated above, there are no specific rules or requirements related to AI in Australian legislation,  including with respect to cybersecurity. Entities in both the public and private sectors have  obligations placed on them regarding data protection, risk management, and incident reporting.  There are also principles, guidance documents, and strategies developed by government agencies  that are relevant to ensuring the protection of AI data and systems. As also noted above,  authorities that regulate particular sectors or entities, such as ASIC and the TGA, have issued  their own guidance on cybersecurity or cyber resilience.  "}
{"page": 15, "image_path": "page_images/2023555908_15.jpg", "ocr_text": "There are several government agencies with responsibilities related to cybersecurity. The Cyber\nand Infrastructure Security Centre (CISC), part of the Department of Home Affairs, commenced\noperations in 2021. Under the current government, the position of Coordinator for Cyber\nSecurity was established in February 2023. It is to be supported by the National Office for Cyber\nSecurity and the Cyber and Infrastructure Security Group, both within the Department of Home\nAffairs.”0 The aim is to “ensure a centrally coordinated approach,” including “triaging” action\nafter a major incident.”!\n\nIn addition, the Australian Cyber Security Centre (ACSC) sits within the Australian Signals\nDirectorate, the agency responsible for electronic and communications intelligence and security.\nThe ACSC includes staff from several agencies and “is a hub for private and public sector\ncollaboration and information-sharing on cyber security, to prevent and combat threats and\nminimise harm to Australians.”72\n\nA. Cyber Security Strategy and Possible Cyber Security Act\n\nIn late 2022, the government appointed a Cyber Security Strategy Expert Advisory Board, which\nis responsible for developing a new national cybersecurity strategy.”\n\nIn early 2023, the advisory board published a discussion paper on the 2023-2030 Australian Cyber\nSecurity Strategy,”4 with submissions closing in April 2023.75 Previous strategies were published\nin 2016 and 2020, and the latter was complemented by Australia’s 2021 International Cyber and\nCritical Technology Engagement Strategy. However, the current government decided to replace the\n2020 strategy with a new seven-year strategy to be completed in 2023.” This strategy will progress\nin parallel with other activities, including the Privacy Act review (discussed below), the digital\nplatform services inquiry, and the National Plan to Combat Cybercrime.”\n\n69 What is the Cyber and Infrastructure Security Centre, Cyber and Infrastructure Security Centre (CISC),\nhttps:/ / perma.cc/E7QF-EHP4.\n\n7” David McGovern, Privacy, Digital Safety and Cyber Security: Budget Resources (Australian Parliamentary\nLibrary, Budget Review 2023-24, May 2023), https:/ / perma.cc/37CH-C6SS.\n\n71 Michelle Grattan, Albanese Government to Appoint Coordinator for Cyber Security, Amid Increasing Threat to\nSystems and Data, The Conversation (Feb. 26, 2023), https:/ / perma.cc/ YEP7-K67].\n\n? Cyber Security, Australian Signals Directorate, https:/ / perma.cc/476B-Y26W.\n21d.\n\n74 Expert Advisory Panel, 2023-2030 Australian Cyber Security Strategy: Discussion Paper (Feb. 2023),\nhttps:/ / perma.cc/CY6J-QSHV.\n\n75 2023-2030 Australian Cyber Security Strategy Discussion Paper, Department of Home Affairs,\nhttps:/ / perma.cc/XL73-DWCN.\n\n76 Denham Sadler, Govt to Appoint Cyber Leader to Run New Office, ACS InformationAge (Feb. 28, 2023),\nhttps:/ / perma.cc/ KZY7-U56Q.\n\n77 Expert Advisory Panel, supra note 74, at 14.\n", "vlm_text": "There are several government agencies with responsibilities related to cybersecurity. The Cyber  and Infrastructure Security Centre (CISC), part of the Department of Home Affairs, commenced  operations in 2021.  Under the current government, the position of Coordinator for Cyber  Security was established in February 2023. It is to be supported by the National Office for Cyber  Security and the Cyber and Infrastructure Security Group, both within the Department of Home  Affairs.  The aim is to “ensure a centrally coordinated approach,” including “triaging” action  after a major incident.    \nIn addition, the Australian Cyber Security Centre (ACSC) sits within the Australian Signals  Directorate, the agency responsible for electronic and communications intelligence and security.  The ACSC includes staff from several agencies and “is a hub for private and public sector  collaboration and information-sharing on cyber security, to prevent and combat threats and  minimise harm to Australians.” 72   \nA.  Cyber Security Strategy and Possible Cyber Security Act  \nIn late 2022, the government appointed a Cyber Security Strategy Expert Advisory Board, which  is responsible for developing a new national cybersecurity strategy.   \nIn early 2023, the advisory board published a discussion paper on the 2023-2030 Australian Cyber  Security Strategy,  with submissions closing in April 2023.  Previous strategies were published  in 2016 and 2020, and the latter was complemented by  Australia’s 2021 International Cyber and  Critical Technology Engagement Strategy . However, the current government decided to replace the  2020 strategy with a new seven-year strategy to be completed in 2023.  This strategy will progress  in parallel with other activities, including the Privacy Act review (discussed below), the digital  platform services inquiry, and the National Plan to Combat Cybercrime.   "}
{"page": 16, "image_path": "page_images/2023555908_16.jpg", "ocr_text": "Safety and Security of Artificial Intelligence Systems: Australia\n\nThe government has stated that it has a goal of Australia becoming the most cyber secure nation\nby 2030.78 In February 2023, the prime minister said that “government policies and regulations,\nbusiness sector systems and measures and our general awareness and capacity as a nation are\nsimply not at the level that we need them to be.””? The government has said that its priorities for\na new cyber security policy include “increasing whole-of-nation protection efforts, ensuring\ncritical infrastructure and government systems are resilient, building sovereign capabilities to\ntackle cyber threats, strengthening international engagement, and growing a national\ncyber workforce.”8°\n\nThe 2023 discussion paper refers to “enhancing and harmonising” the regulatory frameworks for\ncybersecurity:\n\nWe have heard from industry that business owners often do not feel their cyber security\nobligations are clear or easy to follow, both from an operational perspective and as\ncompany directors. There are a range of implicit cyber security obligations placed on\nAustralian businesses and nongovernment entities, including through the corporations,\nconsumer, critical infrastructure, and privacy legislative and regulatory frameworks.\nHowever, it is clear from stakeholder feedback and the increasing frequency and severity\nof major cyber incidents, that more explicit specification of obligations, including some\nform of best practice cyber security standards, is required across the economy to increase\nour national cyber resilience and keep Australians and their data safe.\n\nTo be the most cyber secure nation in the world by 2030, Australians should have\nconfidence that digital products and services sold are fit for purpose and include\nappropriate best practice cyber security protections.\n\nThere may also be opportunities to simplify and streamline existing regulatory\nframeworks. For example, stakeholders have encouraged government to streamline\nreporting obligations and response requirements following a major cyber incident.\n\nIt is clear that a package of regulatory reform is necessary. How this would be\nimplemented, including the potential consideration of a new Cyber Security Act, drawing\ntogether cyber-specific legislative obligations and standards across industry and\ngovernment, and the details of these reforms is something on which feedback will be\nwelcomed. This should also consider whether further developments to the SOCI Act are\nwarranted, such as including customer data and ‘systems’ in the definition of critical assets\nto ensure the powers afforded to government under the SOCI Act extend to major data\nbreaches such as those experienced by Medibank and Optus, not just operational\ndisruptions.*!\n\nUpon the release of the discussion paper, at a roundtable meeting on cybersecurity, the Minister\nfor Home Affairs said that “Australia has a patchwork of policies, laws and frameworks that are\n\n78 Cameron Abbott & Rob Pulham, Australia to be the Most Security Cyber Nation?, National Law Review (Mar. 6,\n2023), https:/ / perma.cc/ NW42-S5CV.\n\n79 Speech, Prime Minister, Cyber Security Roundtable (Feb. 27, 2023), https:/ / perma.cc/ NFH2-EPGR.\n80 Grattan, supra note 71.\n\n81 Expert Advisory Panel, supra note 74, at 17.\n\nThe Law Library of Congress 14\n", "vlm_text": "The government has stated that it has a goal of Australia becoming the most cyber secure nation  by 2030.  In February 2023, the prime minister said that “government policies and regulations,  business sector systems and measures and our general awareness and capacity as a nation are  simply not at the level that we need them to be.” 79  The government has said that its priorities for  a new cyber security policy include “increasing whole-of-nation protection efforts, ensuring  critical infrastructure and government systems are resilient, building sovereign capabilities to  tackle cyber threats, strengthening international engagement, and growing a national  cyber workforce.” 80   \nThe 2023 discussion paper refers to “enhancing and harmonising” the regulatory frameworks for  cybersecurity:  \nWe have heard from industry that business owners often do not feel their cyber security  obligations are clear or easy to follow, both from an operational perspective and as  company directors. There are a range of implicit cyber security obligations placed on  Australian businesses and nongovernment entities, including through the corporations,  consumer, critical infrastructure, and privacy legislative and regulatory frameworks.  However, it is clear from stakeholder feedback and the increasing frequency and severity  of major cyber incidents, that more explicit specification of obligations, including some  form of best practice cyber security standards, is required across the economy to increase  our national cyber resilience and keep Australians and their data safe.   \nTo be the most cyber secure nation in the world by 2030, Australians should have  confidence that digital products and services sold are fit for purpose and include  appropriate best practice cyber security protections.   \nThere may also be opportunities to simplify and streamline existing regulatory  frameworks. For example, stakeholders have encouraged government to streamline  reporting obligations and response requirements following a major cyber incident.   \nIt is clear that a package of regulatory reform is necessary. How this would be  implemented, including the potential consideration of a new  Cyber Security Act , drawing  together cyber-specific legislative obligations and standards across industry and  government, and the details of these reforms is something on which feedback will be  welcomed. This should also consider whether further developments to the SOCI Act are  warranted, such as including customer data and ‘systems’ in the definition of critical assets  to ensure the powers afforded to government under the SOCI Act extend to major data  breaches such as those experienced by Medibank and Optus, not just operational  disruptions.   \nUpon the release of the discussion paper, at a roundtable meeting on cybersecurity, the Minister  for Home Affairs said that “Australia has a patchwork of policies, laws and frameworks that are  not keeping up with the challenges presented by the digital age. Voluntary measures and poorly  executed plans will not get Australia where we need to be to thrive in the contested environment  of 2030.” 82   "}
{"page": 17, "image_path": "page_images/2023555908_17.jpg", "ocr_text": "not keeping up with the challenges presented by the digital age. Voluntary measures and poorly\nexecuted plans will not get Australia where we need to be to thrive in the contested environment\nof 2030.82\n\nB. ACSC Information Security Manual\n\nThe Australian Cyber Security Centre produces the Information Security Manual (ISM). The\npurpose of the ISM, most recently published in March 2023, is to “outline a cyber security\nframework that an organisation can apply, using their risk management framework, to protect\ntheir systems and data from cyber threats.”83 The manual is intended for chief information\nsecurity officers, chief information officers, cyber security professionals, and information\ntechnology managers. It is not mandatory to comply with the ISM as a matter of law, unless\nlegislation or a direction given under legislation compels compliance. The ACSC states that\n\n[w]hile the ISM contains examples of when legislation or laws may be relevant for an\norganisation, there is no comprehensive consideration of such issues. When designing,\noperating and decommissioning systems, an organisation is encouraged to familiarise\nthemselves with relevant legislation, such as the Archives Act 1983, Privacy Act 1988,\nSecurity of Critical Infrastructure Act 2018 and Telecommunications (Interception and Access)\nAct 1979.84\n\nThe ISM contains cyber security principles, which are grouped into four key activities: govern,\nprotect, detect, and respond. The ACSC explains that “[a]n organisation should be able to\ndemonstrate that the cyber security principles are being adhered to within their organisation.”®\n\nThe ACSC further states that\n\n[t]he risk management framework used by the ISM draws from National Institute of\nStandards and Technology (NIST) Special Publication (SCP) 800-37 Rev. 2, Risk\nManagement Framework for Information Systems and Organizations: A System Life Cycle\nApproach for Security and Privacy. Broadly, the risk management framework used by the\nISM has six steps: define the system, select controls, implement controls, assess controls,\nauthorise the system and monitor the system.*¢\n\nC. Guidance on Cyber Supply Chain Risk Management\n\nThe ACSC has published specific guidance “to assist organisations in identifying risks associated\nwith their use of suppliers, manufacturers, distributors and retailers (ie. businesses that\n\n82 Keely McDonough, Australia to Overhaul Cyber Security Laws: The Legal Implications Coming Down the Line, LSG\nOnline (Mar. 6, 2023), https:/ / perma.cc/ HSCM-K9JT.\n\n83 Information Security Manual (ISM), ACSC, https:/ / perma.cc/T5WX-BNPL.\n84 Using the Information Security Manual, ACSC, https:/ / perma.cc/E83J-XYPL.\n8 Id.\n\n86 Id.\n", "vlm_text": "\nB.  ACSC Information Security Manual  \nThe Australian Cyber Security Centre produces the  Information Security Manual  (ISM). The  purpose of the ISM, most recently published in March 2023, is to “outline a cyber security  framework that an organisation can apply, using their risk management framework, to protect  their systems and data from cyber threats.” 83  The manual is intended for chief information  security officers, chief information officers, cyber security professionals, and information  technology managers. It is not mandatory to comply with the ISM as a matter of law, unless  legislation or a direction given under legislation compels compliance. The ACSC states that   \n[w]hile the ISM contains examples of when legislation or laws may be relevant for an  organisation, there is no comprehensive consideration of such issues. When designing,  operating and decommissioning systems, an organisation is encouraged to familiarise  themselves with relevant legislation, such as the  Archives Act 1983 ,  Privacy Act 1988 ,  Security of Critical Infrastructure Act 2018  and  Telecommunications (Interception and Access)  Act 1979 .    \nThe ISM contains cyber security principles, which are grouped into four key activities: govern,  protect, detect, and respond. The ACSC explains that “[a]n organisation should be able to  demonstrate that the cyber security principles are being adhered to within their organisation  $\\prime\\prime85$    \nThe ACSC further states that   \n[t]he risk management framework used by the ISM draws from National Institute of  Standards and Technology (NIST) Special Publication (SCP) 800-37 Rev. 2,  Risk  Management Framework for Information Systems and Organizations: A System Life Cycle  Approach for Security and Privacy . Broadly, the risk management framework used by the  ISM has six steps: define the system, select controls, implement controls, assess controls,  authorise the system and monitor the system.   \nC.  Guidance on Cyber Supply Chain Risk Management \nThe ACSC has published specific guidance “to assist organisations in identifying risks associated  with their use of suppliers, manufacturers, distributors and retailers (i.e. businesses that  constitute their cyber supply chain).” 87  This includes risks due to foreign control or interference,  poor security practices, lack of transparency, access and privileges, and poor business practices.   "}
{"page": 18, "image_path": "page_images/2023555908_18.jpg", "ocr_text": "constitute their cyber supply chain).”8” This includes risks due to foreign control or interference,\npoor security practices, lack of transparency, access and privileges, and poor business practices.\n\nIn addition, the ACSC has published separate guidelines on procurement and outsourcing,\nstating that\n\n[c]yber supply chain risk management activities should be conducted during the\nearliest possible stage of procurement of applications, ICT equipment and services.\nIn particular, an organisation should consider the security risks that may arise as\nsystems, software and hardware are being designed, built, stored, delivered,\ninstalled, operated, maintained and decommissioned. This includes identifying\nand managing jurisdictional, governance, privacy and security risks associated\nwith the use of suppliers, such as application developers, ICT equipment\nmanufacturers, service providers and other organisations involved in distribution\nchannels.88\n\nThe guidance cross-references particular parts of the ISM related to supply chain risk\nmanagement.\n\nD. Federal Privacy Act\n1. Protection of Personal Information\n\nThe Privacy Act “regulates the way individuals’ personal information is handled.”* The\nresponsibilities in the act apply to federal government agencies and to organizations with an\nannual turnover of more than AU$3 million (about US$1.98 million), as well as some small\nbusiness operators, such as private sector health service providers and businesses that sell or\npurchase personal information. Some particular acts and practices of other small business\noperators are also covered.”\n\nThe Australian Privacy Principles (APP) are the “cornerstone” of the privacy protection\nframework in the act. Entities covered by the act are referred to as “APP entities.”*! There are 13\nAPPs that govern standards, rights, and obligations in relation to\n\n87 Identifying Cyber Supply Chain Risks, ACSC (Jan. 7, 2021; last updated May 22, 2023),\nhttps:/ / perma.cc/ LWX4-Y7Y8.\n\n88 Guidelines for Procurement and Outsourcing, ACSC (Mar. 2, 2023), https:/ / perma.cc/P2GJ-KL85.\n\n89 Rights and Responsibilities, Office of the Australian Information Commissioner (OAIC),\nhttps:/ / perma.cc/5GXF-GCK6.\n\nTd.\n\n°! Read the Australian Privacy Principles, OAIC, https://perma.cc/6L9X-8BSN. See also Privacy Act 1988 (Cth) s\n6 (definitions of agency, organisation, APP entity).\n", "vlm_text": "\nIn addition, the ACSC has published separate guidelines on procurement and outsourcing,  stating that   \n[c]yber supply chain risk management activities should be conducted during the  earliest possible stage of procurement of applications, ICT equipment and services.  In particular, an organisation should consider the security risks that may arise as  systems, software and hardware are being designed, built, stored, delivered,  installed, operated, maintained and decommissioned. This includes identifying  and managing jurisdictional, governance, privacy and security risks associated  with the use of suppliers, such as application developers, ICT equipment  manufacturers, service providers and other organisations involved in distribution  channels.   \nThe guidance cross-references particular parts of the ISM related to supply chain risk  management.  \nD.  Federal Privacy Act   \n1.  Protection of Personal Information  \nThe Privacy Act “regulates the way individuals’ personal information is handled.” 89  The  responsibilities in the act apply to federal government agencies and to organizations with an  annual turnover of more than   $\\mathrm{AU}\\mathbb{S}3$   million (about   $\\mathrm{U}S\\S1.98$   million), as well as some small  business operators, such as private sector health service providers and businesses that sell or  purchase personal information. Some particular acts and practices of other small business  operators are also covered.   \nThe Australian Privacy Principles (APP) are the “cornerstone” of the privacy protection  framework in the act. Entities covered by the act are referred to as “APP entities.” 91  There are 13  APPs that govern standards, rights, and obligations in relation to   "}
{"page": 19, "image_path": "page_images/2023555908_19.jpg", "ocr_text": "e the collection, use and disclosure of personal information\n\n¢ anorganisation or agency’s governance and accountability\n\n¢ integrity and correction of personal information\n\ne the rights of individuals to access their personal information.%\n\nThe APPs are intended to be technology neutral. A breach of an APP can lead to regulatory action\nand penalties.%\n\nBroadly, APP entities must have a privacy policy; must only collect personal information that is\nreasonably necessary for, or directly related to, one of the entity’s functions or activities; must not\ncollect sensitive information without an individual’s consent; must collect personal information\nonly by lawful and fair means; must notify an individual of the collection of their personal\ninformation; must not use or disclose the information for a secondary purpose without the\nindividual’s consent; must take reasonable steps to ensure that the personal information held is\nup to date and complete; and must take reasonable steps to protect the information from misuse,\ninterference, and loss, and from unauthorized access, modification, or disclosure. Further\nprinciples relate to individuals being able to access and correct information held about them.\n\nThe OAIC provides detailed guidelines on each of the APPs, including the mandatory\nrequirements and how it will interpret the APPs. For example, for the APP on the security of\npersonal information, the guidelines state that “reasonable steps” include, where relevant, taking\nsteps and implementing strategies in relation to the following:\n\n¢ governance, culture and training\n\n¢ internal practices, procedures and systems\n\ne ICT security\n\n* access security\n\ne third party providers (including cloud computing)\ne data breaches\n\ne physical security\n\ne destruction and de-identification\n\ne standards.\n\nThe OAIC has published guidance related to the use of data analytics and the APPs, which\nrecommends that, for example, entities use de-identified data wherever possible; embed good\nprivacy governance by taking a privacy-by-design approach; conduct privacy assessments for\ndata analytics projects; be open and transparent about privacy practices; know what is being\ncollected; and protect information in line with risk assessments.\n\n% Australian Privacy Principles, OAIC, https:/ / perma.cc/ AUR6-CKDK. See also Privacy Act 1988 (Cth) pt 3 div\n2 &sch1.\n\n% Australian Privacy Principles, supra note 92.\n% Australian Privacy Principles Quick Reference, OAIC, https:/ / perma.cc/ EQR3-P387.\n\n% OAIC, Australian Privacy Principles Guidelines: Privacy Act 1988, para. 11.8 (Dec. 2022),\nhttps:/ / perma.cc/67VV-D9AE.\n\n%6 Guide to Data Analytics and the Australian Privacy Principles, OAIC (Mar. 21, 2018), https: / / perma.cc/ZCB6-\nH33A.\n", "vlm_text": "•   the collection, use and disclosure of personal information  •   an organisation or agency’s governance and accountability  •   integrity and correction of personal information  •   the rights of individuals to access their personal information.   \nThe APPs are intended to be technology neutral. A breach of an APP can lead to regulatory action  and penalties.    \nBroadly, APP entities must have a privacy policy; must only collect personal information that is  reasonably necessary for, or directly related to, one of the entity’s functions or activities; must not  collect sensitive information without an individual’s consent; must collect personal information  only by lawful and fair means; must notify an individual of the collection of their personal  information; must not use or disclose the information for a secondary purpose without the  individual’s consent; must take reasonable steps to ensure that the personal information held is  up to date and complete; and must take reasonable steps to protect the information from misuse,  interference, and loss, and from unauthorized access, modification, or disclosure. Further  principles relate to individuals being able to access and correct information held about them.   \nThe OAIC provides detailed guidelines on each of the APPs, including the mandatory  requirements and how it will interpret the APPs. For example, for the APP on the security of  personal information, the guidelines state that “reasonable steps” include, where relevant, taking  steps and implementing strategies in relation to the following:  \n•   governance, culture and training  •   internal practices, procedures and systems  •   ICT security  •   access security  •   third party providers (including cloud computing)  •   data breaches  •   physical security  •   destruction and de-identification  •   standards.   \nThe OAIC has published guidance related to the use of data analytics and the APPs, which  recommends that, for example, entities use de-identified data wherever possible; embed good  privacy governance by taking a privacy-by-design approach; conduct privacy assessments for  data analytics projects; be open and transparent about privacy practices; know what is being  collected; and protect information in line with risk assessments.   "}
{"page": 20, "image_path": "page_images/2023555908_20.jpg", "ocr_text": "2. Notification of Data Breaches\n\nThe Notifiable Data Breaches Scheme was established by amendments made to the Privacy Act\nin 2017. Under this scheme, entities covered by the Privacy Act 1988 (Cth) are required to notify\nthe OAIC and affected individuals of any “eligible data breach” as soon as practicable.” A\nnotifiable breach is one that is likely to result in serious harm to an individual whose personal\ninformation is involved.\n\nThe OAIC handles complaints, conducts investigations, and takes other regulatory action in\nrelation to data breaches.°” Amendments to the act made in 2022 gave the office new powers to\ninvestigate and gather information related to privacy breaches and increased fines for companies\nthat experience “serious” or “repeated” breaches.1%\n\n3. Review of the Privacy Act\n\nAutomated decision-making (ADM) was considered as part of the government's recently\ncompleted review of the Privacy Act 1988 (Cth).1°! In February 2023, the Privacy Act Review Report\nwas released,! containing 116 proposals to reform the Privacy Act to “adequately protect\nAustralians’ privacy in the digital age.”1°3 The government is now considering its response to\nthe report.104\n\nThe report proposed new transparency requirements “for automated decisions that use personal\ninformation and have a significant effect on individuals. . . . Entities would need to provide\ninformation about types of personal information used in automated decision-making systems\nand how such decisions are made.”! The report specifically contained the following proposals:\n\nProposal 19.1 Privacy policies should set out the types of personal information that will be\nused in substantially automated decisions which have a legal or similarly significant effect\non an individual's rights.\n\n9%” See The Legal 500: Data Protection & Cyber Security Comparative Guide Australia, Gilbert & Tobin (June 8, 2023),\nhttps:/ / perma.cc/ EL8N-X4FG. See also Privacy Act 1988 (Cth) pt 3C.\n\n%8 About the Notifiable Data Breaches Scheme, OAIC, https://perma.cc/J9XV-24HD.\n% Id. See also Privacy Act 1988 (Cth) pt 4.\n\n100 See Victoria Savage, Everything You Need to Know About Australia’s New Cybersecurity Law, LoginTC (Dec. 6,\n2022), https:/ / perma.cc/4BEE-K7VR; Australian Government Serious About Data Privacy: Substantial Increases in\nFines and Enhanced Regulatory Powers, Jones Day (Jan. 2023), https:/ / perma.cc/CG33-CJPL. See also Privacy Act\n1998 (Cth) s 13G.\n\n101 See Review of the Privacy Act 1988, Attorney-General’s Department, https:/ / perma.cc/X9KD-BP8Z.\n102 Attorney-General’s Department, Privacy Act Review: Report 2022 (2022), https:/ /perma.cc/5XXE-8PPF.\n\n103 Privacy Act Review Report: Highlights and Hot Takes, Gilbert & Tobin (Feb. 16, 2023), https:/ / perma.cc/ LA7M-\n8N2N.\n\n104 Review of the Privacy Act 1988, supra note 101.\n105 Privacy Act Review: Report 2022, supra note 102, at 3.\n", "vlm_text": "2.  Notification of Data Breaches  \nThe Notifiable Data Breaches Scheme was established by amendments made to the Privacy Act  in 2017. Under this scheme, entities covered by the Privacy Act 1988 (Cth) are required to notify  the OAIC and affected individuals of any “eligible data breach” as soon as practicable.  A  notifiable breach is one that is likely to result in serious harm to an individual whose personal  information is involved.    \nThe OAIC handles complaints, conducts investigations, and takes other regulatory action in  relation to data breaches.  Amendments to the act made in 2022 gave the office new powers to  investigate and gather information related to privacy breaches and increased fines for companies  that experience “serious” or “repeated” breaches.   \n3.  Review of the Privacy Act  \nAutomated decision-making (ADM) was considered as part of the government’s recently  completed review of the Privacy Act 1988 (Cth).  In February 2023, the  Privacy Act Review Report   was released,  containing 116 proposals to reform the Privacy Act to “adequately protect  Australians’ privacy in the digital age.” 103  The government is now considering its response to  the report.    \nThe report proposed new transparency requirements “for automated decisions that use personal  information and have a significant effect on individuals. . . . Entities would need to provide  information about types of personal information used in automated decision-making systems  and how such decisions are made.” 105  The report specifically contained the following proposals:  \nProposal 19.1 Privacy policies should set out the types of personal information that will be  used in substantially automated decisions which have a legal or similarly significant effect  on an individual’s rights.   "}
{"page": 21, "image_path": "page_images/2023555908_21.jpg", "ocr_text": "Proposal 19.2 High-level indicators of the types of decisions with a legal or similarly\nsignificant effect on an individual’s rights should be included in the Act. This should be\nsupplemented by OAIC Guidance.\n\nProposal 19.3 Introduce a right for individuals to request meaningful information about\nhow substantially automated decisions with legal or similarly significant effect are made.\nEntities will be required to include information in privacy policies about the use of\npersonal information to make substantially automated decisions with legal or similarly\nsignificant effect.\n\nThis proposal should be implemented as part of the broader work to regulate Al and ADM,\nincluding the consultation being undertaken by the Department of Industry, Science and\nResources, 10\n\nThe report further stated that\n\n[g]uidance should be provided to entities to clarify the meaning of ‘substantially\nautomated’, which should not capture decisions where a human decision-maker has\ngenuine oversight of a decision, reviews a decision before it is applied and has discretion\nto alter the decision. Consultation will be required to ensure the parameters of\n‘substantially automated’ are appropriately calibrated.1”\n\nIt also explained how other current and proposed requirements in the Privacy Act “would also\noperate to safeguard the integrity of automated decisions through obligations relating to personal\ninformation used in ADM systems.”108 For example, “the obligation in APP 10 to take reasonable\nsteps to ensure the accuracy and quality of personal information held by entities and the\napplication of the fair and reasonable test may operate to require entities to monitor their ADM\nsystems for bias where the decisions being made would significantly impact individuals.”10\n\nE. AI Ethics Principles\n\nThe AI Ethics Principles are voluntary and intended to “be aspirational and complement - not\nsubstitute - existing AI regulations and practices.”!0 In summary, the principles cover the\nfollowing matters:\n\n¢ Human, societal and environmental wellbeing: AI systems should benefit\nindividuals, society and the environment.\n\n¢ Human-centred values: AI systems should respect human rights, diversity, and the\nautonomy of individuals.\n\ne Fairness: AI systems should be inclusive and accessible, and should not involve or\nresult in unfair discrimination against individuals, communities or groups.\n\n106 Td. at 12, 191-193.\n107 Td. at 191.\n\n108 Td. at 193.\n\n109 Tq,\n\n110 Australia’s AI Ethics Principles, Department of Industry, Science and Resources, https:/ / perma.cc/S23U-\nS2Z2.\n", "vlm_text": "Proposal 19.2 High-level indicators of the types of decisions with a legal or similarly  significant effect on an individual’s rights should be included in the Act. This should be  supplemented by OAIC Guidance.   \nProposal 19.3 Introduce a right for individuals to request meaningful information about  how substantially automated decisions with legal or similarly significant effect are made.  Entities will be required to include information in privacy policies about the use of  personal information to make substantially automated decisions with legal or similarly  significant effect.   \nThis proposal should be implemented as part of the broader work to regulate AI and ADM,  including the consultation being undertaken by the Department of Industry, Science and  Resources.   \nThe report further stated that   \n[g]uidance should be provided to entities to clarify the meaning of ‘substantially  automated’, which should not capture decisions where a human decision-maker has  genuine oversight of a decision, reviews a decision before it is applied and has discretion  to alter the decision. Consultation will be required to ensure the parameters of  ‘substantially automated’ are appropriately calibrated.   \nIt also explained how other current and proposed requirements in the Privacy Act “would also  operate to safeguard the integrity of automated decisions through obligations relating to personal  information used in ADM systems.” 108  For example, “the obligation in APP 10 to take reasonable  steps to ensure the accuracy and quality of personal information held by entities and the  application of the fair and reasonable test may operate to require entities to monitor their ADM  systems for bias where the decisions being made would significantly impact individuals.  ${\\prime\\prime}_{109}$    \nE.  AI Ethics Principles  \nThe AI Ethics Principles are voluntary and intended to “be aspirational and complement – not  substitute – existing AI regulations and practices.” 110  In summary, the principles cover the  following matters:  \n•   Human, societal and environmental wellbeing:  AI systems should benefit  individuals, society and the environment.  •   Human-centred values:  AI systems should respect human rights, diversity, and the  autonomy of individuals.  •   Fairness:  AI systems should be inclusive and accessible, and should not involve or  result in unfair discrimination against individuals, communities or groups.  "}
{"page": 22, "image_path": "page_images/2023555908_22.jpg", "ocr_text": "¢ Privacy protection and security: AI systems should respect and uphold privacy rights\nand data protection, and ensure the security of data.\n\n¢ Reliability and safety: AI systems should reliably operate in accordance with their\nintended purpose.\n\n¢ Transparency and explainability: There should be transparency and responsible\ndisclosure so people can understand when they are being significantly impacted by\nAI, and can find out when an AI system is engaging with them.\n\n¢ Contestability: When an AI system significantly impacts a person, community, group\nor environment, there should be a timely process to allow people to challenge the use\nor outcomes of the AI system.\n\n¢ Accountability: People responsible for the different phases of the AI system lifecycle\nshould be identifiable and accountable for the outcomes of the AI systems, and human\noversight of AI systems should be enabled.\n\nF. SOCI Act\n1. Requirements and Application\n\nThe SOCI Act, which was amended in 2021 and again in 2022,1!2 “was implemented as a response\nto technological changes that have increased cyber connectivity to critical infrastructure.”!!3 One\nof the objects of the act is “imposing enhanced cybersecurity obligations on relevant entities for\nsystems of national significance in order to improve their preparedness for, and ability to respond\nto, cybersecurity incidents.” 14\n\nThe act imposes legal obligations (“positive security obligations”) on critical infrastructure assets.\nFirst, such assets must provide owner and operator information to the Register of Critical\nInfrastructure Assets.!5 This obligation, in part 2 of the act, currently applies to a list of asset\nclasses that includes, for example, broadcasting, domain name system, data storage or processing,\nfood and grocery, hospital, public transport, and electricity.\"°\n\nSecond, specific critical infrastructure assets must report cybersecurity incidents to the ACSC\nunder part 2B of the act. This requirement covers a separate list of asset classes, including those\nlisted above, as well as, for example, banking, education, freight infrastructure and services,\nports, and water.!!7\n\n111 Id.\n\n112 Legislative Information and Reforms: Critical Infrastructure, CISC, https:/ / perma.cc/ MWD6-LJ5Q. For\ninformation on the background policy and consultation process, see Engagement on Critical Infrastructure\nReforms, Department of Homeland Security, https:/ / perma.cc/ H7GE-H66K.\n\n13 Miralis et al., supra note 19.\n14 Security of Critical Infrastructure Act 2018 (Cth) s 3(c).\n15 Td. pt 2.\n\n116 Reporting and Compliance, CISC, https:/ / perma.cc/Q5W9-6QXL. See also Security of Critical Infrastructure\n(Application) Rules (LIN 22/026) 2022 (Cth) r 4, https:// perma.cc/FN7T-N64L.\n\n117 Reporting and Compliance, supra note 116; Security of Critical Infrastructure (Application) Rules (LIN 22/026)\n2022 (Cth) r 5.\n", "vlm_text": "•   Privacy protection and security:  AI systems should respect and uphold privacy rights  and data protection, and ensure the security of data.  •   Reliability and safety:  AI systems should reliably operate in accordance with their  intended purpose.  •   Transparency and explainability:  There should be transparency and responsible  disclosure so people can understand when they are being significantly impacted by  AI, and can find out when an AI system is engaging with them.  •   Contestability:  When an AI system significantly impacts a person, community, group  or environment, there should be a timely process to allow people to challenge the use  or outcomes of the AI system.  •   Accountability:  People responsible for the different phases of the AI system lifecycle  should be identifiable and accountable for the outcomes of the AI systems, and human  oversight of AI systems should be enabled.   \nF.  SOCI Act  \n1.  Requirements and Application  \nThe SOCI Act, which was amended in 2021 and again in 2022,  “was implemented as a response  to technological changes that have increased cyber connectivity to critical infrastructure.” 113  One  of the objects of the act is “imposing enhanced cybersecurity obligations on relevant entities for  systems of national significance in order to improve their preparedness for, and ability to respond  to, cybersecurity incidents.” 114   \nThe act imposes legal obligations (“positive security obligations”) on critical infrastructure assets.  First, such assets must provide owner and operator information to the Register of Critical  Infrastructure Assets.  This obligation, in part 2 of the act, currently applies to a list of asset  classes that includes, for example, broadcasting, domain name system, data storage or processing,  food and grocery, hospital, public transport, and electricity.   \nSecond, specific critical infrastructure assets must report cybersecurity incidents to the ACSC  under part 2B of the act. This requirement covers a separate list of asset classes, including those  listed above, as well as, for example, banking, education, freight infrastructure and services,  ports, and water.   "}
{"page": 23, "image_path": "page_images/2023555908_23.jpg", "ocr_text": "In addition, a much smaller subset of critical infrastructure assets may be privately designated as\nSystems of National Significance.!!8 Enhanced cyber security obligations may apply to such assets\nunder Part 2C of the act, requiring them to\n\n1. develop, update and comply with a cyber security incident response plan\n2. undertake cyber security exercises to build cyber preparedness\n\n3. undertake vulnerability assessments; and\n\n4. provide system information.'!°\n\nThe SOCI Act also enables the government to “assist in the defence of critical infrastructure assets\nfrom cyber security threats, in light of their criticality to the social or economic stability of\nAustralia or its people, the defence of Australia, or national security.” 12°\n\nIn February 2023, CISC published the 2023 Critical Infrastructure Resilience Strategy!?! and the\nCritical Infrastructure Resilience Plan 2023.122\n\nAs noted above, the discussion paper on a new cybersecurity strategy suggests that customer\ndata and “systems” could be included as critical assets under the SOCI Act to empower the\ngovernment to give directions and gather information in response to significant data breaches.1%\n\n2. Risk Management Program\n\nIn February 2023, the Minister for Home Affairs and Cybersecurity approved a new risk\nmanagement program for Australian organizations deemed to be running infrastructure critical\nto the national interest.!24 The program “is the final of three preventative elements of the Security\nof Critical Infrastructure Act 2018 as amended in 2021 and 2022.”!25 The requirements related to\nrisk management programs are contained in Part 2A of the act.\n\nThe Cyber and Infrastructure Security Centre explains that\n\n[t]he Critical Infrastructure Risk Management Program (CIRMP) is intended to uplift core\nsecurity practices that relate to the management of certain critical infrastructure assets. It\naims to ensure responsible entities take a holistic and proactive approach toward\nidentifying, preventing and mitigating risks.\n\nM8 CISC, The Enhanced Cyber Security Obligations Framework (May 2022), https:/ / perma.cc/5NN8-D8HV.\n119 CISC, Protection of Australia’s Critical Infrastructure Summary (Feb. 2023), https:/ / perma.cc/ N)WK-7LT7.\n120 Td,\n\n121 CISC, Critical Infrastructure Resilience Strategy (Feb. 2023), https:/ / perma.cc/ CAF7-NF98.\n\n122 CISC, Critical Infrastructure Resilience Plan (Feb. 2023), https:/ / perma.cc/ KQK9-J9MP.\n\n123 Expert Advisory Panel, supra note 74, at 17.\n\n124 Press Release, Clare O'Neil, World Leading Protection for Australia’s Critical Infrastructure (Feb. 21, 2023),\nhttps:/ / perma.cc/ W4FV-R5JQ.\n\n125 Legislative Information and Reforms: Regulatory Obligations, CISC, https: / / perma.cc/TD5U-BW25.\n", "vlm_text": "In addition, a much smaller subset of critical infrastructure assets may be privately designated as  Systems of National Significance.  Enhanced cyber security obligations may apply to such assets  under Part 2C of the act, requiring them to  \n1. develop, update and comply with a cyber security incident response plan   2. undertake cyber security exercises to build cyber preparedness   3. undertake vulnerability assessments; and   4. provide system information.   \nThe SOCI Act also enables the government to “assist in the defence of critical infrastructure assets  from cyber security threats, in light of their criticality to the social or economic stability of  Australia or its people, the defence of Australia, or national security.  ${\\prime\\prime}_{120}$    \nIn February 2023, CISC published the 2023 Critical Infrastructure Resilience Strategy 121  and the  Critical Infrastructure Resilience Plan 2023.    \nAs noted above, the discussion paper on a new cybersecurity strategy suggests that customer  data and “systems” could be included as critical assets under the SOCI Act to empower the  government to give directions and gather information in response to significant data breaches. \n2.  Risk Management Program  \nIn February 2023, the Minister for Home Affairs and Cybersecurity approved a new risk  management program for Australian organizations deemed to be running infrastructure critical  to the national interest.  The program “is the final of three preventative elements of the Security  of Critical Infrastructure Act 2018 as amended in 2021 and 2022.” 125  The requirements related to  risk management programs are contained in Part 2A of the act.   \nThe Cyber and Infrastructure Security Centre explains that   \n[t]he Critical Infrastructure Risk Management Program (CIRMP) is intended to uplift core  security practices that relate to the management of certain critical infrastructure assets. It  aims to ensure responsible entities take a holistic and proactive approach toward  identifying, preventing and mitigating risks.  "}
{"page": 24, "image_path": "page_images/2023555908_24.jpg", "ocr_text": "Safety and Security of Artificial Intelligence Systems: Australia\n\nResponsible entities of the asset classes in section 4 of the Security of Critical Infrastructure\n(Critical infrastructure risk management program) Rules (LIN 23/006) 2023 (the Rules) are\nrequired to establish, maintain, and comply with a written risk management program that\nmanages the ‘material risk’ of a ‘hazard’ occurring, which could have a relevant impact on\ntheir critical infrastructure asset.\n\nResponsible entities must identify, and as far as is reasonably practicable, take steps to\nminimise or eliminate these ‘material risks’ that could have a ‘relevant impact’ on\ntheir asset.!26\n\nThe Rules specify that the following asset classes are covered by part 2A of the act, and are\ntherefore subject to the CIRMP requirements:\n\n(a) acritical broadcasting asset;\n\n(b) acritical domain name system;\n\n(c) acritical data storage or processing asset;\n(d) acritical electricity asset;\n\n(e) acritical energy market operator asset;\n(f) acritical gas asset;\n\n(g) a designated hospital;\n\n(h) acritical food and grocery asset;\n\n(i) acritical freight infrastructure asset;\n\n(j) acritical freight services asset;\n\n(k) acritical liquid fuel asset;\n\n(1) acritical financial market infrastructure asset mentioned in paragraph 12D(1)(i) of\nthe Act;\n\n(m) a critical water asset.!27\n\nG. Telecommunications Legislation\n\nThe Telecommunications Act 1997 (Cth) regulates carriers and carriage service providers in their\nuse and disclosure of personal information,!8 and the Telecommunications (Interception and\nAccess Act) 1979 (Cth) requires providers of telecommunications services to collect and retain\ncertain types of data for minimum period of two years.!29 Providers must comply with the Privacy\nAct in relation to that data.190\n\nUnder security reforms enacted in 2017, “[a]ll carriers, carriage service providers and carriage\nservice intermediaries are required to do their best to protect networks and facilities from\nunauthorised access and interference.”!5! This includes “maintaining ‘competent supervision’\n\n126 CISC, Critical Infrastructure Risk Management Program (Feb. 2023), https:/ /perma.cc/ K9TY-E4ZK.\n\n227 Security of Critical Infrastructure (Critical infrastructure risk management program) Rules (LIN 23/006)\n2023 r 4 (Cth), https:/ / perma.cc/ BU5H-8GTT.\n\n228 Telecommunications Act 1997 (Cth) pt 13.\n29 Telecommunications (Interception and Access) Act 1979 (Cth) pt 5-1A.\n130 Td. s 187LA.\n\n131 Legislative Information and Reforms: Telecommunications Sector Security (TSS), CISC, https:/ / perma.cc/2SZP-\nPEDQ. See Telecommunications Act 1997 (Cth) pt 14.\n\nThe Law Library of Congress 22\n", "vlm_text": "Responsible entities of the asset classes in section 4 of the  Security of Critical Infrastructure  (Critical infrastructure risk management program) Rules  (LIN 23/006) 2023 (the Rules) are  required to establish, maintain, and comply with a written risk management program that  manages the ‘material risk’ of a ‘hazard’ occurring, which could have a relevant impact on  their critical infrastructure asset.  \nResponsible entities must identify, and as far as is reasonably practicable, take steps to  minimise or eliminate these ‘material risks’ that could have a ‘relevant impact’ on  their asset.   \nThe Rules specify that the following asset classes are covered by part 2A of the act, and are  therefore subject to the CIRMP requirements:  \n(a)   a critical broadcasting asset;  (b)   a critical domain name system;  (c)   a critical data storage or processing asset;  (d)   a critical electricity asset;  (e)   a critical energy market operator asset;  (f)    a critical gas asset;  (g)   a designated hospital;  (h)   a critical food and grocery asset;  (i)    a critical freight infrastructure asset;  (j)    a critical freight services asset;  (k)   a critical liquid fuel asset;  (l)    a critical financial market infrastructure asset mentioned in paragraph 12D(1)(i) of  the Act;   $\\mathbf{(m)}$    a critical water asset.   \nG.  Telecommunications Legislation  \nThe Telecommunications Act 1997 (Cth) regulates carriers and carriage service providers in their  use and disclosure of personal information,  and the Telecommunications (Interception and  Access Act) 1979 (Cth) requires providers of telecommunications services to collect and retain  certain types of data for minimum period of two years.  Providers must comply with the Privacy  Act in relation to that data.   \nUnder security reforms enacted in 2017, “[a]ll carriers, carriage service providers and carriage  service intermediaries are required to do their best to protect networks and facilities from  unauthorised access and interference.” 131  This includes “maintaining ‘competent supervision’  and ‘effective control’ over telecommunications networks and facilities owned or operated by  them.” 132  Furthermore, \n\n "}
{"page": 25, "image_path": "page_images/2023555908_25.jpg", "ocr_text": "and ‘effective control’ over telecommunications networks and facilities owned or operated by\nthem.” 132 Furthermore,\n\ne “Carriers and nominated carriage service providers are required to notify government of\nplanned changes to their networks and services that could compromise their ability to comply\nwith the security obligation.” 155\n\ne “The Secretary of the Department of Home Affairs has the power to obtain information and\ndocuments from carriers, carriage service providers and carriage service intermediaries, to\nmonitor and investigate their compliance with the security obligation.” 154\n\ne “The Minister for Home Affairs has the power to direct a carrier, carriage service provider or\ncarriage service intermediary to do, or not do, a specified thing that is reasonably necessary\nto protect networks and facilities from national security risks.” 195\n\nV. International Collaboration on Cybersecurity Guidance\n\nThe ACSC has worked with the government agencies with responsibilities for national\ncybersecurity in other countries to develop guidance related to particular cybersecurity issues.\n\nIn April 2023, the relevant authorities in the United States, Australia, Canada, United Kingdom,\nGermany, Netherlands, and New Zealand jointly published Shifting the Balance of Cybersecurity\nRisk: Principles and Approaches.1%° This “ first-of-its-kind joint guidance urges manufacturers to take\nurgent steps necessary to ship products that are secure-by-design and -default.”13”7 The U.S.\nCybersecurity and Infrastructure Security Agency (CISA) explains that\n\n[iJn addition to specific technical recommendations, this guidance outlines several core\nprinciples to guide software manufacturers in building software security into their design\nprocesses prior to developing, configuring, and shipping their products.\n\nMany private sector partners have made invaluable contributions toward advancing\nsecurity-by-design and security-by-default. With this joint guide, the authoring agencies\nseek to progress an international conversation about key priorities, investments, and\ndecisions necessary to achieve a future where technology is safe, secure, and resilient by\ndesign and default.158\n\n182 Legislative Information and Reforms: Telecommunications Sector Security (TSS), supra note 131.\n133 Td,\n134 Td.\n135 Td,\n\n136 U.S. Cybersecurity & Infrastructure Security Agency (CISA) et al., Shifting the Balance of Cybersecurity Risk:\nPrinciples and Approaches for Security-byDesign and -Default (Apr. 13, 2023), https: / / perma.cc/ LRC2-UWU3.\n\n137 Security-by-Design and -Default, CISA (last updated June 12, 2023), https:/ / perma.cc/7XG3-H535.\n138 Td,\n", "vlm_text": "\n•   “Carriers and nominated carriage service providers are required to notify government of  planned changes to their networks and services that could compromise their ability to comply  with the security obligation.  $^{\\prime\\prime}{}_{133}$   \n\n •   “The Secretary of the Department of Home Affairs has the power to obtain information and  documents from carriers, carriage service providers and carriage service intermediaries, to  monitor and investigate their compliance with the security obligation.  ${\\prime\\prime}_{134}$   \n\n •   “The Minister for Home Affairs has the power to direct a carrier, carriage service provider or  carriage service intermediary to do, or not do, a specified thing that is reasonably necessary  to protect networks and facilities from national security risks.” 135   \nV.  International Collaboration on Cybersecurity Guidance \nThe ACSC has worked with the government agencies with responsibilities for national  cybersecurity in other countries to develop guidance related to particular cybersecurity issues.  \nIn April 2023, the relevant authorities in the United States, Australia, Canada, United Kingdom,  Germany, Netherlands, and New Zealand jointly published  Shifting the Balance of Cybersecurity  Risk: Principles and Approaches .  This “first-of-its-kind joint guidance urges manufacturers to take  urgent steps necessary to ship products that are secure-by-design and -default.” 137  The U.S.  Cybersecurity and Infrastructure Security Agency (CISA) explains that   \n[i]n addition to specific technical recommendations, this guidance outlines several core  principles to guide software manufacturers in building software security into their design  processes prior to developing, configuring, and shipping their products.  \nMany private sector partners have made invaluable contributions toward advancing  security-by-design and security-by-default. With this joint guide, the authoring agencies  seek to progress an international conversation about key priorities, investments, and  decisions necessary to achieve a future where technology is safe, secure, and resilient by  design and default.   "}
{"page": 26, "image_path": "page_images/2023555908_26.jpg", "ocr_text": "In addition, also in April 2023, the cybersecurity authorities of the United States, United\nKingdom, Australia, Canada, and New Zealand published Cybersecurity Best Practices for Smart\nCities.139 The guidance\n\nprovides an overview of risks to smart cities including expanded and interconnected attack\nsurfaces; information and communications technologies (ICT) supply chain risks; and\nincreasing automation of infrastructure operations. To protect against these risks, the\ngovernment partners offer three recommendations to help communities strengthen their\ncyber posture: secure planning and design, proactive supply chain risk management, and\noperational resilience.1°\n\nVI. Contribution to Development of International AI Standards\n\nStandards Australia’s report, An Artificial Intelligence Standards Roadmap: Making Australia’s Voice\nHeard, states that Australia is participating in the development of Al-related standards by the\nArtificial Intelligence Joint Technical Committee of the International Organization for\nStandardization (ISO) and the International Electrotechnical Commission (IEC) (ISO/IEC JTC\n1/SC 42), the Institute of Electrical and Electronic Engineers (IEEE), and the IEC (including\nthrough OCEANIS, the Open Community for Ethics in Autonomous and Intelligent Systems).'*1\n\nIn 2019, “Standards Australia signed an agreement to enable the adoption of IEEE Standards\nthrough the Standards Australia process, where no suitable ISO or IEC standard is available. This\nenables the adoption of IEEE Standards with the Australian Standards designation under some\ncircumstances.” 142\n\n189 CISA et al., Cybersecurity Best Practices for Smart Cities (Apr. 19, 2023), https:/ / perma.cc/TD99-AG2V.\n40 Cybersecurity Best Practices for Smart Cities, CISA (Apr. 19, 2023), https:/ / perma.cc/ M48K-2AW2.\n\n441 An Artificial Intelligence Standards Roadmap: Making Australia’s Voice Heard, supra note 58, at 23.\n\n142 Td. at 19.\n", "vlm_text": "In addition, also in April 2023, the cybersecurity authorities of the United States, United  Kingdom, Australia, Canada, and New Zealand published  Cybersecurity Best Practices for Smart  Cities .  The guidance  \nprovides an overview of risks to smart cities including expanded and interconnected attack  surfaces; information and communications technologies (ICT) supply chain risks; and  increasing automation of infrastructure operations. To protect against these risks, the  government partners offer three recommendations to help communities strengthen their  cyber posture: secure planning and design, proactive supply chain risk management, and  operational resilience.      \nVI.  Contribution to Development of International AI Standards  \nStandards Australia’s report,  An Artificial Intelligence Standards Roadmap: Making Australia’s Voice  Heard , states that Australia is participating in the development of AI-related standards by the  Artificial Intelligence Joint Technical Committee of the International Organization for  Standardization (ISO) and the   International Electrotechnical Commission (IEC) (ISO/IEC JTC  1/SC 42), the Institute of Electrical and Electronic Engineers (IEEE), and the IEC (including  through OCEANIS, the Open Community for Ethics in Autonomous and Intelligent Systems). \nIn 2019, “Standards Australia signed an agreement to enable the adoption of IEEE Standards  through the Standards Australia process, where no suitable ISO or IEC standard is available. This  enables the adoption of IEEE Standards with the Australian Standards designation under some  circumstances.” 142   "}
{"page": 27, "image_path": "page_images/2023555908_27.jpg", "ocr_text": "Canada\n\nMichael Chalupovitsch, Tariq Ahmad\nForeign Law Specialists\n\nSUMMARY Canada does not currently have a stand-alone law that governs artificial intelligence\n(AI). On June 16, 2022, the Minister of Innovation, Science and Economic Development\nintroduced Bill C-27, which would, among other things, enact the Artificial Intelligence\nand Data Act (AIDA). The proposed legislation requires certain obligations on the\ndesign, development, and use of AI systems and their associated harms, including\nassessing whether a system is high-impact, record keeping, and publishing certain\ninformation on AI systems, among other obligations.\n\nCybersecurity falls under the mandate of the Communications Security Establishment\n(CSE), which is Canada’s signals intelligence agency.! CSE operates the Canadian\nCentre for Cyber Security, which issues security guidance to government, industry,\nresearchers, and the general public. Its AI guidance document sets out common\nmethods of compromising AI systems such as data poisoning, adversarial attacks, and\nmodel inversions.\n\nI. Introduction\n\nCurrently, Canada does not have stand-alone legislation or regulations governing the use of\nartificial intelligence (AI). The 2017 federal budget included $125 million Canadian dollars\n(approximately US$94.11 million) in funding for the establishment of a Pan-Canadian AI Strategy\nunder the auspices of the Canadian Institute for Advanced Studies (CIFAR).2 An assessment of\nthe strategy was conducted by CIFAR in 2020.3\n\nBudget 2021 provided CAD443.8 million (approximately US$334.11 million) over 10 years to\nsupport the commercialization of AI, attract and retain academic talent, enhance research and\ndevelop research centers, and adopt AI standards.4 Canada is a member of the Global Partnership\non AI (GPAI), which was established in 2020.5\n\n1 Cyber security, Communications Security Establishment (CSE), https:/ / perma.cc/L26R-8NTH.\n\n? Department of Finance Canada, Building a Strong Middle Class: Budget 2017 (Mar. 22, 2017),\nhttps:/ / perma.cc/7SB7-8QTA; Pan-Canadian Artificial Intelligence Strategy, Innovation, Science and Economic\nDevelopment Canada, https:/ / perma.cc/P57Q-KE4X.\n\n3 CIFAR, Pan-Canadian Artificial Intelligence Strategy Assessment Report (Oct. 2020), https:/ / perma.cc/5L8L-\nKEAS.\n\n+ Department of Finance Canada, Budget 2021: A Recovery Plan for Jobs, Growth, and Resilience (Apr. 19, 2021),\nhttps:/ / perma.cc/ NSD2-V5DY.\n\n5 About GPAI, GPAI, https:/ / perma.cc/ H435-MCRY. Canada also hosts the International Centre of Expertise in\nMontréal on Artificial Intelligence (CEIMIA). About the International Centre of Expertise in Montréal on Artificial\nIntelligence (CEIMIA), CEIMIA, https:/ / perma.cc/ Q5NE-7X58.\n", "vlm_text": "Canada  \nMichael Chalupovitsch, Tariq Ahmad  Foreign Law Specialists   \n\n(AI). On June 16, 2022, the Minister of Innovation, Science and Economic Development  introduced Bill C-27, which would, among other things, enact the Artificial Intelligence  and Data Act (AIDA). The proposed legislation requires certain obligations on the  design, development, and use of AI systems and their associated harms, including  assessing whether a system is high-impact, record keeping, and publishing certain  information on AI systems, among other obligations.   \nCybersecurity falls under the mandate of the Communications Security Establishment  (CSE), which is Canada’s signals intelligence agency.  CSE operates the Canadian  Centre for Cyber Security, which issues security guidance to government, industry,  researchers, and the general public. Its AI guidance document sets out common  methods of compromising AI systems such as data poisoning, adversarial attacks, and  model inversions.   \nI.  Introduction  \nCurrently, Canada does not have stand-alone legislation or regulations governing the use of  artificial intelligence (AI). The 2017 federal budget included   $\\S125$   million Canadian dollars  (approximately  $\\mathbf{U}S\\S94.11$   million) in funding for the establishment of a Pan-Canadian AI Strategy  under the auspices of the Canadian Institute for Advanced Studies (CIFAR).  An assessment of  the strategy was conducted by CIFAR in 2020.   \nBudget 2021 provided CAD443.8 million (approximately US\\$334.11 million) over 10 years to  support the commercialization of AI, attract and retain academic talent, enhance research and  develop research centers, and adopt AI standards.  Canada is a member of the Global Partnership  on AI (GPAI), which was established in 2020.   "}
{"page": 28, "image_path": "page_images/2023555908_28.jpg", "ocr_text": "Safety and Security of Artificial Intelligence Systems: Canada\n\nII. Overview of the Legal and Policy Framework\n\nOn June 16, 2022, the Minister of Innovation, Science and Economic Development introduced Bill\nC-27, which would, among other things, enact the Artificial Intelligence and Data Act (AIDA).°\n\nAccording to the legislative summary of the bill provided by the Canadian Library of Parliament,\n“t]he AI Act regulates international and interprovincial trade and commerce in artificial\nintelligence systems by establishing requirements for designing, developing and using AI\nsystems and by prohibiting certain behaviours.”’ It is unclear when the AI Act would come into\nforce, as it has yet to be voted on or referred to a committee for study.\n\nThe Bill is also accompanied by a companion document seeking to reassure Canadians regarding\nrisks and concerns surrounding AI and the Government's plans to regulate, but also to reassure\nthe business community that it is not the intention to stifle innovation, stating,\n\n[t]his document aims to reassure Canadians in two key ways. First, the Government\nrecognizes that Canadians have concerns about the risks associated with this emerging\ntechnology and need to know that the Government has a plan to ensure that AI systems\nthat impact their lives are safe. The recently published Report of the Public Awareness\nWorking Group of the Advisory Council on AI reveals significant interest among\nCanadians in the opportunities offered by AI, but also concerns regarding potential harms.\nNearly two-thirds of respondents believed that AI has the potential to cause harm to\nsociety, while 71% believed that it could be trusted if regulated by public authorities. Thus,\nwe aim to reassure Canadians that we have a thoughtful plan to manage this emerging\ntechnology and maintain trust in a growing area of the economy. At the same time, AI\nresearchers and innovators are concerned by the uncertainty that exists regarding future\nregulation. Recognizing that the regulation of this powerful technology is now an\nemerging international norm, many in the field are worried that regulation will be\ninflexible or that it will unfairly stigmatize their field of work. Such an outcome would\nhave significant impacts on opportunities for Canadians and the Canadian economy. This\ndocument aims to reassure actors in the AI ecosystem in Canada that the aim of this Act is\nnot to entrap good faith actors or to chill innovation, but to regulate the most powerful\nuses of this technology that pose the risk of harm. Specifically, this paper is intended to\naddress both of these sets of concerns and provide assurance to Canadians that the risks\nposed by AI systems will not fall through the cracks of consumer protection and human\nrights legislation, while also making it clear that the Government intends to take an agile\napproach that will not stifle responsible innovation or needlessly single out AI developers,\nresearchers, investors or entrepreneurs. What follows is a roadmap for the AIDA,\nexplaining its intent and the Government's key considerations for operationalizing it\nthrough future regulations. It is intended to build understanding among stakeholders and\n\n6 An Act to enact the Consumer Privacy Protection Act, the Personal Information and Data Protection Tribunal\nAct and the Artificial Intelligence and Data Act and to make consequential and related amendments to other\nActs (Bill C-27), 44th Parliament, 1st Session, June 16, 2022, https:/ / perma.cc/594V-V3UN.\n\n7 Sabrina Charland et al., Legislative Summary of Bill C-27: An Act to enact the Consumer Privacy Protection Act, the\nPersonal Information and Data Protection Tribunal Act and the Artificial Intelligence and Data Act and to make\nconsequential and related amendments to other Acts, Library of Parliament Publication No. 44-1-C27-E (July 12,\n2022), https: / / perma.cc/8CYS-MMTH.\n\nThe Law Library of Congress 26\n", "vlm_text": "II.  Overview of the Legal and Policy Framework  \nOn June 16, 2022, the Minister of Innovation, Science and Economic Development introduced Bill  C-27, which would, among other things, enact the Artificial Intelligence and Data Act (AIDA).   \nAccording to the legislative summary of the bill provided by the Canadian Library of Parliament,  “[t]he AI Act regulates international and interprovincial trade and commerce in artificial  intelligence systems by establishing requirements for designing, developing and using AI  systems and by prohibiting certain behaviours.” 7   It is unclear when the AI Act would come into  force, as it has yet to be voted on or referred to a committee for study.  \nThe Bill is also accompanied by a companion document seeking to reassure Canadians regarding  risks and concerns surrounding AI and the Government’s plans to regulate, but also to reassure  the business community that it is not the intention to stifle innovation, stating,  \n[t]his document aims to reassure Canadians in two key ways. First, the Government  recognizes that Canadians have concerns about the risks associated with this emerging  technology and need to know that the Government has a plan to ensure that AI systems  that impact their lives are safe. The recently published Report of the Public Awareness  Working Group of the Advisory Council on AI reveals significant interest among  Canadians in the opportunities offered by AI, but also concerns regarding potential harms.  Nearly two-thirds of respondents believed that AI has the potential to cause harm to  society, while  $71\\%$   believed that it could be trusted if regulated by public authorities. Thus,  we aim to reassure Canadians that we have a thoughtful plan to manage this emerging  technology and maintain trust in a growing area of the economy. At the same time, AI  researchers and innovators are concerned by the uncertainty that exists regarding future  regulation. Recognizing that the regulation of this powerful technology is now an  emerging international norm, many in the field are worried that regulation will be  inflexible or that it will unfairly stigmatize their field of work. Such an outcome would  have significant impacts on opportunities for Canadians and the Canadian economy. This  document aims to reassure actors in the AI ecosystem in Canada that the aim of this Act is  not to entrap good faith actors or to chill innovation, but to regulate the most powerful  uses of this technology that pose the risk of harm. Specifically, this paper is intended to  address both of these sets of concerns and provide assurance to Canadians that the risks  posed by AI systems will not fall through the cracks of consumer protection and human  rights legislation, while also making it clear that the Government intends to take an agile  approach that will not stifle responsible innovation or needlessly single out AI developers,  researchers, investors or entrepreneurs. What follows is a roadmap for the AIDA,  explaining its intent and the Government's key considerations for operational i zing it  through future regulations. It is intended to build understanding among stakeholders and  "}
{"page": 29, "image_path": "page_images/2023555908_29.jpg", "ocr_text": "Safety and Security of Artificial Intelligence Systems: Canada\n\n¢\n\nCanadians on the proposed legislation, as well to support Parliamentary consideration of\nthe Bill.8\n\nOn June 14, 2022, the Minister of Public Safety introduced Bill C-26, An Act respecting cyber\nsecurity, amending the Telecommunications Act and making consequential amendments to other\nActs in the House of Commons.’ The bill enacts the Critical Cyber Systems Protection Act\n(CCSPA). According to the Library of Parliament, the CCSPA\n\nestablishes a cyber security compliance regime for federally regulated critical cyber\ninfrastructure. The CCSPA appears to be patterned after Australia’s Security of Critical\nInfrastructure Act 2018, which was amended under the Security Legislation Amendment\n(Critical Infrastructure) Act 2021, significantly expanding the Australian federal\ngovernment's powers to enforce cyber security obligations for critical infrastructures and\nto intervene in the private sector’s response to cyber incidents affecting critical\ninfrastructures, among other things. Also of note is the United States’ Cyber Incident\nReporting for Critical Infrastructure Act of 2022,which requires critical infrastructure\noperators to report cyber incidents to the Cybersecurity and Infrastructure Security\nAgency, and the United Kingdom’s The Network and Information Systems\nRegulations 2018 derived from the European Union's 2016 Directive on security of network\nand information systems. The overarching objective of all these regimes is to achieve an\nenhanced and common level of security for critical cyber infrastructures and to heighten\nthe situational awareness of the relevant authorities.!°\n\nAccording to Public Safety Canada,\n\n[t]he legislation addresses longstanding gaps in the Government's ability to protect the\nvital services and systems Canadians depend on by enabling it to:\n\ne designate services and systems that are vital to national security or public safety\nin Canada as well as the operators or classes of operators responsible for their\nprotection;\n\n¢ ensure that designated operators are protecting the cyber systems that underpin\nCanada’s critical infrastructure;\n\n¢ ensure that cyber incidents that meet or exceed a specific threshold are reported;\n\n¢ compel action by organizations in response to an identified cyber security threat\nor vulnerability; and\n\n¢ ensure a consistent cross-sectoral approach to cyber security in response to the\ngrowing interdependency of cyber systems.!!\n\n8 The Artificial Intelligence and Data Act (AIDA) - Companion Document, Innovation, Science and Economic\n\nDevelopment Canada, https:/ /perma.cc/73AL-YWPG.\n\n° An Act respecting cyber security, amending the Telecommunications Act and making consequential\n\namendments to other Acts (Bill C-26), 44th Parliament, 1st Session, June 14, 2022, https:/ / perma.cc/T5EK-\n\n5E5Z.\n\n10 Jed Chong et al., Legislative Summary of Bill C-26: An Act respecting cybersecurity, amending the\nTelecommunications Act and making consequential amendments to other Acts, Library of Parliament Publication No.\n44-1-C26-E (Oct. 6, 2022), https:/ / perma.cc/U6U6-ZYLZ.\n\n1 Protecting Critical Cyber Systems, Public Safety Canada, https://perma.cc/ Y67Y-X7ZZ.\n\nThe Law Library of Congress\n\n27\n", "vlm_text": "Canadians on the proposed legislation, as well to support Parliamentary consideration of  the Bill.   \nOn June 14, 2022, the Minister of Public Safety introduced Bill C-26, An Act respecting cyber  security, amending the Telecommunications Act and making consequential amendments to other  Acts in the House of Commons.  The bill enacts the Critical Cyber Systems Protection Act  (CCSPA). According to the Library of Parliament, the CCSPA  \nestablishes a cyber security compliance regime for federally regulated critical cyber  infrastructure. The CCSPA appears to be patterned after Australia’s  Security of Critical  Infrastructure Act 2018 , which was amended under the  Security Legislation Amendment  (Critical  Infrastructure)  Act 2021,  significantly  expanding  the  Australian  federal  government’s powers to enforce cyber security obligations for critical infrastructures and  to intervene in the private sector’s response to cyber incidents affecting critical  infrastructures, among other things. Also of note is the United States’  Cyber Incident  Reporting for Critical Infrastructure Act of 2022 , which requires critical infrastructure  operators to report cyber incidents to the Cybersecurity and Infrastructure Security  Agency,  and  the  United Kingdom’s  The  Network  and  Information  Systems  Regulations 2018  derived from the European Union’s 2016 Directive on security of network  and information systems. The overarching objective of all these regimes is to achieve an  enhanced and common level of security for critical cyber infrastructures and to heighten  the situational awareness of the relevant authorities.   \nAccording to Public Safety Canada,  \n[t]he legislation addresses longstanding gaps in the Government’s ability to protect the  vital services and systems Canadians depend on by enabling it to:  \n•   designate services and systems that are vital to national security or public safety  in Canada as well as the operators or classes of operators responsible for their  protection;  •   ensure that designated operators are protecting the cyber systems that underpin  Canada’s critical infrastructure;  •   ensure that cyber incidents that meet or exceed a specific threshold are reported;  •   compel action by organizations in response to an identified cyber security threat  or vulnerability; and  •   ensure a consistent cross-sectoral approach to cyber security in response to the  growing interdependency of cyber systems.   "}
{"page": 30, "image_path": "page_images/2023555908_30.jpg", "ocr_text": "III. Definition of Artificial Intelligence (AI) Systems\nBill C-27 defines an AI system as\n\na technological system that, autonomously or partly autonomously, processes data related\nto human activities through the use of a genetic algorithm, a neural network, machine\nlearning or another technique in order to generate content or make decisions,\nrecommendations or predictions.!?\n\nPublic Safety Canada’s National Cyber Security Strategy proposes a different definition of AI\nsystems, as follows:\n\n[t]he subfield of computer science concerned with developing intelligent computer\nprograms that can solve problems, learn from experience, understand language, interpret\nvisual scenes, and, in general, behave in a way that would be considered intelligent if\nobserved in a human.’\n\nIV. Cybersecurity of AI\n\nCybersecurity falls under the mandate of the Communications Security Establishment (CSE),\nwhich is Canada’s signals intelligence agency.'* CSE operates the Canadian Centre for Cyber\nSecurity, which issues security guidance to government, industry, researchers, and the general\npublic. Its AI guidance document sets out common methods of compromising Al systems such\nas data poisoning, adversarial attacks, and model inversions.15\n\nThe Treasury Board Secretariat, responsible for the administration of the Canadian federal\ngovernment, issued the Directive on Automated Decision-Making (Directive), with the objective\nof ensuring “that automated decision systems are deployed in a manner that reduces risks to\nclients, federal institutions and Canadian society, and leads to more efficient, accurate, consistent\nand interpretable decisions made pursuant to Canadian law.”!6 All automated decisions are\nsubject to an Algorithmic Impact Assessment, which covers 51 risk questions and 34 mitigation\nquestions.!7 These assessments must be reviewed by the legal service unit in the relevant\ngovernment department or agency, and must also be released on the Open Government portal\navailable to the public.'8 The Directive includes requirements related to transparency, quality\nassurance, and data governance.!9\n\n12 Bill C-27, § 2.\n\n18 National Cyber Security Strategy, Public Safety Canada (2018), https:/ / perma.cc/4BE9-6ERY.\n\n14 Cyber security, Communications Security Establishment (CSE), https:/ /perma.cc/L26R-8NTH.\n\n45 Artificial Intelligence - ITSAP.00.040, Canadian Centre for Cyber Security, https:/ / perma.cc/Z4UM-GVEW.\n\n16 Directive on Automated Decision-Making, Treasury Board Secretariat (Apr. 1, 2019), https:/ / perma.cc/XW7V-\n76ZC.\n\n1” Algorithmic Impact Assessment Tool, Government of Canada, https:/ / perma.cc/GY77-KB7P.\n8 Id.\nTd.\n", "vlm_text": "III.  Definition of Artificial Intelligence (AI) Systems  \nBill C-27 defines an AI system as  \na technological system that, autonomously or partly autonomously, processes data related  to human activities through the use of a genetic algorithm, a neural network, machine  learning or another technique in order to generate content or make decisions,  recommendations or predictions.   \nPublic Safety Canada’s National Cyber Security Strategy proposes a different definition of AI  systems, as follows:  \n[t]he subfield of computer science concerned with developing intelligent computer  programs that can solve problems, learn from experience, understand language, interpret  visual scenes, and, in general, behave in a way that would be considered intelligent if  observed in a human.    \nIV.  Cybersecurity of AI  \nCybersecurity falls under the mandate of the Communications Security Establishment (CSE),  which is Canada’s signals intelligence agency.  CSE operates the Canadian Centre for Cyber  Security, which issues security guidance to government, industry, researchers, and the general  public. Its AI guidance document sets out common methods of compromising AI systems such  as data poisoning, adversarial attacks, and model inversions.   \nThe Treasury Board Secretariat, responsible for the administration of the Canadian federal  government, issued the Directive on Automated Decision-Making (Directive), with the objective  of ensuring “that automated decision systems are deployed in a manner that reduces risks to  clients, federal institutions and Canadian society, and leads to more efficient, accurate, consistent  and interpretable decisions made pursuant to Canadian law.  ${\\prime\\prime}_{16}$   All automated decisions are  subject to an Algorithmic Impact Assessment, which covers 51 risk questions and 34 mitigation  questions.  These assessments must be reviewed by the legal service unit in the relevant  government department or agency, and must also be released on the Open Government portal  available to the public.  The Directive includes requirements related to transparency, quality  assurance, and data governance.   "}
{"page": 31, "image_path": "page_images/2023555908_31.jpg", "ocr_text": "Bill C-27 would also enact a Consumer Privacy Protection Act “to support and promote electronic\ncommerce by protecting personal information that is collected, used or disclosed in the course of\ncommercial activities” 2\n\nA. Data and Data Governance\n\nIn 2019, the Minister of Innovation, Science and Economic Development released the Canada’s\nDigital Charter.2! While not exclusively focused on AI, it included language on data governance\nand supporting the work of the Standards Council of Canada (SCC), ensuring the protection of\ndata through future amendments to the Privacy Act, and expanding the powers of the Office of\nthe Privacy Commissioner (OPC) to ensure compliance. Bill C-27, referenced above, would\nestablish a Personal Information and Data Protection Tribunal to hear appeals from compliance\ndecisions of the OPC.\n\nIn 2019, the SCC established the Canadian Data Governance Standardization Collaborative to\n“accelerate the development of industry-wide standardization strategies for data governance.”\nThe Collaborative consists of members from “government, industry, civil society, Indigenous\norganizations, academia and standards development organizations.” The Collaborative\ndeveloped the Canadian Data Governance Standardization Roadmap?3, “which describes the\ncurrent and desired Canadian standardization landscape” and consists of 35 recommendations\nto “address gaps and explore new areas where standards and conformity assessment are\nneeded.”24 Some of the recommendations involving AI systems include the following:\n\n [t]o standardize terminology and the lifecycle components to lay the groundwork for the\ninteroperability of AI solutions, and specifications for verification and validation?>; and\n\n¢ [t]o standardize the governance approaches in organizations that use or create AI\nsystems, encouraging diverse participation in the development of conformity assessment\nbased standards such as ISO/IEC 42001 Artificial Intelligence Management System\nStandard.”\n\nIn March 2023, the Collaborative was expanded into the AI and Data Governance (AIDG)\nStandardization Collaborative to “address national and international issues related to both AI\nand data governance.”2”7 The expanded Collaborative will “support the development of\n\n2\n\nBill C-27, cl. 2.\n\n21 Innovation, Science and Economic Development Canada, Canada’s Digital Charter in Action: A Plan by\nCanadians, for Canadians (2019), https:/ / perma.cc/9ET4-SA3E.\n\n22 Al and Data Governance, Standards Council of Canada (SCC), https:/ / perma.cc/6ZCN-WRWT.\n231d,\n\n24 SCC, Canadian Data Governance Standardization Roadmap (2019), https:/ / perma.cc/TJZ6-EJE8.\n35 Id. at 33.\n\n26 Td. at 34.\n\n27 Al and Data Governance, supra note 22.\n", "vlm_text": "Bill C-27 would also enact a Consumer Privacy Protection Act “to support and promote electronic  commerce by protecting personal information that is collected, used or disclosed in the course of  commercial activities” 20    \nA.   Data and Data Governance   \nIn 2019, the Minister of Innovation, Science and Economic Development released the Canada’s  Digital Charter.  While not exclusively focused on AI, it included language on data governance  and supporting the work of the Standards Council of Canada (SCC), ensuring the protection of  data through future amendments to the Privacy Act, and expanding the powers of the Office of  the Privacy Commissioner (OPC) to ensure compliance. Bill C-27, referenced above, would  establish a Personal Information and Data Protection Tribunal to hear appeals from compliance  decisions of the OPC.  \nIn 2019, the SCC established the Canadian Data Governance Standardization Collaborative to  “accelerate the development of industry-wide standardization strategies for data governance.”  The Collaborative consists of members from “government, industry, civil society, Indigenous  organizations, academia and standards development organizations.” 22  The Collaborative  developed the Canadian Data Governance Standardization Roadmap 23 , “which describes the  current and desired Canadian standardization landscape” and consists of 35 recommendations  to “address gaps and explore new areas where standards and conformity assessment are  needed.” 24  Some of the recommendations involving AI systems include the following:  \n•   [t]o standardize terminology and the lifecycle components to lay the groundwork for the  interoperability of AI solutions, and specifications for verification and validation 25 ; and  \n•   [t]o standardize the governance approaches in organizations that use or create AI  systems, encouraging diverse participation in the development of conformity assessment  based standards such as ISO/IEC 42001 Artificial Intelligence Management System  Standard.   \nIn March 2023, the Collaborative was expanded into the AI and Data Governance (AIDG)  Standardization Collaborative to “address national and international issues related to both AI  and data governance.” 27  The expanded Collaborative will “support the development of  standardization strategies that are aligned with Canada’s priorities and enable the Canadian AI  and data ecosystem to scale up on the international scene.” 28   "}
{"page": 32, "image_path": "page_images/2023555908_32.jpg", "ocr_text": "Safety and Security of Artificial Intelligence Systems: Canada\n\nstandardization strategies that are aligned with Canada’s priorities and enable the Canadian AI\nand data ecosystem to scale up on the international scene.”\n\nB. Record-Keeping\n\nThe Bill provides for public reporting and authorizes the Minister to order the production of\nrecords related to artificial intelligence systems. Section 60(1) stipulates that\n\n[a]n organization must, in accordance with any prescribed requirements, keep and\nmaintain a record of every breach of security safeguards involving personal information\nunder its control.”\n\nA section of the Bill called “Regulation of Artificial Intelligence Systems in the Private Sector”\nestablishes an obligation on persons who are carrying out regulated activity to keep records in\naccordance with regulations on the manner in which data is anonymized, use or management of\nanonymized data, assessments on high impact systems (and the reasons supporting their\nassessment), measures implemented related to risks, and monitoring of mitigation measures.*”\nThe record keeper must also keep any additional records in respect of the above requirements as\nprovided in accordance with issued regulations.\n\nThe Bill also states that “[a] person who is responsible for a high-impact system must, in\naccordance with the regulations and as soon as feasible, notify the Minister if the use of the system\nresults or is likely to result in material harm.”32\n\nThe Minister may “compel the production of certain information” including records “from\npersons subject to the Act for the purpose of verifying compliance with the Act.”%\n\nC. Transparency and Provision of Information to Users\n\nAccording to the companion document, transparency means “providing the public with\nappropriate information about how high-impact AI systems are being used.” The information\nprovided “should be sufficient to allow the public to understand the capabilities, limitations, and\npotential impacts of the systems.”%5\n\n8 Id.\n\n29 Bill C-27, § 60(1).\n\n30 Td. § 10(1).\n\n31 Id. § 10(2).\n\n32 Id. § 12.\n\n33 Charter Statement for Bill C-27, Government of Canada, https:/ / perma.cc/5VQ6-UAD2.\n\n34 The Artificial Intelligence and Data Act (AIDA) - Companion Document, supra note 8.\n35 Id,\n\nThe Law Library of Congress 30\n", "vlm_text": "\nB.   Record-Keeping  \nThe Bill provides for public reporting and authorizes the Minister to order the production of  records related to artificial intelligence systems. Section 60(1) stipulates that  \n[a]n organization must, in accordance with any prescribed requirements, keep and  maintain a record of every breach of security safeguards involving personal information  under its control.   \nA section of the Bill called “Regulation of Artificial Intelligence Systems in the Private Sector”  establishes an obligation on persons who are carrying out regulated activity to keep records in  accordance with regulations on the manner in which data is anonymized, use or management of  anonymized data, assessments on high impact systems (and the reasons supporting their  assessment), measures implemented related to risks, and monitoring of mitigation measures.   The record keeper must also keep any additional records in respect of the above requirements as  provided in accordance with issued regulations.   \nThe Bill also states that “[a] person who is responsible for a high-impact system must, in  accordance with the regulations and as soon as feasible, notify the Minister if the use of the system  results or is likely to result in material harm.” 32   \nThe Minister  may “compel the production of certain information” including records “from  persons subject to the Act for the purpose of verifying compliance with the Act.” 33   \nC.   Transparency and Provision of Information to Users  \nAccording to the companion document, transparency means “providing the public with  appropriate information about how high-impact AI systems are being used.” 34  The information  provided “should be sufficient to allow the public to understand the capabilities, limitations, and  potential impacts of the systems.” 35   "}
{"page": 33, "image_path": "page_images/2023555908_33.jpg", "ocr_text": "Safety and Security of Artificial Intelligence Systems: Canada\n\nThe Bill requires a person who makes available for use a high-impact system and manages the\noperation of a high-impact system to publish of a plain-language description of the high-impact\nsystem on a publicly available website that includes an explanation of\n\na) how the system is used;\n\nb) the types of content that it generates and the decisions, recommendations or\npredictions that it makes;\n\nc) the mitigation measures established under section 8 in respect of it; and\n\nd) any other information that may be prescribed by regulation.%¢\n\nThe draft law also authorizes the Minister to “publish information about artificial\nintelligence systems posing a serious risk of harm” and “to order a person to publish\ninformation related to their compliance with the Act.”37\n\nD. Human Oversight\n\nAccording to the companion document, “Human Oversight” means that “high-impact AI\nsystems must be designed and developed in such a way as to enable people managing the\noperations of the system to exercise meaningful oversight” and “[t]his includes a level of\ninterpretability appropriate to the context.”38 It adds that “[mJonitoring through measurement\nand assessment of high-impact AI systems and their output, is critical in supporting effective\nhuman oversight.”%?\n\nUnder AIDA, persons responsible for Al systems are obligated to assess whether a system is high-\nimpact, as follows:\n\n[b]usinesses would be expected to institute appropriate accountability mechanisms to\nensure compliance with their obligations under the Act. They would be held accountable\nfor the creation and enforcement of appropriate internal governance processes and policies\nto achieve compliance with the AIDA. Measures would be set through regulation and\nwould be tailored to the context and risks associated with specific regulated activities in\nthe lifecycle of a high-impact AI system.\n\nE. Risk Management System\n\nThe companion document stipulates that AIDA is “intended to protect Canadians, ensure the\ndevelopment of responsible AI in Canada, and to prominently position Canadian firms and\nvalues in global AI development.”! It adds that “[t]he risk-based approach in AIDA, including\nkey definitions and concepts, was designed to reflect and align with evolving international norms\n\n36 Bill C-27, § 11(1), (2).\n\n37 Charter Statement for Bill C-27, supra note 33.\n\n38 The Artificial Intelligence and Data Act (AIDA) - Companion Document, supra note 8.\n399 Id.\n\n41d.\n\n41d.\n\nThe Law Library of Congress 31\n", "vlm_text": "The Bill requires a person who makes available for use a high-impact system and  manages the  operation of a high-impact system  to publish of a plain-language description of the high-impact  system on a publicly available website that includes an explanation of   \na)   how the system is used;   b)   the types of content that it generates and the decisions, recommendations or  predictions that it makes;   c)   the mitigation measures established under section 8 in respect of it; and   d)   any other information that may be prescribed by regulation.   \nThe draft law also authorizes the Minister to “publish information about artificial  intelligence systems posing a serious risk of harm” and “to order a person to publish  information related to their compliance with the Act.” 37   \nD.   Human Oversight  \nAccording to the companion document, “Human Oversight ”  means that “high-impact AI  systems must be designed and developed in such a way as to enable people managing the  operations of the system to exercise meaningful oversight” and “[t]his includes a level of  interpret ability appropriate to the context.” 38  It adds that “[m]onitoring through measurement  and assessment of high-impact AI systems and their output, is critical in supporting effective  human oversight.” 39   \nUnder AIDA, persons responsible for AI systems are obligated to assess whether a system is high- impact, as follows:  \n[b]usinesses would be expected to institute appropriate accountability mechanisms to  ensure compliance with their obligations under the Act. They would be held accountable  for the creation and enforcement of appropriate internal governance processes and policies  to achieve compliance with the AIDA. Measures would be set through regulation and  would be tailored to the context and risks associated with specific regulated activities in  the lifecycle of a high-impact AI system.    \nE.   Risk Management System  \nThe companion document stipulates that AIDA is “intended to protect Canadians, ensure the  development of responsible AI in Canada, and to prominently position Canadian firms and  values in global AI development.” 41  It adds that  “[t]he risk-based approach in AIDA, including  key definitions and concepts, was designed to reflect and align with evolving international norms  in the AI space  ${\\prime\\prime}_{42}$   including the US National Institute of Standards and Technology (NIST) Risk  Management Framework (RMF), “while integrating seamlessly with existing Canadian legal  frameworks.”  "}
{"page": 34, "image_path": "page_images/2023555908_34.jpg", "ocr_text": "Safety and Security of Artificial Intelligence Systems: Canada\n\nin the AI space’*? including the US National Institute of Standards and Technology (NIST) Risk\nManagement Framework (RMF), “while integrating seamlessly with existing Canadian legal\nframeworks.”\n\nF. Quality Management System\nWe were unable to locate information in this area.\nG. Conformity Assessments\n\nAccording to its companion document, The AIDA would “mobilize external expertise in the\nprivate sector, academia, and civil society” to ensure that “enforcement activities” would occur\nthrough\n\ne the designation of external experts as analysts to support administration and\nenforcement of Act;\n\ne the use of AI audits performed by qualified independent auditors; and\n\ne the appointment of an advisory committee to provide the Minister with advice.*\n\nThe companion document also mentions that\n\n[iJn addition, voluntary certifications can play an important role as the ecosystem is\nevolving. The AI and Data Commissioner would assess the progress of the ecosystem over\ntime and ensure that administration and enforcement activities take into account the\ncapabilities and scale of impact of regulated organizations. For example, smaller firms\nwould not be expected to have governance structures, policies, and procedures\ncomparable to those of larger firms with a greater number of employees and a wider range\nof activities. Small and medium-sized businesses would also receive particular assistance\nin adopting the practices needed to meet the requirements.“*\n\nH. Robustness\n\nWe were unable to locate information in this area.\n\nI. Personal Data Protection\n\nThe federal Personal Information Protection and Electronic Documents Act (PIPEDA)* applies\nto all private sector organizations that collect, use, retain, or disclose personal information in the\ncourse of their commercial activities and imposes certain obligations upon them. The\n\nGovernment has proposed the Consumer Privacy Protection Act as part of Bill C-27 to\n“modernize this law in the context of the digital economy, and it is also undertaking broader\n\n21d.\n81d.\n“1d.\n45 Personal Information Protection and Electronic Documents Act, S.C. 2000, c. 5, https:/ / perma.cc/ZB9S-BR99.\n\nThe Law Library of Congress 32\n", "vlm_text": "\nF.   Quality Management System  \nWe were unable to locate information in this area.  \nG.   Conformity Assessments  \nAccording to its companion document, The AIDA would “mobilize external expertise in the  private sector, academia, and civil society” to ensure that “enforcement activities” would occur  through  \n•   the designation of external experts as analysts to support administration and  enforcement of Act;  •   the use of AI audits performed by qualified independent auditors; and  •   the appointment of an advisory committee to provide the Minister with advice.   \nThe companion document also mentions that  \n[i]n addition, voluntary certifications can play an important role as the ecosystem is  evolving. The AI and Data Commissioner would assess the progress of the ecosystem over  time and ensure that administration and enforcement activities take into account the  capabilities and scale of impact of regulated organizations. For example, smaller firms  would not be expected to have governance structures, policies, and procedures  comparable to those of larger firms with a greater number of employees and a wider range  of activities. Small and medium-sized businesses would also receive particular assistance  in adopting the practices needed to meet the requirements.   \nH.   Robustness  \nWe were unable to locate information in this area.  \nI.   Personal Data Protection  \nThe federal Personal Information Protection and Electronic Documents Act (PIPEDA) 45  applies  to all private sector organizations that collect, use, retain, or disclose personal information in the  course of their commercial activities and imposes certain obligations upon them.   The   Government has proposed the Consumer Privacy Protection Act as part of Bill C-27 to  “modernize this law in the context of the digital economy, and it is also undertaking broader  efforts to ensure that laws governing marketplace activities and communications services  keep pace.” 46   "}
{"page": 35, "image_path": "page_images/2023555908_35.jpg", "ocr_text": "efforts to ensure that laws governing marketplace activities and communications services\nkeep pace.”46\n\nAIDA applies to persons who are carrying out a “regulated activity.” A regulated activity is\ndefined under the Bill to include, in the course of international or interprovincial trade and\ncommerce,\n\nprocessing or making available for use any data relating to human activities for the\npurpose of designing, developing or using an artificial intelligence system.‘\n\nPeople who carry out this activity need to “establish measures with respect to the manner in\nwhich data is anonymized” and “establish measures with respect to the management of\nanonymized data.”48\n\nIn AIDA, there is a criminal provision on the possession or use of personal information in the\ncontext of artificial intelligence systems, as follows:\n\n[e]lvery person commits an offence if, for the purpose of designing, developing, using or\nmaking available for use an artificial intelligence system, the person possesses — within\nthe meaning of subsection 4(3) of the Criminal Code — or uses personal information,\nknowing or believing that the information is obtained or derived, directly or indirectly, as\na result of\n\n(a) the commission in Canada of an offence under an Act of Parliament or a\nprovincial legislature; or\n\n(b) an act or omission anywhere that, if it had occurred in Canada, would have\nconstituted such an offence.\n\nV. Adherence to Standardized Risk Management Frameworks\n\nInstitutions in Canada do not appear to have issued their own standardized risk management\nframework but the “current proposed standards” in AIDA “generally reflect those” in NIST’s AI\nRisk Management Framework 1.0 (AI RMF), and “emerging AI regulatory frameworks being\nconsidered in major economies around the world such as the EU.”5°\n\n46 The Artificial Intelligence and Data Act (AIDA) - Companion Document, supra note 8.\n\n47 Bill C-27, § 5(1).\n\n48 Gowling WLG, The Artificial Intelligence and Data Act (AIDA), https:/ / perma.cc/5U4F-2KVR.\n49 Bill C-27, § 38.\n\n5° Artificial Intelligence Risk Management Framework Published by NIST, FASKEN (Feb. 9, 2023),\nhttps:/ / perma.cc/6WTV-A89R.\n", "vlm_text": "\nAIDA applies to persons who are carrying out a “regulated activity.” A regulated activity is  defined under the Bill to include, in the course of international or interprovincial trade and  commerce,   \nprocessing or making available for use any data relating to human activities for the  purpose of designing, developing or using an artificial intelligence system.   \nPeople who carry out this activity need to “establish measures with respect to the manner in  which data is anonymized” and “establish measures with respect to the management of  anonymized data.” 48   \nIn AIDA, there is a criminal provision on the possession or use of personal information in the  context of artificial intelligence systems, as follows:  \n[e]very person commits an offence if, for the purpose of designing, developing, using or  making available for use an artificial intelligence system, the person possesses — within  the meaning of subsection 4(3) of the  Criminal Code  — or uses personal information,  knowing or believing that the information is obtained or derived, directly or indirectly, as  a result of  \n(a)  the commission in Canada of an offence under an Act of Parliament or a  provincial legislature; or  \n(b)   an act or omission anywhere that, if it had occurred in Canada, would have  constituted such an offence.   \nV.  Adherence to Standardized Risk Management Frameworks  \nInstitutions in Canada do not appear to have issued their own standardized risk management  framework but the “current proposed standards” in AIDA “generally reflect those” in NIST’s AI  Risk Management Framework 1.0 (AI RMF), and “emerging AI regulatory frameworks being  considered in major economies around the world such as the EU.” 50   "}
{"page": 36, "image_path": "page_images/2023555908_36.jpg", "ocr_text": "VI. AI Security Policy Across the Supply Chain\n\nScale AI is a technology cluster,5! which is headquartered in Montréal, that “aims to enable\nlogistics and supply chain excellence in Canada through the adoption and use of artificial\nintelligence (AI) powered tools.”52 The National Research Council of Canada (NRC) supports the\nScale AI cluster through its Artificial Intelligence for Logistics program. One of its projects is\n“Cybersecurity for Logistics Projects,” which includes\n\ne secure and resilient fog computing framework for intelligent transportation systems;\n\ne security of data provenance and machine learning for the Internet of Things;\n\nInternet of Things device profiling in smart transportation pathways; and\n\nGPS jammer risk management.\n\nSama, an AI data training company, commenting on the AIDA bill, believes that “the current\nproposed legislation needs to be broader and cover the entire AI supply chain, from data\nacquisition to workers’ rights. The legislation should also include vetting and inspection\nprocesses for procurement by government entities.”54\n\n51 Canada’s Al-Powered Supply Chains Cluster (Scale Al), Innovation, Science and Economic Development Canada,\nhttps:/ / perma.cc/ A88M-BWVE.\n\n52 Id.\n33 Id.\n\n54 Wendy Gonzalez et al., Sama Calls for Canada’s AIDA Regulation to Cover the Entire AI Supply Chain, Sama.com\n(Apr. 21, 2023), https:/ / perma.cc/D4FH-K7GZ.\n", "vlm_text": "VI.  AI Security Policy Across the Supply Chain  \nScale AI is a technology cluster,  which is headquartered in Montréal, that “aims to enable  logistics and supply chain excellence in Canada through the adoption and use of artificial  intelligence (AI) powered tools.” 52  The National Research Council of Canada (NRC) supports the  Scale AI cluster through its Artificial Intelligence for Logistics program. One of its projects is \n\n “Cybersecurity for Logistics Projects,” which includes \n\n \n•   secure and resilient fog computing framework for intelligent transportation systems;  \n\n •   security of data provenance and machine learning for the Internet of Things;  \n\n •   Internet of Things device profiling in smart transportation pathways; and  \n\n •   GPS jammer risk management.   \nSama, an AI data training company, commenting on the AIDA bill, believes that “the current  proposed legislation needs to be broader and cover the entire AI supply chain, from data  acquisition to workers’ rights. The legislation should also include vetting and inspection  processes for procurement by government entities  $^{\\prime\\prime}54$    "}
{"page": 37, "image_path": "page_images/2023555908_37.jpg", "ocr_text": "European Union\n\nJenny Gesley\nForeign Law Specialist\n\nSUMMARY On April 21, 2021, the European Commission published a legislative proposal for an\nArtificial Intelligence Act (draft AI Act). The draft AI Act addresses the risks posed by\nAI systems to the safety or fundamental rights of citizens by following a risk-based\napproach ranging from complete prohibition or mandatory requirements for certain\nhigh-risk AI systems to transparency rules or voluntary compliance with the rules for\nlow-risk AI systems. The EU envisages a possible final adoption of the AI Act at the end\nof 2023.\n\nThe EU cybersecurity legislative framework consists of several pieces of enacted and\nproposed legislation that cover certain aspects linked to cybersecurity from different\nangles. The proposed AI Act contains, among other things, specific requirements\nregarding the cybersecurity of high-risk AI systems, whereas other more general\nlegislative acts address aspects of cybersecurity that are not specific to AI systems.\nFurthermore, the General Data Protection Regulation makes security of personal data a\nprerequisite for the processing of personal data and requires controllers to apply the\nprinciples of security by design and by default.\n\nUnder the draft AI Act, AI systems that would qualify as high-risk would have to be\nregistered in an EU Database for Stand-Alone High-Risk AI Systems and comply with\ndetailed mandatory requirements with regard to risk management systems; the quality\nof data sets used; technical documentation; record keeping; transparency and provision\nof information to users; human oversight; appropriate level of accuracy, robustness,\nand cybersecurity; quality management systems; and ex-ante conformity assessment.\n\nI. Introduction\n\nOn April 21, 2021, the European Commission (Commission) published a legislative proposal for\nan Artificial Intelligence Act (draft AI Act).! The draft AI Act addresses the risks posed by AI\nsystems to the safety or fundamental rights of citizens by following a risk-based approach ranging\nfrom complete prohibition or mandatory requirements for certain high-risk AI systems to\ntransparency rules or voluntary compliance with the rules for low-risk AI systems. Furthermore,\nthe European Union (EU) General Data Protection Regulation (GDPR), which became applicable\nin May 2018, makes security of personal data a prerequisite for processing of personal data and\ncontains rules on automated individual decision-making in article 22.2\n\n1 Proposal for a Regulation of the European Parliament and of the Council Laying Down Harmonised Rules on Artificial\nIntelligence (Artificial Intelligence Act) and Amending Certain Union Legislative Acts, COM (2021) 206 final (Apr. 21,\n2021) (Draft AI Act), https:/ / perma.cc/ RWT9-9D97.\n\n? General Data Protection Regulation (GDPR), 2016 O.J. (L 119) 1, https:/ / perma.cc/7Y47-L7XX.\n", "vlm_text": "European Union  \nJenny Gesley  Foreign Law Specialist \n\nArtificial Intelligence Act (draft AI Act). The draft AI Act addresses the risks posed by  AI systems to the safety or fundamental rights of citizens by following a risk-based  approach ranging from complete prohibition or mandatory requirements for certain  high-risk AI systems to transparency rules or voluntary compliance with the rules for  low-risk AI systems. The EU envisages a possible final adoption of the AI Act at the end  of 2023.    \nThe EU cybersecurity legislative framework consists of several pieces of enacted and  proposed legislation that cover certain aspects linked to cybersecurity from different  angles. The proposed AI Act contains, among other things, specific requirements  regarding the cybersecurity of high-risk AI systems, whereas other more general  legislative acts address aspects of cybersecurity that are not specific to AI systems.  Furthermore, the General Data Protection Regulation makes security of personal data a  prerequisite for the processing of personal data and requires controllers to apply the  principles of security by design and by default.   \nUnder the draft AI Act, AI systems that would qualify as high-risk would have to be  registered in an EU Database for Stand-Alone High-Risk AI Systems and comply with  detailed mandatory requirements with regard to risk management systems; the quality  of data sets used; technical documentation; record keeping; transparency and provision  of information to users; human oversight; appropriate level of accuracy, robustness,  and cybersecurity; quality management systems; and ex-ante conformity assessment .  \nI.  Introduction  \nOn April 21, 2021, the European Commission (Commission) published a legislative proposal for  an Artificial Intelligence Act (draft AI Act).  The draft AI Act addresses the risks posed by AI  systems to the safety or fundamental rights of citizens by following a risk-based approach ranging  from complete prohibition or mandatory requirements for certain high-risk AI systems to  transparency rules or voluntary compliance with the rules for low-risk AI systems. Furthermore,  the European Union (EU) General Data Protection Regulation (GDPR), which became applicable  in May 2018, makes security of personal data a prerequisite for processing of personal data and  contains rules on automated individual decision-making in article 22.    "}
{"page": 38, "image_path": "page_images/2023555908_38.jpg", "ocr_text": "The draft AI Act is subject to the ordinary legislative procedure, meaning the co-legislators of the\nEU, the Council of the European Union (Council) and the European Parliament (EP), must\napprove an identical text in up to three readings.? The Council is made up of one government\nminister from each EU country, whereas the members of the EP are directly elected by the EU\ncitizens. The EP adopts its position first and communicates it to the Council. To speed up the\nlegislative process, there are informal trilogue discussions between representatives of the EP, the\nCouncil, and the Commission. The Council adopted its common position (general approach) on\nthe draft AI Act on December 6, 2022.5 The plenary of the EP adopted its negotiating position in\nits session on June 14, 2023.6 Trilogues commenced after that, with a possible final adoption of\nthe AI Act at the end of 2023.\n\nThe AI Act as an EU regulation would be directly applicable in the EU member states once it\nenters into force without the need for transposition into national law.” The implementation of the\nAI Act would be monitored by national supervisory authorities.\n\nII. Overview of the Legal and Policy Framework\nA. Legislation and Policy\n\nIn 2020, the EU adopted a new “EU Cybersecurity Strategy.”8 It contains proposals for deploying\nregulatory, investment, and policy instruments to address “(1) resilience, technological\nsovereignty and leadership, (2) building operational capacity to prevent, deter and respond, and\n(3) advancing a global and open cyberspace.”? Furthermore, it stated that “[c]ybersecurity must\nbe integrated into all these digital investments, particularly key technologies like Artificial\nIntelligence (AI), encryption and quantum computing, using incentives, obligations and\nbenchmarks.”1°\n\nThe EU cybersecurity legislative framework consists of several pieces of enacted and proposed\nlegislation that cover certain aspects linked to cybersecurity from different angles. The proposed\nAI Act contains, among other things, specific requirements regarding the cybersecurity of high-\n\n3 Consolidated Version of the Treaty on the Functioning of the European Union (TFEU), arts. 289, 294, 2016 O,J.\n(C 202) 47, https: / / perma.cc/ FM38-RYTH.\n\n+ Consolidated Version of the Treaty on European Union (TEU), arts. 14, 16, 2016 O.J. (C 202) 13,\nhttps:/ / perma.cc/9E8Y-B6C5.\n\n5 General Approach, File 2021/0106(COD) (Nov. 25, 2022), https: / / perma.cc/H889-JV69.\n\n6 European Parliament (EP), P9 TA(2023)0236, Artificial Intelligence Act. Amendments Adopted by the European\nParliament on 14 June 2023 on the Proposal for a Regulation of the European Parliament and of the Council on Laying\nDown Harmonised Rules on Artificial Intelligence (Artificial Intelligence Act) and Amending Certain Union Legislative\nActs (COM(2021)0206 - C9-0146/2021 - 2021/0106(COD)), https: / / perma.cc/JF3W-GU84.\n\n? TFEU, art. 288, para. 2.\n\n8 Joint Communication to the European Parliament and the Council. The EU's Cybersecurity Strategy for the Digital\nDecade (EU Cybersecurity Strategy), JOIN(2020) 18 final (Dec. 16, 2020), https:/ / perma.cc/ VUZ8-MUF2.\n\nId. at 4.\n10 Td. at 5 (emphasis added by author).\n", "vlm_text": "The draft AI Act is subject to the ordinary legislative procedure, meaning the co-legislators of the  EU, the Council of the European Union (Council) and the European Parliament (EP), must  approve an identical text in up to three readings.  The Council is made up of one government  minister from each EU country, whereas the members of the EP are directly elected by the EU  citizens.  The EP adopts its position first and communicates it to the Council. To speed up the  legislative process, there are informal trilogue discussions between representatives of the EP, the  Council, and the Commission. The Council adopted its common position (general approach) on  the draft AI Act on December 6, 2022.  The plenary of the EP adopted its negotiating position in  its session on June 14, 2023.  Trilogues commenced after that, with a possible final adoption of  the AI Act at the end of 2023.   \nThe AI Act as an EU regulation would be directly applicable in the EU member states once it  enters into force without the need for transposition into national law.  The implementation of the  AI Act would be monitored by national supervisory authorities.  \nII.  Overview of the Legal and Policy Framework  \nA.   Legislation and Policy  \nIn 2020, the EU adopted a new “EU Cybersecurity Strategy.” 8  It contains proposals for deploying  regulatory, investment, and policy instruments to address “(1) resilience, technological  sovereignty and leadership, (2) building operational capacity to prevent, deter and respond, and  (3) advancing a global and open cyberspace.” 9  Furthermore, it stated that “[c]ybersecurity must  be integrated into all these digital investments, particularly key technologies like  Artificial  Intelligence (AI) , encryption and quantum computing, using incentives, obligations and  benchmarks.” 10   \nThe EU cybersecurity legislative framework consists of several pieces of enacted and proposed  legislation that cover certain aspects linked to cybersecurity from different angles. The proposed  AI Act contains, among other things, specific requirements regarding the cybersecurity of high- risk AI systems, whereas other more general legislative acts address aspects of cybersecurity that  are not specific to AI systems.   "}
{"page": 39, "image_path": "page_images/2023555908_39.jpg", "ocr_text": "risk AI systems, whereas other more general legislative acts address aspects of cybersecurity that\nare not specific to AI systems.\n\n1. Draft AI Act\n\nThe draft Al Act would apply to providers that place AI systems on the EU market or put them\ninto service, irrespective of their location; users of AI systems located within the EU; and\nproviders and users of Al systems ina third country, where the output produced by the Al system\nis used in the EU.!! As mentioned, the proposal adopts a risk-based approach to regulation. It\noutlines four levels of risk: AI systems that pose an unacceptable risk would be completely\nprohibited, high-risk AI systems would be subject to detailed mandatory requirements as\noutlined below, limited risk AI systems would be subject to transparency requirements to alert\nusers that they are interacting with a machine, and providers of Al systems presenting minimal\nor no risk would be encouraged to adopt codes of conduct or to apply the mandatory\nrequirements for high-risk AI systems voluntarily.!2\n\nAI systems that would qualify as high-risk would have to be registered in an EU Database for\nStand-Alone High-Risk AI Systems and comply with detailed mandatory requirements with\nregard to risk management systems; the quality of data sets used; technical documentation;\nrecord keeping; transparency and provision of information to users; human oversight;\nappropriate levels of accuracy, robustness, and cybersecurity; quality management systems; and\nex-ante conformity assessment.!5\n\nThe Commission in its proposal defines high-risk AI systems as AI systems intended to be used\nas a safety component of a product or those that fall under EU safety legislation, for example toys,\naviation, cars, medical devices, or lifts.!4 In addition, Al systems that are deployed in the\nfollowing eight specific areas identified in annex III would automatically qualify as high-risk:\nbiometric identification and categorization of natural persons; management and operation of\ncritical infrastructure; education and vocational training; employment, worker management and\naccess to self-employment; access to and enjoyment of essential private services and public\nservices and benefits; law enforcement; migration, asylum, and border control management; and\nadministration of justice and democratic processes.!5 The Council in its general approach\namended the requirements and added an additional horizontal layer on top of the high-risk\nclassification to ensure that AI systems that are not likely to cause serious fundamental rights\nviolations or other significant risks are not captured.1¢ Likewise, the EP added a horizontal layer,\nbut also expanded the classification of high-risk areas to include harm to people’s health, safety,\nfundamental rights, or environment and influencing voters in political campaigns and in\n\n11 Draft AI Act, art. 2.\n\n12 Jd. arts. 5, 6, 52, 69, annex III.\n\n13 Td. arts. 6, 8-15, 19, 51, 60, annex III.\n\n4d. art. 6, para. 1.\n\n45 |d. art. 6, para. 2 in conjunction with annex III.\n\n16 General Approach, supra note 5, at 5, para. 1.5.\n", "vlm_text": "\n1.   Draft AI Act  \nThe draft AI Act would apply to providers that place AI systems on the EU market or put them  into service, irrespective of their location; users of AI systems located within the EU; and  providers and users of AI systems in a third country, where the output produced by the AI system  is used in the EU.  As mentioned, the proposal adopts a risk-based approach to regulation. It  outlines four levels of risk: AI systems that pose an unacceptable risk would be completely  prohibited, high-risk AI systems would be subject to detailed mandatory requirements as  outlined below, limited risk AI systems would be subject to transparency requirements to alert  users that they are interacting with a machine, and providers of AI systems presenting minimal  or no risk would be encouraged to adopt codes of conduct or to apply the mandatory  requirements for high-risk AI systems voluntarily.   \nAI systems that would qualify as high-risk would have to be registered in an EU Database for  Stand-Alone High-Risk AI Systems and comply with detailed mandatory requirements with  regard to risk management systems; the quality of data sets used; technical documentation;  record keeping; transparency and provision of information to users; human oversight;  appropriate levels of accuracy, robustness, and cybersecurity; quality management systems; and  ex-ante conformity assessment.    \nThe Commission in its proposal defines high-risk AI systems as AI systems intended to be used  as a safety component of a product or those that fall under EU safety legislation, for example toys,  aviation, cars, medical devices, or lifts.  In addition, AI systems that are deployed in the  following eight specific areas identified in annex III would automatically qualify as high-risk:  biometric identification and categorization of natural persons; management and operation of  critical infrastructure; education and vocational training; employment, worker management and  access to self-employment; access to and enjoyment of essential private services and public  services and benefits; law enforcement; migration, asylum, and border control management; and  administration of justice and democratic processes.  The Council in its general approach  amended the requirements and added an additional horizontal layer on top of the high-risk  classification to ensure that AI systems that are not likely to cause serious fundamental rights  violations or other significant risks are not captured.  Likewise, the EP added a horizontal layer,  but also expanded the classification of high-risk areas to include harm to people’s health, safety,  fundamental rights, or environment and influencing voters in political campaigns and in  recommender systems used by social media platforms regulated under the Digital Services  Act (DSA).   "}
{"page": 40, "image_path": "page_images/2023555908_40.jpg", "ocr_text": "recommender systems used by social media platforms regulated under the Digital Services\nAct (DSA).17\n\n2. General Cybersecurity Legislation\n\nIn August 2016, the first EU legislation on cybersecurity, the Network and Information Security\n(NIS) Directive, entered into force.!8 Its aim is to “achiev[e] a high common level of security of\nnetwork and information systems within the Union” by requiring EU Member States to adopt\nnational cybersecurity strategies, designate competent national authorities, set-up computer-\nsecurity incident response teams, and establish security and notification requirements for\noperators of essential services and for digital service providers.!9 The NIS Directive was updated\nin 2022 to address the “expansion of the cyber threat landscape” and the divergent\nimplementation of the NIS Directive in the EU Member States, which led to a fragmentation of\nthe EU internal market (NIS 2 Directive).2” NIS 2 entered into force on January 16, 2023, and\nMember States must transpose it into national law by October 17, 2024.2!\n\nIn 2019, the Cybersecurity Act (CSA) entered into force, which established voluntary “European\ncybersecurity certification schemes for the purpose of ensuring an adequate level of cybersecurity\nfor ICT [information and communication technologies] products, ICT services and ICT processes\nin the Union.””2 The CSA defined “cybersecurity” as “the activities necessary to protect network\nand information systems, the users of such systems, and other persons affected by\ncyber threats.”\n\nLastly, in 2022, the Commission published a proposal for a regulation on horizontal cybersecurity\nrequirements for products with digital elements (Cyber Resilience Act, CRA).”4 It also covers AI\nsystems, including the cybersecurity of products with digital elements that are classified as high-\nrisk Al systems.” According to the explanatory memorandum, the CRA proposal is coherent with\nthe AI Act proposal.” With regard to the relationship between the two proposed acts, recital 29\nof the CRA states that\n\n17 EP, supra note 6, amendments 234, 235, 739, 740; Digital Services Act [DSA], 2022 O,J. (L 277) 1,\nhttps:/ / perma.cc/ Y5S3-Z7YX.\n\n18 NIS Directive, 2016 O.J. (L 194) 1, https:/ / perma.cc/JH4W-FHFB.\n\n197d. art. 1.\n\n20 NIS 2 Directive, recitals 3, 5, 2022 O.J. (L 333) 80, https:/ / perma.cc/ EGT7-7Q3F.\n211d. arts. 41, 45.\n\n22 Cybersecurity Act [CSA], art. 1, 2019 O.J. (L151) 15, https:/ / perma.cc/8E4S-2BPJ.\n23 CSA, art. 2(1).\n\n24 Proposal for a Regulation of the European Parliament and of the Council on Horizontal Cybersecurity Requirements for\nProducts with Digital Elements and Amending Regulation (EU) 2019/1020 (Cyber Resilience Act, CRA),\nCOM/2022/454 final (Sept. 15, 2022), https: / / perma.cc/ N2TV-ZJRD. For more information on the proposed\nCyber Resilience Act, see Jenny Gesley, European Union: Commission Proposes New Cybersecurity Rules for\nProducts with Digital Elements, Global Legal Monitor (Dec. 2, 2022), https:/ / perma.cc/2MRE-AQ4Z.\n\n25 CRA, art. 8.\n26 Td. at 3.\n", "vlm_text": "\n2.   General Cybersecurity Legislation  \nIn August 2016, the first EU legislation on cybersecurity, the Network and Information Security  (NIS) Directive, entered into force.  Its aim is to “achiev[e] a high common level of security of  network and information systems within the Union” by requiring EU Member States to adopt  national cybersecurity strategies, designate competent national authorities, set-up computer- security incident response teams, and establish security and notification requirements for  operators of essential services and for digital service providers.  The NIS Directive was updated  in 2022 to address the “expansion of the cyber threat landscape” and the divergent  implementation of the NIS Directive in the EU Member States, which led to a fragmentation of  the EU internal market (NIS 2 Directive).   NIS 2 entered into force on January 16, 2023, and  Member States must transpose it into national law by October 17, 2024.   \nIn 2019, the Cybersecurity Act (CSA)   entered into force, which established voluntary “European  cybersecurity certification schemes for the purpose of ensuring an adequate level of cybersecurity  for ICT [information and communication technologies] products, ICT services and ICT processes  in the Union.” 22  The CSA defined “cybersecurity” as “the activities necessary to protect network  and information systems, the users of such systems, and other persons affected by  cyber threats.” 23   \nLastly, in 2022, the Commission published a proposal for a regulation on horizontal cybersecurity  requirements for products with digital elements (Cyber Resilience Act, CRA).  It also covers AI  systems, including the cybersecurity of products with digital elements that are classified as high- risk AI systems.  According to the explanatory memorandum, the CRA proposal is coherent with  the AI Act proposal.  With regard to the relationship between the two proposed acts, recital 29  of the CRA states that  "}
{"page": 41, "image_path": "page_images/2023555908_41.jpg", "ocr_text": "[p]roducts with digital elements classified as high-risk AI systems according to Article 6 of\nRegulation [the AI Regulation] which fall within the scope of this Regulation should\ncomply with the essential requirements set out in this Regulation. When those high-risk\nAI systems fulfil the essential requirements of this Regulation, they should be deemed\ncompliant with the cybersecurity requirements set out in Article [Article 15] of Regulation\n[the AI Regulation] in so far as those requirements are covered by the EU declaration of\nconformity or parts thereof issued under this Regulation. As regards the conformity\nassessment procedures relating to the essential cybersecurity requirements of a product\nwith digital elements covered by this Regulation and classified as a high-risk AI system,\nthe relevant provisions of Article 43 of Regulation [the AI Regulation] should apply as a\nrule instead of the respective provisions of this Regulation. However, this rule should not\nresult in reducing the necessary level of assurance for critical products with digital\nelements covered by this Regulation. Therefore, by way of derogation from this rule, high-\nrisk AI systems that fall within the scope of the Regulation [the AI Regulation] and are also\nqualified as critical products with digital elements pursuant to this Regulation and to\nwhich the conformity assessment procedure based on internal control referred to in Annex\nVI of the Regulation [the AI Regulation] applies, should be subject to the conformity\nassessment provisions of this Regulation in so far as the essential requirements of this\nRegulation are concerned. In this case, for all the other aspects covered by Regulation [the\nAI Regulation] the respective provisions on conformity assessment based on internal\ncontrol set out in Annex VI to Regulation [the AI Regulation] should apply.”\n\nThat means that, as a general rule, for products with digital elements that also classify as high-\nrisk AI systems, the CRA conformity assessment procedure would demonstrate compliance with\nthe proposed AI Act requirements, with exceptions for certain AI critical products with\ndigital elements.\n\nB. Agencies\n\nThe European Union Agency for Cybersecurity (ENISA) was established in 2004 and is the EU\nagency that deals with cybersecurity.8 It provides support to Member States, Union institutions,\nbodies, offices, and agencies in improving cybersecurity and acts as a reference point for advice\nand expertise on cybersecurity.2? Among other things, it is tasked with assisting Member States\nin implementing EU cybersecurity legislation, in particular the NIS Directive.5\n\nWith regard to the proposed AI Act, the EU Member States will have to designate one or several\nnational competent authorities, meaning a national supervisory authority, a notifying authority,\nand a market surveillance authority, to supervise the application and implementation of the AI\nAct, in particular for monitoring the compliance of providers of high-risk AI systems with their\nobligations, such as ensuring an appropriate level of cybersecurity.*!\n\n27 CRA, recital 29. This is also regulated in CRA, art. 8.\n28 Cybersecurity Act, arts. 3, 4.\n\n291d.\n\n30 Td. art. 5(2).\n\n31 Draft AI Act, arts. 3(43), 23, 59.\n", "vlm_text": "[p]roducts with digital elements classified as high-risk AI systems according to Article 6 of  Regulation [the AI Regulation] which fall within the scope of this Regulation should  comply with the essential requirements set out in this Regulation. When those high-risk  AI systems fulfil the essential requirements of this Regulation, they should be deemed  compliant with the cybersecurity requirements set out in Article [Article 15] of Regulation  [the AI Regulation] in so far as those requirements are covered by the EU declaration of  conformity or parts thereof issued under this Regulation. As regards the conformity  assessment procedures relating to the essential cybersecurity requirements of a product  with digital elements covered by this Regulation and classified as a high-risk AI system,  the relevant provisions of Article 43 of Regulation [the AI Regulation] should apply as a  rule instead of the respective provisions of this Regulation. However, this rule should not  result in reducing the necessary level of assurance for critical products with digital  elements covered by this Regulation. Therefore, by way of derogation from this rule, high- risk AI systems that fall within the scope of the Regulation [the AI Regulation] and are also  qualified as critical products with digital elements pursuant to this Regulation and to  which the conformity assessment procedure based on internal control referred to in Annex  VI of the Regulation [the AI Regulation] applies, should be subject to the conformity  assessment provisions of this Regulation in so far as the essential requirements of this  Regulation are concerned. In this case, for all the other aspects covered by Regulation [the  AI Regulation] the respective provisions on conformity assessment based on internal  control set out in Annex VI to Regulation [the AI Regulation] should apply.   \nThat means that, as a general rule, for products with digital elements that also classify as high- risk AI systems, the CRA conformity assessment procedure would demonstrate compliance with  the proposed AI Act requirements, with exceptions for certain AI critical products with  digital elements.  \nB.   Agencies  \nThe European Union Agency for Cybersecurity (ENISA) was established in 2004 and is the EU  agency that deals with cybersecurity.  It provides support to Member States, Union institutions,  bodies, offices, and agencies in improving cybersecurity and acts as a reference point for advice  and expertise on cybersecurity.  Among other things, it is tasked with assisting Member States  in implementing EU cybersecurity legislation, in particular the NIS Directive.   \nWith regard to the proposed AI Act, the EU Member States will have to designate one or several  national competent authorities, meaning a national supervisory authority, a notifying authority,  and a market surveillance authority, to supervise the application and implementation of the AI  Act, in particular for monitoring the compliance of providers of high-risk AI systems with their  obligations, such as ensuring an appropriate level of cybersecurity.   "}
{"page": 42, "image_path": "page_images/2023555908_42.jpg", "ocr_text": "III. Definition of Artificial Intelligence (AI) Systems\n\nThe Commission proposal defines AI systems as “[s]oftware that is developed with one or more\nof the techniques and approaches listed in annex I and can, for a given set of human-defined\nobjectives, generate outputs such as content, predictions, recommendations, or decisions\ninfluencing the environments they interact with.”32 The approaches listed in the annex are\nmachine learning approaches, logic- and knowledge-based approaches, and statistical\napproaches.%3\n\nBoth the Council in its general approach and the EP adopted changes to the Commission’s\ndefinition of Al systems. The Council narrowed down the definition of AI systems to systems\ndeveloped through machine learning approaches and logic- and knowledge-based approaches.*4\nIt defined an AI system as\n\na system that is designed to operate with elements of autonomy and that, based on\nmachine and/or human-provided data and inputs, infers how to achieve a given set of\nobjectives using machine learning and/or logic- and knowledge based approaches, and\nproduces system-generated outputs such as content (generative AI systems), predictions,\nrecommendations or decisions, influencing the environments with which the AI\nsystem interacts.%5\n\nThe EP amended the definition of AI systems to align it with the OECD definition.%° An AI system\nis accordingly defined as “a machine-based system that is designed to operate with varying levels\nof autonomy and that can, for explicit or implicit objectives, generate outputs such as predictions,\nrecommendations, or decisions, that influence physical or virtual environments.”*”\n\nIV. Cybersecurity of AI\n\nENISA notes that there are three dimensions with regard to the relationship between\ncybersecurity and AI.*8 They are as follows:\n\n32 Draft AI Act, art. 3(1).\n\n33 Annexes to the Proposal for a Regulation of the European Parliament and of the Council Laying Down Harmonised\nRules on Artificial Intelligence (Artificial Intelligence Act) and Amending Certain Union Legislative Acts, COM (2021)\n206 final (Apr. 21, 2021), annex I, https:/ / perma.cc/T3X7-7WGV.\n\n34 General Approach, supra note 5, at 71, art. 3(1).\n3 Id.\n\n3¢ The OECD defines an AI system as “a machine-based system that can, for a given set of human-defined\nobjectives, make predictions, recommendations, or decisions influencing real or virtual environments. AI\nsystems are designed to operate with varying levels of autonomy.” See Recommendation of the Council on\nArtificial Intelligence, OECD Legal Instruments (May 21, 2019), https:/ / perma.cc/G59U-RL6D.\n\n3” EP, supra note 6, at 109, amendment 165.\n38 ENISA, Securing Machine Learning Algorithms (Dec. 14, 2021), https:/ / perma.cc/2L9L-6N6Z.\n", "vlm_text": "III.  Definition of Artificial Intelligence (AI) Systems  \nThe Commission proposal defines AI systems as “[s]oftware that is developed with one or more  of the techniques and approaches listed in annex I and can, for a given set of human-defined  objectives, generate outputs such as content, predictions, recommendations, or decisions  influencing the environments they interact with.” 32  The approaches listed in the annex are  machine learning approaches, logic- and knowledge-based approaches, and statistical  approaches.   \nBoth the Council in its general approach and the EP adopted changes to the Commission’s  definition of AI systems. The Council narrowed down the definition of AI systems to systems  developed through machine learning approaches and logic- and knowledge-based approaches. It defined an AI system as   \na system that is designed to operate with elements of autonomy and that, based on  machine and/or human-provided data and inputs, infers how to achieve a given set of  objectives using machine learning and/or logic- and knowledge based approaches, and  produces system-generated outputs such as content (generative AI systems), predictions,  recommendations or decisions, influencing the environments with which the AI  system interacts.    \nThe EP   amended the definition of AI systems to align it with the OECD definition.  An AI system  is accordingly defined as “a machine-based system that is designed to operate with varying levels  of autonomy and that can, for explicit or implicit objectives, generate outputs such as predictions,  recommendations, or decisions, that influence physical or virtual   environments.” 37   \nIV.  Cybersecurity of AI  \nENISA notes that there are three dimensions with regard to the relationship between  cybersecurity and AI.  They are as follows:  "}
{"page": 43, "image_path": "page_images/2023555908_43.jpg", "ocr_text": "Safety and Security of Artificial Intelligence Systems: European Union\n\n¢ cybersecurity of AI: lack of robustness and the vulnerabilities of AI models and\nalgorithms;\n\ne Alto support cybersecurity: AI used as a tool/means to create advanced cybersecurity\n(e.g., by developing more effective security controls) and to facilitate the efforts of law\nenforcement and other public authorities to better respond to cybercrime; and\n\n¢ malicious use of AI: malicious/adversarial use of AI to create more sophisticated types\nof attacks.%°\n\nThis report will focus on the first of these three dimensions as outlined in the draft AI Act. The\ndraft AI Act details the requirements for high-risk AI systems in chapter 2.40 As mentioned, once\nan AI system has been classified as high-risk, it would have to comply with mandatory\nrequirements with regard to risk management system; the quality of data sets used; technical\ndocumentation; record keeping; transparency and provision of information to users; human\noversight; appropriate level of accuracy, robustness, and cybersecurity; quality management\nsystem, and ex-ante conformity assessment.\n\nA. Data and Data Governance\n\nThe draft AI Act sets out requirements for data quality, verification of the source of data, and the\nintegrity of data. Article 10 provides that high-risk Al systems which make use of techniques\ninvolving the training of models with data would have to be developed on the basis of training,\nvalidation, and testing data sets that meet certain specified criteria. In particular, the criteria\nwould concern\n\n(a) the relevant design choices;\n\n(b) data collection;\n\n(c) relevant data preparation processing operations, such as annotation, labelling,\ncleaning, enrichment and aggregation;\n\n(d) the formulation of relevant assumptions, notably with respect to the information that\nhe data are supposed to measure and represent;\n\n(e) a prior assessment of the availability, quantity and suitability of the data sets that\nare needed;\n\n(f) examination in view of possible biases;\n\n(g) the identification of any possible data gaps or shortcomings, and how those gaps and\nhortcomings can be addressed.‘\n\nez)\n\nFurthermore, training, validation, and testing data would have to be relevant, representative, free\nof error, and complete with the appropriate statistical properties.42 The data sets would have to\n\n39 ENISA, Cybersecurity of AI and Standardisation 10, para. 2.2 (Mar. 14, 2023), https:/ / perma.cc/TL52-PFMG.\n40 Draft AI Act, arts. 8-15, 19.\n\n41 Td. art. 10, para. 2.\n\n#2 Td. art. 10, para. 3.\n\nThe Law Library of Congress 41\n&\n", "vlm_text": "•   cybersecurity of AI: lack of robustness and the vulnerabilities of AI models and  algorithms;  •   AI to support cybersecurity: AI used as a tool/means to create advanced cybersecurity  (e.g., by developing more effective security controls) and to facilitate the efforts of law  enforcement and other public authorities to better respond to cybercrime; and  •   malicious use of AI: malicious/adversarial use of AI to create more sophisticated types  of attacks.   \nThis report will focus on the first of these three dimensions as outlined in the draft AI Act. The  draft AI Act details the requirements for high-risk AI systems in chapter 2.  As mentioned, once  an AI system has been classified as high-risk, it would have to comply with mandatory  requirements with regard to risk management system; the quality of data sets used; technical  documentation; record keeping; transparency and provision of information to users; human  oversight; appropriate level of accuracy, robustness, and cybersecurity; quality management  system; and ex-ante conformity assessment.    \nA.   Data and Data Governance   \nThe draft AI Act sets out requirements for data quality, verification of the source of data, and the  integrity of data. Article 10 provides that high-risk AI systems which make use of techniques  involving the training of models with data would have to be developed on the basis of training,  validation, and testing data sets that meet certain specified criteria. In particular, the criteria  would concern   \n(a)   the relevant design choices;  (b)   data collection;  (c)   relevant data preparation processing operations, such as annotation, labelling,  cleaning, enrichment and aggregation;  (d)   the formulation of relevant assumptions, notably with respect to the information that  the data are supposed to measure and represent;  (e)   a prior assessment of the availability, quantity and suitability of the data sets that  are needed;  (f)   examination in view of possible biases;   (g)   the identification of any possible data gaps or shortcomings, and how those gaps and  shortcomings can be addressed.   \nFurthermore, training, validation, and testing data would have to be relevant, representative, free  of error, and complete with the appropriate statistical properties.  The data sets would have to  take specific geographical, behavioral, or functional settings for use into account.  The proposal  would allow the processing of sensitive personal data to avoid and correct bias in data sets.   "}
{"page": 44, "image_path": "page_images/2023555908_44.jpg", "ocr_text": "take specific geographical, behavioral, or functional settings for use into account.** The proposal\nwould allow the processing of sensitive personal data to avoid and correct bias in data sets.\n\nFor other high-risk AI systems that do not make use of techniques involving the training of\nmodels, “appropriate data governance and management practices” would be required to ensure\ndata quality, although the proposal does not elaborate on the required adequacy.\n\nB. Record-keeping\n\nRecord-keeping is addressed in article 12. High-risk AI systems would have to be designed and\ndeveloped with logging capabilities that comply with recognized standards or common\nspecifications to ensure the traceability of the system’s functioning, in particular with regard to\nAI systems having the potential to affect adversely the health or safety or the protection of\nfundamental rights of persons or situations leading to a modification of the AI system.* High-\nrisk AI systems intended to be used for the real-time and post remote biometric identification of\nnatural persons would have to be capable to log, at a minimum,\n\n(a) recording of the period of each use of the system (start date and time and end date and\ntime of each use);\n\n(b) the reference database against which input data has been checked by the system;\n\n(c) the input data for which the search has led to a match; and\n\n(d) the identification of the natural persons involved in the verification of the results, as\nreferred to in Article 14 (5).4”\n\nThe EP amended the record-keeping requirements and added requirements to measure and log\nthe energy consumption, resource use, and environmental impact during the lifecycle of the\nsystem.*8 It deleted the requirement for record-keeping of AI systems intended to be used for\nbiometric identification, because under the EP’s proposal, they would be completely prohibited.”\n\nC. Transparency and Provision of Information to Users\nThe draft AI Act aims to enable users to interpret the output of high-risk AI systems and use it\n\nappropriately. It would therefore require high-risk Al systems to be designed and developed in\na transparent way to ensure security control.°° To that end, user guides with concise, complete,\n\nTd. art. 10, para. 4.\n441d. art. 10, para. 5.\n45 Td. art. 10, para. 6.\n\n46\n\nd. art. 12, paras. 1-3.\n47 1d. art. 12, para. 4.\n\n48 EP, supra note 6, amendment 297.\n\n49 Id. amendments 220, 227.\n50 Draft AI Act, art. 1, para. 1.\n", "vlm_text": "\nFor other high-risk AI systems that do not make use of techniques involving the training of  models, “appropriate data governance and management practices” would be required to ensure  data quality, although the proposal does not elaborate on the required adequacy.   \nB.   Record-keeping  \nRecord-keeping is addressed in article 12. High-risk AI systems would have to be designed and  developed with logging capabilities that comply with recognized standards or common  specifications to ensure the traceability of the system’s functioning, in particular with regard to  AI systems having the potential to affect adversely the health or safety or the protection of  fundamental rights of persons or situations leading to a modification of the AI system.  High- risk AI systems intended to be used for the real-time and post remote biometric identification of  natural persons would have to be capable to log, at a minimum,  \n(a)   recording of the period of each use of the system (start date and time and end date and  time of each use);   (b)   the reference database against which input data has been checked by the system;  (c)   the input data for which the search has led to a match; and  (d)   the identification of the natural persons involved in the verification of the results, as  referred to in Article 14 (5).   \nThe EP amended the record-keeping requirements and added requirements to measure and log  the energy consumption, resource use, and environmental impact during the lifecycle of the  system.  It deleted the requirement for record-keeping of AI systems intended to be used for  biometric identification, because under the EP’s proposal, they would be completely prohibited.   \nC.   Transparency and Provision of Information to Users  \nThe draft AI Act aims to enable users to interpret the output of high-risk AI systems and use it  appropriately. It would therefore require high-risk AI systems to be designed and developed in  a transparent way to ensure security control.  To that end, user guides with concise, complete,  correct, clear, relevant, accessible, and comprehensible information would have to be made  available to users.    "}
{"page": 45, "image_path": "page_images/2023555908_45.jpg", "ocr_text": "Safety and Security of Artificial Intelligence Systems: European Union\n\ncorrect, clear, relevant, accessible, and comprehensible information would have to be made\navailable to users.>!\n\nIn particular, the user instructions would have to include\n\n(a) the identity and the contact details of the provider and, where applicable, of its\nauthorised representative;\n(b) the characteristics, capabilities and limitations of performance of the high-risk AI\nsystem, including:\n(i) its intended purpose;\n(ii) the level of accuracy, robustness and cybersecurity referred to in Article 15\nagainst which the high-risk AI system has been tested and validated and which\ncan be expected, and any known and foreseeable circumstances that may have an\nimpact on that expected level of accuracy, robustness and cybersecurity;\n(iii) any known or foreseeable circumstance, related to the use of the high-risk AI\nsystem in accordance with its intended purpose or under conditions of reasonably\nforeseeable misuse, which may lead to risks to the health and safety or\nfundamental rights;\n(iv) its performance as regards the persons or groups of persons on which the\nsystem is intended to be used;\n(v) when appropriate, specifications for the input data, or any other relevant\ninformation in terms of the training, validation and testing data sets used, taking\ninto account the intended purpose of the AI system.\n(c) the changes to the high-risk AI system and its performance which have been pre-\ndetermined by the provider at the moment of the initial conformity assessment, if any;\n(d) the human oversight measures referred to in Article 14, including the technical\nmeasures put in place to facilitate the interpretation of the outputs of AI systems by\nthe users;\n(e) the expected lifetime of the high-risk AI system and any necessary maintenance and\ncare measures to ensure the proper functioning of that AI system, including as regards\nsoftware updates.®2\n\nThe Council would additionally require providers to include illustrative examples to help users\nunderstand the instructions, as well as a description of the mechanism included within the AI\nsystem that allows users to properly collect, store, and interpret the logs.\n\nD. Human Oversight\n\nThe draft AI Act states that human oversight is necessary to “prevent[] or minimi[ze] the risks to\nhealth, safety or fundamental rights that may emerge when a high-risk AI system is used in\naccordance with its intended purpose or under conditions of reasonably foreseeable misuse.”>+\nProviders would therefore be required to “design[] and develop[] [high-risk AI systems] in such\na way, including with appropriate human-machine interface tools, that they can be effectively\n\n51 Jd. art. 13, para. 2\n\n52 ]d. art. 13, para. 3.\n\n53 General Approach, supra note 5, at 44, para. 47 & at 96, art. 13, para. 3(f).\n54 Draft AI Act, art. 14, para. 2.\n\nThe Law Library of Congress 43\n", "vlm_text": "\nIn particular, the user instructions would have to include  \n(a) the identity and the contact details of the provider and, where applicable, of its  authorised representative;  \n(b) the characteristics, capabilities and limitations of performance of the high-risk AI  system, including:  \n(i) its intended purpose;  (ii) the level of accuracy, robustness and cybersecurity referred to in Article 15  against which the high-risk AI system has been tested and validated and which  can be expected, and any known and foreseeable circumstances that may have an  impact on that expected level of accuracy, robustness and cybersecurity;  (iii) any known or foreseeable circumstance, related to the use of the high-risk AI  system in accordance with its intended purpose or under conditions of reasonably  foreseeable misuse, which may lead to risks to the health and safety or  fundamental rights;  (iv) its performance as regards the persons or groups of persons on which the  system is intended to be used;  (v) when appropriate, specifications for the input data, or any other relevant  information in terms of the training, validation and testing data sets used, taking  into account the intended purpose of the AI system.  \n(c) the changes to the high-risk AI system and its performance which have been pre- determined by the provider at the moment of the initial conformity assessment, if any;  (d) the human oversight measures referred to in Article 14, including the technical  measures put in place to facilitate the interpretation of the outputs of AI systems by  the users;  \n(e) the expected lifetime of the high-risk AI system and any necessary maintenance and  care measures to ensure the proper functioning of that AI system, including as regards  software updates.   \nThe Council would additionally require providers to include illustrative examples to help users  understand the instructions, as well as   a description of the mechanism included within the AI  system that allows users to properly collect, store, and interpret the logs.   \nD.   Human Oversight  \nThe draft AI Act states that human oversight is necessary to “prevent[] or minimi[ze] the risks to  health, safety or fundamental rights that may emerge when a high-risk AI system is used in  accordance with its intended purpose or under conditions of reasonably foreseeable misuse.  $^{\\prime\\prime}54$    Providers would therefore be required to “design[] and develop[] [high-risk AI systems] in such  a way, including with appropriate human-machine interface tools, that they can be effectively  overseen by natural persons during the period in which the AI system is in use.” 55  Human  oversight measures should either be identified and built into the high-risk AI system by the  provider or be identified by the provider to be implemented by the user.  These measures must  enable the individual performing oversight to fully understand the system and its limits; identify  automation bias; correctly interpret the system’s output; decide not to use the system or otherwise  disregard, override, or reverse the output; and intervene or interrupt the operation.   Furthermore, for high-risk AI systems intended to be used for biometric identification, any action  or decision would have to be verified and confirmed by at least two natural persons.   "}
{"page": 46, "image_path": "page_images/2023555908_46.jpg", "ocr_text": "overseen by natural persons during the period in which the AI system is in use.” Human\noversight measures should either be identified and built into the high-risk AI system by the\nprovider or be identified by the provider to be implemented by the user.>° These measures must\nenable the individual performing oversight to fully understand the system and its limits; identify\nautomation bias; correctly interpret the system’s output; decide not to use the system or otherwise\ndisregard, override, or reverse the output; and intervene or interrupt the operation.%”\nFurthermore, for high-risk AI systems intended to be used for biometric identification, any action\nor decision would have to be verified and confirmed by at least two natural persons.*8\n\nThe EP in its amendment added that persons performing oversight would need a sufficient level\nof AI literacy and the necessary support and authority to exercise that function.°*?\n\nE. Risk Management System\n\nAll high-risk AI systems would need to have a risk management system established,\nimplemented, documented, and maintained. The risk management system would consist of a\ncontinuous iterative process run throughout the entire lifecycle of the system with regular\nupdating. The following steps would need to be included:\n\n(a) identification and analysis of the known and foreseeable risks associated with each\nhigh-risk AI system;\n\n(b) estimation and evaluation of the risks that may emerge when the high-risk AI system\nis used in accordance with its intended purpose and under conditions of reasonably\nforeseeable misuse;\n\n(c) evaluation of other possibly arising risks based on the analysis of data gathered from\nthe post-market monitoring system referred to in Article 61;\n\n(d) adoption of suitable risk management measures in accordance with the provisions of\nthe following paragraphs.*!\n\nResidual risks judged acceptable would need to be communicated to the user.* However,\nproviders would need to ensure that risks are eliminated or reduced as far as possible through\nadequate design and development or are adequately mitigated and controlled if they cannot be\neliminated, as well as ensure that adequate information is provided. The most appropriate risk\nmanagement measures would need to be identified through testing suitable to achieve the\nintended purpose of the AI system throughout the development process and prior to the placing\n\n55 Id. art. 14, para. 1.\n\n5\n\nd. art. 14, para. 3.\n\n57 ]d. art. 14, para. 4.\n\n58 Id. art. 14, para. 5.\n\n59 EP, supra note 6, amendment 314.\n6 Draft AI Act, art. 9, para. 1.\n\n61 Jd. art. 9, para. 2.\n\n6 |d. art. 9, para. 4.\n\n6 Id.\n\n", "vlm_text": "\nThe EP in its amendment added that persons performing oversight would need a sufficient level  of AI literacy and the necessary support and authority to exercise that function.   \nE.   Risk Management System  \nAll high-risk AI systems would need to have a risk management system established,  implemented, documented, and maintained.  The risk management system would consist of a  continuous iterative process run throughout the entire lifecycle of the system with regular  updating. The following steps would need to be included:  \n(a) identification and analysis of the known and foreseeable risks associated with each  high-risk AI system;  (b) estimation and evaluation of the risks that may emerge when the high-risk AI system  is used in accordance with its intended purpose and under conditions of reasonably  foreseeable misuse;  (c) evaluation of other possibly arising risks based on the analysis of data gathered from  the post-market monitoring system referred to in Article 61;  (d) adoption of suitable risk management measures in accordance with the provisions of  the following paragraphs.   \nResidual risks judged acceptable would need to be communicated to the user.  However,  providers would need to ensure that risks are eliminated or reduced as far as possible through  adequate design and development or are adequately mitigated and controlled if they cannot be  eliminated, as well as ensure that adequate information is provided.  The most appropriate risk  management measures would need to be identified through testing   suitable to achieve the  intended purpose of the AI system   throughout the development process and prior to the placing  on the market.  Special consideration would have to be given to the impact of the AI system on  children or their opportunity to access such a system.   "}
{"page": 47, "image_path": "page_images/2023555908_47.jpg", "ocr_text": "Safety and Security of Artificial Intelligence Systems: European Union\n\non the market.* Special consideration would have to be given to the impact of the AI system on\nchildren or their opportunity to access such a system.®\n\nThe Council would require testing of the AI system in real world conditions. The EP, among\nother things, would like to see experts and external stakeholders involved to eliminate or reduce\nrisks through adequate design and development.*”\n\nF. Quality Management System\n\nProviders would be obligated to establish a quality management system, which should include,\namong other things, security management aspects.* In particular, the quality management\nsystem would have to include at least the following aspects:\n\n(a) astrategy for regulatory compliance, including compliance with conformity assessment\nprocedures and procedures for the management of modifications to the high-risk AI\nsystem;\n\n(b) techniques, procedures and systematic actions to be used for the design, design control\nand design verification of the high-risk AI system;\n\n(c) techniques, procedures and systematic actions to be used for the development, quality\ncontrol and quality assurance of the high-risk AI system;\n\n(d) examination, test and validation procedures to be carried out before, during and after\nthe development of the high-risk AI system, and the frequency with which they have to be\ncarried out;\n\n(e) technical specifications, including standards, to be applied and, where the relevant\nharmonised standards are not applied in full, the means to be used to ensure that the high-\nrisk AI system complies with the requirements set out in Chapter 2 of this Title;\n\n(f) systems and procedures for data management, including data collection, data analysis,\ndata labelling, data storage, data filtration, data mining, data aggregation, data retention\nand any other operation regarding the data that is performed before and for the purposes\nof the placing on the market or putting into service of high-risk AI systems;\n\n(g) the risk management system referred to in Article 9;\n\n(h) the setting-up, implementation and maintenance of a post-market monitoring system,\nin accordance with Article 61;\n\n(i) procedures related to the reporting of serious incidents and of malfunctioning in\naccordance with Article 62;\n\n(j) the handling of communication with national competent authorities, competent\nauthorities, including sectoral ones, providing or supporting the access to data, notified\nbodies, other operators, customers or other interested parties;\n\n(k) systems and procedures for record keeping of all relevant documentation and\ninformation;\n\n(1) resource management, including security of supply related measures;\n\n64 Td. art. 9, paras. 5-7.\n\n6 ]d. art. 9, para. 8.\n\n66 General Approach, supra note 5, at 90, art. 9, para. 6.\n67 EP, supra note 6, amendment 269.\n\n6 Draft AI Act, art. 16 (b), art. 17.\n\nThe Law Library of Congress 45\n", "vlm_text": "\nThe Council would require testing of the AI system in real world conditions.  The EP, among  other things, would like to see experts and external stakeholders involved to eliminate or reduce  risks   through adequate design and development.   \nF.   Quality Management System  \nProviders would be obligated to establish a quality management system, which should include,  among other things, security management aspects.  In particular, the quality management  system would have to include at least the following aspects:  \n(a) a strategy for regulatory compliance, including compliance with conformity assessment  procedures and procedures for the management of modifications to the high-risk AI  system;  (b) techniques, procedures and systematic actions to be used for the design, design control  and design verification of the high-risk AI system;  (c) techniques, procedures and systematic actions to be used for the development, quality  control and quality assurance of the high-risk AI system;  (d) examination, test and validation procedures to be carried out before, during and after  the development of the high-risk AI system, and the frequency with which they have to be  carried out;  (e) technical specifications, including standards, to be applied and, where the relevant  harmonised standards are not applied in full, the means to be used to ensure that the high- risk AI system complies with the requirements set out in Chapter 2 of this Title;  (f) systems and procedures for data management, including data collection, data analysis,  data labelling, data storage, data filtration, data mining, data aggregation, data retention  and any other operation regarding the data that is performed before and for the purposes  of the placing on the market or putting into service of high-risk AI systems;  (g) the risk management system referred to in Article 9;  (h) the setting-up, implementation and maintenance of a post-market monitoring system,  in accordance with Article 61;  (i) procedures related to the reporting of serious incidents and of malfunctioning in  accordance with Article 62;  (j) the handling of communication with national competent authorities, competent  authorities, including sectoral ones, providing or supporting the access to data, notified  bodies, other operators, customers or other interested parties;  (k) systems and procedures for record keeping of all relevant documentation and  information;  (l) resource management, including security of supply related measures;  "}
{"page": 48, "image_path": "page_images/2023555908_48.jpg", "ocr_text": "(m) an accountability framework setting out the responsibilities of the management and\nother staff with regard to all aspects listed in this paragraph.\n\nG. Robustness\n\nAll high-risk AI systems would need to be “designed and developed in such a way that they\nachieve, in the light of their intended purpose, an appropriate level of accuracy, robustness, and\ncybersecurity, and perform consistently in those respects throughout their lifecycle.”” In\nparticular, they should be resilient with regard to errors, faults, or inconsistencies that may occur\nwithin the system or the environment in which the system operates and with regard to attempts\nby unauthorized third parties to alter their use or performance by exploiting the system\nvulnerabilities.71 Robustness of high-risk AI systems could be achieved through technical\nredundancy solutions, such as backup or fail-safe plans.” Feedback loops would have to be duly\naddressed with appropriate mitigation measures.”\n\nFurthermore, technical solutions to achieve cybersecurity would be required to be “appropriate\nto the relevant circumstances and the risks” and include “measures to prevent and control for\nattacks trying to manipulate the training dataset (‘data poisoning’), inputs designed to cause the\nmodel to make a mistake (‘adversarial examples’), or model flaws.”74\n\nThe EP, among other changes, emphasizes that the AI systems would have to be designed and\ndeveloped following the principle of security by design and by default using state-of-the-art\nmeasures.’ It also adds that technical solutions should address measures to prevent trying to\nmanipulate pre-trained components used in training (“model poisoning”) or confidentiality\nattacks.76\n\nH. Conformity Assessments\n\nConformity assessments to evaluate that the AI system complies with all requirements, including\ncybersecurity, are necessary “to ensure a high level of trustworthiness of high-risk Al systems.”””\nAs mentioned, the proposal states that AI systems classified as high-risk may only be placed on\nthe market if they have undergone an ex-ante conformity assessment showing that they comply\nwith the requirements set out above.’® The provider must subsequently draw up an EU\n\n69d. art. 17, para. 1.\n\n70 Id. art. 15, para. 1.\n\n71 |d. art. 15, paras. 3, 4.\n\n72 \\d. art. 15, para. 3.\n\n73 Id.\n\n74 Td. art. 15, para. 4.\n\n75 EP, supra note 6, amendment 321.\n76 Id. amendment 329.\n\n77 Draft AI Act, recital 62.\n\n78 Id. arts. 3(20), 19, 43.\n\n", "vlm_text": "(m) an accountability framework setting out the responsibilities of the management and  other staff with regard to all aspects listed in this paragraph.   \nG.   Robustness  \nAll high-risk AI systems would need to be “designed and developed in such a way that they  achieve, in the light of their intended purpose, an appropriate level of accuracy, robustness, and  cybersecurity, and perform consistently in those respects throughout their lifecycle.” 70  In  particular, they should be resilient with regard to errors, faults, or inconsistencies that may occur  within the system or the environment in which the system operates and with regard to attempts  by unauthorized third parties to alter their use or performance by exploiting the system  vulnerabilities.  Robustness of high-risk AI systems could be achieved through technical  redundancy solutions, such as backup or fail-safe plans.  Feedback loops would have to be duly  addressed with appropriate mitigation measures.    \nFurthermore, technical solutions to achieve cybersecurity would be required to be “appropriate  to the relevant circumstances and the risks” and include “measures to prevent and control for  attacks trying to manipulate the training dataset (‘data poisoning’), inputs designed to cause the  model to make a mistake (‘adversarial examples’), or model flaws.” 74   \nThe EP, among other changes, emphasizes that the AI systems would have to be designed and  developed following the principle of security by design and by default using state-of-the-art  measures.  It also adds that technical solutions should address measures to prevent trying to  manipulate pre-trained components used in training (“model poisoning”) or confidentiality  attacks.   \nH.   Conformity Assessments  \nConformity assessments to evaluate that the AI system complies with all requirements, including  cybersecurity, are necessary “to ensure a high level of trustworthiness of high-risk AI systems.” 77   As mentioned, the proposal states that AI systems classified as high-risk may only be placed on  the market if they have undergone an ex-ante conformity assessment showing that they comply  with the requirements set out above.  The provider must subsequently draw up an EU  declaration of conformity and affix the conformité européenne (CE) marking of conformity to  demonstrate compliance.  Adherence to harmonized standards or common specifications  adopted by the Commission may be used to show compliance.  AI systems intended to be used  as a safety component of a product or for biometric identification and categorization of natural  persons would be subject to third party conformity assessment by independent notified bodies,  whereas all other high-risk AI systems listed in the annex would be subject to internal control  conformity assessments by the providers.  Systems would have to be reassessed after substantial  modifications.  As mentioned, high-risk AI systems that have been certified or for which a  statement of conformity has been issued under a cybersecurity scheme according to the CSA  would be deemed to be in compliance with the cybersecurity requirements set out in the draft  AI Act.   "}
{"page": 49, "image_path": "page_images/2023555908_49.jpg", "ocr_text": "declaration of conformity and affix the conformité européenne (CE) marking of conformity to\ndemonstrate compliance.7? Adherence to harmonized standards or common specifications\nadopted by the Commission may be used to show compliance.*? AI systems intended to be used\nas a safety component of a product or for biometric identification and categorization of natural\npersons would be subject to third party conformity assessment by independent notified bodies,\nwhereas all other high-risk AI systems listed in the annex would be subject to internal control\nconformity assessments by the providers.’! Systems would have to be reassessed after substantial\nmodifications.’ As mentioned, high-risk AI systems that have been certified or for which a\nstatement of conformity has been issued under a cybersecurity scheme according to the CSA\nwould be deemed to be in compliance with the cybersecurity requirements set out in the draft\nAl Act.83\n\nI. Personal Data Protection\n\nThe protection of personal data and the respect for private life are fundamental rights in the EU.*4\nThe GDPR defines personal data as “any information relating to an identified or identifiable\nnatural person (data subject).”8 As a regulation, the GDPR is directly applicable in the EU\nMember States with generally no domestic implementing legislation needed.** Processing of\npersonal data according to the GDPR must comply with the principles of lawfulness, fairness,\nand transparency; purpose limitation; data minimization; accuracy and keeping data up to date;\nstorage limitation; and integrity and confidentiality.8” In particular, integrity and confidentiality\nmeans that data must be “processed in a manner that ensures appropriate security of the personal\ndata, including protection against unauthorised or unlawful processing and against accidental\nloss, destruction or damage, using appropriate technical or organisational measures.”*® Security\nof personal data is therefore a prerequisite for processing of personal data.\n\nArticle 25 employs the principle of security by design and by default. It requires controllers to\nimplement appropriate technical and organizational measures, such as pseudonymization, to\nimplement data-protection principles effectively, taking into account, among other things, the\nstate of the art (security by design).’° Furthermore, controllers must implement appropriate\n\n79 |d. art. 19, para. 1, arts. 48, 49.\n\n80 Td. arts. 40, 41.\n\n81 Td. arts. 33, 43, recitals 64, 65.\n\n82 Td. art. 43, para. 4.\n\n83 Id. art. 42, para. 2. See also above part II.A.2.\n\n84 Charter of Fundamental Rights of the European Union (EU Charter) arts. 7, 8, 2012 O.J. (C 326) 391,\nhttps:/ / perma.cc/ PAX8-4MYJ; TFEU, art. 16, para. 1.\n\n85 GDPR, art. 4, point (1).\n86 TFEU, art. 288, para. 2.\n\n87 GDPR, art. 5, para. 1. For a more detailed overview, see Jenny Gesley, Online Privacy Law (2017 Update):\nEuropean Union (Law Library of Congress, Dec. 2017), https:/ / perma.cc/ BE4N-ACRQ.\n\n88 GDPR, art. 5, para. 1(f).\n89 Td. art. 25, para. 1.\n", "vlm_text": "\nI.   Personal Data Protection  \nThe protection of personal data and the respect for private life are fundamental rights in the EU.   The GDPR defines personal data as “any information relating to an identified or identifiable  natural person (data subject).  $\\prime\\prime85$   As a regulation, the GDPR is directly applicable in the EU  Member States with generally no domestic implementing legislation needed.  Processing of  personal data according to the GDPR must comply with the principles of lawfulness, fairness,  and transparency; purpose limitation; data minimization; accuracy and keeping data up to date;  storage limitation; and integrity and confidentiality.  In particular, integrity and confidentiality  means that data must be “processed in a manner that ensures appropriate security of the personal  data, including protection against unauthorised or unlawful processing and against accidental  loss, destruction or damage, using appropriate technical or organisational measures.” 88  Security  of personal data is therefore a prerequisite for processing of personal data.  \nArticle 25 employs the principle of security by design and by default. It requires controllers to  implement appropriate technical and organizational measures, such as pseudonym iz ation, to  implement data-protection principles effectively, taking into account, among other things, the  state of the art (security by design).  Furthermore, controllers must implement appropriate  technical and organizational measures to ensure that only personal data which are necessary for  each specific purpose of the processing are processed (security by default).  In particular,  personal data must by default not be made accessible to an indefinite number of people without  consent of the data subject.   "}
{"page": 50, "image_path": "page_images/2023555908_50.jpg", "ocr_text": "technical and organizational measures to ensure that only personal data which are necessary for\neach specific purpose of the processing are processed (security by default).% In particular,\npersonal data must by default not be made accessible to an indefinite number of people without\nconsent of the data subject.”\n\nA 2020 report on AI cybersecurity challenges by ENISA observed that\n\n[t]o this end, security can also be an enabler of new types of processing operations,\nespecially related to emerging technologies, such as AI. For instance, the implementation\nof specific security measures, like pseudonymisation or encryption, may bring data to a\nnew format so that it cannot be attributed to a specific data subject without the use of\nadditional information data (like a decryption key). These options could be explored in the\ncontext of AI environment, to shape new relationships between humans and machines, in\na way that individuals are not by default identifiable by machines unless they wish to do\nso. For instance, to revert the effect of the implemented pseudonymisation or encryption.\n\nChapter IV, section 2 of the GDPR is entitled “security of personal data” and details measures\nthat must be taken to ensure a level of security appropriate to the risk and requirements for\nnotification of a data breach. In particular, article 32 provides that depending on the anticipated\nrisks for the rights and freedoms of natural persons, the controller and the processor must\nimplement appropriate technical and organizational measures, such as\n\na) the pseudonymisation and encryption of personal data;\n\nb) the ability to ensure the ongoing confidentiality, integrity, availability and resilience\nof processing systems and services;\n\nc) the ability to restore the availability and access to personal data in a timely manner in\nthe event of a physical or technical incident; and\n\nd) a process for regularly testing, assessing and evaluating the effectiveness of technical\nand organisational measures for ensuring the security of the processing.\n\nTo assess the degree of risk, the controller and processor must take into account risks resulting\nfrom accidental or unlawful destruction, loss, alteration, unauthorized disclosure of, or access to\npersonal data transmitted, stored or otherwise processed. Adherence to an approved code of\nconduct or certification mechanism proves compliance with the above-mentioned requirements.\n\nIf there is a breach of security leading to the accidental or unlawful destruction, loss, alteration,\nunauthorized disclosure of, or access to, personal data transmitted, stored, or otherwise\nprocessed, the data controller has an obligation to notify the supervisory authority without undue\n\n9% Id. art. 25, para. 2.\nId.\n\n% ENISA, Artificial Intelligence Cybersecurity Challenges. Threat Landscape for Artificial Intelligence 9 (Dec. 15, 2020),\nhttps:/ / perma.cc/ H996-7KCA.\n\n%3 GDPR, art. 32, para. 1.\n4 Td. art. 32, para. 2.\n% Id. art. 32, para. 3, arts. 40, 42.\n", "vlm_text": "\nA 2020 report on AI cybersecurity challenges by ENISA observed that   \n[t]o this end, security can also be an enabler of new types of processing operations,  especially related to emerging technologies, such as AI. For instance, the implementation  of specific security measures, like pseudonym is ation or encryption, may bring data to a  new format so that it cannot be attributed to a specific data subject without the use of  additional information data (like a decryption key). These options could be explored in the  context of AI environment, to shape new relationships between humans and machines, in  a way that individuals are not by default identifiable by machines unless they wish to do  so. For instance, to revert the effect of the implemented pseudonym is ation or encryption.   \nChapter IV, section 2 of the GDPR is entitled “security of personal data” and details measures  that must be taken to ensure a level of security appropriate to the risk and requirements for  notification of a data breach. In particular, article 32 provides that depending on the anticipated  risks for the rights and freedoms of natural persons, the controller and the processor must  implement appropriate technical and organizational measures, such as  \na)   the pseudonym is ation and encryption of personal data;  b)   the ability to ensure the ongoing confidentiality, integrity, availability and resilience  of processing systems and services;  c)   the ability to restore the availability and access to personal data in a timely manner in  the event of a physical or technical incident; and  d)   a process for regularly testing, assessing and evaluating the effectiveness of technical  and organisational measures for ensuring the security of the processing.   \nTo assess the degree of risk, the controller and processor must take into account risks resulting  from accidental or unlawful destruction, loss, alteration, unauthorized disclosure of, or access to  personal data transmitted, stored or otherwise processed.  Adherence to an approved code of  conduct or certification mechanism proves compliance with the above-mentioned requirements.   \nIf there is a breach of security leading to the accidental or unlawful destruction, loss, alteration,  unauthorized disclosure of, or access to, personal data transmitted, stored, or otherwise  processed, the data controller has an obligation to notify the supervisory authority without undue  delay.  The data subject must also be informed if the breach is likely to result in a high risk to the  rights and freedoms of natural persons.  Failure to provide notification of a breach may result in  administrative fines. There are two tiers of fines, depending on the nature of the breach. Fines are  either up to €10 million (about US\\$11 million), or in the case of an undertaking, up to  $2\\%$   of the  total worldwide annual turnover of the preceding financial year, whichever is higher, such as  when a data breach is not notified, or up to €20 million (about   $\\mathrm{US}\\S22$   million) or up to  $4\\%$   of the  total worldwide annual turnover, whichever is higher, such as when the basic principles for  processing (such as security of processing) are violated.   "}
{"page": 51, "image_path": "page_images/2023555908_51.jpg", "ocr_text": "delay.% The data subject must also be informed if the breach is likely to result in a high risk to the\nrights and freedoms of natural persons.*” Failure to provide notification of a breach may result in\nadministrative fines. There are two tiers of fines, depending on the nature of the breach. Fines are\neither up to €10 million (about US$11 million), or in the case of an undertaking, up to 2% of the\ntotal worldwide annual turnover of the preceding financial year, whichever is higher, such as\nwhen a data breach is not notified, or up to €20 million (about US$22 million) or up to 4% of the\ntotal worldwide annual turnover, whichever is higher, such as when the basic principles for\nprocessing (such as security of processing) are violated.\n\nV. Adherence to Standardized Risk Management Frameworks\n\nAs mentioned, compliance with the requirements for high-risk AI systems can be shown by\nadhering to harmonized standards or common specifications.” Recital 61 of the draft AI Act\nelaborates that\n\n[s]tandardisation should play a key role to provide technical solutions to providers to\nensure compliance with this Regulation. Compliance with harmonised standards as\ndefined in Regulation (EU) No 1025/2012 of the European Parliament and of the Council\n[Standardization Regulation] should be a means for providers to demonstrate conformity\nwith the requirements of this Regulation. However, the Commission could adopt common\ntechnical specifications in areas where no harmonised standards exist or where they are\ninsufficient.10\n\nThe EP in its amendment elaborates on the role of standard setting organizations with regard to\nensuring accuracy, robustness, and cybersecurity and states that “[w]hile standardisation\norganisations exist to establish standards, coordination on benchmarking is needed to establish\nhow these standardised requirements and characteristics of AI systems should be measured.”1\nIn addition, it adds that\n\nproviders that have already in place quality management systems based on standards such\nas ISO 9001 or other relevant standards, no duplicative quality management system in full\nshould be expected but rather an adaptation of their existing systems to certain aspects\nlinked to compliance with specific requirements of this Regulation. This should also be\nreflected in future standardization activities or guidance adopted by the Commission in\nthis respect.102\n\n96 Id. arts. 4(12), 33.\n\n97 Id. art. 34.\n\n%8 Id. art. 83, paras. 4, 5.\n\n°9 Draft AI Act, arts. 40, 41.\n\n100 Jd. recital 61; Consolidated Version of the Standardization Regulation, 2012 O.J. (L 316) 12,\nhttps:/ / perma.cc/7NR3-DBYA.\n\n101 EP, supra note 6, amendment 85.\n\n102 Td. amendment 89.\n", "vlm_text": "\nV.  Adherence to Standardized Risk Management Frameworks  \nAs mentioned, compliance with the requirements for high-risk AI systems can be shown by  adhering to harmonized standards or common specifications.  Recital 61 of the draft AI Act  elaborates that   \n[s]tandardisation should play a key role to provide technical solutions to providers to  ensure compliance with this Regulation. Compliance with harmonised standards as  defined in Regulation (EU) No 1025/2012 of the European Parliament and of the Council  [Standardization Regulation] should be a means for providers to demonstrate conformity  with the requirements of this Regulation. However, the Commission could adopt common  technical specifications in areas where no harmonised standards exist or where they are  insufficient.   \nThe EP in its amendment elaborates on the role of standard setting organizations with regard to  ensuring accuracy, robustness, and cybersecurity and states that “[w]hile standardisation  organisations exist to establish standards, coordination on benchmarking is needed to establish  how these standardised requirements and characteristics of AI systems should be measured.  ${\\prime\\prime}_{101}$    In addition, it adds that   \nproviders that have already in place   quality management systems based on standards such  as ISO 9001 or other relevant standards, no duplicative quality management system in full  should be expected but rather an adaptation of their existing systems to certain aspects  linked to compliance with specific requirements   of this Regulation. This should also be  reflected in future standardization activities or guidance adopted by the Commission in  this respect.   "}
{"page": 52, "image_path": "page_images/2023555908_52.jpg", "ocr_text": "The EP also emphasizes that standardization requests from the Commission to European\nStandardization Organizations must specify that they have to be consistent and aimed at ensuring\nthat AI systems or foundation models meet the requirements of the draft AI Act, as well as ensure\na balanced representation of all interests in developing the standards.10\n\nENISA recommends, among other things, that specific/technical guidance on how existing\nstandards related to the cybersecurity of software should be applied to AI is developed, the\ninherent features of machine learning are reflected in standards, and to include potential\ncybersecurity concerns in AI standards on trustworthiness by establishing liaisons between\ncybersecurity technical committees and AI technical committees.1%\n\nVI. AI Security Policy Across the Supply Chain\nENISA notes that\n\n[t]he ENISA AI Threat Landscape not only lays the foundation for upcoming cybersecurity\npolicy initiatives and technical guidelines, but also stresses relevant challenges. One area\nof particular significance is that of the supply chain related to AI and accordingly it is\nimportant to highlight the need for an EU ecosystem for secure and trustworthy AI,\nincluding all elements of the AI supply chain. The EU secure AI ecosystem should place\ncybersecurity and data protection at the forefront and foster relevant innovation, capacity-\nbuilding, awareness raising and research and development initiatives.1%\n\nThe draft AI Act places obligations on various economic operators, including distributors. They\nare defined as “any natural or legal person in the supply chain, other than the provider or the\nimporter, that makes an AI system available on the Union market without affecting its\nproperties.” 106 Article 27 discusses the obligations of distributors in detail, in particular they must\nverify that high-risk AI systems bear the conformity marking and that the provider and importer\nhave complied with the requirements of chapter 2 of the draft AI Act.1°7\n\nIn addition, if national market surveillance authorities in the EU Member States find that a\nparticular Al system poses a risk to the health or safety of persons even though it complies with\nthe draft AI Act, they must inform the Commission and include, among other things, the origin\nand the supply chain of the AI system.1°8\n\n103 Tq, amendments 439, 440.\n104 ENISA, supra note 39, at 25.\n105 ENISA, supra note 92, at 5.\n106 Draft AI Act, art. 3(7).\n\n107 Td. art. 27, para. 1.\n\n108 Td. art. 67, paras. 1, 3.\n", "vlm_text": "The EP also emphasizes that standardization requests from the Commission to European  Standardization Organizations must specify that they have to be consistent and aimed at ensuring  that AI systems or foundation models meet the requirements of the draft AI Act, as well as ensure  a balanced representation of all interests in developing the standards.   \nENISA recommends, among other things, that specific/technical guidance on how existing  standards related to the cybersecurity of software should be applied to AI is developed, the  inherent features of machine learning are reflected in standards, and to include potential  cybersecurity concerns in AI standards on trustworthiness by establishing liaisons between  cybersecurity technical committees and AI technical committees.   \nVI.  AI Security Policy Across the Supply Chain  \nENISA notes that  \n[t]he ENISA AI Threat Landscape not only lays the foundation for upcoming cybersecurity  policy initiatives and technical guidelines, but also stresses relevant challenges. One area  of particular significance is that of the supply chain related to AI and accordingly it is  important to highlight the need for an EU ecosystem for secure and trustworthy AI,  including all elements of the AI supply chain. The EU secure AI ecosystem should place  cybersecurity and data protection at the forefront and foster relevant innovation, capacity- building, awareness raising and research and development initiatives.   \nThe draft AI Act places obligations on various economic operators, including distributors. They  are defined as “any natural or legal person in the supply chain, other than the provider or the  importer, that makes an AI system available on the Union market without affecting its  properties.” 106  Article 27 discusses the obligations of distributors in detail, in particular they must  verify that high-risk AI systems bear the conformity marking and that the provider and importer  have complied with the requirements of chapter 2 of the draft AI Act.   \nIn addition, if national market surveillance authorities in the EU Member States find that a  particular AI system poses a risk to the health or safety of persons even though it complies with  the draft AI Act, they must inform the Commission and include, among other things, the origin  and the supply chain of the AI system.   "}
{"page": 53, "image_path": "page_images/2023555908_53.jpg", "ocr_text": "Safety and Security of Artificial Intelligence Systems: European Union\n\nThe more general NIS 2 Directive requires essential and important entities to take appropriate\nand proportionate technical, operational, and organizational cybersecurity risk management\nmeasures and to base them on an all-hazards approach.! Among other things, the measures\nmust include supply chain security, including security-related aspects concerning the\nrelationships between each entity and its direct suppliers or service providers.\n\n109 NIS 2 Directive, art. 21, para. 1.\n10 Td. art. 21, para. 2(d).\n\nThe Law Library of Congress\n\nol\n", "vlm_text": "The more general NIS 2 Directive requires essential and important entities to take appropriate  and proportionate technical, operational, and organizational cybersecurity risk management  measures and to base them on an all-hazards approach.  Among other things, the measures  must include supply chain security, including security-related aspects concerning the  relationships between each entity and its direct suppliers or service providers.   "}
{"page": 54, "image_path": "page_images/2023555908_54.jpg", "ocr_text": "SUMMARY\n\nNew Zealand\n\nKelly Buchanan\nChief, Foreign, Comparative, and\nInternational Law Division II\n\nNew Zealand does not currently have legislation that specifically addresses artificial\nintelligence (AI). However, the government and entities in the private sector,\nparticularly through the AI Forum of New Zealand, have undertaken various studies,\nreports, and discussions related to the development and regulation of AI. This includes\nan algorithm charter signed by various government agencies, and “trustworthy AI”\nprinciples and AI “cornerstones” developed by the AI Forum, as well as strategies and\nplans that reference AI. In addition, the privacy commissioner has published guidance\nfor entities on the use of generative AI.\n\nSeveral existing laws are relevant to the regulation of AI, including the Privacy Act 2020,\nconsumer protection law, legislation related to human rights, criminal law, and laws\nrelated to national security. The New Zealand Information Security Manual as well as\nlegislation and guidance on cyber resilience or security targeted at certain sectors are\nalso relevant to the cybersecurity of AI. In addition, New Zealand’s cybersecurity\nauthorities have worked with international partners to develop guidance on\ncybersecurity topics relevant to AI.\n\nI. Introduction\n\nNew Zealand does not have a dedicated law on artificial intelligence (Al), or specific provisions\nregarding AI in other legislation. The government is actively considering the issues, however,\nand various actions have been taken to promote and guide the use and development of AI in\n\nthe country.\n\nStarting in 2019, the New Zealand government partnered with the Centre for the Fourth\nIndustrial Revolution of the World Economic Forum (WEF) on work that aims to produce a\nroadmap to guide policymakers in regulating AI.1 The Reimagining Regulation for the Age of AI:\nNew Zealand Pilot Project white paper was published in June 2020.2 It describes “a multi-\nstakeholder, evidence-based policy project anchored in New Zealand,”? with different focus\nareas: national conversation, regulatory capability and institutional design, and risk/benefit\nassessment of AI systems for government. A government spokesperson stated,\n\n1 Alastair Farr, Reimagining Regulation in the Age of Artificial Intelligence (AI), Digital.govt.nz (Nov. 11, 2019),\nhttps:/ / perma.cc/ YFJ5-QG9P.\n\n2 World Economic Forum, Reimagining Regulation for the Age of Al: New Zealand Pilot Project - White Paper (June\n2020), https: / / perma.cc/ H2K3-M528.\n\n31d. at 24.\n", "vlm_text": "New Zealand  \nKelly Buchanan  Chief, Foreign, Comparative, and  International Law Division II   \nSUMMARY   New Zealand does not currently have legislation that specifically addresses artificial  intelligence (AI). However, the government and entities in the private sector,  particularly through the AI Forum of New Zealand, have undertaken various studies,  reports, and discussions related to the development and regulation of AI. This includes  an algorithm charter signed by various government agencies, and “trustworthy AI”  principles and AI “cornerstones” developed by the AI Forum, as well as strategies and  plans that reference AI. In addition, the privacy commissioner has published guidance  for entities on the use of generative AI.   \nSeveral existing laws are relevant to the regulation of AI, including the Privacy Act 2020,  consumer protection law, legislation related to human rights, criminal law, and laws  related to national security. The New Zealand Information Security Manual as well as  legislation and guidance on cyber resilience or security targeted at certain sectors are  also relevant to the cybersecurity of AI. In addition, New Zealand’s cybersecurity  authorities have worked with international partners to develop guidance on  cybersecurity topics relevant to AI.  \nI.  Introduction  \nNew Zealand does not have a dedicated law on artificial intelligence (AI), or specific provisions  regarding AI in other legislation. The government is actively considering the issues, however,  and various actions have been taken to promote and guide the use and development of AI in  the country.   \nStarting in 2019, the New Zealand government partnered with the Centre for the Fourth  Industrial Revolution of the World Economic Forum (WEF) on work that aims to produce a  roadmap to guide policymakers in regulating AI.  The  Reimagining Regulation for the Age of AI:  New Zealand Pilot Project  white paper was published in June 2020.  It describes “a multi- stakeholder, evidence-based policy project anchored in New Zealand,” 3  with different focus  areas: national conversation, regulatory capability and institutional design, and risk/benefit  assessment of AI systems for government. A government spokesperson stated,   "}
{"page": 55, "image_path": "page_images/2023555908_55.jpg", "ocr_text": "Safety and Security of Artificial Intelligence Systems: New Zealand\n\n[a]s a small country, New Zealand is seen by WEF as being an ideal test bed for this kind\nof agile thinking ... We have a small, stable democracy, with a government that can move\nquickly. We are well-connected, both internally, across our government and wider society,\nand we have strong relationships with other countries. We are seen as a leading\ndigital nation.*\n\nVarious work has been conducted in the public sector related to the use of AI by government\nagencies and the private sector, including the following:\n\nThe Algorithm Charter for Aotearoa New Zealand,> which was released in July 2020 “to increase\npublic confidence and visibility around the use of algorithms in the public sector.”® Charter\nsignatories are to assess their decisions about the use of algorithms using a risk matrix, and\nto implement a list of commitments related to transparency; partnership (i.e., embedding a Te\nAo Maori - Maori worldview - perspective); engaging with people; data considerations;\nprivacy, ethics, and human rights; and human oversight. The charter followed on from a 2018\nreview of how government agencies use algorithms,’ with the resulting report drawing on\nthe Principles for Safe and Effective Use of Data and Analytics,8 which were developed in 2018 by\nthe privacy commissioner and chief data steward. Other relevant guidance includes the Social\nWellbeing Agency’s Data Protection and Use Policy,? the Ministry of Social Development's\nPrivacy, Human Rights and Ethics Framework, and Nga Tikanga Paihere, a framework that\n“draws on 10 tikanga (Te Ao Maori - Maori world concepts) to help [entities] establish goals,\nboundaries, and principles that guide and inform [their] data practice.”\n\nIn 2018, the Human Rights Commission published a paper titled Privacy, Data and Technology:\nHuman Rights Challenges in the Digital Age, which provides “a high-level summary of the key\ninternational and domestic human rights standards and principles that can guide legal and\n\n4 Stuart Corner, How the NZ Government Will Regulate AI (Mar. 17, 2020), https:/ / perma.cc/2ZVV-HVR6.\n\n5 Stats NZ, Algorithm Charter for Aotearoa New Zealand (July 2020), https:/ / perma.cc/B43C-79DB. See Kelly\nBuchanan, New Zealand: Government Launches Charter on Use of Algorithms by Public Agencies (July 31, 2020),\nhttps:/ / perma.cc/7QVE-D9FY.\n\n6 Algorithm Charter for Aotearoa New Zealand, Data.govt.nz, https:/ / perma.cc/ A3XD-AX8Z.\n? Government Algorithm Transparency and Accountability, Data.govt.nz, https:/ /perma.cc/UZT2-W6CG.\n\n8 Privacy Commissioner & Stats NZ, Principles for the Safe and Effective Use of Data and Analytics (May 2018),\nhttps:/ / perma.cc/ YK8A-HUS8F.\n\n° Social Wellbeing Agency, Data Protection and Use Policy (DPUP) (version 1.2, Jan. 2022),\nhttps:/ / perma.cc/6HM9-6MBK.\n\n10 Ministry of Social Development, The Privacy, Human Rights and Ethics (PHRaE) Framework,\nhttps:/ / perma.cc/ BH5Z-PJF7.\n\n1 Nea Tikanga Paihere, Data.govt.nz, https:/ / perma.cc/ A6ZZ-MAVQ. See also Stats NZ, Nga Tikanga Paihere: A\n\nFramework Guiding Ethical and Culturally Appropriate Data Use - Guidelines 2020 (Dec. 2020),\nhttps:/ / perma.cc/C2VM-S3KZ.\n\nThe Law Library of Congress\n\nol\n\nlos)\n", "vlm_text": "[a]s a small country, New Zealand is seen by WEF as being an ideal test bed for this kind  of agile thinking . . . We have a small, stable democracy, with a government that can move  quickly. We are well-connected, both internally, across our government and wider society,  and we have strong relationships with other countries. We are seen as a leading  digital nation.   \nVarious work has been conducted in the public sector related to the use of AI by government  agencies and the private sector, including the following:  \n\n \n•   The  Algorithm Charter for Aotearoa New Zealand ,  which was released in July 2020 “to increase  public confidence and visibility around the use of algorithms in the public sector.” 6  Charter  signatories are to assess their decisions about the use of algorithms using a risk matrix, and  to implement a list of commitments related to transparency; partnership (i.e., embedding a Te  Ao Māori - Māori worldview – perspective); engaging with people; data considerations;  privacy, ethics, and human rights; and human oversight. The charter followed on from a 2018  review of how government agencies use algorithms,  with the resulting report drawing on  the  Principles for Safe and Effective Use of Data and Analytics ,   which were developed in 2018 by  the privacy commissioner and chief data steward. Other relevant guidance includes the Social  Wellbeing Agency’s  Data Protection and Use Policy ,  the Ministry of Social Development’s  Privacy, Human Rights and Ethics Framework ,  and  Ngā Tikanga Paihere , a framework that  “draws on 10 tikanga (Te Ao Māori - Māori world concepts) to help [entities] establish goals,  boundaries, and principles that guide and inform [their] data practice.” 11   \n\n •   In 2018, the Human Rights Commission published a paper titled  Privacy, Data and Technology:  Human Rights Challenges in the Digital Age , which provides “a high-level summary of the key  international and domestic human rights standards and principles that can guide legal and  policy frameworks in responding to the rapid advance of digital technology.  ${\\prime\\prime}_{12}$   The paper  includes specific discussion of AI, including its use in the criminal justice system.   \n\n "}
{"page": 56, "image_path": "page_images/2023555908_56.jpg", "ocr_text": "policy frameworks in responding to the rapid advance of digital technology.”!2 The paper\nincludes specific discussion of AI, including its use in the criminal justice system.!°\n\ne New Zealand’s innovation agency, Callaghan Innovation, published a white paper titled\nThinking Ahead: Innovation Through Artificial Intelligence in 2018.'4 The paper “predicts how AI\nwill affect our agriculture, digital, energy and health sectors within the next few years. It\ndetails how different AI technologies will disrupt each sector in waves and showcases local\nexamples of Al-powered businesses.”15\n\ne In 2020, the Productivity Commission published a research note on New Zealanders’ Attitudes\nTowards Robots and AI.'° It also considered matters related to algorithms and AI as part of its\ninquiry into technological change and the future of work,!’ and published a joint report with\nthe Australian Productivity Commission in 2019 on growing the digital economy in the two\ncountries, including the use of AI.!8\n\ne In May 2023, the privacy commissioner published guidance outlining “his expectations\naround New Zealand agencies, businesses, and organisations using generative artificial\nintelligence (AI).”19 This includes a statement that “[g]enerative AI tools, capabilities, and\ntheir impact are rapidly evolving. Regulators across the world are actively reviewing the\nsituation, and the Privacy Commissioner has called for New Zealand regulators to come\ntogether to determine how best to protect the rights of New Zealanders.”\n\ne The National Ethics Advisory Committee’s National Ethical Standards provide “general\nprinciples guiding the ethics of biomedicine as they apply to AI,” and “frame standards\napplying these principles to specific circumstances.”2! They state that “[a]ll researchers\nemploying health data in AI systems throughout the AI life cycle . .. should refer to the ethical\n\n?2 Human Rights Commission, Privacy, Data and Technology: Human Rights Challenges in the Digital Age 5 (May\n2018), https: / / perma.cc/Q79G-WA2K.\n\n13 Id. at 44-47.\n\n4 Callaghan Innovation, Thinking Ahead: Innovation Through Artificial Intelligence (2018),\nhttps:/ / perma.cc/ MJHH-D8WN.\n\n15 Waves of AI Disruption for Key New Zealand Business Sectors, Callaghan Innovation (Mar. 26, 2018),\nhttps:/ / perma.cc/FJ5G-5JJD.\n\n16 Dave Heatly, New Zealanders’ Attitudes Towards Robots and AI (Productivity Commission Research Note\n2020/1, Feb. 2020), https:/ / perma.cc/TN8N-MBL2.\n\n1? See Technological Change and the Future of Work, Productivity Commission, https:/ / perma.cc/ YGF6-5LTW.\n\n18 Australian Productivity Commission & New Zealand Productivity Commission, Growing the Digital Economy\nin Australia and New Zealand: Maximising Opportunities for SMEs: Joint Research Report (Jan. 2019),\nhttps:/ / perma.cc/ V4WE-KSK5.\n\n19 Press Release, Privacy Commissioner, Privacy Commissioner Outlines Expectations Around AI Use (May 25,\n2023), https:// perma.cc/K94Z-4XNG.\n\n20 Generative Artificial Intelligence - 15 June 2023 Update, Privacy Commissioner, https:/ / perma.cc/ LW7]-\n4WMN.\n\n21 National Ethics Standards - Part 2, 13. Health Data and New Technologies, National Ethics Advisory Committee,\nhttps:/ / perma.cc/ M75F-DTLN.\n", "vlm_text": "\n•   New Zealand’s innovation agency, Callaghan Innovation, published a white paper titled  Thinking Ahead: Innovation Through Artificial Intelligence  in 2018.  The paper “predicts how AI  will affect our agriculture, digital, energy and health sectors within the next few years. It  details how different AI technologies will disrupt each sector in waves and showcases local  examples of AI-powered businesses.” 15   \n\n •   In 2020, the Productivity Commission published a research note on  New Zealanders’ Attitudes  Towards Robots and AI .  It also considered matters related to algorithms and AI as part of its  inquiry into technological change and the future of work,  and published a joint report with  the Australian Productivity Commission in 2019 on growing the digital economy in the two  countries, including the use of AI.  \n\n •   In May 2023, the privacy commissioner published guidance outlining “his expectations  around New Zealand agencies, businesses, and organisations using generative artificial  intelligence (AI)  $^{\\prime\\prime}{}^{19}$   This includes a statement that “[g]enerative AI tools, capabilities, and  their impact are rapidly evolving. Regulators across the world are actively reviewing the  situation, and the Privacy Commissioner has called for New Zealand regulators to come  together to determine how best to protect the rights of New Zealanders.” 20  \n\n •   The National Ethics Advisory Committee’s  National Ethical Standards  provide “general  principles guiding the ethics of biomedicine as they apply to AI,” and “frame standards  applying these principles to specific circumstances.” 21  They state that “[a]ll researchers  employing health data in AI systems throughout the AI life cycle . . . should refer to the ethical  principles . . . in the absence of a standard that directly applies to their case.” 22  The  government announced funding in August 2022 for three international research projects, led  by research teams in New Zealand, to explore how AI could provide improved access to  health care.   \n\n "}
{"page": 57, "image_path": "page_images/2023555908_57.jpg", "ocr_text": "Safety and Security of Artificial Intelligence Systems: New Zealand\n\nprinciples . . . in the absence of a standard that directly applies to their case.”?2 The\ngovernment announced funding in August 2022 for three international research projects, led\nby research teams in New Zealand, to explore how AI could provide improved access to\nhealth care.\n\ne Under the banner of the Christchurch Call to Action,“4 New Zealand is working with the\nUnited States, Twitter, and Microsoft on an initiative that will “support the creation of new\ntechnology to understand the impacts of algorithms on people’s online experiences.” 5\n\ne The Royal Society, which promotes research and scholarly activity, has published a report\ntitled The Age of Artificial Intelligence in Aotearoa New Zealand, which “outlines what artificial\nintelligence is, how it is or could be used in Aotearoa New Zealand, and the risks that need\nto be managed so that all New Zealanders can prosper in an AI world.”\n\nIn academia, the University of Otago established the Centre for AI and Public Policy in 2018.2”\nThe center particularly examines policy options for New Zealand and “acts in an advisory\ncapacity to the Ministries of Broadcasting, Communications and Digital Media, and Government\nDigital Services.”\n\nThe New Zealand Law Foundation and the University of Otago published a report on Government\nUse of Artificial Intelligence in New Zealand in 2019, as part of a project on artificial intelligence\nand law.39\n\nThe Artificial Intelligence Researchers Association published a paper in 2021 that discusses\ncurrent AI capabilities in New Zealand and makes recommendations for establishing the country\n\n22 Td.\n\n23 $5.1 Million for Research Into Artificial Intelligence to Improve Health Outcomes, Ministry of Business, Innovation\n& Employment (MBIE) (Aug. 19, 2022), https:/ / perma.cc/9QG6-WYHH.\n\n24 See Christchurch Call Story, Christchurch Call, https: / / perma.cc/ RVK8-835K.\n\n25 Christchurch Call Initiative on Algorithmic Outcomes, Christchurch Call (Sept. 22, 2022),\nhttps:/ / perma.cc/7AL3-F3V2.\n\n26 Royal Society, The Age of Artificial Intelligence in Aotearoa, https:/ / perma.cc/ HNB5-MBYH.\n\n27 Researching the Costs and Benefits of Artificial Intelligence, Centre for Artificial Intelligence and Public Policy,\nUniversity of Otago, https://perma.cc/8TT9-3XXX.\n\n28 Id.\n\n29 New Zealand Law Foundation & University of Otago, Government Use of Artificial Intelligence in New Zealand\n(2019), https:// perma.cc/ Y49N-CPWA.\n\n3° Major New Otago Study to Tackle Artificial Intelligence Law and Policy, University of Otago (Jan. 19, 2017),\nhttps:/ / perma.cc/S7SD-PW86; Artificial Intelligence and Law in New Zealand: Outputs, University of Otago &\nNew Zealand Law Foundation, https://perma.cc/9H9J-T48E.\n\nO1\na\n\nThe Law Library of Congress\n", "vlm_text": "\n•   Under the banner of the Christchurch Call to Action,  New Zealand is working with the  United States, Twitter, and Microsoft on an initiative that will “support the creation of new  technology to understand the impacts of algorithms on people’s online experiences.” 25  \n\n •   The Royal Society, which promotes research and scholarly activity, has published a report  titled  The Age of Artificial Intelligence in Aotearoa New Zealand , which “outlines what artificial  intelligence is, how it is or could be used in Aotearoa New Zealand, and the risks that need  to be managed so that all New Zealanders can prosper in an AI world.” 26   \nIn academia, the University of Otago established the Centre for AI and Public Policy in 2018. The center particularly examines policy options for New Zealand and “acts in an advisory  capacity to the Ministries of Broadcasting, Communications and Digital Media, and Government  Digital Services. ” 28    \nThe New Zealand Law Foundation and the University of Otago published a report on  Government  Use of Artificial Intelligence in New Zealand  in 2019,  as part of a project on artificial intelligence  and law.    \nThe Artificial Intelligence Researchers Association published a paper in 2021 that discusses  current AI capabilities in New Zealand and makes recommendations for establishing the country  as “a research centre of excellence and trust in AI.” 31  More recently, in March 2023, it published a  discussion paper on the implications of ChatGPT and large language models for policy makers.   "}
{"page": 58, "image_path": "page_images/2023555908_58.jpg", "ocr_text": "as “a research centre of excellence and trust in AI.”3! More recently, in March 2023, it published a\ndiscussion paper on the implications of ChatGPT and large language models for policy makers.32\n\nIn terms of private sector self-regulation, the Artificial Intelligence Forum of New Zealand (AI\nForum) is a nonprofit organization, funded by its members, that “brings together New Zealand’s\ncommunity of artificial intelligence technology innovators, end users, investor groups, regulators,\nresearchers, educators, entrepreneurs and interested public to work together to find ways to use\nAI to help enable a prosperous, inclusive and thriving future for our nation.”*3 The New Zealand\ngovernment provided foundational support for the forum to undertake research, leading to a\n2018 report titled Artificial Intelligence: Shaping a Future New Zealand.** Among its other endeavors,\nin 2020 the AI Forum published a set of guiding principles for Trustworthy AI in Aotearoa New\nZealand (AI Principles).35 The AI Forum states,\n\n[t]he AI Principles are designed to assist everyone in the AI ecosystem, including in both\nthe private and public sectors. However, we recognise that Government has additional\nobligations and a broader role to play in ensuring AI and other emerging technologies\nserve the long-term public good of New Zealand, including in meeting its obligations\nunder Te Tiriti o Waitangi.\n\nGovernment regulation and regulators have an important role to play here. Self-regulation\nin the form of ethical principles or standards may fill a gap where the law is incomplete or\nout of date, but they are no substitution for democratically-mandated rules backed up by\nthe force of law.%6\n\nII. Overview of the Legal and Policy Framework\n\nThere is not currently any draft or proposed legislation to regulate AI. In 2020, it was reported\nthat “[t]he New Zealand Government plans to regulate the use of artificial intelligence (Al)\nalgorithms by progressively incorporating AI controls into existing regulations and legislation as\nthey are amended and updated, rather than having any specific regulation to control the use\nof AI.”37\n\n31 Homepage, Artificial Intelligence Researchers Association, https:/ /perma.cc/ W8LB-VGLC. See also Artificial\nIntelligence Researchers Association, White Paper: Aotearoa New Zealand Artificial Intelligence - A Strategic\nApproach (Nov. 2021), https://perma.cc/8ZG4-QEAD.\n\n2 Artificial Intelligence Researchers Association, Discussion Paper: ChatGPT & Large Language Models - What Are\nthe Implications for Policy Makers? (Mar. 2023), https:/ / perma.cc/ BU4F-7L9F.\n\n33 About - Harnessing the Power of AI to Enable a Prosperous, Inclusive and Thriving Future New Zealand, AI Forum\nNew Zealand (2022), https:/ /perma.cc/S8M8-YWNK.\n\n34 AI Forum New Zealand, Artificial Intelligence: Shaping a Future New Zealand (May 2018),\nhttps:/ / perma.cc/ LZE6-CXYZ.\n\n35 Trustworthy AI in Aotearoa - The AI Principles AI Forum New Zealand (Mar. 4, 2020), https:/ / perma.cc/P76N-\nJHQV.\n\n3¢ AI Forum New Zealand, Trustworthy Al in Aotearoa: AI Principles 2 (Mar. 2020), https:/ / perma.cc/Q9FR-\nP4ZN.\n\n3? Corner, supra note 4.\n", "vlm_text": "\nIn terms of private sector self-regulation, the Artificial Intelligence Forum of New Zealand (AI  Forum) is a nonprofit organization, funded by its members, that “brings together New Zealand’s  community of artificial intelligence technology innovators, end users, investor groups, regulators,  researchers, educators, entrepreneurs and interested public to work together to find ways to use  AI to help enable a prosperous, inclusive and thriving future for our nation.” 33  The New Zealand  government provided foundational support for the forum to undertake research, leading to a  2018 report titled  Artificial Intelligence: Shaping a Future New Zealand .  Among its other endeavors,  in 2020 the AI Forum published a set of guiding principles for  Trustworthy AI in Aotearoa New  Zealand  (AI Principles).  The AI Forum states,  \n[t]he AI Principles are designed to assist everyone in the AI ecosystem, including in both  the private and public sectors. However, we recognise that Government has additional  obligations and a broader role to play in ensuring AI and other emerging technologies  serve the long-term public good of New Zealand, including in meeting its obligations  under Te Tiriti o Waitangi.  \nGovernment regulation and regulators have an important role to play here. Self-regulation  in the form of ethical principles or standards may fill a gap where the law is incomplete or  out of date, but they are no substitution for democratically-mandated rules backed up by  the force of law.   \nII.  Overview of the Legal and Policy Framework  \nThere is not currently any draft or proposed legislation to regulate AI. In 2020, it was reported  that “[t]he New Zealand Government plans to regulate the use of artificial intelligence (AI)  algorithms by progressively incorporating AI controls into existing regulations and legislation as  they are amended and updated, rather than having any specific regulation to control the use  of AI.” 37   "}
{"page": 59, "image_path": "page_images/2023555908_59.jpg", "ocr_text": "Existing general legislation that may be relevant to regulating AI, including in relation to\ncybersecurity, include the Privacy Act 2020,33 Harmful Digital Communications Act 2015\n(removal of harmful online content),39 Fair Trading Act 1986 (consumer protection law),!°\nHuman Rights Act 1993 (anti-discrimination law),41 New Zealand Bill of Rights Act 1990 (freedom\nfrom unreasonable search and seizure, etc.),42 Crimes Act 1961 (cybercrime offenses),°\nTelecommunications (Interception Capability and Security) Act 2013 (network security duties) ,“4\nand the Intelligence and Security Act 2017 (agency responsibilities for cybersecurity).\n\nIn May 2023, the Ministry of Business, Innovation & Employment (MBIE) published the Digital\nTechnologies Industry Transformation Plan. This document refers to the development of an AI\nstrategy, stating,\n\n[s]ome initial work was developed by MBIE in conjunction with the AI Forum, on draft\n“cornerstones” to underpin a future AI Strategy for Aotearoa New Zealand. These\nincluded the aim that all AI innovation and adoption across New Zealand is done safely\nand ethically, with the full trust and support of New Zealanders. Future work could look\nto advance development of an AI Strategy that helps New Zealand leverage the economic\nopportunities of this technology in a trustworthy way.‘\n\nThe AI Forum’s draft “cornerstones” were published in 2021. They describe six key areas that\n“together, provide a framework for action, setting our priorities for AI in New Zealand.”4” The\ncornerstones are headed “uniquely New Zealand,” “human-centred and trusted AI,”\n“investment in the AI economy,” “preparing the workforce,” and “our place in the world.”4\n\nThe transformation plan also references the Digital Strategy for Aotearoa (DSA), which was\npublished in September 2022. The relevant minister’s foreword to that strategy states,\n\nNew Zealand stands on the precipice of a huge opportunity to design, build and use digital\ntechnologies in world-leading, ethical, equitable ways that reflect the culture and\nuniqueness of our country.\n\n38 Privacy Act 2020, https:/ /perma.cc/UQ39-RZSA.\n\n39 Harmful Digital Communications Act 2015, https:// perma.cc/7M2A-DLMQ.\n\n40 Fair Trading Act 1986, https:/ / perma.cc/3F6Y-BEEU.\n\n#1 Human Rights Act 1998, https: // perma.cc/XA7S-4HL4.\n\n#2 New Zealand Bill of Rights Act 1990, https://perma.cc/PT9Q-UYYD.\n\n#3 Crimes Act 1961 ss 248-252, https:/ / perma.cc/ WAE7-RRQ8.\n\n#4 Telecommunications (Interception Capability and Security) Act 2013, https://perma.cc/ YL3J-AEZQ.\n4 Intelligence and Security Act 2017, https:/ / perma.cc/ PVT7-BQPH.\n\n46 MBIE, Digital Technologies Industry Transformation Plan 32 (May 2023), https:/ / perma.cc/K297-CKU7.\n\n47 Introducing Aotearoa’s Proposed AI Cornerstones, Al Forum New Zealand (Apr. 29, 2021),\nhttps:/ / perma.cc/J5W6-TGQ5.\n\n481d.\n", "vlm_text": "Existing general legislation that may be relevant to regulating AI, including in relation to  cybersecurity, include the Privacy Act 2020,  Harmful Digital Communications Act 2015  (removal of harmful online content),   Fair Trading Act 1986 (consumer protection law), Human Rights Act 1993 (anti-discrimination law),  New Zealand Bill of Rights Act 1990 (freedom  from unreasonable search and seizure, etc.),  Crimes Act 1961 (cybercrime offenses),   Telecommunications (Interception Capability and Security) Act 2013 (network security duties),   and the Intelligence and Security Act 2017 (agency responsibilities for cybersecurity).    \nIn May 2023, the Ministry of Business, Innovation & Employment (MBIE) published the  Digital  Technologies Industry Transformation Plan . This document refers to the development of an AI  strategy, stating,   \n[s]ome initial work was developed by MBIE in conjunction with the AI Forum, on draft  “cornerstones” to underpin a future AI Strategy for Aotearoa New Zealand. These  included the aim that all AI innovation and adoption across New Zealand is done safely  and ethically, with the full trust and support of New Zealanders. Future work could look  to advance development of an AI Strategy that helps New Zealand leverage the economic  opportunities of this technology in a trustworthy way.   \nThe AI Forum’s draft “cornerstones” were published in 2021. They describe six key areas that  “together, provide a framework for action, setting our priorities for AI in New Zealand.” 47  The  cornerstones are headed “uniquely New Zealand,” “human-centred and trusted AI,”  “investment in the AI economy,” “preparing the workforce,” and “our place in the world.” 48   \nThe transformation plan also references the  Digital Strategy for Aotearoa  (DSA), which was  published in September 2022. The relevant minister’s foreword to that strategy states,   \nNew Zealand stands on the precipice of a huge opportunity to design, build and use digital  technologies in world-leading, ethical, equitable ways that reflect the culture and  uniqueness of our country.  "}
{"page": 60, "image_path": "page_images/2023555908_60.jpg", "ocr_text": "Safety and Security of Artificial Intelligence Systems: New Zealand\n\nTo get there, we need to ask ourselves hard questions like:\n\n¢ How can we build on New Zealand’s inherently high levels of trust to benefit\nsociety, and the economy?\n\n¢ What would it mean to be the first country to embrace the ethical deployment of\nArtificial Intelligence?\n\ne Are we courageous enough to unlock the benefits of widespread digital\nthinking?\n\nThe DSA sets out three strategic themes: Trust, Inclusion, and Growth. In relation to Trust, the\nstrategy states that this is essential for digital systems, and that, for example, “[p]eople affected\nby decisions made using data and algorithms should be able to trust that their data was used in\nappropriate ways.” Furthermore,\n\n[t]rustworthy digital and data systems are fair, transparent, secure and accountable. They\nshould centre the needs of people most affected by them and honour Te Tiriti o Waitangi\n/ the Treaty of Waitangi and its principles. Trust is important when collecting data,\nanalysing it with artificial intelligence (AI) and other algorithms, and using it to\nmake decisions.*!\n\nThe DSA also discusses “ getting the right guardrails in place” in order to have trustworthy digital\ntechnologies and provide certainty for people developing and using digital technologies. It states,\n\n[rJules and guidelines must be coherent, comprehensive and easy to follow so people\nunderstand how to fulfil their obligations. Some key legislation already addresses some\naspects of trust, like the Privacy Act 2020 and the Harmful Digital Communications Act\n2015. Legislation to create a trust framework for digital identity services has been\nintroduced to Parliament. Beyond legislation, we have frameworks, guidelines and\nstandards that address various aspects of trust. These include the Algorithm Charter for\nAotearoa New Zealand, Nga Tikanga Paihere guidelines and the Privacy, Human Rights\nand Ethics framework.\n\nOpportunities in the next few years include:\n\ne developing a responsible and ethical approach to digital technologies, including\ngovernance and oversight to ensure trustworthy systems\n\n¢ identifying and filling any gaps in existing rules and standards, including ways of\nprotecting citizens’ rights to access their personal data and understand how it is\nbeing used\n\n¢ progressing the Digital Identity Services Trust Framework legislation\n\ne¢ making the Algorithm Charter for Aotearoa New Zealand operational across the\npublic sector, and exploring opportunities to encourage uptake more broadly\n\n#9 Te Rautaki Matihiko mo Aotearoa: The Digital Strategy for Aotearoa, Digital.govt.nz (Sept. 2022),\nhttps:/ / perma.cc/Q94K-YLJC.\n\n50 Id.\n51 Id.\n\nThe Law Library of Congress 58\n", "vlm_text": "To get there, we need to ask ourselves hard questions like:  \n•   How can we build on New Zealand’s inherently high levels of trust to benefit  society, and the economy?  •   What would it mean to be the first country to embrace the ethical deployment of  Artificial Intelligence?  •   Are we courageous enough to unlock the benefits of widespread digital  thinking? 49   \nThe DSA sets out three strategic themes: Trust, Inclusion, and Growth. In relation to Trust, the  strategy states that this is essential for digital systems, and that, for example, “[p]eople affected  by decisions made using data and algorithms should be able to trust that their data was used in  appropriate ways.” 50  Furthermore,  \n[t]rustworthy digital and data systems are fair, transparent, secure and accountable. They  should centre the needs of people most affected by them and honour Te Tiriti o Waitangi  / the Treaty of Waitangi and its principles. Trust is important when collecting data,  analysing it with artificial intelligence (AI) and other algorithms, and using it to  make decisions.   \nThe DSA also discusses “getting the right guardrails in place” in order to have trustworthy digital  technologies and provide certainty for people developing and using digital technologies. It states,   \n[r]ules and guidelines must be coherent, comprehensive and easy to follow so people  understand how to fulfil their obligations. Some key legislation already addresses some  aspects of trust, like the Privacy Act 2020 and the Harmful Digital Communications Act  2015. Legislation to create a trust framework for digital identity services has been  introduced to Parliament. Beyond legislation, we have frameworks, guidelines and  standards that address various aspects of trust. These include the Algorithm Charter for  Aotearoa New Zealand, Ngā Tikanga Paihere guidelines and the Privacy, Human Rights  and Ethics framework.  \nOpportunities in the next few years include:  \n•   developing a responsible and ethical approach to digital technologies, including  governance and oversight to ensure trustworthy systems  •   identifying and filling any gaps in existing rules and standards, including ways of  protecting citizens’ rights to access their personal data and understand how it is  being used  •   progressing the Digital Identity Services Trust Framework legislation  •   making the Algorithm Charter for Aotearoa New Zealand operational across the  public sector, and exploring opportunities to encourage uptake more broadly  "}
{"page": 61, "image_path": "page_images/2023555908_61.jpg", "ocr_text": "Safety and Security of Artificial Intelligence Systems: New Zealand\n\n¢ coordinating between public and private organisations so they can learn from\neach other.>?\n\nIn terms of “collaboratively shaping the future of digital and data-driven technologies,” the DSA\nlists several opportunities in the next few years, including “advancing an AI Strategy for Aotearoa\nwith the AI Forum.”® It also explains that “[t]he Ministry of Business, Innovation & Employment\n(MBIE) has worked with the AI Forum to develop cornerstones that could underpin a future AI\nStrategy for Aotearoa New Zealand. These include the aim that all AI innovation and adoption\nacross New Zealand is done safely and ethically, with the full support and trust of\nNew Zealanders.”54\n\nThe government also published the first action plan, for 2022-23, under the DSA. The action plan\n“signposts 4 issues that we know we must tackle” for which there currently are no dedicated\nresources “but where we want to do the groundwork for future initiatives” :5>\n\ne data ethics and the development and use of artificial intelligence\n\n¢ enhancing our cyber security\n\ne the challenges and opportunities of digital inclusion\n\ne = digital twins.\nThese issues emerged in the engagement process on the DSA and reflect our shared\nambition to design and use digital technologies in world-leading, ethical and equitable\n\nways that reflect our uniqueness. We will continue to flesh these out in the coming months,\nincluding their resourcing needs and the lead agencies that can drive progress.>°\n\nWith regard to data ethics and AI, the action plan states,\n\nBecause AI is informed by data, our ability to advance ethical and innovative development\nand use of AI depends heavily on building a trustworthy and ethical data ecosystem.\n\nAotearoa New Zealand could be a leading global voice in data governance, promoting\ntrust, addressing risks to privacy, embedding te ao Maori perspectives, and supporting\nnetworking and coordination across industry, research bodies and the wider community.\n\nAs initial steps, we will strive to build data ethics capability within the public sector and\nwill continue to explore the merits of a Centre for Data Ethics and AI. Work in this space\nwould make a particularly strong contribution to the Mahi Tika — Trust pillar of the DSA\nas well as contribute to Mahi Ake — Growth and Mahi Tahi — Inclusion through enabling\nnew business models and establishing a data ecosystem that works for all.5”\n\n52 Id.\n33 Id.\n541d.\n\n55 2022-23 Action Plan for the Digital Strategy for Aotearoa, Digital.govt.nz, https:/ /perma.cc/FAD9-J4AX.\n\n56 Id.\n57 Id.\n\nThe Law Library of Congress\n\n59\n", "vlm_text": "•   coordinating between public and private organisations so they can learn from  each other.   \nIn terms of “collaboratively shaping the future of digital and data-driven technologies,” the DSA  lists several opportunities in the next few years, including “advancing an AI Strategy for Aotearoa  with the AI Forum.” 53  It also explains that “[t]he Ministry of Business, Innovation & Employment  (MBIE) has worked with the AI Forum to develop cornerstones that could underpin a future AI  Strategy for Aotearoa New Zealand. These include the aim that all AI innovation and adoption  across New Zealand is done safely and ethically, with the full support and trust of  New Zealanders.” 54   \nThe government also published the first action plan, for 2022-23, under the DSA. The action plan  “signposts 4 issues that we know we must tackle” for which there currently are no dedicated  resources “but where we want to do the groundwork for future initiatives”: 55   \n•   data ethics and the development and use of artificial intelligence  •   enhancing our cyber security  •   the challenges and opportunities of digital inclusion  •   digital twins.  \nThese issues emerged in the engagement process on the DSA and reflect our shared  ambition to design and use digital technologies in world-leading, ethical and equitable  ways that reflect our uniqueness. We will continue to flesh these out in the coming months,  including their resourcing needs and the lead agencies that can drive progress.   \nWith regard to data ethics and AI, the action plan states,  \nBecause AI is informed by data, our ability to advance ethical and innovative development  and use of AI depends heavily on building a trustworthy and ethical data ecosystem.  \nAotearoa New Zealand could be a leading global voice in data governance, promoting  trust, addressing risks to privacy, embedding te ao Māori perspectives, and supporting  networking and coordination across industry, research bodies and the wider community.  \nAs initial steps, we will strive to build data ethics capability within the public sector and  will continue to explore the merits of a Centre for Data Ethics and AI. Work in this space  would make a particularly strong contribution to the Mahi Tika — Trust pillar of the DSA  as well as contribute to Mahi Ake — Growth and Mahi Tahi — Inclusion through enabling  new business models and establishing a data ecosystem that works for all.   "}
{"page": 62, "image_path": "page_images/2023555908_62.jpg", "ocr_text": "III. Definition of Artificial Intelligence (AI) Systems\n\nThere is no definition of Al systems in New Zealand law. Various entities have provided\ndefinitions of AI in their work, for example,\n\ne The AI Forum, in the Shaping the Future of New Zealand report, defined Al as “advanced digital\ntechnologies that enable machines to reproduce or surpass abilities that would require\nintelligence if humans were to perform them.”%\n\ne Rautaki Hanganga o Aotearoa, New Zealand’s Infrastructure Strategy (2022-2052), defines AI as\n“[t]echnology that enables digital devices to respond to and learn from their environments.\nArtificial intelligence is anticipated to streamline tasks, especially those that are repeatable,\nand continue to learn and develop through completing tasks and receiving feedback.”59\n\ne The Productivity Commission, in its joint report on growing the digital economy in Australia\nand New Zealand, defined Al as “[a] system able to learn and adapt to perform specific tasks\nnormally requiring human-like cognition and intelligence, such as visual perception, speech\nrecognition and problem solving.”\n\ne The Cyber Security Strategy defines AI as “[a] computerised system capable of simulating\nhuman decision making and learning, including performing cognitive functions associated\nwith the human mind including learning and language.”®!\n\ne New Zealand Trade and Enterprise defines AI as “the simulation of human memory, problem\nsolving and decision-making, by machines (most often computer systems).”®\n\nIV. Cybersecurity of AI\n\nA. Relevant Agencies and Roles\n\nThere are two key government agencies with roles specifically related to cybersecurity: the New\nZealand Computer Emergency Response Team (CERT NZ) and the National Cyber Security\nCentre (NCSC). Other agencies also have responsibilities related to cybersecurity, including the\nDepartment of the Prime Minister and Cabinet (DPMC), which includes the National Cyber\nPolicy Office within the National Security Group.®\n\n58 Artificial Intelligence: Shaping a Future New Zealand, supra note 34, at 14.\n\n59 Glossary, Rautaki Hanganga o Aotearoa - New Zealand Infrastructure Strategy, https:/ / perma.cc/9VS7-\n8MK8.\n\n© Growing the Digital Economy in Australia and New Zealand: Maximising Opportunities for SMEs: Joint Research\nReport, supra note 18, at ix.\n\n61 New Zealand Government, New Zealand’s Cyber Security Strategy 2019: Enabling New Zealand to Thrive Online\n16 (2019), https:/ / perma.cc/33PR-5FF5.\n\n62 How Artificial Intelligence Can Help Your Business Grow, myNZTE (Jan. 12, 2022), https:/ /perma.cc/ V6BV-\nTYCL.\n\n63 See Department of the Prime Minister and Cabinet, Briefing to the Incoming Minister for the Digital Economy and\nCommunications 21-22 (Jan. 31, 2023), https:/ /perma.cc/JAT5-UNX9.\n", "vlm_text": "III.  Definition of Artificial Intelligence (AI) Systems  \nThere is no definition of AI systems in New Zealand law. Various entities have provided  definitions of AI in their work, for example, \n\n \n•   The AI Forum, in the  Shaping the Future of New Zealand  report, defined AI as “advanced digital  technologies that enable machines to reproduce or surpass abilities that would require  intelligence if humans were to perform them.” 58  \n\n •   Rautaki Hanganga o Aotearoa ,  New Zealand’s Infrastructure Strategy (2022–2052),  defines AI as  “[t]echnology that enables digital devices to respond to and learn from their environments.  Artificial intelligence is anticipated to streamline tasks, especially those that are repeatable,  and continue to learn and develop through completing tasks and receiving feedback.” 59  \n\n •   The Productivity Commission, in its joint report on growing the digital economy in Australia  and New Zealand, defined AI as “[a] system able to learn and adapt to perform specific tasks  normally requiring human-like cognition and intelligence, such as visual perception, speech  recognition and problem solving.” 60  \n\n •   The Cyber Security Strategy defines AI as “[a] computerised system capable of simulating  human decision making and learning, including performing cognitive functions associated  with the human mind including learning and language.” 61  \n\n •   New Zealand Trade and Enterprise defines AI as “the simulation of human memory, problem  solving and decision-making, by machines (most often computer systems).  ${\\mathrm{\\Omega}}^{\\prime\\prime}62$    \nIV.  Cybersecurity of AI  \nA.  Relevant Agencies and Roles  \nThere are two key government agencies with roles specifically related to cybersecurity: the New  Zealand Computer Emergency Response Team (CERT NZ) and the National Cyber Security  Centre (NCSC). Other agencies also have responsibilities related to cybersecurity, including the  Department of the Prime Minister and Cabinet (DPMC), which includes the National Cyber  Policy Office within the National Security Group.   "}
{"page": 63, "image_path": "page_images/2023555908_63.jpg", "ocr_text": "CERT NZ “works to support businesses, organisations and individuals who are affected (or may\nbe affected) by cyber security incidents.” Its role includes receiving cyber incident reports,\ntracking incidents or attacks, and providing advice and alerts to organizations on responding to\nand preventing attacks.\n\nThe NCSC is part of the Government Communications Security Bureau (GCSB) and “supports\nnationally significant organisations to improve their cyber security,” as well as responding to\n“national-level harm and advanced threats.”® As part of its role, the NCSC administers the\nnetwork security provisions of the Telecommunications (Interception Capability and Security)\nAct 2013. The functions, powers, and duties of the GCSB in relation to cybersecurity are set out\nin the Intelligence and Security Act 2017. The NCSC maintains the New Zealand Information\nSecurity Manual, and the director-general of GCSB is the government chief information\nsecurity officer.%\n\nThe current Cyber Security Strategy was released in 2019. The strategy notes that “[t]he\nemergence of AI is an example of a technological shift where the impact for cyber security is\nlargely unknown.”°7 DPMC’s work program “seeks to progress the five areas” of the strategy.®8\nThese are a cyber security aware and active citizenry, a strong and capable cyber security\nworkforce and ecosystem, an internationally active, resilient, and responsive New Zealand, and\na proactive tackling of cybercrime.®?\n\nThe minister responsible for cybersecurity matters is currently the Minister for the Digital\nEconomy and Communications.”\n\nB. Legislation and Guidance\n\nThe Privacy Act 2020 is “the key legislation in New Zealand governing cybersecurity.”\nHowever, it only deals with personal information. The Intelligence and Security Act 2017\nregulates state-based surveillance and, as noted in Part IV.A, above, establishes the role of the\nGCSB with respect to responding to cybersecurity incidents impacting nationally significant\norganizations. In addition, entities regulated by the Financial Markets Authority (FMA) or the\nReserve Bank “are subject to separate, sector-specific guidance in relation to cyber resilience.”\n\n4 About Us, CERT NZ, https://perma.cc/ VVT3-GRX8.\n\n6 About Us, NCSC, https://perma.cc/D8B5-YVNW.\n\n6 Id.\n\n6? New Zealand’s Cyber Security Strategy 2019: Enabling New Zealand to Thrive Online, supra note 61, at 4.\n\n68 Department of the Prime Minister and Cabinet, Aide-Memoire: Briefing to the Incoming Minister for National\nSecurity Intelligence 9 (Jan. 25, 2023), https:/ / perma.cc/874B-4MYG.\n\n69 New Zealand’s Cyber Security Strategy 2019: Enabling New Zealand to Thrive Online, supra note 61, at 10.\n” Briefing to the Incoming Minister for the Digital Economy and Communications, supra note 63, at 2.\n\n71 Derek Roth-Biester et al., The Privacy, Data Protection and Cybersecurity Law Review: New Zealand, The Law\nReviews (Oct. 27, 2022), https:/ / perma.cc/42UZ-E7YA.\n\n721d.\n", "vlm_text": "CERT NZ “works to support businesses, organisations and individuals who are affected (or may  be affected) by cyber security incidents.” 64  Its role includes receiving cyber incident reports,  tracking incidents or attacks, and providing advice and alerts to organizations on responding to  and preventing attacks.  \nThe NCSC is part of the Government Communications Security Bureau (GCSB) and “supports  nationally significant organisations to improve their cyber security,” as well as responding to  “national-level harm and advanced threats.” 65  As part of its role, the NCSC administers the  network security provisions of the Telecommunications (Interception Capability and Security)  Act 2013. The functions, powers, and duties of the GCSB in relation to cybersecurity are set out  in the Intelligence and Security Act 2017. The NCSC maintains the  New Zealand Information  Security Manual , and the director-general of GCSB is the government chief information  security officer.   \nThe current Cyber Security Strategy was released in 2019. The strategy notes that “[t]he  emergence of AI is an example of a technological shift where the impact for cyber security is  largely unknown.” 67  DPMC’s work program “seeks to progress the five areas” of the strategy.   These are a cyber security aware and active citizenry, a strong and capable cyber security  workforce and ecosystem, an internationally active, resilient, and responsive New Zealand, and  a proactive tackling of cybercrime.     \nThe minister responsible for cybersecurity matters is currently the Minister for the Digital  Economy and Communications.   \nB.  Legislation and Guidance  \nThe Privacy Act 2020 is “the key legislation in New Zealand governing cybersecurity.” 71   However, it only deals with personal information. The Intelligence and Security Act 2017  regulates state-based surveillance and, as noted in Part IV.A, above, establishes the role of the  GCSB with respect to responding to cybersecurity incidents impacting nationally significant  organizations. In addition, entities regulated by the Financial Markets Authority (FMA) or the  Reserve Bank “are subject to separate, sector-specific guidance in relation to cyber resilience.” 72    "}
{"page": 64, "image_path": "page_images/2023555908_64.jpg", "ocr_text": "In December 2022, the New Zealand cabinet agreed to the development of standalone legislation\n“to enhance the cyber resilience of critical infrastructure, at a faster pace than the broader\nresilience reform programme [related to critical infrastructure]. Legislation focused on the cyber\nresilience is intended to be introduced in 2024, with consultation on the process scheduled for the\nfirst half of 2023.”73\n\n1. Privacy Act 2020\n\nThe Privacy Act 2020 repealed and replaced the Privacy Act 1983. The new act “retains the\nprinciple-based approach contained within the former legislation and strengthens the Act’s\nprivacy protections by promoting early intervention and privacy risk management by agencies\n(including by introducing a mandatory privacy breach notification regime).”74 The act contains\n13 privacy principles that govern “how businesses and organisations should collect, handle and\nuse personal information.”75 Under the act, if an organization or business has a privacy breach\nthat has either caused or is likely to cause anyone serious harm, it must notify the privacy\ncommissioner and any affected people as soon as possible.”\n\nAs stated by the privacy commissioner in the guidance on generative AI tools, the Privacy Act is\nintended to be “technology-neutral,” meaning “the same privacy rights and protections apply to\ngenerative AI tools that apply to other activities that use personal information (such as collecting\nand using personal information via paper or computer).””” The guidance states that the privacy\ncommissioner expects agencies considering implementing a generative AI tool to\n\ne have senior leadership approval,\n\ne review whether a generative AI tool is necessary and proportionate,\n\ne conduct a Privacy Impact Assessment,\n\ne be transparent,\n\ne engage with Maori,\n\ne develop procedures about accuracy and access by individuals,\n\ne ensure human review prior to acting, and\n\ne ensure that personal or confidential information is not retained or disclosed by the generative\nAI tool.78\n\n3 Aide-Memoire: Briefing to the Incoming Minister for National Security Intelligence, supra note 68, at 9.\n74 Roth-Biester et al., supra note 71.\n\n75 Privacy Act 2020 and the Privacy Principles, Privacy Commissioner, https:/ / perma.cc/ HST4-BGTN.\n76 Privacy Breaches, Privacy Commissioner, https:/ / perma.cc/9YA4-ZPVU.\n\n7” Generative Artificial Intelligence - 15 June 2023 Update, supra note 20.\n\n8 Id.\n", "vlm_text": "In December 2022, the New Zealand cabinet agreed to the development of standalone legislation  “to enhance the cyber resilience of critical infrastructure, at a faster pace than the broader  resilience reform programme [related to critical infrastructure]. Legislation focused on the cyber  resilience is intended to be introduced in 2024, with consultation on the process scheduled for the  first half of 2023.” 73   \n1.  Privacy Act 2020  \nThe Privacy Act 2020 repealed and replaced the Privacy Act 1983. The new act “retains the  principle-based approach contained within the former legislation and strengthens the Act’s  privacy protections by promoting early intervention and privacy risk management by agencies  (including by introducing a mandatory privacy breach notification regime).  $^{\\prime\\prime}74$   The act contains  13 privacy principles that govern “how businesses and organisations should collect, handle and  use personal information.” 75  Under the act, if an organization or business has a privacy breach  that has either caused or is likely to cause anyone serious harm, it must notify the privacy  commissioner and any affected people as soon as possible.   \nAs stated by the privacy commissioner in the guidance on generative AI tools, the Privacy Act is  intended to be “technology-neutral,” meaning “the same privacy rights and protections apply to  generative AI tools that apply to other activities that use personal information (such as collecting  and using personal information via paper or computer).” 77  The guidance states that the privacy  commissioner expects agencies considering implementing a generative AI tool to \n\n \n•   have senior leadership approval, \n\n •   review whether a generative AI tool is necessary and proportionate, \n\n •   conduct a Privacy Impact Assessment, \n\n •   be transparent, \n\n •   engage with Māori, \n\n •   develop procedures about accuracy and access by individuals, \n\n •   ensure human review prior to acting, and \n\n •   ensure that personal or confidential information is not retained or disclosed by the generative  AI tool.   "}
{"page": 65, "image_path": "page_images/2023555908_65.jpg", "ocr_text": "Safety and Security of Artificial Intelligence Systems: New Zealand\n\n2. New Zealand Information Security Manual\n\nThe New Zealand Information Security Manual (NZISM) “ details processes and controls essential\nfor the protection of all New Zealand Government information and systems. Controls and\nprocesses representing good practice are also provided to enhance the baseline controls.””? While\nit is intended for use by New Zealand government departments and agencies, Crown entities,\nlocal government, and private sector organizations are also encouraged to use the manual.®? It\ncontains chapters on, for example, information security governance, system certification and\naccreditation, information security monitoring and incidents, communications systems and\ndevices, software security, email security, access control and passwords, cryptography, network\nand gateway security, data management, enterprise systems security, and Public Cloud Security.\n\nWith respect to the use of standards within the NZISM, the GSCB explains that\n\nPublished standards are widely used in the continuing development of the NZISM.\nStandards in development are also monitored.\n\nWhile the NZISM may include a subset of, or directly align with international standards,\nnational or joint standards, the NZISM is also designed to reflect New Zealand conditions,\nand the national interests of New Zealand.\n\nCost, adoption rates, international interoperability and obligations, ability to influence\nvendors, and the threat and risk environment can be unique to New Zealand which can\naffect the adoption of some elements of international practice and standards.\n\nExisting international standards are extremely well resourced, researched, reviewed and\nwidely published. Care is taken not to create unique NZ solutions which lack compatibility\nor are not interoperable with international practice and standards.*!\n\n3. Telecommunications (Interception Capability and Security) Act 2013\n\nIn 2020, the GCSB and NCSC published guidelines for network operators regarding the network\nsecurity requirements contained in part 3 of the Telecommunications (Interception Capability and\nSecurity) Act 2013. It explains that the part\n\noutlines a framework under which network operators are required to engage with the\nGCSB about proposed changes and developments with their networks where these\nintersect with national security.\n\nThe framework sets out a path to identify and address, prevent, mitigate, or remove the\nnetwork security risks which may arise.*?\n\n79 Government Communications Security Bureau (GCSB), ISM Document: 1. About Information Security § 1.1.1\n(version 3.6, last updated Sept. 2022), https:/ / perma.cc/93TC-KUTF.\n\n801d. § 1.1.2.\n81 About the NZISM: Standards, GCSB, https: / / perma.cc/ MG9D-YAV8.\n\n82 GCSB, Telecommunications (Interception Capability and Security) Act 2013: Guidelines for Network Operators 3\n(May 27, 2020), https:/ / perma.cc/2ZGB-CHCS.\n\nThe Law Library of Congress 63\n", "vlm_text": "2.  New Zealand Information Security Manual  \nThe New Zealand Information Security Manual (NZISM) “details processes and controls essential  for the protection of all New Zealand Government information and systems. Controls and  processes representing good practice are also provided to enhance the baseline controls.” 79  While  it is intended for use by New Zealand government departments and agencies, Crown entities,  local government, and private sector organizations are also encouraged to use the manual.  It  contains chapters on, for example, information security governance, system certification and  accreditation, information security monitoring and incidents, communications systems and  devices, software security, email security, access control and passwords, cryptography, network  and gateway security, data management, enterprise systems security, and Public Cloud Security.  \nWith respect to the use of standards within the NZISM, the GSCB explains that   \nPublished standards are widely used in the continuing development of the NZISM.   Standards in development are also monitored.  \nWhile the NZISM may include a subset of, or directly align with international standards,  national or joint standards, the NZISM is also designed to reflect New Zealand conditions,  and the national interests of New Zealand.  \nCost, adoption rates, international interoperability and obligations, ability to influence  vendors, and the threat and risk environment can be unique to New Zealand which can  affect the adoption of some elements of international practice and standards.   \nExisting international standards are extremely well resourced, researched, reviewed and  widely published. Care is taken not to create unique NZ solutions which lack compatibility  or are not interoperable with international practice and standards.   \n3.  Telecommunications (Interception Capability and Security) Act 2013  \nIn 2020, the GCSB and NCSC published guidelines for network operators regarding the network  security requirements contained in part 3 of the Telecommunications (Interception Capability and  Security) Act 2013.   It explains that the part   \noutlines a framework under which network operators are required to engage with the  GCSB about proposed changes and developments with their networks where these  intersect with national security.   \nThe framework sets out a path to identify and address, prevent, mitigate, or remove the  network security risks which may arise.    "}
{"page": 66, "image_path": "page_images/2023555908_66.jpg", "ocr_text": "Safety and Security of Artificial Intelligence Systems: New Zealand\n\nUnder the act, network operators are required to register,8> with the register maintained by the\nNew Zealand Police. They must act honestly and in good faith when engaging with the GCSB,\nand have a duty to notify or engage with the GCSB about certain proposed decisions, courses of\naction, or changes in regard to certain parts of their network. There is also a duty to engage with\nthe GCSB if the operator becomes aware of any network security risk.**\n\n4. Guidance for Entities Regulated by the FMA or Reserve Bank\n\nThe FMA published an information sheet on cybersecurity and operational systems resilience in\n2022,85 and the Reserve Bank published guidance on cyber resilience in 2021.86 The Reserve Bank\nexplains that\n\n[s]ince 2019, we have been progressing our work to build cyber resilience in the financial\nsector alongside industry and other public bodies, including the National Cyber Security\nCentre (NCSC), the Computer Emergency Response Team (CERT NZ) and the Financial\nMarkets Authority (FMA).\n\nWe have developed a three-step approach to promoting cyber resilience, which:\n\n1. provides new risk management guidance for the entities we regulate (...)\n\n2. develops an information collection and gathering plan (which is in development and\nwill be released for consultation)\n\n3. enhances coordination across industry, regulators and government agencies on a\ncollective response to cyber incidents.\n\nInformation gathering and sharing is an area where there is a strong case for close\ncoordination among agencies. In developing information gathering and sharing\narrangements, we are working closely with the NCSC, CERT NZ and the FMA to avoid\nduplication and reduce unnecessary compliance costs.*”\n\n5. Trustworthy AI Principles\n\nThe AI Forum’s AI Principles include “reliability, security and privacy,” stating that “AI\nstakeholders must ensure AI systems and related data are reliable, accurate and secure and the\nprivacy of individuals is protected throughout the AI system’s life cycle, with potential risks\nidentified and managed on an ongoing basis.”§ They also include transparency (“[t]he operation\nand impacts of an AI system should be transparent, traceable, auditable and generally explainable\nto a degree appropriate to its use and potential risk profile so outcomes can be understood and\nchallenged, particularly where they relate to people”) and human oversight and accountability\n\n83 Telecommunications (Interception Capability and Security) Act 2013 s 60.\n\n841d.s 46.\n\n85 FMA, Cyber Security & Operational Systems Resilience (June 2022), https:/ / perma.cc/ ZMK3-5W2E.\n\n86 Reserve Bank of New Zealand, Guidance on Cyber Resilience (Apr. 2021), https:/ / perma.cc/9GXX-BY93.\n\n87 Improving Cyber Resilience for Regulated Entities, Reserve Bank of New Zealand (Feb. 28, 2022),\nhttps:/ / perma.cc/ BoDW-RB5P..\n\n88 Trustworthy Al in Aotearoa: AI Principles, supra note 36, at 4.\n\nThe Law Library of Congress 64\n", "vlm_text": "Under the act, network operators are required to register,  with the register maintained by the  New Zealand Police. They must act honestly and in good faith when engaging with the GCSB,  and have a duty to notify or engage with the GCSB about certain proposed decisions, courses of  action, or changes in regard to certain parts of their network. There is also a duty to engage with  the GCSB if the operator becomes aware of any network security risk.   \n4.  Guidance for Entities Regulated by the FMA or Reserve Bank  \nThe FMA published an information sheet on cybersecurity and operational systems resilience in  2022,  and the Reserve Bank published guidance on cyber resilience in 2021.  The Reserve Bank  explains that   \n[s]ince 2019, we have been progressing our work to build cyber resilience in the financial  sector alongside industry and other public bodies, including the National Cyber Security  Centre (NCSC), the Computer Emergency Response Team (CERT NZ) and the Financial  Markets Authority (FMA).  \nWe have developed a three-step approach to promoting cyber resilience, which:  \n1.   provides new risk management guidance for the entities we regulate ( . . . )  2.   develops an information collection and gathering plan (which is in development and  will be released for consultation)  3.   enhances coordination across industry, regulators and government agencies on a  collective response to cyber incidents.  \nInformation gathering and sharing is an area where there is a strong case for close  coordination among agencies. In developing information gathering and sharing  arrangements, we are working closely with the NCSC, CERT NZ and the FMA to avoid  duplication and reduce unnecessary compliance costs.   \n5.  Trustworthy AI Principles \nThe AI Forum’s AI Principles include “reliability, security and privacy,” stating that “AI  stakeholders must ensure AI systems and related data are reliable, accurate and secure and the  privacy of individuals is protected throughout the AI system’s life cycle, with potential risks  identified and managed on an ongoing basis.” 88  They also include transparency (“[t]he operation  and impacts of an AI system should be transparent, traceable, auditable and generally explainable  to a degree appropriate to its use and potential risk profile so outcomes can be understood and  challenged, particularly where they relate to people”) and human oversight and accountability  (“AI stakeholders should retain an appropriate level of human oversight of AI systems and their  outputs. Technologies capable of harming individuals or groups should not be deployed until  stakeholders have determined appropriate accountability and liability”).   "}
{"page": 67, "image_path": "page_images/2023555908_67.jpg", "ocr_text": "(“AI stakeholders should retain an appropriate level of human oversight of AI systems and their\noutputs. Technologies capable of harming individuals or groups should not be deployed until\nstakeholders have determined appropriate accountability and liability”).\n\nV. International Collaboration on Cybersecurity Guidance\n\nNew Zealand's cybersecurity authorities, specifically the NCSC and CERT NZ, have worked with\ninternational partners to develop guidance related to cybersecurity, as noted in the survey on\nAustralia contained in this report.\n\nIn particular, the NCSC was involved in the development of the joint guidance on Cybersecurity\nBest Practices for Smart Cities,” along with authorities in the United States, United Kingdom,\nCanada, and Australia. The NCSC states that the guidance “provides recommendations to\nbalance efficiency and innovation with cyber security, privacy protections, and national security”\nand “encourages organisations to implement these best practices in alignment with their specific\ncyber security requirements to ensure the safe and secure operation of infrastructure systems,\nprotection of citizens’ private data, and security of sensitive government and business data.”%!\n\nThe NCSC and CERT NZ were also both involved in the preparation of the joint guide, by “seven\nlike-minded nations,” titled Shifting the Balance of Cybersecurity Risk: Principles and Approaches for\nSecurity-by-Design and - Default.°2 The NCSC explains that the guide “recommends that software\nmanufacturers adopt secure-by-design and secure-by-default practices, and that customer\norganisations should hold their manufacturers and suppliers to these standards,” and that the\nguidance “serves as a cyber security roadmap for manufacturers of technology and\nassociated products.”%\n\n89 Id.\n% CISA et al., Cybersecurity Best Practices for Smart Cities (Apr. 19, 2023), https:/ / perma.cc/P6YH-FGM7.\n\n9! Joint Guidance: Cyber Security Best Practices for Smart Cities, NCSC (Apr. 20, 2023), https:/ / perma.cc/5NLN-\nS5DB.\n\n% CISA et al., Shifting the Balance of Cybersecurity Risk: Principles and Approaches for Security-by-Design and -\nDefault (Apr. 13, 2023), https:/ / perma.cc/BXR8-8VGM.\n\n%8 Joint Guidance: Principles for Security-by-Design and -Default, NCSC (Apr. 14, 2023), https:/ / perma.cc/FYA9-\n7V8R.\n", "vlm_text": "\nV.  International Collaboration on Cybersecurity Guidance \nNew Zealand’s cybersecurity authorities, specifically the NCSC and CERT NZ, have worked with  international partners to develop guidance related to cybersecurity, as noted in the survey on  Australia contained in this report.   \nIn particular, the NCSC was involved in the development of the joint guidance on  Cybersecurity  Best Practices for Smart Cities ,  along with authorities in the United States, United Kingdom,  Canada, and Australia. The NCSC states that the guidance “provides recommendations to  balance efficiency and innovation with cyber security, privacy protections, and national security”  and “encourages organisations to implement these best practices in alignment with their specific  cyber security requirements to ensure the safe and secure operation of infrastructure systems,  protection of citizens’ private data, and security of sensitive government and business data.” 91   \nThe NCSC and CERT NZ were also both involved in the preparation of the joint guide, by “seven  like-minded nations,” titled  Shifting the Balance of Cybersecurity Risk: Principles and Approaches for  Security-by-Design and – Default .  The NCSC explains that the guide “recommends that software  manufacturers adopt secure-by-design and secure-by-default practices, and that customer  organisations should hold their manufacturers and suppliers to these standards,” and that the  guidance “serves as a cyber security roadmap for manufacturers of technology and  associated products.” 93   "}
{"page": 68, "image_path": "page_images/2023555908_68.jpg", "ocr_text": "SUMMARY\n\nUnited Kingdom\n\nClare Feikert-Ahalt\nSenior Foreign Law Specialist\n\nThe United Kingdom (UK) currently has the third highest number of artificial\nintelligence (AI) companies in the world. It has adopted a pro-innovation approach to\nAI. Rather than introducing a legislative framework and a new regulator for AI, the UK\nemploys its existing regulators. The intent of this light-touch approach is to help enable\nthe development and growth of the AI industry. It has introduced a National AI\nStrategy and is currently working on its proposed AI Framework to help provide\nguidance, clarity, and security for the AI industry as it develops.\n\nWhile there is no Al-specific legislation, there are 18 legal frameworks containing over\n50 pieces of legislation that touch upon AI. Significant pieces of legislation that impact\nAl are the Data Protection Act, which applies when personal data is used in AI, and\nequality laws to ensure that bias is not introduced into AI systems.\n\nThe National Security and Investment Act provides the government with the ability to\nscrutinize and intervene in acquisitions involving AI made by any legal body that could\nharm the UK’s national security.\n\nI. Introduction\n\nIn 2021, the United Kingdom (UK) was the top country in Europe for private investment in\nartificial intelligence (Al) companies.! It has the third highest number of AI companies in the\nworld, behind the United States and China.? The UK government has invested 2.5 billion pounds\n(approximately US$3.15 billion) in AI since 2014,3 and it is estimated that more than 1.3 million\nbusinesses in the UK will use AI and spend £110 billion (approximately US$138 billion) on this\ntechnology by 2024.4\n\n1 Stanford U., Artificial Intelligence Index Report 2022 (2022), https:/ / perma.cc/LS2X-PEQN.\n\n? Guidance: National AI Strategy - HTML Version, Dep’t for Bus., Energy & Indus. Strategy (last updated Dec. 18,\n2022), https: / / perma.cc/EJ2D-HEVN.\n\n3 Dep’t for Sci., Innovation & Tech., CP 815, A Pro-Innovation Approach to AI Regulation 1 (Mar. 2023),\nhttps:/ / perma.cc/5DP6-VXSJ.\n\n4 Press Release, Dep’t for Digit., Culture, Media & Sport & Damian Collins MP, UK Sets Out Proposals for New\nAI Rulebook to Unleash Innovation and Boost Public Trust in the Technology (July 18, 2022),\n\nhttps:/ / perma.cc/F52W-5B4Z; Andrew Evans & Anja Himann, Capital Econ., AI Activity in UK Businesses\n(Dep’t for Digit., Culture, Media, and Sport Jan. 2022), https:/ / perma.cc/F5AG-TTAC.\n", "vlm_text": "United Kingdom  \nClare Feikert-Ahalt  Senior Foreign Law Specialist  \n\nThe United Kingdom (UK) currently has the third highest number of artificial  intelligence (AI) companies in the world. It has adopted a pro-innovation approach to  AI. Rather than introducing a legislative framework and a new regulator for AI, the UK  employs its existing regulators. The intent of this light-touch approach is to help enable  the development and growth of the AI industry. It has introduced a National AI  Strategy and is currently working on its proposed AI Framework to help provide  guidance, clarity, and security for the AI industry as it develops.   \nWhile there is no AI-specific legislation, there are 18 legal frameworks containing over  50 pieces of legislation that touch upon AI. Significant pieces of legislation that impact  AI are the Data Protection Act, which applies when personal data is used in AI, and  equality laws to ensure that bias is not introduced into AI systems.  \nThe National Security and Investment Act provides the government with the ability to  scrutinize and intervene in acquisitions involving AI made by any legal body that could  harm the UK’s national security.   \nI.  Introduction  \nIn 2021, the United Kingdom (UK) was the top country in Europe for private investment in  artificial intelligence (AI) companies.  It has the third highest number of AI companies in the  world, behind the United States and China.  The UK government has invested 2.5 billion pounds  (approximately   $\\mathrm{U}S\\S3.15$   billion) in AI since 2014,  and it is estimated that more than 1.3 million  businesses in the UK will use AI and spend £110 billion (approximately   $\\mathrm{US}\\Phi138$   billion) on this  technology by 2024.    "}
{"page": 69, "image_path": "page_images/2023555908_69.jpg", "ocr_text": "The UK government has adopted a pro-innovation position on both the governance and\nregulation of AI.5 It recently set out its commitment to international engagement “to support\ninteroperability across different regulatory regimes.” The UK is working with international\npartners to help “shape approaches to AI governance under development.”’ It has stated that its\nwork in this area will “reflect the UK’s views on international AI governance and prevent\ndivergence and friction between partners, and guard against abuse of this critical technology.”®\n\nII. Overview of the Legal and Policy Framework\n\nA. Legal Framework\n\nThere is no bespoke legislative framework that regulates AI across the UK. The government has\nstated it believes that legislating at this stage in the development of AI “would risk placing undue\nburdens on businesses”? that could “hold back AI innovation and reduce [its] ability to respond\nquickly and in a proportionate way to future technological advances.” Instead, it is relying on\ncollaboration between government and business.\n\nAl is currently “partially regulated through a patchwork of legal and regulatory requirements\nbuilt for other purposes which now also capture uses of AI technologies.” Its complexity has\nbeen noted:\n\nThere are at least 18 legal frameworks (both pervasive and sector specific legislation) that\nindirectly control the development and use of AI (e.g. consumer rights law, data protection\nlaw, product safety law, etc.). Within this there are over 50 individual pieces of primary\nlegislation that must be considered. Additionally, case law interprets legislation and\ndetermines how it should be applied. This leads to a highly complex regulatory\nenvironment for AI systems that is extremely difficult to understand in its entirety for all\nrelevant stakeholders.!*\n\n5 Dep’t for Digit., Culture, Media & Sport, CP 728, Establishing a Pro-Innovation Approach to Regulating AI (July\n18, 2022), https:/ / perma.cc/4TUB-W533.\n\n© Dep’t for Sci., Innovation & Tech., CP 815, A Pro-Innovation Approach to AI Regulation (Mar. 2023), supra note 3,\nat 3.\n\n? Guidance: National AI Strategy - HTML Version, Dep’t for Bus., Energy & Indus. Strategy, supra note 2.\n81d.\n\n° Dep’t for Sci., Innovation & Tech., CP 815, A Pro-Innovation Approach to AI Regulation (Mar. 2023), supra note 3,\nat 3.\n\nId. 411.\n\n1 Dep’t for Digit., Culture, Media & Sport, Establishing a Pro-Innovation Approach to Regulating AI, supra note 5,\nat 5.\n\n?2 Dep’t for Sci., Innovation & Tech., RPC-DCMS-5260(1), Artificial Intelligence Regulation Impact Assessment § 33\n(Mar. 2023), https:/ / perma.cc/ TNE2-3857.\n", "vlm_text": "The UK government has adopted a pro-innovation position on both the governance and  regulation of AI.  It recently set out its commitment to international engagement “to support  interoperability across different regulatory regimes.” 6  The UK is working with international  partners to help “shape approaches to AI governance under development.” 7  It has stated that its  work in this area will “reflect the UK’s views on international AI governance and prevent  divergence and friction between partners, and guard against abuse of this critical technology.” 8    \nII.    Overview of the Legal and Policy Framework  \nA.  Legal Framework  \nThere is no bespoke legislative framework that regulates AI across the UK. The government has  stated it believes that legislating at this stage in the development of AI “would risk placing undue  burdens on businesses” 9  that could “hold back AI innovation and reduce [its] ability to respond  quickly and in a proportionate way to future technological advances.” 10  Instead, it is relying on  collaboration between government and business.  \nAI is currently “partially regulated through a patchwork of legal and regulatory requirements  built for other purposes which now also capture uses of AI technologies.” 11  Its complexity has  been noted:   \nThere are at least 18 legal frameworks (both pervasive and sector specific legislation) that  indirectly control the development and use of AI (e.g. consumer rights law, data protection  law, product safety law, etc.). Within this there are over 50 individual pieces of primary  legislation that must be considered. Additionally, case law interprets legislation and  determines how it should be applied. This leads to a highly complex regulatory  environment for AI systems that is extremely difficult to understand in its entirety for all  relevant stakeholders.   "}
{"page": 70, "image_path": "page_images/2023555908_70.jpg", "ocr_text": "Examples of cross-sector legislation that regulates aspects of the use and development of AI\ninclude the following:\n\ne Data protection laws, which include provisions on automated decision making and data\nprocessing. Breaches of this act can result in fines of up to £17.5 million (approximately US$22\nmillion), or 4% of a company’s global turnover.!3\n\ne Equality laws, which prohibit the government from discriminating, harassing, or victimizing\nanyone who has a protected characteristic.'4\n\ne The National Security and Investment Act 2021 provides the government with the authority\nto intervene in acquisitions that could harm the national security of the UK, including\ncompanies that develop AI.15\n\nSector-specific legislation is also in place in certain areas, such as for medical devices.'¢\nCompetition law includes provisions on protecting consumers against unfair use of AI in\nalgorithms. Breaches of competition law can result in fines of up to 10% of a company’s global\nturnover, imprisonment, and disqualification of company directors for up to 15 years.!”\n\nThe UK is reviewing its approach to the regulation of AI. A paper by the Department for Digital,\nCulture, Media and Sport, presented to the Parliament in July 2022, notes “[t]he extent to which\nUK laws apply to AI is often a matter of interpretation, making them hard to navigate.”!8 The\npaper and the National AI Strategy, published in September 2021, state that there is a need for\ngreater clarity in both the legal framework and among regulators to ensure there are no overlaps\nor gaps in areas of regulation and that regulators adopt a consistent approach to AI.!9\n\nAn impact assessment on the regulation of AI noted the following:\n\n18 Data Protection Act 2018 (DPA), c. 12, https:/ / perma.cc/5DMR-6FZV; Information Commissioner's Office\n(ICO), Guidance on AI and Data Protection (last updated Mar. 15, 2023), https:/ / perma.cc/4MVP-WNTK; ICO &\nAlan Turing Inst., Explaining Decisions Made With AI, https:/ / perma.cc/7WMB-8JJ2.\n\n14 Equality Act 2010, c. 15, https:/ / perma.cc/52EB-2DQ2; and Equal. & Hum. Rts. Comm’n, HC 1206, Strategic\nPlan 2022-25 (Mar. 29, 2022), https:/ / perma.cc/6YG8-43TT.\n\n145 National Security and Investment Act 2021, c. 25, https:// perma.cc/9VHY-B8UB.\n\n16 Medical Devices Regulations 2002, SI 2002/618, https:/ / perma.cc/4FSR-SRHU. See also Med. & Healthcare\nProducts Regul. Agency, Guidance: Software and AI as a Medical Device Change Programme - Roadmap (last\nupdated Oct. 17, 2022), https:/ / perma.cc/ H2G5-UKG6.\n\n17 Competition Act 1998, c. 41, https:/ / perma.cc/ DX9D-WQJU; Competition & Mkts. Auth., Algorithms: How\nThey Can Reduce Competition and Harm Consumers (2021), https:/ / perma.cc/ YT94-BGH6.\n\n18 Dep’t for Digit., Culture, Media & Sport, Establishing a Pro-Innovation Approach to Regulating AI, supra note 5,\nat 6.\n\n19 HM Gov't, CP 525, National AI Strategy (Sept. 2021), | 27, https:/ / perma.cc/7KTD-VYM2; Dep't for Digit.,\nCulture, Media & Sport, Establishing a Pro-Innovation Approach to Regulating AI, supra note 5, at 6-7.\n", "vlm_text": "Examples of cross-sector legislation that regulates aspects of the use and development of AI  include the following:  \n\n \n•   Data protection laws, which include provisions on automated decision making and data  processing. Breaches of this act can result in fines of up to £17.5 million (approximately  $\\mathrm{US}\\S22$    million), or  $4\\%$   of a company’s global turnover.   \n\n •   Equality laws, which prohibit the government from discriminating, harassing, or victimizing  anyone who has a protected characteristic.  \n\n •   The National Security and Investment Act 2021 provides the government with the authority  to intervene in acquisitions that could harm the national security of the UK, including  companies that develop AI.    \nSector-specific legislation is also in place in certain areas, such as for medical devices.   Competition law includes provisions on protecting consumers against unfair use of AI in  algorithms. Breaches of competition law can result in fines of up to   $10\\%$   of a company’s global  turnover, imprisonment, and disqualification of company directors for up to 15 years.    \nThe UK is reviewing its approach to the regulation of AI. A paper by the Department for Digital,  Culture, Media and Sport, presented to the Parliament in July 2022, notes “[t]he extent to which  UK laws apply to AI is often a matter of interpretation, making them hard to navigate.” 18  The  paper and the National AI Strategy, published in September 2021, state that there is a need for  greater clarity in both the legal framework and among regulators to ensure there are no overlaps  or gaps in areas of regulation and that regulators adopt a consistent approach to AI.    \nAn impact assessment on the regulation of AI noted the following: "}
{"page": 71, "image_path": "page_images/2023555908_71.jpg", "ocr_text": "Safety and Security of Artificial Intelligence Systems: United Kingdom\n\nThe current UK regulatory regime only partially addresses the challenges posed by the\nwidespread adoption and use of AI. This can be summarised as ‘regulatory failure’, which\narises due to the ‘complex patchwork of legal and regulatory requirements’ that are\ncurrently applied to AI systems. Two key issues summarise this regulatory failure. The\nfirst, legal and regulatory requirements are highly complex to understand and will become\nmore so in the future. Secondly, rights, duties and responsibilities are not well defined.”\n\nThe impact assessment further noted the current regime means that the development of\ntechnology is rapidly outpacing the speed at which regulatory oversight and the legislative\nframework can keep up.?! The impact assessment noted “[a] clear governance framework for AI\nsystems, with the agility to keep up with the rapid pace of technological change, will address\ncurrent issues whilst future-proofing the regulatory regime from emerging risks.”\n\nWhile the government has stated it is not currently planning to introduce legislation to regulate\nAI, it has accepted that there may be a need later to “enhance regulatory powers, ensure\nregulatory coordination, or to create new institutional architecture.”\n\nB. AI Policy\n\nThe UK has stated that it intends its approach to the regulation of AI to be context specific, pro-\ninnovation and risk-based, coherent, and proportionate and adaptable, that is, a light- touch,\nforward-looking framework.” It is encouraging regulators to adopt a voluntary or guidance-\nbased approach, which details how the principles interact with relevant legislation to aid\ndevelopers and users with compliance, and the creation of sandboxes for experimenting with\ntechnology.”* It has established the Office for AI to “coordinate cross-government processes to\naccurately assess long term AI and safety risks.”2”\n\nThere are a number of government reports and policies that address, or touch upon, AI.?8 The\ngovernment released its National AI Strategy in 2022. This is a 10-year plan that aims to help\n\n20 Dep’t for Sci., Innovation & Tech., RPC-DCMS-5260(1), Artificial Intelligence Regulation Impact Assessment\n(Mar. 2023), supra note 12, { 32.\n\n2 Td. 434.\n22 d.\n\n23 Dep’t for Digit., Culture, Media & Sport, Establishing a Pro-Innovation Approach to Regulating AI, supra note 5,\nat 17.\n\n241d. at1.\n\n25 Dep’t for Sci., Innovation & Tech., CP 815, A Pro-Innovation Approach to AI Regulation (Mar. 2023), supra note\n3, § 63.\n\n26 Id. sec. 3.3.4.\n\n27 HM Gov't, CP 525, National AI Strategy (Sept. 2021), National AI Strategy, supra note 19, at 60.\n\n28 See, e.g., National Cyber and Security Centre, Principles for the Security of Machine Learning (Aug. 2022),\nhttps:/ / perma.cc/ FU2M-2A8N; Equality and Human Rights Commission, Strategic Plan 2022-25 (Mar. 2022),\nhttps:/ / perma.cc/B7LQ-JNSZ; ICO & Alan Turing Inst., Explaining Decisions Made with AI (Oct. 2022) at 40,\nhttps:/ / perma.cc/ H6PT-WLLH; ICO, Guidance on AI and Data Protection 14 (last updated Mar. 15, 2023),\nhttps:/ / perma.cc/8WJF-GCYJ; Department for Science, Innovation & Technology, Science & Technology\n\nThe Law Library of Congress 69\n", "vlm_text": "The current UK regulatory regime only partially addresses the challenges posed by the  widespread adoption and use of AI. This can be summarised as ‘regulatory failure’, which  arises due to the ‘complex patchwork of legal and regulatory requirements’ that are  currently applied to AI systems. Two key issues summarise this regulatory failure. The  first, legal and regulatory requirements are highly complex to understand and will become  more so in the future. Secondly, rights, duties and responsibilities are not well defined.   \nThe impact assessment further noted the current regime means that the development of  technology is rapidly outpacing the speed at which regulatory oversight and the legislative  framework can keep up.  The impact assessment noted “[a] clear governance framework for AI  systems, with the agility to keep up with the rapid pace of technological change, will address  current issues whilst future-proofing the regulatory regime from emerging risks.” 22    \nWhile   the government has stated it is not currently planning to introduce legislation to regulate  AI, it has accepted that there may be a need later to “enhance regulatory powers, ensure  regulatory coordination, or to create new institutional architecture.” 23   \nB.  AI Policy  \nThe UK has stated that it intends its approach to the regulation of AI to be context specific, pro- innovation and risk-based, coherent, and proportionate and adaptable, that is, a light- touch,  forward-looking framework.  It is encouraging regulators to adopt a voluntary or guidance- based approach, which details how the principles interact with relevant legislation to aid  developers and users with compliance,  and the creation of sandboxes for experimenting with  technology.  It has established the Office for AI to “coordinate cross-government processes to  accurately assess long term AI and safety risks.” 27   \nThere are a number of government reports and policies that address, or touch upon, AI.  The  government released its National AI Strategy in 2022. This is a 10-year plan that aims to help  ensure the UK continues to lead the world in both developing and harnessing AI.  The National  AI Strategy sets out three pillars that aim to develop AI in the UK. Pillar 1 involves “investing in  the long term needs of the AI ecosystem,” which considers the “critical inputs that underpin AI  innovation.” 30  To do this effectively requires supporting the UK’s research, development, and  innovation systems; 31  training, attracting,  and retaining a diverse range of skilled AI  developers; 33  providing access to data and computer resources to developers; 34  providing a pro- innovation environment; and ensuring access to global markets for AI developers.  Under this  pillar, the government aims to narrow the gap between the supply and demand for AI skills.   "}
{"page": 72, "image_path": "page_images/2023555908_72.jpg", "ocr_text": "ensure the UK continues to lead the world in both developing and harnessing AI.” The National\nAI Strategy sets out three pillars that aim to develop AI in the UK. Pillar 1 involves “investing in\nthe long term needs of the AI ecosystem,” which considers the “critical inputs that underpin AI\ninnovation.”*° To do this effectively requires supporting the UK’s research, development, and\ninnovation systems;! training, attracting? and retaining a diverse range of skilled AI\ndevelopers; providing access to data and computer resources to developers;*4 providing a pro-\ninnovation environment; and ensuring access to global markets for AI developers.*> Under this\npillar, the government aims to narrow the gap between the supply and demand for AI skills.%6\n\nPillar 2 aims to support AI to benefit the economy across all sectors and regions. To achieve this,\namong other things, the government will launch programs to help increase both “the\ndevelopment and adoption of AI technologies in high-potential, lower-Al maturity sectors”;\nlaunch a national strategy in health and social care; and publish its defense AI strategy.°”\n\nPillar 3 requires the effective governance of AI in a manner that “encourages innovation,\ninvestment, [and] protects the public and safeguards our fundamental values, while working\nwith global partners to promote the responsible development of AI internationally.”*8 To help\nachieve this, the government has proposed an AI governance framework, discussed below, which\naims to provide clarity for the regime that governs AI and\n\ne support the development of AI assurance tools and services to provide information about AI\nsystems to users and regulators,\n\nFramework (2023), https:/ / perma.cc/ KH94-TTY9; HM Government, National Cyber Strategy 2022 (Dec. 2022),\nhttps:/ / perma.cc/PU3Z-3L9Q; Department for Culture, Media & Sport & the Department for Digital, Culture,\nMedia & Sport, Digital Regulation: Driving Growth and Unlocking Innovation (last updated June 2022),\n\nhttps:/ / perma.cc/9XVU-7DTD; Ministry of Defence, Ambitious, Safe, Responsible: Our Approach to the Delivery of\nAl-enabled Capability in Defence (June 2022), https:/ / perma.cc/5DS9-5NA5.\n\n29 HM Gov't, CP 525, National AI Strategy (Sept. 2021), supra note 19.\n30 Td. at 22.\n\n31 See, e.g., Advanced Rsch. & Invention Agency et al., Advanced Research and Invention Agency (ARIA): Policy\nStatement (Mar. 19, 2021) https:/ /perma.cc/VM7M-3]FF.\n\n32 See, e.g., Home Off., Global Business Mobility Routes: Ver. 5.0, (May 31, 2023), https:/ /perma.cc/KV3D-\n8MQW,; Get Support to Move Your Business to the UK - The Global Entrepreneur Program, Great.gov.uk,\nhttps:/ / perma.cc/ FHZ8-VETH.\n\n33 See, e.g., Dep’t for Educ., CP 338, Skills for Jobs: Lifelong Learning for Opportunity and Growth (Jan. 2021),\nhttps:/ / perma.cc/4QB2-Y3QQ.\n\n34 See further, Dep’t for Digit., Culture, Media & Sport & Dep’t for Sci., Innovation & Tech., National Data\nStrategy (last updated Dec. 9, 2020), https: / / perma.cc/3DVC-GF8M.\n\n35 HM Gov't, CP 525, National AI Strategy (Sept. 2021), supra note 19, at 22.\n36 Id.\n\n37 Id at 48.\n\n38 Id.\n", "vlm_text": "\nPillar 2 aims to support AI to benefit the economy across all sectors and regions. To achieve this,  among other things, the government will launch programs to help increase both “the  development and adoption of AI technologies in high-potential, lower-AI maturity sectors”;  launch a national strategy in health and social care; and publish its defense AI strategy.   \nPillar 3 requires the effective governance of AI in a manner that “encourages innovation,  investment, [and] protects the public and safeguards our fundamental values, while working  with global partners to promote the responsible development of AI internationally.” 38  To help  achieve this, the government has proposed an AI governance framework, discussed below, which  aims to provide clarity for the regime that governs AI and   \n•   support the development of AI assurance tools and services to provide information about AI  systems to users and regulators,      "}
{"page": 73, "image_path": "page_images/2023555908_73.jpg", "ocr_text": "e contribute to the development of global technical standards,\ne help regulators with their responsibilities for Al products and services,\ne work to secure international agreements and standards for AI, and\n\ne safely and ethically deploy AI in the government.*?\n\nThe current proposed AI Framework contains six cross-sectoral principles: pro-innovation,\nproportionate, trustworthy, adaptable, clear, and collaborative built around the following four\nkey elements “designed to empower our existing regulators and promote coherence across the\nregulatory landscape”:\n\ne defining AI based on its unique characteristics to support regulator coordination,\ne adopting a context-specific approach,\n\ne providing a set of cross-sectoral principles to guide regulator responses to AI risks and\nopportunities, and\n\ne delivering new central functions to support regulators to deliver the AI regulatory\nframework, maximizing the benefits of an iterative approach and ensuring that the\nframework is coherent.”\n\nThe framework intends to clarify the government's expectations for responsible AI and describe\ngood governance at all stages of the Al life cycle.*!\n\n8. Existing regulators will be expected to implement the framework underpinned by five\nvalues-focused cross-sectoral principles:\no Safety, security and robustness\no Appropriate transparency and explainability\no Fairness\no Accountability and governance\no Contestability and redress\nThese build on, and reflect our commitment to, the Organisation for Economic Co-\n\noperation and Development (OECD) values-based AI principles, which promote the\nethical use of AI.42\n\n39 Td at 50.\n\n40 Dep’t for Sci., Innovation & Tech., CP 815, A Pro-Innovation Approach to AI Regulation (Mar. 2023), supra note\n3, 4 38.\n\n11d.\n214.4 48.\n", "vlm_text": "•   contribute to the development of global technical standards,  \n\n •   help regulators with their responsibilities for AI products and services,  \n\n •   work to secure international agreements and standards for AI, and  \n\n •   safely and ethically deploy AI in the government.   \nThe current proposed AI Framework contains six cross-sectoral principles: pro-innovation,  proportionate, trustworthy, adaptable, clear, and collaborative built around the following four  key elements “designed to empower our existing regulators and promote coherence across the  regulatory landscape”:  \n\n \n•   defining AI based on its unique characteristics to support regulator coordination,  \n\n •   adopting a context-specific approach,  \n\n •   providing a set of cross-sectoral principles to guide regulator responses to AI risks and  opportunities, and  \n\n •   delivering new central functions to support regulators to deliver the AI regulatory  framework, maximizing the benefits of an iterative approach and ensuring that the  framework is coherent.   \nThe framework intends to clarify the government’s expectations for responsible AI and describe  good governance at all stages of the AI life cycle.   \n8. Existing regulators will be expected to implement the framework underpinned by five  values-focused cross-sectoral principles:     o Safety, security and robustness   o Appropriate transparency and explainability   o Fairness   o Accountability and governance   o Contestability and redress     These build on, and reflect our commitment to, the Organisation for Economic Co- operation and Development (OECD) values-based AI principles, which promote the  ethical use of AI.   "}
{"page": 74, "image_path": "page_images/2023555908_74.jpg", "ocr_text": "The principles will initially operate on a non-statutory basis and will be implemented by current\nregulators based upon their areas of prioritization. The intention behind this approach is to\nprovide clarity and give flexibility to the regulators on how they respond while not impeding the\ninnovation of AI.“4 The government has noted that after a period of time, it “intends to introduce\na statutory obligation on regulators as a ‘duty to regard’ the principles” set out in the Pro-\ninnovation Approach to AI Regulation (2023) paper when parliamentary time allows.\n\nThe UK has also adopted a number of policy changes to help the development of AI in the UK,\namong them, the following:\n\ne new visa routes for those in the Al industry,‘¢\n\ne reformed research and development tax relief, including data and cloud computing costs,4”\nand\n\ne a pilot AI Standards Hub to increase the UK’s participation in the development of global\ntechnical standards for AI.\n\nThe government “currently sees the advantages of a ‘test and learn’ approach given the novelty\nof regulation in this policy area. This means evidence gathering, monitoring and evaluation will\ncontinue throughout the implementation of the framework’* and it has received widespread\nsupport for adopting this method.\n\nIII. Definition of Artificial Intelligence (AI) Systems\n\nWhen considering a definition of AI, the government has stated that “no single definition is going\nto be suitable for every scenario.”*! The UK acknowledged the move by the European Union (EU)\nto provide a definition of AI, but it rejected this approach, noting that it does “not think that it\ncaptures the full application of AI and its regulatory implications. Our concern is that this lack of\n\n81d. 4 55, 57; Dep’t for Sci., Innovation & Tech., RPC-DCMS-5260(1), Artificial Intelligence Regulation Impact\nAssessment (Mar. 2023), supra note 12, § 114.\n\n#4 Dep’t for Sci., Innovation & Tech., CP 815, A Pro-Innovation Approach to AI Regulation (Mar. 2023), supra note\n3, at1.\n\n45 Dep’t for Sci., Innovation & Tech., RPC-DCMS-5260(1), Artificial Intelligence Regulation Impact Assessment\n(Mar. 2023), supra note 12, 5.\n\n46 Work in the UK as a Leader in Digital Technology (Global Talent Visa), Gov.uk, https:/ / perma.cc/ W5M4-TGUX.\n47 Guidance: National AI Strategy - AI Action Plan (July 18, 2022), Gov.uk, https:// perma.cc/E4VV-BJS2.\n\n48 Press Release, Dep’t for Digit., Culture, Media & Sport, Off. for A.I. & the Rt. Hon. Chris Philp MP, New UK\nInitiative to Shape Global Standards for Artificial Intelligence (Jan. 12, 2022), https:/ /perma.cc/5A4H-JYQY.\n\n49 Guidance: National AI Strategy - HTML Version, Dep’t for Bus., Energy & Indus. Strategy, supra note 7.\n\n5° Dep’t for Sci., Innovation & Tech., RPC-DCMS-5260(1), Artificial Intelligence Regulation Impact Assessment\n(Mar. 2023), supra note 12, {| 75.\n\n51 Guidance: National AI Strategy - HTML Version, Dep’t for Bus., Energy & Indus. Strategy, supra note 7.\n", "vlm_text": "The principles will initially operate on a non-statutory basis and will be implemented by current  regulators based upon their areas of prioritization.  The intention behind this approach is to  provide clarity and give flexibility to the regulators on how they respond while not impeding the  innovation of AI.  The government has noted that after a period of time, it “intends to introduce  a statutory obligation on regulators as a ‘duty to regard’ the principles” 45  set out in the Pro- innovation Approach to AI Regulation (2023) paper when parliamentary time allows.  \nThe UK has also adopted a number of policy changes to help the development of AI in the UK,  among them, the following: \n\n \n•   new visa routes for those in the AI industry,  \n\n •   reformed research and development tax relief, including data and cloud computing costs,   and \n\n •   a pilot AI Standards Hub to increase the UK’s participation in the development of global  technical standards for AI.   \nThe government “currently sees the advantages of a ‘test and learn’ approach given the novelty  of regulation in this policy area. This means evidence gathering, monitoring and evaluation will  continue throughout the implementation of the framework” 49  and it has received widespread  support for adopting this method.   \nIII.  Definition of Artificial Intelligence (AI) Systems  \nWhen considering a definition of AI, the government has stated that “no single definition is going  to be suitable for every scenario.” 51  The UK acknowledged the move by the European Union (EU)  to provide a definition of AI, but it rejected this approach, noting that it does “not think that it  captures the full application of AI and its regulatory implications. Our concern is that this lack of  granularity could hinder innovation.  $^{\\prime\\prime}52$   There are multiple definitions of AI across different  government papers and in legislation.  "}
{"page": 75, "image_path": "page_images/2023555908_75.jpg", "ocr_text": "Safety and Security of Artificial Intelligence Systems: United Kingdom\n\ngranularity could hinder innovation.”®? There are multiple definitions of AI across different\ngovernment papers and in legislation.\n\nThe AI Strategy defines AI broadly as “[m]achines that perform tasks normally performed by\nhuman intelligence, especially when the machines learn from data how to do those tasks.”3 The\ngovernment has stated this definition is “sufficient for our purposes.”54\n\nThe proposed AI framework defines AI by reference to the two functional characteristics that\ncause the need for a unique regulatory response: adaptability and autonomy. Specifically,\n\ne = The ‘adaptivity’ of AI can make it difficult to explain the intent or logic of the system’s\noutcomes:\no AI systems are ‘trained’ - once or continually - and operate by inferring\npatterns and connections in data which are often not easily discernible\nto humans.\no Through such training, AI systems often develop the ability to perform new\nforms of inference not directly envisioned by their human programmers.\n¢ The ‘autonomy’ of AI can make it difficult to assign responsibility for outcomes:\no Some AI systems can make decisions without the express intent or ongoing\ncontrol of a human.\n\nA legal definition of artificial intelligence is contained in schedule 3 of the National Security and\nInvestment Act 2021 (Notifiable Acquisition) (Specification of Qualifying Entities) Regulations\n2021, which the government notes is different to the definition contained in the AI Strategy “due\nto the clarity needed for legislation.”® The definition states,\n\n“artificial intelligence” means technology enabling the programming or training of a\ndevice or software to —\n(i) perceive environments through the use of data;\n\n(ii) interpret data using automated processing designed to approximate cognitive\nabilities; and\n\n(iii) make recommendations, predictions or decisions; with a view to achieving a\nspecific objective[.]>”\n\n52 Dep’t for Digit., Culture, Media and Sport, Establishing a Pro-Innovation Approach to Regulating AI, supra\nnote 5, at 8.\n\n53 Guidance: National AI Strategy - HTML Version, Dep’t for Bus., Energy & Indus. Strategy, supra note 7.\n41d.\n\n55 Dep’t for Sci., Innovation & Tech., CP 815, A Pro-Innovation Approach to AI Regulation (Mar. 2023), supra note\n3, 4 39.\n\n5¢ Guidance: National AI Strategy - HTML Version, Dep’t for Bus., Energy & Indus. Strategy, supra note 7.\n\n5? National Security and Investment Act 2021 (Notifiable Acquisition) (Specification of Qualifying Entities)\nRegulations 2021, SI 2021/1264, sch. 3, https:// perma.cc/3TPH-Y6YK.\n\nThe Law Library of Congress\n", "vlm_text": "\nThe AI Strategy defines AI broadly as “[m]achines that perform tasks normally performed by  human intelligence, especially when the machines learn from data how to do those tasks.” 53  The  government has stated this definition is “sufficient for our purposes.” 54    \nThe proposed AI framework defines AI by reference to the two functional characteristics that  cause the need for a unique regulatory response: adaptability and autonomy. Specifically,  \n•   The ‘adaptivity’ of AI can make it difficult to explain the intent or logic of the system’s  outcomes:   o   AI systems are ‘trained’ – once or continually – and operate by inferring  patterns and connections in data which are often not easily discernible  to humans.  o   Through such training, AI systems often develop the ability to perform new  forms of inference not directly envisioned by their human programmers.  •   The ‘autonomy’ of AI can make it difficult to assign responsibility for outcomes:   o   Some AI systems can make decisions without the express intent or ongoing  control of a human.   \nA legal definition of artificial intelligence is contained in schedule 3 of the National Security and  Investment Act 2021 (Notifiable Acquisition) (Specification of Qualifying Entities) Regulations  2021, which the government notes is different to the definition contained in the AI Strategy “due  to the clarity needed for legislation.” 56  The definition states,  \n“artificial intelligence” means technology enabling the programming or training of a  device or software to—  \n(i)   perceive environments through the use of data;   (ii)   interpret data using automated processing designed to approximate cognitive  abilities; and   (iii)   make recommendations, predictions or decisions; with a view to achieving a  specific objective[.] 57    "}
{"page": 76, "image_path": "page_images/2023555908_76.jpg", "ocr_text": "IV. Cybersecurity of AI\n\nThe current proposed regulatory framework intends that the trustworthiness of AI will be\nachieved through using tools such as “assurance techniques, voluntary guidance and technical\nstandards.”5§ The Centre for Data Ethics and Innovation has compiled a portfolio of assurance\ntechniques that can be used by those in the AI industry. The portfolio provides “examples of AI\nassurance techniques being used in the real-world to support the development of\ntrustworthy AI.”59\n\nAn Impact Assessment on AI notes that the rights, duties, and responsibilities relating to Al in\nthe UK are not well defined. It states, “[k]ey gaps in the UK’s current legal frameworks relate to\nindividual rights, safety standards specific to AI, transparency, human involvement,\naccountability, and rights to redress.”° This leads to uncertainty over liability for an AI system,\nwhich “means businesses cannot be certain whether they may be liable for harms related to the\nimplementation of an AI system. It can also create a lack of trust in Al systems by consumers, as\nthey are not aware of who is ultimately responsible if an AI system causes harm.”6!\n\nA. Data and Data Governance\n\nTo help protect against adversarial machine learning, the National Cyber and Security Centre\npublished the Principles for the Security of Machine Learning in August 2022. The principles, which\napply to “anyone developing, deploying or operating a system with a machine learning\ncomponent” aim to “provide context and structure to help scientists, engineers, decision makers\nand risk owners make education decisions about system design and development processes,\nhelping to assess the specific threats to a system.”\n\nThe principles during development are the following:\ne Enable your developers, which means that threats specific to machine learning systems are\n\nunderstood by developers, who should have appropriate tools to assess vulnerabilities.\n\ne Design for security, which means being able to identify whether the system has an error\ncaused by an attack or another reason, what should occur after an error, and what mitigations\nshould be put in place to prevent such an error from occurring.\n\n58 Dep’t for Sci., Innovation & Tech., CP 815, A Pro-Innovation Approach to AI Regulation (Mar. 2023), supra note\n3, 9 16.\n\n59 Guidance CDEI Portfolio of AI Assurance Techniques, Gov.uk (June 7, 2023), https:/ / perma.cc/6EL8-TJ9B.\n\n6 Dep’t for Sci., Innovation & Tech., RPC-DCMS-5260(1), Artificial Intelligence Regulation Impact Assessment,\n(Mar. 2023), supra note 12, 4 35.\n\n“Id. | 36.\n\n62 Nat’l Cyber Sec. Ctr., Principles for the Security of Machine Learning (Aug. 2022), https:/ / perma.cc/FU2M-\n2A8N.\n", "vlm_text": "IV.  Cybersecurity of AI  \nThe current proposed regulatory framework intends that the trustworthiness of AI will be  achieved through using tools such as “assurance techniques, voluntary guidance and technical  standards.” 58  The Centre for Data Ethics and Innovation has compiled a portfolio of assurance  techniques that can be used by those in the AI industry. The portfolio provides “examples of AI  assurance techniques being used in the real-world to support the development of  trustworthy AI.” 59   \nAn Impact Assessment on AI notes that the rights, duties, and responsibilities relating to AI in  the UK are not well defined. It states, “[k]ey gaps in the UK’s current legal frameworks relate to  individual rights, safety standards specific to AI, transparency, human involvement,  accountability, and rights to redress.” 60  This leads to uncertainty over liability for an AI system,  which “means businesses cannot be certain whether they may be liable for harms related to the  implementation of an AI system. It can also create a lack of trust in AI systems by consumers, as  they are not aware of who is ultimately responsible if an AI system causes harm.” 61   \nA.   Data and Data Governance   \nTo help protect against adversarial machine learning, the National Cyber and Security Centre  published the  Principles for the Security of Machine Learning  in August 2022. The principles, which  apply to “anyone developing, deploying or operating a system with a machine learning  component” aim to “provide context and structure to help scientists, engineers, decision makers  and risk owners make education decisions about system design and development processes,  helping to assess the specific threats to a system.” 62    \nThe principles during development are the following: \n\n \n•   Enable your developers, which means that threats specific to machine learning systems are  understood by developers, who should have appropriate tools to assess vulnerabilities.  \n\n \n•   Design for security, which means being able to identify whether the system has an error  caused by an attack or another reason, what should occur after an error, and what mitigations  should be put in place to prevent such an error from occurring.  "}
{"page": 77, "image_path": "page_images/2023555908_77.jpg", "ocr_text": "Minimize an adversary’s knowledge, which means understanding the risks of disclosing\ninformation and making “a balanced assessment of the benefits and risks of sharing\ninformation about [the] systems.”\n\nDesign for security (vulnerabilities), which means that the vulnerability of the system should\nbe continually assessed against the risks.\n\nSecure the supply chain, which means that trusted sources should be used for data and\nmodels and validation and verification processes should be used to mitigate risks and\nmistakes. This can help against data poisoning, where an adversary can mislabel or insert\ntriggers that can result in degraded performance or a loss of integrity in the output.\n\nSecure your infrastructure (development environment), which means that security should be\nintroduced for the training and development environment for the model and anything or\nanyone that enters this environment. Thus, appropriate quality assurance and quality control\nprocesses should be in place for the supply chain.\n\nSecure your infrastructure (digital assets), which means digital assets should be protected at\nall stages.\n\nTrack your asset, which means that the creation, operation and life of models and datasets\nshould be documented. Any changes should be monitored and recorded.\n\nDesign for security (model architecture), which means that the model architecture and\ncapacity should be kept proportionate to the dataset size and requirements.\n\nThe principles during deployment are\n\nSecure your infrastructure (deployment), which means that the implications of the\ninformation available to users about the model should be considered and limited.\n\nDesign for security, which means that the use of the database should be monitored, user\nrequests should be logged, and consideration should be given to implementing an alert\nsystem for potential compromise.\n\nMinimize an adversary’s knowledge, which means that a balance between transparency and\nsecurity should be determined.\n\nThe principles during operation are\n\nDesign for security, which means that, if continual learning is used, its risks are understood,\nand that systems and processes are in place to help prevent an adversary from impacting the\nmodel’s behavior.\n\n6 Id. at 11.\n\n64 Td.\n5 Id.\n", "vlm_text": "•   Minimize an adversary’s knowledge, which means understanding the risks of disclosing  information and making “a balanced assessment of the benefits and risks of sharing  information about [the] systems.” 63  \n\n \n•   Design for security (vulnerabilities), which means that the vulnerability of the system should  be continually assessed against the risks. \n\n \n•   Secure the supply chain, which means that trusted sources should be used for data and  models and validation and verification processes should be used to mitigate risks and  mistakes. This can help against data poisoning, where an adversary can mislabel or insert  triggers that can result in degraded performance or a loss of integrity in the output. \n\n \n•   Secure your infrastructure (development environment), which means that security should be  introduced for the training and development environment for the model and anything or  anyone that enters this environment. Thus, appropriate quality assurance and quality control  processes should be in place for the supply chain. \n\n \n•   Secure your infrastructure (digital assets), which means digital assets should be protected at  all stages.  \n\n \n•   Track your asset, which means that the creation, operation and life of models and datasets  should be documented. Any changes should be monitored and recorded. \n\n \n•   Design for security (model architecture), which means that the model architecture and  capacity should be kept proportionate to the dataset size and requirements.   \nThe principles during deployment are \n\n \n•   Secure your infrastructure (deployment), which means that the implications of the  information available to users about the model should be considered and limited. \n\n \n•   Design for security, which means that the use of the database should be monitored, user  requests should be logged, and consideration should be given to implementing an alert  system for potential compromise. \n\n \n•   Minimize an adversary’s knowledge, which means that a balance between transparency and  security should be determined.   \nThe principles during operation are   \n•   Design for security, which means that, if continual learning is used, its risks are understood,  and that systems and processes are in place to help prevent an adversary from impacting the  model’s behavior.  "}
{"page": 78, "image_path": "page_images/2023555908_78.jpg", "ocr_text": "e Track your asset, which means that when continual learning is conducted, updates should be\nvalidated in the same manner as new models or datasets.\n\nThe principles during the end of life are\n\ne Minimize an adversary’s knowledge, which means that assets should be appropriately\ndecommissioned through archiving or destruction.\n\ne Enable your developers, which means that information learned from the development and\noperation of the model should be documented and shared.°”\n\nB. Recordkeeping\n\nThe UK General Data Protection Regulation (UK GDPR) contains a recordkeeping requirement\nfor personal data, which is intended to help businesses comply with the law and let individuals\nknow how their data is being used and with whom it is being shared. The government believes\nthat the recordkeeping requirement, in its current form, is burdensome and duplicative of other\nrequirements in the UK GDPR. The government notes that while the recordkeeping requirement\nwill be removed, organizations will still need to document the purposes for which data is being\nprocessed, and it is currently working to create a new, flexible system that “encourages\norganisations to focus on the design of their privacy management programme.”\n\nC. Transparency and Provision of Information to Users\n\nTo ensure appropriate standards of transparency and understandability, the government has\nstated that it anticipates regulators will need to set expectations for those involved in AI to\nprovide information about\n\no the nature and purpose of the AI in question including information relating to\nany specific outcome,\n\no the data being used and information relating to training data,\n\no the logic and process used and where relevant information to support\nexplainability of decision-making and outcomes,\n\n© accountability for the AI and any specific outcomes.”\n\nRegulators should consult the existing technical standards to set requirements for\nunderstandability “to ensure appropriate balance between information needs for regulatory\nenforcement (e.g. around safety) and technical tradeoffs with system robustness.””° The proposed\n\n66 Id.\n67 Id.\n\n® Dep't for Digit., Culture, Media & Sport, Data: A New Direction — Government Response to Consultation (last\nupdated June 23, 2022), https:/ / perma.cc/ WPM7-WBG8.\n\n6 Dep’t for Sci., Innovation & Tech., CP 815, A Pro-Innovation Approach to AI Regulation (Mar. 2023), supra note\n3, Annex A.\n\n70 Td.\n", "vlm_text": "•   Track your asset, which means that when continual learning is conducted, updates should be  validated in the same manner as new models or datasets.   \nThe principles during the end of life are \n\n \n•   Minimize an adversary’s knowledge, which means that assets should be appropriately  decommissioned through archiving or destruction. \n\n \n•   Enable your developers, which means that information learned from the development and  operation of the model should be documented and shared.   \nB.   Recordkeeping  \nThe UK General Data Protection Regulation (UK GDPR) contains a recordkeeping requirement  for personal data, which is intended to help businesses comply with the law and let individuals  know how their data is being used and with whom it is being shared. The government believes  that the recordkeeping requirement, in its current form, is burdensome and duplicative of other  requirements in the UK GDPR. The government notes that while the recordkeeping requirement  will be removed, organizations will still need to document the purposes for which data is being  processed, and it is currently working to create a new, flexible system that “encourages  organisations to focus on the design of their privacy management programme.” 68    \nC.   Transparency and Provision of Information to Users  \nTo ensure appropriate standards of transparency and understand ability, the government has  stated that it anticipates regulators will need to set expectations for those involved in AI to  provide information about   \no   the nature and purpose of the AI in question including information relating to  any specific outcome,   o   the data being used and information relating to training data,   o   the logic and process used and where relevant information to support  explainability of decision-making and outcomes,   o   accountability for the AI and any specific outcomes.    \nRegulators should consult the existing technical standards to set requirements for  understand ability “to ensure appropriate balance between information needs for regulatory  enforcement (e.g. around safety) and technical tradeoffs with system robustness.” 70  The proposed  framework notes that the level of transparency and understand ability should be proportionate to  the risks posed by the AI system.   "}
{"page": 79, "image_path": "page_images/2023555908_79.jpg", "ocr_text": "framework notes that the level of transparency and understandability should be proportionate to\nthe risks posed by the AI system.”!\n\nWhen personal data is used in AI, the Information Commissioner’s Office (ICO) notes that, to\nensure a decision made with AI is explainable, certain principles should be followed.\n\nTo ensure that the decisions you make using AI are explainable, you [the operator] should\nfollow four principles:\n\ne be transparent;\ne be accountable;\n¢ consider the context you are operating in; and,\n\ne reflect on the impact of your AI system on the individuals affected, as well as\nwider society.”\nThe ICO has provided six ways that an AI decision can be explained.\n\n¢ Rationale explanation: the reasons that led to a decision, delivered in an accessible\nand non-technical way.\n\n¢ Responsibility explanation: who is involved in the development, management and\nimplementation of an AI system, and who to contact for a human review of a decision.\n\n¢ Data explanation: what data has been used in a particular decision and how.\n\ne Fairness explanation: steps taken across the design and implementation of an AI\nsystem to ensure that the decisions it supports are generally unbiased and fair, and\nwhether or not an individual has been treated equitably.\n\n¢ Safety and performance explanation: steps taken across the design and\nimplementation of an AI system to maximise the accuracy, reliability, security and\nrobustness of its decisions and behaviours.\n\n¢ Impact explanation: steps taken across the design and implementation of an AI system\nto consider and monitor the impacts that the use of an AI system and its decisions has\nor may have on an individual, and on wider society.”\n\nD. Human Oversight\n\nA review by the Committee on Standards in Public Life states, “[h]uman oversight of AI is a\nstandards imperative. To ensure that public bodies remain accountable for automated decision-\nmaking, there needs to be internal control over the AI system, its decision-making process and its\noutcomes.””4 The report found that the type of oversight mechanisms that should be in place\ndepended upon systems and risks posed, noting that if the risk is low, oversight by senior\n\n711d. § 52.\n7? 1CO & Alan Turing Inst., Explaining Decisions Made with AI, supra note 13, at 40.\n31d. at 21.\n\n74 Comm. on Standards in Public Life, Artificial Intelligence and Public Standards § 5.5.3 (Feb. 2020),\nhttps:/ / perma.cc/44S6-5CDL.\n", "vlm_text": "\nWhen personal data is used in AI, the Information Commissioner’s Office (ICO) notes that, to  ensure a decision made with AI is explainable, certain principles should be followed.  \nTo ensure that the decisions you make using AI are explainable, you [the operator] should  follow four principles:  •   be transparent;  •   be accountable;  •   consider the context you are operating in; and,  •   reflect on the impact of your AI system on the individuals affected, as well as  wider society.    \nThe ICO has provided six ways that an AI decision can be explained.  \n•   Rationale explanation:  the reasons that led to a decision, delivered in an accessible  and non-technical way.   •   Responsibility explanation:  who is involved in the development, management and  implementation of an AI system, and who to contact for a human review of a decision.   •   Data explanation:  what data has been used in a particular decision and how.   •   Fairness explanation:  steps taken across the design and implementation of an AI  system to ensure that the decisions it supports are generally unbiased and fair, and  whether or not an individual has been treated equitably.   •   Safety and performance explanation:  steps taken across the design and  implementation of an AI system to maximise the accuracy, reliability, security and  robustness of its decisions and behaviours.   •   Impact explanation:  steps taken across the design and implementation of an AI system  to consider and monitor the impacts that the use of an AI system and its decisions has  or may have on an individual, and on wider society.   \nD.   Human Oversight  \nA review by the Committee on Standards in Public Life states, “[h]uman oversight of AI is a  standards imperative. To ensure that public bodies remain accountable for automated decision- making, there needs to be internal control over the AI system, its decision-making process and its  outcomes.” 74  The report found that the type of oversight mechanisms that should be in place  depended upon systems and risks posed, noting that if the risk is low, oversight by senior  management would be sufficient, but in higher risk areas “external scrutiny may be necessary.” 75   The report further noted “[t]o have complete control over their AI systems, senior leadership need  to have oversight over the whole AI process, from the point of data entry to the implementation  of an AI-assisted decision.” 76    "}
{"page": 80, "image_path": "page_images/2023555908_80.jpg", "ocr_text": "management would be sufficient, but in higher risk areas “external scrutiny may be necessary.”\nThe report further noted “[t]o have complete control over their Al systems, senior leadership need\nto have oversight over the whole AI process, from the point of data entry to the implementation\nof an Al-assisted decision.””6\n\nWhile there is no specific mention of human oversight in the proposed AI framework, it does note\nthat clear lines of accountability should be provided for across the AI life cycle.””7 With regard to\naccountability, the proposed framework provides that any guidance issued from regulators\n“should reflect that accountability’ refers to the expectation that organisations or individuals will\nadopt appropriate measures to ensure the proper functioning, throughout their life cycle, of the\nAI systems that they research, design, develop, train, operate, deploy, or otherwise use.”78 The\ngovernment notes that the establishment of lines of ownership and accountability is essential to\nprovide business certainty and help ensure regulatory compliance.”\n\nReferences to human oversight are made with reference to the use of AI and its interaction with\npersonal data in the UK GDPR, as human interaction in AI systems means that it is not a solely\nautomated process and different provisions of the act apply, as discussed below.*?\n\nE. Risk Management System\n\nThe British Standards Institute (BSI) and the AI Standards Hub has worked with the international\nAl committee to develop technical standards for AI. It is also active at the European level in CEN-\nCENELEC/JTC 21. The BSI notes that one of the key standards it has worked on developing is\nfor risk management in AI, ISO/IEC 23894,8! which was published in February 2022.82 The AI\nStandards Hub notes that, while general principles can be relied upon, there is a\n\nneed to flag key considerations for risk in the AI lifecycle. AI systems operate on a far more\ncomplex level than other technologies, resulting in a greater number of sources of risk.\nThey will introduce new or emerging risks for organisations, with positive or negative\nimplications for strategic objectives, and changes to existing risk profiles.\n\nId.\n76 Id.\n71d. 452.\n78 Id.\n1d.\n\n80 (EU) 2016/679 of the European Parliament and of the Council of 27 April 2016 art. 22,\n\nhttps:/ / perma.cc/ MP6N-EF7W. When the UK left the EU, it incorporated all EU law as it existed on December\n31, 2020, into a new body of domestic law known as “retained EU legislation.” References to the UK GDPR\nthroughout this report refer to the EU GDPR incorporated in the domestic law of the UK.\n\n81 Artificial Intelligence Standards Development, British Standards Inst., https:/ / perma.cc/88D3-8JZ6.\n\n82 ISO, ISO/IEC 23894:2023(en) Information Technology — Artificial Intelligence —Guidance on Risk\nManagement, https:/ / perma.cc/ BU6R-KWK8.\n\n83 Tim McGarr, ISOAEC 23894 - A New Standard for Risk Management of Al, Al Standards Hub,\nhttps:/ / perma.cc/4XF9-EZSL.\n", "vlm_text": "\nWhile there is no specific mention of human oversight in the proposed AI framework, it does note  that clear lines of accountability should be provided for across the AI life cycle.  With regard to  accountability, the proposed framework provides that any guidance issued from regulators  “should reflect that ’accountability‘ refers to the expectation that organisations or individuals will  adopt appropriate measures to ensure the proper functioning, throughout their life cycle, of the  AI systems that they research, design, develop, train, operate, deploy, or otherwise use.” 78  The  government notes that the establishment of lines of ownership and accountability is essential to  provide business certainty and help ensure regulatory compliance.   \nReferences to human oversight are made with reference to the use of AI and its interaction with  personal data in the UK GDPR, as human interaction in AI systems means that it is not a solely  automated process and different provisions of the act apply, as discussed below.   \nE.   Risk Management System  \nThe British Standards Institute (BSI) and the AI Standards Hub has worked with the international  AI committee to develop technical standards for AI. It is also active at the European level in CEN- CENELEC/JTC 21. The BSI notes that one of the key standards it has worked on developing is  for risk management in AI, ISO/IEC 23894,  which was published in February 2022.  The AI  Standards Hub notes that, while general principles can be relied upon, there is a  \nneed to flag key considerations for risk in the AI lifecycle. AI systems operate on a far more  complex level than other technologies, resulting in a greater number of sources of risk.  They will introduce new or emerging risks for organisations, with positive or negative  implications for strategic objectives, and changes to existing risk profiles.   \n75  Id.   76  Id.   77  Id. ¶ 52.  78  Id.  79 \n80  (EU) 2016/679 of the European Parliament and of the Council of 27 April 2016 art. 22,  https://perma.cc/MP6N-EF7W. When the UK left the EU, it incorporated all EU law as it existed on December  31, 2020, into a new body of domestic law known as “retained EU legislation.” References to the UK GDPR  throughout this report refer to the EU GDPR incorporated in the domestic law of the UK.  81   Artificial Intelligence Standards Development , British Standards Inst., https://perma.cc/88D3-8JZ6.   82  ISO, ISO/IEC 23894:2023(en) Information Technology—Artificial Intelligence—Guidance on Risk  Management, https://perma.cc/BU6R-KWK8.  83  Tim McGarr,  ISO/IEC 23894 – A New Standard for Risk Management of AI , AI Standards Hub,  https://perma.cc/4XF9-EZSL.   "}
{"page": 81, "image_path": "page_images/2023555908_81.jpg", "ocr_text": "The AI Standards Hub has noted that the AI standards currently in development\n\nare only starting to address questions in the wide-ranging area of safety, security and\nresilience. Additional standards development efforts will be needed to mitigate the\nsignificant cyber security risks society faces each day. For example, much is covered by\nestablished IT standards (e.g., ISO/IEC 27001 in Cyber Security), but it is likely that we\nwill need a bespoke version of 27001 for the AI domain.*\n\nF. Conformity Assessments\n\nWhile the UK is no longer part of the EU, laws of the EU still impact the country. The government\nhas stated, “AI providers need to ensure that their effort is correctly oriented to the full\ncompliance with the EU AI Act.”®5 BSI is working to help those who will be regulated by the EU\nAI Act by providing readiness assessments and algorithm testing.8° One example of this is that\nBSI is accredited as a notified body for medical devices and in vitro diagnostic devices, and it is\nworking to ensure that Al parts and components of medical devices it is responsible for as this\nbody are compliant with the new rules contained in the EU AI Act.8”\n\nG. Robustness\n\nThe government has determined that regulators will need to introduce guidance to ensure safety,\nsecurity and robustness for AI. This guidance should include\n\n¢ considerations of good cybersecurity practices, such as the NCSC principles for the\nsecurity of machine learning, as a secured system should be capable of maintaining\nthe integrity of information.\n\n¢ considerations of privacy practices such as accessibility only to authorised users and\nsafeguards against bad actors.**\n\nThe government recommends that legal persons review existing technical standards to address\nAI safety, security, testing, data quality and robustness. It intends to ensure that regulatory\nguidance on these standards is clarified.89 To ensure robustness, developers of AI should be aware\n“of the specific security threats that could apply at different stages of the AI lifecycle and embed\n\n84 Tim McGarr, Safety, Security and Resilience in Trustworthy AI, AI Standards Hub, https://perma.cc/ V2EW-\nY4Y7.\n\n85 British Standards Institution: EU AI Act Readiness Assessment and Algorithmic Auditing, Gov.uk (June 6, 2023),\nhttps:/ / perma.cc/2AYN-V224.\n\n86 Td.\n87 d.\n\n88 Dep’t for Sci., Innovation & Tech., CP 815, A Pro-Innovation Approach to AI Regulation (Mar. 2023), supra note\n3, Annex A.\n\n89 Id. Examples given are standards ISO/IEC 24029-2*, ISO/IEC 5259-1*, ISO/IEC 5259-3*, ISO/IEC 5259-4*, &\nISO/IEC TR 5469*.\n\n", "vlm_text": "The AI Standards Hub has noted that the AI standards currently in development   \nare only starting to address questions in the wide-ranging area of safety, security and  resilience. Additional standards development efforts will be needed to mitigate the  significant cyber security risks society faces each day. For example, much is covered by  established IT standards (e.g., ISO/IEC 27001 in Cyber Security), but it is likely that we  will need a bespoke version of 27001 for the AI domain.   \nF.  Conformity Assessments  \nWhile the UK is no longer part of the EU, laws of the EU still impact the country. The government  has stated, “AI providers need to ensure that their effort is correctly oriented to the full  compliance with the EU AI Act.” 85  BSI is working to help those who will be regulated by the EU  AI Act by providing readiness assessments and algorithm testing.  One example of this is that  BSI is accredited as a notified body for medical devices and in vitro diagnostic devices, and it is  working to ensure that AI parts and components of medical devices it is responsible for as this  body are compliant with the new rules contained in the EU AI Act.    \nG.    Robustness  \nThe government has determined that regulators will need to introduce guidance to ensure safety,  security and robustness for AI. This guidance should include   \n•   considerations of good cybersecurity practices, such as the NCSC principles for the  security of machine learning, as a secured system should be capable of maintaining  the integrity of information.   \n•   considerations of privacy practices such as accessibility only to authorised users and  safeguards against bad actors.   \nThe government recommends that legal persons review existing technical standards to address  AI safety, security, testing, data quality and robustness. It intends to ensure that regulatory  guidance on these standards is clarified.  To ensure robustness, developers of AI should be aware  “of the specific security threats that could apply at different stages of the AI lifecycle and embed  resilience to these threats into their systems.” 90  The proposed framework notes that, when  implementing this principle, regulators may require regular tests or   "}
{"page": 82, "image_path": "page_images/2023555908_82.jpg", "ocr_text": "resilience to these threats into their systems.” The proposed framework notes that, when\nimplementing this principle, regulators may require regular tests or\n\ndue diligence on the functioning, resilience and security of a system. Regulators may also\nneed to consider technical standards addressing safety, robustness and security to\nbenchmark the safe and robust performance of AI systems and to provide AI life cycle\nactors with guidance for implementing this principle in their remit.\n\nThe UK has also established a National Cyber Strategy that aims to address cyber security issues,\nincluding Al. The aim of this strategy is to ensure the security and resilience of the UK’s\ncyberspace.%2\n\nH. Personal Data Protection\n\nThe UK incorporated the EU’s General Data Protection Regulation (EU GDPR)* into its national\nlaw through the Data Protection Act 2018 (DPA) on May 23, 2018, and, upon leaving the EU, the\nmajority of EU legislation as it stood on December 31, 2020, was incorporated into a new body of\ndomestic UK law, known as retained EU legislation.®! The ICO has noted that the data protection\nlaw does not define AI, so the legal obligations are not dependent upon how it is defined.®\nHowever, the DPA does apply to AI systems.\n\nThe DPA regulates how personal information may be processed, “requiring personal data to be\nprocessed lawfully and fairly, on the basis of the data subject’s consent or another specified\nbasis.”% The following are the six lawful bases for processing data:\n\nwhere an individual has given valid consent to data processing for a specific purpose,\ne if the processing is necessary for contractual purposes,\n\ne to enable compliance with a legal obligation,\n\ne if it is necessary to protect someone's life,\n\ne to perform a task in the public interest, or\n\ne if the processing is necessary for the official functions of the processor and there is not a good,\noverriding reason to protect the personal data.%”\n\nId. § 52.\n\nId.\n\n°2 HM Government, National Cyber Strategy 2022 (Dec. 2022), https:/ / perma.cc/PU3Z-3L9Q.\n98 EU GDPR.\n\n% DPA, c. 12.\n\n%5 ICO, Guidance on Al and Data Protection, supra note 13.\n\nDPA, c. 12, § 2(1)(a).\n\n°7 ICO, Age Appropriate Design: A Code of Practice for Online Services Annex C (Sept. 2, 2020),\nhttps:/ / perma.cc/376E-YMNX.\n\nm\n\n", "vlm_text": "\ndue diligence on the functioning, resilience and security of a system. Regulators may also  need to consider technical standards addressing safety, robustness and security to  benchmark the safe and robust performance of AI systems and to provide AI life cycle  actors with guidance for implementing this principle in their remit.   \nThe UK has also established a National Cyber Strategy that aims to address cyber security issues,  including AI. The aim of this strategy is to ensure the security and resilience of the UK’s  cyberspace.    \nH.    Personal Data Protection  \nThe UK incorporated the EU’s General Data Protection Regulation (EU GDPR) 93  into its national  law through the Data Protection Act 2018 (DPA) on May 23, 2018, and, upon leaving the EU, the  majority of EU legislation as it stood on December 31, 2020, was incorporated into a new body of  domestic UK law, known as retained EU legislation.  The ICO has noted that the data protection  law does not define AI, so the legal obligations are not dependent upon how it is defined.   However, the DPA does apply to AI systems.  \nThe DPA regulates how personal information may be processed, “requiring personal data to be  processed lawfully and fairly, on the basis of the data subject’s consent or another specified  basis.” 96  The following are the six lawful bases for processing data:  \n\n \n•   where an individual has given valid consent to data processing for a specific purpose,  \n\n •   if the processing is necessary for contractual purposes,  \n\n •   to enable compliance with a legal obligation,  \n\n •   if it is necessary to protect someone’s life,  \n\n •   to perform a task in the public interest, or  \n\n •   if the processing is necessary for the official functions of the processor and there is not a good,  overriding reason to protect the personal data.    "}
{"page": 83, "image_path": "page_images/2023555908_83.jpg", "ocr_text": "The DPA requires that any data collected should be limited in scope, necessary for the reasons it\nis processed, accurate, and kept up to date. It also requires providers of information society\nservices (ISS) adopt a risk-based approach when “using people’s data, based on certain key\nprinciples, rights and obligations.”°%8 AI systems should only collect and process personal\ninformation that is necessary to achieve the intended purpose. Collecting excessive or irrelevant\ndata should be avoided where possible. AI systems should be designed with privacy in mind and\ntechniques that enhance privacy, which include data encryption, should be implemented to\nminimize the risks to individuals’ personal data. Organizations must also ensure that how AI\nsystems process personal data is transparent and that individuals are informed about the\nexistence of automated decision-making, including the logic used in making decisions, and the\nsignificance and potential consequences of the data processing.”\n\nIn its guidance, the ICO notes that, due to AI often involving the systematic and extensive\nprocessing of personal data, profiling and automated decision-making, “[i]n the vast majority of\ncases, the use of AI will involve a type of processing likely to result in a high risk to individuals’\nrights and freedoms, and will therefore trigger the legal requirement for you to undertake a [Data\nProtection Impact Assessment] DPIA.”!° A DPIA involves assessing the necessity and\nproportionality of the data processing and what risks are posed to the rights and freedoms. A\nDPIA must specify how data will be collected, stored, and used; the volume, type, and sensitivity\nof the data; the relationship to the individuals whose data is held; the outcome for the individuals,\nwider society, and the data processor; and whether there are alternatives to AI that pose less risk\nand, if so, why these were not used.!! Failing to conduct a DPIA can result in sanctions from\nthe ICO.\n\nThe ICO has published guidance on data protection compliance and the use of AI. It has noted\nthe general requirements of data protection law mean that a risk-based approach to AI should be\nused, which means\n\n¢ assessing the risks to the rights and freedoms of individuals that may arise when you\nuse AI; and\n\n¢ implementing appropriate and proportionate technical and organisational measures\nto mitigate these risks.102\n\nIf these risks cannot be sufficiently mitigated, the ICO has stated this means a planned AI project\nmay have to be halted.!°3 The ICO notes that it has compiled a number of risk areas, the impact\nof Al in these areas, and the measures that can be taken to “identify, evaluate, minimize, monitor\nand control these risks,” but it has specified that, due to some risk controls being context specific,\n\n%8 Td. at 10.\n°° EU GDPR art. 25.\n\n100 [CO, Guidance on AI and Data Protection, supra note 13. See also ICO, Data Protection Impact Assessments (Mar.\n15, 2023), https:/ / perma.cc/8WJF-GCYJ.\n\n101 1CO, Guidance on AI and Data Protection, supra note 13.\n\n102 ICO, Data Protection Impact Assessments, supra note 100, at 7.\n\n103 Td,\n", "vlm_text": "The DPA requires that any data collected should be limited in scope, necessary for the reasons it  is processed, accurate, and kept up to date. It also requires providers of information society  services (ISS)   adopt a risk-based approach when “using people’s data, based on certain key  principles, rights and obligations.” 98  AI systems should only collect and process personal  information that is necessary to achieve the intended purpose. Collecting excessive or irrelevant  data should be avoided where possible. AI systems should be designed with privacy in mind and  techniques that enhance privacy, which include data encryption, should be implemented to  minimize the risks to individuals’ personal data. Organizations must also ensure that how AI  systems process personal data is transparent and that individuals are informed about the  existence of automated decision-making, including the logic used in making decisions, and the  significance and potential consequences of the data processing.    \nIn its guidance, the ICO notes that, due to AI often involving the systematic and extensive  processing of personal data, profiling and automated decision-making, “[i]n the vast majority of  cases, the use of AI will involve a type of processing likely to result in a high risk to individuals’  rights and freedoms, and will therefore trigger the legal requirement for you to undertake a [Data  Protection Impact Assessment] DPIA.  ${\\prime\\prime}_{100}$   A DPIA involves assessing the necessity and  proportionality of the data processing and what risks are posed to the rights and freedoms. A  DPIA must specify how data will be collected, stored, and used; the volume, type, and sensitivity  of the data; the relationship to the individuals whose data is held; the outcome for the individuals,  wider society, and the data processor; and whether there are alternatives to AI that pose less risk  and, if so, why these were not used.  Failing to conduct a DPIA can result in sanctions from  the ICO.   \nThe ICO has published guidance on data protection compliance and the use of AI. It has noted  the general requirements of data protection law mean that a risk-based approach to AI should be  used, which means   \n•   assessing the risks to the rights and freedoms of individuals that may arise when you  use AI; and   •   implementing appropriate and proportionate technical and organisational measures  to mitigate these risks.   \nIf these risks cannot be sufficiently mitigated, the ICO has stated this means a planned AI project  may have to be halted.  The ICO notes that it has compiled a number of risk areas, the impact  of AI in these areas, and the measures that can be taken to “identify, evaluate, minimize, monitor  and control these risks,” but it has specified that, due to some risk controls being context specific,  these are not an exhaustive list of examples.  It has stated that a zero tolerance approach to risk  is not appropriate or necessary under the law. The only thing necessary is to identify, manage,  and mitigate the risks posed by the use of data.   "}
{"page": 84, "image_path": "page_images/2023555908_84.jpg", "ocr_text": "these are not an exhaustive list of examples. It has stated that a zero tolerance approach to risk\nis not appropriate or necessary under the law. The only thing necessary is to identify, manage,\nand mitigate the risks posed by the use of data.105\n\n1. Solely Automated Decision-Making\n\nArticle 22 of the UK GDPR provides individuals with the right to be informed of the existence of\nsolely automated decision-making that produces legal, or significantly similar, effects as well as\ninformation about the logic involved in the decision-making process and the significance of the\nconsequences of the decision-making on the individual.!% Individuals also have the right to\naccess information on the existence of a solely automated decision-making process, meaning\nthere is no human involvement, that produces legal or similarly significant legal effects, and\ninformation about the logic involved in the decision-making and the potential consequences for\nthe individual. The UK GDPR provides individuals with the right to object to the processing of\ntheir personal data in certain circumstances, including solely automated decision-making\nprocesses, with limited exceptions.1”\n\nIn cases of AI where there is human involvement and, therefore, it is not a solely automated\nprocess, the provisions of the DPA continue to apply, including fairness, transparency, and\naccountability. This means that compliance with the principles contained in Article 5 of the GDPR\nmust be demonstrated, and the data holder must be able to show that the individual whose data\nwas used was treated fairly and in a transparent manner when a decision assisted by AI was\nmade about them.1°8\n\n2. Fairness\n\nThe government has noted that the concept of fairness in the DPA applies to both AI systems and\ntheir use, but that this concept is “highly context-specific . . . and concepts of fairness exist in a\nvariety of legislative frameworks, [so] navigating fairness in the context of Al is a complex\nexercise. Fairness has an evolving meaning in the context of the use of machine learning and AI,\nand there is a question of how unfair outcomes resulting from the use of AI systems can be\nprevented.”1 A recent government consultation found that the data protection regime was not\nsufficiently clear with regard to fairness in the obligations it imposes on bodies that are\ndeveloping and deploying AI systems, but the government does not plan to introduce legislation\non this matter.\n\n104 Td.\n\n105 Td,\n\n106 EU GDPR art. 22.\n\n107 TCO & Alan Turing Inst., Explaining Decisions Made with AI, supra note 13, at 12.\n108 Td. at 13.\n\n109 Dep’t for Digit., Culture, Media & Sport, Data: A New Direction — Government Response to Consultation, supra\nnote 68.\n\n110 Td,\n\n", "vlm_text": "\n1.  Solely Automated Decision-Making   \nArticle 22 of the UK GDPR provides individuals with the right to be informed of the existence of  solely automated decision-making that produces legal, or significantly similar, effects as well as  information about the logic involved in the decision-making process and the significance of the  consequences of the decision-making on the individual.  Individuals also have the right to  access information on the existence of a solely automated decision-making process, meaning  there is no human involvement, that produces legal or similarly significant legal effects, and  information about the logic involved in the decision-making and the potential consequences for  the individual. The UK GDPR provides individuals with the right to object to the processing of  their personal data in certain circumstances, including solely automated decision-making  processes, with limited exceptions.    \nIn cases of AI where there is human involvement and, therefore, it is not a solely automated  process, the provisions of the DPA continue to apply, including fairness, transparency, and  accountability. This means that compliance with the principles contained in Article 5 of the GDPR  must be demonstrated, and the data holder must be able to show that the individual whose data  was used was treated fairly and in a transparent manner when a decision assisted by AI was  made about them.    \n2.  Fairness   \nThe government has noted that the concept of fairness in the DPA applies to both AI systems and  their use, but that this concept is “highly context-specific . . . and concepts of fairness exist in a  variety of legislative frameworks, [so] navigating fairness in the context of AI is a complex  exercise. Fairness has an evolving meaning in the context of the use of machine learning and AI,  and there is a question of how unfair outcomes resulting from the use of AI systems can be  prevented.” 109  A recent government consultation found that the data protection regime was not  sufficiently clear with regard to fairness in the obligations it imposes on bodies that are  developing and deploying AI systems, but the government does not plan to introduce legislation  on this matter.   "}
{"page": 85, "image_path": "page_images/2023555908_85.jpg", "ocr_text": "3. Bias\n\nThe Equality Act applies to the government and a wide range of organizations, including\neducation providers, employers, associations, membership bodies, service providers and those\nwho provide public functions. There are nine characteristics that are protected by the act,\nwhich are\n\n° age,\ne disability,\n\ne gender reassignment,\n\ne marriage and civil partnership,\ne race,\n\ne pregnancy and maternity,\n\ne religion and belief,\n\ne sex, and\n\ne sexual orientation.\n\nAny behavior that discriminates, harasses, or victimizes a person due to one or more of these\ncharacteristics is prohibited.\n\nTo ensure that AI does not discriminate due to any of these characteristics, it must be\ndemonstrated that the AI system does not cause “the decision recipient to be treated worse than\nsomeone else because of one of these protected characteristics; or results in a worse impact on\nsomeone with a protected characteristic than someone without one.”!1\n\nTo help address concerns of bias in AI system, the government intends to introduce a new\ncondition to schedule 1 of the DPA “to enable the processing of sensitive personal data for\nthe purpose of monitoring and correcting bias in AI systems. The new condition will be\nsubject to appropriate safeguards, such as limitations on re-use and the implementation of\nsecurity and privacy preserving measures when processing for this purpose.”'!2 BSI is also\nworking on the development of ISO/IEC TR 24027 to address bias in AI systems and AI-aided\ndecision-making.\"5\n\n4. Storing Personal Data\n\nAny personal data collected must be stored in a manner that enables the identification of the data\nsubject and held for no longer than necessary. Personal data must be processed in a way that\n\n11 [CO & Alan Turing Inst., Explaining Decisions Made with AI, supra note 13, at 14.\n\n12 Dep’t for Digit., Culture, Media & Sport, Data: A New Direction — Government Response to Consultation, supra\nnote 68.\n\n3 Artificial Intelligence Standards Development, British Standards Inst., supra note 81.\n", "vlm_text": "3.  Bias  \nThe Equality Act applies to the government and a wide range of organizations, including  education providers, employers, associations, membership bodies, service providers and those  who provide public functions. There are nine characteristics that are protected by the act,  which are  \n\n \n•   age,  \n\n •   disability,  \n\n •   gender reassignment,  \n\n •   marriage and civil partnership,  \n\n •   race,  \n\n •   pregnancy and maternity,  \n\n •   religion and belief,  \n\n •   sex, and  \n\n •   sexual orientation.  \nAny behavior that discriminates, harasses, or victimizes a person due to one or more of these  characteristics is prohibited.   \nTo ensure that AI does not discriminate due to any of these characteristics, it must be  demonstrated that the AI system does not cause “the decision recipient to be treated worse than  someone else because of one of these protected characteristics; or results in a worse impact on  someone with a protected characteristic than someone without one.” 111   \nTo help address concerns of bias in AI system, the government intends to introduce a new  condition to schedule 1 of the DPA “to enable the processing of sensitive personal data for  the purpose of monitoring and correcting bias in AI systems. The new condition will be  subject to appropriate safeguards, such as limitations on re-use and the implementation of  security and privacy preserving measures when processing for this purpose.” 112  BSI is also  working on the development of ISO/IEC TR 24027 to address bias in AI systems and AI-aided  decision-making.   \n4.  Storing Personal Data  \nAny personal data collected must be stored in a manner that enables the identification of the data  subject and held for no longer than necessary. Personal data must be processed in a way that  ensures the security of the data and protects against unauthorized processing, accidental loss,  destruction, or damage. The DPA places a duty on the data controller to ensure the principles of  the DPA are complied with and demonstrate how this compliance is achieved.  The DPA also  provides for regulatory oversight of its provisions and enforcement mechanisms to ensure it is  implemented properly.    "}
{"page": 86, "image_path": "page_images/2023555908_86.jpg", "ocr_text": "ensures the security of the data and protects against unauthorized processing, accidental loss,\ndestruction, or damage. The DPA places a duty on the data controller to ensure the principles of\nthe DPA are complied with and demonstrate how this compliance is achieved.!\"4 The DPA also\nprovides for regulatory oversight of its provisions and enforcement mechanisms to ensure it is\nimplemented properly.\n\nV. Adherence to Standardized Risk Management Frameworks\n\nThe UK is actively engaged in developing international standards for the use of AI, with BSI and\nthe AI Standard Hub working to contribute to the development of global technical standards. The\nAI Standards Hub aims to provide tools, guidance, and educational materials for developers and\nusers of AI and increase compliance with the standards.5 The government has noted\n“[s]tandards are often used as ‘soft law’ in codes of conduct/ practice and binding/non-binding\nguidance, but it can also be designated as voluntary tools to show legal compliance.”!!6 The\nproposed framework states these standards\n\ncan be used by regulators to complement sector-specific approaches to AI regulation by\nproviding common benchmarks and practical guidance to organisations. Overall, technical\nstandards can embed flexibility into regulatory regimes and drive responsible innovation\nby helping organisations to address Al-related risks.!!”\n\nThe National AI Strategy notes that technical standards help embed “transparency and\naccountability in the design and deployment of technologies. AI technical standards (e.g. for the\naccuracy, explainability and reliability) should ensure that safety, trust and security are the heart\nof AI products and services.”1!8\n\nVI. AI Security Policy Across the Supply Chain\n\nThe government has stated there are difficulties posed when regulating the AI supply chain,\nnotably that overregulation could potentially stifle innovation.\n\nAI supply chains can be complex and opaque, making effective governance of AI and\nsupply chain risk management difficult. Inappropriate allocation of AI risk, liability, and\nresponsibility for Al governance throughout the AI life cycle and within AI supply chains\ncould impact negatively on innovation.\"!9\n\n14 DPA, pt. 2, c. 2.\n\n15 Press Release, Dep’t for Digit., Culture, Media & Sport, Off. for A.I. & the Rt. Hon. Chris Philp MP, New UK\nInitiative to Shape Global Standards for Artificial Intelligence (Jan. 12, 2022), supra note 48.\n\n16 Dep’t for Digit., Culture, Media and Sport, Establishing a Pro-Innovation Approach to Regulating AI, supra note\n5, at 6.\n\n117 Dep’t for Sci., Innovation & Tech., CP 815, A Pro-Innovation Approach to AI Regulation (Mar. 2023), supra note\n3, § 111.\n\nU8 HM Gov't, CP 525, National AI Strategy (Sept. 2021), supra note 19, at 56.\n\n119 Dep’t for Sci., Innovation & Tech., CP 815, A Pro-Innovation Approach to AI Regulation (Mar. 2023), supra note\n3, 4 81.\n", "vlm_text": "\nV.  Adherence to Standardized Risk Management Frameworks  \nThe UK is actively engaged in developing international standards for the use of AI, with BSI and  the AI Standard Hub working to contribute to the development of global technical standards. The  AI Standards Hub aims to provide tools, guidance, and educational materials for developers and  users of AI and increase compliance with the standards.  The government has noted  “[s]tandards are often used as ’soft law’ in codes of conduct/practice and binding/non-binding  guidance, but it can also be designated as voluntary tools to show legal compliance.” 116  The  proposed framework states these standards   \ncan be used by regulators to complement sector-specific approaches to AI regulation by  providing common benchmarks and practical guidance to organisations. Overall, technical  standards can embed flexibility into regulatory regimes and drive responsible innovation  by helping organisations to address AI-related risks.   \nThe National AI Strategy notes that technical standards help embed “transparency and  accountability in the design and deployment of technologies. AI technical standards (e.g. for the  accuracy, explainability and reliability) should ensure that safety, trust and security are the heart  of AI products and services.” 118   \nVI.  AI Security Policy Across the Supply Chain  \nThe government has stated there are difficulties posed when regulating the AI supply chain,  notably that overregulation could potentially stifle innovation.   \nAI supply chains can be complex and opaque, making effective governance of AI and  supply chain risk management difficult. Inappropriate allocation of AI risk, liability, and  responsibility for AI governance throughout the AI life cycle and within AI supply chains  could impact negatively on innovation.    "}
{"page": 87, "image_path": "page_images/2023555908_87.jpg", "ocr_text": "The proposed framework notes that it is too soon to introduce new measures to regulate the AI\nsupply chain as “ [i]t is not yet clear how responsibility and liability for demonstrating compliance\nwith the AI regulatory principles will be or should ideally be, allocated to existing supply chain\nactors within the AI life cycle.”!20 The government intends to rely on assurance techniques and\ntechnical standards to support supply chain risk management to help build trust in AI systems.!2!\n\nThe Principles for the Security of Machine Learning notes that securing the supply chain for the\nsources of data is of significant importance to ensure that AI is working with accurate\ninformation. It advises that the validation and verification process for creation and acquisition of\ndatasets can both protect against data poisoning and help businesses “understand and mitigate\nmistakes or biases in a dataset that can impact performance.”!”\n\nVII. National Security and AI\n\nThe National Security and Investment Act 2021 (2021 Act) establishes a mandatory notification\nsystem for the acquisition of artificial intelligence and provides the government with the\nauthority to intervene in any acquisitions that could harm the national security of the UK. The\ngovernment intends that, in the area of AI, the 2021 Act will “mitigate risks arising from a small\nnumber of potentially concerning actors.” 125\n\nThe 2021 Act covers 17 areas of the economy, which are referred to as notifiable acquisitions, and\nincludes artificial intelligence, as defined in the National Security and Investment Act 2021\n(Notifiable Acquisition) (Specification of Qualifying Entities) Regulations 2021. One of the\nreasons the government has included AI on the notifiable acquisition list is because it is\n“inherently dual-use and potentially easy to repurpose.”!24 The government has stated, “[t]he\nopportunity to use AI positively across the UK economy can only be harnessed if sensitive and\ncritical applications of AI can be protected.”125\n\nUnder the 2021 Act, Al is a notifiable acquisition if a legal person, excluding individuals, such as\na company, limited liability partnership, partnership, trust, unincorporated association or a trust\ngains control of a qualifying entity. The 2021 Act applies to entities formed overseas if they\nconduct activities in the UK, or supply goods or services to individuals in the UK.!% The term\n“qualifying asset” is defined in the 2021 Act as land, tangible (corporeal in Scotland) moveable\nproperty, or “ideas, information or techniques which have industrial, commercial or other\n\n120 Td. ¥ 82.\n\n221 Td. ¥ 84.\n\n122 Nat'l Cyber Sec. Ctr., Principles for the Security of Machine Learning (Aug. 2022), supra note 62, at 15.\n123 Guidance: National AI Strategy - HTML Version, Dep’t for Bus., Energy & Indus. Strategy, supra note 7.\n\n124 Cabinet Off., Guidance: National Security and Investment Act: Details of the 17 Types of Notifiable Acquisitions\n(last updated Apr. 27, 2023), https:/ / perma.cc/879E-2K5R.\n\n125 Td.\n126 National Security and Investment Act 2021, c. 25, § 7.\n", "vlm_text": "The proposed framework notes that it is too soon to introduce new measures to regulate the AI  supply chain as “[i]t is not yet clear how responsibility and liability for demonstrating compliance  with the AI regulatory principles will be or should ideally be, allocated to existing supply chain  actors within the AI life cycle.” 120  The government intends to rely on assurance techniques and  technical standards to support supply chain risk management to help build trust in AI systems.   \nThe  Principles for the Security of Machine Learning  notes that securing the supply chain for the  sources of data is of significant importance to ensure that AI is working with accurate  information. It advises that the validation and verification process for creation and acquisition of  datasets can both protect against data poisoning and help businesses “understand and mitigate  mistakes or biases in a dataset that can impact performance.” 122    \nVII.  National Security and AI  \nThe National Security and Investment Act 2021 (2021 Act) establishes a mandatory notification  system for the acquisition of artificial intelligence and provides the government with the  authority to intervene in any acquisitions that could harm the national security of the UK. The  government intends that, in the area of AI, the 2021 Act will “mitigate risks arising from a small  number of potentially concerning actors.” 123    \nThe 2021 Act covers 17 areas of the economy, which are referred to as notifiable acquisitions, and  includes artificial intelligence, as defined in the National Security and Investment Act 2021  (Notifiable Acquisition) (Specification of Qualifying Entities) Regulations 2021. One of the  reasons the government has included AI on the notifiable acquisition list is because it is  “inherently dual-use and potentially easy to repurpose.” 124  The government has stated, “[t]he  opportunity to use AI positively across the UK economy can only be harnessed if sensitive and  critical applications of AI can be protected.” 125   \nUnder the 2021 Act, AI is a notifiable acquisition if a legal person, excluding individuals, such as  a company, limited liability partnership, partnership, trust, unincorporated association or a trust  gains control of a qualifying entity. The 2021 Act applies to entities formed overseas if they  conduct activities in the UK, or supply goods or services to individuals in the UK.  The term  “qualifying asset” is defined in the 2021 Act as land, tangible (corporeal in Scotland) moveable  property, or “ideas, information or techniques which have industrial, commercial or other  economic value.” 127  The 2021 Act provides a number of examples for the latter category that cover  trade secrets, databases, source code, algorithms, formulae, designs, plans, drawings and  specifications, and software.    "}
{"page": 88, "image_path": "page_images/2023555908_88.jpg", "ocr_text": "economic value.”!2” The 2021 Act provides a number of examples for the latter category that cover\ntrade secrets, databases, source code, algorithms, formulae, designs, plans, drawings and\nspecifications, and software.!28\n\nA legal person gains a control of an entity where it increases the percentage of shares, or voting\nrights, it holds from 25% or less to more than 25%; from 50% or less to more than 50% or from\nless than 75% to 75% or more; acquires voting rights that “enable the person to secure or prevent\nthe passage of any class of resolution governing the affairs of the entity”; or where the acquisition\nresults in a new ability of the legal person to materially influence the policy of the entity.!2° In\ncases where the acquisition does not meet the mandatory notification requirements, a voluntary\nnotification regime exists. If a legal person believes that its acquisition may raise national security\nconcerns, it can voluntarily notify the secretary of state.\n\nThe government has set out a test to help determine whether an acquisition is a\nnotifiable acquisition.\n\ne does the qualifying entity carry on research into, or develop or produce goods,\nsoftware or technology that use AI?\n\ne is the AI work of the qualifying entity used for one of the following applications:\nidentification or tracking, advanced robotics or cyber security?!%°\n\nIf both questions are answered affirmatively, the government must be notified of the\nacquisition.1*! It must then review it and can either clear it, impose conditions, or unwind or block\nthe acquisition in its entirety.\n\nThe secretary of state may also give a “call-in notice” to undertake a national security assessment\nwhen it is believed an event that triggers the provisions of the 2021 Act has occurred, is in\nprogress, or is being contemplated and it has not received a notification. These notices may be\nissued up to five years after the event occurred, provided they are made within six months of the\nsecretary of state becoming aware of the event.!52\n\nDuring the time the acquisition is being assessed, the secretary of state has the power to “impose\ninterim remedies in order to ensure that the effectiveness of the national assessment or\nsubsequent remedies is not prejudiced by action taken by the parties.”'3 At the end of the\nassessment, the secretary of state may notify the parties that no risk has been found and the\n\n17 Id.\n128 Td.\n129 1d. § 8.\n\n130 Cabinet Off., Guidance: National Security and Investment Act: Details of the 17 Types of Notifiable Acquisitions\n(last updated Apr. 27, 2023), supra note 124; National Security and Investment Act 2021 (Notifiable\nAcquisition) (Specification of Qualifying Entities) Regulations 2021, sched. 3, { 2.\n\n131 Cabinet Off., Guidance: National Security and Investment Act: Details of the 17 Types of Notifiable Acquisitions\n(last updated Apr. 27, 2023), supra note 124.\n\n132 Id,\n133 National Security and Investment Act 2021, Explanatory Notes, {| 37.\n", "vlm_text": "\nA legal person gains a control of an entity where it increases the percentage of shares, or voting  rights, it holds from  $25\\%$   or less to more than   $25\\%$  ; from  $50\\%$   or less to more than   $50\\%$   or from  less than  $75\\%$   to   $75\\%$   or more; acquires voting rights that “enable the person to secure or prevent  the passage of any class of resolution governing the affairs of the entity”; or where the acquisition  results in a new ability of the legal person to materially influence the policy of the entity.  In  cases where the acquisition does not meet the mandatory notification requirements, a voluntary  notification regime exists. If a legal person believes that its acquisition may raise national security  concerns, it can voluntarily notify the secretary of state.   \nThe government has set out a test to help determine whether an acquisition is a  notifiable acquisition.  \n•   does the qualifying entity carry on research into, or develop or produce goods,  software or technology that use AI?   •   is the AI work of the qualifying entity used for one of the following applications:  identification or tracking, advanced robotics or cyber security? 130    \nIf both questions are answered affirmatively, the government must be notified of the  acquisition.  It must then review it and can either clear it, impose conditions, or unwind or block  the acquisition in its entirety.  \nThe secretary of state may also give a “call-in notice” to undertake a national security assessment  when it is believed an event that triggers the provisions of the 2021 Act has occurred, is in  progress, or is being contemplated and it has not received a notification. These notices may be  issued up to five years after the event occurred, provided they are made within six months of the  secretary of state becoming aware of the event.    \nDuring the time the acquisition is being assessed, the secretary of state has the power to “impose  interim remedies in order to ensure that the effectiveness of the national assessment or  subsequent remedies is not prejudiced by action taken by the parties.” 133  At the end of the  assessment, the secretary of state may notify the parties that no risk has been found and the  acquisition may proceed, or that there is a national security risk and that an order has been made  to either prevent, remedy or mitigate the risk. Once an order has been made, it must be kept under  review, and it can be varied or revoked. Parties to the acquisition that are subject to an order may  request that the order be reviewed and also have a right to apply to the High Court for judicial  review of the decision, which must be filed within 28 days. The secretary of state, with approval  from the Treasury, may provide financial assistance, such as a loan, guarantee or indemnity to a  legal entity that has a final order made against it.   "}
{"page": 89, "image_path": "page_images/2023555908_89.jpg", "ocr_text": "acquisition may proceed, or that there is a national security risk and that an order has been made\nto either prevent, remedy or mitigate the risk. Once an order has been made, it must be kept under\nreview, and it can be varied or revoked. Parties to the acquisition that are subject to an order may\nrequest that the order be reviewed and also have a right to apply to the High Court for judicial\nreview of the decision, which must be filed within 28 days. The secretary of state, with approval\nfrom the Treasury, may provide financial assistance, such as a loan, guarantee or indemnity to a\nlegal entity that has a final order made against it.154\n\nIf a legal person completes a notifiable acquisition and fails to inform the government and obtain\napproval, the acquisition is void, and the legal person acquiring it can face civil or criminal\npenalties, which can result in either imprisonment for up to five years, a fine, or both.'%5 Failing\nto comply with an order can also result in either imprisonment for up to five years, a fine,\nor both.1%6\n\nVIII. Regulatory Bodies\n\nThe UK has stated that it currently does not intend to create a central regulatory body for AI.157\nInstead, it is adopting a cross-sector approach, using existing regulators that “take a tailored\napproach to the uses of AI in a range of settings.”!°8 To ensure proper access to contestability and\nredress for issues that may arise with AI, the government has stated that the regulators should\ncreate, or update, guidance with information on where complaints should be directed for those\nwho have been adversely impacted by AI.139\n\nThe proposed AI framework envisages that where regulators discover gaps, they will work with\nthe government to determine potential actions to correct it, such as updates to the Regulators\nCode, or new legislation.“° To help provide oversight to ensure the regulatory approach to the\nimplementation of the framework is working well, the government intends to introduce a central\nmonitoring and evaluation framework to ensure that the cross-sector approach\nfunctions effectively.\n\n134 Td. c. 25 § 30.\n\n135 Td. § 32. See also Cabinet Off., Guidance: Check If You Need to Tell the Government About an Acquisition That\nCould Harm the UK’s National Security (last updated Apr. 27, 2023), https:/ / perma.cc/JD6Q-73CW.\n\n136 National Security and Investment Act 2021, c. 25 § 33.\n\n137 Dep’t for Sci., Innovation & Tech., RPC-DCMS-5260(1), Artificial Intelligence Regulation Impact Assessment,\n(Mar. 2023), supra note 12, 4 114.\n\n138 Press Release, Dep’t for Digit., Culture, Media & Sport & Damian Collins MP, UK Sets Out Proposals for\nNew AI Rulebook to Unleash Innovation and Boost Public Trust in the Technology (July 18, 2022), supra\nnote 4.\n\n1389 Dep’t for Sci., Innovation & Tech., CP 815, A Pro-Innovation Approach to AI Regulation (Mar. 2023), supra note\n3, Annex A.\n\n140 Td. J 65.\n141 Id.\n", "vlm_text": "\nIf a legal person completes a notifiable acquisition and fails to inform the government and obtain  approval, the acquisition is void, and the legal person acquiring it can face civil or criminal  penalties, which can result in either imprisonment for up to five years, a fine, or both.  Failing  to comply with an order can also result in either imprisonment for up to five years, a fine,  or both.   \nVIII.  Regulatory Bodies  \nThe UK has stated that it currently does not intend to create a central regulatory body for AI.   Instead, it is adopting a cross-sector approach, using existing regulators that “take a tailored  approach to the uses of AI in a range of settings.” 138  To ensure proper access to contestability and  redress for issues that may arise with AI, the government has stated that the regulators should  create, or update, guidance with information on where complaints should be directed for those  who have been adversely impacted by AI.    \nThe proposed AI framework envisages that where regulators discover gaps, they will work with  the government to determine potential actions to correct it, such as updates to the Regulators  Code, or new legislation.  To help provide oversight to ensure the regulatory approach to the  implementation of the framework is working well, the government intends to introduce a central  monitoring  and  evaluation  framework  to  ensure  that  the  cross-sector  approach  functions effectively.   "}
{"page": 90, "image_path": "page_images/2023555908_90.jpg", "ocr_text": "Although the government is working to provide clarity for regulators, the Regulation of AI\nImpact Assessment notes potential deficiencies in the current approach. It states that the different\nregulators may have varying interpretations of the law, which might lead to both uncertainty and\nan inefficient overlap between the responsibilities of different regulators. The assessment gives\nthe example that both the Equality and Human Rights Commission and the ICO “purport to\nregulate the discriminatory effects of AI.”142 Other concerns that have been raised regarding the\nuse of current regulators include a lack of technical expertise and the variety of powers the\nregulators have at their disposal.143\n\nTo help mitigate the risks posed by AI not falling solely within the remit of one regulator, the\ngovernment is establishing a “central, cross-economy risk function” that allows the government\nto identify, “assess and prioritise Al risks, ensuring that any intervention is proportionate and\nconsistent with levels of risk mitigation activity elsewhere across the economy or AI life cycle.”144\nThe central risk framework will be designed with existing regulators and, where a risk has been\nidentified and prioritized that does not fall within the remit of the current regulators, the central\nrisk function will identify measures to address the gap. The central risk function will also serve\nto help smaller regulators that do not have technical AI expertise understand the risks posed\n\n12 Dep’t for Sci., Innovation & Tech., RPC-DCMS-5260(1), Artificial Intelligence Regulation Impact Assessment\n(Mar. 2023), supra note 12, 4 35.\n\n143 Id.\n\nM44 Dep’t for Sci., Innovation & Tech., CP 815, A Pro-Innovation Approach to AI Regulation (Mar. 2023), supra note\n3, Box 3.2.\n\n145 Id.\n", "vlm_text": "Although the government is working to provide clarity for regulators, the Regulation of AI  Impact Assessment notes potential deficiencies in the current approach. It states that the different  regulators may have varying interpretations of the law, which might lead to both uncertainty and  an inefficient overlap between the responsibilities of different regulators. The assessment gives  the example that both the Equality and Human Rights Commission and the ICO “purport to  regulate the discriminatory effects of AI.  ${\\mathbf\\prime}_{142}$   Other concerns that have been raised regarding the  use of current regulators include a lack of technical expertise and the variety of powers the  regulators have at their disposal.    \nTo help mitigate the risks posed by AI not falling solely within the remit of one regulator, the  government is establishing a ”central, cross-economy risk function” that allows the government  to identify, “assess and prioritise AI risks, ensuring that any intervention is proportionate and  consistent with levels of risk mitigation activity elsewhere across the economy or AI life cycle.” 144 The central risk framework will be designed with existing regulators and, where a risk has been  identified and prioritized that does not fall within the remit of the current regulators, the central  risk function will identify measures to address the gap. The central risk function will also serve  to help smaller regulators that do not have technical AI expertise understand the risks posed  by AI.   "}
{"page": 91, "image_path": "page_images/2023555908_91.jpg", "ocr_text": "Table of Primary Sources\n\nJurisdiction Type of Citation URL\nSource\nAustralia Statute Corporations Act 2001 (Cth) https: / / perma.cc/5S7W-CTXW\nCriminal Code Act 1995 (Cth) https:/ / perma.cc/9YWA-732B\nPrivacy Act 1988 (Cth) https:/ / perma.cc/TP5W-Z238\nSecurity of Critical Infrastructure Act 2018 (Cth) https:/ / perma.cc/QX3Y-QZHL\nTelecommunications (Interception and Access) Act 1979 https:/ / perma.cc/42DG-GN2G\n(Cth)\nTelecommunications Act 1997 (Cth) https:/ / perma.cc/P85S-MMK8 (vol\n1), https:/ / perma.cc/ ET5H-TFCF (vol\n2)\nRegulation | Security of Critical Infrastructure (Application) Rules https:/ / perma.cc/FN7T-N64L\n(LIN 22/026) 2022 (Cth)\nSecurity of Critical Infrastructure (Critical infrastructure https: / / perma.cc/ BUSH-8GTT\nrisk management program) Rules (LIN 23/006) 2023 (Cth)\nCanada Statute An Act respecting cyber security, amending the https: / / perma.cc/T5EK-5E5Z\n\nTelecommunications Act and making consequential\namendments to other Acts (Bill C-26), 44th Parliament, 1st\nSession, June 14, 2022\n\n", "vlm_text": "Table of Primary Sources  \nThis table presents legal references related to cybersecurity and telecommunications laws and regulations in Australia and Canada. It is organized into four columns: Jurisdiction, Type of Source, Citation, and URL.\n\n- **Jurisdiction:** This column lists the countries, which are Australia and Canada.\n  \n- **Type of Source:** This column specifies whether the legal reference is a statute or a regulation.\n\n- **Citation:** This column provides the formal titles of the legal documents. For Australia, several statutes and regulations are cited, including the Corporations Act 2001 (Cth), Criminal Code Act 1995 (Cth), Privacy Act 1988 (Cth), and others. There are also regulations like the Security of Critical Infrastructure (Application) Rules. For Canada, the cited statute is an act related to cybersecurity and telecommunications from the 44th Parliament, 1st Session, dated June 14, 2022.\n\n- **URL:** This column contains perma.cc links to each cited legal document, ensuring stable and permanent access to the digital versions of these documents. Each legal reference has its corresponding URL, with some documents having multiple URLs for different volumes."}
{"page": 92, "image_path": "page_images/2023555908_92.jpg", "ocr_text": "Jurisdiction Type of Citation URL\nSource\nAn Act to enact the Consumer Privacy Protection Act, the | https://perma.cc/594V-V3UN\nPersonal Information and Data Protection Tribunal Act\nand the Artificial Intelligence and Data Act and to make\nconsequential and related amendments to other Acts (Bill\nC-27), 44th Parliament, 1st Session, June 16, 2022\nPersonal Information Protection and Electronic https:/ / perma.cc/ZB9S-BR99\nDocuments Act, S.C. 2000, c. 5\nEuropean Treaty Charter of Fundamental Rights of the European Union https:/ / perma.cc/PAX8-4MYJ\nUnion (EU Charter)\nTreaty on European Union (consolidated version) (TEU) https:/ / perma.cc/9E8Y-B6C5\nTreaty on the Functioning of the European Union https:/ / perma.cc/FM38-RYTH\n(consolidated version) (TFEU)\nLegislative | Cybersecurity Act (CSA) https:/ / perma.cc/8E4S-2BPJ\nAct\n\nDigital Services Act (DSA)\n\nhttps: / / perma.cc/ Y5S3-Z7YX\n\nGeneral Data Protection Regulation (GDPR)\n\nhttps:/ / perma.cc/7Y47-L7XX\n\nNetwork and Information Security (NIS) Directive\n\nhttps:/ / perma.cc/JH4W-FHFB\n\nNIS 2 Directive\n\nhttps: / / perma.cc/ EGT7-7Q3F\n\nProposal for a Regulation of the European Parliament and\nof the Council Laying Down Harmonised Rules on\nArtificial Intelligence (Artificial Intelligence Act) and\nAmending Certain Union Legislative Acts (Draft AI Act)\n\nhttps:/ / perma.cc/ RWT9-9D97\n\n", "vlm_text": "The table lists legal acts and treaties related to data protection and cybersecurity for Canada and the European Union. It includes the following columns:\n\n1. **Jurisdiction** - Represents the origin of the legal document (e.g., European Union).\n2. **Type of Source** - Specifies if the document is a treaty or legislative act.\n3. **Citation** - Provides the title or description of the legal act or treaty.\n4. **URL** - Contains a link to the document.\n\nFor the European Union, there are entries under treaties and legislative acts, including the GDPR and the CSA. The Canadian jurisdiction section has entries related to data protection acts."}
{"page": 93, "image_path": "page_images/2023555908_93.jpg", "ocr_text": "Jurisdiction Type of Citation URL\nSource\nProposal for a Regulation of the European Parliament and | https://perma.cc/ N2TV-ZJRD\nof the Council on Horizontal Cybersecurity Requirements\nfor Products with Digital Elements and Amending\nRegulation (EU) 2019/1020 (Cyber Resilience Act, CRA)\nStandardization Regulation (consolidated version) https:/ / perma.cc/7NR3-DBYA\nNew Zealand Statute Crimes Act 1961 https:/ / perma.cc/ WAE7-RRQ8\nFair Trading Act 1986 https: / / perma.cc/3F6Y-BEEU\nHarmful Digital Communications Act 2015 https:/ / perma.cc/7M2A-DLMQ\nHuman Rights Act 1993 https: / / perma.cc/XA7S-4HL4\nIntelligence and Security Act 2017 https:/ / perma.cc/PVT7-BQPH\nNew Zealand Bill of Rights Act 1990 https:/ / perma.cc/PT9IQ-UYYD\nPrivacy Act 2020 https: / / perma.cc/UQ39-RZSA\nTelecommunications (Interception Capability and https:/ / perma.cc/ YL3J-AEZQ\nSecurity) Act 2013\nUnited Statute Competition Act 1998, c. 41 https:/ / perma.cc/ DX9D-WQJU\nKingdom\n\nData Protection Act 2018, c. 12\n\nhttps:/ / perma.cc/5DMR-6FZV\n\nEquality Act 2010, c. 15 (Great Britain)\n\nhttps:/ / perma.cc/52EB-2DQ2\n\n", "vlm_text": "The table contains legal information organized by jurisdiction, type of source, citation, and URL. Here’s a breakdown:\n\n### Jurisdictions and Details:\n\n1. **Unspecified Jurisdiction:**\n   - Proposal for a Regulation on cybersecurity requirements and the Standardization Regulation.\n   - Links provided for each regulation.\n\n2. **New Zealand:**\n   - Type of Source: Statute\n   - Includes various acts such as the Crimes Act 1961, Fair Trading Act 1986, Human Rights Act 1993, etc.\n   - Each statute is linked to a URL.\n\n3. **United Kingdom:**\n   - Type of Source: Statute\n   - Includes the Competition Act 1998, Data Protection Act 2018, and Equality Act 2010.\n   - Each statute is linked to a URL.\n\n### Columns:\n\n- **Jurisdiction:** Region or governing body.\n- **Type of Source:** Legal category such as Statute.\n- **Citation:** Specific acts or regulations.\n- **URL:** Links to more information."}
{"page": 94, "image_path": "page_images/2023555908_94.jpg", "ocr_text": "Safety and Security of Artificial Intelligence Systems: Table of Primary Sources\n\nJurisdiction Type of Citation URL\nSource\nNational Security and Investment Act 2021, c. 25 https: // perma.cc/9VHY-B8UB\nRegulation | Medical Devices Regulations 2002, SI 2002/618 https: / / perma.cc/4FSR-SRHU\n\nNational Security and Investment Act 2021 (Notifiable\nAcquisition) (Specification of Qualifying Entities)\nRegulations 2021, SI 2021/1264\n\nhttps: //perma.cc/3TPH-Y6YK\n\nThe Law Library of Congress\n", "vlm_text": "The table contains information related to regulations and acts, specifically focusing on three entries under different columns:\n\n1. **Jurisdiction**: The column is empty, indicating that no specific jurisdiction is mentioned in the table.\n\n2. **Type of Source**: All the entries in this column are classified under \"Regulation.\"\n\n3. **Citation**: This column lists the official titles and designations of the three legislative or regulatory documents:\n   - \"National Security and Investment Act 2021, c. 25\"\n   - \"Medical Devices Regulations 2002, SI 2002/618\"\n   - \"National Security and Investment Act 2021 (Notifiable Acquisition) (Specification of Qualifying Entities) Regulations 2021, SI 2021/1264\"\n\n4. **URL**: Each citation is accompanied by a corresponding URL providing a permalink for accessing more details about the respective document:\n   - For the National Security and Investment Act 2021: https://perma.cc/9VHY-B8UB\n   - For the Medical Devices Regulations 2002: https://perma.cc/4FSR-SRHU\n   - For the National Security and Investment Act 2021 (Notifiable Acquisition): https://perma.cc/3TPH-Y6YK\n\nThe table provides quick reference links to access the detailed text or official versions of these legal documents but lacks jurisdictional information."}
