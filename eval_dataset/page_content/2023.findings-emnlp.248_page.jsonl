{"page": 0, "image_path": "doc_images/2023.findings-emnlp.248_0.jpg", "ocr_text": "LoGic-LM: Empowering Large Language Models with\nSymbolic Solvers for Faithful Logical Reasoning\n\nLiangming Pan Alon Albalak\n\nXinyi Wang William Yang Wang\n\nUniversity of California, Santa Barbara\n\n{liangmingpan, alon_albalak, xinyi_wang, wangwilliamyang}@ucsb. edu\n\nAbstract\n\nLarge Language Models (LLMs) have shown\nhuman-like reasoning abilities but still strug-\ngle with complex logical problems. This pa-\nper introduces a novel framework, LOGIC-\nLM, which integrates LLMs with symbolic\nsolvers to improve logical problem-solving.\nOur method first utilizes LLMs to translate\na natural language problem into a symbolic\nformulation. Afterward, a deterministic sym-\nbolic solver performs inference on the for-\nmulated problem. We also introduce a self-\nrefinement module, which utilizes the symbolic\nsolver’s error messages to revise symbolic for-\nmalizations. We demonstrate LOGIC-LM’s ef-\nfectiveness on five logical reasoning datasets:\nProofWriter, PrOntoQA, FOLIO, LogicalDe-\nduction, and AR-LSAT. On average, LOGIC-\nLM achieves a significant performance boost\nof 39.2% over using LLM alone with standard\nprompting and 18.4% over LLM with chain-of-\nthought prompting. Our findings suggest that\nLocic-LM, by combining LLMs with sym-\nbolic logic, offers a promising avenue for faith-\nful logical reasoning. !\n\n1 Introduction\n\nLogical reasoning is a cognitive process that in-\nvolves using evidence, arguments, and logic to ar-\nrive at conclusions or make judgments (Huang and\nChang, 2023). It plays a central role in intelligent\nsystems for problem-solving, decision-making, and\ncritical thinking. Recently, large language models\n(LLMs) (Brown et al., 2020; Ouyang et al., 2022a;\nOpenAL, 2023) have exhibited emergent ability to\n“reason” like human (Wei et al., 2022a). When\nprompted with step-wise explanations of reasoning\n(‘chain of thoughts”), or a simple prompt “Let’s\nthink step by step.”, these models are able to an-\nswer questions with explicit reasoning steps (Wei\net al., 2022b; Kojima et al., 2022).\n\n‘Code and data are publicly available at https: //github.\ncom/teacherpeterpan/Logic-LLM.\n\ne@ Problem G Goal\n\n——\n\n& Problem Symbolic\n\nFormulator Formulation\n\n@ il\n\nSelf-\n\nRefine \\|@2® Symbolic Symbolic\nResult\n\n(© *@ Reasoner\n5 cr Result\n\na- —-\nLgq Interpreter\n\nAnswer\n\nFigure 1: Overview of our LoGICc-LM framework.\n\nDespite the advances of LLMs, they still strug-\ngle with complex logical reasoning problems (Liu\net al., 2023b). Recent studies (Golovneva et al.,\n2023; Ribeiro et al., 2023b; Lyu et al., 2023) found\nthat LLMs occasionally make unfaithful reason-\ning, i.e., the derived conclusion does not follow\nthe previously generated reasoning chain. While\nchain-of-thought may imitate human reasoning pro-\ncesses, the fundamental nature of LLMs remains\nthat of black-box probabilistic models, lacking a\nmechanism to guarantee the faithfulness of reason-\ning (Shanahan, 2022). In contrast, symbolic infer-\nence engines, such as expert systems (Metaxiotis\net al., 2002), are faithful and transparent because\nthe reasoning is based on symbolic-represented\nknowledge and follows well-defined inference rules\nthat adhere to logical principles. The main obsta-\ncle is how to accurately translate a problem into\nsymbolic representations, considering the inherent\nambiguity and flexibility of natural language. This\nis precisely where LLMs excel, making LLMs a\npromising complement to symbolic solvers.\n\nThis drives our exploration of neuro-symbolic\nmethods that integrate LLMs with symbolic reason-\ning. As illustrated in Figure 1, we present LOGIC-\n\n3806\n\nFindings of the Association for Computational Linguistics: EMNLP 2023, pages 3806-3824\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\n", "vlm_text": "L OGIC -LM: Empowering Large Language Models with Symbolic Solvers for Faithful Logical Reasoning \nLiangming Pan Alon Albalak Xinyi Wang William Yang Wang \nUniversity of California, Santa Barbara {liang ming pan, al on alba lak, xinyi_wang, wang william yang}@ucsb.edu \nAbstract \nLarge Language Models (LLMs) have shown human-like reasoning abilities but still strug- gle with complex logical problems. This pa- per introduces a novel framework, L OGIC - LM, which integrates LLMs with symbolic solvers to improve logical problem-solving. Our method first utilizes LLMs to translate a natural language problem into a symbolic formulation. Afterward, a deterministic sym- bolic solver performs inference on the for- mulated problem. We also introduce a self- refinement module, which utilizes the symbolic solver’s error messages to revise symbolic for- mali zat ions. We demonstrate L OGIC -LM’s ef- fec ti ve ness on five logical reasoning datasets: Proof Writer, PrOntoQA, FOLIO, LogicalDe- duction, and AR-LSAT. On average, L OGIC - LM achieves a significant performance boost of  $39.2\\%$   over using LLM alone with standard prompting and   $18.4\\%$   over LLM with chain-of- thought prompting. Our findings suggest that L OGIC -LM, by combining LLMs with sym- bolic logic, offers a promising avenue for faith- ful logical reasoning.   1 \n1 Introduction \nLogical reasoning is a cognitive process that in- volves using evidence, arguments, and logic to ar- rive at conclusions or make judgments ( Huang and Chang ,  2023 ). It plays a central role in intelligent systems for problem-solving, decision-making, and critical thinking. Recently, large language models (LLMs) ( Brown et al. ,  2020 ;  Ouyang et al. ,  2022a ; OpenAI ,  2023 ) have exhibited emergent ability to “reason” like human ( Wei et al. ,  2022a ). When prompted with step-wise explanations of reasoning (“chain of thoughts”), or a simple prompt “Let’s think step by step.”, these models are able to an- swer questions with explicit reasoning steps ( Wei et al. ,  2022b ;  Kojima et al. ,  2022 ). \nThe image is a flowchart depicting the overview of the LOGIC-LM framework. It consists of three main components:\n\n1. **Problem Formulator**: Takes input from \"Problem\" and \"Goal\" and produces a \"Symbolic Formulation.\"\n2. **Symbolic Reasoner**: Uses the symbolic formulation to generate a \"Symbolic Result.\"\n3. **Result Interpreter**: Interprets the symbolic result to provide an \"Answer.\"\n\nThere is also a \"Self-Refine\" component that feeds back into the Problem Formulator to refine the process iteratively.\nDespite the advances of LLMs, they still strug- gle with complex logical reasoning problems ( Liu et al. ,  2023b ). Recent studies ( Golovneva et al. , 2023 ;  Ribeiro et al. ,  2023b ;  Lyu et al. ,  2023 ) found that LLMs occasionally make  unfaithful  reason- ing,  i.e. , the derived conclusion does not follow the previously generated reasoning chain. While chain-of-thought may imitate human reasoning pro- cesses, the fundamental nature of LLMs remains that of black-box probabilistic models, lacking a mechanism to guarantee the faithfulness of reason- ing ( Shanahan ,  2022 ). In contrast,  symbolic infer- ence engines , such as expert systems ( Metaxiotis et al. ,  2002 ), are faithful and transparent because the reasoning is based on symbolic-represented knowledge and follows well-defined inference rules that adhere to logical principles. The main obsta- cle is how to accurately translate a problem into symbolic representations, considering the inherent ambiguity and flexibility of natural language. This is precisely where LLMs excel, making LLMs a promising complement to symbolic solvers. \nThis drives our exploration of neuro-symbolic methods that integrate LLMs with symbolic reason- ing. As illustrated in Figure  1 , we present L OGIC - LM, a novel framework that decomposes a logical reasoning problem into three stages:  Problem For- mulation ,  Symbolic Reasoning , and  Result Inter- pretation . During problem formulation, an LLM converts the natural language description of the problem into an appropriate symbolic formulation, identifying key entities, facts, and rules present in the problem statement. Subsequently, at the symbolic reasoning stage, a deterministic symbolic solver performs inference on the symbolic formula- tion. Lastly, a result interpreter explains the output and maps it to the correct answer. By incorporating LLMs with symbolic solvers, we can exploit the robust natural language understanding capabilities of LLMs to precisely represent the problem using symbolic representations, while also taking advan- tage of the logical faithfulness and transparency offered by symbolic solvers. To improve the accu- racy of the symbolic parsing, we also incorporate the idea of self-refinement to iterative ly revise the generated logical form using the error messages from the symbolic solver as feedback. "}
{"page": 1, "image_path": "doc_images/2023.findings-emnlp.248_1.jpg", "ocr_text": "LM, a novel framework that decomposes a logical\nreasoning problem into three stages: Problem For-\nmulation, Symbolic Reasoning, and Result Inter-\npretation. During problem formulation, an LLM\nconverts the natural language description of the\nproblem into an appropriate symbolic formulation,\nidentifying key entities, facts, and rules present\nin the problem statement. Subsequently, at the\nsymbolic reasoning stage, a deterministic symbolic\nsolver performs inference on the symbolic formula-\ntion. Lastly, a result interpreter explains the output\nand maps it to the correct answer. By incorporating\nLLMs with symbolic solvers, we can exploit the\nrobust natural language understanding capabilities\nof LLMs to precisely represent the problem using\nsymbolic representations, while also taking advan-\ntage of the logical faithfulness and transparency\noffered by symbolic solvers. To improve the accu-\nracy of the symbolic parsing, we also incorporate\nthe idea of self-refinement to iteratively revise the\ngenerated logical form using the error messages\nfrom the symbolic solver as feedback.\n\nWe showcase the adaptability and effective-\nness of LoGic-LM on five logical reasoning\ndatasets: ProofWriter (Tafjord et al., 2021), PrOn-\ntoQA (Saparov and He, 2023), FOLIO (Han et al.,\n2022), AR-LSAT (Zhong et al., 2022), and the Log-\nicalDeduction dataset from BigBench (Srivastava\net al., 2022). These datasets cover a wide range of\nlogical reasoning problems, including:\n\ne Deductive Reasoning problems\n\ne First-Order Logic (FOL) reasoning problems\n\ne Constraint Satisfaction Problems (CSP)\n\ne Analytical Reasoning (AR) problems\nWe integrate four types of symbolic inference tools\ntailored to these problems: 1) logic programming\nengine that supports deductive reasoning through\nforward/backward chaining; 2) FOL inference en-\ngine that derives new conclusions based on FOL\ntules and facts, 3) constraint optimization engine\nthat provides solvers for CSP over finite domains,\nand 4) boolean satisfiability problem (SAT) solver\nthat solves analytical reasoning problems.\n\nOur evaluations show that the strategy of inte-\ngrating LLMs with symbolic solvers performs sig-\nnificantly better than purely relying on LLMs for\nlogical reasoning, with an average improvement\nof 39.2% over the standard prompting and 18.4%\nover the chain-of-thought prompting (§ 4.1). We\nalso find that LoGiIc-LM becomes increasingly ef-\nfective as the required reasoning depth increases\n\n(§ 4.3). Finally, by analyzing the impact of self-\nrefinement, we highlight the effectiveness of incre-\nmentally revising symbolic formalizations when\ninteracting with the symbolic solver (§ 4.4).\n\n2 Related Work\n\nLanguage Models for Logical Reasoning. Re-\ncent works in adapting LLMs for logical reasoning\ntasks can be broadly categorized into two groups:\n1) fine-tuning approaches that optimize LLMs’ rea-\nsoning ability through fine-tuning or training spe-\ncialized modules (Clark et al., 2020; Tafjord et al.,\n2022; Yang et al., 2022), and 2) in-context learning\napproaches that design special prompts to elicit\nLLMs’ step-by-step reasoning capabilities. Typical\nmethods include chain-of-thought prompting (Wei\net al., 2022b; Wang et al., 2023) that generates ex-\nplanations before the final answer and the least-to-\nmost prompting (Zhou et al., 2023) that breaks the\nproblem down into simpler components that can\nbe solved individually. Both the above approaches\nperform reasoning directly over natural language\n(NL), providing greater flexibility than symbolic-\nbased reasoning. However, the intrinsic complexity\nand ambiguity of NL also bring undesired issues\nsuch as unfaithful reasoning and hallucinations.\n\nDifferent from prior works, we use symbolic\nlanguage as the basic unit of reasoning. This effec-\ntively transfers the burden of executing complex,\nprecise reasoning from LLMs to more reliable, in-\nterpretable external symbolic solvers. Simultane-\nously, we leverage the strong in-context learning\nability of LLMs to formulate the NL-based prob-\nlem into suitable symbolic representations, thus\nmaintaining the benefit of flexibility.\n\nAlthough prior works (Mao et al., 2019; Gupta\net al., 2020; Manhaeve et al., 2021; Cai et al., 2021;\nTian et al., 2022; Pryor et al., 2023) also propose\nneuro-symbolic methods to combine neural net-\nworks with symbolic reasoning, these methods suf-\nfer from limitations such as hand-crafted or spe-\ncialized module designs that are not easily gen-\neralizable, or brittleness due to the difficulty of\noptimization. In contrast, we propose a more gen-\neralizable framework that integrates modern LLMs\nwith symbolic logic without the need for training\nor designing complex problem-specific modules.\n\nTool-augmented Language Models. Language\nmodels have inherent limitations such as the inabil-\nity to access up-to-date information, take actions,\nor perform precise mathematical reasoning. To\n\n3807\n", "vlm_text": "\nWe showcase the adaptability and effective- ness of L OGIC -LM on five logical reasoning datasets:  Proof Writer  ( Tafjord et al. ,  2021 ),  PrOn- toQA  ( Saparov and He ,  2023 ),  FOLIO  ( Han et al. , 2022 ),  AR-LSAT  ( Zhong et al. ,  2022 ), and the  Log- ical Deduction  dataset from BigBench ( Srivastava et al. ,  2022 ). These datasets cover a wide range of logical reasoning problems, including: \n•  Deductive Reasoning problems •  First-Order Logic (FOL) reasoning problems •  Constraint Satisfaction Problems (CSP)  Analytical Reasoning (AR) problems \nWe integrate four types of symbolic inference tools tailored to these problems: 1)  logic programming engine  that supports deductive reasoning through forward/backward chaining; 2)  FOL inference en- gine  that derives new conclusions based on FOL rules and facts, 3)  constraint optimization  engine that provides solvers for CSP over finite domains, and 4)  boolean satisfiability problem (SAT) solver that solves analytical reasoning problems. \nOur evaluations show that the strategy of inte- grating LLMs with symbolic solvers performs sig- nificantly better than purely relying on LLMs for logical reasoning, with an average improvement of  $39.2\\%$   over the standard prompting and   $18.4\\%$  over the chain-of-thought prompting   $(\\S\\ 4.1)$  . We also find that L OGIC -LM becomes increasingly ef- fective as the required reasoning depth increases  $(\\S~4.3)$  . Finally, by analyzing the impact of self- refinement, we highlight the effectiveness of incre- mentally revising symbolic formalization s when interacting with the symbolic solver (§  4.4 ). \n\n2 Related Work \nLanguage Models for Logical Reasoning. Re- cent works in adapting LLMs for logical reasoning tasks can be broadly categorized into two groups: 1)  fine-tuning approaches  that optimize LLMs’ rea- soning ability through fine-tuning or training spe- cialized modules ( Clark et al. ,  2020 ;  Tafjord et al. , 2022 ;  Yang et al. ,  2022 ), and 2)  in-context learning approaches  that design special prompts to elicit LLMs’ step-by-step reasoning capabilities. Typical methods include chain-of-thought prompting ( Wei et al. ,  2022b ;  Wang et al. ,  2023 ) that generates ex- planations before the final answer and the least-to- most prompting ( Zhou et al. ,  2023 ) that breaks the problem down into simpler components that can be solved individually. Both the above approaches perform reasoning directly over natural language (NL), providing greater flexibility than symbolic- based reasoning. However, the intrinsic complexity and ambiguity of NL also bring undesired issues such as unfaithful reasoning and hallucinations. \nDifferent from prior works, we use  symbolic language  as the basic unit of reasoning. This effec- tively transfers the burden of executing complex, precise reasoning from LLMs to more reliable, in- ter pre table external symbolic solvers. Simultane- ously, we leverage the strong in-context learning ability of LLMs to formulate the NL-based prob- lem into suitable symbolic representations, thus maintaining the benefit of flexibility. \nAlthough prior works ( Mao et al. ,  2019 ;  Gupta et al. ,  2020 ;  Manhaeve et al. ,  2021 ;  Cai et al. ,  2021 ; Tian et al. ,  2022 ;  Pryor et al. ,  2023 ) also propose neuro-symbolic methods to combine neural net- works with symbolic reasoning, these methods suf- fer from limitations such as hand-crafted or spe- cialized module designs that are not easily gen- eralizable, or brittleness due to the difficulty of optimization. In contrast, we propose a more gen- eralizable framework that integrates modern LLMs with symbolic logic without the need for training or designing complex problem-specific modules. \nTool-augmented Language Models. Language models have inherent limitations such as the inabil- ity to access up-to-date information, take actions, or perform precise mathematical reasoning. To "}
{"page": 2, "image_path": "doc_images/2023.findings-emnlp.248_2.jpg", "ocr_text": "Metals conduct electricity.\n\nInsulators do not conduct electricity.\nIfsomething is made of iron, then it is metal\nNails are made of iron.\n\nBERT is a giant language model.\n\nIs the following statement true, false, or\nunknown? Nails cannot conduct electricity.\n\nNo giant language model could have bad performance.\nIfa language model has good performance, itis used by some researchers.\n‘Awork used by some researchers should be popular.\n\nIf BERT is a giant language model, then the same for GPT3.\n\nIs the following statement true, false, or unknown? GPTS is popular.\n\nProblem Formulator\n\nInan antique car show, there are three vehicles: a tractor,\na convertible, and a minivan. The tractor is the second-\nnewest. The minivan is newer than the convertible.\n\nWhich of the following is true?\n‘A) The tractor is the oldest.\n8) The convertible is the oldest.\nC) The minivan is the oldest.\n\nRules: Facts:\n+ Metal(x, True) + ConductElectricity(x, True)\n+ MadeOflron(x, True) — Metal(x, True)\nFacts:\n\n+ MadeOflron(Nails, True)\n\n+ ConductElectricity(Insulator, False)\n\nQuery:\n\n+ ConductElectricity(Nail, False)\n\n+ Language(bert)\n+ Giant(bert)\nQuery: Polular(gpt3)\n\n+ >(@x(LanguageModel(x) A Giant(x) A sGoodPerformance(x)))\n+ vx(LanguageModel(x) A GoodPerformance(x) > UsedbySomeReseachers(x))\n+ vx (UsedbySomeResearchers(x) — Popular(x))\n\n+ LanguageModel(bert) A Giant(bert) > LanguageModel(gpt3) A Giant(gpt3)\n\nVariables:\ntractor € [1, 2, 3]\nminivan € [1, 2, 3]\nconvertible € [1, 2, 3]\n\nDomain:\n1: oldest\n3: newest\n\nConstraints:\n\ntractor ==2\n\nminivan > convertible\nAllDifferentConstraint(tractor, minivan, convertible)\n\nConductElectricity(Nail, True)\n\nThe statement “Nails cannot\nconduct electricity” is false.\n\nEntailment\n\n\\\n|\n{convertible: 1, tractor: 2, minivan: 3} !\n|\n\nThe statement “GPT3\nis popular” is true.\n\nA) The convertible is the oldest.\n\nFigure 2: Overview of our LOGIC-LM model, which consists of three modules: (1) Problem Formulator generates\na symbolic representation for the input problem with LLMs via in-context learning (2) Symbolic Reasoner performs\nlogical inference on the formulated problem, and (3) Result Interpreter interprets the symbolic answer.\n\naddress this, recent work has begun to augment lan-\nguage models with access to external tools and re-\nsources, such as the information retriever (Nakano\net al., 2021; Shi et al., 2023; Lazaridou et al.,\n2022), calculator (Cobbe et al., 2021), code in-\nterpreter (Wang et al., 2022), planner (Liu et al.,\n2023a), and other pre-trained models (Shen et al.,\n2023). Recent works (Gao et al., 2023; Chen et al.,\n2022) have achieved improved performance on\narithmetic reasoning tasks by generating Python\nprograms that specify the reasoning procedure as\nchained commands in the order of execution. How-\never, this idea has not been extended to logical\nreasoning problems, primarily due to the challenge\nof representing their highly “non-linear” reasoning\nprocedure (e.g., hypothesizing, case-by-case analy-\nsis, and the process of elimination) with functional\nprogramming. Our work provides a novel way\nto solve this within the framework of augmented\nLLMs. Instead of parsing the problem-solving pro-\ncedure as programs, we only describe the problem\nwith symbolic language using LLMs and then of-\nfload the reasoning to external symbolic solvers.\n\nAuto-Formalization. The concept of convert-\ning natural language into symbolic representations\nhas been widely adopted in auto-formalization for\nmathematical reasoning (Wu et al., 2022; Drori\n\net al., 2022; He-Yueya et al., 2023; Jiang et al.,\n2023). These works demonstrate the proficiency\nof LLMs in translating a considerable fraction of\nmathematical problems into formal specifications\ndefined in tools like SymPy (Meurer et al., 2017),\nIsabelle/HOL (Paulson, 1994), and Lean (de Moura\net al., 2015). Mathematical reasoning can be con-\nsidered a specialized subset of logical reasoning,\nprimarily focused on numeric deductions. Due to\nthis numeric specificity, mathematical problems are\noften more readily translatable to symbolic forms.\nIn contrast, logical reasoning covers a wider array\nof problem types, often requiring a deeper under-\nstanding of world knowledge and commonsense\nfor effective parsing into symbolic forms. Despite\nplenty of works studying mathematical reasoning,\nour work pioneers in extending the concept of auto-\nformalization to a broader range of logical reason-\ning tasks with modern LLMs.\n\n3 LoGic-LM\n\nAs shown in Figure 2, the inputs of our model are\na logical reasoning problem P described in natural\nlanguage, along with a goal G in the form of a\nmultiple-choice or free-form question. LoGIc-LM\nthen follows a problem formulation-and-reasoning\nparadigm to solve the problem.\n\n3808\n", "vlm_text": "The image is a diagram outlining the structure of the LOGIC-LM model, which is composed of three main modules: \n\n1. **Problem Formulator**: This module generates a symbolic representation of the input problem using language processing models (LLMs) through a technique called in-context learning. The image shows three different problems being represented with distinct sets of rules, facts, and queries.\n\n2. **Symbolic Reasoner**: This component performs logical inference on the formulated problem using various methods such as Logic Programming, First-order Logic Prover, and Constraint Optimization. Each method is utilized based on the nature of the problem: logic programming is used for problems related to rules, first-order logic provers are used for logical entailment, and constraint optimization is employed for problems involving constraints.\n\n3. **Result Interpreter**: This module interprets the symbolic answer derived by the Symbolic Reasoner to provide a comprehensible answer to the original problem posed.\n\nThe image visually partitions these modules and illustrates how they work together to process queries related to three different scenarios: electricity conduction, language model popularity, and determining the oldest vehicle. Each scenario involves posing a problem, formulating it symbolically, reasoning through logic or constraints, and finally interpreting the result to answer the query.\naddress this, recent work has begun to augment lan- guage models with access to external tools and re- sources, such as the information retriever ( Nakano et al. ,  2021 ;  Shi et al. ,  2023 ;  Lazaridou et al. , 2022 ), calculator ( Cobbe et al. ,  2021 ), code in- terpreter ( Wang et al. ,  2022 ), planner ( Liu et al. , 2023a ), and other pre-trained models ( Shen et al. , 2023 ). Recent works ( Gao et al. ,  2023 ;  Chen et al. , 2022 ) have achieved improved performance on arithmetic reasoning tasks by generating Python programs that specify the reasoning procedure as chained commands in the order of execution. How- ever, this idea has not been extended to logical reasoning problems, primarily due to the challenge of representing their highly “non-linear” reasoning procedure ( e.g. , hypothesizing, case-by-case analy- sis, and the process of elimination) with functional programming. Our work provides a novel way to solve this within the framework of augmented LLMs. Instead of parsing the problem-solving pro- cedure as programs, we only describe the problem with symbolic language using LLMs and then of- fload the reasoning to external symbolic solvers. \nAuto-Formalization. The concept of convert- ing natural language into symbolic representations has been widely adopted in auto-formalization for mathematical reasoning ( Wu et al. ,  2022 ;  Drori et al. ,  2022 ;  He-Yueya et al. ,  2023 ;  Jiang et al. , 2023 ). These works demonstrate the proficiency of LLMs in translating a considerable fraction of mathematical problems into formal specifications defined in tools like SymPy ( Meurer et al. ,  2017 ), Isabelle/HOL ( Paulson ,  1994 ), and Lean ( de Moura et al. ,  2015 ). Mathematical reasoning can be con- sidered a specialized subset of logical reasoning, primarily focused on numeric deductions. Due to this numeric specificity, mathematical problems are often more readily transl a table to symbolic forms. In contrast, logical reasoning covers a wider array of problem types, often requiring a deeper under- standing of world knowledge and commonsense for effective parsing into symbolic forms. Despite plenty of works studying mathematical reasoning, our work pioneers in extending the concept of auto- formalization to a broader range of logical reason- ing tasks with modern LLMs. \n\n3 L OGIC -LM \nAs shown in Figure  2 , the inputs of our model are a logical reasoning problem  $P$   described in natural language, along with a goal    $G$   in the form of a multiple-choice or free-form question. L OGIC -LM then follows a  problem formulation-and-reasoning paradigm to solve the problem. "}
{"page": 3, "image_path": "doc_images/2023.findings-emnlp.248_3.jpg", "ocr_text": "In the Problem Formulation stage, we prompt an\nLLM to translate the problem and the goal into a\ntask-specific symbolic language. In the Symbolic\nReasoning stage, we call a deterministic symbolic\nsolver, e.g., a logic programming engine, to ob-\ntain a symbolic-represented answer. Finally, an\nLLM- or rule-based Result Interpreter is respon-\nsible for translating the answer back to natural\nlanguage. Using this approach, the reasoning is\nguaranteed to be faithful as long as the problem\nformulation is correct since the answer A is the\nresult of executing deterministic algorithms (e.g.,\nforward/backward-chaining) embedded within the\nsymbolic reasoner. Compared to previous methods\nbased on chain-of-thought, our framework reduces\nthe burden of LLMs by shifting their focus from\n“solving the problem by reasoning step-by-step” to\n“representing the problem in symbolic language”.\n\n3.1. Problem Formulator\n\nIntuitively, LLMs may struggle with directly solv-\ning complex reasoning problems. However, they\nhave demonstrated a notable ability to comprehend\ntextual inputs and translate them into formal pro-\ngrams, such as mathematical equations (He-Yueya\net al., 2023) or Python codes (Gao et al., 2023). We\nposit that this capability to formulate problems into\ndifferent languages can be extended to symbolic\nlanguages as well. We leverage the few-shot gener-\nalization ability of LLMs to achieve this. By pro-\nviding the LLM with detailed instructions about the\ngrammar of the symbolic language, alongside a few\ndemonstrations as in-context examples, we observe\nthat LLMs, like InstructGPT (Ouyang et al., 2022b)\nand GPT-4 (OpenAI, 2023), can effectively follow\nthe instructions to identify key entities, facts, and\ntules present in the problem statement, and then\ntranslate these elements into symbolic language\nfollowing our defined grammar.\n\nSpecifically, we use four different symbolic for-\nmulations to cover four common types of logical\nreasoning problems: deductive reasoning, first-\norder logic reasoning, constraint satisfaction prob-\nlem, and analytical reasoning. These formula-\ntions provide a foundation for translating natu-\nral language-based problem statements. By defin-\ning additional problem-specific formulations, our\nframework retains the flexibility to accommodate a\nwider range of reasoning tasks. Next, we will delve\ninto the grammar of each symbolic formulation.\nExamples of each problem type are in Figure 2.\n\nLogic Programming (LP) Language. Deduc-\ntive reasoning typically starts from known facts and\nrules, and iteratively makes new inferences until the\ngoal statement can be proved or disproved (Poole\nand Mackworth, 2010). The Prolog logic pro-\ngramming language (Clocksin and Mellish, 2003;\nKorner et al., 2022) is arguably the most prominent\nsymbolic language to describe deductive reasoning\nproblems. We adopt its grammar to represent a\nproblem as facts, rules, and queries.\n\ne Facts: a fact F is a simple statement with a\npredicate and a set of arguments, formulated as\nP(ai,-+++ ,G@n), where P is the predicate name and\neach argument a; can be a variable, entity, num-\nber, or bool. For example, Age(Peter, 31) means\n“Peter’s age is 31”, and MadeOfIron(Nails, True)\nrepresents the fact “Nails are made of iron”.\n\ne Rules: rules are written in the form of clauses:\nFi A-+-AF mn > Fin4iA-+-A£Fp, where each F; is\na fact and the rule means “‘if the facts Fj,--- , Fy,\nare true, then the facts Fin41--- Fy are also true.”\ne Queries: a query @ is simply another fact re-\nquired to be proved based on known facts and rules.\n\nFirst-Order Logic (FOL). While the logic pro-\ngramming language efficiently represents common\ndeductive reasoning problems, it may fail to rep-\nresent more complex first-order logic (FOL) prob-\nlems. To address this, we also include the FOL\ngrammar (Enderton, 2001) in Appendix A. A prob-\nlem is then parsed into a list of FOL formulas,\nwhich are divided into Premises (the known in-\nformation from the problem) and Conclusion (the\nunknown formula to be proved). An example sen-\ntence and its FOL formula are given in Table 1.\n\nConstraint Satisfaction (CSP). Constraint sat-\nisfaction problems (CSPs) (Kumar, 1992) aims\nto find the value assignment of a set of objects\nthat satisfy a number of constraints. A CSP\nis often defined as a triple (X,D,C), where\n\nX = {x1,--+,%p} is a set of variables, D =\n{D,,--+ , Dy} is a set of their respective domains\nof values, and C = {C},:-+ ,Cm} is a set of con-\n\nstraints. Each variable x; can take on the values\nin the nonempty domain D;. Every constraint Cj\nis a pair (tj, Rj), where t; C X is a subset of k\nvariables and R; is a k-ary relation on the corre-\nsponding subset of domains D;. We use the above\nsyntax to define a CSP problem as variables, do-\nmains, and constraints. An example is given in\nboth Figure 2 and Table 1.\n\n3809\n", "vlm_text": "In the  Problem Formulation  stage, we prompt an LLM to translate the problem and the goal into a task-specific symbolic language. In the  Symbolic Reasoning  stage, we call a deterministic symbolic solver,  e.g. , a logic programming engine, to ob- tain a symbolic-represented answer. Finally, an LLM- or rule-based  Result Interpreter  is respon- sible for translating the answer back to natural language. Using this approach, the reasoning is guaranteed to be faithful as long as the problem formulation is correct since the answer    $A$   is the result of executing deterministic algorithms ( e.g. , forward/backward-chaining) embedded within the symbolic reasoner. Compared to previous methods based on chain-of-thought, our framework reduces the burden of LLMs by shifting their focus from\n\n “ solving  the problem by reasoning step-by-step” to\n\n “ representing  the problem in symbolic language”. \n3.1 Problem Formulator \nIntuitively, LLMs may struggle with directly solv- ing complex reasoning problems. However, they have demonstrated a notable ability to comprehend textual inputs and translate them into formal pro- grams, such as mathematical equations ( He-Yueya et al. ,  2023 ) or Python codes ( Gao et al. ,  2023 ). We posit that this capability to  formulate problems into different languages  can be extended to symbolic languages as well. We leverage the few-shot gener- alization ability of LLMs to achieve this. By pro- viding the LLM with detailed instructions about the grammar of the symbolic language, alongside a few demonstrations as in-context examples, we observe that LLMs, like Instruct GP T ( Ouyang et al. ,  2022b ) and GPT-4 ( OpenAI ,  2023 ), can effectively follow the instructions to identify key entities, facts, and rules present in the problem statement, and then translate these elements into symbolic language following our defined grammar. \nSpecifically, we use four different symbolic for- mulations to cover four common types of logical reasoning problems:  deductive reasoning ,  first- order logic reasoning ,  constraint satisfaction prob- lem , and  analytical reasoning . These formula- tions provide a foundation for translating natu- ral language-based problem statements. By defin- ing additional problem-specific formulations, our framework retains the flexibility to accommodate a wider range of reasoning tasks. Next, we will delve into the grammar of each symbolic formulation. Examples of each problem type are in Figure  2 . \nLogic Programming (LP) Language. Deduc- tive reasoning typically starts from known facts and rules, and iterative ly makes new inferences until the goal statement can be proved or disproved ( Poole and Mackworth ,  2010 ). The  Prolog  logic pro- gramming language ( Clocksin and Mellish ,  2003 ; Körner et al. ,  2022 ) is arguably the most prominent symbolic language to describe deductive reasoning problems. We adopt its grammar to represent a problem as facts, rules, and queries. \n•  Facts : a fact    $F$   is a simple statement with a predicate  and a set of  arguments , formulated as  $P(a_{1},\\cdot\\cdot\\cdot,a_{n})$  , where    $P$   is the predicate name and each argument    $a_{i}$   can be a variable, entity, num- ber, or bool. For example,  Age ( Peter ,  31)  means “Peter’s age is   $31^{\\circ}$  , and  MadeOfIron ( Nails ,  True ) represents the fact “Nails are made of iron”. \n•  Rules : rules are written in the form of cla es:  $F_{1}\\wedge\\cdot\\cdot\\cdot\\wedge F_{m}\\to F_{m+1}\\wedge\\cdot\\cdot\\cdot\\wedge F_{n}$  , whe  $F_{i}$  a fact and the rule means “if the facts  $F_{1},\\cdot\\cdot\\cdot\\ ,F_{m}$   · · · are true, then the facts  $F_{m+1}\\cdot\\cdot\\cdot F_{n}$   are also true.” •  Queries : a query    $Q$   is simply another fact re- quired to be proved based on known facts and rules. \nFirst-Order Logic (FOL). While the logic pro- gramming language efficiently represents common deductive reasoning problems, it may fail to rep- resent more complex first-order logic (FOL) prob- lems. To address this, we also include the FOL grammar ( Enderton ,  2001 ) in Appendix  A . A prob- lem is then parsed into a list of FOL formulas, which are divided into  Premises  (the known in- formation from the problem) and  Conclusion  (the unknown formula to be proved). An example sen- tence and its FOL formula are given in Table  1 . \nConstraint Satisfaction (CSP). Constraint sat- isfaction problems (CSPs) ( Kumar ,  1992 ) aims to find the value assignment of a set of objects that satisfy a number of constraints. A CSP is often defined as a triple    $(X,D,C)$  , where  $X~=~\\{x_{1},\\cdot\\cdot\\cdot,x_{n}\\}$   is a set of variables,    $D\\ =$   $\\{D_{1},\\cdot\\cdot\\cdot\\ ,D_{n}\\}$   is a set of their respective domains of values, and    $C=\\{C_{1},\\cdot\\cdot\\cdot,C_{m}\\}$   is a set of con- straints. Each variable    $x_{i}$   can take on the values in the nonempty domain  $D_{i}$  . Every constraint    $C_{j}$  is a pair    $\\langle t_{j},R_{j}\\rangle$  , wh e    $t_{j}\\subset X$   is a subset of    $k$  variables and  $R_{j}$   is a  k -ary relation on the corre- sponding subset of domains  $D_{j}$  . We use the above syntax to define a CSP problem as variables, do- mains, and constraints. An example is given in both Figure  2  and Table  1 . "}
{"page": 4, "image_path": "doc_images/2023.findings-emnlp.248_4.jpg", "ocr_text": "+ Example 5\nProblem | Formulation NL Sentence Symbolic Formulation Solver Dataset\n. Tf the circuit is complete and Complete(Circuit, True)A\npaduelive LP the circuit has the light bulb Has(Circuit, LightBulb) Pyke A roe\n: es then the light bulb is glowing. — Glowing(LightBulb, True)\nFirst-Order A Czech person wrote a book | Jag3a1(Czech(2) A Author(2, 71)\nLogic FOL in 1946. ABook(2) \\ Publish(r2, 1946)) Proverd FOLIO\nConstraint On a shelf, there are five books. blue_book € {1, 2,3, 4,5} thon-\nSatisfaction CSP The blue book is to the right yellow_book € {1,2,3,4,5} teint LogicalDeduction\n° of the yellow book. blue_book > yellow_book :\naletian _ . repairs(Xena, radios) A\nAnalytical SAT Xena and exactly three other | Cont (ct:technicians], t#Xena| — Z3 AR-LSAT\nReasoning technicians repair radios - -\nA repairs(t, radios))) == 3)\n\nTable 1: A summary of the symbolic formulations (with examples) and symbolic solvers we use for the five datasets\nin our study, representing four different types of logical reasoning problems.\n\nBoolean Satisfiability (SAT) Formulation. SAT\nis the problem of deciding if there is an assignment\nto the variables of a Boolean formula such that\nthe formula is satisfied. Many analytical reasoning\nproblems can be formulated as SAT problems. We\nadopt the grammar defined in Ye et al. (2023) to\nformulate an SAT problem P as (®, 7, Q), where\n® is a set of constraints defined under the theory 7,\nand Q is the query of interest.\n\nTable 1 summarizes the four types of logical\nreasoning problems, their typical datasets, and the\nsymbolic formulation used to represent each type of\nproblem. We also give an example of a natural lan-\nguage statement with its corresponding symbolic\nformulation for each type. Appendix C shows the\nfull prompts we use for the problem formulator.\nTo teach LLMs to better align each statement with\nits corresponding symbolic form, we use the for-\nmat SYMBOLIC_FORMULA ::: NL_STATEMENT\nin in-context examples to enable better grounding.\n\n3.2. Symbolic Reasoner\n\nAfter the problem formulator parses the problem\nP and the goal G' into symbolic representations\nP and G, we call a deterministic external solver\ndepending on the task, to obtain the answer A. Ta-\nble 1 summarizes the symbolic solvers we use for\neach type of logical reasoning problem.\n\nLP System. For deductive reasoning, we incor-\nporate the Pyke expert system (Frederiksen, 2008),\nwhich makes inferences based on the logic pro-\ngramming language. In response to a query, Pyke\nfirst creates a knowledge base, populating it with\nknown facts and rules. Subsequently, it applies\nforward- and backward-chaining algorithms to in-\nfer new facts and substantiate the goal.\n\nFOL Prover. We use Prover9 as the FOL in-\nference engine. Prover9 is an automated theorem\nprover that supports first-order logic and equational\nlogic. It initially converts FOL statements to con-\njunctive normal form (CNF) and then performs\nresolution (Robinson, 1965) on the CNF to deduce\nwhether a conclusion is true, false, or unknown.\n\nCSP Solver. Solving a CSP is to find value as-\nsignments for all variables that satisfy all given\nconstraints. Commonly used algorithms for this\ntask include backtracking, constraint propagation,\nand local search variants. To this end, we incor-\nporate the python-constraint> package which\noffers solvers for CSPs over finite domains.\n\nSAT Solver. For solving SAT problems, we use\nthe Z3 theorem prover (de Moura and Bjgrner,\n2008), a satisfiability modulo theories (SMT)\nsolver developed by Microsoft*. The SMT solver\nprovides algorithms to determine whether a set of\nmathematical formulas is satisfiable. It generalizes\nthe SAT problems to more complex formulas in-\nvolving real numbers, integers, and various data\nstructures such as lists, arrays, bit vectors, and\nstrings. A lot of real-world analytical reasoning\nproblems can be represented as problems of solv-\ning a system of equations.\n\n3.3 Self-Refiner\n\nFor complex problems, generating the correct log-\nical form may become challenging for LLMs. To\naddress this, we introduce a self-refinement mod-\nule that learns to modify inaccurate logical for-\n\nhttps: //www.cs.unm. edu/~mccune/prover9/\n\nShttps://github.com/python-constraint/\npython-constraint\n\n‘https: //github. com/Z3Prover/z3\n\n3810\n", "vlm_text": "The table presents an overview of different problem types and their corresponding formulations in the context of logic and reasoning, along with examples, solvers used, and datasets associated with each problem type. Here's a breakdown of the information provided for each row:\n\n1. **Problem: Deductive Reasoning**\n   - **Formulation:** LP (Logical Programming)\n   - **Example NL Sentence:** \"If the circuit is complete and the circuit has the light bulb then the light bulb is glowing.\"\n   - **Symbolic Formulation:** `Complete(Circuit, True) ∧ Has(Circuit, LightBulb) → Glowing(LightBulb, True)`\n   - **Solver:** Pyke\n   - **Dataset:** ProntoQA, ProofWriter\n\n2. **Problem: First-Order Logic**\n   - **Formulation:** FOL (First-Order Logic)\n   - **Example NL Sentence:** \"A Czech person wrote a book in 1946.\"\n   - **Symbolic Formulation:** `∃x2 ∃x1 (Czech(x1) ∧ Author(x2, x1) ∧ Book(x2) ∧ Publish(x2, 1946))`\n   - **Solver:** Prover9\n   - **Dataset:** FOLIO\n\n3. **Problem: Constraint Satisfaction**\n   - **Formulation:** CSP (Constraint Satisfaction Problem)\n   - **Example NL Sentence:** \"On a shelf, there are five books. The blue book is to the right of the yellow book.\"\n   - **Symbolic Formulation:** \n     - `blue_book ∈ {1, 2, 3, 4, 5}`\n     - `yellow_book ∈ {1, 2, 3, 4, 5}`\n     - `blue_book > yellow_book`\n   - **Solver:** python-constraint\n   - **Dataset:** LogicalDeduction\n\n4. **Problem: Analytical Reasoning**\n   - **Formulation:** SAT (Satisfiability Testing)\n   - **Example NL Sentence:** \"Xena and exactly three other technicians repair radios.\"\n   - **Symbolic Formulation:** \n     - `repairs(Xena, radios) ∧ Count([t:technicians], t ≠ Xena ∧ repairs(t, radios)) = 3`\n   - **Solver:** Z3\n   - **Dataset:** AR-LSAT\n\nEach row defines a specific type of reasoning problem, showing how natural language sentences can be transformed into symbolic logic formulations suitable for various solvers, which are then applied to specific datasets for evaluation or training purposes.\nBoolean Satisfiability (SAT) Formulation. SAT is the problem of deciding if there is an assignment to the variables of a Boolean formula such that the formula is satisfied. Many analytical reasoning problems can be formulated as SAT problems. We adopt the grammar defined in  Ye et al.  ( 2023 ) to rmulate an SAT problem    $\\mathcal{P}$   as    $(\\Phi,{\\mathcal{T}},\\mathcal{Q})$  , wh  $\\Phi$   is a set of constraints defined under the theory  T  , and    $\\mathcal{Q}$   is the query of interest. \nTable  1  summarizes the four types of logical reasoning problems, their typical datasets, and the symbolic formulation used to represent each type of problem. We also give an example of a natural lan- guage statement with its corresponding symbolic formulation for each type. Appendix  C  shows the full prompts we use for the problem formulator. To teach LLMs to better align each statement with its corresponding symbolic form, we use the for- mat  SYMBOLIC _ FORMULA  ::: NL_ STATEMENT in in-context examples to enable better grounding. \n3.2 Symbolic Reasoner \nAfter the problem formulator parses the problem  $P$   and the goal    $G$   into symbolic representations  $\\hat{P}$   and  $\\hat{G}$  , we call a deterministic external solver depending on the task, to obtain the answer  $A$  . Ta- ble  1  summarizes the symbolic solvers we use for each type of logical reasoning problem. \nLP System. For deductive reasoning, we incor- porate the  Pyke  expert system ( Frederiksen ,  2008 ), which makes inferences based on the logic pro- gramming language. In response to a query,  Pyke first creates a knowledge base, populating it with known facts and rules. Subsequently, it applies forward- and backward-chaining algorithms to in- fer new facts and substantiate the goal. \nFOL Prover. We use  Prover  $9^{2}$    as the FOL in- ference engine.  Prover9  is an automated theorem prover that supports first-order logic and equational logic. It initially converts FOL statements to con- junctive normal form (CNF) and then performs resolution ( Robinson ,  1965 ) on the CNF to deduce whether a conclusion is true, false, or unknown. \nCSP Solver. Solving a CSP is to find value as- signments for all variables that satisfy all given constraints. Commonly used algorithms for this task include backtracking, constraint propagation, and local search variants. To this end, we incor- porate the  python-constraint 3   package which offers solvers for CSPs over finite domains. \nSAT Solver. For solving SAT problems, we use the  Z3  theorem prover ( de Moura and Bjørner , 2008 ), a satisfiability modulo theories (SMT) solver developed by Microsoft 4 . The SMT solver provides algorithms to determine whether a set of mathematical formulas is satisfiable. It generalizes the SAT problems to more complex formulas in- volving real numbers, integers, and various data structures such as lists, arrays, bit vectors, and strings. A lot of real-world analytical reasoning problems can be represented as problems of solv- ing a system of equations. \n3.3 Self-Refiner \nFor complex problems, generating the correct log- ical form may become challenging for LLMs. To address this, we introduce a  self-refinement  mod- ule that learns to modify inaccurate logical for- mulations using the error messages from the sym- bolic reasoner as feedback. Recent works ( Chen et al. ,  2023 ;  Madaan et al. ,  2023 ) have adopted sim- ilar ideas to improve code generation, by teaching LLMs to debug their predicted programs via few- shot demonstrations. Here we extend this idea to refine generated logic representations. If the sym- bolic solver returns an execution error, we instruct the LLM to refine the incorrect logical form, by prompting it with the erroneous logic form, the solver’s error message, and a set of demonstrations showing common error cases ( e.g. , a free variable is not bounded to any quantifier in FOL) and their remedies. We run this process iterative ly until ei- ther no error messages are returned, or the maxi- mum number of allowable revisions is reached. "}
{"page": 5, "image_path": "doc_images/2023.findings-emnlp.248_5.jpg", "ocr_text": "mulations using the error messages from the sym-\nbolic reasoner as feedback. Recent works (Chen\net al., 2023; Madaan et al., 2023) have adopted sim-\nilar ideas to improve code generation, by teaching\nLLMs to debug their predicted programs via few-\nshot demonstrations. Here we extend this idea to\nrefine generated logic representations. If the sym-\nbolic solver returns an execution error, we instruct\nthe LLM to refine the incorrect logical form, by\nprompting it with the erroneous logic form, the\nsolver’s error message, and a set of demonstrations\nshowing common error cases (e.g., a free variable\nis not bounded to any quantifier in FOL) and their\nremedies. We run this process iteratively until ei-\nther no error messages are returned, or the maxi-\nmum number of allowable revisions is reached.\n\n3.4 Result Interpreter\n\nFinally, the result interpreter translates the results\nreturned from the symbolic solver back to a natural\nlanguage answer. For certain problems, this can\nbe achieved through predefined rules; for example,\nmapping Entailment to true. However, this pro-\ncess can be more complex for CSPs, e.g., translat-\ning {convertible: 1, tractor: 2, minivan: 3} to “the\nconvertible is the oldest.”. To handle these varying\nlevels of complexity, we designed both rule-based\nand LLM-based result interpreters. Details of the\nresult interpreter are given in Appendix D.\n\n4 Experiments\n\nDatasets. We evaluate LOGIC-LM on five com-\nmon logical reasoning datasets, as follows.\nPrOntoQA (Saparov and He, 2023) is a recent\nsynthetic dataset created to analyze the capacity of\nLLMs for deductive reasoning. We use the hardest\nfictional characters version of the dataset, based on\nthe results in Saparov and He (2023). Each version\nis divided into different subsets depending on the\nnumber of reasoning hops required. We use the\nhardest 5-hop subset for evaluation. Each question\nin PrOntoQA aims to validate a new fact’s veracity,\nsuch as “True or false: Alex is not shy.”.\nProofWriter (Tafjord et al., 2021) is another\ncommonly used dataset for deductive logical rea-\nsoning. Compared with PrOntoQA, the problems\nare expressed in a more naturalistic language form.\nWe use the open-world assumption (OWA) subset\nin which each example is a (problem, goal) pair\nand the label is one of {PROVED, DISPROVED,\nUNKNOWN}. The dataset is divided into five parts,\n\neach part requiring 0, < 1, < 2, < 3, and < 5 hops\nof reasoning, respectively. We evaluate the hardest\ndepth-5 subset. To reduce overall experimentation\ncosts, we randomly sample 600 examples in the\ntest set and ensure a balanced label distribution.\n\nFOLIO (Han et al., 2022) is a challenging\nexpert-written dataset for logical reasoning. The\nproblems are mostly aligned with real-world knowl-\nedge and use highly natural wordings, and the ques-\ntions require complex first-order logic reasoning to\nsolve. We use the entire FOLIO test set for evalua-\ntion, consisting of 204 examples.\n\nLogicalDeduction is a challenging logical rea-\nsoning task from the BigBench (Srivastava et al.,\n2022) collaborative benchmark. The problems are\nmostly about deducing the order of a sequence of\nobjects from a minimal set of conditions. We use\nthe full test set consisting of 300 examples.\n\nAR-LSAT (Zhong et al., 2022) is a dataset that\ncollects all analytical logic reasoning questions\nfrom the Law School Admission Test from 1991 to\n2016. We use the test set which has 231 multiple-\nchoice questions. AR-LSAT is particularly chal-\nlenging, with state-of-the-art models only achiev-\ning performance slightly better than random guess-\ning (Liang et al., 2022; Ribeiro et al., 2023a).\n\nWe convert all examples into a standard multiple-\nchoice format, comprising a problem statement, a\nquestion, and potential answers, as shown in Fig-\nure 2. We also select 1-5 examples from the train-\ning set of each dataset as in-context examples. De-\ntailed data statistics are in Appendix B.\n\nBaselines. We compare our model against two\nbaselines that depend solely on LLMs for logical\nreasoning: 1) Standard LLMs, which leverage in-\ncontext learning to directly answer the question;\nand 2) Chain-of-Thought (CoT) (Wei et al., 2022b),\nwhich adopts a step-by-step problem-solving ap-\nproach, generating explanations before providing\nhe final answer. We separately evaluate the set-\nings that ChatGPT (gpt-3.5-turbo), GPT-3.5\n(text-davinci-003) (Ouyang et al., 2022a) and\nGPT-4 (gpt-4) (OpenAI, 2023) serve as the under-\nlying LLMs for all models. To ensure fair com-\nparisons, we use the same in-context examples for\nall models. For reproducible results, we set the\nemperature to 0 and select the response with the\nhighest probability from LLMs. Since all examples\nare formed as multiple-choice questions, we eval-\nuate model performance based on the accuracy of\nselecting the correct answer.\n\n3811\n", "vlm_text": "\n3.4 Result Interpreter \nFinally, the result interpreter translates the results returned from the symbolic solver back to a natural language answer. For certain problems, this can be achieved through predefined rules; for example, mapping  Entailment  to  true . However, this pro- cess can be more complex for CSPs,  e.g. , translat- ing  {convertible: 1, tractor: 2, minivan:   $3\\rangle$   to “ the convertible is the oldest. ”. To handle these varying levels of complexity, we designed both rule-based and LLM-based result interpreters. Details of the result interpreter are given in Appendix  D . \n4 Experiments \nDatasets. We evaluate L OGIC -LM on five com- mon logical reasoning datasets, as follows. \nPrOntoQA  ( Saparov and He ,  2023 ) is a recent synthetic dataset created to analyze the capacity of LLMs for deductive reasoning. We use the hardest fictional characters  version of the dataset, based on the results in  Saparov and He  ( 2023 ). Each version is divided into different subsets depending on the number of reasoning hops required. We use the hardest 5-hop subset for evaluation. Each question in PrOntoQA aims to validate a new fact’s veracity, such as “True or false: Alex is not shy.”. \nProof Writer  ( Tafjord et al. ,  2021 ) is another commonly used dataset for deductive logical rea- soning. Compared with PrOntoQA, the problems are expressed in a more naturalistic language form. We use the open-world assumption (OWA) subset in which each example is a (problem, goal) pair and the label is one of { PROVED ,  DISPROVED , UNKNOWN }. The dataset is divided into five parts, each part requiring    $0,\\leq1,\\leq2,\\leq3$  , and    $\\leq5$   hops of reasoning, respectively. We evaluate the hardest depth-5  subset. To reduce overall experimentation costs, we randomly sample 600 examples in the test set and ensure a balanced label distribution. \n\nFOLIO  ( Han et al. ,  2022 ) is a challenging expert-written dataset for logical reasoning. The problems are mostly aligned with real-world knowl- edge and use highly natural wordings, and the ques- tions require complex first-order logic reasoning to solve. We use the entire FOLIO test set for evalua- tion, consisting of 204 examples. \nLogical Deduction  is a challenging logical rea- soning task from the BigBench ( Srivastava et al. , 2022 ) collaborative benchmark. The problems are mostly about deducing the order of a sequence of objects from a minimal set of conditions. We use the full test set consisting of 300 examples. \nAR-LSAT  ( Zhong et al. ,  2022 ) is a dataset that collects all analytical logic reasoning questions from the Law School Admission Test from 1991 to 2016. We use the test set which has 231 multiple- choice questions. AR-LSAT is particularly chal- lenging, with state-of-the-art models only achiev- ing performance slightly better than random guess- ing ( Liang et al. ,  2022 ;  Ribeiro et al. ,  2023a ). \nWe convert all examples into a standard multiple- choice format, comprising a problem statement, a question, and potential answers, as shown in Fig- ure  2 . We also select 1-5 examples from the train- ing set of each dataset as in-context examples. De- tailed data statistics are in Appendix  B . \nBaselines. We compare our model against two baselines that depend solely on LLMs for logical reasoning: 1)  Standard  LLMs, which leverage in- context learning to directly answer the question; and 2)  Chain-of-Thought  (CoT) ( Wei et al. ,  2022b ), which adopts a step-by-step problem-solving ap- proach, generating explanations before providing the final answer. We separately evaluate the set- tings that ChatGPT ( gpt-3.5-turbo ), GPT-3.5 ( text-davinci-003 ) ( Ouyang et al. ,  2022a ) and GPT-4 ( gpt-4 ) ( OpenAI ,  2023 ) serve as the under- lying LLMs for all models. To ensure fair com- parisons, we use the same in-context examples for all models. For reproducible results, we set the temperature to 0 and select the response with the highest probability from LLMs. Since all examples are formed as multiple-choice questions, we eval- uate model performance based on the accuracy of selecting the correct answer. "}
{"page": 6, "image_path": "doc_images/2023.findings-emnlp.248_6.jpg", "ocr_text": "ChatGPT (gpt-3.5-turbo) _ |\n\nGPT-3.5 (text-davinci-003) |\n\nGPT-4 (gpt-4)\n\nDataset | Standard CoT Logic-LM | Standard CoT Logic-LM | Standard CoT  Logic-LM\nPrOntoQA | 47.40 ~— 67.80 61.00 | 5180 83.00 85.00 | 77.40 98.79 83.20\nProofWriter | 35.50 49.17 58.33 | 36.16 48.33 7.45 | 52.67 68.11 79.66\nFOLIO | 45.09 57.35 62.74 | 54.60 57.84 61.27 | 69.11 70.58 78.92\nLogicalDeduction | 40.00 42.33 65.67 | 41.33 48.33 62.00 | 71.33 75.25 87.63\nAR-LSAT | 20.34 17.31 26.41 | 22.51 22.51 25.54 | 33.33 35.06 43.04\n\nTable 2: Accuracy of standard promoting (Standard), chain-of-thought promoting (CoT), and our method (LOGIC-\nLM, without self-refinement) on five reasoning datasets. The best results within each base LLM are highlighted.\n\n4.1 Main Results\n\nWe report the results of LOGIC-LM (without self-\nrefinement) and baselines in Table 2. For LOGIC-\nLM, a symbolic solver does not return an answer\nwhen there are grammar errors in the symbolic\nformulation. For these un-executable cases, we\nfall back on using chain-of-thought to predict the\nanswer. We have three major observations.\n\n1. Logic-LM significantly outperforms stan-\n\ndard LLMs and CoT across all datasets. With GPT-\n3.5, our method outperforms standard LLM on all\ndatasets, with an average improvement of 39.2%.\nThis highlights the benefit of combining LLMs\nwith external symbolic solvers for logical reason-\ning. LoGIC-LM also improves CoT by a large mar-\ngin of 18.4% on average, showing that offloading\nthe reasoning to symbolic solvers greatly improves\nfaithfulness compared with pure language-based\nreasoning with CoT.\n2. GPT-4 outperforms GPT-3.5 by a large margin\nof 48.46% on average for the standard prompting.\nThis aligns with the assertion that the main en-\nhancement of GPT-4 lies in its ability to carry out\ncomplex reasoning (OpenAI, 2023). Although this\nmay indicate that the logical reasoning capability\ncan be boosted by scaling up the LLM, we observe\nthat GPT-4 still makes numerous unfaithful reason-\ning errors. By delegating the reasoning to symbolic\nsolvers, our method can further improve GPT-4\nby an average of 24.98% and 10.44% for standard\nprompting and CoT prompting, respectively.\n\n3. While integrating CoT generally enhances\nLLM performance, we find its benefits compara-\n\ntively less substantial or even negative on FOLIO,\nLogicalDeduction, and AR-LSAT, with a modest\nimprovement of 11.75%, 9.41%, and -3.2%, re-\nspectively. On the contrary, the benefits of CoT\non ProntoQA and ProofWriter are 51.59% and\n33.82%, respectively. A plausible explanation is\n\nDataset SR GPT-3.5 GPr-4\nExe_Rate Exe_Acc Exe_Rate Exe_Acc\n= 99.4% 84.9 100.0% 83.2\nPromtoQA 4 100.0% tos 85.0 0.1 100.0% 83.2\n= 873% 73.6 99.0% 79.6\nProofWriter 95.6% 13.3 74.1 10.5 99.0% 79.6\n= 66.7% 618 79.9% 80.4\nFOLIO + 843% T176 64.3125 85.8% 159 79.9 Jos\nLogical = 100.0% 62.0 100.0% 87.6\nDeduction + 100.0% 62.0 100.0% 87.6\n. 113% 57.7 32.6% 60.0\nAR-LSAT 21.8% t05 60.3726 39.8% 172 58.812\n\nTable 3: Analysis of accuracy and execution status of\nLocic-LM. We present the percentage of executable\nlogical formulations (Exe_Rate) together with the accu-\nracy of the execution (Exe_Acc). SR represents before\n(—) and after (++) self-refinement.\n\nthat CoT emulates human forward-chain reasoning:\nbeginning with known facts and sequentially de-\nriving new conclusions until the goal is met. This\nreasoning style aligns well with problems in the\nPrOntoQA and ProofWriter datasets. However,\nFOL and CSP problems often necessitate more\nsophisticated reasoning strategies that are “non-\nlinear” compared to standard forward-chain rea-\nsoning. These include hypothesizing, conditioning,\nrecursive inference, and the process of elimina-\ntion. Compared to CoT, the integration of symbolic\nsolvers is better suited to these reasoning styles,\nhence yielding a more marked improvement on FO-\nLIO (+21.85%), LogicalDeduction (+45.67%), and\nAR-LSAT (+24.14%).\n\n4.2 Effectiveness of Problem Formulator\n\nWe then evaluate how well LLM can translate a\ngiven problem into the symbolic formulation used\nby each symbolic solver. In Table 3, we report the\npercentage of symbolic formulations that are exe-\ncutable by the corresponding symbolic solver for\n\n3812\n", "vlm_text": "The table presents performance metrics for different models (ChatGPT, GPT-3.5, and GPT-4) across various datasets. The metrics are shown for three methods: Standard, CoT (Chain of Thought), and Logic-LM. The datasets include PrOntoQA, ProofWriter, FOLIO, LogicalDeduction, and AR-LSAT. Each cell contains numerical values representing model performance, with some values highlighted in green.\n4.1 Main Results \nWe report the results of L OGIC -LM ( without  self- refinement) and baselines in Table  2 . For L OGIC - LM, a symbolic solver does not return an answer when there are grammar errors in the symbolic formulation. For these un-executable cases, we fall back on using chain-of-thought to predict the answer. We have three major observations. \n1. Logic-LM significantly outperforms stan- dard LLMs and CoT across all datasets. With GPT- 3.5, our method outperforms standard LLM on all datasets, with an average improvement of  $39.2\\%$  . This highlights the benefit of combining LLMs with external symbolic solvers for logical reason- ing. L OGIC -LM also improves CoT by a large mar- gin of   $18.4\\%$   on average, showing that offloading the reasoning to symbolic solvers greatly improves faithfulness compared with pure language-based reasoning with CoT. \n2. GPT-4 outperforms GPT-3.5 by a large margin of   $48.46\\%$   on average for the standard prompting. This aligns with the assertion that the main en- hancement of GPT-4 lies in its ability to carry out complex reasoning ( OpenAI ,  2023 ). Although this may indicate that the logical reasoning capability can be boosted by scaling up the LLM, we observe that GPT-4 still makes numerous unfaithful reason- ing errors. By delegating the reasoning to symbolic solvers, our method can further improve GPT-4 by an average of  $24.98\\%$   and  $10.44\\%$   for standard prompting and CoT prompting, respectively. \n3. While integrating CoT generally enhances LLM performance, we find its benefits compara- tively less substantial or even negative on FOLIO, Logical Deduction, and AR-LSAT, with a modest improvement of   $11.75\\%$  ,   $9.41\\%$  , and   $-3.2\\%$  , re- spectively. On the contrary, the benefits of CoT on ProntoQA and Proof Writer are   $51.59\\%$   and  $33.82\\%$  , respectively. A plausible explanation is \nThis table compares the performance of GPT-3.5 and GPT-4 on various datasets: ProntoQA, ProofWriter, FOLIO, Logical Deduction, and AR-LSAT. It evaluates two metrics: Execution Rate (Exe_Rate) and Execution Accuracy (Exe_Acc), with and without SR (System Rewrite) indicated by '+' and '−'. \n\nKey points include:\n\n- For ProntoQA, GPT-4 maintains a 100% execution rate both with and without SR, but execution accuracy slightly decreases from GPT-3.5 to GPT-4.\n- In the ProofWriter dataset, both GPT-3.5 and GPT-4 achieve high execution rates, with minor differences in accuracy.\n- FOLIO shows improved execution rates and accuracy from \"−\" to \"+\" for both models.\n- In Logical Deduction, GPT-4 shows higher execution accuracy compared to GPT-3.5.\n- For AR-LSAT, GPT-4 shows improved execution rates but a slight decrease in accuracy when SR is applied.\n\nArrows indicate changes in performance with System Rewrite: blue for improvement and red for decline.\nthat CoT emulates human forward-chain reasoning: beginning with known facts and sequentially de- riving new conclusions until the goal is met. This reasoning style aligns well with problems in the PrOntoQA and Proof Writer datasets. However, FOL and CSP problems often necessitate more sophisticated reasoning strategies that are “non- linear” compared to standard forward-chain rea- soning. These include hypothesizing, conditioning, recursive inference, and the process of elimina- tion. Compared to CoT, the integration of symbolic solvers is better suited to these reasoning styles, hence yielding a more marked improvement on FO- LIO   $(+21.85\\%)$  , Logical Deduction   $(+45.67\\%)$  , and AR-LSAT   $(+24.14\\%)$  . \n4.2 Effectiveness of Problem Formulator \nWe then evaluate how well LLM can translate a given problem into the symbolic formulation used by each symbolic solver. In Table  3 , we report the percentage of symbolic formulations that are  exe- cutable  by the corresponding symbolic solver for "}
{"page": 7, "image_path": "doc_images/2023.findings-emnlp.248_7.jpg", "ocr_text": "Accuracy Standard -«-CoT\n\n-@Logic-LM\n\nReasoning Depth\n\nFigure 3: Accuracy of different models for increasing\nsize of reasoning depth on the ProofWriter dataset.\n\neach dataset (Exe_Rate). Generally, LLM demon-\nstrates high proficiency in transcribing problems\ninto symbolic formats, evidenced by its near 100%\nExe_Rate on ProntoQA, ProofWriter, and Logi-\ncalDeduction. However, the high performance on\nthese datasets is somewhat anticipated, given that\ntheir problems are mostly synthetically generated,\nlimiting language variability. When it comes to\ndatasets comprising real-world, expertly crafted\nproblems, such as FOLIO and AR-LSAT, GPT-\n4’s performance is notably less promising, with\nExe_Rate scores of 79.9% and 32.6% respectively.\nThis discrepancy underscores the inherent chal-\nlenges associated with converting real-world prob-\nlems into their logical equivalents.\n\nExe_Rate only reflects the grammar correctness\nof the logical form. We also report the accuracy\nof the executable samples (Exe_Acc) to measure\nWe find that logical\nforms generated by GPT-4 generally achieve high\nExe_Acc, even for the most challenging AR-LSAT\ndataset. Such performance accentuates the poten-\ntial of symbolic solvers in bolstering the model’s\nlogical reasoning prowess, contingent on the pre-\ncise translation of problems into symbolic forms.\n\nthe semantic correctness.\n\n4.3 Robustness of Reasoning\n\nIncorporating symbolic solvers also leads to more\nrobust reasoning. To illustrate this, we report\nthe performance of LoGIC-LM and baselines for\nquestions of varying complexity levels. We ran-\ndomly selected 300 examples from each subset\nof ProofWriter, ensuring a balanced label distri-\nbution. The problems in these subsets require 0,\n<=1, <=2, <=3, and <=5 hops of reasoning, respec-\ntively. The results, shown in Figure 3, indicate\nthat LoGic-LM becomes increasingly effective as\nthe required reasoning depth increases. For exam-\n\nA = + <CoT (GPT-3.5) + Logic-LM (GPT-3.5)\nccuracy — — Cot (GPT-4) 2 Logic-LM (GPT-4)\n85\n79.9 79.41\n80 78.92 78.43\n75\n5 70.58\n\n65 61.27 62.25\n\n60\n55\nRounds i) 1 2 3\n66.7%\n79.9%\n\nGPT-3.5 79.4% 82.4%\n\nGPT-4 85.3% 85.3%\n\nFigure 4: The accuracy for different rounds of self-\nrefinement, with the corresponding executable rates.\n\nple, LoGic-LM outperforms CoT by 7.1%, 5.0%,\n12.7%, 20.0%, and 39.4% on depth-0, depth-1,\ndepth-2, depth-4, and depth-5 problems, respec-\ntively. In LoGic-LM, multi-step logical reasoning\nis delegated to external symbolic solvers, thereby\ntransitioning the challenge of LLM from problem-\nsolving to problem representation. Ideally, the com-\nplexity of formally representing a problem state-\nment in logical form should remain relatively con-\nstant, regardless of whether the questions require\nsimple or complex reasoning. The trends in Fig-\nure 3 validate this assumption. The performance of\nStandard and CoT declines precipitously with the\nescalation of problem complexity. However, this\ntrend is less prominent for LoGic-LM, indicating\nthat the robust reasoning capabilities provided by\nexternal solvers substantially mitigate performance\ndegradation for complex reasoning problems.\n\n4.4 Impact of Self-Refinement\n\nIn Table 3, we find that self-refinement is effective\nin fixing the in-executable symbolic formulations,\nincreasing the Exe_Rate by 5.01 on average. For\nan in-depth analysis, we then evaluate the accu-\nracy and Exe_Rate across different rounds of self-\nrefinement on FOLIO, namely, 0 (no refinement),\n1, 2, and 3 rounds. The results are in Figure 4.\n\nWe find that as the rounds of self-refinement in-\ncrease, the percentage of executable formulations\nconsistently increases, leading to an enhancement\nin the final performance. This suggests that self-\nrefinement serves as an effective tool in aiding the\nLLM to accurately frame the problem. However,\nthe accuracy tends to stagnate in subsequent rounds,\neven though the Exe_Rate continues to increase.\nThis can be attributed to the type of feedback re-\nceived by the self-refiner, which is the error mes-\n\n3813\n", "vlm_text": "The image is a graph showing the accuracy of different models with increasing reasoning depth on the Proof Writer dataset. It displays three lines representing different models:\n\n1. A green line, starting at 81.7 and decreasing to 71.1 as reasoning depth increases.\n2. A blue dashed line, starting at 76.3 and decreasing to 51.\n3. A gray dotted line, starting at 57.7 and decreasing to 33.5.\n\nThe x-axis represents the reasoning depth (from 0 to 5), and the y-axis represents the accuracy percentage (from 30 to 90). Each point on the graph marks the accuracy for a specific reasoning depth.\neach dataset ( Exe_Rate ). Generally, LLM demon- strates high proficiency in transcribing problems into symbolic formats, evidenced by its near   $100\\%$  Exe_Rate  on ProntoQA, Proof Writer, and Logi- cal Deduction. However, the high performance on these datasets is somewhat anticipated, given that their problems are mostly synthetically generated, limiting language variability. When it comes to datasets comprising real-world, expertly crafted problems, such as FOLIO and AR-LSAT, GPT- 4’s performance is notably less promising, with Exe_Rate  scores of  $79.9\\%$   and   $32.6\\%$   respectively. This discrepancy underscores the inherent chal- lenges associated with converting real-world prob- lems into their logical equivalents. \nExe_Rate  only reflects the  grammar  correctness of the logical form. We also report the accuracy of the executable samples ( Exe_Acc ) to measure the  semantic  correctness. We find that logical forms generated by GPT-4 generally achieve high Exe_Acc , even for the most challenging AR-LSAT dataset. Such performance accentuates the poten- tial of symbolic solvers in bolstering the model’s logical reasoning prowess, contingent on the pre- cise translation of problems into symbolic forms. \n4.3 Robustness of Reasoning \nIncorporating symbolic solvers also leads to more robust  reasoning. To illustrate this, we report the performance of L OGIC -LM and baselines for questions of varying complexity levels. We ran- domly selected 300 examples from each subset of Proof Writer, ensuring a balanced label distri- bution. The problems in these subsets require 0,  $<=$  ,  ${<=}2$  ,  $<=3$  , and   ${<=}5$   hops of reasoning, respec- tively. The results, shown in Figure  3 , indicate that L OGIC -LM becomes increasingly effective as the required reasoning depth increases. For exam- \nThe image is a line graph with accuracy on the y-axis and rounds on the x-axis. It compares the performance of CoT (Chain of Thought) and Logic-LM models for GPT-3.5 and GPT-4 over different rounds of self-refinement.\n\n- Orange lines represent GPT-3.5: \n  - CoT (dotted) remains below 60%.\n  - Logic-LM (solid) starts at 61.27% and increases slightly over rounds.\n\n- Green lines represent GPT-4: \n  - CoT (dashed) maintains consistent accuracy around 70.58%.\n  - Logic-LM (solid) starts at 78.92% and stays above 78%, peaking at 79.9%.\n\nThe table below shows executable rates for different rounds:\n- GPT-3.5 improves from 66.7% to 84.3%.\n- GPT-4 improves from 79.9% to 85.8%.\nple, L OGIC -LM outperforms CoT by   $7.1\\%$  ,   $5.0\\%$  ,  $12.7\\%$  ,   $20.0\\%$  , and   $39.4\\%$   on depth-0, depth-1, depth-2, depth-4, and depth-5 problems, respec- tively. In L OGIC -LM, multi-step logical reasoning is delegated to external symbolic solvers, thereby transitioning the challenge of LLM from  problem- solving  to  problem representation . Ideally, the com- plexity of formally representing a problem state- ment in logical form should remain relatively con- stant, regardless of whether the questions require simple or complex reasoning. The trends in Fig- ure  3  validate this assumption. The performance of Standard  and    $C o T$   declines precipitously with the escalation of problem complexity. However, this trend is less prominent for L OGIC -LM, indicating that the robust reasoning capabilities provided by external solvers substantially mitigate performance degradation for complex reasoning problems. \n4.4 Impact of Self-Refinement \nIn Table  3 , we find that self-refinement is effective in fixing the in-executable symbolic formulations, increasing the  Exe_Rate  by 5.01 on average. For an in-depth analysis, we then evaluate the accu- racy and  Exe_Rate  across different rounds of self- refinement on FOLIO, namely, 0 (no refinement), 1, 2, and 3 rounds. The results are in Figure  4 . \nWe find that as the rounds of self-refinement in- crease, the percentage of executable formulations consistently increases, leading to an enhancement in the final performance. This suggests that self- refinement serves as an effective tool in aiding the LLM to accurately frame the problem. However, the accuracy tends to stagnate in subsequent rounds, even though the  Exe_Rate  continues to increase. This can be attributed to the type of feedback re- ceived by the self-refiner, which is the error mes- "}
{"page": 8, "image_path": "doc_images/2023.findings-emnlp.248_8.jpg", "ocr_text": "(A) True\nGenerated symbolic Formulations:\nPredicates:\nNetflixShow(x) # x is a Netflix show.\nPopular(x) # x is popular\nBingeWatch(x, y) # x binge-watches y.\nDownload(x, y) # x downloads y\nShare(x, y, Z) # x shares y to z.\nPremises:\n\n(B) False\n\nConclusion:\nPopular(blackMirror) # \"Black Mirror\" is popular.\nPredicted answer: B\n\nProblem: \"Stranger Things\" is a popular Netflix show. If a Netflix show is popular, Karen will binge-watch it.\nIf and only if Karen binge-watches a Netflix show, she will download it. Karen does not download \"Black Mirror\".\n“Black Mirror\" is a Netflix show. If Karen binge-watches a Netflix show, she will share it to Lisa.\n\nQuestion: Is the following statement true, false, or uncertain? \"Black Mirror\" is popular.\n\n(C) Uncertain\n\nNetflixShow(strangerThings) A Popular(strangerThings) # \"Stranger Things\" is a popular Netflix show\n\nvx (NetflixShow(x) A Popular(x) > BingeWatch(karen, x)) # If a Netflix show is popular, Karen will binge-watch it.\nvx (NetflixShow(x) A BingeWatch(karen, x) © Download(karen, x)) # If and only if Karen ..\nNetflixShow(blackMirror) A -Download(karen, blackMirror) # Karen does not ..\nvx (NetflixShow(x) A BingeWatch(karen, x) » Share(karen, x, lisa)) # If Karen ..\n\n» she will download it.\n“Black Mirror\" is a Netflix show.\n» she will share it to Lisa.\n\nFigure 5: An example of the generated symbolic representation and the predicted answer by LOGIC-LM.\n\nsage from the symbolic solver. This feedback aids\nin converting “invalid” symbolic representations\ninto valid ones. However, a valid symbolic repre-\nsentation does not necessarily equate to a “correct”\nproblem formulation that accurately represents the\nproblem. This issue could be tackled by enhanc-\ning the self-refiner to incorporate feedback beyond\nthe error message, e.g., a reward signal from an\nadditional module evaluating the accuracy of a gen-\nerated symbolic form. We leave this as a promising\ndirection for future exploration.\n\n4.5 Case Study\n\nIn Figure 5, we show an example of the symbolic\nrepresentations generated by GPT-4, together with\nthe predicted answer. In general, LOGIC-LM has\ndemonstrated a potent capacity to interpret com-\nplex problems into symbolic forms. Nonetheless,\nthere remain certain difficulties in accurately un-\nderstanding the semantics of the problem.\n\nWe further analyze some error cases in Fig-\nure 6 of Appendix E. Example 1 shows a case\nwhere GPT-4 generates an incorrect FOL represen-\ntation, stemming from its inability to define ap-\npropriate predicates. Here, instead of creating the\npredicate EasternWildTurkey, the model gener-\nates a constant, WildTurkey(eastern), in which\nWildTurkey is the predicate and eastern is the\nconstant. While this representation is valid in iso-\nlation, it does not interact well with subsequent\nconstants. This inconsistency is a recurring issue\nin GPT-4’s symbolic form generation, illustrating\nthat the model sometimes struggles to maintain an\noverarching understanding of the problem when\nforming logical symbols. Example 3 highlights a\ncase where GPT-4 struggles to interpret specific\n\nexpressions accurately. In this case, the model fails\nto distinguish between the meanings of “below”\nand “above”, resulting in an incorrect constraint\nDan > Eve. Example 4 exemplifies GPT-4’s chal-\nlenge with fully grasping the rules of FOL gram-\nmar, evidenced by the invalid generated formula:\nRating(subway, y) A y > 9. These error cases\nunderscore that transforming problems into logi-\ncal forms remains a challenging task for modern\nLLMs, due to the intricacies of FOL formulation,\nthe innate flexibility of natural language, and the\ncomplexity of global problem comprehension.\n\n5 Conclusion and Future Work\n\nIn this work, we propose a novel approach to ad-\ndress logical reasoning problems by combining\nlarge language models with symbolic solvers. We\nintroduce Logic-LM, one instantiation of such a\nframework, and demonstrate how it significantly\nimproves performance over pure LLMs and chain-\nof-thought prompting techniques.\n\nWhile Logic-LM has proven to be a capable sys-\ntem, it can be further improved with extension to\nmore flexible and powerful logic systems. For ex-\nample, statistical relational learning (SRL) systems\nsuch as Markov logic networks (Richardson and\nDomingos, 2006) and probabilistic soft logic (Bach\net al., 2017) have demonstrated great promise in\nreasoning under uncertainty and integration with\nour framework would enable even more adaptive\nproblem-solving capabilities. Additionally, our\nmethod can be extended to reasoning problems\nrequiring commonsense, which remains a signifi-\ncant challenge as they often require reasoning over\ncomplex and ambiguous rules.\n\n3814\n", "vlm_text": "The image shows a logic problem involving two Netflix shows, \"Stranger Things\" and \"Black Mirror,\" and a person named Karen. The problem is posed in a textual format at the top, detailing conditions regarding Karen's behavior related to these shows based on their popularity and her actions of binge-watching or downloading them.\n\nThe question asks whether the statement \"Black Mirror is popular\" is true, false, or uncertain based on the provided conditions, with options (A) True, (B) False, and (C) Uncertain.\n\nBelow the problem, the image provides a symbolic representation of the predicates and premises related to the problem:\n- Predicates define different properties (e.g., if a show is a Netflix show, if it is popular, if Karen binge-watches it, etc.).\n- Premises list logical statements derived from the problem statement, expressed in symbolic logic.\n\nThe conclusion of this symbolic logic formulation checks if \"Black Mirror\" is popular.\n\nThe predicted answer given at the bottom is (B) False, indicating that according to the logic under the constraints provided, it is concluded that \"Black Mirror\" is not popular.\nsage from the symbolic solver. This feedback aids in converting “invalid” symbolic representations into valid ones. However, a valid symbolic repre- sentation does not necessarily equate to a “correct” problem formulation that accurately represents the problem. This issue could be tackled by enhanc- ing the self-refiner to incorporate feedback beyond the error message,  e.g. , a reward signal from an additional module evaluating the accuracy of a gen- erated symbolic form. We leave this as a promising direction for future exploration. \n4.5 Case Study \nIn Figure  5 , we show an example of the symbolic representations generated by GPT-4, together with the predicted answer. In general, L OGIC -LM has demonstrated a potent capacity to interpret com- plex problems into symbolic forms. Nonetheless, there remain certain difficulties in accurately un- der standing the semantics of the problem. \nWe further analyze some error cases in Fig- ure  6  of Appendix  E . Example 1 shows a case where GPT-4 generates an incorrect FOL represen- tation, stemming from its inability to define ap- propriate predicates. Here, instead of creating the predicate  Eastern Wild Turkey , the model gener- ates a constant,  WildTurkey(eastern) , in which WildTurkey  is the predicate and  eastern  is the constant. While this representation is valid in iso- lation, it does not interact well with subsequent constants. This inconsistency is a recurring issue in GPT-4’s symbolic form generation, illustrating that the model sometimes struggles to maintain an over arching understanding of the problem when forming logical symbols. Example 3 highlights a case where GPT-4 struggles to interpret specific expressions accurately. In this case, the model fails to distinguish between the meanings of “below” and “above”, resulting in an incorrect constraint Dan  $>$   Eve . Example 4 exemplifies GPT-4’s chal- lenge with fully grasping the rules of FOL gram- mar, evidenced by the invalid generated formula: Rating(subway, y)    $\\wedge\\;{\\sf y}\\;>\\;9$  . These error cases underscore that transforming problems into logi- cal forms remains a challenging task for modern LLMs, due to the intricacies of FOL formulation, the innate flexibility of natural language, and the complexity of global problem comprehension. \n\n5 Conclusion and Future Work \nIn this work, we propose a novel approach to ad- dress logical reasoning problems by combining large language models with symbolic solvers. We introduce Logic-LM, one instantiation of such a framework, and demonstrate how it significantly improves performance over pure LLMs and chain- of-thought prompting techniques. \nWhile Logic-LM has proven to be a capable sys- tem, it can be further improved with extension to more flexible and powerful logic systems. For ex- ample, statistical relational learning (SRL) systems such as Markov logic networks ( Richardson and Domingos ,  2006 ) and probabilistic soft logic ( Bach et al. ,  2017 ) have demonstrated great promise in reasoning under uncertainty and integration with our framework would enable even more adaptive problem-solving capabilities. Additionally, our method can be extended to reasoning problems requiring commonsense, which remains a signifi- cant challenge as they often require reasoning over complex and ambiguous rules. "}
{"page": 9, "image_path": "doc_images/2023.findings-emnlp.248_9.jpg", "ocr_text": "Limitations\n\nWe identify two main limitations of LoGic-LM.\nFirst, LOGIC-LM relies on translating reasoning\nproblems into logical formats that can be tackled by\nsymbolic solvers. As a consequence, the model’s\napplicability is inherently bounded by the expres-\nsiveness of the symbolic solver, for example, not all\nproblems can be easily encoded in first-order logic.\nNevertheless, this limitation can be mitigated by\nintegrating a more diverse set of symbolic solvers.\nThe flexible design of LOGIC-LM facilitates this\nintegration. The wide range of reasoning tasks that\nwe can instantiate our LOGIC-LM framework on\nshows its general applicability.\n\nSecond, LoGcic-LM depends on in-context\nlearning coupled with self-refinement to convert\na natural language (NL) problem into the symbolic\nrepresentation. While this method has proven to\nbe effective, it may face difficulties when dealing\nwith logical representations with intricate grammar\nstructures, such as probabilistic soft logic. This\narises from the difficulty in conveying complex\ngrammatical rules to the language model through\na limited number of demonstrations within a con-\nstrained context size. As a potential solution, future\nworks could explore the development of specialized\nmodules to enhance the mapping between NL and\nsymbolic language, e.g., fine-tuning LLMs with\nsynthetic data generated via symbolic solvers.\n\nEthics Statement\n\nThe use of large language models requires a signifi-\ncant amount of energy for computation for training,\nwhich contributes to global warming (Strubell et al.,\n2019). Our work performs few-shot in-context\nlearning instead of training models from scratch, so\nthe energy footprint of our work is less. The large\nlanguage models whose API we use for inference,\nespecially GPT-4, consume significant energy.\n\nAcknowledgements\n\nThis work was supported by the National Science\nFoundation Award #2048122. The views expressed\nare those of the authors and do not reflect the offi-\ncial policy or position of the US government.\n\nReferences\n\nStephen Bach, Matthias Broecheler, Bert Huang, and\nLise Getoor. 2017. Hinge-loss markov random fields\n\nand probabilistic soft logic. Journal of Machine\nLearning Research (JMLR), 18(1):1-67.\n\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-Voss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In\nProceedings of the Annual Conference on Neural\nInformation Processing Systems (NeurIPS).\n\nLe-Wen Cai, Wang-Zhou Dai, Yu-Xuan Huang, Yu-\nFeng Li, Stephen H. Muggleton, and Yuan Jiang.\n2021. Abductive learning with ground knowledge\nbase. In Proceedings of the 30th International Joint\nConference on Artificial Intelligence (IJCAI), pages\n1815-1821.\n\nWenhu Chen, Xueguang Ma, Xinyi Wang, and\nWilliam W. Cohen. 2022. Program of thoughts\nprompting: Disentangling computation from rea-\nsoning for numerical reasoning tasks. CoRR,\nabs/2211.12588.\n\nXinyun Chen, Maxwell Lin, Nathanael Scharli, and\nDenny Zhou. 2023. Teaching large language models\nto self-debug. CoRR, abs/2304.05128.\n\nPeter Clark, Oyvind Tafjord, and Kyle Richardson. 2020.\nTransformers as soft reasoners over language. In Pro-\nceedings of the 29th International Joint Conference\non Artificial Intelligence (IJCAI), pages 3882-3890.\n\nWilliam F Clocksin and Christopher S Mellish. 2003.\nProgramming in PROLOG. Springer Science & Busi-\nness Media.\n\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\nJacob Hilton, Reiichiro Nakano, Christopher Hesse,\nand John Schulman. 2021. Training verifiers to solve\nmath word problems. CoRR, abs/2110.14168.\n\nLeonardo Mendonga de Moura and Nikolaj S. Bjgrner.\n2008. Z3: an efficient SMT solver. In Proceedings of\nthe 14th International Conference of Tools and Algo-\nrithms for the Construction and Analysis of Systems\n(TACAS), volume 4963 of Lecture Notes in Computer\nScience, pages 337-340.\n\nLeonardo Mendonga de Moura, Soonho Kong, Jeremy\nAvigad, Floris van Doorn, and Jakob von Raumer.\n2015. The lean theorem prover (system description).\nIn Proceedings of the 25th International Conference\non Automated Deduction (ICAD), volume 9195 of\nLecture Notes in Computer Science, pages 378-388.\n\nIddo Drori, Sarah Zhang, Reece Shuttleworth, Leonard\nTang, Albert Lu, Elizabeth Ke, Kevin Liu, Linda\nChen, Sunny Tran, Newman Cheng, et al. 2022. A\n\n3815\n", "vlm_text": "Limitations \nWe identify two main limitations of L OGIC -LM. First, L OGIC -LM relies on translating reasoning problems into logical formats that can be tackled by symbolic solvers. As a consequence, the model’s applicability is inherently bounded by the expres- siveness of the symbolic solver, for example, not all problems can be easily encoded in first-order logic. Nevertheless, this limitation can be mitigated by integrating a more diverse set of symbolic solvers. The flexible design of L OGIC -LM facilitates this integration. The wide range of reasoning tasks that we can instantiate our L OGIC -LM framework on shows its general applicability. \nSecond, L OGIC -LM depends on in-context learning coupled with self-refinement to convert a natural language (NL) problem into the symbolic representation. While this method has proven to be effective, it may face difficulties when dealing with logical representations with intricate grammar structures, such as probabilistic soft logic. This arises from the difficulty in conveying complex grammatical rules to the language model through a limited number of demonstrations within a con- strained context size. As a potential solution, future works could explore the development of specialized modules to enhance the mapping between NL and symbolic language,  e.g. , fine-tuning LLMs with synthetic data generated via symbolic solvers. \nEthics Statement \nThe use of large language models requires a signifi- cant amount of energy for computation for training, which contributes to global warming ( Strubell et al. , 2019 ). Our work performs few-shot in-context learning instead of training models from scratch, so the energy footprint of our work is less. The large language models whose API we use for inference, especially GPT-4, consume significant energy. \nAcknowledgements \nThis work was supported by the National Science Foundation Award #2048122. The views expressed are those of the authors and do not reflect the offi- cial policy or position of the US government. \nReferences \nStephen Bach, Matthias Broecheler, Bert Huang, and Lise Getoor. 2017.  Hinge-loss markov random fields \nand probabilistic soft logic . Journal of Machine Learning Research (JMLR) , 18(1):1–67. \nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neel a kant an, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020.  Language models are few-shot learners . In Proceedings of the Annual Conference on Neural Information Processing Systems (NeurIPS) . \nLe-Wen Cai, Wang-Zhou Dai, Yu-Xuan Huang, Yu- Feng Li, Stephen H. Muggleton, and Yuan Jiang. 2021.  Abductive learning with ground knowledge base . In  Proceedings of the 30th International Joint Conference on Artificial Intelligence (IJCAI) , pages 1815–1821. \nWenhu Chen, Xueguang Ma, Xinyi Wang, and William W. Cohen. 2022. Program of thoughts prompting: Disentangling computation from rea- soning for numerical reasoning tasks . CoRR , abs/2211.12588. \nXinyun Chen, Maxwell Lin, Nathanael Schärli, and Denny Zhou. 2023.  Teaching large language models to self-debug .  CoRR , abs/2304.05128. \nPeter Clark, Oyvind Tafjord, and Kyle Richardson. 2020. Transformers as soft reasoners over language . In  Pro- ceedings of the 29th International Joint Conference on Artificial Intelligence (IJCAI) , pages 3882–3890. \nWilliam F Clocksin and Christopher S Mellish. 2003. Programming in PROLOG . Springer Science & Busi- ness Media. \nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021.  Training verifiers to solve math word problems .  CoRR , abs/2110.14168. \nLeonardo Mendonça de Moura and Nikolaj S. Bjørner. 2008.  Z3: an efficient SMT solver . In  Proceedings of the 14th International Conference of Tools and Algo- rithms for the Construction and Analysis of Systems (TACAS) , volume 4963 of  Lecture Notes in Computer Science , pages 337–340. \nLeonardo Mendonça de Moura, Soonho Kong, Jeremy Avigad, Floris van Doorn, and Jakob von Raumer. 2015.  The lean theorem prover (system description) . In  Proceedings of the 25th International Conference on Automated Deduction (ICAD) , volume 9195 of Lecture Notes in Computer Science , pages 378–388. \nIddo Drori, Sarah Zhang, Reece Shuttleworth, Leonard Tang, Albert Lu, Elizabeth Ke, Kevin Liu, Linda Chen, Sunny Tran, Newman Cheng, et al. 2022. A "}
{"page": 10, "image_path": "doc_images/2023.findings-emnlp.248_10.jpg", "ocr_text": "neural network solves, explains, and generates uni-\nversity math problems by program synthesis and few-\nshot learning at human level. Proceedings of the Na-\ntional Academy of Sciences, 119(32):e2123433119.\n\nHerbert B Enderton. 2001. A mathematical introduction\nto logic. Elsevier.\n\nBruce Frederiksen. 2008. Applying expert system tech-\nnology to code reuse with pyke. PyCon: Chicago.\n\nLuyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon,\nPengfei Liu, Yiming Yang, Jamie Callan, and Gra-\nham Neubig. 2023. PAL: program-aided language\nmodels. In Proceedings of the International Con-\nference on Machine Learning (ICML), volume 202,\npages 10764-10799.\n\nOlga Golovneva, Moya Chen, Spencer Poff, Martin\nCorredor, Luke Zettlemoyer, Maryam Fazel-Zarandi,\nand Asli Celikyilmaz. 2023. ROSCOE: A suite of\nmetrics for scoring step-by-step reasoning. In Pro-\nceedings of the IIth International Conference on\nLearning Representations (ICLR).\n\nNitish Gupta, Kevin Lin, Dan Roth, Sameer Singh, and\nMatt Gardner. 2020. Neural module networks for\nreasoning over text. In Proceedings of the 8th In-\nternational Conference on Learning Representations\n(ICLR).\n\nSimeng Han, Hailey Schoelkopf, Yilun Zhao, Zhenting\nQi, Martin Riddell, Luke Benson, Lucy Sun, Eka-\nterina Zubova, Yujie Qiao, Matthew Burtell, David\nPeng, Jonathan Fan, Yixin Liu, Brian Wong, Mal-\ncolm Sailor, Ansong Ni, Linyong Nan, Jungo Kasai,\nTao Yu, Rui Zhang, Shafiq R. Joty, Alexander R. Fab-\nbri, Wojciech Kryscinski, Xi Victoria Lin, Caiming\nXiong, and Dragomir Radev. 2022. FOLIO: natu-\nral language reasoning with first-order logic. CoRR,\nabs/2209.00840.\n\nJoy He-Yueya, Gabriel Poesia, Rose E Wang, and\nNoah D Goodman. 2023. Solving math word prob-\nlems by combining language models with symbolic\nsolvers. CoRR, abs/2304.09102.\n\nJie Huang and Kevin Chen-Chuan Chang. 2023. To-\nwards reasoning in large language models: A survey.\nIn Findings of the 61st Annual Meeting of the Asso-\nciation for Computational Linguistics (ACL), pages\n1049-1065.\n\nAlbert Qiaochu Jiang, Sean Welleck, Jin Peng Zhou,\nTimothée Lacroix, Jiacheng Liu, Wenda Li, Mateja\nJamnik, Guillaume Lample, and Yuhuai Wu. 2023.\nDraft, sketch, and prove: Guiding formal theorem\nprovers with informal proofs. In Proceedings of the\n11th International Conference on Learning Represen-\ntations (ICLR).\n\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2022. Large lan-\nguage models are zero-shot reasoners. In Proceed-\nings of the Annual Conference on Neural Information\nProcessing Systems (NeurIPS).\n\nPhilipp K6rner, Michael Leuschel, Joao Barbosa,\nVitor Santos Costa, Verédnica Dahl, Manuel V.\nHermenegildo, José F. Morales, Jan Wielemaker,\nDaniel Diaz, and Salvador Abreu. 2022. Fifty years\nof prolog and beyond. Theory Pract. Log. Program.,\n22(6):776-858.\n\nVipin Kumar. 1992. Algorithms for constraint-\nsatisfaction problems: A survey. AJ Mag., 13(1):32-\n44.\n\nAngeliki Lazaridou, Elena Gribovskaya, Wojciech\nStokowiec, and Nikolai Grigorev. 2022. Internet-\naugmented language models through few-shot\nprompting for open-domain question answering.\nCoRR, abs/2203.05115.\n\nPercy Liang, Rishi Bommasani, Tony Lee, Dimitris\nTsipras, Dilara Soylu, Michihiro Yasunaga, Yian\nZhang, Deepak Narayanan, Yuhuai Wu, Ananya Ku-\nmar, Benjamin Newman, Binhang Yuan, Bobby Yan,\nCe Zhang, Christian Cosgrove, Christopher D. Man-\nning, Christopher Ré, Diana Acosta-Navas, Drew A.\nHudson, Eric Zelikman, Esin Durmus, Faisal Ladhak,\nFrieda Rong, Hongyu Ren, Huaxiu Yao, Jue Wang,\nKeshav Santhanam, Laurel J. Orr, Lucia Zheng, Mert\nYiiksekg6niil, Mirac Suzgun, Nathan Kim, Neel\nGuha, Niladri S. Chatterji, Omar Khattab, Peter\nHenderson, Qian Huang, Ryan Chi, Sang Michael\nXie, Shibani Santurkar, Surya Ganguli, Tatsunori\nHashimoto, Thomas Icard, Tianyi Zhang, Vishrav\nChaudhary, William Wang, Xuechen Li, Yifan Mai,\nYuhui Zhang, and Yuta Koreeda. 2022. Holistic eval-\nuation of language models. CoRR, abs/2211.09110.\n\nBo Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu, Shiqi\nZhang, Joydeep Biswas, and Peter Stone. 2023a.\nLLM+P: empowering large language models with op-\ntimal planning proficiency. CoRR, abs/2304.11477.\n\nHanmeng Liu, Ruoxi Ning, Zhiyang Teng, Jian Liu, Qiji\nZhou, and Yue Zhang. 2023b. Evaluating the logi-\ncal reasoning ability of chatgpt and GPT-4. CoRR,\nabs/2304.03439.\n\nQing Lyu, Shreya Havaldar, Adam Stein, Li Zhang,\nDelip Rao, Eric Wong, Marianna Apidianaki, and\nChris Callison-Burch. 2023. Faithful chain-of-\nthought reasoning. CoRR, abs/2301.13379.\n\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler\nHallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon,\nNouha Dziri, Shrimai Prabhumoye, Yiming Yang,\nSean Welleck, Bodhisattwa Prasad Majumder,\nShashank Gupta, Amir Yazdanbakhsh, and Peter\nClark. 2023. Self-refine: Iterative refinement with\nself-feedback. CoRR, abs/2303.17651.\n\nRobin Manhaeve, Sebastijan Dumancic, Angelika Kim-\nmig, Thomas Demeester, and Luc De Raedt. 2021.\nNeural probabilistic logic programming in deep-\nproblog. The Journal of Artificial Intelligence (AIJ),\n298:103504.\n\n3816\n", "vlm_text": "neural network solves, explains, and generates uni- versity math problems by program synthesis and few- shot learning at human level.  Proceedings of the Na- tional Academy of Sciences , 119(32):e2123433119. \nHerbert B Enderton. 2001.  A mathematical introduction to logic . Elsevier. \nBruce Frederiksen. 2008.  Applying expert system tech- nology to code reuse with pyke .  PyCon: Chicago . \nLuyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Gra- ham Neubig. 2023.  PAL: program-aided language models . In  Proceedings of the International Con- ference on Machine Learning (ICML) , volume 202, pages 10764–10799. \nOlga Golovneva, Moya Chen, Spencer Poff, Martin Corredor, Luke Z ett le moyer, Maryam Fazel-Zarandi, and Asli Cel i kyi l maz. 2023.  ROSCOE: A suite of metrics for scoring step-by-step reasoning . In  Pro- ceedings of the 11th International Conference on Learning Representations (ICLR) . \nNitish Gupta, Kevin Lin, Dan Roth, Sameer Singh, and Matt Gardner. 2020.  Neural module networks for reasoning over text . In  Proceedings of the 8th In- ter national Conference on Learning Representations (ICLR) . \nSimeng Han, Hailey Schoelkopf, Yilun Zhao, Zhenting Qi, Martin Riddell, Luke Benson, Lucy Sun, Eka- terina Zubova, Yujie Qiao, Matthew Burtell, David Peng, Jonathan Fan, Yixin Liu, Brian Wong, Mal- colm Sailor, Ansong Ni, Linyong Nan, Jungo Kasai, Tao Yu, Rui Zhang, Shafiq R. Joty, Alexander R. Fab- bri, Wojciech Kryscinski, Xi Victoria Lin, Caiming Xiong, and Dragomir Radev. 2022.  FOLIO: natu- ral language reasoning with first-order logic .  CoRR , abs/2209.00840. \nJoy He-Yueya, Gabriel Poesia, Rose E Wang, and Noah D Goodman. 2023.  Solving math word prob- lems by combining language models with symbolic solvers .  CoRR , abs/2304.09102. \nJie Huang and Kevin Chen-Chuan Chang. 2023.  To- wards reasoning in large language models: A survey . In  Findings of the 61st Annual Meeting of the Asso- ciation for Computational Linguistics (ACL) , pages 1049–1065. \nAlbert Qiaochu Jiang, Sean Welleck, Jin Peng Zhou, Timothée Lacroix, Jiacheng Liu, Wenda Li, Mateja Jamnik, Guillaume Lample, and Yuhuai Wu. 2023. Draft, sketch, and prove: Guiding formal theorem provers with informal proofs . In  Proceedings of the 11th International Conference on Learning Represen- tations (ICLR) . \nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu- taka Matsuo, and Yusuke Iwasawa. 2022.  Large lan- guage models are zero-shot reasoners . In  Proceed- ings of the Annual Conference on Neural Information Processing Systems (NeurIPS) . \nPhilipp Körner, Michael Leuschel, João Barbosa, Vítor Santos Costa, Verónica Dahl, Manuel V. Her men egil do, José F. Morales, Jan Wielemaker, Daniel Diaz, and Salvador Abreu. 2022.  Fifty years of prolog and beyond .  Theory Pract. Log. Program. , 22(6):776–858.\nVipin Kumar. 1992. Algorithms for constraint- satisfaction problems: A survey. AI Mag., 13(1):32–44. \nAngeliki Lazaridou, Elena Gri bo vs kaya, Wojciech Stokowiec, and Nikolai Grigorev. 2022.  Internet- augmented language models through few-shot prompting for open-domain question answering . CoRR , abs/2203.05115. \nPercy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Ku- mar, Benjamin Newman, Binhang Yuan, Bobby Yan, Ce Zhang, Christian Cosgrove, Christopher D. Man- ning, Christopher Ré, Diana Acosta-Navas, Drew A. Hudson, Eric Zelikman, Esin Durmus, Faisal Ladhak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue Wang, Keshav Santhanam, Laurel J. Orr, Lucia Zheng, Mert Y ks ekg n l, Mirac Suzgun, Nathan Kim, Neel Guha, Niladri S. Chatterji, Omar Khattab, Peter Henderson, Qian Huang, Ryan Chi, Sang Michael Xie, Shibani Santurkar, Surya Ganguli, Tatsunori Hashimoto, Thomas Icard, Tianyi Zhang, Vishrav Chaudhary, William Wang, Xuechen Li, Yifan Mai, Yuhui Zhang, and Yuta Koreeda. 2022.  Holistic eval- uation of language models .  CoRR , abs/2211.09110. \nBo Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu, Shiqi Zhang, Joydeep Biswas, and Peter Stone. 2023a. LLM  $+\\mathrm{P}$  : empowering large language models with op- timal planning proficiency .  CoRR , abs/2304.11477. \nHanmeng Liu, Ruoxi Ning, Zhiyang Teng, Jian Liu, Qiji Zhou, and Yue Zhang. 2023b.  Evaluating the logi- cal reasoning ability of chatgpt and GPT-4 .  CoRR , abs/2304.03439. \nQing Lyu, Shreya Havaldar, Adam Stein, Li Zhang, Delip Rao, Eric Wong, Marianna Apidianaki, and Chris Callison-Burch. 2023. Faithful chain-of- thought reasoning .  CoRR , abs/2301.13379. \nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Sean Welleck, Bodhi s at twa Prasad Majumder, Shashank Gupta, Amir Yaz dan bakhsh, and Peter Clark. 2023.  Self-refine: Iterative refinement with self-feedback .  CoRR , abs/2303.17651. \nRobin Manhaeve, Sebastijan Dumancic, Angelika Kim- mig, Thomas Demeester, and Luc De Raedt. 2021. Neural probabilistic logic programming in deep- problog .  The Journal of Artificial Intelligence (AIJ) , 298:103504. "}
{"page": 11, "image_path": "doc_images/2023.findings-emnlp.248_11.jpg", "ocr_text": "Jiayuan Mao, Chuang Gan, Pushmeet Kohli, Joshua B.\nTenenbaum, and Jiajun Wu. 2019. The neuro-\nsymbolic concept learner: Interpreting scenes, words,\nand sentences from natural supervision. In Proceed-\nings of the 7th International Conference on Learning\nRepresentations (ICLR).\n\nKostas S. Metaxiotis, Dimitris Askounis, and John E.\nPsarras. 2002. Expert systems in production planning\nand scheduling: A state-of-the-art survey. Journal of\nIntelligent Manufacturing, 13(4):253-260.\n\nAaron Meurer, Christopher P. Smith, Mateusz Pa-\nprocki, Ondrej Certik, Sergey B. Kirpichev, Matthew\nRocklin, Amit Kumar, Sergiu Ivanov, Jason Keith\nMoore, Sartaj Singh, Thilina Rathnayake, Sean Vig,\nBrian E. Granger, Richard P. Muller, Francesco\nBonazzi, Harsh Gupta, Shivam Vats, Fredrik Johans-\nson, Fabian Pedregosa, Matthew J. Curry, Andy R.\nTerrel, Stepan Roucka, Ashutosh Saboo, Isuru Fer-\nnando, Sumith Kulal, Robert Cimrman, and An-\nthony M. Scopatz. 2017. Sympy: symbolic com-\nputing in python. PeerJ Computer Science, 3:e103.\n\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,\nLong Ouyang, Christina Kim, Christopher Hesse,\nShantanu Jain, Vineet Kosaraju, William Saunders,\nXu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen\nKrueger, Kevin Button, Matthew Knight, Benjamin\nChess, and John Schulman. 2021. Webgpt: Browser-\nassisted question-answering with human feedback.\nCoRR, abs/2112.09332.\n\nOpenAI. 2023.\nabs/2303.08774.\n\nGPT-4 technical report. CoRR,\n\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll L. Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray,\nJohn Schulman, Jacob Hilton, Fraser Kelton, Luke\nMiller, Maddie Simens, Amanda Askell, Peter Welin-\nder, Paul F. Christiano, Jan Leike, and Ryan Lowe.\n2022a. Training language models to follow instruc-\ntions with human feedback. In Proceedings of the\nAnnual Conference on Neural Information Process-\ning Systems (NeurIPS).\n\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll L. Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray,\nJohn Schulman, Jacob Hilton, Fraser Kelton, Luke\nMiller, Maddie Simens, Amanda Askell, Peter Welin-\nder, Paul F. Christiano, Jan Leike, and Ryan Lowe.\n2022b. Training language models to follow instruc-\ntions with human feedback. In Proceedings of the\nAnnual Conference on Neural Information Process-\ning Systems (NeurIPS.\n\nLawrence C. Paulson. 1994. Isabelle - A Generic The-\norem Prover (with a contribution by T. Nipkow),\nvolume 828 of Lecture Notes in Computer Science.\nSpringer.\n\nDavid Poole and Alan K. Mackworth. 2010. Artificial\nIntelligence - Foundations of Computational Agents.\nCambridge University Press.\n\nConnor Pryor, Charles Dickens, Eriq Augustine, Alon\nAlbalak, William Yang Wang, and Lise Getoor. 2023.\nNeups!: Neural probabilistic soft logic. In Proceed-\nings of the 32nd International Joint Conference on\nArtificial Intelligence (IJCAI), pages 4145-4153.\n\nDanilo Neves Ribeiro, Shen Wang, Xiaofei Ma,\nHenghui Zhu, Rui Dong, Deguang Kong, Juli-\nette Burger, Anjelica Ramos, Zhiheng Huang,\nWilliam Yang Wang, George Karypis, Bing Xiang,\nand Dan Roth. 2023a. STREET: A multi-task struc-\ntured reasoning and explanation benchmark. In Pro-\nceedings of the Eleventh International Conference on\nLearning Representations (ICLR).\n\nDanilo Neves Ribeiro, Shen Wang, Xiaofei Ma, Henry\nZhu, Rui Dong, Deguang Kong, Juliette Burger, An-\njelica Ramos, William Yang Wang, Zhiheng Huang,\nGeorge Karypis, Bing Xiang, and Dan Roth. 2023b.\nSTREET: A multi-task structured reasoning and ex-\nplanation benchmark. In Proceedings of the 11th\nInternational Conference on Learning Representa-\ntions (ICLR).\n\nMatthew Richardson and Pedro M. Domingos. 2006.\nMarkov logic networks. Machine Learning, 62(1-\n2):107-136.\n\nJohn Alan Robinson. 1965. A machine-oriented logic\nbased on the resolution principle. The Journal of the\nACM (JACM), 12(1):23-41.\n\nAbulhair Saparov and He He. 2023. Language models\nare greedy reasoners: A systematic formal analysis\nof chain-of-thought. In Proceedings of the 11th In-\nternational Conference on Learning Representations\n(ICLR).\n\nMurray Shanahan. 2022. Talking about large language\nmodels. CoRR, abs/2212.03551.\n\nYongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,\nWeiming Lu, and Yueting Zhuang. 2023. Hugging-\ngpt: Solving AI tasks with chatgpt and its friends in\nhuggingface. CoRR, abs/2303.17580.\n\nWeijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon\nSeo, Rich James, Mike Lewis, Luke Zettlemoyer, and\nWen-tau Yih. 2023. REPLUG: retrieval-augmented\nblack-box language models. CoRR, abs/2301.12652.\n\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao,\nAbu Awal Md Shoeb, Abubakar Abid, Adam\nFisch, Adam R. Brown, Adam Santoro, Aditya\nGupta, Adria Garriga-Alonso, Agnieszka Kluska,\nAitor Lewkowycz, Akshat Agarwal, Alethea Power,\nAlex Ray, Alex Warstadt, Alexander W. Kocurek,\nAli Safaya, Ali Tazarv, Alice Xiang, Alicia Par-\nrish, Allen Nie, Aman Hussain, Amanda Askell,\nAmanda Dsouza, Ameet Rahane, Anantharaman S.\nlyer, Anders Andreassen, Andrea Santilli, Andreas\nStuhlmiiller, Andrew M. Dai, Andrew La, Andrew K.\nLampinen, Andy Zou, Angela Jiang, Angelica Chen,\nAnh Vuong, Animesh Gupta, Anna Gottardi, Anto-\nnio Norelli, Anu Venkatesh, Arash Gholamidavoodi,\n\n3817\n", "vlm_text": "Jiayuan Mao, Chuang Gan, Pushmeet Kohli, Joshua B. Tenenbaum, and Jiajun Wu. 2019. The neuro- symbolic concept learner: Interpreting scenes, words, and sentences from natural supervision . In  Proceed- ings of the 7th International Conference on Learning Representations (ICLR) . \nKostas S. Metaxiotis, Dimitris Askounis, and John E. Psarras. 2002.  Expert systems in production planning and scheduling: A state-of-the-art survey .  Journal of Intelligent Manufacturing , 13(4):253–260. \nAaron Meurer, Christopher P. Smith, Mateusz Pa- procki, Ondrej Certík, Sergey B. Kirpichev, Matthew Rocklin, Amit Kumar, Sergiu Ivanov, Jason Keith Moore, Sartaj Singh, Thilina Rathnayake, Sean Vig, Brian E. Granger, Richard P. Muller, Francesco Bonazzi, Harsh Gupta, Shivam Vats, Fredrik Johans- son, Fabian Pedregosa, Matthew J. Curry, Andy R. Terrel, Stepán Roucka, Ashutosh Saboo, Isuru Fer- nando, Sumith Kulal, Robert Cimrman, and An- thony M. Scopatz. 2017.  Sympy: symbolic com- puting in python .  PeerJ Computer Science , 3:e103. \nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman. 2021.  Webgpt: Browser- assisted question-answering with human feedback . CoRR , abs/2112.09332. \nOpenAI. 2023. GPT-4 technical report . CoRR , abs/2303.08774. \nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welin- der, Paul F. Christiano, Jan Leike, and Ryan Lowe. 2022a.  Training language models to follow instruc- tions with human feedback . In  Proceedings of the Annual Conference on Neural Information Process- ing Systems (NeurIPS) . \nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welin- der, Paul F. Christiano, Jan Leike, and Ryan Lowe. 2022b.  Training language models to follow instruc- tions with human feedback . In  Proceedings of the Annual Conference on Neural Information Process- ing Systems (NeurIPS . \nLawrence C. Paulson. 1994.  Isabelle - A Generic The- orem Prover (with a contribution by T. Nipkow) , volume 828 of  Lecture Notes in Computer Science . Springer. \nDavid Poole and Alan K. Mackworth. 2010.  Artificial Intelligence - Foundations of Computational Agents . Cambridge University Press. \nConnor Pryor, Charles Dickens, Eriq Augustine, Alon Albalak, William Yang Wang, and Lise Getoor. 2023. Neupsl: Neural probabilistic soft logic . In  Proceed- ings of the 32nd International Joint Conference on Artificial Intelligence (IJCAI) , pages 4145–4153. \nDanilo Neves Ribeiro, Shen Wang, Xiaofei Ma, Henghui Zhu, Rui Dong, Deguang Kong, Juli- ette Burger, Anjelica Ramos, Zhiheng Huang, William Yang Wang, George Karypis, Bing Xiang, and Dan Roth. 2023a.  STREET: A multi-task struc- tured reasoning and explanation benchmark . In  Pro- ceedings of the Eleventh International Conference on Learning Representations (ICLR) . \nDanilo Neves Ribeiro, Shen Wang, Xiaofei Ma, Henry Zhu, Rui Dong, Deguang Kong, Juliette Burger, An- jelica Ramos, William Yang Wang, Zhiheng Huang, George Karypis, Bing Xiang, and Dan Roth. 2023b. STREET: A multi-task structured reasoning and ex- planation benchmark . In  Proceedings of the 11th International Conference on Learning Representa- tions (ICLR) . \nMatthew Richardson and Pedro M. Domingos. 2006. Markov logic networks .  Machine Learning , 62(1- 2):107–136. \nJohn Alan Robinson. 1965.  A machine-oriented logic based on the resolution principle .  The Journal of the ACM (JACM) , 12(1):23–41. \nAbulhair Saparov and He He. 2023.  Language models are greedy reasoners: A systematic formal analysis of chain-of-thought . In  Proceedings of the 11th In- ter national Conference on Learning Representations (ICLR) . \nMurray Shanahan. 2022.  Talking about large language models .  CoRR , abs/2212.03551. \nYongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. 2023.  Hugging- gpt: Solving AI tasks with chatgpt and its friends in hugging face .  CoRR , abs/2303.17580. \nWeijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Z ett le moyer, and Wen-tau Yih. 2023.  REPLUG: retrieval-augmented black-box language models .  CoRR , abs/2301.12652. \nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R. Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, Agnieszka Kluska, Aitor Lewkowycz, Akshat Agarwal, Alethea Power, Alex Ray, Alex Warstadt, Alexander W. Kocurek, Ali Safaya, Ali Tazarv, Alice Xiang, Alicia Par- rish, Allen Nie, Aman Hussain, Amanda Askell, Amanda Dsouza, Ameet Rahane, Anantha raman S. Iyer, Anders Andreassen, Andrea Santilli, Andreas Stu hl m ller, Andrew M. Dai, Andrew La, Andrew K. Lampinen, Andy Zou, Angela Jiang, Angelica Chen, Anh Vuong, Animesh Gupta, Anna Gottardi, Anto- nio Norelli, Anu Venkatesh, Arash Ghola mid a vo odi, "}
{"page": 12, "image_path": "doc_images/2023.findings-emnlp.248_12.jpg", "ocr_text": "Arfa Tabassum, Arul Menezes, Arun Kirubarajan,\nAsher Mullokandov, Ashish Sabharwal, Austin Her-\nrick, Avia Efrat, Aykut Erdem, Ayla Karakas, and\net al. 2022. Beyond the imitation game: Quantifying\nand extrapolating the capabilities of language models.\nCoRR, abs/2206.04615.\n\nEmma Strubell, Ananya Ganesh, and Andrew McCal-\nlum. 2019. Energy and policy considerations for\ndeep learning in NLP. In Proceedings of the 57th\nAnnual Meeting of the Association for Computational\nLinguistics (ACL), pages 3645-3650.\n\nOyvind Tafjord, Bhavana Dalvi, and Peter Clark. 2021.\nProofwriter: Generating implications, proofs, and\nabductive statements over natural language. In Find-\nings of the 59th Annual Meeting of the Association for\nComputational Linguistics (ACL), pages 3621-3634.\n\nOyvind Tafjord, Bhavana Dalvi Mishra, and Peter Clark.\n2022. Entailer: Answering questions with faithful\nand truthful chains of reasoning. In Proceedings\nof the 2022 Conference on Empirical Methods in\nNatural Language Processing (EMNLP)), pages 2078-\n2093.\n\nJidong Tian, Yitian Li, Wenqing Chen, Liqiang Xiao,\nHao He, and Yaohui Jin. 2022. Weakly supervised\nneural symbolic learning for cognitive tasks. In Pro-\nceedings of 36th Conference on Artificial Intelligence\n(AAAI), pages 5888-5896.\n\nXingyao Wang, Sha Li, and Heng Ji. 2022. Code4struct:\nCode generation for few-shot structured prediction\nfrom natural language. CoRR, abs/2210.12810.\n\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V.\nLe, Ed H. Chi, Sharan Narang, Aakanksha Chowd-\nhery, and Denny Zhou. 2023. Self-consistency im-\nproves chain of thought reasoning in language mod-\nels. In Proceedings of the 11th International Confer-\nence on Learning Representations (ICLR).\n\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,\nBarret Zoph, Sebastian Borgeaud, Dani Yogatama,\nMaarten Bosma, Denny Zhou, Donald Metzler, Ed H.\nChi, Tatsunori Hashimoto, Oriol Vinyals, Percy\nLiang, Jeff Dean, and William Fedus. 2022a. Emer-\ngent abilities of large language models. Transactions\non Machine Learning Research, 2022.\n\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed H. Chi, Quoc Le, and Denny Zhou. 2022b.\nChain of thought prompting elicits reasoning in large\nlanguage models. CoRR, abs/2201.11903.\n\nYuhuai Wu, Albert Qiaochu Jiang, Wenda Li, Markus N.\nRabe, Charles Staats, Mateja Jamnik, and Christian\nSzegedy. 2022. Autoformalization with large lan-\nguage models. In Proceedings of the Annual Con-\nference on Neural Information Processing Systems\n(NeurIPS).\n\nKaiyu Yang, Jia Deng, and Danqi Chen. 2022. Gen-\nerating natural language proofs with verifier-guided\nsearch. In Proceedings of the 2022 Conference on\n\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 89-105.\n\nXi Ye, Qiaochu Chen, Isil Dillig, and Greg Durrett.\n2023. Satisfiability-aided language models using\ndeclarative prompting. In Proceedings of the An-\nnual Conference on Neural Information Processing\nSystems (NeurIPS).\n\nWanjun Zhong, Siyuan Wang, Duyu Tang, Zenan Xu,\nDaya Guo, Yining Chen, Jiahai Wang, Jian Yin, Ming\nZhou, and Nan Duan. 2022. Analytical reasoning of\ntext. In Findings of the 2022 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies\n(NAACL-HLT), pages 2306-2319.\n\nDenny Zhou, Nathanael Schirli, Le Hou, Jason Wei,\nNathan Scales, Xuezhi Wang, Dale Schuurmans,\nClaire Cui, Olivier Bousquet, Quoc V. Le, and Ed H.\nChi. 2023. Least-to-most prompting enables complex\nreasoning in large language models. In Proceedings\nof the 11th International Conference on Learning\nRepresentations (ICLR).\n\n3818\n", "vlm_text": "Arfa Tabassum, Arul Menezes, Arun Ki rub a rajan, Asher Mull ok and ov, Ashish Sabharwal, Austin Her- rick, Avia Efrat, Aykut Erdem, Ayla Karakas, and et al. 2022.  Beyond the imitation game: Quantifying and extrapolating the capabilities of language models . CoRR , abs/2206.04615. \nEmma Strubell, Ananya Ganesh, and Andrew McCal- lum. 2019.  Energy and policy considerations for deep learning in NLP . In  Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (ACL) , pages 3645–3650. \nOyvind Tafjord, Bhavana Dalvi, and Peter Clark. 2021. Proof writer: Generating implications, proofs, and abductive statements over natural language . In  Find- ings of the 59th Annual Meeting of the Association for Computational Linguistics (ACL) , pages 3621–3634. \nOyvind Tafjord, Bhavana Dalvi Mishra, and Peter Clark. 2022.  Entailer: Answering questions with faithful and truthful chains of reasoning . In  Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pages 2078– 2093. \nEmpirical Methods in Natural Language Processing (EMNLP) , pages 89–105. \nXi Ye, Qiaochu Chen, Isil Dillig, and Greg Durrett. 2023.  Satisfiability-aided language models using declarative prompting . In  Proceedings of the An- nual Conference on Neural Information Processing Systems (NeurIPS) . Wanjun Zhong, Siyuan Wang, Duyu Tang, Zenan Xu, Daya Guo, Yining Chen, Jiahai Wang, Jian Yin, Ming Zhou, and Nan Duan. 2022.  Analytical reasoning of text . In  Findings of the 2022 Conference of the North American Chapter of the Association for Computa- tional Linguistics: Human Language Technologies (NAACL-HLT) , pages 2306–2319. Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc V. Le, and Ed H. Chi. 2023.  Least-to-most prompting enables complex reasoning in large language models . In  Proceedings of the 11th International Conference on Learning Representations (ICLR) . \nJidong Tian, Yitian Li, Wenqing Chen, Liqiang Xiao, Hao He, and Yaohui Jin. 2022.  Weakly supervised neural symbolic learning for cognitive tasks . In  Pro- ceedings of 36th Conference on Artificial Intelligence (AAAI) , pages 5888–5896. \nXingyao Wang, Sha Li, and Heng Ji. 2022.  Code 4 struct: Code generation for few-shot structured prediction from natural language .  CoRR , abs/2210.12810. \nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V. Le, Ed H. Chi, Sharan Narang, Aakanksha Chowd- hery, and Denny Zhou. 2023.  Self-consistency im- proves chain of thought reasoning in language mod- els . In  Proceedings of the 11th International Confer- ence on Learning Representations (ICLR) . \nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. 2022a.  Emer- gent abilities of large language models .  Transactions on Machine Learning Research , 2022. \nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed H. Chi, Quoc Le, and Denny Zhou. 2022b. Chain of thought prompting elicits reasoning in large language models .  CoRR , abs/2201.11903. \nYuhuai Wu, Albert Qiaochu Jiang, Wenda Li, Markus N. Rabe, Charles Staats, Mateja Jamnik, and Christian Szegedy. 2022.  Auto formalization with large lan- guage models . In  Proceedings of the Annual Con- ference on Neural Information Processing Systems (NeurIPS) . \nKaiyu Yang, Jia Deng, and Danqi Chen. 2022.  Gen- erating natural language proofs with verifier-guided search . In  Proceedings of the 2022 Conference on "}
{"page": 13, "image_path": "doc_images/2023.findings-emnlp.248_13.jpg", "ocr_text": "A Syntax for First-order Logic (FOL)\n\nName FOL Notation\nConstant lowercase letters\nVariable L,Y, 00°\nAtom P(ai,+++ ,@n)\nNegation aP\nConjunction an Me AP,\nDisjunction Py. p VP,\nImplication Pi > Pp\nEquivalence Pio Pp\nExistential Quantifier | 4zP(x,---)\nUniversal Quantifier | V2P( -)\n\nTable 4: First-Order Logic Grammar.\n\nB Dataset Statistics\n\nDataset Reasoning | Test Size | #Opts\nPrOntoQA Deductive 500 2\nProofWriter Deductive 600 3\nFOLIO FOL 204 3\nLogicalDeduction CSP 300 3,5,7\nAR-LSAT AR 230 5\nTable 5: Statistics of the logical reasoning datasets.\n\nC_ Prompt Examples\n\nIn this section we provide examples of the prompts\nused for each dataset and method. Prompts for stan-\ndard in-context learning contain 2 demonstrations\nconsisting of 3 parts each: a context, a question,\nand options. Prompts for chain-of-thought prompt-\ning contain 2 demonstrations consisting of 5 parts\neach: a task description, a context, a question, op-\ntions, and a chain of reasoning. Prompts for Logic-\nLM contain 2 demonstrations with 5 parts each: a\ntask description, a context, a question, options, and\na domain-specific symbolic program. For brevity,\nwe show only a single demonstration for each set-\nting in the following sections.\n\nC.1_ PrOntoQA Prompts\n\nStandard In-Context Learning\n\nContext: Jompuses are not shy. Jompuses are yumpuses.\n(--+ more context here ---)\nZumpuses are rompuses. Max is a yumpus.\n\nQuestion: Is the following statement true or false?\nMax is sour.\n\nOptions:\nA) True\nB) False\n\nThe correct option is: B\n\nChain-of-Thought Prompting\n\nTask Description: Given a problem statement as\ncontexts, the task is to answer a logical reasoning\nquestion.\n\nContext: Jompuses are not shy. Jompuses are yumpuses.\n(--+ more context here ---)\nZumpuses are rompuses. Max is a yumpus.\n\nQuestion: Is the following statement true or false?\nMax is sour.\n\nOptions:\nA) True\nB) False\n\nReasoning: Max is a yumpus. Each yumpus is a dumpus.\n(-++ more reasoning here ---)\n\nTumpuses are not sour. So Max is not sour.\n\nThe correct option is: B\n\nLogic-LM\n\nTask Description: You are given a problem description\nand a question. The task is to:\n\n1) define all the predicates in the problem\n\n2) parse the problem into logic rules based on\n\nthe defined predicates\n\n3) write all the facts mentioned in the problem\n\n4) parse the question into the logic form\n\nContext: Each jompus is fruity.\n(-+» more context here ---)\n\nRompuses are zumpuses. Alex is a tumpus.\n\nQuestion: True or false: Alex is not shy.\n\nPredicates:\nJompus (\\$x, bool) Does x belong to Jompus?\n(-++ more predicates here ---)\n\nZumpus (\\$x, bool) Does x belong to Zumpus?\n\nFacts:\nTumpuses (Alex, True)\n\nRules:\nJompus ($x, True) >>> Fruity($x, True)\n(--» more rules here ---)\n\nDumpus(\\$x, True) >>> Rompus(\\$x, True)\n\nQuery:\nShy(Alex, False)\n\n3819\n\n", "vlm_text": "The table lists components of First-Order Logic (FOL) along with their notations:\n\n1. **Constant** - Represented by lowercase letters.\n2. **Variable** - Examples include \\( x, y, z, \\ldots \\).\n3. **Atom** - Represented as \\( P(a_1, \\ldots, a_n) \\).\n4. **Negation** - Symbolized by \\( \\neg P \\).\n5. **Conjunction** - Shown as \\( P_1 \\land P_2 \\) or \\( P_1 \\land \\cdots \\land P_n \\).\n6. **Disjunction** - Represented by \\( P_1 \\lor P_2 \\) or \\( P_1 \\lor \\cdots \\lor P_n \\).\n7. **Implication** - Denoted as \\( P_1 \\rightarrow P_2 \\).\n8. **Equivalence** - Shown as \\( P_1 \\leftrightarrow P_2 \\).\n9. **Existential Quantifier** - Represented by \\( \\exists x P(x, \\cdots) \\).\n10. **Universal Quantifier** - Denoted by \\( \\forall x P(x, \\cdots) \\).\nThe table lists the details of different datasets characterized by the following columns:\n\n1. **Dataset**: Name of the dataset.\n2. **Reasoning**: Type of reasoning each dataset pertains to.\n3. **Test Size**: Number of test samples in each dataset.\n4. **#Opts**: Number of options per test question.\n\nHere are the specific details:\n\n- **PrOntoQA**: Deductive reasoning, test size of 500, 2 options.\n- **ProofWriter**: Deductive reasoning, test size of 600, 3 options.\n- **FOLIO**: FOL reasoning, test size of 204, 3 options.\n- **LogicalDeduction**: CSP reasoning, test size of 300, options of 3, 5, 7.\n- **AR-LSAT**: AR reasoning, test size of 230, 5 options.\nC Prompt Examples \nIn this section we provide examples of the prompts used for each dataset and method. Prompts for stan- dard in-context learning contain 2 demonstrations consisting of 3 parts each: a context, a question, and options. Prompts for chain-of-thought prompt- ing contain 2 demonstrations consisting of 5 parts each: a task description, a context, a question, op- tions, and a chain of reasoning. Prompts for Logic- LM contain 2 demonstrations with 5 parts each: a task description, a context, a question, options, and a domain-specific symbolic program. For brevity, we show only a single demonstration  for each set- ting in the following sections. \nC.1 PrOntoQA Prompts Standard In-Context Learning \nThe table contains a logical reasoning question.\n\n- **Context**: \n  - Jompuses are not shy.\n  - Jompuses are yumpuses.\n  - Zumpuses are rompuses.\n  - Max is a yumpus.\n\n- **Question**: \n  - Is the following statement true or false?\n  - Max is sour.\n\n- **Options**: \n  - A) True \n  - B) False\n\n- **The correct option is**: B (False)\nChain-of-Thought Prompting \nThe table contains a logical reasoning task. Here's a breakdown:\n\n- **Task Description**: Solve a logical reasoning problem based on given contexts.\n- **Context**: \n  - Jompuses are not shy and are yumpuses.\n  - Zumpuses are rompuses.\n  - Max is a yumpus.\n\n- **Question**: Is the statement \"Max is sour\" true or false?\n- **Options**: \n  - A) True\n  - B) False\n\n- **Reasoning**: \n  - Max is a yumpus, and each yumpus is a dumpus.\n  - Tumpuses are not sour; therefore, Max is not sour.\n\n- **Correct Option**: B (False)\nLogic-LM \nThe table contains a logic problem titled \"Logic-EM\" with several sections:\n\n1. **Task Description**: Instructions on parsing a logic problem and defining predicates, rules, facts, and questions.\n\n2. **Context**: Contains statements used for logical deductions:\n   - \"Each jompus is fruity.\"\n   - \"Rompuses are zumpses. Alex is a tumpus.\"\n\n3. **Question**: A logic question to evaluate:\n   - \"True or false: Alex is not shy.\"\n\n4. **Predicates**: Definitions of logical predicates, e.g., \n   - `Jompus($x, bool)` asks if x belongs to Jompus.\n   - `Zumpus($x, bool)` asks if x belongs to Zumpus.\n\n5. **Facts**: Known truths, such as:\n   - `Tumpuses(Alex, True)`\n\n6. **Rules**: Logical rules for inference, e.g.,\n   - `Jompus($x, True) >>> Fruity($x, True)`\n   - `Dumpus($x, True) >>> Rompus($x, True)`\n\n7. **Query**: The logical representation of the question:\n   - `Shy(Alex, False)`"}
{"page": 14, "image_path": "doc_images/2023.findings-emnlp.248_14.jpg", "ocr_text": "C.2 ProofWriter Prompts\n\nStandard In-Context Learning\n\nLogic-LM\n\nTask Description:\nand a question.\n\nContext: The cow is blue. The cow is round.\n(++ more context here ---)\nIf the cow is cold and the cow visits the lion then\n\nthe lion sees the squirrel.\n\nQuestion: Based on the above information, is the\nfollowing statement true, false, or unknown?\nThe tiger is not young.\n\nOptions:\nA) True\nB) False\nC) Unknown\n\nThe correct option is: B\n\nYou are given a problem description\n\nThe task is to:\n\n1) define all the predicates in the problem\n2) parse the problem into logic rules based on\nthe defined predicates\n\n3) write all the facts mentioned in the problem\n4) parse the question into the logic form\n\nContext: Anne is quiet.\n(-+» more context here ---)\nAll red people are young.\n\nErin is furry.\n\nChain-of-Thought Prompting\n\nTask Description: Given a problem statement as\n\ncontexts, the task is to answer a logical reasoning\nquestion.\nContext: The cow is blue. The cow is round.\n\n(-++ more context here ---)\nIf the cow is cold and the cow visits the lion then\nthe lion sees the squirrel.\n\nQuestion: Based on the above information, is the\nfollowing statement true, false, or unknown?\nThe tiger is not young.\n\nOptions:\nA) True\nB) False\nC) Unknown\n\nReasoning: The tiger likes the cow.\nThe tiger likes the squirrel.\n\n(-+» more reasoning here ---)\n\nIf something is nice and it sees the tiger then\nit is young. So the tiger is young.\n\nThe correct option is: B\n\nQuestion: Based on the above information, is the\nfollowing statement true, false, or unknown?\nAnne is white.\n\nPredicates:\n\nQuiet ($x, bool) Is x quiet?\n\nFurry ($x, bool) Is x furry?\n\n(-++ more predicates here ---)\n\nWhite($x, bool) Is x white?\n\nYoung ($x, bool) Is x young?\n\nFacts:\n\nQuite(Anne, True) ::: Anne is quiet.\n\n(++ more facts here ---)\n\nWhite(Harry, True) ::: Harry is white.\n\nRules:\n\nYoung($x, True) >>> Furry($x, True)\n\nare furry.\n\n(-++ more rules here ---)\n\nRed($x, True) >>> Young ($x,\nare young.\n\nTrue)\n\nQuery:\n\nWhite(Anne, True) Anne is white\n\nYoung people\n\nAll red people\n\n3820\n", "vlm_text": "C.2 Proof Writer Prompts \nLogic-LM \nStandard In-Context Learning \nContext : The cow is blue. The cow is round. ( · · ·  more context here  · · ·  ) If the cow is cold and the cow visits the lion then the lion sees the squirrel. Question : Based on the above information , is the following statement true , false , or unknown? The tiger is not young. Options : A) True B) False C) Unknown The correct option is: B \nChain-of-Thought Prompting \nTask Description : Given a problem statement as contexts , the task is to answer a logical reasoning question. Context : The cow is blue. The cow is round. ( · · ·  more context here  · · ·  ) If the cow is cold and the cow visits the lion then the lion sees the squirrel. Question : Based on the above information , is the following statement true , false , or unknown? The tiger is not young. Options : A) True B) False C) Unknown Reasoning : The tiger likes the cow. The tiger likes the squirrel. ( · · ·  more reasoning here  · · ·  ) If something is nice and it sees the tiger then it is young. So the tiger is young. The correct option is: B "}
{"page": 15, "image_path": "doc_images/2023.findings-emnlp.248_15.jpg", "ocr_text": "C.3. FOLIO Prompts\n\nStandard In-Context Learning\n\nLogic-LM\n\nContext: All people who regularly drink coffee are\ndependent on caffeine.\n\n( more context here )\n\nIf Rina is not a person dependent on caffeine and\na student, then Rina is either a person dependent\non caffeine and a student, or neither a person\ndependent on caffeine nor a student.\n\nQuestion: Based on the above information, is the\nfollowing statement true, false, or uncertain? Rina\nis a person who jokes about being addicted to\ncaffeine or unaware that caffeine is a drug.\n\nOptions:\nA) True\nB) False\nC) Uncertain\n\nThe correct option is: A\n\nChain-of-Thought Prompting\n\nTask Description: Given a problem statement as\ncontexts, the task is to answer a logical reasoning\nquestion.\n\nContext: The Blake McFall Company Building is a\ncommercial warehouse listed on the National Register\nof Historic Places.\n\n¢ more context here )\n\nJohn works at the Emmet Building.\n\nQuestion: Based on the above information, is the\nfollowing statement true, false, or uncertain?\nThe Blake McFall Company Building is located in\nPortland, Oregon.\n\nOptions:\nA) True\nB) False\nC) Uncertain\n\nReasoning: The Blake McFall Company Building is\nanother name for the Emmet Building.\n\n(-++ more reasoning here ---)\n\nTherefore, the Blake McFall Company Building is\nlocated in Portland, Oregon.\n\nThe correct option is: A\n\nTask Description: Given a problem description and a\nquestion. The task is to parse the problem and\nthe question into first-order logic formulas.\nThe grammar of the first-order logic formula is\ndefined as follows:\n\n1) logical conjunction: expr1 A expr2\n\n2) logical disjunction: expr1 V expr2\n\n3) logical exclusive disjunction: exprl ® expr2\n\n4) logical negation: —expr1\n\n5) expr1 implies expr2: exprl —> expr2\n\n6) expr1 if and only if expr2: exprl «+ expr2\n\n7) logical universal quantification: V x\n\n8) logical existential quantification: 3 x\n\nOutput format: logic form description\n\nContext: All people who regularly drink coffee are\ndependent on caffeine.\n\n( more context here )\n\nIf Rina is not a person dependent on caffeine and a\nstudent, then Rina is either a person dependent\n\non caffeine and a student, or neither a person\ndependent on caffeine nor a student.\n\nQuestion: Based on the above information, is the\nfollowing statement true, false, or uncertain?\nRina is either a person who jokes about being\naddicted to caffeine or is unaware that caffeine\nis a drug.\n\nPredicates:\nDependent (x) x is a person dependent on caffeine\n(--+ more predicates here ---)\n\nStudent (x) x is a student\n\nPremises:\n\nVx (Drinks(x) —> Dependent (x)) All people who\nregularly drink coffee are dependent on\ncaffeine.\n\n(-++ more premises here ---)\n\nYx (Jokes(x) —> >Unaware(x)) No one who jokes\nabout being addicted to caffeine is unaware\nthat caffeine is a drug.\n\nConclusion:\n\nJokes(rina) ® Unaware(rina) Rina is either a\nperson who jokes about being addicted to\ncaffeine or is unaware that caffeine is a drug.\n\n3821\n\n", "vlm_text": "C.3 FOLIO Prompts \nStandard In-Context Learning \nContext : All people who regularly drink coffee are dependent on caffeine. \nIf Rina is not a person dependent on caffeine and a student , then Rina is either a person dependent on caffeine and a student , or neither a person dependent on caffeine nor a student. \nQuestion : Based on the above information , is the following statement true , false , or uncertain? Rina is a person who jokes about being addicted to caffeine or unaware that caffeine is a drug. \nOptions : A) True B) False C) Uncertain The correct option is: A \nChain-of-Thought Prompting \nTask Description : Given a problem statement as contexts , the task is to answer a logical reasoning question. Context : The Blake McFall Company Building is a commercial warehouse listed on the National Register of Historic Places. ( · · ·  more context here  · · ·  ) John works at the Emmet Building. Question : Based on the above information , is the following statement true , false , or uncertain? The Blake McFall Company Building is located in Portland , Oregon. \nOptions \nA) True B) False C) Uncertain \nReasoning : The Blake McFall Company Building is another name for the Emmet Building. ( · · ·  more reasoning here  · · ·  ) Therefore , the Blake McFall Company Building is located in Portland , Oregon. \nLogic-LM \nTask Description : Given a problem description and a question. The task is to parse the problem and the question into first -order logic formulas. The grammar of the first -order logic formula is defined as follows: \n2) logical disjunction: expr1  ∨ expr2 3) logical exclusive disjunction: expr1  $\\oplus$  expr2 \ndependent on caffeine. ( · · ·  more context here  · · ·  ) If Rina is not a person dependent on caffeine and a student , then Rina is either a person dependent on caffeine and a student , or neither a person dependent on caffeine nor a student. \nQuestion : Based on the above information , is the following statement true , false , or uncertain? Rina is either a person who jokes about being addicted to caffeine or is unaware that caffeine is a drug. \nPredicates : \nDependent(x) :::   $\\mathsf{X}$   is a person dependent on caffeine ( · · ·  more predica s here  · · ·  ) Student(x) ::: x is a student\n\n \nPremises : \n∀ x (Drinks(x)    $\\rightarrow$  Dependent  $({\\mathsf{x}})$  ) ::: All people who regularly drink coffee are dependent on caffeine.\n\n ( · · ·  more p  here  · · ·  )\n\n ∀ x (Jokes(x)  $({\\mathsf{x}})\\ \\to$   →¬ Unaware  $({\\mathsf{x}})$  ) ::: No one who jokes about being addicted to caffeine is unaware that caffeine is a drug. \nConclusion : \nJokes(rina)    $\\oplus$  Unaware(rina) ::: Rina is either a person who jokes about being addicted to caffeine or is unaware that caffeine is a drug. "}
{"page": 16, "image_path": "doc_images/2023.findings-emnlp.248_16.jpg", "ocr_text": "C.4 LogicalDeduction Prompts\n\nStandard In-Context Learning\n\nLogic-LM\n\nContext: The following paragraphs each describe a\nset of seven objects arranged in a fixed order.\n¢ more context here )\n\nEve finished below Ada. Rob finished below Joe.\n\nQuestion: Which of the following is true?\n\nOptions:\n\nA) Ana finished third.\nB) Eve finished third.\nC) Ada finished third.\nD) Dan finished third.\nE) Rob finished third.\nF) Amy finished third.\nG) Joe finished third.\n\nThe correct option is: A\n\nChain-of-Thought Prompting\n\nTask Description: Given a problem statement as\ncontexts, the task is to answer a logical reasoning\nquestion.\n\nContext: The following paragraphs each describe a\nset of five objects arranged in a fixed order.\n\n¢ more context here )\n\nThe raven is the third from the left.\n\nQuestion: Which of the following is true?\n\nOptions:\n\nA) The quail is the rightmost.\nB) The owl is the rightmost.\nC) The raven is the rightmost.\nD) The falcon is the rightmost.\nE) The robin is the rightmost.\n\nReasoning: The owl is the leftmost. This means owl\nis not the rightmost.\n\n(-++ more reasoning here ---)\n\nThis means raven is also not the rightmost. So,\nthe answer is: A) The quail is the rightmost.\n\nThe correct option is: A\n\nTask Description: You are given a problem description.\nThe task is to parse the problem as a constraint\nsatisfaction problem, defining the domain,\nvariables, and contraints.\n\nContext: The following paragraphs each describe a\nset of three objects arranged in a fixed order.\n( more context here )\n\nThe minivan is newer than the convertible.\n\nQuestion: Which of the following is true?\n\nOptions:\nA) The station wagon is the second-newest.\nB) The convertible is the second-newest.\nC) The minivan is the second-newest.\n\nDomain:\n1: oldest\n3: newest\n\nVariables:\nstation\\_wagon CIN] [1, 2, 3]\nconvertible [IN] [1, 2, 3]\nminivan [CIN] (1, 2, 3]\n\nConstraints:\n\nstation\\_wagon == 1\noldest.\n\nminivan > convertible\nthe convertible.\n\nAl1DifferentConstraint ([station\\_wagon, convertible,\nminivan]) All vehicles have different\nvalues.\n\nThe station wagon is the\n\nThe minivan is newer than\n\nQuery:\nA) station\\_wagon\nsecond-newest.\nB) convertible == 2\nsecond-newest.\nC) minivan == 2\n\nThe station wagon is the\n\nThe convertible is the\n\nThe minivan is the second-newest\n\n3822\n\n", "vlm_text": "C.4 Logical Deduction Prompts \nStandard In-Context Learning \nContext : The following paragraphs each describe a set of seven objects arranged in a fixed order. ( · · ·  more context here  · · ·  ) Eve finished below Ada. Rob finished below Joe. \nQuestion : Which of the following is true? \nOptions \nA) Ana finished third. B) Eve finished third. C) Ada finished third. D) Dan finished third. E) Rob finished third. F) Amy finished third. G) Joe finished third. \nChain-of-Thought Prompting \nTask Description : Given a problem statement as contexts , the task is to answer a logical reasoning question. \nContext : The following paragraphs each describe a set of five objects arranged in a fixed order. ( · · ·  more context here  · · ·  ) The raven is the third from the left. \nQuestion : Which of the following is true? \nOptions : \nA) The quail is the rightmost. B) The owl is the rightmost. C) The raven is the rightmost. D) The falcon is the rightmost. E) The robin is the rightmost. \nLogic-LM \nTask Description : You are given a problem description. The task is to parse the problem as a constraint satisfaction problem , defining the domain , variables , and contraints. \nContext : The following paragraphs each describe a set of three objects arranged in a fixed order. ( · · ·  more context here  · · ·  ) The minivan is newer than the convertible. \nQuestion : Which of the following is true? \nOptions : \nA) The station wagon is the second -newest. B) The convertible is the second -newest. C) The minivan is the second -newest. \nDomain \n1: oldest 3: newest \nVariables \nstation\\_wagon [IN] [1, 2, 3] convertible [IN] [1, 2, 3] minivan [IN] [1, 2, 3] \nConstraints : \nstation\\_wagon   $==\\quad1$   ::: The station wagon is the \nminivan  $>$   convertible ::: The minivan is newer than the convertible. \nAll Different Constraint ([ station\\_wagon , convertible , minivan ]) ::: All vehicles have different values. \nQuery \nA) station\\_wagon   $==2$   ::: The station wagon is the second -newest. B) convertible   $==2$   ::: The convertible is the \nReasoning : The owl is the leftmost. This means owl is not the rightmost. ( · · ·  more reasoning here  · · ·  ) This means raven is also not the rightmost. So , the answer is: A) The quail is the rightmost. The correct option is: A "}
{"page": 17, "image_path": "doc_images/2023.findings-emnlp.248_17.jpg", "ocr_text": "C.5 AR-LSAT Prompts\n\nStandard In-Context Learning\n\nLogic-LM\n\nContext: During a single week, from Monday through\nFriday, tours will be conducted of a company's\nthree divisions: Operations, Production, and\nSales. Exactly five tours will be conducted\nthat week, one each day. (.-- more context here\n\n) If the Operations division is toured on\nThursday, then the Production division is\ntoured on Friday.\n\nQuestion: Which one of the following CANNOT be true\nof the week's tour schedule?\n\nOptions:\n\nA) The division that is toured on Monday is also\ntoured on Tuesday.\n\nB) The division that is toured on Monday is also\ntoured on Friday.\n\nC) The division that is toured on Tuesday is also\ntoured on Thursday.\n\nD) The division that is toured on Wednesday is also\ntoured on Friday.\n\nE) The division that is toured on Thursday is also\ntoured on Friday.\n\nThe correct option is: C\n\nChain-of-Thought Prompting\n\nTask Description: Given a problem statement as\ncontexts, the task is to answer a logical reasoning\nquestion.\n\nContext: During a single week, from Monday through\nFriday, tours will be conducted of a company's\nthree divisions: Operations, Production, and\nSales. Exactly five tours will be conducted\nthat week, one each day. (.-- more context here\n\n) If the Operations division is toured on\nThursday, then the Production division is\ntoured on Friday.\n\nQuestion: Which one of the following CANNOT be true\nof the week's tour schedule?\n\nOptions:\n\nA) The division that is toured on Monday is also\ntoured on Tuesday.\n\nB) The division that is toured on Monday is also\ntoured on Friday.\n\nC) The division that is toured on Tuesday is also\ntoured on Thursday.\n\nD) The division that is toured on Wednesday is also\ntoured on Friday.\n\nE) The division that is toured on Thursday is also\ntoured on Friday.\n\nReasoning: Since Thursday and Friday already have\ntours planned, only Monday, Tuesday and Wednesday\ntours need to be determined.\n\n(-++ more reasoning here ---)\n\nA different division is toured on Thursday.\nTherefore, the final answer is C.\n\nThe correct option is: C\n\nTask Description: You are given a problem description.\nThe task is to parse the problem as a constraint\nsatisfaction problem, defining the domain,\nvariables, and contraints.\n\nContext: A travel magazine has hired six interns -\nFarber, Gombarick, Hall, Jackson, Kanze, and\nLha - to assist in covering three stories:\n\nRomania, Spain, and Tuscany. (--- more context here\n) Jackson is assigned to Tuscany. Kanze is\nnot assigned to Spain.\n\nQuestion: Which one of the following interns CANNOT\nbe assigned to Tuscany?\n\nOptions:\n(A) Farber\n(B) Gombarick\n(C) Hall\n\n(D) Kanze\n\n(E) Lha\n\nDeclarations:\nstories = EnumSort([Romania, Spain, Tuscany])\nassistants = EnumSort([photographer , writer])\n\n(-++ more declarations here ---)\n\ntrained = Function(Linterns] -> [assistants])\n\nConstraints:\n\ntrained(Gombarick) == trained(Lha) Gombarick and\nLha will be trained in the same field\n\ntrained(Farber) != trained (Kanze) Farber and\nKanze will be trained in different fields\n\n(-++ more contraints here ---)\n\nassigned(Jackson) == Tuscany Jackson is assigned\nto Tuscany\n\nassigned(Kanze) != Spain Kanze is not assigned\nto Spain\n\nOptions:\n\nis_unsat(assigned(Farber) == Tuscany) ::: (A)\n\nis_unsat(assigned(Gombarick) == Tuscany) ::: (B)\n\ncc).\n\nis_unsat (assigned (Hall) Tuscany)\nis_unsat (assigned (Kanze) Tuscany) (D)\nis_unsat(assigned(Lha) == Tuscany) (E)\n\nD_ Result Interpreter Implementation\n\nFor PrOntoQA and ProofWriter, the Pyke logic\nprogramming engine returns the inferred value\nof the variable in the query or Unknown if the\nvariable cannot be determined. For example, for\nthe query ConductElectricity(Nail, a), Pyke\nmay return x =True. By comparing with the goal\nstatement ConductElectricity(Nail, False),\nwe can know that goal to be proved is False.\nFor FOLIO, the FOL inference engine directly re-\nturns the veracity label of the goal as ENTAILMENT,\nCONTRADICTION, and CONTINGENT, which can be\nmapped to True, False, and Unknown, respectively.\nFor LogicalDeduction, the solver returns all the\npossible value assignments in an array. We write\ntules to parse each option into the corresponding\nvalue and check it is in the generated array. For AR-\nLSAT, we attempt to separately prove each option\nto find the correct answer.\n\nE Example Generations of LoGIC-LM\n\n3823\n\n", "vlm_text": "C.5 AR-LSAT Prompts \nStandard In-Context Learning \nThe table presents a logic puzzle regarding the scheduling of tours for a company's three divisions—Operations, Production, and Sales—over a week from Monday to Friday. The key points from the table are:\n\n- Context: There will be exactly five tours conducted, one each day, for the three divisions.\n- If the Operations division is toured on Thursday, then the Production division is toured on Friday.\n- The question asks which one of five given options cannot be true regarding the week's tour schedule.\n- Options provided are combinations of tours being conducted on multiple days for different divisions (e.g., the division toured on Monday is also toured on Tuesday).\n\nThe correct answer, option C, states: \"The division that is toured on Tuesday is also toured on Thursday.\" This cannot be true given the context of the tour schedules.\nChain-of-Thought Prompting \nTask Description : Given a problem statement as contexts , the task is to answer a logical reasoning question. Context : During a single week , from Monday through Friday , tours will be conducted of a company 's three divisions: Operations , Production , and Sales. Exactly five tours will be conducted that week , one each day.  ( · · ·  more context here · · ·  )  If the Operations division is toured on Thursday , then the Production division is toured on Friday. Question : Which one of the following CANNOT be true of the week's tour schedule? Options : A) The division that is toured on Monday is also toured on Tuesday. B) The division that is toured on Monday is also toured on Friday. C) The division that is toured on Tuesday is also toured on Thursday. D) The division that is toured on Wednesday is also toured on Friday. E) The division that is toured on Thursday is also toured on Friday. Reasoning : Since Thursday and Friday already have tours planned , only Monday , Tuesday and Wednesday tours need to be determined. ( · · ·  more reasoning here  · · ·  ) A different division is toured on Thursday. Therefore , the final answer is C. The correct option is: C \nLogic-LM \nThe image contains a problem description formatted as a constraint satisfaction problem. It includes a task description, a context, a question, options, declarations, constraints, and solution options. The context involves a travel magazine hiring six interns to assist in covering stories in Romania, Spain, and Tuscany.\n\nKey elements of the image:\n- **Task Description:** Parsing the problem as a constraint satisfaction problem.\n- **Context:** Six interns are tasked to cover three stories; specific constraints and assignments are outlined.\n- **Question:** Identifying which intern cannot be assigned to Tuscany.\n- **Options:** Lists potential interns (Farber, Gombarick, Hall, Kanze, Lha) with labels (A) to (E).\n- **Declarations:** Defines domains and functions related to the problem, such as stories and assistant roles.\n- **Constraints:** Provides specific conditions on assignments and training of interns.\n- **Solution Options:** Checks unsatisfiability of each intern being assigned to Tuscany.\n\nBased on constraints and given assignments, the task is to deduce which intern cannot be assigned to Tuscany.\nD Result Interpreter Implementation \nFor PrOntoQA and Proof Writer, the  Pyke  logic programming engine returns the inferred value of the variable in the query or  Unknown  if the variable cannot be determined. For example, for the query  Conduct Electricity(Nail,    $x.$  ) ,  Pyke may return  $x=\\mathsf{T r u e}$  . By comparing with the goal statement  Conduct Electricity(Nail, False) , we can know that goal to be proved is  False . For FOLIO, the FOL inference engine directly re- turns the veracity label of the goal as  ENTAILMENT , CONTRADICTION , and  CONTINGENT , which can be mapped to  True ,  False , and  Unknown , respectively. For Logical Deduction, the solver returns all the possible value assignments in an array. We write rules to parse each option into the corresponding value and check it is in the generated array. For AR- LSAT, we attempt to separately prove each option to find the correct answer. \nE Example Generations of L OGIC -LM "}
{"page": 18, "image_path": "doc_images/2023.findings-emnlp.248_18.jpg", "ocr_text": "Example 1\n\nProblem: There are six types of wild turkeys: Eastern wild turkey, Osceola wild turkey, Gould's wild turkey, Merriam's wild turkey, Rio Grande\nwild turkey, and Ocellated wild turkey. Tom is not an Eastern wild turkey. Tom is not an Osceola wild turkey. Tom is also not a Gould's wild\nturkey, or a Merriam's wild turkey, or a Rio Grande wild turkey. Tom is a wild turkey.\n\nQuestion: Based on the above information, is the following statement true, false, or uncertain? Tom is an Ocellated wild turkey.\n\n(A) True (B) False (C) Uncertain\n\nPredicted logic programs:\nPremises:\nd1drurkeyleastern)—A_WildTurkey. 1a) -A-WildTurkey(gould) a-WildTuek $ #-WildTurkey(eie_grande)_A-WildTurkey(ocellated)\n= Vx (WildTurkey(x) > (OcellatedWildTurkey(x) V EasternWildTurkey(x) V OsceolaWildTurkey(x) V GouldsWildTurkey(x) V MerriamsWildTurkey(x)\nV RioGrandeWildTurkey(x)))\nWildTurkey(tom) A EasternWildTurkey(tom) A +OsceolaWildTurkey(tom) A ~GouldsWildTurkey(tom) A -MerriamsWildTurkey(tom) A\n-RioGrandeWildTurkey (tom)\nConclusion:\nOcellatedWildTurkey (tom)\n\nPredicted answer: -< > A\n\nExample 2\n\nProblem: A Japanese game company created the game the Legend of Zelda. All games in the Top 10 list are made by Japanese game companies. If a\ngame sells more than one million copies, then it will be selected into the Top 10 list. The Legend of Zelda sold more than one million copies.\nQuestion: Based on the above information, is the following statement true, false, or uncertain? The Legend of Zelda is in the Top 10 list.\n\n(A) True (B) False (C) Uncertain\n\nPredicted logic programs:\nPremises:\nJapaneseGameCompany(legend_of_zelda)\nvx (Top1@List(x) > JapaneseGameCompany(x))\nvx (SellMoreThanOneMillionCopies(x) > Top1oList(x))\nSellMoreThanOneMillionCopies(legend_of_zelda)\nConclusion:\nTop1List (legend_of_zelda)\n\nPredicted answer: A\n\nExample 3\n\nProblem: The following paragraphs each describe a set of five objects arranged in a fixed order. The statements are logically consistent within\neach paragraph. In a golf tournament, there were five golfers: Rob, Eve, Eli, Amy, and Dan. Dan finished second. Amy finished below Eve. Dan\nfinished above Eve. Amy finished above Eli.\n\nQuestion: Which of the following is true?\n\n(A) Rob finished third  (B) Eve finished third (C) Eli finished third (D) Amy finished third  (E) Dan finished third\n\nPredicted logic programs:\n\nDomain: Variables: Constraints:\n1: first Rob [IN] [1, 2, 3, 4, 5] Dan == 2\n5: last Eve [IN] [1, 2, 3, 4, 5] Amy > Eve\nEli [IN] [1, 2, 3, 4, 5] Dan>Eve > Dan < Eve\nAmy [IN] [1, 2, 3, 4, 5] Amy < Eli\nDan [IN] [1, 2, 3, 4, 5] AllDifferentConstraint([Rob, Eve, Eli, Amy, Dan])\nQuery:\n(A) Rob == (B) Eve == 3 (C) Eli (D) Amy == (E) Dan == 3\n\nPredicted answer: £- — B\n\nExample 4\n\nProblem: If the restaurant is listed in Yelp’s recommendations, then the restaurant does not receive many negative reviews. All restaurants with\na rating greater than 9 are listed in Yelp’s recommendations. Some restaurants that do not provide take-out service receive many negative\nreviews. All restaurants that are popular among local residents have ratings greater than 9. Subway has a rating greater than 9 or is popular\namong local residents.\n\nQuestion: Based on the above information, is the following statement true, false, or uncertain? Subway provides take-out service and does not\nreceive many negative reviews.\n\n(A) True (B) False (C) Uncertain\n\nPredicted logic programs:\n\nPremises:\nvx (Listed(x) > -Negativereviews(x))\nvx_(Ratingis yay Listed()}) > vx (GreaterThanNine(x) > Listed(x))\nax (.TakeOut(x) A NegativeReviews(x))\nvx_(Ropular(x) > _(Ratingtig y}A-¥>9)) > vx (Popular(x) > GreaterThanNine(x))\ning (subway; —y)-4-y 9-9-4 Popular(subway} > GreaterThanNine(subway) V Popular(subway)\nConclusion:\n\nTakeOut (subway) A -NegativeReviews(subway)\nPredicted answer: —C- > A\n\nFigure 6: Examples of generated symbolic representations and predicted answers. The incorrect segment(s) and\ntheir correspondence in the problem are marked in red, and the correct revisions are marked in green.\n\n3824\n", "vlm_text": "Example 1 \nProblem:  There are six types of wild turkeys: Eastern wild turkey, Osceola wild turkey, Gould's wild turkey, Merriam's wild turkey, Rio Grande  wild turkey, and Ocellated wild turkey.  Tom is not an Eastern wild turkey. Tom is not an Osceola wild turkey. Tom is also not a Gould's wild  turkey, or a Merriam's wild turkey, or a Rio Grande wild turkey. Tom is a wild turkey. Question:  Based on the above information, is the following statement true, false, or uncertain? Tom is an Ocellated wild turkey. (A) True                (B) False            (C) Uncertain Predicted logic programs: Premises: WildTurkey(eastern)  ∧ WildTurkey(osceola)  ∧ WildTurkey(gould)  ∧ WildTurkey(merriams)  ∧ WildTurkey(rio_grande)  ∧ WildTurkey(ocellated)   $\\rightarrow~\\forall\\times$   (WildTurkey(x)  $\\rightarrow$   (Ocellated Wild Turkey(x)  ∨ Eastern Wild Turkey(x)  ∨ Osceola Wild Turkey(x)  ∨ Gould s Wild Turkey(x)  ∨ Merriam s Wild Turkey(x)  ∨ Rio Grande Wild Turkey(x))) WildTurkey(tom)  ∧ ¬Eastern Wild Turkey(tom)  ∧ ¬Osceola Wild Turkey(tom)  ∧ ¬Gould s Wild Turkey(tom)  ∧ ¬Merriam s Wild Turkey(tom)  ∧ ¬Rio Grande Wild Turkey(tom) Conclusion: Ocellated Wild Turkey(tom) Predicted answer:  C  → A \nExample 2 \nProblem:  A Japanese game company created the game the Legend of Zelda. All games in the Top 10 list are made by Japanese game companies. If a  game sells more than one million copies, then it will be selected into the Top 10 list. The Legend of Zelda sold more than one million copies. Question:  Based on the above information, is the following statement true, false, or uncertain? The Legend of Zelda is in the Top 10 list. (A) True                (B) False            (C) Uncertain Predicted logic programs: Premises: Japanese Game Company(legend of zelda) ∀ x (Top10List(x)  $\\rightarrow$   Japanese Game Company(x)) ∀ x (Sell More Than One Million Copies(x)  $\\rightarrow$   Top10List(x)) Sell More Than One Million Copies(legend of zelda)Conclusion: Top10List(legend of zelda) Predicted answer:  A \nExample 3 \nThe table describes a logic puzzle involving the placement of five golfers (Rob, Eve, Eli, Amy, and Dan) in a fixed order based on given constraints. The problem statement clarifies that:\n\n1. Dan finished above Eve.\n2. Dan finished second.\n3. Amy finished below Eve.\n4. Dan finished above Eve.\n5. There is a constraint enforcing all positions must be different, meaning no two golfers can share the same position.\n\nThese are represented under 'Constraints' in the table:\n- Dan == 2 (meaning Dan finished second)\n- Amy > Eve (meaning Amy finished below Eve)\n- Dan > Eve → Dan < Eve (a clarification in constraints, showing that Dan finished above Eve)\n- Amy < Eli (meaning Amy finished before Eli)\n- An 'AllDifferentConstraint' ensures each golfer has a unique position.\n\nThe domain for each golfer is given as [1, 2, 3, 4, 5], representing their possible finishing positions.\n\nThe question asks which of the provided options is true concerning who finished third:\n- (A) Rob finished third\n- (B) Eve finished third\n- (C) Eli finished third\n- (D) Amy finished third\n- (E) Dan finished third\n\nUltimately, the 'Predicted answer' at the bottom indicates that option (B) \"Eve finished third\" is expected to be correct based on the logic provided.\nExample 4 \nProblem:  If the restaurant is listed in Yelp’s recommendations, then the restaurant does not receive many negative reviews.  All restaurants with  a rating greater than 9 are listed in   ${\\sf Y e1p}^{\\prime}\\,{\\sf s}$   recommendations.  Some restaurants that do not provide take-out service receive many negative  reviews.  All restaurants that are popular among local residents have ratings greater than 9. Subway has a rating greater than 9 or is popular  among local residents. Question:  Based on the above information, is the following statement true, false, or uncertain? Subway provides take-out service and does not  receive many negative reviews. (A) True                (B) False            (C) Uncertain Predicted logic programs: Premises: ∀ x (Listed(x)  $\\rightarrow$   ¬Negative Reviews(x))  ∀ x (Rating(x, y)  ∧ y > 9 → Listed(x))  $\\rightarrow~\\forall\\times$   (Greater Than Nine(x) → Listed(x)) ∃ x (¬TakeOut(x)  ∧ Negative Reviews(x))  ∀ ∧  $\\rightarrow~\\forall\\times$   $\\rightarrow$  x (Popular(x) → (Rating(x, y)  y > 9))  (Popular(x)  Greater Than Nine(x)) Rating(subway, y)  ∧ y > 9  ∨ Popular(subway)    $\\rightarrow$  Greater Than Nine(subway)  ∨ Popular(subway) Conclusion: TakeOut(subway)  ∧ ¬Negative Reviews(subway) Predicted answer:  $\\mathsf{\\Pi}_{-}\\mathsf{\\!\\!\\in\\!\\!\\!A}$  \nFigure 6: Examples of generated symbolic representations and predicted answers. The incorrect segment(s) and their correspondence in the problem are marked in  red , and the correct revisions are marked in  green . "}
