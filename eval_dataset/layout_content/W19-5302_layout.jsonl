{"layout": 0, "type": "text", "text": "Results of the WMT19 Metrics Shared Task: Segment-Level and Strong MT Systems Pose Big Challenges ", "text_level": 1, "page_idx": 0, "bbox": [82, 61, 515, 97], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 1, "type": "text", "text": "Qingsong Ma Tencent-CSIG, AI Evaluation Lab qingsong.mqs@gmail.com ", "page_idx": 0, "bbox": [113.468994140625, 102.7589340209961, 290, 147.0993194580078], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 2, "type": "text", "text": "Ondřej Bojar Charles University, MFF ÚFAL bojar@ufal.mff.cuni.cz ", "page_idx": 0, "bbox": [118.16699981689453, 158.38392639160156, 280.207763671875, 203.77931213378906], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 3, "type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0, "bbox": [154, 224, 208, 236], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 4, "type": "text", "text": "This paper presents the results of the WMT19  Metrics Shared Task. Par- ticipants were asked to score the out- puts of the translations systems compet- ing in the  WMT19  News Translation Task with automatic metrics. 13 research groups submitted 24 metrics, 10 of which are reference-less “metrics” and constitute submissions to the joint task with  WMT19 Quality Estimation Task, “QE as a Met- ric”. In addition, we computed 11 baseline metrics, with 8 commonly applied base- lines (BLEU, SentBLEU, NIST, WER, PER, TER, CDER, and chrF) and 3 reim- plementations (chrF  $^+$  , sacreBLEU-BLEU, and sacreBLEU-chrF). Metrics were evalu- ated on the system level, how well a given metric correlates with the  WMT19  offi- cial manual ranking, and segment level, how well the metric correlates with human judgements of segment quality. This year, we use direct assessment (DA) as our only form of manual evaluation. ", "page_idx": 0, "bbox": [89, 253.6031494140625, 273, 530.7339477539062], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 5, "type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0, "bbox": [72, 550, 165, 564], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 6, "type": "text", "text": "To determine system performance in machine translation (MT), it is often more practical to use an automatic evaluation, rather than a manual one. Manual/human evaluation can be costly and time consuming, and so an au- tomatic evaluation metric, given that it suffi- ciently correlates with manual evaluation, can be useful in developmental cycles. In studies involving hyperparameter tuning or architec- ture search, automatic metrics are necessary as the amount of human effort implicated in manual evaluation is generally prohibitively large. As objective, reproducible quantities, metrics can also facilitate cross-paper compar- ", "page_idx": 0, "bbox": [72, 574.5314331054688, 290, 766.1287231445312], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 7, "type": "text", "text": "Johnny Tian-Zheng Wei UMass Amherst, CICS jwei@umass.edu ", "page_idx": 0, "bbox": [327.3370056152344, 102.75894165039062, 469.38873291015625, 147.0993194580078], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 8, "type": "text", "text": "Yvette Graham Dublin City University, ADAPT graham.yvette@gmail.com ", "page_idx": 0, "bbox": [315, 158.38392639160156, 481.38580322265625, 202.72332763671875], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 9, "type": "text", "text": "isons. The WMT Metrics Shared Task  $^1$    annu- ally serves as a venue to validate the use of existing metrics (including baselines such as BLEU), and to develop new ones; see  Koehn and Monz  ( 2006 ) through  Ma et al.  ( 2018 ). ", "page_idx": 0, "bbox": [307, 217.2454376220703, 525, 290.859619140625], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 10, "type": "text", "text": "In the setup of our Metrics Shared Task, an automatic metric compares an MT sys- tem’s output translations with manual ref- erence translations to produce: either (a) system-level  score, i.e. a single overall score for the given MT system, or (b)  segment-level scores for each of the output translations, or both. ", "page_idx": 0, "bbox": [307, 288.950439453125, 525, 399.253662109375], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 11, "type": "text", "text": "This year we teamed up with the organizers of the QE Task and hosted “QE as a Metric” as a joint task. In the setup of the Quality Esti- mation Task ( Fonseca et al. ,  2019 ), no human- produced translations are provided to estimate the quality of output translations. Quality es- timation (QE) methods are built to assess MT output based on the source or based on the translation itself. In this task, QE developers were invited to perform the same scoring as standard metrics participants, with the excep- tion that they refrain from using a reference translation in production of their scores. We then evaluate the QE submissions in exactly the same way as regular metrics are evalu- ated, see below. From the point of view of correlation with manual judgements, there is no difference in metrics using or not using ref- erences. ", "page_idx": 0, "bbox": [307, 397.344482421875, 525, 656.687744140625], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 12, "type": "text", "text": "The source, reference texts, and MT sys- tem outputs for the Metrics task come from the News Translation Task ( Barrault et al. , 2019 , which we denote as Findings 2019). The texts were drawn from the news domain and involve translations of English (en) to/from Czech (cs), German (de), Finnish (fi), Gu- jarati (gu), Kazakh (kk), Lithuanian (lt), Rus- sian (ru), and Chinese (zh), but excluding cs- en (15 language pairs). Three other language pairs not including English were also manu- ally evaluated as part of the News Translation Task: German  $\\rightarrow$  Czech and German  $\\leftrightarrow$  French. In total, metrics could participate in 18 lan- guage pairs, with 10 target languages. ", "page_idx": 0, "bbox": [307, 654.779541015625, 525, 737.983642578125], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 13, "type": "text", "text": "", "page_idx": 1, "bbox": [72, 61.472442626953125, 290, 185.3246307373047], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 14, "type": "text", "text": "In the following, we first give an overview of the task (Section  2 ) and summarize the base- line (Section  3 ) and submitted (Section  4 ) met- rics. The results for system- and segment-level evaluation are provided in Sections  5.1  and 5.2 , respectively, followed by a joint discussion Section  6 . ", "page_idx": 1, "bbox": [72, 184.10842895507812, 290, 280.86163330078125], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 15, "type": "text", "text": "2 Task Setup ", "text_level": 1, "page_idx": 1, "bbox": [71, 293, 157, 306], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 16, "type": "text", "text": "This year, we provided task participants with one test set for each examined language pair, i.e. a set of source texts (which are commonly ignored by MT metrics), corresponding MT outputs (these are the key inputs to be scored) and a reference translation (held out for the participants of “QE as a Metric” track). ", "page_idx": 1, "bbox": [72, 313.35943603515625, 290, 410.1126708984375], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 17, "type": "text", "text": "In the system-level, metrics aim to corre- late with a system’s score which is an aver- age over many human judgments of segment translation quality produced by the given sys- tem. In the segment-level, metrics aim to produce scores that correlate best with a hu- man ranking judgment of two output trans- lations for a given source segment (more on the manual quality assessment in Section  2.3 ). Participants were free to choose which lan- guage pairs and tracks (system/segment and reference-based/reference-free) they wanted to take part in. ", "page_idx": 1, "bbox": [72, 408.896484375, 290, 586.9456176757812], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 18, "type": "text", "text": "2.1 Source and Reference Texts ", "text_level": 1, "page_idx": 1, "bbox": [71, 598, 253, 611], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 19, "type": "text", "text": "The source and reference texts we use are newstest2019  from this year’s WMT News Translation Task (see Findings 2019). This set contains approximately 2,000 sentences for each translation direction (except Gujarati, Kazakh and Lithuanian which have approx- imately 1,000 sentences each, and German to/from French which has 1701 sentences). ", "page_idx": 1, "bbox": [72, 614.4854125976562, 290, 724.7886352539062], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 20, "type": "text", "text": "The reference translations provided in  new- stest2019  were created in the same direc- tion as the MT systems were translating. ", "page_idx": 1, "bbox": [72, 723.5724487304688, 290, 766.128662109375], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 21, "type": "text", "text": "The exceptions are German  $\\rightarrow$  Czech where both sides are translations from English and German  $\\leftrightarrow$  French which followed last years’ practice. Last year and the years before, the dataset consisted of two halves, one originat- ing in the source language and one in the tar- get language. This however lead to adverse artifacts in MT evaluation. ", "page_idx": 1, "bbox": [307, 61.472442626953125, 525, 171.77462768554688], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 22, "type": "text", "text": "2.2 System Outputs ", "text_level": 1, "page_idx": 1, "bbox": [306, 180, 424, 193], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 23, "type": "text", "text": "The results of the Metrics Task are affected by the actual set of MT systems participating in a given translation direction. On one hand, if all systems are very close in their transla- tion quality, then even humans will struggle to rank them. This in turn will make the task for MT metrics very hard. On the other hand, if the task includes a wide range of systems of varying quality, correlating with humans should be generally easier, see Section  6.1  for a discussion on this. One can also expect that if the evaluated systems are of different types, they will exhibit different error patterns and various MT metrics can be differently sensi- tive to these patterns. ", "page_idx": 1, "bbox": [307, 195.52444458007812, 525, 400.670654296875], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 24, "type": "text", "text": "This year, all MT systems included in the Metrics Task come from the News Translation Task (see Findings 2019). There are however still noticeable differences among the various language pairs. ", "page_idx": 1, "bbox": [307, 398.762451171875, 525, 468.41668701171875], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 25, "type": "text", "text": "•  Unsupervised MT Systems. The German  $\\rightarrow$  Czech research systems were trained in an unsupervised fashion, i.e. without the access to parallel Czech- German texts (except for a couple of thousand sentences used primarily for val- idation). We thus expect the research German-Czech systems to be “more cre- ative” and depart further away from the references. The online systems in this language directions are however standard MT systems so the German-Czech evalu- ation could be to some extent bimodal. ", "page_idx": 1, "bbox": [315, 475.7289733886719, 525, 653.8436889648438], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 26, "type": "text", "text": "•  EU Election. The French  $\\leftrightarrow$  German translation was focused on a sub-domain of news, namely texts related EU Elec- tion. Various MT system developers may have invested more or less time to the do- main adaptation. ", "page_idx": 1, "bbox": [315, 660.5900268554688, 525, 743.8596801757812], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 27, "type": "text", "text": "•  Regular News Tasks Systems.  These ", "page_idx": 1, "bbox": [315, 750.6050415039062, 525, 766.1832275390625], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 28, "type": "text", "text": "are all the other MT systems in the evaluation; differing in whether they are trained only on WMT provided data (“Constrained”, or “Unconstrained”) as in the previous years. All the freely avail- able web services (online MT systems) are deemed unconstrained. ", "page_idx": 2, "bbox": [93, 61.472442626953125, 290, 158.2256317138672], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 29, "type": "text", "text": "Overall, the results are based on 233 systems across 18 language pairs. ", "page_idx": 2, "bbox": [72, 167.8984375, 290, 196.90562438964844], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 30, "type": "text", "text": "2.3 Manual Quality Assessment ", "text_level": 1, "page_idx": 2, "bbox": [71, 208, 252, 221], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 31, "type": "text", "text": "Direct Assessment (DA,  Graham et al. ,  2013 , 2014a ,  2016 ) was employed as the source of the “golden truth” to evaluate metrics again this year. The details of this method of human evaluation are provided in Findings 2019. ", "page_idx": 2, "bbox": [72, 222.679443359375, 290, 292.3346252441406], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 32, "type": "text", "text": "The basis of DA is to collect a large number of quality assessments (a number on a scale of 1–100, i.e. effectively a continuous scale) for the outputs of all MT systems. These scores are then standardized per annotator. ", "page_idx": 2, "bbox": [72, 290.7854309082031, 290, 360.43963623046875], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 33, "type": "text", "text": "In the past years, the underlying man- ual scores were reference-based (human judges had access to the same reference translation as the MT quality metric). This year, the of- ficial  WMT19  scores are reference-based (or “monolingual”) for some language pairs and reference-free (or “bilingual”) for others. ", "page_idx": 2, "bbox": [72, 358.89044189453125, 290, 455.6446533203125], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 34, "type": "text", "text": "Due to these different types of golden truth collection, reference-based language pairs are in a closer match with the standard reference- based metrics, while the reference-free lan- guage pairs are better fit for the “QE as a metric” subtask. ", "page_idx": 2, "bbox": [72, 454.095458984375, 290, 537.2996826171875], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 35, "type": "text", "text": "Note that system-level manual scores are different than those of the segment-level. Since for segment-level evaluation, collecting enough DA judgements for each segment is infeasible, so we resort to converting DA judgements to golden truth expressed as relative rankings, see Section  2.3.2 . ", "page_idx": 2, "bbox": [72, 535.75048828125, 290, 605.4047241210938], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 36, "type": "text", "text": "", "page_idx": 2, "bbox": [307, 61.472442626953125, 525, 90.4796371459961], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 37, "type": "text", "text": "The exact methods used to calculate corre- lations of participating metrics with the golden truth are described below, in the two sections for system-level evaluation (Section  5.1 ) and segment-level evaluation (Section  5.2 ). ", "page_idx": 2, "bbox": [307, 89.12344360351562, 525, 158.7786407470703], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 38, "type": "text", "text": "2.3.1 System-level Golden Truth: DA ", "text_level": 1, "page_idx": 2, "bbox": [305, 169, 521, 181], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 39, "type": "text", "text": "For the system-level evaluation, the collected continuous DA scores, standardized for each annotator, are averaged across all assessed seg- ments for each MT system to produce a scalar rating for the system’s performance. ", "page_idx": 2, "bbox": [307, 182.75143432617188, 525, 252.4066162109375], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 40, "type": "text", "text": "The underlying set of assessed segments is different for each system. Thanks to the fact that the system-level DA score is an average over many judgments, mean scores are consis- tent and have been found to be reproducible ( Graham et al. ,  2013 ). For more details see Findings 2019. ", "page_idx": 2, "bbox": [307, 251.0504150390625, 525, 347.80364990234375], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 41, "type": "text", "text": "2.3.2 Segment-level Golden Truth: daRR ", "text_level": 1, "page_idx": 2, "bbox": [305, 357, 504, 383], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 42, "type": "text", "text": "Starting from  Bojar et al.  ( 2017 ), when WMT fully switched to DA, we had to come up with a solid golden standard for segment-level judge- ments. Standard DA scores are reliable only when averaged over sufficient number of judg- ments. ", "page_idx": 2, "bbox": [307, 385.3264465332031, 525, 468.5306701660156], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 43, "type": "text", "text": "Fortunately, when we have at least two DA scores for translations of the same source in- put, it is possible to convert those DA scores into a relative ranking judgement, if the dif- ference in DA scores allows conclusion that one translation is better than the other. In the following, we denote these re-interpreted DA judgements as “ daRR ”, to distinguish it clearly from the relative ranking (“RR”) golden truth used in the past years. ", "page_idx": 2, "bbox": [307, 467.1734313964844, 525, 604.5746459960938], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 44, "type": "table", "page_idx": 3, "img_path": "layout_images/W19-5302_0.jpg", "bbox": [73, 61, 289, 341], "page_size": [595.280029296875, 841.8900146484375], "ocr_text": "DA>1 Ave DA pairs DARR\nde-en 2,000 16.0 239,220 85,365\nfi-en 1,996 9.5 83,168 38,307\ngu-en 1,016 11.0 55,880 31,139\nkk-en 1,000 11.0 55,000 27,094\nIt-en 1,000 11.0 55,000 21,862\nru-en 1,999 11.9 131,766 46,172\nzh-en 2,000 10.1 95,174 31,070\nen-cs 1,997 9.1 75,560 27,178\nen-de 1,997 19.1 347,109 99,840\nen-fi 1,997 81 59,129 31,820\nen-gu 998 6.9 21,854 11,355\nen-kk 998 9.0 37,032 18,172\nen-It 998 9.0 36,435 17,401\nen-ru) 1,997 8.7 69,503 24,334\nen-zh 1,997 9.8 87,501 18,658\nde-cs 1,997 8.5 65,039 35,793\nde-fr 1,605 4.1 12,055 4,862\nfr-de 1,224 3.0 4,258 1,369\n", "vlm_text": "The table appears to present data on language pairs and associated metrics. Here's a breakdown of the columns:\n\n- **Language Pair (e.g., de-en, fi-en)**: Indicates the source and target languages in a language pair. \"de-en\" means a translation from German to English, \"fi-en\" translates Finnish to English, and so on.\n\n- **DA>1**: This likely represents some count or threshold related to direct assessments (DA) that are greater than 1. It details the number of instances or samples that meet this criterion for each language pair.\n\n- **Ave**: Represents the average (possibly an average score, assessment, or rating) for the given language pair.\n\n- **DA pairs**: Indicates the total number of DA pairs or instances analyzed or assessed for the given language pair.\n\n- **dARR**: This could be a metric related to ARR (possibly Annual Recurrence Rate or a similar metric specific to the context), adjusted or detailed for each language pair.\n\nThis table seems to involve linguistic data, possibly related to translation quality assessments or evaluations across various language pairs. The specific meanings of \"DA>1\", \"Ave\", \"DA pairs\", and \"dARR\" would require additional context regarding the methodology or study design to fully interpret."}
{"layout": 45, "type": "text", "text": "newstest2019 ", "page_idx": 3, "bbox": [144.99200439453125, 340.88397216796875, 217.27569580078125, 356.462158203125], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 46, "type": "text", "text": "Table 1: Number of judgements for DA converted to  daRR  data; “DA  $.>$  1” is the number of source input sentences in the manual evaluation where at least two translations of that same source input segment received a DA judgement; “Ave” is the average number of translations with at least one DA judgement available for the same source input sentence; “DA pairs” is the number of all possi- ble pairs of translations of the same source input resulting from “DA  $>$  1”; and “ daRR ” is the num- ber of DA pairs with an absolute difference in DA scores greater than the 25 percentage point mar- gin. ", "page_idx": 3, "bbox": [72, 367.1651611328125, 290, 524.7440185546875], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 47, "type": "text", "text": "From the complete set of human assess- ments collected for the News Translation Task, all possible pairs of DA judgements attributed to distinct translations of the same source were converted into  daRR  better/worse judge- ments. Distinct translations of the same source input whose DA scores fell within 25 percentage points (which could have been deemed equal quality) were omitted from the evaluation of segment-level metrics. Conver- sion of scores in this way produced a large set of  daRR  judgements for all language pairs, shown in Table  1  due to combinatorial ad- vantage of extracting  daRR  judgements from all possible pairs of translations of the same source input. We see that only German-French and esp. French-German can suffer from in- sufficient number of these simulated pairwise comparisons. ", "page_idx": 3, "bbox": [72, 543.970458984375, 290, 708.4697265625], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 48, "type": "text", "text": "", "page_idx": 3, "bbox": [307, 61.472442626953125, 525, 158.2256317138672], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 49, "type": "text", "text": "The  daRR  judgements serve as the golden standard for segment-level evaluation in WMT19 . ", "page_idx": 3, "bbox": [307, 156.31643676757812, 525, 198.87362670898438], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 50, "type": "text", "text": "3 Baseline Metrics ", "text_level": 1, "page_idx": 3, "bbox": [306, 209, 424, 221], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 51, "type": "text", "text": "In addition to validating popular metrics, in- cluding baselines metrics serves as comparison and prevents “loss of knowledge” as mentioned by  Bojar et al.  ( 2016 ). ", "page_idx": 3, "bbox": [307, 227.0624542236328, 525, 283.16864013671875], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 52, "type": "text", "text": "Moses scorer  $^{-6}$    is one of the MT evaluation tools that aggregated several useful metrics over the time. Since  Macháček and Bojar ( 2013 ), we have been using Moses scorer to provide most of the baseline metrics and kept encouraging authors of well-performing MT metrics to include them in Moses scorer. The baselines we report are: ", "page_idx": 3, "bbox": [307, 277.3004455566406, 525, 391.5616455078125], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 53, "type": "text", "text": "BLEU and NIST  The metrics  BLEU ( Papineni et al. , 2002 ) and NIST ( Doddington , 2002 ) were computed using mteval-v13a.pl 8 from the OpenMT Evaluation Campaign. The tool includes its own tokeniza- tion. We run  mteval  with the flag --international-tokenization . ", "page_idx": 3, "bbox": [312, 398.6579895019531, 525, 509.3102722167969], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 54, "type": "text", "text": "TER, WER, PER and CDER.  The met- rics  TER  ( Snover et al. ,  2006 ),  WER , PER  and  CDER  ( Leusch et al. ,  2006 ) were produced by the Moses scorer, which is used in Moses model optimization. We used the standard tokenizer script as available in Moses toolkit for tokeniza- tion. ", "page_idx": 3, "bbox": [312, 515.6939697265625, 525, 626.0616455078125], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 55, "type": "text", "text": "sentBLEU.  The metric  sentBLEU  is com- puted using the script  sentence-bleu , a part of the Moses toolkit. It is a ", "page_idx": 3, "bbox": [312, 632.72900390625, 525, 675.3516235351562], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 56, "type": "table", "page_idx": 4, "img_path": "layout_images/W19-5302_1.jpg", "table_footnote": "” denotes that the metric took part in (some of the language pairs) of the segment- and/or system-level ” indicates that the metric didn’t participate the track (Seg/Sys-level). A metric is learned if it is trained on a QE or metric evaluation dataset (i.e. pretraining or parsers don’t count, but  http://github.com/moses-smt/ − • ” indicates that the system-level scores are implied, simply taking arithmetic (macro-)average of segment-level scores. “ training on WMT 2017 metrics task data does). For the baseline metrics available in the Moses toolkit, paths are relative to Table 2: Participants of WMT19 Metrics Shared Task. “ . ⊘ evaluation. “ ", "bbox": [91, 59, 497, 771], "page_size": [595.280029296875, 841.8900146484375], "ocr_text": "TSTA/OTNTATYS/woo“qny3t3//:daaq. (6102 ‘O'T) OUN @ . Aires oyueutes TAS Z-ISIA.\nTSTA/OTRIATYD/wooqnyyT3//:daaq. (6102 ‘OT) OUN @ ° Ayurepurts oryueures Z-ISIA.\n‘ PWS JO “ATTQ, @ ° ‘ d TL-aasn 2\n‘ PWS JO “aTTy, D ° ‘ d aasn L\né é @ i é a +INO a\ni i @ . i t Ina 2\n= (610z “Te 0 vAeysaoyURA) NyIRT, JO “ATTA @ ° sok sSuIppoqute prom [enyxoywo0o WISV'I a\n- (610Z “Te yo eXvysAoyuRA) NyIeT, Jo “ATE, @ . sox “qoad Soy LIV “qute prom yenyxeyu0o qT\n= (z10z ‘orsodog) LaVay ‘Avsweatag AO wyqnq @ ° uooIxe] [Wal ‘sqoid Soy JT WVUEDPpsod-T WAI\n- (z10z ‘orsodog) LaVay ‘Aysmamg Ano wyqnq @ e UOdTXET TINA “sqoad Soy PT aWAHHdUON-TIVET\nTSTA/OTATHT YS /woo-qnyats//:daaq (6102 OT) OUN @ . Aquerruts orjrentes TAS” T-ISIA\nTSTA/OTNINTYD /woo- qnyyT3//:daaq. (6102 ‘O'T) OUN @ ° AyLreUrts o1yuRUIes TASIA\nTSTA/OTNINTUD/wooqny3T3//:daaq. (6102 ‘O'T) OUN @ ° AyLreTUIS oTyURUTES O-ISIA,\n- (e610Z “Te @ MoH) UOpuOCT eSeqfoD yereduy @ e QOURISIP JOAOUL PIOM OdGINM.\ndeud/ueuexoyx/woo* qny3t3//:daaq (610Z “Te J vIMUTTYSOX) “ATU, Ueyzodoropy OAYOT, @ e sosvaydesed ‘soouerejar-oponsd did\n- (610% ‘NH pue ony) Aystoatuy Suppeg @ e SPUSUIUSTTR pIOM (AdOD+XVINAS) 0'% ++ HOELANY\na (610z ‘NH pue ond) Aysroarug Suppeg @ . syUOUMTUSTTE prom (XVINAS) 0'% ++HOaLayY ‘\nYOdaT/weyzeod/woo-qnyat3//:daaq (g10z ‘210% “Te 99 We) Lavay ‘Mustang AiO wyqnq @ . somyeoy OSIM] oovyns ayOdat =\naOdaT/ueIeod/woo *qny3tB//:daaq (10% ‘210% “Te 9° Ue) Ld VV ‘Auswatrg AV1D uqud @ ° Soe} OTJSMBUT] sows VuOdaT Ea\n9x07U09-UT-TeASqU/WeXTyTU/WOD *qnyyT3//:d3aaq (610Z “Te 3 AMTVPY) ouMoIayy Jo “aTI_, @ e sox SUOT}{tasoIdor RMU pours, WISH m\neoueysTaITPyPpepuesxg/9t-yIMI/woo“qnyat3//:daay. (610Z “Te 3 AoypuRyg) “ATA UoIPEY HLM @ . sods yIpa ‘soweystp ype “rey eicicl\n‘ugLoezeyg/gt-qanz/wooqny3t3//:daaq (e9T0z% “Te yo Suey) “ATAQ UsTPeyY HLM @ . sod4y yIpo ‘eoueystp ype “repo WALOVUVHO\n9x09U09-uT-TeASqU/WeyTATU/WoD “qnyyT3//:daqy (610% “Te 3 AMTVPY) euMo]eyy JO “AaTI_, @ . sSuIppaquia prom [enyxeyuo0. uLyad\nqeeq/otaefoueas/uooqnyst3//:daay (GTO ‘We,eUTIg pue 9IAcfouRys) OOT] ‘urepsoysury jo “aug, D . sod sear} uoTyeynuried ‘surers-0 “eT yaaa\nNaTger9es/3sodfu/woo- qnyat3//:daaq (egt0z) Id ° = surer3-0 AuHO-) A TAayovs\nNaIger2es/3sodfu/wos-qny3t8//:daaq (egt0z) Is0g e = suTeis- NATE-NaATaTUoOVs\ndzyo/otaodod-w/woo- qnyy13//:daaq (2102) 9}40dog @ e sureis-0 Jopereyp + q7uHO\nayo /2taodod-w/woo-qnyyt3//:daaq (g10z) 9}1a0dog @ e sureis-0 Jopereyp aHO\nqoyeNTeAS/ IEW (Teposepsesom) (9002) ‘Te 9° YsnoyT . - sod4y y1po ‘aoueystp ype wado ty\nzoqenTene/3.10m (Teposepsesow) (€00Z) ‘Te 2 tpsney ° = sodA} yIpo ‘soueystp ype Yad 8,\nTOZENTeAS/4I0u (Ieposepsesow) (9002) ‘Te 39 Joaoug ° = sodA} ype ‘soueystp ype WAL e\nToVeNTeAS/7IEU (Teposepsesow) (9002) ‘Te 3 Tsne'T . - aouR\\sIp UleyysuaAeT VaM o\n[d- egTa-Teaequ/otreueS/sydtios (reposepsesom) (z00z) w0ySurppoq ° = surer3-0 LSIN\n[d- egpa-Teaequ/otreues/sydtaos (reposepsesom) (Z00Z) ‘Te 9 tuourdeg ° = surer3-0 Nata\nNeTq-e0uejUes /4Teu (Leposepsesow) - e suTeis- OATAINAS\nAqyiqeyreay yuedpryieg /uoryeyD skg Bag jpolulieseT somnyrog dO,\n\nJOAe'T Surs109g\n", "vlm_text": "The image represents a table that provides a comparison of various metrics used primarily for machine translation evaluation. It is organized into several columns that display information about different metrics, their features, whether they are learned or not, how they score at the segment and system levels, citations, participant institutions, and availability links. Each row corresponds to a particular metric.\n\nHere's a breakdown of the table content:\n\n- **Metrics**: This column lists the evaluation metrics. Examples include BLEU, METEOR, TER, LEPOR, etc.\n- **Features**: This column provides information on the features used by each metric, such as n-grams, contextual embeddings, edit distance, semantic similarity, and others.\n- **Learned?**: This column indicates whether the metric is learned ('yes') or not ('no') for each metric.\n- **Scoring Level**: Divided into 'Seg' (Segment) and 'Sys' (System) columns, it indicates whether the metric scores at the segment or system level with symbols (• for scores, ◌ for no scores, and various other symbols).\n- **Citation**: This column provides references to publications that describe or use the metrics, formatted with the authors' names and publication years.\n- **Participant Institutions**: Lists institutions like universities and organizations associated with each metric.\n- **Availability**: Provides URLs or links (formatted as hyperlinks) to repositories or locations where the metrics can be accessed or implemented.\n\nThe metrics are compared based on their characteristics, usage, and availability, offering a comprehensive view for researchers and practitioners in natural language processing fields, especially those focused on machine translation."}
{"layout": 57, "type": "text", "text": "smoothed version of  BLEU  for scoring at the segment-level. We used the stan- dard tokenizer script as available in Moses toolkit for tokenization. ", "page_idx": 5, "bbox": [93, 61.472442626953125, 290, 117.5786361694336], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 58, "type": "text", "text": "chrF and chrF+.  The metrics  chrF  and chrF  $+$   ( Popović ,  2015 ,  2017 ) are com- puted using their original Python im- plementation, see Table  2 . We ran chrF++.py  with the parameters  -nw 0 -b 3  to obtain the  chrF  score and with -nw 1 -b 3  to obtain the  chrF  $+$   score. Note that  chrF  intentionally removes all spaces before matching the    $n$  -grams, deto- kenizing the segments but also concate- nating words. ", "page_idx": 5, "bbox": [76, 134.323974609375, 290, 285.339599609375], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 59, "type": "text", "text": "sacreBLEU-BLEU and sacreBLEU- chrF.  The metrics  sacreBLEU-BLEU and  sacreBLEU-chrF  ( Post ,  2018a ) are re-implementation of BLEU and chrF respectively. We ran  sacreBLEU-chrF with the same parameters as  chrF , but their scores are slightly different. The sig- nature strings produced by sacreBLEU for BLEU and chrF respectively are BLEU+case.lc+lang.de-en+numrefs.  $1+$  smooth.exp+tok.intl+version.1.3.6 and chrF3+case.mixed+lang.de-en +numchars.  $^{6+}$  numrefs.1+space.False+ tok.13a+version.1.3.6 . ", "page_idx": 5, "bbox": [76, 302.0849914550781, 290, 494.0313415527344], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 60, "type": "text", "text": "The baselines serve in system and segment- level evaluations as customary:  BLEU ,  TER , WER ,  PER ,  CDER ,  sacreBLEU-BLEU and  sacreBLEU-chrF  for system-level only; sentBLEU  for segment-level only and  chrF for both. ", "page_idx": 5, "bbox": [71, 512.7745361328125, 290, 595.978759765625], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 61, "type": "text", "text": "Chinese word segmentation is unfortunately not supported by the tokenization scripts men- tioned above. For scoring Chinese with base- line metrics, we thus pre-processed MT out- puts and reference translations with the script tokenizeChinese.py 11   by Shujian Huang, which separates Chinese characters from each other and also from non-Chinese parts. ", "page_idx": 5, "bbox": [71, 596.508544921875, 290, 706.8106689453125], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 62, "type": "text", "text": "4 Submitted Metrics ", "text_level": 1, "page_idx": 5, "bbox": [307, 64, 437, 76], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 63, "type": "text", "text": "Table  2  lists the participants of the  WMT19 Shared Metrics Task, along with their metrics and links to the source code where available. We have collected 24 metrics from a total of 13 research groups, with 10 reference-less “met- rics” submitted to the joint task “QE as a Met- rich” with WMT19 Quality Estimation Task. The rest of this section provides a brief sum- ", "page_idx": 5, "bbox": [307, 83.10344696044922, 525, 193.83163452148438], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 64, "type": "text", "text": "mary of all the metrics that participated. ", "page_idx": 5, "bbox": [307, 191.92343139648438, 504.3924865722656, 207.38162231445312], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 65, "type": "text", "text": "4.1 BEER ", "text_level": 1, "page_idx": 5, "bbox": [307, 217, 369, 230], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 66, "type": "text", "text": "BEER  ( Stanojević and Sima’an ,  2015 ) is a trained evaluation metric with a linear model that combines sub-word feature indicators (character n-grams) and global word order fea- tures (skip bigrams) to achieve a language ag- nostic and fast to compute evaluation metric. BEER has participated in previous years of the evaluation task. ", "page_idx": 5, "bbox": [307, 233.50543212890625, 525, 343.8086242675781], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 67, "type": "text", "text": "4.2 BERTr ", "text_level": 1, "page_idx": 5, "bbox": [307, 354, 372, 366], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 68, "type": "text", "text": "BERTr ( Mathur et al. ,  2019 ) uses contextual word embeddings to compare the MT output with the reference translation. ", "page_idx": 5, "bbox": [307, 369.93243408203125, 525, 412.4886474609375], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 69, "type": "text", "text": "The BERTr score of a translation is the average recall score over all tokens, us- ing a relaxed version of token matching based on BERT embeddings: namely, com- puting the maximum cosine similarity be- tween the embedding of a reference to- ken against any token in the MT out- put. BERTr uses  bert base uncased  em- beddings for the to-English language pairs, and  bert base multilingual cased  embed- dings for all other language pairs. ", "page_idx": 5, "bbox": [307, 411.0064697265625, 525, 561.9567260742188], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 70, "type": "text", "text": "4.3 CharacTER ", "text_level": 1, "page_idx": 5, "bbox": [307, 573, 400, 584], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 71, "type": "text", "text": "CharacTER  ( Wang et al. ,  2016b , a ), identi- cal to the 2016 setup, is a character-level met- ric inspired by the commonly applied transla- tion edit rate (TER). It is defined as the mini- mum number of character edits required to ad- just a hypothesis, until it completely matches the reference, normalized by the length of the hypothesis sentence.  CharacTER  calculates the character-level edit distance while per- forming the shift edit on word level. Unlike the strict matching criterion in  TER , a hy- pothesis word is considered to match a refer- ence word and could be shifted, if the edit dis- tance between them is below a threshold value. The Levenshtein distance between the refer- ence and the shifted hypothesis sequence is computed on the character level. In addition, the lengths of hypothesis sequences instead of reference sequences are used for normalizing the edit distance, which effectively counters the issue that shorter translations normally achieve lower  TER . ", "page_idx": 5, "bbox": [307, 588.0804443359375, 525, 766.1296997070312], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 72, "type": "text", "text": "", "page_idx": 6, "bbox": [71, 61.472442626953125, 290, 185.3246307373047], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 73, "type": "text", "text": "Similarly to other character-level metrics, CharacTER  is generally applied to non- tokenized outputs and references, which also holds for this year’s submission with one ex- ception. This year tokenization was carried out for en-ru hypotheses and references be- fore calculating the scores, since this results in large improvements in terms of correlations. For other language pairs, no tokenizer was used for pre-processing. ", "page_idx": 6, "bbox": [71, 184.65444946289062, 290, 322.05560302734375], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 74, "type": "text", "text": "4.4 EED ", "text_level": 1, "page_idx": 6, "bbox": [71, 335, 126, 347], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 75, "type": "text", "text": "EED  ( Stanchev et al. ,  2019 ) is a character- based metric, which builds upon  CDER . It is defined as the minimum number of opera- tions of an extension to the conventional edit distance containing a “jump” operation. The edit distance operations (insertions, deletions and substitutions) are performed at the char- acter level and jumps are performed when a blank space is reached. Furthermore, the cov- erage of multiple characters in the hypothesis is penalised by the introduction of a coverage penalty. The sum of the length of the refer- ence and the coverage penalty is used as the normalisation term. ", "page_idx": 6, "bbox": [71, 352.49444580078125, 290, 544.0927124023438], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 76, "type": "text", "text": "4.5 ESIM ", "text_level": 1, "page_idx": 6, "bbox": [71, 557, 131, 569], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 77, "type": "text", "text": "Enhanced Sequential Inference Model ( ESIM ; Chen et al. ,  2017 ;  Mathur et al. ,  2019 ) is a neural model proposed for Natural Language Inference that has been adapted for MT evalu- ation. It uses cross-sentence attention and sen- tence matching heuristics to generate a repre- sentation of the translation and the reference, which is fed to a feedforward regressor. The metric is trained on singly-annotated Direct Assessment data that has been collected for evaluating WMT systems: all WMT 2018 to- English data for the to-English language pairs, and all WMT 2018 data for all other language pairs. ", "page_idx": 6, "bbox": [71, 574.5314331054688, 290, 766.1296997070312], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 78, "type": "text", "text": "4.6 hLEPORb_baseline, hLEPORa_baseline ", "text_level": 1, "page_idx": 6, "bbox": [305, 63, 449, 91], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 79, "type": "text", "text": "The submitted metric  hLEPOR_baseline  is a metric based on the factor combination of length penalty, precision, recall, and position difference penalty. The weighted harmonic mean is applied to group the factors together with tunable weight parameters. The system- level score is calculated with the same formula but with each factor weighted using weight es- timated at system-level and not at segment- level. ", "page_idx": 6, "bbox": [307, 94.09644317626953, 525, 231.49761962890625], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 80, "type": "text", "text": "In this submitted baseline version,  hLE- POR_baseline  was not tuned for each lan- guage pair separately but the default weights were applied across all submitted language pairs. Further improvements can be achieved by tuning the weights according to the devel- opment data, adding morphological informa- tion and applying n-gram factor scores into it (e.g. part-of-speech, n-gram precision and n-gram recall that were added into LEPOR in WMT13.). The basic model factors and further development with parameters setting were described in the paper ( Han et al. ,  2012 ) and ( Han et al. ,  2013 ). ", "page_idx": 6, "bbox": [307, 230.49642944335938, 525, 422.0946350097656], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 81, "type": "text", "text": "For sentence-level score, only hLE- PORa_baseline  was submitted with scores calculated as the weighted harmonic mean of all the designed factors using default parameters. ", "page_idx": 6, "bbox": [307, 421.0924377441406, 525, 490.7476501464844], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 82, "type": "text", "text": "For system-level score, both hLEPORa_baseline and hLE- PORb_baseline were submitted, where hLEPORa_baseline is the the average score of all sentence-level scores, and  hLE- PORb_baseline  is calculated via the same sentence-level hLEPOR equation but replac- ing each factor value with its system-level counterpart. ", "page_idx": 6, "bbox": [307, 489.7464599609375, 525, 613.5986938476562], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 83, "type": "text", "text": "4.7 Meteor++_2.0 (syntax), Meteor++_2.0 (syntax+copy) ", "text_level": 1, "page_idx": 6, "bbox": [305, 624, 505, 653], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 84, "type": "text", "text": "Meteor++ 2.0  ( Guo and Hu ,  2019 ) is a metric based on Meteor ( Denkowski and Lavie ,  2014 ) that takes syntactic-level para- phrase knowledge into consideration, where paraphrases may sometimes be skip-grams. i.e. (protect...from, protect...against). As the original Meteor-based metrics only pay attention to consecutive string matching, they perform badly when reference-hypothesis pairs contain skip n-gram paraphrases.  Me- teor++ 2.0  extracts the knowledge from the Paraphrase Database (PPDB;  Bannard and Callison-Burch ,  2005 ) and integrates it into Meteor-based metrics. ", "page_idx": 6, "bbox": [307, 655.826416015625, 525, 766.129638671875], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 85, "type": "text", "text": "", "page_idx": 7, "bbox": [72, 61.472442626953125, 290, 144.6766357421875], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 86, "type": "text", "text": "4.8 PReP ", "text_level": 1, "page_idx": 7, "bbox": [71, 154, 131, 166], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 87, "type": "text", "text": "PReP  ( Yoshimura et al. ,  2019 ) is a method for filtering pseudo-references to achieve a good match with a gold reference. ", "page_idx": 7, "bbox": [72, 169.1484375, 290, 211.7046356201172], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 88, "type": "text", "text": "At the beginning, the source sentence is translated with some off-the-shelf MT sys- tems to create a set of pseudo-references. (Here the MT systems were Google Translate and Microsoft Bing Translator.) The pseudo- references are then filtered using BERT ( De- vlin et al. ,  2019 ) fine-tuned on the MPRC corpus ( Dolan and Brockett ,  2005 ), estimat- ing the probability of the paraphrase between gold reference and pseudo-references. Thanks to the high quality of the underlying MT sys- tems, a large portion of their outputs is indeed considered as a valid paraphrase. ", "page_idx": 7, "bbox": [72, 209.9104461669922, 290, 387.9586486816406], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 89, "type": "text", "text": "The final metric score is calculated sim- ply with SentBLEU with these multiple ref- erences. ", "page_idx": 7, "bbox": [72, 386.1644592285156, 290, 428.7206726074219], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 90, "type": "text", "text": "4.9 WMDO ", "text_level": 1, "page_idx": 7, "bbox": [72, 438, 142, 450], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 91, "type": "text", "text": "WMDO  ( Chow et al. ,  2019b ) is a metric based on distance between distributions in the se- mantic vector space. Matching in the seman- tic space has been investigated for translation evaluation, but the constraints of a transla- tion’s word order have not been fully explored. Building on the Word Mover’s Distance metric and various word embeddings,  WMDO  intro- duces a fragmentation penalty to account for fluency of a translation. This word order ex- tension is shown to perform better than stan- dard WMD, with promising results against other types of metrics. ", "page_idx": 7, "bbox": [72, 453.19244384765625, 290, 631.24072265625], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 92, "type": "text", "text": "4.10 YiSi-0, YiSi-1, YiSi-1_srl, YiSi-2, YiSi-2_srl ", "text_level": 1, "page_idx": 7, "bbox": [71, 640, 290, 666], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 93, "type": "text", "text": "YiSi ( Lo ,  2019 ) is a unified semantic MT qual- ity evaluation and estimation metric for lan- guages with different levels of available re- sources. ", "page_idx": 7, "bbox": [72, 669.261474609375, 290, 725.36767578125], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 94, "type": "text", "text": "YiSi-1 is a MT evaluation metric that mea- sures the semantic similarity between a ma- chine translation and human references by aggregating the idf-weighted lexical semantic similarities based on the contextual embed- dings extracted from BERT and optionally in- corporating shallow semantic structures (de- noted as YiSi-1_srl). ", "page_idx": 7, "bbox": [72, 723.572509765625, 290, 766.1296997070312], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 95, "type": "text", "text": "", "page_idx": 7, "bbox": [307, 61.472442626953125, 525, 131.1276397705078], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 96, "type": "text", "text": "YiSi-0 is the degenerate version of YiSi-1 that is ready-to-deploy to any language. It uses longest common character substring to measure the lexical similarity. ", "page_idx": 7, "bbox": [307, 129.4864501953125, 525, 185.5916290283203], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 97, "type": "text", "text": "YiSi-2 is the bilingual, reference-less version for MT quality estimation, which uses the con- textual embeddings extracted from BERT to evaluate the crosslingual lexical semantic simi- larity between the input and MT output. Like YiSi-1, YiSi-2 can exploit shallow semantic structures as well (denoted as YiSi-2_srl). ", "page_idx": 7, "bbox": [307, 183.95144653320312, 525, 280.7046203613281], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 98, "type": "text", "text": "4.11 QE Systems ", "text_level": 1, "page_idx": 7, "bbox": [307, 292, 407, 303], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 99, "type": "text", "text": "In addition to the submitted standard metrics, 10 quality estimation systems were submitted to the “QE as a Metric” track. The submitted QE systems are evaluated in the same settings as metrics to facilitate comparison. Their de- scriptions can be found in the Findings of the WMT 2019 Shared Task on Quality Estima- tion ( Fonseca et al. ,  2019 ). ", "page_idx": 7, "bbox": [307, 305.9904479980469, 525, 416.2926940917969], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 100, "type": "text", "text": "5 Results ", "text_level": 1, "page_idx": 7, "bbox": [307, 427, 370, 440], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 101, "type": "text", "text": "We discuss system-level results for news task systems in Section  5.1 . The segment-level re- sults are in Section  5.2 . ", "page_idx": 7, "bbox": [307, 446.33544921875, 525, 488.8916320800781], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 102, "type": "text", "text": "5.1 System-Level Evaluation ", "text_level": 1, "page_idx": 7, "bbox": [306, 499, 470, 511], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 103, "type": "text", "text": "As in previous years, we employ the Pearson correlation (  $r$  ) as the main evaluation measure for system-level metrics. The Pearson correla- tion is as follows: ", "page_idx": 7, "bbox": [307, 514.1774291992188, 525, 570.2836303710938], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 104, "type": "equation", "text": "\n$$\nr={\\frac{\\sum_{i=1}^{n}(H_{i}-{\\overline{{H}}})(M_{i}-{\\overline{{M}}})}{{\\sqrt{\\sum_{i=1}^{n}(H_{i}-{\\overline{{H}}})^{2}}}{\\sqrt{\\sum_{i=1}^{n}(M_{i}-{\\overline{{M}}})^{2}}}}}\n$$\n ", "text_format": "latex", "page_idx": 7, "bbox": [331, 588, 501, 621], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 105, "type": "text", "text": "where    $H_{i}$   are human assessment scores of all systems in a given translation direction,    $M_{i}$  are the corresponding scores as predicted by a given metric.  $\\overline{H}$   and    $\\overline{{M}}$   are their means, respectively. ", "page_idx": 7, "bbox": [307, 628.46044921875, 525, 698.1156005859375], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 106, "type": "text", "text": "Since some metrics, such as  BLEU , aim to achieve a strong positive correlation with hu- man assessment, while error metrics, such as TER , aim for a strong negative correlation we compare metrics via the absolute value  $|r|$   of a ", "page_idx": 7, "bbox": [307, 696.4744262695312, 525, 766.128662109375], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 107, "type": "table", "page_idx": 8, "img_path": "layout_images/W19-5302_2.jpg", "bbox": [93, 185, 503, 588], "page_size": [595.280029296875, 841.8900146484375], "ocr_text": "de-en fi-en  gu-en  kk-en  It-en ru-en zh-en\nn 16 12 11 11 11 14 15\nCorrelation |r| |r| Ir| |r| |r| |r| |r\nBEER 0.906 0.993 0.952 0.986 0.947 0.915 0.942\nBERTR 0.926 0.984 0.938 0.990 0.948 0.971 0.974\nBLEU 0.849 0.982 0.834 0.946 0.961 0.879 0.899\nCDER 0.890 0.988 0.876 0.967 0.975 0.892 0.917\nCHARACTER 0.898 0.990 0.922 0.953 0.955 0.923 0.943\nCHRF 0.917 0.992 0.955 0.978 0.940 0.945 0.956\nCHRF+ 0.916 0.992 0.947 0.976 0.940 0.945 0.956\nEED 0.903 0.994 0.976 0.980 0.929 0.950 0.949\nESIM 0.941 0.971 0.885 0.986 0.989 0.968 0.988\nHLEPORA_ BASELINE = a aa 0.975 = a 0.947\nHLEPORB_ BASELINE _ = = 0.975 0.906 = 0.947\nMETEOR++_2.0(SYNTAX) 0.887 0.995 0.909 0.974 0.928 0.950 0.948\nMETEOR _2.0(SYNTAX+coPy) 0.896 0.995 0.900 0.971 0.927 0.952 0.952\nNIST 0.813 0.986 0.930 0.942 0.944 0.925 0.921\nPER 0.883 0.991 0.910 0.737 0.947 0.922 0.952\nPREP 0.575 0.614 0.773 0.776 0.494 0.782 0.592\nSACREBLEU.BLEU 0.813 0.985 0.834 0.946 0.955 0.873 0.903\nSACREBLEU.cHRF 0.910 0.990 0.952 0.969 0.935 0.919 0.955\nTER 0.874 0.984 0.890 0.799 0.960 0.917 0.840\nWER 0.863 0.983 0.861 0.793 0.961 0.911 0.820\nWMDO 0.872 0.987 0.983 0.998 0.900 0.942 0.943\nYISI-O 0.902 0.993 0.993 0.991 0.927 0.958 0.937\nYISE-1 0.949 0.989 0.924 0.994 0.981 0.979 0.979\nYISI-1_ SRL 0.950 0.989 0.918 0.994 0.983 0.978 0.977\nQE as a Metric:\nIBM1-MORPHEME 0.345 0.740 aa = 0.487 a —\nIBM1-POS4GRAM 0.339 = = _ = = =\nLASIM 0.247 = = _ = 0.310 =\nLP 0.474 — _ _ _ 0.488 —\nUNI 0.846 0.930 = — = 0.805 =\nUNI+ 0.850 0.924 = — = 0.808 =\nYISI-2 0.796 0.642 0.566 0.324 0.442 0.339 0.940\nYISI-2_SRL 0.804 a oa - = a 0.947\n\nnewstest2019\n\n", "vlm_text": "The table provides correlation values measuring the performance of various metrics in natural language processing tasks across different language pairs. These metrics are evaluated using Pearson correlation coefficients (`|r|`) for language pairs translating into English (en) from German (de), Finnish (fi), Gujarati (gu), Kazakh (kk), Lithuanian (lt), Russian (ru), and Chinese (zh). \n\nThe metrics are divided into two main categories: \n\n1. **General Metrics:** These include BEER, BERTr, BLEU, CDER, CHRF (Character F), ESIM, NIST, PER, TER, and others. Each metric has a correlation value indicating its performance on a specific language pair.\n\n2. **QE as a Metric:** This set includes IBM1-Morpheme, IBM1-POS4Gram, LASIM, LP, UNI, UNI+, and Yisi variations. These metrics are marked especially in QE (Quality Estimation) tasks which might be used to evaluate the quality of translations without the need for reference translations.\n\nThe numbers under each language pair represent the strength of the correlation between the metric score and human judgments of translation quality for that specific translation direction. Higher correlation values (closer to 1) indicate better alignment with human judgments.\n\nThe table caption is \"None,\" indicating there might not be a provided textual description for the table. The data pertains to \"newstest2019,\" likely indicating it's based on translation tasks or competitions from that year."}
{"layout": 108, "type": "table", "page_idx": 9, "img_path": "layout_images/W19-5302_3.jpg", "bbox": [100, 199, 499, 575], "page_size": [595.280029296875, 841.8900146484375], "ocr_text": "en-cs en-de en-fi en-gu en-kk_ en-It en-ru_ en-zh\nn 11 22 12 11 iL 12 12 12\nCorrelation |r| |r| |r| |r| Ir| |r| |r| |r|\nBEER 0.990 0.983 0.989 0.829 0.971 0.982 0.977 0.803\nBLEU 0.897 0.921 0.969 0.737 0.852 0.989 0.986 0.901\nCDER 0.985 0.973 0.978 0.840 0.927 0.985 0.993 0.905\nCHARACTER 0.994 0.986 0.968 0.910 0.936 0.954 0.985 0.862\nCHRF 0.990 0.979 0.986 0.841 0.972 0.981 0.943 0.880\nCHRF+ 0.991 0.981 0.986 0.848 0.974 0.982 0.950 0.879\nEED 0.993 0.985 0.987 0.897 0.979 0.975 0.967 0.856\nESIM - 0.991 0.957 - 0.980 0.989 0.989 0.931\nHLEPORA_ BASELINE - - - 0.841 0.968 - - -\nHLEPORB_ BASELINE _ - - 0.841 0.968 0.980 - -\nNIST 0.896 0.321 0.971 0.786 0.930 0.993 0.988 0.884\nPER 0.976 0.970 0.982 0.839 0.921 0.985 0.981 0.895\nSACREBLEU.BLEU 0.994 0.969 0.966 0.736 0.852 0.986 0.977 0.801\nSACREBLEU.cHRF 0.983 0.976 0.980 0.841 0.967 0.966 0.985 0.796\nTER 0.980 0.969 0.981 0.865 0.940 0.994 0.995 0.856\nWER 0.982 0.966 0.980 0.861 0.939 0.991 0.994 0.875\nYISI-0 0.992 0.985 0.987 0.863 0.974 0.974 0.953 0.861\nYISr-1 0.962 0.991 0.971 0.909 0.985 0.963 0.992 0.951\nYISI-1__ SRL - 0.991 - - - - = 0.948\nQE as a Metric:\nIBM1-MORPHEME 0.871 0.870 0.084 = = 0.810 = =\nIBM1-POS4GRAM = 0.393 = = = = = =\nLASIM - 0.871 = = os ae 0.823 aa\nLP = 0.569 = = = = 0.661 —\nUNI 0.028 0.841 0.907 = aa = 0.919 =\nUNI+ = = i = = S 0.918 =\nUSFD = 0.224 = = os ae 0.857 aa\nUSFD-TL = 0.091 = = — = 0.771 =\nYISI-2 0.324 0.924 0.696 0.314 0.339 0.055 0.766 0.097\nYISI-2_ SRL - 0.936 - - - - - 0.118\n\nnewstest2019\n", "vlm_text": "This table presents performance metrics for different language pairs on a test dataset labeled as \"newstest2019\". It is divided into two main parts: \n\n1. **Correlation**: This section displays the correlation results of various evaluation metrics across multiple language pairs. Each language pair has two columns: \n   - The first indicates the type of correlation or metric applied (e.g., BEER, BLEU, etc.), showing its value.\n   - The second shows the absolute value of the correlation (|r|).\n\n   The languages are represented by their codes (e.g., en-cs for English-Czech, en-de for English-German, etc.). Some notable metrics in this section include:\n   - **BEER**: Showing high correlation across most language pairs except en-gu.\n   - **BLEU**: Generally high correlation, slightly lower for en-gu.\n   - **sacreBLEU.BLEU** and **sacreBLEU.chrF**: Two variations of the BLEU metric, also displaying strong correlations. \n\n2. **QE as a Metric**: This section lists Quality Estimation metrics used for evaluation, such as IBM1-Morpheme, LASIM, and YISI-2. These metrics typically have lower correlation values compared to the \"Correlation\" metrics, indicating weaker predictive power for these language pairs.\n\nThe correlation values are bolded if they are particularly strong in relation to other metrics, highlighting the best-performing metrics for each language pair. This table is used to assess the reliability and applicability of different metrics for machine translation evaluations across diverse linguistic contexts."}
{"layout": 109, "type": "text", "text": "Table 4: Absolute Pearson correlation of out-of-English system-level metrics with DA human assessment in newstest2019; correlations of metrics not significantly outperformed by any other for that language pair are highlighted in bold. ", "page_idx": 9, "bbox": [72, 578.5701293945312, 525, 616.59814453125], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 110, "type": "image", "page_idx": 10, "img_path": "layout_images/W19-5302_4.jpg", "img_caption": "Figure 1: System-level metric significance test results for DA human assessment for into English and out-of English language pairs (newstest2019): Green cells denote a statistically significant increase in correlation with human assessment for the metric in a given row over the metric in a given column according to Williams test. ", "bbox": [70, 65, 528, 750], "page_size": [595.280029296875, 841.8900146484375], "ocr_text": "an\nbe aro\n\n20 syntax\n\nAe\nLeu aLEu\n\ne-\n\nee\n\nisis\n|\nSaree,\nwana\n\nMeteor..20:5\n\n. Meteor. 20 syntax coy.\n\nHes\n2osyniax\n. NEB balling\nPLEPORS baseline\nYio ar\nMido\nQarerer\neee\nYeo\nYeo\nxP\n\nOER,\nSeegpueuateu\nat\n\neR\nWen,\nBase\n\nmB\n\nBEER\nEED\nYisio\nch\nchef.\nPER\nTER\na ‘sacreBLEU.chrF\nWER\nDER\nVisit\nNIST\nBLEU\nCharacTER\nsacreBLEU.BLEU\nESIM\nUNI\nYisi2\nibmt-morpheme\n\nWER\nDER\nVis\nNIST\nBLEU\nUNI\n\nCharacTER\n\n‘sacreBLEU.BLEU\nESIM\n\nsacreBLEU.chrF\n\nvis\njbm1.morpheme\n\nic}\n5\n\noS\n*\n\nTER\nnist\n\nWER\n\nESIM\n\nBLEU\nsacreBLEU.BLEU\nCbER\n\nPER\n\nBEER\n\noni,\n\noh\nhLEPORD_baseline\nEED\n\nYisi.o\nsacreBLEU.chiF\nVisit\n\nCharacTER\n\nibm! -morpheme\nYisi2\n\nTER\nNisT\n\nWER\n\nESIM\n\nBLEU\n‘sacreBLEU.BLEU\nPER\n\nBEER\n\nEED\n\nYisi.o\nsacreBLEU.chvF\n\nVisi\n\nCharacTER,\nbt morpheme\nYisi2\n\n‘CDER\n\nALEPORD_basel\n\nsscroBLEUBLEU\n“GharsctER\n\nROGSEELELEGE gaye\nwigs by eB esha gaze\nzB gq 2\ngo 3 Z\n; : 5\nen-gu\n: :\na\nGoghebeseesagebage\nQHROS soos sack 2 ase\ngargs 2dae\n: gee 'G\né @ ge a\n8 56\nge\nen-ru\n\n=\nPel\na\nUN!\nUN\nUSED\n\ngu-en\n\n_20.syntax\nMeteor. -2.0.yntax copy.\nqeR oh\nesi\n\nG5eR\n\nWer\n\nBLEU\nSacreBLEUSLEU\nPree\nMisia\n\ngt 7 ag ey oo\na and q\nru-en\n\noe\nPreatcucne\nEa\nHor. 2\nNe FONE ay\nAero tate\nwe\nme\nPepe\nFee\n3\nae\na\nr\ni\ni\nen-de\nsacreBLEUBLEU ea\nYisio ep\"\niS\noh ea\nSEER E\nshe\nshir ‘SacreBLEU.chrF\nGER eet\nSeer ‘sacreBLEU.BLEU\nwer . ig\nPER VE on\nsus ee\nner Bayt morphome\nNIST ire\nIbe morpheme Be eran\nvsi2 fees\nun USFD.TL\nSS BOE Ba GEC\nespe a8\n8 EE:\ni 4\nen-kk\nCharacTER: = ee YiSi.A\nvisu esi\nEED = EED\neR\nYiSi.O Ld YiSiO\nWER Lt che\none Sten\nsacreBLEU.chrF @ LEPORa baseline\none LEPORD-bassine\nRLEPORa baseine acroSLEU. GW\nhLEPORb baseline Wek\ncoer CharacTER:\neae NIST.\nBeer NST,\nNIST coe\nBLED sacreBLEU.BLEU\nsacreBLEUBLEU Pid\nYS vee\nL2LEEEDESAaS\nea eee\n8\nBg 3\ne232 6 ¢\nS55\na\nen-zh\nve sia\nBER. “iSi.1_srl\nser Ei\nser\neit coer\nae BLEU\nCharacren hee\nSSSHTEL owe Nst\nfaereBLEUBLEU che\nae Wer\nEs CharacTER\nMiso veo\niH EED\noi FED\nBEER\nsacroBLEU BLEU\ntaste Seeresle Use\nYeDT. YiSi.2_srl\nWS (Siz\nty vee\n\nsacreBLEU BLEU\n‘sacreBLEU.chrF\n", "vlm_text": "The image shows a series of heatmaps representing system-level metric significance test results for direct assessment (DA) human evaluation of machine translation metrics. Each subplot corresponds to a specific language pair, such as de-en (German to English), fi-en (Finnish to English), etc.\n\n- The rows and columns represent different translation evaluation metrics.\n- Green cells indicate a statistically significant improvement in correlation with human assessment for the metric in the row over the metric in the column, determined using Williams' test.\n- The intensity of the green color may suggest the level of significance, though this is not explicitly stated.\n\nThis visualization is comparing how well different metrics correlate with human judgments for translations into and out of English across various language pairs."}
{"layout": 111, "type": "text", "text": "given metric’s correlation with human assess- ment. ", "page_idx": 11, "bbox": [71, 61.472442626953125, 290, 90.4796371459961], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 112, "type": "text", "text": "5.1.1 System-Level Results ", "text_level": 1, "page_idx": 11, "bbox": [71, 99, 227, 112], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 113, "type": "text", "text": "Tables  3 ,  4  and  5  provide the system-level cor- relations of metrics evaluating translation of newstest2019. The underlying texts are part of the WMT19 News Translation test set (new- stest2019) and the underlying MT systems are all MT systems participating in the WMT19 News Translation Task. ", "page_idx": 11, "bbox": [71, 112.8524398803711, 290, 209.60562133789062], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 114, "type": "text", "text": "As recommended by  Graham and Bald- win  ( 2014 ), we employ Williams significance test ( Williams ,  1959 ) to identify differences in correlation that are statistically significant. Williams test is a test of significance of a dif- ference in dependent correlations and there- fore suitable for evaluation of metrics. Corre- lations not significantly outperformed by any other metric for the given language pair are highlighted in bold in Tables  3 ,  4  and  5 . ", "page_idx": 11, "bbox": [71, 207.9474334716797, 290, 345.3486328125], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 115, "type": "text", "text": "Since pairwise comparisons of metrics may be also of interest, e.g. to learn which metrics significantly outperform the most widely em- ployed metric  BLEU , we include significance test results for every competing pair of metrics including our baseline metrics in Figure  1  and Figure  2 . ", "page_idx": 11, "bbox": [71, 343.689453125, 290, 440.4436340332031], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 116, "type": "text", "text": "This year, the increased number of systems participating in the news tasks has provided a larger sample of system scores for testing met- rics. Since we already have sufficiently con- clusive results on genuine MT systems, we do not need to generate hybrid system results as in  Graham and Liu  ( 2016 ) and past metrics tasks. ", "page_idx": 11, "bbox": [71, 438.784423828125, 290, 549.0875854492188], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 117, "type": "text", "text": "5.2 Segment-Level Evaluation ", "text_level": 1, "page_idx": 11, "bbox": [71, 559, 242, 571], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 118, "type": "text", "text": "Segment-level evaluation relies on the man- ual judgements collected in the News Trans- lation Task evaluation. This year, again we were unable to follow the methodology out- lined in  Graham et al.  ( 2015 ) for evaluation of segment-level metrics because the sampling of sentences did not provide sufficient number of assessments of the same segment. We there- fore convert pairs of DA scores for compet- ing translations to  daRR  better/worse prefer- ences as described in Section  2.3.2 . ", "page_idx": 11, "bbox": [71, 574.2804565429688, 290, 725.2306518554688], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 119, "type": "text", "text": "We measure the quality of metrics’ segment- level scores against the  daRR  golden truth us- ing a Kendall’s Tau-like formulation, which is an adaptation of the conventional Kendall’s Tau coefficient. Since we do not have a to- tal order ranking of all translations, it is not possible to apply conventional Kendall’s Tau ( Graham et al. ,  2015 ). ", "page_idx": 11, "bbox": [71, 723.5724487304688, 290, 766.129638671875], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 120, "type": "text", "text": "", "page_idx": 11, "bbox": [307, 61.472442626953125, 525, 131.1276397705078], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 121, "type": "text", "text": "Our Kendall’s Tau-like formulation,    $\\tau$  , is as follows: ", "page_idx": 11, "bbox": [307, 129.4014434814453, 525, 158.40863037109375], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 122, "type": "equation", "text": "\n$$\n\\tau=\\frac{|C o n c o r d a n t|-|D i s c o r d a n t|}{|C o n c o r d a n t|+|D i s c o r d a n t|}\n$$\n ", "text_format": "latex", "page_idx": 11, "bbox": [335, 165, 498, 194], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 123, "type": "text", "text": "where  Concordant  is the set of all human com- parisons for which a given metric suggests the same order and  Discordant  is the set of all human comparisons for which a given metric disagrees. The formula is not specific with re- spect to ties, i.e. cases where the annotation says that the two outputs are equally good. ", "page_idx": 11, "bbox": [307, 201.72544860839844, 525, 298.4786376953125], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 124, "type": "text", "text": "The way in which ties (both in human and metric judgement) were incorporated in com- puting Kendall    $\\tau$   has changed across the years of WMT Metrics Tasks. Here we adopt the version used in WMT17  daRR  evaluation. For a detailed discussion on other options, see also  Macháček and Bojar  ( 2014 ). ", "page_idx": 11, "bbox": [307, 296.75244140625, 525, 393.5056457519531], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 125, "type": "text", "text": "Whether or not a given comparison of a pair of distinct translations of the same source in- put, s 1  and s 2 , is counted as a concordant (Conc) or disconcordant (Disc) pair is defined by the following matrix: ", "page_idx": 11, "bbox": [307, 391.7794494628906, 525, 461.4346618652344], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 126, "type": "table", "page_idx": 11, "img_path": "layout_images/W19-5302_5.jpg", "bbox": [316, 466, 520, 536], "page_size": [595.280029296875, 841.8900146484375], "ocr_text": "Metric\n\nSi <s2 Si =s2 S81 > S82\nE $1 < 82 | Conc Disc Disc\nE 8S; = 89\n= sj >s2| Disc Disc Conc\n", "vlm_text": "The table presents a comparison between human judgments and metric evaluations regarding the size relation of two entities, denoted as \\(s_1\\) and \\(s_2\\). The table is organized into rows and columns:\n\n- The leftmost column represents human judgments about the relationship of sizes \\(s_1\\) and \\(s_2\\): whether \\(s_1 < s_2\\), \\(s_1 = s_2\\), or \\(s_1 > s_2\\).\n- The top row indicates metric evaluations regarding the same comparison: \\(s_1 < s_2\\), \\(s_1 = s_2\\), or \\(s_1 > s_2\\).\n\nThe intersection of each row and column provides the result of this comparison:\n- \"Conc\" (concordant) indicates that the human judgment and metric evaluation agree.\n- \"Disc\" (discordant) indicates a disagreement between the human judgment and metric evaluation.\n- A dash (\"-\") indicates scenarios that are not applicable or where no specific information is provided.\n\nHere's the breakdown:\n- When humans think \\(s_1 < s_2\\) and the metric agrees (\\(s_1 < s_2\\)), it is \"Conc\".\n- When humans think \\(s_1 < s_2\\) but the metric disagrees (\\(s_1 = s_2\\) or \\(s_1 > s_2\\)), it is \"Disc\".\n- When humans think \\(s_1 = s_2\\), no matter what the metric indicates (\\(s_1 < s_2\\), \\(s_1 = s_2\\), \\(s_1 > s_2\\)), the outcome is unspecified (\"-\").\n- When humans think \\(s_1 > s_2\\) and the metric disagrees (\\(s_1 < s_2\\) or \\(s_1 = s_2\\)), it is \"Disc\".\n- When humans think \\(s_1 > s_2\\) and the metric agrees (\\(s_1 > s_2\\)), it is \"Conc\"."}
{"layout": 127, "type": "text", "text": "In the notation of  Macháček and Bojar ( 2014 ), this corresponds to the setup used in WMT12 (with a different underlying method of manual judgements, RR): ", "page_idx": 11, "bbox": [307, 537.1124267578125, 525, 593.2186279296875], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 128, "type": "equation", "text": "", "text_format": "latex", "page_idx": 11, "bbox": [357, 596, 474, 668], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 129, "type": "text", "text": "The key differences between the evaluation used in WMT14–WMT16 and evaluation used in WMT17–WMT19 were (1) the move from RR to daRR and (2) the treatment of ties. In the years 2014-2016, ties in metrics scores were not penalized. With the move to daRR, where the quality of the two candidate translations ", "page_idx": 11, "bbox": [307, 669.3754272460938, 525, 766.129638671875], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 130, "type": "table", "page_idx": 12, "img_path": "layout_images/W19-5302_6.jpg", "bbox": [192, 99, 405, 401], "page_size": [595.280029296875, 841.8900146484375], "ocr_text": "de-cs de-fr_ fr-de\nn 11 11 10\nCorrelation |r| Ir| |r|\nBEER 0.978 0.941 0.848\nBLEU 0.941 0.89 0.864\nCDER 0.864 0.949 0.852\nCHARACTER 0.965 0.928 0.849\nCHRF 0.974 0.93 0.864\nCHRF+ 0.972 0.936 0.848\nEED 0.982 0.940 0.851\nESIM 0.980 0.950 0.942\nHLEPORA_BASELINE 0.941 0.814 -\nHLEPORB_BASELINE 0.959 0.814 -\nNIST 0.954 0.916 0.862\nPER 0.875 0.857 0.899\nSACREBLEU-BLEU 0.869 0.89 0.869\nSACREBLEU-CHRF 0.975 0.952 0.882\nTER 0.890 0.956 0.895\nWER 0.872 0.956 0.894\nYISI-0 0.978 0.952 0.820\nYISr-1 0.973 0.969 0.908\nYISI-1__ SRL - - 0.912\nQE as a Metric:\nIBM1-MORPHEME 0.355 0.509 0.625\nIBM1-POS4GRAM = 0.085 0.478\nYISI-2 0.606 0.721 0.530\n\nnewstest2019\n\n", "vlm_text": "This table presents correlation data for different metrics used to evaluate machine translation. It provides absolute correlation values (|r|) between various evaluation metrics and human judgments for translations between three language pairs: German-Czech (de-cs), German-French (de-fr), and French-German (fr-de). \n\n### Language Pairs and Sample Sizes:\n- **de-cs:** 11 samples\n- **de-fr:** 11 samples\n- **fr-de:** 10 samples\n\n### Evaluation Metrics:\nThe table includes correlation values for a variety of metrics:\n1. **BEER:** \n   - de-cs: 0.978\n   - de-fr: 0.941\n   - fr-de: 0.848\n2. **BLEU:** \n   - de-cs: 0.941\n   - de-fr: 0.891\n   - fr-de: 0.864\n3. **CDER:** \n   - de-cs: 0.864\n   - de-fr: 0.949\n   - fr-de: 0.852\n4. **CHARACTER:** \n   - de-cs: 0.965\n   - de-fr: 0.928\n   - fr-de: 0.849\n5. **chrF:** \n   - de-cs: 0.974\n   - de-fr: 0.931\n   - fr-de: 0.864\n6. **chrF+:** \n   - de-cs: 0.972\n   - de-fr: 0.936\n   - fr-de: 0.848\n7. **EED:**\n   - de-cs: 0.982\n   - de-fr: 0.940\n   - fr-de: 0.851\n8. **ESIM:** \n   - de-cs: 0.980\n   - de-fr: 0.950\n   - fr-de: 0.942\n9. **hLEPORA__baseline:**\n   - de-cs: 0.941\n   - de-fr: 0.814\n   - fr-de: (not available)\n10. **hLEPORB__baseline:** \n    - de-cs: 0.959\n    - de-fr: 0.814\n    - fr-de: (not available)\n11. **NIST:** \n    - de-cs: 0.954\n    - de-fr: 0.916\n    - fr-de: 0.862\n12. **PER:** \n    - de-cs: 0.875\n    - de-fr: 0.857\n    - fr-de: 0.899\n13. **SacreBLEU-BLEU:** \n    - de-cs: 0.869\n    -"}
{"layout": 131, "type": "text", "text": "Table 5: Absolute Pearson correlation of system-level metrics for language pairs not involving English with DA human assessment in newstest2019; correlations of metrics not significantly outperformed by any other for that language pair are highlighted in bold. ", "page_idx": 12, "bbox": [72, 407.2081604003906, 525, 445.2351379394531], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 132, "type": "image", "page_idx": 12, "img_path": "layout_images/W19-5302_7.jpg", "img_caption": "Figure 2: System-level metric significance test results for DA human assessment in newstest2019 for German to Czech, German to French and French to German; green cells denote a statistically significant increase in correlation with human assessment for the metric in a given row over the metric in a given column according to Williams test. ", "bbox": [71, 531, 527, 718], "page_size": [595.280029296875, 841.8900146484375], "ocr_text": "Sor bem\n\nGaegeea\n\naggesg\n8\n\nde-cs\n\nsacreBLEU.BLEU\n\nCDER\n\nYisi2\n‘bmt morpheme\n\nED\nESIM\n\nYisi.o\n\nBEER\nsacreBLEU.cheF\none\n\nVisit\n\noh.\n\nCharacter\nRLEPORD_baseline\nNIST\n\nBLEU\nRLEPORa_baseline\nTER\n\nPER\n\nWER\nsacreBLEU.BLEU\nCODER\n\nYisi2\nibmi-morpheme\n\nYisi\n\nde-fr\n\nWER\nTER\n\nYisi.o\nsacreBLEU.cnr\nSIM\n\nBEER\n\nED\n\nohr.\n\ncue\n\nCharacTER\nNIST\n\nBLEU\nsacreBLEU.BLEU\nPER\n\nDER\n‘bt morpheme\nTomi posagram\n\nYsia\n\nALEPORa_baseline\nYisi2\nibmi.morpheme\nTbmt pos4gram\n\nsacreBLEU.chtF\n\nfr-de\n\nates\n\nVisio\n\nBEER\nchr:\nlbmt.morpheme\n\nCharacTER\n\nVisi2\n\nibmmt pos4gram\n\nESIM\nYisi.1_set\n\nVisit\n\nPER\n\nTER\n\nWER\nsacreBLEU.chiF\nsacreBLEU.BLEU\nchee\n\nBLEU\n\nNIST.\n\nCDER\n\nEED\n\nCharacTER\nBEER\n\nont,\n\nVisio\nibmt.morpheme\nYisi2\nibmi:pos4gram\n", "vlm_text": "The image shows three heatmap-style figures for system-level metric significance test results concerning DA human assessment in the newstest2019 for translations from German to Czech (de-cs), German to French (de-fr), and French to German (fr-de). The figures compare various automatic evaluation metrics in terms of their correlation with human assessments. Each figure is a matrix where rows and columns represent different evaluation metrics, such as EED, ESIM, BLEU, and others.\n\nIn the heatmaps, the cells are colored green to indicate where there is a statistically significant increase in correlation with human assessment for the metric in the row compared to the metric in the column, according to Williams' test. If a cell is gray, it suggests no statistically significant increase in correlation for the corresponding comparison. For instance, in the de-cs (German to Czech) heatmap, the Y-axis lists metrics like EED, ESIM, and more, while the X-axis lists the same metrics horizontally, marking intersections with green if a significant difference is noted. Similar structures and interpretations apply to the de-fr and fr-de heatmaps."}
{"layout": 133, "type": "table", "page_idx": 13, "img_path": "layout_images/W19-5302_8.jpg", "bbox": [86, 229, 511, 544], "page_size": [595.280029296875, 841.8900146484375], "ocr_text": "de-en fi-en gu-en_ kk-en lIt-en ru-en zh-en\nHuman Evaluation DARR DARR  DARR DARR- DARR- DARR- DARR\nn 85,365 38,307 31,139 27,094 21,862 46,172 31,070\nBEER 0.128 0.283 0.260 0.421 0.315 0.189 0.371\nBERTR 0.142 0.33 0.291 0.421 0.353 0.195 0.399\nCHARACTER 0.101 0.253 0.190 0.340 0.254 0.155 0.337\nCHRF 0.122 0.286 0.256 0.389 0.301 0.180 0.371\nCHRF+ 0.125 0.289 0.257 =—-0.394_ (0.303 0.182 0.374\nEED 0.120 0.28 0.264 = =0.392 0.298 0.176 0.376\nESIM 0.167 0.337 0.303 0.435 0.359 0.201 0.396\nHLEPORA_ BASELINE = = = 0.372 = = 0.339\nMETEOR++_ 2.0(SYNTAX) 0.084 0.274 0.237 =-0.395 (0.291 0.156 0.370\nMETEOR++_2.0(SYNTAX+COPY) 0.094 0.273 0.244 0.402 0.287 0.163 0.367\nPREP 0.030 0.197 0.192 0.386 0.193 0.124 = 0.267\nSENTBLEU 0.056 = 0.233 0.188 0.377 0.262 0.125 0.323\nWMDO 0.096 0.28 0.260 0.420 (0.300 0.162 0.362\nYISI-0 0.117 0.27 0.263 0.402 ~—0.289 0.178 = 0.355\nYISE-1 0.164 0.347 0.312 0.440 0.376 0.217 0.426\nYISI-1_ SRL 0.199 0.346 0.306 0.442 0.380 0.222 0.431\nQE as a Metric:\nIBM1-MORPHEME —0.074 0.009 = —- 0.069 — _\nIBM1-POS4GRAM —0.153 - = = = — —\nLASIM —0.024 i = = ome 0.022 _\nLP —0.096 - - - — —0.035 -\nUNI 0.022 0.202 ~ = — 0.084 i\nUNI+ 0.015 0.211 _ = — 0.089 =\nYISL-2 0.068 0.126 —0.001 0.096 = 0.075 0.053 0.253\nYISI-2_ SRL 0.068 - — - — — 0.246\n\nnewstest2019\n", "vlm_text": "The table presents evaluation metrics for different language pairs in machine translation, focusing on human evaluation (DArr) and QE (Quality Estimation) as a metric. Each section of the table contains measurements of various metrics applied to translated text to assess its quality and accuracy.\n\n1. **Language Pairs and DARR Values**:\n   - Language pairs are listed at the top as column headers (de-en, fi-en, gu-en, kk-en, lt-en, ru-en, zh-en).\n   - Each pair has a corresponding `n` value, representing the number of data points used in their evaluation (e.g., 85,365 for de-en).\n\n2. **Evaluation Metrics**:\n   - The first segment of the table covers the `Human Evaluation` metrics and their corresponding scores for each language pair.\n   - Lists different metrics such as BEER, BERTr, Character, chrF, chrF+, EED, and others, giving a score for each language pair.\n   - The bolded numbers indicate the highest score for each language pair among those evaluation metrics.\n\n3. **QE as a Metric**:\n   - Contains various methods used for Quality Estimation, including IBM1-Morpheme, IBM1-Pos4Gram, LASIM, LP, UNI, UNI+, YiSi-2, and YiSi-2_SRl.\n   - Provides scores for each metric regarding their effectiveness in quality estimation.\n\n4. **Data and Evaluation Notes**:\n   - The data set \"newstest2019\" is mentioned at the bottom, indicating the source of the evaluation data.\n   - Certain metrics do not have scores for specific language pairs (represented by dashes), indicating either a lack of data or non-applicability of that metric for those pairs.\n\nThe table is essentially comparing various automatic evaluation metrics and quality estimation methods for assessing translations from several input languages to English, using the \"newstest2019\" dataset."}
{"layout": 134, "type": "text", "text": "Table 6: Segment-level metric results for to-English language pairs in newstest2019: absolute Kendall’s Tau formulation of segment-level metric scores with DA scores; correlations of metrics not significantly outperformed by any other for that language pair are highlighted in bold. ", "page_idx": 13, "bbox": [72, 549.7991333007812, 525, 587.8271484375], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 135, "type": "table", "page_idx": 14, "img_path": "layout_images/W19-5302_9.jpg", "table_footnote": "Table 7: Segment-level metric results for out-of-English language pairs in newstest2019: absolute Kendall’s Tau formulation of segment-level metric scores with DA scores; correlations of metrics not significantly outperformed by any other for that language pair are highlighted in bold. ", "bbox": [71, 61, 526, 392], "page_size": [595.280029296875, 841.8900146484375], "ocr_text": "en-cs_ en-de en-fi en-gu en-kk en-lt en-ru- en-zh\nHuman Evaluation DARR DARR  DARR DARR DARR- DARR- DARR_ DARR\nn 27,178 99,840 31,820 11,355 18,172 17,401 24,334 18,658\nBEER 0.443 0.316 0.514 0.537 0.516 0.441 0.542 0.232\nCHARACTER 0.349 0.264 0.404 0.500 0.351 0.311 0.432 0.094\nCHRF 0.455 0.326 0.514 0.534 0.479 0.446 0.539 0.301\nCHRF+ 0.458 0.327 0.514 0.538 0.491 0.448 0.543 0.296\nEED 0.431 0.315 0.508 0.568 0.518 0.425 0.546 0.257\nESIM i 0.329 0.511 = 0.510 0.428 0.572 0.339\nHLEPORA_ BASELINE - = _ 0.463 0.390 - = i\nSENTBLEU 0.367 0.248 0.396 0.465 0.392 0.334 0.469 0.270\nYISI-0 0.406 0.304 0.483 0.539 0.494 0.402 0.535 0.266\nYISr-1 0.475 0.351 0.537 0.551 0.546 0.470 0.585 0.355\nYISI-1_ SRL - 0.368 - - - — — 0.361\nQE as a Metric:\nIBM1-MORPHEME —0.135 —0.003 —0.005 = — —0.165 = =\nIBM1-POS4GRAM — 0.123 a =\nLASIM _ 0.147 = _ = = —0.24 =\nLP — -—0.119 - - = — 0.158 =\nUNI 0.060 0.129 0.351 _ — — 0.226 _\nUNI+ a a — = = 0.222 =\nUSFD — —0.029 _ = = = 0.136 =\nUSFD-TL — —0.037 = a = = 0.191 a\nYISI-2 0.069 0.212 0.239 0.147 0.187 0.003 —0.155 0.044\nYISI-2_ SRL _ 0.236 — 0.034\n\nnewstest2019\n", "vlm_text": "This table appears to be a comparison of evaluation metrics and quality estimation (QE) metrics for machine translation across several language pairs involving English (en) being translated into other languages (represented by their language codes like cs, de, fi, etc.). The table shows the performance of different evaluation metrics on translation outputs for these language pairs.\n\n1. **Headers:**\n   - It compares the human evaluation results (using the `ΔARR` metric) for different language pairs: en-cs, en-de, en-fi, en-gu, en-kk, en-lt, en-ru, and en-zh.\n   - Outputs are reported for a specified dataset/benchmark: 'newstest2019'.\n\n2. **Metrics Compared:**\n   - The table is divided into two main sections:\n     - **Human Evaluation with n (number of samples):** Lists evaluation metrics like BEER, CHRF, EED, ESIM, hLEPOR, sentBLEU, YiSi variations, and their corresponding scores for each language pair.\n     - **QE as a Metric:** Lists quality estimation metrics like IBM1-MORPHEME, IBM1-POS4GRAM, LASIM, LP, UNI, UNI+, USFD, USFD-TL, and YiSi-2 variations.\n   \n3. **Values Represented:**\n   - Each cell represents a score or a value from an evaluation metric applied to a specific language pair. These scores are likely correlations or performance figures of these metrics correlating to human judgment.\n   - The bold numbers represent the highest scores achieved by an evaluation metric for each language pair.\n\n4. **Observations:**\n   - Different metrics perform better for different language pairs.\n   - Some metrics do not have corresponding values for certain language pairs, which might indicate missing data or that the metric is not applicable there."}
{"layout": 136, "type": "table", "page_idx": 14, "img_path": "layout_images/W19-5302_10.jpg", "bbox": [70, 437, 294, 657], "page_size": [595.280029296875, 841.8900146484375], "ocr_text": "de-cs de-fr fr-de\nHuman Evaluation DARR DARR  DARR\nn 35,793 4,862 1,369\nBEER 0.337 0.293 0.265\nCHARACTER 0.232 0.251 0.224\nCHRF 0.326 0.284 0.275\nCHRF+ 0.326 0.284 0.278\nEED 0.345 0.301 0.267\nESIM 0.331 0.290 0.289\nHLEPORA_BASELINE 0.207 0.239 =\nSENTBLEU 0.203 0.235 0.179\nYISI-O 0.331 0.296 0.277\nYISE-L 0.376 0.349 0.310\nYISI-1_ SRL - - 0.299\nQE as a Metric:\nIBM 1-MORPHEME 0.048 —0.013  —0.053\nIBM1-POS4GRAM = —0.074 —0.097\nYISI-2 0.199 0.186 0.066\n\nnewstest2019\n", "vlm_text": "The table presents the performance of various evaluation metrics on translations between three language pairs: German-Czech (de-cs), German-French (de-fr), and French-German (fr-de) from the \"newstest2019\" dataset. The first row indicates the number of human evaluation samples (\\( n \\)) for each pair. The subsequent rows list different metrics and their corresponding scores for each language pair:\n\n- **Human Evaluation (n):** \n  - de-cs: 35,793\n  - de-fr: 4,862\n  - fr-de: 1,369\n\n- **Listed Metrics and their Scores:**\n  - BEER: Scores are 0.337 (de-cs), 0.293 (de-fr), 0.265 (fr-de)\n  - CHARACTER: 0.232 (de-cs), 0.251 (de-fr), 0.224 (fr-de)\n  - CHRF: 0.326 (de-cs), 0.284 (de-fr), 0.275 (fr-de)\n  - CHRF+: 0.326 (de-cs), 0.284 (de-fr), 0.278 (fr-de)\n  - EED: 0.345 (de-cs), 0.301 (de-fr), 0.267 (fr-de)\n  - ESIM: 0.331 (de-cs), 0.290 (de-fr), 0.289 (fr-de)\n  - HLEPORA_BASELINE: 0.207 (de-cs), 0.239 (de-fr), not available for fr-de\n  - SENTBLEU: 0.203 (de-cs), 0.235 (de-fr), 0.179 (fr-de)\n  - YISI-0: 0.331 (de-cs), 0.296 (de-fr), 0.277 (fr-de)\n  - YISI-1: 0.376 (de-cs), 0.349 (de-fr), 0.310 (fr-de)\n  - YISI-1_SRL: Not available for de-cs, 0.299 (de-fr), 0.299 (fr-de)\n\n**Quality Estimation (QE) as a Metric:**\n  - IBM1-MORPHEME: 0.048 (de-cs), -0.013 (de-fr), -0.053 (fr-de)\n  - IBM1-POS4GRAM: Not available for de-cs, -0.074 (de-fr), -0.097 (fr-de)\n  - YISI-2: 0.199 (de-cs), 0.186 (de-fr), 0.066 (fr-de)\n\nThe highest scores for each language pair are bolded in the table. These bold values indicate"}
{"layout": 137, "type": "text", "text": "Table 8: Segment-level metric results for language pairs not involving English in newstest2019: ab- solute Kendall’s Tau formulation of segment-level metric scores with DA scores; correlations of met- rics not significantly outperformed by any other for that language pair are highlighted in bold. ", "page_idx": 14, "bbox": [72, 662.0531616210938, 290, 735.9462280273438], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 138, "type": "text", "text": "is deemed substantially different and no ties in human judgements arise, it makes sense to penalize ties in metrics’ predictions in order to promote discerning metrics. ", "page_idx": 14, "bbox": [307, 416.6114501953125, 525, 472.7176513671875], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 139, "type": "text", "text": "Note that the penalization of ties makes our evaluation asymmetric, dependent on whether the metric predicted the tie for a pair where humans predicted    $<$  , or    $>$  . It is now impor- tant to interpret the meaning of the compar- ison identically for humans and metrics. For error metrics, we thus reverse the sign of the metric score prior to the comparison with hu- man scores: higher scores have to indicate bet- ter translation quality. In WMT19, the origi- nal authors did this for CharacTER. ", "page_idx": 14, "bbox": [307, 471.2584533691406, 525, 622.2086791992188], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 140, "type": "text", "text": "To summarize, the  WMT19  Metrics Task for segment-level evaluation: ", "page_idx": 14, "bbox": [307, 620.7484741210938, 525, 649.7556762695312], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 141, "type": "text", "text": "• ensures that error metrics are first con- verted to the same orientation as the hu- man judgements, i.e. higher score indi- cating higher translation quality, • excludes all human ties (this is already implied by the construction of  daRR from DA judgements), ", "page_idx": 14, "bbox": [315, 658.6114501953125, 525, 766.128662109375], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 142, "type": "image", "page_idx": 15, "img_path": "layout_images/W19-5302_11.jpg", "img_caption": "Figure 3:  daRR  segment-level metric significance test results for into English and out-of English language pairs (newstest2019): Green cells denote a significant win for the metric in a given row over the metric in a given column according bootstrap resampling. ", "bbox": [70, 68, 527, 748], "page_size": [595.280029296875, 841.8900146484375], "ocr_text": "de-en\n\nYisi-t_er\n\nth\n[Bmnt-morpheme\n\niP\nnt-postgram\n\nsanbeyenseg eens\n* & 55,\nSE\nPs\ni\nEs\n\nkk-en\n\nesi\n\nBente\n\nBech\n\nwoo\n\n¥S-0\n\nMeloor++ 2.0(syntaxtcopy)\nMeteor++_20(syntax)\nches\n\nEED\n\nhe\n\nPREP\n\nsentBLEU\n\nHLEPORS baseline\nCharacTER\n\nvisi-2\n\nall\n\none\nPReP\nssenfQLEU\n\nESIM\nBERT\nEER\nwnbo\nvisio\nHLEPORA baseline\nYisi2\n\ntax copy.\n\n‘ateor. 20.\n\nYisit_st\nVisit\n\nMeteor. 2.0.\nsyntan\noe\nEEO\n(CharacTER\n\nzh-en\n\nhe\nMeteor++ 2.0(syntax)\nMetoor+_2.0(syntaxtcopy)\nWMDO\n\nvisio\nALEPORa baseline\n‘CharacTER\nsentBLEU\n\nSIM\nED\none\n\nYs.\nvisit\n\neERT\n\nEER\n\noh\nMetaor._2.0.synta\nHLEPORa baseline\n\nMeteor.\n\nYisi-1\nohrF+\nBEER\n\nchrF\n\nESIM\n\nEED\nYisi-0\nCharacTER\nsentBLEU\nUNI\n\nYisi-2\nibm1—morpheme\n\nce Ek saqgrrzsae\nBEHERHSHUS as\n> oa en FE\nge [3\n55 5\nge 8\n5\nen-It\nYiSi-1\nchrF+\nchrF\nBEER\nESIM\nEED\nYiSi-0\nsentBLEU\nCharacTER:\nYiSi-2\nibm1-morpheme\nsekaes Daoa eo\ngeese gage\n5509 GHas\n- ® 276\n2 2\nos\nE\n2\n\nYiSiA\n\nfi-en\n\nal\n\nEeD\n\nMeteor. 2.0:sytax.\n\nMeteor\n\nYisis\nVisi\nBERT\n‘oh,\nene\nBEER\nwntoo\nYguo\n\nesi\n\n20 syntaccopy.\nibe morphems\n\nIt-en\n\nal\n\nYisit_t\nVisit\n\nESIM\n\nBERT\n\nBEER\n\nhi\n\nne\n\n‘wnDo\n\nED\n\n__2.0.ayntax.\n\nVisio\nMeteor._2.0.yntax.copy.\n‘senfQLEU\n\nCcharacTER\n\nPREP\n\nWisi2\n\nibm moxpheme\n\nMeteor.\n\nen-cs\n\nELE QoDENZe\nBema gadugse\na een\na aoe s\nes e\n\n82 2\n\n6 3\n\nE\n\na\n\nen-gu\n\na\n\nEED\n\nYisi.t\ncheF\n\nYisi.0\n\nchr.\n\nBEER\n\nCharacTER\n\nsentBLEU\nYisi\n\nhLEPORa_baseline\n\nen-ru\n\nESIM\nEED\nchr.\n\nBEER\nchrF\n\nYiSi.O\n\nsentBLEU\n\n7%\n\nvisit\nYisct_stt\n\nesi\n\nBERT\n\nche +\n\none\n\nBEER\n\nwutoo\n\nEED\n\nMetoores 20(eynax)\nteore+2.0syntax\nMeteor 2 olsntaxecony\nCharacter\n\nsentBLEU\n\nUne\n\nUNT\n\nPREP\n\nYisi-2\n\nlbmt-morpheme\n\nYisi-t_stt\nYs\n\nesia\n\nBERT\n\nBEER\n\none +\n\nee\n\nwutoo\n\nED\n\nMoteor++ 2.0(sytax)\nYisi-0\n\nMeteore+ 2.0(syntaxecopy)\nsentBLEU\n\nCharacTER\n\nPREP\n\nYis-2\n\nlomt-morpheme\n\nYisi-1\nhres\n\nche\n\nBEER\nEED\nYiSi-0\nsentBLEU\nCharacTER\nYiSi-2\n\nUNI\nibm1—morpheme\n\nchrFs\nBEER\n\nchrF\n\nCharacTER\nsentBLEU\nhLEPORa_baseline\nYisi-2\n\nYiSi-1\nESIM\nEED\nchrF+\nBEER\nchrF\nYisi-0\nsentBLEU\nCharacTER\nUNI\n\nUNI+\nUSFD-TL\nUSFD\nYiSi-2\n\nLP\n\nLASIM\n\nvisit\nVisits\n\nYisi.t\n\nSim\nBERT\n\nEED\nBEER\n\nED\nvisio\n\nSIM\n\nchrF\n\ngu-en\n\n-2.0(yoanscopy)\nMeteorss_2 Oey)\nPace\nChaecTER\nseniBLEU\nvere\nShetes\nPease\nBS i 2\na3\nFe\ni\n2\nru-en\n\nYisi.0\n\nchr.\nsentBLEU\n\nchrF.\n\nohrF\nsent8LEU\n\nLEPORa_baseline\n\nMeteors 2.046\nCharacTER veyetas)\nSenlteu\n\nPiae\n\nUN\nUN\nYisi-2\ncasi\ne\n\nCharacTER\nsent6LEU\nYisi-2 stl\nYisi-2\n\nLASIM\n\nUNI\nipmt-morpheme\nUSFD\n\nibmt—posdgram\n\nipmt.posdgram\n\nYisi-1\n\nEED\n\nBEER\nSIM\nYisi-0\nchr\n\nchiF\nsentBLEU\nhLEPORa_baseline\nCharacTER\nYisi-2\n\nYisi.2\n\nCharacTER\n\n-zh\n\ni}\n5\n\nYiSi-1\nESiIM\nchr\nchrE+\nsentBLEU\nYisi-0\nEED\nBEER\nCharacTER\nYisi-2\n\nEED\nBEER\n\nCharacTER\n\nYiSi2\nYiSi.2_srl\n", "vlm_text": "The image presents a series of matrices showing the results of significance testing for various segment-level metrics used in machine translation evaluation. Each matrix corresponds to a different language pair involving English, with both \"into English\" and \"out-of English\" categories represented. The language pairs are: \n- de-en (German to English)\n- fi-en (Finnish to English)\n- gu-en (Gujarati to English)\n- kk-en (Kazakh to English)\n- lt-en (Lithuanian to English)\n- ru-en (Russian to English)\n- zh-en (Chinese to English)\n- en-cs (English to Czech)\n- en-de (English to German)\n- en-fi (English to Finnish)\n- en-gu (English to Gujarati)\n- en-kk (English to Kazakh)\n- en-lt (English to Lithuanian)\n- en-ru (English to Russian)\n- en-zh (English to Chinese)\n\nEach cell within a matrix is color-coded, with green cells indicating a statistically significant win for the metric listed on the row over the metric listed on the column, as determined by a bootstrap resampling method. The metrics compared include various evaluation metrics such as Yisi-1, chrF, BLEU, BERT, ESIM, and others, illustrating which metrics perform better in assessing translations between specific language pairs."}
{"layout": 143, "type": "image", "page_idx": 16, "img_path": "layout_images/W19-5302_12.jpg", "img_caption": "Figure 4:  daRR  segment-level metric significance test results for German to Czech, German to French and French to German (newstest2019): Green cells denote a significant win for the metric in a given row over the metric in a given column according bootstrap resampling. ", "bbox": [69, 61, 528, 234], "page_size": [595.280029296875, 841.8900146484375], "ocr_text": "EED\nBEER\nESIM\n\nYisio\n\ncht.\n\ncher\nCharacTER\n\nnLEPORa_baseline\n\nde-cs\n\nsentBLEU\nYisi2\nibmt.morpheme\n\nYisi-\nEED\n\nBEER\n\nEsiM\n\nYisi-0\n\nchiF+\n\nchiE\n\nCharacTER\nhLEPORa baseline\nsentBLEU\n\nYisi-2\nibmt-morpheme\n\nVisi\n\nED\nVisio\nBEER\n\nESM\n\nchrF.\n\nchrF\nCharacTER\n\nhLEPORa_baseline\n\nde-fr\n\nsentBLEU\n\nYisi2\n\nibmt.morpheme\nllbmt.posdgram\n\nYisi-\nEED\n\nYisi-0\n\nBEER\n\nesiM\n\nchrF+\n\nchrF\n\nCharacTER\nhLEPORa baseline\nsentBLEU\n\nvisi-2\nibm1—-morpheme\nibmt-pos4gram\n\nchr,\n\nhE\nEED\n\nBEER\nCharacTER\n\nfr-de\n\nsentBLEU\n\nYisi2\n\nibmt.morpheme\n\nibmt.pos4gram\n\nYisi-1\nYiSi-1_stl\n\nESIM\n\nchrF +\n\nYisi-0\n\nchr\n\nFED\n\nBEER\nCharacTER\nsentBLEU\nYisi-2\niibm1—morpheme\nibm1-pos4gram\n", "vlm_text": "The image consists of three separate heatmaps displaying the results of significance tests of segment-level metrics across different language pairs: German to Czech (de-cs), German to French (de-fr), and French to German (fr-de), based on newstest2019 data. The green cells in each heatmap indicate a significant win for the metric in the corresponding row when compared to the metric in the column, as determined by bootstrap resampling.\n\nFor each language pair, a list of metrics is compared:\n- For German to Czech (de-cs), the metrics include Yisi-1, EED, BEER, ESIM, Yisi-0, chrF+, chrF, CharacTER, hLEPORa_baseline, sentBLEU, Yisi-2, and ibm1-morpheme.\n- For German to French (de-fr), the metrics include Yisi-1, EED, Yisi-0, BEER, ESIM, chrF+, chrF, CharacTER, hLEPORa_baseline, sentBLEU, Yisi-2, ibm1-morpheme, and ibm1-pos4gram.\n- For French to German (fr-de), the metrics include Yisi-1, Yisi-1_srl, ESIM, chrF+, Yisi-0, chrF, EED, BEER, CharacTER, sentBLEU, Yisi-2, ibm1-morpheme, and ibm1-pos4gram.\n\nOverall, Yisi-1 shows the most significant wins across the metrics in each of the language pairs, as indicated by many green cells in its row, implying it performs better compared to several other metrics."}
{"layout": 144, "type": "text", "text": "• counts metric’s ties as a  Discordant  pairs. ", "page_idx": 16, "bbox": [80, 257.4464416503906, 290, 272.9046325683594], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 145, "type": "text", "text": "We employ bootstrap resampling ( Koehn , 2004 ;  Graham et al. ,  2014b ) to estimate con- fidence intervals for our Kendall’s Tau for- mulation, and metrics with non-overlapping  $95\\%$   confidence intervals are identified as hav- ing statistically significant difference in perfor- mance. ", "page_idx": 16, "bbox": [71, 279.19342041015625, 290, 375.9466552734375], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 146, "type": "text", "text": "5.2.1 Segment-Level Results ", "text_level": 1, "page_idx": 16, "bbox": [70, 384, 234, 396], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 147, "type": "text", "text": "Results of the segment-level human evaluation for translations sampled from the News Trans- lation Task are shown in Tables  6 ,  7  and  8 , where metric correlations not significantly out- performed by any other metric are highlighted in bold. Head-to-head significance test results for differences in metric performance are in- cluded in Figures  3  and  4 . ", "page_idx": 16, "bbox": [71, 396.8454284667969, 290, 507.148681640625], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 148, "type": "text", "text": "6 Discussion ", "text_level": 1, "page_idx": 16, "bbox": [70, 516, 153, 530], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 149, "type": "text", "text": "This year, human data was collected from reference-based evaluations (or “monolin- gual”) and reference-free evaluations (or “bilingual”). The reference-based (mono- lingual) evaluations were obtained with the help of anonymous crowdsourcing, while the reference-less (bilingual) evaluations were mainly from MT researchers who committed their time contribution to the manual evalua- tion for each submitted system. ", "page_idx": 16, "bbox": [71, 535.3534545898438, 290, 672.7546997070312], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 150, "type": "text", "text": "6.1 Stability across MT Systems ", "text_level": 1, "page_idx": 16, "bbox": [70, 682, 257, 695], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 151, "type": "text", "text": "The observed performance of metrics depends on the underlying texts and systems that par- ticipate in the News Translation Task (see Sec- tion  2 ). For the strongest MT systems, distin- guishing which system outputs are better is ", "page_idx": 16, "bbox": [71, 696.4744262695312, 290, 766.128662109375], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 152, "type": "image", "page_idx": 16, "img_path": "layout_images/W19-5302_13.jpg", "img_caption": "Figure 5: Pearson correlations of  sacreBLEU- BLEU  for English-German system-level evalua- tion for all systems (left) down to only top 4 sys- tems (right). The y-axis spans from -1 to   $+1$  , base- line metrics for the language pair in grey. ", "bbox": [305, 254, 527, 420], "page_size": [595.280029296875, 841.8900146484375], "ocr_text": "sacreBLEU-BLEU\n\n2018161412108 6 4\n", "vlm_text": "The image is a plot showing the Pearson correlations between sacreBLEU and BLEU scores for English-German system-level evaluation. The x-axis represents the number of systems evaluated, ranging from all systems (approximately 20) on the left to only the top 4 systems on the right. The y-axis, which spans from -1 to +1, represents the Pearson correlation values. The colored lines denote correlations for different numbers of top systems included, with the main magenta line representing the trend as fewer top systems are considered. The baseline metrics for the language pair are displayed in grey, providing a comparative reference for the correlation values. The plot demonstrates a decline in correlation values as the number of top systems considered decreases from left to right."}
{"layout": 153, "type": "text", "text": "hard, even for human assessors. On the other hand, if the systems are spread across a wide performance range, it will be easier for metrics to correlate with human judgements. ", "page_idx": 16, "bbox": [307, 438.8844299316406, 525, 494.9896545410156], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 154, "type": "text", "text": "To provide a more reliable view, we created plots of Pearson correlation when the under- lying set of MT systems is reduced to top    $n$  ones. One sample such plot is in Figure  5 , all language pairs and most of the metrics are in Appendix  A . ", "page_idx": 16, "bbox": [307, 493.1324768066406, 525, 576.336669921875], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 155, "type": "text", "text": "As the plot documents, the official correla- tions reported in Tables  3  to  5  can lead to wrong conclusions.  sacreBLEU-BLEU  cor- relates at .969 when all systems are considered, but as we start considering only the top    $n$   sys- tems, the correlation falls relatively quickly. With 10 systems, we are below .5 and when only the top 6 or 4 systems are considered, the correlation falls even to the negave val- ues. Note that correlations point estimates (the value in the y-axis) become noiser with the decreasing number of the underlying MT systems. ", "page_idx": 16, "bbox": [307, 574.4794311523438, 525, 752.5277099609375], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 156, "type": "text", "text": "Figure  6  explains the situation and illus- ", "page_idx": 16, "bbox": [318, 750.6704711914062, 525, 766.1287231445312], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 157, "type": "image", "page_idx": 17, "img_path": "layout_images/W19-5302_14.jpg", "bbox": [69, 59, 292, 249], "page_size": [595.280029296875, 841.8900146484375], "ocr_text": "All systems\n\n0.5\n\n0.45\n\nnai\n\nNatgeses\n\n0.5\n\n-0.5\n\n<5\n\nDA\n\ni 5 en 21\n", "vlm_text": "The image is a scatter plot with a line plot overlay. It shows a comparison between \"DA\" (Direct Assessment) on the x-axis and \"SacreBLEU-BLEU\" on the y-axis. The legend indicates different line styles for \"Top 4\", \"Top 6\", \"Top 8\", \"Top 10\", \"Top 12\", \"Top 15\", and \"All systems.\" Each category has a different color and line style.\n\n- Purple dots of varying sizes represent data points, indicating some distribution or correlation between the two metrics.\n- The lines of different colors represent trends or fits for various subsets of the data (e.g., Top 4, Top 6, etc.).\n- The black line appears to be a baseline or reference for \"All systems.\"\n\nThe plot analyzes the correlation or relationship between the two metrics for multiple categories of systems or data sets."}
{"layout": 158, "type": "text", "text": "trates the sensitivity of the observed correla- tions to the exact set of systems. On the full set of systems, the single outlier (the worst- performing system called  en_de_task ) helps to achieve a great positive correlation. The majority of MT systems however form a cloud with Pearson correlation around .5 and the top 4 systems actually exhibit a negative corre- lation of the human score and  sacreBLEU- BLEU . ", "page_idx": 17, "bbox": [71, 274.783447265625, 290, 412.1846618652344], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 159, "type": "text", "text": "In Appendix  A , baseline metrics are plotted in grey in all the plots, so that their trends can be observed jointly. In general, most baselines have similar correlations, as most baselines use similar features (n-gram or word-level features, with the exception of  chrF ). In a number of language pairs (de-en, de-fr, en-de, en-kk, lt- en, ru-en, zh-en), baseline correlations tend to- wards 0 (no correlation) or even negative Pear- son correlation. For a widely applied metric such as  sacreBLEU-BLEU , our analysis re- veals weak correlation in comparing top state- of-the-art systems in these language pairs, es- pecially in en-de, de-en, ru-en, and zh-en. ", "page_idx": 17, "bbox": [71, 411.10845947265625, 290, 602.7057495117188], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 160, "type": "text", "text": "We will restrict our analysis to those lan- guage pairs where the baseline metrics have an obvious downward trend (de-en, de-fr, en-de, en-kk, lt-en, ru-en, zh-en). Examining the top-  $n$   correlation in the submitted metrics (not in- cluding QE systems), most metrics show the same degredation in correlation as the base- lines. We note  BERTr  as the one exception consistently degrading less and retaining pos- itive correlation compared to other submitted metrics and baselines, in the language pairs where it participated. ", "page_idx": 17, "bbox": [71, 601.6295776367188, 290, 766.1298217773438], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 161, "type": "text", "text": "For QE systems, we noticed that in some in- stances, QE systems have upward correlation trends when other metrics and baselines have downward trends. For instance,  LP ,  UNI , and  $\\mathrm{UNII+}$   in the de-en language pair,  YiSi-2  in en-kk, and  UNI  and  UNI+  in ru-en. These results suggest that QE systems such as  UNI and  UNI+  perform worse on judging systems of wide ranging quality, but better for top per- forming systems, or perhaps for systems closer in quality. ", "page_idx": 17, "bbox": [306, 61.472442626953125, 525, 212.42262268066406], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 162, "type": "text", "text": "If our method of human assessment is sound, we should believe that  BLEU , a widely ap- plied metric, is no longer a reliable metric for judging our best systems. Future investiga- tions are needed to understand when  BLEU applies well, and why  BLEU  is not effective for output from our state of the art models. ", "page_idx": 17, "bbox": [306, 211.388427734375, 525, 308.1416015625], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 163, "type": "text", "text": "Metrics and QE systems such as  BERTr , ESIM ,  YiSi  that perform well at judging our best systems often use more semantic features compared to our n-gram/char-gram based baselines. Future metrics may want to explore a) whether semantic features such as contextual word embeddings are achieving se- mantic understanding and b) whether seman- tic understanding is the true source of a met- ric’s performance gains. ", "page_idx": 17, "bbox": [306, 307.107421875, 525, 444.5086364746094], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 164, "type": "text", "text": "It should be noted that  some  language pairs do not show the strong degrading pattern with top-  $n$   systems this year, for instance en-cs, en- gu, en-ru, or kk-en. English-Chinese is partic- ularly interesting because we see a clear trend towards  better  correlations as we reduce the set of underlying systems to the top scoring ones. ", "page_idx": 17, "bbox": [306, 443.47344970703125, 525, 553.7766723632812], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 165, "type": "text", "text": "6.2 Overall Metric Performance ", "text_level": 1, "page_idx": 17, "bbox": [306, 566, 489, 578], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 166, "type": "text", "text": "6.2.1 System-Level Evaluation ", "text_level": 1, "page_idx": 17, "bbox": [306, 585, 480, 597], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 167, "type": "text", "text": "In system-level evaluation, the series of  YiSi metrics achieve the highest correlations in sev- eral language pairs and it is not significantly outperformed by any other metrics (denoted as a “win” in the following) for almost all lan- guage pairs. ", "page_idx": 17, "bbox": [306, 599.8804321289062, 525, 683.0846557617188], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 168, "type": "text", "text": "The new metric  ESIM  performs best on 5 language languages (18 language pairs) and obtains 11 “wins” out of 16 language pairs in which  ESIM  participated. ", "page_idx": 17, "bbox": [306, 682.0504760742188, 525, 738.1557006835938], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 169, "type": "text", "text": "The metric  EED  performs better for lan- guage pairs out-of English and excluding En- glish compared to into-English language pairs, achieving 7 out of 11 “wins” there. ", "page_idx": 17, "bbox": [306, 737.1215209960938, 525, 766.1296997070312], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 170, "type": "text", "text": "", "page_idx": 18, "bbox": [71, 61.472442626953125, 290, 90.4796371459961], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 171, "type": "text", "text": "6.2.2 Segment-Level Evaluation ", "text_level": 1, "page_idx": 18, "bbox": [71, 107, 252, 120], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 172, "type": "text", "text": "For segment-level evaluation, most language pairs are quite discerning, with only one or two metrics taking the “winner” position (of not being significantly surpassed by others). Only French-German differs, with all metrics performing similarly except the significantly worse  sentBLEU . ", "page_idx": 18, "bbox": [71, 126.57444763183594, 290, 223.32762145996094], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 173, "type": "text", "text": "YiSi-1_srl  stands out as the “winner” for all language pairs in which it participated. The excluded language pairs were probably due to the lack of semantic information re- quired by  YiSi-1_srl . YiSi-1  participated all language pairs and its correlations are com- parable with those of  YiSi-1_srl . ", "page_idx": 18, "bbox": [71, 224.2564239501953, 290, 321.00958251953125], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 174, "type": "text", "text": "ESIM  obtain 6 “winners” out of all 18 lan- guages pairs. ", "page_idx": 18, "bbox": [71, 321.93841552734375, 290, 350.94561767578125], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 175, "type": "text", "text": "Both  YiSi  and  ESIM  are based on neu- ral networks ( YiSi  via word and phrase em- beddings, as well as other types of available resources,  ESIM  via sentence embeddings). This is a confirmation of a trend observed last year. ", "page_idx": 18, "bbox": [71, 351.8744201660156, 290, 435.0786437988281], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 176, "type": "text", "text": "6.2.3 QE Systems as Metrics ", "text_level": 1, "page_idx": 18, "bbox": [71, 452, 237, 464], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 177, "type": "text", "text": "Generally, correlations for the standard reference-based metrics are obviously better than those in “QE as a Metric” track, both when using monolingual and bilingual golden truth. ", "page_idx": 18, "bbox": [71, 471.1734619140625, 290, 540.8286743164062], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 178, "type": "text", "text": "In system-level evaluation, correlations for “QE as a Metric” range from 0.028 to 0.947 across all language pairs and all metrics but they are very unstable. Even for a single metric, take  UNI  for example, the correla- tions range from 0.028 to 0.930 across language pairs. ", "page_idx": 18, "bbox": [71, 541.7574462890625, 290, 638.5107421875], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 179, "type": "text", "text": "In segment-level evaluation, correlations for QE metrics range from -0.153 to 0.351 across all language pairs and show the same instabil- ity across language pairs for a given metric. ", "page_idx": 18, "bbox": [71, 639.4395751953125, 290, 695.5457763671875], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 180, "type": "text", "text": "In either case, we do not see any pattern that could explain the behaviour, e.g. whether the manual evaluation was monolingual or bilingual, or the characteristics of the given language pair. ", "page_idx": 18, "bbox": [71, 696.474609375, 290, 766.1288452148438], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 181, "type": "text", "text": "6.3 Dependence on Implementation ", "text_level": 1, "page_idx": 18, "bbox": [306, 65, 510, 76], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 182, "type": "text", "text": "As it already happened in the past, we had multiple implementations for some metrics, BLEU  and  chrF  in particular. ", "page_idx": 18, "bbox": [307, 79.44544219970703, 526, 122.00163269042969], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 183, "type": "text", "text": "The detailed configuration of  BLEU  and sacreBLEU-BLEU  differ and hence their scores and correlation results are different. ", "page_idx": 18, "bbox": [307, 120.43344116210938, 526, 162.98963928222656], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 184, "type": "text", "text": "chrF  and  sacreBLEU-chrF  use the same parameters and should thus deliver the same scores but we still observe some differences, leading to different correlations. For instance for German-French Pearson correlation,  chrF obtains 0.931 (no win) but  sacreBLEU- chrF  reaches 0.952, tying for a win with other metrics. ", "page_idx": 18, "bbox": [307, 161.42144775390625, 526, 271.7236328125], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 185, "type": "text", "text": "We thus fully support the call for clarity by Post  ( 2018b ) and invite authors of metrics to include their implementations either in Moses scorer or sacreBLEU to achieve a long-term assessment of their metric. ", "page_idx": 18, "bbox": [307, 270.1554260253906, 526, 339.81060791015625], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 186, "type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 18, "bbox": [307, 351, 391, 363], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 187, "type": "text", "text": "This paper summarizes the results of  WMT19 shared task in machine translation evaluation, the Metrics Shared Task. Participating met- rics were evaluated in terms of their correla- tion with human judgement at the level of the whole test set (system-level evaluation), as well as at the level of individual sentences (segment-level evaluation). ", "page_idx": 18, "bbox": [307, 370.2714538574219, 526, 480.5746765136719], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 188, "type": "text", "text": "We reported scores for standard metrics re- quiring the reference as well as quality estima- tion systems which took part in the track “QE as a metric”, joint with the Quality Estimation task. ", "page_idx": 18, "bbox": [307, 479.0065002441406, 526, 548.6607055664062], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 189, "type": "text", "text": "For system-level, best metrics reach over 0.95 Pearson correlation or better across sev- eral language pairs. As expected, QE sys- tems are visibly in all language pairs but they can also reach high system-level correlations, up to .947 (Chinese-English) or .936 (English- German) by  YiSi-1_srl  or over .9 for multi- ple language pairs by  UNI . ", "page_idx": 18, "bbox": [307, 547.092529296875, 526, 657.394775390625], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 190, "type": "text", "text": "An important caveat is that the correlations are heavily affected by the underlying set of MT systems. We explored this by reducing the set of systems to top-  $n$   ones for various    $n\\mathrm{s}$  and found out that for many language pairs, system-level correlations are much worse when based on only the better performing systems. With both good and bad MT systems partic- ipating in the news task, the metrics results can be overly optimistic compared to what we get when evaluating state-of-the-art systems. ", "page_idx": 18, "bbox": [307, 655.8265991210938, 526, 766.1288452148438], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 191, "type": "text", "text": "", "page_idx": 19, "bbox": [72, 61.472442626953125, 290, 104.02863311767578], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 192, "type": "text", "text": "In terms of segment-level Kendall’s    $\\tau$   re- sults, the standard metrics correlations varied between 0.03 and 0.59, and QE systems ob- tained even negative correlations. ", "page_idx": 19, "bbox": [72, 102.2264404296875, 290, 158.3326416015625], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 193, "type": "text", "text": "The results confirm the observation from the last year, namely metrics based on word or sentence-level embeddings ( YiSi  and  ESIM ), achieve the highest performance. ", "page_idx": 19, "bbox": [72, 156.53045654296875, 290, 212.63563537597656], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 194, "type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 19, "bbox": [72, 224, 177, 236], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 195, "type": "text", "text": "Results in this shared task would not be pos- sible without tight collaboration with organiz- ers of the WMT News Translation Task. We would like to thank Marcin Junczys-Dowmunt for the suggestion to examine metrics perfor- mance across varying subsets of MT systems, as we did in Appendix  A . ", "page_idx": 19, "bbox": [72, 241.74745178222656, 290, 338.5006408691406], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 196, "type": "text", "text": "This study was supported in parts by the grants 19-26934X (NEUREM3) of the Czech Science Foundation, ADAPT Centre for Dig- ital Content Technology ( www.adaptcentre. ie ) at Dublin City University funded un- der the SFI Research Centres Programme (Grant 13/RC/2106) co-funded under the European Regional Development Fund, and Charles University Research Programme “Pro- gres” Q18+Q48. ", "page_idx": 19, "bbox": [72, 336.6984558105469, 290, 474.09967041015625], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 197, "type": "text", "text": "References ", "text_level": 1, "page_idx": 19, "bbox": [72, 497, 135, 509], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 198, "type": "text", "text": "Colin Bannard and Chris Callison-Burch. 2005. Paraphrasing with bilingual parallel corpora . In Proceedings of the 43rd Annual Meeting on Asso- ciation for Computational Linguistics , ACL ’05, pages 597–604, Stroudsburg, PA, USA. Associ- ation for Computational Linguistics. ", "page_idx": 19, "bbox": [72, 513.7731323242188, 290, 582.6851196289062], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 199, "type": "text", "text": "Loïc Barrault, Ondřej Bojar, Marta R. Costa- jussà, Christian Federmann, Mark Fishel, Yvette Graham, Barry Haddow, Matthias Huck, Philipp Koehn, Shervin Malmasi, Christof Monz, Mathias Müller, Santanu Pal, Matt Post, and Marcos Zampieri. 2019. Findings of the 2019 Conference on Machine Translation (WMT19). In  Proceedings of the Fourth Con- ference on Machine Translation , Florence, Italy. Association for Computational Linguistics. ", "page_idx": 19, "bbox": [72, 588.920166015625, 290, 701.6670532226562], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 200, "type": "text", "text": "Ondřej Bojar, Christian Federmann, Barry Had- dow, Philipp Koehn, Matt Post, and Lucia Spe- cia. 2016. Ten Years of WMT Evaluation Cam- paigns: Lessons Learnt. In  Proceedings of the LREC 2016 Workshop “Translation Evaluation ", "page_idx": 19, "bbox": [72, 707.9021606445312, 290, 765.8551025390625], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 201, "type": "text", "text": "– From Fragmented Tools and Data Sets to an Integrated Ecosystem” , pages 27–34, Portorose, Slovenia. ", "page_idx": 19, "bbox": [318, 62.55907440185547, 525, 98.57415008544922], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 202, "type": "text", "text": "Ondřej Bojar, Yvette Graham, and Amir Kamran. 2017. Results of the WMT17 metrics shared task. In  Proceedings of the Second Confer- ence on Machine Translation, Volume 2: Shared Tasks Papers , Copenhagen, Denmark. Associa- tion for Computational Linguistics. ", "page_idx": 19, "bbox": [307, 103.8051528930664, 525, 172.71615600585938], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 203, "type": "text", "text": "Qian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si Wei, Hui Jiang, and Diana Inkpen. 2017. Enhanced lstm for natural language inference. In  Proceed- ings of the 55th Annual Meeting of the Associ- ation for Computational Linguistics (Volume 1: Long Papers) , pages 1657–1668. ", "page_idx": 19, "bbox": [307, 177.94715881347656, 525, 246.858154296875], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 204, "type": "text", "text": "Julian Chow, Pranava Madhyastha, and Lucia Specia. 2019a. Wmdo: Fluency-based word mover’s distance for machine translation eval- uation. In  Proceedings of Fourth Conference on Machine Translation . ", "page_idx": 19, "bbox": [307, 252.08917236328125, 525, 310.04217529296875], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 205, "type": "text", "text": "Julian Chow, Lucia Specia, and Pranava Mad- hyastha. 2019b. WMDO: Fluency-based Word Mover’s Distance for Machine Translation Eval- uation. In  Proceedings of the Fourth Conference on Machine Translation , Florence, Italy. Asso- ciation for Computational Linguistics. ", "page_idx": 19, "bbox": [307, 315.2731628417969, 525, 384.18414306640625], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 206, "type": "text", "text": "Michael Denkowski and Alon Lavie. 2014.  Meteor Universal: Language Specific Translation Evalu- ation for Any Target Language . In  Proceedings of the Ninth Workshop on Statistical Machine Translation , pages 376–380, Baltimore, Mary- land, USA. Association for Computational Lin- guistics. ", "page_idx": 19, "bbox": [307, 389.4151611328125, 525, 469.28515625], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 207, "type": "text", "text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.  BERT: Pre-training of deep bidirectional transformers for language understanding . In  Proceedings of the 2019 Con- ference of the North American Chapter of the Association for Computational Linguistics: Hu- man Language Technologies, Volume 1 (Long and Short Papers) , pages 4171–4186, Minneapo- lis, Minnesota. Association for Computational Linguistics. ", "page_idx": 19, "bbox": [307, 474.5161437988281, 525, 587.2631225585938], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 208, "type": "text", "text": "George Doddington. 2002. Automatic Evalua- tion of Machine Translation Quality Using N- gram Co-occurrence Statistics . In  Proceedings of the Second International Conference on Hu- man Language Technology Research , HLT ’02, pages 138–145, San Francisco, CA, USA. Mor- gan Kaufmann Publishers Inc. ", "page_idx": 19, "bbox": [307, 592.494140625, 525, 672.3641357421875], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 209, "type": "text", "text": "William B. Dolan and Chris Brockett. 2005.  Au- tomatically constructing a corpus of sentential paraphrases . In  Proceedings of the Third Inter- national Workshop on Paraphrasing (IWP2005) . ", "page_idx": 19, "bbox": [307, 677.5951538085938, 525, 724.5891723632812], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 210, "type": "text", "text": "Erick Fonseca, Lisa Yankovskaya, André F. T. Martins, Mark Fishel, and Christian Feder- mann. 2019. Findings of the WMT 2019 Shared ", "page_idx": 19, "bbox": [307, 729.8201293945312, 525, 765.8541259765625], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 211, "type": "text", "text": "Task on Quality Estimation. In  Proceedings of the Fourth Conference on Machine Translation , Florence, Italy. Association for Computational Linguistics. ", "page_idx": 20, "bbox": [82, 62.539146423339844, 290, 109.53314971923828], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 212, "type": "text", "text": "Yvette Graham and Timothy Baldwin. 2014.  Test- ing for Significance of Increased Correlation with Human Judgment . In  Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pages 172–176, Doha, Qatar. Association for Computational Linguistics. ", "page_idx": 20, "bbox": [72, 117.1821517944336, 290, 197.05215454101562], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 213, "type": "text", "text": "Yvette Graham, Timothy Baldwin, Alistair Mof- fat, and Justin Zobel. 2013. Continuous Mea- surement Scales in Human Evaluation of Ma- chine Translation. In  Proceedings of the 7th Lin- guistic Annotation Workshop & Interoperability with Discourse , pages 33–41, Sofia, Bulgaria. As- sociation for Computational Linguistics. ", "page_idx": 20, "bbox": [72, 204.7021484375, 290, 284.5721435546875], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 214, "type": "text", "text": "Yvette Graham, Timothy Baldwin, Alistair Mof- fat, and Justin Zobel. 2014a.  Is Machine Trans- lation Getting Better over Time? In  Proceed- ings of the 14th Conference of the European Chapter of the Association for Computational Linguistics , pages 443–451, Gothenburg, Swe- den. Association for Computational Linguistics. ", "page_idx": 20, "bbox": [72, 292.2211608886719, 290, 372.09210205078125], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 215, "type": "text", "text": "Yvette Graham, Timothy Baldwin, Alistair Mof- fat, and Justin Zobel. 2016.  Can machine trans- lation systems be evaluated by the crowd alone . Natural Language Engineering , FirstView:1–28. ", "page_idx": 20, "bbox": [72, 379.74114990234375, 290, 426.73516845703125], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 216, "type": "text", "text": "Yvette Graham and Qun Liu. 2016. Achieving Ac- curate Conclusions in Evaluation of Automatic Machine Translation Metrics. In  Proceedings of the 15th Annual Conference of the North Amer- ican Chapter of the Association for Computa- tional Linguistics: Human Language Technolo- gies , San Diego, CA. Association for Computa- tional Linguistics. ", "page_idx": 20, "bbox": [72, 434.3841552734375, 290, 525.2130737304688], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 217, "type": "text", "text": "Yvette Graham, Nitika Mathur, and Timothy Baldwin. 2014b. Randomized significance tests in machine translation. In  Proceedings of the ACL 2014 Ninth Workshop on Statistical Ma- chine Translation , pages 266–274. Association for Computational Linguistics. ", "page_idx": 20, "bbox": [72, 532.8631591796875, 290, 601.7741088867188], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 218, "type": "text", "text": "Yvette Graham, Nitika Mathur, and Timo- thy Baldwin. 2015. Accurate Evaluation of Segment-level Machine Translation Metrics. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Com- putational Linguistics Human Language Tech- nologies , Denver, Colorado. ", "page_idx": 20, "bbox": [72, 609.4231567382812, 290, 689.2940673828125], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 219, "type": "text", "text": "Yinuo Guo and Junfeng Hu. 2019. Meteor++ 2.0: Adopt Syntactic Level Paraphrase Knowl- edge into Machine Translation Evaluation. In Proceedings of the Fourth Conference on Ma- chine Translation , Florence, Italy. Association for Computational Linguistics. ", "page_idx": 20, "bbox": [72, 696.9431762695312, 290, 765.8551025390625], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 220, "type": "text", "text": "Aaron L.-F. Han, Derek F. Wong, and Lidia S. Chao. 2012. Lepor: A robust evaluation metric for machine translation with augmented factors. In  Proceedings of the 24th International Con- ference on Computational Linguistics (COLING 2012) , pages 441–450. Association for Computa- tional Linguistics. ", "page_idx": 20, "bbox": [307, 62.539146423339844, 525, 142.40914916992188], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 221, "type": "text", "text": "Aaron L.-F. Han, Derek F. Wong, Lidia S. Chao, Liangye He, Yi Lu, Junwen Xing, and Xiaodong Zeng. 2013. Language-independent model for machine translation evaluation with reinforced factors. In  Machine Translation Summit XIV , pages 215–222. International Association for Machine Translation. ", "page_idx": 20, "bbox": [307, 148.858154296875, 525, 228.7281494140625], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 222, "type": "text", "text": "Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In  Proc. of Empirical Methods in Natural Language Process- ing , pages 388–395, Barcelona, Spain. Associa- tion for Computational Linguistics. ", "page_idx": 20, "bbox": [307, 235.17715454101562, 525, 293.129150390625], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 223, "type": "text", "text": "Philipp Koehn and Christof Monz. 2006.  Manual and Automatic Evaluation of Machine Trans- lation Between European Languages . In  Pro- ceedings of the Workshop on Statistical Ma- chine Translation , StatMT ’06, pages 102–121, Stroudsburg, PA, USA. Association for Compu- tational Linguistics. ", "page_idx": 20, "bbox": [307, 299.5781555175781, 525, 379.4481201171875], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 224, "type": "text", "text": "Gregor Leusch, Nicola Ueffing, and Hermann Ney. 2003. A novel string-to-string distance measure with applications to machine translation evalu- ation. In  Proceedings of Mt Summit IX , pages 240–247. ", "page_idx": 20, "bbox": [307, 385.89715576171875, 525, 443.84912109375], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 225, "type": "text", "text": "Gregor Leusch, Nicola Ueffing, and Hermann Ney. 2006. CDER: Efficient MT Evaluation Using Block Movements. In  In Proceedings of EACL , pages 241–248. ", "page_idx": 20, "bbox": [307, 450.29815673828125, 525, 497.2911376953125], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 226, "type": "text", "text": "Chi-kiu Lo. 2019. YiSi - a Unified Semantic MT Quality Evaluation and Estimation Metric for Languages with Different Levels of Available Re- sources. In  Proceedings of the Fourth Conference on Machine Translation , Florence, Italy. Asso- ciation for Computational Linguistics. ", "page_idx": 20, "bbox": [307, 503.74017333984375, 525, 572.651123046875], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 227, "type": "text", "text": "Qingsong Ma, Ondřej Bojar, and Yvette Graham. 2018. Results of the WMT18 metrics shared task: Both characters and embeddings achieve good performance. In  Proceedings of the Third Conference on Machine Translation, Volume 2: Shared Task Papers , Brussels, Belgium. Associ- ation for Computational Linguistics. ", "page_idx": 20, "bbox": [307, 579.1001586914062, 525, 658.9700927734375], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 228, "type": "text", "text": "Matouš Macháček and Ondřej Bojar. 2014. Re- sults of the WMT14 metrics shared task. In Proceedings of the Ninth Workshop on Statisti- cal Machine Translation , pages 293–301, Balti- more, MD, USA. Association for Computational Linguistics. ", "page_idx": 20, "bbox": [307, 665.4191284179688, 525, 734.330078125], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 229, "type": "text", "text": "Matouš Macháček and Ondřej Bojar. 2013.  Results of the WMT13 Metrics Shared Task . In  Proceed- ", "page_idx": 20, "bbox": [307, 740.7791748046875, 525, 765.8541870117188], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 230, "type": "text", "text": "ings of the Eighth Workshop on Statistical Ma- chine Translation , pages 45–51, Sofia, Bulgaria. Association for Computational Linguistics. ", "page_idx": 21, "bbox": [82, 62.55907440185547, 290, 98.57415008544922], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 231, "type": "text", "text": "Nitika Mathur, Tim Baldwin, and Trevor Cohn. 2019. Putting evaluation in context: Contextual embeddings improve machine translation evalu- ation. In  Proc. of ACL (short papers) . To ap- pear. Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU: A Method for Au- tomatic Evaluation of Machine Translation. In Proceedings of the 40th Annual Meeting on Asso- ciation for Computational Linguistics , ACL ’02, pages 311–318. Maja Popovic. 2012.  Morpheme- and POS-based IBM1 and language model scores for translation quality estimation . In  Proceedings of the Sev- enth Workshop on Statistical Machine Trans- lation, WMT@NAACL-HLT 2012, June 7-8, 2012, Montréal, Canada , pages 133–137. Maja Popović. 2015. chrF: character n-gram F- score for automatic MT evaluation . In  Proceed- ings of the Tenth Workshop on Statistical Ma- chine Translation , Lisboa, Portugal. Association for Computational Linguistics. Maja Popović. 2017. chrF++: words helping char- acter n-grams. In  Proceedings of the Second Conference on Machine Translation, Volume 2: Shared Tasks Papers , Copenhagen, Denmark. Association for Computational Linguistics. Matt Post. 2018a.  A call for clarity in reporting BLEU scores . In  Proceedings of the Third Con- ference on Machine Translation: Research Pa- pers , pages 186–191, Belgium, Brussels. Associ- ation for Computational Linguistics. Matt Post. 2018b. A call for clarity in reporting bleu scores. In  Proceedings of the Third Confer- ence on Machine Translation , Belgium, Brus- sels. Association for Computational Linguistics. Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of translation edit rate with targeted hu- man annotation. In  In Proceedings of Associa- tion for Machine Translation in the Americas , pages 223–231. Peter Stanchev, Weiyue Wang, and Hermann Ney. 2019. EED: Extended Edit Distance Measure for Machine Translation. In  Proceedings of the Fourth Conference on Machine Translation , Flo- rence, Italy. Association for Computational Lin- guistics. Miloš Stanojević and Khalil Sima’an. 2015.  BEER 1.1: ILLC UvA submission to metrics and tun- ing task . In  Proceedings of the Tenth Workshop on Statistical Machine Translation , Lisboa, Por- tugal. Association for Computational Linguis- tics. ", "page_idx": 21, "bbox": [72, 102.96614837646484, 290, 765.8541259765625], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 232, "type": "text", "text": "Weiyue Wang, Jan-Thorsten Peter, Hendrik Rosendahl, and Hermann Ney. 2016a. Charac- ter: Translation edit rate on character level. In ACL 2016 First Conference on Machine Trans- lation , pages 505–510, Berlin, Germany. Weiyue Wang, Jan-Thorsten Peter, Hendrik Rosendahl, and Hermann Ney. 2016b. Charac- Ter: Translation Edit Rate on Character Level. In  Proceedings of the First Conference on Ma- chine Translation , Berlin, Germany. Association for Computational Linguistics. Evan James Williams. 1959.  Regression analysis , volume 14. Wiley New York. Elizaveta Yankovskaya, Andre Tättar, and Mark Fishel. 2019. Quality Estimation and Transla- tion Metrics via Pre-trained Word and Sentence Embeddings. In  Proceedings of the Fourth Con- ference on Machine Translation , Florence, Italy. Association for Computational Linguistics. Ryoma Yoshimura, Hiroki Shimanaka, Yukio Mat- sumura, Hayahide Yamagishi, and Mamoru Ko- machi. 2019. Filtering Pseudo-References by Paraphrasing for Automatic Evaluation of Ma- chine Translation. In  Proceedings of the Fourth Conference on Machine Translation , Florence, Italy. Association for Computational Linguis- tics. ", "page_idx": 21, "bbox": [307, 62.539146423339844, 525, 397.453125], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 233, "type": "text", "text": "A Correlations for Top-N Systems ", "text_level": 1, "page_idx": 22, "bbox": [72, 63, 283, 77], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 234, "type": "image", "page_idx": 22, "img_path": "layout_images/W19-5302_15.jpg", "bbox": [71, 93, 527, 256], "page_size": [595.280029296875, 841.8900146484375], "ocr_text": "CharacTER\n\nNIST\n\nYiSi-2\n\nibm1-morpheme\n\nLEPORa\n\nww\n\nsacreBLEU-chrF\n", "vlm_text": "The image contains a series of line graphs that appear to represent different performance metrics or evaluation scores over a series of data points, possibly linked to a task like machine translation. Each graph is labeled with a different metric at the top: BEER, CharacTER, EED, ESIM, LEPORa, LEPORb, NIST, PER, TER, YiSi-0, YiSi-1, YiSi-2, ibm1-morpheme, sacreBLEU-BLEU, and sacreBLEU-chrF. The x-axis of each graph is marked with the numbers 4, 6, 8, and 9, representing different data points, while the y-axis on each is scaled from -1 to 1. The lines plotted represent the values of the metrics across these data points, with each line graph showing a trend for the specific metric in question. The subtitle \"A.1 de-cs\" suggests that the data could be related to the German-Czech (de-cs) language pair."}
{"layout": 235, "type": "image", "page_idx": 22, "img_path": "layout_images/W19-5302_16.jpg", "img_caption": "A.2 de-en ", "bbox": [71, 270, 527, 536], "page_size": [595.280029296875, 841.8900146484375], "ocr_text": "es\n“} LASIM\n“k PER\n\n~} WMDO\n\n“} YiSi-2_srl\n\nBERTr\n\nPReP\n\nYiSi-O\n\nibm1-morpheme\n\nCharacTER EED ESIM\n\nGaiee Galen Ebene\n\nMeteor++_2.0 Meteor++_2.0(+copy)NIST\nTER UNI UNI+\n\nles\na\na\n\ni-1 YiSi-1_srl YiSi-2\n\nibm1-pos4gram sacreBLEU-BLEU sacreBLEU-chrF\n\njaca Rana Coax: GanbeA AGA\n\nda 12 10 8 6 4 141210 8 6 4 14 12 10 8 6 4 141210 8 6 4 14 1210 8 6 4\n", "vlm_text": "The image shows a series of line graphs comparing different metrics or methods for evaluating translation quality. Each graph is labeled with a different name representing a metric or evaluation method, such as BEER, BERTtr, CharacTER, Meteor++, etc. The x-axis of each graph is labeled with numbers starting from 14 down to 4, which might represent different evaluation sets, versions, or data points, while the y-axis is labeled from -1 to 1, possibly indicating score values or relative effectiveness. The lines in each graph indicate how each metric's evaluation changes across different evaluation sets or conditions. The colors of the lines and labels are distinct to differentiate between the metrics."}
{"layout": 236, "type": "text", "text": "A.3 de-fr ", "text_level": 1, "page_idx": 22, "bbox": [72, 548, 129, 558.75], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 237, "type": "image", "page_idx": 22, "img_path": "layout_images/W19-5302_17.jpg", "bbox": [71, 559.25, 527, 766], "page_size": [595.280029296875, 841.8900146484375], "ocr_text": "4 BEER CharacTER BED ESIM LEPORa\n—_—_——_—_[= =\n(e)\n\n“} LEPORb NIST PER TER YiSi-0\n\n0\n\n7h YiSi-1 YiSi-2 ibm1-morpheme ibm1-pos4gram sacreBLEU-BLEU\n\n() alr eee jae dies\n4 9 8 6 4 9 8 6 4 9\n\nmA:\n\n1 sacreBLEU-chrF 9 8 6 8 6\nSS ee ae,\n", "vlm_text": "The image contains multiple line graphs arranged in a grid format. Each graph appears to represent some form of measurement or evaluation metric over different conditions or experiments. The line plots, with varying line colors and labels above them, suggest a comparison across different metrics or scenarios.\n\nEach graph is labeled with different evaluation metrics for machine translation or text generation quality. The labels are:\n- BEER\n- CharacTER\n- EED\n- ESIM\n- LEPORa\n- LEPORb\n- NIST\n- PER\n- TER\n- YiSi-0\n- YiSi-1\n- YiSi-2\n- ibm1-morpheme\n- ibm1-pos4gram\n- sacreBLEU-BLEU\n- sacreBLEU-chrF\n\nThe y-axis of the graphs seems to range from -1 to 1, while the x-axis is labeled with numbers from 4 to 9, which could represent different datasets, models, or scenarios. The shaded regions around each line plot may indicate confidence intervals or variability."}
{"layout": 238, "type": "text", "text": "A.4 en-cs ", "text_level": 1, "page_idx": 23, "bbox": [72, 64, 130, 76], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 239, "type": "image", "page_idx": 23, "img_path": "layout_images/W19-5302_18.jpg", "bbox": [70, 87, 528, 246], "page_size": [595.280029296875, 841.8900146484375], "ocr_text": "1\n(e)\n\nat\n1\n\n0\n\ncal\nab\n\nBEER\n\nTER\n\nibm1-morpheme\n\nCharacTER\n\nUNI\n\nreBLE\n\n-BLE\n\nEED. NIST\n\nYiSi-O YiSi-1\nSVE CO\n\nPER\n\nYiSi-2\n\n", "vlm_text": "The image is a set of small line graphs arranged in a grid, each representing different metrics for evaluating some data or model performance. These metrics include BEER, CharacTER, EED, NIST, PER, TER, UNI, YiSi-0, YiSi-1, YiSi-2, ibm1-morpheme, sacreBLEU-BLEU, and sacreBLEU-chrF, as indicated by the labels in different colors above each graph. The x-axis of the graphs is labeled with numbers possibly indicating some form of scoring or dimension. Each graph shows lines representing performance metrics or scores, with some variations across the x-axis values, but the lack of specific axis labels or context means the specific nature of the data being represented is unclear from the image alone."}
{"layout": 240, "type": "text", "text": "A.5 en-de ", "text_level": 1, "page_idx": 23, "bbox": [72, 276, 133, 288], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 241, "type": "image", "page_idx": 23, "img_path": "layout_images/W19-5302_19.jpg", "bbox": [70, 297, 527, 554], "page_size": [595.280029296875, 841.8900146484375], "ocr_text": "CharacTER\n\n2018161412108 6 4 2018161412108 6 4 2018161412108 6 4 2018161412108 6 4\n\n401816141210 8 64\n", "vlm_text": "The image consists of a series of line graphs comparing different evaluation metrics in natural language processing or machine translation tasks. Each graph depicts the scores of a specific evaluation metric as the x-axis (labeled with numbers like 2018, 16, 14, etc.) changes, possibly representing different years or experiment conditions. The y-axis is labeled from -1 to 1 and likely represents the performance scores of the evaluation metrics.\n\nSome of the metrics being compared include:\n\n- BEER\n- CharacTER\n- EED\n- ESIM\n- LASIM\n- LP\n- NIST\n- PER\n- TER\n- UNI\n- USFD\n- USFD-TL\n- YiSi-0\n- YiSi-1\n- YiSi-1_srl\n- YiSi-2\n- YiSi-2_srl\n- ibm1-morpheme\n- ibm1-pos4gram\n- sacreBLEU-BLEU\n- sacreBLEU-chrF\n\nEach graph is color-coded by the name of the metric it represents. The gray lines in each graph likely represent other data related to the metric, such as baseline scores or confidence intervals. The graphs are presented in a grid layout to facilitate comparison between the different metrics."}
{"layout": 242, "type": "text", "text": "A.6 en-fi ", "text_level": 1, "page_idx": 23, "bbox": [72, 585, 127, 597], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 243, "type": "image", "page_idx": 23, "img_path": "layout_images/W19-5302_20.jpg", "bbox": [71, 607, 527, 765], "page_size": [595.280029296875, 841.8900146484375], "ocr_text": ", BEER CharacTER BED: ESIM NIST.\n\n7h PER TER: UNI YiSi-0 YiSi-1\nie}\n=k YiSi-2 ibm1-morpheme sacreBLEU-BLEU sacreBLEU-chrF 10\n\nto 8 6 641008 ~« «6lhlUu4 100C«t“<i‘~iSC‘iC KCC\n", "vlm_text": "The image contains a series of line plots organized in a grid format. Each plot represents a specific metric or evaluation measure, listed with different names like BEER, CharacTER, EED, ESIM, NIST, PER, TER, UNI, YiSi-0, YiSi-1, YiSi-2, ibm1-morpheme, sacreBLEU-BLEU, and sacreBLEU-chrF. The x-axis of each plot ranges from 4 to 10, and the y-axis ranges from -1 to 1. The plots seem to indicate trends or performance levels of these metrics, with lines of different colors representing the data points across the x-axis. The coloring and positioning of the lines suggest some comparisons or evaluations are being depicted, possibly related to algorithms, translations, machine learning models, or statistical performance measures. Without additional context or caption information, it is not possible to determine what specific aspect is being evaluated or compared in these plots."}
{"layout": 244, "type": "text", "text": "A.7 en-gu ", "page_idx": 24, "bbox": [72.0, 61.40699005126953, 132.28369140625, 76.98518371582031], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 245, "type": "image", "page_idx": 24, "img_path": "layout_images/W19-5302_21.jpg", "bbox": [70, 102, 527, 263], "page_size": [595.280029296875, 841.8900146484375], "ocr_text": "CharacTER\n\nPER:\n\nsacreBLEU-BLEU\nSeen\n\nEED LEPORa\n—[Sor\"—\n\nTER YiSi-0\n—\"\n\nsacreBLEU-chrF 9 8\n——\"\n\nLEPORb\n\nYiSi-1\n", "vlm_text": "The image contains a series of small line graphs arranged in a grid layout. Each graph represents the performance metric of a different evaluation measure used in natural language processing or machine translation. The evaluation measures are labeled above each graph: BEER, CharacTER, EED, LEPORa, LEPORb, NIST, PER, TER, YiSi-0, YiSi-1, YiSi-2, sacreBLEU-BLEU, and sacreBLEU-chrF.\n\nThe x-axis of each graph appears to have numerical values (9, 8, 6, 4), possibly indicating different data points or models. The y-axis is labeled with values from -1 to 1, which likely denote the scaled scores. Most lines in the graphs have scores that hover around the upper part of the scale, indicating performance or correlation levels. YiSi-2 stands out with a more varied line, showing fluctuations that break away from the horizontal alignment observed in other measures."}
{"layout": 246, "type": "text", "text": "A.8 en-kk ", "text_level": 1, "page_idx": 24, "bbox": [72, 318, 133, 330], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 247, "type": "image", "page_idx": 24, "img_path": "layout_images/W19-5302_22.jpg", "bbox": [70, 353, 528, 516], "page_size": [595.280029296875, 841.8900146484375], "ocr_text": "LEPORa\n\nYiSi-O\n", "vlm_text": "The image is a collection of line charts, each representing different metrics or evaluation scores. There are twelve individual plots, each labeled with a different metric or evaluation method: BEER, CharacTER, EED, ESIM, LEPORa, LEPORb, NIST, PER, TER, YiSi-0, YiSi-1, YiSi-2, sacreBLEU-BLEU, and sacreBLEU-chrF.\n\nEach plot contains a line indicating the trend of the metric from position 9 to 4 along the horizontal axis. The lines in each plot vary in color, matching the label color. There are also gray lines and shaded areas behind the main lines, which could represent additional data or variability across multiple conditions. The vertical axis range runs from -1 to 1. The segmented background suggests different zones or categories on the horizontal axis."}
{"layout": 248, "type": "text", "text": "A.9en-lt", "text_level": 1, "page_idx": 24, "bbox": [72, 570, 128, 582], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 249, "type": "image", "page_idx": 24, "img_path": "layout_images/W19-5302_23.jpg", "bbox": [71, 605, 527, 765], "page_size": [595.280029296875, 841.8900146484375], "ocr_text": ", BEER D ESIM LEPO\n; 2 z\n\ni NIST PER TER: YiSi-0 YiSi-1\n0 \\\n\nsacreBLEU-BLEU sacreBLEU-chrF 10\n— ae 7\n\n", "vlm_text": "The image shows a series of line graphs that display the performance of various metrics over a given range. Each graph represents a different metric related to natural language processing or machine translation, including BEER, CharacTER, EED, ESIM, LEPORb, NIST, PER, TER, YiSi-0, YiSi-1, YiSi-2, ibm1-morpheme, sacreBLEU-BLEU, and sacreBLEU-chrF. These metrics are used to evaluate the quality of translated text against a reference translation. The x-axis in each graph is labeled with values possibly indicating different data points or time periods (10 to 4), while the y-axis ranges from -1 to 1, possibly representing standardized scores. Each metric is represented by a distinct color line for easy differentiation among the graphs. Some graphs also include additional gray lines that might represent other variations or benchmarks in the evaluation."}
{"layout": 250, "type": "image", "page_idx": 25, "img_path": "layout_images/W19-5302_24.jpg", "img_caption": "A.10 en-ru ", "bbox": [71, 64, 526, 284], "page_size": [595.280029296875, 841.8900146484375], "ocr_text": "1 BEER\n\nCharacTER\nNIST\nUSFD\n\narma oa\n\nsacreBLEU-BLEU\nNN\n\nBED ESIM\nSS\n\nPER TER\ndt\nUSED-TL YiSi-O\n\nsacreBLEU-chrF 10 8\n\n4\n\n1\n\nLASIM\n\nseme\n\nUNI\n_——\n—\nYiSi-\n\n0 8 6 4\n\ni-1\n", "vlm_text": "This image appears to be a collection of line charts showing the results of different metrics or evaluation scores across several models or datasets. Each small plot is labeled with a metric name such as \"BEER\", \"CharacTER\", \"EED\", \"ESIM\", \"LASIM\", \"LP\", \"NIST\", \"PER\", \"TER\", \"UNI\", \"UNI+\", \"USFD\", \"USFD-TL\", \"YiSi-0\", \"YiSi-1\", \"YiSi-2\", \"sacreBLEU-BLEU\", and \"sacreBLEU-chrF\". The x-axis of these plots seems to represent a discrete numerical scale from 10 to 4, while the y-axis appears to range from -1 to 1. The main trend line for each metric is further differentiated by color, indicating the performance trend or behavior of that particular metric across the given x-axis values. It seems the intent of this image is to compare different metrics in a text (likely machine translation or language evaluation) setting. The background lines in gray might represent other data points or comparisons for context."}
{"layout": 251, "type": "image", "page_idx": 25, "img_path": "layout_images/W19-5302_25.jpg", "img_caption": "A.11 en-zh ", "bbox": [71, 304, 526, 477], "page_size": [595.280029296875, 841.8900146484375], "ocr_text": "CharacTER\n\nTER\n———__\n\nYiSi-2_srl\n\nEED\n——_——\n\nYiSi-0\n\nsacreBLEU-BLEU\n—_\n\nESIM\n\nYiSi-1\n\nsacreBLEU-chrF\n\nNIST\n——_—\n\nYiSi-1 srl\n\n10 8 6 4\n", "vlm_text": "The image contains a series of line graphs comparing different evaluation metrics. Each graph represents a different metric, with labels such as BEER, CharacTER, EED, ESIM, NIST, PER, TER, YiSi-0, YiSi-1, YiSi-2, sacreBLEU-BLEU, and sacreBLEU-chrF. The x-axis ranges from 10 to 4, and the y-axis ranges from -1 to 1. The metrics are likely used to evaluate machine translation or text similarity models. Each graph likely shows performance trends over a series of evaluations."}
{"layout": 252, "type": "text", "text": "A.12 fi-en ", "text_level": 1, "page_idx": 25, "bbox": [72, 496, 133, 509], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 253, "type": "image", "page_idx": 25, "img_path": "layout_images/W19-5302_26.jpg", "bbox": [72, 511, 527, 767], "page_size": [595.280029296875, 841.8900146484375], "ocr_text": ", BEER ine, haracTER\n\nie) , TN\n\nMeteor++ _ 2.0 Meteor++ _2.0(+copy NIST =\n“} TER NI Nit WMDO a\n\n4 YiSi-1 YiSi-1_srl YiSi-2 ibm1-morpheme sacreBLEU-BLEU\n\n. \\ VY ox\n\n2 sacreBLEU-chrF 10 8 6 4 10 8 6 4 10 8 6 4 10 8 6 4\n\nPR\n\nfo}\n\n0\n\n“ho 8 6 4\n", "vlm_text": "This image contains a series of small line graphs organized in a grid pattern. Each graph represents a different method or metric, as indicated by the text above each plot, and plots a line that typically starts near the 1 on the vertical axis and generally trends downward towards the 4 on the horizontal axis. The methods or metrics labeled include BEER, BERT r, CharacTER, EED, ESIM, Meteor++ 2.0, Meteor++ 2.0(+copy), NIST, PER, PReP, TER, UNI, UNI+, WMDO, YiSi-0, YiSi-1, YiSi-1 srl, YiSi-2, ibm1-morpheme, sacreBLEU-BLEU, and sacreBLEU-chrF. These labels likely represent different evaluation metrics or algorithms used in a specific domain, possibly relating to machine translation or text analysis, given the names of the metrics and common evaluation standards in such areas. The graphs seem to demonstrate how these metrics perform or change over different data points or evaluation scenarios."}
{"layout": 254, "type": "text", "text": "A.13fr-de", "text_level": 1, "page_idx": 26, "bbox": [72, 63, 135, 76], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 255, "type": "image", "page_idx": 26, "img_path": "layout_images/W19-5302_27.jpg", "bbox": [71, 87, 526, 247], "page_size": [595.280029296875, 841.8900146484375], "ocr_text": "1 BEER\n\nCharacTER\n\nTER:\n\nibm1-morpheme\n\nEED\n\nYiSi-O\n\nibm1-pos4gram__\n\nESIM\n\nYiSi-1\n\nsacreBLEU-BLEU\n\nYiSi-1_srl\n\nsacreBLEU-chrF\n", "vlm_text": "This image appears to be a series of small line graphs displaying scores or values for different metrics commonly used in evaluating natural language processing tasks, such as machine translation. The metrics shown include names like BEER, CharacTER, EED, ESIM, NIST, PER, TER, YiSi-0, YiSi-1, YiSi-1_srl, YiSi-2, ibm1-morpheme, ibm1-pos4gram, sacreBLEU-BLEU, and sacreBLEU-chrF. These metrics are placed in individual panels, with a consistency in scale ranging from -1 to 1 on the y-axis. The x-axis in each graph ranges from 8 to 4, with a decreasing trend line visible only for YiSi-2, while the other graphs maintain relatively flat lines near the zero mark. The graphs appear to be part of a visualization for evaluating or comparing different machine translation quality metrics."}
{"layout": 256, "type": "text", "text": "A.14 gu-en ", "page_idx": 26, "bbox": [72.0, 273.1409912109375, 138.55642700195312, 288.71917724609375], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 257, "type": "image", "page_idx": 26, "img_path": "layout_images/W19-5302_28.jpg", "bbox": [72, 300, 526, 506], "page_size": [595.280029296875, 841.8900146484375], "ocr_text": ", BEER\n0\n\nfi Meteor++ 2.0\n\nie}\n\na1\n7 TER\n\nBERTr\n\nCharacTER\n\nMeteor++ 2.0(tcopy NIST\n\nWMDO\n\nsacreBLEU-BLEU\n\nYiSi-O\n\nsacreBLEU-chrF\n\nPER\n\nYiSi-1\n\nESIM\n\nPReP.\n\nYiSi-1_srl\n", "vlm_text": "The image consists of a series of small line graphs, each labeled with different metrics or evaluation methods commonly used in natural language processing or machine translation. These labels include BEER, BERT, CharacTER, and others. Each graph appears to show a score or evaluation trend over a range (indicated by numbers 9 to 4 along the x-axis). The y-axis seems to range from -1 to 1. Each line graph represents a different evaluation metric, and slight variations or trends can be observed in the lines across different metrics."}
{"layout": 258, "type": "text", "text": "A.15 kk-en ", "text_level": 1, "page_idx": 26, "bbox": [72, 537, 140, 549], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 259, "type": "image", "page_idx": 26, "img_path": "layout_images/W19-5302_29.jpg", "bbox": [72, 560, 526, 766], "page_size": [595.280029296875, 841.8900146484375], "ocr_text": "BERTr\nLEPORb\n\nPReP\n\nYiSi-1_ srl\n\nCharacTER\n\nMeteor++ 2.0\n\nYiSi-2\n\nEED\n\nMeteor++ 2.0(+copy)NIST\n\nESIM\n\nYiSi-O\n\nsacreBLEU-chrF\n", "vlm_text": "The image is a collection of 16 line graphs, each depicting data trends across a scale running from 9 to 4 on the x-axis and -1 to 1 on the y-axis. Each graph represents different metrics or methodologies used to evaluate translation quality, such as BEER, BERTtr, CharacTER, EED, ESIM, LEPOR (a and b versions), Meteor++ 2.0, NIST, PER, PReP, TER, WMDO, YiSi-0, YiSi-1, YiSi-1 srl, YiSi-2, sacreBLEU-BLEU, sacreBLEU-chrF, etc. These graphs likely display correlations or performance comparisons of these metrics as they relate to some aspect of translation evaluation, machine translation, or similar topics. The lines on each graph show similar trends, mostly clustered between 0 and 1, suggesting minimal variation across the scale presented."}
{"layout": 260, "type": "image", "page_idx": 27, "img_path": "layout_images/W19-5302_30.jpg", "bbox": [71, 131, 528, 340], "page_size": [595.280029296875, 841.8900146484375], "ocr_text": ", BEER BERTr CharacTER EED ESIM\n\n+ LEPORb Meteor++ 2.0 Meteor++_ 2.0(+copy)NIST. PER\n\n0\n\n“t PReP TER WMDO YiSi-0 YiSi-1\n\n0\n\n— YiSi-1_ srl YiSi-2 ibm1- ee sacrepee’ -BLEU sacreBLEU- -chrF\n9 8 6 4\n\ntog 6 4\n", "vlm_text": "The image consists of multiple small line plots arranged in a grid format, each representing different metrics or models used for evaluation purposes. Each plot is labeled at the top with the name of a metric or model, such as \"BEER,\" \"BERTr,\" \"CharacTER,\" \"EED,\" \"ESIM,\" and so on. The Y-axis of each plot ranges from -1 to 1, and the X-axis ranges from 9 to 4. Each line plot depicts the performance of the respective metric or model across this range. The purpose of these plots appears to be a comparison of the performance trends of different metrics or models, possibly in the context of natural language processing evaluation."}
{"layout": 261, "type": "text", "text": "A.17 ru-en ", "text_level": 1, "page_idx": 27, "bbox": [72, 445, 138, 457], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 262, "type": "image", "page_idx": 27, "img_path": "layout_images/W19-5302_31.jpg", "bbox": [70, 510, 527, 766], "page_size": [595.280029296875, 841.8900146484375], "ocr_text": "1, BEER\n\nqT + sacreBLEU- BLEU\n\nBERTr\n\nsacreBLEU-chrF\n\nRol Bl\n\n“a2 10 8 6\n\n4 12 10 8 6\n\n4\n\nCharacTER\n\nESIM\n\nfaa San ae\n\nMeteor++_2.0\n\nMeteor++_2.0(+copy)NIST\n\nTER\n\n—_\n\n12 10 8 6\n\nYiSi-1_ srl an\n\n4 12 10 8 6 4 12 10 8 6 4\n", "vlm_text": "The image shows a set of line graphs evaluating various metrics or models. Each small plot corresponds to a different metric or model name labeled at the top. The plots are arranged in multiple columns, with names such as BEER, BERT, CharacTER, EED, and others. \n\nOn each plot, the x-axis seems to be labeled with numbers likely indicating a scale, while the y-axis range is from 1 to -1. Each plot has a colored line representing performance that trends downward as it moves rightwards, possibly indicating decreasing performance or evaluations over a given range. Overall, it seems the image compares different evaluation or performance metrics for various models or conditions."}
{"layout": 263, "type": "text", "text": "A.18 zh-en ", "text_level": 1, "page_idx": 28, "bbox": [72, 64, 138, 73.75], "page_size": [595.280029296875, 841.8900146484375]}
{"layout": 264, "type": "image", "page_idx": 28, "img_path": "layout_images/W19-5302_32.jpg", "bbox": [71, 74.25, 526, 326], "page_size": [595.280029296875, 841.8900146484375], "ocr_text": ", BEER BERTr CharacTER ESIM\n\n6 , ,\n-1\n\n1 LEPORa LEPORb Meteor++_2.0 Meteors | any eS\n\nYiSi-1 srl\n\nYiSi-2_srl sacreBLEU-BLEU\n— a ~ 4 oe\n\n~} sacreBLEU-chrF 1312 10 8 6 4 1312 10 8 6 4 1312 10 8 6 4 1312 10 8 6 4\n:\n\n0 Z\n\n“4312 10 8 6 4\n", "vlm_text": "This image contains a series of line plots arranged in a grid format, each representing the performance of different language evaluation metrics over time or across different conditions. The metrics shown include BEER, BERTr, CharacTER, EED, ESIM, LEPORa, LEPORb, Meteor++ 2.0, Meteor++ 2.0(+copy), NIST, PER, PReP, TER, WMDO, YiSi-0, YiSi-1, YiSi-1_srl, YiSi-2, YiSi-2_srl, sacreBLEU-BLEU, and sacreBLEU-chrF. Each plot appears to indicate some form of evaluation score or performance measure over a number of conditions or iterations, numbered 13, 12, 10, 8, 6, and 4, although the exact nature of the evaluation is not specified. The different colors of the lines correspond to the different metrics."}
