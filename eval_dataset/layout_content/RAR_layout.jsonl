{"layout": 0, "type": "text", "text": "RAR :  R etrieving    $\\mathbf{A}$  nd    $\\mathrm{R}$  anking Augmented MLLMs for Visual Recognition ", "text_level": 1, "page_idx": 0, "bbox": [154, 111, 459, 147], "page_size": [612.0, 792.0]}
{"layout": 1, "type": "text", "text": "Ziyu Liu  $^{*1,4}$  , Zeyi Sun  $^{*2,4}$  , Yuhang Zang  $^4$  , Wei Li  $^6$  , Pan Zhang  $^4$  , Xiaoyi Dong  $^4$  , Yuanjun Xiong  $^5$  , Dahua Lin  $^{3,4}$  , Jiaqi Wang † 4  $^{1}$  Wuhan University  $^2$  Shanghai Jiao Tong University 3 The Chinese University of Hong Kong 4 Shanghai AI Laboratory 5 MThreads, Inc. 6 Nanyang Technological University 2020302121195@whu.edu.cn, szy2023@sjtu.edu.cn, {zangyuhang, zhangpan, dongxiaoyi, wangjiaqi}@pjlab.org.cn https://github.com/Liuziyu77/RAR ", "page_idx": 0, "bbox": [141.70697021484375, 169.26202392578125, 473.14703369140625, 265.50201416015625], "page_size": [612.0, 792.0]}
{"layout": 2, "type": "text", "text": "Abstract.  CLIP (Contrastive Language–Image Pre-training) uses con- trastive learning from noise image-text pairs to excel at recognizing a wide array of candidates, yet its focus on broad associations hinders the precision in distinguishing subtle differences among fine-grained items. Conversely, Multimodal Large Language Models (MLLMs) excel at clas- sifying fine-grained categories, thanks to their substantial knowledge from pre-training on web-level corpora. However, the performance of MLLMs declines with an increase in category numbers, primarily due to growing complexity and constraints of limited context window size. To synergize the strengths of both approaches and enhance the few- shot/zero-shot recognition abilities for datasets characterized by exten- sive and fine-grained vocabularies, this paper introduces  RAR , a  R etrieving A nd  R anking augmented method for MLLMs. We initially establish a multi-modal retriever based on CLIP to create and store explicit memory for different categories beyond the immediate context window. During in- ference,  RAR  retrieves the top-  $k$   similar results from the memory and uses MLLMs to rank and make the final predictions. Our proposed ap- proach not only addresses the inherent limitations in fine-grained recog- nition but also preserves the model’s comprehensive knowledge base, significantly boosting accuracy across a range of vision-language recog- nition tasks. Notably, our approach demonstrates a significant improve- ment in performance on 5 fine-grained visual recognition benchmarks, 11 few-shot image recognition datasets, and the 2 object detection datasets under the zero-shot recognition setting. ", "page_idx": 0, "bbox": [163, 296.4947814941406, 461, 559.9010009765625], "page_size": [612.0, 792.0]}
{"layout": 3, "type": "text", "text": "Keywords:  MLLM · Fine-Grained · Few-shot · Zero-shot Recognition ", "page_idx": 0, "bbox": [163, 570.7015380859375, 449, 582.1068115234375], "page_size": [612.0, 792.0]}
{"layout": 4, "type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0, "bbox": [134, 602, 229, 616], "page_size": [612.0, 792.0]}
{"layout": 5, "type": "text", "text": "The CLIP (Contrastive Language–Image Pre-training) [ 41 ] model and its diverse variants [ 8 ,  26 ,  45 ] provide flexible and robust performance across a wide array of visual-language understanding tasks. Despite its successes, we observe that ", "page_idx": 0, "bbox": [134, 629.6768798828125, 480, 666.1198120117188], "page_size": [612.0, 792.0]}
{"layout": 6, "type": "image", "page_idx": 1, "img_path": "layout_images/RAR_0.jpg", "img_caption": "Fig. 1: Upper left : our motivation about the drawbacks of CLIP and MLLM. Our RAR  can seamlessly integrate into MLLMs to improve the few-shot/zero-shot abilities on classification ( upper right ) and detection ( bottom ) datasets. ", "bbox": [132, 119, 483, 360], "page_size": [612.0, 792.0], "ocr_text": "Motivation\n\nClassification+RAR\n\nClassification Datasets\n\nLIP, fails in fn\n\nFine-grained\n\ni BAE 146.300\ni ATR\n\n| Vaket2\nt\n\nDHC-8-100\nDornier 328\n\nCaltech101 UCF 101. Imagenet\n\nre saison dates\n8\n\nforLLava fr CLIPANN gag\nfarcLiP » 70 Ea eran\nliz 5 « ea @\n| F | i i:\n\nRarpR DID abot\n\n| Large Vocabulary || topl: pastry improved detection ability\nDetection Datasets top2: pinwheel b et\n\ntop3: doughnut L ise |\n\n( top1: doughnut\ntop2: pastry\ntop3: pinwheel\n\n‘CLIPSOTA Ours CLIP Ours ©\n\nV3Det\n\nGT: Doughnut\n\n", "vlm_text": "The image presents a visual summary of the research study on enhancing the performance of CLIP and MLLM using RAR. Here's an overview:\n\n- **Motivation**: Highlights the challenges of using VLMs like CLIP for fine-grained classification and MLLM for large vocabulary classification. It displays the performance issues with particular aircraft models and classification datasets.\n\n- **Classification+RAR**: Shows improved accuracy on classification datasets, both fine-grained (like Food101 and Flowers102) and common (like Caltech101). It illustrates the correction of a misclassification from \"Azalea\" to \"Clematis\" using RAR, and overall improved accuracy across datasets.\n\n- **Detection+RAR**: Demonstrates improvements in detection on large vocabulary datasets such as LVIS and V3Det. The RAR approach enhances detection by reranking and correcting initial predictions, evidenced by higher AP scores.\n\nOverall, the image emphasizes the seamless integration of RAR into MLLMs to improve few-shot/zero-shot abilities in both classification and detection tasks."}
{"layout": 7, "type": "text", "text": "CLIP’s performance begins to wane when faced with datasets characterized by vast vocabularies or fine-grained categories. As shown in the upper left of Fig.  1 , the decline is largely attributable to the inherent ambiguity of language descrip- tions and the challenges posed by synonyms, which can confound the model’s ability to distinguish between closely related but distinct classes. ", "page_idx": 1, "bbox": [134, 374.6560363769531, 480, 435.00994873046875], "page_size": [612.0, 792.0]}
{"layout": 8, "type": "text", "text": "Parallel to these developments, Multi-modal Large Language Models (MLLMs) have emerged as a powerful class of generative models, exemplified by the likes of GPT-4V [ 38 ] and analogous advancements [ 1 – 3 , 6 , 29 , 40 , 49 , 55 , 56 , 60 ]. MLLMs, pre-trained on extensive corpora with substantial knowledge, demonstrate re- markable proficiency in identifying fine-grained categories when the total num- ber of candidates remains manageable. Nevertheless, MLLMs’ efficacy is simi- larly compromised in scenarios involving extensive vocabularies and fine-grained categorizations (upper left of Fig.  1 ). The core of the issue lies in MLLMs fac- ing significant challenges in managing large context windows ( e.g ., maximum 2k tokens for LLaVA1.5 [ 28 ]), a critical requirement for accurate processing and interpreting tasks that demand a nuanced understanding of vast vocabularies and subtle distinctions. ", "page_idx": 1, "bbox": [134, 436.4129943847656, 486.5244140625, 580.452880859375], "page_size": [612.0, 792.0]}
{"layout": 9, "type": "text", "text": "To address these challenges, we propose augmenting standard MLLMs with our  RAR , a retrieving-and-ranking augmented technique. Our  RAR  enables models to dynamically incorporate external knowledge into the processing and generation workflows. By augmenting MLLMs with external knowledge sources, we address challenges related to language ambiguity, synonym handling, and the limitations imposed by limited context windows when dealing with vast vocab- ularies. Our method uses the inherent strength of MLLMs in generalizing from ", "page_idx": 1, "bbox": [134, 581.8558959960938, 480, 666.119873046875], "page_size": [612.0, 792.0]}
{"layout": 10, "type": "text", "text": "existing knowledge while addressing their limitations in visual recognition. We first construct a multi-modal retriever that creates and stores multimodal em- beddings for visual images and text descriptions. As shown in Fig.  1 , upon receiv- ing an input image at the inference stage, our approach retrieves the top-  $k$   class names most similar to the image. Subsequently, the MLLMs rank these retrieved candidate results as the final prediction results. To bolster the MLLMs’ rank- ing performance, we explore fine-tuning with ranking format data or in-context learning examples without training. By integrating our retrieval-augmented de- sign, our approach seeks to bridge the gap between the broad generalization capabilities of MLLMs and the need for precise, fine-grained categorization, of- fering a path forward that preserves the model’s extensive knowledge base while significantly boosting its performance on downstream tasks. ", "page_idx": 2, "bbox": [134, 116.46308135986328, 480, 260.503173828125], "page_size": [612.0, 792.0]}
{"layout": 11, "type": "text", "text": "To evaluate our method’s efficacy, we conducted benchmarks in three ar- eas: (1) fine-grained visual recognition across 5 benchmarks, (2) few-shot image recognition across 11 datasets, and (3) zero-shot object recognition on 2 object detection datasets with vast vocabularies ( e.g ., 13204 classes of V3Det [ 48 ]). As presented in the right part of Fig.  1 , our findings reveal that our approach no- tably enhances few-shot learning abilities, yielding an average improvement of  $6.2\\%$   over 11 image classification datasets under the 4-shot setting. Furthermore, our method achieves a    $6.4\\%$   improvement on the LVIS dataset and a    $1.5\\%$   gain on the V3Det dataset in zero-shot object recognition performance. ", "page_idx": 2, "bbox": [134, 260.18621826171875, 480, 368.3600769042969], "page_size": [612.0, 792.0]}
{"layout": 12, "type": "text", "text": "In summary, our key contributions are outlined as follows: (1) We conduct an in-depth analysis of the strengths and weaknesses of VLMs and MLLMs in processing fine-grained datasets. (2) To enhance the fine-grained few-shot and zero-shot perception capabilities of MLLMs, we introduce  RAR  with a multi- modal retriever and the inference pipeline based on retrieving and ranking. (3) Our  RAR  can be seamlessly integrated into various MLLMs in a plug-and-play manner. (4) Through rigorous testing across 11 classification datasets and 2 ob- ject detection datasets, we demonstrate that our method outperforms baselines on a variety of visual recognition tasks. ", "page_idx": 2, "bbox": [134, 368.0431213378906, 480, 476.21697998046875], "page_size": [612.0, 792.0]}
{"layout": 13, "type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2, "bbox": [134, 495, 237, 508], "page_size": [612.0, 792.0]}
{"layout": 14, "type": "text", "text": "Contrastive Language-Image Pre-training (CLIP)  [ 41 ] understands im- ages and texts by contrastive learning from a vast amount of visual data paired with natural language descriptions. CLIP has robust capabilities in downstream tasks including image-text retrieval [ 54 ], zero-shot classification [ 12 ,  58 ], and open-vocabulary perception [ 13 ,  59 ]. Following CLIP, many subsequent vision- language models [ 8 , 10 , 18 , 23 , 24 , 26 , 33 , 46 , 53 , 57 ] are proposed to further improve the vision-language understanding abilities. There are also works done to improve CLIP in zero-shot perception tasks [ 27 , 42 , 44 , 52 ]. However, simple dot-product between two uni modality features can lead to sub-optimal results for fine-grained classification. In this paper, we demonstrate that CLIP faces challenges in mak- ing accurate zero-shot predictions for fine-grained classes, and how our proposed method can effectively re-rank these predictions to improve the accuracy. ", "page_idx": 2, "bbox": [134, 522.0611572265625, 480, 666.1209716796875], "page_size": [612.0, 792.0]}
{"layout": 15, "type": "text", "text": "Multimodal Large Language Models  (MLLMs) such as GPT4V [ 38 ], repre- sent a significant evolution in the landscape of Large Language Models (LLMs) by integrating visual images as input tokens alongside textual information. The integration is facilitated through the use of an additional vision encoder [ 41 ] and a bridging mechanism [ 1 – 3 , 6 , 29 , 40 , 49 , 55 , 56 , 60 ]. MLLMs significantly en- hance the interaction between humans and AI in more natural and intuitive ways and demonstrate remarkable capabilities in understanding and generating multi-modal content. Despite their prowess, our research uncovers a nuanced limitation: MLLMs tend to under perform in tasks requiring vast vocabularies, where distinguishing subtle differences among different categories is crucial. How- ever, we prove that MLLMs exhibit a strong ability to excel in the re-ranking of top results obtained through vision-language models such as CLIP. Fine-R [ 31 ] first delves into leveraging MLLMs for fine-grained perception tasks by prompt design for better descriptions and attributes. We find a new way to prompt it with possible candidates to help screening and achieve better performance. ", "page_idx": 3, "bbox": [134, 116.44315338134766, 480, 296.3691711425781], "page_size": [612.0, 792.0]}
{"layout": 16, "type": "text", "text": "Retrieval-Augmented Generation  (RAG) [ 21 ] refers to the solution of in- corpora ting knowledge from external databases for LLMs, which helps reduce hallucination, continuous knowledge updates, and integration of domain-specific information. Specifically, RAG models first retrieve the relevant knowledge to the given text query from the external knowledge base and then augment the LLMs with the retrieved knowledge. In computer vision, some previous works explore retrieval-augmented approaches with VLMs for long-tailed classification [ 17 , 32 ], image-text retrieval [ 30 ] or image generation [ 54 ]. Different from previous works, our paper first designs a retrieval-augmented solution for MLLMs. Our research investigates how incorporating image-image retrieval, image-text retrieval, and an explicit memory component can enhance the zero-shot and few-shot capabil- ities of MLLMs. ", "page_idx": 3, "bbox": [134, 295.7712707519531, 480, 439.8310241699219], "page_size": [612.0, 792.0]}
{"layout": 17, "type": "text", "text": "3 Methodology ", "text_level": 1, "page_idx": 3, "bbox": [134, 457, 232, 469], "page_size": [612.0, 792.0]}
{"layout": 18, "type": "text", "text": "We first provide the background information on CLIP, MLLMs, and retrieval- augmentation in LLMs (Sec.  3.1 ). Then we present the multi-modal retriever (Sec.  3.2 ) module of  RAR  and how to apply  RAR  on downstream tasks via retrieving and ranking (Sec.  3.3 ). ", "page_idx": 3, "bbox": [134, 479.800048828125, 480, 528.197998046875], "page_size": [612.0, 792.0]}
{"layout": 19, "type": "text", "text": "3.1 Preliminaries ", "text_level": 1, "page_idx": 3, "bbox": [134, 544, 228, 555], "page_size": [612.0, 792.0]}
{"layout": 20, "type": "text", "text": "CLIP  is a model combining an image encoder    $\\varPhi_{\\mathrm{img}}$   and a text encoder    $\\varPhi_{\\mathrm{{ext}}}$   that uses contrastive learning to understand and align images and text by training on a vast dataset gathered from the web. The core mechanism of CLIP involves mapping an input image  $\\mathcal{L}$   to its most semantically similar category    $c\\in\\mathcal C$  : ", "page_idx": 3, "bbox": [134, 562.1701049804688, 480, 610.5880126953125], "page_size": [612.0, 792.0]}
{"layout": 21, "type": "equation", "text": "\n$$\np(y=c|\\mathbf{x})=\\arg\\operatorname*{max}_{c\\in\\mathcal{C}}\\cos(\\varPhi_{\\mathrm{img}}(\\mathcal{Z}),\\varPhi_{\\mathrm{stat}}(c))\\,,\n$$\n ", "text_format": "latex", "page_idx": 3, "bbox": [213, 617, 401, 635], "page_size": [612.0, 792.0]}
{"layout": 22, "type": "text", "text": "where    represents the predicted category,    $\\mathcal{C}$   refers to the whole categories list  $y$  and    $\\cos(\\cdot,\\cdot)$   denotes to the cosine similarity. ", "page_idx": 3, "bbox": [134, 641.6320190429688, 480, 666.1199951171875], "page_size": [612.0, 792.0]}
{"layout": 23, "type": "text", "text": "Multimodal Large Language Models  such as GPT4V [ 38 ] learning to gener- ate predictions over sequences of tokens that span both image and text modali- ties. The MLLM model    $f$  , parameterized by weights    $\\theta$  , conditioned on the input sequences    $\\mathbf{x}=\\left(x_{1},\\cdot\\cdot\\cdot,x_{L_{i n}}\\right)$   of length    $L_{i n}$  , which consist of both text tokens  $\\mathbf{x}_{\\mathrm{{tot}}}$   and visual tokens    $\\mathbf{x}_{\\mathrm{img}}$  . The    $\\mathbf{x}_{\\mathrm{img}}$   are extracted from the input image    $\\mathcal{L}$  via the image encoder    $\\varPhi_{\\mathrm{img}}$  . MLLM model forecast a sequence of output tokens  $\\mathbf{y}=(y_{1},.\\,.\\,.\\,,y_{L_{o u t}})$   of length    $L_{o u t}$   as follows: ", "page_idx": 4, "bbox": [134, 116.44315338134766, 480, 202.2220458984375], "page_size": [612.0, 792.0]}
{"layout": 24, "type": "equation", "text": "\n$$\np_{\\theta}(\\mathbf{y}|\\mathbf{x})=\\prod_{l=1}^{L_{o u t}}\\,p_{\\theta}(y_{l}|\\mathbf{x},\\mathbf{y}_{\\leq l-1})=\\prod_{l=1}^{L_{o u t}}\\,\\mathrm{softmax}(f(\\mathbf{x},\\mathbf{y}_{\\leq l-1};\\theta))_{y_{l}}\\,,\n$$\n ", "text_format": "latex", "page_idx": 4, "bbox": [150, 209, 451, 231], "page_size": [612.0, 792.0]}
{"layout": 25, "type": "text", "text": "where    $\\mathbf{y}_{\\leq l-1}:=(y_{1},.\\,.\\,.\\,,y_{l-1})$   refers to the mechanism that predicts the distri- bution of the next token considering all previously generated tokens. ", "page_idx": 4, "bbox": [134, 239.37603759765625, 480, 263.864013671875], "page_size": [612.0, 792.0]}
{"layout": 26, "type": "text", "text": "Retrieval-Augmentation in Large Language Models  introduces a retrieval module    $R$   with the LLM parameterized by    $\\theta$   for generation. The retrieval module  $R$   is designed to process an input sequence    $\\mathbf{x}$   against an external memory of documents    $\\mathcal{M}$  , efficiently selecting a subset of documents    $M\\subseteq\\mathcal{M}$  . Th  subset  $M$   is then fed along with the original input sequence    $\\mathbf{x}$   into the LLM  θ , which uses both the input and the context provided by retrieved results to generate the target output    $\\mathbf{y}$  : ", "page_idx": 4, "bbox": [134, 263.4060974121094, 480, 347.6899108886719], "page_size": [612.0, 792.0]}
{"layout": 27, "type": "equation", "text": "\n$$\np_{\\theta}(\\mathbf{y}\\vert\\mathbf{x},M)=\\prod_{l=1}^{L_{o u t}}p_{\\theta}(y_{l}\\vert\\mathbf{x},M,\\mathbf{y}_{\\le l-1}).\n$$\n ", "text_format": "latex", "page_idx": 4, "bbox": [219, 356, 395, 377], "page_size": [612.0, 792.0]}
{"layout": 28, "type": "text", "text": "3.2 Multimodal Retriever ", "text_level": 1, "page_idx": 4, "bbox": [133, 394, 271, 405], "page_size": [612.0, 792.0]}
{"layout": 29, "type": "text", "text": "The multimodal retriever is essentially responsible for querying a large multi- modal external memory or database to find information relevant to the input query or context. In the process of multimodal retriever, the main challenge lies in efficiently encoding and storing a large volume of images/text embeddings for quick, accurate retrieval. Recognizing the main challenge, as shown in Fig.  2 , we have developed a multi-modal retriever that creates and stores multimodal em- beddings, with a focus on optimizing retrieval speed through index construction techniques. ", "page_idx": 4, "bbox": [134, 414.20404052734375, 480, 510.42291259765625], "page_size": [612.0, 792.0]}
{"layout": 30, "type": "text", "text": "Extracting the Multi-modal Embeddings.  We use the CLIP model dis- cussed in Sec.  3.1  to extract the multi-modal embeddings. Given a data sample  $(x_{i},c_{i})$   from the dataset    $\\mathcal{D}$   containing the image    and class name   , we use the  $x_{i}$   $c_{i}$  CLIP image encod  $\\varPhi_{\\mathrm{img}}$   to extract the image embedd  $e_{\\mathrm{img}}\\in\\mathbb{R}^{d}$    and the LIP text encoder  $\\varPhi_{\\mathrm{text}}$   to extract the mbedding  e  $e_{\\mathrm{test}}\\in\\mathbb{R}^{d}$   ∈ . The symbol d  refers to the feature dimension ( e.g .,  d  = 576  for CLIP ViT-B/16). The image and text embeddings are stored in the memory    $\\mathcal{M}$   for retrieval (will discuss in Sec.  3.3 ). In some zero-shot settings, the image embedding is not available and we merely store the text embedding into the memory. ", "page_idx": 4, "bbox": [134, 509.9650573730469, 480, 618.159912109375], "page_size": [612.0, 792.0]}
{"layout": 31, "type": "text", "text": "Fast Retrieval Optimization.  The brute force search is the common method for designing the retriever, which requires iteration over all vectors in the memory  $\\mathcal{M}$   to co pute similarity scores ( e.g ., cosine similarity) and subsequently identify the top- k  results. Although the brute force method is inherently straightforward, ", "page_idx": 4, "bbox": [134, 617.7020263671875, 480, 666.1199340820312], "page_size": [612.0, 792.0]}
{"layout": 32, "type": "image", "page_idx": 5, "img_path": "layout_images/RAR_1.jpg", "img_caption": "Fig. 2: Pipeline of  RAR . (a)  We design a  multimodal retriever  that extracts the image or text embeddings and stores embeddings in an external mem ry    $\\mathcal{M}$  .  (b) For the inference stage of downstream recognition tasks, we  retrieve  top-  $\\cdot k$   categories from the memory and use MLLMs to refine the retrieved results as the final prediction through  ranking . ", "bbox": [132, 121, 483, 354], "page_size": [612.0, 792.0], "ocr_text": "(a) Multimodal Retriever\n\nDatabase Image\nFeature Embeddings\n\nal\n\nFeature Index\n\n(oe)\n\n7\n\nt_y\n\n> Labeis| |---| | “Image-Image k-NN\nRetrieving 1\nImage-Text k-NN\nee!\n\nEmbeddings\n\n’\nRetrieved Top-K Categories tae Mee\nG & tf G & | butterfly\nl Ranking\ntw\n=\" MLM\nCH CC Oe Cx\n\nCombining\n\n", "vlm_text": "The image depicts a two-part pipeline for a process labeled \"RAR\":\n\n1. **Multimodal Retriever (a)**:\n   - **Image Encoder**: Extracts image feature embeddings from a dataset.\n   - **Feature Index**: Stores the embeddings and indexes them for retrieval.\n   - **Memory ($\\mathcal{M}$)**: External storage for embeddings.\n   - **Retrieving Process**: Utilizes k-nearest neighbors (k-NN) for image-image and image-text retrieval.\n\n2. **Retrieving & Ranking (b)**:\n   - **Inference Stage**: An image is encoded into embeddings.\n   - **Top-K Categories**: Retrieved from memory based on similarity.\n   - **Ranking**: Multimodal Large Language Models (MLLMs) are used to refine and rank these categories.\n   - **Final Prediction**: Outputs the predicted label, e.g., \"Monarch butterfly.\"\n\nThe pipeline aims to enhance recognition tasks by combining multi-modal data retrieval and ranking processes."}
{"layout": 33, "type": "text", "text": "its efficiency markedly diminishes as the dataset escalates to the magnitude of millions of embeddings. To enhance the speed of retrieval, we implement an index system that uses the HNSW(Hierarchical Navigable Small World) algorithm [ 35 ]. The adoption of the HNSW methodology facilitates a significant dimensionality reduction, thereby enabling the construction of a more condensed index. Specif- ically, vectors in a    $\\mathbb{R}^{d}$    space of dimension    $d$   are transformed into a reduced    $\\frac{d}{9}$  dimensional space. This reduction in dimensionality plays a pivotal role in en- hancing the speed of the retrieval process. ", "page_idx": 5, "bbox": [134, 374.2200012207031, 480, 470.43896484375], "page_size": [612.0, 792.0]}
{"layout": 34, "type": "text", "text": "Pre-processing for Detection Datasets.  In object detection datasets, our methodology for extracting image embeddings    $e_{\\mathrm{img}}$   is slightly different from the approach discussed previously. As presented in Fig.  3 , we apply two additional pre-processing steps: cropping and blurring. Some previous works have proposed similar methods in CLIP like [ 33 , 53 ]. In the object detection dataset, an image typically contains multiple objects of varying sizes. Some objects may dominate a large portion of the image, whereas others occupy minimal space. Accordingly, our object detection procedure begins with  cropping  the image regions based on proposal bounding box coordinates, subsequently  resizing  the cropped region to a fixed proportion. Moreover, unlike image classification tasks the objects of interest generally appear large and centrally positioned, the objects within object detection datasets are smaller and their positions more varied. To help the MLLMs understand the objects to be detected, we employ a  blurring  technique on the non-target areas surrounding the objects of interest. The blurring strategy is designed to direct the MLLMs’ focus toward the relevant objects, thereby facilitating their identification in object detection tasks. ", "page_idx": 5, "bbox": [134, 474.2400817871094, 480, 666.1199340820312], "page_size": [612.0, 792.0]}
{"layout": 35, "type": "image", "page_idx": 6, "img_path": "layout_images/RAR_2.jpg", "img_caption": "Fig. 3:  Extending our multimodal retriever to  zero-shot recognition  on object detec- tion datasets such as LVIS [ 14 ] and V3Det [ 48 ]. Compared to the classification datasets, we apply the additional pre-processing techniques such as  cropping  and  resizing  to extract the image embeddings. ", "bbox": [133, 115, 481, 267], "page_size": [612.0, 792.0], "ocr_text": "(a) Pre-process (b) Embedding & Retrieve\n\n(SS\n\nBbox1: carnation, bouquet, flower arrangement,\nBbox2: pepper_mill, saltshaker, chopping_board,\nBbox3: flowerpot, vase, glass (drink container), ...\n\n@ Bbox!: flower_arrangement\n® Bbox2: saltshaker\n5 @ Bbox3: vase\n\nbbox2\n\nbbox3\n\n", "vlm_text": "The image is a diagram illustrating a system for object recognition on detection datasets, featuring two main sections:\n\n1. **(a) Pre-process**: \n   - An image with multiple bounding boxes is shown. Each bounding box highlights different objects (e.g., flowers and a vase).\n   - These objects are cropped and resized to create individual embeddings using an Image Encoder.\n\n2. **(b) Embedding & Retrieve**:\n   - The image embeddings are used in a k-nearest neighbors (k-NN) search.\n   - An index is created for these embeddings, linked to a memory storage (Memory M).\n   - The retrieval process associates each bounding box with possible labels: \n     - Bbox1 with objects like \"flower arrangement\"\n     - Bbox2 with \"saltshaker\"\n     - Bbox3 with \"vase\".\n\nThe diagram illustrates how pre-processing techniques like cropping and resizing assist in zero-shot recognition for object detection tasks using multimodal retrieval methods."}
{"layout": 36, "type": "text", "text": "3.3 Inference with    $\\mathbf{R}$  etrieving  A nd    $\\mathbf{R}$  anking ", "text_level": 1, "page_idx": 6, "bbox": [133, 282, 362, 293], "page_size": [612.0, 792.0]}
{"layout": 37, "type": "text", "text": "After successfully constructing memory    $\\mathcal{M}$   by using our multimodal retriever, our next step is to integrate the memory with the retrieval process and use MLLMs to rank the retrieval results and enhance the performance in few-shot/zero- shot perception tasks. ", "page_idx": 6, "bbox": [134, 304.10601806640625, 489.28399658203125, 352.5039367675781], "page_size": [612.0, 792.0]}
{"layout": 38, "type": "text", "text": "For example, in the inference stage of the few-shot image classification task, we first use the visual encoder    $\\varPhi_{\\mathrm{img}}$   to process the input image and obtain the corresponding image embedding   e . The visual encoder is identical to the encoder used in our multi-modal retriever. The image embedding   e  is then navigated through the previously constructed memory index and ranked by similarity to identify the top-  $k$  elated images. Consequently, memory    $\\mathcal{M}$   yields th  names of the retrieved top- k  categories, denoted as    $\\{c_{1},c_{2},c_{3},...,c_{k}\\}$  . The top-k retrieved results serve as a preliminary filter, narrowing down the vast possibilities to those most likely relevant, based on historical data and the semantic closeness of stored labels to the image content. ", "page_idx": 6, "bbox": [134, 352.6219787597656, 480, 472.7518310546875], "page_size": [612.0, 792.0]}
{"layout": 39, "type": "text", "text": "Since these cropped sub-images are usually small, CLIP’s ability to extract features from these low-resolution images is limited. Therefore, in the object de- tection task, we do not perform image-to-image retrieval but use CLIP’s inherent image-text interaction capabilities to conduct image-to-text retrieval. Finally, we also obtain the top-  $k$   category information with the highest similarity. ", "page_idx": 6, "bbox": [134, 472.8688659667969, 480, 533.2227783203125], "page_size": [612.0, 792.0]}
{"layout": 40, "type": "text", "text": "Following the retrieval phase, the retrieved category labels alongside image embedding e  are integrated and sent to the MLLMs through our ranking prompt. The MLLMs, combining the internal knowledge and the retrieved information, make the final prediction of the image category. Our proposed inference process, using both the retrieval results from our memory bank and subsequent ranking by the MLLM, ensures a more accurate and con textually aware classification prediction. Our design represents a significant advancement in few-shot image classification, enabling our system to handle a wide variety of images and cate- gories with high precision and flexibility. ", "page_idx": 6, "bbox": [134, 533.33984375, 480, 641.5147705078125], "page_size": [612.0, 792.0]}
{"layout": 41, "type": "text", "text": "Ranking Prompt Format.  Fig.  4  presents our ranking prompt format. The process begins with the prompt  ‘Sort the optional categories: [class a, ", "page_idx": 6, "bbox": [134, 641.6119384765625, 480, 666.50830078125], "page_size": [612.0, 792.0]}
{"layout": 42, "type": "image", "page_idx": 7, "img_path": "layout_images/RAR_3.jpg", "img_caption": "Fig. 4: Ranking Prompt examples  for few-shot image classification. The fine- grained image examples are from Stanford Cars [ 20 ]. We incorporate the initial top-  $\\cdot k$  retrieved results ( e.g .,    $k=5$  ) into our ranking prompts and use the MLLMs to rank the retrieved results and make the final prediction. ", "bbox": [132, 114, 482, 271], "page_size": [612.0, 792.0], "ocr_text": "Ranking Prompt Example\n\nMercedes-Benz\nE-Class Sedan\n\nMercedes-Benz Mercedes-Benz Mercedes-Benz 2010 BMW Mercedes-Benz\nS-Class Sedan ® C-Class Sedan E-Cla\n\ns Sedan WMS Sedan % —SL-Cl:\n\nCoupe %\n\nSorted these categories:[ Mercedes-Benz S-Class Sedan, Mercedes-Benz C-Class Sedan, Mercedes-Benz E-Class\nSedan, 2010 BMW MS Sedan, Mercedes-Benz SL-Class Coupe ].\n\nEy\nTop-k for high to low: [ Mercedes-Benz E-Class Sedan, Mercedes-Benz S-Class Sedan,\nMercedes-Benz C-Class Sedan, Mercedes-Benz SL-Class Coupe, 2010 BMW MS Sedan ]\n\n", "vlm_text": "The image is a visual example of a ranking prompt for few-shot image classification. It includes:\n\n1. An image labeled \"Mercedes-Benz E-Class Sedan\" on the left.\n2. A series of retrieved car images on the right, each labeled with their respective model names:\n   - Mercedes-Benz S-Class Sedan (incorrect)\n   - Mercedes-Benz E-Class Sedan (correct)\n   - 2010 BMW M5 Sedan (incorrect)\n   - Mercedes-Benz SL-Class Coupe (incorrect)\n3. An explanation showing the sorted categories based on their relevance.\n4. A top-k list (in descending order of likelihood for correct match) is shown, highlighting the Mercedes-Benz E-Class Sedan as the top result.\n\nThis illustrates the process of using few-shot learning and ranking prompts to classify images."}
{"layout": 43, "type": "text", "text": "class b, class c, class d, class e]’ , which is dynamically generated to include the top-k class names retrieved from our multimodal retriever. Our method uses the MLLM’s ability to rank these retrieved class names. Unlike traditional approaches that might rely solely on the initial retrieval order, our MLLM employs advanced linguistic and semantic analysis to assess the contex- tual appropriateness of each class name with the input image. ", "page_idx": 7, "bbox": [134, 282.7560119628906, 480, 355.0649108886719], "page_size": [612.0, 792.0]}
{"layout": 44, "type": "text", "text": "Fine-tuning for Ranking.  When directly applying MLLMs to ranking the retrieved results, MLLMs may predict some errors such as beyond the given list or occasional misalignment. To fully exploit the ranking potential of MLLMs for downstream tasks, while avoiding the consumption of extensive computational resources for training MLLMs, we selected a small-scale classification dataset to fine-tune the MLLMs. The primary goal of fine-tuning was to enable MLLMs to improve their ranking ability such as following the format of prompts and returning results as required. ", "page_idx": 7, "bbox": [134, 354.52301025390625, 480, 450.7618103027344], "page_size": [612.0, 792.0]}
{"layout": 45, "type": "text", "text": "To create our fine-tuning data, we use the CLIP image encoder    $\\varPhi_{\\mathrm{img}}$   to extract the embeddings of two disjoint subsets of images    $\\mathcal{D}_{a}$   and    $\\mathcal{D}_{b}$  , both drawn from the FGVC-Aircraft dataset. We provide the ablation studies in Sec.  4.5 about using different datasets to construct the fine-tuning data. Our observation reveals that the MLLM demonstrates robustness to the choice of fine-tuning datasets, with only marginal differences in performance outcomes. ", "page_idx": 7, "bbox": [134, 450.2388610839844, 480, 522.5477294921875], "page_size": [612.0, 792.0]}
{"layout": 46, "type": "text", "text": "For each image in    $\\mathcal{D}_{b}$  , we apply the    $k$  -NN clustering algorithm to find the top 20 most similar images in    $\\mathcal{D}_{a}$   including their c tegories. Afterward, we select 16 sets from these 20 images, each set comprising  k  images, and retain those groups that contain images of the same category as    $\\mathcal{D}_{b}$  . We then shuffled the category labels for these sets. Using the prompts shown in Fig.  4 , we create a dataset comprising roughly 30,000 entries, with the original sequence of categories serv- ing as the ground-truth label. In summary, we build the fine-tuning data aiming to bolster the MLLM’s ranking performance. ", "page_idx": 7, "bbox": [134, 522.0247802734375, 480, 618.2447509765625], "page_size": [612.0, 792.0]}
{"layout": 47, "type": "text", "text": "In-Context Learning for Ranking.  In-context learning presents a valuable alternative to fine-tuning with ranking examples, particularly due to its flexibility and lower requirement for specialized data preparation. While fine-tuning with ranking examples has proven to be highly effective, it necessitates a substantial amount of curated data and computational resources for training. In contrast, in- context learning uses the model’s existing knowledge by providing it with specific examples directly within the input prompt, guiding the model to understand and execute the task of ranking without the need for explicit re-training. Here we elaborate on the application of in-context learning with MLLMs to rank the retrieved results. To effectively guide the MLLMs in comprehending the ranking task, we use the prompt format similar to Fig.  4  and integrate a specific ranking example into the prompts. Please refer to the Appendix  B  for our structured in-context learning prompt. Please refer to Sec.  4.5  for the ablation studies of discussing the difference between using fine-tuning or in-context learning for ranking. ", "page_idx": 7, "bbox": [134, 617.701904296875, 480, 666.1207275390625], "page_size": [612.0, 792.0]}
{"layout": 48, "type": "text", "text": "", "page_idx": 8, "bbox": [134, 116.46308135986328, 482, 248.54815673828125], "page_size": [612.0, 792.0]}
{"layout": 49, "type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 8, "bbox": [135, 268, 229, 280], "page_size": [612.0, 792.0]}
{"layout": 50, "type": "text", "text": "In this section, we present our experiment step (Sec.  4.1 ) and conduct experi- ments on different tasks such as fine-grained visual recognition (Sec.  4.2 ), few- shot image recognition (Sec.  4.3 ) and zero-shot object recognition (Sec.  4.4 ). We also provide the ablation studies about our design choices (Sec.  4.5 ). ", "page_idx": 8, "bbox": [134, 292.2992248535156, 482, 340.6971435546875], "page_size": [612.0, 792.0]}
{"layout": 51, "type": "text", "text": "4.1 Experimental Setup ", "text_level": 1, "page_idx": 8, "bbox": [134, 359, 260, 370], "page_size": [612.0, 792.0]}
{"layout": 52, "type": "text", "text": "Datasets and Evaluation Metrics.  We follow previous work [ 31 ] to choose 5 datasets for  fine-grained visual recognition  (Bird-200 [ 47 ], Cars-196 [ 20 ], Dog-120 [ 19 ], Flower-102 [ 37 ], and Pet-37 [ 39 ]) and report the clustering accuracy (cACC) and semantic similarity accuracy (sACC) as evaluation metrics. ", "page_idx": 8, "bbox": [134, 378.4512634277344, 482, 426.8691101074219], "page_size": [612.0, 792.0]}
{"layout": 53, "type": "text", "text": "For  few-shot image recognition , we select 11 datasets including gen- eral objects (ImageNet [ 7 ], Caltech101 [ 11 ]), textual (DTD [ 4 ]), scene objects (SUN397 [ 51 ]), satellite images (EuroSAT [ 15 ]), facial expressions (RAF-DB [ 25 ]), car types (Stanford Cars [ 20 ]) and fine-grained datasets (FGVC-Aircraft [ 34 ], Oxford Flowers [ 37 ], Food101 [ 37 ] and Oxford Pets [ 39 ]). We report the top-1 accuracy (  $\\%$  ) for all these classification datasets. ", "page_idx": 8, "bbox": [134, 426.32122802734375, 482, 498.6500549316406], "page_size": [612.0, 792.0]}
{"layout": 54, "type": "text", "text": "Additionally, we also select two benchmarks for our  zero-shot object recog- nition  setting: (1) The LVIS [ 14 ] dataset that encompasses over 164,000 images and 1,203 categories. We report the AP  $\\mathbf{r}$  ,   $\\mathrm{AP_{c}}$  , AP  $\\mathrm{f}$  , and   $\\mathrm{AP_{all}}$   metrics for rare, common, frequent, and all categories. (2) V3Det [ 48 ] dataset encompasses an immense number of 13204 categories of real-world images. For V3Det, we report the standard mAP metric of the object detection task. ", "page_idx": 8, "bbox": [134, 498.1011657714844, 482, 570.4300537109375], "page_size": [612.0, 792.0]}
{"layout": 55, "type": "text", "text": "Implementation Details.  We employ a frozen CLIP ViT B/16 model as the visual encoder    $\\varPhi_{\\mathrm{img}}$   to encode the input images and extract the corresponding image embeddings. For the retrieval process, we search the stored embeddings in memory    $\\mathcal{M}$   using the W algorithm [ 35 ]. We use    $k=5$   for the top-  $k$   results, with a solo exception  k  = 4  in the  4 -shot few-shot setting. To improve the ranking ability of MLLMs, we prepare  30 k fine-tuning data from the FGVC-Aircraft dataset. In the fine-tuning process, we train the model with one epoch with a learning rate of    $1e^{-5}$    on our fine-tuning data and subsequently evaluate the ", "page_idx": 8, "bbox": [134, 569.8811645507812, 482, 666.1210327148438], "page_size": [612.0, 792.0]}
{"layout": 56, "type": "table", "page_idx": 9, "img_path": "layout_images/RAR_4.jpg", "table_caption": "Table 1:  Fine-grained visual recognition across 5 datasets. We follow [ 31 ] to report the averaged clustering accuracy (cACC,   $\\%$  ) and semantic similarity accuracy (sACC,   $\\%$  ) results over 10 runs. The best and second-best results are colored  Green  and Red , respectively. ", "bbox": [132, 113, 483, 234], "page_size": [612.0, 792.0], "ocr_text": "Bird-200 Car-196 Dog-120 Flower-102 Pet-37 Average\ncACC sACC cACC sACC cACC sACC cACC sACC cACC  sACC | cACC sAcc\nWordNet-+CLIP. 39.3 57.7 18.3 33.3 53.9 70.6 42.1 49.8 55.4 61.9 41.8 54.7\nBLIP-2 30.9 56.8 43.1 57.9 39.0 58.6 61.9 59.1 61.3 60.5 47.2 58.6\nCaSED 25.6 50.1 269 41.4 38.0 55.9 67.2 52.3 60.9 63.6 43.7 52.6\nFineR. 51.1 69.5 49.2 63.5 48.1 64.9 63.8 51.3 72.9 72.4 57.0 64.3\n¢ (Ours) | 51.6 69.5 53.2 63.6 50.0 65.2 63.7 53.2 74.1 74.8 58.5, 65.3,\n\n", "vlm_text": "This table compares the performance of different models on various datasets using two metrics: cACC and sACC. The datasets are Bird-200, Car-196, Dog-120, Flower-102, and Pet-37. An average score is also provided.\n\nThe models compared are:\n- WordNet+CLIP\n- BLIP-2\n- CaSED\n- FineR\n- RAR (Ours)\n\nEach cell contains a performance score, with higher scores generally highlighted in green. The RAR model appears to have competitive scores, showing improvement in several categories."}
{"layout": 57, "type": "text", "text": "performance across additional datasets. We present the ablation studies about the hyper-parameters such as the value of    $k$   and the fine-tuning data source in the Sec.  4.5 . ", "page_idx": 9, "bbox": [134, 260.30804443359375, 480, 296.7509765625], "page_size": [612.0, 792.0]}
{"layout": 58, "type": "text", "text": "4.2 Fine-Grained Visual Recognition ", "text_level": 1, "page_idx": 9, "bbox": [133, 316, 327, 328], "page_size": [612.0, 792.0]}
{"layout": 59, "type": "text", "text": "We first evaluate our  RAR  on the  fine-grained visual recognition  setting defined in previous work [ 31 ]. We use only 3 unlabelled images per category to build our memory    $\\mathcal{M}$   for retrieving. Please refer to Appendix  C  for more implementation details. ", "page_idx": 9, "bbox": [134, 336.95703125, 480, 385.35595703125], "page_size": [612.0, 792.0]}
{"layout": 60, "type": "text", "text": "Baselines.  We follow [ 31 ] to select four representative methods as our baselines to compare with: WordNet [ 36  $^+$  CLIP, BLIP-2 [ 22 ], CaSED [ 5 ], and FineR [ 31 ]. Averaged Results over 5 Datasets.  Tab.  1  summarizes the results and our RAR  achieves the top performance on both the cACC   $(58.5\\%)$  ) and sACC   $(65.3\\%$  ) metrics. The WordNet+CLIP and CaSED baselines rely solely on CLIP for class name retrieval, yet often yield inaccurate predictions. In contrast, our method adds the additional ranking process with MLLMs, which increases the likelihood of correctly predicting those accurate yet initially lower-ranked candidates and thereby boosting the performance. Besides, FineR uses MLLM ( e.g ., BLIP-2) for fine-grained recognition via multi-round questioning-answering processes, which may demand more computational resources and struggle to scale efficiently with large vocabulary datasets. Conversely, our approach first retrieves candidates and then lets MLLMs make predictions on the candidates, optimizing both ac- curacy and efficiency. ", "page_idx": 9, "bbox": [134, 385.0500793457031, 480, 553.3128662109375], "page_size": [612.0, 792.0]}
{"layout": 61, "type": "text", "text": "4.3 Few-Shot Image Recognition ", "text_level": 1, "page_idx": 9, "bbox": [132, 572, 306, 585], "page_size": [612.0, 792.0]}
{"layout": 62, "type": "text", "text": "The few-shot setting aims to enable a model to recognize new objects with only a few examples for each new category. Few-shot learning faces substantial chal- lenges when applied to fine-grained datasets, which consist of numerous highly similar classes yet are accompanied by only a minimal amount of training data. Baselines.  For  few-shot image recognition , we introduce two baselines including CLIP and MLLMs. The first is the CLIP [ 41 ] model combined with    $k$  -NN to ", "page_idx": 9, "bbox": [134, 593.5189208984375, 480, 666.119873046875], "page_size": [612.0, 792.0]}
{"layout": 63, "type": "table", "page_idx": 10, "img_path": "layout_images/RAR_5.jpg", "table_caption": "Table 2:  Few-shot image classification across 11 datasets. We report the top-1 accuracy  $(\\%)$  ) under the 4-shot and 8-shot settings. Here our  RAR  uses the LLaVA1.5 [ 28 ] as the MLLM to rank the retrieved results. The symbol ‘-’ denotes to the LLaVA model fails to make the predictions due to the limited window size. ", "bbox": [133, 113, 482, 305], "page_size": [612.0, 792.0], "ocr_text": "Method Common Fine-Grained\ng\n3 §& a § 3\n® gs e¢ 2& $$ a sg 2 2 3 &\na a a 5 5 a iS) 8 & 3 %\n4 oO a nA a a P & n 4 le}\n4-shot\nCLIP+KNN 42.1 87.9 14.2 514 67.6 47.5 646 84.5 49.2 62.6 55.6\nLLaVA1.5 Finetuning - 88.4 24.9 - 48.2 46.6 58.9 13.2 = 66.4 28.9\nR (LLaVA1.5) 51.0 92.1 27.7 58.8 74.8 53.9 69.6 804 54.4 71.4 60.9\n+9.9 +4.2 +13.5 +7.4 +7.2 +64 45.0 -4.1 45.2 +8.8 +5.3\n8-shot\nCLIP+KNN 47.6 90.6 28.2 56.8 72.8 53.2 683 89.5 56.1 683 618\nLLaVA1.5 Finetuning - 92.1 24.9 - 48.2 54.7 66.5 30.1 - 72.5 46.1\ntAR (LLaVA1.5) 56.5 93.5 46.9 63.4 81.5 59.3 74.3 87.3 61.2 76.6 67.7\nAa +8.9 +2.9 418.7 +6.6 +8.7 +6.1 +6.0 -2.2 +5.1 +8.3 +5.9\n\n", "vlm_text": "The table compares the performance of different methods across various datasets. There are two main methods compared: \"CLIP+KNN\" and \"RAR (LLaVA1.5)\" with \"LLaVA1.5 Finetuning\" as a reference. The datasets are divided into \"Common\" and \"Fine-Grained\" categories. \n\n### Categories and Datasets:\n- **Common**: ImageNet, Caltech101, RAF-DB, SUN397, EuroSAT, DTD, UCF-101\n- **Fine-Grained**: Flower102, StanfordCars, Food101, OxfordPets\n\n### Rows:\n1. **4-shot**: Evaluates performance using four samples.\n   - \"CLIP+KNN\" and \"RAR (LLaVA1.5)\" performances are compared.\n   - The \"Δ\" row shows the difference or improvement of \"RAR (LLaVA1.5)\" over the other methods.\n\n2. **8-shot**: Evaluates performance using eight samples.\n   - Similar structure as the 4-shot with comparisons and differences highlighted.\n\n### Highlights:\n- The numbers represent performance metrics (e.g., accuracy) for each method on each dataset.\n- Improvements or changes are highlighted in green and blue, showing how \"RAR (LLaVA1.5)\" performs relative to the others.\n- Average performances for each method across datasets are included.\n\nOverall, \"RAR (LLaVA1.5)\" shows generally better performance (highlighted in green) compared to \"CLIP+KNN\"."}
{"layout": 64, "type": "text", "text": "retrieve predictions based on few-shot examples. The second is the LLaVA model directly fine-tuning with LoRA [ 16 ] on few-shot examples. ", "page_idx": 10, "bbox": [134, 327.592041015625, 481, 352.0799865722656], "page_size": [612.0, 792.0]}
{"layout": 65, "type": "text", "text": "Averaged Results on 11 Datasets.  Tab.  2  summarizes the few-shot results on 11 datasets, including 4 fine-grained datasets. Compared to the CLIP initial retrieval results (top row), our  RAR  (third row) with ranking facilitates a notable increase in classification accuracy. On average, our approach boosts the top-1 accuracy from 57.0 to 63.2 (  ${\\%}$  ) on the 4-shot setting, and from 63.0 to 69.8 (  $\\%$  ) on the 8-shot setting. Such improvements illustrate the ranking process of MLLMs effectively uses a nuanced understanding of context and detail to better align predictions with ground truth. Additionally, we observe that LLaVA1.5 + fine- tuning (second row) baseline under performs in datasets with large vocabularies such as ImageNet due to the constraint of LLMs’ context window. Thanks to the retrieved candidates, our  RAR  works for datasets with a vast of categories and is a potent tool in refining classification decisions, proving particularly useful in handling the diverse and challenging landscape of image classification tasks. ", "page_idx": 10, "bbox": [134, 351.57611083984375, 481, 507.5908508300781], "page_size": [612.0, 792.0]}
{"layout": 66, "type": "text", "text": "4.4 Zero-Shot Object Recognition ", "text_level": 1, "page_idx": 10, "bbox": [133, 525, 312, 537], "page_size": [612.0, 792.0]}
{"layout": 67, "type": "text", "text": "Given the pre-existing object proposals such as ground-truth box annotations, the zero-shot object recognition task measures the model’s capability of aligning regions with textual class descriptions. ", "page_idx": 10, "bbox": [134, 545.8038940429688, 481, 582.2478637695312], "page_size": [612.0, 792.0]}
{"layout": 68, "type": "text", "text": "Baselines.  We select two representative papers CLIP [ 41 ] and RegionCLIP [ 57 ] and report their performances as the baseline results. Besides, we apply our method on a range of cutting-edge open-source MLLMs, including LLaVA1.5 [ 28 ], QWen-VL [ 2 ] and InternLM-XC2 [ 9 ]. ", "page_idx": 10, "bbox": [134, 581.7429809570312, 481, 630.161865234375], "page_size": [612.0, 792.0]}
{"layout": 69, "type": "text", "text": "Main Results on LVIS.  Tab.  3  presents the results that reveal notable im- provements in all the metrics when applying our  RAR . Specifically, when comb- ing with the recent InternLM-XC2 [ 9 ] model, our approach yielded an 8.4 (  $\\%$  ) ", "page_idx": 10, "bbox": [134, 629.656982421875, 481, 666.119873046875], "page_size": [612.0, 792.0]}
{"layout": 70, "type": "table", "page_idx": 11, "img_path": "layout_images/RAR_6.jpg", "table_caption": "Table 3:  Zero-shot object recognition on LVIS [ 14 ] v1.0  validation  set. ", "bbox": [133, 114, 312, 238], "page_size": [612.0, 792.0], "ocr_text": "IAP, AP. AP; APan\n\n40.6 53.1 59.2 48.7\nCLIP w/ mask 40.8 53.5 59.6 49.2\nRegionCLIP 50.1 50.1 51.7 50.7\n\\R (LLaVA1.5) 58.7 57.9 54.4 56.2\nA +8.6 +7.8 +2.7 +5.5\n\n59.6 57.5 53.7 56.4\n+9.5 +7.4 +2.0 +5.7\n\n\\R (InternLM-XC2)} 60.2 58.0 54.3 57.1\n+10.1 +7.9 +2.6 +6.4\n\nCLIP w/ box\n\nRAR (Qwen-VL)\nA\n\n", "vlm_text": "The table compares performance metrics (AP_r, AP_c, AP_f, AP_all) for different models and configurations. Here's a breakdown:\n\n- **Models/Configurations:**\n  - CLIP with box\n  - CLIP with mask\n  - RegionCLIP\n  - RAR (LLaVA1.5)\n  - RAR (Qwen-VL)\n  - RAR (InternLM-XC2)\n\n- **Metrics:**\n  - **AP_r**: Average Precision for region\n  - **AP_c**: Average Precision for classification\n  - **AP_f**: Average Precision for function\n  - **AP_all**: Overall Average Precision\n\n- **Key Observations:**\n  - The RAR models show improvements (indicated by the delta symbol and green highlights) over the baseline models (CLIP variants).\n  - RAR (InternLM-XC2) displays the highest improvements across most metrics.\n\nThe delta values indicate improvements compared to the RegionCLIP baseline."}
{"layout": 71, "type": "table", "page_idx": 11, "img_path": "layout_images/RAR_7.jpg", "table_caption": "Table 4:  Zero-shot object recognition on V3Det [ 48 ]  validation  set with 13,204 cat- egories. ", "bbox": [315, 116, 483, 236], "page_size": [612.0, 792.0], "ocr_text": "JAP, AP,, AP; APan\nCLIP w/ box 7.2 129 128 9.8\nRAR (LLaVAL.5) oo Mae ise aia\nA Tae TOS Ti. +15\nAR (Qwen-VL) 9.6 12.7 13.7 108\nA +2.4 -0.2 +0.9 +1.0\nRAR (InternLM-XC2)| 10.1 13.1 14.5 11.3\n\n+2.9 +0.2 41.7 41.5\n\n", "vlm_text": "The table presents performance metrics for various models in object detection or recognition, possibly using the CLIP model with bounding boxes. Here's a breakdown:\n\n- **Columns**:\n  - **APs, APm, AP1, APall**: These are likely Average Precision metrics, indicating performance at different scales or conditions (small, medium, certain threshold, and overall).\n\n- **Rows**:\n  - **CLIP w/ box**: Baseline model with performance scores of 7.2, 12.9, 12.8, and 9.8 respectively.\n  - **RAR (LLaVA1.5)**: Achieves scores of 9.9, 13.2, 13.9, and 11.1, with improvements of +2.7, +0.3, +1.1, and +1.3.\n  - **RAR (Qwen-VL)**: Achieves scores of 9.6, 12.7, 13.7, and 10.8, with improvements of +2.4, -0.2, +0.9, and +1.0.\n  - **RAR (InternLM-XC2)**: Achieves scores of 10.1, 13.1, 14.5, and 11.3, with improvements of +2.9, +0.2, +1.7, and +1.5.\n\nThe numbers in green highlight improvements over the baseline, while red numbers likely indicate no improvement or a decrease."}
{"layout": 72, "type": "text", "text": "point increase over the CLIP baseline and a 6.4   $(\\%)$  ) enhancement relative to Re- gionCLIP [ 57 ]. These advancements underscore the efficacy of using an external memory for retrieval assistance coupled with the ranking prowess of MLLMs. ", "page_idx": 11, "bbox": [134, 256, 480, 292.8500061035156], "page_size": [612.0, 792.0]}
{"layout": 73, "type": "text", "text": "Comparison with Rare Classes Results   $\\left(\\mathbf{A}\\mathbf{P_{r}}\\right)$  .  We find an interesting ob- servation from the experimental results presented in Tab.  3 . For the CLIP model, we observe a progressive increase in performance from   $\\mathrm{AP_{r}}$   through AP  $\\mathrm{c}$   to AP  $^\\dag$  , which indicates a gradation in precision across varying class frequencies. How- ever, employing our method yields a different trend, where  the peak performance is achieved on   $A P_{r}$  , surpassing the CLIP model by as much as  19 . 6  percentage points. This significant leap in performance suggests a substantial advantage of our method when it comes to rare categories. The integration of our  RAR to MLLMs plays a pivotal role here, as it demonstrates a heightened ability to discriminate among the rare classes. Our observation could be attributed to the fact that our retrieving and reranking mechanism effectively pools relevant information from the external memory, providing the MLLMs with a richer con- text for rare class identification. Moreover, the ranking capability of MLLMs ensures that even the lesser-represented classes receive adequate attention dur- ing the classification process. Our  RAR  achieves a robust enhancement in the model’s ability to discern and accurately classify objects that are infrequently en- countered, addressing one of the significant challenges in long-tailed distribution datasets. ", "page_idx": 11, "bbox": [134, 293, 480, 508.9298095703125], "page_size": [612.0, 792.0]}
{"layout": 74, "type": "text", "text": "Main Results on V3Det.  To further test the effectiveness of using MLLMs for ranking in scenarios with an extremely large number of fine-grained categories, we conducted additional experiments on V3Det [ 49 ]. The experimental results in Tab.  4  reveal that our  RAR  has achieved a commendable improvement in perfor- mance, surpassing the CLIP baseline by 1.5 percentage points in overall average precision (  $.A P_{a l l.}$  ) with InternLM-XC2. Such an improvement is particularly sig- nificant given the complexity of the V3Det dataset, which presents a challenging array of 13,204 distinct classes. The MLLMs, with the aid of our retrieving and ranking mechanisms, have once again demonstrated their robust performance in the domain of object detection datasets. Using our retrieval-augmented approach allows MLLMs to navigate the extensive and fine-grained category landscape of V3Det effectively. ", "page_idx": 11, "bbox": [134, 509.2189636230469, 480, 653.27880859375], "page_size": [612.0, 792.0]}
{"layout": 75, "type": "text", "text": "Qualitative Results.  Fig.  5  presents the visualization results about ranking ", "page_idx": 11, "bbox": [134, 653.5669555664062, 480, 666.1198120117188], "page_size": [612.0, 792.0]}
{"layout": 76, "type": "image", "page_idx": 12, "img_path": "layout_images/RAR_8.jpg", "img_caption": "Fig. 5: Visualization of the ranking examples  for zero-shot object recognition on LVIS [ 14 ]  validation  set. Given the top retrieved predictions, our  RAR  uses MLLMs to select the correct class names accurately. ", "bbox": [132, 121, 482, 242], "page_size": [612.0, 792.0], "ocr_text": "Retrieved\n\n© [rin_(non_jewelryy, ‘pennant, ‘mail_slot, tearring!,\n\ncrubbing_brush’]\n\n2) [‘slipper_(footwear)’,'flipper_(footwear)’, ‘glove’, 'ski_boot’, 'sock']\n\n© (sportswear’, tennis_racket, racket’, !polo_ shirt, ‘tank top_(clothing)']\n\n@) ['tennis_racket’,'short_pants', ‘sportswear’, ‘tennis_ball’, knee_pad!]\n\nReranked\nearring\nglove\n\npolo_shirt\n\nshort_pants\n", "vlm_text": "The image is a table showcasing the process of reranking class names for zero-shot object recognition. It includes three columns:\n\n1. **Objects**: Displays images with highlighted objects.\n2. **Retrieved**: Lists the initially retrieved class names for each highlighted object.\n3. **Reranked**: Shows the correctly identified class names after reranking.\n\n- The first row highlights an object and retrieves multiple names, with \"earring\" being the correct class.\n- The second row identifies a \"glove\".\n- The third row initially includes \"polo_shirt\" as a correct retrieval for the object.\n- The fourth row correctly reranks \"short_pants\".\n\nThe table demonstrates how MLLMs (multi-label learning models) are used for accurate label selection."}
{"layout": 77, "type": "text", "text": "examples of our approach on LVIS  validation  set. The CLIP &  $K$  -NN approach provides an extensive list of object predictions, albeit with the caveat that the most accurate label might not always emerge as the top-1 choice. The incor- poration of MLLMs in our  RAR  significantly streamlines the prediction pro- cess, yielding more precise and relevant object labels. The visualization results demonstrate that our  RAR  meets the need for fine-grained and large vocabulary recognition. ", "page_idx": 12, "bbox": [134, 254.177001953125, 480, 338.44091796875], "page_size": [612.0, 792.0]}
{"layout": 78, "type": "text", "text": "4.5 Ablation Experiments ", "text_level": 1, "page_idx": 12, "bbox": [134, 358, 272, 369], "page_size": [612.0, 792.0]}
{"layout": 79, "type": "text", "text": "Effects of the parameter    $k$  .  We delve into the impact of the hyper-parameter  $k$   on few-shot image recognition setting, as detailed in Tab.  5 . We report the results of  RAR  with the LLaVA1.5 as the MLLM. Our findings reveal that our  RAR  demonstrates a remarkable robustness to variations in    $k$  , with only minor differences observed across a broad spectrum of values from  3  to  7 . Such a consistency suggests that  RAR ’s ability to generalize from a few examples is not significantly influenced by the choice of    $k$  . Consequently, based on the averaged results, we select    $k=5$   as the default choice. ", "page_idx": 12, "bbox": [134, 378.11602783203125, 480, 474.3548278808594], "page_size": [612.0, 792.0]}
{"layout": 80, "type": "text", "text": "Different Fine-tuning data.  We study the importance of using different fine- tuning datasets for ranking. We select two representative datasets: FGVC-Aircraft and Stanford-Cars as the data sources for constructing the fine-tuning data. Our selection is motivated by their diverse characteristics and relevance in visual recognition tasks, providing a comprehensive basis for fine-tuning. Subsequently, we fine-tune the  RAR  with different MLLMs (QWen-VL and InternLM-XC2) on these two datasets, aiming to investigate how different data sources influence per- formance. To thoroughly assess the impact of using different fine-tuning datasets, we evaluate the fine-tuned  RAR  across a diverse set of 10 additional datasets. ", "page_idx": 12, "bbox": [134, 473.9979553222656, 484.3724060058594, 582.1928100585938], "page_size": [612.0, 792.0]}
{"layout": 81, "type": "text", "text": "Tab.  6  presents the results. We observe that  RAR  is not sensitive to changes in the fine-tuning dataset for ranking, thereby confirming its viability as a gen- eralizable and reliable method for enhancing the performance of MLLMs. The consistency in results, irrespective of the fine-tuning data source, underlines the robustness of our fine-tuning strategy. Despite these minor variations, the over- all performance of using FGVC-Aircrafts (82.7 % , top row) is higher than using Stanford Cars (82.0 % , second row) for QWen-VL, and we observe the same trend ", "page_idx": 12, "bbox": [134, 581.8558349609375, 480, 666.1207885742188], "page_size": [612.0, 792.0]}
{"layout": 82, "type": "table", "page_idx": 13, "img_path": "layout_images/RAR_9.jpg", "table_caption": "Table 6: Ablation studies  about (1) using different datasets for fine-tuning and (2) fine-tuning  vs . in-context learning. The symbols   $\\mathrm{^\\circF^{\\prime}}$   and   $^\\mathrm{\\textregistered}$   stand for fine-tuning on the FGVC-Aircraft or Stanford-Cars datasets. ", "bbox": [133, 114, 482, 221], "page_size": [612.0, 792.0], "ocr_text": "[| k=3 k= k=5 =6 k=7\nDTD 70.27 71.34 71.93 71.93 71.99\nFlowers102 | 96.18 95.57 95.62 95.66 95.57\nOxford-pets| 80.21 80.38 79.91 79.72 79.42\nEurosat 92.38 92.48 93.28 92.52 92.59\nAverage 84.76 84.96 85.19 84.96 84.90\n\n", "vlm_text": "This table presents a comparison of values across different datasets (DTD, Flowers102, Oxford-pets, Eurosat) and parameters labeled \\(k = 3\\) to \\(k = 7\\). The table shows specific numerical results for each combination, with averages calculated for each parameter \\(k\\).\n\nHighlighted in green are the highest values within each row and column, indicating the best performance or result for that particular dataset or parameter value. Highlights in red typically indicate the lowest values, but they seem absent here, suggesting a focus on top results.\n\nThe 'Average' row provides the mean of values for each \\(k\\), helping identify overall performance trends across datasets."}
{"layout": 83, "type": "table", "page_idx": 13, "img_path": "layout_images/RAR_10.jpg", "bbox": [136, 225, 478, 329], "page_size": [612.0, 792.0], "ocr_text": "Method Strategy Common Fine-Grained\n\n= 2\n3 S gy 8 2 ©\n2 2 8 s 3 2 = gs €|&\n& Ej a 2 5 a mz S 3 & é]\n= Z, g : & 3\ng cI < 5 5 a is) & 8 FJ 5\nFine-tune In-Context| 4 iS) EA B a a is & & 3° <\nF x 75.8 95.5 66.0 72.7 90.7 72.5 81.4 97.5 88.1 82.7\ns x 75.3 949 65.1 73.1 881 71.0 81.1 958 88.3 82.0\n(QWen-VL) x v 72.0 934 636 65.6 86.2 66.8 76.5 95.6 84.7 78.7\nF x 71.5 944 72.7 69.7 91.7 69.9 77.6 93.2 83.9 80.4\ns x 71.5 94.7 71.2 69.7 90.3 69.9 77.5 92.0 83.6 80.0\n(InternLM-XC2) | X v 69.2 941 66.0 69.7 91.8 68.9 66.1 95.7 85.7 78.6\n\n", "vlm_text": "The table presents performance metrics for different methods evaluated on several datasets. Here's a breakdown:\n\n- **Columns:**\n  - **Method**: Lists the methods used, e.g., RAR with specific models (QWen-VL, InternLM-XC2).\n  - **Strategy**: Indicates whether fine-tuning (F) or in-context learning (S) strategies were applied.\n  - **Common and Fine-Grained**: Categories of datasets used for evaluation, which include:\n    - Common: ImageNet, Caltech101, RAF-DB, SUN397, EuroSAT, DTD, UCF101\n    - Fine-Grained: Flower102, Food101, OxfordPets\n  - **Average**: Represents the average performance across all datasets.\n\n- **Values**: The cells contain numeric performance values, where green-highlighted numbers indicate the highest performance for that specific dataset and strategy combination."}
{"layout": 84, "type": "text", "text": "for InternLM-XC2. Based on our findings, we adopt the FGVC-Aircraft dataset as our preferred choice for fine-tuning. ", "page_idx": 13, "bbox": [134, 349.5200500488281, 480, 374.00799560546875], "page_size": [612.0, 792.0]}
{"layout": 85, "type": "text", "text": "Fine-tuning  vs . In-Context Learning.  We validate the effectiveness of fine- tuning the MLLM or just in-context learning (training-free) for ranking. The results are illustrated in Tab.  6 . We select two distinct groups for comparison. The first group (top and fourth rows) involves models that are fine-tuned using the FGVC-Aircraft dataset, while the second group (third and bottom rows) consists of models with in-context learning prompts for ranking. The results show a consistent improvement in accuracy for the fine-tuned model across almost all datasets for both QWen-VL and InternLM-XC2. The notable enhancement in performance across a diverse range of datasets highlights the efficacy of our fine-tuning strategy. The results substantiate that fine-tuning the MLLM with target datasets like FGVC-Aircraft significantly bolsters the model’s ranking capabilities. ", "page_idx": 13, "bbox": [134, 373.41009521484375, 480, 517.4698486328125], "page_size": [612.0, 792.0]}
{"layout": 86, "type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 13, "bbox": [133, 534, 221, 547], "page_size": [612.0, 792.0]}
{"layout": 87, "type": "text", "text": "In this paper, we highlight the potential of combining retrieving and ranking with multi-modal large language models to revolutionize perception tasks such as fine-grained recognition, zero-shot image recognition, and few-shot object recog- nition. Motivated by the limited zero-shot/few-shot of CLIP and MLLMs on fine-grained datasets, our  RAR  designs the pipeline that uses MLLM to rank the retrieved results. Our proposed approach can be seamlessly integrated into various MLLMs for real-world applications where the variety and volume of cat- egories continuously expand. Our method opens up new avenues for research in augmenting the MLLM’s abilities with the retrieving-augmented solution and could be beneficial for other tasks such as reasoning and generation in future works.\n\n ", "page_idx": 13, "bbox": [134, 557.9459228515625, 480, 666.119873046875], "page_size": [612.0, 792.0]}
{"layout": 88, "type": "text", "text": "", "page_idx": 14, "bbox": [134, 116.46308135986328, 480, 140.9510498046875], "page_size": [612.0, 792.0]}
{"layout": 89, "type": "text", "text": "References ", "text_level": 1, "page_idx": 14, "bbox": [135, 162, 197, 175], "page_size": [612.0, 792.0]}
{"layout": 90, "type": "text", "text": "1. Awadalla, A., Gao, I., Gardner, J., Hessel, J., Hanafy, Y., Zhu, W., Marathe, K., Bitton, Y., Gadre, S., Sagawa, S., Jitsev, J., Kornblith, S., Koh, P.W., Ilharco, G., Wortsman, M., Schmidt, L.: Open flamingo: An open-source framework for training large auto regressive vision-language models. arXiv.org (2023)  2 ,  4\n\n 2. Bai, J., Bai, S., Yang, S., Wang, S., Tan, S., Wang, P., Lin, J., Zhou, C., Zhou, J.: Qwen-VL: A frontier large vision-language model with versatile abilities. arXiv.org (2023)  2 ,  4 ,  11\n\n 3. Chen, L., Li, J., Dong, X., Zhang, P., He, C., Wang, J., Zhao, F., Lin, D.: Sharegpt4v: Improving large multi-modal models with better captions. arXiv preprint arXiv:2311.12793 (2023)  2 ,  4\n\n 4. Cimpoi, M., Maji, S., Kokkinos, I., Mohamed, S., Vedaldi, A.: Describing textures in the wild. In: CVPR (2014)  9 ,  19\n\n 5. Conti, A., Fini, E., Mancini, M., Rota, P., Wang, Y., Ricci, E.: Vocabulary-free image classification. In: NeurIPS (2024)  10\n\n 6. Dai, W., Li, J., Li, D., Tiong, A.M.H., Zhao, J., Wang, W., Li, B., Fung, P., Hoi, S.: Instruct blip: Towards general-purpose vision-language models with instruction tuning (2023)  2 ,  4\n\n 7. Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: ImageNet: A large-scale hierarchical image database. In: CVPR (2009)  9 ,  19\n\n 8. Dong, X., Bao, J., Zheng, Y., Zhang, T., Chen, D., Yang, H., Zeng, M., Zhang, W., Yuan, L., Chen, D., Wen, F., Yu, N.: Maskclip: Masked self-distillation advances contrastive language-image pre training. In: Proceedings of the IEEE/CVF Con- ference on Computer Vision and Pattern Recognition (CVPR). pp. 10995–11005 (June 2023)  1 ,  3\n\n 9. Dong, X., Zhang, P., Zang, Y., Cao, Y., Wang, B., Ouyang, L., Wei, X., Zhang, S., Duan, H., Cao, M., et al.: InternLM-XComposer2: Mastering free-form text-image composition and comprehension in vision-language large model. arXiv preprint arXiv:2401.16420 (2024)  11\n\n 10. Fang, Y., Wang, W., Xie, B., Sun, Q., Wu, L., Wang, X., Huang, T., Wang, X., Cao, Y.: EVA: Exploring the limits of masked visual representation learning at scale. In: CVPR (2023)  3\n\n 11. Fei-Fei, L., Fergus, R., Perona, P.: Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object cate- gories. In: CVPR workshop (2004)  9 ,  19\n\n 12. Gao, P., Geng, S., Zhang, R., Ma, T., Fang, R., Zhang, Y., Li, H., Qiao, Y.: Clip- Adapter: Better vision-language models with feature adapters. IJCV (2023)  3\n\n 13. Gu, X., Lin, T.Y., Kuo, W., Cui, Y.: Open-vocabulary object detection via vision and language knowledge distillation. In: ICLR (2022)  3\n\n 14. Gupta, A., Dollar, P., Girshick, R.: LVIS: A dataset for large vocabulary instance segmentation. In: CVPR (2019)  7 ,  9 ,  12 ,  13 ,  26 ,  27 ,  28\n\n 15. Helber, P., Bischke, B., Dengel, A., Borth, D.: Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification. IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens. (2019)  9 ,  19 ", "page_idx": 14, "bbox": [134, 188.35452270507812, 480, 665.80908203125], "page_size": [612.0, 792.0]}
{"layout": 91, "type": "text", "text": "16. Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., Chen, W.: LoRA: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685 (2021)  11\n\n 17. Iscen, A., Fathi, A., Schmid, C.: Improving image recognition by retrieving from web-scale image-text data. In: CVPR (2023)  4\n\n 18. Jia, C., Yang, Y., Xia, Y., Chen, Y.T., Parekh, Z., Pham, H., Le, Q., Sung, Y.H., Li, Z., Duerig, T.: Scaling up visual and vision-language representation learning with noisy text supervision. In: ICML (2021)  3\n\n 19. Khosla, A., Jaya deva prakash, N., Yao, B., Li, F.F.: Novel dataset for fine-grained image categorization: Stanford dogs. In: CVPR workshop (2011)  9 ,  19\n\n 20. Krause, J., Stark, M., Deng, J., Fei-Fei, L.: 3d object representations for fine- grained categorization. In: ICCV workshops (2013)  8 ,  9 ,  19\n\n 21. Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., Küttler, H., Lewis, M., Yih, W.t., Rock t s chel, T., et al.: Retrieval-Augmented generation for knowledge-intensive nlp tasks. NeurIPS (2020)  4\n\n 22. Li, J., Li, D., Savarese, S., Hoi, S.: BLIP-2: Boots trapping language-image pre- training with frozen image encoders and large language models. In: ICML (2023) 10\n\n 23. Li, J., Li, D., Xiong, C., Hoi, S.: BLIP: Boots trapping language-image pre-training for unified vision-language understanding and generation. In: ICML (2022)  3\n\n 24. Li, L.H., Zhang, P., Zhang, H., Yang, J., Li, C., Zhong, Y., Wang, L., Yuan, L., Zhang, L., Hwang, J.N., et al.: Grounded language-image pre-training. In: CVPR (2022)  3\n\n 25. Li, S., Deng, W., Du, J.: Reliable crowd sourcing and deep locality-preserving learn- ing for expression recognition in the wild. In: CVPR (2017)  9 ,  19\n\n 26. Li, Y., Fan, H., Hu, R., Fei chten hofer, C., He, K.: Scaling language-image pre- training via masking. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 23390–23400 (June 2023)  1 ,  3\n\n 27. Liang, F., Wu, B., Dai, X., Li, K., Zhao, Y., Zhang, H., Zhang, P., Vajda, P., Marculescu, D.: Open-vocabulary semantic segmentation with mask-adapted clip. In: CVPR (2023)  3\n\n 28. Liu, H., Li, C., Li, Y., Lee, Y.J.: Improved baselines with visual instruction tuning. arXiv preprint arXiv:2310.03744 (2023)  2 ,  11 ,  24\n\n 29. Liu, H., Li, C., Wu, Q., Lee, Y.J.: Visual instruction tuning. In: NeurIPS (2024) 2 ,  4\n\n 30. Liu, H., Son, K., Yang, J., Liu, C., Gao, J., Lee, Y.J., Li, C.: Learning customized visual models with retrieval-augmented knowledge. In: CVPR (2023)  4\n\n 31. Liu, M., Roy, S., Li, W., Zhong, Z., Sebe, N., Ricci, E.: Democratizing fine-grained visual recognition with large language models. In: ICLR (2024)  4 ,  9 ,  10 ,  22 ,  23\n\n 32. Long, A., Yin, W., Ajanthan, T., Nguyen, V., Purkait, P., Garg, R., Blair, A., Shen, C., van den Hengel, A.: Retrieval augmented classification for long-tail visual recognition. In: CVPR (2022)  4\n\n 33. Lüddecke, T., Ecker, A.: Image segmentation using text and image prompts. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recog- nition (CVPR). pp. 7086–7096 (June 2022)  3 ,  6\n\n 34. Maji, S., Rahtu, E., Kannala, J., Blaschko, M., Vedaldi, A.: Fine-grained visual classification of aircraft. arXiv preprint arXiv:1306.5151 (2013)  9 ,  19\n\n 35. Malkov, Y.A., Yashunin, D.A.: Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs. TPAMI (2018)  6 ,  9\n\n 36. Miller, G.A.: WordNet: a lexical database for english. Communications of the ACM (1995)  10 ", "page_idx": 15, "bbox": [134, 117.39651489257812, 480, 665.8092651367188], "page_size": [612.0, 792.0]}
{"layout": 92, "type": "text", "text": "37. Nilsback, M.E., Zisserman, A.: Automated flower classification over a large number of classes. In: ICVGIP (2008)  9 ,  19\n\n 38. OpenAI: GPT-4V(ision) system card (2023),  https://openai.com/research/ gpt-4v-system-card  2 ,  4 ,  5 ,  25\n\n 39. Parkhi, O.M., Vedaldi, A., Zisserman, A., Jawahar, C.: Cats and dogs. In: CVPR (2012)  9 ,  19\n\n 40. Peng, Z., Wang, W., Dong, L., Hao, Y., Huang, S., Ma, S., Wei, F.: Kosmos-2: Grounding multimodal large language models to the world. arXiv.org (2023)  2 ,  4\n\n 41. Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from natural language supervision. In: ICML (2021)  1 ,  3 ,  4 ,  10 ,  11 ,  26 ,  27\n\n 42. Sh ted rit ski, A., Rupprecht, C., Vedaldi, A.: What does clip know about a red circle? visual prompt engineering for vlms. arXiv preprint arXiv:2304.06712 (2023)  3\n\n 43. Soomro, K., Zamir, A.R., Shah, M.: UCF101: A dataset of 101 human actions classes from videos in the wild. arXiv preprint arXiv:1212.0402 (2012)  19\n\n 44. Subramania n, S., Merrill, W., Darrell, T., Gardner, M., Singh, S., Rohrbach, A.: Reclip: A strong zero-shot baseline for referring expression comprehension. In: Pro- ceedings of the 60th Annual Meeting of the Association for Computational Lin- guistics (Volume 1: Long Papers). pp. 5198–5215 (2022)  3\n\n 45. Sun, Q., Fang, Y., Wu, L., Wang, X., Cao, Y.: Eva-clip: Improved training tech- niques for clip at scale. arXiv preprint arXiv:2303.15389 (2023)  1\n\n 46. Sun, Z., Fang, Y., Wu, T., Zhang, P., Zang, Y., Kong, S., Xiong, Y., Lin, D., Wang, J.: Alpha-CLIP: A clip model focusing on wherever you want. arXiv preprint arXiv:2312.03818 (2023)  3\n\n 47. Wah, C., Branson, S., Welinder, P., Perona, P., Belongie, S.: Caltech-ucsd birds- 200-2011 (2011)  9 ,  19\n\n 48. Wang, J., Zhang, P., Chu, T., Cao, Y., Zhou, Y., Wu, T., Wang, B., He, C., Lin, D.: V3Det: Vast vocabulary visual detection dataset. In: ICCV (2023)  3 ,  7 ,  9 ,  12 , 28\n\n 49. Wang, W., Lv, Q., Yu, W., Hong, W., Qi, J., Wang, Y., Ji, J., Yang, Z., Zhao, L., Song, X., Xu, J., Xu, B., Li, J., Dong, Y., Ding, M., Tang, J.: Cogvlm: Visual expert for pretrained language models (2023)  2 ,  4 ,  12\n\n 50. Wu, W., Yao, H., Zhang, M., Song, Y., Ouyang, W., Wang, J.: GPT4Vis: What can gpt-4 do for zero-shot visual recognition? arXiv preprint arXiv:2311.15732 (2023) 25\n\n 51. Xiao, J., Hays, J., Ehinger, K.A., Oliva, A., Torralba, A.: SUN database: Large- scale scene recognition from abbey to zoo. In: CVPR (2010)  9 ,  19\n\n 52. Xu, X., Xiong, T., Ding, Z., Tu, Z.: Masqclip for open-vocabulary universal im- age segmentation. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 887–898 (2023)  3\n\n 53. Yang, L., Wang, Y., Li, X., Wang, X., Yang, J.: Fine-grained visual prompting (2023)  3 ,  6\n\n 54. Yasunaga, M., Aghajanyan, A., Shi, W., James, R., Leskovec, J., Liang, P., Lewis, M., Z ett le moyer, L., Yih, W.t.: Retrieval-augmented multimodal language model- ing. In: ICML (2023)  3 ,  4\n\n 55. Ye, Q., Xu, H., Xu, G., Ye, J., Yan, M., Zhou, Y., Wang, J., Hu, A., Shi, P., Shi, Y., et al.: mplug-owl: Modular iz ation empowers large language models with multi modality. arXiv.org (2023)  2 ,  4\n\n 56. Zhang, P., Wang, X.D.B., Cao, Y., Xu, C., Ouyang, L., Zhao, Z., Ding, S., Zhang, S., Duan, H., Yan, H., et al.: Internlm-xcomposer: A vision-language large model for advanced text-image comprehension and composition. arXiv preprint arXiv:2309.15112 (2023)  2 ,  4\n\n ", "page_idx": 16, "bbox": [134, 117.39651489257812, 480, 665.8089599609375], "page_size": [612.0, 792.0]}
{"layout": 93, "type": "text", "text": "57. Zhong, Y., Yang, J., Zhang, P., Li, C., Codella, N., Li, L.H., Zhou, L., Dai, X., Yuan, L., Li, Y., et al.: RegionCLIP: Region-based language-image pre training. In: CVPR (2022)  3 ,  11 ,  12\n\n 58. Zhou, K., Yang, J., Loy, C.C., Liu, Z.: Learning to prompt for vision-language models. IJCV (2022)  3\n\n 59. Zhou, X., Girdhar, R., Joulin, A., Krähenbühl, P., Misra, I.: Detecting twenty- thousand classes using image-level supervision. In: ECCV (2022)  3\n\n 60. Zhu, D., Chen, J., Shen, X., Li, X., Elhoseiny, M.: MiniGPT-4: Enhancing vision- language understanding with advanced large language models. arXiv preprint arXiv:2304.10592 (2023)  2 ,  4 In this appendix, we provide a series of detailed supporting materials to aid ", "page_idx": 17, "bbox": [134, 117.39651489257812, 480, 249.23306274414062], "page_size": [612.0, 792.0]}
{"layout": 94, "type": "text", "text": "in a deeper understanding of our work. Firstly, in Appendix  A , we introduce the fourteen image classification datasets involved in our experiments, including seven common datasets and seven fine-grained datasets, as well as two large- scale vocabulary detection datasets. Following that, in Appendix  B , we provide detailed information on the prompts used in our  RAR , as well as the prompts used in corresponding ablation studies. In Appendix  C , we supplement details on the structure and experimental aspects of  RAR , dividing the content into three sections: Fine-Grained Visual Recognition, Few-Shot Image Classification, and Zero-Shot Region Recognition. ", "page_idx": 18, "bbox": [134, 116.46308135986328, 480, 236.5931396484375], "page_size": [612.0, 792.0]}
{"layout": 95, "type": "text", "text": "A Dataset Statistics ", "text_level": 1, "page_idx": 18, "bbox": [134, 262, 262, 273], "page_size": [612.0, 792.0]}
{"layout": 96, "type": "text", "text": "In this section, we delve deeper into the specifics of the fourteen classification and two detection datasets employed in our research. The classification datasets encompass a wide range, from general categories that cover a broad spectrum of common objects to fine-grained types that focus on more specific, detailed distinctions within a particular category. The detection datasets, on the other hand, are extensive, encompassing tens of thousands of object categories. These datasets are designed to challenge the model’s ability to identify and categorize objects from a vast array of possible classes. The long-tail nature of these datasets poses a significant challenge for our  RAR  model. ", "page_idx": 18, "bbox": [134, 291.77618408203125, 480, 399.9510498046875], "page_size": [612.0, 792.0]}
{"layout": 97, "type": "text", "text": "A.1 Classification Datasets ", "text_level": 1, "page_idx": 18, "bbox": [135, 424, 277, 435], "page_size": [612.0, 792.0]}
{"layout": 98, "type": "text", "text": "In the experimental part, we use a total of fourteen image classification datasets, including seven fine-grained classification datasets and seven common classifica- tion datasets. Fine-grained image classification datasets include: Bird-200 [ 47 ], Stanford Cars [ 20 ], Dog-120 [ 19 ], Oxford Flowers [ 37 ], Oxford Pets [ 39 ], FGVC- Aircraft [ 34 ], and Food101 [ 37 ]. Common image classification datasets include: ImageNet [ 7 ], Caltech101 [ 11 ], RAF-DB [ 25 ], Sun397 [ 51 ], Eurosat [ 15 ], DTD [ 4 ], and UCF-101 [ 43 ]. We present all the utilized datasets in Figure 1. And in Tab.  7 , we list the statistics and sources of these datasets in detail. ", "page_idx": 18, "bbox": [134, 449.1571044921875, 480, 545.3759765625], "page_size": [612.0, 792.0]}
{"layout": 99, "type": "text", "text": "In our fine-grained visual recognition experiments, we employed the follow- ing datasets: Bird-200, Stanford Cars, Dog-120, Flowers-102, and Oxford pets. In each dataset, we selected 3 images from the training set to construct our mem- ory and conducted tests on the corresponding validation sets. In our few-shot image classification experiments, we used the FGVC-Aircraft dataset to build fine-tune data and tested our  RAR  model across eleven classification datasets: Stanford Cars, Flower-102, Oxford Pets, Food101, ImageNet, Caltech101, RAF- DB, Sun397, Eurosat, DTD, and UCF-101. We selected either 4 or 8 images from the training set of each dataset to place into memory, corresponding to 4-shot and 8-shot settings, respectively, and conducted tests across all validation sets. ", "page_idx": 18, "bbox": [134, 545.9910278320312, 480, 666.1209716796875], "page_size": [612.0, 792.0]}
{"layout": 100, "type": "image", "page_idx": 19, "img_path": "layout_images/RAR_11.jpg", "img_caption": "Fig. 6: Datsets  used in our experiments. We select 14 classification datasets (7 fine- grained and 7 common) and 2 object detection datasets as our benchmarks. ", "bbox": [133, 120, 481, 294], "page_size": [612.0, 792.0], "ocr_text": "Datasets\n\nTine’ Grained Giasafiestion) Dataset\n\n1 4\naf\n\nFood101 Stanford Cars Flowerl02._ FGVC-Aircraft Oxford pets Dog-120\n\nDetection Dataset\n\nCommon Classification Dataset\n\n“ie BF ce BS\n\nSUN397 Eurosat Caltech101 UCF101 DID Imagenet RAF-DB\n\ns. =\n\n", "vlm_text": "The image lists datasets used in experiments, divided into three categories:\n\n1. **Fine-Grained Classification Datasets:**\n   - Food101\n   - Stanford Cars\n   - Flower102\n   - FGVC-Aircraft\n   - Oxford Pets\n   - Dog-120\n   - Bird-200\n\n2. **Common Classification Datasets:**\n   - SUN397\n   - Eurosat\n   - Caltech101\n   - UCF101\n   - DOTD\n   - Imagenet\n   - RAF-DB\n\n3. **Detection Datasets:**\n   - LVIS\n   - V3Det\n\nThese serve as benchmarks for classification and object detection tasks."}
{"layout": 101, "type": "table", "page_idx": 19, "img_path": "layout_images/RAR_12.jpg", "table_caption": "Table 7:  Statistics for the classification and detection datasets used in our three set- tings: fine-grained visual recognition, few-shot image recognition, and zero-shot region recognition. ", "bbox": [133, 300, 482, 485], "page_size": [612.0, 792.0], "ocr_text": "Settings Dataset Categories Evaluation Metrics Source link\n. . Bird-200 200 cACC, sACC Bird website\nFine-Grained Car-196 196 eng Eyton Kaggle\nVisual R . Dog-120 120 c. >, sACC Tensorflow\nASMBEAECOE Flower-102 102 cACC, sACC Tensorflow\nPet-37 37 cACC, sACC Tensorflow\nRAF-DB % Accuracy RAEF-DB website\nEurosat 10 Accuracy Tensorflow\ni E > Aircraft curacy 3VC website\nFewsohot lapee Caltech101 101 Accuracy Tensorflow\nRecog. Food101 101 Accuracy Tensorflow\nUCF-101 101 Accuracy Tensorflow\nSUN397 397 Accuracy Tensorflow\nImageNet 1000 Accuracy Tensorflow\nZero-Shot LVIS 1203 mAP LVIS website\nRegion Recog. V3Det 13204 mAP Githul\n\n", "vlm_text": "The table presents various datasets and their details categorized under different settings for visual recognition tasks:\n\n1. **Settings**: The tasks are categorized into Fine-Grained Visual Recognition, Few-Shot Image Recognition, and Zero-Shot Region Recognition.\n\n2. **Dataset**: Lists the specific datasets used for each setting.\n\n3. **Categories**: Indicates the number of categories within each dataset.\n\n4. **Evaluation Metrics**: Specifies the metrics used to evaluate performance:\n   - For Fine-Grained Visual Recognition: cACC, sACC\n   - For Few-Shot Image Recognition: Accuracy\n   - For Zero-Shot Region Recognition: mAP\n\n5. **Source Link**: Provides the source links for accessing the datasets, with references to specific websites and platforms like Tensorflow, Kaggle, FGVC website, RAF-DB website, LVIS website, and Github."}
{"layout": 102, "type": "text", "text": "A.2 Detection Datasets ", "text_level": 1, "page_idx": 19, "bbox": [134, 497, 260, 510], "page_size": [612.0, 792.0]}
{"layout": 103, "type": "text", "text": "In our Zero-Shot Region Recognition experiments, we utilized two large-scale vocabulary detection datasets, namely LVIS and V3Det. The LVIS dataset, de- veloped by Facebook AI researchers, stands out with its extensive coverage, including 164,000 images and about 2,000,000 high-quality instance segmenta- tion annotations that span over 1,000 object classes. This dataset is particularly notable for its long-tail distribution, which means it includes a large number of infrequent or rare object classes in addition to the common ones. This diver- sity challenges our model to recognize and differentiate between a wide array of objects, including those that are less common and hence more challenging to identify accurately. ", "page_idx": 19, "bbox": [134, 521.3160400390625, 480, 641.4450073242188], "page_size": [612.0, 792.0]}
{"layout": 104, "type": "text", "text": "The V3Det dataset complements LVIS by offering an even broader scope. With its 245,000 images distributed across an impressive 13,204 categories, ", "page_idx": 19, "bbox": [134, 641.632080078125, 480, 666.1199951171875], "page_size": [612.0, 792.0]}
{"layout": 105, "type": "image", "page_idx": 20, "img_path": "layout_images/RAR_13.jpg", "img_caption": "Fig. 7: GPT4V Example  for Stanford Cars and FGVC Aircraft.  Green  for ground truth,  blue  for characteristics analyzed by GPT-4V. ", "bbox": [134, 118, 482, 402], "page_size": [612.0, 792.0], "ocr_text": "Please play the role of a classification expert, and\nsort the provided categories from high to low\naccording to the top 5 similarity with the input\nimage. Here are the optional categories:['2012 Aston\nMartin V8 Vantage Convertible’, '2012 Aston Martin\nVirage Coupe’, '2012 Aston Martin Virage\nConvertible’, '2012 Audi TTS Coupe’, '2012 Aston\n\nMartin V8 Vantage Coupe']\n\nBased on the input image and the provided\ncategories, the sorting from high to low similarity\nwith the car in the image is as follows:\n\n1, 2012 Aston Martin V8 Vantage Coupe\n\n2, 2012 Aston Martin Virage Coupe\n\n3. 2012 Aston Martin V8 Vantage Convertible\n\n4, 2012 Aston Martin Virage Convertible\n\n5, 2012 Audi TTS Coupe\n\nWhy did you give this order?\n\nThe car in the image is an Aston Martin, and its\nstyling most closely resembles the Vantage model\nline, It's a coupe, not a convertible, so coupes have\nbeen ranked higher than convertibles. The Audi TTS\nCoupe, while also a coupe, is from a different\nmanufacturer and thus has distinct styling cues that\nare less similar to the vehicle in the image.\n\nPlease play the role of a classification expert, and\nsort the provided categories from high to low\naccording to the top 5 similarity with the input\nimage. Here are the optional categories: ['737-800',\n\"757-300, '767-200', 'A340-300, \"MD-90']\n\nBased on the input image and the provided\ncategories, the sorting from high to low similarity\nwith the aircraft in the image is as follows:\n\n1. 757-300\n\n2. 767-200\n\n3. MD-90\n\n4, 737-800\n\n5, 4340-300\n\n| Why did you give this order?\n\nThe aircraft in the image is a Boeing 757, which is\nidentifiable by its long fuselage, narrow-body, and\ncharacteristic nose shape. The -300 series has an\nextended fuselage compared to other variants, which\nappears to match the aircraft pictured. The 767-200\nand MD-90 have some similarities but differ in\nspecific design elements such as winglets and\nengine placement. The 737-800 is a shorter aircraft\nwith a different wing design. The A340-300 is a\nwide-body aircraft with four engines, making it\nquite distinct from the 757-300.\n\n", "vlm_text": "The image showcases two examples of GPT-4V analyzing and classifying vehicles based on images. \n\n**Left Side: Car Analysis**\n- A red car is pictured, and GPT-4V is tasked with sorting provided car categories by similarity.\n- Categories include various Aston Martin models and an Audi.\n- The model identified with the highest similarity is the \"2012 Aston Martin V8 Vantage Coupe.\"\n\n**Right Side: Aircraft Analysis**\n- An aircraft is pictured, and GPT-4V is tasked with sorting provided aircraft categories by similarity.\n- Categories include various Boeing and other aircraft models.\n- The model identified with the highest similarity is the \"757-300.\"\n\nThe green text indicates ground truth, and the blue text highlights the characteristics analyzed by GPT-4V."}
{"layout": 106, "type": "text", "text": "V3Det brings an unprecedented level of diversity to the table. The dataset in- cludes 1,753,000 meticulously annotated bounding boxes, making it an invaluable resource for developing and testing detection algorithms capable of handling a wide variety of object types. Its large number of categories ensures that the dataset has a comprehensive representation of the visual world, making it an ideal testing ground for our Zero-Shot Region Recognition experiments. ", "page_idx": 20, "bbox": [134, 411.4320373535156, 482, 483.7409362792969], "page_size": [612.0, 792.0]}
{"layout": 107, "type": "text", "text": "B Prompt Formats ", "text_level": 1, "page_idx": 20, "bbox": [134, 499, 256, 513], "page_size": [612.0, 792.0]}
{"layout": 108, "type": "text", "text": "In this section, we delve into the detailed design of our prompts. We have crafted distinct prompts for various tasks to test the capabilities of the baseline model and our  RAR  model in visual recognition. ", "page_idx": 20, "bbox": [134, 522.0799560546875, 482, 558.52392578125], "page_size": [612.0, 792.0]}
{"layout": 109, "type": "text", "text": "In our  RAR  pipeline, the prompt primarily serves to merge the input image with the category information retrieved from memory. It guides MLLMs to rank the retrieved candidate object categories based on similarity. Our prompt format is as follows: ", "page_idx": 20, "bbox": [134, 557.9459228515625, 482, 606.3449096679688], "page_size": [612.0, 792.0]}
{"layout": 110, "type": "text", "text": "Please play the role of a classification expert, and sort the provided categories from high to low according to the {top-k} similarity with the input image. Here are the optional categories:{categories}. ", "page_idx": 20, "bbox": [134, 606.7532958984375, 482, 642.5984497070312], "page_size": [612.0, 792.0]}
{"layout": 111, "type": "text", "text": "Here,   $^{\\circ}\\{\\mathrm{top-k}\\}^{\\prime}$   is replaced with the number of categories input. And ‘{cate- gories}’ is replaced with the top-  $\\cdot\\mathrm{k}$   categories retrieved from memory. ", "page_idx": 20, "bbox": [134, 641, 482, 666.1199340820312], "page_size": [612.0, 792.0]}
{"layout": 112, "type": "image", "page_idx": 21, "img_path": "layout_images/RAR_14.jpg", "img_caption": "Fig. 8: GPT4V Example  for Flowers102, Pets37 and Food101.  Green  for ground truth,  blue  for characteristics analyzed by GPT-4V. ", "bbox": [133, 118, 482, 434], "page_size": [612.0, 792.0], "ocr_text": "Please play the role of a\nclassification expert, and sort\nthe provided categories from\nhigh to low according to the\ntop 5 similarity with the input\nimage. Here are the optional\ncategories:['lenten rose’,\n‘japanese anemone’, ‘tree\npoppy’, ‘columbine’, 'camellia']\n\not)\n\na\nPlease play the role of a\nclassification expert, and sort\nthe provided categories from\nhigh to low according to the\ntop 5 similarity with the input\nimage. Here are the optional\ncategories: ['yorkshire_terrier’,\n‘chihuahua','miniature_pinsche\n1 shavanese’,'beagle']\n\nBased on the input image and\nthe provided categories, here\nis the sorting of the categories\nfrom high to low according to\ntheir similarity with the flower\nin the image:\n\n1, Japanese anemone\n\n2. Tree poppy\n\n3. Camellia\n\n4, Lenten rose\n\n5. Columbine\n\nBased on the input image and\nthe provided categories, the\nsorting from high to low\nsimilarity with the dog in the\nimage is:\n\n1. Chihuahua\n\n2. Miniature Pinscher\n\n3. Yorkshire Terrier\n\n4, Havanese\n\n5. Beagle\n\nWhy did you give this order?\n\nWhy did you give this order?\n\nThe flower in the image\nappears to be a Japanese\nanemone, which typically has\nwhite petals surrounding a\nyellow center of stamens and a\ngreenish pistil, matching the\none shown, The other flowers\nlisted have different\ncharacteristics that do not\nmatch as closely.\n\nThe dog in the image is a\nChihuahua, identifiable by its\nsmall size, prominent ears, and\nfacial features. Miniature\nPinschers are somewhat similar\nin size and ear shape, while\nYorkshire Terriers, Havanese,\nand Beagles have distinct\ndifferences in coat, size, and\nfacial structure compared to the\nChihuahua,\n\nPlease play the role of a\nclassification expert, and sort\nthe provided categories from\nhigh to low according to the top\n5 similarity with the input\nimage. Here are the optional\ncategories:\n\n(donuts, breakfast_burrito’,\n‘falafel’, hummus’, ‘beet_salad']\n\nBased on the input image and\nthe provided categories, the\nsorting from high to low\nsimilarity with the food items\nin the image is:\n\n1. Falafel\n\n2. Hummus\n\n3. Beet Salad\n\n4, Breakfast Burrito\n\n5, Donuts\n\n‘Why did you give this order?\n\nThe dish in the image appears\nto include falafel balls and a\nside of hummus, which are\ntypically found in Middle\nEastern cuisine. The other\nitems, such as beet salad,\nbreakfast burrito, and donuts,\ndo not seem to be present in the\nimage or resemble the food\nshown.\n\n", "vlm_text": "The image displays three panels, each with a category classification task involving different types of items (flowers, dogs, and foods). Each panel includes:\n\n1. **Image and Task Description**:\n   - The left panel shows a flower, the middle panel shows a dog, and the right panel shows food.\n   - Each task involves sorting provided categories based on similarity to the image.\n\n2. **GPT-4V Analysis**:\n   - Below each image, the AI provides an ordered list based on similarity to the image.\n   - For the flower, \"Japanese anemone\" is most similar.\n   - For the dog, \"Chihuahua\" is most similar.\n   - For the food, \"Falafel\" is most similar.\n\n3. **Rationale Behind the Analysis**:\n   - Explanations are given for each ordered list, detailing specific characteristics in the images that led to the classifications, such as petal color for flowers, physical features for the dog, and identifiable food components.\n\nThe panels highlight GPT-4V’s ability to perform image classification and provide detailed reasoning for its choices."}
{"layout": 113, "type": "text", "text": "Additionally, to assess the visual recognition and ranking capabilities of MLLMs themselves, we have prepared a prompt with examples to serve as input for the model. Our structured in-context learning prompt is as follows: ", "page_idx": 21, "bbox": [134, 445.4760437011719, 480, 481.91998291015625], "page_size": [612.0, 792.0]}
{"layout": 114, "type": "text", "text": "“Please play the role of a classification expert, and sort the provided categories from high to low according to the top 5 similarity with the input image. Here are the optional categories:{categories}. Your answer should follow the following format, like:[‘category A’, ‘category B’, ‘category C’, ‘category D’, ‘category E’]. Only choose five categories, and no further information.” ", "page_idx": 21, "bbox": [134, 482.38031005859375, 496.8561706542969, 554.0914916992188], "page_size": [612.0, 792.0]}
{"layout": 115, "type": "text", "text": "When testing the  RAR  pipeline with MLLMs, ‘{categories}’ is replaced with all the category names of each dataset. ", "page_idx": 21, "bbox": [134, 553.177001953125, 480, 577.6649169921875], "page_size": [612.0, 792.0]}
{"layout": 116, "type": "text", "text": "C More Implemented Details and Experiments ", "text_level": 1, "page_idx": 21, "bbox": [134, 595, 422, 610], "page_size": [612.0, 792.0]}
{"layout": 117, "type": "text", "text": "C.1 Fine-Grained Visual Recognition ", "text_level": 1, "page_idx": 21, "bbox": [133, 620, 329, 634], "page_size": [612.0, 792.0]}
{"layout": 118, "type": "text", "text": "In the fine-grained visual recognition section, we first evaluate our  RAR  on the setting defined in previous work [ 31 ]. For each category in the five datasets, ", "page_idx": 21, "bbox": [134, 641.6319580078125, 480, 666.1199340820312], "page_size": [612.0, 792.0]}
{"layout": 119, "type": "image", "page_idx": 22, "img_path": "layout_images/RAR_15.jpg", "img_caption": "Fig. 9: Evaluation on C  $\\mathbf{LIP+KN}$   for Caltech101, Flowers102, RAF-DB, Pets37, DTD and UCF101. We report the top-1, 5, 10, 15, 20 accuracy   $(\\%)$   under the 4-shot settings. ", "bbox": [133, 115, 481, 317], "page_size": [612.0, 792.0], "ocr_text": "Accuracy (%)\n\nAccuracy (%)\n\n100.\n\n90\n\n80.\n\n70)\n\nTop-k Accuracy on Caltech101\n\n927 99,81\neel\n\ni 3 io is 20\nTop-k\n\n‘Top-k Accuracy on Pets37\nTop Accuracy gay B=\ngee =\nlaced\ni 5 0\n\nio\nTop-k\n\n‘Top-k Accuracy on DTD\n\nas) Top curacy\n0\nEos: 3\n270! ae\nBe\ngay\n3] /\n5047.30\ni rr) Fa\nTop-k\nTop-k Accuracy on RAF-DB\n100) Topek Accuracy\nsare ~\n5 “\nz 6930-~\nco a\n5 aps\ngo\n204426\ni 5 10 15\nTop-k\n\nans\n\nEc)\n\n‘Top-k Accuracy on Flowers102\n200} ee\nai seg\n\n= 6\n\nz /\n\n= sa\n\nBoal /\n\ngo /\n\n< 35 |\nbo\n\n‘sss0 == ep Accuracy\nbahar Pe. y\ni$ rt) Fo %0\nTop-k\n‘Top-k Accuracy on UCF-101\n95 au\n—— pk Accuracy 5.4, 2\n90; z a\n\nBes ae\n\nB75,\n\n2 /\nwo\n65 6460\n\niS ro %\n\ni\nTop-k\n", "vlm_text": "The image contains six line graphs showing the top-k accuracy percentages for different datasets under 4-shot settings. The datasets are Caltech101, DTD (Describable Textures Dataset), Flowers102, Pets37, RAF-DB, and UCF101. Each graph depicts how accuracy increases as the value of k in top-k increases from 1 to 20.\n\n- **Caltech101**: Starts at 87.91% for top-1 and reaches 99.51% for top-20.\n- **DTD**: Starts at 50.45% for top-1 and reaches 87.53% for top-20.\n- **Flowers102**: Starts at 84.50% for top-1 and reaches 99.76% for top-20.\n- **Pets37**: Starts at 55.60% for top-1 and reaches 98.34% for top-20.\n- **RAF-DB**: Starts at 14.26% for top-1 and reaches 89.99% for top-20.\n- **UCF101**: Starts at 64.60% for top-1 and reaches 94.90% for top-20.\n\nThe overall trend indicates that accuracy improves with higher k values across all datasets."}
{"layout": 120, "type": "image", "page_idx": 22, "img_path": "layout_images/RAR_16.jpg", "img_caption": "Fig. 10: Evaluation on MLLMs  for Caltech101, Flowers102. We report the test results using 10, 15, 20, 25, and 30 category names as inputs. ", "bbox": [132, 341, 482, 483], "page_size": [612.0, 792.0], "ocr_text": "MLLMs tested on Caltech101\n\nMLLMs tested on Flowers102\n91.56\n2 ay = Top-k Accuracy —— Top-k Accuracy\n90 99.70\n~ .\nES _\n389 =\n8 Se\n3” 87.34\n*\nSey\n~s29.72\n86 eS a\n275 S26 72\n8s id\nTo 15 20 10\nTop-k\n\n15 20 Ey\n\n30\nTop-k\n", "vlm_text": "The image contains two line graphs evaluating the performance of Multimodal Large Language Models (MLLMs) on two different datasets: Caltech101 and Flowers102. Each graph shows how the Top-k accuracy changes as the number of category names used as inputs increases.\n\n1. **Left Graph (Caltech101):**\n   - The y-axis represents accuracy in percentage.\n   - The x-axis represents the number of category names used as inputs, ranging from 10 to 30.\n   - The trend shows a general decrease in accuracy from 91.56% at 10 category names to 85.23% at 30 category names.\n\n2. **Right Graph (Flowers102):**\n   - The y-axis represents accuracy in percentage.\n   - The x-axis represents the number of category names used as inputs, also ranging from 10 to 30.\n   - The trend also shows a decrease in accuracy, from 45.59% at 10 category names to 26.72% at 30 category names.\n\nBoth graphs use a dashed line to connect data points, illustrating how accuracy decreases as more category names are used as input in the model testing for both datasets."}
{"layout": 121, "type": "text", "text": "we select three unlabeled images to form a 3-shot setting. Then, we extract embeddings using the CLIP B/16 model and store them in memory. The labels for each image correspond to the predictions in [31]. We then test the validation set using the  RAR  pipeline and measure the results with Clustering Accuracy (cACC) and Semantic Similarity (sACC). ", "page_idx": 22, "bbox": [134, 512.7150268554688, 480, 573.0679931640625], "page_size": [612.0, 792.0]}
{"layout": 122, "type": "text", "text": "Evaluation Metrics.  In the fine-grained visual recognition section, we use two synergistic metrics: Clustering Accuracy (cACC) and Semantic Similarity (sACC) to evaluate our method, following [ 31 ]. Clustering Accuracy (cACC) mainly assesses the accuracy of clustering images within the same category, without considering the semantic relatedness of category labels. Complementing this, Semantic Similarity (sACC) measures the similarity between the names of categories in the clusters and the ground truth. ", "page_idx": 22, "bbox": [134, 581.8361206054688, 480, 666.1199951171875], "page_size": [612.0, 792.0]}
{"layout": 123, "type": "table", "page_idx": 23, "img_path": "layout_images/RAR_17.jpg", "table_caption": "Table 8:  Few-shot image classification across 11 datasets. We report the top-1 accuracy (  $\\%$  ) under the 1-shot, 2-shot, 4-shot, 8-shot and 16-shot settings. The CLIP  $^+$  KNN method does not utilize the text encoder of CLIP. Instead, we employ the visual encoder to extract image features, and then apply the KNN algorithm to these features. Here our  RAR  uses the LLaVA1.5 [ 28 ] as the MLLM to rank the retrieved results. The symbol ‘-’ denotes to the LLaVA model fails to make the predictions due to the limited window size. ", "bbox": [133, 113, 483, 457], "page_size": [612.0, 792.0], "ocr_text": "Method Common Fine-Grained\n= EI a\n2 S \" By o 2\n2 ¢ 8&8 & 8 ES ¢ EF gs §| $\n3 g 7 cA a 7 3 2 a Es g\n& 2 & 5 ° Q [=n 2 3 Ee) 5 Ea\nEB; = Zz g c 3 EI 3 & g\ng a = =] A B o 2 Ss S & zi\n4 is} o i) g a i=) a a 4 fo} <\n1-shot\nCLIP+KNN 29.2 75.9 11.3 37.7 53.9 35.1 47.8 66.7 326 453 413 | 43.3\nEna 5 Finetuning 84.1 © 24.9 - 48.2 223 354 459 - 39.2 16.3 | -\n.AR (LLaVA1.5) 40.3 85.2 348 46.5 62.4 38.1 57.4 504 38.3 57.6 47.0 | 50.7\n4 +10.5 +9.3 +23.5 +8.8 +8.5 +3.0 49.6 -16.3 +5.7 +12.3 +5.7|+7.4\n2-shot\nCLIP+KNN 36.1 829 11.7 446 58.7 41.2 585 789 409 541 49.0 | 506\nLLaVA1.5 Finetuning| — - 53.1 24.9 - 48.2 223 38.7 10.03 - 382 163 | -\nRAR (LLaVA1.5) 46.8 89.2 27.9 53.1 68.6 47.9 66.5 54.7 45.9 65.4 54.7 | 57.4\nA $10.7 +6.3 +16.2 +8.5 +9.9 +6.7 +8.0 -24.2 +5.0 411.3 +5.7|+6.8\n4-shot\nCLIP+KNN 42.1 87.9 142 514 676 47.5 646 84.5 49.2 62.6 55.6 | 57.0\nLLaVA1.5 Finetuning| — - 88.4 24.9 - 48.2 466 589 13.2  - 66.4 289 | -\n(LLaVA1.5) 51.0 92.1 27.7 58.8 74.8 53.9 69.6 804 54.4 71.4 60.9 | 63.2\n$9.9 +4.2 413.5 +7.4 47.2 46.4 +5.0 -4.1 +5.2 +88 45.3 | +6.2\n8-shot\nCLIP+KNN 47.6 90.6 28.2 56.8 728 53.2 683 89.5 561 683 61.8 | 63.0\nLLaVA1.5 Finetuning| — - 92.1 24.9 - 48.2 54.7 66.5 30.1 - 72.5 46.1 -\nRAR (LLaVA1.5) 56.5 93.5 46.9 63.4 81.5 59.3 74.3 87.3 61.2 76.6 67.7 | 69.8\nA +8.9 +2.9 +18.7 +66 +8.7 +6.1 +6.0 -2.2 45.1 +8.3 +5.9] +6.8\n16-shot\nCLIP+-KNN 52.0 92.4 35.0 61.2 78.7 57. 70.6 92.1 63.2 71.8 68.3 | 67.5\nLLaVA1.5 Finetuning| — - 94.1 24.9 - 50.6 63 747 59.0 - 62.4 | -\nAR (LLaVA1.5) 60.3 94.1 53.1 68.0 84.8 63.7 75.9 92.1 67.8 79.4 72.7 | 73.8\nA +8.3 41.7 418.1 46.8 +6.1 46.2 45.3 40.0 +4.6 +7.6 +4.4| +6.3\n\n", "vlm_text": "The table compares the performance results of different methods on image recognition tasks across various datasets. The table is divided into two main categories: \"Common\" and \"Fine-Grained,\" each containing several datasets. The methods evaluated include:\n\n1. CLIP + KNN\n2. LLaVA1.5 Finetuning\n3. RAR (LLaVA1.5)\n\nEach method's performance is displayed as a percentage. The table is further subdivided based on the number of shots (or examples) provided for training: 1-shot, 2-shot, 4-shot, 8-shot, and 16-shot. \n\nFor each method, the \"RAR (LLaVA1.5)\" row shows performance metrics, and the \"Δ\" row indicates the improvement in performance over the previous best approach – \"LLaVA1.5 Finetuning\" – for that particular shot setting. Improvements are highlighted with the corresponding positive difference from the previous method.\n\nPerformance average across all datasets for each method is also presented in the last column. The \"RAR (LLaVA1.5)\" consistently shows improvements over \"LLaVA1.5 Finetuning\" as denoted by the values in green."}
{"layout": 124, "type": "text", "text": "C.2 Few-Shot Image Classification ", "text_level": 1, "page_idx": 23, "bbox": [132, 479, 316, 491], "page_size": [612.0, 792.0]}
{"layout": 125, "type": "text", "text": "In this section, we delve deeper into some intriguing observations and motivations behind our study. Additionally, we have included an array of expanded test results in this part, encompassing classification tests from 1-shot to 16-shot, tests for top-5 accuracy, and we have further expanded our memory to explore the potential capabilities of  RAR . ", "page_idx": 23, "bbox": [134, 498.1700439453125, 480, 558.5239868164062], "page_size": [612.0, 792.0]}
{"layout": 126, "type": "text", "text": "More Discussion about Motivation.  In the field of image classification, es- pecially when facing the challenges of fine-grained image categorization, can MLLMs prove competent and effective? To further explore the potential of MLLMs in image classification tasks, we employed the GPT-4V model to test se- lected images from our fine-grained datasets. Initially, we used the CLIP+KNN method to select 5 candidate images and their categories for a single image, en- suring that these candidates are at the top-5 in similarity among all images in memory, thus guaranteeing minimal differences between the chosen categories. Additionally, we intentionally selected examples that CLIP failed to classify cor- ", "page_idx": 23, "bbox": [134, 557.9261474609375, 480, 666.1199951171875], "page_size": [612.0, 792.0]}
{"layout": 127, "type": "table", "page_idx": 24, "img_path": "layout_images/RAR_18.jpg", "table_caption": "Table 9:  Evaluation on 11 datasets, reporting the top-5 accuracy. We use the 4-shot setting. ", "bbox": [132, 114, 482, 234], "page_size": [612.0, 792.0], "ocr_text": "Fine-Grained\n\nCommon\n\nMethod\n\no8er0ay\n\nS}9qgP1OJXO\n\nTOTPOML\n\nsiegproyueyg\n\nZOTIOMOT.T\n\nTor-aon\n\nala\n\nLysomq\n\nL6€,\n\nNs\n\nad-ava\n\nTOTO\n\nyonoSuury\n\n87.6 | 80.8\n\n97.66 48.0 78.9 91.5 70.5 85.4 96.5 79.1 86.2\n\n| 671\n\n(LLaVA1.5) |\n\nCLIP+KNN\n\n71\n+1..\n\n69.7 97.7 53.8 80.1\n+2.6 +0.1 +5.8 +1.2\n\nra\n", "vlm_text": "The table compares the performance of two methods, CLIP+KNN and RAR (LLaVA1.5), across various datasets. The datasets are divided into two categories: Common and Fine-Grained. The table displays the following information:\n\n- CLIP+KNN performance scores for each dataset, as well as an average score.\n- RAR (LLaVA1.5) performance scores for each dataset and an average score.\n- The improvement (Δ) of RAR (LLaVA1.5) over CLIP+KNN for each dataset and on average.\n\nThe datasets listed are:\n\n**Common:**\n- ImageNet\n- Caltech101\n- RAF-DB\n- SUN397\n- EuroSAT\n- DTD\n- UCF-101\n\n**Fine-Grained:**\n- Flower102\n- Stanford Cars\n- Food101\n- Oxford Pets\n\nRAR (LLaVA1.5) shows improvements over CLIP+KNN in most datasets, as indicated by the positive values in the Δ row."}
{"layout": 128, "type": "table", "page_idx": 24, "img_path": "layout_images/RAR_19.jpg", "table_caption": "Table 10:  Evaluation on 11 datasets, reporting the top-1 accuracy. The GPT4V [ 38 ] results are copied from [ 50 ]. ", "bbox": [133, 248, 482, 400], "page_size": [612.0, 792.0], "ocr_text": "Method Common Fine-Grained\ng\n= E ,\n\n= #8 a 2\n\ng 3 D eA wa cy s € a g g\n\n2 =a 2 8 £ €@ 8 & & § | 8\n\nEe] & = Ed a a 5 Es a & 6|4\nGPT-4V 62.0 95.5 58.5 va? 36.2 59.1 81.6 70.6 58.3, 80.1 92.6 68.4\nRAR (LLaVA1.5) 73.4 94.6 73.8 70.6 93.3 71.9 79.1 95.6 72.6 86.2 79.9 81.0\nA 411.4 -0.9 415.3 +12.9 +57.1 412.8 -2.5 425.0 +14.3 +6.1 -12.7|+12.6\nRAR (Intern-IXC2)| 71.5 94.4 T23. 69.7 91.7 69.9 77.6 93.2 65.4 83.9 79.3 79.0,\nA 49.5 -1.1 +14.2 412.0 +55.5 +10.8 -4.0 +22.6 +7.1 4+3.8 -13.3/+10.6\nRAR (Qwen-VL) 75.8 95.5 66.0 72.7 90.7 72.5 81.4 97.5 81.6 87.2 88.1 82.6\nA +13.8 +0.0 +7.5 +5.0 +54.5 413.4 -0.2 +26.9 +23.3 +7.1 -4.5 |+14.2\n\n", "vlm_text": "This table compares different methods for performing tasks across various datasets. Here's a breakdown of the table structure:\n\n- **Methods Compared**: GPT-4V, RAR (LLaVA1.5), RAR (Intern-IXC2), and RAR (Qwen-VL).\n\n- **Datasets**: It includes results for both \"Common\" datasets (ImageNet, Caltech101, RAF-DB, SUN397, EuroSAT, DTD, UCF-101) and \"Fine-Grained\" datasets (Flower102, StanfordCars, Food101, OxfordPets).\n\n- **Results**: Each cell contains:\n  - The score achieved by the method.\n  - Δ (Delta) represents the improvement or change in the score compared to GPT-4V.\n\n- **Average Column**: Presents the average score across all datasets for each method.\n\n- **Color Coding**: Green text indicates improvement (positive change) compared to GPT-4V.\n\nKey Insights:\n- RAR methods generally improve upon GPT-4V, especially in \"EuroSAT,\" \"Flower102,\" and \"Average.\"\n- RAR (Qwen-VL) tends to have the highest scores and biggest improvements (Δ) across several datasets."}
{"layout": 129, "type": "text", "text": "rectly, increasing the complexity of the task. Subsequently, we presented these images and categories to GPT-4V, utilizing the prompt described in Appendix  B , prompting GPT-4V to rank all categories by similarity. During this process, we also requested GPT-4V to provide the rationale for its classifications, allowing us to analyze the specific role of MLLMs in classification tasks based on the reasons provided by GPT-4V. Fig.  7  and Fig.  8  presents several examples of five fine-grained classification datasets. ", "page_idx": 24, "bbox": [134, 423.94305419921875, 480, 508.2069396972656], "page_size": [612.0, 792.0]}
{"layout": 130, "type": "text", "text": "From the examples in Fig.  7  and Fig.  8 , it is evident that GPT-4V is ca- pable of effectively analyzing the main feature information of objects in images during fine-grained image classification tasks. For instance, it identifies key char- act eris tics such as “ coupe ” (a two-door car), “ long fuselage ” (long body of an aircraft), and “ prominent ears ” (noticeably protruding ears), which are crucial for distinguishing between similar categories. Sometimes, these detailed aspects may be overlooked by the CLIP model, leading to classification errors. Therefore, adopting a method of initial retrieval followed by deeper analysis, firstly filter- ing through the numerous fine-grained categories and then using MLLMs for further examination to select the most accurate answer, proves to be an effective approach for fine-grained image classification tasks. ", "page_idx": 24, "bbox": [134, 508.876953125, 480, 640.9619140625], "page_size": [612.0, 792.0]}
{"layout": 131, "type": "text", "text": "Simultaneously, we assessed CLIP’s accuracy in handling a variety of clas- sification datasets. We selected six datasets: Caltech101, Flower102, RAF-DB, ", "page_idx": 24, "bbox": [134, 641.6319580078125, 480, 666.1199340820312], "page_size": [612.0, 792.0]}
{"layout": 132, "type": "table", "page_idx": 25, "img_path": "layout_images/RAR_20.jpg", "table_caption": "Table 11: Cropping ablation of CLIP [ 41 ] zero-shot classification  on LVIS [ 14 ] with ground truth proposals. Different behaviors can be seen before and after blurring with respect to different object scales. ", "bbox": [132, 113, 482, 293], "page_size": [612.0, 792.0], "ocr_text": "Crop scale Blurring| 1.0 12 14 16 18 2.0 22 24 26 28 3.0\nx 46.7 47.0 46.6 46.4 43.4 43.040.940.7 37.7 37.1 36.2\n\nAP v 47.9 51.3 52.2 53.9 53.3 52.952.651.8 51.2 50.3 49.8\n\nx 39.5 40.9 446 44.8 444 44.242.943.3 41.2 40.5 39.8\n\nts v 33.6 35.2 41.4 43.2 45.6 46.3 46.746.9 47.4 47.4 47.3\n\nx 61.5 61.3 56.4 55.2 49.5 48.6 44.4 43.7 39.9 39.0 38.5\n\nAP v 63.5 64.2 66.1 68.3 65.2 64.2 63.462.2 61.0 59.2 58.6\n\nx 59.1 57.2 51.1 50.1 45.6 44.4 41.440.9 38.0 37.8 37.2\n\nAPI v 72.4 71.3 69.5 69.6 67.0 65.2 62.960.7 59.6 57.4 55.2\n\n", "vlm_text": "The table shows the results of some performance metrics (AP, APs, APm, AP1) under different crop scales and blurring conditions.\n\n- Crop scale values range from 1.0 to 3.0.\n- There are two blurring conditions, indicated by symbols: ✔️ and ✘.\n- AP metrics are evaluated: \n  - AP\n  - APs (small objects)\n  - APm (medium objects)\n  - AP1\n- Each cell contains numeric scores corresponding to the performance of each metric under different conditions.\n- Certain scores are highlighted in green or red.\n\nThis table likely pertains to a comparison of model performance under varying image preprocessing conditions."}
{"layout": 133, "type": "text", "text": "Pets37, DTD, and UCF101, and tested the CLIP+KNN method for top 1, 5, 10, 15, and 20 accuracy, with results presented in Fig.  9 . We observed that as the top-k value increased, the classification accuracy improved rapidly, reaching over  $90\\%$   in four of the six datasets when top-k reached 10. This indicates that CLIP shows significant advantages as the number of predicted categories increases, complementing MLLMs’ ability to discern among similar categories. ", "page_idx": 25, "bbox": [134, 321.42803955078125, 480, 393.7369384765625], "page_size": [612.0, 792.0]}
{"layout": 134, "type": "text", "text": "Following the experimental design in Fig.  9 , we used MLLMs to rank cate- gories when expanding the number of categories. We chose two datasets, Cal- tech101 and Flowers102, and used 10, 15, 20, 25, 30 categories as input to MLLMs, ensuring these included the correct category. As shown in Fig.  10 , the distinction ability of MLLMs gradually decreased as the number of categories input into MLLMs increased. ", "page_idx": 25, "bbox": [134, 396.281982421875, 480, 468.59088134765625], "page_size": [612.0, 792.0]}
{"layout": 135, "type": "text", "text": "Hence, we found that MLLMs and CLIP have complementary advantages in classification tasks. CLIP initially narrows down the correct answer to a smaller set through preliminary screening, while MLLMs can finely select the correct an- swer from this set. Our  RAR  combines the strengths of both CLIP and MLLMs, first finding likely correct candidates through CLIP and retrieval, and then ac- curately selecting the correct answer through MLLMs’ ranking, thus achieving outstanding results across multiple classification datasets. ", "page_idx": 25, "bbox": [134, 471.1369323730469, 480, 555.4008178710938], "page_size": [612.0, 792.0]}
{"layout": 136, "type": "text", "text": "More Evaluation Results.  In our few-shot image classification experiments, we employed the CLIP B/16 model to extract embeddings from n images in each category, which were then stored in memory for testing the accuracy of n-shot experiments. To accelerate retrieval speed, we initially use the HNSW algorithm to transform the original 576-dimensional vectors into 64-dimensional indices before storing the image embeddings in memory. HNSW is a commonly used Approximate Nearest Neighbor (ANN) algorithm, primarily aimed at quickly finding the k nearest elements to a query in a large set of candidates. To demon- strate the effectiveness of our method, we included results from 1-shot, 2-shot, ", "page_idx": 25, "bbox": [134, 557.9259643554688, 480, 666.119873046875], "page_size": [612.0, 792.0]}
{"layout": 137, "type": "image", "page_idx": 26, "img_path": "layout_images/RAR_21.jpg", "img_caption": "Fig. 11: Metric curve visualization of CLIP [ 41 ] zero-shot classification  on LVIS [ 14 ] with ground truth proposals. Different behaviors can be seen before and after blurring with respect to different object’s scales. ", "bbox": [134, 123, 481, 382], "page_size": [612.0, 792.0], "ocr_text": "1.00\n\n125,\n\n1.50\n\n175:\n\n2.00\nCrop Scale\n\n2.25\n\n2.50\n\n2.75\n\n3.00\n", "vlm_text": "The image is a graph showing the metric curve visualization of CLIP's zero-shot classification performance on the LVIS dataset using ground truth proposals. The graph plots Average Precision (AP) against various crop scales. It includes different colored and styled lines representing various metrics:\n\n- **APs (small objects)** and **APs_c** (blurred): Blue lines with triangles\n- **APm (medium objects)** and **APm_c** (blurred): Green lines with crosses\n- **APl (large objects)** and **APl_c** (blurred): Red lines with squares\n\nThe graph illustrates how performance varies with changes in object scale and the effect of blurring on classification accuracy."}
{"layout": 138, "type": "text", "text": "and 16-shot experiments in the supplementary materials, alongside the results of 4-shot and 8-shot experiments, all of which are presented in Tab.  8 . ", "page_idx": 26, "bbox": [134, 401.21002197265625, 480, 425.6979675292969], "page_size": [612.0, 792.0]}
{"layout": 139, "type": "text", "text": "From the 1-shot to 16-shot experiments,  RAR ’s results showed an improve- ment over the CLIP+KNN method by    $7.4\\%$  ,    $6.8\\%$  ,    $6.2\\%$  ,    $6.8\\%$  , and    $6.3\\%$   re- spectively, averaging a  6.7%  percentage point increase, and significantly outper- forming the performance of the LLaVa model itself. This outcome demonstrates the excellence of  RAR  in image classification tasks (including fine-grained image classification), achieved by integrating the strengths of MLLMs and retrieval techniques. ", "page_idx": 26, "bbox": [134, 428.4389953613281, 480, 512.702880859375], "page_size": [612.0, 792.0]}
{"layout": 140, "type": "text", "text": "Top-5 Accuracy Results.  Moreover, in the experiments conducted for our paper, we selected the top 5 retrieved results for ranking. To test the s cal ability of this method, we conducted a new experiment using the top 10 retrieved results, ranking these ten categories and then assessing the accuracy of the top 5. In this experiment, we utilized a 4-shot setting, the result is shown in Tab.  9 . ", "page_idx": 26, "bbox": [134, 515.4230346679688, 480, 575.796875], "page_size": [612.0, 792.0]}
{"layout": 141, "type": "text", "text": "The final results demonstrate that although the top 5 accuracy achieved by CLIP+KNN was already high, our  RAR  method still managed to make compre- hensive improvements on this basis. The average top 5 accuracy across eleven datasets increased by    $1.3\\%$  . ", "page_idx": 26, "bbox": [134, 578.5379638671875, 480, 626.9359130859375], "page_size": [612.0, 792.0]}
{"layout": 142, "type": "text", "text": "Extension to the whole Training Set.  To further explore the potential of RAR , we expanded the memory size to include all images from the training set stored in memory. We then compared the performance of  RAR  under this setup with that of GPT-4V across multiple image classification datasets. The results are presented in Tab.  10 . ", "page_idx": 26, "bbox": [134, 629.6570434570312, 480, 666.119873046875], "page_size": [612.0, 792.0]}
{"layout": 143, "type": "text", "text": "", "page_idx": 27, "bbox": [134, 116.46308135986328, 480, 140.9510498046875], "page_size": [612.0, 792.0]}
{"layout": 144, "type": "text", "text": "The results in Tab.  10  show that, regardless of whether the base model is LLaVa, Intern-IXC2, or Qwen-VL,  RAR  significantly outperforms GPT-4V in terms of accuracy. Across eleven datasets, the average precision of  RAR  exceeds that of GPT-4V by 12.5 percentage points. It is observed that even 7B MLLMs, when integrated into the  RAR  pipeline, far surpass the classification capabilities of GPT-4V across multiple image classification datasets. ", "page_idx": 27, "bbox": [134, 140.37408447265625, 480, 212.68212890625], "page_size": [612.0, 792.0]}
{"layout": 145, "type": "text", "text": "C.3 Zero-Shot Region Recognition ", "text_level": 1, "page_idx": 27, "bbox": [132, 231, 315, 243], "page_size": [612.0, 792.0]}
{"layout": 146, "type": "text", "text": "We carefully study how to adapt CLIP and MLLMs pretrained on full images to region-level recognition tasks. Zero-shot LVIS [ 14 ] AP metric under different crop scales and object scales are reported in Fig.  11  and Tab.  11 . Based on this experiment, we conclude with two major observations: Firstly, a proper amount of blurring can significantly improve classification accuracy. This trick can help leave enough context information while keeping the foreground object promi- nent. Secondly, for objects with different scales, different crop scales should be adapted to maximize classification accuracy. As shown in Fig.  11 , after blur- ring, Different object scale AP curves behave differently with respect to crop scale. We contribute this phenomenon to the resolution shift of CLIP input im- ages. Therefore, we make two adaptations for CLIP and MLLMs for region-level recognition: Gaussian blurring and adaptive crop scale. We adopt the hyperpa- rameters of these two tricks on the LVIS training set and find these adaptions not only fit for the LVIS validation set but also other detection datasets like V3Det [ 48 ]. ", "page_idx": 27, "bbox": [134, 249.96319580078125, 480, 429.8680114746094], "page_size": [612.0, 792.0]}
