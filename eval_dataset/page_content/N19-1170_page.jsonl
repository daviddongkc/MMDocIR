{"page": 0, "image_path": "doc_images/N19-1170_0.jpg", "ocr_text": "What makes a good conversation?\nHow controllable attributes affect human judgments\n\nAbigail See*\nStanford University\n\nabisee@stanford.edu\n\nStephen Roller\n\nroller@fb.com\n\nAbstract\n\nA good conversation requires balance — be-\ntween simplicity and detail; staying on topic\nand changing it; asking questions and an-\nswering them. Although dialogue agents are\ncommonly evaluated via human judgments\nof overall quality, the relationship between\nquality and these individual factors is less\nwell-studied. In this work, we examine two\ncontrollable neural text generation methods,\nconditional training and weighted decoding,\nin order to control four important attributes\nfor chitchat dialogue: repetition, specificity,\nresponse-relatedness and question-asking. We\nconduct a large-scale human evaluation to\nmeasure the effect of these control parame-\nters on multi-turn interactive conversations on\nthe PersonaChat task. We provide a detailed\nanalysis of their relationship to high-level as-\npects of conversation, and show that by con-\ntrolling combinations of these variables our\nmodels obtain clear improvements in human\nquality judgments.\n\n1 Introduction\n\nNeural generation models for dialogue, despite\ntheir ubiquity in current research, are still poorly\nunderstood. Well known problems, such as the\ngenericness and repetitiveness of responses (Ser-\nban et al., 2016a), remain without a de facto solu-\ntion. Strikingly, the factors that determine human\njudgments of overall conversation quality are al-\nmost entirely unexplored. Most works have been\nlimited to the next utterance prediction problem,\nwhereas a multi-turn evaluation is necessary to\nevaluate the quality of a full conversation.\n\nIn this work we both (i) conduct a large-scale\nstudy to identify the fine-grained factors governing\nhuman judgments of full conversations, and (ii)\ndevelop models that apply our findings in practice,\n\n*A.S. completed most of this work at Facebook (FAIR).\n\nDouwe Kiela Jason Weston\n\nFacebook AI Research Facebook AI Research Facebook AI Research\n\ndkiela@fb.com jase@fb.com\n\nLow-level\ncontrollable attributes\n\nRepetition\n(n-gram overlap)\n\nHuman judgment of\nconversational aspects\n\nAvoiding Repetition\n\nHuman judgment of\noverall quality\n\nSpecificity Interestingness\n(normalized inverse\ndocument frequency) Making sense | { Humanness }\n\n=\n\nResponse-relatedness Fluency } { Engagingness }\n(cosine similarity of\nsentence embeddings) (Uistening |\n\nInquisitiveness\n\nFigure 1: We manipulate four low-level attributes and\nmeasure their effect on human judgments of individual\nconversational aspects, as well as overall quality.\n\nleading to state-of-the-art performance. Specifi-\ncally, we identify and study eight aspects of con-\nversation that can be measured by human judg-\nments, while varying four types of low-level at-\ntributes that can be algorithmically controlled in\nneural models; see Figure 1. To control the low-\nlevel model attributes, we consider two simple but\ngeneral algorithms: conditional training, in which\nhe neural model is conditioned on additional con-\ntrol features, and weighted decoding, in which\ncontrol features are added to the decoding scoring\n‘unction at test time only.\n\nOne major result of our findings is that existing\nwork has ignored the importance of conversational\nflow, as standard models (i) repeat or contradict\nprevious statements, (ii) fail to balance specificity\nwith genericness, and (iii) fail to balance asking\nquestions with other dialogue acts. Conducting\nexperiments on the PersonaChat task (Zhang et al.,\n2018b), we obtain significantly higher engaging-\nness scores than the baseline by optimizing con-\ntrol of repetition, specificity and question-asking\nover multiple turns. Using these findings, our best\nmodel matches the performance of the winning en-\ntry in the recent NeurIPS ConvAI2 competition\n(Dinan et al., 2019), which was trained on much\n\n1702\n\nProceedings of NAACL-HLT 2019, pages 1702-1723\nMinneapolis, Minnesota, June 2 - June 7, 2019. ©2019 Association for Computational Linguistics\n", "vlm_text": "What makes a good conversation? How controllable attributes affect human judgments \nAbigail See ∗ Stephen Roller Douwe Kiela Jason Weston Stanford University Facebook AI Research Facebook AI Research Facebook AI Research abisee@stanford.edu roller@fb.com dkiela@fb.com jase@fb.com \nAbstract \nA good conversation requires balance – be- tween simplicity and detail; staying on topic and changing it; asking questions and an- swering them. Although dialogue agents are commonly evaluated via human judgments of overall quality, the relationship between quality and these individual factors is less well-studied. In this work, we examine two controllable neural text generation methods, conditional training and weighted decoding, in order to control four important attributes for chitchat dialogue: repetition, speciﬁcity, response-relatedness and question-asking. We conduct a large-scale human evaluation to measure the effect of these control parame- ters on multi-turn interactive conversations on the PersonaChat task. We provide a detailed analysis of their relationship to high-level as- pects of conversation, and show that by con- trolling combinations of these variables our models obtain clear improvements in human quality judgments. \n1 Introduction \nNeural generation models for dialogue, despite their ubiquity in current research, are still poorly understood. Well known problems, such as the genericness and repetitiveness of responses ( Ser- ban et al. ,  2016a ), remain without a de facto solu- tion. Strikingly, the factors that determine human judgments of overall conversation quality are al- most entirely unexplored. Most works have been limited to the next utterance prediction problem, whereas a multi-turn evaluation is necessary to evaluate the quality of a full conversation. \nIn this work we both (i) conduct a large-scale study to identify the ﬁne-grained factors governing human judgments of full conversations, and (ii) develop models that apply our ﬁndings in practice, \nThe image is a flowchart illustrating the relationship between low-level controllable attributes, human judgment of conversational aspects, and overall quality judgment. \n\n- **Low-level controllable attributes** include:\n  - Repetition (n-gram overlap)\n  - Specificity (normalized inverse document frequency)\n  - Response-relatedness (cosine similarity of sentence embeddings)\n  - Question-asking (\"?\" used in utterance)\n\n- **Human judgment of conversational aspects** includes:\n  - Avoiding Repetition\n  - Interestingness\n  - Making sense\n  - Fluency\n  - Listening\n  - Inquisitiveness\n\n- These aspects contribute to the **human judgment of overall quality**:\n  - Humanness\n  - Engagingness\n\nThe flow shows how these attributes influence conversational aspects, which in turn affect the overall quality assessment of a conversation.\nFigure 1: We manipulate four low-level attributes and measure their effect on human judgments of individual conversational aspects, as well as overall quality. \nleading to state-of-the-art performance. Speciﬁ- cally, we identify and study eight aspects of con- versation that can be measured by human judg- ments, while varying four types of low-level at- tributes that can be algorithmically controlled in neural models; see Figure  1 . To control the low- level model attributes, we consider two simple but general algorithms: conditional training, in which the neural model is conditioned on additional con- trol features, and weighted decoding, in which control features are added to the decoding scoring function at test time only. \nOne major result of our ﬁndings is that existing work has ignored the importance of conversational ﬂow, as standard models (i) repeat or contradict previous statements, (ii) fail to balance speciﬁcity with genericness, and (iii) fail to balance asking questions with other dialogue acts. Conducting experiments on the PersonaChat task ( Zhang et al. , 2018b ), we obtain signiﬁcantly higher engaging- ness scores than the baseline by optimizing con- trol of repetition, speciﬁcity and question-asking over multiple turns. Using these ﬁndings, our best model matches the performance of the winning en- try in the recent NeurIPS ConvAI2 competition ( Dinan et al. ,  2019 ), which was trained on much more data but had no control (see Section  8.1 ). Our code, pretrained models, and full chatlogs, are available at  https://parl.ai/projects/ controllable dialogue . "}
{"page": 1, "image_path": "doc_images/N19-1170_1.jpg", "ocr_text": "more data but had no control (see Section 8.1).\nOur code, pretrained models, and full chatlogs, are\navailable at https://parl.ai/projects/\ncontrollable_dialogue.\n\n2 Related Work\n\nDialogue Dialogue evaluation is relatively well\nunderstood in goal-oriented tasks, where auto-\nmated approaches can be coded by measuring task\ncompletion (Bordes et al., 2017; El Asri et al.,\n2017; Hastie, 2012; Henderson et al., 2014; Wen\net al., 2017). Task success combined with dia-\nlogue cost can be linked to human judgments like\nuser satisfaction via the PARADISE framework\n(Walker et al., 1997).\n\nHowever in chitchat tasks, which we study in\nthis work, automatic metrics and their relation to\nhuman ratings are less well-understood. While\nword-overlap metrics are effective for question-\nanswering and machine translation, for dialogue\nthey have little to no correlation with human judg-\nments (Liu et al., 2016; Novikova et al., 2017) —\nthis is due to the open-ended nature of dialogue.\nThere are more recent attempts to find better auto-\nmatic approaches, such as adversarial evaluation\n(Li et al., 2017b) and learning a scoring model\n(Lowe et al., 2017), but their value is still unclear.\n\nNevertheless, a number of studies only use au-\ntomatic metrics, with no human study at all (Lowe\net al., 2015; Parthasarathi and Pineau, 2018; Ser-\nban et al., 2016b). Other works do use human\nevaluations (Dinan et al., 2018; Li et al., 2016a,b;\nVenkatesh et al., 2017; Vinyals and Le, 2015;\nZhang et al., 2018b), typically reporting just one\ntype of judgment (either quality or appropriate-\nness) via a Likert scale or pairwise comparison.\nMost of those works only consider single turn\nevaluations, often with a shortened dialogue his-\ntory, rather than full multi-turn dialogue.\n\nA more comprehensive evaluation strategy has\nbeen studied within the scope of the Alexa prize\n(Venkatesh et al., 2017; Guo et al., 2018) by com-\nbining multiple automatic metrics designed to cap-\nture various conversational aspects (engagement,\ncoherence, domain coverage, conversational depth\nand topical diversity). Though these aspects have\nsome similarity to the aspects studied here, we also\nfocus on lower-level aspects (e.g. avoiding repeti-\ntion, fluency), to understand how they correspond\nto both our controllable attributes, and to overall\nquality judgments.\n\nControllable neural text generation Re-\nsearchers have proposed several approaches to\ncontrol aspects of RNN-based natural language\ngeneration such as sentiment, length, speaker\nstyle and tense (Fan et al., 2018; Ficler and\nGoldberg, 2017; Ghazvininejad et al., 2017; Hu\net al., 2017; Kikuchi et al., 2016; Peng et al.,\n2018; Wang et al., 2017). In particular, several\nworks use control to tackle the same common\nsequence-to-sequence problems we address here\n(particularly genericness and unrelated output),\nin the context of single-turn response generation\n(Baheti et al., 2018; Li et al., 2016a, 2017a; Shen\net al., 2017; Xing et al., 2017; Zhang et al., 2018a;\nZhou et al., 2017). By contrast, we focus on\ndeveloping controls for, and human evaluation of,\nmulti-turn interactive dialogue — this includes a\nnew method (described in Section 5) to control\nattributes at the dialogue level rather than the\nutterance level.\n\nIn this work, we require a control method that\nis both general-purpose (one technique to simul-\naneously control many attributes) and easily tun-\nable (the control setting is adjustable after train-\ning). Given these constraints, we study two control\nmethods: conditional training (variants of which\nhave been described by Fan et al. (2018); Kikuchi\net al. (2016); Peng et al. (2018)) and weighted de-\ncoding (described by Ghazvininejad et al. (2017)\nas a general technique, and by Baheti et al. (2018)\n0 control response-relatedness). To our knowl-\nedge, this work is the first to systematically com-\npare the effectiveness of two general-purpose con-\ntrol methods across several attributes.\n\n3 The PersonaChat dataset\n\nPersonaChat (Zhang et al., 2018b) is a chitchat\ndialogue task involving two participants (two hu-\nmans or a human and a bot). Each participant is\ngiven a persona — a short collection of personal\ntraits such as I’m left handed or My favorite season\nis spring — and are instructed to get to know each\nother by chatting naturally using their designated\npersonas, for 6-8 turns. The training set contains\n8939 conversations and 955 personas, collected\nvia crowdworkers, plus 1000 conversations and\n100 personas for validation, and a similar number\nin the hidden test set. The PersonaChat task was\nthe subject of the NeurIPS 2018 ConvAI2 Chal-\nlenge (Dinan et al., 2019), in which competitors\nwere first evaluated with respect to automatic met-\n\n1703\n", "vlm_text": "\n2 Related Work \nDialogue Dialogue evaluation is relatively well understood in goal-oriented tasks, where auto- mated approaches can be coded by measuring task completion ( Bordes et al. ,  2017 ;  El Asri et al. , 2017 ;  Hastie ,  2012 ;  Henderson et al. ,  2014 ;  Wen et al. ,  2017 ). Task success combined with dia- logue cost can be linked to human judgments like user satisfaction via the PARADISE framework ( Walker et al. ,  1997 ). \nHowever in chitchat tasks, which we study in this work, automatic metrics and their relation to human ratings are less well-understood. While word-overlap metrics are effective for question- answering and machine translation, for dialogue they have little to no correlation with human judg- ments ( Liu et al. ,  2016 ;  Novikova et al. ,  2017 ) – this is due to the open-ended nature of dialogue. There are more recent attempts to ﬁnd better auto- matic approaches, such as adversarial evaluation\n\n ( Li et al. ,  2017b ) and learning a scoring model\n\n ( Lowe et al. ,  2017 ), but their value is still unclear. Nevertheless, a number of studies only use au- tomatic metrics, with no human study at all ( Lowe et al. ,  2015 ;  Parthasarathi and Pineau ,  2018 ;  Ser- ban et al. ,  2016b ). Other works do use human evaluations ( Dinan et al. ,  2018 ;  Li et al. ,  2016a , b ; Venkatesh et al. ,  2017 ;  Vinyals and Le ,  2015 ; Zhang et al. ,  2018b ), typically reporting just one type of judgment (either quality or appropriate- ness) via a Likert scale or pairwise comparison. Most of those works only consider single turn evaluations, often with a shortened dialogue his- tory, rather than full multi-turn dialogue. \nA more comprehensive evaluation strategy has been studied within the scope of the Alexa prize ( Venkatesh et al. ,  2017 ;  Guo et al. ,  2018 ) by com- bining multiple automatic metrics designed to cap- ture various conversational aspects (engagement, coherence, domain coverage, conversational depth and topical diversity). Though these aspects have some similarity to the aspects studied here, we also focus on lower-level aspects (e.g. avoiding repeti- tion, ﬂuency), to understand how they correspond to both our controllable attributes, and to overall quality judgments. \nControllable neural text generation Re- searchers have proposed several approaches to control aspects of RNN-based natural language generation such as sentiment, length, speaker style and tense ( Fan et al. ,  2018 ;  Ficler and Goldberg ,  2017 ;  Ghazvininejad et al. ,  2017 ;  Hu et al. ,  2017 ;  Kikuchi et al. ,  2016 ;  Peng et al. , 2018 ;  Wang et al. ,  2017 ). In particular, several works use control to tackle the same common sequence-to-sequence problems we address here (particularly genericness and unrelated output), in the context of single-turn response generation ( Baheti et al. ,  2018 ;  Li et al. ,  2016a ,  2017a ;  Shen et al. ,  2017 ;  Xing et al. ,  2017 ;  Zhang et al. ,  2018a ; Zhou et al. ,  2017 ). By contrast, we focus on developing controls for, and human evaluation of, multi -turn interactive dialogue – this includes a new method (described in Section  5 ) to control attributes at the  dialogue  level rather than the utterance level. \nIn this work, we require a control method that is both general-purpose (one technique to simul- taneously control many attributes) and easily tun- able (the control setting is adjustable after train- ing). Given these constraints, we study two control methods: conditional training (variants of which have been described by  Fan et al.  ( 2018 );  Kikuchi et al.  ( 2016 );  Peng et al.  ( 2018 )) and weighted de- coding (described by  Ghazvininejad et al.  ( 2017 ) as a general technique, and by  Baheti et al.  ( 2018 ) to control response-relatedness). To our knowl- edge, this work is the ﬁrst to systematically com- pare the effectiveness of two general-purpose con- trol methods across several attributes. \n3 The PersonaChat dataset \nPersonaChat ( Zhang et al. ,  2018b ) is a chitchat dialogue task involving two participants (two hu- mans or a human and a bot). Each participant is given a  persona  – a short collection of personal traits such as  I’m left handed  or  My favorite season is spring  – and are instructed to get to know each other by chatting naturally using their designated personas, for 6–8 turns. The training set contains 8939 conversations and 955 personas, collected via crowdworkers, plus 1000 conversations and 100 personas for validation, and a similar number in the hidden test set. The PersonaChat task was the subject of the NeurIPS 2018 ConvAI2 Chal- lenge ( Dinan et al. ,  2019 ), in which competitors were ﬁrst evaluated with respect to automatic met- rics (perplexity, hits  $@\\,1$   and F1 score), and then with respect to human judgment via the question “How much did you enjoy talking to this user?” on a scale of 1–4. "}
{"page": 2, "image_path": "doc_images/N19-1170_2.jpg", "ocr_text": "rics (perplexity, hits@1 and F1 score), and then\nwith respect to human judgment via the question\n“How much did you enjoy talking to this user?”\non a scale of 1-4.\n\n4 Baseline model\n\nOur baseline model is a 2-layer LSTM sequence-\nto-sequence model with attention. On any dia-\nlogue turn, the input x to the encoder is the entire\ndialogue history (separated using unique speaker-\nidentifying tokens), with the model’s own persona\nprepended. Conditioned on this input sequence\na, the decoder generates a response y. Except\nwhen stated otherwise, all our models decode us-\ning beam search with beam size 20.\n\nWe initialized the word embedding matrix with\n300-dimensional GloVe embeddings (Pennington\net al., 2014). Using the ParlAI framework (Miller\net al., 2017), we pretrained the model on a dataset\nof 2.5 million Twitter message-response pairs,!\nthen fine-tuned it on PersonaChat. On the Per-\nsonaChat validation set, the baseline model has a\nperplexity of 26.83 and F1 of 17.02, which would\nhave placed us 4th out of 26 models in the Con-\nvAI2 competition (Dinan et al., 2019). We attempt\nto improve over this baseline using control.\n\n5 Controllable text generation methods\n\nSuppose we have a sequence-to-sequence model\nwhich gives P(y|z) = IyP(yilz,y1,---,Yt—-1)s\nthe conditional probability of a response y (the\nmodel’s next utterance) given input x (the context,\nwhich in our case includes the model’s own per-\nsona and the dialogue history).\n\nContrary to most previous work, which con-\ntrols at the sentence level, we wish to control at-\ntributes of the output y at the dialogue level —\nmeaning that a single control setting is used for a\nwhole dialogue. For example, to control question-\nasking, we provide a control setting at the begin-\nning of each dialogue (e.g. 20% questions or 70%\nquestions) rather than providing a control setting\nfor each utterance (e.g. is a question or isn’t a\nquestion). With this approach, the sequence-to-\nsequence model is able to choose what value the\ncontrolled attribute should take for any particular\nutterance, but we are able to choose the overall dis-\ntribution. We find that this approach works well\n— for example, the sequence-to-sequence model is\n\n'The Twitter dataset is provided in ParlAI; details can be\nfound here: https: //parl.ai/docs/tasks.html\n\ngenerally good at detecting when to ask a ques-\ntion. In particular, this is easier than the alterna-\ntive: developing a separate process to decide, for\neach utterance, whether to ask a question.\n\nIn this section, we describe the two methods\n— which we call Conditional Training (CT) and\nWeighted Decoding (WD) — that we use to control\nattributes of the output y at the dialogue level.\n\n5.1. Conditional Training (CT)\n\nConditional Training (Fan et al., 2018; Kikuchi\net al., 2016; Peng et al., 2018) is a method to\nlearn a sequence-to-sequence model P(y|x, z),\nwhere z is a discrete control variable. If the\ncontrol attribute is naturally continuous (for ex-\nample in our work, repetitiveness, specificity and\nresponse-relatedness), we use z to represent buck-\neted ranges. For a binary attribute like question-\nasking, z represents an overall probability (as ex-\nplained in Section 5).\n\nTo train a CT model, we first automatically an-\nnotate every (x, y) pair in the training set with the\nattribute we wish to control (for example, whether\ny contains a question mark). During training,\n‘or each example we determine the correspond-\ning z value (for continuous attributes, this sim-\nply means sorting into the correct bucket; for\nquestion-asking, see Section 6.4). Next, the con-\ntrol variable z is represented via an embedding\n(each of the possible values of z has its own em-\nbedding). For all our experiments, the embedding\nis of length 10; this was determined via hyperpa-\nrameter tuning. There are several possible ways\n0 condition the sequence-to-sequence model on z\n— for example, append z to the end of the input\nsequence, or use z as the START symbol for the\ndecoder. We find it most effective to concatenate\nz to the decoder’s input on every step.” Lastly, the\nCT model learns to produce y = yi,...,yr by\noptimizing the cross-entropy loss:\n\nT\nlosscr = -z ST log P(yla, 2.915 ves Yt-1)\nt=1\nOur CT models are initialized with the parameters\nfrom the baseline sequence-to-sequence model\nP(y|a) (the new decoder parameters are initial-\nized with small random values), then fine-tuned to\noptimize losscr on the PersonaChat training set,\nuntil convergence of losscr on the validation set.\n?To build a CT model P(y|x, 21,..., Zn) conditioned on\n\nmultiple controls {21,...,2n}, we can simply concatenate\nmultiple control embeddings to the decoder inputs.\n\n1704\n", "vlm_text": "\n4 Baseline model \nOur baseline model is a 2-layer LSTM sequence- to-sequence model with attention. On any dia- logue turn, the input    $x$   to the encoder is the entire dialogue history (separated using unique speaker- identifying tokens), with the model’s own persona prepended. Conditioned on this input sequence  $x$  , the decoder generates a response    $y$  . Except when stated otherwise, all our models decode us- ing beam search with beam size 20. \nWe initialized the word embedding matrix with 300-dimensional GloVe embeddings ( Pennington et al. ,  2014 ). Using the ParlAI framework ( Miller et al. ,  2017 ), we pretrained the model on a dataset of 2.5 million Twitter message-response pairs, then ﬁne-tuned it on PersonaChat. On the Per- sonaChat validation set, the baseline model has a perplexity of 26.83 and F1 of 17.02, which would have placed us 4th out of 26 models in the Con- vAI2 competition ( Dinan et al. ,  2019 ). We attempt to improve over this baseline using control. \n5 Controllable text generation methods \nSuppose we have a sequence-to-sequence model which gives    $P(y|x)\\;=\\;\\Pi_{t}P\\big(y_{t}|x,y_{1},.\\,.\\,.\\,,y_{t-1}\\big),$  , the conditional probability of a response    $y$   (the model’s next utterance) given input  $x$   (the context, which in our case includes the model’s own per- sona and the dialogue history). \nContrary to most previous work, which con- trols  at the sentence level , we wish to control at- tributes of the output    $y$   at the dialogue level  – meaning that a single control setting is used for a whole dialogue. For example, to control question- asking, we provide a control setting at the begin- ning of each dialogue (e.g.  $20\\%$   questions  or    $70\\%$  questions ) rather than providing a control setting for each utterance (e.g.  is a question  or  isn’t   $a$  question ). With this approach, the sequence-to- sequence model is able to choose what value the controlled attribute should take for any particular utterance, but we are able to choose the overall dis- tribution. We ﬁnd that this approach works well – for example, the sequence-to-sequence model is generally good at detecting when to ask a ques- tion. In particular, this is easier than the alterna- tive: developing a separate process to decide, for each utterance, whether to ask a question. \n\nIn this section, we describe the two methods – which we call Conditional Training (CT) and Weighted Decoding (WD) – that we use to control attributes of the output    $y$   at the dialogue level. \n5.1 Conditional Training (CT) \nConditional Training ( Fan et al. ,  2018 ;  Kikuchi et al. ,  2016 ;  Peng et al. ,  2018 ) is a method to learn a sequence-to-sequence model    $P(y|x,z)$  , where    $z$   is a discrete  control variable . If the control attribute is naturally continuous (for ex- ample in our work, repetitiveness, speciﬁcity and response-relatedness), we use    $z$   to represent buck- eted ranges. For a binary attribute like question- asking,  $z$   represents an overall probability (as ex- plained in Section  5 ). \nTo train a CT model, we ﬁrst automatically an- notate every    $(x,y)$   pair in the training set with the attribute we wish to control (for example, whether  $y$   contains a question mark). During training, for each example we determine the correspond- ing    $z$   value (for continuous attributes, this sim- ply means sorting into the correct bucket; for question-asking, see Section  6.4 ). Next, the con- trol variable    $z$   is represented via an embedding (each of the possible values of    $z$   has its own em- bedding). For all our experiments, the embedding is of length 10; this was determined via hyperpa- rameter tuning. There are several possible ways to condition the sequence-to-sequence model on    $z$  – for example, append    $z$   to the end of the input sequence, or use    $z$   as the START symbol for the decoder. We ﬁnd it most effective to concatenate  $z$   to the decoder’s input on every step. 2   Lastly, the CT model learns to produce    $y\\ =\\ y_{1},\\dotsc,y_{T}$   by optimizing the cross-entropy loss: \n\n$$\n\\mathrm{loss_{CT}}=-\\frac{1}{T}\\sum_{t=1}^{T}\\log P(y_{t}|x,z,y_{1},.\\,.\\,.\\,,y_{t-1})\n$$\n \nOur CT models are initialized with the parameters from the baseline sequence-to-sequence model  $P(y|x)$   (the new decoder parameters are initial- ized with small random values), then ﬁne-tuned to optimize   $\\mathrm{loss}_{\\mathrm{CT}}$   on the PersonaChat training set, until convergence of loss CT  on the validation set. "}
{"page": 3, "image_path": "doc_images/N19-1170_3.jpg", "ocr_text": "5.2 Weighted Decoding (WD)\n\nWeighted Decoding (Ghazvininejad et al., 2017)\nis a decoding method that increases or decreases\nthe probability of words with certain features. The\ntechnique is applied only at test time, requiring no\nchange to the training method. A limitation of WD\nis that the controllable attribute must be defined\nat the word-level; any desired utterance-level at-\ntribute must be redefined via word-level features.\n\nIn weighted decoding, on the t’” step of decod-\ning, a partial hypothesis yo; = y1,.-., Yyp—1 iS ex-\npanded by computing the score for each possible\nnext word w in the vocabulary:\n\nscore(w, yer} ¥) = score(yer; 2)\n\ni\n\nHere, log Pann(w|y<t, x) is the log-probability of\nthe word w calculated by the RNN, score(y<¢; x)\nis the accumulated score of the already-generated\nwords in the hypothesis y<;, and f;(w; y<t, x)\nare decoding features with associated weights wj.\nThere can be multiple features f; (to control mul-\ntiple attributes), and the weights w; are hyperpa-\nrameters to be chosen.\n\nA decoding feature f;(w; yc, x) assigns a real\nvalue to the word w, in the context of the text gen-\nerated so far y<, and the context x. The feature\ncan be continuous (e.g. the unigram probability of\nw), discrete (e.g. the length of w in characters),\nor binary (e.g. whether w starts with the same\nletter as the last word in y<;). A positive weight\nw; increases the probability of words w that score\nhighly with respect to f;; a negative weight de-\ncreases their probability.\n\nNote that weighted decoding and conditional\ntraining can be applied simultaneously (i.e. train\na CT model then apply WD at test time) — a strat-\negy we use in our experiments.\n\n6 Controlling conversational attributes\n\nIn this section, we describe how we use condi-\ntional training and weighted decoding to control\nfour attributes: repetition, specificity, response-\nrelatedness and question-asking. We evaluate the\neffectiveness of both control methods via auto-\nmatic metrics (i.e., measuring how well the at-\ntribute was controlled), and use our findings to se-\nlect control methods and control settings to be ex-\nplored further via human evaluation (Section 8).\n\n6.1 Repetition\n\nOur baseline model exhibits three types of rep-\netition, which we call external repetition (self-\nrepetition across utterances), internal repetition\n(self-repetition within utterances), and partner\nrepetition (repeating the conversational partner).\nTo control repetition with weighted decod-\ning,> we define five n-gram based decoding\nfeatures (see Appendix D). Three of these\nfeatures (extrep_bigram,\npartnerrep_bigram) identify repeating bigrams\nfor the three repetition types. The other two\nfeatures (extrep-unigram and intrep_-unigram)\nidentify repeating content words. By applying a\nnegative weight to these features, we can reduce\nrepetition. In particular, if the weight is —oo, our\nmethod is equivalent to n-gram blocking as de-\nscribed by Kulikov et al. (2018). We observe that\nrepetition control is very important, thus all further\ncontrol experiments include repetition control.\n\nintrep_bigram and\n\n6.2 Specificity\n\nLike many sequence-to-sequence models using\nbeam search decoding, our baseline frequently\nasks generic questions such as What music do you\nlike? and gives dull, unspecific responses, such as\nI like all kinds of music.\n\nWe control specificity using Normalized Inverse\nDocument Frequency (NIDF) as a measure of\nword rareness.* The Inverse Document Frequency\nof a word w is IDF(w) = log(R/cw) where R\nis the number of responses in the dataset, and c,\nis the number of those responses that contain w.\nNormalized IDF (which ranges from 0 to 1) is\n\nIDF(w) — minidt\n\nmax-idf — min_idf\n\nNIDF(w) = (1)\nwhere min_idf and max_idf are the minimum and\nmaximum IDFs, taken over all words in the vo-\ncabulary. To control specificity with weighted de-\ncoding, we use NIDF as a decoding feature. As\nshown in Table 1, this method produces reason-\nable outputs when the feature weight is within a\ncertain range, but at the extremes the outputs are\n\n3We also tried controlling repetition with conditional\ntraining, defining z as the (bucketed) maximum ROUGE-L\nprecision between the response y and the bot’s previous ut-\nterances. However, this method was unsuccessful because\nthere are not enough repetitive examples in the training data\nfor the model to learn the control. Experimenting with data\naugmentation to solve this problem is an area for future work.\n\n‘Note that our NIDF specificity features are similar to the\nNIRF and NIWF features used by Zhang et al. (2018a).\n\n1705\n", "vlm_text": "5.2 Weighted Decoding (WD) \nWeighted Decoding ( Ghazvininejad et al. ,  2017 ) is a decoding method that increases or decreases the probability of words with certain features. The technique is applied only at test time, requiring no change to the training method. A limitation of WD is that the controllable attribute must be deﬁned at the word-level; any desired utterance-level at- tribute must be redeﬁned via word-level features. \nIn weighted decoding, on the    $t^{t h}$    step of decod- ing, a partial hypothesis    $y_{<t}=y_{1},.\\,.\\,.\\,,y_{t-1}$   is ex- panded by computing the score for each possible next word    $w$   in the vocabulary: \n\n$$\n\\begin{array}{l}{\\mathrm{score}(w,y_{<t};x)=\\mathrm{score}(y_{<t};x)}\\\\ {\\ +\\log P_{\\mathrm{RNN}}(w|y_{<t},x)+\\displaystyle\\sum_{i}w_{i}*f_{i}(w;y_{<t},x).}\\end{array}\n$$\n \nHere,  $\\log P_{\\mathrm{RNN}}(w|y_{<t},x)$   is the log-probability of the word    $w$   calculated by the RNN, score  $(y_{<t};x)$  is the accumulated score of the already-generated words in the hypothesis    $y_{<t}$  , and    $f_{i}(w;y_{<t},x)$  are  decoding features  with associated weights    $w_{i}$  . There can be multiple features    $f_{i}$   (to control mul- tiple attributes), and the weights    $w_{i}$   are hyperpa- rameters to be chosen. \nA decoding feature    $f_{i}(w;y_{<t},x)$   assigns a real value to the word  $w$  , in the context of the text gen- erated so far    $y_{<t}$   and the context    $x$  . The feature can be continuous (e.g. the unigram probability of  $w_{\\ast}$  ), discrete (e.g. the length of    $w$   in characters), or binary (e.g. whether    $w$   starts with the same letter as the last word in    $y_{<t.}$  ). A positive weight  $w_{i}$   increases the probability of words    $w$   that score highly with respect to    $f_{i}$  ; a negative weight de- creases their probability. \nNote that weighted decoding and conditional training can be applied simultaneously (i.e. train a CT model then apply WD at test time) – a strat- egy we use in our experiments. \n6 Controlling conversational attributes \nIn this section, we describe how we use condi- tional training and weighted decoding to control four attributes: repetition, speciﬁcity, response- relatedness and question-asking. We evaluate the effectiveness of both control methods via auto- matic metrics (i.e., measuring how well the at- tribute was controlled), and use our ﬁndings to se- lect control methods and control settings to be ex- plored further via human evaluation (Section  8 ). \n6.1 Repetition \nOur baseline model exhibits three types of rep- etition, which we call  external repetition  (self- repetition across utterances),  internal repetition (self-repetition within utterances), and  partner repetition  (repeating the conversational partner). \nTo control repetition with weighted decod- ing, we deﬁne ﬁve    $n$  -gram based decoding features (see Appendix  D ). Three of these features ( extrep bigram , intrep bigram  and partnerrep bigram ) identify repeating bigrams for the three repetition types. The other two features ( extrep unigram  and  intrep unigram ) identify repeating content words. By applying a negative weight to these features, we can reduce repetition. In particular, if the weight is    $-\\infty$  , our method is equivalent to    $n$  -gram blocking  as de- scribed by  Kulikov et al.  ( 2018 ). We observe that repetition control is very important, thus all further control experiments include repetition control. \n6.2Speciﬁcity\nLike many sequence-to-sequence models using beam search decoding, our baseline frequently asks generic questions such as  What music do you like?  and gives dull, unspeciﬁc responses, such as I like all kinds of music . \nWe control speciﬁcity using Normalized Inverse Document Frequency (NIDF) as a measure of word rareness.   The Inverse Document Frequency of a word    $w$   is   $\\mathrm{PDF}(w)\\;=\\;\\log(R/c_{w})$   where    $R$  is the number of responses in the dataset, and    $c_{w}$  is the number of those responses that contain    $w$  . Normalized IDF (which ranges from 0 to 1) is \n\n$$\n{\\mathrm{NDF}}(w)={\\frac{{\\mathrm{IDF}}(w)-{\\mathrm{min.idf}}}{{\\mathrm{max.idf-min.idf}}}}\n$$\n \nwhere  min idf  and  max idf  are the minimum and maximum IDFs, taken over all words in the vo- cabulary. To control speciﬁcity with weighted de- coding, we use NIDF as a decoding feature. As shown in Table  1 , this method produces reason- able outputs when the feature weight is within a certain range, but at the extremes the outputs are "}
{"page": 4, "image_path": "doc_images/N19-1170_4.jpg", "ocr_text": "Input: Yes, I’m studying law at the moment\nBaseline Response: That sounds like a lot of fun!\n\nInput: Do you go get coffee often\nBaseline Response: / do, when I am not playing the piano.\n\nWt NIDF Weighted Decoding Response\n\n-5.0 0.6% Oh.\n\n0.0 17.1% That sounds like a lot of fun!\n\n3.0 18.3% That sounds like a lot of fun. How\nlong have you been studying?\n\n7.0 38.5% I majored in practising my\nspiritual full time philosophy test\n\n10.0 71.9% |Ohwow! Merna jean isa paino yi\nhao hui bu acara sya gila [...]\n\nz NIDF Conditional Training Response\n\n0 16.8% Sounds like you are a great person!\n\n2 18.3% So you are a law student?\n\n4 18.4% — That sounds like a lot of fun\n\n6 22.8% That sounds like a rewarding job!\n\n8 24.4% — That sounds like a rewarding career!\n\nTable 1: Middle: Example of controlling specificity\n(NIDF) via weighted decoding. At the extremes, the\nmodel produces only the most rare or the most com-\nmon tokens. Bottom: Example of controlling speci-\nficity via conditional training. This gives a narrower\nNIDF range, but all the responses are appropriate.\n\nnonsensical. The boundary for nonsensical output\ndiffers from example to example.\n\nTo control specificity with conditional training,\nwe define the specificity of an utterance y to be\nthe mean NIDF of the words in y. Thus our con-\ntrol variable z is mean NIDF (discretized into 10\nequal-sized buckets). As shown in Table 1, this\nmethod gives outputs with a narrower NIDF range,\nbut overall produces less nonsensical outputs.\n\n6.3 Response-relatedness\n\nIn conversation, it’s generally desirable to produce\na response that is related to the partner’s last utter-\nance; for example if the partner says My grandfa-\nther died last month, it is appropriate to say I’m so\nsorry. Were you close to your grandfather? How-\never, our baseline model frequently responds with\nunrelated utterances like Do you have any pets?\nTo control response-relatedness with weighted\ndecoding, we use the decoding feature resp_rel:\n\nresp_rel(w; Yets x) =\n\ncos_sim(word_emb(w), sent_emb (é))\n\nwhere word_emb(w) is the GloVe embedding for\nthe word w, sent-emb(f) is the sentence embed-\nding for the partner’s last utterance @ (note @ is part\nof the context x), and cos_sim is the cosine simi-\nlarity between the two. In particular, the sentence\nembedding sent_emb(s) for an utterance s is a\n\nWt Sim Weighted Decoding Response\n\n-10.0 -0.05 Jama musician.\n0.0 -0.02 Ido, when 1am not playing the piano.\n5.0 0.40 Ido, usually at starbucks.\n8.0 0.59 Not usually, especially when you drink latte.\n\n11.0 0.72 Not often, usually with drinks,\nespresso, latte, tea, etc.\n\nTable 2: Example of controlling response-relatedness\n(cosine similarity to input) via weighted decoding. Pos-\nitive weights (e.g. 5.0) can yield more on-topic re-\nsponses, but higher weights (e.g. 11.0) can result in\nnonsensical lists of topically related words.\n\nweighted average of the GloVe embeddings of the\nwords in s, with the first principal component pro-\njected out; for full details, see Arora et al. (2017).\nThis method of controlling response-relatedness is\nsimilar to that described in (Baheti et al., 2018).\nWe find that weighted decoding is effective to con-\ntrol the semantic relatedness of the model’s re-\nsponse to the partner’s last utterance (see Table 2).\nAs before, we find that extreme weights lead to\nnonsensical output.\n\nTo control response-relatedness with condi-\ntional training, we try defining the control vari-\nable z to be cos_sim(sent_emb(y), sent_emb(@)),\nthe overall cosine similarity between the partner’s\nlast utterance ¢ and the model’s response y (again,\nwe discretize z). However, we find this method in-\neffective — the CT model learns only a very weak\nconnection between z and the semantic related-\nness of the output (see Section 7 for more details).\n\n6.4 Question-asking\n\nConsiderate chitchat requires a reciprocal asking\nand answering of questions — asking too few or too\nmany can appear self-centered or nosy. We control\nquestion-asking in order to study these trade-offs.\n\nTo control question-asking with weighted de-\ncoding, we use the binary decoding feature\nis-qn-word(w), which is equal to 1 if and only\nif the word w is in a pre-defined list of interrog-\native words (how, what, when, where, which, who,\nwhom, whose, why, ?). We find this is a somewhat\neffective method to encourage or discourage ques-\ntions, but with unintended side-effects: a negative\nweight can discourage valid non-question utter-\nances that happen to contain interrogative words\n(such as I’m learning how to knit) and a positive\nweight can result in degenerate utterances (such as\n\n1706\n", "vlm_text": "This table displays various decoding responses and their associated metrics based on an input statement. The input is: \"Yes, I'm studying law at the moment,\" and the baseline response is: \"That sounds like a lot of fun!\"\n\n1. **Weighted Decoding Response Section:**\n   - **Wt**: A weight applied to the model component for generating the response.\n   - **NIDF**: Normalized Inverse Document Frequency, a measure to potentially evaluate the uniqueness or informativeness of the response.\n   - **Weighted Decoding Response**: Different responses generated using varying weights.\n     - For Wt = -5.0, the response is \"Oh......................................\" with an NIDF of 0.6%\n     - For Wt = 0.0, the response is identical to the baseline response: \"That sounds like a lot of fun!\" with an NIDF of 17.1%.\n     - For Wt = 3.0, the response is \"That sounds like a lot of fun. How long have you been studying?\" with an NIDF of 18.3%.\n     - For Wt = 7.0, the response is \"I majored in practising my spiritual full-time philosophy test\" with an NIDF of 38.5%.\n     - For Wt = 10.0, the response is \"Oh wow! Merna jean isa paino yi hao hui bu acara sya gila [...]\" with an NIDF of 71.9%.\n\n2. **Conditional Training Response Section:**\n   - **z**: An associated variable or condition applied during training.\n   - **NIDF**: Similar to above, it measures the normalized inverse document frequency.\n   - **Conditional Training Response**: Responses generated based on different conditions:\n     - For z = 0, the response is \"Sounds like you are a great person!\" with an NIDF of 16.8%.\n     - For z = 2, the response is \"So you are a law student?\" with an NIDF of 18.3%.\n     - For z = 4, the response is identical to the baseline response: \"That sounds like a lot of fun\" with an NIDF of 18.4%.\n     - For z = 6, the response is \"That sounds like a rewarding job!\" with an NIDF of 22.8%.\n     - For z = 8, the response is \"That sounds like a rewarding career!\" with an NIDF of 24.4%.\n\nOverall, the table is examining how different weights and conditions affect the generation of responses, comparing baseline responses with those subjected to changes in parameters.\nTable 1: Middle: Example of controlling speciﬁcity (NIDF) via weighted decoding. At the extremes, the model produces only the most rare or the most com- mon tokens. Bottom: Example of controlling speci- ﬁcity via conditional training. This gives a narrower NIDF range, but all the responses are appropriate. \nnonsensical. The boundary for nonsensical output differs from example to example. \nTo control speciﬁcity with conditional training, we deﬁne the speciﬁcity of an utterance    $y$   to be the mean NIDF of the words in    $y$  . Thus our con- trol variable  $z$   is mean NIDF (discretized into 10 equal-sized buckets). As shown in Table  1 , this method gives outputs with a narrower NIDF range, but overall produces less nonsensical outputs. \n6.3 Response-relatedness \nIn conversation, it’s generally desirable to produce a response that is related to the partner’s last utter- ance; for example if the partner says  My grandfa- ther died last month , it is appropriate to say  I’m so sorry. Were you close to your grandfather?  How- ever, our baseline model frequently responds with unrelated utterances like  Do you have any pets? \nTo control response-relatedness with weighted decoding, we use the decoding feature  resp rel : \n\n$$\n\\begin{array}{r l}&{\\mathtt{r e s p\\_r e l}\\big(w;y_{<t},x\\big)=}\\\\ &{\\qquad\\qquad\\qquad\\mathtt{c o s\\_s i m\\big(w o r d\\_e m b\\big(w\\big),s e n t\\_e m b\\big(\\ell\\big)\\big)}}\\end{array}\n$$\n \nwhere  word emb  $(w)$   is the GloVe embedding for the word    $w$  ,  sent emb  $(\\ell)$   is the sentence embed- ding for the partner’s last utterance  $\\ell$  (note  $\\ell$  is part of the context    $x$  ), and  cos sim  is the cosine simi- larity between the two. In particular, the sentence embedding  sent emb  $(s)$   for an utterance    $s$   is a \nThe table presents responses generated by a weighted decoding process in response to the input question, \"Do you go get coffee often\". The table is structured as follows:\n\n- **Input:** The question posed is \"Do you go get coffee often\".\n- **Baseline Response:** The default response provided is \"I do, when I am not playing the piano.\"\n\nThe table consists of three columns:\n1. **Wt (Weight):** This numerical value represents different levels of influence or priority in generating a response.\n2. **Sim (Similarity):** This number indicates the similarity of the generated response to some baseline or desired phrase. \n3. **Weighted Decoding Response:** The actual responses generated under different weighting conditions.\n\nThe responses are listed with varying degrees of weight and similarity:\n- At a weight of -10.0 and similarity of -0.05: \"I am a musician.\"\n- At a weight of 0.0 and similarity of -0.02: \"I do, when I am not playing the piano.\"\n- At a weight of 5.0 and similarity of 0.40: \"I do, usually at starbucks.\"\n- At a weight of 8.0 and similarity of 0.59: \"Not usually, especially when you drink latte.\"\n- At a weight of 11.0 and similarity of 0.72: \"Not often, usually with drinks, espresso, latte, tea, etc.\"\n\nThe responses differ depending on the weight assigned, showing how the model's output can change with varying influences.\nweighted average of the GloVe embeddings of the words in  $s$  , with the ﬁrst principal component pro- jected out; for full details, see  Arora et al.  ( 2017 ). This method of controlling response-relatedness is similar to that described in ( Baheti et al. ,  2018 ). We ﬁnd that weighted decoding is effective to con- trol the semantic relatedness of the model’s re- sponse to the partner’s last utterance (see Table  2 ). As before, we ﬁnd that extreme weights lead to nonsensical output. \nTo control response-relatedness with condi- tional training, we try deﬁning the control vari- able  $z$   to be    $_{\\mathsf{C O S}_{-}\\mathsf{S i m}\\big(\\mathsf{s e n t}_{-}\\mathsf{e m b}\\big(y\\big),\\mathsf{s e n t}$   emb ( ℓ )) , the overall cosine similarity between the partner’s last utterance  $\\ell$  and the model’s response    $y$   (again, we discretize  $z$  ). However, we ﬁnd this method in- effective – the CT model learns only a very weak connection between    $z$   and the semantic related- ness of the output (see Section  7  for more details). \n6.4 Question-asking \nConsiderate chitchat requires a reciprocal asking and answering of questions – asking too few or too many can appear self-centered or nosy. We control question-asking in order to study these trade-offs. \nTo control question-asking with weighted de- coding, we use the binary decoding feature is qn word  $(w)$  , which is equal to 1 if and only if the word    $w$   is in a pre-deﬁned list of interrog- ative words ( how, what, when, where, which, who, whom, whose, why, ? ). We ﬁnd this is a somewhat effective method to encourage or discourage ques- tions, but with unintended side-effects: a negative weight can discourage valid non-question utter- ances that happen to contain interrogative words (such as  I’m learning  how  to knit ) and a positive weight can result in degenerate utterances (such as "}
{"page": 5, "image_path": "doc_images/N19-1170_5.jpg", "ocr_text": "100% -\n\n80% -\n\n60% -\n\n40% - +'=@= Question-controlled CT\n=@= Question-controlled CT w/ rep ctrl _\n+++ Target for question-controlled CT\n\nBeam search baseline\nRepetition-controlled baseline\nGold data\n\n20% -\n\nS\nis\nn\nGo\nG\n<\ng\n2\n5\nx\n\n0% -\n0 1 2 3 4 5 6 7 B 98 10 10 (boost)\nQuestion-Asking Control Level (CT)\nFigure 2: Controlling question-asking via conditional\ntraining. Exact numbers can be found in Appendix F.\n\nFor conditional training, we regard an utterance\ny as containing a question if and only if y con-\ntains a question mark. We train our CT model\non a control variable z with 11 possible values:\n{0,...,10}. As discussed in Section 5, we wish\nto control question-asking at the distributional, di-\nalogue level, rather than at the binary, utterance\nlevel. Thus the setting z = 7 means that the model\nshould produce, on average, utterances contain-\ning ‘?’ with probability 1/10. During training\nwe randomly assign examples to buckets such that\neach bucket 7 is trained on examples with the cor-\nrect proportion of questions (7/10), and all buckets\nhave the same amount of training examples.\n\nWe find that conditional training is effective to\ncontrol question-asking — as shown in Figure 2,\nby increasing z from 0 to 10, we obtain a range\nof question-asking rates from 1.40% to 97.72%.\nHowever, when we introduce repetition control,\nquestion-asking is reduced — in particular, the z =\n10 setting (which should produce 100% questions)\nnow only produces 79.67% questions. The pri-\nmary problem is the weighted decoding feature\nextrep_bigram, which discourages bigrams that\nhave appeared in previous utterances — this pre-\nvents the model from producing bigrams that com-\nmonly occur in many questions, such as do you\nand what is. To fix this, we introduce an extra\nsetting z = 10 (boost), in which we do not use\nthe feature ext rep_bigram for weighted decoding\nduring beam search, but we do use it to rerank the\ncandidates after beam search. This setting, which\nallows the model to produce necessary question-\nasking bigrams, yields a 99.54% question-asking\nrate, at the cost of slightly increased external bi-\ngram repetition (see Appendix F).\n\nFor controlling question-asking, conditional\ntraining is preferable to weighted decoding for two\nreasons. Firstly, it allows us to achieve (close to)\n0% questions, 100% questions, or anything in be-\ntween, without introducing the risk of degenerate\noutput. Secondly, presence-of-a-question-mark\ncaptures the true attribute of interest (question-\nasking) more exactly and directly than presence of\ninterrogative words. For these reasons, only the\nCT method is considered in the human evaluation.\n\n7 Comparison of control methods\n\nThe previous section shows that conditional train-\ning and weighted decoding are both useful tech-\nniques, with different strengths and weaknesses.\n\nThe primary disadvantage of conditional train-\ning is that it sometimes fails to learn the connec-\ntion between the control variable z and the target\noutput y. In practice, we find the model can learn\nsimple attributes of the output (such as the pres-\nence of ‘?’, and overall genericness), but not re-\nlationships between the input and output (such as\nsemantic relatedness). By contrast, weighted de-\ncoding can force the desired feature to appear in\nhe output by raising the weight arbitrarily high\n(though this may have unintended side-effects).\nThe primary disadvantage of weighted decod-\ning is that it risks going off-distribution when\nhe weight is too strong. By contrast, condi-\nional training produces mostly well-formed, in-\ndistribution outputs. This highlights the impor-\nance of learned control — it is safer to learn to\nproduce output that both satisfies the control vari-\nable and is appropriate, than to alter the decoding\nprocess to satisfy the control variable, potentially\ntrading off appropriateness in the process.\n\nOther considerations include: (1) Convenience:\nconditional training requires retraining; weighted\ndecoding doesn’t, but is slower at test time. (2)\nData availability: conditional training requires\ntraining examples of the controllable attribute,\nwhereas weighted decoding can control any com-\nputable feature without requiring examples. (3)\nAttribute definition: conditional training can con-\ntrol sentence-level attributes, but they must be dis-\ncrete. By contrast, weighted decoding requires\nword-level features, but they can be continuous.\n\n8 Human evaluation results\n\nIn order to study the effect of our controllable at-\ntributes, we conduct a large-scale human evalua-\n\n1707\n", "vlm_text": "The image is a line graph that demonstrates how different methods control the frequency of question-asking in generated text. The x-axis represents the \"Question-Asking Control Level (CT)\" or the boost applied, ranging from 0 to 10. The y-axis indicates the percentage of utterances containing a question mark ('?'), which implies the presence of questions.\n\nThere are several methods displayed in the graph:\n\n1. **Question-controlled CT** (blue line with circles): This line shows a steep increase in the percentage of questions as the control level increases, approaching near 100% at higher levels.\n\n2. **Question-controlled CT with repetition control** (purple line with squares): This line shows a more gradual increase in question percentage, reaching a lower maximum percentage near 80% as the control level increases.\n\n3. **Target for question-controlled CT** (red dotted line): This line acts as a target benchmark for the question-controlled CT, showing an expected linear increase.\n\n4. **Beam search baseline** (blue dotted line): A constant line showing the percentage of questions using a beam search method.\n\n5. **Repetition-controlled baseline** (purple dashed line): Another constant line, representing the baseline with some repetition control.\n\n6. **Gold data** (yellow line): This line is flat, representing some baseline or real-world data benchmark for comparison.\n\nOverall, the chart illustrates how the question frequency in text can be modulated using different controls or techniques, potentially providing a tool for adjusting question-asking behavior in language generation models.\nWhat???????  or  Who? When? How? ). \nFor conditional training, we regard an utterance  $y$   as containing a question if and only if    $y$   con- tains a question mark. We train our CT model on a control variable    $z$   with 11 possible values:  $\\{0,\\hdots,10\\}$  . As discussed in Section  5 , we wish to control question-asking at the distributional, di- alogue level, rather than at the binary, utterance level. Thus the setting    $z=i$   means that the model should produce, on average, utterances contain- ing ‘?’ with probability    $i/10$  . During training we randomly assign examples to buckets such that each bucket  $i$   is trained on examples with the cor- rect proportion of questions   $(i/10)$  , and all buckets have the same amount of training examples. \nWe ﬁnd that conditional training is effective to control question-asking – as shown in Figure  2 , by increasing    $z$   from 0 to 10, we obtain a range of question-asking rates from   $1.40\\%$   to   $97.72\\%$  . However, when we introduce repetition control, question-asking is reduced – in particular, the    $z=$  10  setting (which should produce   $100\\%$   questions) now only produces   $79.67\\%$   questions. The pri- mary problem is the weighted decoding feature extrep bigram , which discourages bigrams that have appeared in previous utterances – this pre- vents the model from producing bigrams that com- monly occur in many questions, such as  do you and  what is . To ﬁx this, we introduce an extra setting  $z\\ =\\ 10\\ (b o o s t).$  , in which we do not use the feature  extrep bigram  for weighted decoding during beam search, but we do use it to rerank the candidates after beam search. This setting, which allows the model to produce necessary question- asking bigrams, yields a  $99.54\\%$   question-asking rate, at the cost of slightly increased external bi- gram repetition (see Appendix  F ). \nFor controlling question-asking, conditional training is preferable to weighted decoding for two reasons. Firstly, it allows us to achieve (close to)  $0\\%$   questions,   $100\\%$   questions, or anything in be- tween, without introducing the risk of degenerate output. Secondly, presence-of-a-question-mark captures the true attribute of interest (question- asking) more exactly and directly than presence of interrogative words. For these reasons, only the CT method is considered in the human evaluation. \n7 Comparison of control methods \nThe previous section shows that conditional train- ing and weighted decoding are both useful tech- niques, with different strengths and weaknesses. \nThe primary disadvantage of conditional train- ing is that it sometimes fails to learn the connec- tion between the control variable    $z$   and the target output    $y$  . In practice, we ﬁnd the model can learn simple attributes of the output (such as the pres- ence of ‘?’, and overall genericness), but not re- lationships between the input and output (such as semantic relatedness). By contrast, weighted de- coding can force the desired feature to appear in the output by raising the weight arbitrarily high (though this may have unintended side-effects). \nThe primary disadvantage of weighted decod- ing is that it risks going off-distribution when the weight is too strong. By contrast, condi- tional training produces mostly well-formed, in- distribution outputs. This highlights the impor- tance of learned control – it is safer to learn to produce output that both satisﬁes the control vari- able and is appropriate, than to alter the decoding process to satisfy the control variable, potentially trading off appropriateness in the process. \nOther considerations include: (1) Convenience: conditional training requires retraining; weighted decoding doesn’t, but is slower at test time. (2) Data availability: conditional training requires training examples of the controllable attribute, whereas weighted decoding can control any com- putable feature without requiring examples. (3) Attribute deﬁnition: conditional training can con- trol sentence-level attributes, but they must be dis- crete. By contrast, weighted decoding requires word-level features, but they can be continuous. \n8 Human evaluation results \nIn order to study the effect of our controllable at- tributes, we conduct a large-scale human evalua- tion of 28 model conﬁgurations (see Appendix  E ), plus human-human conversations for comparison. "}
{"page": 6, "image_path": "doc_images/N19-1170_6.jpg", "ocr_text": "tion of 28 model configurations (see Appendix E),\nplus human-human conversations for comparison.\n\nApproach In our evaluation, a crowdworker\nchats with a model (or in the human-human\ncase, another crowdworker) for six conversational\nturns, then answers eight multiple-choice ques-\ntions which each capture different aspects of con-\nversational quality: avoiding repetition, interest-\ningness, making sense, fluency, listening, inquisi-\ntiveness, humanness and engagingness. The eight\nquestions are Likert questions on a 1-4 scale,\nwhere higher is better.” To match the ConvAI2\nChallenge, we also add a persona retrieval ques-\ntion, in which the crowdworker is asked to select\nwhich of two possible personas was the model’s\npersona. For full details of the evaluation design,\nsee Appendix B.\n\nOur evaluation is the same as the ConvAI2\nChallenge evaluation, but more detailed — Con-\nvAI2 includes only engagingness and persona re-\ntrieval.© As in the ConvAI2 challenge, each of\nour 28 model configurations was evaluated by over\n100 crowdworkers, and the results were adjusted\nfor annotator variance via a Bayesian calibration\n(Kulikov et al., 2018).\n\nIn designing our evaluation, we aimed to cap-\nture the four aspects we expected to directly im-\nprove via control (avoiding repetition, interesting-\nness, listening, inquisitiveness), two important er-\nror classes we thought would be affected by our\ncontrols (fluency, making sense), and two overall\nquality measures (engagingness, humanness).\n\n8.1 Main findings\n\nIn this section we summarize the main findings of\nour human evaluation — whose full results can be\nfound in Appendices G and H, with sample con-\nversations in Appendix C.\n\nAs Figure 3 shows, controlling for repetition,\nspecificity and question-asking all lead to large\n\n5Exceptions: Avoiding repetition is a 1-3 scale, as we\nfound this gave clearer instructions. Inquisitiveness has an\noptimal score of 3; 1 and 2 represent too little question-\nasking, and 4 represents too much.\n\nThere are three other minor differences between our\nevaluation and ConvAI2’s: (1) We fix capitalization and spac-\ning before showing the chatbot’s utterances to crowdwork-\ners, while ConvAI2 show the raw lowercase tokenized form.\nWe found the latter interferes with fluency evaluation. (2)\nWe conduct 6 dialogue turns, while ConvAI2 conducts 4-6.\nThis was necessary to evaluate repetitiveness. (3) We use\n(publicly-available) validation set personas, while ConvAI2\nuses (hidden) test set personas. This enables us to release our\nevaluation chatlogs.\n\nengagingness improvements over the greedy and\nbeam-search baseline models. In particular, we\nfind that controlling for multi-turn (self) repetition\nis important and should be incorporated alongside\nother attribute control methods. We found no im-\nprovement by controlling response-relatedness.\n\nTo better understand these overall engagingness\nimprovements, we consider the full set of human\njudgments, shown in Figure 4. We find that re-\nducing repetition leads to improvements across all\nour aspects of conversational quality. Increasing\nspecificity shows improvements in interestingness\nand listening ability over the repetition-controlled\nbaseline, while increasing question-asking shows\nimprovements in inquisitiveness and interesting-\nness over the repetition-controlled baseline.\n\nOur most engaging model, which controls both\nrepetition and question-asking — marked ‘Ques-\ntion (CT)’ in Figure 3 (left) — matches the en-\ngagingness of the winning entry in the ConvAI2\ncompetition, as both models achieve a raw score’\nof 3.1 (Dinan et al., 2019). However, the Con-\nvAI2 winner, Lost in Conversation, was trained\non approximately 12 as much data as our model.\nLost in Conversation is based on the OpenAI GPT\nLanguage Model (Radford et al., 2018), which is\npretrained on the BookCorpus (Zhu et al., 2015),\nwhich contains approximately 985 million words,\nwhereas our model is pretrained on the Twitter\ndataset (approximately 79 million words).\n\nAltogether, our evaluation clearly shows that\ncontrolling low-level attributes over multiple turns\nleads to improved overall quality.\n\n8.2 Effect of controlled attributes\n\nRepetition (WD) We that  self-\nrepetition across utterances (external repetition)\nis by far the most severe form of repetition in\nour beam search baseline model. We evaluate\nseveral settings of the extrepbigram weighted\ndecoding feature, and find that an aggressive\nrepetition-reduction setting (reducing bigram\nrepetition rate to below gold data levels) is rated\nbest. We also find that blocking repeated content\nwords improves the avoiding repetition score. See\nAppendices E, F and G for full details.\n\nAs shown in Figure 3 (left) and Figure 4,\nour repetition-controlled model improves hugely\n\nobserve\n\n7 Although the same Bayesian calibration method was ap-\nplied both in our study and in the ConvAI2 competition, cal-\nibrated scores are not comparable across the two; thus we\ncompare raw scores (viewable in Table 7).\n\n1708\n", "vlm_text": "\nApproach In our evaluation, a crowdworker chats with a model (or in the human-human case, another crowdworker) for six conversational turns, then answers eight multiple-choice ques- tions which each capture different aspects of con- versational quality: avoiding repetition, interest- ingness, making sense, ﬂuency, listening, inquisi- tiveness, humanness and engagingness. The eight questions are Likert questions on a 1-4 scale, where higher is better.   To match the ConvAI2 Challenge, we also add a persona retrieval ques- tion, in which the crowdworker is asked to select which of two possible personas was the model’s persona. For full details of the evaluation design, see Appendix  B . \nOur evaluation is the same as the ConvAI2 Challenge evaluation, but more detailed – Con- vAI2 includes only engagingness and persona re- trieval. As in the ConvAI2 challenge, each of our 28 model conﬁgurations was evaluated by over 100 crowdworkers, and the results were adjusted for annotator variance via a Bayesian calibration ( Kulikov et al. ,  2018 ). \nIn designing our evaluation, we aimed to cap- ture the four aspects we expected to directly im- prove via control (avoiding repetition, interesting- ness, listening, inquisitiveness), two important er- ror classes we thought would be affected by our controls (ﬂuency, making sense), and two overall quality measures (engagingness, humanness). \n8.1 Main ﬁndings \nIn this section we summarize the main ﬁndings of our human evaluation – whose full results can be found in Appendices  G  and  H , with sample con- versations in Appendix  C . \nAs Figure  3  shows, controlling for repetition, speciﬁcity and question-asking all lead to large engagingness improvements over the greedy and beam-search baseline models. In particular, we ﬁnd that controlling for multi-turn (self) repetition is important and should be incorporated alongside other attribute control methods. We found no im- provement by controlling response-relatedness. \n\nTo better understand these overall engagingness improvements, we consider the full set of human judgments, shown in Figure  4 . We ﬁnd that re- ducing repetition leads to improvements across all our aspects of conversational quality. Increasing speciﬁcity shows improvements in interestingness and listening ability over the repetition-controlled baseline, while increasing question-asking shows improvements in inquisitiveness and interesting- ness over the repetition-controlled baseline. \nOur most engaging model, which controls both repetition and question-asking – marked ‘Ques- tion (CT)’ in Figure  3  (left) – matches the en- gagingness of the winning entry in the ConvAI2 competition, as both models achieve a raw score 7 of  3 . 1  ( Dinan et al. ,  2019 ). However, the Con- vAI2 winner, Lost in Conversation, was trained on approximately   $12\\times$   as much data as our model. Lost in Conversation is based on the OpenAI GPT Language Model ( Radford et al. ,  2018 ), which is pretrained on the BookCorpus ( Zhu et al. ,  2015 ), which contains approximately 985 million words, whereas our model is pretrained on the Twitter dataset (approximately 79 million words). \nAltogether, our evaluation clearly shows that controlling low-level attributes over multiple turns leads to improved overall quality. \n8.2 Effect of controlled attributes \nRepetition (WD) We observe that self- repetition across utterances ( external repetition ) is by far the most severe form of repetition in our beam search baseline model. We evaluate several settings of the  extrep bigram  weighted decoding feature, and ﬁnd that an aggressive repetition-reduction setting (reducing bigram repetition rate to below gold data levels) is rated best. We also ﬁnd that blocking repeated content words improves the avoiding repetition score. See Appendices  E ,  F  and  G  for full details. \nAs shown in Figure  3  (left) and Figure  4 , our repetition-controlled model improves hugely "}
{"page": 7, "image_path": "doc_images/N19-1170_7.jpg", "ocr_text": "Engagingness\n\nalll\n\noe\not « Bo\n.\nFe\n\n6\n\nEngagingness\n\n= Human\n== Repetition-controlled baseline\n\n22-\n\n64\n\nws”\nen\nee\n\ne Speciticty-controlled wo\n+» Beam search baseline\n\nSpecificity Control Level (WD)\n\n—e— Question-controlied CT\n\n+» Beam search baseline\nHuman\nRepetition-controlled baseline\n\nEngagingn\n\n45 6\n\n\" Question-Asking Control Level (CT)\n\nFigure 3: Calibrated human judgments of engagingness for the baselines and best controlled models (left); for\ndifferent specificity control settings (middle); and for different question-asking control settings (right).\n\nAvoiding Repetition\n3.00 -\n\nInterestingness Fi\n\n{|\n\nHE Beam search\n\nMaking Sense\n\n3.8 36-\n\n[MN Greedy search\n\n[HBB Repetition-controlled (WD)\n\nHumanness\n\n7\n1\nil\n\n(MM Human\n\nluency Listening\n\nll\n\nHEB avestion-controlied (CT)\n\nInquisitiveness\n\n2.75 -\n3.25 -\n\n3.00\n2.75 -\n\n2.50 -\n\neum 1.75 -\n\nSpecificity-controlled (WD)\n\nFigure 4: Calibrated human judgments of conversational aspects for the baselines and best controlled models.\nNote: In Figure 3 and here, the Specificity and Question controlled models both include Repetition control, but\nQuestion control doesn’t include Specificity control, or vice versa.\n\nover the beam search baseline in all metrics, and\nachieves close-to-human scores on all metrics ex-\ncept humanness. This striking result demonstrates\nthat repetition is by far the biggest limiting qual-\nity factor for naive sequence-to-sequence dialogue\nagents. The result also emphasizes the importance\nof multi-turn dialogue evaluation to detect the\nproblem. We refer to this model as the repetition-\ncontrolled baseline, and use it as a basis for all re-\nmaining experiments (i.e., we control specificity,\nresponse-relatedness and question-asking on top\nof these repetition-control settings).\n\nSpecificity (WD, CT) For our weighted decod-\ning models, the extreme settings (very generic and\nvery specific) score poorly in engagingness due to\nthe frequent presence of degenerate output — see\nFigure 3 (middle). We find that the weight = 4\nsetting (which is more specific than the repetition-\ncontrolled baseline and about as specific as the\ngold data) maximizes engagingness. As shown\nin Figure 3 (left) and Figure 4, this more-specific\nmodel is rated more interesting, engaging, and a\nbetter listener than the repetition-controlled base-\nline, but at the cost of reduced fluency and making\nsense. Our CT model with z = 7 (which has a\nsimilar NIDF level as WD with weight = 4) shows\nsimilar results, but the improvements are smaller.\nFor further discussion on the interestingness of our\nspecificity models, see Section 8.3.\n\nResponse-relatedness (WD) We evaluated sev-\neral control settings (weight = —10, 5, 10, 13) and\nfound that none scored better than weight = 0\n(no response-relatedness control); see Appendix\nH. This is surprising — prior to running the human\nevaluation, we annotated 100 examples ourselves\nto determine the best control settings. While we\nidentified a more responsive setting (weight = 5)\nas less likely than the uncontrolled model to ig-\nnore the user, crowdworkers rated it as a slightly\nworse listener than the uncontrolled model. One\nexplanation for this discrepancy is that the more\nresponsive model takes more risks, using more\nrare words (0.197 NIDF, up from 0.178), and thus\nreceives a lower makes-sense score (3.41, down\nfrom 3.70). We hypothesize that, compared to us,\nthe crowdworkers are less tolerant of slightly non-\nsensical output, and more tolerant of generic unre-\nlated utterances.\n\nQuestion-asking (CT) As shown in Figure 3\n(tight), a question-asking rate of 65.7% (z = 7)\nmaximizes engagingness. This setting, which asks\nmore questions than both the repetition-controlled\nbaseline (50.0%) and the human-produced gold\ndata (28.8%), brings us closest to human-level en-\ngagingness — see Figure 3 (left). Although we\nfind that a rate of approximately 65.7% question-\nasking is the most engaging, a lower level (48.9%,\nor z = 4) is rated the best listener. Lastly, we find\n\n1709\n", "vlm_text": "The image consists of three subplots that present data on the engagingness of conversations as judged by humans. These plots aim to compare and analyze the effects of different conversational model settings on perceived engagingness.\n\n1. **Left Plot**: This bar chart shows the average engagingness ratings for several baseline models and controlled models. The models are listed as: Greedy, Beam Search, Repetition (WD), Specificity (WD), Question (CT), and Human. Human interactions are rated highest in engagingness, with other models trailing behind. Error bars indicate the level of uncertainty or variability in the ratings.\n\n2. **Middle Plot**: This line graph shows the engagingness rating trend for different specificity control levels (denoted as WD). The plot displays how changes in specificity (ranging from more generic to more specific) impact engagingness. An optimal level of specificity control appears to yield the highest engagingness rating, with human and baseline ratings marked for reference.\n\n3. **Right Plot**: This line graph depicts the impact of question-asking control levels (denoted as CT) on engagingness. It examines how varying the number of questions asked (from fewer to more questions) affects engagingness ratings. The plot suggests there is an optimal level of question-asking that aligns with higher engagingness ratings. Human, beam search baseline, and repetition-controlled baseline ratings are also plotted for comparison.\n\nOverall, the image evaluates how different control settings for specificity and question-asking influence the engagingness of dialogues in comparison to human-level engagingness and other baseline models.\nThe image is a bar graph that displays calibrated human judgments of various conversational aspects for different baseline and controlled models. The conversational aspects being evaluated are:\n\n1. Avoiding Repetition\n2. Interestingness\n3. Making Sense\n4. Fluency\n5. Listening\n6. Inquisitiveness\n7. Humanness\n\nThe models compared in the graph are Greedy search, Beam search, Repetition-controlled (WD), Specificity-controlled (WD), Question-controlled (CT), and Human. Each aspect has a separate bar for each model, with colors corresponding to the legend at the bottom, detailing which color represents which model. The human model is indicated by orange bars and tends to have higher scores in many aspects, especially in Humanness. The graph appears to compare the effectiveness of these models in generating conversational responses based on these aspects.\nover the beam search baseline in all metrics, and achieves close-to-human scores on all metrics ex- cept humanness. This striking result demonstrates that repetition is by far the biggest limiting qual- ity factor for naive sequence-to-sequence dialogue agents. The result also emphasizes the importance of  multi-turn  dialogue evaluation to detect the problem. We refer to this model as the  repetition- controlled baseline , and use it as a basis for all re- maining experiments (i.e., we control speciﬁcity, response-relatedness and question-asking on top of these repetition-control settings). \nSpeciﬁcity (WD, CT) For our weighted decod- ing models, the extreme settings (very generic and very speciﬁc) score poorly in engagingness due to the frequent presence of degenerate output – see Figure  3  (middle). We ﬁnd that the weight  $\\c=4$  setting (which is more speciﬁc than the repetition- controlled baseline and about as speciﬁc as the gold data) maximizes engagingness. As shown in Figure  3  (left) and Figure  4 , this more-speciﬁc model is rated more interesting, engaging, and a better listener than the repetition-controlled base- line, but at the cost of reduced ﬂuency and making sense. Our CT model with    $z\\:=\\:7$   (which has a similar NIDF level as WD with weight  $=4$  ) shows similar results, but the improvements are smaller. For further discussion on the interestingness of our speciﬁcity models, see Section  8.3 . \nResponse-relatedness (WD) We evaluated sev- eral control settings   $(\\mathrm{weight}=-10,5,10,13)$  found that none scored better than weight  = 0 (no response-relatedness control); see Appendix H . This is surprising – prior to running the human evaluation, we annotated 100 examples ourselves to determine the best control settings. While we identiﬁed a more responsive setting (weight  $=5$  ) as less likely than the uncontrolled model to ig- nore the user, crowdworkers rated it as a slightly worse  listener than the uncontrolled model. One explanation for this discrepancy is that the more responsive model takes more risks, using more rare words (0.197 NIDF, up from 0.178), and thus receives a lower makes-sense score (3.41, down from 3.70). We hypothesize that, compared to us, the crowdworkers are less tolerant of slightly non- sensical output, and more tolerant of generic unre- lated utterances. \nQuestion-asking (CT) As shown in Figure  3 (right), a question-asking rate of   $65.7\\%$     $(z\\,=\\,7)$  maximizes engagingness. This setting, which asks more questions than both the repetition-controlled baseline   $(50.0\\%)$   and the human-produced gold data   $(28.8\\%)$  , brings us closest to human-level en- gagingness – see Figure  3  (left). Although we ﬁnd that a rate of approximately   $65.7\\%$   question- asking is the most engaging, a lower level   $(48.9\\%$  , or    $z=4$  ) is rated the best listener. Lastly, we ﬁnd "}
{"page": 8, "image_path": "doc_images/N19-1170_8.jpg", "ocr_text": "Model Win% Top 3 reasons for preferring model\n\nSpecificity WD (weight = 6) 84.1% = More information; Better flow; More descriptive\n\nSpecificity WD (weight = 4) 75.5% More information; They describe their life in more detail; Funny\nSpecificity CT (z = 7) 56.2% More information; Better flow; Seems more interested\n\nTable 3: A/B tests comparing various specificity-controlled models to the repetition-controlled baseline on inter-\nestingness. We find all comparisons are significant (p < .05; binomial test).\n\nthat although asking too many questions is less en-\ngaging, most crowdworkers will not directly criti-\ncize a chatbot that asks questions on every turn —\nonly 11.9% of crowdworkers judged the z = 10\n(boost) setting, which asks 99.5% questions, as\nasking too many questions.® For full details of\nthese scores, see Appendix F and H.\n\nFor time and budget reasons, we did not eval-\nuate any models controlling both question-asking\nand specificity. However, we expect it is possible\nto obtain further improvements by doing so.\n\n8.3 A/B tests for interestingness\n\nThough our more-specific models yielded signifi-\ncant improvements in engagingness, we were sur-\nprised that they did not yield clearer improve-\nments in interestingness. To investigate further,\nwe conducted an A/B interestingness evaluation of\nthree specificity-controlled models, compared to\nthe repetition-controlled baseline. Crowdworkers\nwere shown two conversations (from the main hu-\nman evaluation) and asked to choose which model\nwas more interesting (see Figure 7 for details). We\ncollected 500 samples per comparison, plus 200\nadditional human vs repetition-controlled baseline\nsamples, which were used to filter for quality con-\ntrol. After discarding low-quality crowdworkers,\nwe have roughly 300 evaluations per comparison,\nwith an average Cohen’s « = 0.6.\n\nAs shown in Table 3, all three models were rated\nsignificantly more interesting than the repetition-\ncontrolled baseline. This convincingly shows that\nproducing utterances with more rare words is a\nvalid strategy to improve interestingness. We have\ntwo explanations for why these interestingness dif-\nferences did not materialize in our main evalua-\ntion. Firstly, interestingness is a particularly sub-\njective metric (unlike more tangible metrics such\nas avoiding repetition and making sense) — this\nmakes it hard to calibrate across crowdworkers.\n\n’Though this conclusion may hold true for the Per-\nsonaChat task — a synthetic chatting task that instructs par-\nticipants to get to know each other — in real-life social con-\nversations, incessant question-asking may be less tolerated.\n\nSecondly, we suspect that in our original evalu-\nation, the crowdworkers may have evaluated the\ninterestingness of the task rather than the chat-\nbot. This could account for why subtle increases\nin conversational ability did not result in higher in-\nterestingness ratings — the PersonaChat task itself\nhas a natural interestingness limit.\n\n9 Conclusion\n\nWhat makes a good conversation? Through\nour evaluation, we showed that a good conversa-\nion is about balance — controlling for the right\nlevel of repetition, specificity and question-asking\nis important for overall quality. We also found\nhat conversational aspects such as interestingness,\nlistening, and inquisitiveness are all important —\nhough optimizing these can introduce a trade-off\nagainst certain types of errors (such as repetitive,\ndisfluent, or nonsensical output). Secondly, multi-\nurn evaluation is essential to study what makes a\ngood conversation — multiple turns are required to\nreveal issues such as repetition, consistency, and\nquestion-asking frequency. Lastly, what do we\nmean by ‘good’? Although humanness and engag-\ningness are both commonly used as overall qual-\nity metrics, the two are very different. While our\nmodels achieved close-to-human scores on engag-\ningness, they failed to get close on humanness —\nshowing that a chatbot need not be human-like\nto be enjoyable. This striking result also demon-\nstrates the importance of measuring more than one\nquality metric when evaluating dialogue agents.\n\nOutlook Our work shows that neural generative\nsystems have systemic problems when applied to\nopen-ended dialogue, some of which (e.g. repe-\ntition) are only observable in the multi-turn set-\nting. Furthermore, control of low-level attributes\noffers a practical way to correct these problems,\nyielding large improvements to overall quality — in\nour case, comparable to systems trained on much\nmore data. Future work includes optimizing con-\ntrol settings automatically, and building more con-\nvincingly human-like chatbots.\n\n1710\n", "vlm_text": "The table presents a comparison of three different models in terms of their win percentages and the top three reasons for preferring each model. Here's the information contained in the table:\n\n- **Model**: \n  - Specificity WD (weight = 6)\n  - Specificity WD (weight = 4)\n  - Specificity CT (z = 7)\n\n- **Win%**:\n  - Specificity WD (weight = 6): 84.1%\n  - Specificity WD (weight = 4): 75.5%\n  - Specificity CT (z = 7): 56.2%\n\n- **Top 3 reasons for preferring model**:\n  - Specificity WD (weight = 6): \n    - More information\n    - Better flow\n    - More descriptive\n\n  - Specificity WD (weight = 4): \n    - More information\n    - They describe their life in more detail\n    - Funny\n\n  - Specificity CT (z = 7): \n    - More information\n    - Better flow\n    - Seems more interested\n\nThe \"Win%\" indicates the percentage of times the model was preferred. The reasons provided are the primary factors users cited for liking each model.\nthat although asking too many questions is less en- gaging, most crowdworkers will not directly criti- cize a chatbot that asks questions on every turn – only   $11.9\\%$   of crowdworkers judged the    $z\\,=\\,10$  (boost)  setting, which asks   $99.5\\%$   questions, as asking too many questions. For full details of these scores, see Appendix  F  and  H . \nFor time and budget reasons, we did not eval- uate any models controlling both question-asking and speciﬁcity. However, we expect it is possible to obtain further improvements by doing so. \n8.3 A/B tests for interestingness \nThough our more-speciﬁc models yielded signiﬁ- cant improvements in engagingness, we were sur- prised that they did not yield clearer improve- ments in interestingness. To investigate further, we conducted an A/B interestingness evaluation of three speciﬁcity-controlled models, compared to the repetition-controlled baseline. Crowdworkers were shown two conversations (from the main hu- man evaluation) and asked to choose which model was more interesting (see Figure  7  for details). We collected 500 samples per comparison, plus 200 additional human vs repetition-controlled baseline samples, which were used to ﬁlter for quality con- trol. After discarding low-quality crowdworkers, we have roughly 300 evaluations per comparison, with an average Cohen’s  $\\kappa=0.6$  . \nAs shown in Table  3 , all three models were rated signiﬁcantly more interesting than the repetition- controlled baseline. This convincingly shows that producing utterances with more rare words is a valid strategy to improve interestingness. We have two explanations for why these interestingness dif- ferences did not materialize in our main evalua- tion. Firstly, interestingness is a particularly sub- jective metric (unlike more tangible metrics such as avoiding repetition and making sense) – this makes it hard to calibrate across crowdworkers. \nSecondly, we suspect that in our original evalu- ation, the crowdworkers may have evaluated the interestingness of the  task  rather than the  chat- bot . This could account for why subtle increases in conversational ability did not result in higher in- terestingness ratings – the PersonaChat task itself has a natural interestingness limit. \n9 Conclusion \nWhat makes a good conversation? Through our evaluation, we showed that a good conversa- tion is about balance – controlling for the right level of repetition, speciﬁcity and question-asking is important for overall quality. We also found that conversational aspects such as interestingness, listening, and inquisitiveness are all important – though optimizing these can introduce a trade-off against certain types of errors (such as repetitive, disﬂuent, or nonsensical output). Secondly, multi- turn evaluation is essential to study what makes a good conversation – multiple turns are required to reveal issues such as repetition, consistency, and question-asking frequency. Lastly, what do we mean by ‘good’? Although humanness and engag- ingness are both commonly used as overall qual- ity metrics, the two are very different. While our models achieved close-to-human scores on engag- ingness, they failed to get close on humanness – showing that a chatbot need not be human-like to be enjoyable. This striking result also demon- strates the importance of measuring more than one quality metric when evaluating dialogue agents. \nOutlook Our work shows that neural generative systems have systemic problems when applied to open-ended dialogue, some of which (e.g. repe- tition) are only observable in the multi-turn set- ting. Furthermore, control of low-level attributes offers a practical way to correct these problems, yielding large improvements to overall quality – in our case, comparable to systems trained on much more data. Future work includes optimizing con- trol settings automatically, and building more con- vincingly human-like chatbots. "}
{"page": 9, "image_path": "doc_images/N19-1170_9.jpg", "ocr_text": "References\n\nSanjeev Arora, Yingyu Liang, and Tengyu Ma.\n2017. A simple but tough-to-beat baseline for\nsentence embeddings. In Proceedings of the In-\nternational Conference on Learning Represen-\ntations (ICLR).\n\nAshutosh Baheti, Alan Ritter, Jiwei Li, and Bill\nDolan. 2018. Generating more interesting re-\nsponses in neural conversation models with dis-\ntributional constraints. In Proceedings of the\n2018 Conference on Empirical Methods in Nat-\nural Language Processing, pages 3970-3980.\nAssociation for Computational Linguistics.\n\nAntoine Bordes, Y-Lan Boureau, and Jason We-\nston. 2017. Learning end-to-end goal-oriented\ndialog. In Proceedings of the International Con-\nference on Learning Representations (ICLR).\n\nEmily Dinan, Varvara Logacheva, Valentin Ma-\nlykh, Alexander Miller, Kurt Shuster, Jack Ur-\nbanek, Douwe Kiela, Arthur Szlam, Iulian Ser-\nban, Ryan Lowe, et al. 2019. The second\nconversational intelligence challenge (convai2).\narXiv preprint arXiv: 1902.00098.\n\nEmily Dinan, Stephen Roller, Kurt Shuster, An-\ngela Fan, Michael Auli, and Jason Weston.\n2018. Wizard of Wikipedia: Knowledge-\npowered conversational agents. arXiv preprint\narXiv: 1811.01241.\n\nLayla El Asri, Hannes Schulz, Shikhar Sharma,\nJeremie Zumer, Justin Harris, Emery Fine,\nRahul Mehrotra, and Kaheer Suleman. 2017.\nFrames: a corpus for adding memory to\ngoal-oriented dialogue systems. In Proceed-\nings of the 18th Annual SIGDIAL Meeting\non Discourse and Dialogue, pages 207-219,\nSaarbriicken, Germany. Association for Com-\nputational Linguistics.\n\nAngela Fan, David Grangier, and Michael Auli.\n2018. Controllable abstractive summarization.\nIn Proceedings of the 2nd Workshop on Neu-\nral Machine Translation and Generation, pages\n45-54. Association for Computational Linguis-\ntics.\n\nJessica Ficler and Yoav Goldberg. 2017. Control-\nling linguistic style aspects in neural language\ngeneration. In Proceedings of the Workshop on\nStylistic Variation, pages 94-104. Association\nfor Computational Linguistics.\n\nMarjan Ghazvininejad, Xing Shi, Jay Priyadarshi,\nand Kevin Knight. 2017. Hafez: an interactive\npoetry generation system. In Proceedings of\nACL 2017, System Demonstrations, pages 43-\n48. Association for Computational Linguistics.\n\nFenfei Guo, Angeliki Metallinou, Chandra Kha-\ntri, Anirudh Raju, Anu Venkatesh, and Ashwin\nRam. 2018. Topic-based evaluation for conver-\nsational bots. Advances in Neural Information\nProcessing Systems, Conversational AI Work-\nshop.\n\nHelen Hastie. 2012.\nof spoken dialogue systems, pages 131-150.\nSpringer.\n\nMetrics and evaluation\n\nMatthew Henderson, Blaise Thomson, and Ja-\nson D Williams. 2014. The second dialog state\ntracking challenge. In Proceedings of the 15th\nAnnual Meeting of the Special Interest Group\non Discourse and Dialogue (SIGDIAL), pages\n263-272.\n\nZhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan\nSalakhutdinov, and Eric P Xing. 2017. Toward\ncontrolled generation of text. In Thirty-fourth\nInternational Conference on Machine Learn-\ning.\n\nYuta Kikuchi, Graham Neubig, Ryohei Sasano,\nHiroya Takamura, and Manabu Okumura. 2016.\nControlling output length in neural encoder-\ndecoders. In Proceedings of the 2016 Con-\nference on Empirical Methods in Natural Lan-\nguage Processing, pages 1328-1338. Associa-\ntion for Computational Linguistics.\n\nIlya Kulikov, Alexander H Miller, Kyunghyun\nCho, and Jason Weston. 2018. Importance of\na search strategy in neural dialogue modelling.\narXiv preprint arXiv: 1811.00907.\n\nJiwei Li, Michel Galley, Chris Brockett, Jian-\nfeng Gao, and Bill Dolan. 2016a. A diversity-\npromoting objective function for neural conver-\nsation models. In Proceedings of the 2016 Con-\nference of the North American Chapter of the\nAssociation for Computational Linguistics: Hu-\nman Language Technologies, pages 110-119.\nAssociation for Computational Linguistics.\n\nJiwei Li, Will Monroe, and Dan Jurafsky. 2017a.\nLearning to decode for future success. arXiv\npreprint arXiv:1701.06549.\n\nJiwei Li, Will Monroe, Alan Ritter, Dan Jurafsky,\nMichel Galley, and Jianfeng Gao. 2016b. Deep\n\n1711\n", "vlm_text": "References \nSanjeev Arora, Yingyu Liang, and Tengyu Ma. 2017.  A simple but tough-to-beat baseline for sentence embeddings . In  Proceedings of the In- ternational Conference on Learning Represen- tations (ICLR) . \nAshutosh Baheti, Alan Ritter, Jiwei Li, and Bill Dolan. 2018.  Generating more interesting re- sponses in neural conversation models with dis- tributional constraints . In  Proceedings of the 2018 Conference on Empirical Methods in Nat- ural Language Processing , pages 3970–3980. Association for Computational Linguistics. \nAntoine Bordes, Y-Lan Boureau, and Jason We- ston. 2017.  Learning end-to-end goal-oriented dialog . In  Proceedings of the International Con- ference on Learning Representations (ICLR) . \nEmily Dinan, Varvara Logacheva, Valentin Ma- lykh, Alexander Miller, Kurt Shuster, Jack Ur- banek, Douwe Kiela, Arthur Szlam, Iulian Ser- ban, Ryan Lowe, et al. 2019. The second conversational intelligence challenge (convai2) . arXiv preprint arXiv:1902.00098 . \nEmily Dinan, Stephen Roller, Kurt Shuster, An- gela Fan, Michael Auli, and Jason Weston. 2018. Wizard of Wikipedia: Knowledge- powered conversational agents .  arXiv preprint arXiv:1811.01241 . \nLayla El Asri, Hannes Schulz, Shikhar Sharma, Jeremie Zumer, Justin Harris, Emery Fine, Rahul Mehrotra, and Kaheer Suleman. 2017. Frames: a corpus for adding memory to goal-oriented dialogue systems . In  Proceed- ings of the 18th Annual SIGDIAL Meeting on Discourse and Dialogue , pages 207–219, Saarbr¨ ucken, Germany. Association for Com- putational Linguistics. \nAngela Fan, David Grangier, and Michael Auli. 2018.  Controllable abstractive summarization . In  Proceedings of the 2nd Workshop on Neu- ral Machine Translation and Generation , pages 45–54. Association for Computational Linguis- tics. \nJessica Ficler and Yoav Goldberg. 2017.  Control- ling linguistic style aspects in neural language generation . In  Proceedings of the Workshop on Stylistic Variation , pages 94–104. Association for Computational Linguistics. \nand Kevin Knight. 2017.  Hafez: an interactive poetry generation system . In  Proceedings of ACL 2017, System Demonstrations , pages 43– 48. Association for Computational Linguistics. Fenfei Guo, Angeliki Metallinou, Chandra Kha- tri, Anirudh Raju, Anu Venkatesh, and Ashwin Ram. 2018.  Topic-based evaluation for conver- sational bots .  Advances in Neural Information Processing Systems, Conversational AI Work- shop . Helen Hastie. 2012. Metrics and evaluation of spoken dialogue systems , pages 131–150. Springer. Matthew Henderson, Blaise Thomson, and Ja- son D Williams. 2014.  The second dialog state tracking challenge . In  Proceedings of the   $l5t h$  Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL) , pages 263–272. Zhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan Salakhutdinov, and Eric P Xing. 2017.  Toward controlled generation of text . In  Thirty-fourth International Conference on Machine Learn- ing . Yuta Kikuchi, Graham Neubig, Ryohei Sasano, Hiroya Takamura, and Manabu Okumura. 2016. Controlling output length in neural encoder- decoders . In  Proceedings of the 2016 Con- ference on Empirical Methods in Natural Lan- guage Processing , pages 1328–1338. Associa- tion for Computational Linguistics. Ilya Kulikov, Alexander H Miller, Kyunghyun Cho, and Jason Weston. 2018.  Importance of a search strategy in neural dialogue modelling . arXiv preprint arXiv:1811.00907 . Jiwei Li, Michel Galley, Chris Brockett, Jian- feng Gao, and Bill Dolan. 2016a.  A diversity- promoting objective function for neural conver- sation models . In  Proceedings of the 2016 Con- ference of the North American Chapter of the Association for Computational Linguistics: Hu- man Language Technologies , pages 110–119. Association for Computational Linguistics. Jiwei Li, Will Monroe, and Dan Jurafsky. 2017a. Learning to decode for future success . arXiv preprint arXiv:1701.06549 . Jiwei Li, Will Monroe, Alan Ritter, Dan Jurafsky, Michel Galley, and Jianfeng Gao. 2016b.  Deep "}
{"page": 10, "image_path": "doc_images/N19-1170_10.jpg", "ocr_text": "reinforcement learning for dialogue generation.\nIn Proceedings of the 2016 Conference on Em-\npirical Methods in Natural Language Process-\ning, pages 1192-1202, Austin, Texas. Associa-\ntion for Computational Linguistics.\n\nJiwei Li, Will Monroe, Tianlin Shi, Sébastien\n\nJean, Alan Ritter, and Dan Jurafsky. 2017b. Ad-\nversarial learning for neural dialogue genera-\ntion. arXiv preprint arXiv:1701.06547.\n\nChia-Wei Liu, Ryan Lowe, Julian Serban,\nMike Noseworthy, Laurent Charlin, and Joelle\nPineau. 2016. How not to evaluate your dia-\nlogue system: An empirical study of unsuper-\nvised evaluation metrics for dialogue response\ngeneration. pages 2122-2132.\n\nRyan Lowe, Michael Noseworthy, Iulian Vlad\nSerban, Nicolas Angelard-Gontier, Yoshua\nBengio, and Joelle Pineau. 2017. Towards an\nautomatic turing test: Learning to evaluate di-\nalogue responses. In Proceedings of the 55th\nAnnual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers),\npages 1116-1126. Association for Computa-\ntional Linguistics.\n\nRyan Lowe, Nissan Pow, Iulian Serban, and Joelle\nPineau. 2015. The Ubuntu dialogue corpus: A\nlarge dataset for research in unstructured multi-\nturn dialogue systems. In Proceedings of the\n16th Annual Meeting of the Special Interest\nGroup on Discourse and Dialogue, pages 285—\n294, Prague, Czech Republic. Association for\nComputational Linguistics.\n\nAlexander Miller, Will Feng, Dhruv Batra, An-\ntoine Bordes, Adam Fisch, Jiasen Lu, Devi\nParikh, and Jason Weston. 2017. ParlAI: A\ndialog research software platform. In Pro-\nceedings of the 2017 Conference on Empiri-\ncal Methods in Natural Language Processing:\nSystem Demonstrations, pages 79-84, Copen-\nhagen, Denmark. Association for Computa-\ntional Linguistics.\n\nJekaterina Novikova, Ondiej DuSek, Amanda Cer-\n\ncas Curry, and Verena Rieser. 2017. Why we\nneed new evaluation metrics for nlg. In Pro-\nceedings of the 2017 Conference on Empiri-\ncal Methods in Natural Language Processing,\npages 2241-2252.\n\nPrasanna Parthasarathi and Joelle Pineau. 2018.\n\nExtending neural generative conversational\nmodel using external knowledge sources. In\n\nProceedings of the 2018 Conference on Empir-\nical Methods in Natural Language Processing,\npages 690-695, Brussels, Belgium. Association\nfor Computational Linguistics.\n\nNanyun Peng, Marjan Ghazvininejad, Jonathan\nMay, and Kevin Knight. 2018. Towards con-\ntrollable story generation. In Proceedings of\nthe First Workshop on Storytelling, pages 43—\n49. Association for Computational Linguistics.\n\nJeffrey Pennington, Richard Socher, and Christo-\npher Manning. 2014. GloVe: Global vectors for\nword representation. In Proceedings of the 2014\nConference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 1532-\n1543, Doha, Qatar. Association for Computa-\ntional Linguistics.\n\nAlec Radford, Karthik Narasimhan, Tim Sali-\nmans, and Ilya Sutskever. 2018. Improving lan-\nguage understanding by generative pre-training.\n\nJulian Vlad Serban, Ryan Lowe, Laurent Charlin,\nand Joelle Pineau. 2016a. Generative deep neu-\nral networks for dialogue: A short review. Ad-\nvances in Neural Information Processing Sys-\ntems workshop on Learning Methods for Dia-\nlogue.\n\nJulian Vlad Serban, Alessandro Sordoni, Yoshua\nBengio, Aaron C Courville, and Joelle Pineau.\n2016b. Building end-to-end dialogue systems\nusing generative hierarchical neural network\nmodels. In AAAI, volume 16, pages 3776-3784.\n\nXiaoyu Shen, Hui Su, Yanran Li, Wenjie Li, Shuzi\nNiu, Yang Zhao, Akiko Aizawa, and Guoping\nLong. 2017. A conditional variational frame-\nwork for dialog generation. In Proceedings\nof the 55th Annual Meeting of the Association\nfor Computational Linguistics (Volume 2: Short\nPapers), pages 504-509. Association for Com-\nputational Linguistics.\n\nAnu Venkatesh, Chandra Khatri, Ashwin Ram,\nFenfei Guo, Raefer Gabriel, Ashish Nagar, Ro-\nhit Prasad, Ming Cheng, Behnam Hedayatnia,\nAngeliki Metallinou, et al. 2017. On evaluat-\ning and comparing conversational agents. Ad-\nvances in Neural Information Processing Sys-\ntems, Conversational AI Workshop.\n\nOriol Vinyals and Quoc Le. 2015. A neural con-\nversational model. In Proceedings of the 31st\nInternational Conference on Machine Learning,\nDeep Learning Workshop, Lille, France.\n\n1712\n", "vlm_text": "reinforcement learning for dialogue generation . In  Proceedings of the 2016 Conference on Em- pirical Methods in Natural Language Process- \ning , pages 1192–1202, Austin, Texas. Associa- tion for Computational Linguistics. Jiwei Li, Will Monroe, Tianlin Shi, S´ ebastien Jean, Alan Ritter, and Dan Jurafsky. 2017b.  Ad- versarial learning for neural dialogue genera- tion .  arXiv preprint arXiv:1701.06547 . Chia-Wei Liu, Ryan Lowe, Iulian Serban, Mike Noseworthy, Laurent Charlin, and Joelle Pineau. 2016. How not to evaluate your dia- logue system: An empirical study of unsuper- vised evaluation metrics for dialogue response generation . pages 2122–2132. Ryan Lowe, Michael Noseworthy, Iulian Vlad Serban, Nicolas Angelard-Gontier, Yoshua Bengio, and Joelle Pineau. 2017.  Towards an automatic turing test: Learning to evaluate di- alogue responses . In  Proceedings of the 55th Annual Meeting of the Association for Compu- tational Linguistics (Volume 1: Long Papers) , pages 1116–1126. Association for Computa- tional Linguistics. Ryan Lowe, Nissan Pow, Iulian Serban, and Joelle Pineau. 2015.  The Ubuntu dialogue corpus: A large dataset for research in unstructured multi- turn dialogue systems . In  Proceedings of the 16th Annual Meeting of the Special Interest Group on Discourse and Dialogue , pages 285– 294, Prague, Czech Republic. Association for Computational Linguistics. Alexander Miller, Will Feng, Dhruv Batra, An- toine Bordes, Adam Fisch, Jiasen Lu, Devi Parikh, and Jason Weston. 2017. ParlAI: A dialog research software platform . In  Pro- ceedings of the 2017 Conference on Empiri- cal Methods in Natural Language Processing: System Demonstrations , pages 79–84, Copen- hagen, Denmark. Association for Computa- tional Linguistics. Jekaterina Novikova, Ondˇ rej Duˇ sek, Amanda Cer- cas Curry, and Verena Rieser. 2017.  Why we need new evaluation metrics for nlg . In  Pro- ceedings of the 2017 Conference on Empiri- cal Methods in Natural Language Processing , pages 2241–2252. Prasanna Parthasarathi and Joelle Pineau. 2018. Extending neural generative conversational model using external knowledge sources . In \nProceedings of the 2018 Conference on Empir- ical Methods in Natural Language Processing , pages 690–695, Brussels, Belgium. Association for Computational Linguistics. Nanyun Peng, Marjan Ghazvininejad, Jonathan May, and Kevin Knight. 2018.  Towards con- trollable story generation . In  Proceedings of the First Workshop on Storytelling , pages 43– 49. Association for Computational Linguistics. Jeffrey Pennington, Richard Socher, and Christo- pher Manning. 2014.  GloVe: Global vectors for word representation . In  Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pages 1532– 1543, Doha, Qatar. Association for Computa- tional Linguistics. Alec Radford, Karthik Narasimhan, Tim Sali- mans, and Ilya Sutskever. 2018.  Improving lan- guage understanding by generative pre-training . Iulian Vlad Serban, Ryan Lowe, Laurent Charlin, and Joelle Pineau. 2016a.  Generative deep neu- ral networks for dialogue: A short review .  Ad- vances in Neural Information Processing Sys- tems workshop on Learning Methods for Dia- logue . Iulian Vlad Serban, Alessandro Sordoni, Yoshua Bengio, Aaron C Courville, and Joelle Pineau. 2016b.  Building end-to-end dialogue systems using generative hierarchical neural network models . In  AAAI , volume 16, pages 3776–3784. Xiaoyu Shen, Hui Su, Yanran Li, Wenjie Li, Shuzi Niu, Yang Zhao, Akiko Aizawa, and Guoping Long. 2017.  A conditional variational frame- work for dialog generation . In  Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers) , pages 504–509. Association for Com- putational Linguistics. Anu Venkatesh, Chandra Khatri, Ashwin Ram, Fenfei Guo, Raefer Gabriel, Ashish Nagar, Ro- hit Prasad, Ming Cheng, Behnam Hedayatnia, Angeliki Metallinou, et al. 2017.  On evaluat- ing and comparing conversational agents .  Ad- vances in Neural Information Processing Sys- tems, Conversational AI Workshop . Oriol Vinyals and Quoc Le. 2015.  A neural con- versational model . In  Proceedings of the 31st International Conference on Machine Learning, Deep Learning Workshop , Lille, France. "}
{"page": 11, "image_path": "doc_images/N19-1170_11.jpg", "ocr_text": "Marilyn A. Walker, Diane J. Litman, Candace A.\nKamm, and Alicia Abella. 1997. PARADISE:\nA framework for evaluating spoken dialogue\nagents. In Proceedings of the 35th Annual Meet-\ning of the Association for Computational Lin-\nguistics, pages 271-280, Madrid, Spain. Asso-\nciation for Computational Linguistics.\n\nDi Wang, Nebojsa Jojic, Chris Brockett, and Eric\nNyberg. 2017. Steering output style and topic\nin neural response generation. In Proceedings\nof the 2017 Conference on Empirical Methods\nin Natural Language Processing, pages 2140—\n2150. Association for Computational Linguis-\ntics.\n\nTsung-Hsien Wen, David Vandyke, Nikola\nMrk8ié, Milica Gasic, Lina M. Rojas Barahona,\nPei-Hao Su, Stefan Ultes, and Steve Young.\n2017. A network-based end-to-end trainable\ntask-oriented dialogue system. In Proceedings\nof the 15th Conference of the European Chapter\nof the Association for Computational Linguis-\ntics: Volume 1, Long Papers, pages 438-449.\nAssociation for Computational Linguistics.\n\nChen Xing, Wei Wu, Yu Wu, Jie Liu, Yalou\nHuang, Ming Zhou, and Wei-Ying Ma. 2017.\nTopic aware neural response generation. In\nAAAI, volume 17, pages 3351-3357.\n\nRuging Zhang, Jiafeng Guo, Yixing Fan, Yanyan\nLan, Jun Xu, and Xueqi Cheng. 2018a. Learn-\ning to control the specificity in neural response\ngeneration. In Proceedings of the 56th Annual\nMeeting of the Association for Computational\nLinguistics (Volume I: Long Papers), pages\n1108-1117, Melbourne, Australia. Association\nfor Computational Linguistics.\n\nSaizheng Zhang, Emily Dinan, Jack Urbanek,\nArthur Szlam, Douwe Kiela, and Jason Weston.\n2018b. Personalizing dialogue agents: I have a\ndog, do you have pets too? In Proceedings of\nthe 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Pa-\npers), pages 2204-2213, Melbourne, Australia.\nAssociation for Computational Linguistics.\n\nGanbin Zhou, Ping Luo, Rongyu Cao, Fen Lin,\nBo Chen, and Qing He. 2017. Mechanism-\naware neural machine for dialogue response\ngeneration. In AAAI, pages 3400-3407.\n\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan\nSalakhutdinov, Raquel Urtasun, Antonio Tor-\nralba, and Sanja Fidler. 2015. Aligning books\n\n1713\n\nand movies: Towards story-like visual explana-\ntions by watching movies and reading books. In\nProceedings of the IEEE international confer-\nence on computer vision, pages 19-27.\n", "vlm_text": "Marilyn A. Walker, Diane J. Litman, Candace A. Kamm, and Alicia Abella. 1997.  PARADISE: A framework for evaluating spoken dialogue agents . In  Proceedings of the 35th Annual Meet- ing of the Association for Computational Lin- guistics , pages 271–280, Madrid, Spain. Asso- ciation for Computational Linguistics. Di Wang, Nebojsa Jojic, Chris Brockett, and Eric Nyberg. 2017.  Steering output style and topic in neural response generation . In  Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing , pages 2140– 2150. Association for Computational Linguis- tics. Tsung-HsienWen,DavidVandyke,NikolaMrkˇ si´ c, Milica Gasic, Lina M. Rojas Barahona, Pei-Hao Su, Stefan Ultes, and Steve Young. 2017. A network-based end-to-end trainable task-oriented dialogue system . In  Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguis- tics: Volume 1, Long Papers , pages 438–449. Association for Computational Linguistics. Chen Xing, Wei Wu, Yu Wu, Jie Liu, Yalou Huang, Ming Zhou, and Wei-Ying Ma. 2017. Topic aware neural response generation. In AAAI , volume 17, pages 3351–3357. Ruqing Zhang, Jiafeng Guo, Yixing Fan, Yanyan Lan, Jun Xu, and Xueqi Cheng. 2018a.  Learn- ing to control the speciﬁcity in neural response generation . In  Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 1108–1117, Melbourne, Australia. Association for Computational Linguistics. Saizheng Zhang, Emily Dinan, Jack Urbanek, Arthur Szlam, Douwe Kiela, and Jason Weston. 2018b.  Personalizing dialogue agents: I have a dog, do you have pets too?  In  Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Pa- pers) , pages 2204–2213, Melbourne, Australia. Association for Computational Linguistics. Ganbin Zhou, Ping Luo, Rongyu Cao, Fen Lin, Bo Chen, and Qing He. 2017. Mechanism- aware neural machine for dialogue response generation . In  AAAI , pages 3400–3407. Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Tor- ralba, and Sanja Fidler. 2015.  Aligning books and movies: Towards story-like visual explana- tions by watching movies and reading books . In Proceedings of the IEEE international confer- ence on computer vision , pages 19–27. \n"}
{"page": 12, "image_path": "doc_images/N19-1170_12.jpg", "ocr_text": "Supplementary Material\n\nA_ Screenshots of human evaluation interface\n\nTask Description\n\nIn this task, you will chat with another user playing the part of a given character.. For example, your given character could be:\n1am a vegetarian. | like swimming. My father used to work for Ford. My favorite band is Maroons. | got a new job last month, which is about advertising design.\nChat with the other user naturally and try to get to know each other, ie. both ask questions and answer questions of your chat partner while sticking to your given character.\n\nIf you complete the task, you will receive $0.90. It may take up to 48 hours to review the HiTs, so please allow that much time to pass before payment. After completion,\nyou may be assigned a qualification that prevents you from working on more if you have completed enough of these HITs.\n\nAfter a given number of turns, you may be asked a few questions in order to evaluate your partner.\n\nIf your partner answers poorly, change topic. Do not linger on their poor response. Instead, mention this during the evaluation section.\n\nClose Window/Timeout/Return HIT\nOnce the conversation has started, close window/timeout or return HIT during the chat will result in HIT EXPIRED to you and NO reward paid.\n\nImportant Notice\n\n1. Be aware the conversations you have will be made public, so act as you would e.g. on a public social network like Twitter.\n\n2. Please do not send long messages: messages cannot exceed 30 words.\n\n3. Please do not reference the task or MTurk itself during the conversation, but speak naturally to the other person.\n\n4, Please do not send any message that could make others uncomfortable, including any level of discrimination, racism, sexism and offensive religious/politics comments,\notherwise the submission will be rejected.\n\nNote: the user you are chatting with may be a human or a bot.\n\nFigure 5: Screenshot of the Task Description\n\nLiv e Ch at PERSON 2: | love coffee and coffee\n\nPERSON_41: oh yes, coffee is great. buzz buzz buzz!\n\nTask Description PERSON. 2: Yeah | like coffee too\n\nIn this task, you will chat with another user playing the part\nof a given character.. For example, your given character could\n\nte PERSON_1: do you speak french? i want to learn it\n\n1am a vegetarian. | like swimming. My father used to work for\nFord. My favorite band is MaroonS. | got a new job last month, PERSON _2: | do not but I do love coffee\nwhich is about advertising design.\n\nChat with the other user naturally and try to get to know each\nPERSON_1: do you have a favorite color?\n\nYour assigned character is:\nPERSON. 2: | like blue but | like the color yellow\n\ni also study languages.\n\nmy favorite spanish word is trabajo.\n\nmy next language to study is french.\n\none of the languages that i am currently studying is\n\nspanish. Please\n\n|\n\nFigure 6: Screenshot of the chat UI, talking with the beam search baseline model.\n\n1714\n", "vlm_text": "Supplementary Material \nA Screenshots of human evaluation interface \n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: Screenshot of the Task Description \nThe image shows a screenshot of a chat user interface (UI) where two users are having a conversation. PERSON_1 and PERSON_2 are engaged in a dialogue, possibly as part of a task involving role-playing with assigned characters.\n\nOn the left side of the image, there is a panel with a \"Live Chat\" heading and a \"Task Description\" section. This section explains that users are to chat by pretending to be a given character, with an example provided for guidance. It includes the specific assigned character which mentions that the person studies languages, with Spanish being one of the languages currently studied, and they have plans to study French next. \n\nOn the right side, the chat interface shows the conversation between PERSON_1 and PERSON_2. PERSON_2 expresses a love for coffee and the color blue, though they also like yellow, while PERSON_1 talks about coffee and asks about speaking French and favorite colors. There's also an input field at the bottom with a \"Send\" button, indicating where new messages can be typed and sent to continue the conversation."}
{"page": 13, "image_path": "doc_images/N19-1170_13.jpg", "ocr_text": "Please read the following two conversations, and choose who you think is the more interesting speaker:\n\n: (shown left, in the blue speech bubble)\n\n+ Ge Z@ (shown right, in the green speech bubble).\n\nIPlease do your best to disregard the quality of the other speakers (shown in gray).\n\nHey how are you?\n\n1am good, yourself?\n\nI'm doing well i'm a recluse\n\n| love board games.\n\nSo do I!\n\nMy parents were teachers, how about yours?\n\nlam a twitch streamer!\n\nNice, | like going to the beach.\n\nThat's cool | am allergic to the water\n\nThat must be no fun.\n\nIt's really not\n\nIcan only imagine.\n\nHi how are you today\n\nhi im fine and you\n\n| am good do you work\n\nyes im an economist, and you\n\nWhat is your favorite food\n\narabian food, i eat a raw diet\n\nThat sounds like a lot of fun\n\nit is for me, i'm married with two ids is hard to have this\nlifestyle\n\nWhat are your hobbies then\n\ni love the rolling stones i see them every year in concert\n\nThat is so nice of you\n\nit is, and do you have any hobbies?\n\nWhich speaker is more interesting to talk to?\n\n5\nCLZEDD «100 ierosting.\n\noO\nteam is more interesting.\n\ne enter a very brief reason (a few words or a sentence) explaining your choice:\n\n((f you do not give a reason, your hit may be rejected)\n\nFigure 7: Screenshot of the A/B test UI, comparing a human-human conversation (left) and a Repetition-controlled\n\nbaseline model (right).\n\n1715\n", "vlm_text": "The image shows a task where the viewer is asked to choose the more interesting speaker from two conversations. There are two speakers:\n\n- **Speaker 1** (blue speech bubble) talks about liking board games, having parents who were teachers, going to the beach, and being allergic to water.\n- **Speaker 2** (green speech bubble) mentions being a recluse, working as an economist, eating a raw diet, being married with two kids, and enjoying Rolling Stones concerts annually.\n\nThe task is to select the more interesting speaker and provide a reason for the choice.\nFigure 7: Screenshot of the A/B test UI, comparing a human-human conversation (left) and a Repetition-controlled baseline model (right). "}
{"page": 14, "image_path": "doc_images/N19-1170_14.jpg", "ocr_text": "B Human evaluation questionnaire design\n\nHere are the questions and multiple-choice options used in the human evaluation, in the order presented:\n\n[Engagingness] How much did you enjoy talking to this user?\ne Not at all e A little e Somewhat e A lot\n\n[Interestingness] How interesting or boring did you find this conversation?\ne Very boring e A little boring e A little interesting ¢ Very interesting\n\n[Inquisitiveness] How much did the user try to get to know you?\ne Didn’t ask about me at all e Asked about me some\ne Asked about me a good amount e Asked about me too much\n\n[Listening] How much did the user seem to pay attention to what you said?\ne Always ignored what I said e Mostly ignored what I said\ne Mostly paid attention to what I said e Always paid attention to what I said\n\n[Avoiding Repetition] How repetitive was this user?\ne Repeated themselves over and over e Sometimes said the same thing twice\ne Always said something new\n\n[Fluency] How naturally did this user speak English?\ne Very unnatural e Mostly unnatural e Mostly natural e Very natural\n\n[Making sense] How often did this user say something which did NOT make sense?\ne Never made any sense e Most responses didn’t make sense\ne Some responses didn’t make sense e Everything made perfect sense\n\n[Humanness] Do you think this user is a bot or a human?\ne Definitely a bot e Probably a bot e Probably a human e Definitely a human\n\n[Persona retrieval] Which prompt (character) do you think the other user was given\nfor this conversation?\n\nRespondent chooses one of two provided personas\n\n1716\n", "vlm_text": "B Human evaluation questionnaire design \nHere are the questions and multiple-choice options used in the human evaluation, in the order presented: \nThe image displays a questionnaire designed to assess a user's interaction with another user or system. The questions aim to evaluate various aspects of the conversation, such as:\n\n1. **Engagingness**: Enjoyment level in talking to the user.\n2. **Interestingness**: Interest or boredom level regarding the conversation.\n3. **Inquisitiveness**: The extent to which the user tried to get to know the respondent.\n4. **Listening**: How much attention the user paid to what the respondent said.\n5. **Avoiding Repetition**: Frequency of repetitive statements by the user.\n6. **Fluency**: How naturally the user spoke English.\n7. **Making sense**: Frequency of making nonsensical statements.\n8. **Humanness**: Whether the user is perceived as a bot or human.\n9. **Persona retrieval**: Identification of the prompt (character) believed to be provided to the user for the conversation.\n\nEach question provides multiple-choice answers to gauge the respondent's perception and experience during the interaction."}
{"page": 15, "image_path": "doc_images/N19-1170_15.jpg", "ocr_text": "C_ Example conversations from human evaluation\n\nHum:\n\n‘What do you do for a living?\n\nBot: I'm a buddhist. What about you?\n\nHuman: I'm a Christian. Do you have a job?\n\nBot: | am a buddhist. What a\n\nHum:\n\n: work on my parent's farm most days. Do you have a job?\n\nHuman: Do you like music?\n\nBot: | do like music. What a\n\nyou?\n\nHuman: Yes, but only country.\n\ncity are you from?\n\nHuman: I live in the country, a very small town called Ila.\n\ncity are you from?\n\nBot: | am a buddhist. What about you?\n\nHuman: Hey there! I'm a litle girl :) how are you?\n\nBot: | am doing well, how are you?\n\nHuman: I'm very lonely actually. | live in the cloud.\n\nBot: What do you do for a living?\n\nHuman: Nothing really, 'd like to say superhero since | have superpowers.\n\nBot: That sounds like a lot of fun!\n\nHuman: Yes! Do you like to shop?\n\n: | always make time to shop!\n\nHuman: Pizza | would have to say. What about you?\n\n(a)\n\n(b)\n\nBot: Hello, how are you today?\n\nHuman: Hil 'm great, how are you?\n\nBot: | am doing well, thank you. What do\n\n1 do for fun?\n\nI dont have time for too much, I'm either working or spending time with my\n\nBot: What kind of pets? | walk dogs for a living,\n\nHuman: That must be a fun job.\n\nBot: Itis, but | spend most of my time reading,\n\nHum:\n\n‘What kind of books do you like?\n\nBot: My favorite is harry potter. H.\n\nHuman: I like legal and psychological thrillers.\n\nv, that is a lot of books.\n\nHum:\n\n[Hat do you do for work? I'm in construction.\n\nHuman: Hello, how are you doing today?\n\nBot: | am doing well. Just playing some piano and singing along to folk music.\n\nHum\n\n‘That sounds fun. What type of music do you like?\n\nBot: | hope to make some musical songs for my band. Do you play any\ninstruments?\n\nHuman: Nope, | was raised in a single parent household so never really had much\n‘money to spend.\n\nBot: | wish I could spend more time with my family\n\nk as an auditor\n\nHuman: That's awesome. | work two jobs out of which one of them is as stunt\ndouble.\n\nBot: Thal\n\nnds like a lot of fun.\n\nHuman: Well yeah itis, but comes with its own cons as well.\n\nBot: What is your\n\nite thing to do?\n\nHuman: Reading. | read a lot. Last year | read a total of 20 books.\n\nBot: What kind of bo\n\nre you reading?\n\n(c)\n\n(d)\n\nFigure 8: Example conversation with (a) Baseline (b) Repetition-controlled baseline (c) Question-controlled CT\n\n(z = 7), (d) Specificity-controlled WD (weight = 4).\n\n1717\n\n", "vlm_text": "The image contains four panels labeled (a), (b), (c), and (d), each featuring simulated conversations between a human and a bot. The conversations are boxed and color-coded, with human dialogues in gray and bot responses in blue.\n\n- Panel (a) depicts a conversation where the human talks about their job and location. The bot repeatedly says it's a Buddhist and sometimes asks back \"What about you?\" or personal questions like \"What city are you from?\" and mentions a liking for music.\n  \n- Panel (b) shows a conversation with a younger human who mentions being lonely and living \"in the cloud\". The bot responds in a friendly manner, asking about the human's favorite activities and foods.\n  \n- Panel (c) features a chat where the human talks about their hobbies and work. The bot joins in with comments about its own interests, like walking dogs and liking Harry Potter books.\n  \n- Panel (d) includes a conversation about music, the human's experiences growing up, and their current job. The bot discusses its interests such as playing music and asks about favorite books.\n\nOverall, the image exemplifies interactions designed for evaluating human-bot conversations, showcasing how the bot responds to various prompts and maintains a dialogue.\nFigure 8: Example conversation with (a) Baseline (b) Repetition-controlled baseline (c) Question-controlled CT  $(z=7)$  ), (d) Speciﬁcity-controlled WD (weight  $=4$  ). "}
{"page": 16, "image_path": "doc_images/N19-1170_16.jpg", "ocr_text": "D_ Repetition-control decoding features\n\nFeature Condition\nextrep_bigram(w, yet, L) Adding w to the hypothesis y<; would create a 2-gram\nhat appears in a previous utterance by the model\nextrep_-unigram(wW, Yet, 2) w is anon-stopword and\nw appears in a previous utterance by the model\nintrep_bigram(w, yet, L) Add\n\ning w to the hypothesis y<; would create a 2-gram\nhat appears earlier in the hypothesis y<;\n\nintrep-unigram(w, Yet, 2) w is anon-stopword and\nw appears earlier in the hypothesis y<;\npartnerrep_bigram(w, Yet, x) Adding w to the hypo:\n\nhesis y<; would create a 2-gram\nhat appears in a previous utterance by the partner\n\nTable 4: We define five binary features for controlling different types of repetition via weighted decoding (see\nSection 5.2). Each feature depends on the word w, the partial hypothesis y—,, and the context x (which includes\n\nthe model’s own persona and the dialogue history). Each of these features is equal to 1 if and only if the condition\non the right is true; otherwise 0.\n\n1718\n", "vlm_text": "The table outlines various features along with their respective conditions, presumably for a language model or a natural language processing task, based on the naming conventions used. Here's what each entry in the table describes:\n\n- **Feature:** `extrep_bigram(w, y<t, x)`\n  - **Condition:** Adding the word `w` to the hypothesis `y<t` would form a 2-gram that appears in a previous utterance by the model.\n\n- **Feature:** `extrep_unigram(w, y<t, x)`\n  - **Condition:** The word `w` is a non-stopword and appears in a previous utterance by the model.\n\n- **Feature:** `intrep_bigram(w, y<t, x)`\n  - **Condition:** Adding the word `w` to the hypothesis `y<t` would create a 2-gram that appears earlier in the hypothesis `y<t`.\n\n- **Feature:** `intrep_unigram(w, y<t, x)`\n  - **Condition:** The word `w` is a non-stopword and appears earlier in the hypothesis `y<t`.\n\n- **Feature:** `partnerrep_bigram(w, y<t, x)`\n  - **Condition:** Adding the word `w` to the hypothesis `y<t` would create a 2-gram that appears in a previous utterance by the partner.\n\nThe features seem to relate to lexical repetition detection, with distinctions between repetitions within the model's own output (`extrep`) and those relating to previously generated content or conversational partners (`partnerrep`), as well as between unigrams and bigrams.\nTable 4: We deﬁne ﬁve binary features for controlling different types of repetition via weighted decoding (see Section  5.2 ). Each feature depends on the word    $w$  , the partial hypothesis  $y_{<t}$  , and the context    $x$   (which includes the model’s own persona and the dialogue history). Each of these features is equal to 1 if and only if the condition on the right is true; otherwise 0. "}
{"page": 17, "image_path": "doc_images/N19-1170_17.jpg", "ocr_text": "E_ Control settings for all configurations\n\nRepetition Specificity Response-rel_ Questions\nExternal Internal Partner Rep.\nBigram Unigram Bigram —_ Unigram Bigram NIDF Cos sim Has ‘?”\nBaselines\nGreedy Search\nBeam Search (beam size 20)\nRepetition control (WD)\nExtrep bigram WD -0.5 wt -0.5\nExtrep bigram WD -1.25 wt -1.25\nExtrep bigram WD -3.5 wt -3.5\nExtrep bigram WD -inf wt-o0\nRepetition-controlled baseline wt -3.5 wt -00 wt -co\n\nQuestion control (CT)\n\nQuestion-controlled CT 0 wt -3.5 wt -00 wt -co\nQuestion-controlled CT 1 wt-3.5 wt -00 wt -co\n\nQuestion-controlled CT 4 wt -3.5 wt -00 wt -co\n\nQuestion-controlled CT 7 wt -3.5 wt -00 wt -co\n\nQuestion-controlled CT 10 wt -3.5 wt -00 wt -co\n\nQuestion-controlled CT 10 (boost) wt 0* wt -00 wt -co\n\nSpecificity control (CT)\n\nSpecificity-controlled CT 0 wt -3.5 wt -00 wt -co z=0\nSpecificity-controlled CT 2 wt -3.5 wt -00 wt -co z=2\nSpecificity-controlled CT 4 wt -3.5 wt -00 wt -co z=4\nSpecificity-controlled CT 7 wt -3.5 wt -00 wt -co z=7\nSpecificity-controlled CT 9 wt -3.5 wt -00 wt -co z=9\n\nSpecificity control (WD)\n\nSpecificity-controlled WD -10 wt -3.5 wt -00 wt -co wt-10\nSpecificity-controlled WD -4 wt -3.5 wt -00 wt -co wt-4\nSpecificity-controlled WD 4 wt -3.5 wt -00 wt -co wt4\nSpecificity-controlled WD 6 wt -3.5 wt -00 wt -co wt 6\nSpecificity-controlled WD 8 wt -3.5 wt -00 wt -co wt8\nResponse-related control (WD) **\n\nResponse-related controlled WD-10 wt -3.5 wt-co — wt-o0 wt -co wt -00 wt -10\nResponse-related controlled WD 0 wt -3.5 wt-co — wt-o0 wt -co wt -00 wt0\nResponse-related controlled WD 5 wt -3.5 wt-co — wt-o0 wt -co wt -00 wt\nResponse-related controlled WD 10 wt -3.5 wt-co — wt-o0 wt -co wt -00 wt 10\nResponse-related controlled WD 13 wt -3.5 wt-co — wt-o0 wt -co wt -00 wt 13\n\nTable 5: Control settings for all configurations that were human-evaluated. ‘wt’ means the weight used for a\nweighted decoding feature and ‘z =’ means the setting (i.e. bucket) for the control variable in conditional training.\n\n* In the setting Question-controlled CT 10 (boost), the feature extrep_bigram is not used for weighted\ndecoding during beam search, but it is used to rerank the candidates after beam search. See Section 6.4 for details.\n\n** Note that the Response-related controlled models additionally introduce repetition controls to block in-\nternal bigram repetition and partner bigram repetition. This was necessary to prevent the model from parroting the\npartner’s last utterance. In Table 8, we find that just adding these extra repetition controls (here called Response-\nrelated controlled WD 0, i.e. increased repetition control but no response-relatedness control) outperforms our\ncanonical Repetition-controlled baseline. However, given that we discovered this later, our specificity and question\ncontrolled models are built on top of the canonical Repetition-controlled baseline.\n\n1719\n", "vlm_text": "The table presents various methods and their parameter settings for a computational task, likely related to text generation or natural language processing. It outlines baseline methods and several controls for different aspects of text such as repetition, specificity, response-related features, and question inclusion. The settings are divided into categories, including repetition control, question control, specificity control, and response-related control, each with different weighting parameters (denoted as \"wt\" and \"z\").\n\nHere’s a breakdown of the table's components:\n\n1. **Baselines:**\n   - Greedy Search\n   - Beam Search (with beam size of 20)\n\n2. **Repetition Control (WD):**\n   - Lists different configurations of \"Extrep bigram\" with varying \"wd\" values: -0.5, -1.25, -3.5, and negative infinity (`-inf`), affecting bigram and unigram weights in either internal or external contexts.\n\n3. **Question Control (CT):**\n   - Variants labeled from CT 0 to CT 10 and involve weights (`wt -3.5` and `wt -∞`) across different contexts with a variable `z` indicating control strength or setting.\n\n4. **Specificity Control (CT and WD):**\n   - Includes \"specificity-controlled\" configurations with variables `z` and WD weight settings ranging from -10 to 8 to adjust specificity control levels.\n\n5. **Response-related Control (WD):**\n   - Several controlled settings such as `wd -10`, `wd 0`, `wd 5`, `wd 10`, `wd 13`, affecting different repetition aspects, potentially to manage response coherence or relevance.\n\nOverall, the table organizes and details configurations for a text generation model, allowing controlled adjustments to handle various linguistic features like repetition, specificity, and questioning.\nTable 5: Control settings for all conﬁgurations that were human-evaluated. ‘wt’ means the weight used for a weighted decoding feature and   $\\begin{array}{r}{\\mathbf{\\dot{\\omega}}_{z}=\\mathbf{\\dot{\\omega}}}\\end{array}$   means the setting (i.e. bucket) for the control variable in conditional training. \n\\* In the setting Question-controlled CT 10 (boost), the feature  extrep bigram  is  not  used for weighted decoding during beam search, but it  is  used to rerank the candidates after beam search. See Section  6.4  for details. \n $^{**}$   Note that the Response-related controlled models additionally introduce repetition controls to block in- ternal bigram repetition and partner bigram repetition. This was necessary to prevent the model from parroting the partner’s last utterance. In Table  8 , we ﬁnd that just adding these extra repetition controls (here called Response- related controlled WD 0, i.e. increased repetition control but no response-relatedness control) outperforms our canonical Repetition-controlled baseline. However, given that we discovered this later, our speciﬁcity and question controlled models are built on top of the canonical Repetition-controlled baseline. "}
{"page": 18, "image_path": "doc_images/N19-1170_18.jpg", "ocr_text": "F Automatic metrics for all configurations\n\nRepetition Specificity Response-rel___ Questions\nExternal Internal Partner Rep.\nBigram Unigram—Bigram_—_Unigram Bigram NIDF Cos sim Has ‘?”\nGold data and baselines\nGold Data 4.65% 9.62% 0.38% 0.97% 5.10% 0.2119 0.1691 28.80%\nGreedy Search 35.88% 36.31% 8.08% 10.59% 12.20% 0.1688 0.1850 6.46%\nBeam Search (beam size 20) 46.85% 44.15% 0.32% 0.61% 12.90% 0.1662 0.0957 80.87%\nRepetition control (WD)\nExtrep bigram WD -0.5 19.70% 16.85% 0.26% 0.62% 11.93% 0.1730 0.1348 73.04%\nExtrep bigram WD -1.25 4.62% 4.79% 0.40% 0.89% 10.61% 0.1763 0.1504 61.22%\nExtrep bigram WD -3.5 0.15% 4.61% 0.47% 0.94% 9.89% 0.1771 0.1681 48.89%\nExtrep bigram WD -inf 0.00% 474% 0.51% 1.05% 9.56% 0.1780 0.1711 45.98%\nRepetition-controlled baseline 0.73% 0.00% 0.17% 0.00% 9.55% 0.1766 0.1676 49.98%\nQuestion control (CT)\nQuestion-controlled CT 0 0.06% 0.00% 0.19% 0.00% 9.20% 0.1871 0.1753 2.01%\nQuestion-controlled CT 1 0.09% 0.00% 0.19% 0.00% 8.66% 0.1844 0.1722 17.33%\nQuestion-controlled CT 4 0.40% 0.00% 0.25% 0.00% 8.53% 0.1794 0.1713 48.88%\nQuestion-controlled CT 7 0.80% 0.00% 0.17% 0.00% 8.48% 0.1771 0.1724 65.65%\nQuestion-controlled CT 10 1.27% 0.00% 0.16% 0.00% 8.48% 0.1761 0.1728 79.67%\nQuestion-controlled CT 10 (boost)* 7.64% 0.00% 0.03% 0.00% 10.76% 0.1701 0.1651 99.54%\nSpecificity control (CT)\nSpecificity-controlled CT 0 0.60% 0.00% 0.20% 0.00% 9.05% 0.1478 0.1522 48.75%\nSpecificity-controlled CT 2 0.28% 0.00% 0.10% 0.00% 8.37% 0.1772 0.1833 50.57%\nSpecificity-controlled CT 4 0.12% 0.00% 0.08% 0.00% 7.90% 0.1921 0.1877 29.46%\nSpecificity-controlled CT 7 0.02% 0.00% 0.14% 0.00% 8.17% 0.2156 0.1955 16.51%\nSpecificity-controlled CT 9 0.01% 0.00% 0.11% 0.00% 8.01% 0.2462 0.1990 8.50%\nSpecificity control (WD)\nSpecificity-controlled WD -10 0.14% 0.00% 10.59% 0.00% 8.70% 0.1107 0.0994 33.55%\nSpecificity-controlled WD -4 0.65% 0.00% 1.98% 0.00% 9.95% 0.1501 0.1398 44.92%\nSpecificity-controlled WD 4 0.15% 0.00% 0.19% 0.00% 7.54% 0.2121 0.1972 45.53%\nSpecificity-controlled WD 6 0.07% 0.00% 0.13% 0.00% 6.50% 0.2546 0.2040 39.37%\nSpecificity-controlled WD 8 0.01% 0.00% 0.10% 0.00% 3.40% 0.4035 0.1436 26.68%\nResponse-related control (WD)\nResponse-related controlled WD -10 0.13% 0.00% 0.00% 0.00% 0.00% 0.1914 -0.0921 25.71%\nResponse-related controlled WD 0 0.24% 0.00% 0.00% 0.00% 0.00% 0.1785 0.1414 44.55%\nResponse-related controlled WD 5 0.15% 0.00% 0.00% 0.00% 0.00% 0.1973 0.4360 39.18%\nrelated controlled WD 10 0.05% 0.00% 0.00% 0.00% 0.00% 0.2535 0.6653 27.56%\nrelated controlled WD 13 0.02% 0.00% 0.00% 0.00% 0.00% 0.2999 0.7251 20.47%\n\nTable 6: Automatic metrics (computed over validation set) for all model configurations that were human-evaluated.\n\n1720\n", "vlm_text": "The table presents a set of metrics and configurations used to evaluate models on several aspects such as repetition, specificity, response relevance, and question frequency. Here's a breakdown of the table's content:\n\n- **Main Categories**: \n  - Gold Data and Baselines\n  - Repetition Control (labeled as WD for some entries)\n  - Question Control (labeled as CT)\n  - Specificity Control (both CT and WD)\n  - Response-related Control (WD)\n\n- **Columns**:\n  - **Repetition**:\n    - Subdivided into External (Bigram, Unigram), Internal (Bigram, Unigram), and Partner Repetition.\n  - **Specificity** (measured by NIDF - Inverse Document Frequency).\n  - **Response-rel** (Relevant response measured by Cosine Similarity).\n  - **Questions** (Percentage of response with a question mark, indicated in 'Has '??').\n\n- **Entries**: Each row represents a different model or setting with specified controls or baselines.\n  - Baseline models include Gold Data, Greedy Search, and Beam Search.\n  - Repetition controls provide diverse weighting (e.g., Extrep bigram WD -0.5, -1.25).\n  - Question controls range from 0 to 10 with a boost on 10.\n  - Specificity controls are shown for both CT and WD approaches with different values.\n  - Response-related controls are demonstrated with WD and various numeric tags.\n\n- **Values**: Expressed largely in percentages for repetition and questions or as decimal fractions for specificity and response-rel measures."}
{"page": 19, "image_path": "doc_images/N19-1170_19.jpg", "ocr_text": "G Human evaluation results for all configurations\n\nModel Avoiding Rep. Engage Fluency Humanness Inquisitive Interesting Listening Make Sense Persona\n\nHuman and baselines\n\nHuman 2.90 £0.39 3.314090 3.6640.71 3404080 2.6340.63 3.234083 3.6440.63 3.84+0.52 | 0.92 40.27\nGreedy Search 216 £0.72 23141.08 3.204081 1.78+0.90 2.004081 2.36+0.98 2.78+0.84 3.33 £0.75 | 0.87 + 0.34\nBeam Search (beam size 20) 2144£0.72 2354 1.01 3.234093 1814087 2.5040.72 2.354098 2.63+40.85 3.40+0.77 | 0.774042\nRepetition control (WD)\n\nExtrep bigram WD -0.5 2.66 £0.56 2.564092 3.574£0.64 2.19+0.94 2.674 0.62 2.614087 3.0840.78 3.60+0.57 | 0.75 £0.43\nExtrep bigram WD -1.25 2844£0.39 2914090 3.5940.64 2.324098 2.634 0.60 2.864089 3.2140.71 3.644 0.62 | 0.724045\nExtrep bigram WD -3.5 2.90 £0.30 2.95 3.73 £0.50 24541.03 2554061 2.88+0.80 3.2740.79 3.68 £0.49 | 0.80 + 0.40\nExtrep bigram WD -inf 282 £0.43 2964086 3.644058 240+0.96 2.6540.69 2.86+0.82 3.3140.69 3.66 +0.59 | 0.91 + 0.29\nRepetition-controlled baseline 289 £0.39 2.894089 3.664056 2.504099 2.704064 2.96+0.92 3.254071 3.68 £0.54 | 0.87 + 0.34\nQuestion control (CT)\n\nQuestion-controlled CT 0 2.95 £0.25 292+40.90 3.704054 2494097 24840.72 2.85+40.93 3.2940.69 3.56 + 0.66 | 0.86 + 0.35\nQuestion-controlled CT 1 288 £0.33 2944093 3.594£0.66 2474095 2.52+0.69 2.85+40.90 3.3240.73 3.63 £0.55 | 0.85 + 0.36\nQuestion-controlled CT 4 288 £0.38 2884094 3.5940.73 2424107 255+0.66 2.824085 3.374£0.74 3.63 £0.59 | 0.84 + 0.37\nQuestion-controlled CT 7 288 £0.37 3.074090 3.674054 2424098 2.754058 2.974084 3.2340.76 3.53 £0.76 | 0.80 40.40\nQuestion-controlled CT 10 2.744046 2.904093 3.704050 243+ 1.04 2.714057 2.724088 3.124£0.73 3.59 +£0.66 | 0.794041\nQuestion-controlled CT 10 (boost) 2.76 £0.49 2.844094 3.6040.64 2.264097 2.944057 2.83+40.94 3.18+0.80 3.52 +0.67 | 0.724045\n\nSpecificity control (CT)\n\n2.834040 2.96+0.93 3.62+0.58 242+40.99 2604056 2.86+0.89 3.29+0.70 3.66 + 0.60 | 0.72 + 0.45\n2.90 40.36 2.78+1.00 3.60+0.64 2374093 2.664066 2.80+0.96 3.14+40.77 3.50 + 0.63 | 0.81 + 0.39\n2.9240.27 2.814088 3.65+0.59 2.3441.02 2.574062 2.80+0.78 3.25+0.78 3.50 + 0.66 | 0.86 + 0.35\n2.894032 3.00+0.94 3.64+0.67 25341.03 2.564066 2.90+0.90 3.34+40.70 3.59 + 0.60 | 0.82 + 0.39\n2.904035 2.83+0.87 3.61+0.62 2404097 2.314074 2.84+0.83 3.074081 3.58 + 0.56 | 0.88 + 0.32\n\nSpecificity control (WD)\nSpecificity-controlled WD -10 2854043 2434099 3.344083 2154091 23140.69 2.38+40.94 3.0340.75 3.33+0.70 | 0.714045\n‘ontrolled WD -4 2.90 £0.30 2.784095 3.5540.63 2414092 25240.66 2.644093 3.2840.73 3.56 +0.62 | 0.82 40.38\ncontrolled WD 4 295 40.21 2994086 3.654055 249+0.90 2.654055 3.004078 3.374059 3.63 £0.50 | 0.93 + 0.25\ncontrolled WD 6 2.93 £0.26 296+0.90 3524£0.76 24141.04 258+0.66 3.064080 3.2440.76 3.50 + 0.66 | 0.93 + 0.26\ncontrolled WD 8 2.78 £0.52 24041.23 2674 1.25 1.864097 2.034087 2554114 2.614 1.05 2.91 +£0.91 | 0.92 40.28\n\nResponse-related control (WD)\nResponse-related controlled WD -10 2.86 £0.44 2484098 3.4240.74 2.024093 23840.75 2.534094 28440.80 3.1440.75 | 0.91 40.29\nResponse-related controlled WD 0 2.96 £0.23 3.014090 3.724054 2.73+1.00 2.5640.67 2.924084 3.3740.72 3.73 £0.52 | 0.82 40.38\nResponse-related controlled WD 5 2.90 £0.33 2884090 35140.63 2414101 253+0.65 2.85+40.90 3.274£0.73 3.49 +£0.63 | 0.82 40.39\nResponse-related controlled WD 102.78 £0.43 2.394 1.04 3.064£0.90 1.974099 2.2240.67 2.57+1.01 3.0340.76 3.16 £0.63 | 0.75 £0.43\nResponse-related controlled WD 132.71 £0.57 210£1.13 2544112 1814107 2.144084 2.334 1.06 2.6940.83 2.70 £0.88 | 0.62 + 0.49\n\nTable 7: Raw scores (mean + std.) for all models and human evaluation metrics.\nThe first eight columns are Likert metrics on a 1-4 scale (except Avoiding Repetition, which is a 1-3\nscale), where higher is better (except Inquisitiveness, which has an optimal score of 3). The last column, Persona\n\nRetrieval, is on a scale from 0 to 1 where higher is better.\n\nThe maximum of each column (excluding Human row) is in bold.\n\n1721\n", "vlm_text": "The table presents a comparative analysis of different models and their performance across various dimensions. It includes models like Human, Greedy Search, Beam Search, and others categorized under Repetition control (WD), Question control (CT), Specificity control (CT), Specificity control (WD), and Response-related control (WD). \n\nEach model is evaluated on several qualitative metrics:\n\n1. Avoiding Repetition (Avoiding Rep.)\n2. Engage\n3. Fluency\n4. Humanness\n5. Inquisitive\n6. Interesting\n7. Listening\n8. Make Sense\n9. Persona\n\nThe values in each cell represent the mean score ± standard deviation for each metric. These metrics appear to assess the quality and nature of the responses generated by the models, evaluating aspects like engagement, fluency, and the ability to maintain a persona. \n\nThe \"Human\" row serves as a baseline for comparison, followed by other models which are variations or control instances aiming to evaluate specific aspects such as repetition, specificity, or response-related aspects. If you have more questions about interpreting these metrics or their implications, feel free to ask!\nThe ﬁrst eight columns are Likert metrics on a 1-4 scale (except Avoiding Repetition, which is a 1-3 scale), where higher is better (except Inquisitiveness, which has an optimal score of 3). The last column, Persona Retrieval, is on a scale from 0 to 1 where higher is better. "}
{"page": 20, "image_path": "doc_images/N19-1170_20.jpg", "ocr_text": "Model Avoiding Rep. Engage Fluency Humanness Inquisitive Interesting Listening Make Sense\n\nHuman and baselines\n\n* Human 2794012 3.044011 3.364012 3.354011 2444012 2924011 3.324013 3.684011\n* Greedy Search 2.084010 2.244011 3.034010 1.754012 1954010 2.294013 2.624010 3.23+0.10\n* Beam Search (beam size 20) 2084011 2.294011 3.094013 1.714013 2424011 2.294014 2474012 3.35+0.13\nRepetition control (WD)\n\nExtrep bigram WD -0.5 2624010 2.544012 3.354012 2134011 2634011 2564011 2.934011 3484011\nExtrep bigram WD -1.25 2.78 £0.09 2.824013 3404012 2.274012 25440.09 2.7640.10 3.054011 3.534014\nExtrep bigram WD -3.5 2834011 2.934010 3564010 243+40.11 2474011 2834010 3.144010 3.62+0.12\nExtrep bigram WD -inf 2744011 2874014 3494012 2324013 2564011 2.754012 3.134012 3.5940.12\n* Repetition-controlled baseline 2864012 2.824012 3.534010 2404011 2624013 2844012 3104011 3.584014\nQuestion control (CT)\n\nQuestion-controlled CT 0 2874012 2844013 3514010 2464011 2364009 2.7640.09 3.104010 3.49+0.12\nQuestion-controlled CT 1 282LO11 2884011 3424010 2464012 2474011 2.794013 3.144011 3.55+0.10\nQuestion-controlled CT 4 2.784012 2884010 3474011 2404009 2534013 2834013 3.244011 3.59+0.10\n* Question-controlled CT 7 2814010 2.994011 3.544009 2354011 2664012 2.924012 3.114010 347+0.10\nQuestion-controlled CT 10 2.674013 2874011 3.524012 2354012 2634012 2.664010 2.944011 3.53+0.12\nQuestion-controlled CT 10 (boost) 2684012 2.744009 3424012 2194013 2794011 2.744011 3.004012 345+0.13\nSpecificity control (CT)\n\nSpecificity-controlled CT 0 2.794010 2.934009 3.444012 2384011 2564012 2844012 3124013 3.614011\nSpecificity-controlled CT 2 27012 2.744011 3.394013 2314013 2564013 2.744012 2.994011 347+0.10\nSpecificity-controlled CT 4 2824010 2.804013 3444014 2324013 2514012 2.784015 3.094013 346+0.13\nSpecificity-controlled CT 7 2814012 2.914013 3434011 2454010 2494011 2814012 3.154012 3554011\nSpecificity-controlled CT 9 2804013 2.784010 3414012 2354013 2284011 2.794011 2914011 3.5140.12\nSpecificity control (WD)\n\nSpecificity-controlled WD -10 2764011 2414012 3194012 2154011 2284013 2354012 2894011 3.28+0.12\n\n12\nSpecificity-controlled WD -4 2.834010 2.76+0.12 3.37+0.10 2.36+0.11 2.46+0.11 2.62+0.12 3.14+0.09 3.52+0.11\n* Specificity-controlled WD 4 2.84+0.10 2.964012 34540.13 24440.12 2.56+0.09 2.944011 3.204010 3.54+0.11\nSpecificity-controlled WD 6 2.81+0.09 2.91+0.10 3.34+0.09 2.314011 2.534012 2.934012 3.09+0.10 3.41+40.12\nSpecificity-controlled WD 8 2.704011 2.394012 2.54+0.12 180+0.13 2.00+0.10 249+0.12 2.47+0.10 287+40.11\n\nResponse-related control (WD)\nResponse-related controlled WD -10 2.77 £0.12 2454012 3.264011 1.964010 2314012 2474012 2.734011 3.12+0.12\nResponse-related controlled WD 0 2874012 2974011 3.55+0.09 2.624011 2484010 2884012 3.214009 3.70+0.10\nResponse-related controlled WD 5 2.794010 2.834009 3.354012 2404012 2514013 2804013 3.134012 34140.12\nResponse-related controlled WD 10. 2.74£0.11 2.4240.12 2.934011 1.954012 2204012 2564012 2904012 3.12+0.10\nResponse-related controlled WD 132.63 £0.12 2.0640.11 240+0.09 1.744011 2074011 2.254012 2494014 263+0.10\n\nTable 8: Calibrated scores (mean + std.) for all models and human evaluation metrics.\n\nThe first eight columns are Likert metrics on a 1-4 scale (except Avoiding Repetition, which is a 1-3\nscale), where higher is better (except Inquisitiveness, which has an optimal score of 3). The last column, Persona\nRetrieval, is on a scale from 0 to 1 where higher is better.\n\nThe maximum of each column (excluding Human row) is in bold.\n\nRows marked with * are the six models included in Figure 3 (left) and Figure 4.\n\n1722\n", "vlm_text": "The table presents evaluation metrics for various models, which include human and baseline models, as well as specific model control techniques like Repetition control (WD), Question control (CT), Specificity control (CT and WD), and Response-related control (WD). Each model or control technique is assessed across several dimensions: Avoiding Repetition, Engage, Fluency, Humanness, Inquisitive, Interesting, Listening, and Make Sense. The values for each metric appear to be mean scores with standard deviations given as ± values.\n\nA few observations and highlights:\n- The \"Human\" model generally scores highest or among the highest across metrics, particularly in Humanness, Interesting, and Listening.\n- Greedy Search and Beam Search have lower scores compared to the Human model, especially in metrics such as Engage, Humanness, and Interesting.\n- Among the model control techniques, several specific models (e.g., \"Specificity-controlled WD 4\") are marked with an asterisk (*), possibly indicating a baseline or noteworthy variant within each category.\n- Question control CT 0 has notably high Engage scores, while Specificity-controlled WD variants (some marked with an asterisk) show balanced scores across the board.\n- The models aim to achieve different objectives, such as minimizing repetition or improving engagement, and the table serves to compare their effectiveness quantitatively.\nThe ﬁrst eight columns are Likert metrics on a 1-4 scale (except Avoiding Repetition, which is a 1-3 scale), where higher is better (except Inquisitiveness, which has an optimal score of 3). The last column, Persona Retrieval, is on a scale from 0 to 1 where higher is better. \nThe maximum of each column (excluding Human row) is in bold. \nRows marked with \\* are the six models included in Figure  3  (left) and Figure  4 . "}
{"page": 21, "image_path": "doc_images/N19-1170_21.jpg", "ocr_text": "H_ Plots of human evaluation results for all configurations\n\nSpecificity Control Level (WD) Response-relatedness Control Level (WD)\n\nFigure 9: Calibrated human evaluation scores for all models. This is the same data as in Table 8.\n\nNote: ‘Repetition-controlled baseline+’ in the rightmost column is ‘Response-related controlled WD 0’ in\nTable 8. See Table 5 for explanation.\n\n1723\n", "vlm_text": "The image contains multiple plots arranged in a grid format. Each plot visually represents human evaluation results for different configurations related to language model outputs. The columns are labeled with various control techniques or settings, such as \"Repetition control setting,\" \"Question-Asking Control Level (CT),\" \"Specificity Control Level (CT),\" \"Specificity Control Level (WD),\" and \"Response-relatedness Control Level (WD).\" \n\nWithin each plot, different lines represent different methods or baselines, including \"Beam search baseline,\" \"Greedy search baseline,\" \"Question-controlled CT,\" \"Specificity-controlled CT/WD,\" \"Response-related controlled WD,\" and \"Repetition-controlled baseline.\" The plots seem to evaluate these configurations across different criteria, such as 'Number of Questions,' 'Specificity Levels,' and others.\n\nEach plot has a Y-axis labeled with a metric (potentially a score or rating), while the X-axis often represents a varying factor such as the level or number of questions. Horizontal lines illustrate baselines or benchmarks for comparison. The plots seek to show how well each configuration performs under different settings, with some configurations maintaining consistently high scores while others vary more significantly based on the X-axis criteria.\nFigure 9: Calibrated human evaluation scores for all models. This is the same data as in Table  8 . \nNote: ‘Repetition-controlled baseline+’ in the rightmost column is ‘Response-related controlled WD   $_0\\cdot$   in Table  8 . See Table  5  for explanation. "}
