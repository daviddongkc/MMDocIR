{"page": 0, "image_path": "doc_images/N18-1187_0.jpg", "ocr_text": "Dialogue Learning with Human Teaching and Feedback in End-to-End\nTrainable Task-Oriented Dialogue Systems\n\nBing Liu’; Gokhan Tiir?, Dilek Hakkani-Tiir’, Pararth Shah”, Larry Heck*!\n1Carnegie Mellon University, Pittsburgh, PA, USA\n2Google Research, Mountain View, CA,USA 3Samsung Research, Mountain View, CA, USA\nliubing@cmu.edu, {dilekh, pararth}@google.com,\n{gokhan.tur, larry.heck}@ieee.org\n\nAbstract\n\nIn this work, we present a hybrid learn-\ning method for training task-oriented dialogue\nsystems through online user interactions. Pop-\nular methods for learning task-oriented dia-\nlogues include applying reinforcement learn-\ning with user feedback on supervised pre-\ntraining models. Efficiency of such learning\nmethod may suffer from the mismatch of di-\nalogue state distribution between offline train-\ning and online interactive learning stages. To\naddress this challenge, we propose a hybrid\nimitation and reinforcement learning method,\nwith which a dialogue agent can effectively\nlearn from its interaction with users by learn-\ning from human teaching and feedback. We\ndesign a neural network based task-oriented\ndialogue agent that can be optimized end-to-\nend with the proposed learning method. Ex-\nperimental results show that our end-to-end\ndialogue agent can learn effectively from the\nmistake it makes via imitation learning from\nuser teaching. Applying reinforcement learn-\ning with user feedback after the imitation\nlearning stage further improves the agent’s ca-\npability in successfully completing a task.\n\n1 Introduction\n\nTask-oriented dialogue systems assist users to\ncomplete tasks in specific domains by understand-\ning user’s request and aggregate useful informa-\ntion from external resources within several dia-\nlogue turns. Conventional task-oriented dialogue\nsystems have a complex pipeline (Rudnicky et al.,\n1999; Raux et al., 2005; Young et al., 2013) con-\nsisting of independently developed and modularly\nconnected components for natural language un-\nderstanding (NLU) (Mesnil et al., 2015; Liu and\nLane, 2016; Hakkani-Tiir et al., 2016), dialogue\nstate tracking (DST) (Henderson et al., 2014c;\n\n“Work done while the author was an intern at Google.\n+ Work done while at Google Research.\n\nMrkSic¢ et al., 2016), and dialogue policy learn-\ning (Gasic and Young, 2014; Shah et al., 2016; Su\net al., 2016, 2017). These system components are\nusually trained independently, and their optimiza-\ntion targets may not fully align with the overall\nsystem evaluation criteria (e.g. task success rate\nand user satisfaction). Moreover, errors made in\nthe upper stream modules of the pipeline propa-\ngate to downstream components and get amplified,\nmaking it hard to track the source of errors.\n\nTo address these limitations with the con-\nventional task-oriented dialogue systems, re-\ncent efforts have been made in designing end-\nto-end learning solutions with neural network\nbased methods. Both supervised learning (SL)\nbased (Wen et al., 2017; Bordes and Weston,\n2017; Liu and Lane, 2017a) and deep reinforce-\nment learning (RL) based systems (Zhao and Es-\nkenazi, 2016; Li et al., 2017; Peng et al., 2017)\nhave been studied in the literature. Comparing to\nchit-chat dialogue models that are usually trained\noffline using single-turn context-response pairs,\ntask-oriented dialogue model involves reasoning\nand planning over multiple dialogue turns. This\nmakes it especially important for a system to be\nable to learn from users in an interactive manner.\nComparing to SL models, systems trained with\nRL by receiving feedback during users interac-\ntions showed improved model robustness against\ndiverse dialogue scenarios (Williams and Zweig,\n2016; Liu and Lane, 2017b).\n\nA critical step in learning RL based task-\noriented dialogue models is dialogue policy learn-\ning. Training dialogue policy online from scratch\ntypically requires a large number of interactive\nlearning sessions before an agent can reach a satis-\nfactory performance level. Recent works (Hender-\nson et al., 2008; Williams et al., 2017; Liu et al.,\n2017) explored pre-training the dialogue model\nusing human-human or human-machine dialogue\n\n2060\n\nProceedings of NAACL-HLT 2018, pages 2060-2069\nNew Orleans, Louisiana, June 1 - 6, 2018. ©2018 Association for Computational Linguistics\n", "vlm_text": "Dialogue Learning with Human Teaching and Feedback in End-to-End Trainable Task-Oriented Dialogue Systems \nBing Liu 1 , Gokhan T¨ ur 2 , Dilek Hakkani-T¨ ur 2 , Pararth Shah 2 , Larry Heck 3 † 1 Carnegie Mellon University, Pittsburgh, PA, USA \n2 Google Research, Mountain View, CA,USA  3 Samsung Research, Mountain View, CA, USA liubing@cmu.edu ,  { dilekh,pararth } @google.com , { gokhan.tur,larry.heck } @ieee.org \nAbstract \nIn this work, we present a hybrid learn- ing method for training task-oriented dialogue systems through online user interactions. Pop- ular methods for learning task-oriented dia- logues include applying reinforcement learn- ing with user feedback on supervised pre- training models. Efﬁciency of such learning method may suffer from the mismatch of di- alogue state distribution between ofﬂine train- ing and online interactive learning stages. To address this challenge, we propose a hybrid imitation and reinforcement learning method, with which a dialogue agent can effectively learn from its interaction with users by learn- ing from human teaching and feedback. We design a neural network based task-oriented dialogue agent that can be optimized end-to- end with the proposed learning method. Ex- perimental results show that our end-to-end dialogue agent can learn effectively from the mistake it makes via imitation learning from user teaching. Applying reinforcement learn- ing with user feedback after the imitation learning stage further improves the agent’s ca- pability in successfully completing a task. \n1 Introduction \nTask-oriented dialogue systems assist users to complete tasks in speciﬁc domains by understand- ing user’s request and aggregate useful informa- tion from external resources within several dia- logue turns. Conventional task-oriented dialogue systems have a complex pipeline ( Rudnicky et al. , 1999 ;  Raux et al. ,  2005 ;  Young et al. ,  2013 ) con- sisting of independently developed and modularly connected components for natural language un- derstanding (NLU) ( Mesnil et al. ,  2015 ;  Liu and Lane ,  2016 ;  Hakkani-T¨ ur et al. ,  2016 ), dialogue state tracking (DST) ( Henderson et al. ,  2014c ; \nMrkˇ si´ c et al. ,  2016 ), and dialogue policy learn- ing ( Gasic and Young ,  2014 ;  Shah et al. ,  2016 ;  Su et al. ,  2016 ,  2017 ). These system components are usually trained independently, and their optimiza- tion targets may not fully align with the overall system evaluation criteria (e.g. task success rate and user satisfaction). Moreover, errors made in the upper stream modules of the pipeline propa- gate to downstream components and get ampliﬁed, making it hard to track the source of errors. \nTo address these limitations with the con- ventional task-oriented dialogue systems, re- cent efforts have been made in designing end- to-end learning solutions with neural network based methods. Both supervised learning (SL) based ( Wen et al. ,  2017 ;  Bordes and Weston , 2017 ;  Liu and Lane ,  2017a ) and deep reinforce- ment learning (RL) based systems ( Zhao and Es- kenazi ,  2016 ;  Li et al. ,  2017 ;  Peng et al. ,  2017 ) have been studied in the literature. Comparing to chit-chat dialogue models that are usually trained ofﬂine using single-turn context-response pairs, task-oriented dialogue model involves reasoning and planning over multiple dialogue turns. This makes it especially important for a system to be able to learn from users in an interactive manner. Comparing to SL models, systems trained with RL by receiving feedback during users interac- tions showed improved model robustness against diverse dialogue scenarios ( Williams and Zweig , 2016 ;  Liu and Lane ,  2017b ). \nA critical step in learning RL based task- oriented dialogue models is dialogue policy learn- ing. Training dialogue policy online from scratch typically requires a large number of interactive learning sessions before an agent can reach a satis- factory performance level. Recent works ( Hender- son et al. ,  2008 ;  Williams et al. ,  2017 ;  Liu et al. , 2017 ) explored pre-training the dialogue model using human-human or human-machine dialogue corpora before performing interactive learning with RL to address this concern. A potential draw- back with such pre-training approach is that the model may suffer from the mismatch of dialogue state distributions between supervised training and interactive learning stages. While interacting with users, the agent’s response at each turn has a di- rect inﬂuence on the distribution of dialogue state that the agent will operate on in the upcoming di- alogue turns. If the agent makes a small mistake and reaches an unfamiliar state, it may not know how to recover from it and get back to a normal dialogue trajectory. This is because such recovery situation may be rare for good human agents and thus are not well covered in the supervised train- ing corpus. This will result in compounding er- rors in a dialogue which may lead to failure of a task. RL exploration might ﬁnally help to ﬁnd cor- responding actions to recover from a bad state, but the search process can be very inefﬁcient. "}
{"page": 1, "image_path": "doc_images/N18-1187_1.jpg", "ocr_text": "corpora before performing interactive learning\nwith RL to address this concern. A potential draw-\nback with such pre-training approach is that the\nmodel may suffer from the mismatch of dialogue\nstate distributions between supervised training and\ninteractive learning stages. While interacting with\nusers, the agent’s response at each turn has a di-\nrect influence on the distribution of dialogue state\nthat the agent will operate on in the upcoming di-\nalogue turns. If the agent makes a small mistake\nand reaches an unfamiliar state, it may not know\nhow to recover from it and get back to a normal\ndialogue trajectory. This is because such recovery\nsituation may be rare for good human agents and\nthus are not well covered in the supervised train-\ning corpus. This will result in compounding er-\nrors in a dialogue which may lead to failure of a\ntask. RL exploration might finally help to find cor-\nresponding actions to recover from a bad state, but\nthe search process can be very inefficient.\n\nTo ameliorate the effect of dialogue state distri-\nbution mismatch between offline training and RL\ninteractive learning, we propose a hybrid imitation\nand reinforcement learning method. We first let\nthe agent to interact with users using its own pol-\nicy learned from supervised pre-training. When an\nagent makes a mistake, we ask users to correct the\nmistake by demonstrating the agent the right ac-\ntions to take at each turn. This user corrected dia-\nlogue sample, which is guided by the agent’s own\npolicy, is then added to the existing training cor-\npus. We fine-tune the dialogue policy with this di-\nalogue sample aggregation (Ross et al., 2011) and\ncontinue such user teaching process for a number\nof cycles. Since asking for user teaching at each\ndialogue turn is costly, we want to reduce this user\nteaching cycles as much as possible and continue\nthe learning process with RL by collecting simple\nforms of user feedback (e.g. a binary feedback,\npositive or negative) only at the end of a dialogue.\n\nOur main contributions in this work are:\n\ne We design a neural network based task-\noriented dialogue system which can be op-\ntimized end-to-end for natural language un-\nderstanding, dialogue state tracking, and dia-\nlogue policy learning.\n\ne We propose a hybrid imitation and reinforce-\nment learning method for end-to-end model\ntraining in addressing the challenge with dia-\nlogue state distribution mismatch between of-\nfline training and interactive learning.\n\nThe remainder of the paper is organized as fol-\nlows. In section 2, we discuss related work in\nbuilding end-to-end task-oriented dialogue sys-\ntems. In section 3, we describe the proposed\nmodel and learning method in detail. In Section\n4, we describe the experiment setup and discuss\nthe results. Section 5 gives the conclusions.\n\n2 Related Work\n\nPopular approaches in learning task-oriented\ndialogue include modeling the task as a par-\ntially observable Markov Decision Process\n(POMDP) (Young et al., 2013). RL can be applied\nin the POMDP framework to learn dialogue\npolicy online by interacting with users (Gasi¢é\net al., 2013). The dialogue state and system action\nspace have to be carefully designed in order to\nmake the policy learning tractable (Young et al.,\n2013), which limits the model’s usage to restricted\ndomains.\n\nRecent efforts have been made in designing\nend-to-end solutions for task-oriented dialogues,\ninspired by the success of encoder-decoder based\nneural network models in non-task-oriented con-\nversational systems (Serban et al., 2015; Li et al.,\n2016). Wen et al. (Wen et al., 2017) designed an\nend-to-end trainable neural dialogue model with\nmodularly connected system components. This\nsystem is a supervised learning model which is\nevaluated on fixed dialogue corpora. It is un-\nknown how well the model performance gener-\nalizes to unseen dialogue state during user inter-\nactions. Our system is trained by a combina-\ntion of supervised and deep RL methods, as it is\nshown that RL may effectively improve dialogue\nsuccess rate by exploring a large dialogue action\nspace (Henderson et al., 2008; Li et al., 2017).\n\nBordes and Weston (2017) proposed a task-\noriented dialogue model using end-to-end memory\nnetworks. In the same line of research, people ex-\nplored using query-regression networks (Seo et al.,\n2016), gated memory networks (Liu and Perez,\n2017), and copy-augmented networks (Eric and\nManning, 2017) to learn the dialogue state. These\nsystems directly select a final response from a list\nof response candidates conditioning on the dia-\nlogue history without doing slot filling or user goal\ntracking. Our model, on the other hand, explic-\nitly tracks user’s goal for effective integration with\nknowledge bases (KBs). Robust dialogue state\n\ntracking has been shown (Jurciéek et al., 2012) to\n\n2061\n", "vlm_text": "\nTo ameliorate the effect of dialogue state distri- bution mismatch between ofﬂine training and RL interactive learning, we propose a hybrid imitation and reinforcement learning method. We ﬁrst let the agent to interact with users using its own pol- icy learned from supervised pre-training. When an agent makes a mistake, we ask users to correct the mistake by demonstrating the agent the right ac- tions to take at each turn. This user corrected dia- logue sample, which is guided by the agent’s own policy, is then added to the existing training cor- pus. We ﬁne-tune the dialogue policy with this di- alogue sample aggregation ( Ross et al. ,  2011 ) and continue such user teaching process for a number of cycles. Since asking for user teaching at each dialogue turn is costly, we want to reduce this user teaching cycles as much as possible and continue the learning process with RL by collecting simple forms of user feedback (e.g. a binary feedback, positive or negative) only at the end of a dialogue. Our main contributions in this work are: \n•  We design a neural network based task- oriented dialogue system which can be op- timized end-to-end for natural language un- derstanding, dialogue state tracking, and dia- logue policy learning. \n•  We propose a hybrid imitation and reinforce- ment learning method for end-to-end model training in addressing the challenge with dia- logue state distribution mismatch between of- ﬂine training and interactive learning. \nThe remainder of the paper is organized as fol- lows. In section 2, we discuss related work in building end-to-end task-oriented dialogue sys- tems. In section 3, we describe the proposed model and learning method in detail. In Section 4, we describe the experiment setup and discuss the results. Section 5 gives the conclusions. \n2 Related Work \nPopular approaches in learning task-oriented dialogue include modeling the task as a par- tially observable Markov Decision Process (POMDP) ( Young et al. ,  2013 ). RL can be applied in the POMDP framework to learn dialogue policy online by interacting with users ( Gaˇ si´ et al. ,  2013 ). The dialogue state and system action space have to be carefully designed in order to make the policy learning tractable ( Young et al. , 2013 ), which limits the model’s usage to restricted domains. \nRecent efforts have been made in designing end-to-end solutions for task-oriented dialogues, inspired by the success of encoder-decoder based neural network models in non-task-oriented con- versational systems ( Serban et al. ,  2015 ;  Li et al. , 2016 ). Wen et al. ( Wen et al. ,  2017 ) designed an end-to-end trainable neural dialogue model with modularly connected system components. This system is a supervised learning model which is evaluated on ﬁxed dialogue corpora. It is un- known how well the model performance gener- alizes to unseen dialogue state during user inter- actions. Our system is trained by a combina- tion of supervised and deep RL methods, as it is shown that RL may effectively improve dialogue success rate by exploring a large dialogue action space ( Henderson et al. ,  2008 ;  Li et al. ,  2017 ). \nBordes and Weston ( 2017 ) proposed a task- oriented dialogue model using end-to-end memory networks. In the same line of research, people ex- plored using query-regression networks ( Seo et al. , 2016 ), gated memory networks ( Liu and Perez , 2017 ), and copy-augmented networks ( Eric and Manning ,  2017 ) to learn the dialogue state. These systems directly select a ﬁnal response from a list of response candidates conditioning on the dia- logue history without doing slot ﬁlling or user goal tracking. Our model, on the other hand, explic- itly tracks user’s goal for effective integration with knowledge bases (KBs). Robust dialogue state tracking has been shown ( Jurˇ c´ ıˇ cek et al. ,  2012 ) to be critical in improving dialogue success in task completion. "}
{"page": 2, "image_path": "doc_images/N18-1187_2.jpg", "ocr_text": "be critical in improving dialogue success in task\ncompletion.\n\nDhingra et al. (2017) proposed an end-to-end\nRL dialogue agent for information access. Their\nmodel focuses on bringing differentiability to the\nKB query operation by introducing a “soft” re-\ntrieval process in selecting the KB entries. Such\nsoft-KB lookup is prone to entity updates and ad-\nditions in the KB, which is common in real world\ninformation systems. In our model, we use sym-\nbolic queries and leave the selection of KB enti-\nties to external services (e.g. a recommender sys-\ntem), as entity ranking in real world systems can\nbe made with much richer features (e.g. user pro-\nfiles, location and time context, etc.). Quality of\nthe generated symbolic query is directly related\nto the belief tracking performance. In our pro-\nposed end-to-end system, belief tracking can be\noptimized together with other system components\n(e.g. language understanding and policy) during\ninteractive learning with users.\n\nWilliams et al. (2017) proposed a hybrid code\nnetwork for task-oriented dialogue that can be\ntrained with supervised and reinforcement learn-\ning. They show that RL performed with a super-\nvised pre-training model using labeled dialogues\nimproves learning speed dramatically. They did\nnot discuss the potential issue of dialogue state\ndistribution mismatch between supervised pre-\ntraining and RL interactive learning, which is ad-\ndressed in our dialogue learning framework.\n\n3 Proposed Method\n\nFigure 1 shows the overall system architecture\nof the proposed end-to-end task-oriented dialogue\nmodel. We use a hierarchical LSTM neural net-\nwork to encode a dialogue with a sequence of\nturns. User input to the system in natural lan-\nguage format is encoded to a continuous vector via\na bidirectional LSTM utterance encoder. This user\nutterance encoding, together with the encoding of\nthe previous system action, serves as the input to a\ndialogue-level LSTM. State of this dialogue-level\nLSTM maintains a continuous representation of\nthe dialogue state. Based on this state, the model\ngenerates a probability distribution over candidate\nvalues for each of the tracked goal slots. A query\ncommand can then be formulated with the state\ntracking outputs and issued to a knowledge base to\nretrieve requested information. Finally, the system\nproduces a dialogue action, which is conditioned\n\non information from the dialogue state, the esti-\nmated user’s goal, and the encoding of the query\nresults . This dialogue action, together with the\nuser goal tracking results and the query results, is\nused to generate the final natural language system\nresponse via a natural language generator (NLG).\nWe describe each core model component in detail\nin the following sections.\n\n3.1 Utterance Encoding\n\nWe use a bidirectional LSTM to encode the user\nutterance to a continuous representation. We refer\nto this LSTM as the utterance-level LSTM. The\nuser utterance vector is generated by concatenat-\ning the last forward and backward LSTM states.\nLet U, = (wi, we, ..., w7,) be the user utterance\nat turn k with T; words. These words are firstly\nmapped to an embedding space, and further serve\nas the step inputs to the bidirectional LSTM. Let\nhy and hy represent the forward and backward\nLSTM state outputs at time step t. The user ut-\nterance vector U;, is produced by: U, = (hr, ial,\nwhere hy, and iy are the last states in the forward\nand backward LSTMs.\n\n3.2. Dialogue State Tracking\n\nDialogue state tracking, or belief tracking, main-\nains the state of a conversation, such as user’s\ngoals, by accumulating evidence along the se-\nquence of dialogue turns. Our model maintains\nhe dialogue state in a continuous form in the\ndialogue-level LSTM (LSTMp) state sj. sx is up-\ndated after the model processes each dialogue turn\nby taking in the encoding of user utterance U; and\nhe encoding of the previous turn system output\nAj_1. This dialogue state serves as the input to the\ndialogue state tracker. The tracker updates its es-\nimation of the user’s goal represented by a list of\nslot-value pairs. A probability distribution P(I7’)\nis maintained over candidate values for each goal\nslot typem € M:\n\n8p = LSTMp(sg-1, (Uk, Ar—i]) (1)\nwhere SlotDist,, is a single hidden layer MLP\nwith softmax activation over slot type m € M.\n\n3.3, KB Operation\n\nThe dialogue state tracking outputs are used to\nform an API call command to retrieve information\nfrom a knowledge base. The API call command is\n\n2062\n", "vlm_text": "\nDhingra et al. ( 2017 ) proposed an end-to-end RL dialogue agent for information access. Their model focuses on bringing different i ability to the KB query operation by introducing a “soft” re- trieval process in selecting the KB entries. Such soft-KB lookup is prone to entity updates and ad- ditions in the KB, which is common in real world information systems. In our model, we use sym- bolic queries and leave the selection of KB enti- ties to external services (e.g. a recommender sys- tem), as entity ranking in real world systems can be made with much richer features (e.g. user pro- ﬁles, location and time context, etc.). Quality of the generated symbolic query is directly related to the belief tracking performance. In our pro- posed end-to-end system, belief tracking can be optimized together with other system components (e.g. language understanding and policy) during interactive learning with users. \nWilliams et al. ( 2017 ) proposed a hybrid code network for task-oriented dialogue that can be trained with supervised and reinforcement learn- ing. They show that RL performed with a super- vised pre-training model using labeled dialogues improves learning speed dramatically. They did not discuss the potential issue of dialogue state distribution mismatch between supervised pre- training and RL interactive learning, which is ad- dressed in our dialogue learning framework. \n3 Proposed Method \nFigure  1  shows the overall system architecture of the proposed end-to-end task-oriented dialogue model. We use a hierarchical LSTM neural net- work to encode a dialogue with a sequence of turns. User input to the system in natural lan- guage format is encoded to a continuous vector via a bidirectional LSTM utterance encoder. This user utterance encoding, together with the encoding of the previous system action, serves as the input to a dialogue-level LSTM. State of this dialogue-level LSTM maintains a continuous representation of the dialogue state. Based on this state, the model generates a probability distribution over candidate values for each of the tracked goal slots. A query command can then be formulated with the state tracking outputs and issued to a knowledge base to retrieve requested information. Finally, the system produces a dialogue action, which is conditioned on information from the dialogue state, the esti- mated user’s goal, and the encoding of the query results . This dialogue action, together with the user goal tracking results and the query results, is used to generate the ﬁnal natural language system response via a natural language generator (NLG). We describe each core model component in detail in the following sections. \n\n3.1 Utterance Encoding \nWe use a bidirectional LSTM to encode the user utterance to a continuous representation. We refer to this LSTM as the utterance-level LSTM. The user utterance vector is generated by concatenat- ing the last forward and backward LSTM states. Let    $\\mathbf{U}_{k}=\\left(w_{1},w_{2},...,w_{T_{k}}\\right)$   be the user utterance at turn    $k$   with    $T_{k}$   words. These words are ﬁrstly mapped to an embedding space, and further serve as the step inputs to the bidirectional LSTM. Let →  and    $\\hat{h}_{t}$   represent the forward and backward LSTM state outputs at time step    $t$  . The user ut- terance vector    $U_{k}$   is produced by:    $U_{k}=[\\overrightarrow{h_{T_{k}}},\\overleftarrow{h_{1}}]$    , where  $\\overrightarrow{h_{T_{k}}}$   and  $\\overleftarrow{h_{1}}$   are the last states in the forward and backward LSTMs. \n3.2 Dialogue State Tracking \nDialogue state tracking, or belief tracking, main- tains the state of a conversation, such as user’s goals, by accumulating evidence along the se- quence of dialogue turns. Our model maintains the dialogue state in a continuous form in the dialogue-level LSTM   $(\\mathrm{LSTM_{D}})$  ) state    $s_{k}$  .    $s_{k}$   is up- dated after the model processes each dialogue turn by taking in the encoding of user utterance    $U_{k}$   and the encoding of the previous turn system output  $A_{k-1}$  . This dialogue state serves as the input to the dialogue state tracker. The tracker updates its es- timation of the user’s goal represented by a list of slot-value pairs. A probability distribution    $P(l_{k}^{m})$  is maintained over candidate values for each goal slot type    $m\\in M$  : \n\n$$\n\\begin{array}{r l}&{s_{k}=\\mathrm{{LSTM}_{D}}(s_{k-1},\\ [U_{k},\\ A_{k-1}])}\\\\ &{P(l_{k}^{m}\\mid\\mathbf{U}_{\\le k},\\ \\mathbf{A}_{<k})=\\mathrm{{Slat}D i s t}_{m}(s_{k})}\\end{array}\n$$\n \nwhere    $\\mathrm{StotDiss}_{m}$   is a single hidden layer MLP with  softmax  activation over slot type    $m\\in M$  . \n3.3 KB Operation \nThe dialogue state tracking outputs are used to form an API call command to retrieve information from a knowledge base. The API call command is "}
{"page": 3, "image_path": "doc_images/N18-1187_3.jpg", "ocr_text": "System dialogue act\nembedding at turn k-1\n\nDialogue State ‘\nTracking 4\n\n| date=Thursday\n\nBi-LSTM\n\nLSTM Knowledge\n\nBase\n\nSystem: Ok, what\ntime do you prefer?\n\nNatural Language\nGenerator\n\nUser: Movie for the day\nafter tomorrow, please Utterance Encoder Dialogue State\nUser utterance\n\nencoding at turn k\n\n+— request(time)\n\n‘time=none\n\nQuery results\nSystem dialogue act encoding\n\nat turn k\n\nPolicy\nNetwork\n\nFigure 1: Proposed end-to-end task-oriented dialogue system architecture.\n\nproduced by replacing the tokens in a query com-\nmand template with the best hypothesis for each\ngoal slot from the dialogue state tracking output.\nAlternatively, an n-best list of API calls can be\ngenerated with the most probable candidate values\nfor the tracked goal slots. In interfacing with KBs,\ninstead of using a soft KB lookup as in (Dhingra\net al., 2017), our model sends symbolic queries to\nthe KB and leaves the ranking of the KB entities\nto an external recommender system. Entity rank-\ning in real world systems can be made with much\nricher features (e.g. user profiles, local context,\netc.) in the back-end system other than just fol-\nlowing entity posterior probabilities conditioning\non a user utterance. Hence ranking of the KB en-\ntities is not a part of our proposed neural dialogue\nmodel. In this work, we assume that the model re-\nceives a ranked list of KB entities according to the\nissued query and other available sources, such as\nuser models.\n\nOnce the KB query results are returned, we save\nthe retrieved entities to a queue and encode the re-\nsult summary to a vector. Rather then encoding the\nreal KB entity values as in (Bordes and Weston,\n2017; Eric and Manning, 2017), we only encode a\nsummary of the query results (i.e. item availabil-\nity and number of matched items). This encoding\nserves as a part of the input to the policy network.\n\n3.4 Dialogue Policy\n\nA dialogue policy selects the next system action\nin response to the user’s input based on the cur-\nrent dialogue state. We use a deep neural network\nto model the dialogue policy. There are three in-\nputs to the policy network, (1) the dialogue-level\nLSTM state s;,, (2) the log probabilities of candi-\ndate values from the belief tracker v;,, and (3) the\n\nSystem action\n\nattumk\n\n} Policy Network\n\nSlot value logi\n\nEx\n\nQuery results\nencoding\n\nLSTM Dialogue State, Sf;\n\nFigure 2: Dialogue state and policy network.\n\nencoding of the query results summary E;. The\npolicy network emits a system action in the form\nof a dialogue act conditioning on these inputs:\n\nPlax | Uck, Ack, Er) = PolicyNet(s:, vz, Ex)\n(3)\n\nwhere v;, represents the concatenated log probabil-\nities of candidate values for each goal slot, E;, is\nthe encoding of query results, and PolicyNet is a\nsingle hidden layer MLP with softmax activation\nfunction over all system actions.\n\nThe emitted system action is finally used to pro-\nduce a system response in natural language format\nby combining the state tracker outputs and the re-\ntrieved KB entities. We use a template based NLG\nin this work. The delexicalised tokens in the NLG\ntemplate are replaced by the values from either the\nestimated user goal values or the KB entities, de-\npending on the emitted system action.\n\n3.5 Supervised Pre-training\n\nBy connecting all the system components, we have\nan end-to-end model for task-oriented dialogue.\nEach system component is a neural network that\ntakes in underlying system component’s outputs\n\n2063\n", "vlm_text": "The image depicts a proposed architecture for an end-to-end task-oriented dialogue system. Here's a breakdown of the components and flow within the system:\n\n1. **User Input**: The dialogue starts with a user input, shown as \"User: Movie for the day after tomorrow, please\".\n\n2. **Bi-LSTM Utterance Encoder**: The user input is processed through a bidirectional Long Short-Term Memory (Bi-LSTM) encoder to generate an encoding of the user's utterance at turn k.\n\n3. **System Dialogue Act Embedding**: The system includes an embedding of the dialogue act performed by the system at the previous turn (k-1).\n\n4. **LSTM Dialogue State**: This component processes the user utterance encoding along with the previous system dialogue act embedding to update the dialogue state.\n\n5. **Dialogue State Tracking**: The updated dialogue state is tracked, identifying slots or variables like \"date\" which is set to \"Thursday\", and \"time\" which is set to \"none\".\n\n6. **Knowledge Base**: The tracked dialogue state can query an external knowledge base to retrieve relevant information, resulting in a query result encoding.\n\n7. **Policy Network**: Based on the dialogue state and query results, the system uses a policy network to determine the next system dialogue act at turn k, which in this case is \"request(time)\".\n\n8. **Natural Language Generator**: This takes the determined system dialogue act and generates a natural language response, shown as \"System: Ok, what time do you prefer?\".\n\nThe system is designed to process user requests and generate relevant responses in a task-oriented manner, utilizing components like encoders, a policy network, and a natural language generator to manage dialogue turns.\nproduced by replacing the tokens in a query com- mand template with the best hypothesis for each goal slot from the dialogue state tracking output. Alternatively, an n-best list of API calls can be generated with the most probable candidate values for the tracked goal slots. In interfacing with KBs, instead of using a soft KB lookup as in ( Dhingra et al. ,  2017 ), our model sends symbolic queries to the KB and leaves the ranking of the KB entities to an external recommender system. Entity rank- ing in real world systems can be made with much richer features (e.g. user proﬁles, local context, etc.) in the back-end system other than just fol- lowing entity posterior probabilities conditioning on a user utterance. Hence ranking of the KB en- tities is not a part of our proposed neural dialogue model. In this work, we assume that the model re- ceives a ranked list of KB entities according to the issued query and other available sources, such as user models. \nOnce the KB query results are returned, we save the retrieved entities to a queue and encode the re- sult summary to a vector. Rather then encoding the real KB entity values as in ( Bordes and Weston , 2017 ;  Eric and Manning ,  2017 ), we only encode a summary of the query results (i.e. item availabil- ity and number of matched items). This encoding serves as a part of the input to the policy network. \n3.4 Dialogue Policy \nA dialogue policy selects the next system action in response to the user’s input based on the cur- rent dialogue state. We use a deep neural network to model the dialogue policy. There are three in- puts to the policy network, (1) the dialogue-level LSTM state    $s_{k}$  , (2) the log probabilities of candi- date values from the belief tracker  $v_{k}$  , and (3) the \nThe image depicts a high-level architecture of a dialogue state and policy network used in a dialogue system. The key components of the diagram are:\n\n1. **LSTM Dialogue State (`s_k`)**: At the bottom of the diagram, an LSTM (Long Short-Term Memory) network is used to manage and update the dialogue state, denoted as `s_k`.\n\n2. **Query Results Encoding (`E_k`)**: This component encodes the results of queries, which may be used to inform the policy network. It's shown on the right and feeds into the policy network.\n\n3. **Slot Value Logits (`v_k`)**: This component generates logits for slot values, which are inputs into the policy network. It forms a connection between the LSTM dialogue state and the policy network.\n\n4. **Policy Network**: Consists of a series of processing layers that take inputs from both the slot value logits and the LSTM dialogue state, aiming to produce a suitable system action at a given turn, labeled as `a_k`.\n\n5. **System Action at Turn (`a_k`)**: The output of the policy network is the action taken by the system at a specific turn in the dialogue, which is shown at the top of the diagram.\n\nOverall, the design represents a framework for selecting system actions in a dialogue based on the current state and relevant inputs.\nencoding of the query results summary    $E_{k}$  . The policy network emits a system action in the form of a dialogue act conditioning on these inputs: \n\n$$\nP(a_{k}\\mid U_{\\leq k},\\;A_{<k},\\;E_{\\leq k})=\\mathrm{PoisyNet}(s_{k},v_{k},E_{k})\n$$\n \nwhere    $v_{k}$   represents the concatenated log probabil- ities of candidate values for each goal slot,    $E_{k}$   is the encoding of query results, and  PolicyNet  is a single hidden layer MLP with  softmax  activation function over all system actions. \nThe emitted system action is ﬁnally used to pro- duce a system response in natural language format by combining the state tracker outputs and the re- trieved KB entities. We use a template based NLG in this work. The delexicalised tokens in the NLG template are replaced by the values from either the estimated user goal values or the KB entities, de- pending on the emitted system action. \n3.5 Supervised Pre-training \nBy connecting all the system components, we have an end-to-end model for task-oriented dialogue. Each system component is a neural network that takes in underlying system component’s outputs in a continuous form that is fully differentiable, and the entire system (utterance encoding, dia- logue state tracking, and policy network) can be trained end-to-end. "}
{"page": 4, "image_path": "doc_images/N18-1187_4.jpg", "ocr_text": "in a continuous form that is fully differentiable,\nand the entire system (utterance encoding, dia-\nlogue state tracking, and policy network) can be\ntrained end-to-end.\n\nWe first train the system in a supervised man-\nner by fitting task-oriented dialogue samples. The\nmodel predicts the true user goal slot values and\nthe next system action at each turn of a dia-\nlogue. We optimize the model parameter set 6 by\nminimizing a linear interpolation of cross-entropy\nlosses for dialogue state tracking and system ac-\ntion prediction:\n\nK M\nmin - [ S> Am log PUP\" |W <i, Acts Eck: 8)\nk=1 m=1\n+g log P(ak|U <p, Ack, Bp: 9) |\n\n(4)\n\nwhere s are the linear interpolation weights for\nthe cost of each system output. 1;\"* is the ground\ntruth label for the tracked user goal slot type m €\nM at the kth turn, and aj, is the true system action\nin the corpus.\n\n3.6 Imitation Learning with Human\nTeaching\n\nOnce obtaining a supervised training dialogue\nagent, we further let the agent to learn interactively\nfrom users by conducting task-oriented dialogues.\nSupervised learning succeeds when training and\ntest data distributions match. During the agent’s\ninteraction with users, any mistake made by the\nagent or any deviation in the user’s behavior may\nlead to a different dialogue state distribution than\nthe one that the supervised learning agent saw dur-\ning offline training. A small mistake made by the\nagent due to this covariate shift (Ross and Bagnell,\n2010; Ross et al., 2011) may lead to compound-\ning errors which finally lead to failure of a task.\nTo address this issue, we propose a dialogue imi-\ntation learning method which allows the dialogue\nagent to learn from human teaching. We let the\nsupervised training agent to interact with users us-\ning its learned dialogue policy 79(a|s). With this,\nwe collect additional dialogue samples that are\nguided by the agent’s own policy, rather than by\nthe expert policy as those in the supervised train-\ning corpora. When the agent make mistakes, we\nask users to correct the mistakes and demonstrate\nthe expected actions and predictions for the agent\nto make. Such user teaching precisely addresses\n\n2064\n\nAlgorithm 1 Dialogue Learning with Human\nTeaching and Feedback\n1: Train model end-to-end on dialogue samples\nD with MLE and obtain policy 79(a|s) > eq 4\n2: for learning iteration k = 1: K do\n\n3: Run zo(als) with user to collect new\ndialogue samples D;\n\n4: Ask user to correct the mistakes in the\ntracked user’s goal for each dialogue turn\nin D,\n\n5: Add the newly labeled dialogue samples\nto the existing corpora: D + DUD;\n\n6: Train model end-to-end on D and obtain\nan updated policy (as) peq4\n\n7: end for\n\n8: for learning iteration k = 1: N do\n\n9: Run 79(a|s) with user for a new dialogue\n\n10: Collect user feedback as reward r\n\nIl: Update model end-to-end and obtain an\nupdated policy z9(a|s) peq 5\n\n12: end for\n\nthe limitations of the currently learned dialogue\nmodel, as these newly collected dialogue samples\nare driven by the agent’s own policy. Specifically,\nin this study we let an expert user to correct the\nmistake made by the agent in tracking the user’s\ngoal at the end of each dialogue turn. This new\nbatch of annotated dialogues are then added to the\nexisting training corpus. We start the next round\nof supervised model training on this aggregated\ncorpus to obtain an updated dialogue policy, and\ncontinue this dialogue imitation learning cycles.\n\n3.7 Reinforcement Learning with Human\nFeedback\n\nLearning from human teaching can be costly, as\nit requires expert users to provide corrections at\neach dialogue turn. We want to minimize the num-\nber of such imitation dialogue learning cycles and\ncontinue to improve the agent via a form of super-\nvision signal that is easier to obtain. After the imi-\nation learning stage, we further optimize the neu-\nral dialogue system with RL by letting the agent\n0 interact with users and learn from user feed-\nback. Different from the turn-level corrections in\nhe imitation dialogue learning stage, the feedback\nis only collected at the end of a dialogue. A pos-\nitive reward is collected for successful tasks, and\na zero reward is collected for failed tasks. A step\npenalty is applied to each dialogue turn to encour-\n\n", "vlm_text": "\nWe ﬁrst train the system in a supervised man- ner by ﬁtting task-oriented dialogue samples. The model predicts the true user goal slot values and the next system action at each turn of a dia- logue. We optimize the model parameter set  $\\theta$   by minimizing a linear interpolation of cross-entropy losses for dialogue state tracking and system ac- tion prediction: \n\n$$\n\\begin{array}{r l r}{\\lefteqn{\\operatorname*{min}_{\\theta}\\sum_{k=1}^{K}-\\Big[\\sum_{m=1}^{M}\\lambda_{l^{m}}\\log P(l_{k}^{m*}|\\mathbf{U}_{\\le k},\\mathbf{A}_{<k},\\mathbf{E}_{<k};\\theta)}}\\\\ &{}&{\\quad\\quad+\\lambda_{a}\\log P(a_{k}^{*}|\\mathbf{U}_{\\le k},\\mathbf{A}_{<k},\\mathbf{E}_{\\le k};\\theta)\\,\\Big]\\quad}\\end{array}\n$$\n \nwhere    $\\lambda\\mathbf{s}$   are the linear interpolation weights for the cost of each system output.    $l_{k}^{m*}$  is the ground th labe  for the tracked user goal slot type    $m\\in$   $M$   at the  k th turn, and  $a_{k}^{*}$    is the true system action in the corpus. \n3.6 Imitation Learning with Human Teaching \nOnce obtaining a supervised training dialogue agent, we further let the agent to learn interactively from users by conducting task-oriented dialogues. Supervised learning succeeds when training and test data distributions match. During the agent’s interaction with users, any mistake made by the agent or any deviation in the user’s behavior may lead to a different dialogue state distribution than the one that the supervised learning agent saw dur- ing ofﬂine training. A small mistake made by the agent due to this covariate shift ( Ross and Bagnell , 2010 ;  Ross et al. ,  2011 ) may lead to compound- ing errors which ﬁnally lead to failure of a task. To address this issue, we propose a dialogue imi- tation learning method which allows the dialogue agent to learn from human teaching. We let the supervised training agent to interact with users us- ing its learned dialogue policy  $\\pi_{\\boldsymbol{\\theta}}(a|s)$  . With this, we collect additional dialogue samples that are guided by the agent’s own policy, rather than by the expert policy as those in the supervised train- ing corpora. When the agent make mistakes, we ask users to correct the mistakes and demonstrate the expected actions and predictions for the agent to make. Such user teaching precisely addresses Algorithm 1  Dialogue Learning with Human Teaching and Feedback \n\n1:  Train model end-to-end on dialogue samples  $D$   with MLE and obt  $\\pi_{\\theta}(a|s)\\vartriangleright{\\bf e q}\\,4$  2:  for  learning iteration  k  $k=1:K$   do 3: Run    $\\pi_{\\theta}(a|s)$   with user to collect new dialogue samples    $D_{\\pi}$  4: Ask user to correct the mistakes in the tracked user’s goal for each dialogue turn in    $D_{\\pi}$  5: Add the newly labeled dialogue samples to the existing corpora:    $D\\leftarrow D\\cup D_{\\pi}$  6: Train model end-to-end on    $D$   and obtain an updated policy    $\\pi_{\\boldsymbol{\\theta}}(a|s)$   $\\triangleright\\mathrm{eq}\\,4$  7:  end for 8:  for  learning iteration    $k=1:N$   do 9: Run    $\\pi_{\\boldsymbol{\\theta}}(a|s)$   with user for a new dialogue 10: Collect user feedback as reward    $r$  11: Update model end-to-end and obtain an updated policy  $\\pi_{\\theta}(a|s)$  ▷ eq 5 \n12:  end for \nthe limitations of the currently learned dialogue model, as these newly collected dialogue samples are driven by the agent’s own policy. Speciﬁcally, in this study we let an expert user to correct the mistake made by the agent in tracking the user’s goal at the end of each dialogue turn. This new batch of annotated dialogues are then added to the existing training corpus. We start the next round of supervised model training on this aggregated corpus to obtain an updated dialogue policy, and continue this dialogue imitation learning cycles. \n3.7 Reinforcement Learning with Human Feedback \nLearning from human teaching can be costly, as it requires expert users to provide corrections at each dialogue turn. We want to minimize the num- ber of such imitation dialogue learning cycles and continue to improve the agent via a form of super- vision signal that is easier to obtain. After the imi- tation learning stage, we further optimize the neu- ral dialogue system with RL by letting the agent to interact with users and learn from user feed- back. Different from the turn-level corrections in the imitation dialogue learning stage, the feedback is only collected at the end of a dialogue. A pos- itive reward is collected for successful tasks, and a zero reward is collected for failed tasks. A step penalty is applied to each dialogue turn to encour- age the agent to complete the task in fewer steps. In this work, we only use task-completion as the metric in designing the dialogue reward. One can extend it by introducing additional factors to the reward functions, such as naturalness of interac- tions or costs associated with KB queries. "}
{"page": 5, "image_path": "doc_images/N18-1187_5.jpg", "ocr_text": "age the agent to complete the task in fewer steps.\nIn this work, we only use task-completion as the\nmetric in designing the dialogue reward. One can\nextend it by introducing additional factors to the\nreward functions, such as naturalness of interac-\ntions or costs associated with KB queries.\n\nTo encourage the agent to explore the dialogue\naction space, we let the agent to follow a softmax\npolicy during RL training by sampling system ac-\ntions from the policy network outputs. We apply\nREINFORCE algorithm (Williams, 1992) in op-\ntimizing the network parameters. The objective\nfunction can be written as J,(0) = Eg [Rx] =\nn paw ares] , with y € (0, 1) being the dis-\ncount factor. With likelihood ratio gradient esti-\nmator, the gradient of the objective function can\nbe derived as:\n\nVoJk(0) = VoE [Rx]\n= > To (ax|s~) Vo log 79 (ax|s~) Re\n\nak\n\n= Eo [Vo log ro (ax|sx) Rx]\n(5)\n\nThis last expression above gives us an unbiased\ngradient estimator.\n\n4 Experiments\n\n4.1 Datasets\n\nWe evaluate the proposed method on DSTC2\n(Henderson et al., 2014a) dataset in restaurant\nsearch domain and an internally collected dialogue\ncorpus! in movie booking domain. The movie\nbooking dialogue corpus has an average number of\n8.4 turns per dialogue. Its training set has 100K di-\nalogues, and the development set and test set each\nhas 10K dialogues.\n\nThe movie booking dialogue corpus is gener-\nated (Shah et al., 2018) using a finite state ma-\nchine based dialogue agent and an agenda based\nuser simulator (Schatzmann et al., 2007) with nat-\nural language utterances rewritten by real users.\nThe user simulator can be configured with differ-\nent personalities, showing various levels of ran-\ndomness and cooperativeness. This user simula-\ntor is also used to interact with our end-to-end\ntraining agent during imitation and reinforcement\nlearning stages. We randomly select a user profile\n\n'The dataset can be accessed via https:\n//github.com/google-research-datasets/\nsimulated-dialogue\n\nwhen conducting each dialogue simulation. Dur-\ning model evaluation, we use an extended set of\nnatural language surface forms over the ones used\nduring training time to evaluate the generalization\ncapability of the proposed end-to-end model in\nhandling diverse natural language inputs.\n\n4.2. Training Settings\n\nThe size of the dialogue-level and utterance-level\nLSTM state is set as 200 and 150 respectively.\nWord embedding size is 300. Embedding size for\nsystem action and slot values is set as 32. Hidden\nlayer size of the policy network is set as 100. We\nuse Adam optimization method (Kingma and Ba,\n2014) with initial learning rate of le-3. Dropout\nrate of 0.5 is applied during supervised training to\nprevent the model from over-fitting.\n\nIn imitation learning, we perform mini-batch\nmodel update after collecting every 25 dialogues.\nSystem actions are sampled from the learned pol-\nicy to encourage exploration. The system action\nis defined with the act and slot types from a dia-\nlogue act (Henderson et al., 2013). For example,\nthe dialogue act “con firm(date = monday)” is\nmapped to a system action “con firm_date” and\na candidate value “monday” for slot type “date”.\nThe slot types and values are from the dialogue\nstate tracking output.\n\nIn RL optimization, we update the model with\nevery mini-batch of 25 samples. Dialogue is con-\nsidered successful based on two conditions: (1)\nhe goal slot values estimated from dialogue state\ntracking fully match to the user’s true goal values,\nand (2) the system is able to confirm with the user\nhe tracked goal values and offer an entity which\nis finally accepted by the user. Maximum allowed\nnumber of dialogue turn is set as 15. A positive\nreward of +15.0 is given at the end of a success-\nul dialogue, and a zero reward is given to a failed\ncase. We apply a step penalty of -1.0 for each turn\n0 encourage shorter dialogue for task completion.\n\n4.3 Supervised Learning Results\n\nTable 4.3 and Table 4.3 show the supervised learn-\ning model performance on DSTC2 and the movie\nbooking corpus. Evaluation is made on DST accu-\nracy. For the evaluation on DSTC2 corpus, we use\nthe live ASR transcriptions as the user input utter-\nances. Our proposed model achieves near state-of-\nthe-art dialogue state tracking results on DSTC2\ncorpus, on both individual slot tracking and joint\nslot tracking, comparing to the recent published\n\n2065\n", "vlm_text": "\nTo encourage the agent to explore the dialogue action space, we let the agent to follow a softmax policy during RL training by sampling system ac- tions from the policy network outputs. We apply REINFORCE algorithm ( Williams ,  1992 ) in op- timizing the network parameters. The objective function can be written as    $J_{k}(\\theta)\\;=\\;\\mathbb{E}_{\\theta}\\left[R_{k}\\right]\\;=$   $\\mathbb{E}_{\\theta}\\left[\\sum_{t=0}^{K-k}\\gamma^{t}r_{k+t}\\right]$  hP i , with  $\\gamma\\in[0,1)$   being the dis- count factor. With likelihood ratio gradient esti- mator, the gradient of the objective function can be derived as: \n\n$$\n\\begin{array}{r l}&{\\nabla_{\\theta}J_{k}(\\theta)=\\nabla_{\\theta}\\mathbb{E}_{\\theta}\\left[R_{k}\\right]}\\\\ &{\\qquad\\qquad=\\displaystyle\\sum_{a_{k}}\\pi_{\\theta}(a_{k}|s_{k})\\nabla_{\\theta}\\log\\pi_{\\theta}(a_{k}|s_{k})R_{k}}\\\\ &{\\qquad\\qquad=\\mathbb{E}_{\\theta}\\left[\\nabla_{\\theta}\\log\\pi_{\\theta}(a_{k}|s_{k})R_{k}\\right]}\\end{array}\n$$\n \nThis last expression above gives us an unbiased gradient estimator. \n4 Experiments \n4.1 Datasets \nWe evaluate the proposed method on DSTC2 ( Henderson et al. ,  2014a ) dataset in restaurant search domain and an internally collected dialogue corpus 1   in movie booking domain. The movie booking dialogue corpus has an average number of 8.4 turns per dialogue. Its training set has 100K di- alogues, and the development set and test set each has 10K dialogues. \nThe movie booking dialogue corpus is gener- ated ( Shah et al. ,  2018 ) using a ﬁnite state ma- chine based dialogue agent and an agenda based user simulator ( Schatzmann et al. ,  2007 ) with nat- ural language utterances rewritten by real users. The user simulator can be conﬁgured with differ- ent personalities, showing various levels of ran- domness and cooperativeness. This user simula- tor is also used to interact with our end-to-end training agent during imitation and reinforcement learning stages. We randomly select a user proﬁle when conducting each dialogue simulation. Dur- ing model evaluation, we use an extended set of natural language surface forms over the ones used during training time to evaluate the generalization capability of the proposed end-to-end model in handling diverse natural language inputs. \n\n4.2 Training Settings \nThe size of the dialogue-level and utterance-level LSTM state is set as 200 and 150 respectively. Word embedding size is 300. Embedding size for system action and slot values is set as 32. Hidden layer size of the policy network is set as 100. We use Adam optimization method ( Kingma and Ba , 2014 ) with initial learning rate of 1e-3. Dropout rate of 0.5 is applied during supervised training to prevent the model from over-ﬁtting. \nIn imitation learning, we perform mini-batch model update after collecting every 25 dialogues. System actions are sampled from the learned pol- icy to encourage exploration. The system action is deﬁned with the act and slot types from a dia- logue act ( Henderson et al. ,  2013 ). For example, the dialogue act “  $\\ \\cdot c o n f i r m(d a t e=m o n d a y)\"$   is mapped to a system action “ confirm date ” and a candidate value “ monday ” for slot type “ date ”. The slot types and values are from the dialogue state tracking output. \nIn RL optimization, we update the model with every mini-batch of 25 samples. Dialogue is con- sidered successful based on two conditions: (1) the goal slot values estimated from dialogue state tracking fully match to the user’s true goal values, and (2) the system is able to conﬁrm with the user the tracked goal values and offer an entity which is ﬁnally accepted by the user. Maximum allowed number of dialogue turn is set as 15. A positive reward of   $+15.0$   is given at the end of a success- ful dialogue, and a zero reward is given to a failed case. We apply a step penalty of -1.0 for each turn to encourage shorter dialogue for task completion. \n4.3 Supervised Learning Results \nTable  4.3  and Table  4.3  show the supervised learn- ing model performance on DSTC2 and the movie booking corpus. Evaluation is made on DST accu- racy. For the evaluation on DSTC2 corpus, we use the live ASR transcriptions as the user input utter- ances. Our proposed model achieves near state-of- the-art dialogue state tracking results on DSTC2 corpus, on both individual slot tracking and joint slot tracking, comparing to the recent published results using RNN ( Henderson et al. ,  2014b ) and neural belief tracker (NBT) ( Mrkˇ si´ c et al. ,  2016 ). In the movie booking domain, our model also achieves promising performance on both individ- ual slot tracking and joint slot tracking accuracy. Instead of using ASR hypothesis as model input as in DSTC2, here we use text based input which has much lower noise level in the evaluation of the movie booking tasks. This partially explains the higher DST accuracy in the movie booking do- main comparing to DSTC2. "}
{"page": 6, "image_path": "doc_images/N18-1187_6.jpg", "ocr_text": "results using RNN (Henderson et al., 2014b) and\nneural belief tracker (NBT) (MrkSié et al., 2016).\nIn the movie booking domain, our model also\nachieves promising performance on both individ-\nual slot tracking and joint slot tracking accuracy.\nInstead of using ASR hypothesis as model input\nas in DSTC2, here we use text based input which\nhas much lower noise level in the evaluation of the\nmovie booking tasks. This partially explains the\nhigher DST accuracy in the movie booking do-\nmain comparing to DSTC2.\n\nModel Area Food Price Joint\nRNN 92 86 86 69\nRNN+sem. dict 92 86 92 71\nNBT 90 84 94 72\n\nOur SL model 90 84 92 72\n\nTable 1: Dialogue state tracking results on DSTC2\n\nGoal slot Accuracy\nNum of Tickets 98.22\nMovie 91.86\nTheater Name 97.33\nDate 99.31\nTime 97.71\nJoint 84.57\n\nTable 2: DST results on movie booking dataset\n\n4.4 Imitation and RL Results\n\nEvaluations of interactive learning with imitation\nand reinforcement learning are made on metrics\nof (1) task success rate, (2) dialogue turn size, and\n(3) DST accuracy. Figures 3, 4, and 5 show the\nlearning curves for the three evaluation metrics.\nIn addition, we compare model performance on\ntask success rate using two different RL training\nsettings, the end-to-end training and the policy-\nonly training, to show the advantages of perform-\ning end-to-end system optimization with RL.\nTask Success Rate As shown in the learning\ncurves in Figure 3, the SL model performs poorly.\nThis might largely due to the compounding er-\nrors caused by the mismatch of dialogue state dis-\ntribution between offline training and interactive\nlearning. We use an extended set of user NLG\ntemplates during interactive evaluation. Many of\nthe test NLG templates are not seen by the super-\nvised training agent. Any mistake made by the\nagent in understanding the user’s request may lead\nto compounding errors in the following dialogue\n\nTask Success Rate over Time (smoothed)\n\nSo\na\n\nTask Success Rate\nOo\nwu\n\n0.4 >» SL Baseline\n—@ SL+RL\nKIKI IE EIDE DEE Tar SL +L 500 +. RE\n03 —k- SL+1L 1000 + RL\n0 2000 4000 6000 8000 10000\n\nInteractive Dialogue Learning Sessions\n\nFigure 3: Interactive learning curves on task success\nrate.\n\nturns, which cause final task failure. The red curve\n(SL + RL) shows the performance of the model\nthat has RL applied on the supervised pre-training\nmodel. We can see that interactive learning with\nRL using a weak form of supervision from user\nfeedback continuously improves the task success\nrate with the growing number of user interactions.\nWe further conduct experiments in learning dia-\nlogue model from scratch using only RL (i.e. with-\nout supervised pre-training), and the task success\nrate remains at a very low level after 10K dialogue\nsimulations. We believe that it is because the di-\nalogue state space is too complex for the agent\no learn from scratch, as it has to learn a good\nNLU model in combination with a good policy to\ncomplete the task. The yellow curve (SL + IL\n500 + RL) shows the performance of the model\nhat has 500 episodes of imitation learning over\nhe SL model and continues with RL optimization.\nIt is clear from the results that applying imitation\nlearning on supervised training model efficiently\nimproves task success rate. RL optimization af-\ner imitation learning increases the task success\nrate further. The blue curve (SL + IL 1000 +\nRL) shows the performance of the model that has\n1000 episodes of imitation learning over the SL\nmodel and continues with RL. Similarly, it shows\nhints that imitation learning may effectively adapt\nhe supervised training model to the dialogue state\ndistribution during user interactions.\n\nAverage Dialogue Turn Size Figure 4 shows\nhe curves for the average turn size of successful\ndialogues. We observe decreasing number of dia-\nlogue turns in completing a task along the grow-\ning number of interactive learning sessions. This\nshows that the dialogue agent learns better strate-\ngies in successfully completing the task with fewer\n\n2066\n", "vlm_text": "\nThe table presents the performance of different models (RNN, RNN+sem. dict, NBT, and Our SL model) across four categories: Area, Food, Price, and Joint. The values in the table appear to be percentages representing the accuracy or performance score of each model in the respective category.\n\n- RNN achieves scores of 92 for Area, 86 for Food, 86 for Price, and 69 for Joint.\n- RNN with a semantic dictionary (RNN+sem. dict) scores 92 for Area, 86 for Food, 92 for Price, and 71 for Joint.\n- NBT scores 90 for Area, 84 for Food, 94 for Price, and 72 for Joint.\n- Our SL model scores 90 for Area, 84 for Food, 92 for Price, and 72 for Joint. \n\nThese performance results highlight variances in each model's effectiveness, particularly notable in the Joint category, where scores range from 69 to 72.\nThe table provides the accuracy rates for recognizing different goal slots in a task or application related to booking or scheduling. Here is the breakdown:\n\n- Num of Tickets: 98.22% accuracy\n- Movie: 91.86% accuracy\n- Theater Name: 97.33% accuracy\n- Date: 99.31% accuracy\n- Time: 97.71% accuracy\n- Joint accuracy (overall or combined accuracy for recognizing all slots together): 84.57% \n\nThis suggests the model or system performs very well in individual slot recognition, particularly for Date, but has a lower accuracy when considering all slots simultaneously.\n4.4 Imitation and RL Results \nEvaluations of interactive learning with imitation and reinforcement learning are made on metrics of (1) task success rate, (2) dialogue turn size, and (3) DST accuracy. Figures  3 ,  4 , and  5  show the learning curves for the three evaluation metrics. In addition, we compare model performance on task success rate using two different RL training settings, the end-to-end training and the policy- only training, to show the advantages of perform- ing end-to-end system optimization with RL. \nTask Success Rate As shown in the learning curves in Figure  3 , the SL model performs poorly. This might largely due to the compounding er- rors caused by the mismatch of dialogue state dis- tribution between ofﬂine training and interactive learning. We use an extended set of user NLG templates during interactive evaluation. Many of the test NLG templates are not seen by the super- vised training agent. Any mistake made by the agent in understanding the user’s request may lead to compounding errors in the following dialogue \nThe image is a line graph depicting the task success rate over time across different interactive dialogue learning sessions. The title of the graph is \"Task Success Rate over Time (smoothed).\" The x-axis represents interactive dialogue learning sessions ranging from 0 to 10,000, and the y-axis represents the task success rate ranging from 0.3 to 0.7.\n\nThere are four different learning methods plotted on the graph:\n\n1. **SL Baseline**: Represented by light blue 'x' marks, this line remains constant at a task success rate of around 0.3.\n2. **SL + RL**: Represented by red pentagon-shaped points, this line increases steadily and reaches a task success rate of slightly above 0.5 after 10,000 sessions.\n3. **SL + IL 500 + RL**: Represented by yellow triangle-shaped points, this line starts similarly to the SL + RL line but increases more sharply, reaching a success rate between 0.55 and 0.6.\n4. **SL + IL 1000 + RL**: Represented by blue star-shaped points, this line quickly achieves high success rates, stabilizing around 0.65.\n\nThe graph includes dashed vertical lines and circles highlighting the points where the task success rates for \"SL + IL 500 + RL\" and \"SL + IL 1000 + RL\" initially surpass that of \"SL + RL.\" An inset legend helps differentiate between the lines' representations.\nturns, which cause ﬁnal task failure. The red curve  $\\left(\\mathrm{SL}\\ +\\ \\mathrm{RL}\\right)$   shows the performance of the model that has RL applied on the supervised pre-training model. We can see that interactive learning with RL using a weak form of supervision from user feedback continuously improves the task success rate with the growing number of user interactions. We further conduct experiments in learning dia- logue model from scratch using only RL (i.e. with- out supervised pre-training), and the task success rate remains at a very low level after 10K dialogue simulations. We believe that it is because the di- alogue state space is too complex for the agent to learn from scratch, as it has to learn a good NLU model in combination with a good policy to complete the task. The yellow curve   $(\\tt S L\\mathrm{~\\pm~}+\\mathrm{~\\pm~}\\mathrm{LL}$   $500\\ \\mathrm{~+~}\\ \\mathrm{RL}$  ) shows the performance of the model that has 500 episodes of imitation learning over the SL model and continues with RL optimization. It is clear from the results that applying imitation learning on supervised training model efﬁciently improves task success rate. RL optimization af- ter imitation learning increases the task success rate further. The blue curve   $\\left(\\mathrm{SL}\\ \\ +\\ \\ \\mathbb{L}\\ \\ 10\\,0\\,0\\ \\ +}$  RL ) shows the performance of the model that has 1000 episodes of imitation learning over the SL model and continues with RL. Similarly, it shows hints that imitation learning may effectively adapt the supervised training model to the dialogue state distribution during user interactions. \nAverage Dialogue Turn Size Figure  4  shows the curves for the average turn size of successful dialogues. We observe decreasing number of dia- logue turns in completing a task along the grow- ing number of interactive learning sessions. This shows that the dialogue agent learns better strate- gies in successfully completing the task with fewer "}
{"page": 7, "image_path": "doc_images/N18-1187_7.jpg", "ocr_text": "Average Turn Size over Time (smoothed)\n\nCKD DEI DE DEH DEH IE DHE HEHE DENK\n\ned\no\n\nel\no\n\nAverage Turn Size\n~\nuw\n\n> SL Baseline\n—@ SL+RL\n“he SL+1L500+RL\n—k- SL + IL 1000 + RL\n\n0 2000 4000 6000 8000\nInteractive Dialogue Learning Sessions\n\na\nun\n\n10000\n\nFigure 4: Interactive learning curves on average dia-\nlogue turn size.\n\nnumber of dialogue turns. The red curve with\nRL applied directly after supervised pre-training\nmodel gives the lowest average number of turns\nat the end of the interactive learning cycles, com-\nparing to models with imitation dialogue learn-\ning. This seems to be contrary to our observa-\ntion in Figure 3 that imitation learning with hu-\nman teaching helps in achieving higher task suc-\ncess rate. By looking into the generated dialogues,\nwe find that the SL + RL model can handle easy\ntasks well but fails to complete more challenging\ntasks. Such easy tasks typically can be handled\nwith fewer number of turns, which result in the\nlow average turn size for the SL + RL model.\nOn the other hand, the imitation plus RL models\nattempt to learn better strategies to handle those\nmore challenging tasks, resulting in higher task\nsuccess rates and also slightly increased dialogue\nlength comparing to SL + RL model.\n\nDialogue State Tracking Accuracy Similar\nto the results on task success rate, we see that im-\nitation learning with human teaching quickly im-\nproves dialogue state tracking accuracy in just a\nfew hundred interactive learning sessions. The\njoint slots tracking accuracy in the evaluation of\nSL model using fixed corpus is 84.57% as in Table\n4.3. The accuracy drops to 50.51% in the interac-\ntive evaluation with the introduction of new NLG\ntemplates. Imitation learning with human teach-\ning effectively adapts the neural dialogue model to\nthe new user input and dialogue state distributions,\nimproving the DST accuracy to 67.47% after only\n500 imitation dialogue learning sessions. Another\nencouraging observation is that RL on top of SL\nmodel and IL model not only improves task suc-\ncess rate by optimizing dialogue policy, but also\n\nAverage DST Accuracy over Time (smoothed)\n\n0.80\nI\n> 0.75\no\n£\n3 0.70\nbs)\n<\n0.65\nWw\na\n¥,0.60\ni\no SL Baseline\n2 0.55 -@ SL+RL\n“he SL+1L500 + RL\n0.50 KEKE MEE ME EM IOM safes’ SL FL 1000 + RL\n\n0 2000 4000 6000 8000 10000\n\nInteractive Dialogue Learning Sessions\n\nFigure 5: Interactive learning curves on dialogue state\ntracking accuracy.\n\nTask Success Rate over Time (smoothed)\n\nS\nN\n\no\na\n\ne\nnecator?\n\nTask Success Rate\nOo\nu\n\neo? @ 5% SL Baseline\n0.4 o* @ SL + policy-only RL\nee? ~@ SL + end-to-end RL\nMMMM MDE TR -SL + IL 1000 + policy-only RL\n\n—tk- SL + IL 1000 + end-to-end RL\n\n0 2000 4000 6000 8000 10000\nInteractive Dialogue Learning Sessions\n\no\nw\n\nFigure 6: Interactive learning curves on task success\nrate with different RL training settings.\n\nfurther improves dialogue state tracking perfor-\nmance. This shows the benefits of performing end-\no-end optimization of the neural dialogue model\nwith RL during interactive learning.\n\nEnd-to-End RL Optimization To further show\nhe benefit of performing end-to-end optimization\nof dialogue agent, we compare models with two\ndifferent RL training settings, the end-to-end train-\ning and the policy-only training. End-to-end RL\ntraining is what we applied in previous evaluation\nsections, in which the gradient propagates from\nsystem action output layer all the way back to\nhe natural language user input layer. Policy-only\ntraining refers to only updating the policy network\nparameters during interactive learning with RL,\nwith all the other underlying system parameters\nfixed. The evaluation results are shown in Fig-\nure 6. From these learning curves, we see clear\nadvantage of performing end-to-end model update\nin achieving higher dialogue task success rate dur-\ning interactive learning comparing to only updat-\ning the policy network.\n\n2067\n", "vlm_text": "The image is a graph showing interactive learning curves related to dialogue systems, specifically depicting how the average dialogue turn size changes over time, across various training sessions. The x-axis represents the number of interactive dialogue learning sessions (up to 10,000), and the y-axis represents the average turn size (ranging from 6.0 to 9.0).\n\nThere are four different lines on the graph, each representing a different method or combination of methods for training dialogue systems:\n1. The light blue 'x' markers (SL Baseline): Representing the supervised learning baseline.\n2. The red diamond markers (SL + RL): Representing a combination of supervised learning with reinforcement learning.\n3. The yellow triangle markers (SL + IL 500 + RL): Representing a combination of supervised learning, interactive learning (500 sessions), and reinforcement learning.\n4. The blue star markers (SL + IL 1000 + RL): Representing a combination of supervised learning, interactive learning (1000 sessions), and reinforcement learning.\n\nThe graph demonstrates how each method impacts the dialogue turn size over the course of the learning sessions. The average turn size starts above 8.0 for most methods and trends downward with the progression of learning sessions, with the SL + RL line showing the most significant decrease.\nnumber of dialogue turns. The red curve with RL applied directly after supervised pre-training model gives the lowest average number of turns at the end of the interactive learning cycles, com- paring to models with imitation dialogue learn- ing. This seems to be contrary to our observa- tion in Figure  3  that imitation learning with hu- man teaching helps in achieving higher task suc- cess rate. By looking into the generated dialogues, we ﬁnd that the  SL   $+$   RL  model can handle easy tasks well but fails to complete more challenging tasks. Such easy tasks typically can be handled with fewer number of turns, which result in the low average turn size for the    $S\\mathbb{L}\\;\\;+\\;\\;\\mathbb{R}\\mathbb{L}$   model. On the other hand, the imitation plus RL models attempt to learn better strategies to handle those more challenging tasks, resulting in higher task success rates and also slightly increased dialogue length comparing to  $S\\mathbb{L}\\;\\;+\\;\\;\\mathbb{R}\\mathbb{L}$   model. \nDialogue State Tracking Accuracy Similar to the results on task success rate, we see that im- itation learning with human teaching quickly im- proves dialogue state tracking accuracy in just a few hundred interactive learning sessions. The joint slots tracking accuracy in the evaluation of SL model using ﬁxed corpus is   $84.57\\%$   as in Table 4.3 . The accuracy drops to  $50.51\\%$   in the interac- tive evaluation with the introduction of new NLG templates. Imitation learning with human teach- ing effectively adapts the neural dialogue model to the new user input and dialogue state distributions, improving the DST accuracy to  $67.47\\%$   after only 500 imitation dialogue learning sessions. Another encouraging observation is that RL on top of SL model and IL model not only improves task suc- cess rate by optimizing dialogue policy, but also \nThe image is a line graph depicting the interactive learning curves of dialogue state tracking (DST) accuracy over time. The x-axis represents the number of interactive dialogue learning sessions, ranging from 0 to 10,000. The y-axis shows the average DST accuracy, ranging from 0.50 to 0.80. \n\nThere are four different lines on the graph, each representing a different approach:\n\n1. **SL Baseline** (cyan x's): This line maintains a constant accuracy of about 0.50 throughout the sessions. It is a baseline using supervised learning only.\n\n2. **SL + RL** (red hexagons): This line starts at about 0.50 and shows a gradual increase in accuracy, plateauing around 0.65. It represents a combination of supervised learning and reinforcement learning.\n\n3. **SL + IL 500 + RL** (yellow triangles): This line also starts at about 0.50 but rises more quickly to around 0.72, where it plateaus. It indicates the use of supervised learning, 500 interactive learning steps, and reinforcement learning.\n\n4. **SL + IL 1000 + RL** (blue stars): This line starts similarly, rising steeply to about 0.75, where it plateaus. It represents supervised learning, 1000 interactive learning steps, and reinforcement learning.\n\nThe graph highlights two specific points with large circles: one at around 200 for the blue stars line and another around 2500 for the yellow triangles line. These circles likely highlight significant improvement points or benchmarks of interest in terms of accuracy and interactivity. The chart also mentions that the accuracy data is smoothed over time.\nThe image is a line graph depicting the interactive learning curves for task success rate over time with different reinforcement learning (RL) training settings. The x-axis represents the number of interactive dialogue learning sessions, ranging from 0 to 10,000. The y-axis shows the task success rate, ranging from 0.3 to 0.7.\n\nThe graph includes five different lines, each representing different RL training settings:\n1. **SL Baseline** - represented by a line with x-marks, shows a relatively flat success rate around 0.3.\n2. **SL + policy-only RL** - represented by a line with pentagons, starts near 0.4 and gradually increases to about 0.55.\n3. **SL + end-to-end RL** - represented by a line with squares, follows a similar upward trend as the policy-only RL but starts slightly higher and reaches around 0.58.\n4. **SL + IL 1000 + policy-only RL** - represented by a line with hexagons, starts higher than the standard policy-only RL and increases to around 0.6.\n5. **SL + IL 1000 + end-to-end RL** - represented by a line with stars, starts the highest and reaches above 0.65, demonstrating the most significant improvement in task success rate among the settings.\n\nThe legend in the image clarifies the representation of each training setting with specific markers for easy identification. The line graph visually demonstrates the varying success rates achieved by each method over time, with the interactive learning session sizes on the x-axis contributing to the performance escalation in task success rate on the y-axis.\nfurther improves dialogue state tracking perfor- mance. This shows the beneﬁts of performing end- to-end optimization of the neural dialogue model with RL during interactive learning. \nEnd-to-End RL Optimization  To further show the beneﬁt of performing end-to-end optimization of dialogue agent, we compare models with two different RL training settings, the end-to-end train- ing and the policy-only training. End-to-end RL training is what we applied in previous evaluation sections, in which the gradient propagates from system action output layer all the way back to the natural language user input layer. Policy-only training refers to only updating the policy network parameters during interactive learning with RL, with all the other underlying system parameters ﬁxed. The evaluation results are shown in Fig- ure  6 . From these learning curves, we see clear advantage of performing end-to-end model update in achieving higher dialogue task success rate dur- ing interactive learning comparing to only updat- ing the policy network. "}
{"page": 8, "image_path": "doc_images/N18-1187_8.jpg", "ocr_text": "4.5 Human User Evaluations\n\nWe further evaluate the proposed method with\nhuman judges recruited via Amazon Mechanical\nTurk. Each judge is asked to read a dialogue be-\ntween our model and user simulator and rate each\nsystem turn on a scale of | (frustrating) to 5 (opti-\nmal way to help the user). Each turn is rated by 3\ndifferent judges. We collect and rate 100 dialogues\nfor each of the three models: (i) SL model, (ii) SL\nmodel followed by 1000 episodes of IL, (iii) SL\nand IL followed by RL. Table 3 lists the mean and\nstandard deviation of human scores overall sys-\ntem turns. Performing interactive learning with\nimitation and reinforcement learning clearly im-\nproves the quality of the model according to hu-\nman judges.\n\nModel Score\n\nSL 3.987 + 0.086\nSL + IL 1000 4.378 + 0.082\nSL+IL 1000+RL | 4.603 + 0.067\n\nTable 3: Human evaluation results. Mean and standard\ndeviation of crowd worker scores (between | to 5).\n\n5 Conclusions\n\nIn this work, we focus on training task-oriented\ndialogue systems through user interactions, where\nthe agent improves through communicating with\nusers and learning from the mistake it makes. We\npropose a hybrid learning approach for such sys-\ntems using end-to-end trainable neural network\nmodel. We present a hybrid imitation and rein-\nforcement learning method, where we firstly train\na dialogue agent in a supervised manner by learn-\ning from dialogue corpora, and continuously to\nimprove it by learning from user teaching and\nfeedback with imitation and reinforcement learn-\ning. We evaluate the proposed learning method\nwith both offline evaluation on fixed dialogue cor-\npora and interactive evaluation with users. Exper-\nimental results show that the proposed neural dia-\nlogue agent can effectively learn from user teach-\ning and improve task success rate with imitation\nlearning. Applying reinforcement learning with\nuser feedback after imitation learning with user\nteaching improves the model performance further,\nnot only on the dialogue policy but also on the\ndialogue state tracking in the end-to-end training\nframework.\n\nReferences\n\nAntoine Bordes and Jason Weston. 2017. Learning\nend-to-end goal-oriented dialog. In International\nConference on Learning Representations.\n\nBhuwan Dhingra, Lihong Li, Xiujun Li, Jianfeng Gao,\nYun-Nung Chen, Faisal Ahmed, and Li Deng. 2017.\nTowards end-to-end reinforcement learning of dia-\n\nlogue agents for information access. In ACL.\n\nMihail Eric and Christopher D Manning. 2017. A\ncopy-augmented sequence-to-sequence architecture\ngives good performance on task-oriented dialogue.\nIn EACL.\n\nMilica GaSié, Catherine Breslin, Matthew Henderson,\nDongho Kim, Martin Szummer, Blaise Thomson,\nPirros Tsiakoulis, and Steve Young. 2013. On-\nine policy optimisation of bayesian spoken dialogue\nsystems via human interaction. In JCASSP.\n\nMilica Gasic and Steve Young. 2014. Gaussian pro-\ncesses for pomdp-based dialogue manager optimiza-\ntion. IEEE/ACM Transactions on Audio, Speech,\nand Language Processing .\n\nDilek Hakkani-Tiir, Gokhan Tiir, Asli Celikyilmaz,\nYun-Nung Chen, Jianfeng Gao, Li Deng, and Ye-\nYi Wang. 2016. Multi-domain joint semantic frame\nparsing using bi-directional rnn-lstm. In Jnter-\nspeech.\n\nJames Henderson, Oliver Lemon, and Kallirroi\nGeorgila. 2008. Hybrid reinforcement/supervised\nlearning of dialogue policies from fixed data sets.\nComputational Linguistics .\n\nMatthew Henderson, Blaise Thomson, and Jason\nWilliams. 2013. Dialog state tracking challenge 2 &\n3. http: //camdial.org/~mh521/dstc/.\n\nMatthew Henderson, Blaise Thomson, and Jason\nWilliams. 2014a. The second dialog state tracking\nchallenge. In SIGDIAL.\n\nMatthew Henderson, Blaise Thomson, and Steve\nYoung. 2014b. Robust dialog state tracking using\ndelexicalised recurrent neural networks and unsu-\npervised gate. In JEEE SLT.\n\nMatthew Henderson, Blaise Thomson, and Steve\nYoung. 2014c. Word-based dialog state tracking\nwith recurrent neural networks. In SJGDIAL.\n\nFilip Juréiéek, Blaise Thomson, and Steve Young.\n2012. Reinforcement learning for parameter esti-\nmation in statistical spoken dialogue systems. Com-\nputer Speech & Language 26(3):168-192.\n\nDiederik Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. In International\nConference on Learning Representations.\n\nJiwei Li, Michel Galley, Chris Brockett, Georgios P\nSpithourakis, Jianfeng Gao, and Bill Dolan. 2016. A\npersona-based neural conversation model. In ACL.\n\n2068\n", "vlm_text": "4.5 Human User Evaluations \nWe further evaluate the proposed method with human judges recruited via Amazon Mechanical Turk. Each judge is asked to read a dialogue be- tween our model and user simulator and rate each system turn on a scale of 1 (frustrating) to 5 (opti- mal way to help the user). Each turn is rated by 3 different judges. We collect and rate 100 dialogues for each of the three models: (i) SL model, (ii) SL model followed by 1000 episodes of IL, (iii) SL and IL followed by RL. Table  3  lists the mean and standard deviation of human scores overall sys- tem turns. Performing interactive learning with imitation and reinforcement learning clearly im- proves the quality of the model according to hu- man judges. \nThe table presents a comparison of different models based on their scores. It consists of two columns: \"Model\" and \"Score.\" \n\n- The \"Model\" column lists the models being compared. There are three models:\n  1. SL\n  2. SL + IL 1000\n  3. SL + IL 1000 + RL\n\n- The \"Score\" column lists the scores achieved by each model along with a margin of error or uncertainty. The scores are as follows:\n  1. SL: 3.987 ± 0.086\n  2. SL + IL 1000: 4.378 ± 0.082\n  3. SL + IL 1000 + RL: 4.603 ± 0.067\n\nThese scores likely represent some form of performance measure for each model, where a higher score indicates better performance. The numbers following the ± symbol represent the standard deviation or margin of error for each score.\nTable 3: Human evaluation results. Mean and standard deviation of crowd worker scores (between 1 to 5). \n5 Conclusions \nIn this work, we focus on training task-oriented dialogue systems through user interactions, where the agent improves through communicating with users and learning from the mistake it makes. We propose a hybrid learning approach for such sys- tems using end-to-end trainable neural network model. We present a hybrid imitation and rein- forcement learning method, where we ﬁrstly train a dialogue agent in a supervised manner by learn- ing from dialogue corpora, and continuously to improve it by learning from user teaching and feedback with imitation and reinforcement learn- ing. We evaluate the proposed learning method with both ofﬂine evaluation on ﬁxed dialogue cor- pora and interactive evaluation with users. Exper- imental results show that the proposed neural dia- logue agent can effectively learn from user teach- ing and improve task success rate with imitation learning. Applying reinforcement learning with user feedback after imitation learning with user teaching improves the model performance further, not only on the dialogue policy but also on the dialogue state tracking in the end-to-end training framework. \nReferences \nAntoine Bordes and Jason Weston. 2017. Learning end-to-end goal-oriented dialog. In  International Conference on Learning Representations . \nBhuwan Dhingra, Lihong Li, Xiujun Li, Jianfeng Gao, Yun-Nung Chen, Faisal Ahmed, and Li Deng. 2017. Towards end-to-end reinforcement learning of dia- logue agents for information access. In  ACL . \nMihail Eric and Christopher D Manning. 2017. A copy-augmented sequence-to-sequence architecture gives good performance on task-oriented dialogue. In  EACL . \nMilica Gaˇ si´ c, Catherine Breslin, Matthew Henderson, Dongho Kim, Martin Szummer, Blaise Thomson, Pirros Tsiakoulis, and Steve Young. 2013. On- line policy optimisation of bayesian spoken dialogue systems via human interaction. In  ICASSP . \nMilica Gasic and Steve Young. 2014. Gaussian pro- cesses for pomdp-based dialogue manager optimiza- tion. IEEE/ACM Transactions on Audio, Speech, and Language Processing  . \nDilek Hakkani-T¨ ur, G¨ okhan T¨ ur, Asli Celikyilmaz, Yun-Nung Chen, Jianfeng Gao, Li Deng, and Ye- Yi Wang. 2016. Multi-domain joint semantic frame parsing using bi-directional rnn-lstm. In  Inter- speech . \nJames Henderson, Oliver Lemon, and Kallirroi Georgila. 2008. Hybrid reinforcement/supervised learning of dialogue policies from ﬁxed data sets. Computational Linguistics  . \nMatthew Henderson, Blaise Thomson, and Jason Williams. 2013. Dialog state tracking challenge 2 & 3.  http://camdial.org/˜mh521/dstc/ . \nMatthew Henderson, Blaise Thomson, and Jason Williams. 2014a. The second dialog state tracking challenge. In  SIGDIAL . \nMatthew Henderson, Blaise Thomson, and Steve Young. 2014b. Robust dialog state tracking using delexicalised recurrent neural networks and unsu- pervised gate. In  IEEE SLT . \nMatthew Henderson, Blaise Thomson, and Steve Young. 2014c. Word-based dialog state tracking with recurrent neural networks. In  SIGDIAL . \nFilip Jurˇ c´ ıˇ cek, Blaise Thomson, and Steve Young. 2012. Reinforcement learning for parameter esti- mation in statistical spoken dialogue systems.  Com- puter Speech & Language  26(3):168–192. \nDiederik Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. In  International Conference on Learning Representations . \nJiwei Li, Michel Galley, Chris Brockett, Georgios P Spithourakis, Jianfeng Gao, and Bill Dolan. 2016. A persona-based neural conversation model. In  ACL . "}
{"page": 9, "image_path": "doc_images/N18-1187_9.jpg", "ocr_text": "Xuijun Li, Yun-Nung Chen, Lihong Li, and Jianfeng\nGao. 2017. End-to-end task-completion neural dia-\nlogue systems. arXiv preprint arXiv: 1703.01008 .\n\nBing Liu and Jan Lane. 2016. Joint online spoken lan-\nguage understanding and language modeling with\nrecurrent neural networks. In SJGDJAL.\n\nBing Liu and Ian Lane. 2017a. An end-to-end trainable\nneural network model with belief tracking for task-\noriented dialog. In Interspeech.\n\nBing Liu and Jan Lane. 2017b. Iterative policy learning\nin end-to-end trainable task-oriented neural dialog\nmodels. In Proceedings of IEEE ASRU.\n\nBing Liu, Gokhan Tur, Dilek Hakkani-Tur, Pararth\nShah, and Larry Heck. 2017. End-to-end optimiza-\ntion of task-oriented dialogue model with deep rein-\nforcement learning. In NIPS Workshop on Conver-\nsational Al.\n\nFei Liu and Julien Perez. 2017.\nmemory networks. In EACL.\n\nGated end-to-end\n\nGrégoire Mesnil, Yann Dauphin, Kaisheng Yao,\nYoshua Bengio, Li Deng, Dilek Hakkani-Tur, Xi-\naodong He, Larry Heck, Gokhan Tur, Dong Yu, et al.\n2015. Using recurrent neural networks for slot fill-\ning in spoken language understanding. IEEE/ACM\nTransactions on Audio, Speech and Language Pro-\ncessing (TASLP) .\n\nNikola Mrksi¢é, Diarmuid O Séaghdha, Tsung-Hsien\nWen, Blaise Thomson, and Steve Young. 2016.\nNeural belief tracker: Data-driven dialogue state\ntracking. arXiv preprint arXiv: 1606.03777 .\n\nBaolin Peng, Xiujun Li, Lihong Li, Jianfeng Gao,\nAsli Celikyilmaz, Sungjin Lee, and Kam-Fai Wong.\n2017. Composite task-completion dialogue policy\nlearning via hierarchical deep reinforcement learn-\ning. In Proceedings of EMNLP.\n\nAntoine Raux, Brian Langner, Dan Bohus, Alan W\nBlack, and Maxine Eskenazi. 2005. Lets go pub-\nlic! taking a spoken dialog system to the real world.\nIn Interspeech.\n\nStéphane Ross and Drew Bagnell. 2010. Efficient re-\nductions for imitation learning. In Proceedings of\nthe thirteenth international conference on artificial\nintelligence and statistics. pages 661-668.\n\nStéphane Ross, Geoffrey J Gordon, and Drew Bagnell.\n2011. A reduction of imitation learning and struc-\ntured prediction to no-regret online learning. In Jn-\nternational Conference on Artificial Intelligence and\nStatistics. pages 627-635.\n\nAlexander I Rudnicky, Eric H Thayer, Paul C Constan-\ntinides, Chris Tchou, R Shern, Kevin A Lenzo, Wei\nXu, and Alice Oh. 1999. Creating natural dialogs in\nthe carnegie mellon communicator system. In Eu-\nrospeech.\n\nJost Schatzmann, Blaise Thomson, Karl Weilhammer,\nHui Ye, and Steve Young. 2007. Agenda-based user\nsimulation for bootstrapping a pomdp dialogue sys-\ntem. In NAACL-HLT.\n\nMinjoon Seo, Ali Farhadi, and Hannaneh Hajishirzi.\n2016. Query-regression networks for machine com-\nprehension. arXiv preprint arXiv: 1606.04582 .\n\nJulian V Serban, Alessandro Sordoni, Yoshua Bengio,\nAaron Courville, and Joelle Pineau. 2015. Build-\ning end-to-end dialogue systems using generative hi-\nerarchical neural network models. arXiv preprint\narXiv:1507.04808 .\n\nPararth Shah, Dilek Hakkani-Tiir, Liu Bing, and\nGokhan Tiir. 2018. Bootstrapping a neural conver-\nsational agent with dialogue self-play, crowdsourc-\ning and on-line reinforcement learning. In NAACL-\nHLT.\n\nPararth Shah, Dilek Hakkani-Tiir, and Larry Heck.\n2016. Interactive reinforcement learning for task-\noriented dialogue management. In NJPS 2016 Deep\nLearning for Action and Interaction Workshop.\n\nPei-Hao Su, Pawel Budzianowski, Stefan Ultes, Mil-\nica Gasic, and Steve Young. 2017. Sample-efficient\nactor-critic reinforcement learning with supervised\ndata for dialogue management. In SJGDJAL.\n\nPei-Hao Su, Milica Gasic, Nikola Mrksic, Lina Rojas-\nBarahona, Stefan Ultes, David Vandyke, Tsung-\nHsien Wen, and Steve Young. 2016. On-line active\nreward learning for policy optimisation in spoken di-\nalogue systems. In ACL.\n\nTsung-Hsien Wen, David Vandyke, Nikola Mrksié,\nMilica Gasié, Lina M. Rojas-Barahona, Pei-Hao Su,\nStefan Ultes, and Steve Young. 2017. A network-\nbased end-to-end trainable task-oriented dialogue\nsystem. In EACL.\n\nJason D Williams, Kavosh Asadi, and Geoffrey Zweig.\n2017. Hybrid code networks: practical and efficient\nend-to-end dialog control with supervised and rein-\nforcement learning. In ACL.\n\nJason D Williams and Geoffrey Zweig. 2016. End-\nto-end Istm-based dialog control optimized with su-\npervised and reinforcement learning. arXiv preprint\narXiv:1606.01269 .\n\nRonald J Williams. 1992. Simple statistical gradient-\nfollowing algorithms for connectionist reinforce-\nment learning. Machine learning .\n\nSteve Young, Milica GaSi¢, Blaise Thomson, and Ja-\nson D Williams. 2013. Pomdp-based statistical spo-\nken dialog systems: A review. Proceedings of the\nIEEE 101(5):1160-1179.\n\nTiancheng Zhao and Maxine Eskenazi. 2016. Towards\nend-to-end learning for dialog state tracking and\nmanagement using deep reinforcement learning. In\nSIGDIAL.\n\n2069\n", "vlm_text": "Xuijun Li, Yun-Nung Chen, Lihong Li, and Jianfeng Gao. 2017. End-to-end task-completion neural dia- logue systems.  arXiv preprint arXiv:1703.01008  . Bing Liu and Ian Lane. 2016. Joint online spoken lan- guage understanding and language modeling with recurrent neural networks. In  SIGDIAL . Bing Liu and Ian Lane. 2017a. An end-to-end trainable neural network model with belief tracking for task- oriented dialog. In  Interspeech . Bing Liu and Ian Lane. 2017b. Iterative policy learning in end-to-end trainable task-oriented neural dialog models. In  Proceedings of IEEE ASRU . Bing Liu, Gokhan Tur, Dilek Hakkani-Tur, Pararth Shah, and Larry Heck. 2017. End-to-end optimiza- tion of task-oriented dialogue model with deep rein- forcement learning. In  NIPS Workshop on Conver- sational AI . Fei Liu and Julien Perez. 2017. Gated end-to-end memory networks. In  EACL . Gr´ egoire Mesnil, Yann Dauphin, Kaisheng Yao, Yoshua Bengio, Li Deng, Dilek Hakkani-Tur, Xi- aodong He, Larry Heck, Gokhan Tur, Dong Yu, et al. 2015. Using recurrent neural networks for slot ﬁll- ing in spoken language understanding.  IEEE/ACM Transactions on Audio, Speech and Language Pro- cessing (TASLP)  . Nikola Mrkˇ si´ c, Diarmuid O S´ eaghdha, Tsung-Hsien Wen, Blaise Thomson, and Steve Young. 2016. Neural belief tracker: Data-driven dialogue state tracking.  arXiv preprint arXiv:1606.03777  . Baolin Peng, Xiujun Li, Lihong Li, Jianfeng Gao, Asli Celikyilmaz, Sungjin Lee, and Kam-Fai Wong. 2017. Composite task-completion dialogue policy learning via hierarchical deep reinforcement learn- ing. In  Proceedings of EMNLP . Antoine Raux, Brian Langner, Dan Bohus, Alan W Black, and Maxine Eskenazi. 2005. Lets go pub- lic! taking a spoken dialog system to the real world. In  Interspeech . St´ ephane Ross and Drew Bagnell. 2010. Efﬁcient re- ductions for imitation learning. In  Proceedings of the thirteenth international conference on artiﬁcial intelligence and statistics . pages 661–668. St´ ephane Ross, Geoffrey J Gordon, and Drew Bagnell. 2011. A reduction of imitation learning and struc- tured prediction to no-regret online learning. In  In- ternational Conference on Artiﬁcial Intelligence and Statistics . pages 627–635. Alexander I Rudnicky, Eric H Thayer, Paul C Constan- tinides, Chris Tchou, R Shern, Kevin A Lenzo, Wei Xu, and Alice Oh. 1999. Creating natural dialogs in the carnegie mellon communicator system. In  Eu- rospeech . \nJost Schatzmann, Blaise Thomson, Karl Weilhammer, Hui Ye, and Steve Young. 2007. Agenda-based user simulation for bootstrapping a pomdp dialogue sys- tem. In  NAACL-HLT . Minjoon Seo, Ali Farhadi, and Hannaneh Hajishirzi. 2016. Query-regression networks for machine com- prehension.  arXiv preprint arXiv:1606.04582  . Iulian V Serban, Alessandro Sordoni, Yoshua Bengio, Aaron Courville, and Joelle Pineau. 2015. Build- ing end-to-end dialogue systems using generative hi- erarchical neural network models. arXiv preprint arXiv:1507.04808  . Pararth Shah, Dilek Hakkani-T¨ ur, Liu Bing, and Gokhan T¨ ur. 2018. Bootstrapping a neural conver- sational agent with dialogue self-play, crowdsourc- ing and on-line reinforcement learning. In  NAACL- HLT . Pararth Shah, Dilek Hakkani-T¨ ur, and Larry Heck. 2016. Interactive reinforcement learning for task- oriented dialogue management. In  NIPS 2016 Deep Learning for Action and Interaction Workshop . Pei-Hao Su, Pawel Budzianowski, Stefan Ultes, Mil- ica Gasic, and Steve Young. 2017. Sample-efﬁcient actor-critic reinforcement learning with supervised data for dialogue management. In  SIGDIAL . Pei-Hao Su, Milica Gasic, Nikola Mrksic, Lina Rojas- Barahona, Stefan Ultes, David Vandyke, Tsung- Hsien Wen, and Steve Young. 2016. On-line active reward learning for policy optimisation in spoken di- alogue systems. In  ACL . Tsung-Hsien Wen, David Vandyke, Nikola Mrkˇ si´ c, Milica Gaˇ si´ c, Lina M. Rojas-Barahona, Pei-Hao Su, Stefan Ultes, and Steve Young. 2017. A network- based end-to-end trainable task-oriented dialogue system. In  EACL . Jason D Williams, Kavosh Asadi, and Geoffrey Zweig. 2017. Hybrid code networks: practical and efﬁcient end-to-end dialog control with supervised and rein- forcement learning. In  ACL . Jason D Williams and Geoffrey Zweig. 2016. End- to-end lstm-based dialog control optimized with su- pervised and reinforcement learning.  arXiv preprint arXiv:1606.01269  . Ronald J Williams. 1992. Simple statistical gradient- following algorithms for connectionist reinforce- ment learning.  Machine learning  . Steve Young, Milica Gaˇ si´ c, Blaise Thomson, and Ja- son D Williams. 2013. Pomdp-based statistical spo- ken dialog systems: A review.  Proceedings of the IEEE  101(5):1160–1179. Tiancheng Zhao and Maxine Eskenazi. 2016. Towards end-to-end learning for dialog state tracking and management using deep reinforcement learning. In SIGDIAL . "}
