{"page": 0, "image_path": "doc_images/P19-1598_0.jpg", "ocr_text": "Barack’s Wife Hillary:\nUsing Knowledge Graphs for Fact-Aware Language Modeling\n\nRobert L. Logan IV*\nMatt Gardner\n\nNelson F. Liu’\n\nMatthew E. Peters’\nSameer Singh*\n\n“University of California, Irvine, CA, USA\n+ University of Washington, Seattle, WA, USA\n§ Allen Institute for Artificial Intelligence, Seattle, WA, USA\n\n{rlogan, sameer}@uci.edu, {mattg, matthewp}@allenai.org, nfliu@cs.washington.edu\n\nAbstract\n\nModeling human language requires the ability\nto not only generate fluent text but also en-\ncode factual knowledge. However, traditional\nanguage models are only capable of remem-\nbering facts seen at training time, and often\nhave difficulty recalling them. To address this,\nwe introduce the knowledge graph language\nmodel (KGLM), a neural language model with\nmechanisms for selecting and copying facts\nrom a knowledge graph that are relevant to\nthe context. These mechanisms enable the\nmodel to render information it has never seen\nbefore, as well as generate out-of-vocabulary\ntokens. We also introduce the Linked WikiText-\n2 dataset,! a corpus of annotated text aligned to\nthe Wikidata knowledge graph whose contents\n(roughly) match the popular WikiText-2 bench-\nmark (Merity et al., 2017). In experiments, we\ndemonstrate that the KGLM achieves signifi-\ncantly better performance than a strong base-\nline language model. We additionally com-\npare different language models’ ability to com-\nplete sentences requiring factual knowledge,\nand show that the KGLM outperforms even\nvery large language models in generating facts.\n\n1 Introduction\n\nFor language models to generate plausible sen-\ntences, they must be both syntactically coherent as\nwell as consistent with the world they describe. Al-\nthough language models are quite skilled at generat-\ning grammatical sentences, and previous work has\nshown that language models also possess some de-\ngree of common-sense reasoning and basic knowl-\nedge (Vinyals and Le, 2015; Serban et al., 2016;\nTrinh and Le, 2019), their ability to generate fac-\ntually correct text is quite limited. The clearest\nlimitation of existing language models is that they,\nat best, can only memorize facts observed during\n\n\"https: //rloganiv.github.io/linked-wikitext-2\n\n[Super Mario Land] is a [1989] [side-scrolling]\n[platform video game] developed and published\nby | Nintendo| as a {launch title] for their [Game\nBoy] (handheld game console].\n\nSuper Mario Land | PUBLISHER, | Nintendo\n\nlaunch game\n\nPUBLICATION MANUFACTURER\n\nDaTE\n\n21 Apt 1989 platform game\nside-scrolling video game]\n\nINSTANCE OF\n\n{handheld game console\n\nFigure 1: Linked WikiText-2 Example. A localized\nknowledge graph containing facts that are (possibly)\nconveyed in the sentence above. The graph is built by it-\neratively linking each detected entity to Wikidata, then\nadding any relations to previously mentioned entities.\nNote that not all entities are connected, potentially due\nto missing relations in Wikidata.\n\ntraining. For instance, when conditioned on the text\nat the top of Figure 1, an AWD-LSTM language\nmodel (Merity et al., 2018) trained on Wikitext-2\nassigns higher probability to the word “PlaySta-\ntion” than “Game Boy”, even though this sentence\nappears verbatim in the training data. This is not\nsurprising—existing models represent the distribu-\ntion over the entire vocabulary directly, whether\nthey are common words, references to real world\nentities, or factual information like dates and num-\nbers. As a result, language models are unable to\ngenerate factually correct sentences, do not gen-\neralize to rare/unseen entities, and often omit rare\ntokens from the vocabulary (instead generating UN-\nKNOWN tokens).\n\nWe introduce the knowledge graph language\nmodel (KGLM), a neural language model with\nmechanisms for selecting and copying information\nfrom an external knowledge graph. The KGLM\nmaintains a dynamically growing local knowledge\n\n5962\n\nProceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5962-5971\nFlorence, Italy, July 28 - August 2, 2019. ©2019 Association for Computational Linguistics\n", "vlm_text": "Barack’s Wife Hillary: Using Knowledge Graphs for Fact-Aware Language Modeling \nRobert L. Logan  $\\mathbf{I}\\mathbf{V}^{*}$  Nelson F. Liu †§ Matthew E. Peters § Matt Gardner § Sameer Singh ∗ \n∗ University of California, Irvine, CA, USA †  University of Washington, Seattle, WA, USA §  Allen Institute for Artiﬁcial Intelligence, Seattle, WA, USA {rlogan, sameer}@uci.edu ,  {mattg, matthewp}@allenai.org ,  nﬂiu@cs.washington.edu \nAbstract \nModeling human language requires the ability to not only generate ﬂuent text but also en- code factual knowledge. However, traditional language models are only capable of remem- bering facts seen at training time, and often have difﬁculty recalling them. To address this, we introduce the knowledge graph language model (KGLM), a neural language model with mechanisms for selecting and copying facts from a knowledge graph that are relevant to the context. These mechanisms enable the model to render information it has never seen before, as well as generate out-of-vocabulary tokens. We also introduce the  Linked WikiText- 2  dataset,   a corpus of annotated text aligned to the Wikidata knowledge graph whose contents (roughly) match the popular  WikiText-2  bench- mark ( Merity et al. ,  2017 ). In experiments, we demonstrate that the KGLM achieves signiﬁ- cantly better performance than a strong base- line language model. We additionally com- pare different language models’ ability to com- plete sentences requiring factual knowledge, and show that the KGLM outperforms even very large language models in generating facts. \n1 Introduction \nFor language models to generate plausible sen- tences, they must be both syntactically coherent as well as consistent with the world they describe. Al- though language models are quite skilled at generat- ing grammatical sentences, and previous work has shown that language models also possess some de- gree of common-sense reasoning and basic knowl- edge ( Vinyals and Le ,  2015 ;  Serban et al. ,  2016 ; Trinh and Le ,  2019 ), their ability to generate  fac- tually correct  text is quite limited. The clearest limitation of existing language models is that they, at best, can only memorize facts observed during \n[ Super Mario Land ]  is a  [ 1989 ]  [ side-scrolling ] [ platform video game ]  developed and published by  [ Nintendo ]  as a  [ launch title ]  for their  [ Game Boy ]  [ handheld game console ] . \nThe image is a localized knowledge graph representing connections between various entities related to the video game \"Super Mario Land.\" Each entity is linked to a corresponding Wikidata entry, indicated by a Q-number (e.g., Q647249 for \"Super Mario Land\"). The entities are connected by relationships which convey specific information:\n\n1. \"Super Mario Land\" has its publication date as \"21 April 1989\" and is of the genre \"platform game.\"\n2. It was published by \"Nintendo,\" which is also associated with a \"launch game.\"\n3. The game \"Super Mario Land\" was released on the \"Game Boy,\" which in turn is described as a \"handheld game console,\" an instance attributed to the \"Game Boy\" by its manufacturer, \"Nintendo.\"\n4. \"Game Boy\" is additionally classified under the genre \"side-scrolling video game\" linked to the \"platform game\" category.\n\nThe graph shows that not every entity is connected, possibly due to missing relations in the Wikidata entries.\ntraining. For instance, when conditioned on the text at the top of Figure  1 , an AWD-LSTM language model ( Merity et al. ,  2018 ) trained on  Wikitext-2 assigns higher probability to the word “ PlaySta- tion ” than “ Game Boy ”, even though this sentence appears verbatim in the training data. This is not surprising—existing models represent the distribu- tion over the entire vocabulary directly, whether they are common words, references to real world entities, or factual information like dates and num- bers. As a result, language models are unable to generate factually correct sentences, do not gen- eralize to rare/unseen entities, and often omit rare tokens from the vocabulary (instead generating  UN- KNOWN  tokens). \nWe introduce the  knowledge graph language model  (KGLM), a neural language model with mechanisms for selecting and copying information from an external knowledge graph. The KGLM maintains a dynamically growing  local knowledge graph , a subset of the knowledge graph that con- tains entities that have already been mentioned in the text, and their related entities. When generating entity tokens, the model either decides to render a new entity that is absent from the local graph, thereby growing the local knowledge graph, or to render a fact from the local graph. When render- ing, the model combines the standard vocabulary with tokens available in the knowledge graph, thus supporting numbers, dates, and other rare tokens. "}
{"page": 1, "image_path": "doc_images/P19-1598_1.jpg", "ocr_text": "graph, a subset of the knowledge graph that con-\ntains entities that have already been mentioned in\nthe text, and their related entities. When generating\nentity tokens, the model either decides to render\na new entity that is absent from the local graph,\nthereby growing the local knowledge graph, or to\nrender a fact from the local graph. When render-\ning, the model combines the standard vocabulary\nwith tokens available in the knowledge graph, thus\nsupporting numbers, dates, and other rare tokens.\n\nFigure | illustrates how the KGLM works. Ini-\ntially, the graph is empty and the model uses the\nentity Super Mario Land to render the first three\ntokens, thus adding it and its relations to the local\nknowledge graph. After generating the next two to-\nkens (“is’”’, “a’’) using the standard language model,\nthe model selects Super Mario Land as the parent\nentity, Publication Date as the relation to render,\nand copies one of the tokens of the date entity as\nthe token (“/989” in this case).\n\nTo facilitate research on knowledge graph-based\nlanguage modeling, we collect the distantly su-\npervised Linked WikiText-2 dataset. The underly-\ning text closely matches WikiText-2 (Merity et al.,\n2017), a popular benchmark for language model-\ning, allowing comparisons against existing mod-\nels. The tokens in the text are linked to entities in\nWikidata (Vrandecié and Krétzsch, 2014) using a\ncombination of human-provided links and off-the-\nshelf linking and coreference models. We also use\nrelations between these entities in Wikidata to con-\nstruct plausible reasons for why an entity may have\nbeen mentioned: it could either be related to an\nentity that is already mentioned (including itself)\nor a brand new, unrelated entity for the document.\n\nWe train and evaluate the KGLM on Linked\nWikiText-2. When compared against AWD-LSTM,\na recent and performant language model, KGLM\nobtains not only a lower overall perplexity, but also\na substantially lower unknown-penalized perplex-\nity (Ueberla, 1994; Ahn et al., 2016), a metric that\nallows fair comparisons between models that accu-\nrately model rare tokens and ones that predict them\nto be unknown. We also compare factual com-\npletion capabilities of these models, where they\npredict the next word after a factual sentence (e.g.,\n“Barack is married to __”’) and show that KGLM\nis significantly more accurate. Lastly, we show that\nthe model is able to generate accurate facts for rare\nentities, and can be controlled via modifications\nthe knowledge graph.\n\n2 Knowledge Graph Language Model\n\nIn this section we introduce a language model that\nis conditioned on an external, structured knowledge\nsource, which it uses to generate factual text.\n\n2.1 Problem Setup and Notation\n\nA language model defines a probability distribution\nover each token within a sequence, conditioned on\nthe sequence of tokens observed so far. We denote\nthe random variable representing the next token as\na, and the sequence of the tokens before t as x <;,\ni.e. language models compute p(¢|°<¢). RNN lan-\nguage models (Mikolov et al., 2010) parameterize\nthis distribution using a recurrent structure:\n\np(xt|v<t) = softmax(W),h; + b),\n\ndd)\nh, = RNN(hy—1, X/-1).\n\nWe use LSTMs (Hochreiter and Schmidhuber,\n1997) as the recurrent module in this paper.\n\nA knowledge graph (KG) is a directed, labeled\ngraph consisting of entities € as nodes, with edges\ndefined over a set of relations R, ie. KG =\n{(p,r,e) |p € E,r © R,e € E}, where p is a par-\nent entity with relation r to another entity e. Prac-\ntical KGs have other aspects that make this for-\nmulation somewhat inexact: some relations are to\nliteral values, such as numbers and dates, facts\nmay be expressed as properties on relations, and\nentities have aliases as the set of strings that can\nrefer to the entity. We also define a local knowl-\nedge graph for a subset of entities Ec; as KGa, =\n{(p,r,e) |p € Ear,r © Rie € E}, ie. contains\nentities €<; and all facts they participate in.\n\n2.2 Generative KG Language Model\n\nThe primary goal of the knowledge graph lan-\nguage model (KGLM) is to enable a neural lan-\nguage model to generate entities and facts from\na knowledge graph. To encourage the model to\ngenerate facts that have appeared in the context\nalready, KGLM will maintain a local knowledge\ngraph containing all facts involving entities that\nhave appeared in the context. As the model decides\nto refer to entities that have not been referred to\nyet, it will grow the local knowledge graph with\nadditional entities and facts to reflect the new entity.\nFormally, we will compute p(x+, €:|v<r, Ect)\nwhere «x <; is the sequence of observed tokens, E<;\nis the set of entities mentioned in ve,, and KG <; is\nthe local knowledge graph determined by €<;, as\ndescribed above. The generative process is:\n\n5963\n", "vlm_text": "\nFigure  1  illustrates how the KGLM works. Ini- tially, the graph is empty and the model uses the entity  Super Mario Land  to render the ﬁrst three tokens, thus adding it and its relations to the local knowledge graph. After generating the next two to- kens   $(^{**}i s^{**},{^{*}a^{*}})$   using the standard language model, the model selects  Super Mario Land  as the parent entity,  Publication Date  as the relation to render, and copies one of the tokens of the date entity as the token (“ 1989 ” in this case). \nTo facilitate research on knowledge graph-based language modeling, we collect the distantly su- pervised  Linked WikiText-2  dataset. The underly- ing text closely matches  WikiText-2  ( Merity et al. , 2017 ), a popular benchmark for language model- ing, allowing comparisons against existing mod- els. The tokens in the text are linked to entities in Wikidata ( Vrandeˇ ci´ c and Krötzsch ,  2014 ) using a combination of human-provided links and off-the- shelf linking and coreference models. We also use relations between these entities in Wikidata to con- struct plausible reasons for why an entity may have been mentioned: it could either be related to an entity that is already mentioned (including itself) or a brand new, unrelated entity for the document. \nWe train and evaluate the KGLM on  Linked WikiText-2 . When compared against AWD-LSTM, a recent and performant language model, KGLM obtains not only a lower overall perplexity, but also a substantially lower  unknown-penalized  perplex- ity ( Ueberla ,  1994 ;  Ahn et al. ,  2016 ), a metric that allows fair comparisons between models that accu- rately model rare tokens and ones that predict them to be  unknown . We also compare  factual com- pletion  capabilities of these models, where they predict the next word after a factual sentence (e.g., “ Barack is married to ”) and show that KGLM is signiﬁcantly more accurate. Lastly, we show that the model is able to generate accurate facts for rare entities, and can be  controlled  via modiﬁcations the knowledge graph. \n2 Knowledge Graph Language Model \nIn this section we introduce a language model that is conditioned on an external, structured knowledge source, which it uses to generate factual text. \n2.1 Problem Setup and Notation \nA  language model  deﬁnes a probability distribution over each token within a sequence, conditioned on the sequence of tokens observed so far. We denote the random variable representing the next token as  $x_{t}$   and the sequence of the tokens before    $t$   as    $\\boldsymbol{x}_{<t}$  , i.e. language models compute    $p(x_{t}|\\boldsymbol{x}_{<t})$  . RNN lan- guage models ( Mikolov et al. ,  2010 ) parameterize this distribution using a recurrent structure: \n\n$$\n\\begin{array}{r}{p(x_{t}|\\boldsymbol{x}_{<t})=\\mathrm{softmax}(\\mathbf{W}_{h}\\mathbf{h}_{t}+\\mathbf{b}),}\\\\ {\\mathbf{h}_{t}=\\mathbf{RNN}(\\mathbf{h}_{t-1},\\mathbf{x}_{t-1}).\\qquad}\\end{array}\n$$\n \nWe use LSTMs ( Hochreiter and Schmidhuber , 1997 ) as the recurrent module in this paper. \nA  knowledge graph  (KG) is a directed, labeled graph consisting of entities    $\\mathcal{E}$   as nodes, with edges deﬁned over a set of relations    $\\mathcal{R}$  , i.e.  $\\boldsymbol{\\kappa}\\boldsymbol{\\mathcal{G}}\\;=$   $\\{(p,r,e)\\mid p\\in\\mathcal{E},r\\in\\mathcal{R},e\\in\\mathcal{E}\\}$  , where  $p$   is a par- ent entity with relation    $r$   to another entity    $e$  . Prac- tical KGs have other aspects that make this for- mulation somewhat inexact: some relations are to literal values , such as numbers and dates, facts may be expressed as  properties  on relations, and entities have  aliases  as the set of strings that can refer to the entity. We also deﬁne a  local knowl- edge graph  for a subset of entities    $\\mathcal{E}_{<t}$   as    $\\mathcal{K G}_{<t}=$   $\\{(p,r,e)\\mid p\\in\\mathcal{E}_{<t},r\\in\\mathcal{R},e\\in\\mathcal{E}\\}$  , i.e. contains entities  $\\mathcal{E}_{<t}$   and all facts they participate in. \n2.2 Generative KG Language Model \nThe primary goal of the knowledge graph lan- guage model (KGLM) is to enable a neural lan- guage model to generate entities and facts from a knowledge graph. To encourage the model to generate facts that have appeared in the context already, KGLM will maintain a local knowledge graph containing all facts involving entities that have appeared in the context. As the model decides to refer to entities that have not been referred to yet, it will grow the local knowledge graph with additional entities and facts to reﬂect the new entity. \nFormally, we will compute    $p(x_{t},\\mathcal{E}_{t}|x_{<t},\\mathcal{E}_{<t})$  where    $\\boldsymbol{x}_{<t}$   is the sequence of observed tokens,  $\\mathcal{E}_{<t}$  is the set of entities mentioned in  $\\boldsymbol{x}_{<t}$  , and    $\\mathcal{K G}_{<t}$   is the local knowledge graph determined by    $\\mathcal{E}_{<t}$  , as described above. The generative process is: "}
{"page": 2, "image_path": "doc_images/P19-1598_2.jpg", "ocr_text": "Super Mario Land is a 1989 side-scrolling platform video game developed and published by Nintendo\n\nt parent from local entities Pr er xX\nSuper r,\nplatform game O Mario Land t Nintendo standard vocabulary\nPUBLISHER\nside-scrolling game (_) a\nRelation t ~ the\n[RSE D Bi ———+ super mario Land (_) ||\nExisting Entity Game Boy\nO ty company\ndog\nplatform\npick from all entities Ey game\nli yf\nAAA Inc. O aliases of er\nO Kabushiki\nMention of a Distribution over standard Koppai\nNew Entity Sony Inc. oO vocabulary and aliases of e;\n© — |\nZzyzx, CA oO\nNot an é= (7) Distribution over\nEntity Mention t7 standard vocabulary\n\nFigure 2: KGLM Illustration. When trying to generate the token following “published by”, the model first decides\nthe type of the mention (¢,) to be a related entity (darker indicates higher probability), followed by identifying the\nparent (p;), relation (r;), and entity to render (e;) from the local knowledge graph as (Super Mario Land, Publisher,\nNintendo). The final distribution over the words includes the standard vocabulary along with aliases of Nintendo,\nand the model selects “Nintendo” as the token x;. Facts related to Nintendo will be added to the local graph.\n\ne Decide the type of x, which we denote by\nt;: whether it is a reference to an entity in\nKG <; (related), a reference to an entity not in\nKG <t (new), or not an entity mention (9).\n\ne If t; = new then choose the upcoming entity e;\nfrom the set of all entities €.\n\ne If t; = related then:\n\n— Choose a parent entity p; from €<;.\n\n— Choose a factual relation 7; to render,\nre € {(p.7,€) € KG celp = pr}.\n\n— Choose e; as one of the tail entities,\nen € {e (pt. Tt,e) € KG ct}.\n\ne Ift; =@thene; = 9.\n\ne Generate x; conditioned on e;, potentially copy-\ning one of e;’s aliases.\n\ne Ife, ¢ Ext, then Ec (441) — Ex U {er},\nelse E<(t41) — Ext.\n\nFor the model to refer to an entity it has already\nmentioned, we introduce a Reflexive relation that\nself-relates, i.e. p = e for (p, Reflexive, e).\n\nAn illustration of this process and the variables\nis provided in Figure 2, for generating a token in\nthe middle of the same sentence as in Figure 1.\nAmongst the three mention types (¢;), the model\nchooses a reference to existing entity, which re-\nquires picking a fact to render. As the parent entity\nof this fact (p;), the model picks Super Mario Land,\nand then follows the Publisher relation (r;) to se-\n\nlect Nintendo as the entity to render (e;). When\nrendering Nintendo as a token 2;, the model has an\nexpanded vocabulary available to it, containing the\nstandard vocabulary along with all word types in\nany of the aliases of e;.\n\nMarginalizing out the KG There is a mismatch\nbetween our initial task requirement, p(x¢|2<t),\nand the model we describe so far, which computes\np(x, Er|ver, Ect). We will essentially marginal-\nize out the local knowledge graph to compute the\nprobability of the tokens, i.e. p(x) = )\\¢ p(x, €).\nWe will clarify this, along with describing the train-\ning and the inference/decoding algorithms for this\nmodel and other details of the setup, in Section 4.\n\n2.3 Parameterizing the Distributions\n\nThe parametric distributions used in the generative\nprocess above are defined as follows. We begin\nby computing the hidden state h; using the for-\nmula in Eqn (1). We then split the vector into\nthree components: hy = [hy2; hyp; hy], which\nare respectively used to predict words, parents, and\nrelations. The type of the token, t;, is computed\nusing a single-layer softmax over h;., to predict\none of {new, related, 0}.\n\nPicking an Entity We also introduce pretrained\nembeddings for all entities and relations in the\n\n5964\n", "vlm_text": "The image is an illustration of a process involving a model, likely a Knowledge Graph Language Model (KGLM), generating a token based on the context of a sentence and related knowledge graph information. The caption explains the model's operation: \n\n1. **Token Type Decision**: The model first determines the type of mention ($t_t$) following the phrase \"published by\". Here, it decides on \"Relation to Existing Entity\".\n\n2. **Entity Selection**: Once the type is decided, the model identifies the parent entity ($p_t$) from a pool of local entities, selecting \"Super Mario Land\".\n\n3. **Relation Identification**: Given the parent entity, the model chooses a relation ($r_t$), which is \"PUBLISHER\" for this instance.\n\n4. **Entity Rendering**: Using the parent entity and the selected relation, the model identifies the specific entity to render ($e_t$), which is \"Nintendo\" in this case, from the local knowledge graph.\n\n5. **Token Generation**: The model then generates the next token ($x_t$) — \"Nintendo\" — from a distribution that includes both the standard vocabulary and aliases for \"Nintendo\".\n\nThis detailed chain of reasoning allows the model to update its local graph with facts related to \"Nintendo\" for further context-driven generation tasks. The image uses boxes, circles, and arrows to represent the different steps and choices made during this process.\n•  Decide the  type  of    $x_{t}$  , which we denote by  $t_{t}$  : whether it is a reference to an entity in  $\\mathcal{K G}_{<t}$   ( related ), a reference to an entity not in  $\\mathcal{K G}_{<t}$   ( new ), or not an entity mention   $(\\emptyset)$  .\n\n •  If    $t_{t}=$   new  then choose the upcoming entity    $e_{t}$  from the set of all entities  $\\mathcal{E}$  .\n\n •  If    $t_{t}=$  related  then: –  Choose a parent entity  $p_{t}$   from    $\\mathcal{E}_{<t}$  . –  Choose a factual relation    $r_{t}$   to render,  $r_{t}\\in\\{(p,r,e)\\in\\mathcal{K G}_{<t}|p=p_{t}\\}.$  . –  Choose  $e_{t}$   as one of the tail entities,  $e_{t}\\in\\{e|(p_{t},r_{t},e)\\in\\mathcal{K G}_{<t}\\}.$  .\n\n •  If    $t_{t}=\\emptyset$  then    $e_{t}=\\emptyset$  .\n\n •  Generate    $x_{t}$   conditioned on  $e_{t}$  , potentially copy- ing one of    $e_{t}$  ’s aliases.\n\n •  If    $e_{t}\\notin\\mathcal{E}_{<t}$  , then    $\\mathcal{E}_{<(t+1)}\\gets\\mathcal{E}_{<t}\\cup\\left\\{e_{t}\\right\\},$  , else    $\\mathcal{E}_{<(t+1)}\\leftarrow\\mathcal{E}_{<t}$  . \nFor the model to refer to an entity it has already mentioned, we introduce a  Reﬂexive  relation that self-relates, i.e.  $p=e$   for    $(p,R e f I e x i v e,e)$  . \nAn illustration of this process and the variables is provided in Figure  2 , for generating a token in the middle of the same sentence as in Figure  1 . Amongst the three mention types   $(t_{t})$  , the model chooses a reference to existing entity, which re- quires picking a fact to render. As the parent entity of this fact   $(p_{t})$  , the model picks  Super Mario Land , and then follows the  Publisher  relation   $(r_{t})$   to se- lect  Nintendo  as the entity to render   $(e_{t})$  . When rendering  Nintendo  as a token    $x_{t}$  , the model has an expanded  vocabulary available to it, containing the standard vocabulary along with all word types in any of the aliases of    $e_{t}$  . \n\nMarginalizing out the KG  There is a mismatch between our initial task requirement,    $p(x_{t}|\\boldsymbol{x}_{<t})$  , and the model we describe so far, which computes  $p(x_{t},\\mathcal{E}_{t}|x_{<t},\\mathcal{E}_{<t})$  . We will essentially  marginal- ize  out the local knowledge graph to compute the probability of the tokens, i.e.    $\\begin{array}{r}{p(\\mathbf{x})=\\sum_{\\pmb{\\varepsilon}}p(\\mathbf{x},\\pmb{\\mathcal{E}})}\\end{array}$  . We will clarify this, along with describing the train- ing and the inference/decoding algorithms for this model and other details of the setup, in Section  4 . \n2.3 Parameterizing the Distributions \nThe parametric distributions used in the generative process above are deﬁned as follows. We begin by computing the hidden state    $\\mathbf{h}_{t}$   using the for- mula in Eqn  ( 1 ) . We then split the vector into three components:    $\\mathbf{h}_{t}\\,=\\,\\left[\\mathbf{h}_{t,x};\\mathbf{h}_{t,p};\\mathbf{h}_{t,r}\\right]$  , which are respectively used to predict words, parents, and relations. The type of the token,    $t_{t}$  , is computed using a single-layer softmax over    $\\mathbf{h}_{t,x}$   to predict one of  { new ,  related ,  $\\varnothing\\}$  . \nPicking an Entity  We also introduce pretrained embeddings for all entities and relations in the knowledge graph, denoted by    ${\\bf v}_{e}$   for entity    $e$   and  $\\mathbf{v}_{r}$   for relation    $r$  . To select    $e_{t}$   from all entities in case  $t_{t}=$   new , we use: "}
{"page": 3, "image_path": "doc_images/P19-1598_3.jpg", "ocr_text": "knowledge graph, denoted by v- for entity e and\nv, for relation r. To select e; from all entities in\ncase t; = new, we use:\n\np(er) = softmax(ve - (hip + h,,,))\n\nover all e € €. The reason we add h,,, and h,,, is\nto mimic the structure of TransE, which we use to\nobtain entity and relation embeddings. Details on\nTransE will be provided in Section 4. For mention\nof a related entity, t; = related, we pick a parent\nentity p; using\n\nP(p) = softmax(v, - hyp)\nover all p € &, then pick the relation r; using\np(rt) = softmax(v;. - hy)\n\nover all r € {r|(pi,7,e) € KGi}. The combina-\ntion of p,; and r; determine the entity e; (which\nmust satisfy (pz, 77, ez) € KGz; if there are multi-\nple options one is chosen at random).\n\nRendering the Entity If ce, = 9, i.e. there is\nno entity to render, we use the same distribution\nover the vocabulary as in Eqn (1) - a softmax using\nhy,x. If there is an entity to render, we construct\nthe distribution over the original vocabulary and\na vocabulary containing all the tokens that appear\nin aliases of e;. This distribution is conditioned\non e; in addition to z;. To compute the scores\nover the original vocabulary, h;, is replaced by\n\ntx = Woroj[ht,c3 Ve,] where Woroj is a learned\nweight matrix that projects the concatenated vector\ninto the same vector space as hy...\n\nTo obtain probabilities for words in the alias\nvocabulary, we use a copy mechanism Gu et al.\n(2016). The token sequences comprising each alias\n{a;} are embedded then encoded using an LSTM\nto form vectors aj. Copy scores are computed as:\n\np(a = aj) « exp [> ((hi,.)” Weopy) a)|\n\n3 Linked WikiText-2\n\nModeling aside, one of the primary barriers to in-\ncorporating factual knowledge into language mod-\nels is that training data is hard to obtain. Standard\nlanguage modeling corpora consist only of text,\nand thus are unable to describe which entities or\nfacts each token is referring to. In contrast, while\nrelation extraction datasets link text to a knowledge\n\ngraph, the text is made up of disjoint sentences that\ndo not provide sufficient context to train a pow-\nerful language model. Our goals are much more\naligned to the data-to-text task (Ahn et al., 2016;\nLebret et al., 2016; Wiseman et al., 2017; Yang\net al., 2017; Gardent et al., 2017; Ferreira et al.,\n2018), where a small table-sized KB is provided to\ngenerate a short piece of text; we are interested in\nlanguage models that dynamically decide the facts\nto incorporate from the knowledge graph, guided\nby the discourse.\n\nFor these reasons we introduce the Linked\nWikiText-2 dataset, consisting of (approximately)\nthe same articles appearing in the WikiText-2 lan-\nguage modeling corpus, but linked to the Wiki-\ndata (Vrande¢ié and Krétzsch, 2014) knowledge\ngraph. Because the text closely matches, mod-\nels trained on Linked WikiText-2 can be compared\nto models trained on WikiText-2. Furthermore,\nbecause many of the facts in Wikidata are de-\nrived from Wikipedia articles, the knowledge graph\nhas a good coverage of facts expressed in the\ntext. The dataset is available for download at:\nhttps://rloganiv.github.io/linked-wikitext-2. Our\nsystem annotates one document at a time, and con-\nsists of entity linking, relation annotations, and\npost-processing. The following paragraphs de-\nscribe each step in detail.\n\nInitial entity annotations We begin by identify-\ning an initial set of entity mentions within the text.\nThe primary source of these mentions is the human-\nprovided links between Wikipedia articles. When-\never a span of text is linked to another Wikipedia\narticle, we associate its corresponding Wikidata\nentity with the span. While article links provide a\nlarge number of gold entity annotations, they are in-\nsufficient for capturing all of the mentions in the ar-\nticle since entities are only linked the first time they\noccur. Accordingly, we use the neural-el (Gupta\net al., 2017) entity linker to identify additional links\nto Wikidata, and identify coreferences using Stan-\nford CoreNLP? to cover pronouns, nominals, and\nother tokens missed by the linker.\n\nLocal knowledge graph The next step iteratively\ncreates a generative story for the entities using rela-\ntions in the knowledge graph as well as identifies\nnew entities. To do this, we process the text token\nby token. Each time an entity is encountered, we\nadd all of the related entities in Wikidata as candi-\n\n*https://stanfordnip.github.io/CoreNLP/\n\n5965\n", "vlm_text": "\n\n$$\np(e_{t})=\\mathrm{softmax}(\\mathbf{v}_{e}\\cdot(\\mathbf{h}_{t,p}+\\mathbf{h}_{t,r}))\n$$\n \nover all    $e\\in\\mathcal E$  . The reason we add  $\\mathbf{h}_{t,p}$   and    $\\mathbf{h}_{t,r}$   is to mimic the structure of TransE, which we use to obtain entity and relation embeddings. Details on TransE will be provided in Section  4 . For mention of a related entity,    $t_{t}=$  related , we pick a parent entity  $p_{t}$   using \n\n$$\np(p_{t})=\\mathrm{softmax}(\\mathbf{v}_{p}\\cdot\\mathbf{h}_{t,p})\n$$\n \nover all  $p\\in\\mathcal{E}_{t}$  , then pick the relation    $r_{t}$   using \n\n$$\np(r_{t})=\\mathrm{softmax}(\\mathbf{v}_{r}\\cdot\\mathbf{h}_{t,r})\n$$\n \nover all    $r\\,\\in\\,\\{r|(p_{t},r,e)\\,\\in\\,\\mathcal{K G}_{t}\\}$  . The combina- tion of    $p_{t}$   and    $r_{t}$   determine the entity    $e_{t}$   (which must satisfy    $\\left(p_{t},r_{t},e_{t}\\right)\\in\\mathcal{K G}_{t}$  ; if there are multi- ple options one is chosen at random). \nRendering the Entity  If    $e_{t}~=~\\emptyset$  , i.e. there is no entity to render, we use the same distribution over the vocabulary as in Eqn  ( 1 )  - a softmax using  $\\mathbf{h}_{t,x}$  . If there is an entity to render, we construct the distribution over the original vocabulary  and a vocabulary containing all the tokens that appear in aliases of    $e_{t}$  . This distribution is conditioned on    $e_{t}$   in addition to    $x_{t}$  . To compute the scores over the original vocabulary,    $\\mathbf{h}_{t,x}$   is replaced by  $\\mathbf{h}_{t,x}^{\\prime}=\\mathbf{W}_{\\mathrm{proj}}[\\mathbf{h}_{t,x};\\mathbf{v}_{e_{t}}]$   where    $\\mathbf{W}_{\\mathrm{proj}}$    is a learned weight matrix that projects the concatenated vector into the same vector space as  $\\mathbf{h}_{t,x}$  . \nTo obtain probabilities for words in the alias vocabulary, we use a copy mechanism  Gu et al.\n\n ( 2016 ). The token sequences comprising each alias\n\n  $\\{a_{j}\\}$   are embedded then encoded using an LSTM to form vectors    $\\mathbf{a}_{j}$  . Copy scores are computed as: \n\n$$\np(x_{t}=a_{j})\\propto\\exp\\left[\\sigma\\left(\\left(\\mathbf{h}_{t,x}^{\\prime}\\right)^{T}\\mathbf{W}_{\\mathrm{conv}}\\right)\\mathbf{a}_{j}\\right]\n$$\n \nLinked WikiText-2 3 \nModeling aside, one of the primary barriers to in- corporating factual knowledge into language mod- els is that training data is hard to obtain. Standard language modeling corpora consist only of text, and thus are unable to describe which entities or facts each token is referring to. In contrast, while relation extraction datasets link text to a knowledge graph, the text is made up of disjoint sentences that do not provide sufﬁcient context to train a pow- erful language model. Our goals are much more aligned to the  data-to-text  task ( Ahn et al. ,  2016 ; Lebret et al. ,  2016 ;  Wiseman et al. ,  2017 ;  Yang et al. ,  2017 ;  Gardent et al. ,  2017 ;  Ferreira et al. , 2018 ), where a small table-sized KB is provided to generate a short piece of text; we are interested in language models that dynamically decide the facts to incorporate from the knowledge graph, guided by the discourse. \n\nFor these reasons we introduce the  Linked WikiText-2  dataset, consisting of (approximately) the same articles appearing in the  WikiText-2  lan- guage modeling corpus, but linked to the Wiki- data ( Vrandeˇ ci´ c and Krötzsch ,  2014 ) knowledge graph. Because the text closely matches, mod- els trained on  Linked WikiText-2  can be compared to models trained on  WikiText-2 . Furthermore, because many of the facts in Wikidata are de- rived from Wikipedia articles, the knowledge graph has a good coverage of facts expressed in the text. The dataset is available for download at: https://rloganiv.github.io/linked-wikitext-2 . Our system annotates one document at a time, and con- sists of entity linking, relation annotations, and post-processing. The following paragraphs de- scribe each step in detail. \nInitial entity annotations  We begin by identify- ing an initial set of entity mentions within the text. The primary source of these mentions is the human- provided links between Wikipedia articles. When- ever a span of text is linked to another Wikipedia article, we associate its corresponding Wikidata entity with the span. While article links provide a large number of gold entity annotations, they are in- sufﬁcient for capturing all of the mentions in the ar- ticle since entities are only linked the ﬁrst time they occur. Accordingly, we use the neural-el ( Gupta et al. ,  2017 ) entity linker to identify additional links to Wikidata, and identify coreferences using Stan- ford  $\\scriptstyle{\\mathrm{CoreNLP}}^{2}$    to cover pronouns, nominals, and other tokens missed by the linker. \nLocal knowledge graph  The next step iteratively creates a generative story for the entities using rela- tions in the knowledge graph as well as identiﬁes new entities. To do this, we process the text token by token. Each time an entity is encountered, we add all of the related entities in Wikidata as candi- "}
{"page": 4, "image_path": "doc_images/P19-1598_4.jpg", "ocr_text": "Tokens x; Super Mario Land is a 1989 side - scrolling platform video game developed\n\nMention type t; new 0 0 related new related 0\n\nEntity Mentioned; “SML_ 0 @ 04-21-1989 SIDE_SCROLL PVG 0\n\nRelation 7: 0 0 @ pub date 0 genre 0\n\nParent Entity p; 0 oo By i) ‘SML_ i)\n\na- and published by Nintendo as a launch title for their Game Boy handheld game console\nt O 0 0 related 0 O new 0 O related related 0\ne 0 0 o [NN oo 0 0  (GAME_BOY) HGC )\nre 0 0 0 pub @ @ 0 0 0 = R:manu / platform instance of 0\npo 0 o BM oo 0 9 o Win) Sy GAME_BOY =)\n\nTable 1: Example Annotation of the sentence from Figure 1, including corresponding variables from Figure 2.\nNote that Game Boy has multiple parent and relation annotations, as the platform for Super Mario Land and as\nmanufactured by Nintendo. Wikidata identifiers are made human-readable (e.g., SML is Q647249) for clarity.\n\ndates for matching. If one of these related entities\nis seen later in the document, we identify the entity\nas a parent for the later entity. Since multiple re-\nlations may appear as explanations for each token,\nwe allow a token to have multiple facts.\n\nExpanding the annotations Since there may be\nentities that were missed in the initial set, as well\nas non-entity tokens of interest such as dates and\nquantities we further expand the entity annotations\nusing string matching. For entities, we match the\nset of aliases provided in Wikidata. For dates, we\ncreate an exhaustive list of all of the possible ways\nof expressing the date (e.g. \"December 7, 1941\",\n\"7-12-1941\", \"1941\", ...). We perform a similar\napproach for quantities, using the pint library in\nPython to handle the different ways of expressing\nunits (e.g. \"g\", \"gram\", ...). Since there are many\nways to express a numerical quantity, we only ren-\nder the quantity at the level of precision supplied\nby Wikidata, and do not perform unit conversions.\n\nExample Annotation An example annotation is\nprovided in Table | corresponding to the instance in\nFigure 1, along with the variables that correspond\nto the generative process of the knowledge graph\nlanguage model (KGLM). The entity mentioned for\nmost tokens here are human-provided links, apart\nfrom “J989” that is linked to 04-21-1989 by the\nstring matching process. The annotations indicate\nwhich of the entities are new and related based on\nwhether they are reachable by entities linked so far,\nclearly making a mistake for side-scrolling game\nand platform video game due to missing links in\nWikidata. Finally, multiple plausible reasons for\nGame Boy are included: it’s the platform for Super\nMario Land and it is manufactured by Nintendo,\neven though only the former is more relevant here.\n\nTrain Dev Test\nDocuments 600 60 60\nTokens 2,019,195 207,982 236,062\nVocab. Size 33,558 - -\nMention Tokens 207,803 21,226 = 24,441\nMention Spans 122,983 12,214 15,007\nUnique Entities 41,058 5,415 5,625\nUnique Relations 1,291 484 504\n\nTable 2: Linked WikiText-2 Corpus Statistics.\n\nEven with these omissions and mistakes, it is clear\nthat the annotations are rich and detailed, with a\nhigh coverage, and thus should prove beneficial for\ntraining knowledge graph language models.\n\nDataset Statistics Statistics for Linked WikiText-2\nare provided in Table 2. In this corpus, more than\n10% of the tokens are considered entity tokens, i.e.\nthey are generated as factual references to informa-\ntion in the knowledge graph. Each entity is only\nmentioned a few times (less than 5 on average, with\na long tail), and with more than thousand different\nrelations. Thus it is clear that regular language\nmodels would not be able to generate factual text,\nand there is a need for language models to be able\nto refer to external sources of information.\n\nDifferences from WikiText-2 Although our\ndataset is designed to closely replicate WikiText-2,\nthere are some differences that prevent direct com-\nparison. Firstly, there are minor variations in text\nacross articles due to edits between download dates.\nSecondly, according to correspondence with Merity\net al. (2017), WikiText-2 was collected by querying\nthe Wikipedia Text API. Because this API discards\nuseful annotation information (e.g. article links),\nLinked WikiText-2 instead was created by directly\nfrom the article HTML.\n\n5966\n", "vlm_text": "This table appears to be a structured representation of a text passage about the video game \"Super Mario Land.\" It breaks down the passage into tokens, mentions, entities, relations, and parent entities. Here's how the table is organized and what each row signifies:\n\n1. **Tokens (xt):** These are the individual words or tokens in the text passage. The phrase in the table is: \"Super Mario Land is a 1989 side-scrolling platform video game developed and published by Nintendo as a launch title for their Game Boy handheld game console.\"\n\n2. **Mention type (tt):** This represents the type of mention a token has. For example, \"new\" denotes a new mention of an entity, while \"related\" suggests a reference to an existing concept.\n\n3. **Entity Mentioned (et):** This denotes the entity associated with a token. Examples in the table include \"SML\" (presumably for Super Mario Land), \"SIDE_SCROLL,\" and \"NIN\" (likely referring to Nintendo).\n\n4. **Relation (rt):** This indicates the relationship between entities. For instance, \"pub date\" connects \"1989\" with a publication date of \"Super Mario Land,\" and \"genre\" relates \"video game\" to \"SML.\"\n\n5. **Parent Entity (pt):** This identifies the parent entity connected to other entities through relationships. For example, \"SML\" is the parent entity for the publication date \"04-21-1989\" and the genre \"PVG\" (platform video game).\n\nEntities are color-coded in the table (e.g., \"SML\" in green, \"NIN\" in orange, and \"GAME_BOY\" in brown), which helps distinguish different types of entities and their relevant connections. The table provides a detailed breakdown of how entities and their relationships are identified within the text passage.\ndates for matching. If one of these related entities is seen later in the document, we identify the entity as a parent for the later entity. Since multiple re- lations may appear as  explanations  for each token, we allow a token to have multiple facts. \nExpanding the annotations  Since there may be entities that were missed in the initial set, as well as non-entity tokens of interest such as dates and quantities we further expand the entity annotations using string matching. For entities, we match the set of aliases provided in Wikidata. For dates, we create an exhaustive list of all of the possible ways of expressing the date (e.g. \" December 7, 1941 \", \" 7-12-1941 \", \" 1941 \", ...). We perform a similar approach for quantities, using the  pint  library in Python to handle the different ways of expressing units (e.g. \" g \", \" gram \", ...). Since there are many ways to express a numerical quantity, we only ren- der the quantity at the level of precision supplied by Wikidata, and do not perform unit conversions. \nExample Annotation  An example annotation is provided in Table  1  corresponding to the instance in Figure  1 , along with the variables that correspond to the generative process of the knowledge graph language model (KGLM). The entity mentioned for most tokens here are human-provided links, apart from “ 1989 ” that is linked to  04-21-1989  by the string matching process. The annotations indicate which of the entities are  new  and  related  based on whether they are reachable by entities linked so far, clearly making a mistake for  side-scrolling game and  platform video game  due to missing links in Wikidata. Finally, multiple plausible reasons for Game Boy  are included: it’s the platform for  Super Mario Land  and it is manufactured by  Nintendo , even though only the former is more relevant here. \nThe table provides quantitative details about datasets used in machine learning or natural language processing, likely related to a task involving named entities and relationships between them across different dataset splits: Training (Train), Development (Dev), and Testing (Test). Here's a breakdown of the data:\n\n- **Documents**: \n  - Train: 600\n  - Dev: 60\n  - Test: 60\n  \n- **Tokens** (total number of word tokens):\n  - Train: 2,019,195\n  - Dev: 207,982\n  - Test: 236,062\n  \n- **Vocab. Size** (number of unique words in the dataset):\n  - Train: 33,558\n  - Dev: Not provided\n  - Test: Not provided\n  \n- **Mention Tokens** (tokens that are part of entity mentions):\n  - Train: 207,803\n  - Dev: 21,226\n  - Test: 24,441\n  \n- **Mention Spans** (number of spans or occurrences of entity mentions):\n  - Train: 122,983\n  - Dev: 12,214\n  - Test: 15,007\n  \n- **Unique Entities** (number of distinct entities):\n  - Train: 41,058\n  - Dev: 5,415\n  - Test: 5,625\n  \n- **Unique Relations** (number of distinct types of relations):\n  - Train: 1,291\n  - Dev: 484\n  - Test: 504\n\nThis table reflects the distribution and composition of data available for training, tuning (development), and evaluating (testing) a model.\nEven with these omissions and mistakes, it is clear that the annotations are rich and detailed, with a high coverage, and thus should prove beneﬁcial for training knowledge graph language models. \nDataset Statistics  Statistics for  Linked WikiText-2 are provided in Table  2 . In this corpus, more than  $10\\%$   of the tokens are considered entity tokens, i.e. they are generated as factual references to informa- tion in the knowledge graph. Each entity is only mentioned a few times (less than  5  on average, with a long tail), and with more than thousand different relations. Thus it is clear that regular language models would not be able to generate factual text, and there is a need for language models to be able to refer to external sources of information. \nDifferences from  WikiText-2 Although our dataset is designed to closely replicate  WikiText-2 , there are some differences that prevent direct com- parison. Firstly, there are minor variations in text across articles due to edits between download dates. Secondly, according to correspondence with  Merity et al.  ( 2017 ),  WikiText-2  was collected by querying the Wikipedia Text API. Because this API discards useful annotation information (e.g. article links), Linked WikiText-2  instead was created by directly from the article HTML. "}
{"page": 5, "image_path": "doc_images/P19-1598_5.jpg", "ocr_text": "4 Training and Inference for KGLM\n\nIn this section, we describe the training and infer-\nence algorithm for KGLM.\n\nPretrained KG Embeddings During evaluation,\nwe may need to make predictions on entities and\nrelations that have not been seen during training.\nAccordingly, we use fixed entity and relations em-\nbeddings pre-trained using TransE (Bordes et al.,\n2013) on Wikidata. Given (p,r, e), we learn em-\nbeddings v,, vr and ve to minimize the distance:\n\n(Vp, Vr, Ve) = lp + Vp — vell? :\nWe use a max-margin loss to learn the embeddings:\nL = max (0,7 + 5 (Vp; Vr, Ve) — 6 (V),, Vrs VE)\n\nwhere ¥ is the margin, and either p’ or e’ is a ran-\ndomly chosen entity embedding.\n\nTraining with Linked WikiText-2 Although the\ngenerative process in KGLM involves many steps,\ntraining the model on Linked WikiText-2 is straight-\nforward. Our loss objective is the negative log-\nlikelihood of the training data:\n\n(0) = S log p(ae, E:|vet, Ect; 9),\nt\n\nwhere 9 is the set of model parameters. Note that\nif an annotation has multiple viable parents such as\nGame Boy in 1, then we marginalize over all of the\nparents. Since all random variables are observed,\ntraining can performed using off-the-shelf gradient-\nbased optimizers.\n\nInference While observing annotations makes the\nmodel easy to train, we do not assume that the\nmodel has access to annotations during evaluation.\nFurthermore, as discussed in Section 2.2, the goal\nin language modelling is to measure the marginal\nprobability p(x) = }¢ p(x, €) not the joint proba-\nbility. However, this sum is intractable to compute\ndue to the large combinatorial space of possible\nannotations. We address this problem by approxi-\nmating the marginal distribution using importance\nsampling. Given samples from a proposal distribu-\ntion q(E|x) the marginal distribution is:\n\n_y~., _woPé)\np(x) = DPE) = » a(Ejx) 1 EP)\n\nw~ Ly ee)\nND G(Elx)\n\nThis approach is used to evaluate models in Ji et al.\n(2017) and Dyer et al. (2016). Following Ji et al.\n(2017), we compute g (€|x) using a discriminative\nversion of our model that predicts annotations for\nthe current token instead of for the next token.\n\n5 Experiments\n\nTo evaluate the proposed language model, we\nfirst introduce the baselines, followed by an evalua-\ntion using perplexity of held-out corpus, accuracy\non fact completion, and an illustration of how the\nmodel uses the knowledge graph.\n\n5.1 Evaluation Setup\n\nBaseline Models We compare KGLM to the fol-\nlowing baseline models:\n\ne AWD-LSTM (Merity et al., 2018): strong\nLSTM-based model used as the foundation of\nmost state-of-the-art models on WikiText-2.\nENTITYNLM (Ji et al., 2017): an LSTM-based\nlanguage model with the ability to track entity\nmentions. Embeddings for entities are created dy-\nnamically, and are not informed by any external\nsources of information.\n\nEntityCopyNet: a variant of the KGLM where\nt, = new for all mentions, i.e. entities are\nselected from € and entity aliases are copied, but\nrelations in the knowledge graph are unused.\n\nHyperparameters We pre-train 256 dimensional\nentity and relation embeddings for all entities\nwithin two hops of the set of entities that occur in\nLinked WikiText-2 using TransE with margin y = 1.\nWeights are tied between all date embeddings and\nbetween all quantity embeddings to save memory.\nFollowing Merity et al. (2018) we use 400 dimen-\nsional word embeddings and a 3 layer LSTM with\nhidden dimension 1150 to encode tokens. We also\nemploy the same regularization strategy (DropCon-\nnect (Wan et al., 2013) + Dropout(Srivastava et al.,\n2014)) and weight tying approach. However, we\nperform optimization using Adam (Kingma and Ba,\n2015) with learning rate le-3 instead of NT-ASGD,\nhaving found that it is more stable.\n\n5.2. Results\n\nPerplexity We evaluate our model using the stan-\ndard perplexity metric: exp G 4 log p(x):\nHowever, perplexity suffers from the issue that it\n\n5967\n", "vlm_text": "4 Training and Inference for KGLM \nIn this section, we describe the training and infer- ence algorithm for KGLM. \nPretrained KG Embeddings  During evaluation, we may need to make predictions on entities and relations that have not been seen during training. Accordingly, we use ﬁxed entity and relations em- beddings pre-trained using TransE ( Bordes et al. , 2013 ) on Wikidata. Given    $(p,r,e)$  , we learn em- beddings    $\\mathbf{v}_{p},\\,\\mathbf{v}_{r}$   and    ${\\bf v}_{e}$   to minimize the distance: \n\n$$\n\\delta(\\mathbf{v}_{p},\\mathbf{v}_{r},\\mathbf{v}_{e})=\\left\\|\\mathbf{v}_{p}+\\mathbf{v}_{r}-\\mathbf{v}_{e}\\right\\|^{2}.\n$$\n \nWe use a max-margin loss to learn the embeddings: \n\n$$\n\\mathcal{L}=\\operatorname*{max}\\left(0,\\gamma+\\delta\\left(\\mathbf{v}_{p},\\mathbf{v}_{r},\\mathbf{v}_{e}\\right)-\\delta\\left(\\mathbf{v}_{p}^{\\prime},\\mathbf{v}_{r},\\mathbf{v}_{e}^{\\prime}\\right)\\right)\n$$\n \nwhere  $\\gamma$   is the margin, and either  $p^{\\prime}$    or    $e^{\\prime}$    is a ran- domly chosen entity embedding. \nTraining with  Linked WikiText-2  Although the generative process in KGLM involves many steps, training the model on  Linked WikiText-2  is straight- forward. Our loss objective is the negative log- likelihood of the training data: \n\n$$\n\\ell(\\Theta)=\\sum_{t}\\log p(x_{t},\\mathcal{E}_{t}|\\boldsymbol{x}_{<t},\\mathcal{E}_{<t};\\Theta),\n$$\n \nwhere    $\\Theta$   is the set of model parameters. Note that if an annotation has multiple viable parents such as Game Boy  in  1 , then we marginalize over all of the parents. Since all random variables are observed, training can performed using off-the-shelf gradient- based optimizers. \nInference  While observing annotations makes the model easy to train, we do not assume that the model has access to annotations during evaluation. Furthermore, as discussed in Section  2.2 , the goal in language modelling is to measure the marginal probability  $\\begin{array}{r}{p(\\mathbf{x})=\\sum_{\\pmb{\\varepsilon}}p(\\mathbf{x},\\pmb{\\mathcal{E}})}\\end{array}$   not the joint proba- bility. However, this sum is intractable to compute due to the large combinatorial space of possible annotations. We address this problem by approxi- mating the marginal distribution using importance sampling. Given samples from a proposal distribu- tion    $q(\\pmb{\\mathcal{E}}|\\mathbf{x})$   the marginal distribution is: \n\n$$\n\\begin{array}{l}{p\\displaystyle(\\mathbf{x})=\\sum_{\\pmb{\\varepsilon}}p\\left(\\mathbf{x},\\pmb{\\mathcal{E}}\\right)=\\sum_{\\pmb{\\varepsilon}}\\frac{p\\left(\\mathbf{x},\\pmb{\\mathcal{E}}\\right)}{q\\left(\\pmb{\\mathcal{E}}|\\mathbf{x}\\right)}q\\left(\\pmb{\\mathcal{E}}|\\mathbf{x}\\right)}\\\\ {\\quad\\approx\\displaystyle\\frac{1}{N}\\sum_{\\pmb{\\varepsilon}\\sim q}\\frac{p\\left(\\mathbf{x},\\pmb{\\mathcal{E}}\\right)}{q\\left(\\pmb{\\mathcal{E}}|\\mathbf{x}\\right)}}\\end{array}\n$$\n \nThis approach is used to evaluate models in  Ji et al.\n\n ( 2017 ) and  Dyer et al.  ( 2016 ). Following  Ji et al.\n\n ( 2017 ), we compute    $q\\left(\\pmb{\\mathscr{E}}|\\mathbf{x}\\right)$   using a discriminative version of our model that predicts annotations for the current token instead of for the next token. \n5 Experiments \nTo evaluate the proposed language model, we ﬁrst introduce the baselines, followed by an evalua- tion using perplexity of held-out corpus, accuracy on fact completion, and an illustration of how the model uses the knowledge graph. \n5.1 Evaluation Setup \nBaseline Models  We compare KGLM to the fol- lowing baseline models:\n\n \n•  AWD-LSTM  ( Merity et al. ,  2018 ): strong LSTM-based model used as the foundation of most state-of-the-art models on  WikiText-2 .\n\n •  E NTITY NLM  ( Ji et al. ,  2017 ): an LSTM-based language model with the ability to track entity mentions. Embeddings for entities are created dy- namically, and are not informed by any external sources of information.\n\n •  EntityCopyNet:  a variant of the KGLM where  $t_{t}~=$   new  for all mentions, i.e. entities are selected from  $\\mathcal{E}$   and entity aliases are copied, but relations in the knowledge graph are unused. \nHyperparameters  We pre-train 256 dimensional entity and relation embeddings for all entities within two hops of the set of entities that occur in Linked WikiText-2  using TransE with margin  $\\gamma=1$  . Weights are tied between all date embeddings and between all quantity embeddings to save memory. Following  Merity et al.  ( 2018 ) we use 400 dimen- sional word embeddings and a 3 layer LSTM with hidden dimension 1150 to encode tokens. We also employ the same regularization strategy (DropCon- nect ( Wan et al. ,  2013 )  $^+$  Dropout( Srivastava et al. , 2014 )) and weight tying approach. However, we perform optimization using Adam ( Kingma and Ba , 2015 ) with learning rate 1e-3 instead of NT-ASGD, having found that it is more stable. \n5.2 Results \nPerplexity  We evaluate our model using the stan- dard  perplexity  metric:  $\\begin{array}{r}{\\exp\\Big(\\frac{1}{T}\\sum_{t=1}^{T}\\dot{\\log p(x_{t})}\\Big)}\\end{array}$  . However, perplexity suffers from the issue that it "}
{"page": 6, "image_path": "doc_images/P19-1598_6.jpg", "ocr_text": "PPL UPP\nENTITYNLM‘ (Jiet al., 2017) 85.4 189.2\nEntityCopyNet* 76.1 144.0\nAWD-LSTM (Merity et al., 2018) 74.8 165.8\nKGLM* 44.1 88.5\n\nTable 3: Perplexity Results on Linked WikiText-2. Re-\nsults for models marked with * are obtained using im-\nportance sampling.\n\noverestimates the probability of out-of-vocabulary\ntokens when they are mapped to a single UNK\ntoken. This is problematic for comparing the per-\nformance of the KGLM to traditional language\nmodels on Linked WikiText-2 since there are a large\nnumber of rare entities whose alias tokens are out-\nof-vocabulary. That is, even if the KGLM identifies\nthe correct entity and copies the correct alias token\nwith high probability, other models can attain bet-\nter perplexity by assigning a higher probability to\nUNK. Accordingly, we also measure unknown pe-\nnalized perplexity (UPP) (a.k.a adjusted perplexity)\nintroduced by Ueberla (1994), and used recently\nby Ahn et al. (2016) and Spithourakis and Riedel\n(2018). This metric penalizes the probability of\nUNK tokens by evenly dividing their probability\nmass over U, the set of tokens that get mapped\nto UNK . We can be compute UPP by replacing\np(UNK) in the perplexity above by zqP(UNK),\nwhere |2/| is estimated from the data.\n\nWe present the model perplexities in Table 3. To\nmarginalize over annotations, perplexities for the\nENTITYNLM, EntityCopyNet, and KGLM are es-\ntimated using the importance sampling approach\ndescribed in Section 4. We observe that the KGLM\nattains substantially lower perplexity than the other\nentity-based language models (44.1 vs. 76.1/85.4),\nproviding strong evidence that leveraging knowl-\nedge graphs is crucial for accurate language mod-\neling. Furthermore, KGLM significantly outper-\nforms all models in unknown penalized perplexity,\ndemonstrating its ability to generate rare tokens.\n\nFact Completion Since factual text generation\nis our primary objective, we evaluate the ability\nof language models to complete sentences with\nfactual information. We additionally compare with\nthe small GPT-2 (Radford et al., 2019), a language\nmodel trained on a much larger corpus of text. We\nselect 6 popular relations from Freebase, and write\na simple completion template for each, such as “X”\nwas born in __” for the birthplace relation. We\n\nAWD- KGLM\n\nyst™M GPT2 oracle NEL\nnation-capital 0/0 6/7 0/0 0/4\nbirthloc 0/9 14/14 94/95 85/92\nbirthdate 0/25 8/9 65 / 68 61/67\nspouse 0/0 2/3 2/2 1/19\ncity-state 0/13 62/62 9/59 4/59\nbook-author 0/2 0/0 61/62 25/28\nAverage 0.0/8.2 15.3/15.8 38.5/47.7  29.3/44.8\n\nTable 4: Fact Completion. Top-k accuracy\n\n(@1/@5,%) for predicting the next token for an incom-\nplete factual sentence. See examples in Table 5.\n\ngenerate sentences for these templates for a number\nof (X,Y) pairs for which the relation holds, and\nmanually examine the first token generated by each\nlanguage model to determine whether it is correct.\nTable 4 presents performance of each language\nmodel on the relations. The oracle KGLM is given\nthe correct entity annotation for X, while the NEL\nKGLM uses the discriminative model used for im-\nportance sampling combined with the NEL entity\nlinker to produce an entity annotation for X.\nAmongst models trained on the same data, both\nKGLM variants significantly outperform AWD-\nLSTM; they produce accurate facts, while AWD-\nLSTM produced generic, common words. KGLMs\nare also competitive with models trained on orders\nof magnitude more data, producing factual com-\npletions that require specific knowledge, such as\nbirthplaces, dates, and authors. However, they do\nnot capture facts or relations that frequently appear\nin large corpora, like the cities within states.? It is\nencouraging to see that the KGLM with automatic\nlinking performs comparably to oracle linking.\nWe provide examples in Table 5 to highlight\nqualitative differences between KGLM, trained on\n600 documents, and the recent state-of-the-art lan-\nguage model, GPT-2, trained on the WebText cor-\npus with over 8 million documents (Radford et al.,\n2019). For examples that both models get factu-\nally correct or incorrect, the generated tokens by\nKGLM are often much more specific, as opposed\nto selection of more popular/generic tokens (GPT-2\noften predicts “New York” as the birthplace, even\nfor popular entities). KGLM, in particular, gets\nfactual statements correct when the head or tail en-\ntities are rare, while GPT-2 can only complete facts\nfor more-popular entities while using more-generic\ntokens (such as “January” instead of “20”).\n\n3This is not a failure of the KG, but of the model’s ability\nto pick the correct relation from the KG given the prompt.\n\n5968\n", "vlm_text": "The table presents a comparison of four models in terms of two metrics: PPL (Perplexity) and UPP (presumably Uncertainty Perplexity or an equivalent metric).\n\n1. **ENTITYNLM** (referencing Ji et al., 2017):\n   - PPL: 85.4\n   - UPP: 189.2\n\n2. **EntityCopyNet**:\n   - PPL: 76.1\n   - UPP: 144.0\n\n3. **AWD-LSTM** (referencing Merity et al., 2018):\n   - PPL: 74.8\n   - UPP: 165.8\n\n4. **KGLM**:\n   - PPL: 44.1 (highlighted as the lowest perplexity value)\n   - UPP: 88.5\n\nThe KGLM model outperforms the others in this table, yielding the lowest scores in both PPL and UPP, which implies better performance in terms of these metrics.\noverestimates the probability of out-of-vocabulary tokens when they are mapped to a single UNK token. This is problematic for comparing the per- formance of the KGLM to traditional language models on  Linked WikiText-2  since there are a large number of rare entities whose alias tokens are out- of-vocabulary. That is, even if the KGLM identiﬁes the correct entity and copies the correct alias token with high probability, other models can attain bet- ter perplexity by assigning a higher probability to UNK. Accordingly, we also measure  unknown pe- nalized perplexity  (UPP) (a.k.a  adjusted perplexity ) introduced by  Ueberla  ( 1994 ), and used recently by  Ahn et al.  ( 2016 ) and  Spithourakis and Riedel ( 2018 ). This metric penalizes the probability of UNK tokens by evenly dividing their probability mass over  $\\mathcal{U}$  , the set of tokens that get mapped to UNK . We can be compute UPP by replacing  $p(\\mathbf{U}\\mathbf{N}\\mathbf{K})$   in the perplexity above by  $\\textstyle{\\frac{1}{|{\\mathcal{U}}|}}p(\\mathbf{U}\\mathbf{N}\\mathbf{K})$  , where    $|\\mathcal{U}|$   is estimated from the data. \nWe present the model perplexities in Table  3 . To marginalize over annotations, perplexities for the E NTITY NLM, EntityCopyNet, and KGLM are es- timated using the importance sampling approach described in Section  4 . We observe that the KGLM attains substantially lower perplexity than the other entity-based language models (44.1 vs. 76.1/85.4), providing strong evidence that leveraging knowl- edge graphs is crucial for accurate language mod- eling. Furthermore, KGLM signiﬁcantly outper- forms all models in unknown penalized perplexity, demonstrating its ability to generate rare tokens. \nFact Completion  Since factual text generation is our primary objective, we evaluate the ability of language models to complete sentences with factual information. We additionally compare with the  small  GPT-2 ( Radford et al. ,  2019 ), a language model trained on a much larger corpus of text. We select  6  popular relations from Freebase, and write a simple  completion  template for each, such as “  $X$  was born in ” for the  birthplace  relation. We \nThe table appears to present a comparison of the performance of different language models or systems—AWD-LSTM, GPT-2, and KGLM—in extracting or generating factual information across various categories. Here’s a more detailed breakdown:\n\n- **Columns:**\n  - The table has four columns labeled with different models or configurations: AWD-LSTM, GPT-2, and two configurations of KGLM (Oracle and NEL).\n\n- **Rows:**\n  - Each row represents a specific category or type of factual information:\n    - `nation-capital`\n    - `birthloc` (birth location)\n    - `birthdate`\n    - `spouse`\n    - `city-state`\n    - `book-author`\n  \n- **Values:**\n  - The values in the table are given in a \"correct/total\" format for each category and model, indicating how many facts were generated or extracted correctly out of the total number attempted.\n  \n  Example Interpretation:\n  - For the `nation-capital` category:\n    - AWD-LSTM scored 0 out of 0.\n    - GPT-2 scored 6 out of 7.\n    - KGLM (Oracle) scored 0 out of 0.\n    - KGLM (NEL) scored 0 out of 4.\n  \n- **Average:**\n  - The last row represents the average performance of each model across all categories. It seems like there might be two averages or two conditions noted, as indicated by a pair of numbers like \"0.0/8.2\" for AWD-LSTM and \"15.3/15.8\" for GPT-2.\n\nThis suggests the table is most likely used in the context of research to compare these models' abilities to handle structured or factual data. The numbers provide insight into each model's effectiveness in generating or retrieving specific types of factual information.\ngenerate sentences for these templates for a number of    $(X,Y)$   pairs for which the relation holds, and manually examine the ﬁrst token generated by each language model to determine whether it is correct. \nTable  4  presents performance of each language model on the relations. The  oracle  KGLM is given the correct entity annotation for    $X$  , while the  NEL KGLM uses the discriminative model used for im- portance sampling combined with the NEL entity linker to produce an entity annotation for    $X$  . \nAmongst models trained on the same data, both KGLM variants signiﬁcantly outperform AWD- LSTM; they produce accurate facts, while AWD- LSTM produced generic, common words. KGLMs are also competitive with models trained on orders of magnitude more data, producing factual com- pletions that require speciﬁc knowledge, such as birthplaces, dates, and authors. However, they do not capture facts or relations that frequently appear in large corpora, like the cities within states.   It is encouraging to see that the KGLM with automatic linking performs comparably to oracle linking. \nWe provide examples in Table  5  to highlight qualitative differences between KGLM, trained on 600  documents, and the recent state-of-the-art lan- guage model, GPT-2, trained on the WebText cor- pus with over  8  million documents ( Radford et al. , 2019 ). For examples that both models get factu- ally correct or incorrect, the generated tokens by KGLM are often much more speciﬁc, as opposed to selection of more popular/generic tokens (GPT-2 often predicts “New York” as the birthplace, even for popular entities). KGLM, in particular, gets factual statements correct when the head or tail en- tities are rare, while GPT-2 can only complete facts for more-popular entities while using more-generic tokens (such as “ January ” instead of   $\"20\"$  ). "}
{"page": 7, "image_path": "doc_images/P19-1598_7.jpg", "ocr_text": "Input Sentence Gold GPT-2 KGLM\nBoth correct Paris Hilton was born in___ New York City New 1981\nArnold Schwarzenegger was born on ___ 1947-07-30 July 30\nBob Dylan was born in ___ Duluth New Duluth\nKGLM correct Barack Obama was born on ___ 1961-08-04 January August\nUlysses is a book that was written by ___ James Joyce a James\nSt. Louis is a city in the state of Missouri Missouri Oldham\nGPTv2 correct — Richard Nixon was born on ___ 1913-01-09 January 20\nKanye West is married to___ Kim Kardashian Kim the\nBoth incorrect The capital of Indiais____ New Delhi the a\nMadonna is married to Carlos Leon a Alex\n\nTable 5: Completion Examples. Examples of fact completion by KGLM and GPT-2, which has been trained on\na much larger corpus. GPT-2 tends to produce very common and general tokens, such as one of a few popular\ncities to follow “born in”. KGLM sometimes makes mistakes in linking to the appropriate fact in the KG, however,\nthe generated facts are more specific and contain rare tokens. We omit AWD-LSTM from this figure as it rarely\n\nee\n\nproduced tokens apart from the generic “the” or “a”, or\n\nEffect of changing the KG For most language\nmodels, it is difficult to control their generation\nsince factual knowledge is entangled with gener-\nation capabilities of the model. For KGLM, an\nadditional benefit of its use of an external source\nof knowledge is that KGLM is directly control-\nlable via modifications to the KG. To illustrate this\ncapability with a simple example, we create com-\npletion of “Barack Obama was born on _” with\nthe original fact (Barack Obama, birthDate, 1961-\n08-04), resulting in the top three decoded tokens\nas “August”, “4”, “1961”. After changing the birth\ndate to 2013-03-21, the top three decoded tokens\nbecome “March”, “21”, “2013”. Thus, changing\nthe fact in the knowledge graph directly leads to a\ncorresponding change in the model’s prediction.\n\n6 Related Work\n\nKnowledge-based language models Our work\ndraws inspiration from two existing knowledge-\nbased language models:\n\n(i) ENTITYNLM (Ji et al., 2017) which im-\nproves a language model’s ability to track entities\nby jointly modeling named entity recognition and\ncoreference. Our model similarly tracks entities\nthrough a document, improving its ability to gener-\nate factual information by modeling entity linking\nand relation extraction.\n\nGi) The neural knowledge language model\n(NKLM) (Ahn et al., 2016) which established the\nidea of leveraging knowledge graphs in neural lan-\nguage models. The main differentiating factor be-\ntween the KGLM and NKLM is that the KGLM\noperates on an entire knowledge graph and can be\n\n“(UNK)”.\n\nevaluated on text without additional conditioning\ninformation, whereas the NKLM operates on a rel-\natively smaller set of predefined edges emanating\nfrom a single entity, and requires that entity be pro-\nvided as conditioning information ahead of time.\nThis requirement precludes direct comparison be-\ntween NKLM and the baselines in Section 5.\n\nData-to-text generation Our work is also related\nto the task of neural data-to-text generation. For\na survey of early non-neural text generation meth-\nods we refer the reader to Reiter and Dale (1997).\nRecent neural methods have been applied to gener-\nating text from tables of sports statistics (Wiseman\net al., 2017), lists and tables (Yang et al., 2017), and\nWikipedia info-boxes (Lebret et al., 2016). The pri-\nmary difference between these works and ours is\nour motivation. These works focus on generating\ncoherent text within a narrow domain (e.g. sports,\nrecipes, introductory sentences), and optimize met-\nrics such as BLEU and METEOR score. Our focus\ninstead is to use a large source of structured knowl-\nedge to improve language model’s ability to handle\nrare tokens and facts on a broad domain of topics,\nand our emphasis is on improving perplexity.\n\nGeneral language modeling Also related are the\nrecent papers proposing modifications to the AWD-\nLSTM that improve performance on Wikitext-\n2 (Gong et al., 2018; Yang et al., 2018; Krause\net al., 2018). We chose to benchmark against AWD-\nLSTM since these contributions are orthogonal,\nand many of the techniques are compatible with\nthe KGLM. KGLM improves upon AWD-LSTM,\nand we expect using KGLM in conjunction with\nthese methods will yield further improvement.\n\n5969\n", "vlm_text": "The table presents a comparison between two AI language models, GPT-2 and KGLM, in their ability to provide correct answers to various fill-in-the-blank input sentences. There are four categories of outcomes:\n\n1. **Both correct**: Both models give correct responses matching the \"Gold\" (true) answers.\n   - For example, for the input \"Paris Hilton was born in ____\", the gold answer is \"New York City\", GPT-2 predicts \"New\", and KGLM predicts \"1981\", where only the former makes sense contextually, but maybe not factually.\n\n2. **KGLM correct**: Only KGLM gives the correct answer.\n   - For instance, Bob Dylan's birthplace in \"Bob Dylan was born in ____\" is gold-answered as \"Duluth\", which KGLM gets right, whereas GPT-2 predicts \"New\".\n\n3. **GPTv2 correct**: Only GPT-2 gives the correct answer.\n   - An example is providing Richard Nixon's birth date in \"Richard Nixon was born on ____\", where the gold answer and GPT-2's prediction both start with \"January\" actually captures context and format, though not matching gold exactly.\n\n4. **Both incorrect**: Neither model gives the correct answer.\n   - For example, answering \"The capital of India is ____\", where the gold answer is \"New Delhi\", GPT-2 predicts \"the\" and KGLM predicts \"a\", neither of which is correct.\n\nThis table seems to be assessing the performance and accuracy of these language models on generating human-like responses that align with factual answers for a set of input prompts.\nEffect of changing the KG  For most language models, it is difﬁcult to control their generation since  factual  knowledge is entangled with gener- ation capabilities of the model. For KGLM, an additional beneﬁt of its use of an external source of knowledge is that KGLM is directly control- lable via modiﬁcations to the KG. To illustrate this capability with a simple example, we create com- pletion of “ Barack Obama was born on ” with the original fact ( Barack Obama ,  birthDate ,  1961- 08-04 ), resulting in the top three decoded tokens as “ August ”, “ 4 ”, “ 1961 ”. After changing the birth date to  2013-03-21 , the top three decoded tokens become “ March ”, “ 21 ”, “ 2013 ”. Thus, changing the fact in the knowledge graph directly leads to a corresponding change in the model’s prediction. \n6 Related Work \nKnowledge-based language models  Our work draws inspiration from two existing knowledge- based language models: \n(i) E NTITY NLM ( Ji et al. ,  2017 ) which im- proves a language model’s ability to track entities by jointly modeling named entity recognition and coreference. Our model similarly tracks entities through a document, improving its ability to gener- ate factual information by modeling entity linking and relation extraction. \n(ii) The neural knowledge language model (NKLM) ( Ahn et al. ,  2016 ) which established the idea of leveraging knowledge graphs in neural lan- guage models. The main differentiating factor be- tween the KGLM and NKLM is that the KGLM operates on an entire knowledge graph and can be evaluated on text without additional conditioning information, whereas the NKLM operates on a rel- atively smaller set of predeﬁned edges emanating from a single entity, and requires that entity be pro- vided as conditioning information ahead of time. This requirement precludes direct comparison be- tween NKLM and the baselines in Section  5 . \n\nData-to-text generation  Our work is also related to the task of neural data-to-text generation. For a survey of early non-neural text generation meth- ods we refer the reader to  Reiter and Dale  ( 1997 ). Recent neural methods have been applied to gener- ating text from tables of sports statistics ( Wiseman et al. ,  2017 ), lists and tables ( Yang et al. ,  2017 ), and Wikipedia info-boxes ( Lebret et al. ,  2016 ). The pri- mary difference between these works and ours is our motivation. These works focus on generating coherent text within a narrow domain (e.g. sports, recipes, introductory sentences), and optimize met- rics such as BLEU and METEOR score. Our focus instead is to use a large source of structured knowl- edge to improve language model’s ability to handle rare tokens and facts on a broad domain of topics, and our emphasis is on improving perplexity. \nGeneral language modeling  Also related are the recent papers proposing modiﬁcations to the AWD- LSTM that improve performance on  Wikitext- 2  ( Gong et al. ,  2018 ;  Yang et al. ,  2018 ;  Krause et al. ,  2018 ). We chose to benchmark against AWD- LSTM since these contributions are orthogonal, and many of the techniques are compatible with the KGLM. KGLM improves upon AWD-LSTM, and we expect using KGLM in conjunction with these methods will yield further improvement. "}
{"page": 8, "image_path": "doc_images/P19-1598_8.jpg", "ocr_text": "7 Conclusions and Future Work\n\nBy relying on memorization, existing language\nmodels are unable to generate factually correct text\nabout real-world entities. In particular, they are\nunable to capture the long tail of rare entities and\nword types like numbers and dates. In this work,\nwe proposed the knowledge graph language model\n(KGLM), a neural language model that can access\nan external source of facts, encoded as a knowledge\ngraph, in order to generate text. Our implementa-\ntion is available at: https: //github.com/rloganiv/\nkglm-model. We also introduced Linked WikiText-\n2 containing text that has been aligned to facts in\nthe knowledge graph, allowing efficient training\nof the model. Linked WikiText-2 is freely avail-\nable for download at: https: //rloganiv.github.io/\nlinked-wikitext-2. In our evaluation, we showed\nthat by utilizing this graph, the proposed KGLM\nis able to generate higher-quality, factually correct\ntext that includes mentions of rare entities and spe-\ncific tokens like numbers and dates.\n\nThis work lays the groundwork for future re-\nsearch into knowledge-aware language modeling.\nThe limitations of the KGLM model, such as the\nneed for marginalization during inference and re-\nliance on annotated tokens, raise new research prob-\nlems for advancing neural NLP models. Our dis-\ntantly supervised approach to dataset creation can\nbe used with other knowledge graphs and other\nkinds of text as well, providing opportunities for\naccurate language modeling in new domains.\n\nAcknowledgements\n\nFirst and foremost, we would like to thank Stephen\nMerity for sharing the materials used to collect the\nWikiText-2 dataset, and Nitish Gupta for modify-\ning his entity linker to assist our work. We would\nalso like to thank Dheeru Dua and Anthony Chen\nfor their thoughtful feedback. This work was sup-\nported in part by Allen Institute of Artificial In-\ntelligence (AI2), and in part by NSF award #IIS-\n1817183. The views expressed are those of the\nauthors and do not reflect the official policy or po-\nsition of the funding agencies.\n\nReferences\n\nSungjin Ahn, Heeyoul Choi, Tanel Parnamaa, and\nYoshua Bengio. 2016. A neural knowledge language\nmodel. ArXiv:1608.00318.\n\nAntoine Bordes, Nicolas Usunier, Alberto Garcia-\nDuran, Jason Weston, and Oksana Yakhnenko.\n2013. Translating embeddings for modeling multi-\nrelational data. In Proc. of NeurIPS.\n\nChris Dyer, Adhiguna Kuncoro, Miguel Ballesteros,\nand Noah A. Smith. 2016. Recurrent neural network\ngrammars. In Proc. of NAACL.\n\nThiago Castro Ferreira, Diego Moussallem, Emiel\nKrahmer, and Sander Wubben. 2018. Enriching the\nWebNLG corpus. In Proc. of INLG.\n\nClaire Gardent, Anastasia Shimorina, Shashi Narayan,\nand Laura Perez-Beltrachini. 2017. The WebNLG\nchallenge: Generating text from RDF data. In Proc.\nof INLG.\n\nChengyue Gong, Di He, Xu Tan, Tao Qin, Liwei Wang,\nand Tie-Yan Liu. 2018. Frage: frequency-agnostic\nword representation. In Proc. of NeurIPS.\n\nJiatao Gu, Zhengdong Lu, Hang Li, and Victor O.K.\nLi. 2016. Incorporating copying mechanism in\nsequence-to-sequence learning. In Proc. of ACL.\n\nNitish Gupta, Sameer Singh, and Dan Roth. 2017. En-\ntity linking via joint encoding of types, descriptions,\nand context. In Proc. of EMNLP.\n\nSepp Hochreiter and Jiirgen Schmidhuber. 1997.\nLong short-term memory. Neural computation,\n9(8):1735-1780.\n\nYangfeng Ji, Chenhao Tan, Sebastian Martschat, Yejin\nChoi, and Noah A. Smith. 2017. Dynamic entity\nrepresentations in neural language models. In Proc.\nof EMNLP.\n\nDiederik P. Kingma and Jimmy Ba. 2015. Adam:\nA method for stochastic optimization. In Proc. of\nICLR.\n\nBen Krause, Emmanuel Kahembwe, Iain Murray, and\nSteve Renals. 2018. Dynamic evaluation of neural\nsequence models. In Proc. of ICML.\n\nRémi Lebret, David Grangier, and Michael Auli. 2016.\nNeural text generation from structured data with\napplication to the biography domain. In Proc. of\nEMNLP.\n\nStephen Merity, Nitish Shirish Keskar, and Richard\nSocher. 2018. Regularizing and optimizing LSTM\nlanguage models. In Proc. of ICLR.\n\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2017. Pointer sentinel mixture mod-\nels. In Proc. of ICLR.\n\n5970\n", "vlm_text": "7 Conclusions and Future Work \nBy relying on memorization, existing language models are unable to generate factually correct text about real-world entities. In particular, they are unable to capture the long tail of rare entities and word types like numbers and dates. In this work, we proposed the  knowledge graph language model (KGLM), a neural language model that can access an external source of facts, encoded as a knowledge graph, in order to generate text. Our implementa- tion is available at:  https://github.com/rloganiv/ kglm-model . We also introduced  Linked WikiText- 2  containing text that has been aligned to facts in the knowledge graph, allowing efﬁcient training of the model.  Linked WikiText-2  is freely avail- able for download at:  https://rloganiv.github.io/ linked-wikitext-2 . In our evaluation, we showed that by utilizing this graph, the proposed KGLM is able to generate higher-quality, factually correct text that includes mentions of rare entities and spe- ciﬁc tokens like numbers and dates. \nThis work lays the groundwork for future re- search into knowledge-aware language modeling. The limitations of the KGLM model, such as the need for marginalization during inference and re- liance on annotated tokens, raise new research prob- lems for advancing neural NLP models. Our dis- tantly supervised approach to dataset creation can be used with other knowledge graphs and other kinds of text as well, providing opportunities for accurate language modeling in new domains. \nAcknowledgements \nFirst and foremost, we would like to thank Stephen Merity for sharing the materials used to collect the WikiText-2  dataset, and Nitish Gupta for modify- ing his entity linker to assist our work. We would also like to thank Dheeru Dua and Anthony Chen for their thoughtful feedback. This work was sup- ported in part by Allen Institute of Artiﬁcial In- telligence (AI2), and in part by NSF award #IIS- 1817183. The views expressed are those of the authors and do not reﬂect the ofﬁcial policy or po- sition of the funding agencies. \nReferences \nYoshua Bengio. 2016. A neural knowledge language model. ArXiv:1608.00318. Antoine Bordes, Nicolas Usunier, Alberto Garcia- Duran, Jason Weston, and Oksana Yakhnenko. 2013. Translating embeddings for modeling multi- relational data. In  Proc. of NeurIPS . Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. 2016. Recurrent neural network grammars. In  Proc. of NAACL . Thiago Castro Ferreira, Diego Moussallem, Emiel Krahmer, and Sander Wubben. 2018. Enriching the WebNLG corpus. In  Proc. of INLG . Claire Gardent, Anastasia Shimorina, Shashi Narayan, and Laura Perez-Beltrachini. 2017. The WebNLG challenge: Generating text from RDF data. In  Proc. of INLG . Chengyue Gong, Di He, Xu Tan, Tao Qin, Liwei Wang, and Tie-Yan Liu. 2018. Frage: frequency-agnostic word representation. In  Proc. of NeurIPS . Jiatao Gu, Zhengdong Lu, Hang Li, and Victor O.K. Li. 2016. Incorporating copying mechanism in sequence-to-sequence learning. In  Proc. of ACL . Nitish Gupta, Sameer Singh, and Dan Roth. 2017.  En- tity linking via joint encoding of types, descriptions, and context . In  Proc. of EMNLP . Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long short-term memory. Neural computation , 9(8):1735–1780. Yangfeng Ji, Chenhao Tan, Sebastian Martschat, Yejin Choi, and Noah A. Smith. 2017. Dynamic entity representations in neural language models. In  Proc. of EMNLP . Diederik P. Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In  Proc. of ICLR . Ben Krause, Emmanuel Kahembwe, Iain Murray, and Steve Renals. 2018. Dynamic evaluation of neural sequence models. In  Proc. of ICML . Rémi Lebret, David Grangier, and Michael Auli. 2016. Neural text generation from structured data with application to the biography domain. In  Proc. of EMNLP . Stephen Merity, Nitish Shirish Keskar, and Richard Socher. 2018. Regularizing and optimizing LSTM language models. In  Proc. of ICLR . Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. 2017. Pointer sentinel mixture mod- els. In  Proc. of ICLR . "}
{"page": 9, "image_path": "doc_images/P19-1598_9.jpg", "ocr_text": "Tomas Mikolov, Martin Karafidt, Lukas Burget, Jan\nCernocky, and Sanjeev Khudanpur. 2010. Recurrent\nneural network based language model. In Proc. of\nINTERSPEECH.\n\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. Techni-\ncal report, OpenAI.\n\nEhud Reiter and Robert Dale. 1997. Building applied\nnatural language generation systems. Natural Lan-\nguage Engineering, 3(1):57-87.\n\nIulian V. Serban, Alessandro Sordoni, Yoshua Bengio,\nAaron Courville, and Joelle Pineau. 2016. Building\nend-to-end dialogue systems using generative hierar-\nchical neural network models. In Proc. of AAAI.\n\nGeorgios P. Spithourakis and Sebastian Riedel. 2018.\nNumeracy for language models: Evaluating and im-\nproving their ability to predict numbers. In Proc. of\nACL.\n\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,\nIlya Sutskever, and Ruslan Salakhutdinov. 2014.\nDropout: a simple way to prevent neural networks\nfrom overfitting. The Journal of Machine Learning\nResearch, 15(1):1929-1958.\n\nTrieu H. Trinh and Quoc V. Le. 2019. Do language\nmodels have common sense? In Proc. of ICLR.\n\nJoerg Ueberla. 1994. Analysing a simple language\nmodelA-some general conclusions for language\nmodels for speech recognition. Computer Speech &\nLanguage, 8(2):153 — 176.\n\nOriol Vinyals and Quoc V. Le. 2015. A neural con-\nversational model. Proc. of ICML Deep Learning\nWorkshop.\n\nDenny Vrande¢éié and Markus Krotzsch. 2014. Wiki-\ndata: A free collaborative knowledgebase. Commu-\nnications of the ACM, 57(10):78-85.\n\nLi Wan, Matthew Zeiler, Sixin Zhang, Yann LeCun,\nand Rob Fergus. 2013. Regularization of neural net-\nworks using dropconnect. In Proc. of ICML.\n\nSam Wiseman, Stuart M. Shieber, and Alexander M.\nRush. 2017. Challenges in data-to-document gener-\nation. In Proc. of EMNLP.\n\nZhilin Yang, Zihang Dai, Ruslan Salakhutdinov, and\nWilliam W Cohen. 2018. Breaking the softmax bot-\ntleneck: A high-rank RNN language model. In Proc.\nof ICLR.\n\nZichao Yang, Phil Blunsom, Chris Dyer, and Wang\n\nLing. 2017. Reference-aware language models. In\nProc. of EMNLP.\n\n5971\n", "vlm_text": "Tomáš Mikolov, Martin Karaﬁát, Lukáš Burget, Jan Cernock\\` y, and Sanjeev Khudanpur. 2010. Recurrent neural network based language model. In  Proc. of INTERSPEECH . Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. Techni- cal report, OpenAI. Ehud Reiter and Robert Dale. 1997. Building applied natural language generation systems.  Natural Lan- guage Engineering , 3(1):57–87. Iulian V. Serban, Alessandro Sordoni, Yoshua Bengio, Aaron Courville, and Joelle Pineau. 2016. Building end-to-end dialogue systems using generative hierar- chical neural network models. In  Proc. of AAAI . Georgios P. Spithourakis and Sebastian Riedel. 2018. Numeracy for language models: Evaluating and im- proving their ability to predict numbers. In  Proc. of ACL . Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: a simple way to prevent neural networks from overﬁtting.  The Journal of Machine Learning Research , 15(1):1929–1958. Trieu H. Trinh and Quoc V. Le. 2019. Do language models have common sense? In  Proc. of ICLR. Joerg Ueberla. 1994. Analysing a simple language modelÂ · some general conclusions for language models for speech recognition .  Computer Speech & Language , 8(2):153 – 176. Oriol Vinyals and Quoc V. Le. 2015. A neural con- versational model.  Proc. of ICML Deep Learning Workshop . Denny Vrandeˇ ci´ c and Markus Krötzsch. 2014.  Wiki- data: A free collaborative knowledgebase .  Commu- nications of the ACM, 57(10):78–85.Li Wan, Matthew Zeiler, Sixin Zhang, Yann LeCun, and Rob Fergus. 2013. Regularization of neural net- works using dropconnect. In  Proc. of ICML . Sam Wiseman, Stuart M. Shieber, and Alexander M. Rush. 2017. Challenges in data-to-document gener- ation. In  Proc. of EMNLP . Zhilin Yang, Zihang Dai, Ruslan Salakhutdinov, and William W Cohen. 2018. Breaking the softmax bot- tleneck: A high-rank RNN language model. In  Proc. of ICLR . Zichao Yang, Phil Blunsom, Chris Dyer, and Wang Ling. 2017. Reference-aware language models. In Proc. of EMNLP . "}
