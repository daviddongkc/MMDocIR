{"layout": 0, "type": "text", "text": "Combating Adversarial Misspellings with Robust Word Recognition ", "text_level": 1, "page_idx": 0, "bbox": [89, 67, 510, 88], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 1, "type": "text", "text": "Danish Pruthi Bhuwan Dhingra Zachary C. Lipton Carnegie Mellon University Pittsburgh, USA { ddanish, bdhingra } @cs.cmu.edu ,  zlipton@cmu.edu ", "page_idx": 0, "bbox": [137.0780029296875, 113.2550048828125, 463.4548645019531, 178.052001953125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 2, "type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0, "bbox": [159, 223, 204, 235], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 3, "type": "text", "text": "To combat adversarial spelling mistakes, we propose placing a word recognition model in front of the downstream classiﬁer. Our word recognition models build upon the RNN semi- character architecture, introducing several new backoff  strategies for handling rare and un- seen words. Trained to recognize words cor- rupted by random adds, drops, swaps, and keyboard mistakes, our method achieves    $32\\%$  relative (and    $3.3\\%$   absolute) error reduction over the vanilla semi-character model. No- tably, our pipeline confers robustness on the downstream classiﬁer, outperforming both ad- versarial training and off-the-shelf spell check- ers. Against a BERT model ﬁne-tuned for sen- timent analysis, a single adversarially-chosen character attack lowers accuracy from    $90.3\\%$  to    $45.8\\%$  . Our defense restores accuracy to  $75\\%^{1}$  . Surprisingly, better word recognition does not always entail greater robustness. Our analysis reveals that robustness also depends upon a quantity that we denote the  sensitivity . ", "page_idx": 0, "bbox": [87, 245.11553955078125, 274, 508.17938232421875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 4, "type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0, "bbox": [72, 517, 155, 531], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 5, "type": "text", "text": "Despite the rapid progress of deep learning tech- niques on diverse supervised learning tasks, these models remain brittle to subtle shifts in the data distribution. Even when the permissible changes are conﬁned to barely-perceptible perturbations, training robust models remains an open challenge. Following the discovery that imperceptible attacks could cause image recognition models to misclas- sify examples ( Szegedy et al. ,  2013 ), a veritable sub-ﬁeld has emerged in which authors iteratively propose attacks and countermeasures. ", "page_idx": 0, "bbox": [71, 538.8298950195312, 290, 687.46728515625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 6, "type": "text", "text": "For all the interest in adversarial computer vi- sion, these attacks are rarely encountered out- side of academic research. However, adversarial ", "page_idx": 0, "bbox": [71, 687.870849609375, 290, 728.1143188476562], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 7, "type": "table", "page_idx": 0, "img_path": "layout_images/P19-1561_0.jpg", "table_caption": "Table 1: Adversarial spelling mistakes inducing senti- ment misc lassi cation and word-recognition defenses. ", "bbox": [306, 219, 526, 389], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "Alteration Movie Review Label\nOaisinai A triumph, relentless and beautiful *\nMEME in its downbeat darkness\nSwe A triumph, relentless and beuatiful\n‘wap in its downbeat darkness ~\nD A triumph, relentless and beautiful\nFOP in its dwnbeat darkness ~\nDefens A triumph, relentless and beautiful\n+ Detense in its downbeat darkness +\nA triumph, relentless and beautiful\n+ Defense +\n\nin its downbeat darkness\n", "vlm_text": "The table displays different versions of a movie review under the category \"Alteration\" with corresponding \"Label\" assigned to each one. Here's a breakdown of the table:\n\n1. **Original:**\n   - Movie Review: \"A triumph, relentless and beautiful in its downbeat darkness\"\n   - Label: +\n\n2. **Swap:**\n   - Movie Review: \"A triumph, relentless and beautiful in its downbeat darkness\" (the word \"beautiful\" is in red)\n   - Label: −\n\n3. **Drop:**\n   - Movie Review: \"A triumph, relentless and beautiful in its dwnbeat darkness\" (the word \"downbeat\" is misspelled as \"dwnbeat\")\n   - Label: −\n\n4. **+ Defense:**\n   - Movie Review: \"A triumph, relentless and beautiful in its downbeat darkness\" (the word \"beautiful\" is in blue)\n   - Label: +\n\n5. **+ Defense:**\n   - Movie Review: \"A triumph, relentless and beautiful in its downbeat darkness\" (the word \"downbeat\" is in blue)\n   - Label: +\n\nThus, the table explores how different text alterations in the movie review affect its label, with the \"+ Defense\" entries suggesting a corrected or enhanced version that maintains the positive label despite potential textual manipulations."}
{"layout": 8, "type": "text", "text": "misspellings constitute a  longstanding real-world problem . Spammers continually bombard email servers, subtly misspelling words in efforts to evade spam detection while preserving the emails’ intended meaning ( Lee and Ng ,  2005 ;  Fumera et al. ,  2006 ). As another example, programmatic censorship on the Internet has spurred communi- ties to adopt similar methods to communicate sur- reptitiously ( Bitso et al. ,  2013 ). ", "page_idx": 0, "bbox": [307, 412.0359802246094, 525, 533.574462890625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 9, "type": "text", "text": "In this paper, we focus on adversarially-chosen spelling mistakes in the context of text classiﬁca- tion, addressing the following attack types: drop- ping, adding, and swapping internal characters within words. These perturbations are inspired by psycho linguistic studies ( Rawlinson ,  1976 ;  Matt Davis ,  2003 ) which demonstrated that humans can comprehend text altered by jumbling internal char- acters, provided that the ﬁrst and last characters of each word remain unperturbed. ", "page_idx": 0, "bbox": [307, 535.0390014648438, 525, 670.12646484375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 10, "type": "text", "text": "First, in experiments addressing both BiLSTM and ﬁne-tuned BERT models, comprising four different input formats: word-only, char-only, word+char, and word-piece ( Wu et al. ,  2016 ), we demonstrate that an adversary can degrade a clas- siﬁer’s performance to that achieved by random guessing.  This requires altering just two charac- ters per sentence . Such modiﬁcations might ﬂip words either to a different word in the vocabu- lary or, more often, to the out-of-vocabulary to- ken  UNK  . Consequently, adversarial edits can de- grade a word-level model by transforming the in- formative words to  UNK  . Intuitively, one might suspect that word-piece and character-level mod- els would be less susceptible to spelling attacks as they can make use of the residual word con- text. However, our experiments demonstrate that character and word-piece models are in fact  more vulnerable.  We show that this is due to the ad- versary’s effective capacity for ﬁner grained ma- nipulations on these models. While against a word-level model, the adversary is mostly lim- ited to  UNK  -ing words, against a word-piece or character-level model, each character-level add, drop, or swap produces a distinct input, providing the adversary with a greater set of options. ", "page_idx": 0, "bbox": [307, 671.591064453125, 525, 766.031494140625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 11, "type": "text", "text": "", "page_idx": 1, "bbox": [71, 63.68701934814453, 290, 320.7184753417969], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 12, "type": "text", "text": "Second, we evaluate ﬁrst-line techniques in- cluding data augmentation and adversarial train- ing, demonstrating that they offer only marginal beneﬁts here, e.g., a BERT model achieving  90 . 3 accuracy on a sentiment classiﬁcation task, is degraded to  64 . 1  by an adversarially-chosen  1 - character swap in the sentence, which can only be restored to  69 . 2  by adversarial training. ", "page_idx": 1, "bbox": [71, 325.0350036621094, 290, 433.0245361328125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 13, "type": "text", "text": "Third (our primary contribution), we propose a task-agnostic defense, attaching a word recog- nition model that predicts each word in a sen- tence given a full sequence of (possibly mispelled) inputs. The word recognition model’s outputs comprise the input to a downstream classiﬁcation model. Our word recognition models build upon the RNN-based semi-character word recognition model due to  Sakaguchi et al.  ( 2017 ). While our word recognizers are trained on domain-speciﬁc text from the task at hand, they often predict  UNK at test time, owing to the small domain-speciﬁc vocabulary. To handle unobserved and rare words, we propose several  backoff  strategies including falling back on a generic word recognizer trained on a larger corpus. Incorporating our defenses, BERT models subject to 1-character attacks are restored to  88 . 3 ,  81 . 1 ,  78 . 0  accuracy for swap, drop, add attacks respectively, as compared to 69 . 2 ,  63 . 6 , and  50 . 0  for adversarial training ", "page_idx": 1, "bbox": [71, 437.341064453125, 290, 707.9214477539062], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 14, "type": "text", "text": "Fourth, we offer a detailed qualitative analysis, demonstrating that a low word error rate alone is insufﬁcient for a word recognizer to confer robust- ness on the downstream task. Additionally, we ﬁnd that it is important that the recognition model supply few degrees of freedom to an attacker. We provide a metric to quantify this notion of  sensi- tivity  in word recognition models and study its re- lation to robustness empirically. Models with low sensitivity  and  word error rate are most robust. ", "page_idx": 1, "bbox": [71, 712.2379760742188, 290, 766.0314331054688], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 15, "type": "text", "text": "", "page_idx": 1, "bbox": [307, 63.68701934814453, 525, 144.57852172851562], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 16, "type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1, "bbox": [307, 157, 397, 172], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 17, "type": "text", "text": "Several papers address adversarial attacks on NLP systems. Changes to text, whether word- or character-level, are all perceptible, raising some questions about what should rightly be considered an adversarial example ( Ebrahimi et al. ,  2018b ; Belinkov and Bisk ,  2018 ).  Jia and Liang  ( 2017 ) address the reading comprehension task, show- ing that by appending  distractor sentences  to the end of stories from the SQuAD dataset ( Rajpurkar et al. ,  2016 ), they could cause models to output in- correct answers. Inspired by this work,  Glockner et al.  ( 2018 ) demonstrate an attack that breaks en- tailment systems by replacing a single word with either a synonym or its hypernym. Recently,  Zhao et al.  ( 2018 ) investigated the problem of producing natural-seeming adversarial examples, noting that adversarial examples in NLP are often ungram- matical ( Li et al. ,  2016 ). ", "page_idx": 1, "bbox": [307, 181.87904357910156, 525, 425.3606262207031], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 18, "type": "text", "text": "In related work on character-level attacks, Ebrahimi et al.  ( 2018b , a ) explored gradient-based methods to generate string edits to fool classiﬁca- tion and translation systems, respectively. While their focus is on efﬁcient methods for generat- ing adversaries, ours is on improving the worst case adversarial performance. Similarly,  Belinkov and Bisk  ( 2018 ) studied how synthetic and natu- ral noise affects character-level machine transla- tion. They considered structure invariant represen- tations and adversarial training as defenses against such noise. Here, we show that an auxiliary word recognition model, which can be trained on unla- beled data, provides a strong defense. ", "page_idx": 1, "bbox": [307, 426.7341613769531, 525, 616.0196533203125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 19, "type": "text", "text": "Spelling correction ( Kukich ,  1992 ) is often viewed as a sub-task of grammatical error correc- tion (  $\\mathrm{Mg}$   et al. ,  2014 ;  Schmaltz et al. ,  2016 ). Clas- sic methods rely on a source language model and a noisy channel model to ﬁnd the most likely correc- tion for a given word ( Mays et al. ,  1991 ;  Brill and Moore ,  2000 ). Recently, neural techniques have been applied to the task ( Sakaguchi et al. ,  2017 ; Li et al. ,  2018 ), which model the context and or- thography of the input together. Our work extends the ScRNN model of  Sakaguchi et al.  ( 2017 ). ", "page_idx": 1, "bbox": [307, 617.3942260742188, 525, 766.0316162109375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 20, "type": "text", "text": "3 Robust Word Recognition ", "text_level": 1, "page_idx": 2, "bbox": [71, 64, 223, 76], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 21, "type": "text", "text": "To tackle character-level adversarial attacks, we introduce a simple two-stage solution, placing a word recognition model   $(W)$   before the down- stream classiﬁer   $(C)$  . Under this scheme, all inputs are classiﬁed by the comp ed mo l  $C\\circ W$  . This modular approach, with  W  and  C  trained sepa- rately, offers several beneﬁts: (i) we can deploy the same word recognition model for multiple down- stream classiﬁcation tasks/models; and (ii) we can train the word recognition model with larger unla- beled corpora. ", "page_idx": 2, "bbox": [71, 85.22303009033203, 291, 233.86056518554688], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 22, "type": "text", "text": "Against adversarial mistakes, two important factors govern the robustness of this combined model:    $W$  ’s  accuracy  in recognizing misspelled words and    $W$  ’s  sensitivity  to adversarial perturba- tions on the same input. We discuss these aspects in detail below. ", "page_idx": 2, "bbox": [71, 234.5150909423828, 291, 315.4065856933594], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 23, "type": "text", "text": "3.1 ScRNN with Backoff ", "text_level": 1, "page_idx": 2, "bbox": [71, 326, 195, 338], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 24, "type": "text", "text": "We now describe semi-character RNNs for word recognition, explain their limitations, and suggest techniques to improve them. ", "page_idx": 2, "bbox": [71, 343.49212646484375, 291, 383.7356262207031], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 25, "type": "text", "text": "ScRNN Model Inspired by the psycholinguis- tic studies ( Matt Davis ,  2003 ;  Rawlinson ,  1976 ), Sakaguchi et al.  ( 2017 ) proposed a semi-character based RNN (ScRNN) that processes a sentence of words with misspelled characters, predict- ing the correct words at each step. Let    $s\\_=$   $\\{w_{1},w_{2},.\\,.\\,.\\,,w_{n}\\}$   denote the input sentence, a se- quence of constituent words    $w_{i}$  . Each input word  $(w_{i})$   is represented by concatenating (i) a one hot vector of the ﬁrst character   $(\\mathbf{w_{i1}})$  ; (ii) a one hot representation of the last character (  $\\mathbf{\\dot{w}i l}$  , where  $l$   is the length of word    $w_{i}$  ); and (iii) a bag of characters representation of the internal characters  $\\begin{array}{r l}{(\\sum_{j=2}^{l-1}\\mathbf{w_{ij}})}\\end{array}$  . ScRNN treats the ﬁrst and the last characters individually, and is agnostic to the or- dering of the internal characters. Each word, rep- resented accordingly, is then fed into a BiLSTM cell. At each sequence step, the training target is the correct corresponding word (output dimension equal to vocabulary size), and the model is opti- mized with cross-entropy loss. ", "page_idx": 2, "bbox": [71, 391.98040771484375, 291, 676.5026245117188], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 26, "type": "text", "text": "Backoff Variations While  Sakaguchi et al. ( 2017 ) demonstrate strong word recognition per- formance, a drawback of their evaluation setup is that they only attack and evaluate on the subset of words that are a part of their training vocabu- lary. In such a setting, the word recognition per- formance is unreasonably dependant on the cho- sen vocabulary size. In principle, one can design models to predict (correctly) only a few chosen words, and ignore the remaining majority and still reach  $100\\%$   accuracy.  For the adversarial setting, rare and unseen words in the wild are particularly critical, as they provide opportunities for the at- tackers.  A reliable word-recognizer should handle these cases gracefully. Below, we explore different ways to  back off  when the ScRNN predicts  UNK (a frequent outcome for rare and unseen words): ", "page_idx": 2, "bbox": [71, 684.7474975585938, 291, 766.0316162109375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 27, "type": "text", "text": "", "page_idx": 2, "bbox": [307, 63.68720245361328, 526, 212.32449340820312], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 28, "type": "text", "text": "•  Pass-through : word-recognizer passes on the (possibly misspelled) word as is. ", "page_idx": 2, "bbox": [318, 217.50132751464844, 526, 244.58853149414062], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 29, "type": "text", "text": "•  Backoff to neutral word : Alternatively, noting that passing UNK  -predicted words through unchanged exposes the downstream model to potentially corrupted text, we con- sider backing off to a neutral word like ‘a’, which has a similar distribution across classes. ", "page_idx": 2, "bbox": [318, 252.0453338623047, 526, 346.8785095214844], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 30, "type": "text", "text": "•  Backoff to background model : We also consider falling back upon a more generic word recognition model trained upon a larger, less-specialized corpus whenever the fore- ground word recognition model predicts UNK   2 . Figure  1  depicts this scenario pic- torially. ", "page_idx": 2, "bbox": [318, 354.3352966308594, 526, 449.1684875488281], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 31, "type": "text", "text": "Empirically, we ﬁnd that the background model (by itself) is less accurate, because of the large number of words it is trained to predict. Thus, it is best to train a precise foreground model on an in-domain corpus and focus on frequent words, and then to resort to a general-purpose background model for rare and unobserved words. Next, we delineate our second consideration for building ro- bust word-recognizers. ", "page_idx": 2, "bbox": [307, 454.7380065917969, 526, 576.2764892578125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 32, "type": "text", "text": "3.2 Model Sensitivity ", "text_level": 1, "page_idx": 2, "bbox": [307, 584, 413, 597], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 33, "type": "text", "text": "In computer vision, an important factor determin- ing the success of an adversary is the norm con- straint on the perturbations allowed to an image  $(||\\mathbf{x}\\,-\\,\\mathbf{x}^{\\prime}||_{\\infty}\\ <\\ \\epsilon)$  . Higher values of    $\\epsilon$   lead to a higher chance of mis-classiﬁcation for at least one  $\\mathbf{x}^{\\prime}$  . Defense methods such as quantization (  $\\mathrm{Xu}$  et al. ,  2017 ) and thermometer encoding ( Buck- man et al. ,  2018 ) try to reduce the space of pertur- bations available to the adversary by making the model invariant to small changes in the input. ", "page_idx": 2, "bbox": [307, 602.2440795898438, 526, 737.33251953125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 34, "type": "text", "text": "", "text_level": 1, "page_idx": 3, "bbox": [142, 63, 376, 72], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 35, "type": "image", "page_idx": 3, "img_path": "layout_images/P19-1561_1.jpg", "img_caption": "", "bbox": [141, 77, 450, 223], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "im r il\n\nQ=O0= ©\n\nI Jaaciernt| g\n\na\n\nForeground Model\n\nflabll e\n\n|\n(+)\n\nSemi-character\nRepresentation\n", "vlm_text": "The image illustrates a diagram representing a hybrid model combining two components: a \"Background Model\" and a \"Foreground Model\". \n\n- **Background Model**: Shown in gray with a sequence of nodes labeled \\( h_1, h_2, ..., h_n \\).\n- **Foreground Model**: Shown in green with corresponding nodes labeled the same as the background model, indicating a parallel process.\n- **Input Structure**: Each node in the models receives input from blue boxes containing semi-character representations, such as “t |eend| r”.\n- **UNK Node**: Located in the foreground model's flow, suggesting it handles unknown inputs.\n- **Directional Arrows**: Indicate information flow; both models seem to process sequences in a left-to-right manner with some feedback loops within the foreground model.\n\nThe caption mentions \"Semi-character Representation\", indication that the diagram deals with text processing or language modeling."}
{"layout": 36, "type": "text", "text": "Figure 1: A schematic sketch of our proposed word recognition system, consisting of a  foreground  and a  back- ground  model. We train the foreground model on the smaller, domain-speciﬁc dataset, and the background model on a larger dataset (e.g., the IMDB movie corpus). We train both models to reconstruct the correct word from the orthography and context of the individual words, using synthetically corrupted inputs during training. Subse- quently, we invoke the background model whenever the foreground model predicts  UNK  . ", "page_idx": 3, "bbox": [72, 231.8785400390625, 525, 291.70452880859375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 37, "type": "text", "text": "In NLP, we often get such invariance for free, e.g., for a word-level model, most of the pertur- bations produced by our character-level adversary lead to an  UNK  at its input. If the model is robust to the presence of these  UNK  tokens, there is little room for an adversary to manipulate it. Character- level models, on the other hand, despite their supe- rior performance in many tasks, do not enjoy such invariance. This characteristic invariance could be exploited by an attacker. Thus, to limit the number of different inputs to the classiﬁer, we wish to re- duce the number of distinct word recognition out- puts that an attacker can induce, not just the num- ber of words on which the model is “fooled”. We denote this property of a model as its  sensitivity . ", "page_idx": 3, "bbox": [71, 313.29498291015625, 290, 516.1295166015625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 38, "type": "text", "text": "We can quantify this notion for a word recogni- tion system    $W$   as the expected number of unique outputs it assigns to a set of adversarial pertur- bations. Given a sentence    $s$   from the set of sen- tences  $s$  , let    $A(s)\\,=\\,s_{1}{}^{\\prime},s_{2}{}^{\\prime},.\\,.\\,.\\,,s_{n}{}^{\\prime}$    denote  e set of    $n$   perturbations to it under attack type  A , and let    $V$   be the function that maps strings to an input representation for the downstream classiﬁer. For a word level model,  $V$   would transform sen- tences to a sequence of word ids, mapping OOV words to the same  UNK  ID. Whereas, for a char (or word  $^+$  char, word-piece) model,    $V$   would map inputs to a sequence of character IDs. Formally, sensitivity is deﬁned as ", "page_idx": 3, "bbox": [71, 516.5510864257812, 290, 705.83642578125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 39, "type": "equation", "text": "\n$$\nS_{W,V}^{A}=\\mathbb{E}_{s}\\left[\\frac{\\#u(V\\circ W(s_{1}^{\\prime}),.\\,.\\,,V\\circ W(s_{n}^{\\prime}))}{n}\\right],\n$$\n ", "text_format": "latex", "page_idx": 3, "bbox": [69, 725, 296, 755], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 40, "type": "text", "text": "where    $V\\circ W(s_{i})$   returns the input representation (of the downstream classiﬁer) for the output string produced by the word-recognizer    $W$   using    $s_{i}$   and #  $_u(\\cdot)$   counts the number of unique arguments. ", "page_idx": 3, "bbox": [307, 313, 526, 367.0884704589844], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 41, "type": "text", "text": "Intuitively, we expect a high value of    $S_{W,V}^{A}$    to lead to a lower robustness of the downstream clas- siﬁer, since the adversary has more degrees of freedom to attack the classiﬁer. Thus, when using word recognition as a defense, it is prudent to de- sign a low sensitivity system with a low error rate. However, as we will demonstrate, there is often a trade-off between sensitivity and error rate. ", "page_idx": 3, "bbox": [307, 367, 526, 476.61749267578125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 42, "type": "text", "text": "3.3 Synthesizing Adversarial Attacks ", "text_level": 1, "page_idx": 3, "bbox": [307, 491, 487, 503], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 43, "type": "text", "text": "Suppose we are given a classiﬁer    $C\\,:\\,\\mathcal{S}\\,\\rightarrow\\,\\mathcal{Y}$  which maps natural language sentences  s  $s\\in\\mathcal S$   ∈S  to a label from a predeﬁned set    $y\\in\\mathcal{Y}$  . An adversary for this classiﬁer is a function  A  which maps a sen- tence    $s$   to its perturbed versions    $\\{s_{1}^{\\prime},s_{2}^{\\prime},\\ldots,s_{n}^{\\prime}\\}$  } such that each  $s_{i}^{\\prime}$    is close to    $s$   under some notion of distance between sentences. We deﬁne the ro- bustness of classiﬁer    $C$   to the adversary    $A$   as: ", "page_idx": 3, "bbox": [307, 509, 526, 617.5125122070312], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 44, "type": "equation", "text": "\n$$\nR_{C,A}=\\mathbb{E}_{s}\\left[\\operatorname*{min}_{{s^{\\prime}}\\in A(s)}\\mathbb{1}[C({s^{\\prime}})=y]\\right],\n$$\n ", "text_format": "latex", "page_idx": 3, "bbox": [336, 628, 496, 659], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 45, "type": "text", "text": "where  $y$   represents the ground truth label for    $s$  . In practice, a real-world adversary may only be able to query the classiﬁer a few times, hence    $R_{C,A}$  represents the  worst-case  adversarial performance of    $C$  . Methods for generating adversarial exam- ples, such as HotFlip ( Ebrahimi et al. ,  2018b ), fo- cus on efﬁcient algorithms for searching the  min above. Improving    $R_{C,A}$   would imply better ro- bustness against all these methods. ", "page_idx": 3, "bbox": [307, 671.591064453125, 526, 766.031494140625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 46, "type": "text", "text": "", "page_idx": 4, "bbox": [72, 63.68701934814453, 290, 90.38247680664062], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 47, "type": "text", "text": "Allowed Perturbations    $\\left(A(s)\\right)$  We explore ad- versaries which perturb sentences with four types of character-level edits: (1) Swap: swapping two adjacent internal characters of a word. (2) Drop: removing an internal character of a word. (3) Key- board: substituting an internal character with ad- jacent characters of QWERTY keyboard (4) Add: inserting a new character internally in a word. In line with the psycho linguistic studies ( Matt Davis , 2003 ;  Rawlinson ,  1976 ), to ensure that the pertur- bations do not affect human ability to comprehend the sentence, we only allow the adversary to edit the internal characters of a word, and not edit stop- words or words shorter than  4  characters. ", "page_idx": 4, "bbox": [72, 98, 290, 288.2475280761719], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 48, "type": "text", "text": "Attack Strategy For    $I$  -character  attacks, we try all possible perturbations listed above until we ﬁnd an adversary that ﬂips the model prediction. For  2-character  attacks, we greedily ﬁx the edit which had the least conﬁdence among 1-character attacks, and then try all the allowed perturbations on the remaining words. Higher order attacks can be performed in a similar manner. The greedy strategy reduces the computation required to ob- tain higher order attacks 3 , but also means that the robustness score is an upper bound on the true ro- bustness of the classiﬁer. ", "page_idx": 4, "bbox": [72, 296.4342956542969, 290, 459.0135803222656], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 49, "type": "text", "text": "4 Experiments and Results ", "text_level": 1, "page_idx": 4, "bbox": [72, 470, 218, 483], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 50, "type": "text", "text": "In this section, we ﬁrst discuss our experiments on the word recognition systems. ", "page_idx": 4, "bbox": [72, 491.94110107421875, 290, 518.6355590820312], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 51, "type": "text", "text": "4.1 Word Error Correction ", "text_level": 1, "page_idx": 4, "bbox": [71, 529, 207, 541], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 52, "type": "text", "text": "Data : We evaluate the spell correctors from  § 3  on movie reviews from the Stanford Sentiment Tree- bank (SST) ( Socher et al. ,  2013 ). The SST dataset consists of  8544  movie reviews, with a vocabu- lary of over 16K words. As a background cor- pus, we use the IMDB movie reviews ( Maas et al. , 2011 ), which contain  54 K movie reviews, and a vocabulary of over 78K words. The two datasets do not share any reviews in common. The spell- correction models are evaluated on their ability to correct misspellings. The test setting consists of reviews where each word (with length    $\\geq\\ 4$  , barring stopwords) is attacked by one of the at- tack types (from swap, add, drop and keyboard at- tacks). In the  all  attack setting, we mix all attacks by randomly choosing one for each word. This most closely resembles a real world attack setting. ", "page_idx": 4, "bbox": [72, 546.2384033203125, 290, 735.91650390625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 53, "type": "text", "text": "", "page_idx": 4, "bbox": [307, 63.68701934814453, 525, 103.93148803710938], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 54, "type": "text", "text": "Experimental Setup In addition to our word recognition models, we also compare to After The Deadline (ATD), an open-source spell cor- rector 4 . We found ATD to be the best freely- available corrector 5 . We refer the reader to  Sak- aguchi et al.  ( 2017 ) for comparisons of ScRNN to other anonymized commercial spell checkers. ", "page_idx": 4, "bbox": [307, 112.89329528808594, 525, 207.72653198242188], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 55, "type": "text", "text": "For the ScRNN model, we use a single-layer Bi- LSTM with a hidden dimension size of  50 . The input representation consists of  198  dimensions, which is thrice the number of unique characters ( 66 ) in the vocabulary. We cap the vocabulary size to  10 K words, whereas we use the entire vo- cabulary of  78470  words when we backoff to the background model. For training these networks, we corrupt the movie reviews according to all at- tack types, i.e., applying one of the  4  attack types to each word, and trying to reconstruct the original words via cross entropy loss. ", "page_idx": 4, "bbox": [307, 208.5890655517578, 525, 370.7756042480469], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 56, "type": "table", "page_idx": 4, "img_path": "layout_images/P19-1561_2.jpg", "table_caption": "Word Recognition ", "table_footnote": "Table 2: Word Error Rates (WER) of ScRNN with each backoff strategy, plus ATD and an ScRNN trained only on the background corpus ( 78 K vocabulary) The error rates include  $5.25\\%$   OOV words. ", "bbox": [306, 382, 526, 546], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "Spell-Corrector Swap Drop Add_ Key\n\nAll\n\nATD 22 126 13.3 69\n\n11.2\n\nScRNN (78K) 6.3 10.2 8.7 9.8\n\n8.7\n\nScRNN (10K) w/ Backoff Variants\n\nPass-Through 8.5 10.5 10.7. 11.2\nNeutral 8.7 10.9 108 11.4\nBackground 5.4 8.1 6.4 7.6\n\n10.2\n10.6\n6.9\n\n", "vlm_text": "The table presents a comparison of the effectiveness of different spell-correction methods across various types of spelling errors. There are three spell-correctors listed: ATD, ScRNN with a dataset of 78K words, and ScRNN with a dataset of 10K words using three backoff variants (Pass-Through, Neutral, and Background). The types of spelling errors considered in the table are Swap, Drop, Add, Key, and All. The numbers in the table represent some form of metric or score—likely error rates or percentages. Lower numbers would typically indicate better performance in correcting that type of spelling error. For instance, the Background variant of ScRNN (10K) has the lowest scores across all types of errors compared to the other methods listed."}
{"layout": 57, "type": "text", "text": "Results We calculate the word error rates (WER) of each of the models for different at- tacks and present our ﬁndings in Table  2 . Note that ATD incorrectly predicts  11 . 2  words for ev- ery  100  words (in the ‘all’ setting), whereas, all of the backoff variations of the ScRNN reconstruct better. The most accurate variant involves backing off to the background model, resulting in a low er- ror rate of  $6.9\\%$  , leading to the best performance on word recognition. This is a    $32\\%$   relative error reduction compared to the vanilla ScRNN model with a pass-through backoff strategy. We can at- tribute the improved performance to the fact that there are    $5.25\\%$   words in the test corpus that are unseen in the training corpus, and are thus only recoverable by backing off to a larger corpus. No- tably, only training on the larger background cor- pus does worse, at  $8.7\\%$  , since the distribution of word frequencies is different in the background corpus compared to the foreground corpus. ", "page_idx": 4, "bbox": [307, 568.7402954101562, 525, 704.2213745117188], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 58, "type": "text", "text": "", "page_idx": 5, "bbox": [71, 63.68701934814453, 290, 198.77554321289062], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 59, "type": "text", "text": "4.2 Robustness to adversarial attacks ", "text_level": 1, "page_idx": 5, "bbox": [72, 214, 253, 226], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 60, "type": "text", "text": "We use sentiment analysis and paraphrase detec- tion as downstream tasks, as for these two tasks, 1 - 2  character edits do not change the output labels. ", "page_idx": 5, "bbox": [71, 234.19007873535156, 290, 274.4345397949219], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 61, "type": "text", "text": "Experimental Setup For sentiment classiﬁca- tion, we systematically study the effect of character-level adversarial attacks on two architec- tures and four different input formats. The ﬁrst architecture encodes the input sentence into a se- quence of embeddings, which are then sequen- tially processed by a BiLSTM. The ﬁrst and last states of the BiLSTM are then used by the soft- max layer to predict the sentiment of the input. We consider three input formats for this architecture: (1) Word-only: where the input words are encoded using a lookup table; (2) Char-only: where the input words are encoded using a separate single- layered BiLSTM over their characters; and (3) Word + Char: where the input words are encoded using a concatenation of (1) and (2)   6 . ", "page_idx": 5, "bbox": [71, 287.3383483886719, 290, 504.1146545410156], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 62, "type": "text", "text": "The second architecture uses the ﬁne-tuned BERT model ( Devlin et al. ,  2018 ), with an input format of word-piece tokenization. This model has recently set a new state-of-the-art on sev- eral NLP benchmarks, including the sentiment analysis task we consider here. All models are trained and evaluated on the binary version of the sentence-level Stanford Sentiment Tree- bank ( Socher et al. ,  2013 ) dataset with only pos- itive and negative reviews. ", "page_idx": 5, "bbox": [71, 506.11419677734375, 290, 641.20263671875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 63, "type": "text", "text": "We also consider the task of paraphrase detec- tion. Here too, we make use of the ﬁne-tuned BERT ( Devlin et al. ,  2018 ), which is trained and evaluated on the Microsoft Research Paraphrase Corpus (MRPC) ( Dolan and Brockett ,  2005 ). ", "page_idx": 5, "bbox": [71, 643.2022094726562, 290, 710.5446166992188], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 64, "type": "text", "text": "Baseline defense strategies Two common methods for dealing with adversarial examples include: (1) data augmentation ( DA ) ( Krizhevsky et al. ,  2012 ); and (2) adversarial training ( Adv ) ( Goodfellow et al. ,  2014 ). In  DA , the trained model is ﬁne-tuned after augmenting the training set with an equal number of examples randomly attacked with a 1-character edit. In  Adv , the trained model is ﬁne-tuned with additional adver- sarial examples (selected at random) that produce incorrect predictions from the current-state classi- ﬁer. The process is repeated iteratively, generating and adding newer adversarial examples from the updated classiﬁer model, until the adversarial accuracy on dev set stops improving. ", "page_idx": 5, "bbox": [307, 63.29429244995117, 527, 266.5215759277344], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 65, "type": "text", "text": "Results In Table  3 , we examine the robustness of the sentiment models under each attack and de- fense method. In the absence of any attack or defense, BERT (a word-piece model) performs the best   $(90.3\\%^{7})$  ) followed by word+char mod- els   $(80.5\\%)$  , word-only models   $(79.2\\%)$   and then char-only models   $(70.3\\%)$  ). However, even single- character attacks (chosen adversarially) can be catastrophic, resulting in a signiﬁcantly degraded performance of    $46\\%$  ,    $57\\%$  ,  $59\\%$   and    $33\\%$  , respec- tively under the ‘all’ setting. ", "page_idx": 5, "bbox": [307, 275.0593566894531, 527, 424.0896301269531], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 66, "type": "text", "text": "Intuitively, one might suppose that word-piece and character-level models would be more robust to such attacks given they can make use of the remaining context. However, we ﬁnd that they are the more susceptible. To see why, note that the word ‘beautiful’ can only be altered in a few ways for word-only models, either leading to an UNK  or an existing vocabulary word, whereas, word-piece and character-only models treat each unique character combination differently. This provides more variations that an attacker can ex- ploit. Following similar reasoning,  add  and  key attacks pose a greater threat than  swap  and  drop attacks. The robustness of different models can be ordered as word-only    $>$   word+char  $>$   char-only    $\\sim$  word-piece, and the efﬁcacy of different attacks as add  $>\\mathsf{k e y}>\\mathsf{d r o p}>\\mathsf{s w a p.}$  ", "page_idx": 5, "bbox": [307, 424.82916259765625, 527, 654.761474609375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 67, "type": "text", "text": "Next, we scrutinize the effectiveness of defense methods when faced against adversarially chosen attacks. Clearly from table  3 , DA and Adv are not ", "page_idx": 5, "bbox": [307, 655.5010375976562, 527, 695.7444458007812], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 68, "type": "table", "page_idx": 6, "img_path": "layout_images/P19-1561_3.jpg", "table_caption": "Sentiment Analysis  ( 1 -char attack/ 2 -char attack) ", "table_footnote": "Table 3: Accuracy of various classiﬁcation models, with and without defenses, under adversarial attacks. Even 1 -character attacks signiﬁcantly degrade classiﬁer performance. Our defenses confer robustness, recovering over  $76\\%$   of the original accuracy, under the ‘all’ setting for all four model classes. ", "bbox": [71, 62, 527, 465], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "Model No attack Swap Drop Add Key All\nWord-Level Models\nBiLSTM 79.2 (64.3/53.6)  (63.7/52.7)  (60.0/43.2) — (60.2/42.4) — (58.6/40.2)\nBiLSTM + ATD 79.3 (76.2/75.3)  (66.5/59.9)  (55.6/47.5)  (62.6/57.6) — (55.8/37.0)\nBiLSTM + Pass-through 79.3 (78.6/78.5)  (69.1/65.3)  (65.0/59.2)  (69.6/65.6) — (63.2/52.4)\nBiLSTM + Background 78.8 (78.9/78.4)  (69.6/66.8)  (62.6/56.4) — (68.2/62.2) — (59.6/49.0)\nBiLSTM + Neutral 80.1 (80.1/79.9)  (72.4/70.2) — (67.2/61.2) — (69.0/64.6) — (63.2/54.0)\nChar-Level Models\nBiLSTM 70.3 (53.6/42.9)  (48.8/37.1)  (33.8/14.8)  (40.8/22.0) — (32.6/14.0)\nBiLSTM + ATD 71.0 (66.6/65.2)  (58.0/53.0)  (54.6/44.4)  (61.6/57.5) — (46.5/35.4)\nBiLSTM + Pass-through 70.3 (65.8/62.9)  (58.3/54.2) (54.0/44.2)  (58.8/52.4) — (51.6/39.8)\nBiLSTM + Background 70.1 (70.3/69.8)  (60.4/57.7) — (57.4/52.6) — (58.8/54.2) — (53.6/47.2)\nBiLSTM + Neutral 70.7 (70.7/70.7)  (62.1/60.5) — (57.8/53.6) (61.4/58.0) — (55.2/48.4)\nWord+Char Models\nBiLSTM 80.5 (63.9/52.3)  (62.8/50.8)  (57.8/39.8)  (58.4/40.8) — (56.6/35.6)\nBiLSTM + ATD 80.8 (78.0/77.3)  (67.7/60.9) — (55.6/50.5)  (68.7/64.6) — (48.5/37.4)\nBiLSTM + Pass-through 80.1 (79.0/78.7)  (69.5/65.7) — (64.0/59.0)  (66.0/62.0) — (61.5/56.5)\nBiLSTM + Background 79.5 (79.6/79.0)  (69.7/66.7) — (62.0/57.0)  (65.0/56.5) — (59.4/49.8)\nBiLSTM + Neutral 79.5 (79.5/79.4)  (71.2/68.8) — (65.0/59.0) — (65.5/61.5) — (61.5/55.5)\nWord-piece Models\nBERT 90.3 (64.1/47.4)  (59.2/39.9)  (46.2/26.4)  (54.3/34.9) — (45.8/24.6)\nBERT + DA 90.2 (68.3/50.6)  (62.7/39.9)  (43.6/17.0) _ (57.7/32.4) — (41.0/15.8)\nBERT + Adv 89.6 (69.2/52.9)  (63.6/40.5)  (50.0/22.0) (60.1/36.6) — (47.0/20.2)\nBERT + ATD 89.0 (84.5/84.5)  (73.0/64.0)  (77.0/69.5) — (80.0/75.0) — (67.0/55.0)\nBERT + Pass-through 89.8 (85.5/83.9)  (78.9/75.0)  (70.4/64.4) — (75.3/70.3) —_ (68.0/58.5)\nBERT + Background 89.3 (89.1/89.1) — (79.3/76.5)  (76.5/71.0) — (77.5/74.4) —_ (73.0/67.5)\nBERT + Neutral 88.3 (88.3/88.3)  (81.1/79.5) — (78.0/74.0) — (78.8/76.8) —_ (75.0/68.0)\n", "vlm_text": "This table presents the performance (likely accuracy or another metric) of different models under various types of text perturbations or attacks. The models are categorized into four groups: Word-Level, Char-Level, Word+Char, and Word-piece Models. Each of these groups contains entries for different variations of the model architecture like BiLSTM or BERT with enhancements such as ATD, Pass-through, Background, and Neutral.\n\nThe table columns include:\n\n- **No attack**: The performance without any manipulations.\n- **Swap, Drop, Add, Key**: Performance under different types of perturbations.\n- **All**: Averages or overall performance across all types of attacks.\n\nValues in parentheses represent two numbers, possibly different metrics or measurements under the same condition. Numbers in bold seem to indicate the best performance for each perturbation type within a sub-category."}
{"layout": 69, "type": "text", "text": "effective in this case. We observed that despite a low training error, these models were not able to generalize to attacks on newer words at test time. ATD spell corrector is the most effective on key- board attacks, but performs poorly on other attack types, particularly the add attack strategy. ", "page_idx": 6, "bbox": [71, 487.2359924316406, 290, 568.12744140625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 70, "type": "text", "text": "The ScRNN model with pass-through backoff offers better protection, bringing back the adver- sarial accuracy within  $5\\%$   range for the swap at- tack. It is also effective under other attack classes, and can mitigate the adversarial effect in word- piece models by    $21\\%$  , character-only models by  $19\\%$  , and in word, and word  $^+$  char models by over  $4.5\\%$   . This suggests that the direct training signal of word error correction is more effective than the indirect signal of sentiment classiﬁcation available to DA and Adv for model robustness. ", "page_idx": 6, "bbox": [71, 572.6390380859375, 290, 721.2764892578125], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 71, "type": "text", "text": "We observe additional gains by using back- ground models as a backoff alternative, because of its lower word error rate (WER), especially, under the swap and drop attacks. However, these gains do not consistently translate in all other settings, as lower WER is necessary but not sufﬁcient. Be- sides lower error rate, we ﬁnd that a solid defense should furnish the attacker the fewest options to attack, i.e. it should have a low sensitivity. As we shall see in section    $\\S~4.3$  , the backoff neutral variation has the lowest sensitivity due to mapping UNK  predictions to a ﬁxed neutral word. Thus, it results in the highest robustness on most of the at- tack types for all four model classes. ", "page_idx": 6, "bbox": [71, 725.7869873046875, 290, 766.031494140625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 72, "type": "text", "text": "", "page_idx": 6, "bbox": [307, 487.23602294921875, 525, 635.8734130859375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 73, "type": "table", "page_idx": 6, "img_path": "layout_images/P19-1561_4.jpg", "bbox": [310, 649, 521, 729], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "All attacks\n\nModel No Attack\n\n1-char  2-char\nBERT 89.0 60.0 31.0\nBERT + ATD 89.9 75.8 61.6\nBERT + Pass-through 89.0 84.5 81.5\n\nBERT + Neutral 84.0 82.5 82.5\n\n", "vlm_text": "The table presents a comparison of different models' performance under scenarios with and without attacks. These models are evaluated using some kind of performance or accuracy metric, as demonstrated by the numerical values in the table. Here’s an outline of the table structure and data:\n\n- **Columns**:\n  - The first column specifies the model variants.\n  - The second column, labeled \"No Attack,\" shows the performance metric for each model when there is no attack.\n  - The subsequent columns, labeled under \"All attacks,\" show performance for two types of attacks: \"1-char\" and \"2-char.\"\n\n- **Rows**:\n  - The first row describes the standard BERT model's performance:\n    - Without attacks: 89.0\n    - With 1-char attacks: 60.0\n    - With 2-char attacks: 31.0\n  \n  - The second row refers to \"BERT + ATD\":\n    - Without attacks: 89.9\n    - With 1-char attacks: 75.8\n    - With 2-char attacks: 61.6\n\n  - The third row describes \"BERT + Pass-through\":\n    - Without attacks: 89.0\n    - With 1-char attacks: 84.5 (Bold)\n    - With 2-char attacks: 81.5\n\n  - The last row describes \"BERT + Neutral\":\n    - Without attacks: 84.0\n    - With 1-char attacks: 82.5\n    - With 2-char attacks: 82.5 (Bold)\n\nFrom this table, we can infer that:\n- The BERT model's performance significantly drops under attack conditions.\n- The \"BERT + Pass-through\" and \"BERT + Neutral\" models maintain relatively higher performance under the 1-char and 2-char attacks compared to standard BERT.\n- \"BERT + Pass-through\" has the highest improvement in performance with 1-char attacks than other models, as indicated by the bold value of 84.5.\n- \"BERT + Neutral\" has the highest performance under 2-char attack, indicated by the bold value of 82.5."}
{"layout": 74, "type": "text", "text": "Table 4: Accuracy of BERT, with and without defenses, on MRPC when attacked under the ‘all’ attack setting. ", "page_idx": 6, "bbox": [307, 738.2185668945312, 525, 762.1785278320312], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 75, "type": "table", "page_idx": 7, "img_path": "layout_images/P19-1561_5.jpg", "table_caption": "Sensitivity Analysis ", "table_footnote": "Table 5: Sensitivity values for word recognizers. Neu- tral backoff shows lowest sensitivity. ", "bbox": [70, 61, 291, 225], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "Backoff Swap Drop Add Key All\n\nClosed Vocabulary Models (word-only)\n\nPass-Through 17.6 19.7 0.8 73 113\nBackground 19.5 22.3 1.1 95 13.1\nNeutral 17.5 19.7 0.8 72 113\n\nOpen Vocab. Models (char/word+char/word-piece)\n\nPass-Through 39.6 35.3 19.2 26.9 30.3\nBackground 20.7 25.1 1.3 116 14.7\nNeutral 17.5 19.7 0.8 72 113\n", "vlm_text": "The table presents data comparing different models with respect to various criteria. It is divided into two main sections: Closed Vocabulary Models (word-only) and Open Vocabulary Models (char/word+char/word-piece). \n\nFor each section, the models are further divided into three types: Pass-Through, Background, and Neutral. The table measures their performance across five tasks: Swap, Drop, Add, Key, and All.\n\n- **Closed Vocabulary Models (word-only):**\n  - **Pass-Through:** Swap (17.6), Drop (19.7), Add (0.8), Key (7.3), All (11.3)\n  - **Background:** Swap (19.5), Drop (22.3), Add (1.1), Key (9.5), All (13.1)\n  - **Neutral:** Swap (17.5), Drop (19.7), Add (0.8), Key (7.2), All (11.3)\n\n- **Open Vocabulary Models (char/word+char/word-piece):**\n  - **Pass-Through:** Swap (39.6), Drop (35.3), Add (19.2), Key (26.9), All (30.3)\n  - **Background:** Swap (20.7), Drop (25.1), Add (1.3), Key (11.6), All (14.7)\n  - **Neutral:** Swap (17.5), Drop (19.7), Add (0.8), Key (7.2), All (11.3)"}
{"layout": 76, "type": "text", "text": "Table  4  shows the accuracy of BERT on  $200\\,\\mathrm{ex}\\cdot$  - amples from the dev set of the MRPC paraphrase detection task under various attack and defense settings. We re-trained the ScRNN model vari- ants on the MRPC training set for these experi- ments. Again, we ﬁnd that simple  1 - 2  character attacks can bring down the accuracy of BERT sig- niﬁcantly   $(89\\%$   to    $31\\%$  ). Word recognition mod- els can provide an effective defense, with both our pass-through and neutral variants recovering most of the accuracy. While the neutral backoff model is effective on  2 -char attacks, it hurts performance in the  no attack  setting, since it incorrectly mod- iﬁes certain correctly spelled entity names. Since the two variants are already effective, we did not train a background model for this task. ", "page_idx": 7, "bbox": [71, 245, 290, 461.6055603027344], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 77, "type": "text", "text": "4.3 Understanding Model Sensitivity ", "text_level": 1, "page_idx": 7, "bbox": [71, 471, 250, 484], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 78, "type": "text", "text": "Experimental setup To study model sensitiv- ity, for each sentence, we perturb one randomly- chosen word and replace it with all possible per- turbations under a given attack type. The resulting set of perturbed sentences is then fed to the word recognizer (whose sensitivity is to be estimated). As described in equation  1 , we count the number of unique predictions from the output sentences. Two corrections are considered unique if they are mapped differently by the downstream classiﬁer. ", "page_idx": 7, "bbox": [71, 487.8123474121094, 290, 623.2925415039062], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 79, "type": "text", "text": "Results The neutral backoff variant has the low- est sensitivity (Table  5 ). This is expected, as it returns a ﬁxed neutral word whenever the ScRNN predicts an  UNK  , therefore reducing the number of unique outputs it predicts. Open vocabulary (i.e. char-only, word+char, word-piece) down- stream classiﬁers consider every unique combi- nation of characters differently, whereas word- only classiﬁers internally treat all out of vocab- ulary (OOV) words alike. Hence, for char-only, ", "page_idx": 7, "bbox": [71, 630.5504150390625, 290, 766.0314331054688], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 80, "type": "image", "page_idx": 7, "img_path": "layout_images/P19-1561_6.jpg", "img_caption": "Figure 2: Effect of sensitivity and word error rate on robustness (depicted by the bubble sizes) in word-only models (left) and char-only models (right). ", "bbox": [306, 62, 527, 231], "page_size": [595.2760009765625, 841.8900146484375], "ocr_text": "WER\n\n12\n\n11\n\n10\n\n11\n63.2 @\n632 55.2 .\ni 10 51.6\nBo\n=\n@ Pass-through 8\n@ Background 591g,\nNeutral 7 e\n= 53.6\n12 13 10 20 30\nSensitivity Sensitivity\n\n", "vlm_text": "The image consists of two scatter plots, each representing the relationship between sensitivity and word error rate (WER) for two types of models: word-only models (left plot) and char-only models (right plot). The bubble sizes in the plots depict robustness. \n\nThe left plot shows:\n- Three data points, represented by bubbles of different colors: blue (Pass-through), orange (Background), and green (Neutral).\n- Sensitivity ranges from about 11.4 to 12.6, and WER ranges from 9.5 to 11.5.\n- Two overlapping green and blue bubbles correspond to WER of 11 and sensitivity of approximately 12, with robustness value 63.2.\n- An orange bubble represents a WER of around 10.5 and a sensitivity of around 12.7, with robustness value 59.6.\n\nThe right plot shows:\n- Three data points, again represented by differently colored bubbles: blue, orange, and green.\n- Sensitivity ranges from about 10 to 30, and WER from 6.5 to 11.\n- A large green bubble corresponds to a WER of 11 and sensitivity of around 12, with robustness value 55.2.\n- An orange bubble represents a WER of 7 and sensitivity of about 10, with robustness value 53.6.\n- A blue bubble shows a WER of approximately 10 and a sensitivity of approximately 30, with robustness value 51.6.\n\nThe legend explains the colors of the bubbles: blue for Pass-through, orange for Background, and green for Neutral."}
{"layout": 81, "type": "text", "text": "word+char, and word-piece models, the pass- through version is more sensitive than the back- ground variant, as it passes words as is (and each combination is considered uniquely). However, for word-only models, pass-through is less sen- sitive as all the OOV character combinations are rendered identical. ", "page_idx": 7, "bbox": [307, 240.4650421142578, 525, 334.9055480957031], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 82, "type": "text", "text": "Ideally, a preferred defense is one with low sen- sitivity and word error rate. In practice, however, we see that a low error rate often comes at the cost of sensitivity. We visualize this trade-off in Fig- ure  2 , where we plot WER and sensitivity on the two axes, and depict the robustness when using different backoff variants. Generally, sensitivity is the more dominant factor out of the two, as the er- ror rates of the considered variants are reasonably low. ", "page_idx": 7, "bbox": [307, 335.30908203125, 525, 470.3976135253906], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 83, "type": "text", "text": "Human Intelligibility We verify if the senti- ment (of the reviews) is preserved with char-level attacks. In a human study with  50  attacked (and subsequently misclassiﬁed), and 50 unchanged re- views, it was noted that  48  and  49 , respectively, preserved the sentiment. ", "page_idx": 7, "bbox": [307, 477.6744079589844, 525, 558.9586181640625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 84, "type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 7, "bbox": [306, 568, 384, 583], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 85, "type": "text", "text": "As character and word-piece inputs become com- monplace in modern NLP pipelines, it is worth highlighting the vulnerability they add. We show that minimally-doctored attacks can bring down accuracy of classiﬁers to random guess- ing. We recommend word recognition as a safe- guard against this and build upon RNN-based semi-character word recognizers. We discover that when used as a defense mechanism, the most ac- curate word recognition models are not always the most robust against adversarial attacks. Addition- ally, we highlight the need to control the sensitiv- ity of these models to achieve high robustness. ", "page_idx": 7, "bbox": [307, 590.295166015625, 525, 766.0316162109375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 86, "type": "text", "text": "6 Acknowledgements ", "text_level": 1, "page_idx": 8, "bbox": [70, 65, 189, 75], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 87, "type": "text", "text": "The authors are grateful to Graham Neubig, Ed- uard Hovy, Paul Michel, Mansi Gupta, and An- tonios Anastasopoulos for suggestions and feed- back. ", "page_idx": 8, "bbox": [72, 85.81201934814453, 290, 139.60549926757812], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 88, "type": "text", "text": "References ", "text_level": 1, "page_idx": 8, "bbox": [71, 165, 128, 176], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 89, "type": "text", "text": "Yonatan Belinkov and Yonatan Bisk. 2018. Synthetic and natural noise both break neural machine transla- tion. In  International Conference on Learning Rep- ", "page_idx": 8, "bbox": [72, 184.2205810546875, 290, 218.1434783935547], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 90, "type": "text", "text": "resentations (ICLR) . Constance Bitso, Ina Fourie, and Theo JD Bothma. 2013. Trends in transition from classical cen- sorship to internet censorship: selected country overviews. Innovation: journal of appropriate librarianship and information work in Southern Africa , 2013(46):166–191. Eric Brill and Robert C. Moore. 2000.  An improved error model for noisy channel spelling correction . In  Proceedings of the 38th Annual Meeting on As- sociation for Computational Linguistics , ACL   $^{'}00$  , pages 286–293, Stroudsburg, PA, USA. Association for Computational Linguistics. Jacob Buckman, Aurko Roy, Colin Raffel, and Ian Goodfellow. 2018. Thermometer encoding: One hot way to resist adversarial examples. International Conference on Learning Representations (ICLR) . Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understand- ing.  arXiv preprint arXiv:1810.04805 . William B Dolan and Chris Brockett. 2005. Automati- cally constructing a corpus of sentential paraphrases. In  Proceedings of the Third International Workshop on Paraphrasing (IWP2005) . Javid Ebrahimi, Daniel Lowd, and Dejing Dou. 2018a. On adversarial examples for character-level neural machine translation. In  International Conference on Computational Linguistics (COLING) . Javid Ebrahimi, Anyi Rao, Daniel Lowd, and Dejing Dou. 2018b. Hotﬂip: White-box adversarial exam- ples for nlp. In  Association for Computational Lin- guistics (ACL) . Giorgio Fumera, Ignazio Pillai, and Fabio Roli. 2006. Spam ﬁltering based on the analysis of text infor- mation embedded into images.  Journal of Machine Learning Research (JMLR) . Max Glockner, Vered Shwartz, and Yoav Goldberg. 2018. Breaking nli systems with sentences that re- quire simple lexical inferences. In  Association for Computational Linguistics (ACL) . ", "page_idx": 8, "bbox": [72, 217.0975341796875, 290, 765.7653198242188], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 91, "type": "text", "text": "Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. 2014. Explaining and harnessing adversar- ial examples. In  International Conference on Learn- ing Representations (ICLR) . Robin Jia and Percy Liang. 2017. Adversarial exam- ples for evaluating reading comprehension systems. Empirical Methods in Natural Language Processing (EMNLP) . Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hin- ton. 2012. Imagenet classiﬁcation with deep con- volutional neural networks. In  Advances in neural information processing systems (NIPS) . Karen Kukich. 1992. Techniques for automatically correcting words in text.  Acm Computing Surveys (CSUR) , 24(4):377–439. Honglak Lee and Andrew Y Ng. 2005. Spam deobfus- cation using a hidden markov model. In  CEAS . Hao Li, Yang Wang, Xinyu Liu, Zhichao Sheng, and Si Wei. 2018. Spelling error correction using a nested rnn model and pseudo training data.  arXiv preprint arXiv:1811.00238 . Jiwei Li, Will Monroe, and Dan Jurafsky. 2016. Un- derstanding neural networks through representation erasure.  arXiv preprint arXiv:1612.08220 . Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. 2011.  Learning word vectors for sentiment analysis . In  Association for Computational Linguistics (ACL) . Matt Davis. 2003. Psycho linguistic evi- dence on scrambled letters in reading. https://www.mrc-cbu.cam.ac.uk/ people/matt.davis/cmabridge/ . Eric Mays, Fred J. Damerau, and Robert L. Mercer. 1991.  Context based spelling correction .  Informa- tion Processing & Management , 27(5):517 – 522. Hwee Tou Ng, Siew Mei Wu, Ted Briscoe, Christian Hadiwinoto, Raymond Hendy Susanto, and Christo- pher Bryant. 2014. The conll-2014 shared task on grammatical error correction. In  Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task , pages 1–14. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD:   $100{,}000{+}$   questions for machine comprehension of text. In  Empirical Meth- ods in Natural Language Processing (EMNLP) . Graham Ernest Rawlinson. 1976.  The signiﬁcance of letter position in word recognition . Ph.D. thesis, University of Nottingham. Keisuke Sakaguchi, Kevin Duh, Matt Post, and Ben- jamin Van Durme. 2017. Robsut wrod reocginiton via semi-character recurrent neural network. In  As- sociation for the Advancement of Artiﬁcial Intelli- gence (AAAI) . ", "page_idx": 8, "bbox": [307, 64.5614013671875, 525, 765.7649536132812], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 92, "type": "text", "text": "Allen Schmaltz, Yoon Kim, Alexander M. Rush, and Stuart Shieber. 2016. Sentence-level grammatical error identiﬁcation as sequence-to-sequence correc- tion . In  Proceedings of the 11th Workshop on In- novative Use of NLP for Building Educational Ap- plications , pages 242–251, San Diego, CA. Associ- ation for Computational Linguistics. ", "page_idx": 9, "bbox": [72, 64.56158447265625, 290, 142.32041931152344], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 93, "type": "text", "text": "Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositional it y over a sentiment tree- bank. In  Empirical Methods in Natural Language Processing (EMNLP) . ", "page_idx": 9, "bbox": [72, 150.240478515625, 290, 217.0403289794922], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 94, "type": "text", "text": "Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. 2013. Intriguing properties of neural networks.  arXiv preprint arXiv:1312.6199 . ", "page_idx": 9, "bbox": [72, 224.96038818359375, 290, 269.84228515625], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 95, "type": "text", "text": "Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. 2016. Google’s neural ma- chine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144 . ", "page_idx": 9, "bbox": [72, 277.7623291015625, 290, 355.52020263671875], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 96, "type": "text", "text": "Weilin Xu, David Evans, and Yanjun Qi. 2017. Feature squeezing: Detecting adversarial examples in deep neural networks.  arXiv preprint arXiv:1704.01155 . ", "page_idx": 9, "bbox": [72, 363.4412536621094, 290, 397.36322021484375], "page_size": [595.2760009765625, 841.8900146484375]}
{"layout": 97, "type": "text", "text": "Zhengli Zhao, Dheeru Dua, and Sameer Singh. 2018. Generating natural adversarial examples. In  Inter- national Conference on Learning Representations (ICLR) . ", "page_idx": 9, "bbox": [72, 405.2842712402344, 290, 450.16522216796875], "page_size": [595.2760009765625, 841.8900146484375]}
