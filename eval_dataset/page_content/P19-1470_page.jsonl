{"page": 0, "image_path": "doc_images/P19-1470_0.jpg", "ocr_text": "com\n\nTZ: Commonsense Transformers\n\nfor Automatic Knowledge Graph Construction\n\nAntoine Bosselut °* Hannah Rashkin °® Maarten Sap °® Chaitanya Malaviya ®\nAsli Celikyilmaz* Yejin Choi °*\n© Allen Institute for Artificial Intelligence, Seattle, WA, USA\nPaul G. Allen School of Computer Science & Engineering, Seattle, WA, USA\n*Microsoft Research, Redmond, WA, USA\n\nAbstract\n\nWe present the first comprehensive study\non automatic knowledge base construction\nfor two prevalent commonsense knowledge\ngraphs: ATOMIC (Sap et al., 2019) and Con-\nceptNet (Speer et al., 2017). Contrary to\nmany conventional KBs that store knowledge\nwith canonical templates, commonsense KBs\nonly store loosely structured open-text de-\nscriptions of knowledge. We posit that an\nimportant step toward automatic common-\nsense completion is the development of gen-\nerative models of commonsense knowledge,\nand propose COMmonsEnse_ Transformers\n(COMET) that learn to generate rich and\ndiverse commonsense descriptions in natural\nlanguage. Despite the challenges of com-\nmonsense modeling, our investigation reveals\npromising results when implicit knowledge\nfrom deep pre-trained language models is\ntransferred to generate explicit knowledge in\ncommonsense knowledge graphs. Empirical\nresults demonstrate that COMET is able to\ngenerate novel knowledge that humans rate as\nhigh quality, with up to 77.5% (ATOMIC) and\n91.7% (ConceptNet) precision at top 1, which\napproaches human performance for these re-\nsources. Our findings suggest that using gen-\nerative commonsense models for automatic\ncommonsense KB completion could soon be\na plausible alternative to extractive methods.\n\n1 Introduction\n\nWhen reading text, humans make commonsense\ninferences that frame their understanding of the\nnarrative being presented. For machines to achieve\nthis capability, they must be able to acquire rele-\nvant and correct commonsense for an unbounded\nset of situations. In this work, we cast common-\nsense acquisition as knowledge base construction\nand investigate whether large-scale language mod-\nels can effectively learn to generate the knowledge\n\nAutomatic KB\nCompletion\n\nCommonsense Knowledge Bases\n(seen events)\n\nPersonX\nputs their\n\ncomfort\nPersonY\n\nhaving }Gause>\n7e\\ arest\n\nFigure 1: COMET learns from an existing knowledge\nbase (solid lines) to be able to generate novel nodes and\nedges (dashed lines).\n\nConceptNet\n\nUnseen Events\n\nnecessary to automatically construct a common-\nsense knowledge base (KB).\n\nAutomatic KB construction is a long-standing\ngoal of artificial intelligence research due to the\ndifficulty of achieving high concept coverage in\nhigh-precision curated KBs (Lenat, 1995; Miller,\n1995). Previous work has developed models capa-\nble of reading and extracting semi-structured text\n(Suchanek et al., 2007; Hoffart et al., 2013; Auer\net al., 2007; Bollacker et al., 2008) and unstruc-\ntured text (Dong et al., 2014; Carlson et al., 2010;\nNakashole et al., 2011, 2012; Niu, 2012) into re-\nlational schemas that can be queried for down-\nstream applications. A common thread of these\napproaches, however, is the focus on encyclope-\ndic knowledge, which lends itself to a well-defined\nspace of entities and relations that can be modeled.\n\nCommonsense knowledge, however, does not\ncleanly fit into a schema comparing two entities\nwith a known relation, leading current approaches\n\n4762\n\nProceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4762-4779\nFlorence, Italy, July 28 - August 2, 2019. ©2019 Association for Computational Linguistics\n", "vlm_text": "COMET : Commonsense Transformers for Automatic Knowledge Graph Construction \nAntoine Bosselut   ♦♠ Hannah Rashkin  $\\diamondsuit$  Maarten Sap  $\\diamondsuit$  Chaitanya Malaviya   ♦ Asli Celikyilmaz   ♣ Yejin Choi   ♦♠ ♦ Allen Institute for Artiﬁcial Intelligence, Seattle, WA, USA ♠ Paul G. Allen School of Computer Science & Engineering, Seattle, WA, USA ♣ Microsoft Research, Redmond, WA, USA \nAbstract \nWe present the ﬁrst comprehensive study on automatic knowledge base construction for two prevalent commonsense knowledge graphs: A TOMIC  ( Sap et al. ,  2019 ) and Con- ceptNet ( Speer et al. ,  2017 ). Contrary to many conventional KBs that store knowledge with canonical templates, commonsense KBs only store loosely structured open-text de- scriptions of knowledge. We posit that an important step toward automatic common- sense completion is the development of  gen- erative  models of commonsense knowledge, and propose  COM mons E nse  T ransformers  $(\\mathbb{C}\\mathbb{O}\\mathbb{M}\\mathbb{E}\\mathbb{T}\\mathcal{O})$  ) that learn to generate rich and diverse commonsense descriptions in natural language. Despite the challenges of com- monsense modeling, our investigation reveals promising results when implicit knowledge from deep pre-trained language models is transferred to generate explicit knowledge in commonsense knowledge graphs. Empirical results demonstrate that  COMET  is able to generate novel knowledge that humans rate as high quality, with up to  $77.5\\%$   (A TOMIC ) and  $91.7\\%$   (ConceptNet) precision at top 1, which approaches human performance for these re- sources. Our ﬁndings suggest that using gen- erative commonsense models for automatic commonsense KB completion could soon be a plausible alternative to extractive methods. \n1 Introduction \nWhen reading text, humans make commonsense inferences that frame their understanding of the narrative being presented. For machines to achieve this capability, they must be able to acquire rele- vant and correct commonsense for an unbounded set of situations. In this work, we cast common- sense acquisition as knowledge base construction and investigate whether large-scale language mod- els can effectively learn to generate the knowledge \nThe image illustrates how COMET, an AI model, learns to generate new knowledge from existing knowledge bases. It comprises two sections:\n\n1. **Commonsense Knowledge Bases (seen events)**: \n   - Examples from knowledge bases like Atomic and ConceptNet.\n   - Consists of nodes and edges showing relationships:\n     - **Atomic**: Includes actions like \"PersonX puts their arms around PersonY\" with attributes (xAttr) such as \"caring\" and reactions (oReact) like \"feels loved.\"\n     - **ConceptNet**: Shows events like \"nap\" with sub-events (HasSubevent) leading to \"having a rest\" and causing \"energy.\"\n\n2. **Automatic KB Completion (unseen events)**:\n   - Demonstrates how COMET infers new connections and nodes:\n     - Predicts relationships such as \"xNeed\" for \"PersonX goes to the store\" needing \"bring a wallet.\"\n     - Generates events like \"loving towards PersonY\" and \"having fun\" from known events.\n\nDashed lines signify newly inferred nodes and connections, illustrating COMET's capability to extend knowledge beyond the initial data.\nnecessary to automatically construct a common- sense knowledge base (KB). \nAutomatic KB construction is a long-standing goal of artiﬁcial intelligence research due to the difﬁculty of achieving high concept coverage in high-precision curated KBs ( Lenat ,  1995 ;  Miller , 1995 ). Previous work has developed models capa- ble of reading and extracting semi-structured text ( Suchanek et al. ,  2007 ;  Hoffart et al. ,  2013 ;  Auer et al. ,  2007 ;  Bollacker et al. ,  2008 ) and unstruc- tured text ( Dong et al. ,  2014 ;  Carlson et al. ,  2010 ; Nakashole et al. ,  2011 ,  2012 ;  Niu ,  2012 ) into re- lational schemas that can be queried for down- stream applications. A common thread of these approaches, however, is the focus on encyclope- dic knowledge, which lends itself to a well-deﬁned space of entities and relations that can be modeled. \nCommonsense knowledge, however, does not cleanly ﬁt into a schema comparing two entities with a known relation, leading current approaches "}
{"page": 1, "image_path": "doc_images/P19-1470_1.jpg", "ocr_text": "Multi-headed Attention\n\nTransformer Block\n¢\n\nhe\nLayer Normalization\n\ng’\n\nLayer Normalization\n\nMulti-headed Attention\n\n(no!..,ne4} ah\n\n(b)\n\nCommonsense Transformer (COMeT)\n[MASK] [MASK] have boat <END>\n+ + t t t\nVocab || Vocab Vocab Vocab | | Vocab\nt i f i f\n(Block ) (Gee) ” (Beet) - ic (Beet)\nPoo\" F TE aan) t\ntr t-\" iad te ft\nBlock }( Block } *** ( Block Block } ( Block\naN ®. 2. ® ®\n\n€o Po Gi Pi E}s| Pisi cig\nPersonX sails ..<xNeed> .. sail boat\n(c)\n\nFigure 2: Model diagram. (a) In the multi-headed attention module, the key, value, and query all pass through a\nhead-specific projection before a scaled dot-product attention is computed between them. The outputs of the heads\nare concatenated and projected. (b) Inside the transformer block, the outputs of all the previous layer blocks from\nearlier time steps are input to the multi-headed attention with the preceding block for the current time step as the\nquery. (c) Each token is an input to a first-layer block along with all preceding tokens. Dotted lines indicate outputs\nto all future blocks in the next layer and inputs from all preceding blocks in the previous layer.\n\nto model “entities\" as natural language phrases\nand relations as any concept that can link them\n(Li et al., 2016; Sap et al., 2019). OpenIE ap-\nproaches display this property of open text enti-\nties and relations (Etzioni et al., 2011; Fader et al.,\n2011; Mausam et al., 2012), but being extrac-\ntive, they only capture knowledge that is explic-\nitly mentioned in text, limiting their applicability\nfor capturing commonsense knowledge, which is\noften implicit (Gordon and Van Durme, 2013).\n\nMeanwhile, recent progress in training deep\ncontextualized language models (Peters et al.,\n2018; Radford et al., 2018; Devlin et al., 2018)\nprovides an opportunity to explore beyond extrac-\ntive methods as an avenue for commonsense KB\nconstruction. These large-scale language models\ndisplay impressive performance when their under-\nlying representations are tuned to solve end tasks,\nachieving state-of-the-art results on a variety of\ncomplex problems. In this work, we define the\nCOMmonsEnse Transformer (COMET), which\nconstructs commonsense KBs by using existing\ntuples as a seed set of knowledge on which to\ntrain. Using this seed set, a pre-trained language\nmodel learns to adapt its learned representations to\nknowledge generation, and produces novel tuples\nthat are high quality.\n\nWe summarize our contributions in this work as\nfollows. First, we develop a generative approach\nto knowledge base construction. A model must\nlearn to produce new nodes and identify edges be-\n\nween existing nodes by generating phrases that\ncoherently complete an existing seed phrase and\nrelation type!. Second, we develop a framework\nfor using large-scale transformer language models\no learn to produce commonsense knowledge tu-\nples”. Finally, we perform an empirical study on\nhe quality, novelty, and diversity of the common-\nsense knowledge produced by our approach for\nwo domains, ATOMIC and ConceptNet, as well as\nan efficiency study on the number of seed tuples\nneeded to learn an effective knowledge model.\nThe results indicate that COMET is able to pro-\nduce high quality tuples as human judges find that\n77.5% of generated tuples for ATOMIC events and\n91.7% of generated tuples for ConceptNet rela-\nions are correct.\n\n2 Learning to Generate Commonsense\n\nCOMET is an adaptation framework for construct-\ning commonsense knowledge bases from language\nmodels by training the language model on a seed\nset of knowledge tuples. These tuples provide\nCOMET with the KB structure and relations that\nmust be learned, and COMET learns to adapt the\nlanguage model representations learned from pre-\ntraining to add novel nodes and edges to the seed\nknowledge graph.\n\n‘Demo is available at https://mosaickg.apps.\nallenai.org/\n\n?Code is available at https://github.com/\natcbosselut/comet-commonsense\n\n4763\n", "vlm_text": "The image is a diagram explaining the architecture of a model, focusing on three main components: (a) Multi-headed Attention, (b) Transformer Block, and (c) Commonsense Transformer (COMeT).\n\n(a) Multi-headed Attention: This part of the diagram shows how key (K), value (V), and query (Q) inputs are processed through head-specific projections. Each attention head computes scaled dot-product attention, and the outputs are concatenated and linearly projected.\n\n(b) Transformer Block: This section depicts a single layer of a transformer block, illustrating how multi-headed attention interacts with layer normalization and a feedforward network. The input includes all previous layer blocks from earlier steps, and the current block query is used.\n\n(c) Commonsense Transformer (COMeT): This explains how tokens progress through the transformer architecture. Each token is input to the first-layer block with preceding tokens. Dotted lines signify outputs to future blocks and inputs from prior blocks. The attention mechanism produces a contextual understanding of sequences.\n\nTogether, these components illustrate the flow and transformation of data through a transformer-based model, specifically tailored for a commonsense knowledge task in COMeT.\nto model “entities\" as natural language phrases and relations as any concept that can link them ( Li et al. ,  2016 ;  Sap et al. ,  2019 ). OpenIE ap- proaches display this property of open text enti- ties and relations ( Etzioni et al. ,  2011 ;  Fader et al. , 2011 ;  Mausam et al. ,  2012 ), but being extrac- tive, they only capture knowledge that is explic- itly mentioned in text, limiting their applicability for capturing commonsense knowledge, which is often implicit ( Gordon and Van Durme ,  2013 ). \nMeanwhile, recent progress in training deep contextualized language models ( Peters et al. , 2018 ;  Radford et al. ,  2018 ;  Devlin et al. ,  2018 ) provides an opportunity to explore beyond extrac- tive methods as an avenue for commonsense KB construction. These large-scale language models display impressive performance when their under- lying representations are tuned to solve end tasks, achieving state-of-the-art results on a variety of complex problems. In this work, we deﬁne the COM mons E nse  T ransformer    $(\\mathbb{C}\\mathbb{O}\\mathbb{M}\\mathbb{E}\\mathbb{T}\\mathcal{O})$  ), which constructs commonsense KBs by using existing tuples as a seed set of knowledge on which to train. Using this seed set, a pre-trained language model learns to adapt its learned representations to knowledge generation, and produces novel tuples that are high quality. \nWe summarize our contributions in this work as follows. First, we develop a generative approach to knowledge base construction. A model must learn to produce new nodes and identify edges be- tween existing nodes by generating phrases that coherently complete an existing seed phrase and relation type 1 . Second, we develop a framework for using large-scale transformer language models to learn to produce commonsense knowledge tu- ples 2 . Finally, we perform an empirical study on the quality, novelty, and diversity of the common- sense knowledge produced by our approach for two domains, A TOMIC  and ConceptNet, as well as an efﬁciency study on the number of seed tuples needed to learn an effective knowledge model. The results indicate that  COMET  is able to pro- duce high quality tuples as human judges ﬁnd that\n\n  $77.5\\%$   of generated tuples for A TOMIC  events and\n\n  $91.7\\%$   of generated tuples for ConceptNet rela- tions are correct. \n\n2 Learning to Generate Commonsense \nCOMET  is an adaptation framework for construct- ing commonsense knowledge bases from language models by training the language model on a seed set of knowledge tuples. These tuples provide COMET  with the KB structure and relations that must be learned, and  COMET  learns to adapt the language model representations learned from pre- training to add novel nodes and edges to the seed knowledge graph. "}
{"page": 2, "image_path": "doc_images/P19-1470_2.jpg", "ocr_text": "2.1. Task\n\nMore specifically, the problem assumes COMET is\ngiven a training knowledge base of natural lan-\nguage tuples in {s,r,o} format, where s is the\nphrase subject of the tuple, r is the relation of the\ntuple, and o is the phrase object of the tuple. For\nexample, a ConceptNet tuple relating to “taking\na nap\" would be: (s=“take a nap\", r=Causes,\no=“have energy\"). The task is to generate o given\ns and r as inputs.\n\nNotation We define X* = {z9,..., xj,)} as the\ntokens that make up the subject of the relation,\nX\"” = {26, vey Uh as the tokens that make up\nthe relation of the tuple, and X° = {2§,..., 0/,)}\nas the tokens that make up the object of the tuple.\nThe embedding for any word x is denoted as e.\n\n2.2 Transformer Language Model\n\nWhile COMET is agnostic to the language model\nwith which it is initialized, in this work, we use\nthe transformer language model architecture in-\ntroduced in Radford et al. (2018) (GPT), which\nuses multiple transformer blocks of multi-headed\nscaled dot product attention and fully connected\nlayers to encode input text (Vaswani et al., 2017).\nFigure 2 depicts different components of the GPT\narchitecture and we define each component in\nmore depth below.\n\nTransformer Block As shown in Figure 2(b),\neach transformer layer / contains an architecturally\nidentical transformer block (though with unique\ntrainable parameters) that applies the following\ntransformations to the input to the block:\n\ngi = MuttiAtTtTn(h!“!) (dd)\ngi = LayerNorm(g! + h!-!) (2)\nh! = FFN(g') (3)\nh! = LAYERNoRM(h! + g!) (4)\n\nwhere MULTIATTN is a multi-headed self-\nattention mechanism (defined below), FFN is\na two-layer feed-forward network, and LAYER-\nNORM represents a layer normalization (Ba et al.,\n2016) operation that is applied to the output of\nthe self-attention and the feedforward network.\nNote that the inputs to the LAYERNORM opera-\ntions contain a residual connection that sums the\noutput of and input to the previous operation.\n\nMulti-headed Attention The multi-headed at-\ntention module of each transformer block, shown\nin Figure 2(a), is identical to the one originally de-\nfined by Vaswani et al. (2017). The attention func-\ntion receives three inputs, a query Q, key Ky, and\nvalue V. The attention is made of multiple heads\nthat each compute a unique scaled dot product at-\ntention distribution over V using Q and K:\n\nT\nATTENTION(Q, K,V) = sofas ( 2 yv\n(5)\n\nwhere d;, is the dimensionality of the input vectors\nrepresenting the query, key and value. For each\nof the heads, Q, K, and V are uniquely projected\nprior to the attention being computed:\n\nHi, = ATTENTION(QW2, KW ,VW,\") (6)\n\nwhere H; is the output of a single attention head\nand we, W, and WY are head-specific projec-\ntions for Q, Kk, and V, respectively. The outputs\nof the attention heads H; are then concatenated:\n\nMULTIH(Q, K, V) = [A1;..; JW? (7)\n\nwhere W° is an output projection of the concate-\nnated outputs of the attention heads. As shown in\nFigure 2(c), we follow Radford et al. (2018) and\nuse the output of the previous layer’s transformer\nblock as the query input for the multi-headed at-\ntention of the next block. The keys and values are\noutputs of the previous layer’s block for all pre-\nceding time steps:\n\nMuLTIATTIN(A}-!) = MuttiH(hi-1 hl} n+)\n(8)\n\nwhere hi\"! = {h!-!} 2, is the set of previous\nlayer transformer block outputs for time steps pre-\nceding t.\n\nInput Encoder As input to the model, we repre-\nsent a knowledge tuple {s, 7,0} as a concatenated\nsequence of the words of each item of the tuple:\n\nX = {X°, x7, X°} (9)\n\nSince the transformer (a self-attention model) has\nno concept of ordering of tokens, a position em-\nbedding »; is initialized for each absolute position\nin the sequence (Vaswani et al., 2017). For any\ninput word x; € X, our encoding of the input is\n\n4764\n", "vlm_text": "More speciﬁcally, the problem assumes  COMET  is given a training knowledge base of natural lan- guage tuples in    $\\{s,r,o\\}$   format, where    $s$   is the phrase subject of the tuple,    $r$   is the relation of the tuple, and    $o$   is the phrase object of the tuple. For example, a ConceptNet tuple relating to “taking a nap\" would be: (  $\\stackrel{\\cdot}{s=}^{\\cdot}$  “take a nap\",    $r{=}\\mathrm{C}$  auses ,  $o{=}^{c}$  “have energy\"). The task is to generate    $o$   given  $s$   and    $r$   as inputs. \nNotation We deﬁne    $X^{s}\\,=\\,\\{x_{0}^{s},...,x_{|s|}^{s}\\}$   as the | | tokens that make up the subject of the relation,  $X^{r}\\ =\\ \\{x_{0}^{r},...,x_{|r|}^{r}\\}$   as the tokens that make up | | the relation of the tuple, and    $X^{o}\\,=\\,\\{x_{0}^{o},...,x_{|o|}^{o}\\}$  | | as the tokens that make up the object of the tuple. The embedding for any word  $x$   is denoted as    $e$  . \n2.2 Transformer Language Model \nWhile  COMET  is agnostic to the language model with which it is initialized, in this work, we use the transformer language model architecture in- troduced in  Radford et al.  ( 2018 ) (GPT), which uses multiple transformer blocks of multi-headed scaled dot product attention and fully connected layers to encode input text ( Vaswani et al. ,  2017 ). Figure  2  depicts different components of the GPT architecture and we deﬁne each component in more depth below. \nTransformer Block As shown in Figure  2 (b), each transformer layer    $l$   contains an architecturally identical transformer block (though with unique trainable parameters) that applies the following transformations to the input to the block: \n\n$$\n\\begin{array}{r l}&{\\tilde{g}^{l}=\\mathbf{MULIATTN}\\big(h^{l-1}\\big)}\\\\ &{g^{l}=\\mathbf{LAYERNORM}\\big(\\tilde{g}^{l}+h^{l-1}\\big)}\\\\ &{\\tilde{h}^{l}=\\mathbf{FFN}(g^{l})}\\\\ &{h^{l}=\\mathbf{LAYERNORM}\\big(\\tilde{h}^{l}+g^{l}\\big)}\\end{array}\n$$\n \nwhere M ULTI A TTN is a multi-headed self- attention mechanism (deﬁned below), FFN is a two-layer feed-forward network, and L AYER - N ORM  represents a layer normalization ( Ba et al. , 2016 ) operation that is applied to the output of the self-attention and the feedforward network. Note that the inputs to the L AYER N ORM  opera- tions contain a residual connection that sums the output of and input to the previous operation. \nMulti-headed Attention The multi-headed at- tention module of each transformer block, shown in Figure  2 (a), is identical to the one originally de- ﬁned by  Vaswani et al.  ( 2017 ). The attention func- tion receives three inputs, a query    $Q$  , key    $K$  , and value  $V$  . The attention is made of multiple  heads that each compute a unique scaled dot product at- tention distribution over    $V$  using    $Q$   and    $K$  : \n\n$$\n\\operatorname{ATESmTilde{ON}}(Q,K,V)=\\operatorname{softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d_{k}}}\\right)V\n$$\n \nwhere  $d_{k}$   is the dimensionality of the input vectors representing the query, key and value. For each of the heads,    $Q,K$  , and    $V$  are uniquely projected prior to the attention being computed: \n\n$$\nH_{i}=\\mathrm{ATANTION}(Q W_{i}^{Q},K W_{i}^{K},V W_{i}^{V})\n$$\n \nwhere    $H_{i}$   is the output of a single attention head and    $W_{i}^{Q},W_{i}^{K}$  , and    $W_{i}^{V}$  are head-speciﬁc projec- tions for    $Q,\\,K$  , and    $V$  , respectively. The outputs of the attention heads  $H_{i}$   are then concatenated: \n\n$$\n\\mathrm{MLTH}(\\mathrm{Q},\\mathrm{K},\\mathrm{V})=[H_{1};...;H_{b}]W^{O}\n$$\n \nwhere    $W^{O}$    is an output projection of the concate- nated outputs of the attention heads. As shown in Figure  2 (c), we follow  Radford et al.  ( 2018 ) and use the output of the previous layer’s transformer block as the query input for the multi-headed at- tention of the next block. The keys and values are outputs of the previous layer’s block for all pre- ceding time steps: \n\n$$\n\\mathbf{MULTilde{M}}_{t}=\\mathbf{MULTilde{H}}\\big(h_{t}^{l-1},\\mathbf{h}_{t}^{l-1},\\mathbf{h}_{t}^{l-1}\\big)\n$$\n \nwhere    $\\mathbf{h}_{t}^{l-1}~=~\\{h^{l-1}\\}_{<t}$   { }  is the set of previous layer transformer block outputs for time steps pre- ceding    $t$  . \nInput Encoder As input to the model, we repre- sent a knowledge tuple    $\\{s,r,o\\}$   as a concatenated sequence of the words of each item of the tuple: \n\n$$\n\\mathbf{X}=\\{X^{s},X^{r},X^{o}\\}\n$$\n \nSince the transformer (a self-attention model) has no concept of ordering of tokens, a position em- bedding    $p_{t}$   is initialized for each absolute position in the sequence ( Vaswani et al. ,  2017 ). For any input word    $x_{t}\\in\\mathbf{X}$  , our encoding of the input is "}
{"page": 3, "image_path": "doc_images/P19-1470_3.jpg", "ocr_text": "ATOMIC Input Template and ConceptNet Relation-only Input Template\n\n[sie] resttne [ron [oto]\n\nPersonX goes to the mall [MASK] <xIntent> to buy clothes\n\nConceptNet Relation to Language Input Template\n\ngo to mall [MASK] [MASK] has prerequisite [MASK] have money\n\nFigure 3: Input token setup for training configurations.\nFor the ATOMIC dataset, the tokens of the subject, X*\n(e.g., PersonX goes to the mall) are followed by mask-\ning tokens, which is followed by a single relation token\nX” (e.g., xIntent), and then the object tokens X°\n(e.g., to buy clothes). The model receives the same in-\nput for ConceptNet, except that a second set of mask-\ning tokens separate X\" and X° because X” can have a\nvariable number of tokens for ConceptNet (§5.2)\n\nthe sum of its word embedding, e; with a position\nembedding encoding its absolute position in the\nsequence X:\n\nnoi=et+py (10)\n\nwhere p; is the position embedding for time step ¢,\nand h is the input to the first transformer layer.\n\n3 Training COMET\n\nCOMET is trained to learn to produce the phrase\nobject o of a knowledge tuple given the tuple’s\nphrase subject s and relation r. More specifically,\ngiven the concatenation of the tokens of s and r:\n[X*, X\"] as input, the model must learn to gener-\nate the tokens of o: X° (See §2.1 for definitions of\nthese variables).\n\nLoss Function To achieve this goal, COMET is\ntrained to maximize the conditional loglikelihood\nof predicting the phrase object tokens, X°:\n\nIsl+Ir/-+lo|\n\nL=- > log P(xt\\r<r)\n\nt=|s|+|r|\n\ndy\n\nwhere |s|, |r|, and |o| are the number of tokens\nin the subject phrase, relation, and object phrase,\nrespectively. Figure 3 outlines how the tokens in s,\nr, and o are organized for different training tasks.\n\nDatasets COMET relies on a seed set of knowl-\nedge tuples from an existing KB to learn to pro-\nduce commonsense knowledge. In this work,\nwe use ATOMIC and ConceptNet as knowledge\nseed sets, but other commonsense knowledge re-\nsources could have been used as well as COMET is\ndomain-agnostic.\n\nInitialization Parameters are initialized to the fi-\nnal language model weights from Radford et al.\n(2018). Additional special tokens that are added\nto the vocabulary for fine tuning (e.g., relation em-\nbeddings such as oReact for ATOMIC and IsA\nfor ConceptNet) are initialized by sampling from\nthe standard normal distribution.\n\nHyperparameters Following Radford et al.\n(2018)’s design of the GPT model, we initialize\nCOMET with 12 layers, 768-dimensional hidden\nstates, and 12 attention heads. We use a dropout\nrate of 0.1 and use GeLU (Hendrycks and Gimpel,\n2016) units as activation functions. During train-\ning, our batch size is 64. Other dataset-specific\nhyperparameters are provided in Appendix A.1.\n\n4 ATOMIC Experiments\n\nThe ATOMIC dataset*, released by Sap et al.\n(2019), contains 877K tuples covering a variety\nof social commonsense knowledge around specific\nevent prompts (e.g., “X goes to the store”). Specif-\nically, ATOMIC distills its commonsense in nine\ndimensions, covering the event’s causes (e.g., “X\nneeds to drive there”), its effects on the agent (e.g.,\n“to get food’) and its effect on other direct (or\nimplied) participants (e.g., “Others will be fed”).\nMore details about ATOMIC can be found in Ap-\npendix D. For our experiments, ATOMIC events\n(e.g., “X goes to the store”) are phrase subjects, s,\nhe dimension (e.g., x Intent) is the phrase rela-\nion, 7, and the causes/effects (e.g., “to get food’’)\nare phrase objects, o. We use the training splits\nrom Sap et al. (2019), resulting in 710k training,\n80k development, and 87k test tuples respectively.\n\n4.1 Setup\n\nMetrics Following Sap et al. (2019), we eval-\nuate our method using BLEU-2 as an automatic\nevaluation metric. We also report the perplexity\nof the model on its gold generations. The remain-\ning automatic metrics in Table 1 measure the pro-\nportion of generated tuples and generated objects\nwhich are not in the training set. We report the\nproportion of all generated tuples that are novel\n(% N/T sro) and that have a novel object (% N/T\no)*. To show that these novel objects are diverse\n(ie., the same novel object is not the only one be-\ning generated), we also report the number of novel\n\nShttps ://nomes.cs.washington.edu/\n~msap/atomic/\n+4 new o represents a new node in the knowledge graph\n\n4765\n", "vlm_text": "The table appears to depict a model for translating between a structured knowledge representation format and a natural language format using tokens. The illustration is split into two main sections. \n\n1. The first section at the top translates a natural language sentence into token components:\n   - **s tokens**: Represent the subject tokens, which in the example given is \"PersonX goes to the mall\".\n   - **mask tokens**: Represent locations in the structure where a relation or intent is implied or needs to be inferred; marked by [MASK].\n   - **r token**: Represents the relation token, which in the example is \"<xIntent>\".\n   - **o tokens**: Represent the object tokens, depicted here as \"to buy clothes\".\n\n2. The second section below bears the title \"ConceptNet Relation to Language Input Template\", illustrating a generic template structure:\n   - The setup is similar with **s tokens**, **mask tokens**, **r tokens**, and **o tokens** laid out linearly.\n   - The template seems to imply an additional set of mask tokens, indicating it can include structures with more complex relationships or additional contexts needing inference.\n\nThis design is likely used for tasks related to natural language understanding or machine learning, where mapping between structured knowledge databases like ConceptNet and natural language is necessary.\nFigure 3: Input token setup for training conﬁgurations. For the A TOMIC  dataset, the tokens of the subject,    $X^{s}$  (e.g., PersonX goes to the mall) are followed by mask- ing tokens, which is followed by a single relation token  $X^{r}$    (e.g.,  xIntent ), and then the object tokens    $X^{o}$  (e.g., to buy clothes). The model receives the same in- put for ConceptNet, except that a second set of mask- ing tokens separate  $X^{r}$    and    $X^{o}$    because    $X^{r}$    can have a variable number of tokens for ConceptNet (§ 5.2 ) \nthe sum of its word embedding,  $e_{t}$   with a position embedding encoding its absolute position in the sequence  $\\mathbf{X}$  : \n\n$$\nh_{t}^{0}=e_{t}+p_{t}\n$$\n \nwhere    $p_{t}$   is the position embedding for time step  $t$  , and  $h^{0}$    is the input to the ﬁrst transformer layer. \n3 Training  COMET \nCOMET  is trained to learn to produce the phrase object    $o$   of a knowledge tuple given the tuple’s phrase subject    $s$   and relation    $r$  . More speciﬁcally, given the concatenation of the tokens of    $s$   and    $r$  :  $[X^{s},X^{r}]$   as input, the model must learn to gener- ate the tokens of  $o$  :    $X^{o}$    (See   $\\S2.1$   for deﬁnitions of these variables). \nLoss Function To achieve this goal,  COMET  is trained to maximize the conditional loglikelihood of predicting the phrase object tokens,    $X^{o}$  : \n\n$$\n\\mathcal{L}=-\\sum_{t=|s|+|r|}^{|s|+|r|+|o|}\\log P(x_{t}|x_{<t})\n$$\n \nwhere    $|s|,\\,|r|$  , and    $|o|$   are the number of tokens in the subject phrase, relation, and object phrase, respectively. Figure  3  outlines how the tokens in  $s$  ,  $r$  , and    $o$   are organized for different training tasks. \nDatasets COMET  relies on a seed set of knowl- edge tuples from an existing KB to learn to pro- duce commonsense knowledge. In this work, we use A TOMIC  and ConceptNet as knowledge seed sets, but other commonsense knowledge re- sources could have been used as well as  COMET  is domain-agnostic. \nInitialization Parameters are initialized to the ﬁ- nal language model weights from  Radford et al. ( 2018 ). Additional special tokens that are added to the vocabulary for ﬁne tuning (e.g., relation em- beddings such as  oReact  for A TOMIC  and  IsA for ConceptNet) are initialized by sampling from the standard normal distribution. \nHyperparameters Following  Radford et al. ( 2018 )’s design of the GPT model, we initialize COMET  with 12 layers, 768-dimensional hidden states, and 12 attention heads. We use a dropout rate of 0.1 and use GeLU ( Hendrycks and Gimpel , 2016 ) units as activation functions. During train- ing, our batch size is 64. Other dataset-speciﬁc hyperparameters are provided in Appendix  A.1 . \n4 A TOMIC  Experiments \nThe A TOMIC  dataset 3 , released by  Sap et al. ( 2019 ), contains 877K tuples covering a variety of social commonsense knowledge around speciﬁc event prompts (e.g., “X goes to the store”). Specif- ically, A TOMIC  distills its commonsense in nine dimensions, covering the event’s causes (e.g., “X needs to drive there”), its effects on the agent (e.g., “to get food”) and its effect on other direct (or implied) participants (e.g., “Others will be fed”). More details about A TOMIC  can be found in Ap- pendix  D . For our experiments, A TOMIC  events (e.g., “X goes to the store”) are phrase subjects,    $s$  , the dimension (e.g.,  xIntent ) is the phrase rela- tion,  $r$  , and the causes/effects (e.g., “to get food”) are phrase objects,    $o$  . We use the training splits from  Sap et al.  ( 2019 ), resulting in 710k training,  $80\\mathrm{k}$  development, and   $87\\mathrm{k}$   test tuples respectively. \n4.1Setup\nMetrics Following  Sap et al.  ( 2019 ), we eval- uate our method using BLEU-2 as an automatic evaluation metric. We also report the perplexity of the model on its gold generations. The remain- ing automatic metrics in Table  1  measure the pro- portion of generated tuples and generated objects which are not in the training set. We report the proportion of all generated tuples that are novel  $(\\%\\;\\mathrm{N}/\\mathrm{T}\\;s r o)$   and that have a novel object (  $\\%$   N/T  $o)^{4}$  . To show that these novel objects are diverse (i.e., the same novel object is not the only one be- ing generated), we also report the number of novel "}
{"page": 4, "image_path": "doc_images/P19-1470_4.jpg", "ocr_text": "Model PPL* BLEU-2) N/T sro®° N/To NiUo\n9ENC9DEC (Sap et al., 2019) - 10.01 100.00 8.61 40.77\nNearestNeighbor (Sap et al., 2019) - 6.61 - - -\nEvent2(IN) VOLUN (Sap et al., 2019) - 9.67 100.00 9.52 45.06\nEvent2PERSONX/Y (Sap et al., 2019) - 9.24 100.00 8.22 41.66\nEvent2PRE/Post (Sap et al., 2019) - 9.93 100.00 7.38 41.99\nCOMET (- pretrain) 15.42 13.88 100.00 7.25. 45.71\nCOMET 11.14 15.10 100.00 9.71 51.20\n\nTable 1: Automatic evaluations of quality and novelty for generations of ATOMIC commonsense. No novelty\nscores are reported for the NearestNeighbor baseline because all retrieved sequences are in the training set.\n\nModel || oEffect oReact oWant xAttr xEffect xIntent xNeed xReact xWant || Avg\n9Enc9Dec (Sap et al., 2019) 22.92 32.92 35.50 52.20 47.52 51.70 48.74 63.57 51.56 || 45.32\nEvent2(In)voluntary (Sap et al., 2019) 26.46 36.04 34.70 52.58 46.76 61.32 49.82 71.22 52.44 || 47.93\nEvent2PersonX/Y (Sap et al., 2019) 24.72 33.80 35.08 52.98 48.86 53.93 54.05 66.42 54.04 || 46.41\nEvent2Pre/Post (Sap et al., 2019) 26.26 34.48 35.78 52.20 46.78 57.77 47.94 72.22 47.94 || 46.76\nCOMET (- pretrain) | 25.90 35.40 40.76 48.04 47.20 58.88 59.16 64.52 65.66 || 49.50\nCOMET | 29.02 37.68 44.48 57.48 55.50 68.32 64.24 76.18 75.16 | 56.45\n\nTable 2: Human score of generations of ATOMIC commonsense. We present comparisons to the baselines from\n\nSap et al. (2019). Underlined results are those where COMET is not significantly better at p < 0.05\n\nobjects as a function of the set of unique objects\nproduced for all test set events (% N/U 0).\n\nFinally, we perform a human evaluation using\nworkers from Amazon Mechanical Turk (AMT).\nWorkers are asked to identify whether a model\ngeneration of ATOMIC commonsense adequately\ncompletes a plausible tuple of phrase subject, rela-\ntion, and phrase object. Following the setup of Sap\net al. (2019), we evaluate 100 randomly selected\nevents from the test set. For each event and rela-\ntion type, 10 candidates are generated using beam\nsearch and the full beam is evaluated by five differ-\nent workers. Overall, n=5000 ratings are produced\nper relation (100 events x 5 workers x 10 candi-\ndates). The reported Avg in Table 2 is an aver-\nage of these scores, yielding n=45000 total ratings\nfor each model. We use Pitman’s test (Noreen,\n1989) with 100k permutations to test for statis-\ntical significance. Because 50 different hypothe-\nses are tested (9 relations + the total), the Holm-\nBonferroni method (Holm, 1979) is used to correct\nsignificance thresholds. Example events from the\ndevelopment set and their generated phrase objects\nare available in Table 5.\n\nBaselines We report the performance of our\nmethod against the models trained in Sap et al.\n(2019) that use LSTM sequence-to-sequence mod-\nels (Sutskever et al., 2014) to encode the input sub-\nject and relation and produce an output object.\n\nAblations To evaluate how pre-training on a\nlarge corpus helps the model learn to produce\nknowledge, we train a version of COMET that is\nnot initialized with pre-trained weights (COMET (-\npretrain)). We also evaluate the data efficiency of\nour method by training models on different pro-\nportions of the training data. Finally, because\nhe ultimate goal of our method is to be able\no perform high-quality, diverse knowledge base\nconstruction, we explore how various decoding\nschemes affect the quality of candidate knowledge\nuples. We present the effect of the following gen-\neration strategies: argmax greedy decoding, beam\nsearch with beam sizes, b=2, 5, 10, and top-k sam-\npling with k = 5, 10. For each decoding method,\nwe conduct the human evaluation on the number\nof final candidates produced by each method.\n\n4.2 Results\n\nOverall performance The BLEU-2 results in\nTable 1 indicate that COMET exceeds the perfor-\nmance of all baselines, achieving a 51% relative\nimprovement over the top performing model of\nSap et al. (2019). More interesting, however, is the\nresult of the human evaluation, where COMET re-\nported a statistically significant relative Avg per-\nformance increase of 18% over the top baseline,\n\nSap et al. (2019)’s models were trained with a different\nvocabulary so a direct perplexity comparison is not possible.\n\n°All test set s do not appear in the training set so all full\ntuples must be novel.\n\n4766\n", "vlm_text": "The table presents performance metrics for various models on specific evaluation criteria. Here's a breakdown of the content:\n\n### Model Names and References\n- **9ENC9DEC:** Sap et al., 2019\n- **NearestNeighbor:** Sap et al., 2019\n- **Event2(IN)VOLUN:** Sap et al., 2019\n- **Event2PERSONX/Y:** Sap et al., 2019\n- **Event2PRE/POST:** Sap et al., 2019\n- **COMET (- pretrain)**\n- **COMET**\n\n### Performance Metrics\n- **PPL:** Perplexity, with a superscript indicating a specific condition or note (mentioned as 5).\n- **BLEU-2:** Bilingual Evaluation Understudy Score using 2-gram precision.\n- **N/T sro:** Normalized by type (or attribute) evaluation score, with a superscript (6) indicating further specifics.\n- **N/T o:** Normalized by type (or attribute) score.\n- **N/U o:** Normalized by type (or attribute) unconditioned score.\n\n### Results\n- **Perplexity (PPL):** Only provided for COMET models: 15.42 for \"COMET (- pretrain)\" and 11.14 for \"COMET\".\n- **BLEU-2 Scores:** Range from 6.61 for NearestNeighbor to a peak of 15.10 for COMET.\n- **N/T sro:** Consistently 100.00 for all models presented.\n- **N/T o Scores:** Range from 7.25 for \"COMET (- pretrain)\" to 9.71 for COMET.\n- **N/U o Scores:** Range from 40.77 for 9ENC9DEC to the highest at 51.20 for COMET.\n\nThe COMET model, particularly without pre-training and fully trained, shows superior performance in the BLEU-2 and N/U o metrics compared to other models listed.\nThe table compares different models based on various metrics related to event understanding. Here's a breakdown:\n\n- **Models**: \n  - 9Enc9Dec (Sap et al., 2019)\n  - Event2(In)voluntary (Sap et al., 2019)\n  - Event2PersonX/Y (Sap et al., 2019)\n  - Event2Pre/Post (Sap et al., 2019)\n  - COMET (- pretrain)\n  - COMET\n\n- **Metrics**:\n  - oEffect\n  - oReact\n  - oWant\n  - xAttr\n  - xEffect\n  - xIntent\n  - xNeed\n  - xReact\n  - xWant\n  - Avg\n\n- **Values**: Each model's performance is measured, with specific values given for each metric.\n\n- **Observations**: \n  - COMET (pretrained and non-pretrained) models generally show better performance across most metrics compared to baseline models (9Enc9Dec, Event2*).\n  - The COMET model has the highest average score, indicating superior overall performance.\nobjects as a function of the set of  unique  objects produced for all test set events   $({\\%}\\,\\mathbf{N}/\\mathbf{U}\\,{\\it o})$  . \nFinally, we perform a human evaluation using workers from Amazon Mechanical Turk (AMT). Workers are asked to identify whether a model generation of A TOMIC  commonsense adequately completes a plausible tuple of phrase subject, rela- tion, and phrase object. Following the setup of  Sap et al.  ( 2019 ), we evaluate 100 randomly selected events from the test set. For each event and rela- tion type, 10 candidates are generated using beam search and the full beam is evaluated by ﬁve differ- ent workers. Overall,  $\\scriptstyle{\\mathfrak{n}}=5000$   ratings are produced per relation (100 events    $\\times\\,5$   workers    $\\times\\ 10$   candi- dates). The reported  Avg  in Table  2  is an aver- age of these scores, yielding   $\\mathsf{n{=}45000}$   total ratings for each model. We use Pitman’s test ( Noreen , 1989 ) with   $100\\mathbf{k}$   permutations to test for statis- tical signiﬁcance. Because 50 different hypothe- ses are tested (9 relations   $^+$   the total), the Holm- Bonferroni method ( Holm ,  1979 ) is used to correct signiﬁcance thresholds. Example events from the development set and their generated phrase objects are available in Table  5 . \nBaselines We report the performance of our method against the models trained in  Sap et al. ( 2019 ) that use LSTM sequence-to-sequence mod- els ( Sutskever et al. ,  2014 ) to encode the input sub- ject and relation and produce an output object. \nAblations To evaluate how pre-training on a large corpus helps the model learn to produce knowledge, we train a version of    $\\mathbb{C O M E T}$   that is not initialized with pre-trained weights ( COMET  (- pretrain)). We also evaluate the data efﬁciency of our method by training models on different pro- portions of the training data. Finally, because the ultimate goal of our method is to be able to perform high-quality, diverse knowledge base construction, we explore how various decoding schemes affect the quality of candidate knowledge tuples. We present the effect of the following gen- eration strategies: argmax greedy decoding, beam search with beam sizes,  $\\tt b\\mathrm{=}2$  , 5, 10, and top-  $\\cdot k$   sam- pling with  $\\mathbf{k}=5$  , 10. For each decoding method, we conduct the human evaluation on the number of ﬁnal candidates produced by each method. \n4.2 Results \nOverall performance The BLEU-2 results in Table  1  indicate that  COMET  exceeds the perfor- mance of all baselines, achieving a   $51\\%$   relative improvement over the top performing model of Sap et al.  ( 2019 ). More interesting, however, is the result of the human evaluation, where  COMET  re- ported a statistically signiﬁcant relative  Avg  per- formance increase of   $18\\%$   over the top baseline, "}
{"page": 5, "image_path": "doc_images/P19-1470_5.jpg", "ocr_text": "COMET Decoding method l| oEffect oReact oWant xAttr xEffect xIntent xNeed xReact xWant | Avg\nTop-5 random sampling (n=2500 per relation) 34.60 44.04 35.56 64.56 55.68 58.84 46.68 80.96 58.52 || 53.27\nTop-10 random sampling (n=5000 per relation) 25.20 37.42 27.34 49.20 47.34 47.06 38.24 72.60 48.10 || 43.61\nBeam search - 2 beams (n=1000 per relation) 43.70 54.20 47.60 84.00 51.10 73.80 50.70 85.80 78.70 || 63.29\nBeam search - 5 beams (n=2500 per relation) 37.12 45.36 42.04 63.64 61.76 63.60 57.60 78.64 68.40 || 57.57\nBeam search - 10 beams (n=5000 per relation) 29.02 37.68 4448 57.48 55.50 68.32 64.24 76.18 75.16 || 56.45\nGreedy decoding (n=500 per relation) 61.20 69.80 80.00 77.00 53.00 89.60 85.60 92.20 89.40 | 77.53\nHuman validation of gold ATOMIC ll 84.62 86.13 83.12 78.44 83.92 91.37 81.98 95.18 90.90 l| 86.18\n\nTable 3: Human evaluation testing effect of different decoding schemes on candidate tuple quality. The number of\nratings made per relation for each decoding method is provided in the first column.\n\n%traindata PPL BLEU-2 N/To N/Uo\n1% train 23.81 5.08 7.24 49.36\n\n10% train = 13.74 12.72 9.54 58.34\n\n50% train 11.82 13.97 9.32 50.37\n\nFULL (- pretrain) 15.18 13.22 714 44.55\nFULL train = 11.13 14.34 9.51 50.05\n\nTable 4: Effect of amount of training data on automatic\nevaluation of commonsense generations\n\nEvent2IN(VOLUN). This performance increase is\nconsistent, as well, with an improvement being\nobserved across every relation type. In addition\nto the quality improvements, Table | shows that\nCOMET produces more novel tuple objects than\nthe baselines, as well.\n\nLearning knowledge from language  Signifi-\ncant differences were also observed between the\nperformance of the model whose weights were ini-\ntialized with the pre-trained parameters from the\nGPT model of Radford et al. (2018) and a model\nwith the same architecture that was trained from\nrandom initialization. This 14% relative improve-\nment in overall human performance confirms that\nthe language representations learned by the GPT\nmodel are transferable to generating natural lan-\nguage commonsense knowledge.\n\nEffect of decoding algorithm In Table 3, we\nshow the effect of different generation policies on\nknowledge quality. The most interesting result\nis that using greedy decoding to produce knowl-\nedge tuples only results in a 10% relative perfor-\nmance gap compared to a human evaluation of\nthe ATOMIC test set, showing that the knowledge\nproduced by the model approaches human perfor-\nmance. While producing more total candidates\ndoes lower overall performance, quality assess-\n\nSeed Concept Relation Generated Plausible\nX holds out X’s hand to Y xAttr helpful v\nX meets Y eyes xAttr intense v\nX watches Y every __ xAttr observant v\nX eats red meat xEffect gets fat v\nX makes crafts xEffect gets dirty v\nX turns X’s phone xEffect — getsatext\nXpours___over Y’shead oEffect gets hurt v\nX takes Y’s head off oEffect bleeds v\nX pisses on Y’s bonfire oEffect gets burned\n\nX spoils somebody rotten xIntent to be mean\n\nX gives Y some pills xIntent  tohelp v\nX provides for Y’s needs xIntent tobe helpful v\nX explains Y’s reasons xNeed to know Y v\nX fulfils X’s needs xNeed to have a plan v\nX gives Y everything xNeed to buy something v\nX eats pancakes xReact satisfied v\nX makes ____at work xReact proud v\nX moves house xReact happy v\nX gives birth to the Y oReact happy v\nX gives Y’s friend __ oReact grateful v\nX goes with friends oReact happy v\nX gets all the supplies xWant to make a list v\nX murders Y’s wife xWant to hide the body v\nX starts shopping xWant to go home v\nX develops Y theory oWant to thank X v\nX offer Y a position oWant to accept the job v\nX takes out fordinner — oWant to eat v\n\nTable 5: Generations that were randomly selected\nfrom a subset of novel generations from the ATOMIC\ndevelopment set. A novel generation is a sro tuple not\nfound in the training set. Manual evaluation of each tu-\nple indicates whether the tuple is considered plausible\nby a human annotator.\n\nments still hover around 55%’ for a beam size of\n10. This result suggests that COMET could be ef-\nfective with human evaluators in the loop to con-\nfirm the correctness of generated tuples.\n\nEfficiency of learning from seed tuples Be-\ncause not all domains will have large available\ncommonsense KBs on which to train, we explore\nhow varying the amount of training data avail-\nable for learning affects the quality and novelty\nof the knowledge that is produced. Our results in\nTable 4 indicate that even with only 10% of the\navailable training data, the model is still able to\n\n7This number is partially low due to the many “none” ref-\nerences in the oEffect, oReact, oWant categories. In\nany set of 10 candidates, “none\" can only be predicted once,\nwhich causes most candidates in the beam to be incorrect if\n“none\" is the appropriate answer.\n\n4767\n", "vlm_text": "The table presents the performance of different decoding methods used in the COMET framework for generating commonsense inferences, evaluated against a human baseline on the ATOMIC dataset. Each row represents one specific decoding method or human validation, and each column corresponds to a different types of commonsense relation: oEffect, oReact, oWant, xAttr, xEffect, xIntent, xNeed, xReact, xWant, and Avg (average performance across all relations).\n\nHere are the details for each row:\n- **Top-5 random sampling (n=2500 per relation):** Reports scores for each relation and an average score of 53.27.\n- **Top-10 random sampling (n=5000 per relation):** Lower scores than Top-5 random sampling with an average of 43.61.\n- **Beam search - 2 beams (n=1000 per relation):** Improved scores over random sampling, highest score of 84.00 for xAttr, with an average of 63.29.\n- **Beam search - 5 beams (n=2500 per relation):** Scores lower than with 2 beams, with an average of 57.57.\n- **Beam search - 10 beams (n=5000 per relation):** Shows further reduction in scores, averaging 56.45.\n- **Greedy decoding (n=500 per relation):** Generally highest scores among decoding methods, especially for oWant, xIntent, xNeed, xReact, xWant, and an average of 77.53.\n- **Human validation of gold ATOMIC:** Serves as the positive control with high scores across all relations, averaging 86.18.\n\nFrom these results, we observe that greedy decoding performs best among automated methods but still does not reach the performance level of human validation.\nThe table displays the results of different training data percentages on model performance metrics. Here is the information provided:\n\n- **% train data**: This indicates the different amounts of training data used in the model training scenarios. The percentage columns are labeled as \"1% train,\" \"10% train,\" \"50% train,\" \"FULL (- pretrain),\" and \"FULL train.\"\n\n- **PPL** (Perplexity): This column shows the perplexity score for each training scenario. Lower perplexity indicates better performance:\n  - 1% train: 23.81\n  - 10% train: 13.74\n  - 50% train: 11.82\n  - FULL (- pretrain): 15.18\n  - FULL train: 11.13\n\n- **BLEU-2**: This column indicates the BLEU score (a metric for evaluating the quality of text, especially in tasks like machine translation) calculated on bi-grams:\n  - 1% train: 5.08\n  - 10% train: 12.72\n  - 50% train: 13.97\n  - FULL (- pretrain): 13.22\n  - FULL train: 14.34\n\n- **N/T °** (presumably a specific metric, possibly related to novelty or type): \n  - 1% train: 7.24\n  - 10% train: 9.54 (bolded)\n  - 50% train: 9.32\n  - FULL (- pretrain): 7.14\n  - FULL train: 9.51\n\n- **N/U °** (presumably a specific metric, possibly related to novelty or uniqueness): \n  - 1% train: 49.36\n  - 10% train: 58.34 (bolded)\n  - 50% train: 50.37\n  - FULL (- pretrain): 44.55\n  - FULL train: 50.05\n\nThe bold values in the N/T and N/U columns indicate the highest scores in those category scenarios, suggesting the percentages or settings where the model performs best according to those specific metrics.\nEvent2I N (V OLUN ). This performance increase is consistent, as well, with an improvement being observed across every relation type. In addition to the quality improvements, Table  1  shows that COMET  produces more novel tuple objects than the baselines, as well. \nLearning knowledge from language Signiﬁ- cant differences were also observed between the performance of the model whose weights were ini- tialized with the pre-trained parameters from the GPT model of  Radford et al.  ( 2018 ) and a model with the same architecture that was trained from random initialization. This   $14\\%$   relative improve- ment in overall human performance conﬁrms that the language representations learned by the GPT model are transferable to generating natural lan- guage commonsense knowledge. \nEffect of decoding algorithm In Table  3 , we show the effect of different generation policies on knowledge quality. The most interesting result is that using greedy decoding to produce knowl- edge tuples only results in a   $10\\%$   relative perfor- mance gap compared to a human evaluation of the A TOMIC  test set, showing that the knowledge produced by the model approaches human perfor- mance. While producing more total candidates does lower overall performance, quality assess- \nThe table contains four columns titled \"Seed Concept,\" \"Relation,\" \"Generated,\" and \"Plausible.\" Here's a summary of the content:\n\n1. **Seed Concept**: Actions or scenarios involving \"X\" and \"Y\" (e.g., \"X holds out X's hand to Y\").\n\n2. **Relation**: Categories describing the type of relation (e.g., xAttr, xEffect, oEffect, xIntent, xNeed, xReact, oReact, xWant, oWant).\n\n3. **Generated**: Descriptions or outcomes generated from the seed concept and relation (e.g., \"helpful,\" \"observant,\" \"gets fat\").\n\n4. **Plausible**: Checkmarks indicating whether the generated description or outcome is plausible for the given seed concept and relation.\n\nSome rows contain incomplete actions or scenarios with blanks (e.g., \"X watches Y every ____,\" \"X makes ___ at work\"). Each relation type is marked with specific prefixes (e.g., \"x\" for actions by X, \"o\" for actions affecting Y).\nTable 5: Generations that were  randomly selected from a subset of  novel  generations from the A TOMIC development set. A novel generation is a  sro  tuple not found in the training set. Manual evaluation of each tu- ple indicates whether the tuple is considered plausible by a human annotator. \nments still hover around   $55\\%^{7}$    for a beam size of 10. This result suggests that  COMET  could be ef- fective with human evaluators in the loop to con- ﬁrm the correctness of generated tuples. \nEfﬁciency of learning from seed tuples Be- cause not all domains will have large available commonsense KBs on which to train, we explore how varying the amount of training data avail- able for learning affects the quality and novelty of the knowledge that is produced. Our results in Table  4  indicate that even with only   $10\\%$   of the available training data, the model is still able to produce generations that are coherent, adequate, and novel. Using only   $1\\%$   of the training data clearly diminishes the quality of the produced gen- erations, with signiﬁcantly lower observed results across both quality and novelty metrics. Interest- ingly, we note that training the model without pre- trained weights performs comparably to training with   $10\\%$   of the seed tuples, quantifying the im- pact of using pre-trained language representations. "}
{"page": 6, "image_path": "doc_images/P19-1470_6.jpg", "ocr_text": "produce generations that are coherent, adequate,\nand novel. Using only 1% of the training data\nclearly diminishes the quality of the produced gen-\nerations, with significantly lower observed results\nacross both quality and novelty metrics. Interest-\ningly, we note that training the model without pre-\ntrained weights performs comparably to training\nwith 10% of the seed tuples, quantifying the im-\npact of using pre-trained language representations.\n\n5 ConceptNet Experiments\n\nThe ConceptNet dataset®, provided by Li et al.\n(2016), consists of tuples obtained from the Open\nMind Common Sense (OMCS) entries in Concept-\nNet 5 (Speer et al., 2017). Tuples are in the stan-\ndard sro form — (e.g., take a nap, Causes, have\nenergy). The most confident 1200 tuples were\nused to create the test set, while the next 1200\ntuples were used to create two development sets,\nwhich we combine in this work. The 100k version\nof the training set was used to train models, which\ncontains 34 relation types.\n\n5.1 Setup\n\nMetrics We evaluate our models that generate\nConceptNet relations using the following metrics.\nFirst, we report the perplexity of the gold relations\nin the test set (PPL). To evaluate the quality of gen-\nerated knowledge, we also report the number of\ngenerated positive examples in the test set that are\nscored as correct by the pre-trained Bilinear AVG\nmodel developed by Li et al. (2016).? For a given\nsro tuple, this model produces a probability for\nwhether the tuple is correct. We threshold scores\nat 50% probability to identify positive predictions.\nOn the completion task originally proposed in Li\net al. (2016), this model achieved 92.5% accuracy\non the test set, indicating that it is a strong proxy\nfor automatically evaluating whether a generated\ntuple is correct. Finally, we report the same nov-\nelty metrics as for ATOMIC: N/T sro and N/T o.\n\nBaselines As a baseline, we re-implement\nthe BiLSTM model proposed by Saito et al.\n(2018) with minor modifications outlined in Ap-\npendix A.2. This model is trained to learn to en-\ncode knowledge in both directions: sr — o and\n\nSnhttps://ttic.uchicago.edu/~kgimpel/\ncommonsense.html\n\n° A pre-trained model can be found at https:\n//ttic.uchicago.edu/~kgimpel/comsense_\nresources/ckbc-demo.tar.gz\n\nModel PPL Score N/T sro N/To Human\nLSTM - s - 60.83 86.25 7.83 63.86\nCKBG (Saito et al., 2018) - S717 86.25 8.67 53.95\nCOMET (- pretrain) 8.05 89.25 36.17 6.00 83.49\nCOMET - RELTOK 4.39 95.17 56.42 2.62 92.11\nCOMET 4.32 95.25 59.25 3.75 91.69\n\nTable 6: ConceptNet generation Results\n\nor — s to help augment a knowledge base com-\npletion model. It is only evaluated on the sr — o\ntuple generation task, however. For posterity, we\nalso include the result from a LSTM model that is\nonly trained on the sr — o task (LSTM - s).\n\nAblations We include the following ablations\nof our full model. First, we evaluate how pre-\ntraining on a large-scale corpus (Radford et al.,\n2018) helps performance by training a comparison\nmodel from scratch, denoted COMET (- pretrain)\nin Table 6. Second, in our main model, we map\nrelation names to natural language (e.g., IsA >\n“is a’; HasSubevent — “has subevent”) so the\nmodel can learn to represent these concepts with\nlanguage, as opposed to learning a special embed-\nding from scratch for each relation (Levy et al.,\n2017). As an ablation, we train a model with-\nout converting relation tokens to natural language\n(e.g., IsA & “is a”), which we denote COMET -\nRELTOK.\n\n5.2 Results\n\nQuality Our results indicate that high-quality\nknowledge can be generated by the model: the low\nperplexity scores in Table 6 indicate high model\nconfidence in its predictions, while the high clas-\nsifier score (95.25%) indicates that the KB com-\npletion model of Li et al. (2016) scores the gener-\nated tuples as correct in most of the cases. While\nadversarial generations could be responsible for\nhis high score, a human evaluation (following\nhe same design as for ATOMIC) scores 91.7% of\ngreedily decoded tuples as correct. Randomly se-\nlected examples provided in Table 7 also point to\nhe quality of knowledge produced by the model.\n\nNovelty In addition to being high quality, the\ngenerated tuples from COMET are also novel, with\n59.25% of the tuples not being present in the train-\ning set, showing that the model is capable of gen-\nerating new edges between nodes, and even cre-\nating new nodes — 3.75% of o nodes are novel —\nto extend the size of the knowledge graph. One\nshortcoming, however, is that novel generations\n\n4768\n", "vlm_text": "\n5 ConceptNet Experiments \nThe ConceptNet dataset 8 , provided by  Li et al. ( 2016 ), consists of tuples obtained from the Open Mind Common Sense (OMCS) entries in Concept- Net 5 ( Speer et al. ,  2017 ). Tuples are in the stan- dard  sro  form – (e.g., take a nap,  Causes , have energy). The most conﬁdent 1200 tuples were used to create the test set, while the next 1200 tuples were used to create two development sets, which we combine in this work. The   $100\\mathbf{k}$   version of the training set was used to train models, which contains 34 relation types. \n5.1Setup\nMetrics We evaluate our models that generate ConceptNet relations using the following metrics. First, we report the perplexity of the gold relations in the test set (PPL). To evaluate the quality of gen- erated knowledge, we also report the number of generated positive examples in the test set that are scored as correct by the pre-trained Bilinear AVG model developed by  Li et al.  ( 2016 ).   For a given sro  tuple, this model produces a probability for whether the tuple is correct. We threshold scores at  $50\\%$   probability to identify positive predictions. On the completion task originally proposed in  Li et al.  ( 2016 ), this model achieved  $92.5\\%$   accuracy on the test set, indicating that it is a strong proxy for automatically evaluating whether a generated tuple is correct. Finally, we report the same nov- elty metrics as for A TOMIC :  N/T  sro  and  N/T    $o$  . \nBaselines As a baseline, we re-implement the BiLSTM model proposed by  Saito et al. ( 2018 ) with minor modiﬁcations outlined in Ap- pendix  A.2 . This model is trained to learn to en- code knowledge in both directions:    $s r\\,\\rightarrow\\,o$   and \nThis table compares the performance of different models on several evaluation metrics. The columns in the table represent the following:\n\n- **Model**: The name or type of the model being evaluated.\n- **PPL**: Perplexity, a measure of how well a probability model predicts a sample.\n- **Score**: A performance metric for the model, although the specific nature of this score is not defined here.\n- **N/T\\(_{sro}\\)**: A metric related to precision or performance, possibly representing the accuracy for a specific task or dataset involving subject-relation-object (sro) triples.\n- **N/T\\(_{o}\\)**: Similar to N/T\\(_{sro}\\), this likely measures another aspect of task performance involving object triples.\n- **Human**: A metric comparing the model's performance to human performance on the same task.\n\nThe models mentioned include:\n\n- LSTM - \\(s\\): Long Short-Term Memory model with a specific setting denoted by \"s\".\n- CKBG (Saito et al., 2018): A model or method by Saito et al. from 2018.\n- COMET variants: These include COMET with various settings like no pretraining (- pretrain), RELTOK, and a base version without additional specifications.\n\nThe table highlights in bold the best performance under each column. Notably, the COMET model (without any specific variant label) shows superior performance across most metrics, including the lowest perplexity (4.32), highest score (95.25), and closest approximation to human performance (91.69).\n $o r\\,\\rightarrow\\,s$   to help augment a knowledge base com- pletion model. It is only evaluated on the    $s r\\rightarrow o$  tuple generation task, however. For posterity, we also include the result from a LSTM model that is only trained on the    $s r\\rightarrow o$   task (LSTM -  s ). \nAblations We include the following ablations of our full model. First, we evaluate how pre- training on a large-scale corpus ( Radford et al. , 2018 ) helps performance by training a comparison model from scratch, denoted  COMET  (- pretrain) in Table  6 . Second, in our main model, we map relation names to natural language (e.g.,    $\\tt T S\\bar{A}\\rightarrow$  “is a”;  HasSubevent  $\\rightarrow$  “has subevent”) so the model can learn to represent these concepts with language, as opposed to learning a special embed- ding from scratch for each relation ( Levy et al. , 2017 ). As an ablation, we train a model with- out converting relation tokens to natural language (e.g.,  $\\tt I S\\tt A\\ne$ “is a”), which we denote COMET -R EL T OK . \n5.2 Results \nQuality Our results indicate that high-quality knowledge can be generated by the model: the low perplexity scores in Table  6  indicate high model conﬁdence in its predictions, while the high clas- siﬁer score   $(95.25\\%)$   indicates that the KB com- pletion model of  Li et al.  ( 2016 ) scores the gener- ated tuples as correct in most of the cases. While adversarial generations could be responsible for this high score, a human evaluation (following the same design as for A TOMIC ) scores   $91.7\\%$   of greedily decoded tuples as correct. Randomly se- lected examples provided in Table  7  also point to the quality of knowledge produced by the model. \nNovelty In addition to being high quality, the generated tuples from  COMET  are also novel, with  $59.25\\%$   of the tuples not being present in the train- ing set, showing that the model is capable of gen- erating new edges between nodes, and even cre- ating new nodes –   $3.75\\%$   of    $o$   nodes are novel – to extend the size of the knowledge graph. One shortcoming, however, is that novel generations "}
{"page": 7, "image_path": "doc_images/P19-1470_7.jpg", "ocr_text": "x\n3\nRg\n\n75%\n\n50%\n\nClassifier Accuracy\n\n25%\n\n% of novel tuples\nAccuracy\n0.0 0.33 05 0.67 1.0\nEdit Distance\n\n% of tuples with edit distance >=\n\n0%\n\nFigure 4: The percentage of novel ConceptNet de-\nvelopment set tuples per minimum edit distance from\ntraining tuples. In green: classifier-scored accuracy of\neach subset.\n\nare sometimes simplified forms of tuples from the\ntraining set. In Table 7, for example, the tuple\n“doctor CapableOf save life” is not present in\nthe training set, but “doctor CapableOf save\nperson life” is. Many tuples, however, are com-\npletely novel, such as “bird bone HasProperty\nfragile” and “driftwood AtLocation beach”,\nwhich have no related tuples in the training set.\nTo explore further, we investigate by how much\nnovel tuples from the development set differ from\ntraining set phrase objects for the same s,7 using\nminimum edit distance of phrase objects. We mea-\nsure the edit distance of phrase object oge, in the\ntuple (s, 7, Odey) to the o;,,, from the nearest train-\ning tuple (s, 7, O¢rn). Edit distance is measured us-\ning word tokens (excluding stop words) and nor-\nmalized by the maximum number of words in ogey\nOr Otrn. The maximum edit distance is one (i.e.,\nentirely different word sequences) and the mini-\nmum edit distance is zero (i.e., the same sequence\nexcluding stopwords). Figure 4 shows the percent-\nage of novel development set tuples that have an\nedit distance from the closest training set tuple of\nat least the value on the x-axis. Over 75% of the\nnovel tuples have objects that are a normalized edit\ndistance of >= 0.5 from the training phrase ob-\njects, indicating that most of the novel phrase ob-\njects have significantly different word sequences\nfrom their closest analogues in the training set.\n\nLearning knowledge from language Simi-\nlarly to ATOMIC, we explore how pre-training\nCOMET on a large language corpus affects its\nability to generalize commonsense. This effect\nis apparent in Table 6, with a clear improve-\nment on automatic and human evaluations by the\npretrained COMET over the randomly initialized\n\nSeed Relation Completion —_ Plausible\npiece Partof machine v\nbread Isa food v\noldsmobile IsA car v\nhappiness IsA feel v\nmath IsA subject v\nmango IsA fruit v\nmaine IsA state v\nplanet AtLocation space v\ndust AtLocation fridge\n\npuzzle AtLocation your mind ®\ncollege AtLocation town v\ndental chair AtLocation dentist v\nfinger AtLocation your finger\n\nsing Causes you feel good v\ndoctor Capableot save life v\npost office Capableof receive letter v\ndove Symbol0f£ purity v\nsun HasProperty big v\nbird bone HasProperty fragile v\nearth HasA many plant v\nyard UsedFor play game v\nget pay HasPrerequisite work v\nprinton printer HasPrerequisite get printer v\nplay game HasPrerequisite have game v\nlive HasLastSubevent die v\nswim HasSubevent get wet v\nsit down MotivatedByGoal you be tire v\nall paper ReceivesAction recycle v\nchair Madeot wood v\nearth DefinedAs planet v\n\nTable 7: Randomly selected and novel generations\nfrom the ConceptNet development set. Novel genera-\ntions are sro tuples not found in the training set. Man-\nual evaluation of each tuple indicates whether the tuple\nis considered plausible by a human annotator\n\nmodel. Qualitatively, we observe this effect in Ta-\nble 7 with the generated example tuple “mango\nIsA fruit\", which is not present in the training set.\nThe only tuple containing the “mango\" entity in\nthe training set is “mango UsedFor salsa\", which\nis not informative enough. As confirmation, we\nobserve that the output from COMET (- pretrain) is\n“mango IsA spice”, which could be a reasonable\ninference given the information about “mango” in\nthe seed set of knowledge.\n\nRepresenting relations with language While\nthe automatic metrics point to insignificant differ-\nences when comparing models with symbol re-\nlations and those with natural language relations\n(Table 6), examples can provide qualitative in-\nsights into the benefits of representing relations as\nlanguage. While the only non-ornithological ref-\nerence to a “dove\" in the ConceptNet training set\nis “dove CapableoOf fly”, our model learns to\ngeneralize to produce the tuple “dove Symbol0f\npurity”. The model that uses symbol relation em-\nbeddings only manages to produce the relation\n“dove SymbolOf submarine”, which seems to\nrelate “submarine” to a more nautical (and unre-\nlated) word sense of “‘dove\".\n\n4769\n", "vlm_text": "The image is a graph showing two curves. The x-axis represents the edit distance, ranging from 0 to 1. The y-axis on the left represents the percentage of novel tuples with an edit distance greater than or equal to a given value, while the y-axis on the right represents classifier accuracy.\n\n- The blue shaded area indicates the percentage of novel ConceptNet development set tuples for different minimum edit distances from the training tuples, with percentages ranging from 0% to 100%.\n- The green line represents the classifier's accuracy for each subset of the tuples at different edit distances, with accuracy values ranging from 0.5 to 1.0.\n\nThe graph shows that as the edit distance increases, the percentage of novel tuples generally decreases, and the classifier's accuracy remains high, close to 100%, but slightly decreases as the edit distance increases.\nare sometimes simpliﬁed forms of tuples from the training set. In Table  7 , for example, the tuple “doctor  CapableOf  save life” is not present in the training set, but “doctor  CapableOf  save person life” is. Many tuples, however, are com- pletely novel, such as “bird bone  HasProperty fragile” and “driftwood  AtLocation  beach”, which have no related tuples in the training set. \nTo explore further, we investigate by how much novel tuples from the development set differ from training set phrase objects for the same    $s,r$   using minimum edit distance of phrase objects. We mea- sure the edit distance of phrase object    $o_{d e v}$   in the tuple    $(s,r,o_{d e v})$   to the  $o_{t r n}$   from the nearest train- ing tuple    $(s,r,o_{t r n})$  . Edit distance is measured us- ing word tokens (excluding stop words) and nor- malized by the maximum number of words in  $o_{d e v}$  or    $o_{t r n}$  . The maximum edit distance is one (i.e., entirely different word sequences) and the mini- mum edit distance is zero (i.e., the same sequence excluding stopwords). Figure  4  shows the percent- age of novel development set tuples that have an edit distance from the closest training set tuple of at least the value on the   $\\mathbf{X}$  -axis. Over  $75\\%$   of the novel tuples have objects that are a normalized edit distance of    $>=0.5$   from the training phrase ob- jects, indicating that most of the novel phrase ob- jects have signiﬁcantly different word sequences from their closest analogues in the training set. \nLearning knowledge from language Simi- larly to A TOMIC , we explore how pre-training COMET on a large language corpus affects its ability to generalize commonsense. This effect is apparent in Table  6 , with a clear improve- ment on automatic and human evaluations by the pretrained  COMET  over the randomly initialized \nThe table consists of four columns: Seed, Relation, Completion, and Plausible. Here's a breakdown of its contents:\n\n1. **Seed**: Represents the initial concept or item.\n2. **Relation**: Describes the relationship between the Seed and the Completion.\n3. **Completion**: Provides the outcome or associated concept related to the Seed.\n4. **Plausible**: Indicates whether the relationship is considered plausible (✓), not plausible (X), or uncertain (🤔).\n\nExamples include:\n- \"piece\" is a \"PartOf\" a \"machine\" and is plausible (✓).\n- \"bread\" is an \"IsA\" \"food\" and is plausible (✓).\n- \"puzzle\" is \"AtLocation\" \"your mind\" with uncertain plausibility (🤔).\n\nSeveral different types of relationships are shown, such as \"IsA,\" \"AtLocation,\" \"Causes,\" \"CapableOf,\" \"SymbolOf,\" \"HasProperty,\" \"UsedFor,\" \"HasPrerequisite,\" \"HasSubevent,\" \"MotivatedByGoal,\" \"ReceivesAction,\" \"MadeOf,\" and \"DefinedAs.\"\nmodel. Qualitatively, we observe this effect in Ta- ble  7  with the generated example tuple “mango IsA  fruit\", which is not present in the training set. The only tuple containing the “mango\" entity in the training set is “mango  UsedFor  salsa\", which is not informative enough. As conﬁrmation, we observe that the output from  COMET  (- pretrain) is “mango  IsA  spice”, which could be a reasonable inference given the information about “mango\" in the seed set of knowledge. \nRepresenting relations with language While the automatic metrics point to insigniﬁcant differ- ences when comparing models with symbol re- lations and those with natural language relations (Table  6 ), examples can provide qualitative in- sights into the beneﬁts of representing relations as language. While the only non-ornithological ref- erence to a “dove\" in the ConceptNet training set is “dove  CapableOf  ﬂy”, our model learns to generalize to produce the tuple “dove  SymbolOf purity”. The model that uses symbol relation em- beddings only manages to produce the relation “dove  SymbolOf  submarine”, which seems to relate “submarine\" to a more nautical (and unre- lated) word sense of “dove\". "}
{"page": 8, "image_path": "doc_images/P19-1470_8.jpg", "ocr_text": "6 Related Work\n\nKnowledge base construction Previous work\nhas looked at constructing knowledge bases as re-\nlational schemas using expert knowledge (Lenat,\n1995; Bodenreider, 2004; Miller, 1995), semi-\nstructured text extraction (Suchanek et al., 2007;\nHoffart et al., 2013; Auer et al., 2007; Bol-\nlacker et al., 2008) and unstructured text extraction\n(Dong et al., 2014; Carlson et al., 2010; Nakashole\net al., 2011, 2012; Niu, 2012). In our work, we fo-\ncus on construction of commonsense knowledge\nbases which require the use of open-text events\nrather than a well-defined relational schema struc-\nture. Other work in information extraction can\nalso be applied to knowledge base construction\nwith open-text entities (Soderland et al., 2010; Et-\nzioni et al., 2011; Fader et al., 2011; Mausam et al.,\n2012; Fan et al., 2010; Cui et al., 2018), but these\nmethods typically extract explicitly stated text re-\nlations. Conversely, our approach generates new\nknowledge that is often unstated in text, as com-\nmonsense information typically is (Gordon and\nVan Durme, 2013).\n\nCommonsense knowledge base completion\nExisting work on generation of novel common-\nsense knowledge has also used ConceptNet and\nATOMIC as underlying KBs. Specifically, Li et al.\n(2016) proposed a set of neural network models\nfor scoring tuples in ConceptNet. Our work differs\nfrom this approach as their models evaluate full tu-\nples rather than learning to generate the phrases to\nmake new nodes in the knowledge graph. Saito\net al. (2018) builds upon this work by proposing a\njoint model for completion and generation of com-\nmonsense tuples. Their work, however, focuses on\nusing tuple generation to augment their KB com-\npletion model, rather than to increase coverage in\ncommonsense KB construction. Finally, Sap et al.\n(2019) use LSTM encoder-decoder models to gen-\nerate commonsense knowledge about social situa-\ntions. We use transformers and investigate the ef-\nfect of using pre-trained language representations\n(Radford et al., 2018) to initialize them.\n\nTransformers and pre-training Finally, our\nwork builds on previous work on adapting pre-\ntrained language models for various sequence la-\nbeling, classification, and NLI end tasks (Rad-\nford et al., 2018; Peters et al., 2018; Devlin et al.,\n2018). Our research investigates how pre-trained\nlanguage models can be used for large-scale com-\n\nmonsense KB construction by generating new\ngraph nodes and edges between nodes.\n\n7 Conclusion\n\nWe introduce COMmonsense Transformers\n(COMET) for automatic construction of common-\nsense knowledge bases. COMET is a framework\nfor adapting the weights of language models to\nlearn to produce novel and diverse common-\nsense knowledge tuples. Empirical results on\ntwo commonsense knowledge bases, ATOMIC\nand ConceptNet, show that COMET frequently\nproduces novel commonsense knowledge that\nhuman evaluators deem to be correct. These\npositive results point to future work in extend-\ning the approach to a variety of other types of\nknowledge bases, as well as investigating whether\nCOMET can learn to produce OpenlE-style\nknowledge tuples for arbitrary knowledge seeds.\n\nAcknowledgments\n\nWe thank Thomas Wolf, Ari Holtzman, Chandra\nBhagavatula, Peter Clark, Rob Dalton, Ronan Le\nBras, Rowan Zellers and Scott Yih for helpful dis-\ncussions over the course of this project, as well as\nthe anonymous reviewers for their insightful com-\nments. This research was supported in part by\nNSF (IIS-1524371, I[S-1714566, NRI-1525251),\nDARPA under the CwC program through the ARO\n(W91LINF-15-1-0543), and Samsung Research.\nThis material is based, in part, upon work sup-\nported by the National Science Foundation Gradu-\nate Research Fellowship Program under Grant No.\nDGE- 1256082.\n\nReferences\n\nSéren Auer, Christian Bizer, Georgi Kobilarov, Jens\nLehmann, Richard Cyganiak, and Zachary G. Ives.\n2007. Dbpedia: A nucleus for a web of open data.\nIn ISWC/ASWC.\n\nJimmy Ba, Ryan Kiros, and Geoffrey E. Hinton. 2016.\nLayer normalization. CoRR, abs/1607.06450.\n\nOlivier Bodenreider. 2004. The unified medical lan-\nguage system (umls): Integrating biomedical termi-\nnology. Nucleic acids research, 32:D267—70.\n\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and\nTomas Mikolov. 2017. Enriching word vectors with\nsubword information. Transactions of the Associa-\ntion for Computational Linguistics, 5:135-146.\n\n4770\n", "vlm_text": "6 Related Work \nKnowledge base construction Previous work has looked at constructing knowledge bases as re- lational schemas using expert knowledge ( Lenat , 1995 ;  Bodenreider ,  2004 ;  Miller ,  1995 ), semi- structured text extraction ( Suchanek et al. ,  2007 ; Hoffart et al. ,  2013 ;  Auer et al. ,  2007 ;  Bol- lacker et al. ,  2008 ) and unstructured text extraction ( Dong et al. ,  2014 ;  Carlson et al. ,  2010 ;  Nakashole et al. ,  2011 ,  2012 ;  Niu ,  2012 ). In our work, we fo- cus on construction of commonsense knowledge bases which require the use of open-text events rather than a well-deﬁned relational schema struc- ture. Other work in information extraction can also be applied to knowledge base construction with open-text entities ( Soderland et al. ,  2010 ;  Et- zioni et al. ,  2011 ;  Fader et al. ,  2011 ;  Mausam et al. , 2012 ;  Fan et al. ,  2010 ;  Cui et al. ,  2018 ), but these methods typically extract explicitly stated text re- lations. Conversely, our approach generates new knowledge that is often unstated in text, as com- monsense information typically is ( Gordon and Van Durme ,  2013 ). \nCommonsense knowledge base completion Existing work on generation of novel common- sense knowledge has also used ConceptNet and A TOMIC  as underlying KBs. Speciﬁcally,  Li et al. ( 2016 ) proposed a set of neural network models for scoring tuples in ConceptNet. Our work differs from this approach as their models evaluate full tu- ples rather than learning to generate the phrases to make new nodes in the knowledge graph.  Saito et al.  ( 2018 ) builds upon this work by proposing a joint model for completion and generation of com- monsense tuples. Their work, however, focuses on using tuple generation to augment their KB com- pletion model, rather than to increase coverage in commonsense KB construction. Finally,  Sap et al. ( 2019 ) use LSTM encoder-decoder models to gen- erate commonsense knowledge about social situa- tions. We use transformers and investigate the ef- fect of using pre-trained language representations ( Radford et al. ,  2018 ) to initialize them. \nTransformers and pre-training Finally, our work builds on previous work on adapting pre- trained language models for various sequence la- beling, classiﬁcation, and NLI end tasks ( Rad- ford et al. ,  2018 ;  Peters et al. ,  2018 ;  Devlin et al. , 2018 ). Our research investigates how pre-trained language models can be used for large-scale com- monsense KB construction by generating new graph nodes and edges between nodes. \n\n7 Conclusion \nWe introduce COMmonsense Transformers ( COMET ) for automatic construction of common- sense knowledge bases.  COMET  is a framework for adapting the weights of language models to learn to produce novel and diverse common- sense knowledge tuples. Empirical results on two commonsense knowledge bases, A TOMIC and ConceptNet, show that  COMET frequently produces novel commonsense knowledge that human evaluators deem to be correct. These positive results point to future work in extend- ing the approach to a variety of other types of knowledge bases, as well as investigating whether COMET can learn to produce OpenIE-style knowledge tuples for arbitrary knowledge seeds. \nAcknowledgments \nWe thank Thomas Wolf, Ari Holtzman, Chandra Bhagavatula, Peter Clark, Rob Dalton, Ronan Le Bras, Rowan Zellers and Scott Yih for helpful dis- cussions over the course of this project, as well as the anonymous reviewers for their insightful com- ments. This research was supported in part by NSF (IIS-1524371, IIS-1714566, NRI-1525251), DARPA under the CwC program through the ARO (W911NF-15-1-0543), and Samsung Research. This material is based, in part, upon work sup- ported by the National Science Foundation Gradu- ate Research Fellowship Program under Grant No. DGE-1256082. \nReferences \nSören Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann, Richard Cyganiak, and Zachary G. Ives. 2007. Dbpedia: A nucleus for a web of open data. In  ISWC/ASWC . Jimmy Ba, Ryan Kiros, and Geoffrey E. Hinton. 2016. Layer normalization.  CoRR , abs/1607.06450. Olivier Bodenreider. 2004.  The uniﬁed medical lan- guage system (umls): Integrating biomedical termi- nology .  Nucleic acids research , 32:D267–70. Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2017. Enriching word vectors with subword information.  Transactions of the Associa- tion for Computational Linguistics , 5:135–146. "}
{"page": 9, "image_path": "doc_images/P19-1470_9.jpg", "ocr_text": "Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim\nSturge, and Jamie Taylor. 2008. Freebase: A col-\nlaboratively created graph database for structuring\nhuman knowledge. In Proceedings of the 2008\nACM SIGMOD International Conference on Man-\nagement of Data, SIGMOD ’08, pages 1247-1250,\nNew York, NY, USA. ACM.\n\nAndrew Carlson, Justin Betteridge, Bryan Kisiel, Burr\nSettles, Estevam R. Hruschka, Jr., and Tom M.\nMitchell. 2010. Toward an architecture for never-\nending language learning. In Proceedings of the\nTwenty-Fourth AAAI Conference on Artificial Intel-\nligence, AAAY 10, pages 1306-1313. AAAI Press.\n\nLei Cui, Furu Wei, and Ming Zhou. 2018. Neural open\ninformation extraction. In ACL.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv: 1810.04805.\n\nXin Dong, Evgeniy Gabrilovich, Geremy Heitz, Wilko\nHorn, Ni Lao, Kevin Murphy, Thomas Strohmann,\nShaohua Sun, and Wei Zhang. 2014. Knowledge\nvault: A web-scale approach to probabilistic knowl-\nedge fusion. In Proceedings of the 20th ACM\nSIGKDD International Conference on Knowledge\nDiscovery and Data Mining, KDD ’14, pages 601—\n610, New York, NY, USA. ACM.\n\nOren Etzioni, Anthony Fader, Janara Christensen,\nStephen Soderland, and Mausam. 2011. Open infor-\nmation extraction: The second generation. In IJCAI.\n\nAnthony Fader, Stephen Soderland, and Oren Etzioni.\n2011. Identifying relations for open information ex-\ntraction. In Proceedings of the conference on empir-\nical methods in natural language processing, pages\n1535-1545. Association for Computational Linguis-\ntics.\n\nJames Fan, David A. Ferrucci, David Gondek, and\nAditya Kalyanpur. 2010. Prismatic: Inducing\nknowledge from a large scale lexicalized relation re-\nsource. In NAACL-HLT 2010.\n\nJonathan Gordon and Benjamin Van Durme. 2013. Re-\nporting bias and knowledge acquisition. In Proceed-\nings of the 2013 workshop on Automated knowledge\nbase construction, pages 25-30. ACM.\n\nDan Hendrycks and Kevin Gimpel. 2016. Bridging\nnonlinearities and stochastic regularizers with gaus-\nsian error linear units. CoRR, abs/1606.08415.\n\nSepp Hochreiter and Jiirgen Schmidhuber. 1997. Long\nshort-term memory. Neural Computation, 9(8).\n\nJohannes Hoffart, Fabian M. Suchanek, Klaus\nBerberich, and Gerhard Weikum. 2013. Yago2: A\nspatially and temporally enhanced knowledge base\nfrom wikipedia. Artificial Intelligence, 194:28 —\n61. Artificial Intelligence, Wikipedia and Semi-\nStructured Resources.\n\nSture Holm. 1979. A simple sequentially rejective\nmultiple test procedure. Scandinavian Journal of\nStatistics, 6(2):65—70.\n\nDouglas B Lenat. 1995. Cyc: A large-scale investment\nin knowledge infrastructure. Communications of the\nACM, 38(11):33-38.\n\nOmer Levy, Minjoon Seo, Eunsol Choi, and Luke S.\nZettlemoyer. 2017. Zero-shot relation extraction via\nreading comprehension. In CoNLL.\n\nXiang Li, Aynaz Taheri, Lifu Tu, and Kevin Gimpel.\n2016. Commonsense knowledge base completion.\nIn ACL, volume 1, pages 1445-1455.\n\nMausam, Michael Schmitz, Stephen Soderland, Robert\nBart, and Oren Etzioni. 2012. Open language learn-\ning for information extraction. In EMNLP-CoNLL.\n\nGeorge A. Miller. 1995. Wordnet: A lexical database\nfor english. Commun. ACM, 38(11):39-41.\n\nNdapandula Nakashole, Martin Theobald, and Gerhard\nWeikum. 2011. Scalable knowledge harvesting with\nhigh precision and high recall. In Proceedings of\nthe Fourth ACM International Conference on Web\nSearch and Data Mining, WSDM 11, pages 227-\n236, New York, NY, USA. ACM.\n\nNdapandula Nakashole, Gerhard Weikum, and Fabian\nSuchanek. 2012. Patty: A taxonomy of relational\npatterns with semantic types. In Proceedings of\nthe 2012 Joint Conference on Empirical Methods\nin Natural Language Processing and Computational\nNatural Language Learning, pages 1135-1145. As-\nsociation for Computational Linguistics.\n\nFeng Niu. 2012. Web-scale Knowledge-base Construc-\ntion via Statistical Inference and Learning. Ph.D.\nthesis, Madison, WI, USA. AAI3524067.\n\nEric W Noreen. 1989. Computer intensive methods for\nhypothesis testing: An introduction. Wiley, NY.\n\nJeffrey Pennington, Richard Socher, and Christo-\npher D. Manning. 2014. Glove: Global vectors for\nword representation. In EMNLP.\n\nMatthew E. Peters, Mark Neumann, Mohit lyyer,\nMatthew Gardner, Christopher Clark, Kenton Lee,\nand Luke S. Zettlemoyer. 2018. Deep contextual-\nized word representations. CoRR, abs/1802.05365.\n\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training. URL https://s3-\nus-west-2. amazonaws. com/openai-assets/research-\ncovers/languageunsupervised/language under-\nstanding paper. pdf.\n\nItsumi Saito, Kyosuke Nishida, Hisako Asano, and\nJunji Tomita. 2018. Commonsense knowledge base\ncompletion and generation. In Proceedings of the\n22nd Conference on Computational Natural Lan-\nguage Learning, pages 141-150.\n\n4771\n", "vlm_text": "Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. 2008.  Freebase: A col- laboratively created graph database for structuring human knowledge . In  Proceedings of the 2008 ACM SIGMOD International Conference on Man- agement of Data , SIGMOD ’08, pages 1247–1250, New York, NY, USA. ACM. Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr Settles, Estevam R. Hruschka, Jr., and Tom M. Mitchell. 2010.  Toward an architecture for never- ending language learning . In  Proceedings of the Twenty-Fourth AAAI Conference on Artiﬁcial Intel- ligence , AAAI’10, pages 1306–1313. AAAI Press. Lei Cui, Furu Wei, and Ming Zhou. 2018. Neural open information extraction. In  ACL . Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understand- ing.  arXiv preprint arXiv:1810.04805 . Xin Dong, Evgeniy Gabrilovich, Geremy Heitz, Wilko Horn, Ni Lao, Kevin Murphy, Thomas Strohmann, Shaohua Sun, and Wei Zhang. 2014. Knowledge vault: A web-scale approach to probabilistic knowl- edge fusion . In  Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , KDD ’14, pages 601– 610, New York, NY, USA. ACM. Oren Etzioni, Anthony Fader, Janara Christensen, Stephen Soderland, and Mausam. 2011. Open infor- mation extraction: The second generation. In  IJCAI . Anthony Fader, Stephen Soderland, and Oren Etzioni. 2011. Identifying relations for open information ex- traction. In  Proceedings of the conference on empir- ical methods in natural language processing , pages 1535–1545. Association for Computational Linguis- tics. James Fan, David A. Ferrucci, David Gondek, and Aditya Kalyanpur. 2010. Prismatic: Inducing knowledge from a large scale lexicalized relation re- source. In  NAACL-HLT 2010 . Jonathan Gordon and Benjamin Van Durme. 2013. Re- porting bias and knowledge acquisition. In  Proceed- ings of the 2013 workshop on Automated knowledge base construction , pages 25–30. ACM. Dan Hendrycks and Kevin Gimpel. 2016. Bridging nonlinearities and stochastic regularizers with gaus- sian error linear units.  CoRR , abs/1606.08415. Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long short-term memory.  Neural Computation , 9(8). Johannes Hoffart, Fabian M. Suchanek, Klaus Berberich, and Gerhard Weikum. 2013.  Yago2: A spatially and temporally enhanced knowledge base from wikipedia . Artiﬁcial Intelligence , 194:28 – 61. Artiﬁcial Intelligence, Wikipedia and Semi- Structured Resources. \nSture Holm. 1979. A simple sequentially rejective multiple test procedure. Scandinavian Journal of Statistics , 6(2):65–70. Douglas B Lenat. 1995. Cyc: A large-scale investment in knowledge infrastructure.  Communications of the ACM , 38(11):33–38. Omer Levy, Minjoon Seo, Eunsol Choi, and Luke S. Zettlemoyer. 2017. Zero-shot relation extraction via reading comprehension. In  CoNLL . Xiang Li, Aynaz Taheri, Lifu Tu, and Kevin Gimpel. 2016. Commonsense knowledge base completion. In  ACL , volume 1, pages 1445–1455. Mausam, Michael Schmitz, Stephen Soderland, Robert Bart, and Oren Etzioni. 2012. Open language learn- ing for information extraction. In  EMNLP-CoNLL . George A. Miller. 1995.  Wordnet: A lexical database for english .  Commun. ACM , 38(11):39–41. Ndapandula Nakashole, Martin Theobald, and Gerhard Weikum. 2011.  Scalable knowledge harvesting with high precision and high recall . In  Proceedings of the Fourth ACM International Conference on Web Search and Data Mining , WSDM ’11, pages 227– 236, New York, NY, USA. ACM. Ndapandula Nakashole, Gerhard Weikum, and Fabian Suchanek. 2012.  Patty: A taxonomy of relational patterns with semantic types . In  Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning , pages 1135–1145. As- sociation for Computational Linguistics. Feng Niu. 2012.  Web-scale Knowledge-base Construc- tion via Statistical Inference and Learning . Ph.D. thesis, Madison, WI, USA. AAI3524067. Eric W Noreen. 1989.  Computer intensive methods for hypothesis testing: An introduction.  Wiley, NY. Jeffrey Pennington, Richard Socher, and Christo- pher D. Manning. 2014. Glove: Global vectors for word representation. In  EMNLP . Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matthew Gardner, Christopher Clark, Kenton Lee, and Luke S. Zettlemoyer. 2018. Deep contextual- ized word representations.  CoRR , abs/1802.05365. Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language under- standing by generative pre-training.  URL https://s3- us-west-2. amazonaws. com/openai-assets/research- covers/language unsupervised/language under- standing paper. pdf . Itsumi Saito, Kyosuke Nishida, Hisako Asano, and Junji Tomita. 2018. Commonsense knowledge base completion and generation. In  Proceedings of the 22nd Conference on Computational Natural Lan- guage Learning , pages 141–150. "}
{"page": 10, "image_path": "doc_images/P19-1470_10.jpg", "ocr_text": "Maarten Sap, Ronan LeBras, Emily Allaway, Chan-\ndra Bhagavatula, Nicholas Lourie, Hannah Rashkin,\nBrendan Roof, Noah A Smith, and Yejin Choi. 2019.\nAtomic: An atlas of machine commonsense for if-\nthen reasoning. In AAAI.\n\nStephen Soderland, Brendan Roof, Bo Qin, Shi Xu,\nMausam, and Oren Etzioni. 2010. Adapting open\ninformation extraction to domain-specific relations.\nAl Magazine, 31:93-102.\n\nRobyn Speer, Joshua Chin, and Catherine Havasi.\n2017. Conceptnet 5.5: An open multilingual graph\nof general knowledge. In Thirty-First AAAI Confer-\nence on Artificial Intelligence.\n\nFabian M. Suchanek, Gjergji Kasneci, and Gerhard\nWeikum. 2007. Yago: A core of semantic knowl-\nedge. In Proceedings of the 16th International Con-\nference on World Wide Web, WWW ’07, pages 697—\n706, New York, NY, USA. ACM.\n\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.\nSequence to sequence learning with neural net-\nworks. In Advances in Neural Information Process-\ning Systems.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In NIPS.\n\n4772\n", "vlm_text": "Maarten Sap, Ronan LeBras, Emily Allaway, Chan- dra Bhagavatula, Nicholas Lourie, Hannah Rashkin, Brendan Roof, Noah A Smith, and Yejin Choi. 2019. Atomic: An atlas of machine commonsense for if- then reasoning. In  AAAI . Stephen Soderland, Brendan Roof, Bo Qin, Shi Xu, Mausam, and Oren Etzioni. 2010. Adapting open information extraction to domain-speciﬁc relations. AI Magazine , 31:93–102. Robyn Speer, Joshua Chin, and Catherine Havasi. 2017. Conceptnet 5.5: An open multilingual graph of general knowledge. In  Thirty-First AAAI Confer- ence on Artiﬁcial Intelligence . Fabian M. Suchanek, Gjergji Kasneci, and Gerhard Weikum. 2007.  Yago: A core of semantic knowl- edge . In  Proceedings of the 16th International Con- ference on World Wide Web , WWW ’07, pages 697– 706, New York, NY, USA. ACM. Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to sequence learning with neural net- works. In  Advances in Neural Information Process- ing Systems . Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In  NIPS . "}
{"page": 11, "image_path": "doc_images/P19-1470_11.jpg", "ocr_text": "A Additional Training Details\n\nA.1_ Training Hyperparameters\n\nATOMIC For ATOMIC, we use a maximum\nlearning rate of 6.25e-5 with a warmup period\nof 100 minibatches. After, we decay the learn-\ning rate linearly until the end of training. We\ntrain for 50k minibatches and use early stopping.\nWe clip gradients when their norm is greater than\n1. The remainder of our hyperparameters are the\nsame as in Radford et al. (2018). We use the\npublic HuggingFace implementation of the GPT\nmodel as a base for our experiments available\nat: https: //github.com/huggingface/\npytorch-openai-transformer-I1nm.\n\nConceptNet For ConceptNet, we use a maxi-\nmum learning rate of le-5 and a warm-up period\nof 200 minibatches. The learning rate is decayed\nlinearly until the end of training, which lasts for\n100k minibatches. All other hyperparameters are\nthe same as for training on the ATOMIC corpus.\n\nA.2 ConceptNet baseline\n\nWe train the ConceptNet baseline with a learning\nrate of le-4 for 100k minibatches. Early stopping\nis used with the validation loss. Similarly to Saito\net al. (2018), we use 200-dimension hidden states\nand 200-dimensional word embeddings. We use a\nsingle-layer bidirectional LSTM (Hochreiter and\nSchmidhuber, 1997) to encode the first phrase and\na single-layer unidirectional LSTM to decode the\ntarget phrase. Relation embeddings are concate-\nnated with the word embeddings of the decoder\nbefore being input to the decoder LSTM. We set\nthe dropout rate to 0.2 before the output projection\nlayer and after the word embedding layers. We\noutline the following differences between our re-\nimplementation of the model of Saito et al. (2018)\nand their original implementation and the reason\nfor the change.\n\n1. We use Glove (Pennington et al., 2014) em-\nbeddings rather than fastText embeddings\n(Bojanowski et al., 2017) to initialize word\nembeddings. Because the model indicated\nthat 200-dimensional word embeddings were\nused, we could not use the pretrained em-\nbeddings provided by the fastText group!.\nIn Saito et al. (2018), the authors de-\nscribed training their fastText embeddings on\n\n‘https://fasttext.cc/\n\nWikipedia. With no reference to the precise\ncorpus used, we opted to use Glove embed-\ndings to initialize the word embeddings of the\nencoder and decoder instead.\n\nN\n\n. We use the Adam optimizer with learning\nrate of 0.0001, rather than SGD with a learn-\ning rate of 1.0 because after training both\nmodels, we found that the Adam-trained\nmodel performed better on development set\nperplexity. We also do not use weight de-\ncay, as this seemed to lower validation per-\nformance, as well.\n\nww\n\n. We do not train the generation model jointly\nwith the completion model. We only train\nan individual generator. The results of Saito\net al. (2018) did not show a significant differ-\nence in generation performance between the\ntwo on the ConceptNet dataset.\n\n4. We train a second baseline (LSTM - s) that\ndoes not learn to produce relations in both di-\nrections (i.e., sr — o and or — s). Instead if\nonly learns parameters that can produce rela-\ntions in the forward direction (sr — 0)\n\n5. We do not decay the learning rate because it\nwas unclear from the original paper what the\nexact learning rate schedule was.\n\nB_ Additional Evaluation Details\n\nB.1 Human Evaluations\n\nWe used Amazon Mechanical Turk to get ratings\nof model output accuracy. We selected seed con-\ncepts and relations from the test set and generated\ncompletions using each model to create (s,7r, 0)\ntuples. For ATOMIC, we selected tuples by choos-\ning all possible relations (9) for each of 100 ran-\ndomly selected seed concepts (900 total (s,7)\npairs) following the procedure from Sap et al.\n(2019). For ConceptNet, we used the full test set\n(1200 total (s, 7) pairs).\n\nFor Beam-2/5/10 and top-5/10 sampling gener-\nations, we used the model to generate 2, 5, or 10\n(respectively) possible completions (0) per (s, 1)\npair. Workers were shown the full set and asked\nto select all of the o that are valid completions for\nthe (s,7) pair. Each set of tuples was rated by 5\nworkers.\n\nFor greedy sampling generations, we used the\nmodel to generate one possible completion (0) per\n\n4773\n", "vlm_text": "A Additional Training Details \nA.1 Training Hyperparameters \nA TOMIC For A TOMIC , we use a maximum learning rate of 6.25e-5 with a warmup period of 100 minibatches. After, we decay the learn- ing rate linearly until the end of training. We train for   $50\\mathrm{k}$   minibatches and use early stopping. We clip gradients when their norm is greater than 1. The remainder of our hyperparameters are the same as in  Radford et al.  ( 2018 ). We use the public HuggingFace implementation of the GPT model as a base for our experiments available at:  https://github.com/huggingface/ pytorch-openai-transformer-lm . \nConceptNet For ConceptNet, we use a maxi- mum learning rate of 1e-5 and a warm-up period of 200 minibatches. The learning rate is decayed linearly until the end of training, which lasts for 100k minibatches. All other hyperparameters are the same as for training on the A TOMIC  corpus. \nA.2 ConceptNet baseline \nWe train the ConceptNet baseline with a learning rate of 1e-4 for  $100\\mathbf{k}$   minibatches. Early stopping is used with the validation loss. Similarly to  Saito et al.  ( 2018 ), we use 200-dimension hidden states and 200-dimensional word embeddings. We use a single-layer bidirectional LSTM ( Hochreiter and Schmidhuber ,  1997 ) to encode the ﬁrst phrase and a single-layer unidirectional LSTM to decode the target phrase. Relation embeddings are concate- nated with the word embeddings of the decoder before being input to the decoder LSTM. We set the dropout rate to 0.2 before the output projection layer and after the word embedding layers. We outline the following differences between our re- implementation of the model of  Saito et al.  ( 2018 ) and their original implementation and the reason for the change. \n1. We use Glove ( Pennington et al. ,  2014 ) em- beddings rather than fastText embeddings ( Bojanowski et al. ,  2017 ) to initialize word embeddings. Because the model indicated that 200-dimensional word embeddings were used, we could not use the pretrained em- beddings provided by the fastText group 1 . In  Saito et al.  ( 2018 ), the authors de- scribed training their fastText embeddings on \nWikipedia. With no reference to the precise corpus used, we opted to use Glove embed- dings to initialize the word embeddings of the encoder and decoder instead. \n2. We use the Adam optimizer with learning rate of 0.0001, rather than SGD with a learn- ing rate of 1.0 because after training both models, we found that the Adam-trained model performed better on development set perplexity. We also do not use weight de- cay, as this seemed to lower validation per- formance, as well. \n3. We do not train the generation model jointly with the completion model. We only train an individual generator. The results of  Saito et al.  ( 2018 ) did not show a signiﬁcant differ- ence in generation performance between the two on the ConceptNet dataset. \n4. We train a second baseline   $(\\operatorname{LSTM}\\cdot\\,s)$   that does not learn to produce relations in both di- rections (i.e.,    $s r\\rightarrow o$   and  $o r\\rightarrow s$  ). Instead if only learns parameters that can produce rela- tions in the forward direction (  $\\acute{\\left(s r\\rightarrow o\\right)}$  ) \n5. We do not decay the learning rate because it was unclear from the original paper what the exact learning rate schedule was. \nB Additional Evaluation Details \nB.1 Human Evaluations \nWe used Amazon Mechanical Turk to get ratings of model output accuracy. We selected seed con- cepts and relations from the test set and generated completions using each model to create    $(s,r,o)$  tuples. For A TOMIC , we selected tuples by choos- ing all possible relations (9) for each of 100 ran- domly selected seed concepts (900 total    $(s,r)$  pairs) following the procedure from  Sap et al.\n\n ( 2019 ). For ConceptNet, we used the full test set\n\n (1200 total    $(s,r)$   pairs). \nFor Beam-2/5/10 and top-5/10 sampling gener- ations, we used the model to generate 2, 5, or 10 (respectively) possible completions   $(o)$   per    $(s,r)$  pair. Workers were shown the full set and asked to select all of the    $o$   that are valid completions for the    $(s,r)$   pair. Each set of tuples was rated by 5 workers. \nFor greedy sampling generations, we used the model to generate one possible completion   $(o)$   per  $(s,r)$   pair. Workers were shown the completed tu- ple    $(s,r,o)$   and asked whether it is valid or not. Each tuple was rated by 5 workers. "}
{"page": 12, "image_path": "doc_images/P19-1470_12.jpg", "ocr_text": "(s,r) pair. Workers were shown the completed tu-\nple (s,r,0) and asked whether it is valid or not.\nEach tuple was rated by 5 workers.\n\nWe measure accuracy as the percentage of dis-\ntinct worker responses where the (s, 7,0) tuple is\nmarked as valid (i.e., PGP\n\nC Example Outputs\n\nAdditional examples can be seen in Figures 5,\n6, and 7 that are produced using the demo at\nhttps://mosaickg.apps.allenai.\norg.\n\nD_ Additional Training Experiments\n\nIn addition to the more naive setups for knowl-\nedge graph completion, we explore various multi-\ntask and hierarchical learning setups on top of the\ntaxonomy of commonsense relations given by Sap\net al. (2019), which group together along vari-\nous axes (e.g., related to agent/theme, related to\ncauses/effects, etc.).\n\nD.1 Multi-relation Training\n\nFor the ATOMIC corpus, we experiment with mul-\ntiple multi-task training setups, similar to Sap et al.\n(2019). First, we train an individual model for\neach relation type (OReact, oEffect, etc.),\nwhich we denote as COMET - 9LM in the Table 9.\nWe also experiment with various information-\nsharing dataset configurations that organize differ-\nent relations across common dimensions. We out-\nline these dimensions and the makeup of each split\nin Table 9. For ConceptNet, all models are always\ntrained on all relation types jointly. Results on\nautomatic evaluation metrics are provided in Ta-\nble 11. Because there did not seem to be signif-\nicant differences between these performances and\nthat of COMET - FULL, we did not run additional\nexperiments on these ablations.\n\nD.2 Concept Hierarchy Training\n\nLeveraging the prior knowledge that certain re-\nlation types in the ATOMIC knowledge graph\nare linked to each other, we explore provid-\ning these group identities as additional tokens\nin the relation. For example, when generating\nthe completion of a xReact relation, the model\nwould receive as input the following meta-tokens:\n<xReact>, <X>, <POST>, <Involuntary>\n— thereby providing common context with other\nrelations that are part of the same groupings (e.g.,\n\ngenerating a phrase for a xWant relation would\nreceive the <X> and <POST> tokens as input,\nbut not <Involuntary>). Depending on the\nrelation for a particular training example (e.g.,\nxReact), a set of meta-tokens are appended to\nhe relation tokens, X\", that provide hierarchi-\ncal relational information, allowing the model to\nshare information across relation types. We pro-\nvide a more in-depth description of the category\nhierarchy training combinations in Table 10. Re-\nsults on human evaluation metrics are provided in\nTable 12. Because the model with the hierarchi-\ncal meta-tokens performed worse than the regular\nCOMET, we did not run additional experiments on\nhis ablations.\n\n4774\n", "vlm_text": "\nWe measure accuracy as the percentage of dis- tinct worker responses where the    $(s,r,o)$   tuple is marked as valid (i.e.,  $\\frac{\\#v a l i d}{5\\cdot|\\left(s,r,o\\right)|})$  ). ·| | \nC Example Outputs \nAdditional examples can be seen in Figures  5 , 6 , and  7  that are produced using the demo at https://mosaickg.apps.allenai. org . \nD Additional Training Experiments \nIn addition to the more naive setups for knowl- edge graph completion, we explore various multi- task and hierarchical learning setups on top of the taxonomy of commonsense relations given by  Sap et al.  ( 2019 ), which group together along vari- ous axes (e.g., related to agent/theme, related to causes/effects, etc.). \ngenerating a phrase for a  xWant  relation would receive the    $<\\!\\mathrm{X}\\!>$   and    $\\scriptstyle<\\operatorname{POST>}$   tokens as input, but not  <Involuntary  $>$  ). Depending on the relation for a particular training example (e.g., xReact ), a set of meta-tokens are appended to the relation tokens,    $X^{r}$  , that provide hierarchi- cal relational information, allowing the model to share information across relation types. We pro- vide a more in-depth description of the category hierarchy training combinations in Table  10 . Re- sults on human evaluation metrics are provided in Table  12 . Because the model with the hierarchi- cal meta-tokens performed worse than the regular COMET , we did not run additional experiments on this ablations. \nD.1 Multi-relation Training \nFor the A TOMIC  corpus, we experiment with mul- tiple multi-task training setups, similar to  Sap et al. ( 2019 ). First, we train an individual model for each relation type ( oReact, oEffect , etc.), which we denote as  COMET  - 9LM in the Table  9 . We also experiment with various information- sharing dataset conﬁgurations that organize differ- ent relations across common dimensions. We out- line these dimensions and the makeup of each split in Table  9 . For ConceptNet, all models are always trained on all relation types jointly. Results on automatic evaluation metrics are provided in Ta- ble  11 . Because there did not seem to be signif- icant differences between these performances and that of  COMET  - F ULL , we did not run additional experiments on these ablations. \nD.2 Concept Hierarchy Training \nLeveraging the prior knowledge that certain re- lation types in the A TOMIC  knowledge graph are linked to each other, we explore provid- ing these group identities as additional tokens in the relation. For example, when generating the completion of a  xReact  relation, the model would receive as input the following meta-tokens:\n\n <xReact> ,    $<\\!\\mathrm{X}\\!>$  ,    $\\scriptstyle<\\operatorname{POST>}$  ,  <Involuntary>\n\n – thereby providing common context with other relations that are part of the same groupings (e.g., "}
{"page": 13, "image_path": "doc_images/P19-1470_13.jpg", "ocr_text": "Because PersonX wanted\n\nCauses for PersonX\n\nBefore, PersonX needed\n\nAttributes of PersonX\n\nPersonX is seen as\n\nAs a result, PersonX feels\n\nEffects on PersonX\n\nAs aresult, PersonX wants\n\nto be helpful\n\nto be a leader\n\nto inform\n\nto help persony\n\nto be a good friend\n\nto be with persony\nto be a leader\n\nto be a teacher\n\nto know persony\nnone\n\nhelpful\nsmart\nconfident\nleader\ninformative\n\nhelpful\ngood\nhappy\nsatisfied\nrelieved\n\nto be a leader\nto make sure they understand\n\nPersonX gives PersonyY a pep talk\n\nPersonX then\n\nAs a result, others feel\n\nEffects on others\n\nAs aresult, others want\n\nOthers then\n\nto make persony understand\nto give persony a lecture\nto make sure persony understands\n\ngets yelled at\n\ngets tired\n\nnone\n\npersonx gets yelled at\npersonx is listened to\n\ngrateful\nnone\ninformed\ngood\nannoyed\n\nto thank personx\nto listen to personx\nto listen\n\nto get better\n\nnone\n\nlistens to personx\nlearns something new\nlearns something\nnone\n\nlistens\n\nFigure 5: Example outputs for the event \"PersonX gives PersonY a pep talk\" from COMET trained on the ATOMIC\n\nknowledge graph\n\n4775\n", "vlm_text": "The image is a diagram showing potential outcomes and attributes related to the event \"PersonX gives PersonY a pep talk,\" based on the COMET model trained on the ATOMIC knowledge graph. The diagram is structured as follows:\n\n1. **Causes for PersonX:**\n   - Because PersonX wanted:\n     - To be helpful\n     - To be a leader\n     - To inform\n     - To help PersonY\n     - To be a good friend\n   - Before, PersonX needed:\n     - To be with PersonY\n     - To be a leader\n     - To be a teacher\n     - To know PersonY\n     - None\n\n2. **Attributes of PersonX:**\n   - PersonX is seen as:\n     - Helpful\n     - Smart\n     - Confident\n     - Leader\n     - Informative\n\n3. **Effects on PersonX:**\n   - As a result, PersonX feels:\n     - Helpful\n     - Good\n     - Happy\n     - Satisfied\n     - Relieved\n   - As a result, PersonX wants:\n     - To be a leader\n     - To make sure they understand\n     - To make PersonY understand\n     - To give PersonY a lecture\n     - To make sure PersonY understands\n   - PersonX then:\n     - Gets yelled at\n     - Gets tired\n     - None\n     - PersonX gets yelled at\n     - PersonX is listened to\n\n4. **Effects on Others:**\n   - As a result, others feel:\n     - Grateful\n     - None\n     - Informed\n     - Good\n     - Annoyed\n   - As a result, others want:\n     - To thank PersonX\n     - To listen to PersonX\n     - To listen\n     - To get better\n     - None\n   - Others then:\n     - Listen to PersonX\n     - Learn something new\n     - Learn something\n     - None\n     - Listen\n\nThis diagram illustrates the potential causes, perceptions, effects, and actions resulting from the event of giving a pep talk."}
{"page": 14, "image_path": "doc_images/P19-1470_14.jpg", "ocr_text": "to be entertained\nto have fun\n\nto watch a movie\nto see a movie\nentertainment\n\nBecause PersonX wanted\n\nCauses for PersonX\n\nto have money\n\nto get tickets\n\nto go to the theater\nnone\n\nmoney\n\nBefore, PersonX needed\n\ncurious\nbored\ninterested\nexcited\nfun\n\nAttributes of PersonX PersonX is seen as\n\nhappy\nentertained\nexcited\nsatisfied\nrelaxed\n\nAs a result, PersonX feels\n\nbuy a ticket\n\ngo to the theater\ngo to theater\n\nto buy a ticket\n\nto go to the theater\n\nEffects on PersonX As a result, PersonX wants\n\nEric wants to see a movie\n\nbuys a ticket\n\ngoes to theater\ngoes to the theater\ngets bored\n\nnone\n\nPersonX then\n\nnone\nhappy\nentertained\nexcited\nsatisfied\n\nAs a result, others feel\n\nnone\n\nto have fun\n\nto go home\n\nto watch the movie\nto go to the movie\n\nEffects on others As a result, others want\n\nnone\n\nreviews the movie\nthey watch the movi\nreviews movie\n\nthey go to the theat«\n\nOthers then\n\nFigure 6: Example outputs for the event \"Eric wants to see a movie\" from COMET trained on the ATOMIC knowl-\nedge graph. COMET is able to generalize beyond the templates of the ATOMIC knowledge graph (i.e., PersonX)\nand can be used directly with names.\n\n4776\n", "vlm_text": "The image is a diagram representing various inferred outcomes and states related to the event \"Eric wants to see a movie.\" This diagram is created using the COMET model trained on the ATOMIC knowledge graph. It illustrates different causal and associative relationships stemming from the event. The categories include:\n\n1. **Causes for PersonX**: \n   - Why PersonX (Eric) wanted to see a movie (e.g., to be entertained, to have fun).\n   - What PersonX needed beforehand (e.g., to have money, to get tickets).\n\n2. **Attributes of PersonX**:\n   - How PersonX is perceived by others (e.g., curious, bored, interested).\n\n3. **Effects on PersonX**:\n   - How PersonX feels as a result (e.g., happy, entertained, excited).\n   - What PersonX might want to do as a result (e.g., buy a ticket, go to the theater).\n   - Immediate actions PersonX might take (e.g., buys a ticket, goes to the theater).\n\n4. **Effects on Others**:\n   - How others feel as a result (e.g., happy, entertained, excited).\n   - What others might want as a result (e.g., to have fun, to go home).\n   - Immediate actions others might take (e.g., reviews the movie, goes to the theater).\n\nThe diagram demonstrates COMET's ability to generalize and predict potential outcomes and attributes related to a specific event by applying the ATOMIC knowledge graph's structure."}
{"page": 15, "image_path": "doc_images/P19-1470_15.jpg", "ocr_text": "to borrow a car\n\nto have transportatio\nto have a ride\n\nto go to the store\n\nto go to a party\n\nBecause PersonX wanted\n\nCauses for PersonX\n\nto have a car\n\nto find a car\n\nnone\n\nto ask her permissior\nto find her car\n\nBefore, PersonX needed\n\nneedy\nhopeful\ndependent\ncurious\ndesperate\n\nAttributes of PersonX PersonX is seen as\n\nrelieved\ngrateful\n\nhappy\nsatisfied\nthankful\n\nAs a result, PersonX feels\n\nto borrow a car\n\nto drive to the store\nto get a ride\n\nto go to the store\nto ask for a ride\n\nEffects on PersonX Asa result, PersonX wants\n\nTom asked Jessica if he could use her car\n\ngets rejected\nnone\n\ngets yelled at\ngets denied\ngets a ride\n\nPersonX then\n\nnone\nhelpful\ngrateful\nhappy\nflattered\n\nAs a result, others feel\n\nto say no\n\nto help him\n\nto get their car back\nnone\n\nto go to the store\n\nEffects on others As a result, others want\n\nnone\nsays no\n\nno effect\n\ngets into trouble\nsays yes\n\nOthers then\n\nFigure 7: Example outputs for the event \"Tom asked Jessica if he could use her car\" from COMET trained on the\nATOMIC knowledge graph\n\n4777\n", "vlm_text": "The image is a knowledge graph depicting various inferred outcomes and perceptions related to the event \"Tom asked Jessica if he could use her car.\" It is generated by COMET, trained on the ATOMIC knowledge graph, and is structured to show nodes branching into different categories:\n\n1. **Causes for PersonX (Tom)**:\n   - Because PersonX wanted: to borrow a car, to have transportation, to have a ride, to go to the store, to go to a party.\n   - Before, PersonX needed: to have a car, to find a car, none, to ask her permission, to find her car.\n\n2. **Attributes of PersonX**:\n   - PersonX is seen as: needy, hopeful, dependent, curious, desperate.\n\n3. **Effects on PersonX**:\n   - As a result, PersonX feels: relieved, grateful, happy, satisfied, thankful.\n   - As a result, PersonX wants: to borrow a car, to drive to the store, to get a ride, to go to the store, to ask for a ride.\n   - PersonX then: gets rejected, none, gets yelled at, gets denied, gets a ride.\n\n4. **Effects on others**:\n   - As a result, others feel: none, helpful, grateful, happy, flattered.\n   - As a result, others want: to say no, to help him, to get their car back, none, to go to the store.\n   - Others then: none, says no, no effect, gets into trouble, says yes.\n\nEach of these relationships explores different potential motivations, perceptions, emotional reactions, and subsequent actions stemming from the event where Tom asks Jessica if he can use her car."}
{"page": 16, "image_path": "doc_images/P19-1470_16.jpg", "ocr_text": "Event Description Example Completion:\n\nPerson X puts Person X’s trust in Person Y\n\noEffect The effect the event has on others be- _ is considered trustworthy\nsides Person X is believed\ngains Person X’s loyalty\n\noReact The reaction of others besides Person __ trusted\nX to the event honored\ntrustworthy\noWant What others besides Person X may work with Person X\nwant to do after the event partner with Person X\nto help Person X\nxAttr How Person X might be described faithful\ngiven their part in the event hopeful\ntrusting\nxEffect The effect that the event would have _ gets relieved\non Person X stays faithful\nIs betrayed\nxIntent The reason why X would cause the _ to be trusting\nevent his or her help/guidance/advice\nto be friends\nxNeed What Person X might need to do be- __ to be friends with Person Y\nfore the event to have heard a lot of good things about Per-\nson Y\n\nto get to know Person Y\n\nxReact The reaction that Person X would _ trusting\nhave to the event safe, not alone\nunderstood\nxWant What Person X may want to do after _ to rely on Person Y\nthe event to go into business with Person Y\n\nto make sure that their heart feeling is right\n\nTable 8: Definitions of the relations in ATOMIC. Events in ATOMIC center around the personal situations of a\ncentral figure, Person X, with potentially more participants.\n\nOrganization Description Relations\nPERSON The training set is split into relations JT; ={xAttr, xEffect, xIntent,\nX/Y for the subjects of the event (Person X) xNeed, xReact, xWant}\nand relations for other participants in JT) = {oEffect, oReact, oWant}\nthe event\n\nPRE/POST Event preconditions are jointly trained JT) ={xIntent, xNeed}\n(i.e., intentions, needs). Event postcon- T2 = {oEffect, oReact, oWant,\nditions are jointly trained. xEffect, xReact, xWant}\n\n(IN)VOLUN _ Involuntary relations are trained jointly, 7) ={oWant, xIntent, xNeed, xWant}\nsuch as reactions and effects. Volun- T2 ={oEffect, oReact, xAttr,\ntary relations are trained jointly, suchas xEffect, xReact}\nneeds, wants, and intents.\n\nFULL The training set is made up of all relae 7) ={oEffect, oReact, oWant, xAttr,\ntions and the model is trained jointlyon xEffect, xIntent, xNeed, xReact,\nall of them xWant}\n\nTable 9: Multi-relation training setups. Following Sap et al. (2019), the xAt tr relation is not included in the\nPRE/POST training configuration\n\n4778\n", "vlm_text": "This table outlines different types of events and their descriptions, along with example completions related to a scenario involving trust. Here are the elements:\n\n1. **oEffect**: How others besides Person X view Person Y when Person X puts trust in them. Examples include being considered trustworthy.\n\n2. **oReact**: Others' reactions to the event. Examples are feeling trusted and honored.\n\n3. **oWant**: What others might want to do after the event. Examples include working with or helping Person X.\n\n4. **xAttr**: How Person X might be described due to their actions. Examples are faithful and trusting.\n\n5. **xEffect**: The impact on Person X. Examples include relief and staying faithful.\n\n6. **xIntent**: Why Person X causes the event. Examples are to be trusting or to seek guidance.\n\n7. **xNeed**: What Person X might need before the event. Examples are friendship with Person Y or hearing positive things about them.\n\n8. **xReact**: Person X's reaction to the event. Examples are trusting and feeling safe.\n\n9. **xWant**: What Person X might want to do after the event. Examples include relying on or doing business with Person Y.\nThis table appears to organize and describe different ways of structuring a training set for a machine learning model, likely in the domain of understanding events or actions and the associated relations or contexts. The table consists of three columns: \n\n1. **Organization**: This column lists the different organizational schemes of the training data, including \"Person X/Y,\" \"Pre/Post,\" \"(In)Volun,\" and \"Full.\"\n\n2. **Description**: This column provides a brief explanation of each organizational scheme. For example:\n   - \"Person X/Y\" involves splitting the training set into relations for subjects of the event (Person X) and other participants (Person Y).\n   - \"Pre/Post\" combines joint training of event preconditions and postconditions.\n   - \"(In)Volun\" differentiates between involuntary and voluntary relations, with the former focusing on reactions and effects and the latter on needs, wants, and intents.\n   - \"Full\" is a comprehensive approach where the training set includes all relations, and the model is jointly trained on them.\n\n3. **Relations**: This column lists specific types of relationships that are part of each organizational scheme. \n   - The notations like `xAttr`, `xEffect`, `xIntent`, etc., likely represent different types of relations or factors considered for person X and others. \"x\" might denote the subject or actor involved directly in the event, while \"o\" could represent other participants.\n   - Each scheme has a combination of these relations divided into two sets, T1 and T2, indicating different groupings or categories used in the training. \n\nIn essence, the table outlines potential ways to structure and train a model based on different relational features associated with events and their participants."}
{"page": 17, "image_path": "doc_images/P19-1470_17.jpg", "ocr_text": "Meta-Token Description Relations\n\n<X> Appended to relations that describe an xAttr, xEffect, xIntent, xNeed,\nattribute of Person X xReact, xWant\n<Y> Appended to relations that describes an oEffect, oReact, oWant\nattribute of a participant that is not Per-\nson X\n<Pre> Appended to relations that correspond xIntent, xNeed\nto pre-conditions of the event\n<Post> Appended to relations that correspond oEffect, oReact, oWant,\nto post-conditions of the event xEffect, xReact, xWant\n<Voluntary> Appended to relations that correspond oWant, xIntent, xNeed, xWant\n\nto voluntary dimensions of the situation\n\n<Involuntary> Appended to relations that correspond oEffect, oReact, xAttr,\nto involuntary dimensions of the situa- xEffect, xReact\ntion\n\nTable 10: Category hierarchy meta-tokens, along with the description and the relations to which they are appended\n\nModel PPL? BLEU-2 N/T srot N/To NiUo\nCOMET- 9LM 11.72 14.89 100.00 9.45 49.89\nCOMET- (IN) VOLUN 11.38 14.99 100.00 8.60 48.36\nCOMET- PERSONX/Y 11.30 15.21 100.00 9.12 49.59\nCOMET- Pre/Post 11.35 14.88 100.00 9.86 51.86\nCOMET- FULL - pretrain) 15.42 13.88 100.00. 7.25 45.71\nCOMET- FULL 11.14 15.10 100.00 9.71 51.20\n\nCOMET- FULL (+ hierarchy meta-tokens) 10.98 15.27 100.00 10.03 51.97\n\nTable 11: Automatic evaluations of quality and novelty for generations of ATOMIC commonsense that are trained\nwith the training set split along different relation types. The training splits are outlined in Table 9.\n\nModel l oEffect oReact oWant xAttr xEffect xIntent xNeed xReact xWant l Total\nCOMET | 29.02 37.68 44.48 57.48 55.50 68.32 64.24 76.18 75.16 | 56.45\n\nCOMET (+ hierarchy meta-tokens) 28.46 38.96 43.64 51.90 50.84 63.00 63.98 66.20 75.82 || 53.64\n\nTable 12: Human score of generations of ATOMIC commonsense for the regular COMET model and the COMET +\ncategory meta tokens\n\n4779\n", "vlm_text": "The table presented is a structured list of meta-tokens, their descriptions, and corresponding relations. Each row of the table includes:\n\n1. **Meta-Token**: This column lists the tokens which are used to modify or append to specific relations.\n   \n2. **Description**: This describes how the meta-token is used in relation to other elements, such as attributes or conditions.\n\n3. **Relations**: This lists the type of relations that the meta-token affects or is associated with. These include different categories like xAttr, xEffect, xIntent, etc.\n\nHere is a detailed breakdown of the table's content:\n\n- **<X>**: \n  - **Description**: Appended to relations that describe an attribute of Person X.\n  - **Relations**: xAttr, xEffect, xIntent, xNeed, xReact, xWant.\n\n- **<Y>**: \n  - **Description**: Appended to relations that describe an attribute of a participant that is not Person X.\n  - **Relations**: oEffect, oReact, oWant.\n\n- **<Pre>**:\n  - **Description**: Appended to relations that correspond to pre-conditions of the event.\n  - **Relations**: xIntent, xNeed.\n\n- **<Post>**:\n  - **Description**: Appended to relations that correspond to post-conditions of the event.\n  - **Relations**: oEffect, oReact, oWant, xEffect, xReact, xWant.\n\n- **<Voluntary>**:\n  - **Description**: Appended to relations that correspond to voluntary dimensions of the situation.\n  - **Relations**: oWant, xIntent, xNeed, xWant.\n\n- **<Involuntary>**:\n  - **Description**: Appended to relations that correspond to involuntary dimensions of the situation.\n  - **Relations**: oEffect, oReact, xAttr, xEffect, xReact.\n\nThe table is organized to provide a clear and concise way to understand how different meta-tokens are used to modify relations in a structured data setting, likely related to attributes and states of individuals or events in a narrative or modeling context.\nThe table presents a comparison of different models of the COMET framework, evaluating them across several metrics: PPL (Perplexity), BLEU-2, N/T sr, N/T o, and N/U o. The specific variations of the COMET model evaluated include:\n\n1. COMET-9LM\n2. COMET-(In)Volun\n3. COMET-PersonX/Y\n4. COMET-Pre/Post\n5. COMET-Full (- pretrain)\n6. COMET-Full\n7. COMET-Full (+ hierarchy meta-tokens)\n\nEach model is assessed with the following statistics:\n- PPL³ (Perplexity, a measure of how well a probability distribution predicts a sample)\n- BLEU-2 (Bilingual Evaluation Understudy, a score for evaluating the quality of text)\n- N/T sr⁴ (not expanded in the image)\n- N/T o (not expanded in the image)\n- N/U o (not expanded in the image)\n\nThe COMET-Full (+ hierarchy meta-tokens) model shows the best performance in terms of PPL and BLEU-2 compared to other model variations.\nThe table presents performance metrics of two models, COMET and COMET (+ hierarchy meta-tokens), across various categories. The categories listed are: oEffect, oReact, oWant, xAttr, xEffect, xIntent, xNeed, xReact, xWant, and Total.\n\n- For each category, the table provides numerical values representing the performance of each model.\n- Bolded numbers indicate the higher value between the two models for that specific category.\n\nHere is a summarized breakdown:\n\n1. **oEffect**:\n   - COMET: 29.02 (higher)\n   - COMET (+ hierarchy meta-tokens): 28.46\n\n2. **oReact**:\n   - COMET: 37.68\n   - COMET (+ hierarchy meta-tokens): 38.96 (higher)\n\n3. **oWant**:\n   - COMET: 44.48 (higher)\n   - COMET (+ hierarchy meta-tokens): 43.64\n\n4. **xAttr**:\n   - COMET: 57.48 (higher)\n   - COMET (+ hierarchy meta-tokens): 51.90\n\n5. **xEffect**:\n   - COMET: 55.50 (higher)\n   - COMET (+ hierarchy meta-tokens): 50.84\n\n6. **xIntent**:\n   - COMET: 68.32 (higher)\n   - COMET (+ hierarchy meta-tokens): 63.00\n\n7. **xNeed**:\n   - COMET: 64.24 (higher)\n   - COMET (+ hierarchy meta-tokens): 63.98\n\n8. **xReact**:\n   - COMET: 76.18 (higher)\n   - COMET (+ hierarchy meta-tokens): 66.20\n\n9. **xWant**:\n   - COMET: 75.16\n   - COMET (+ hierarchy meta-tokens): 75.82 (higher)\n\n10. **Total**:\n    - COMET: 56.45 (higher)\n    - COMET (+ hierarchy meta-tokens): 53.64\n\nOverall, COMET performs better in most categories, but COMET (+ hierarchy meta-tokens) performs better in oReact and xWant. The Total score is higher for COMET, indicating that, overall, it has superior performance across these categories compared to COMET (+ hierarchy meta-tokens)."}
